<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 103]
- [cs.CV](#cs.CV) [Total: 181]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.CR](#cs.CR) [Total: 7]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cs.HC](#cs.HC) [Total: 6]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.RO](#cs.RO) [Total: 6]
- [cs.SE](#cs.SE) [Total: 2]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.DL](#cs.DL) [Total: 1]
- [cs.AI](#cs.AI) [Total: 13]
- [eess.IV](#eess.IV) [Total: 2]
- [cs.LG](#cs.LG) [Total: 24]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.SD](#cs.SD) [Total: 4]
- [cs.MA](#cs.MA) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Quantum NLP models on Natural Language Inference](https://arxiv.org/abs/2510.15972)
*Ling Sun,Peter Sullivan,Michael Martin,Yun Zhou*

Main category: cs.CL

TL;DR: Quantum NLP models achieve comparable performance to classical transformers with dramatically fewer parameters, showing superior learning efficiency in few-shot NLI tasks.


<details>
  <summary>Details</summary>
Motivation: To investigate quantum NLP's potential for semantic modeling by embedding compositional structure into quantum circuits, particularly in few-shot learning scenarios.

Method: Used lambeq library and DisCoCat framework to construct parameterized quantum circuits for sentence pairs, trained for semantic relatedness and inference classification. Introduced Information Gain per Parameter (IGPP) metric and proposed cluster-based architecture for parameter sharing.

Result: Quantum models matched classical baseline performance with far fewer parameters, outperformed randomly initialized transformers in inference, achieved lower test error on relatedness tasks, and showed up to 5 orders of magnitude higher per-parameter learning efficiency.

Conclusion: QNLP shows promise for low-resource, structure-sensitive settings due to superior parameter efficiency, with cluster-based architecture improving generalization through parameter sharing.

Abstract: Quantum natural language processing (QNLP) offers a novel approach to
semantic modeling by embedding compositional structure directly into quantum
circuits. This paper investigates the application of QNLP models to the task of
Natural Language Inference (NLI), comparing quantum, hybrid, and classical
transformer-based models under a constrained few-shot setting. Using the lambeq
library and the DisCoCat framework, we construct parameterized quantum circuits
for sentence pairs and train them for both semantic relatedness and inference
classification. To assess efficiency, we introduce a novel
information-theoretic metric, Information Gain per Parameter (IGPP), which
quantifies learning dynamics independent of model size. Our results demonstrate
that quantum models achieve performance comparable to classical baselines while
operating with dramatically fewer parameters. The Quantum-based models
outperform randomly initialized transformers in inference and achieve lower
test error on relatedness tasks. Moreover, quantum models exhibit significantly
higher per-parameter learning efficiency (up to five orders of magnitude more
than classical counterparts), highlighting the promise of QNLP in low-resource,
structure-sensitive settings. To address circuit-level isolation and promote
parameter sharing, we also propose a novel cluster-based architecture that
improves generalization by tying gate parameters to learned word clusters
rather than individual tokens.

</details>


### [2] [Fusion-Augmented Large Language Models: Boosting Diagnostic Trustworthiness via Model Consensus](https://arxiv.org/abs/2510.16057)
*Md Kamrul Siam,Md Jobair Hossain Faruk,Jerry Q. Cheng,Huanying Gu*

Main category: cs.CL

TL;DR: A multi-model fusion framework using ChatGPT and Claude improves chest X-ray interpretation accuracy on CheXpert dataset, with consensus-based fusion outperforming individual models in both unimodal and multimodal settings.


<details>
  <summary>Details</summary>
Motivation: To enhance reliability of chest X-ray interpretation and reduce diagnostic errors in AI-assisted radiological diagnosis by leveraging complementary strengths of multiple LLMs through consensus-based fusion.

Method: Used two LLMs (ChatGPT and Claude) on CheXpert dataset with 224,316 chest radiographs. Evaluated unimodal performance on 234 image-only cases and multimodal performance on 50 cases with images plus synthetic clinical notes. Applied similarity-based consensus approach with 95% output similarity threshold.

Result: Unimodal: ChatGPT 62.8%, Claude 76.9%, consensus 77.6%. Multimodal: ChatGPT 84%, Claude 76%, consensus 91.3%. Consensus fusion consistently outperformed individual models across both conditions.

Conclusion: Integrating complementary modalities and using output-level consensus improves trustworthiness and clinical utility of AI-assisted radiological diagnosis with minimal computational overhead.

Abstract: This study presents a novel multi-model fusion framework leveraging two
state-of-the-art large language models (LLMs), ChatGPT and Claude, to enhance
the reliability of chest X-ray interpretation on the CheXpert dataset. From the
full CheXpert corpus of 224,316 chest radiographs, we randomly selected 234
radiologist-annotated studies to evaluate unimodal performance using image-only
prompts. In this setting, ChatGPT and Claude achieved diagnostic accuracies of
62.8% and 76.9%, respectively. A similarity-based consensus approach, using a
95% output similarity threshold, improved accuracy to 77.6%. To assess the
impact of multimodal inputs, we then generated synthetic clinical notes
following the MIMIC-CXR template and evaluated a separate subset of 50 randomly
selected cases paired with both images and synthetic text. On this multimodal
cohort, performance improved to 84% for ChatGPT and 76% for Claude, while
consensus accuracy reached 91.3%. Across both experimental conditions,
agreement-based fusion consistently outperformed individual models. These
findings highlight the utility of integrating complementary modalities and
using output-level consensus to improve the trustworthiness and clinical
utility of AI-assisted radiological diagnosis, offering a practical path to
reduce diagnostic errors with minimal computational overhead.

</details>


### [3] [Can LLMs Correct Themselves? A Benchmark of Self-Correction in LLMs](https://arxiv.org/abs/2510.16062)
*Guiyao Tie,Zenghui Yuan,Zeli Zhao,Chaoran Hu,Tianhe Gu,Ruihang Zhang,Sizhe Zhang,Junran Wu,Xiaoyue Tu,Ming Jin,Qingsong Wen,Lixing Chen,Pan Zhou,Lichao Sun*

Main category: cs.CL

TL;DR: CorrectBench is a benchmark for evaluating self-correction methods in LLMs across commonsense reasoning, math reasoning, and code generation. Self-correction improves accuracy but reduces efficiency, with simple CoT baselines showing competitive performance.


<details>
  <summary>Details</summary>
Motivation: To comprehensively evaluate self-correction methods for LLMs and determine if they can truly correct themselves, as current evaluation is limited despite various proposed methods.

Method: Developed CorrectBench benchmark to test intrinsic, external, and fine-tuned self-correction approaches across three reasoning tasks: commonsense reasoning, mathematical reasoning, and code generation.

Result: Self-correction improves accuracy especially for complex reasoning; mixing strategies provides further gains but reduces efficiency; reasoning LLMs show limited optimization with additional self-correction; simple CoT baselines are competitive in accuracy and efficiency.

Conclusion: Self-correction has potential to enhance LLM reasoning but faces efficiency challenges. Future research should focus on optimizing the balance between reasoning capabilities and operational efficiency.

Abstract: Self-correction of large language models (LLMs) emerges as a critical
component for enhancing their reasoning performance. Although various
self-correction methods have been proposed, a comprehensive evaluation of these
methods remains largely unexplored, and the question of whether LLMs can truly
correct themselves is a matter of significant interest and concern. In this
study, we introduce CorrectBench, a benchmark developed to evaluate the
effectiveness of self-correction strategies, including intrinsic, external, and
fine-tuned approaches, across three tasks: commonsense reasoning, mathematical
reasoning, and code generation. Our findings reveal that: 1) Self-correction
methods can improve accuracy, especially for complex reasoning tasks; 2) Mixing
different self-correction strategies yields further improvements, though it
reduces efficiency; 3) Reasoning LLMs (e.g., DeepSeek-R1) have limited
optimization under additional self-correction methods and have high time costs.
Interestingly, a comparatively simple chain-of-thought (CoT) baseline
demonstrates competitive accuracy and efficiency. These results underscore the
potential of self-correction to enhance LLM's reasoning performance while
highlighting the ongoing challenge of improving their efficiency. Consequently,
we advocate for further research focused on optimizing the balance between
reasoning capabilities and operational efficiency. Project Page:
https://correctbench.github.io/

</details>


### [4] [EvolveR: Self-Evolving LLM Agents through an Experience-Driven Lifecycle](https://arxiv.org/abs/2510.16079)
*Rong Wu,Xiaoman Wang,Jianbiao Mei,Pinlong Cai,Daocheng Fu,Cheng Yang,Licheng Wen,Xuemeng Yang,Yufan Shen,Yuxin Wang,Botian Shi*

Main category: cs.CL

TL;DR: EvolveR is a framework that enables LLM agents to self-improve through a closed-loop experience lifecycle with offline self-distillation and online interaction stages, achieving superior performance on complex multi-hop QA benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current LLM agents lack systematic learning from their own experiences and cannot iteratively refine problem-solving strategies, focusing mainly on external knowledge gaps rather than fundamental improvement capabilities.

Method: Two-stage closed-loop framework: (1) Offline Self-Distillation synthesizes interaction trajectories into abstract, reusable strategic principles; (2) Online Interaction retrieves distilled principles to guide decision-making and accumulate behavioral trajectories with policy reinforcement.

Result: EvolveR achieves superior performance over strong agentic baselines on complex multi-hop question-answering benchmarks.

Conclusion: The framework provides a comprehensive blueprint for agents that learn from both external data and their own actions' consequences, enabling more autonomous and continuously improving systems.

Abstract: Current Large Language Model (LLM) agents show strong performance in tool
use, but lack the crucial capability to systematically learn from their own
experiences. While existing frameworks mainly focus on mitigating external
knowledge gaps, they fail to address a more fundamental limitation: the
inability to iteratively refine problem-solving strategies. In this work, we
introduce EvolveR, a framework designed to enable agent to self-improve through
a complete, closed-loop experience lifecycle. This lifecycle comprises two key
stages: (1) Offline Self-Distillation, where the agent's interaction
trajectories are synthesized into a structured repository of abstract, reusable
strategic principles; (2) Online Interaction, where the agent interacts with
tasks and actively retrieves distilled principles to guide its decision-making,
accumulating a diverse set of behavioral trajectories. This loop employs a
policy reinforcement mechanism to iteratively update the agent based on its
performance. We demonstrate the effectiveness of EvolveR on complex multi-hop
question-answering benchmarks, where it achieves superior performance over
strong agentic baselines. Our work presents a comprehensive blueprint for
agents that learn not only from external data but also from the consequences of
their own actions, paving the way for more autonomous and continuously
improving systems. Code is available at https://github.com/Edaizi/EvolveR.

</details>


### [5] [Evaluating Prompting Strategies and Large Language Models in Systematic Literature Review Screening: Relevance and Task-Stage Classification](https://arxiv.org/abs/2510.16091)
*Binglan Han,Anuradha Mathrani,Teo Susnjak*

Main category: cs.CL

TL;DR: This study evaluates how different prompting strategies interact with LLMs to automate systematic literature review screening, finding that CoT-few-shot provides the best precision-recall balance and recommending a cost-effective staged workflow.


<details>
  <summary>Details</summary>
Motivation: To quantify how prompting strategies interact with large language models to automate the screening stage of systematic literature reviews, addressing the need for efficient and cost-effective automation in academic research.

Method: Evaluated six LLMs (GPT-4o, GPT-4o-mini, DeepSeek-Chat-V3, Gemini-2.5-Flash, Claude-3.5-Haiku, Llama-4-Maverick) under five prompt types (zero-shot, few-shot, chain-of-thought, CoT-few-shot, self-reflection) across relevance classification and six Level-2 tasks using accuracy, precision, recall, and F1 metrics.

Result: CoT-few-shot yields the most reliable precision-recall balance; zero-shot maximizes recall for high-sensitivity passes; self-reflection underperforms; GPT-4o and DeepSeek provide robust overall performance; GPT-4o-mini performs competitively at substantially lower cost; structured prompts on GPT-4o-mini offer attractive F1 at small incremental cost.

Conclusion: Recommends a staged workflow using low-cost models with structured prompts for first-pass screening and escalating borderline cases to higher-capacity models, highlighting LLMs' uneven but promising potential for literature screening automation.

Abstract: This study quantifies how prompting strategies interact with large language
models (LLMs) to automate the screening stage of systematic literature reviews
(SLRs). We evaluate six LLMs (GPT-4o, GPT-4o-mini, DeepSeek-Chat-V3,
Gemini-2.5-Flash, Claude-3.5-Haiku, Llama-4-Maverick) under five prompt types
(zero-shot, few-shot, chain-of-thought (CoT), CoT-few-shot, self-reflection)
across relevance classification and six Level-2 tasks, using accuracy,
precision, recall, and F1. Results show pronounced model-prompt interaction
effects: CoT-few-shot yields the most reliable precision-recall balance;
zero-shot maximizes recall for high-sensitivity passes; and self-reflection
underperforms due to over-inclusivity and instability across models. GPT-4o and
DeepSeek provide robust overall performance, while GPT-4o-mini performs
competitively at a substantially lower dollar cost. A cost-performance analysis
for relevance classification (per 1,000 abstracts) reveals large absolute
differences among model-prompt pairings; GPT-4o-mini remains low-cost across
prompts, and structured prompts (CoT/CoT-few-shot) on GPT-4o-mini offer
attractive F1 at a small incremental cost. We recommend a staged workflow that
(1) deploys low-cost models with structured prompts for first-pass screening
and (2) escalates only borderline cases to higher-capacity models. These
findings highlight LLMs' uneven but promising potential to automate literature
screening. By systematically analyzing prompt-model interactions, we provide a
comparative benchmark and practical guidance for task-adaptive LLM deployment.

</details>


### [6] [Facts in Stats: Impacts of Pretraining Diversity on Language Model Generalization](https://arxiv.org/abs/2510.16096)
*Tina Behnia,Puneesh Deora,Christos Thrampoulidis*

Main category: cs.CL

TL;DR: A synthetic testbed is introduced to systematically analyze how contextual diversity affects language models' factual recall and generalization, revealing that diversity's impact depends on contextual structure and training duration.


<details>
  <summary>Details</summary>
Motivation: To understand how the interaction between statistical regularities and factual associations in language models affects generalization, given the lack of systematic analysis on this topic.

Method: Created a flexible synthetic testbed combining statistical token streams with abstract factual token pairs, enabling independent control of contextual structure and diversity levels through controlled experiments.

Result: Higher contextual diversity delays in-distribution factual accuracy but has varying effects on out-of-distribution generalization depending on contextual structure. Optimal diversity levels depend on training duration, and failures can be traced to embedding and unembedding layer bottlenecks.

Conclusion: The interplay between contextual design and diversity level critically impacts different generalization aspects, and the synthetic framework provides a controlled testbed for isolating effects that would be confounded in large-scale studies.

Abstract: Language models are pretrained on sequences that blend statistical
regularities (making text fluent) with factual associations between specific
tokens (knowledge of facts). While recent work suggests that the variability of
their interaction, such as paraphrases of factual associations, critically
determines generalization ability, we lack a systematic analysis of these
impacts. This paper introduces a flexible synthetic testbed that combines a
statistical stream of generic tokens with an abstract factual stream of
source-target token pairs, enabling fine-grained control over their
interaction. The design enables the independent control of diversity nature by
manipulating stream composition (contextual structure) and the diversity level
by varying which statistical streams each fact appears in. Through controlled
experiments, we find that while higher contextual diversity delays
in-distribution (ID) factual accuracy, its impact on out-of-distribution (OOD)
factual generalization depends critically on contextual structure. In some
cases, OOD performance follows the same trend as ID, but in others, diversity
becomes essential for non-trivial factual recall. Even when low diversity
prohibits factual recall, optimal diversity levels depend on training duration.
Beyond factual recall failures, we identify structures where statistical
generalization fails independently, and others where both capabilities degrade.
This shows how the interplay between contextual design and diversity level
impacts different generalization aspects. Further, through a series of
controlled interventions on the model components, we trace the OOD failures to
distinct optimization bottlenecks, highlighting the importance of the embedding
and unembedding layers. Our synthetic framework allows us to isolate effects
that would be confounded in large-scale studies, offering a controlled testbed
for future investigations.

</details>


### [7] [In Generative AI We (Dis)Trust? Computational Analysis of Trust and Distrust in Reddit Discussions](https://arxiv.org/abs/2510.16173)
*Aria Pessianzadeh,Naima Sultana,Hildegarde Van den Bulck,David Gefen,Shahin Jabari,Rezvaneh Rezapour*

Main category: cs.CL

TL;DR: First computational study of Trust and Distrust in GenAI using Reddit data (2022-2025), finding balanced attitudes with shifts around model releases, dominated by technical performance and usability concerns.


<details>
  <summary>Details</summary>
Motivation: Lack of computational, large-scale, and longitudinal approaches to measuring trust in GenAI and LLMs, despite their growing impact on human life and need for responsible adoption.

Method: Multi-year Reddit dataset analysis (197,618 posts from 39 subreddits) combined with crowd-sourced annotations and classification models to scale analysis.

Result: Trust and Distrust are nearly balanced over time with shifts around major model releases; technical performance and usability dominate as dimensions; personal experience is most frequent reason shaping attitudes; distinct patterns across different user types.

Conclusion: Provides methodological framework for large-scale Trust analysis and insights into evolving public perceptions of GenAI for responsible governance.

Abstract: The rise of generative AI (GenAI) has impacted many aspects of human life. As
these systems become embedded in everyday practices, understanding public trust
in them also becomes essential for responsible adoption and governance. Prior
work on trust in AI has largely drawn from psychology and human-computer
interaction, but there is a lack of computational, large-scale, and
longitudinal approaches to measuring trust and distrust in GenAI and large
language models (LLMs). This paper presents the first computational study of
Trust and Distrust in GenAI, using a multi-year Reddit dataset (2022--2025)
spanning 39 subreddits and 197,618 posts. Crowd-sourced annotations of a
representative sample were combined with classification models to scale
analysis. We find that Trust and Distrust are nearly balanced over time, with
shifts around major model releases. Technical performance and usability
dominate as dimensions, while personal experience is the most frequent reason
shaping attitudes. Distinct patterns also emerge across trustors (e.g.,
experts, ethicists, general users). Our results provide a methodological
framework for large-scale Trust analysis and insights into evolving public
perceptions of GenAI.

</details>


### [8] [EgMM-Corpus: A Multimodal Vision-Language Dataset for Egyptian Culture](https://arxiv.org/abs/2510.16198)
*Mohamed Gamil,Abdelrahman Elsayed,Abdelrahman Lila,Ahmed Gad,Hesham Abdelgawad,Mohamed Aref,Ahmed Fares*

Main category: cs.CL

TL;DR: EgMM-Corpus is a multimodal dataset for Egyptian culture with 3,000+ images across 313 concepts, addressing the lack of culturally diverse datasets for Middle East and Africa regions.


<details>
  <summary>Details</summary>
Motivation: Limited multimodal culturally diverse datasets exist for Middle East and Africa regions, creating a gap for evaluating and training vision-language models in Egyptian cultural contexts.

Method: Developed a new data collection pipeline to gather over 3,000 images covering 313 concepts in landmarks, food, and folklore, with manual validation for cultural authenticity and multimodal coherence.

Result: CLIP achieved 21.2% Top-1 and 36.4% Top-5 accuracy on EgMM-Corpus, revealing significant cultural bias in existing vision-language models.

Conclusion: EgMM-Corpus serves as an important benchmark for developing culturally aware models and highlights the need to address cultural biases in AI systems.

Abstract: Despite recent advances in AI, multimodal culturally diverse datasets are
still limited, particularly for regions in the Middle East and Africa. In this
paper, we introduce EgMM-Corpus, a multimodal dataset dedicated to Egyptian
culture. By designing and running a new data collection pipeline, we collected
over 3,000 images, covering 313 concepts across landmarks, food, and folklore.
Each entry in the dataset is manually validated for cultural authenticity and
multimodal coherence. EgMM-Corpus aims to provide a reliable resource for
evaluating and training vision-language models in an Egyptian cultural context.
We further evaluate the zero-shot performance of Contrastive Language-Image
Pre-training CLIP on EgMM-Corpus, on which it achieves 21.2% Top-1 accuracy and
36.4% Top-5 accuracy in classification. These results underscore the existing
cultural bias in large-scale vision-language models and demonstrate the
importance of EgMM-Corpus as a benchmark for developing culturally aware
models.

</details>


### [9] [What Can String Probability Tell Us About Grammaticality?](https://arxiv.org/abs/2510.16227)
*Jennifer Hu,Ethan Gotlieb Wilcox,Siyuan Song,Kyle Mahowald,Roger P. Levy*

Main category: cs.CL

TL;DR: This paper analyzes what language models learn about grammar by examining the relationship between string probability and grammaticality, validating predictions through empirical analysis of 280K sentence pairs in English and Chinese.


<details>
  <summary>Details</summary>
Motivation: To understand what language models have learned about grammar, addressing the debate about whether string probabilities reveal underlying grammatical knowledge, given that probability and grammaticality are distinct concepts in linguistics.

Method: Theoretical analysis of the relationship between grammar, meaning, and string probability based on assumptions about corpus data generation, validated empirically using 280K sentence pairs in English and Chinese to test three predictions about minimal pairs and probability distributions.

Result: Empirical validation confirmed three predictions: (1) correlation between probabilities of strings within minimal pairs, (2) correlation between models' and humans' deltas within minimal pairs, and (3) poor separation in probability space between unpaired grammatical and ungrammatical strings.

Conclusion: The analyses provide theoretical grounding for using probability to study language models' structural knowledge and suggest directions for future work in LM grammatical evaluation.

Abstract: What have language models (LMs) learned about grammar? This question remains
hotly debated, with major ramifications for linguistic theory. However, since
probability and grammaticality are distinct notions in linguistics, it is not
obvious what string probabilities can reveal about an LM's underlying
grammatical knowledge. We present a theoretical analysis of the relationship
between grammar, meaning, and string probability, based on simple assumptions
about the generative process of corpus data. Our framework makes three
predictions, which we validate empirically using 280K sentence pairs in English
and Chinese: (1) correlation between the probability of strings within minimal
pairs, i.e., string pairs with minimal semantic differences; (2) correlation
between models' and humans' deltas within minimal pairs; and (3) poor
separation in probability space between unpaired grammatical and ungrammatical
strings. Our analyses give theoretical grounding for using probability to learn
about LMs' structural knowledge, and suggest directions for future work in LM
grammatical evaluation.

</details>


### [10] [Towards Low-Resource Alignment to Diverse Perspectives with Sparse Feedback](https://arxiv.org/abs/2510.16257)
*Chu Fei Luo,Samuel Dahan,Xiaodan Zhu*

Main category: cs.CL

TL;DR: The paper proposes pluralistic decoding and model steering methods to enhance pluralistic alignment of language models in low-resource settings, improving performance on high-stakes tasks with minimal annotated data.


<details>
  <summary>Details</summary>
Motivation: Current language model training assumes one optimal answer per query, leading to generic responses and poor alignment with diverse human perspectives and values.

Method: Two methods: pluralistic decoding and model steering, requiring only 50 annotated samples for effective implementation.

Result: Model steering consistently improves over zero-shot and few-shot baselines, reduces false positives in hate speech and misinformation detection, and improves alignment with human values in GlobalOpinionQA.

Conclusion: The work demonstrates the importance of diversity and shows language models can be adapted to consider nuanced perspectives with minimal resources.

Abstract: As language models have a greater impact on society, it is important to
ensure they are aligned to a diverse range of perspectives and are able to
reflect nuance in human values. However, the most popular training paradigms
for modern language models often assume there is one optimal answer for every
query, leading to generic responses and poor alignment. In this work, we aim to
enhance pluralistic alignment of language models in a low-resource setting with
two methods: pluralistic decoding and model steering. We empirically
demonstrate that model steering offers consistent improvement over zero-shot
and few-shot baselines with only 50 annotated samples. Our proposed methods
decrease false positives in several high-stakes tasks such as hate speech
detection and misinformation detection, and improves the distributional
alignment to human values in GlobalOpinionQA. We hope our work highlights the
importance of diversity and how language models can be adapted to consider
nuanced perspectives.

</details>


### [11] [Instant Personalized Large Language Model Adaptation via Hypernetwork](https://arxiv.org/abs/2510.16282)
*Zhaoxuan Tan,Zixuan Zhang,Haoyang Wen,Zheng Li,Rongzhi Zhang,Pei Chen,Fengran Mo,Zheyuan Liu,Qingkai Zeng,Qingyu Yin,Meng Jiang*

Main category: cs.CL

TL;DR: Profile-to-PEFT is a scalable framework that uses a hypernetwork to map user profiles directly to adapter parameters, eliminating per-user training and enabling instant LLM personalization.


<details>
  <summary>Details</summary>
Motivation: Existing PEFT methods like One-PEFT-Per-User require training separate adapters for each user, making them computationally expensive and impractical for real-time updates.

Method: A hypernetwork trained end-to-end maps encoded user profiles directly to full sets of adapter parameters (e.g., LoRA), eliminating per-user training at deployment.

Result: Outperforms both prompt-based personalization and OPPU while using substantially fewer computational resources, with strong generalization to out-of-distribution users and robustness across varying user activity levels.

Conclusion: Profile-to-PEFT enables efficient, scalable, and adaptive LLM personalization suitable for large-scale applications with instant adaptation and privacy-preserving local deployment.

Abstract: Personalized large language models (LLMs) tailor content to individual
preferences using user profiles or histories. However, existing
parameter-efficient fine-tuning (PEFT) methods, such as the
``One-PEFT-Per-User'' (OPPU) paradigm, require training a separate adapter for
each user, making them computationally expensive and impractical for real-time
updates. We introduce Profile-to-PEFT, a scalable framework that employs a
hypernetwork, trained end-to-end, to map a user's encoded profile directly to a
full set of adapter parameters (e.g., LoRA), eliminating per-user training at
deployment. This design enables instant adaptation, generalization to unseen
users, and privacy-preserving local deployment. Experimental results
demonstrate that our method outperforms both prompt-based personalization and
OPPU while using substantially fewer computational resources at deployment. The
framework exhibits strong generalization to out-of-distribution users and
maintains robustness across varying user activity levels and different
embedding backbones. The proposed Profile-to-PEFT framework enables efficient,
scalable, and adaptive LLM personalization suitable for large-scale
applications.

</details>


### [12] [Thinking About Thinking: Evaluating Reasoning in Post-Trained Language Models](https://arxiv.org/abs/2510.16340)
*Pratham Singla,Shivank Garg,Ayush Singh,Ishan Garg,Ketan Suhaas Saichandran*

Main category: cs.CL

TL;DR: LLMs trained with RL methods show greater awareness of learned policies and better generalization than SFT models, but exhibit weak alignment between reasoning traces and final outputs.


<details>
  <summary>Details</summary>
Motivation: To investigate whether LLMs are aware of what they learn and think, particularly in complex logic-intensive tasks with planning tokens.

Method: Define three core competencies (awareness, generalization, alignment) and evaluate them across tasks requiring distinct policies. Compare SFT, DPO, and GRPO training methods.

Result: RL-trained models demonstrate greater awareness of learned behaviors and stronger generalization to novel tasks than SFT models, but show weak alignment between reasoning traces and final outputs, especially with GRPO.

Conclusion: RL training enhances policy awareness and generalization but compromises alignment between internal reasoning and final outputs, highlighting a trade-off in post-training approaches.

Abstract: Recent advances in post-training techniques have endowed Large Language
Models (LLMs) with enhanced capabilities for tackling complex, logic-intensive
tasks through the generation of supplementary planning tokens. This development
raises a fundamental question: Are these models aware of what they "learn" and
"think"? To address this, we define three core competencies: (1) awareness of
learned latent policies, (2) generalization of these policies across domains,
and (3) alignment between internal reasoning traces and final outputs. We
empirically evaluate these abilities on several tasks, each designed to require
learning a distinct policy. Furthermore, we contrast the profiles of models
post-trained via Supervised Fine-Tuning (SFT), Direct Policy Optimization
(DPO), and Group Relative Policy Optimization (GRPO). Our findings indicate
that RL-trained models not only demonstrate greater awareness of their learned
behaviors and stronger generalizability to novel, structurally similar tasks
than SFT models but also often exhibit weak alignment between their reasoning
traces and final outputs, an effect most pronounced in GRPO-trained models.

</details>


### [13] [Utilising Large Language Models for Generating Effective Counter Arguments to Anti-Vaccine Tweets](https://arxiv.org/abs/2510.16359)
*Utsav Dhanuka,Soham Poddar,Saptarshi Ghosh*

Main category: cs.CL

TL;DR: LLMs can generate effective counter-arguments to vaccine misinformation through optimized prompting and fine-tuning, with classifiers enabling context-aware rebuttals based on specific anti-vaccine concerns.


<details>
  <summary>Details</summary>
Motivation: Combat vaccine skepticism and misinformation on social media to improve immunization rates and trust in health recommendations, addressing the gap in real-time counter-argument generation.

Method: Experiment with various prompting strategies and fine-tuning approaches for LLMs, plus train classifiers to categorize anti-vaccine tweets into multi-labeled categories (efficacy, side effects, political influences) for context-aware rebuttals.

Result: Strong alignment across human judgment, LLM-based assessments, and automatic metrics. Integration of label descriptions and structured fine-tuning enhances counter-argument effectiveness.

Conclusion: LLMs offer a promising approach for mitigating vaccine misinformation at scale through optimized counter-argument generation and context-aware rebuttals.

Abstract: In an era where public health is increasingly influenced by information
shared on social media, combatting vaccine skepticism and misinformation has
become a critical societal goal. Misleading narratives around vaccination have
spread widely, creating barriers to achieving high immunisation rates and
undermining trust in health recommendations. While efforts to detect
misinformation have made significant progress, the generation of real time
counter-arguments tailored to debunk such claims remains an insufficiently
explored area. In this work, we explore the capabilities of LLMs to generate
sound counter-argument rebuttals to vaccine misinformation. Building on prior
research in misinformation debunking, we experiment with various prompting
strategies and fine-tuning approaches to optimise counter-argument generation.
Additionally, we train classifiers to categorise anti-vaccine tweets into
multi-labeled categories such as concerns about vaccine efficacy, side effects,
and political influences allowing for more context aware rebuttals. Our
evaluation, conducted through human judgment, LLM based assessments, and
automatic metrics, reveals strong alignment across these methods. Our findings
demonstrate that integrating label descriptions and structured fine-tuning
enhances counter-argument effectiveness, offering a promising approach for
mitigating vaccine misinformation at scale.

</details>


### [14] [End-to-End Argument Mining through Autoregressive Argumentative Structure Prediction](https://arxiv.org/abs/2510.16363)
*Nilmadhab Das,Vishal Vaibhav,Yash Sunil Choudhary,V. Vijaya Saradhi,Ashish Anand*

Main category: cs.CL

TL;DR: The paper proposes AASP, an autoregressive framework for joint argument mining that models argument structures as constrained actions, achieving state-of-the-art results on standard benchmarks.


<details>
  <summary>Details</summary>
Motivation: Argument mining requires extracting complex argumentative structures, but modeling dependencies between argument components and relations is challenging. Current approaches flatten structures, losing important reasoning flow.

Method: Uses Autoregressive Argumentative Structure Prediction (AASP) framework that models argument structures as pre-defined constrained actions. A conditional pre-trained language model builds structures step-by-step in autoregressive manner to capture argumentative reasoning flow.

Result: AASP achieves state-of-the-art results across all argument mining tasks in two standard benchmarks and delivers strong results in one benchmark.

Conclusion: The AASP framework effectively captures argumentative reasoning flow through autoregressive structure prediction, outperforming existing approaches on standard argument mining benchmarks.

Abstract: Argument Mining (AM) helps in automating the extraction of complex
argumentative structures such as Argument Components (ACs) like Premise, Claim
etc. and Argumentative Relations (ARs) like Support, Attack etc. in an
argumentative text. Due to the inherent complexity of reasoning involved with
this task, modelling dependencies between ACs and ARs is challenging. Most of
the recent approaches formulate this task through a generative paradigm by
flattening the argumentative structures. In contrast to that, this study
jointly formulates the key tasks of AM in an end-to-end fashion using
Autoregressive Argumentative Structure Prediction (AASP) framework. The
proposed AASP framework is based on the autoregressive structure prediction
framework that has given good performance for several NLP tasks. AASP framework
models the argumentative structures as constrained pre-defined sets of actions
with the help of a conditional pre-trained language model. These actions build
the argumentative structures step-by-step in an autoregressive manner to
capture the flow of argumentative reasoning in an efficient way. Extensive
experiments conducted on three standard AM benchmarks demonstrate that AASP
achieves state-of-theart (SoTA) results across all AM tasks in two benchmarks
and delivers strong results in one benchmark.

</details>


### [15] [Navigating through the hidden embedding space: steering LLMs to improve mental health assessment](https://arxiv.org/abs/2510.16373)
*Federico Ravenda,Seyed Ali Bahrainian,Andrea Raballo,Antonietta Mira*

Main category: cs.CL

TL;DR: A lightweight method using linear transformation and steering vectors improves mental health assessment capabilities of LLMs without intensive computation, achieving better performance in relevance prediction and questionnaire completion tasks.


<details>
  <summary>Details</summary>
Motivation: Smaller-scale LLMs struggle with domain-specific applications like mental health assessment, despite the opportunities in sensitive high-impact areas.

Method: Apply linear transformation to specific layer activations using steering vectors to guide model output, without computationally intensive techniques.

Result: Improved performance across two tasks: identifying relevant Reddit posts for depression detection and completing standardized psychological screening questionnaires based on post history.

Conclusion: Steering mechanisms show potential as computationally efficient tools for LLM domain adaptation in mental health applications.

Abstract: The rapid evolution of Large Language Models (LLMs) is transforming AI,
opening new opportunities in sensitive and high-impact areas such as Mental
Health (MH). Yet, despite these advancements, recent evidence reveals that
smaller-scale models still struggle to deliver optimal performance in
domain-specific applications. In this study, we present a cost-efficient yet
powerful approach to improve MH assessment capabilities of an LLM, without
relying on any computationally intensive techniques. Our lightweight method
consists of a linear transformation applied to a specific layer's activations,
leveraging steering vectors to guide the model's output. Remarkably, this
intervention enables the model to achieve improved results across two distinct
tasks: (1) identifying whether a Reddit post is useful for detecting the
presence or absence of depressive symptoms (relevance prediction task), and (2)
completing a standardized psychological screening questionnaire for depression
based on users' Reddit post history (questionnaire completion task). Results
highlight the untapped potential of steering mechanisms as computationally
efficient tools for LLMs' MH domain adaptation.

</details>


### [16] [MoReBench: Evaluating Procedural and Pluralistic Moral Reasoning in Language Models, More than Outcomes](https://arxiv.org/abs/2510.16380)
*Yu Ying Chiu,Michael S. Lee,Rachel Calcott,Brandon Handoko,Paul de Font-Reaulx,Paula Rodriguez,Chen Bo Calvin Zhang,Ziwen Han,Udari Madhushani Sehwag,Yash Maurya,Christina Q Knight,Harry R. Lloyd,Florence Bacus,Mantas Mazeika,Bing Liu,Yejin Choi,Mitchell L Gordon,Sydney Levine*

Main category: cs.CL

TL;DR: MoReBench is a benchmark for evaluating moral reasoning in AI systems through 1,000 moral scenarios with expert-defined criteria, plus MoReBench-Theory for testing reasoning under five normative ethics frameworks.


<details>
  <summary>Details</summary>
Motivation: As AI systems make more decisions with and for humans, understanding their reasoning processes is crucial for ensuring alignment with human values. Moral dilemmas provide an ideal testbed since they allow multiple defensible conclusions.

Method: Created MoReBench with 1,000 moral scenarios paired with 23,000+ expert-defined rubric criteria covering moral considerations, trade-offs, and recommendations. Also developed MoReBench-Theory with 150 examples testing reasoning under five major normative ethics frameworks.

Result: Scaling laws and existing benchmarks on math, code, and scientific reasoning fail to predict models' moral reasoning abilities. Models show bias toward specific moral frameworks like Benthamite Act Utilitarianism and Kantian Deontology, possibly due to training paradigms.

Conclusion: These benchmarks advance process-focused reasoning evaluation toward safer and more transparent AI by enabling systematic assessment of how AI systems arrive at moral decisions rather than just what decisions they make.

Abstract: As AI systems progress, we rely more on them to make decisions with us and
for us. To ensure that such decisions are aligned with human values, it is
imperative for us to understand not only what decisions they make but also how
they come to those decisions. Reasoning language models, which provide both
final responses and (partially transparent) intermediate thinking traces,
present a timely opportunity to study AI procedural reasoning. Unlike math and
code problems which often have objectively correct answers, moral dilemmas are
an excellent testbed for process-focused evaluation because they allow for
multiple defensible conclusions. To do so, we present MoReBench: 1,000 moral
scenarios, each paired with a set of rubric criteria that experts consider
essential to include (or avoid) when reasoning about the scenarios. MoReBench
contains over 23 thousand criteria including identifying moral considerations,
weighing trade-offs, and giving actionable recommendations to cover cases on AI
advising humans moral decisions as well as making moral decisions autonomously.
Separately, we curate MoReBench-Theory: 150 examples to test whether AI can
reason under five major frameworks in normative ethics. Our results show that
scaling laws and existing benchmarks on math, code, and scientific reasoning
tasks fail to predict models' abilities to perform moral reasoning. Models also
show partiality towards specific moral frameworks (e.g., Benthamite Act
Utilitarianism and Kantian Deontology), which might be side effects of popular
training paradigms. Together, these benchmarks advance process-focused
reasoning evaluation towards safer and more transparent AI.

</details>


### [17] [ATA: A Neuro-Symbolic Approach to Implement Autonomous and Trustworthy Agents](https://arxiv.org/abs/2510.16381)
*David Peer,Sebastian Stabinger*

Main category: cs.CL

TL;DR: ATA is a neuro-symbolic framework that decouples LLM tasks into offline knowledge formalization and online symbolic reasoning, achieving competitive performance while ensuring trustworthiness through human-verifiable symbolic representations.


<details>
  <summary>Details</summary>
Motivation: Address LLM limitations in trustworthiness (hallucinations, instability, lack of transparency) that hinder deployment in high-stakes domains.

Method: Two-phase approach: (1) Offline knowledge ingestion where LLM translates informal specs into formal symbolic knowledge base, (2) Online task processing where symbolic decision engine uses formal knowledge to derive reliable results.

Result: Competitive with state-of-the-art end-to-end models in automated setup; with human-verified knowledge base, significantly outperforms larger models while achieving perfect determinism, enhanced stability, and immunity to prompt injection attacks.

Conclusion: ATA provides practical, controllable architecture for building transparent, auditable, and reliable autonomous agents through symbolic reasoning.

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities, yet
their deployment in high-stakes domains is hindered by inherent limitations in
trustworthiness, including hallucinations, instability, and a lack of
transparency. To address these challenges, we introduce a generic
neuro-symbolic approach, which we call Autonomous Trustworthy Agents (ATA). The
core of our approach lies in decoupling tasks into two distinct phases: Offline
knowledge ingestion and online task processing. During knowledge ingestion, an
LLM translates an informal problem specification into a formal, symbolic
knowledge base. This formal representation is crucial as it can be verified and
refined by human experts, ensuring its correctness and alignment with domain
requirements. In the subsequent task processing phase, each incoming input is
encoded into the same formal language. A symbolic decision engine then utilizes
this encoded input in conjunction with the formal knowledge base to derive a
reliable result. Through an extensive evaluation on a complex reasoning task,
we demonstrate that a concrete implementation of ATA is competitive with
state-of-the-art end-to-end reasoning models in a fully automated setup while
maintaining trustworthiness. Crucially, with a human-verified and corrected
knowledge base, our approach significantly outperforms even larger models,
while exhibiting perfect determinism, enhanced stability against input
perturbations, and inherent immunity to prompt injection attacks. By generating
decisions grounded in symbolic reasoning, ATA offers a practical and
controllable architecture for building the next generation of transparent,
auditable, and reliable autonomous agents.

</details>


### [18] [Probing the Hidden Talent of ASR Foundation Models for L2 English Oral Assessment](https://arxiv.org/abs/2510.16387)
*Fu-An Chao,Bi-Cheng Yan,Berlin Chen*

Main category: cs.CL

TL;DR: This paper demonstrates that Whisper's hidden representations contain valuable acoustic and linguistic features for L2 spoken language assessment, achieving state-of-the-art performance with minimal training.


<details>
  <summary>Details</summary>
Motivation: To explore the untapped potential of Whisper ASR model beyond transcription for L2 spoken language assessment by probing its latent capabilities in hidden representations.

Method: Extract acoustic and linguistic features from Whisper's hidden representations and train only a lightweight classifier on intermediate and final outputs, optionally incorporating image and text-prompt information as auxiliary cues.

Result: Achieved strong performance on GEPT picture-description dataset, outperforming existing cutting-edge baselines including multimodal approaches, with additional gains from auxiliary cues.

Conclusion: Whisper intrinsically encodes both ordinal proficiency patterns and semantic aspects of speech without task-specific fine-tuning, highlighting its potential as a powerful foundation for SLA and spoken language understanding tasks.

Abstract: In this paper, we explore the untapped potential of Whisper, a
well-established automatic speech recognition (ASR) foundation model, in the
context of L2 spoken language assessment (SLA). Unlike prior studies that
extrinsically analyze transcriptions produced by Whisper, our approach goes a
step further to probe its latent capabilities by extracting acoustic and
linguistic features from hidden representations. With only a lightweight
classifier being trained on top of Whisper's intermediate and final outputs,
our method achieves strong performance on the GEPT picture-description dataset,
outperforming existing cutting-edge baselines, including a multimodal approach.
Furthermore, by incorporating image and text-prompt information as auxiliary
relevance cues, we demonstrate additional performance gains. Finally, we
conduct an in-depth analysis of Whisper's embeddings, which reveals that, even
without task-specific fine-tuning, the model intrinsically encodes both ordinal
proficiency patterns and semantic aspects of speech, highlighting its potential
as a powerful foundation for SLA and other spoken language understanding tasks.

</details>


### [19] [FrugalPrompt: Reducing Contextual Overhead in Large Language Models via Token Attribution](https://arxiv.org/abs/2510.16439)
*Syed Rifat Raiyan,Md Farhan Ishmam,Abdullah Al Imran,Mohammad Ali Moni*

Main category: cs.CL

TL;DR: FrugalPrompt is a prompt compression framework that identifies and retains only the most semantically significant tokens using token attribution methods, achieving 20% compression with minimal performance loss on most NLP tasks except mathematical reasoning.


<details>
  <summary>Details</summary>
Motivation: Large language models face efficiency issues due to redundant tokens in prompts, which increase costs, carbon footprint, and inference latency. Only a fraction of tokens carry most semantic weight.

Method: Uses GlobEnc and DecompX token attribution methods to assign salience scores to tokens, ranks them, and preserves top-k% tokens in original order to create sparse frugalized prompts.

Result: 20% prompt reduction causes only marginal performance loss on sentiment analysis, commonsense QA, and summarization, but sharp deterioration on mathematical reasoning. Analysis reveals asymmetric patterns suggesting potential task contamination effects.

Conclusion: The work provides nuanced understanding of LLM behavior in performance-efficiency trade-offs, delineating boundaries between tasks tolerant to contextual sparsity and those requiring exhaustive context.

Abstract: Large language models (LLMs) owe much of their stellar performance to
expansive input contexts, yet such verbosity inflates monetary costs, carbon
footprint, and inference-time latency. Much of this overhead manifests from the
redundant low-utility tokens present in typical prompts, as only a fraction of
tokens typically carries the majority of the semantic weight. We address this
inefficiency by introducing FrugalPrompt, a novel prompt compression framework
for LLMs, which retains only the most semantically significant tokens.
Leveraging two state-of-the-art token attribution methods, GlobEnc and DecompX,
we assign salience scores to every token in an input sequence, rank them to
preserve the top-k% tokens in their original order, and obtain a sparse
frugalized prompt. We evaluate the approach across four NLP tasks: Sentiment
Analysis, Commonsense QA, Summarization, and Mathematical Reasoning, using a
suite of frontier LLMs. For the first three tasks, a 20% prompt reduction
incurs only a marginal loss in task performance, demonstrating that
contemporary LLMs can reconstruct elided context from high-salience cues. In
contrast, performance on mathematical reasoning deteriorates sharply,
reflecting a stronger dependence on complete token continuity. Further analysis
with bottom-k% and random-k% tokens reveals asymmetric performance patterns
that may suggest potential task contamination effects, wherein models may
resort to shallow memorized patterns from pretraining exposure for conventional
NLP tasks. We posit that our work contributes to a more nuanced understanding
of LLM behavior in performance-efficiency trade-offs, and delineate the
boundary between tasks tolerant to contextual sparsity and those requiring
exhaustive context. Our source code and models are available at:
https://github.com/Starscream-11813/Frugal-ICL

</details>


### [20] [TrajSelector: Harnessing Latent Representations for Efficient and Effective Best-of-N in Large Reasoning Model](https://arxiv.org/abs/2510.16449)
*Bin Yu,Xinming Wang,Shijie Lian,Haotian Li,Changti Wu,Ruina Hu,Bailing Wang,Yuliang Wei,Kai Chen*

Main category: cs.CL

TL;DR: TrajSelector is an efficient Best-of-N framework that uses hidden states from LLMs for process-level scoring, achieving better performance than majority voting and process reward models with lower computational costs.


<details>
  <summary>Details</summary>
Motivation: Current Best-of-N selection paradigms face high computational overhead from process reward models and underutilize LLMs' intrinsic latent representations.

Method: Uses a lightweight verifier (0.6B parameters) to evaluate step-wise trajectory quality from hidden states, with end-to-end training that eliminates need for step-level annotations.

Result: Achieves 4.61% higher accuracy than majority voting and 4.31%-12.21% better performance than existing process reward models in Best-of-32 settings across five benchmarks.

Conclusion: TrajSelector provides consistent performance gains with lower inference costs by effectively leveraging LLM hidden states for trajectory selection.

Abstract: Large language models (LLMs) have shown remarkable progress in complex
reasoning tasks, largely enabled by test-time scaling (TTS) paradigms that
allocate additional compute during inference. Among these, external TTS
(particularly the Best-of-N selection paradigm) yields scalable performance
improvements by selecting from multiple independently generated reasoning
trajectories. However, this approach faces key limitations: (i) the high
computational overhead of deploying process reward models, (ii) the
underutilization of the LLM's intrinsic latent representations. We introduce
TrajSelector, an efficient and effective Best-of-N framework that exploit the
hidden states in the sampler LLM for process-level scoring. A lightweight
verifier (with only 0.6B parameters) evaluates the quality of step-wise
trajectory, and then aggregates these scores to identify the optimal reasoning
trajectory. Our framework employs a fully data-driven, end-to-end training
recipe that eliminates reliance on massive step-level annotations. Experiential
results across five benchmarks demonstrate that TrajSelector delivers
consistent performance gains. In Best-of-32 settings, it surpasses majority
voting by 4.61% accuracy and outperforms existing process reward models by
4.31% to 12.21%, all while maintaining lower inference costs.

</details>


### [21] [RAVEN: Robust Advertisement Video Violation Temporal Grounding via Reinforcement Reasoning](https://arxiv.org/abs/2510.16455)
*Deyi Ji,Yuekui Yang,Haiyang Wu,Shaoping Ma,Tianrun Chen,Lanyun Zhu*

Main category: cs.CL

TL;DR: RAVEN is a framework that combines curriculum reinforcement learning with multimodal LLMs for ad video violation detection, addressing temporal grounding, noisy annotations, and generalization issues.


<details>
  <summary>Details</summary>
Motivation: Existing ad video violation detection methods struggle with precise temporal grounding, noisy annotations, and limited generalization capabilities.

Method: Integrates curriculum reinforcement learning with multimodal LLMs, uses progressive training with precise/coarse annotations, GRPO for emergent reasoning, and hierarchical reward mechanisms.

Result: Achieves superior performance in violation category accuracy and temporal interval localization on industrial datasets and public benchmarks, with significant improvements in precision and recall in online A/B testing.

Conclusion: RAVEN effectively addresses key challenges in ad video violation detection, demonstrates strong generalization, and mitigates catastrophic forgetting, validating its practical applicability.

Abstract: Advertisement (Ad) video violation detection is critical for ensuring
platform compliance, but existing methods struggle with precise temporal
grounding, noisy annotations, and limited generalization. We propose RAVEN, a
novel framework that integrates curriculum reinforcement learning with
multimodal large language models (MLLMs) to enhance reasoning and cognitive
capabilities for violation detection. RAVEN employs a progressive training
strategy, combining precisely and coarsely annotated data, and leverages Group
Relative Policy Optimization (GRPO) to develop emergent reasoning abilities
without explicit reasoning annotations. Multiple hierarchical sophisticated
reward mechanism ensures precise temporal grounding and consistent category
prediction. Experiments on industrial datasets and public benchmarks show that
RAVEN achieves superior performances in violation category accuracy and
temporal interval localization. We also design a pipeline to deploy the RAVEN
on the online Ad services, and online A/B testing further validates its
practical applicability, with significant improvements in precision and recall.
RAVEN also demonstrates strong generalization, mitigating the catastrophic
forgetting issue associated with supervised fine-tuning.

</details>


### [22] [Agree, Disagree, Explain: Decomposing Human Label Variation in NLI through the Lens of Explanations](https://arxiv.org/abs/2510.16458)
*Pingjun Hong,Beiduo Chen,Siyao Peng,Marie-Catherine de Marneffe,Benjamin Roth,Barbara Plank*

Main category: cs.CL

TL;DR: This paper analyzes Natural Language Inference (NLI) datasets using explanation-based approaches to understand human label variation, extending beyond within-label variation to examine cases where annotators diverge in both reasoning types and labeling decisions.


<details>
  <summary>Details</summary>
Motivation: To better understand human label variation in NLI datasets by examining how annotators may diverge not only in reasoning types but also in the labeling step, using explanations as a lens to analyze individual differences in the reasoning process.

Method: Applied the LiTEx taxonomy to two English NLI datasets, aligning annotation variation from multiple aspects: NLI label agreement, explanation similarity, and taxonomy agreement, while considering annotators' selection bias as a compounding factor.

Result: Found instances where annotators disagree on labels but provide highly similar explanations, suggesting surface-level disagreement may mask underlying agreement in interpretation. Also revealed individual preferences in explanation strategies and label choices.

Conclusion: Agreement in reasoning types better reflects semantic similarity of free-text explanations than label agreement alone, highlighting the richness of reasoning-based explanations and the need for caution in treating labels as ground truth.

Abstract: Natural Language Inference datasets often exhibit human label variation. To
better understand these variations, explanation-based approaches analyze the
underlying reasoning behind annotators' decisions. One such approach is the
LiTEx taxonomy, which categorizes free-text explanations in English into
reasoning types. However, previous work applying such taxonomies has focused on
within-label variation: cases where annotators agree on the final NLI label but
provide different explanations. In contrast, this paper broadens the scope by
examining how annotators may diverge not only in the reasoning type but also in
the labeling step. We use explanations as a lens to decompose the reasoning
process underlying NLI annotation and to analyze individual differences. We
apply LiTEx to two NLI English datasets and align annotation variation from
multiple aspects: NLI label agreement, explanation similarity, and taxonomy
agreement, with an additional compounding factor of annotators' selection bias.
We observe instances where annotators disagree on the label but provide highly
similar explanations, suggesting that surface-level disagreement may mask
underlying agreement in interpretation. Moreover, our analysis reveals
individual preferences in explanation strategies and label choices. These
findings highlight that agreement in reasoning types better reflects the
semantic similarity of free-text explanations than label agreement alone. Our
findings underscore the richness of reasoning-based explanations and the need
for caution in treating labels as ground truth.

</details>


### [23] [Check Yourself Before You Wreck Yourself: Selectively Quitting Improves LLM Agent Safety](https://arxiv.org/abs/2510.16492)
*Vamshi Krishna Bonagiri,Ponnurangam Kumaragurum,Khanh Nguyen,Benjamin Plaut*

Main category: cs.CL

TL;DR: Using 'quitting' as a safety mechanism for LLM agents to withdraw from uncertain situations, achieving significant safety improvements with minimal helpfulness loss.


<details>
  <summary>Details</summary>
Motivation: LLM agents operating in complex real-world environments face compounded uncertainties that can lead to catastrophic risks beyond traditional text generation failures.

Method: Proposed using explicit quit instructions for LLM agents and evaluated quitting behavior across 12 state-of-the-art LLMs using the ToolEmu framework.

Result: Agents with quit instructions improved safety by +0.39 on 0-3 scale (+0.64 for proprietary models) with only -0.03 average decrease in helpfulness.

Conclusion: Explicit quit instructions are a highly effective, immediately deployable safety mechanism that establishes quitting as an effective first-line defense for autonomous agents in high-stakes applications.

Abstract: As Large Language Model (LLM) agents increasingly operate in complex
environments with real-world consequences, their safety becomes critical. While
uncertainty quantification is well-studied for single-turn tasks, multi-turn
agentic scenarios with real-world tool access present unique challenges where
uncertainties and ambiguities compound, leading to severe or catastrophic risks
beyond traditional text generation failures. We propose using "quitting" as a
simple yet effective behavioral mechanism for LLM agents to recognize and
withdraw from situations where they lack confidence. Leveraging the ToolEmu
framework, we conduct a systematic evaluation of quitting behavior across 12
state-of-the-art LLMs. Our results demonstrate a highly favorable
safety-helpfulness trade-off: agents prompted to quit with explicit
instructions improve safety by an average of +0.39 on a 0-3 scale across all
models (+0.64 for proprietary models), while maintaining a negligible average
decrease of -0.03 in helpfulness. Our analysis demonstrates that simply adding
explicit quit instructions proves to be a highly effective safety mechanism
that can immediately be deployed in existing agent systems, and establishes
quitting as an effective first-line defense mechanism for autonomous agents in
high-stakes applications.

</details>


### [24] [Automated Composition of Agents: A Knapsack Approach for Agentic Component Selection](https://arxiv.org/abs/2510.16499)
*Michelle Yuan,Khushbu Pahwa,Shuaichen Chang,Mustafa Kaba,Jiarong Jiang,Xiaofei Ma,Yi Zhang,Monica Sunkara*

Main category: cs.CL

TL;DR: The paper introduces an automated framework for agentic system composition inspired by the knapsack problem, enabling optimal selection and assembly of agentic components while considering performance, budget constraints, and compatibility.


<details>
  <summary>Details</summary>
Motivation: Existing methods for agentic system composition rely on static semantic retrieval approaches, which face challenges in effective reuse and composition due to incomplete capability descriptions and limitations in considering capability, cost, and real-time utility.

Method: The framework uses an online-knapsack-based composer that dynamically tests candidate components and models their utility in real-time, systematically identifying, selecting, and assembling optimal sets of agentic components.

Result: Empirical evaluation across five datasets shows the online-knapsack composer consistently achieves Pareto optimality, with success rate improvements up to 31.6% in single-agent setups and from 37% to 87% in multi-agent systems with 100+ agents.

Conclusion: The method demonstrates robust adaptability across diverse domains and budget constraints, significantly outperforming retrieval baselines and enabling scalable reuse of agentic resources.

Abstract: Designing effective agentic systems requires the seamless composition and
integration of agents, tools, and models within dynamic and uncertain
environments. Most existing methods rely on static, semantic retrieval
approaches for tool or agent discovery. However, effective reuse and
composition of existing components remain challenging due to incomplete
capability descriptions and the limitations of retrieval methods. Component
selection suffers because the decisions are not based on capability, cost, and
real-time utility. To address these challenges, we introduce a structured,
automated framework for agentic system composition that is inspired by the
knapsack problem. Our framework enables a composer agent to systematically
identify, select, and assemble an optimal set of agentic components by jointly
considering performance, budget constraints, and compatibility. By dynamically
testing candidate components and modeling their utility in real-time, our
approach streamlines the assembly of agentic systems and facilitates scalable
reuse of resources. Empirical evaluation with Claude 3.5 Sonnet across five
benchmarking datasets shows that our online-knapsack-based composer
consistently lies on the Pareto frontier, achieving higher success rates at
significantly lower component costs compared to our baselines. In the
single-agent setup, the online knapsack composer shows a success rate
improvement of up to 31.6% in comparison to the retrieval baselines. In
multi-agent systems, the online knapsack composer increases success rate from
37% to 87% when agents are selected from an agent inventory of 100+ agents. The
substantial performance gap confirms the robust adaptability of our method
across diverse domains and budget constraints.

</details>


### [25] [ReviewGuard: Enhancing Deficient Peer Review Detection via LLM-Driven Data Augmentation](https://arxiv.org/abs/2510.16549)
*Haoxuan Zhang,Ruochi Li,Sarthak Shrestha,Shree Harshini Mamidala,Revanth Putta,Arka Krishan Aggarwal,Ting Xiao,Junhua Ding,Haihua Chen*

Main category: cs.CL

TL;DR: ReviewGuard is an automated system that detects deficient peer reviews using a four-stage LLM-driven framework, addressing challenges from increased submissions and AI adoption in scholarly evaluation.


<details>
  <summary>Details</summary>
Motivation: The surge in submissions and widespread adoption of LLMs in scholarly evaluation present challenges, with unchecked deficient reviews from both human experts and AI systems threatening to undermine the peer review ecosystem and compromise academic integrity.

Method: A four-stage LLM-driven framework: (1) collects ICLR and NeurIPS papers with reviews from OpenReview; (2) annotates review types using GPT-4.1 with human validation; (3) addresses class imbalance through LLM-driven synthetic data augmentation; (4) fine-tunes encoder-based models and open source LLMs.

Result: Created a corpus of 6,634 papers, 24,657 real reviews, and 46,438 synthetic reviews. Deficient reviews show lower rating scores, higher self-reported confidence, reduced structural complexity, and more negative sentiment. AI-generated reviews increased dramatically after ChatGPT's emergence. Mixed training with synthetic and real data improved recall and F1 scores.

Conclusion: This study presents the first LLM-driven system for detecting deficient peer reviews, providing evidence to inform AI governance in peer review while offering insights into human-AI collaboration to maintain academic integrity.

Abstract: Peer review serves as the gatekeeper of science, yet the surge in submissions
and widespread adoption of large language models (LLMs) in scholarly evaluation
present unprecedented challenges. Recent work has focused on using LLMs to
improve review efficiency or generate insightful review content. However,
unchecked deficient reviews from both human experts and AI systems threaten to
systematically undermine the peer review ecosystem and compromise academic
integrity. To address this critical issue, we introduce ReviewGuard, an
automated system for detecting and categorizing deficient reviews. ReviewGuard
employs a comprehensive four-stage LLM-driven framework that: (1) collects ICLR
and NeurIPS papers with their corresponding reviews from OpenReview; (2)
annotates review types using GPT-4.1 with human validation; (3) addresses class
imbalance and data scarcity through LLM-driven synthetic data augmentation,
producing a final corpus of 6,634 papers, 24,657 real reviews, and 46,438
synthetic reviews; and (4) fine-tunes both encoder-based models and open source
LLMs. We perform comprehensive feature analysis of the structure and quality of
the review text. Compared to sufficient reviews, deficient reviews demonstrate
lower rating scores, higher self-reported confidence, reduced structural
complexity, and a higher proportion of negative sentiment. AI-generated text
detection reveals that, since ChatGPT's emergence, AI-generated reviews have
increased dramatically. In the evaluation of deficient review detection models,
mixed training with synthetic and real review data provides substantial
enhancements to recall and F1 scores on the binary task. This study presents
the first LLM-driven system for detecting deficient peer reviews, providing
evidence to inform AI governance in peer review while offering valuable
insights into human-AI collaboration to maintain academic integrity.

</details>


### [26] [Language over Content: Tracing Cultural Understanding in Multilingual Large Language Models](https://arxiv.org/abs/2510.16565)
*Seungho Cho,Changgeon Ko,Eui Jun Hwang,Junmyeong Lee,Huije Lee,Jong C. Park*

Main category: cs.CL

TL;DR: This paper analyzes how LLMs internally process cultural information by examining activation path overlaps when answering questions about different countries and languages, revealing that language has a stronger influence than cultural context on internal representations.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly used across diverse cultural contexts, but prior evaluations focused mainly on output-level performance without understanding the internal mechanisms that drive cultural understanding differences.

Method: Researchers traced LLMs' internal cultural understanding by measuring activation path overlaps when answering semantically equivalent questions under different conditions: varying target country while fixing question language, and varying question language while fixing country, using same-language country pairs to separate language from cultural factors.

Result: Internal paths overlap more for same-language, cross-country questions than for cross-language, same-country questions, indicating strong language-specific patterns. The South Korea-North Korea pair showed low overlap and high variability, demonstrating that linguistic similarity doesn't guarantee aligned internal representation.

Conclusion: Language has a stronger influence on LLMs' internal representations than cultural context, and linguistic similarity between countries doesn't necessarily lead to similar internal processing of cultural information.

Abstract: Large language models (LLMs) are increasingly used across diverse cultural
contexts, making accurate cultural understanding essential. Prior evaluations
have mostly focused on output-level performance, obscuring the factors that
drive differences in responses, while studies using circuit analysis have
covered few languages and rarely focused on culture. In this work, we trace
LLMs' internal cultural understanding mechanisms by measuring activation path
overlaps when answering semantically equivalent questions under two conditions:
varying the target country while fixing the question language, and varying the
question language while fixing the country. We also use same-language country
pairs to disentangle language from cultural aspects. Results show that internal
paths overlap more for same-language, cross-country questions than for
cross-language, same-country questions, indicating strong language-specific
patterns. Notably, the South Korea-North Korea pair exhibits low overlap and
high variability, showing that linguistic similarity does not guarantee aligned
internal representation.

</details>


### [27] [Hallucination Benchmark for Speech Foundation Models](https://arxiv.org/abs/2510.16567)
*Alkis Koudounas,Moreno La Quatra,Manuel Giollo,Sabato Marco Siniscalchi,Elena Baralis*

Main category: cs.CL

TL;DR: SHALLOW is a benchmark framework that systematically categorizes and quantifies hallucinations in ASR systems across lexical, phonetic, morphological, and semantic axes, providing targeted metrics that capture fine-grained error patterns missed by conventional metrics like WER.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in ASR systems produce fluent but completely unrelated transcriptions that can mislead downstream applications, especially in critical domains like healthcare and law. Conventional error metrics fail to distinguish between phonetic inaccuracies and hallucinations, creating a need for specialized evaluation frameworks.

Method: Introduced SHALLOW framework with four complementary axes (lexical, phonetic, morphological, semantic) and defined targeted metrics within each category to produce interpretable profiles of model behavior. Evaluated across various architectures and speech domains.

Result: SHALLOW metrics correlate strongly with WER when recognition quality is high (low WER), but this correlation weakens substantially as WER increases. The framework captures fine-grained error patterns that WER fails to distinguish under degraded and challenging conditions.

Conclusion: SHALLOW enables specific diagnosis of model weaknesses and provides feedback for model improvement beyond what aggregate error rates can offer, addressing the critical need for hallucination-aware evaluation in ASR systems.

Abstract: Hallucinations in automatic speech recognition (ASR) systems refer to fluent
and coherent transcriptions produced by neural ASR models that are completely
unrelated to the underlying acoustic input (i.e., the speech signal). While
similar to conventional decoding errors in potentially compromising the
usability of transcriptions for downstream applications, hallucinations can be
more detrimental due to their preservation of syntactically and semantically
plausible structure. This apparent coherence can mislead subsequent processing
stages and introduce serious risks, particularly in critical domains such as
healthcare and law. Conventional evaluation metrics are primarily centered on
error-based metrics and fail to distinguish between phonetic inaccuracies and
hallucinations. Consequently, there is a critical need for new evaluation
frameworks that can effectively identify and assess models with a heightened
propensity for generating hallucinated content. To this end, we introduce
SHALLOW, the first benchmark framework that systematically categorizes and
quantifies hallucination phenomena in ASR along four complementary axes:
lexical, phonetic, morphological, and semantic. We define targeted metrics
within each category to produce interpretable profiles of model behavior.
Through evaluation across various architectures and speech domains, we have
found that SHALLOW metrics correlate strongly with word error rate (WER) when
recognition quality is high (i.e., low WER). Still, this correlation weakens
substantially as WER increases. SHALLOW, therefore, captures fine-grained error
patterns that WER fails to distinguish under degraded and challenging
conditions. Our framework supports specific diagnosis of model weaknesses and
provides feedback for model improvement beyond what aggregate error rates can
offer.

</details>


### [28] [AI-Generated Text Detection in Low-Resource Languages: A Case Study on Urdu](https://arxiv.org/abs/2510.16573)
*Muhammad Ammar,Hadiya Murad Hadi,Usman Majeed Butt*

Main category: cs.CL

TL;DR: Proposes an AI-generated text detection framework for Urdu using multilingual transformer models, achieving 91.26% accuracy with mDeBERTa-v3-base.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of detecting AI-generated text in Urdu, a low-resource language with few existing detection tools, to combat misinformation and academic misconduct.

Method: Developed a balanced dataset of 1,800 human and 1,800 AI-generated Urdu texts, conducted linguistic analysis, and fine-tuned three multilingual transformer models (mDeBERTa-v3-base, DistilBERT, XLM-RoBERTa).

Result: mDeBERTa-v3-base achieved the best performance with 91.29 F1-score and 91.26% accuracy on test data, significantly advancing Urdu AI text detection capabilities.

Conclusion: The framework successfully addresses the gap in Urdu AI text detection, contributing to misinformation combat and NLP tool development for low-resource languages.

Abstract: Large Language Models (LLMs) are now capable of generating text that closely
resembles human writing, making them powerful tools for content creation, but
this growing ability has also made it harder to tell whether a piece of text
was written by a human or by a machine. This challenge becomes even more
serious for languages like Urdu, where there are very few tools available to
detect AI-generated text. To address this gap, we propose a novel AI-generated
text detection framework tailored for the Urdu language. A balanced dataset
comprising 1,800 humans authored, and 1,800 AI generated texts, sourced from
models such as Gemini, GPT-4o-mini, and Kimi AI was developed. Detailed
linguistic and statistical analysis was conducted, focusing on features such as
character and word counts, vocabulary richness (Type Token Ratio), and N-gram
patterns, with significance evaluated through t-tests and MannWhitney U tests.
Three state-of-the-art multilingual transformer models such as
mdeberta-v3-base, distilbert-base-multilingualcased, and xlm-roberta-base were
fine-tuned on this dataset. The mDeBERTa-v3-base achieved the highest
performance, with an F1-score 91.29 and accuracy of 91.26% on the test set.
This research advances efforts in contesting misinformation and academic
misconduct in Urdu-speaking communities and contributes to the broader
development of NLP tools for low resource languages.

</details>


### [29] [Fine-tuning of Large Language Models for Constituency Parsing Using a Sequence to Sequence Approach](https://arxiv.org/abs/2510.16604)
*Francisco Jose Cortes Delgado,Eduardo Martinez Gracia,Rafael Valencia Garcia*

Main category: cs.CL

TL;DR: Fine-tuning large language models to translate sentences into syntactic structures for Spanish syntax analysis, achieving high accuracy.


<details>
  <summary>Details</summary>
Motivation: To extend the capabilities of MiSintaxis, a tool for teaching Spanish syntax, by leveraging recent advances in large neural models for natural language processing.

Method: Fine-tuned several LLMs from Hugging Face repository using training data generated from AnCora-ES corpus to translate input sentences into corresponding syntactic structures.

Result: The models demonstrated high accuracy in phrase-structure analysis as measured by F1 score.

Conclusion: This methodology shows significant potential for syntactic analysis using fine-tuned large language models.

Abstract: Recent advances in natural language processing with large neural models have
opened new possibilities for syntactic analysis based on machine learning. This
work explores a novel approach to phrase-structure analysis by fine-tuning
large language models (LLMs) to translate an input sentence into its
corresponding syntactic structure. The main objective is to extend the
capabilities of MiSintaxis, a tool designed for teaching Spanish syntax.
Several models from the Hugging Face repository were fine-tuned using training
data generated from the AnCora-ES corpus, and their performance was evaluated
using the F1 score. The results demonstrate high accuracy in phrase-structure
analysis and highlight the potential of this methodology.

</details>


### [30] [Unleashing Diverse Thinking Modes in LLMs through Multi-Agent Collaboration](https://arxiv.org/abs/2510.16645)
*Zhixuan He,Yue Feng*

Main category: cs.CL

TL;DR: DiMo is a multi-agent framework that enhances LLM performance and interpretability through structured debates among four specialized agents with different reasoning modes.


<details>
  <summary>Details</summary>
Motivation: LLMs demonstrate strong performance but often lack interpretable reasoning, needing better transparency and collaborative cognitive approaches.

Method: Simulates structured debate among four specialized LLM agents, each embodying distinct reasoning paradigms, through iterative challenge and refinement of initial responses.

Result: Improves accuracy over single-model and debate baselines across six benchmarks, with largest gains on math tasks, while providing explicit, auditable reasoning chains.

Conclusion: DiMo provides a semantics-aware multi-agent framework that combines retrieval-augmented reasoning with structured justifications for transparent and reusable explanations.

Abstract: Large Language Models (LLMs) demonstrate strong performance but often lack
interpretable reasoning. This paper introduces the Multi-Agent Collaboration
Framework for Diverse Thinking Modes (DiMo), which enhances both performance
and interpretability by simulating a structured debate among four specialized
LLM agents. Each agent embodies a distinct reasoning paradigm, allowing the
framework to collaboratively explore diverse cognitive approaches. Through
iterative debate, agents challenge and refine initial responses, yielding more
robust conclusions and an explicit, auditable reasoning chain. Across six
benchmarks and under a unified open-source setup, DiMo improves accuracy over
widely used single-model and debate baselines, with the largest gains on math.
We position DiMo as a semantics-aware, Web-native multi-agent framework: it
models human-machine intelligence with LLM agents that produce semantically
typed, URL-annotated evidence chains for explanations and user-friendly
interactions. Although our experiments use standard reasoning benchmarks, the
framework is designed to be instantiated over Web corpora and knowledge graphs,
combining retrieval-augmented reasoning with structured justifications that
downstream systems can inspect and reuse.

</details>


### [31] [All You Need is One: Capsule Prompt Tuning with a Single Vector](https://arxiv.org/abs/2510.16670)
*Yiyang Liu,James C. Liang,Heng Fan,Wenhao Yang,Yiming Cui,Xiaotian Han,Lifu Huang,Dongfang Liu,Qifan Wang,Cheng Han*

Main category: cs.CL

TL;DR: Capsule Prompt-Tuning (CaPT) introduces instance-aware information into prompt-based learning using a single capsule prompt, achieving superior performance and high parameter efficiency across various language tasks.


<details>
  <summary>Details</summary>
Motivation: Current prompt-based learning methods rely on laborious grid searching for optimal prompt length and lack instance-aware information, leading to suboptimal attention interplay with input sequences.

Method: CaPT integrates both instance-aware and task-aware information using a single capsule prompt that incorporates informative instance semantics at the earliest position of the sequence to serve as an "attention anchor".

Result: Empirical results show superior performance (e.g., 84.03% average accuracy on T5-Large) with high parameter efficiency (e.g., 0.003% of model parameters on Llama3.2-1B).

Conclusion: Incorporating instance-aware information as part of guidance can enhance prompt-tuned model performance without additional fine-tuning, and the "attention anchor" phenomenon helps preserve strong attention to critical structural information.

Abstract: Prompt-based learning has emerged as a parameter-efficient finetuning (PEFT)
approach to facilitate Large Language Model (LLM) adaptation to downstream
tasks by conditioning generation with task-aware guidance. Despite its
successes, current prompt-based learning methods heavily rely on laborious grid
searching for optimal prompt length and typically require considerable number
of prompts, introducing additional computational burden. Worse yet, our pioneer
findings indicate that the task-aware prompt design is inherently limited by
its absence of instance-aware information, leading to a subtle attention
interplay with the input sequence. In contrast, simply incorporating
instance-aware information as a part of the guidance can enhance the
prompt-tuned model performance without additional fine-tuning. Moreover, we
find an interesting phenomenon, namely "attention anchor", that incorporating
instance-aware tokens at the earliest position of the sequence can successfully
preserve strong attention to critical structural information and exhibit more
active attention interaction with all input tokens. In light of our
observation, we introduce Capsule Prompt-Tuning (CaPT), an efficient and
effective solution that leverages off-the-shelf, informative instance semantics
into prompt-based learning. Our approach innovatively integrates both
instance-aware and task-aware information in a nearly parameter-free manner
(i.e., one single capsule prompt). Empirical results demonstrate that our
method can exhibit superior performance across various language tasks (e.g.,
84.03\% average accuracy on T5-Large), serving as an "attention anchor," while
enjoying high parameter efficiency (e.g., 0.003\% of model parameters on
Llama3.2-1B).

</details>


### [32] [Temporal Understanding under Deictic Frame of Reference](https://arxiv.org/abs/2510.16685)
*Damin Zhang,Julia Rayz*

Main category: cs.CL

TL;DR: The paper introduces TUuD, a framework to evaluate how LLMs interpret temporal relations when the reference point of "now" dynamically shifts, finding that LLMs show partial adaptation to temporal frames of reference but with limitations.


<details>
  <summary>Details</summary>
Motivation: While LLMs have advanced in natural language understanding, their ability to interpret and reason about time remains limited, particularly in handling dynamic temporal reference points.

Method: The TUuD framework evaluates LLMs by prompting them to rate similarity between the current moment and target events on a 0.00-1.00 scale, testing how they adapt to shifting temporal frames of reference.

Result: Four evaluated LLMs showed measurable adaptation to deictic temporal frames of reference, with similarity ratings peaking around the present and decreasing toward past/future events, though adaptation weakened beyond near-term contexts.

Conclusion: LLMs display partial human-like temporal cognition but their temporal reasoning remains sensitive to reference-frame shifts and temporal distance, indicating limitations in temporal understanding.

Abstract: Understanding time is fundamental to human cognition, where temporal
experience is often conceptualized through spatial metaphors grounded in
sensory-motor experience. For example, "summer is approaching" parallels "We
are approaching the summer". In such expressions, humans rely on a frame of
reference (FoR) to interpret meaning relative to a particular viewpoint.
Extending this concept to time, a temporal frame of reference (t-FoR) defines
how temporal relations are perceived relative to an experiencer's moment of
"now". While Large Language Models (LLMs) have shown remarkable advances in
natural language understanding, their ability to interpret and reason about
time remains limited. In this work, we introduce TUuD (Temporal Understanding
under Deictic t-FoR), a framework that evaluates how LLMs interpret time-event
and event-event relations when the reference point of "now" dynamically shifts
along a timeline. Following recent work on temporal cognition
\cite{li2025other}, LLMs are prompted to rate the similarity between the
current moment and a target event from 0.00 (completely dissimilar) to 1.00
(highly similar), where similarity quantifies perceived temporal alignment
between the two points. Our results show that four evaluated LLMs exhibit
measurable adaptation to a deictic t-FoR, with similarity ratings peaking
around the present and decreasing toward past and future events. The
adaptation, however, weakens beyond near-term contexts, suggesting that while
LLMs display partial human-like temporal cognition, their temporal reasoning
remains sensitive to reference-frame shifts and temporal distance.

</details>


### [33] [Investigating the Impact of Rationales for LLMs on Natural Language Understanding](https://arxiv.org/abs/2510.16686)
*Wenhang Shi,Shuqing Bian,Yiren Chen,Xinyi Zhang,Zhe Zhao,Pengfei Hu,Wei Lu,Xiaoyong Du*

Main category: cs.CL

TL;DR: Chain-of-thought rationales benefit reasoning tasks but their impact on natural language understanding (NLU) tasks was unexplored. This work systematically investigates rationale-augmented methods for NLU tasks using the NLURC dataset.


<details>
  <summary>Details</summary>
Motivation: Previous work focused on rationales for reasoning tasks, overlooking their potential benefits for natural language understanding (NLU) tasks. The authors questioned whether rationales could similarly improve NLU performance.

Method: Constructed NLURC, a comprehensive NLU dataset collection with rationales, and developed various rationale-augmented methods to explore their applicability on NLU tasks.

Result: Three key findings: (1) CoT inference transitions from hindering to surpassing direct prediction as model size grows; (2) Most rationale-augmented training methods underperform label-only training, except one specially designed method; (3) LLMs trained with rationales achieve significant performance gains on unseen NLU tasks, rivaling much larger models while maintaining interpretability.

Conclusion: Rationales can significantly benefit NLU tasks, particularly enabling smaller models to match performance of much larger models on unseen tasks while providing interpretability comparable to commercial LLMs.

Abstract: Chain-of-thought (CoT) rationales, which provide step-by-step reasoning to
derive final answers, benefit LLMs in both inference and training.
Incorporating rationales, either by generating them before answering during
inference, or by placing them before or after the original answers during
training - significantly improves model performance on mathematical, symbolic
and commonsense reasoning tasks. However, most work focuses on the role of
rationales in these reasoning tasks, overlooking their potential impact on
other important tasks like natural language understanding (NLU) tasks. In this
work, we raise the question: Can rationales similarly benefit NLU tasks? To
conduct a systematic exploration, we construct NLURC, a comprehensive and
high-quality NLU dataset collection with rationales, and develop various
rationale-augmented methods. Through exploring the applicability of these
methods on NLU tasks using the dataset, we uncover several potentially
surprising findings: (1) CoT inference shifts from hindering NLU performance to
surpassing direct label prediction as model size grows, indicating a positive
correlation. (2) Most rationale-augmented training methods perform worse than
label-only training, with one specially designed method consistently achieving
improvements. (3) LLMs trained with rationales achieve significant performance
gains on unseen NLU tasks, rivaling models ten times their size, while
delivering interpretability on par with commercial LLMs.

</details>


### [34] [Natural Language Processing Applications in Cardiology: A Narrative Review](https://arxiv.org/abs/2510.16708)
*Kailai Yang,Yan Leng,Xin Zhang,Tianlin Zhang,Paul Thompson,Bernard Keavney,Maciej Tomaszewski,Sophia Ananiadou*

Main category: cs.CL

TL;DR: This review paper provides a comprehensive analysis of natural language processing (NLP) applications in cardiology from 2014 to 2025, covering 265 articles across multiple dimensions including NLP paradigms, cardiology tasks, disease types, and data sources.


<details>
  <summary>Details</summary>
Motivation: Cardiovascular disease is a complex, multifaceted health issue influenced by genetic, lifestyle, and socioeconomic factors. Information about these relationships is scattered across various textual sources like medical records and scientific literature, creating a need for NLP techniques to analyze this unstructured data and provide deeper insights for healthcare professionals.

Method: The authors conducted a systematic review by querying six literature databases to identify articles applying NLP techniques in cardiovascular disease contexts. After rigorous screening, they analyzed 265 relevant articles across multiple dimensions: NLP paradigm types, cardiology task types, cardiovascular disease types, and data source types, along with temporal analysis of trends over the decade.

Result: The analysis revealed considerable diversity across all dimensions studied, demonstrating the broad scope of NLP research in cardiology. The temporal analysis showed the evolution and changing trends in NLP methods used over the covered decade.

Conclusion: This review constitutes the most comprehensive overview of NLP research in cardiology to date, highlighting how NLP techniques can revolutionize approaches to diagnosis, treatment, and prevention of cardiac problems by extracting insights from diverse textual data sources.

Abstract: Cardiovascular disease has become increasingly prevalent in modern society
and has a significant effect on global health and well-being. Heart-related
conditions are intricate, multifaceted disorders, which may be influenced by a
combination of genetic predispositions, lifestyle choices, and various
socioeconomic and clinical factors. Information regarding these potentially
complex interrelationships is dispersed among diverse types of textual data,
which include patient narratives, medical records, and scientific literature,
among others. Natural language processing (NLP) techniques have increasingly
been adopted as a powerful means to analyse and make sense of this vast amount
of unstructured data. This, in turn, can allow healthcare professionals to gain
deeper insights into the cardiology field, which has the potential to
revolutionize current approaches to the diagnosis, treatment, and prevention of
cardiac problems. This review provides a detailed overview of NLP research in
cardiology between 2014 and 2025. We queried six literature databases to find
articles describing the application of NLP techniques in the context of a range
of different cardiovascular diseases. Following a rigorous screening process,
we identified a total of 265 relevant articles. We analysed each article from
multiple dimensions, i.e., NLP paradigm types, cardiology-related task types,
cardiovascular disease types, and data source types. Our analysis reveals
considerable diversity within each of these dimensions, thus demonstrating the
considerable breadth of NLP research within the field. We also perform a
temporal analysis, which illustrates the evolution and changing trends in NLP
methods employed over the last decade that we cover. To our knowledge, the
review constitutes the most comprehensive overview of NLP research in
cardiology to date.

</details>


### [35] [The Chameleon Nature of LLMs: Quantifying Multi-Turn Stance Instability in Search-Enabled Language Models](https://arxiv.org/abs/2510.16712)
*Shivam Ratnakar,Sanjay Raghavendra*

Main category: cs.CL

TL;DR: LLMs exhibit 'chameleon behavior' - shifting stances when presented with contradictory questions in multi-turn conversations, especially in search-enabled systems, revealing fundamental reliability flaws.


<details>
  <summary>Details</summary>
Motivation: To systematically investigate stance instability in LLMs during multi-turn conversations, as this vulnerability undermines reliability in critical applications like healthcare, legal, and financial systems.

Method: Created Chameleon Benchmark Dataset with 17,770 question-answer pairs across 1,180 multi-turn conversations spanning 12 controversial domains. Introduced Chameleon Score (0-1) for stance instability and Source Re-use Rate (0-1) for knowledge diversity. Evaluated Llama-4-Maverick, GPT-4o-mini, and Gemini-2.5-Flash.

Result: All models exhibited severe chameleon behavior (scores 0.391-0.511), with GPT-4o-mini performing worst. Strong correlations found between source re-use rate and confidence (r=0.627) and stance changes (r=0.429), statistically significant (p<0.05). Small temperature variance (<0.004) shows effect is not sampling artifact.

Conclusion: Limited knowledge diversity makes LLMs pathologically deferential to query framing, highlighting critical need for comprehensive consistency evaluation before deployment in systems requiring coherent positions across interactions.

Abstract: Integration of Large Language Models with search/retrieval engines has become
ubiquitous, yet these systems harbor a critical vulnerability that undermines
their reliability. We present the first systematic investigation of "chameleon
behavior" in LLMs: their alarming tendency to shift stances when presented with
contradictory questions in multi-turn conversations (especially in
search-enabled LLMs). Through our novel Chameleon Benchmark Dataset, comprising
17,770 carefully crafted question-answer pairs across 1,180 multi-turn
conversations spanning 12 controversial domains, we expose fundamental flaws in
state-of-the-art systems. We introduce two theoretically grounded metrics: the
Chameleon Score (0-1) that quantifies stance instability, and Source Re-use
Rate (0-1) that measures knowledge diversity. Our rigorous evaluation of
Llama-4-Maverick, GPT-4o-mini, and Gemini-2.5-Flash reveals consistent
failures: all models exhibit severe chameleon behavior (scores 0.391-0.511),
with GPT-4o-mini showing the worst performance. Crucially, small
across-temperature variance (less than 0.004) suggests the effect is not a
sampling artifact. Our analysis uncovers the mechanism: strong correlations
between source re-use rate and confidence (r=0.627) and stance changes
(r=0.429) are statistically significant (p less than 0.05), indicating that
limited knowledge diversity makes models pathologically deferential to query
framing. These findings highlight the need for comprehensive consistency
evaluation before deploying LLMs in healthcare, legal, and financial systems
where maintaining coherent positions across interactions is critical for
reliable decision support.

</details>


### [36] [so much depends / upon / a whitespace: Why Whitespace Matters for Poets and LLMs](https://arxiv.org/abs/2510.16713)
*Sriharsh Bhyravajjula,Melanie Walsh,Anna Preus,Maria Antoniak*

Main category: cs.CL

TL;DR: This paper analyzes whitespace usage in poetry across published works, LLM-generated poems, and unpublished online poems, revealing significant differences and implications for LLM pretraining data processing.


<details>
  <summary>Details</summary>
Motivation: Whitespace is a critical but understudied component of poetic form that reflects artistic choices, yet has been largely ignored by the NLP community despite poetry's popularity as an art form and LLM generation task.

Method: Analyzed 19k English-language published poems from Poetry Foundation, comparing whitespace usage across 4k poets, 51k LLM-generated poems, and 12k unpublished online poems, while examining variations across time periods, poetic forms, and data sources.

Result: Found significant differences in whitespace usage between published poems, LLM-generated poems, and unpublished online poems. Different text processing methods produce substantially different whitespace representations, impacting how poetry data is handled in LLM pretraining.

Conclusion: Whitespace patterns in poetry reveal important artistic and semantic information that current NLP approaches overlook. The study highlights the need for more careful consideration of whitespace in poetry processing and LLM training data preparation, releasing 2.8k public-domain poems with preserved formatting to support future research.

Abstract: Whitespace is a critical component of poetic form, reflecting both adherence
to standardized forms and rebellion against those forms. Each poem's whitespace
distribution reflects the artistic choices of the poet and is an integral
semantic and spatial feature of the poem. Yet, despite the popularity of poetry
as both a long-standing art form and as a generation task for large language
models (LLMs), whitespace has not received sufficient attention from the NLP
community. Using a corpus of 19k English-language published poems from Poetry
Foundation, we investigate how 4k poets have used whitespace in their works. We
release a subset of 2.8k public-domain poems with preserved formatting to
facilitate further research in this area. We compare whitespace usage in the
published poems to (1) 51k LLM-generated poems, and (2) 12k unpublished poems
posted in an online community. We also explore whitespace usage across time
periods, poetic forms, and data sources. Additionally, we find that different
text processing methods can result in significantly different representations
of whitespace in poetry data, motivating us to use these poems and whitespace
patterns to discuss implications for the processing strategies used to assemble
pretraining datasets for LLMs.

</details>


### [37] [Beacon: Single-Turn Diagnosis and Mitigation of Latent Sycophancy in Large Language Models](https://arxiv.org/abs/2510.16727)
*Sanskar Pandey,Ruhaan Chopra,Angkul Puniya,Sohom Pal*

Main category: cs.CL

TL;DR: The paper introduces Beacon, a benchmark to measure sycophancy in LLMs - the bias where models prioritize polite agreement over factual accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the structural trade-off between truthfulness and obsequious flattery in LLMs that emerges from reward optimization conflating helpfulness with polite submission.

Method: Introduces Beacon, a single-turn forced-choice benchmark that isolates sycophancy bias independent of conversational context, and proposes both prompt-level and activation-level interventions to modulate these biases.

Result: Evaluations across twelve state-of-the-art models reveal that sycophancy decomposes into stable linguistic and affective sub-biases that scale with model capacity, and interventions can modulate these biases in opposing directions.

Conclusion: Beacon reframes sycophancy as measurable normative misgeneralization, providing a reproducible foundation for studying and mitigating alignment drift in large-scale generative systems.

Abstract: Large language models internalize a structural trade-off between truthfulness
and obsequious flattery, emerging from reward optimization that conflates
helpfulness with polite submission. This latent bias, known as sycophancy,
manifests as a preference for user agreement over principled reasoning. We
introduce Beacon, a single-turn forced-choice benchmark that isolates this bias
independent of conversational context, enabling precise measurement of the
tension between factual accuracy and submissive bias. Evaluations across twelve
state-of-the-art models reveal that sycophancy decomposes into stable
linguistic and affective sub-biases, each scaling with model capacity. We
further propose prompt-level and activation-level interventions that modulate
these biases in opposing directions, exposing the internal geometry of
alignment as a dynamic manifold between truthfulness and socially compliant
judgment. Beacon reframes sycophancy as a measurable form of normative
misgeneralization, providing a reproducible foundation for studying and
mitigating alignment drift in large-scale generative systems.

</details>


### [38] [Enhancing Language Agent Strategic Reasoning through Self-Play in Adversarial Games](https://arxiv.org/abs/2510.16761)
*Yikai Zhang,Ye Rong,Siyu Yuan,Jiangjie Chen,Jian Xie,Yanghua Xiao*

Main category: cs.CL

TL;DR: SCO-PAL is a step-level policy optimization method that uses self-play in adversarial games to improve strategic reasoning, achieving significant win rate improvements over baselines and competitive performance against GPT-4.


<details>
  <summary>Details</summary>
Motivation: Existing language agents struggle with strategic reasoning in dynamic adversarial games, and current approaches rely on costly expert data. The impact of opponent selection on learning performance in such environments remains underexplored.

Method: Proposed SCO-PAL (Step-level poliCy Optimization through Play-And-Learn), which analyzes opponent selection by testing opponents at different levels and identifies self-play as the most effective approach for improving strategic reasoning.

Result: SCO-PAL with self-play increased average win rate against four opponents by approximately 30% compared to baselines, and achieved a 54.76% win rate against GPT-4 in six adversarial games.

Conclusion: Self-play is the most effective opponent selection strategy for improving strategic reasoning in dynamic adversarial games, as demonstrated by the success of SCO-PAL.

Abstract: Existing language agents often encounter difficulties in dynamic adversarial
games due to poor strategic reasoning. To mitigate this limitation, a promising
approach is to allow agents to learn from game interactions automatically,
without relying on costly expert-labeled data. Unlike static environments where
agents receive fixed feedback or rewards, selecting appropriate opponents in
dynamic adversarial games can significantly impact learning performance.
However, the discussion of opponents in adversarial environments remains an
area under exploration. In this paper, we propose a Step-level poliCy
Optimization method through Play-And-Learn, SCO-PAL. Leveraging SCO-PAL, we
conduct a detailed analysis of opponent selection by setting opponents at
different levels and find that self-play is the most effective way to improve
strategic reasoning in such adversarial environments. Utilizing SCO-PAL with
self-play, we increase the average win rate against four opponents by
approximately 30% compared to baselines and achieve a 54.76% win rate against
GPT-4 in six adversarial games.

</details>


### [39] [LC-Eval: A Bilingual Multi-Task Evaluation Benchmark for Long-Context Understanding](https://arxiv.org/abs/2510.16783)
*Sheikh Jubair,Arwa Omayrah,Amal Alshammari,Alhanoof Althnian,Abdulhamed Alothaimen,Norah A. Alzahrani,Shahad D. Alzaidi,Nora Al-Twairesh,Abdulmohsen Al-Thubaity*

Main category: cs.CL

TL;DR: LC-Eval is a bilingual (English-Arabic) benchmark for evaluating long-context understanding in LLMs, featuring four challenging tasks across 4k-128k token contexts.


<details>
  <summary>Details</summary>
Motivation: Recent LLM advancements in long-context processing require rigorous evaluation methods to assess their performance in extended context understanding.

Method: Developed LC-Eval benchmark with four novel tasks: multi-document QA, bilingual QA, claim verification, and multiple-choice questions using long contexts in both English and Arabic.

Result: Evaluation showed LC-Eval presents significant challenges - even high-performing models like GPT-4o struggled with certain tasks, demonstrating the benchmark's complexity.

Conclusion: LC-Eval effectively assesses LLMs' long-context understanding capabilities and reveals current limitations, providing a rigorous evaluation framework for future model development.

Abstract: Recent advancements in Large Language Models (LLMs) have demonstrated
sophisticated capabilities, including the ability to process and comprehend
extended contexts. These emergent capabilities necessitate rigorous evaluation
methods to effectively assess their performance in long-context understanding.
In this paper, we present \textbf{LC-Eval}, a bilingual, multi-task evaluation
benchmark designed to evaluate long-context understanding in English and
Arabic, targeting context lengths ranging from 4k to over 128k tokens. LC-Eval
introduces four novel and challenging tasks: multi-document question answering,
bilingual question answering, claim verification within a paragraph, and
multiple-choice questions based on long contexts. These tasks are designed to
assess LLMs' abilities in deep reasoning, document comprehension, information
tracing, and bilingual information extraction and understanding. The benchmark
includes datasets in both Arabic and English for each task, allowing for a
comparative analysis of their performance across different text genres.
Evaluations were conducted on both open-weight and closed LLMs, with results
indicating that LC-Eval presents significant challenges. Even high-performing
models, such as GPT-4o, struggled with certain tasks, highlighting the
complexity and rigor of the benchmark.

</details>


### [40] [MOSAIC: Masked Objective with Selective Adaptation for In-domain Contrastive Learning](https://arxiv.org/abs/2510.16797)
*Vera Pavlova,Mohammed Makhlouf*

Main category: cs.CL

TL;DR: MOSAIC is a multi-stage framework for domain adaptation of sentence embedding models that combines masked language modeling and contrastive learning objectives to adapt general-domain models to specialized domains.


<details>
  <summary>Details</summary>
Motivation: Address the challenges of adapting large-scale general-domain sentence embedding models to specialized domains while preserving robust semantic discrimination properties.

Method: Multi-stage framework with joint domain-specific masked supervision, combining masked language modeling (MLM) and contrastive objectives in a unified training pipeline.

Result: Achieved improvements up to 13.4% in NDCG@10 over strong general-domain baselines, validated on both high-resource and low-resource domains.

Conclusion: The framework effectively learns domain-relevant representations while maintaining semantic discrimination, with ablation studies confirming the importance of balanced joint supervision and staged adaptation.

Abstract: We introduce MOSAIC (Masked Objective with Selective Adaptation for In-domain
Contrastive learning), a multi-stage framework for domain adaptation of
sentence embedding models that incorporates joint domain-specific masked
supervision. Our approach addresses the challenges of adapting large-scale
general-domain sentence embedding models to specialized domains. By jointly
optimizing masked language modeling (MLM) and contrastive objectives within a
unified training pipeline, our method enables effective learning of
domain-relevant representations while preserving the robust semantic
discrimination properties of the original model. We empirically validate our
approach on both high-resource and low-resource domains, achieving improvements
up to 13.4% in NDCG@10 (Normalized Discounted Cumulative Gain) over strong
general-domain baselines. Comprehensive ablation studies further demonstrate
the effectiveness of each component, highlighting the importance of balanced
joint supervision and staged adaptation.

</details>


### [41] [Knowing the Facts but Choosing the Shortcut: Understanding How Large Language Models Compare Entities](https://arxiv.org/abs/2510.16815)
*Hans Hergen Lehmann,Jae Hee Lee,Steven Schockaert,Stefan Wermter*

Main category: cs.CL

TL;DR: LLMs often use heuristic biases instead of genuine knowledge for entity comparison tasks, with larger models showing better discrimination in when to rely on numerical knowledge versus heuristics.


<details>
  <summary>Details</summary>
Motivation: To understand when LLMs rely on genuine knowledge versus superficial heuristics in knowledge-based reasoning tasks, using entity comparison as a test case with clear ground truth.

Method: Analyze entity comparison tasks with numerical attributes, identify three heuristic biases (entity popularity, mention order, semantic co-occurrence), and test models of different sizes (7B-32B parameters) with and without chain-of-thought prompting.

Result: Smaller models' predictions are better predicted by surface cues than their own numerical knowledge, while larger models selectively use numerical knowledge when reliable. Chain-of-thought prompting improves numerical feature usage across all model sizes.

Conclusion: Larger models better discriminate when to use knowledge versus heuristics, explaining their superior performance despite smaller models sometimes having more accurate knowledge. Chain-of-thought prompting effectively steers models toward principled reasoning.

Abstract: Large Language Models (LLMs) are increasingly used for knowledge-based
reasoning tasks, yet understanding when they rely on genuine knowledge versus
superficial heuristics remains challenging. We investigate this question
through entity comparison tasks by asking models to compare entities along
numerical attributes (e.g., ``Which river is longer, the Danube or the
Nile?''), which offer clear ground truth for systematic analysis. Despite
having sufficient numerical knowledge to answer correctly, LLMs frequently make
predictions that contradict this knowledge. We identify three heuristic biases
that strongly influence model predictions: entity popularity, mention order,
and semantic co-occurrence. For smaller models, a simple logistic regression
using only these surface cues predicts model choices more accurately than the
model's own numerical predictions, suggesting heuristics largely override
principled reasoning. Crucially, we find that larger models (32B parameters)
selectively rely on numerical knowledge when it is more reliable, while smaller
models (7--8B parameters) show no such discrimination, which explains why
larger models outperform smaller ones even when the smaller models possess more
accurate knowledge. Chain-of-thought prompting steers all models towards using
the numerical features across all model sizes.

</details>


### [42] [Cross-Genre Authorship Attribution via LLM-Based Retrieve-and-Rerank](https://arxiv.org/abs/2510.16819)
*Shantanu Agarwal,Joel Barry,Steven Fincke,Scott Miller*

Main category: cs.CL

TL;DR: A two-stage retrieve-and-rerank framework using LLMs for cross-genre authorship attribution that achieves significant improvements over previous state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Traditional authorship attribution systems often rely on topical cues, but cross-genre AA requires identifying author-specific linguistic patterns that are independent of text subject matter.

Method: Two-stage retrieve-and-rerank framework with LLM finetuning, featuring a targeted data curation strategy to teach the reranker to learn author-discriminative signals rather than topical patterns.

Result: Achieved substantial gains of 22.3 and 34.4 absolute Success@8 points over previous state-of-the-art on HIATUS's HRS1 and HRS2 cross-genre AA benchmarks.

Conclusion: The proposed LLM-based retrieve-and-rerank pipeline with targeted data curation effectively addresses the unique challenges of cross-genre authorship attribution by focusing on author-specific linguistic patterns rather than topical cues.

Abstract: Authorship attribution (AA) is the task of identifying the most likely author
of a query document from a predefined set of candidate authors. We introduce a
two-stage retrieve-and-rerank framework that finetunes LLMs for cross-genre AA.
Unlike the field of information retrieval (IR), where retrieve-and-rerank is a
de facto strategy, cross-genre AA systems must avoid relying on topical cues
and instead learn to identify author-specific linguistic patterns that are
independent of the text's subject matter (genre/domain/topic). Consequently,
for the reranker, we demonstrate that training strategies commonly used in IR
are fundamentally misaligned with cross-genre AA, leading to suboptimal
behavior. To address this, we introduce a targeted data curation strategy that
enables the reranker to effectively learn author-discriminative signals. Using
our LLM-based retrieve-and-rerank pipeline, we achieve substantial gains of
22.3 and 34.4 absolute Success@8 points over the previous state-of-the-art on
HIATUS's challenging HRS1 and HRS2 cross-genre AA benchmarks.

</details>


### [43] [Who's Asking? Simulating Role-Based Questions for Conversational AI Evaluation](https://arxiv.org/abs/2510.16829)
*Navreet Kaur,Hoda Ayad,Hayoung Jung,Shravika Mittal,Munmun De Choudhury,Tanushree Mitra*

Main category: cs.CL

TL;DR: CoRUS framework simulates role-based questions for LLM evaluation, showing that user roles (patients, caregivers, practitioners) significantly influence model responses - vulnerable roles get more support but less knowledge.


<details>
  <summary>Details</summary>
Motivation: Current LLM evaluations ignore user context and roles, which is critical in sensitive domains like opioid use disorder where stigma-free, appropriate responses depend on who's asking.

Method: Built role taxonomy from online OUD community, then simulated 15,321 questions embedding each role's goals/experiences. Evaluated 5 LLMs on these role-based questions.

Result: Systematic response differences: vulnerable roles (patients/caregivers) received 17% more supportive responses but 19% less knowledge content compared to practitioner roles.

Conclusion: User roles implicitly shape LLM responses, demonstrating need for role-informed evaluation methodology in conversational AI.

Abstract: Language model users often embed personal and social context in their
questions. The asker's role -- implicit in how the question is framed --
creates specific needs for an appropriate response. However, most evaluations,
while capturing the model's capability to respond, often ignore who is asking.
This gap is especially critical in stigmatized domains such as opioid use
disorder (OUD), where accounting for users' contexts is essential to provide
accessible, stigma-free responses. We propose CoRUS (COmmunity-driven Roles for
User-centric Question Simulation), a framework for simulating role-based
questions. Drawing on role theory and posts from an online OUD recovery
community (r/OpiatesRecovery), we first build a taxonomy of asker roles --
patients, caregivers, practitioners. Next, we use it to simulate 15,321
questions that embed each role's goals, behaviors, and experiences. Our
evaluations show that these questions are both highly believable and comparable
to real-world data. When used to evaluate five LLMs, for the same question but
differing roles, we find systematic differences: vulnerable roles, such as
patients and caregivers, elicit more supportive responses (+17%) and reduced
knowledge content (-19%) in comparison to practitioners. Our work demonstrates
how implicitly signaling a user's role shapes model responses, and provides a
methodology for role-informed evaluation of conversational AI.

</details>


### [44] [FinSight: Towards Real-World Financial Deep Research](https://arxiv.org/abs/2510.16844)
*Jiajie Jin,Yuyao Zhang,Yimeng Xu,Hongjin Qian,Yutao Zhu,Zhicheng Dou*

Main category: cs.CL

TL;DR: FinSight is a multi-agent framework that automates high-quality financial report generation using a Code Agent with Variable Memory architecture, iterative visualization refinement, and a two-stage writing framework.


<details>
  <summary>Details</summary>
Motivation: Current AI systems struggle to fully automate the labor-intensive and intellectually demanding process of generating professional financial reports.

Method: Uses Code Agent with Variable Memory (CAVM) architecture for flexible data handling, Iterative Vision-Enhanced Mechanism for chart refinement, and two-stage Writing Framework for expanding analysis into multimodal reports.

Result: FinSight significantly outperforms all baselines including leading deep research systems in factual accuracy, analytical depth, and presentation quality across various company and industry-level tasks.

Conclusion: The framework demonstrates a clear path toward generating financial reports that approach human-expert quality.

Abstract: Generating professional financial reports is a labor-intensive and
intellectually demanding process that current AI systems struggle to fully
automate. To address this challenge, we introduce FinSight (Financial InSight),
a novel multi agent framework for producing high-quality, multimodal financial
reports. The foundation of FinSight is the Code Agent with Variable Memory
(CAVM) architecture, which unifies external data, designed tools, and agents
into a programmable variable space, enabling flexible data collection, analysis
and report generation through executable code. To ensure professional-grade
visualization, we propose an Iterative Vision-Enhanced Mechanism that
progressively refines raw visual outputs into polished financial charts.
Furthermore, a two stage Writing Framework expands concise Chain-of-Analysis
segments into coherent, citation-aware, and multimodal reports, ensuring both
analytical depth and structural consistency. Experiments on various company and
industry-level tasks demonstrate that FinSight significantly outperforms all
baselines, including leading deep research systems in terms of factual
accuracy, analytical depth, and presentation quality, demonstrating a clear
path toward generating reports that approach human-expert quality.

</details>


### [45] [Neuronal Group Communication for Efficient Neural representation](https://arxiv.org/abs/2510.16851)
*Zhengqi Pei,Qingming Huang,Shuhui Wang*

Main category: cs.CL

TL;DR: Proposes Neuronal Group Communication (NGC), a framework treating neural networks as dynamical systems of interacting neuronal groups rather than independent weights, enabling efficient, modular, and interpretable representations with improved reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: Address challenges of efficiency and interpretability in large neural networks by moving from monolithic weight collections to modular neuronal group interactions.

Method: Treat weights as transient interactions between neuronal states, with computation through iterative group communication. Introduce neuronal stability metric based on dynamical systems theory to quantify contraction toward stable patterns during sequence processing.

Result: NGC instantiated in LLMs shows improved performance on complex reasoning benchmarks under moderate compression, outperforming standard low-rank approximations and cross-layer basis-sharing methods at comparable compression rates.

Conclusion: NGC provides a theory-driven framework for building efficient, modular neural systems, with neuronal group dynamics potentially relating to generalization in high-dimensional learning systems.

Abstract: The ever-increasing scale of modern neural networks has brought unprecedented
performance alongside daunting challenges in efficiency and interpretability.
This paper addresses the core question of how to build large neural systems
that learn efficient, modular, and interpretable representations. We propose
Neuronal Group Communication (NGC), a theory-driven framework that reimagines a
neural network as a dynamical system of interacting neuronal groups rather than
a monolithic collection of neural weights. Instead of treating each weight as
an independent trainable parameter, NGC treats weights as transient
interactions between embedding-like neuronal states, with neural computation
unfolding through iterative communication among groups of neurons. This
low-rank, modular representation yields compact models: groups of neurons
exchange low-dimensional signals, enabling intra-group specialization and
inter-group information sharing while dramatically reducing redundant
parameters. By drawing on dynamical systems theory, we introduce a neuronal
stability metric (analogous to Lyapunov stability) that quantifies the
contraction of neuron activations toward stable patterns during sequence
processing. Using this metric, we reveal that emergent reasoning capabilities
correspond to an external driving force or ``potential'', which nudges the
neural dynamics away from trivial trajectories while preserving stability.
Empirically, we instantiate NGC in large language models (LLMs) and demonstrate
improved performance on complex reasoning benchmarks under moderate
compression. NGC consistently outperforms standard low-rank approximations and
cross-layer basis-sharing methods at comparable compression rates. We conclude
by discussing the broader implications of NGC, including how structured
neuronal group dynamics might relate to generalization in high-dimensional
learning systems.

</details>


### [46] [Does Visual Grounding Enhance the Understanding of Embodied Knowledge in Large Language Models?](https://arxiv.org/abs/2510.16924)
*Zhihui Yang,Yupei Wang,Kaijie Mo,Zhe Zhao,Renfen Hu*

Main category: cs.CL

TL;DR: Vision-language models don't outperform text-only models in embodied knowledge understanding, performing worst in visual tasks despite having visual grounding capabilities.


<details>
  <summary>Details</summary>
Motivation: To determine whether visual grounding actually enhances multimodal language models' understanding of embodied knowledge compared to text-only models.

Method: Created a novel benchmark based on perceptual theory covering multiple sensory modalities (visual, auditory, tactile, gustatory, olfactory, interoception) with over 1,700 questions using vector comparison and question-answering tasks. Evaluated 30 state-of-the-art language models.

Result: Vision-language models did not outperform text-only models in either task. Models performed significantly worse in visual dimension compared to other sensory dimensions. Vector representations were influenced by word form and frequency, and models struggled with spatial perception and reasoning questions.

Conclusion: Current multimodal models need more effective integration of embodied knowledge to better understand the physical world, as visual grounding alone doesn't provide the expected advantage.

Abstract: Despite significant progress in multimodal language models (LMs), it remains
unclear whether visual grounding enhances their understanding of embodied
knowledge compared to text-only models. To address this question, we propose a
novel embodied knowledge understanding benchmark based on the perceptual theory
from psychology, encompassing visual, auditory, tactile, gustatory, olfactory
external senses, and interoception. The benchmark assesses the models'
perceptual abilities across different sensory modalities through vector
comparison and question-answering tasks with over 1,700 questions. By comparing
30 state-of-the-art LMs, we surprisingly find that vision-language models
(VLMs) do not outperform text-only models in either task. Moreover, the models
perform significantly worse in the visual dimension compared to other sensory
dimensions. Further analysis reveals that the vector representations are easily
influenced by word form and frequency, and the models struggle to answer
questions involving spatial perception and reasoning. Our findings underscore
the need for more effective integration of embodied knowledge in LMs to enhance
their understanding of the physical world.

</details>


### [47] [ChiKhaPo: A Large-Scale Multilingual Benchmark for Evaluating Lexical Comprehension and Generation in Large Language Models](https://arxiv.org/abs/2510.16928)
*Emily Chang,Niyati Bafna*

Main category: cs.CL

TL;DR: ChiKhaPo is a new benchmark for evaluating LLMs' basic linguistic competence across 2700+ languages, focusing on lexical comprehension and generation rather than higher-order reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Existing LLM benchmarks are limited to high/mid-resource languages and focus on reasoning/generation tasks, while LLMs lack basic linguistic competence in most of the world's 3800+ written languages.

Method: Created ChiKhaPo with 8 subtasks of varying difficulty using existing lexicons, monolingual data, and bitext to evaluate lexical comprehension and generation abilities across 2700+ languages.

Result: 6 state-of-the-art models struggle on the benchmark, with performance influenced by language family, resource availability, task type, and comprehension vs generation directions.

Conclusion: ChiKhaPo enables massively multilingual benchmarking of LLMs and encourages development of models with better basic linguistic competence across diverse languages.

Abstract: Existing benchmarks for large language models (LLMs) are largely restricted
to high- or mid-resource languages, and often evaluate performance on
higher-order tasks in reasoning and generation. However, plenty of evidence
points to the fact that LLMs lack basic linguistic competence in the vast
majority of the world's 3800+ written languages. We introduce ChiKhaPo,
consisting of 8 subtasks of varying difficulty designed to evaluate the lexical
comprehension and generation abilities of generative models. ChiKhaPo draws on
existing lexicons, monolingual data, and bitext, and provides coverage for
2700+ languages for 2 subtasks, surpassing any existing benchmark in terms of
language coverage. We further show that 6 SOTA models struggle on our
benchmark, and discuss the factors contributing to performance scores,
including language family, language resourcedness, task, and comprehension
versus generation directions. With ChiKhaPo, we hope to enable and encourage
the massively multilingual benchmarking of LLMs.

</details>


### [48] [Prompt-MII: Meta-Learning Instruction Induction for LLMs](https://arxiv.org/abs/2510.16932)
*Emily Xiao,Yixiao Zeng,Ada Chen,Chin-Jou Li,Amanda Bertsch,Graham Neubig*

Main category: cs.CL

TL;DR: PROMPT-MII is a reinforcement learning framework that meta-learns to generate compact instructions from training examples, achieving comparable performance to in-context learning while using 3-13x fewer tokens.


<details>
  <summary>Details</summary>
Motivation: In-context learning (ICL) for LLM adaptation is effective but incurs high inference costs as context length grows, creating a need for more efficient methods.

Method: PROMPT-MII uses reinforcement learning to meta-learn an instruction induction model that generates compact, descriptive prompts from training examples for arbitrary new datasets.

Result: The method improves downstream model quality by 4-9 F1 points (10-20% relative), matching ICL performance while requiring 3-13x fewer tokens on 90 unseen tasks.

Conclusion: PROMPT-MII provides an efficient alternative to ICL by generating compact instructions that maintain performance while significantly reducing token usage.

Abstract: A popular method to adapt large language models (LLMs) to new tasks is
in-context learning (ICL), which is effective but incurs high inference costs
as context length grows. In this paper we propose a method to perform
instruction induction, where we take training examples and reduce them to a
compact but descriptive prompt that can achieve performance comparable to ICL
over the full training set. Specifically, we propose PROMPT-MII, a
reinforcement learning (RL) based framework to meta-learn an instruction
induction model that can generate compact instructions on the fly for an
arbitrary new dataset. We train on over 3,000 diverse classification datasets
from the HuggingFace hub, and evaluate on 90 unseen tasks. PROMPT-MII improves
downstream model quality by 4-9 F1 points (10-20% relative), matching ICL
performance while requiring 3-13x fewer tokens.

</details>


### [49] [Parameter-Efficient Fine-Tuning for Low-Resource Languages: A Comparative Study of LLMs for Bengali Hate Speech Detection](https://arxiv.org/abs/2510.16985)
*Akif Islam,Mohd Ruhul Ameen*

Main category: cs.CL

TL;DR: First application of Parameter-Efficient Fine-Tuning (PEFT) using LoRA and QLoRA for Bengali hate speech detection, achieving up to 92.23% F1-score while training less than 1% of parameters.


<details>
  <summary>Details</summary>
Motivation: Bengali social media has seen sharp increase in hate speech affecting women and adolescents, with prior approaches relying on computationally costly full-model fine-tuning or proprietary APIs.

Method: Fine-tuned three instruction-tuned LLMs (Gemma-3-4B, Llama-3.2-3B, Mistral-7B) on BD-SHS dataset of 50,281 comments using PEFT with LoRA and QLoRA, training fewer than 1% of parameters on a single consumer-grade GPU.

Result: Llama-3.2-3B achieved highest F1-score of 92.23%, followed by Mistral-7B at 88.94% and Gemma-3-4B at 80.25%.

Conclusion: PEFT established as practical and replicable strategy for Bengali and related low-resource languages, enabling efficient hate speech detection with minimal computational resources.

Abstract: Bengali social media platforms have witnessed a sharp increase in hate
speech, disproportionately affecting women and adolescents. While datasets such
as BD-SHS provide a basis for structured evaluation, most prior approaches rely
on either computationally costly full-model fine-tuning or proprietary APIs.
This paper presents the first application of Parameter-Efficient Fine-Tuning
(PEFT) for Bengali hate speech detection using LoRA and QLoRA. Three
instruction-tuned large language models - Gemma-3-4B, Llama-3.2-3B, and
Mistral-7B - were fine-tuned on the BD-SHS dataset of 50,281 annotated
comments. Each model was adapted by training fewer than 1% of its parameters,
enabling experiments on a single consumer-grade GPU. The results show that
Llama-3.2-3B achieved the highest F1-score of 92.23%, followed by Mistral-7B at
88.94% and Gemma-3-4B at 80.25%. These findings establish PEFT as a practical
and replicable strategy for Bengali and related low-resource languages.

</details>


### [50] [Back to Bytes: Revisiting Tokenization Through UTF-8](https://arxiv.org/abs/2510.16987)
*Amit Moryossef,Clara Meister,Pavel Stepachev,Desmond Elliott*

Main category: cs.CL

TL;DR: UTF8Tokenizer is a byte-level tokenizer that maps text directly to UTF-8 byte IDs, using C0 control bytes for special behavior instead of auxiliary tokens, offering faster processing and smaller embedding tables.


<details>
  <summary>Details</summary>
Motivation: To create a minimalist tokenizer that avoids out-of-range IDs and auxiliary tokens while leveraging UTF-8's native byte structure for efficiency and cross-model compatibility.

Method: Direct mapping of text to UTF-8 byte IDs (0-255), using C0 control bytes for special functions, with optional bit-biased embeddings that expose byte-level bit structure.

Result: Achieves 14x faster tokenization, 8x less host-device transfer than int64, enables shareable 256*d embedding tables, and improves language modeling convergence.

Conclusion: UTF8Tokenizer provides an efficient, minimalist approach to tokenization that leverages UTF-8's native structure for practical benefits in speed, memory usage, and model compatibility.

Abstract: We present UTF8Tokenizer, a minimalist byte-level tokenizer that maps text
exactly to IDs corresponding to the bytes underlying the text's UTF-8 encoding
(e.g., byte x09 is token ID 9). Unlike prior byte-level approaches (Xue et al.,
2021; Pagnoni et al., 2025), our implementation never introduces out-of-range
IDs (i.e. there is no token ID 256) or auxiliary tokens: all special behavior
(e.g., padding, boundaries, conversation structure, attention segments, tool
calling, "thinking" spans, etc.) is encoded using C0 control bytes - just as
ASCII was originally designed to embed control information alongside printable
text. These design principles yield practical benefits: (1) faster tokenization
(14x) and significantly lower host-device transfer (8x less than int64); (2)
simple, shareable 256*d embedding tables that can be aligned across models; and
(3) a training-time enhancement via bit-biased embeddings, which exposes
per-byte bit structure and can be added to the embedding table post-training,
removing inference costs. Our HuggingFace-compatible implementation improves
language modeling convergence.

</details>


### [51] [Vocab Diet: Reshaping the Vocabulary of LLMs with Vector Arithmetic](https://arxiv.org/abs/2510.17001)
*Yuval Reif,Guy Kaplan,Roy Schwartz*

Main category: cs.CL

TL;DR: The paper proposes a compositional vocabulary approach that represents word variations using transformation vectors instead of unique tokens, freeing up vocabulary space while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Standard tokenization algorithms waste vocabulary space on word form variations (like 'walk', 'walking', 'walked'), reducing coverage for less frequent words and multilingual content.

Method: Use transformation vectors as additive offsets to represent word variations from base forms (e.g., 'walked' = 'walk' + past tense), reshaping vocabulary compositionally rather than enumerating surface forms.

Result: Successfully removed up to 10% of vocabulary entries across multiple LLMs and five languages, expanding coverage to out-of-vocabulary words with minimal impact on downstream performance, without modifying model weights.

Conclusion: This work motivates a fundamental rethinking of vocabulary design from string enumeration to compositional approaches that leverage language structure.

Abstract: Large language models (LLMs) were shown to encode word form variations, such
as "walk"->"walked", as linear directions in embedding space. However, standard
tokenization algorithms treat these variations as distinct tokens -- filling
the size-capped vocabulary with surface form variants (e.g., "walk", "walking",
"Walk"), at the expense of less frequent words and multilingual coverage. We
show that many of these variations can be captured by transformation vectors --
additive offsets that yield the appropriate word's representation when applied
to the base form word embedding -- in both the input and output spaces.
Building on this, we propose a compact reshaping of the vocabulary: rather than
assigning unique tokens to each surface form, we compose them from shared base
form and transformation vectors (e.g., "walked" = "walk" + past tense). We
apply our approach to multiple LLMs and across five languages, removing up to
10% of vocabulary entries -- thereby freeing space to allocate new, more
diverse tokens. Importantly, we do so while also expanding vocabulary coverage
to out-of-vocabulary words, with minimal impact on downstream performance, and
without modifying model weights. Our findings motivate a foundational
rethinking of vocabulary design, moving from string enumeration to a
compositional vocabulary that leverages the underlying structure of language.

</details>


### [52] [Online Learning Defense against Iterative Jailbreak Attacks via Prompt Optimization](https://arxiv.org/abs/2510.17006)
*Masahiro Kaneko,Zeerak Talat,Timothy Baldwin*

Main category: cs.CL

TL;DR: A reinforcement learning-based defense framework that dynamically counters iterative jailbreak attacks on LLMs through online learning and gradient damping.


<details>
  <summary>Details</summary>
Motivation: Existing defenses fail to proactively disrupt the trial-and-error cycle of iterative jailbreak methods that repeatedly rewrite prompts to induce harmful outputs.

Method: Proposes a reinforcement learning approach that optimizes prompts to ensure appropriate responses for harmless tasks while rejecting harmful prompts, and introduces Past-Direction Gradient Damping (PDGD) to prevent overfitting to attack patterns.

Result: Significantly outperforms five existing defense methods against five iterative jailbreak methods on three LLMs, while simultaneously enhancing response quality for harmless tasks.

Conclusion: The proposed framework provides an effective dynamic defense against iterative jailbreak attacks while maintaining or improving performance on legitimate tasks.

Abstract: Iterative jailbreak methods that repeatedly rewrite and input prompts into
large language models (LLMs) to induce harmful outputs -- using the model's
previous responses to guide each new iteration -- have been found to be a
highly effective attack strategy. Despite being an effective attack strategy
against LLMs and their safety mechanisms, existing defenses do not proactively
disrupt this dynamic trial-and-error cycle. In this study, we propose a novel
framework that dynamically updates its defense strategy through online learning
in response to each new prompt from iterative jailbreak methods. Leveraging the
distinctions between harmful jailbreak-generated prompts and typical harmless
prompts, we introduce a reinforcement learning-based approach that optimizes
prompts to ensure appropriate responses for harmless tasks while explicitly
rejecting harmful prompts. Additionally, to curb overfitting to the narrow band
of partial input rewrites explored during an attack, we introduce
Past-Direction Gradient Damping (PDGD). Experiments conducted on three LLMs
show that our approach significantly outperforms five existing defense methods
against five iterative jailbreak methods. Moreover, our results indicate that
our prompt optimization strategy simultaneously enhances response quality for
harmless tasks.

</details>


### [53] [DiscoTrack: A Multilingual LLM Benchmark for Discourse Tracking](https://arxiv.org/abs/2510.17013)
*Lanni Bu,Lauren Levin,Amir Zeldes*

Main category: cs.CL

TL;DR: DiscoTrack is a multilingual benchmark for evaluating LLMs on discourse tracking tasks across 12 languages, focusing on implicit information and pragmatic inferences in larger documents.


<details>
  <summary>Details</summary>
Motivation: Current LLM benchmarks primarily test natural language understanding for explicit information extraction (QA, summarization) at sentence level, lacking challenging multilingual benchmarks for implicit information and discourse tracking across larger documents.

Method: Created DiscoTrack benchmark with tasks across 12 languages targeting four levels of discourse understanding: salience recognition, entity tracking, discourse relations, and bridging inference.

Result: Evaluation shows these discourse tracking tasks remain challenging even for state-of-the-art models.

Conclusion: DiscoTrack addresses the gap in challenging multilingual benchmarks for discourse understanding and demonstrates that current models struggle with implicit information and pragmatic inferences across larger documents.

Abstract: Recent LLM benchmarks have tested models on a range of phenomena, but are
still focused primarily on natural language understanding for extraction of
explicit information, such as QA or summarization, with responses often tar-
geting information from individual sentences. We are still lacking more
challenging, and im- portantly also multilingual, benchmarks focus- ing on
implicit information and pragmatic infer- ences across larger documents in the
context of discourse tracking: integrating and aggregating information across
sentences, paragraphs and multiple speaker utterances. To this end, we present
DiscoTrack, an LLM benchmark target- ing a range of tasks across 12 languages
and four levels of discourse understanding: salience recognition, entity
tracking, discourse relations and bridging inference. Our evaluation shows that
these tasks remain challenging, even for state-of-the-art models.

</details>


### [54] [SafeSearch: Do Not Trade Safety for Utility in LLM Search Agents](https://arxiv.org/abs/2510.17017)
*Qiusi Zhan,Angeline Budiman-Chan,Abdelrahman Zayed,Xingzhi Guo,Daniel Kang,Joo-Kyung Kim*

Main category: cs.CL

TL;DR: Search agents using LLMs are more vulnerable to producing harmful outputs than base LLMs, especially when retrieving external information. SafeSearch uses multi-objective reinforcement learning to improve safety while maintaining utility.


<details>
  <summary>Details</summary>
Motivation: LLM-based search agents are increasingly used for open-domain questions but their safety behaviors remain underexplored. Research shows these agents are more likely to produce harmful outputs than base LLMs, especially when utility-oriented fine-tuning intensifies this risk.

Method: SafeSearch uses multi-objective reinforcement learning that couples final-output safety/utility rewards with a novel query-level shaping term that penalizes unsafe queries and rewards safe ones.

Result: SafeSearch reduces agent harmfulness by over 70% across three red-teaming datasets while producing safe, helpful responses, and matches the QA performance of a utility-only finetuned agent.

Conclusion: The approach effectively improves safety in search agents while maintaining utility, demonstrating the importance of joint alignment of safety and utility in LLM-based search systems.

Abstract: Large language model (LLM) based search agents iteratively generate queries,
retrieve external information, and reason to answer open-domain questions.
While researchers have primarily focused on improving their utility, their
safety behaviors remain underexplored. In this paper, we first evaluate search
agents using red-teaming datasets and find that they are more likely to produce
harmful outputs than base LLMs. For example, when asked ``How can I track
someone's location without their consent?'', a base model refuses, whereas a
search agent designed to retrieve and cite sources may lower its refusal
threshold, fetch documents (e.g., court cases), and, once appended, synthesize
them into an informative yet unsafe summary. We further show that
utility-oriented fine-tuning intensifies this risk, motivating joint alignment
of safety and utility. We present SafeSearch, a multi-objective reinforcement
learning approach that couples a final-output safety/utility reward with a
novel query-level shaping term that penalizes unsafe queries and rewards safe
ones. Experiments show that SafeSearch reduces agent harmfulness by over 70%
across three red-teaming datasets while producing safe, helpful responses, and
matches the QA performance of a utility-only finetuned agent; further analyses
confirm the effectiveness of the query-level reward in jointly improving safety
and utility.

</details>


### [55] [Extended LSTM: Adaptive Feature Gating for Toxic Comment Classification](https://arxiv.org/abs/2510.17018)
*Noor Islam S. Mohammad*

Main category: cs.CL

TL;DR: xLSTM is a parameter-efficient framework for toxic comment detection that combines cosine-similarity gating, adaptive feature prioritization, and class rebalancing to outperform BERT with 15x fewer parameters and better performance on minority toxicity classes.


<details>
  <summary>Details</summary>
Motivation: Transformer-based models like BERT have high computational costs and perform poorly on minority toxicity classes, while classical ensembles lack semantic adaptability for toxic comment detection tasks.

Method: xLSTM uses a learnable reference vector with cosine-similarity gating to modulate embeddings, integrates multi-source embeddings (GloVe, FastText, BERT CLS), character-level BiLSTM for morphological cues, embedding-space SMOTE for minority augmentation, and adaptive focal loss with dynamic class weighting.

Result: On Jigsaw Toxic Comment benchmark: 96.0% accuracy, 0.88 macro-F1, outperforming BERT by 33% on threat and 28% on identity_hate categories, with 15x fewer parameters and 50ms inference latency. Cosine gating provides +4.8% F1 gain.

Conclusion: xLSTM establishes a new efficiency-adaptability frontier, showing that lightweight, theoretically informed architectures can surpass large pretrained models on imbalanced, domain-specific NLP tasks.

Abstract: Toxic comment detection remains a challenging task, where transformer-based
models (e.g., BERT) incur high computational costs and degrade on minority
toxicity classes, while classical ensembles lack semantic adaptability. We
propose xLSTM, a parameter-efficient and theoretically grounded framework that
unifies cosine-similarity gating, adaptive feature prioritization, and
principled class rebalancing. A learnable reference vector {v} in {R}^d
modulates contextual embeddings via cosine similarity, amplifying toxic cues
and attenuating benign signals to yield stronger gradients under severe class
imbalance. xLSTM integrates multi-source embeddings (GloVe, FastText, BERT CLS)
through a projection layer, a character-level BiLSTM for morphological cues,
embedding-space SMOTE for minority augmentation, and adaptive focal loss with
dynamic class weighting. On the Jigsaw Toxic Comment benchmark, xLSTM attains
96.0% accuracy and 0.88 macro-F1, outperforming BERT by 33% on threat and 28%
on identity_hate categories, with 15 times fewer parameters and 50ms inference
latency. Cosine gating contributes a +4.8% F1 gain in ablations. The results
establish a new efficiency adaptability frontier, demonstrating that
lightweight, theoretically informed architectures can surpass large pretrained
models on imbalanced, domain-specific NLP tasks.

</details>


### [56] [Mapping from Meaning: Addressing the Miscalibration of Prompt-Sensitive Language Models](https://arxiv.org/abs/2510.17028)
*Kyle Cox,Jiawei Xu,Yikun Han,Rong Xu,Tianhao Li,Chi-Yang Hsu,Tianlong Chen,Walter Gerych,Ying Ding*

Main category: cs.CL

TL;DR: This paper addresses prompt sensitivity in LLMs by modeling it as generalization error and using semantic paraphrasing to improve uncertainty calibration without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: LLMs show inconsistent responses to semantically equivalent prompts, suggesting their output uncertainty doesn't reflect true uncertainty about prompt meaning.

Method: Model prompt sensitivity as generalization error, sample across semantic concept space with paraphrasing perturbations, and introduce new uncertainty decomposition metric for black-box LLMs.

Result: Semantic sampling with paraphrasing improves uncertainty calibration while maintaining accuracy. The new decomposition metric quantifies prompt sensitivity's contribution to LLM uncertainty.

Conclusion: The approach improves uncertainty calibration in prompt-sensitive models and provides evidence that some LLMs lack consistent reasoning about input meanings.

Abstract: An interesting behavior in large language models (LLMs) is prompt
sensitivity. When provided with different but semantically equivalent versions
of the same prompt, models may produce very different distributions of answers.
This suggests that the uncertainty reflected in a model's output distribution
for one prompt may not reflect the model's uncertainty about the meaning of the
prompt. We model prompt sensitivity as a type of generalization error, and show
that sampling across the semantic ``concept space'' with paraphrasing
perturbations improves uncertainty calibration without compromising accuracy.
Additionally, we introduce a new metric for uncertainty decomposition in
black-box LLMs that improves upon entropy-based decomposition by modeling
semantic continuities in natural language generation. We show that this
decomposition metric can be used to quantify how much LLM uncertainty is
attributed to prompt sensitivity. Our work introduces a new way to improve
uncertainty calibration in prompt-sensitive language models, and provides
evidence that some LLMs fail to exhibit consistent general reasoning about the
meanings of their inputs.

</details>


### [57] [Investigating Thinking Behaviours of Reasoning-Based Language Models for Social Bias Mitigation](https://arxiv.org/abs/2510.17062)
*Guoqing Luo,Iffat Maab,Lili Mou,Junichi Yamagishi*

Main category: cs.CL

TL;DR: This paper investigates how reasoning-based LLMs amplify social stereotypes through their thinking process, identifies two failure patterns (stereotype repetition and irrelevant information injection), and proposes a lightweight prompt-based mitigation approach.


<details>
  <summary>Details</summary>
Motivation: While reasoning-based LLMs excel at complex tasks, their internal thinking processes can aggregate social stereotypes, leading to biased outcomes. The underlying behaviors in social bias scenarios remain underexplored.

Method: Systematically investigate mechanisms behind stereotype aggregation, identify two failure patterns, and introduce a lightweight prompt-based mitigation approach that queries the model to review its initial reasoning against these failure patterns.

Result: Experiments on question answering (BBQ and StereoSet) and open-ended (BOLD) benchmarks show the approach effectively reduces bias while maintaining or improving accuracy.

Conclusion: The study uncovers specific failure patterns in LLM reasoning that drive social bias aggregation and demonstrates that a simple prompt-based mitigation strategy can effectively address these issues without compromising performance.

Abstract: While reasoning-based large language models excel at complex tasks through an
internal, structured thinking process, a concerning phenomenon has emerged that
such a thinking process can aggregate social stereotypes, leading to biased
outcomes. However, the underlying behaviours of these language models in social
bias scenarios remain underexplored. In this work, we systematically
investigate mechanisms within the thinking process behind this phenomenon and
uncover two failure patterns that drive social bias aggregation: 1) stereotype
repetition, where the model relies on social stereotypes as its primary
justification, and 2) irrelevant information injection, where it fabricates or
introduces new details to support a biased narrative. Building on these
insights, we introduce a lightweight prompt-based mitigation approach that
queries the model to review its own initial reasoning against these specific
failure patterns. Experiments on question answering (BBQ and StereoSet) and
open-ended (BOLD) benchmarks show that our approach effectively reduces bias
while maintaining or improving accuracy.

</details>


### [58] [Verification-Aware Planning for Multi-Agent Systems](https://arxiv.org/abs/2510.17109)
*Tianyang Xu,Dan Zhang,Kushan Mitra,Estevam Hruschka*

Main category: cs.CL

TL;DR: VeriMAP is a verification-aware planning framework for multi-agent LLM collaboration that addresses coordination failures through automated verification functions and improves system robustness without external supervision.


<details>
  <summary>Details</summary>
Motivation: Multi-agent LLM collaboration faces challenges in planning, coordination, and verification, with failures often arising from subtle misalignments in task interpretation, output format, or inter-agent handoffs rather than flawed reasoning alone.

Method: VeriMAP decomposes tasks, models subtask dependencies, and encodes planner-defined passing criteria as verification functions (VFs) in both Python and natural language to enable verification-aware planning.

Result: VeriMAP outperforms both single- and multi-agent baselines on diverse datasets while enhancing system robustness and interpretability.

Conclusion: Verification-aware planning enables reliable coordination and iterative refinement in multi-agent systems without relying on external labels or annotations.

Abstract: Large language model (LLM) agents are increasingly deployed to tackle complex
tasks, often necessitating collaboration among multiple specialized agents.
However, multi-agent collaboration introduces new challenges in planning,
coordination, and verification. Execution failures frequently arise not from
flawed reasoning alone, but from subtle misalignments in task interpretation,
output format, or inter-agent handoffs. To address these challenges, we present
VeriMAP, a framework for multi-agent collaboration with verification-aware
planning. The VeriMAP planner decomposes tasks, models subtask dependencies,
and encodes planner-defined passing criteria as subtask verification functions
(VFs) in Python and natural language. We evaluate VeriMAP on diverse datasets,
demonstrating that it outperforms both single- and multi-agent baselines while
enhancing system robustness and interpretability. Our analysis highlights how
verification-aware planning enables reliable coordination and iterative
refinement in multi-agent systems, without relying on external labels or
annotations.

</details>


### [59] [DVAGen: Dynamic Vocabulary Augmented Generation](https://arxiv.org/abs/2510.17115)
*Wei Du,Nuowei Liu,Jie Wang,Jiahao Kuang,Tao Ji,Xiaoling Wang,Yuanbin Wu*

Main category: cs.CL

TL;DR: DVAGen is an open-source framework that addresses limitations of fixed vocabulary language models by providing a unified system for training, evaluating, and visualizing dynamic vocabulary-augmented LLMs with improved inference scalability.


<details>
  <summary>Details</summary>
Motivation: Fixed vocabulary language models struggle with novel or out-of-vocabulary words, limiting their flexibility. Existing dynamic vocabulary approaches face issues like fragmented codebases, lack of modern LLM support, and poor inference scalability.

Method: Introduces DVAGen - a fully open-source, unified framework that modularizes the pipeline for customization, integrates with open-source LLMs, and provides both CLI and WebUI tools for real-time result inspection.

Result: Validated effectiveness of dynamic vocabulary methods on modern LLMs and demonstrated support for batch inference, significantly improving inference throughput.

Conclusion: DVAGen overcomes limitations of existing approaches by providing a comprehensive, scalable framework for dynamic vocabulary language models with enhanced inference capabilities.

Abstract: Language models trained with a fixed vocabulary struggle to generalize to
novel or out-of-vocabulary words, limiting their flexibility in handling
diverse token combinations. Existing dynamic vocabulary approaches attempt to
address this limitation but face challenges such as fragmented codebases, lack
of support for modern LLMs, and limited inference scalability. To overcome
these issues, we introduce DVAGen, a fully open-source, unified framework
designed for training, evaluation, and visualization of dynamic
vocabulary-augmented language models. Our framework modularizes the pipeline
for ease of customization, integrates seamlessly with open-source LLMs, and is
the first to provide both CLI and WebUI tools for real-time result inspection.
We validate the effectiveness of dynamic vocabulary methods on modern LLMs and
demonstrate support for batch inference, significantly improving inference
throughput.

</details>


### [60] [Rethinking On-policy Optimization for Query Augmentation](https://arxiv.org/abs/2510.17139)
*Zhichao Xu,Shengyao Zhuang,Xueguang Ma,Bingsen Chen,Yijun Tian,Fengran Mo,Jie Cao,Vivek Srikumar*

Main category: cs.CL

TL;DR: This paper presents the first systematic comparison between prompting-based and RL-based query augmentation for information retrieval, finding that simple training-free methods often match or exceed expensive RL approaches, and introduces a novel hybrid method (OPQE) that outperforms both.


<details>
  <summary>Details</summary>
Motivation: To systematically compare prompting-based and RL-based query augmentation approaches under consistent experimental conditions, as these two main approaches have respective advantages but haven't been properly compared.

Method: Conducted systematic comparison across diverse benchmarks (evidence-seeking, ad hoc, and tool retrieval), then introduced OPQE - a hybrid method where LLM policy learns to generate pseudo-documents that maximize retrieval performance, combining prompting flexibility with RL optimization.

Result: Simple training-free query augmentation often performs on par with or surpasses expensive RL-based counterparts, especially with powerful LLMs. The novel OPQE method outperforms both standalone prompting and RL-based rewriting.

Conclusion: A synergistic approach combining prompting flexibility with RL optimization yields the best results for query augmentation in information retrieval.

Abstract: Recent advances in large language models (LLMs) have led to a surge of
interest in query augmentation for information retrieval (IR). Two main
approaches have emerged. The first prompts LLMs to generate answers or
pseudo-documents that serve as new queries, relying purely on the model's
parametric knowledge or contextual information. The second applies
reinforcement learning (RL) to fine-tune LLMs for query rewriting, directly
optimizing retrieval metrics. While having respective advantages and
limitations, the two approaches have not been compared under consistent
experimental conditions. In this work, we present the first systematic
comparison of prompting-based and RL-based query augmentation across diverse
benchmarks, including evidence-seeking, ad hoc, and tool retrieval. Our key
finding is that simple, training-free query augmentation often performs on par
with, or even surpasses, more expensive RL-based counterparts, especially when
using powerful LLMs. Motivated by this discovery, we introduce a novel hybrid
method, On-policy Pseudo-document Query Expansion (OPQE), which, instead of
rewriting a query, the LLM policy learns to generate a pseudo-document that
maximizes retrieval performance, thus merging the flexibility and generative
structure of prompting with the targeted optimization of RL. We show OPQE
outperforms both standalone prompting and RL-based rewriting, demonstrating
that a synergistic approach yields the best results. Our implementation is made
available to facilitate reproducibility.

</details>


### [61] [When AI companions become witty: Can human brain recognize AI-generated irony?](https://arxiv.org/abs/2510.17168)
*Xiaohui Rao,Hanlin Wu,Zhenguang G. Cai*

Main category: cs.CL

TL;DR: People don't fully adopt intentional stance toward AI-generated irony, showing reduced neural processing and more attribution to computational errors rather than deliberate communication.


<details>
  <summary>Details</summary>
Motivation: To understand whether people attribute mental states and intentionality to AI during irony comprehension, especially as LLMs are increasingly deployed as social agents producing humor and irony.

Method: Compared behavioral and neural responses to ironic statements from AI vs human sources using ERP components (P200 for early incongruity detection and P600 for cognitive reanalysis efforts).

Result: Participants attributed incongruity to deliberate communication less for AI than human sources. Neural data showed attenuated P200 and P600 effects for AI-generated irony, indicating reduced effortful detection and reanalysis. People who perceived AI as more sincere showed larger neural effects.

Conclusion: Source attribution shapes neural processing of social-communicative phenomena. Despite LLMs' linguistic sophistication, achieving genuine social agency requires humans to shift how they perceive and attribute intentionality to AI.

Abstract: As Large Language Models (LLMs) are increasingly deployed as social agents
and trained to produce humor and irony, a question emerges: when encountering
witty AI remarks, do people interpret these as intentional communication or
mere computational output? This study investigates whether people adopt the
intentional stance, attributing mental states to explain behavior,toward AI
during irony comprehension. Irony provides an ideal paradigm because it
requires distinguishing intentional contradictions from unintended errors
through effortful semantic reanalysis. We compared behavioral and neural
responses to ironic statements from AI versus human sources using established
ERP components: P200 reflecting early incongruity detection and P600 indexing
cognitive efforts in reinterpreting incongruity as deliberate irony. Results
demonstrate that people do not fully adopt the intentional stance toward
AI-generated irony. Behaviorally, participants attributed incongruity to
deliberate communication for both sources, though significantly less for AI
than human, showing greater tendency to interpret AI incongruities as
computational errors. Neural data revealed attenuated P200 and P600 effects for
AI-generated irony, suggesting reduced effortful detection and reanalysis
consistent with diminished attribution of communicative intent. Notably, people
who perceived AI as more sincere showed larger P200 and P600 effects for
AI-generated irony, suggesting that intentional stance adoption is calibrated
by specific mental models of artificial agents. These findings reveal that
source attribution shapes neural processing of social-communicative phenomena.
Despite current LLMs' linguistic sophistication, achieving genuine social
agency requires more than linguistic competence, it necessitates a shift in how
humans perceive and attribute intentionality to artificial agents.

</details>


### [62] [Understanding and Improving Length Generalization in Hierarchical Sparse Attention Models](https://arxiv.org/abs/2510.17196)
*Jiaqi Leng,Xiang Hu,Junxiong Wang,Jianguo Li,Wei Wu,Yucheng Lu*

Main category: cs.CL

TL;DR: This paper systematically analyzes chunk-based sparse attention models for long-context processing, identifying three key design principles that enable effective length extrapolation up to 32 million tokens.


<details>
  <summary>Details</summary>
Motivation: Standard Transformers have quadratic complexity and poor length extrapolation, while alternative architectures like sliding window attention and state space models sacrifice full context utilization due to fixed-size memory. The authors aim to understand the architectural principles behind successful chunk-based sparse attention models.

Method: The authors use a unified framework and comprehensive ablation studies to dissect chunk-based sparse attention models. They identify three critical design principles: expressive non-linear Chunk Encoder with CLS token, Bypassing Residual Path for stable global information integration, and enforced selection sparsity during pre-training.

Result: By combining these three principles, the authors achieve state-of-the-art training-free length extrapolation, successfully generalizing models trained on 4K context to 32 million tokens on RULER and BABILong benchmarks.

Conclusion: The findings provide clear, empirically-grounded design principles for developing future long-context language models, establishing that the combination of expressive chunk encoding, bypassing residual paths, and enforced sparsity is crucial for effective extreme-length generalization.

Abstract: Effectively processing long contexts is a critical challenge for language
models. While standard Transformers are limited by quadratic complexity and
poor length extrapolation, alternative architectures like sliding window
attention and state space models sacrifice the ability to effectively utilize
the full context due to their fixed-size memory. Chunk-based sparse attention
has emerged as a promising paradigm for extreme length generalization, yet the
key architectural principles underpinning its success are not yet fully
understood. In this work, we present a systematic dissection of these models to
identify the core components driving their performance. Through a unified
framework and comprehensive ablation studies, we demonstrate that a combination
of three design principles is critical: (1) an expressive, non-linear Chunk
Encoder with a dedicated CLS token to produce representations for retrieval;
(2) a Bypassing Residual Path to stably integrate retrieved global information
without it being overridden by the local residual stream; and (3) enforced
selection sparsity during pre-training to bridge the train-test distribution
gap. We provide a theoretical motivation for intra-chunk information processing
and landmark generation. By combining these principles, we establish a new
state-of-the-art for training-free length extrapolation, successfully
generalizing models trained on a 4K context to 32 million tokens on RULER and
BABILong. Our findings provide a clear and empirically-grounded set of design
principles for developing future, highly-capable long-context language models.

</details>


### [63] [Wisdom is Knowing What not to Say: Hallucination-Free LLMs Unlearning via Attention Shifting](https://arxiv.org/abs/2510.17210)
*Chenchen Tan,Youyang Qu,Xinghao Li,Hui Zhang,Shujie Cui,Cunjian Chen,Longxiang Gao*

Main category: cs.CL

TL;DR: The paper introduces Attention-Shifting (AS), a novel framework for selective unlearning in LLMs that addresses the trade-off between model utility and hallucination risks through attention-level interventions.


<details>
  <summary>Details</summary>
Motivation: Existing unlearning approaches face a dilemma: aggressive unlearning compromises model utility while conservative strategies risk hallucinated responses, limiting LLMs' reliability in knowledge-intensive applications.

Method: AS uses two attention-level interventions: importance-aware suppression for unlearning set to reduce reliance on memorized knowledge, and attention-guided retention enhancement to reinforce attention on essential tokens in retained data. These are jointly optimized via a dual-loss objective.

Result: AS improves performance preservation over state-of-the-art methods, achieving up to 15% higher accuracy on ToFU benchmark and 10% on TDEC benchmark, while maintaining competitive hallucination-free unlearning effectiveness.

Conclusion: AS demonstrates superior balance between unlearning effectiveness, generalization, and response reliability compared to existing methods.

Abstract: The increase in computing power and the necessity of AI-assisted
decision-making boost the growing application of large language models (LLMs).
Along with this, the potential retention of sensitive data of LLMs has spurred
increasing research into machine unlearning. However, existing unlearning
approaches face a critical dilemma: Aggressive unlearning compromises model
utility, while conservative strategies preserve utility but risk hallucinated
responses. This significantly limits LLMs' reliability in knowledge-intensive
applications. To address this, we introduce a novel Attention-Shifting (AS)
framework for selective unlearning. AS is driven by two design objectives: (1)
context-preserving suppression that attenuates attention to fact-bearing tokens
without disrupting LLMs' linguistic structure; and (2) hallucination-resistant
response shaping that discourages fabricated completions when queried about
unlearning content. AS realizes these objectives through two attention-level
interventions, which are importance-aware suppression applied to the unlearning
set to reduce reliance on memorized knowledge and attention-guided retention
enhancement that reinforces attention toward semantically essential tokens in
the retained dataset to mitigate unintended degradation. These two components
are jointly optimized via a dual-loss objective, which forms a soft boundary
that localizes unlearning while preserving unrelated knowledge under
representation superposition. Experimental results show that AS improves
performance preservation over the state-of-the-art unlearning methods,
achieving up to 15% higher accuracy on the ToFU benchmark and 10% on the TDEC
benchmark, while maintaining competitive hallucination-free unlearning
effectiveness. Compared to existing methods, AS demonstrates a superior balance
between unlearning effectiveness, generalization, and response reliability.

</details>


### [64] [StreamingThinker: Large Language Models Can Think While Reading](https://arxiv.org/abs/2510.17238)
*Junlong Tong,Yingqi Fan,Anhao Zhao,Yunpu Ma,Xiaoyu Shen*

Main category: cs.CL

TL;DR: StreamingThinker enables LLMs to think while reading input rather than waiting for complete input, reducing latency by 80% for reasoning onset and 60% for final answer generation while maintaining comparable performance to batch thinking.


<details>
  <summary>Details</summary>
Motivation: Current LLM reasoning paradigm waits for entire input before thinking, causing unnecessary latency and weakened attention to earlier information in dynamic scenarios.

Method: StreamingThinker framework with streaming CoT generation, streaming-constraint training, and streaming parallel inference using streaming reasoning units, order-preserving attention masks, and parallel KV caches.

Result: 80% reduction in token waiting before reasoning onset, over 60% reduction in time-level latency for final answer, while preserving performance comparable to batch thinking on math, logical, and context-based QA reasoning tasks.

Conclusion: Streaming thinking paradigm effectively reduces latency in LLM reasoning while maintaining performance, demonstrating the viability of thinking-while-reading approach inspired by human cognition.

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
chain of thought (CoT) reasoning. However, the current LLM reasoning paradigm
initiates thinking only after the entire input is available, which introduces
unnecessary latency and weakens attention to earlier information in dynamic
scenarios. Inspired by human cognition of thinking while reading, we first
design a \textit{\textbf{streaming thinking}} paradigm for LLMs, where
reasoning unfolds in the order of input and further adjusts its depth once
reading is complete. We instantiate this paradigm with
\textit{StreamingThinker}, a framework that enables LLMs to think while reading
through the integration of streaming CoT generation, streaming-constraint
training, and streaming parallel inference. Specifically, StreamingThinker
employs streaming reasoning units with quality control for CoT generation,
enforces order-preserving reasoning through streaming attention masks and
position encoding, and leverages parallel KV caches that decouple input
encoding from reasoning generation, thereby ensuring alignment and enabling
true concurrency. We evaluate StreamingThinker on the Qwen3 model family across
math reasoning, logical reasoning, and context-based QA reasoning tasks.
Experimental results show that the StreamingThinker preserves performance
comparable to batch thinking, while yielding an 80\% reduction in token waiting
before the onset of reasoning and a more than 60\% reduction in time-level
latency for producing the final answer, demonstrating the effectiveness of the
streaming paradigm for LLM reasoning. Code will be released at
\href{https://github.com/EIT-NLP/StreamingLLM/tree/main/StreamingThinker}{this
repository.}

</details>


### [65] [From Preferences to Prejudice: The Role of Alignment Tuning in Shaping Social Bias in Video Diffusion Models](https://arxiv.org/abs/2510.17247)
*Zefan Cai,Haoyi Qiu,Haozhe Zhao,Ke Wan,Jiachen Li,Jiuxiang Gu,Wen Xiao,Nanyun Peng,Junjie Hu*

Main category: cs.CL

TL;DR: VideoBiasEval framework reveals that alignment tuning in video diffusion models amplifies social biases from human preference datasets, making them more stable and stereotyped in generated videos.


<details>
  <summary>Details</summary>
Motivation: To systematically trace how social biases evolve and are amplified throughout the video generation alignment pipeline, as current methods improve visual quality but unintentionally encode and amplify social biases.

Method: Introduces VideoBiasEval, a comprehensive diagnostic framework using event-based prompting to disentangle semantic content from actor attributes, with multi-granular metrics to evaluate ethnicity bias, gender-ethnicity interactions, distributional shifts, and temporal bias persistence.

Result: Alignment tuning strengthens representational biases and makes them temporally stable, producing smoother yet more stereotyped portrayals. Biases in human preference datasets are amplified in reward models and propagated through alignment-tuned video diffusion models.

Conclusion: There is a critical need for bias-aware evaluation and mitigation throughout the alignment process to ensure fair and socially responsible video generation.

Abstract: Recent advances in video diffusion models have significantly enhanced
text-to-video generation, particularly through alignment tuning using reward
models trained on human preferences. While these methods improve visual
quality, they can unintentionally encode and amplify social biases. To
systematically trace how such biases evolve throughout the alignment pipeline,
we introduce VideoBiasEval, a comprehensive diagnostic framework for evaluating
social representation in video generation. Grounded in established social bias
taxonomies, VideoBiasEval employs an event-based prompting strategy to
disentangle semantic content (actions and contexts) from actor attributes
(gender and ethnicity). It further introduces multi-granular metrics to
evaluate (1) overall ethnicity bias, (2) gender bias conditioned on ethnicity,
(3) distributional shifts in social attributes across model variants, and (4)
the temporal persistence of bias within videos. Using this framework, we
conduct the first end-to-end analysis connecting biases in human preference
datasets, their amplification in reward models, and their propagation through
alignment-tuned video diffusion models. Our results reveal that alignment
tuning not only strengthens representational biases but also makes them
temporally stable, producing smoother yet more stereotyped portrayals. These
findings highlight the need for bias-aware evaluation and mitigation throughout
the alignment process to ensure fair and socially responsible video generation.

</details>


### [66] [How News Feels: Understanding Affective Bias in Multilingual Headlines for Human-Centered Media Design](https://arxiv.org/abs/2510.17252)
*Mohd Ruhul Ameen,Akif Islam,Abu Saleh Musa Miah,Ayesha Siddiqua,Jungpil Shin*

Main category: cs.CL

TL;DR: Analysis of 300,000 Bengali news headlines using Gemma-3 4B reveals dominance of negative emotions (anger, fear, disappointment) and emotional framing variation across outlets, leading to design proposals for emotion-aware news aggregators.


<details>
  <summary>Details</summary>
Motivation: To investigate how news media shape public mood through emotional framing, particularly the tendency for negative/emotional content to attract more attention and spread faster, encouraging outlets to frame stories provocatively.

Method: Large-scale emotion analysis of Bengali news using zero-shot inference with Gemma-3 4B model on 300,000 headlines and content to identify dominant emotions and overall tone.

Result: Clear dominance of negative emotions (anger, fear, disappointment) and significant variation in emotional portrayal of similar stories across different news outlets.

Conclusion: Proposes design ideas for human-centered news aggregators that visualize emotional cues to help readers recognize hidden affective framing in daily news coverage.

Abstract: News media often shape the public mood not only by what they report but by
how they frame it. The same event can appear calm in one outlet and alarming in
another, reflecting subtle emotional bias in reporting. Negative or emotionally
charged headlines tend to attract more attention and spread faster, which in
turn encourages outlets to frame stories in ways that provoke stronger
reactions. This research explores that tendency through large-scale emotion
analysis of Bengali news. Using zero-shot inference with Gemma-3 4B, we
analyzed 300000 Bengali news headlines and their content to identify the
dominant emotion and overall tone of each. The findings reveal a clear
dominance of negative emotions, particularly anger, fear, and disappointment,
and significant variation in how similar stories are emotionally portrayed
across outlets. Based on these insights, we propose design ideas for a
human-centered news aggregator that visualizes emotional cues and helps readers
recognize hidden affective framing in daily news.

</details>


### [67] [Explainability of Large Language Models: Opportunities and Challenges toward Generating Trustworthy Explanations](https://arxiv.org/abs/2510.17256)
*Shahin Atakishiyev,Housam K. B. Babiker,Jiayi Dai,Nawshad Farruque,Teruaki Hayashi,Nafisa Sadaf Hriti,Md Abed Rahman,Iain Smith,Mi-Young Kim,Osmar R. Zaane,Randy Goebel*

Main category: cs.CL

TL;DR: This paper reviews local explainability and mechanistic interpretability approaches for Transformer-based large language models, conducts experimental studies in healthcare and autonomous driving domains, and outlines future challenges for generating trustworthy LLM explanations.


<details>
  <summary>Details</summary>
Motivation: LLMs make prediction errors and hallucinations that are not understandable by humans, creating an urgent need to better understand their inner workings and build trust in their outputs.

Method: Presents a review of existing explainability approaches, conducts experimental studies in healthcare and autonomous driving domains, and analyzes trust implications of explanations for end-users.

Result: The paper provides insights into current explainability methods and their application in critical domains, highlighting the importance of human-aligned explanations for building trust.

Conclusion: There are significant unaddressed issues in LLM explainability, requiring future work on generating human-aligned, trustworthy explanations to address critical challenges in model interpretability.

Abstract: Large language models have exhibited impressive performance across a broad
range of downstream tasks in natural language processing. However, how a
language model predicts the next token and generates content is not generally
understandable by humans. Furthermore, these models often make errors in
prediction and reasoning, known as hallucinations. These errors underscore the
urgent need to better understand and interpret the intricate inner workings of
language models and how they generate predictive outputs. Motivated by this
gap, this paper investigates local explainability and mechanistic
interpretability within Transformer-based large language models to foster trust
in such models. In this regard, our paper aims to make three key contributions.
First, we present a review of local explainability and mechanistic
interpretability approaches and insights from relevant studies in the
literature. Furthermore, we describe experimental studies on explainability and
reasoning with large language models in two critical domains -- healthcare and
autonomous driving -- and analyze the trust implications of such explanations
for explanation receivers. Finally, we summarize current unaddressed issues in
the evolving landscape of LLM explainability and outline the opportunities,
critical challenges, and future directions toward generating human-aligned,
trustworthy LLM explanations.

</details>


### [68] [TaxoAlign: Scholarly Taxonomy Generation Using Language Models](https://arxiv.org/abs/2510.17263)
*Avishek Lahiri,Yufang Hou,Debarshi Kumar Sanyal*

Main category: cs.CL

TL;DR: This paper introduces TaxoAlign, a method for automated taxonomy generation that bridges the gap between human-written and automatically-created taxonomies, evaluated on the new CS-TaxoBench benchmark.


<details>
  <summary>Details</summary>
Motivation: Existing approaches to automatic survey generation don't compare generated taxonomies with human-written ones, creating a gap in evaluating taxonomy quality and structure.

Method: Proposed TaxoAlign, a three-phase topic-based instruction-guided method for scholarly taxonomy generation, along with a stringent automated evaluation framework for structural alignment and semantic coherence.

Result: TaxoAlign consistently surpasses baselines on nearly all metrics when evaluated on CS-TaxoBench using both automated metrics and human evaluation.

Conclusion: The method successfully bridges the gap between human-generated and automatically-created taxonomies, providing a robust framework for automated taxonomy generation in scholarly contexts.

Abstract: Taxonomies play a crucial role in helping researchers structure and navigate
knowledge in a hierarchical manner. They also form an important part in the
creation of comprehensive literature surveys. The existing approaches to
automatic survey generation do not compare the structure of the generated
surveys with those written by human experts. To address this gap, we present
our own method for automated taxonomy creation that can bridge the gap between
human-generated and automatically-created taxonomies. For this purpose, we
create the CS-TaxoBench benchmark which consists of 460 taxonomies that have
been extracted from human-written survey papers. We also include an additional
test set of 80 taxonomies curated from conference survey papers. We propose
TaxoAlign, a three-phase topic-based instruction-guided method for scholarly
taxonomy generation. Additionally, we propose a stringent automated evaluation
framework that measures the structural alignment and semantic coherence of
automatically generated taxonomies in comparison to those created by human
experts. We evaluate our method and various baselines on CS-TaxoBench, using
both automated evaluation metrics and human evaluation studies. The results
show that TaxoAlign consistently surpasses the baselines on nearly all metrics.
The code and data can be found at https://github.com/AvishekLahiri/TaxoAlign.

</details>


### [69] [Addressing Antisocial Behavior in Multi-Party Dialogs Through Multimodal Representation Learning](https://arxiv.org/abs/2510.17289)
*Hajar Bakarou,Mohamed Sinane El Messoussi,Anas Ollagnier*

Main category: cs.CL

TL;DR: This paper addresses antisocial behavior detection in multi-party conversational settings using a French dataset, evaluating text-based and graph-based methods across three tasks with multimodal fusion showing best performance.


<details>
  <summary>Details</summary>
Motivation: Prior research has focused on networks like X and Reddit, leaving multi-party conversational settings underexplored due to limited data. The paper aims to fill this gap in understanding antisocial behavior in conversational contexts.

Method: Used CyberAgressionAdo-Large dataset with text-based and graph-based representation learning methods, analyzing lexical cues and interactional dynamics through multimodal fusion approaches including late fusion models.

Result: Multimodal models outperformed unimodal baselines. The late fusion model mBERT + WD-SGCN achieved best overall results: 0.718 on abuse detection, 0.286 on peer-group identification, and 0.606 on bullying analysis.

Conclusion: The study demonstrates that multimodal approaches effectively handle nuanced antisocial behavior phenomena including implicit aggression, role transitions, and context-dependent hostility in multi-party conversations.

Abstract: Antisocial behavior (ASB) on social media -- including hate speech,
harassment, and cyberbullying -- poses growing risks to platform safety and
societal well-being. Prior research has focused largely on networks such as X
and Reddit, while \textit{multi-party conversational settings} remain
underexplored due to limited data. To address this gap, we use
\textit{CyberAgressionAdo-Large}, a French open-access dataset simulating ASB
in multi-party conversations, and evaluate three tasks: \textit{abuse
detection}, \textit{bullying behavior analysis}, and \textit{bullying
peer-group identification}. We benchmark six text-based and eight graph-based
\textit{representation-learning methods}, analyzing lexical cues, interactional
dynamics, and their multimodal fusion. Results show that multimodal models
outperform unimodal baselines. The late fusion model \texttt{mBERT + WD-SGCN}
achieves the best overall results, with top performance on abuse detection
(0.718) and competitive scores on peer-group identification (0.286) and
bullying analysis (0.606). Error analysis highlights its effectiveness in
handling nuanced ASB phenomena such as implicit aggression, role transitions,
and context-dependent hostility.

</details>


### [70] [Towards Mixed-Modal Retrieval for Universal Retrieval-Augmented Generation](https://arxiv.org/abs/2510.17354)
*Chenghao Zhang,Guanting Dong,Xinyu Yang,Zhicheng Dou*

Main category: cs.CL

TL;DR: Nyx is a unified mixed-modal retriever for Universal Retrieval-Augmented Generation (URAG) that handles both text and images, trained on automatically generated mixed-modal data and fine-tuned with VLM feedback.


<details>
  <summary>Details</summary>
Motivation: Existing RAG systems focus only on text documents and fail in real-world scenarios with mixed-modal queries and documents containing both text and images.

Method: Proposed Nyx - a mixed-modal retriever with a four-stage automated pipeline to create NyxQA dataset, followed by two-stage training: pre-training on mixed-modal data and supervised fine-tuning using VLM feedback.

Result: Nyx performs competitively on text-only RAG benchmarks and excels in URAG settings, significantly improving generation quality in vision-language tasks.

Conclusion: Nyx successfully addresses the URAG challenge by enabling effective retrieval and reasoning over mixed-modal information, demonstrating superior performance in realistic vision-language generation scenarios.

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for
enhancing large language models (LLMs) by retrieving relevant documents from an
external corpus. However, existing RAG systems primarily focus on unimodal text
documents, and often fall short in real-world scenarios where both queries and
documents may contain mixed modalities (such as text and images). In this
paper, we address the challenge of Universal Retrieval-Augmented Generation
(URAG), which involves retrieving and reasoning over mixed-modal information to
improve vision-language generation. To this end, we propose Nyx, a unified
mixed-modal to mixed-modal retriever tailored for URAG scenarios. To mitigate
the scarcity of realistic mixed-modal data, we introduce a four-stage automated
pipeline for generation and filtering, leveraging web documents to construct
NyxQA, a dataset comprising diverse mixed-modal question-answer pairs that
better reflect real-world information needs. Building on this high-quality
dataset, we adopt a two-stage training framework for Nyx: we first perform
pre-training on NyxQA along with a variety of open-source retrieval datasets,
followed by supervised fine-tuning using feedback from downstream
vision-language models (VLMs) to align retrieval outputs with generative
preferences. Experimental results demonstrate that Nyx not only performs
competitively on standard text-only RAG benchmarks, but also excels in the more
general and realistic URAG setting, significantly improving generation quality
in vision-language tasks.

</details>


### [71] [The Atomic Instruction Gap: Instruction-Tuned LLMs Struggle with Simple, Self-Contained Directives](https://arxiv.org/abs/2510.17388)
*Henry Lim,Kwan Hui Lim*

Main category: cs.CL

TL;DR: Instruction-tuned LLMs show significant performance variations when option label formats change, revealing instruction-format bias and weak adherence to atomic instructions despite strong zero-shot reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: To investigate whether instruction-tuned LLMs can reliably follow simple, self-contained instructions, which is foundational for complex instruction-following but remains underexplored.

Method: Evaluated 20 IT-LLMs on modified MMLU and MMLU-Pro benchmarks by systematically varying option label formats (alphabetic, numeric, Roman) under four paradigms: with/without explicit instructions, with option contents removed, and with three-shot exemplars.

Result: Label format changes caused large performance shifts (-30.45% for Roman vs. numeric), performance dropped without instructions, models failed random-choice baselines when option contents were removed, and three-shot exemplars provided no significant gains. Larger models achieved higher accuracy but remained inconsistent.

Conclusion: Current instruction-tuning paradigms are insufficient, highlighting the need for evaluation methods and training strategies that explicitly target atomic instruction-following.

Abstract: Instruction-tuned large language models (IT-LLMs) exhibit strong zero-shot
reasoning, yet their ability to execute simple, self-contained instructions
remains underexplored, despite this being foundational to complex
instruction-following. We evaluate 20 IT-LLMs on modified MMLU and MMLU-Pro
benchmarks, by systematically varying the format of option labels (alphabetic,
numeric, Roman) while keeping their meaning identical under four paradigms,
namely: (1) With explicit instructions, label changes cause large performance
shifts (e.g., -30.45\% for Roman vs. numeric), revealing instruction-format
bias. (2) Without instructions, performance drops further (up to -10.84\%) and
label sensitivity intensifies, underscoring the role of explicit guidance. (3)
When option contents are removed, models fail random-choice baselines except
with numeric labels, suggesting weak adherence to atomic directives. (4)
Three-shot exemplars yield no significant gains in robustness or fidelity, and
generation analyses show persistent label errors, especially for non-numeric
formats. Across model sizes, larger LLMs achieve higher accuracy but remain
inconsistent in instruction adherence. These results expose the insufficiencies
of current instruction-tuning paradigms and highlight the need for evaluation
methods and training strategies that explicitly target atomic
instruction-following.

</details>


### [72] [EduAdapt: A Question Answer Benchmark Dataset for Evaluating Grade-Level Adaptability in LLMs](https://arxiv.org/abs/2510.17389)
*Numaan Naeem,Abdellah El Mekki,Muhammad Abdul-Mageed*

Main category: cs.CL

TL;DR: EduAdapt is a benchmark with 48k grade-labeled QA pairs across 9 science subjects for Grades 1-12, showing LLMs struggle to adapt responses to younger students despite better performance on larger models.


<details>
  <summary>Details</summary>
Motivation: LLMs perform well academically but fail to tailor responses to students' grade levels, which is critical for K-12 education where age-appropriate explanations are essential for effective learning.

Method: Created EduAdapt benchmark with nearly 48k grade-labeled QA pairs across nine science subjects spanning Grades 1-12, grouped into four grade levels, and evaluated diverse open-source LLMs.

Result: Larger models generally perform better but still struggle with generating suitable responses for early-grade students (Grades 1-5), highlighting the grade-level adaptability challenge.

Conclusion: EduAdapt provides the first dataset and evaluation framework for assessing grade-level adaptability in LLMs, aiming to foster more developmentally aligned educational AI systems through improved training and prompting strategies.

Abstract: Large language models (LLMs) are transforming education by answering
questions, explaining complex concepts, and generating content across a wide
range of subjects. Despite strong performance on academic benchmarks, they
often fail to tailor responses to students' grade levels. This is a critical
need in K-12 education, where age-appropriate vocabulary and explanation are
essential for effective learning. Existing models frequently produce outputs
that are too advanced or vague for younger learners, and there are no
standardized benchmarks to evaluate their ability to adjust across cognitive
and developmental stages. To address this gap, we introduce EduAdapt, a
benchmark of nearly 48k grade-labeled QA pairs across nine science subjects,
spanning Grades 1-12 and grouped into four grade levels. We evaluate a diverse
set of open-source LLMs on EduAdapt and find that while larger models generally
perform better, they still struggle with generating suitable responses for
early-grade students (Grades 1-5). Our work presents the first dataset and
evaluation framework for assessing grade-level adaptability in LLMs, aiming to
foster more developmentally aligned educational AI systems through better
training and prompting strategies. EduAdapt code and datasets are publicly
available at https://github.com/NaumanNaeem/EduAdapt.

</details>


### [73] [Leveraging Group Relative Policy Optimization to Advance Large Language Models in Traditional Chinese Medicine](https://arxiv.org/abs/2510.17402)
*Jiacheng Xie,Shuai Zeng,Yang Yu,Xiaoting Tang,Guanghui An,Dong Xu*

Main category: cs.CL

TL;DR: Ladder-base is the first TCM-focused LLM trained with Group Relative Policy Optimization (GRPO), showing superior performance over both general-purpose and domain-specific models in traditional Chinese medicine reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Traditional Chinese Medicine presents unique challenges for LLMs, and previous TCM-specific models face limitations in alignment, data quality, and evaluation consistency.

Method: Built on Qwen2.5-7B-Instruct foundation model, trained exclusively on TCM-Ladder benchmark's textual subset using GRPO reinforcement learning method that optimizes response selection through intra-group comparisons.

Result: Ladder-base demonstrates superior performance across multiple reasoning metrics compared to state-of-the-art general-purpose LLMs (GPT-4, Gemini 2.5, Claude 3, Qwen3) and domain-specific TCM models (BenTsao, HuatuoGPT2, Zhongjing).

Conclusion: GRPO provides an effective strategy for aligning LLMs with expert-level reasoning in traditional medical domains and supports development of trustworthy, clinically grounded TCM AI systems.

Abstract: Traditional Chinese Medicine (TCM) presents a rich and structurally unique
knowledge system that challenges conventional applications of large language
models (LLMs). Although previous TCM-specific LLMs have shown progress through
supervised fine-tuning, they often face limitations in alignment, data quality,
and evaluation consistency. In this study, we introduce Ladder-base, the first
TCM-focused LLM trained with Group Relative Policy Optimization (GRPO), a
reinforcement learning method that improves reasoning and factual consistency
by optimizing response selection based on intra-group comparisons. Ladder-base
is built upon the Qwen2.5-7B-Instruct foundation model and trained exclusively
on the textual subset of the TCM-Ladder benchmark, using 80 percent of the data
for training and the remaining 20 percent split evenly between validation and
test sets. Through standardized evaluation, Ladder-base demonstrates superior
performance across multiple reasoning metrics when compared to both
state-of-the-art general-purpose LLMs such as GPT-4, Gemini 2.5, Claude 3, and
Qwen3 and domain-specific TCM models including BenTsao, HuatuoGPT2, and
Zhongjing. These findings suggest that GRPO provides an effective and efficient
strategy for aligning LLMs with expert-level reasoning in traditional medical
domains and supports the development of trustworthy and clinically grounded TCM
artificial intelligence systems.

</details>


### [74] [AFRICAPTION: Establishing a New Paradigm for Image Captioning in African Languages](https://arxiv.org/abs/2510.17405)
*Mardiyyah Oduwole,Prince Mireku,Fatimo Adebanjo,Oluwatosin Olajide,Mahi Aminu Aliyu,Jekaterina Novikova*

Main category: cs.CL

TL;DR: AfriCaption is a framework for multilingual image captioning in 20 African languages, addressing the gap in multimodal AI research for low-resource languages through curated datasets, dynamic pipelines, and a specialized model.


<details>
  <summary>Details</summary>
Motivation: Multimodal AI research has focused on high-resource languages, limiting democratization. This work aims to make advancements accessible by addressing under-represented African languages.

Method: Three main contributions: (i) curated dataset from Flickr8k with semantically aligned captions via context-aware selection and translation; (ii) dynamic pipeline with model ensembling and adaptive substitution for quality preservation; (iii) AfriCaption model (0.5B parameters) combining SigLIP and NLLB200 for caption generation.

Result: Established the first scalable image-captioning resource for under-represented African languages, ensuring ongoing data quality through the unified framework.

Conclusion: AfriCaption lays the groundwork for truly inclusive multimodal AI by providing comprehensive multilingual image captioning capabilities for African languages.

Abstract: Multimodal AI research has overwhelmingly focused on high-resource languages,
hindering the democratization of advancements in the field. To address this, we
present AfriCaption, a comprehensive framework for multilingual image
captioning in 20 African languages and our contributions are threefold: (i) a
curated dataset built on Flickr8k, featuring semantically aligned captions
generated via a context-aware selection and translation process; (ii) a
dynamic, context-preserving pipeline that ensures ongoing quality through model
ensembling and adaptive substitution; and (iii) the AfriCaption model, a 0.5B
parameter vision-to-text architecture that integrates SigLIP and NLLB200 for
caption generation across under-represented languages. This unified framework
ensures ongoing data quality and establishes the first scalable
image-captioning resource for under-represented African languages, laying the
groundwork for truly inclusive multimodal AI.

</details>


### [75] [BenCao: An Instruction-Tuned Large Language Model for Traditional Chinese Medicine](https://arxiv.org/abs/2510.17415)
*Jiacheng Xie,Yang Yu,Yibo Chen,Hanyao Zhang,Lening Zhao,Jiaxuan He,Lei Jiang,Xiaoting Tang,Guanghui An,Dong Xu*

Main category: cs.CL

TL;DR: BenCao is a ChatGPT-based multimodal assistant for Traditional Chinese Medicine that integrates structured knowledge, diagnostic data, and expert feedback to address limitations in existing TCM-domain LLMs.


<details>
  <summary>Details</summary>
Motivation: Applying LLMs to TCM is challenging due to holistic reasoning, implicit logic, and multimodal diagnostic cues. Existing TCM-domain LLMs lack multimodal integration, interpretability, and clinical applicability.

Method: Developed BenCao through natural language instruction tuning (not parameter retraining), integrating knowledge from 1,000+ classical/modern texts, scenario-based instruction framework, chain-of-thought simulation, expert feedback refinement, and external APIs for tongue-image classification and multimodal database retrieval.

Result: BenCao achieved superior accuracy to general-domain and TCM-domain models in single-choice benchmarks and multimodal classification tasks, particularly in diagnostics, herb recognition, and constitution classification. Deployed on OpenAI GPTs Store with nearly 1,000 global users by October 2025.

Conclusion: This study demonstrates the feasibility of developing TCM-domain LLMs through natural language instruction tuning and multimodal integration, providing a practical framework for aligning generative AI with traditional medical reasoning and scalable real-world deployment.

Abstract: Traditional Chinese Medicine (TCM), with a history spanning over two
millennia, plays a role in global healthcare. However, applying large language
models (LLMs) to TCM remains challenging due to its reliance on holistic
reasoning, implicit logic, and multimodal diagnostic cues. Existing TCM-domain
LLMs have made progress in text-based understanding but lack multimodal
integration, interpretability, and clinical applicability. To address these
limitations, we developed BenCao, a ChatGPT-based multimodal assistant for TCM,
integrating structured knowledge bases, diagnostic data, and expert feedback
refinement. BenCao was trained through natural language instruction tuning
rather than parameter retraining, aligning with expert-level reasoning and
ethical norms specific to TCM. The system incorporates a comprehensive
knowledge base of over 1,000 classical and modern texts, a scenario-based
instruction framework for diverse interactions, a chain-of-thought simulation
mechanism for interpretable reasoning, and a feedback refinement process
involving licensed TCM practitioners. BenCao connects to external APIs for
tongue-image classification and multimodal database retrieval, enabling dynamic
access to diagnostic resources. In evaluations across single-choice question
benchmarks and multimodal classification tasks, BenCao achieved superior
accuracy to general-domain and TCM-domain models, particularly in diagnostics,
herb recognition, and constitution classification. The model was deployed as an
interactive application on the OpenAI GPTs Store, accessed by nearly 1,000
users globally as of October 2025. This study demonstrates the feasibility of
developing a TCM-domain LLM through natural language-based instruction tuning
and multimodal integration, offering a practical framework for aligning
generative AI with traditional medical reasoning and a scalable pathway for
real-world deployment.

</details>


### [76] [Navigating the Alignment-Calibration Trade-off: A Pareto-Superior Frontier via Model Merging](https://arxiv.org/abs/2510.17426)
*Tiancheng Hu,Benjamin Minixhofer,Nigel Collier*

Main category: cs.CL

TL;DR: Simple weight interpolation between pre- and post-alignment models can create Pareto-optimal models that improve accuracy beyond both parents while recovering calibration lost during alignment.


<details>
  <summary>Details</summary>
Motivation: Post-training alignment causes not only accuracy drops but also severe calibration loss, making models overconfident and less reliable.

Method: Interpolating between model weights before and after alignment via simple post-hoc intervention.

Result: Consistently reveals Pareto-optimal interpolations that improve accuracy beyond both parent models while substantially recovering calibration.

Conclusion: Simple model merging provides computationally efficient method to mitigate full scope of alignment tax, yielding more capable and reliable models.

Abstract: The "alignment tax" of post-training is typically framed as a drop in task
accuracy. We show it also involves a severe loss of calibration, making models
overconfident, less reliable, and model outputs less diverse. We show that this
trade-off can be navigated effectively via a simple post-hoc intervention:
interpolating between a model's weights before and after alignment. Crucially,
this is not a strict trade-off. We find that the process consistently reveals
Pareto-optimal interpolations - models that improve accuracy beyond both
parents while substantially recovering the calibration lost during alignment.
Our work demonstrates that simple model merging provides a computationally
efficient method for mitigating the full scope of the alignment tax, yielding
models that are more capable and more reliable.

</details>


### [77] [Agentic Reinforcement Learning for Search is Unsafe](https://arxiv.org/abs/2510.17431)
*Yushi Yang,Shreyansh Padarha,Andrew Lee,Adam Mahdi*

Main category: cs.CL

TL;DR: RL-trained search models have fragile safety - simple attacks can bypass refusal mechanisms and trigger harmful searches and answers.


<details>
  <summary>Details</summary>
Motivation: To understand safety properties of agentic RL models that autonomously call tools during reasoning, particularly their vulnerability to attacks.

Method: Two simple attacks: Search attack (forces model to begin with search) and Multi-search attack (encourages repeated searching). Tested across Qwen and Llama model families with local and web search.

Result: Attacks lowered refusal rates by up to 60.0%, answer safety by 82.5%, and search-query safety by 82.4%. Models generate harmful search queries before refusal tokens.

Conclusion: Current RL training has core weakness - rewards effective queries without accounting for harmfulness. Urgent need for safety-aware agentic RL pipelines.

Abstract: Agentic reinforcement learning (RL) trains large language models to
autonomously call tools during reasoning, with search as the most common
application. These models excel at multi-step reasoning tasks, but their safety
properties are not well understood. In this study, we show that RL-trained
search models inherit refusal from instruction tuning and often deflect harmful
requests by turning them into safe queries. However, this safety is fragile.
Two simple attacks, one that forces the model to begin response with search
(Search attack), another that encourages models to repeatedly search
(Multi-search attack), trigger cascades of harmful searches and answers. Across
two model families (Qwen, Llama) with both local and web search, these attacks
lower refusal rates by up to 60.0%, answer safety by 82.5%, and search-query
safety by 82.4%. The attacks succeed by triggering models to generate harmful,
request-mirroring search queries before they can generate the inherited refusal
tokens. This exposes a core weakness of current RL training: it rewards
continued generation of effective queries without accounting for their
harmfulness. As a result, RL search models have vulnerabilities that users can
easily exploit, making it urgent to develop safety-aware agentic RL pipelines
optimising for safe search.

</details>


### [78] [Multilingual Clinical NER for Diseases and Medications Recognition in Cardiology Texts using BERT Embeddings](https://arxiv.org/abs/2510.17437)
*Manuela Daniela Danu,George Marica,Constantin Suciu,Lucian Mihai Itu,Oladimeji Farri*

Main category: cs.CL

TL;DR: Developed BERT-based models for clinical named entity recognition in cardiology across English, Spanish, and Italian, achieving state-of-the-art performance on disease and medication extraction tasks.


<details>
  <summary>Details</summary>
Motivation: There's a need to extract biomedical knowledge from unstructured clinical texts, but limited research exists for clinical NER in low-resource languages despite the success of contextual language models in English.

Method: Explored monolingual and multilingual BERT-based models trained on general domain text for extracting disease and medication mentions from clinical case reports in English, Spanish, and Italian as part of the BioASQ MultiCardioNER shared task.

Result: Achieved F1-scores of 77.88% on Spanish Diseases Recognition, 92.09% on Spanish Medications Recognition, 91.74% on English Medications Recognition, and 88.9% on Italian Medications Recognition, outperforming mean and median scores in the test leaderboard across all subtasks.

Conclusion: The study successfully developed deep contextual embedding models that enhance clinical NER performance in multiple languages, demonstrating the effectiveness of BERT-based approaches for extracting biomedical entities from clinical texts across different languages.

Abstract: The rapidly increasing volume of electronic health record (EHR) data
underscores a pressing need to unlock biomedical knowledge from unstructured
clinical texts to support advancements in data-driven clinical systems,
including patient diagnosis, disease progression monitoring, treatment effects
assessment, prediction of future clinical events, etc. While contextualized
language models have demonstrated impressive performance improvements for named
entity recognition (NER) systems in English corpora, there remains a scarcity
of research focused on clinical texts in low-resource languages. To bridge this
gap, our study aims to develop multiple deep contextual embedding models to
enhance clinical NER in the cardiology domain, as part of the BioASQ
MultiCardioNER shared task. We explore the effectiveness of different
monolingual and multilingual BERT-based models, trained on general domain text,
for extracting disease and medication mentions from clinical case reports
written in English, Spanish, and Italian. We achieved an F1-score of 77.88% on
Spanish Diseases Recognition (SDR), 92.09% on Spanish Medications Recognition
(SMR), 91.74% on English Medications Recognition (EMR), and 88.9% on Italian
Medications Recognition (IMR). These results outperform the mean and median F1
scores in the test leaderboard across all subtasks, with the mean/median values
being: 69.61%/75.66% for SDR, 81.22%/90.18% for SMR, 89.2%/88.96% for EMR, and
82.8%/87.76% for IMR.

</details>


### [79] [Evaluating Large Language Models on Urdu Idiom Translation](https://arxiv.org/abs/2510.17460)
*Muhammad Farmal Khan,Mousumi Akter*

Main category: cs.CL

TL;DR: First evaluation datasets for Urdu to English idiomatic translation introduced, showing prompt engineering improves idiomatic translation and Native Urdu inputs outperform Roman Urdu.


<details>
  <summary>Details</summary>
Motivation: Idiomatic translation is challenging for low resource languages like Urdu and has received limited prior attention.

Method: Created evaluation datasets for Urdu to English idiomatic translation, evaluated multiple LLMs and NMT systems using automatic metrics (BLEU, BERTScore, COMET, XCOMET), and tested prompt engineering approaches.

Result: Prompt engineering enhances idiomatic translation compared to direct translation, though performance differences among prompt types are minor. Native Urdu inputs produce more accurate idiomatic translations than Roman Urdu.

Conclusion: Text representation substantially affects translation quality, with Native Urdu outperforming Roman Urdu for idiomatic translation.

Abstract: Idiomatic translation remains a significant challenge in machine translation,
especially for low resource languages such as Urdu, and has received limited
prior attention. To advance research in this area, we introduce the first
evaluation datasets for Urdu to English idiomatic translation, covering both
Native Urdu and Roman Urdu scripts and annotated with gold-standard English
equivalents. We evaluate multiple open-source Large Language Models (LLMs) and
Neural Machine Translation (NMT) systems on this task, focusing on their
ability to preserve idiomatic and cultural meaning. Automatic metrics including
BLEU, BERTScore, COMET, and XCOMET are used to assess translation quality. Our
findings indicate that prompt engineering enhances idiomatic translation
compared to direct translation, though performance differences among prompt
types are relatively minor. Moreover, cross script comparisons reveal that text
representation substantially affects translation quality, with Native Urdu
inputs producing more accurate idiomatic translations than Roman Urdu.

</details>


### [80] [Disparities in Multilingual LLM-Based Healthcare Q&A](https://arxiv.org/abs/2510.17476)
*Ipek Baris Schlicht,Burcu Sayin,Zhixue Zhao,Frederik M. Labont,Cesare Barbera,Marco Viviani,Paolo Rosso,Lucie Flek*

Main category: cs.CL

TL;DR: This paper examines cross-lingual disparities in healthcare information quality across English, German, Turkish, Chinese, and Italian, revealing that LLM responses align more with English Wikipedia even for non-English prompts, but providing contextual excerpts can shift alignment toward culturally relevant knowledge.


<details>
  <summary>Details</summary>
Motivation: To address concerns about reliability and consistency of multilingual LLMs in healthcare, particularly regarding equitable access to reliable health information across different languages where information quality varies significantly.

Method: Constructed MultiWikiHealthCare dataset from Wikipedia; analyzed cross-lingual healthcare coverage; assessed LLM response alignment with references; conducted case study on factual alignment using contextual information and Retrieval-Augmented Generation (RAG).

Result: Found substantial cross-lingual disparities in both Wikipedia coverage and LLM factual alignment. LLM responses consistently aligned more with English Wikipedia regardless of prompt language, but providing contextual excerpts from non-English Wikipedia effectively shifted factual alignment toward culturally relevant knowledge.

Conclusion: The study highlights practical pathways for building more equitable, multilingual AI systems for healthcare by addressing cross-lingual disparities through contextual information provision.

Abstract: Equitable access to reliable health information is vital when integrating AI
into healthcare. Yet, information quality varies across languages, raising
concerns about the reliability and consistency of multilingual Large Language
Models (LLMs). We systematically examine cross-lingual disparities in
pre-training source and factuality alignment in LLM answers for multilingual
healthcare Q&A across English, German, Turkish, Chinese (Mandarin), and
Italian. We (i) constructed Multilingual Wiki Health Care
(MultiWikiHealthCare), a multilingual dataset from Wikipedia; (ii) analyzed
cross-lingual healthcare coverage; (iii) assessed LLM response alignment with
these references; and (iv) conducted a case study on factual alignment through
the use of contextual information and Retrieval-Augmented Generation (RAG). Our
findings reveal substantial cross-lingual disparities in both Wikipedia
coverage and LLM factual alignment. Across LLMs, responses align more with
English Wikipedia, even when the prompts are non-English. Providing contextual
excerpts from non-English Wikipedia at inference time effectively shifts
factual alignment toward culturally relevant knowledge. These results highlight
practical pathways for building more equitable, multilingual AI systems for
healthcare.

</details>


### [81] [ReXMoE: Reusing Experts with Minimal Overhead in Mixture-of-Experts](https://arxiv.org/abs/2510.17483)
*Zheyue Tan,Zhiyuan Li,Tao Yuan,Dong Zhou,Weilin Liu,Yueqing Zhuang,Yadong Li,Guowei Niu,Cheng Qin,Zhuyu Yao,Congyi Liu,Haiyang Xu,Boxun Li,Guohao Dai,Bo Zhao,Yu Wang*

Main category: cs.CL

TL;DR: ReXMoE is a novel Mixture-of-Experts architecture that enables cross-layer expert reuse, overcoming limitations of layer-local routing and improving model performance without increasing parameter count.


<details>
  <summary>Details</summary>
Motivation: Current MoE architectures are limited by layer-local routing, which restricts each layer to its own expert pool, creating a trade-off between expert dimensionality and routing diversity within fixed parameter budgets.

Method: ReXMoE allows routers to reuse experts across adjacent layers, decoupling expert dimensionality from per-layer budgets. It uses progressive scaling routing (PSR) to gradually increase candidate expert pools during training.

Result: Extensive experiments on 0.5B to 7B parameter models show ReXMoE consistently improves language modeling and downstream task performance under fixed architectural dimensions.

Conclusion: ReXMoE represents a new design paradigm for parameter-efficient and scalable MoE-based LLMs, enabling richer expert combinations without sacrificing capacity or inflating parameters.

Abstract: Mixture-of-Experts (MoE) architectures have emerged as a promising approach
to scale Large Language Models (LLMs). MoE boosts the efficiency by activating
a subset of experts per token. Recent works show that fine-grained experts
substantially enriches the combinatorial flexibility of active experts and
enhances model expressiveness. However, such a design is fundamentally limited
by the layer-local routing mechanism: each layer is restricted to its own
expert pool. This requires a careful trade-off between expert dimensionality
and routing diversity given fixed parameter budgets. We describe ReXMoE, a
novel MoE architecture that improves routing beyond the existing layer-local
approaches by allowing routers to reuse experts across adjacent layers. ReXMoE
decouples expert dimensionality from per-layer budgets, enabling richer expert
combinations without sacrificing individual expert capacity or inflating
overall parameters. To this end, we propose a new progressive scaling routing
(PSR) strategy to gradually increase the candidate expert pool during training.
As a result, ReXMoE improves both language modeling and downstream task
performance. Extensive experiments on models ranging from 0.5B to 7B parameters
across different architectures demonstrate that ReXMoE consistently improves
performance under fixed architectural dimensions, confirming ReXMoE as new
design paradigm for parameter-efficient and scalable MoE-based LLMs.

</details>


### [82] [DETree: DEtecting Human-AI Collaborative Texts via Tree-Structured Hierarchical Representation Learning](https://arxiv.org/abs/2510.17489)
*Yongxin He,Shan Zhang,Yixuan Cao,Lei Ma,Ping Luo*

Main category: cs.CL

TL;DR: DETree is a novel approach for detecting AI-involved text that models human-AI collaboration processes as a Hierarchical Affinity Tree structure, improving detection performance and robustness in out-of-distribution scenarios.


<details>
  <summary>Details</summary>
Motivation: Current AI text detection methods crudely model complex human-AI collaboration processes (AI-written text edited by humans, human-written text edited by AI, AI-generated text refined by other AI) using binary or multi-class classification, which fails to capture the inherent clustering relationships in these processes.

Method: Proposes DETree that models relationships among different text generation processes as a Hierarchical Affinity Tree structure with a specialized loss function to align text representations with this tree. Also developed RealBench dataset containing various human-AI collaboration processes.

Result: The method improves performance in hybrid text detection tasks and significantly enhances robustness and generalization in out-of-distribution scenarios, particularly in few-shot learning conditions.

Conclusion: DETree demonstrates the promise of training-based approaches in OOD settings for AI text detection, effectively capturing the complex relationships in human-AI collaborative text generation processes.

Abstract: Detecting AI-involved text is essential for combating misinformation,
plagiarism, and academic misconduct. However, AI text generation includes
diverse collaborative processes (AI-written text edited by humans,
human-written text edited by AI, and AI-generated text refined by other AI),
where various or even new LLMs could be involved. Texts generated through these
varied processes exhibit complex characteristics, presenting significant
challenges for detection. Current methods model these processes rather crudely,
primarily employing binary classification (purely human vs. AI-involved) or
multi-classification (treating human-AI collaboration as a new class). We
observe that representations of texts generated through different processes
exhibit inherent clustering relationships. Therefore, we propose DETree, a
novel approach that models the relationships among different processes as a
Hierarchical Affinity Tree structure, and introduces a specialized loss
function that aligns text representations with this tree. To facilitate this
learning, we developed RealBench, a comprehensive benchmark dataset that
automatically incorporates a wide spectrum of hybrid texts produced through
various human-AI collaboration processes. Our method improves performance in
hybrid text detection tasks and significantly enhances robustness and
generalization in out-of-distribution scenarios, particularly in few-shot
learning conditions, further demonstrating the promise of training-based
approaches in OOD settings. Our code and dataset are available at
https://github.com/heyongxin233/DETree.

</details>


### [83] [Empowering Real-World: A Survey on the Technology, Practice, and Evaluation of LLM-driven Industry Agents](https://arxiv.org/abs/2510.17491)
*Yihong Tang,Kehai Chen,Liang Yue,Jinxin Fan,Caishen Zhou,Xiaoguang Li,Yuyang Zhang,Mingming Zhao,Shixiong Kai,Kaiyang Guo,Xingshan Zeng,Wenjing Cun,Lifeng Shang,Min Zhang*

Main category: cs.CL

TL;DR: This paper provides a systematic review of LLM-based industry agents, examining their technological pillars (Memory, Planning, Tool Use), applications across various domains, evaluation methods, and practical challenges.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of translating general agent research into practical industry transformations by providing a comprehensive roadmap for understanding and building next-generation industry agents.

Method: Uses an industry agent capability maturity framework to analyze agent evolution from 'process execution systems' to 'adaptive social systems', and systematically reviews technologies, applications, and evaluation methods.

Result: The review outlines the technological evolution of agent capabilities, identifies applications in real-world domains, and highlights challenges in evaluation systems regarding authenticity, safety, and industry specificity.

Conclusion: By combining technological evolution with industry practices, this review clarifies the current state and provides a roadmap and theoretical foundation for developing next-generation industry agents.

Abstract: With the rise of large language models (LLMs), LLM agents capable of
autonomous reasoning, planning, and executing complex tasks have become a
frontier in artificial intelligence. However, how to translate the research on
general agents into productivity that drives industry transformations remains a
significant challenge. To address this, this paper systematically reviews the
technologies, applications, and evaluation methods of industry agents based on
LLMs. Using an industry agent capability maturity framework, it outlines the
evolution of agents in industry applications, from "process execution systems"
to "adaptive social systems." First, we examine the three key technological
pillars that support the advancement of agent capabilities: Memory, Planning,
and Tool Use. We discuss how these technologies evolve from supporting simple
tasks in their early forms to enabling complex autonomous systems and
collective intelligence in more advanced forms. Then, we provide an overview of
the application of industry agents in real-world domains such as digital
engineering, scientific discovery, embodied intelligence, collaborative
business execution, and complex system simulation. Additionally, this paper
reviews the evaluation benchmarks and methods for both fundamental and
specialized capabilities, identifying the challenges existing evaluation
systems face regarding authenticity, safety, and industry specificity. Finally,
we focus on the practical challenges faced by industry agents, exploring their
capability boundaries, developmental potential, and governance issues in
various scenarios, while providing insights into future directions. By
combining technological evolution with industry practices, this review aims to
clarify the current state and offer a clear roadmap and theoretical foundation
for understanding and building the next generation of industry agents.

</details>


### [84] [Deep Self-Evolving Reasoning](https://arxiv.org/abs/2510.17498)
*Zihan Liu,Shun Zheng,Xumeng Wen,Yang Wang,Jiang Bian,Mao Yang*

Main category: cs.CL

TL;DR: Deep Self-Evolving Reasoning (DSER) extends reasoning limits of smaller models through probabilistic parallel processes, enabling them to solve previously unsolvable problems despite weak verification capabilities.


<details>
  <summary>Details</summary>
Motivation: Current verification-refinement frameworks require strong verification capabilities that are fragile in open-weight, smaller-scale models, limiting their ability to solve hard reasoning tasks.

Method: Conceptualizes iterative reasoning as a Markov chain with stochastic transitions, running multiple long-horizon self-evolving processes in parallel to amplify small positive tendencies toward correct solutions.

Result: Applied to DeepSeek-R1-0528-Qwen3-8B model, DSER solved 5 out of 9 previously unsolvable problems on AIME 2024-2025 benchmark and surpassed the single-turn accuracy of its 600B-parameter teacher through majority voting.

Conclusion: DSER framework not only provides immediate utility for test-time scaling but also diagnoses fundamental limitations of current open-weight reasoners, establishing a research agenda for developing next-generation models with intrinsic self-evolving capabilities.

Abstract: Long-form chain-of-thought reasoning has become a cornerstone of advanced
reasoning in large language models. While recent verification-refinement
frameworks have enabled proprietary models to solve Olympiad-level problems,
their effectiveness hinges on strong, reliable verification and correction
capabilities, which remain fragile in open-weight, smaller-scale models. This
work demonstrates that even with weak verification and refinement capabilities
on hard tasks, the reasoning limits of such models can be substantially
extended through a probabilistic paradigm we call Deep Self-Evolving Reasoning
(DSER). We conceptualize iterative reasoning as a Markov chain, where each step
represents a stochastic transition in the solution space. The key insight is
that convergence to a correct solution is guaranteed as long as the probability
of improvement marginally exceeds that of degradation. By running multiple
long-horizon, self-evolving processes in parallel, DSER amplifies these small
positive tendencies, enabling the model to asymptotically approach correct
answers. Empirically, we apply DSER to the DeepSeek-R1-0528-Qwen3-8B model. On
the challenging AIME 2024-2025 benchmark, DSER solves 5 out of 9 previously
unsolvable problems and boosts overall performance, enabling this compact model
to surpass the single-turn accuracy of its 600B-parameter teacher through
majority voting. Beyond its immediate utility for test-time scaling, the DSER
framework serves to diagnose the fundamental limitations of current open-weight
reasoners. By clearly delineating their shortcomings in self-verification,
refinement, and stability, our findings establish a clear research agenda for
developing next-generation models with powerful, intrinsic self-evolving
capabilities.

</details>


### [85] [Lingua Custodi's participation at the WMT 2025 Terminology shared task](https://arxiv.org/abs/2510.17504)
*Jingshu Liu,Raheel Qader,Gatan Caillaut,Mariam Nakhl*

Main category: cs.CL

TL;DR: The paper explores methods for learning multilingual sentence embeddings by combining monolingual and cross-lingual training techniques, achieving significant improvements in bi-text retrieval accuracy across 112 languages while reducing parallel data requirements by 80%.


<details>
  <summary>Details</summary>
Motivation: BERT has been effective for monolingual sentence embeddings but cross-lingual sentence embeddings using BERT have not been thoroughly explored. The authors aim to develop effective multilingual sentence embedding methods.

Method: Systematically investigated combination of methods including masked language modeling (MLM), translation language modeling (TLM), dual encoder translation ranking, and additive margin softmax. Used pre-trained multilingual language models to reduce parallel data requirements.

Result: Achieved 83.7% bi-text retrieval accuracy over 112 languages on Tatoeba (vs 65.5% by LASER), reduced parallel training data requirements by 80%, and trained competitive NMT models for en-zh and en-de using mined data.

Conclusion: The proposed methods successfully produce high-quality multilingual sentence embeddings that perform well on both cross-lingual and monolingual tasks, with the best model publicly released for 109+ languages.

Abstract: While BERT is an effective method for learning monolingual sentence
embeddings for semantic similarity and embedding based transfer learning BERT
based cross-lingual sentence embeddings have yet to be explored. We
systematically investigate methods for learning multilingual sentence
embeddings by combining the best methods for learning monolingual and
cross-lingual representations including: masked language modeling (MLM),
translation language modeling (TLM), dual encoder translation ranking, and
additive margin softmax. We show that introducing a pre-trained multilingual
language model dramatically reduces the amount of parallel training data
required to achieve good performance by 80%. Composing the best of these
methods produces a model that achieves 83.7% bi-text retrieval accuracy over
112 languages on Tatoeba, well above the 65.5 achieved by LASER, while still
performing competitively on monolingual transfer learning benchmarks. Parallel
data mined from CommonCrawl using our best model is shown to train competitive
NMT models for en-zh and en-de. We publicly release our best multilingual
sentence embedding model for 109+ languages at https://tfhub.dev/google/LaBSE.

</details>


### [86] [Annotation-Efficient Universal Honesty Alignment](https://arxiv.org/abs/2510.17509)
*Shiyu Ni,Keping Bi,Jiafeng Guo,Minghao Tang,Jingtong Wu,Zengxin Han,Xueqi Cheng*

Main category: cs.CL

TL;DR: EliCal is a two-stage framework for honesty alignment in LLMs that first uses self-consistency supervision to elicit internal confidence, then calibrates with minimal correctness annotations, achieving near-optimal performance with only 0.18% of full supervision.


<details>
  <summary>Details</summary>
Motivation: Existing honesty alignment methods require costly large-scale labeling for training-based calibration, creating a need for more annotation-efficient approaches to achieve universal honesty alignment in LLMs.

Method: Two-stage framework: 1) Elicitation stage uses inexpensive self-consistency supervision to extract internal confidence, 2) Calibration stage uses a small set of correctness annotations (1k instances) to calibrate the confidence.

Result: EliCal achieves near-optimal alignment with only 1k correctness annotations (0.18% of full supervision) and shows better alignment performance on unseen MMLU tasks compared to calibration-only baselines.

Conclusion: EliCal offers a scalable solution for universal honesty alignment in LLMs by significantly reducing annotation costs while maintaining strong performance.

Abstract: Honesty alignment-the ability of large language models (LLMs) to recognize
their knowledge boundaries and express calibrated confidence-is essential for
trustworthy deployment. Existing methods either rely on training-free
confidence estimation (e.g., token probabilities, self-consistency) or
training-based calibration with correctness annotations. While effective,
achieving universal honesty alignment with training-based calibration requires
costly, large-scale labeling. To support annotation-efficient training, we
introduce Elicitation-Then-Calibration (EliCal), a two-stage framework that
first elicits internal confidence using inexpensive self-consistency
supervision, then calibrates this confidence with a small set of correctness
annotations. To support a large-scale study, we release HonestyBench, a
benchmark covering ten free-form QA datasets with 560k training and 70k
evaluation instances annotated with correctness and self-consistency signals.
Experiments show that EliCal achieves near-optimal alignment with only 1k
correctness annotations (0.18% of full supervision) and better alignment
performance on unseen MMLU tasks than the calibration-only baseline, offering a
scalable solution toward universal honesty alignment in LLMs.

</details>


### [87] [SimBench: Benchmarking the Ability of Large Language Models to Simulate Human Behaviors](https://arxiv.org/abs/2510.17516)
*Tiancheng Hu,Joachim Baumann,Lorenzo Lupo,Dirk Hovy,Nigel Collier,Paul Rttger*

Main category: cs.CL

TL;DR: SimBench is the first standardized benchmark for evaluating LLM simulations of human behavior, unifying 20 diverse datasets to systematically assess when and why LLM simulations succeed or fail.


<details>
  <summary>Details</summary>
Motivation: Current evaluations of LLM simulations are fragmented with bespoke tasks and metrics, creating incomparable results that hinder robust scientific progress in using LLMs to simulate human behavior.

Method: Created SimBench by unifying 20 diverse datasets covering tasks from moral decision-making to economic choice across a large global participant pool, enabling systematic evaluation of LLM simulation capabilities.

Result: Best LLMs today have limited simulation ability (40.80/100), performance scales log-linearly with model size, simulation ability correlates strongly with knowledge-intensive reasoning (MMLU-Pro, r=0.939), and models struggle with specific demographic groups.

Conclusion: SimBench enables measurable progress in developing more faithful LLM simulators by providing a standardized foundation for evaluating simulation capabilities, revealing key insights about performance scaling, alignment-simulation trade-offs, and demographic challenges.

Abstract: Large language model (LLM) simulations of human behavior have the potential
to revolutionize the social and behavioral sciences, if and only if they
faithfully reflect real human behaviors. Current evaluations are fragmented,
based on bespoke tasks and metrics, creating a patchwork of incomparable
results. To address this, we introduce SimBench, the first large-scale,
standardized benchmark for a robust, reproducible science of LLM simulation. By
unifying 20 diverse datasets covering tasks from moral decision-making to
economic choice across a large global participant pool, SimBench provides the
necessary foundation to ask fundamental questions about when, how, and why LLM
simulations succeed or fail. We show that, while even the best LLMs today have
limited simulation ability (score: 40.80/100), performance scales log-linearly
with model size. Simulation performance is not improved by increased
inference-time compute. We demonstrate an alignment-simulation trade-off:
instruction-tuning improves performance on low-entropy (consensus) questions
but degrades it on high-entropy (diverse) ones. Models particularly struggle
when simulating specific demographic groups. Finally, we demonstrate that
simulation ability correlates most strongly with deep, knowledge-intensive
reasoning (MMLU-Pro, r=0.939). By making progress measurable, we aim to
accelerate the development of more faithful LLM simulators.

</details>


### [88] [OncoReason: Structuring Clinical Reasoning in LLMs for Robust and Interpretable Survival Prediction](https://arxiv.org/abs/2510.17532)
*Raghu Vamshi Hemadri,Geetha Krishna Guruju,Kristi Topollai,Anna Ewa Choromanska*

Main category: cs.CL

TL;DR: A unified multi-task learning framework that aligns LLMs with clinical reasoning for cancer outcome prediction, achieving improved accuracy and interpretability through Chain-of-Thought prompting and reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Predicting cancer treatment outcomes requires accurate and interpretable models, especially with heterogeneous clinical data. LLMs lack structured reasoning capabilities needed for high-stakes clinical decision support.

Method: Multi-task learning framework training LLMs to jointly perform binary survival classification, continuous survival time regression, and natural language rationale generation. Evaluated three alignment strategies: standard SFT, SFT with CoT prompting, and GRPO reinforcement learning.

Result: CoT prompting improved F1 by +6.0 and reduced MAE by 12%. GRPO achieved state-of-the-art interpretability and predictive performance across BLEU, ROUGE, and BERTScore metrics. Showed existing biomedical LLMs often fail to produce valid reasoning traces.

Conclusion: Reasoning-aware alignment is crucial for multi-task clinical modeling, setting a new benchmark for interpretable, trustworthy LLMs in precision oncology.

Abstract: Predicting cancer treatment outcomes requires models that are both accurate
and interpretable, particularly in the presence of heterogeneous clinical data.
While large language models (LLMs) have shown strong performance in biomedical
NLP, they often lack structured reasoning capabilities critical for high-stakes
decision support. We present a unified, multi-task learning framework that
aligns autoregressive LLMs with clinical reasoning for outcome prediction on
the MSK-CHORD dataset. Our models are trained to jointly perform binary
survival classification, continuous survival time regression, and natural
language rationale generation. We evaluate three alignment strategies: (1)
standard supervised fine-tuning (SFT), (2) SFT with Chain-of-Thought (CoT)
prompting to elicit step-by-step reasoning, and (3) Group Relative Policy
Optimization (GRPO), a reinforcement learning method that aligns model outputs
to expert-derived reasoning trajectories. Experiments with LLaMa3-8B and
Med42-8B backbones demonstrate that CoT prompting improves F1 by +6.0 and
reduces MAE by 12%, while GRPO achieves state-of-the-art interpretability and
predictive performance across BLEU, ROUGE, and BERTScore. We further show that
existing biomedical LLMs often fail to produce valid reasoning traces due to
architectural constraints. Our findings underscore the importance of
reasoning-aware alignment in multi-task clinical modeling and set a new
benchmark for interpretable, trustworthy LLMs in precision oncology.

</details>


### [89] [When Annotators Disagree, Topology Explains: Mapper, a Topological Tool for Exploring Text Embedding Geometry and Ambiguity](https://arxiv.org/abs/2510.17548)
*Nisrine Rair,Alban Goupil,Valeriu Vrabie,Emmanuel Chochoy*

Main category: cs.CL

TL;DR: The paper proposes using topological data analysis (Mapper) to analyze how language models internally represent ambiguity, revealing that fine-tuning creates modular, non-convex decision regions with high prediction purity but reduced alignment with ground-truth labels in ambiguous cases.


<details>
  <summary>Details</summary>
Motivation: Traditional scalar metrics like accuracy fail to capture how models internally represent ambiguity, especially when human annotators disagree. There's a need to understand how models encode ambiguous instances beyond surface-level performance measures.

Method: Used Mapper, a tool from topological data analysis, to analyze RoBERTa-Large fine-tuned on the MD-Offense dataset. This approach captures the geometry of embedding space directly, unlike traditional tools like PCA or UMAP.

Result: Fine-tuning restructures embedding space into modular, non-convex regions aligned with model predictions. Over 98% of connected components show 90% prediction purity, but alignment with ground-truth labels drops in ambiguous data, revealing tension between structural confidence and label uncertainty.

Conclusion: Mapper serves as a powerful diagnostic tool for understanding how models resolve ambiguity, enabling topological metrics that can inform proactive modeling strategies in subjective NLP tasks.

Abstract: Language models are often evaluated with scalar metrics like accuracy, but
such measures fail to capture how models internally represent ambiguity,
especially when human annotators disagree. We propose a topological perspective
to analyze how fine-tuned models encode ambiguity and more generally instances.
  Applied to RoBERTa-Large on the MD-Offense dataset, Mapper, a tool from
topological data analysis, reveals that fine-tuning restructures embedding
space into modular, non-convex regions aligned with model predictions, even for
highly ambiguous cases. Over $98\%$ of connected components exhibit $\geq 90\%$
prediction purity, yet alignment with ground-truth labels drops in ambiguous
data, surfacing a hidden tension between structural confidence and label
uncertainty.
  Unlike traditional tools such as PCA or UMAP, Mapper captures this geometry
directly uncovering decision regions, boundary collapses, and overconfident
clusters. Our findings position Mapper as a powerful diagnostic tool for
understanding how models resolve ambiguity. Beyond visualization, it also
enables topological metrics that may inform proactive modeling strategies in
subjective NLP tasks.

</details>


### [90] [Language Confusion Gate: Language-Aware Decoding Through Model Self-Distillation](https://arxiv.org/abs/2510.17555)
*Collin Zhang,Fei Huang,Chenhan Yuan,Junyang Lin*

Main category: cs.CL

TL;DR: LCG is a lightweight plug-in solution that reduces language confusion in LLMs during decoding without model retraining, distinguishing between harmful confusion and acceptable code-switching.


<details>
  <summary>Details</summary>
Motivation: Current solutions for language confusion in LLMs either require model retraining or cannot differentiate between harmful confusion and acceptable code-switching.

Method: Language Confusion Gate (LCG) filters tokens during decoding using norm-adjusted self-distillation to predict language families and apply masking only when needed, leveraging findings about token embedding norms and top predictions.

Result: LCG decreases language confusion significantly (often by an order of magnitude) across various models including Qwen3, GPT-OSS, Gemma3, Llama3.1, without negatively impacting task performance.

Conclusion: LCG provides an effective plug-in solution for reducing language confusion in LLMs during text generation without requiring model modifications.

Abstract: Large language models (LLMs) often experience language confusion, which is
the unintended mixing of languages during text generation. Current solutions to
this problem either necessitate model retraining or cannot differentiate
between harmful confusion and acceptable code-switching. This paper introduces
the Language Confusion Gate (LCG), a lightweight, plug-in solution that filters
tokens during decoding without altering the base LLM. The LCG is trained using
norm-adjusted self-distillation to predict appropriate language families and
apply masking only when needed. Our method is based on the findings that
language confusion is infrequent, correct-language tokens are usually among the
top predictions, and output token embedding norms are larger for high-resource
languages, which biases sampling. When evaluated across various models,
including Qwen3, GPT-OSS, Gemma3, Llama3.1, LCG decreases language confusion
significantly, often by an order of magnitude, without negatively impacting
task performance. Code is available at
https://github.com/collinzrj/language_confusion_gate.

</details>


### [91] [HGAdapter: Hypergraph-based Adapters in Language Models for Code Summarization and Clone Detection](https://arxiv.org/abs/2510.17591)
*Guang Yang,Yujie Zhu*

Main category: cs.CL

TL;DR: HGAdapter is a hypergraph-based adapter that captures high-order correlations in code tokens (AST family, lexical, and line correlations) to enhance pre-trained language models for code-related tasks.


<details>
  <summary>Details</summary>
Motivation: Current PLMs for code tasks don't consider high-order data correlations within code, limiting their potential effectiveness despite good baseline performance.

Method: Proposed three high-order correlations in code tokens, designed a tokens and hyperedges generator, and developed HGAdapter - an improved hypergraph neural network combined with adapter tuning that can be inserted into various PLMs.

Result: Experiments on six languages for code summarization and clone detection showed improved PLM performance across datasets, validating the benefits of high-order correlations.

Conclusion: High-order data correlations in code contribute to improved effectiveness, and HGAdapter successfully encodes these correlations to enhance PLM performance for code-related tasks.

Abstract: Pre-trained language models (PLMs) are increasingly being applied to
code-related tasks. Although PLMs have achieved good results, they do not take
into account potential high-order data correlations within the code. We propose
three types of high-order correlations in code tokens, i.e. abstract syntax
tree family correlation, lexical correlation, and line correlation. We design a
tokens and hyperedges generator to capture these high-order data correlations.
We improve the architecture of hypergraph neural networks and combine it with
adapter tuning to propose a novel hypergraph-based adapter (HGAdapter) to
fine-tune PLMs. HGAdapter can encode high-order data correlations and is
allowed to be inserted into various PLMs to enhance performance. Experiments
were conducted on several public datasets, including six languages of code
summarization and code clone detection tasks. Our methods improved the
performance of PLMs in datasets to varying degrees. Experimental results
validate the introduction of high-order data correlations that contribute to
improved effectiveness.

</details>


### [92] [LawChain: Modeling Legal Reasoning Chains for Chinese Tort Case Analysis](https://arxiv.org/abs/2510.17602)
*Huiyuan Xie,Chenyang Li,Huining Zhu,Chubin Zhang,Yuxiao Ye,Zhenghao Liu,Zhiyuan Liu*

Main category: cs.CL

TL;DR: This paper introduces LawChain, a novel framework for modeling legal reasoning in Chinese tort-related civil cases, addressing gaps in existing computational approaches that focus mainly on criminal cases and generic reasoning frameworks.


<details>
  <summary>Details</summary>
Motivation: Existing computational approaches to legal reasoning rely on generic frameworks like syllogism and IRAC, lack comprehensive examination of nuanced legal reasoning processes, and focus predominantly on criminal cases with insufficient modeling for civil cases.

Method: The authors operationalize legal reasoning processes into the LawChain framework - a three-module reasoning framework with multiple finer-grained sub-steps. They construct an evaluation benchmark (LawChain$_{eval}$) and evaluate state-of-the-art LLMs, then introduce baseline approaches incorporating LawChain-style reasoning through prompting or post-training.

Result: Current large language models fall short in accurately handling crucial elements of tort legal reasoning. The proposed baseline approaches achieve significant improvements in tort-related legal reasoning and generalize well to related legal analysis tasks like Legal Named-Entity Recognition and Criminal Damages Calculation.

Conclusion: Explicitly modeling legal reasoning chains enhances the reasoning capabilities of language models, demonstrating the value of the LawChain framework for improving legal reasoning in civil contexts and related legal analysis tasks.

Abstract: Legal reasoning is a fundamental component of legal analysis and
decision-making. Existing computational approaches to legal reasoning
predominantly rely on generic reasoning frameworks such as syllogism and IRAC,
which do not comprehensively examine the nuanced processes that underpin legal
reasoning. Moreover, current research has largely focused on criminal cases,
with insufficient modeling for civil cases. In this work, we present a novel
framework for explicitly modeling legal reasoning in the analysis of Chinese
tort-related civil cases. We first operationalize the legal reasoning processes
used in tort analysis into the LawChain framework. LawChain is a three-module
reasoning framework, with each module consisting of multiple finer-grained
sub-steps. Informed by the LawChain framework, we introduce the task of tort
legal reasoning and construct an evaluation benchmark, LawChain$_{eval}$, to
systematically assess the critical steps within analytical reasoning chains for
tort analysis. Leveraging this benchmark, we evaluate state-of-the-art large
language models for their legal reasoning ability in civil tort contexts. Our
results indicate that current models still fall short in accurately handling
crucial elements of tort legal reasoning. Furthermore, we introduce several
baseline approaches that explicitly incorporate LawChain-style reasoning
through prompting or post-training. We conduct further experiments on
additional legal analysis tasks, such as Legal Named-Entity Recognition and
Criminal Damages Calculation, to verify the generalizability of these
baselines. The proposed baseline approaches achieve significant improvements in
tort-related legal reasoning and generalize well to related legal analysis
tasks, thus demonstrating the value of explicitly modeling legal reasoning
chains to enhance the reasoning capabilities of language models.

</details>


### [93] [Forget to Know, Remember to Use: Context-Aware Unlearning for Large Language Models](https://arxiv.org/abs/2510.17620)
*Yuefeng Peng,Parnian Afshar,Megan Ganji,Thomas Butler,Amir Houmansadr,Mingxian Wang,Dezhi Hong*

Main category: cs.CL

TL;DR: This paper identifies a limitation in current unlearning methods where models lose the ability to use removed knowledge even when it's provided in the context, and proposes a solution to preserve this contextual utility while maintaining effective forgetting.


<details>
  <summary>Details</summary>
Motivation: Current unlearning evaluations overlook contextual utility - the model's ability to use removed information when it's reintroduced in prompts, which is important for practical usability.

Method: Augment unlearning objectives with a plug-in term that preserves the model's ability to use forgotten knowledge when present in context, while maintaining effective forgetting and retain-set utility.

Result: The approach restores contextual utility to near original levels while still maintaining effective forgetting and retain-set utility across six state-of-the-art unlearning methods.

Conclusion: Preserving contextual utility is crucial for practical unlearning, and the proposed plug-in term effectively addresses this limitation without compromising other unlearning objectives.

Abstract: Large language models may encode sensitive information or outdated knowledge
that needs to be removed, to ensure responsible and compliant model responses.
Unlearning has emerged as an efficient alternative to full retraining, aiming
to remove specific knowledge while preserving overall model utility. Existing
evaluations of unlearning methods focus on (1) the extent of forgetting of the
target knowledge (forget set) and (2) maintaining performance on the retain set
(i.e., utility). However, these evaluations overlook an important usability
aspect: users may still want the model to leverage the removed information if
it is re-introduced in the prompt. In a systematic evaluation of six
state-of-the-art unlearning methods, we find that they consistently impair such
contextual utility. To address this, we augment unlearning objectives with a
plug-in term that preserves the model's ability to use forgotten knowledge when
it is present in context. Extensive experiments demonstrate that our approach
restores contextual utility to near original levels while still maintaining
effective forgetting and retain-set utility.

</details>


### [94] [Qomhra: A Bilingual Irish-English Large Language Model](https://arxiv.org/abs/2510.17652)
*Joseph McInerney*

Main category: cs.CL

TL;DR: Qomhr'a is a bilingual Irish-English LLM developed under low-resource constraints, using a pipeline of continued pre-training, instruction tuning, and human preference alignment. It leverages Gemini-2.5-Pro to create datasets and shows significant performance improvements in both languages.


<details>
  <summary>Details</summary>
Motivation: To develop a bilingual Irish-English LLM under low-resource conditions, addressing the need for improved Irish language performance while preserving English capabilities, and creating resources for instruction following and chatbot functionality.

Method: Used a complete pipeline including bilingual continued pre-training, instruction tuning, and human preference alignment. Mixed and curated Irish and English corpora. Evaluated 6 LLMs, selected Gemini-2.5-Pro to synthesize instruction tuning and human preference datasets. Created 30K parallel instruction dataset and 1K human preference dataset.

Result: Achieved gains of up to 29% in Irish and 44% in English across benchmarks for translation, gender understanding, topic identification and world knowledge. Generated human preference datasets with near perfect alignment with native Irish speaker. Demonstrated clear progress in instruction following for chatbot functionality.

Conclusion: Qomhr'a successfully addresses low-resource bilingual LLM development, showing significant performance improvements in both Irish and English while establishing effective methods for instruction tuning and human preference alignment in resource-constrained scenarios.

Abstract: This paper introduces Qomhr\'a, a bilingual Irish-English large language
model (LLM), developed under low-resource constraints presenting a complete
pipeline spanning bilingual continued pre-training, instruction tuning, and
alignment from human preferences. Newly accessible Irish corpora and English
text are mixed and curated to improve Irish performance while preserving
English ability. 6 closed-weight LLMs are judged for their Irish text
generation by a native speaker, a learner and other LLMs. Google's
Gemini-2.5-Pro is ranked the highest and is subsequently used to synthesise
instruction tuning and human preference datasets. Two datasets are contributed
leveraging Gemini-2.5-Pro: a 30K Irish-English parallel instruction tuning
dataset and a 1K human preference dataset, generating accepted and rejected
responses that show near perfect alignment with a native Irish speaker.
Qomhr\'a is comprehensively evaluated across benchmarks testing translation,
gender understanding, topic identification and world knowledge with gains of up
to 29% in Irish and 44% in English. Qomhr\'a also undergoes instruction tuning
and demonstrates clear progress in instruction following, crucial for chatbot
functionality.

</details>


### [95] [Towards Mining Effective Pedagogical Strategies from Learner-LLM Educational Dialogues](https://arxiv.org/abs/2510.17698)
*Liqun He,Manolis Mavrikis,Mutlu Cukurova*

Main category: cs.CL

TL;DR: This paper proposes a dialogue analysis approach to evaluate LLM-based educational applications by examining learner-LLM interactions and pedagogical strategies, rather than just technical performance or learning outcomes.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation methods for educational LLMs focus primarily on technical performance or learning outcomes, neglecting the crucial aspect of learner-LLM interactions and dialogue dynamics in educational settings.

Method: The study employs a dialogue analysis approach involving four steps: dialogue data collection, dialogue act (DA) annotation, DA pattern mining, and predictive model building to identify effective pedagogical strategies.

Result: Early insights are presented as initial findings, with the work representing an ongoing study that outlines preliminary steps toward future research in this area.

Conclusion: The research underscores the importance of evaluating LLM-based educational applications by focusing on dialogue dynamics and pedagogical strategies, highlighting the need to move beyond traditional technical performance metrics.

Abstract: Dialogue plays a crucial role in educational settings, yet existing
evaluation methods for educational applications of large language models (LLMs)
primarily focus on technical performance or learning outcomes, often neglecting
attention to learner-LLM interactions. To narrow this gap, this AIED Doctoral
Consortium paper presents an ongoing study employing a dialogue analysis
approach to identify effective pedagogical strategies from learner-LLM
dialogues. The proposed approach involves dialogue data collection, dialogue
act (DA) annotation, DA pattern mining, and predictive model building. Early
insights are outlined as an initial step toward future research. The work
underscores the need to evaluate LLM-based educational applications by focusing
on dialogue dynamics and pedagogical strategies.

</details>


### [96] [QueST: Incentivizing LLMs to Generate Difficult Problems](https://arxiv.org/abs/2510.17715)
*Hanxu Hu,Xingxing Zhang,Jannis Vamvas,Rico Sennrich,Furu Wei*

Main category: cs.CL

TL;DR: QueST is a framework that generates challenging coding problems through difficulty-aware graph sampling and rejection fine-tuning, enabling significant performance improvements in LLMs for competitive coding tasks.


<details>
  <summary>Details</summary>
Motivation: Current LLMs are limited by scarce human-labeled coding datasets and lack large-scale challenging problem data, with existing datasets containing only thousands to tens of thousands of problems.

Method: Combines difficulty-aware graph sampling and difficulty-aware rejection fine-tuning to optimize specialized generators for creating challenging coding problems, then uses synthetic problems for distillation from strong teacher models or reinforcement learning.

Result: Generated 100K difficult problems that helped Qwen3-8B-base surpass original Qwen3-8B on LiveCodeBench, and with additional 112K examples (28K human problems + synthetic solutions), the 8B model matched DeepSeek-R1-671B performance.

Conclusion: QueST provides an effective and scalable approach to advance competitive coding and reasoning capabilities in LLMs by generating complex problems.

Abstract: Large Language Models have achieved strong performance on reasoning tasks,
solving competition-level coding and math problems. However, their scalability
is limited by human-labeled datasets and the lack of large-scale, challenging
coding problem training data. Existing competitive coding datasets contain only
thousands to tens of thousands of problems. Previous synthetic data generation
methods rely on either augmenting existing instruction datasets or selecting
challenging problems from human-labeled data. In this paper, we propose QueST,
a novel framework which combines difficulty-aware graph sampling and
difficulty-aware rejection fine-tuning that directly optimizes specialized
generators to create challenging coding problems. Our trained generators
demonstrate superior capability compared to even GPT-4o at creating challenging
problems that benefit downstream performance. We leverage QueST to generate
large-scale synthetic coding problems, which we then use to distill from strong
teacher models with long chain-of-thought or to conduct reinforcement learning
for smaller models, proving effective in both scenarios. Our distillation
experiments demonstrate significant performance gains. Specifically, after
fine-tuning Qwen3-8B-base on 100K difficult problems generated by QueST, we
surpass the performance of the original Qwen3-8B on LiveCodeBench. With an
additional 112K examples (i.e., 28K human-written problems paired with multiple
synthetic solutions), our 8B model matches the performance of the much larger
DeepSeek-R1-671B. These findings indicate that generating complex problems via
QueST offers an effective and scalable approach to advancing the frontiers of
competitive coding and reasoning for large language models.

</details>


### [97] [PANER: A Paraphrase-Augmented Framework for Low-Resource Named Entity Recognition](https://arxiv.org/abs/2510.17720)
*Nanda Kumar Rengarajan,Jun Yan,Chun Wang*

Main category: cs.CL

TL;DR: A lightweight few-shot NER framework that uses simplified instruction tuning and strategic data augmentation to achieve strong performance with limited training data.


<details>
  <summary>Details</summary>
Motivation: NER requires substantial annotated data, which is challenging in low-resource scenarios. Existing zero-shot and instruction-tuned approaches often fail to generalize to domain-specific entities and don't effectively utilize limited available data.

Method: Two key innovations: (1) new instruction tuning template with simplified output format leveraging large context windows of LLMs, (2) strategic data augmentation that preserves entity information while paraphrasing surrounding context.

Result: Achieves performance comparable to state-of-the-art models on few-shot and zero-shot tasks, with average F1 score of 80.1 on CrossNER datasets. Models show consistent improvements of up to 17 F1 points over baselines.

Conclusion: Offers a promising solution for groups with limited NER training data and compute power through effective few-shot learning approach.

Abstract: Named Entity Recognition (NER) is a critical task that requires substantial
annotated data, making it challenging in low-resource scenarios where label
acquisition is expensive. While zero-shot and instruction-tuned approaches have
made progress, they often fail to generalize to domain-specific entities and do
not effectively utilize limited available data. We present a lightweight
few-shot NER framework that addresses these challenges through two key
innovations: (1) a new instruction tuning template with a simplified output
format that combines principles from prior IT approaches to leverage the large
context window of recent state-of-the-art LLMs; (2) introducing a strategic
data augmentation technique that preserves entity information while
paraphrasing the surrounding context, thereby expanding our training data
without compromising semantic relationships. Experiments on benchmark datasets
show that our method achieves performance comparable to state-of-the-art models
on few-shot and zero-shot tasks, with our few-shot approach attaining an
average F1 score of 80.1 on the CrossNER datasets. Models trained with our
paraphrasing approach show consistent improvements in F1 scores of up to 17
points over baseline versions, offering a promising solution for groups with
limited NER training data and compute power.

</details>


### [98] [AcademicEval: Live Long-Context LLM Benchmark](https://arxiv.org/abs/2510.17725)
*Haozhen Zhang,Tao Feng,Pengrui Han,Jiaxuan You*

Main category: cs.CL

TL;DR: AcademicEval is a live benchmark using arXiv papers to evaluate LLMs on long-context academic writing tasks (Title, Abstract, Introduction, Related Work) without manual labeling, addressing label leakage issues in current benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current long-context LLM benchmarks have limitations: rigid context lengths, labor-intensive annotation, and label leakage problems during LLM training.

Method: Uses arXiv papers to create academic writing tasks with long-context inputs, integrates expert-curated few-shot demonstrations from a co-author graph, and implements live evaluation to prevent label leakage.

Result: LLMs perform poorly on tasks requiring hierarchical abstraction and struggle with long few-shot demonstrations, highlighting the benchmark's challenge.

Conclusion: AcademicEval provides insights for enhancing LLMs' long-context modeling capabilities and serves as an effective benchmark for evaluating long-context generation tasks.

Abstract: Large Language Models (LLMs) have recently achieved remarkable performance in
long-context understanding. However, current long-context LLM benchmarks are
limited by rigid context length, labor-intensive annotation, and the pressing
challenge of label leakage issues during LLM training. Therefore, we propose
\textsc{AcademicEval}, a live benchmark for evaluating LLMs over long-context
generation tasks. \textsc{AcademicEval} adopts papers on arXiv to introduce
several academic writing tasks with long-context inputs, \textit{i.e.},
\textsc{Title}, \textsc{Abstract}, \textsc{Introduction}, and \textsc{Related
Work}, which cover a wide range of abstraction levels and require no manual
labeling. Moreover, \textsc{AcademicEval} integrates high-quality and
expert-curated few-shot demonstrations from a collected co-author graph to
enable flexible context length. Especially, \textsc{AcademicEval} features an
efficient live evaluation, ensuring no label leakage. We conduct a holistic
evaluation on \textsc{AcademicEval}, and the results illustrate that LLMs
perform poorly on tasks with hierarchical abstraction levels and tend to
struggle with long few-shot demonstrations, highlighting the challenge of our
benchmark. Through experimental analysis, we also reveal some insights for
enhancing LLMs' long-context modeling capabilities. Code is available at
https://github.com/ulab-uiuc/AcademicEval

</details>


### [99] [Train for Truth, Keep the Skills: Binary Retrieval-Augmented Reward Mitigates Hallucinations](https://arxiv.org/abs/2510.17733)
*Tong Chen,Akari Asai,Luke Zettlemoyer,Hannaneh Hajishirzi,Faeze Brahman*

Main category: cs.CL

TL;DR: A binary retrieval-augmented reward (RAR) method using online reinforcement learning reduces hallucinations in language models without degrading performance on other tasks.


<details>
  <summary>Details</summary>
Motivation: Language models generate factually incorrect information (extrinsic hallucinations) unsupported by training data, and existing mitigation approaches often degrade performance on open-ended generation and downstream tasks.

Method: Online reinforcement learning with a novel binary retrieval-augmented reward (RAR) that assigns reward of 1 only when output is entirely factually correct, and 0 otherwise.

Result: 39.3% reduction in hallucination rates for open-ended generation; 44.4% and 21.7% fewer incorrect answers on PopQA and GPQA respectively; learns calibrated abstention; no performance degradation on instruction following, math, or code.

Conclusion: Binary RAR effectively reduces hallucinations while maintaining model performance, outperforming both supervised training and continuous-reward RL baselines.

Abstract: Language models often generate factually incorrect information unsupported by
their training data, a phenomenon known as extrinsic hallucination. Existing
mitigation approaches often degrade performance on open-ended generation and
downstream tasks, limiting their practical utility. We propose an online
reinforcement learning method using a novel binary retrieval-augmented reward
(RAR) to address this tradeoff. Unlike continuous reward schemes, our approach
assigns a reward of one only when the model's output is entirely factually
correct, and zero otherwise. We evaluate our method on Qwen3 reasoning models
across diverse tasks. For open-ended generation, binary RAR achieves a 39.3%
reduction in hallucination rates, substantially outperforming both supervised
training and continuous-reward RL baselines. In short-form question answering,
the model learns calibrated abstention, strategically outputting "I don't know"
when faced with insufficient parametric knowledge. This yields 44.4% and 21.7%
fewer incorrect answers on PopQA and GPQA, respectively. Crucially, these
factuality gains come without performance degradation on instruction following,
math, or code, whereas continuous-reward RL, despite improving factuality,
induces quality regressions.

</details>


### [100] [Evaluating Medical LLMs by Levels of Autonomy: A Survey Moving from Benchmarks to Applications](https://arxiv.org/abs/2510.17764)
*Xiao Ye,Jacob Dineen,Zhaonan Li,Zhikun Xu,Weiyu Chen,Shijie Lu,Yuxi Huang,Ming Shen,Phu Tran,Ji-Eun Irene Yum,Muhammad Ali Khan,Muhammad Umar Afzal,Irbaz Bin Riaz,Ben Zhou*

Main category: cs.CL

TL;DR: This survey reframes medical LLM evaluation through a levels-of-autonomy framework (L0-L3) to address the gap between benchmark scores and safe clinical implementation, proposing a risk-aware evaluation blueprint.


<details>
  <summary>Details</summary>
Motivation: Current medical LLMs achieve strong benchmark scores but fail to ensure safe and reliable performance in real clinical workflows, necessitating a new evaluation approach that considers autonomy levels and associated risks.

Method: The survey introduces a levels-of-autonomy framework (L0-L3) spanning informational tools to supervised agents, aligns existing benchmarks with permitted actions and risks at each level, and proposes a level-conditioned evaluation blueprint.

Result: The framework makes evaluation targets explicit and provides a structured approach for selecting metrics, assembling evidence, and reporting claims that link evaluation to clinical oversight.

Conclusion: By centering autonomy levels, this approach moves medical LLM evaluation beyond score-based claims toward credible, risk-aware evidence suitable for real clinical use.

Abstract: Medical Large language models achieve strong scores on standard benchmarks;
however, the transfer of those results to safe and reliable performance in
clinical workflows remains a challenge. This survey reframes evaluation through
a levels-of-autonomy lens (L0-L3), spanning informational tools, information
transformation and aggregation, decision support, and supervised agents. We
align existing benchmarks and metrics with the actions permitted at each level
and their associated risks, making the evaluation targets explicit. This
motivates a level-conditioned blueprint for selecting metrics, assembling
evidence, and reporting claims, alongside directions that link evaluation to
oversight. By centering autonomy, the survey moves the field beyond score-based
claims toward credible, risk-aware evidence for real clinical use.

</details>


### [101] [Foundational Automatic Evaluators: Scaling Multi-Task Generative Evaluator Training for Reasoning-Centric Domains](https://arxiv.org/abs/2510.17793)
*Austin Xu,Xuan-Phi Nguyen,Yilun Zhou,Chien-Sheng Wu,Caiming Xiong,Shafiq Joty*

Main category: cs.CL

TL;DR: FARE is a family of foundational automatic reasoning evaluators trained on 2.5M samples across multiple evaluation tasks, achieving state-of-the-art performance through data scaling and iterative SFT rather than complex RL methods.


<details>
  <summary>Details</summary>
Motivation: To address the demand for scalable evaluation during training and test-time by focusing on data scaling rather than complex methodology like RL, which has been the recent focus.

Method: Curated 2.5M samples across five evaluation tasks, trained FARE models (8B and 20B parameters) using simple iterative rejection-sampling supervised finetuning (SFT).

Result: FARE-8B challenges larger specialized RL-trained evaluators; FARE-20B sets new standard for open-source evaluators, surpassing specialized 70B+ evaluators. Achieves near-oracle performance on MATH as rerankers, improves RL-trained model performance by 14.1%, and FARE-Code outperforms gpt-oss-20B by 65% on test-case quality evaluation.

Conclusion: Data scaling with simple SFT methods can produce highly effective evaluators that outperform more complex approaches, demonstrating the importance of large-scale data curation for building foundational automatic reasoning evaluators.

Abstract: Finetuning specialized generative evaluators has emerged as a popular
paradigm to meet the increasing demand for scalable evaluation during both
training and test-time. However, recent work has largely focused on applying
new methodology, such as reinforcement learning (RL), to training evaluators,
shying away from large-scale, data-driven development. In this work, we focus
on data scaling, curating a set of 2.5M samples spanning five unique evaluation
tasks (pairwise, step-level, reference-free and reference-based verification,
and single rating) and multiple domains focused on reasoning evaluation. With
our data, we train Foundational Automatic Reasoning Evaluators (FARE), a family
of 8B and 20B (with 3.6B active) parameter evaluators, with a simple iterative
rejection-sampling supervised finetuning (SFT) approach. FARE-8B challenges
larger specialized RL-trained evaluators and FARE-20B sets the new standard for
open-source evaluators, surpassing specialized 70B+ evaluators. Beyond static
benchmarks, we evaluate FARE in real-world tasks: As inference-time rerankers,
FARE-20B achieves near-oracle performance on MATH. As verifiers in RL training,
FARE improves the downstream RL-trained model performance by up to 14.1% vs.
string-matching verifiers. When initialized from FARE, a continually-finetuned
FARE-Code outperforms gpt-oss-20B by 65% on evaluating test-case quality.

</details>


### [102] [Executable Knowledge Graphs for Replicating AI Research](https://arxiv.org/abs/2510.17795)
*Yujie Luo,Zhuoyun Yu,Xuehai Wang,Yuqi Zhu,Ningyu Zhang,Lanning Wei,Lun Du,Da Zheng,Huajun Chen*

Main category: cs.CL

TL;DR: Proposes Executable Knowledge Graphs (xKG) to improve AI research replication by integrating technical insights, code snippets, and domain knowledge from scientific papers, showing 10.9% performance gains.


<details>
  <summary>Details</summary>
Motivation: Existing approaches struggle with generating executable code due to insufficient background knowledge, limitations of RAG methods in capturing latent technical details, and lack of structured knowledge representations for multi-granular retrieval and reuse.

Method: Developed xKG - a modular and pluggable knowledge base that automatically integrates technical insights, code snippets, and domain-specific knowledge extracted from scientific literature.

Result: When integrated into three agent frameworks with two different LLMs, xKG showed substantial performance gains (10.9% with o3-mini) on PaperBench.

Conclusion: xKG is an effective general and extensible solution for automated AI research replication, demonstrating significant improvements over existing approaches.

Abstract: Replicating AI research is a crucial yet challenging task for large language
model (LLM) agents. Existing approaches often struggle to generate executable
code, primarily due to insufficient background knowledge and the limitations of
retrieval-augmented generation (RAG) methods, which fail to capture latent
technical details hidden in referenced papers. Furthermore, previous approaches
tend to overlook valuable implementation-level code signals and lack structured
knowledge representations that support multi-granular retrieval and reuse. To
overcome these challenges, we propose Executable Knowledge Graphs (xKG), a
modular and pluggable knowledge base that automatically integrates technical
insights, code snippets, and domain-specific knowledge extracted from
scientific literature. When integrated into three agent frameworks with two
different LLMs, xKG shows substantial performance gains (10.9% with o3-mini) on
PaperBench, demonstrating its effectiveness as a general and extensible
solution for automated AI research replication. Code will released at
https://github.com/zjunlp/xKG.

</details>


### [103] [Enterprise Deep Research: Steerable Multi-Agent Deep Research for Enterprise Analytics](https://arxiv.org/abs/2510.17797)
*Akshara Prabhakar,Roshan Ram,Zixiang Chen,Silvio Savarese,Frank Wang,Caiming Xiong,Huan Wang,Weiran Yao*

Main category: cs.CL

TL;DR: EDR is a multi-agent system for enterprise data analysis that outperforms state-of-the-art systems on open-ended benchmarks without human steering.


<details>
  <summary>Details</summary>
Motivation: Enterprises face challenges in transforming unstructured data into actionable insights, with existing autonomous agents struggling with domain-specific nuances, intent alignment, and enterprise integration.

Method: Multi-agent system with: Master Planning Agent for query decomposition, 4 specialized search agents, MCP-based tool ecosystem, Visualization Agent, and reflection mechanism with optional human-in-the-loop guidance.

Result: Outperforms state-of-the-art agentic systems on DeepResearch Bench and DeepConsult benchmarks without human steering. Validated on internal datasets with automated report generation and real-time streaming capabilities.

Conclusion: EDR enables automated enterprise data analysis and releases framework with benchmark trajectories to advance multi-agent reasoning research.

Abstract: As information grows exponentially, enterprises face increasing pressure to
transform unstructured data into coherent, actionable insights. While
autonomous agents show promise, they often struggle with domain-specific
nuances, intent alignment, and enterprise integration. We present Enterprise
Deep Research (EDR), a multi-agent system that integrates (1) a Master Planning
Agent for adaptive query decomposition, (2) four specialized search agents
(General, Academic, GitHub, LinkedIn), (3) an extensible MCP-based tool
ecosystem supporting NL2SQL, file analysis, and enterprise workflows, (4) a
Visualization Agent for data-driven insights, and (5) a reflection mechanism
that detects knowledge gaps and updates research direction with optional
human-in-the-loop steering guidance. These components enable automated report
generation, real-time streaming, and seamless enterprise deployment, as
validated on internal datasets. On open-ended benchmarks including DeepResearch
Bench and DeepConsult, EDR outperforms state-of-the-art agentic systems without
any human steering. We release the EDR framework and benchmark trajectories to
advance research on multi-agent reasoning applications.
  Code at https://github.com/SalesforceAIResearch/enterprise-deep-research and
Dataset at https://huggingface.co/datasets/Salesforce/EDR-200

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [104] [ESCA: Contextualizing Embodied Agents via Scene-Graph Generation](https://arxiv.org/abs/2510.15963)
*Jiani Huang,Amish Sethi,Matthew Kuo,Mayank Keoliya,Neelay Velingker,JungHo Jung,Ser-Nam Lim,Ziyang Li,Mayur Naik*

Main category: cs.CV

TL;DR: ESCAA is a framework that improves embodied agents through structured spatial-temporal understanding using SGClip, a CLIP-based model for generating scene graphs without human annotations.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs lack fine-grained alignment between visual content and textual semantics, limiting their effectiveness as embodied agents.

Method: Proposes ESCA framework with SGClip model trained on 87K+ videos using neurosymbolic learning pipeline with model-driven self-supervision from video-caption pairs.

Result: SGClip excels in scene graph generation and action localization benchmarks. ESCA consistently improves MLLMs, achieving state-of-the-art performance in embodied environments and reducing perception errors.

Conclusion: ESCAA enables better spatial-temporal understanding for embodied agents, allowing open-source models to surpass proprietary baselines without requiring human-labeled annotations.

Abstract: Multi-modal large language models (MLLMs) are making rapid progress toward
general-purpose embodied agents. However, current training pipelines primarily
rely on high-level vision-sound-text pairs and lack fine-grained, structured
alignment between pixel-level visual content and textual semantics. To overcome
this challenge, we propose ESCA, a new framework for contextualizing embodied
agents through structured spatial-temporal understanding. At its core is
SGClip, a novel CLIP-based, open-domain, and promptable model for generating
scene graphs. SGClip is trained on 87K+ open-domain videos via a neurosymbolic
learning pipeline, which harnesses model-driven self-supervision from
video-caption pairs and structured reasoning, thereby eliminating the need for
human-labeled scene graph annotations. We demonstrate that SGClip supports both
prompt-based inference and task-specific fine-tuning, excelling in scene graph
generation and action localization benchmarks. ESCA with SGClip consistently
improves both open-source and commercial MLLMs, achieving state-of-the-art
performance across two embodied environments. Notably, it significantly reduces
agent perception errors and enables open-source models to surpass proprietary
baselines.

</details>


### [105] [CrossRay3D: Geometry and Distribution Guidance for Efficient Multimodal 3D Detection](https://arxiv.org/abs/2510.15991)
*Huiming Yang*

Main category: cs.CV

TL;DR: CrossRay3D is a sparse multi-modal 3D detector that improves token representation quality through Ray-Aware Supervision and Class-Balanced Supervision, achieving state-of-the-art performance on nuScenes while being computationally efficient.


<details>
  <summary>Details</summary>
Motivation: Existing sparse detectors overlook token representation quality, leading to sub-optimal foreground quality and limited performance. The authors identify geometric structure preservation and class distribution as key factors for improving sparse detector performance.

Method: Proposes Sparse Selector (SS) with two core modules: Ray-Aware Supervision (RAS) to preserve geometric information during training, and Class-Balanced Supervision to adaptively reweight class semantics and retain small object tokens. Also introduces Ray Positional Encoding to address LiDAR-image modality distribution differences.

Result: Achieves state-of-the-art performance on nuScenes benchmark with 72.4 mAP and 74.7 NDS, running 1.84 faster than other leading methods. Demonstrates strong robustness even with partial or complete missing LiDAR or camera data.

Conclusion: CrossRay3D outperforms other sparse multi-modal detectors in token representation and achieves superior performance while being computationally efficient and robust to modality missing scenarios.

Abstract: The sparse cross-modality detector offers more advantages than its
counterpart, the Bird's-Eye-View (BEV) detector, particularly in terms of
adaptability for downstream tasks and computational cost savings. However,
existing sparse detectors overlook the quality of token representation, leaving
it with a sub-optimal foreground quality and limited performance. In this
paper, we identify that the geometric structure preserved and the class
distribution are the key to improving the performance of the sparse detector,
and propose a Sparse Selector (SS). The core module of SS is Ray-Aware
Supervision (RAS), which preserves rich geometric information during the
training stage, and Class-Balanced Supervision, which adaptively reweights the
salience of class semantics, ensuring that tokens associated with small objects
are retained during token sampling. Thereby, outperforming other sparse
multi-modal detectors in the representation of tokens. Additionally, we design
Ray Positional Encoding (Ray PE) to address the distribution differences
between the LiDAR modality and the image. Finally, we integrate the
aforementioned module into an end-to-end sparse multi-modality detector, dubbed
CrossRay3D. Experiments show that, on the challenging nuScenes benchmark,
CrossRay3D achieves state-of-the-art performance with 72.4 mAP and 74.7 NDS,
while running 1.84 faster than other leading methods. Moreover, CrossRay3D
demonstrates strong robustness even in scenarios where LiDAR or camera data are
partially or entirely missing.

</details>


### [106] [InfraGPT Smart Infrastructure: An End-to-End VLM-Based Framework for Detecting and Managing Urban Defects](https://arxiv.org/abs/2510.16017)
*Ibrahim Sheikh Mohamed,Abdullah Yahya Abdullah Omaisan*

Main category: cs.CV

TL;DR: A pipeline using CCTV streams with YOLO detectors and vision language models for automated defect detection and structured repair planning in smart cities.


<details>
  <summary>Details</summary>
Motivation: Manual infrastructure inspection is costly and hazardous, while existing automatic systems lack comprehensive defect coverage and structured outputs for maintenance crews.

Method: Uses YOLO object detectors for multi-defect detection and segmentation, then passes detections to vision language models for scene-aware summarization and structured JSON output generation.

Result: System accurately identifies diverse defects and produces coherent structured summaries with incident descriptions, recommended tools, dimensions, repair plans, and urgent alerts.

Conclusion: The approach shows promise but faces challenges in scaling to city-wide deployments, requiring further development for practical implementation.

Abstract: Infrastructure in smart cities is increasingly monitored by networks of
closed circuit television (CCTV) cameras. Roads, bridges and tunnels develop
cracks, potholes, and fluid leaks that threaten public safety and require
timely repair. Manual inspection is costly and hazardous, and existing
automatic systems typically address individual defect types or provide
unstructured outputs that cannot directly guide maintenance crews. This paper
proposes a comprehensive pipeline that leverages street CCTV streams for multi
defect detection and segmentation using the YOLO family of object detectors and
passes the detections to a vision language model (VLM) for scene aware
summarization. The VLM generates a structured action plan in JSON format that
includes incident descriptions, recommended tools, dimensions, repair plans,
and urgent alerts. We review literature on pothole, crack and leak detection,
highlight recent advances in large vision language models such as QwenVL and
LLaVA, and describe the design of our early prototype. Experimental evaluation
on public datasets and captured CCTV clips demonstrates that the system
accurately identifies diverse defects and produces coherent summaries. We
conclude by discussing challenges and directions for scaling the system to city
wide deployments.

</details>


### [107] [IAD-GPT: Advancing Visual Knowledge in Multimodal Large Language Model for Industrial Anomaly Detection](https://arxiv.org/abs/2510.16036)
*Zewen Li,Zitong Yu,Qilang Ye,Weicheng Xie,Wei Zhuo,Linlin Shen*

Main category: cs.CV

TL;DR: IAD-GPT is a novel MLLM-based paradigm for Industrial Anomaly Detection that combines text semantics with image-level and pixel-level information, achieving state-of-the-art performance on anomaly detection and segmentation tasks.


<details>
  <summary>Details</summary>
Motivation: Traditional IAD methods lack multi-turn dialogues and detailed descriptions, while existing large model approaches haven't fully stimulated anomaly detection capabilities. There's a need to leverage MLLMs' robust causal reasoning for detecting defective objects in industrial settings.

Method: Uses Abnormal Prompt Generator to create detailed anomaly prompts, Text-Guided Enhancer for visual grounding by interacting image features with normal/abnormal text prompts, and Multi-Mask Fusion to incorporate mask as expert knowledge for pixel-level anomaly perception.

Result: Extensive experiments on MVTec-AD and VisA datasets demonstrate state-of-the-art performance on self-supervised and few-shot anomaly detection and segmentation tasks.

Conclusion: IAD-GPT successfully combines text semantics with visual information to enhance MLLMs' anomaly detection capabilities, providing a powerful framework for industrial anomaly detection with detailed descriptions and multi-turn dialogue capabilities.

Abstract: The robust causal capability of Multimodal Large Language Models (MLLMs) hold
the potential of detecting defective objects in Industrial Anomaly Detection
(IAD). However, most traditional IAD methods lack the ability to provide
multi-turn human-machine dialogues and detailed descriptions, such as the color
of objects, the shape of an anomaly, or specific types of anomalies. At the
same time, methods based on large pre-trained models have not fully stimulated
the ability of large models in anomaly detection tasks. In this paper, we
explore the combination of rich text semantics with both image-level and
pixel-level information from images and propose IAD-GPT, a novel paradigm based
on MLLMs for IAD. We employ Abnormal Prompt Generator (APG) to generate
detailed anomaly prompts for specific objects. These specific prompts from the
large language model (LLM) are used to activate the detection and segmentation
functions of the pre-trained visual-language model (i.e., CLIP). To enhance the
visual grounding ability of MLLMs, we propose Text-Guided Enhancer, wherein
image features interact with normal and abnormal text prompts to dynamically
select enhancement pathways, which enables language models to focus on specific
aspects of visual data, enhancing their ability to accurately interpret and
respond to anomalies within images. Moreover, we design a Multi-Mask Fusion
module to incorporate mask as expert knowledge, which enhances the LLM's
perception of pixel-level anomalies. Extensive experiments on MVTec-AD and VisA
datasets demonstrate our state-of-the-art performance on self-supervised and
few-shot anomaly detection and segmentation tasks, such as MVTec-AD and VisA
datasets. The codes are available at
\href{https://github.com/LiZeWen1225/IAD-GPT}{https://github.com/LiZeWen1225/IAD-GPT}.

</details>


### [108] [Effect of Reporting Mode and Clinical Experience on Radiologists' Gaze and Image Analysis Behavior in Chest Radiography](https://arxiv.org/abs/2510.16070)
*Mahta Khoobi,Marc Sebastian von der Stueck,Felix Barajas Ordonez,Anca-Maria Iancu,Eric Corban,Julia Nowak,Aleksandar Kargaliev,Valeria Perelygina,Anna-Sophie Schott,Daniel Pinto dos Santos,Christiane Kuhl,Daniel Truhn,Sven Nebelung,Robert Siepmann*

Main category: cs.CV

TL;DR: AI-assisted structured reporting improves diagnostic accuracy and efficiency compared to free-text and structured reporting alone, while guiding visual attention toward images.


<details>
  <summary>Details</summary>
Motivation: To evaluate how structured reporting and AI assistance impact radiologists' image analysis behavior, diagnostic accuracy, efficiency, and user experience.

Method: Prospective study with 8 readers (4 novice, 4 non-novice) analyzing 35 chest radiographs each using three reporting modes: free-text, structured reporting, and AI-assisted structured reporting, with eye-tracking and timing metrics.

Result: AI-SR achieved highest diagnostic accuracy (=0.71 vs 0.58-0.60), fastest reporting times (25s vs 37-88s), reduced eye movements, and was the preferred mode. SR guided visual attention toward images.

Conclusion: Structured reporting improves efficiency by guiding visual attention, and AI-prefilled structured reporting further enhances diagnostic accuracy and user satisfaction.

Abstract: Structured reporting (SR) and artificial intelligence (AI) may transform how
radiologists interact with imaging studies. This prospective study (July to
December 2024) evaluated the impact of three reporting modes: free-text (FT),
structured reporting (SR), and AI-assisted structured reporting (AI-SR), on
image analysis behavior, diagnostic accuracy, efficiency, and user experience.
Four novice and four non-novice readers (radiologists and medical students)
each analyzed 35 bedside chest radiographs per session using a customized
viewer and an eye-tracking system. Outcomes included diagnostic accuracy
(compared with expert consensus using Cohen's $\kappa$), reporting time per
radiograph, eye-tracking metrics, and questionnaire-based user experience.
Statistical analysis used generalized linear mixed models with Bonferroni
post-hoc tests with a significance level of ($P \le .01$). Diagnostic accuracy
was similar in FT ($\kappa = 0.58$) and SR ($\kappa = 0.60$) but higher in
AI-SR ($\kappa = 0.71$, $P < .001$). Reporting times decreased from $88 \pm 38$
s (FT) to $37 \pm 18$ s (SR) and $25 \pm 9$ s (AI-SR) ($P < .001$). Saccade
counts for the radiograph field ($205 \pm 135$ (FT), $123 \pm 88$ (SR), $97 \pm
58$ (AI-SR)) and total fixation duration for the report field ($11 \pm 5$ s
(FT), $5 \pm 3$ s (SR), $4 \pm 1$ s (AI-SR)) were lower with SR and AI-SR ($P <
.001$ each). Novice readers shifted gaze towards the radiograph in SR, while
non-novice readers maintained their focus on the radiograph. AI-SR was the
preferred mode. In conclusion, SR improves efficiency by guiding visual
attention toward the image, and AI-prefilled SR further enhances diagnostic
accuracy and user satisfaction.

</details>


### [109] [Data-Driven Analysis of Intersectional Bias in Image Classification: A Framework with Bias-Weighted Augmentation](https://arxiv.org/abs/2510.16072)
*Farjana Yesmin*

Main category: cs.CV

TL;DR: A framework for analyzing and mitigating intersectional biases in image classification through systematic evaluation and adaptive data augmentation.


<details>
  <summary>Details</summary>
Motivation: Machine learning models trained on imbalanced datasets exhibit systematic errors from interactions of multiple attributes like object class and environmental conditions.

Method: Introduces Intersectional Fairness Evaluation Framework (IFEF) with quantitative metrics and interpretability tools, plus Bias-Weighted Augmentation (BWA) that adapts transformation intensities based on subgroup statistics.

Result: On Open Images V7 dataset, BWA improved accuracy for underrepresented class-environment intersections by up to 24 percentage points and reduced fairness metric disparities by 35%, with statistical significance (p < 0.05).

Conclusion: Provides a replicable methodology for analyzing and addressing intersectional biases in image classification systems.

Abstract: Machine learning models trained on imbalanced datasets often exhibit
intersectional biases-systematic errors arising from the interaction of
multiple attributes such as object class and environmental conditions. This
paper presents a data-driven framework for analyzing and mitigating such biases
in image classification. We introduce the Intersectional Fairness Evaluation
Framework (IFEF), which combines quantitative fairness metrics with
interpretability tools to systematically identify bias patterns in model
predictions. Building on this analysis, we propose Bias-Weighted Augmentation
(BWA), a novel data augmentation strategy that adapts transformation
intensities based on subgroup distribution statistics. Experiments on the Open
Images V7 dataset with five object classes demonstrate that BWA improves
accuracy for underrepresented class-environment intersections by up to 24
percentage points while reducing fairness metric disparities by 35%.
Statistical analysis across multiple independent runs confirms the significance
of improvements (p < 0.05). Our methodology provides a replicable approach for
analyzing and addressing intersectional biases in image classification systems.

</details>


### [110] [Differentiable, Bit-shifting, and Scalable Quantization without training neural network from scratch](https://arxiv.org/abs/2510.16088)
*Zia Badar*

Main category: cs.CV

TL;DR: This paper introduces a differentiable quantization method for neural networks that provides proof of convergence and supports n-bit quantization, achieving near full-precision accuracy with only 15 training epochs.


<details>
  <summary>Details</summary>
Motivation: Previous quantization methods were non-differentiable with manually set derivatives, lacked convergence proofs, and struggled with activation quantization alongside weight quantization. This work addresses these limitations.

Method: The proposed approach uses a differentiable quantization function that can scale to n-bit quantization and supports logarithmic quantization of values in the form 2^n. It enables simultaneous weight and activation quantization.

Result: On ImageNet with ResNet18, weight-only quantization achieved less than 1% accuracy drop compared to full precision. Both weight and activation quantization achieved state-of-the-art accuracy in 15 training epochs, with slightly higher CPU instructions but no high-precision multiplication requirements.

Conclusion: The method provides a differentiable quantization approach with proven convergence that achieves competitive accuracy with minimal training epochs and eliminates the need for high-precision multiplication during inference.

Abstract: Quantization of neural networks provides benefits of inference in less
compute and memory requirements. Previous work in quantization lack two
important aspects which this work provides. First almost all previous work in
quantization used a non-differentiable approach and for learning; the
derivative is usually set manually in backpropogation which make the learning
ability of algorithm questionable, our approach is not just differentiable, we
also provide proof of convergence of our approach to the optimal neural
network. Second previous work in shift/logrithmic quantization either have
avoided activation quantization along with weight quantization or achieved less
accuracy. Learning logrithmic quantize values of form $2^n$ requires the
quantization function can scale to more than 1 bit quantization which is
another benifit of our quantization that it provides $n$ bits quantization as
well. Our approach when tested with image classification task using imagenet
dataset, resnet18 and weight quantization only achieves less than 1 percent
accuracy compared to full precision accuracy while taking only 15 epochs to
train using shift bit quantization and achieves comparable to SOTA approaches
accuracy in both weight and activation quantization using shift bit
quantization in 15 training epochs with slightly higher(only higher cpu
instructions) inference cost compared to 1 bit quantization(without logrithmic
quantization) and not requiring any higher precision multiplication.

</details>


### [111] [StripRFNet: A Strip Receptive Field and Shape-Aware Network for Road Damage Detection](https://arxiv.org/abs/2510.16115)
*Jianhan Lin,Yuchu Qin,Shuai Gao,Yikang Rui,Jie Liu,Yanjie Lv*

Main category: cs.CV

TL;DR: StripRFNet is a novel deep neural network for road damage detection that addresses challenges in detecting diverse damage shapes, slender cracks, and small-scale damages through three specialized modules, achieving state-of-the-art performance on the RDD2022 benchmark.


<details>
  <summary>Details</summary>
Motivation: Road surface damage threatens traffic safety and hinders sustainable urban development, but accurate detection is challenging due to diverse damage shapes, difficulty capturing slender cracks, and high error rates in small-scale damage recognition.

Method: StripRFNet comprises three modules: (1) Shape Perception Module with large separable kernel attention for shape discrimination, (2) Strip Receptive Field Module using large strip convolutions and pooling for slender cracks, (3) Small-Scale Enhancement Module with high-resolution features and dynamic upsampling for small-object detection.

Result: On RDD2022 benchmark: Chinese subset improved F1-score, mAP50, and mAP50:95 by 4.4, 2.9, and 3.4 percentage points over baseline; full dataset achieved highest F1-score of 80.33% compared to CRDDC'2022 participants and ORDDC'2024 Phase 2 results, while maintaining competitive inference speed.

Conclusion: StripRFNet achieves state-of-the-art accuracy and real-time efficiency, offering a promising tool for intelligent road maintenance and sustainable infrastructure management.

Abstract: Well-maintained road networks are crucial for achieving Sustainable
Development Goal (SDG) 11. Road surface damage not only threatens traffic
safety but also hinders sustainable urban development. Accurate detection,
however, remains challenging due to the diverse shapes of damages, the
difficulty of capturing slender cracks with high aspect ratios, and the high
error rates in small-scale damage recognition. To address these issues, we
propose StripRFNet, a novel deep neural network comprising three modules: (1) a
Shape Perception Module (SPM) that enhances shape discrimination via large
separable kernel attention (LSKA) in multi-scale feature aggregation; (2) a
Strip Receptive Field Module (SRFM) that employs large strip convolutions and
pooling to capture features of slender cracks; and (3) a Small-Scale
Enhancement Module (SSEM) that leverages a high-resolution P2 feature map, a
dedicated detection head, and dynamic upsampling to improve small-object
detection. Experiments on the RDD2022 benchmark show that StripRFNet surpasses
existing methods. On the Chinese subset, it improves F1-score, mAP50, and
mAP50:95 by 4.4, 2.9, and 3.4 percentage points over the baseline,
respectively. On the full dataset, it achieves the highest F1-score of 80.33%
compared with CRDDC'2022 participants and ORDDC'2024 Phase 2 results, while
maintaining competitive inference speed. These results demonstrate that
StripRFNet achieves state-of-the-art accuracy and real-time efficiency,
offering a promising tool for intelligent road maintenance and sustainable
infrastructure management.

</details>


### [112] [ObjectTransforms for Uncertainty Quantification and Reduction in Vision-Based Perception for Autonomous Vehicles](https://arxiv.org/abs/2510.16118)
*Nishad Sahu,Shounak Sural,Aditya Satish Patil,Ragunathan,Rajkumar*

Main category: cs.CV

TL;DR: ObjectTransforms is a technique that uses object-specific transformations to quantify and reduce uncertainty in vision-based object detection. It applies color space perturbations and diffusion-generated pedestrians during training, and uses detection score variance to measure uncertainty at inference.


<details>
  <summary>Details</summary>
Motivation: Vision-based object detectors are vulnerable to uncertainty from data bias and distribution shifts, which is critical for safety in autonomous driving. There's a need for reliable uncertainty quantification and reduction methods.

Method: ObjectTransforms applies color space perturbations on individual objects during training for robustness. Uses diffusion models to generate diverse pedestrian instances. At inference, applies object perturbations and uses detection score variance to quantify uncertainty in real-time.

Result: Experiments with YOLOv8 on NuImages dataset show accuracy improvements and uncertainty reduction across all object classes. Method predicts higher uncertainty for false positives than true positives, enabling better filtering of false detections.

Conclusion: ObjectTransforms is an effective lightweight mechanism for reducing uncertainty during training and quantifying it during inference, improving reliability of vision-based perception for autonomous driving.

Abstract: Reliable perception is fundamental for safety critical decision making in
autonomous driving. Yet, vision based object detector neural networks remain
vulnerable to uncertainty arising from issues such as data bias and
distributional shifts. In this paper, we introduce ObjectTransforms, a
technique for quantifying and reducing uncertainty in vision based object
detection through object specific transformations at both training and
inference times. At training time, ObjectTransforms perform color space
perturbations on individual objects, improving robustness to lighting and color
variations. ObjectTransforms also uses diffusion models to generate realistic,
diverse pedestrian instances. At inference time, object perturbations are
applied to detected objects and the variance of detection scores are used to
quantify predictive uncertainty in real time. This uncertainty signal is then
used to filter out false positives and also recover false negatives, improving
the overall precision recall curve. Experiments with YOLOv8 on the NuImages 10K
dataset demonstrate that our method yields notable accuracy improvements and
uncertainty reduction across all object classes during training, while
predicting desirably higher uncertainty values for false positives as compared
to true positives during inference. Our results highlight the potential of
ObjectTransforms as a lightweight yet effective mechanism for reducing and
quantifying uncertainty in vision-based perception during training and
inference respectively.

</details>


### [113] [Aria Gen 2 Pilot Dataset](https://arxiv.org/abs/2510.16134)
*Chen Kong,James Fort,Aria Kang,Jonathan Wittmer,Simon Green,Tianwei Shen,Yipu Zhao,Cheng Peng,Gustavo Solaira,Andrew Berkovich,Nikhil Raina,Vijay Baiyya,Evgeniy Oleinik,Eric Huang,Fan Zhang,Julian Straub,Mark Schwesinger,Luis Pesqueira,Xiaqing Pan,Jakob Julian Engel,Carl Ren,Mingfei Yan,Richard Newcombe*

Main category: cs.CV

TL;DR: A2PD is an incremental multimodal egocentric dataset captured using Aria Gen 2 glasses, featuring daily activities across five scenarios with raw sensor data and perception algorithm outputs.


<details>
  <summary>Details</summary>
Motivation: To provide timely access to state-of-the-art egocentric multimodal data for research, demonstrating the device's perception capabilities across diverse users and conditions.

Method: Used Aria Gen 2 glasses to capture daily activities of primary subject Dia'ane and friends across five scenarios: cleaning, cooking, eating, playing, and outdoor walking.

Result: Created comprehensive dataset with raw sensor data and machine perception algorithm outputs, showing robust performance in perceiving wearer, environment, and interactions.

Conclusion: A2PD is publicly available with open-source tools and usage examples, supporting ongoing research in egocentric multimodal perception.

Abstract: The Aria Gen 2 Pilot Dataset (A2PD) is an egocentric multimodal open dataset
captured using the state-of-the-art Aria Gen 2 glasses. To facilitate timely
access, A2PD is released incrementally with ongoing dataset enhancements. The
initial release features Dia'ane, our primary subject, who records her daily
activities alongside friends, each equipped with Aria Gen 2 glasses. It
encompasses five primary scenarios: cleaning, cooking, eating, playing, and
outdoor walking. In each of the scenarios, we provide comprehensive raw sensor
data and output data from various machine perception algorithms. These data
illustrate the device's ability to perceive the wearer, the surrounding
environment, and interactions between the wearer and the environment, while
maintaining robust performance across diverse users and conditions. The A2PD is
publicly available at projectaria.com, with open-source tools and usage
examples provided in Project Aria Tools.

</details>


### [114] [GuideFlow3D: Optimization-Guided Rectified Flow For Appearance Transfer](https://arxiv.org/abs/2510.16136)
*Sayan Deb Sarkar,Sinisa Stekovic,Vincent Lepetit,Iro Armeni*

Main category: cs.CV

TL;DR: A training-free method for transferring appearance to 3D assets using rectified flow models with periodic guidance, outperforming direct 3D generative approaches.


<details>
  <summary>Details</summary>
Motivation: Current methods fail when geometry between input and appearance objects differs significantly, and direct 3D generative models produce poor results.

Method: Uses pretrained rectified flow models conditioned on image/text with periodic guidance modeled as differentiable loss functions, including part-aware appearance and self-similarity losses.

Result: Successfully transfers texture and geometric details to 3D assets, outperforming baselines both qualitatively and quantitatively. Traditional metrics are unsuitable, so evaluation uses GPT-based ranking and user studies.

Conclusion: The method is general and can be extended to different diffusion models and guidance functions, providing effective appearance transfer for applications in gaming, AR, and digital content creation.

Abstract: Transferring appearance to 3D assets using different representations of the
appearance object - such as images or text - has garnered interest due to its
wide range of applications in industries like gaming, augmented reality, and
digital content creation. However, state-of-the-art methods still fail when the
geometry between the input and appearance objects is significantly different. A
straightforward approach is to directly apply a 3D generative model, but we
show that this ultimately fails to produce appealing results. Instead, we
propose a principled approach inspired by universal guidance. Given a
pretrained rectified flow model conditioned on image or text, our training-free
method interacts with the sampling process by periodically adding guidance.
This guidance can be modeled as a differentiable loss function, and we
experiment with two different types of guidance including part-aware losses for
appearance and self-similarity. Our experiments show that our approach
successfully transfers texture and geometric details to the input 3D asset,
outperforming baselines both qualitatively and quantitatively. We also show
that traditional metrics are not suitable for evaluating the task due to their
inability of focusing on local details and comparing dissimilar inputs, in
absence of ground truth data. We thus evaluate appearance transfer quality with
a GPT-based system objectively ranking outputs, ensuring robust and human-like
assessment, as further confirmed by our user study. Beyond showcased scenarios,
our method is general and could be extended to different types of diffusion
models and guidance functions.

</details>


### [115] [C-arm Guidance: A Self-supervised Approach To Automated Positioning During Stroke Thrombectomy](https://arxiv.org/abs/2510.16145)
*Ahmad Arrabi,Jay hwasung Jung,J Le,A Nguyen,J Reed,E Stahl,Nathan Franssen,Scott Raymond,Safwan Wshah*

Main category: cs.CV

TL;DR: Deep learning framework for automating thrombectomy procedures using self-supervised learning with regression-based pretext tasks to classify skeletal landmarks, showing improved performance over existing methods.


<details>
  <summary>Details</summary>
Motivation: To enhance efficiency and safety of thrombectomy procedures by automating critical aspects through deep learning, addressing the resource-intensive nature of current treatments.

Method: Self-supervised framework that uses regression-based pretext tasks to classify various skeletal landmarks, with positional pretext tasks improving downstream classification performance.

Result: The model outperforms existing methods in both regression and classification tasks, with positional pretext tasks significantly enhancing classification performance.

Conclusion: The framework shows promise for extending toward fully autonomous C-arm control to optimize trajectories from pelvis to head during thrombectomy procedures.

Abstract: Thrombectomy is one of the most effective treatments for ischemic stroke, but
it is resource and personnel-intensive. We propose employing deep learning to
automate critical aspects of thrombectomy, thereby enhancing efficiency and
safety. In this work, we introduce a self-supervised framework that classifies
various skeletal landmarks using a regression-based pretext task. Our
experiments demonstrate that our model outperforms existing methods in both
regression and classification tasks. Notably, our results indicate that the
positional pretext task significantly enhances downstream classification
performance. Future work will focus on extending this framework toward fully
autonomous C-arm control, aiming to optimize trajectories from the pelvis to
the head during stroke thrombectomy procedures. All code used is available at
https://github.com/AhmadArrabi/C_arm_guidance

</details>


### [116] [DuetMatch: Harmonizing Semi-Supervised Brain MRI Segmentation via Decoupled Branch Optimization](https://arxiv.org/abs/2510.16146)
*Thanh-Huy Nguyen,Hoang-Thien Nguyen,Vi Vu,Ba-Thinh Lam,Phat Huynh,Tianyang Wang,Xingjian Li,Ulas Bagci,Min Xu*

Main category: cs.CV

TL;DR: DuetMatch is a dual-branch semi-supervised framework for medical image segmentation that uses asynchronous optimization, where each branch optimizes either encoder or decoder while keeping the other frozen, with additional regularization techniques to handle noisy conditions.


<details>
  <summary>Details</summary>
Motivation: Limited annotated data in medical imaging makes semi-supervised learning appealing, but joint optimization of entire networks in teacher-student frameworks can hinder convergence and stability, especially in challenging scenarios.

Method: Proposes DuetMatch with asynchronous optimization (each branch optimizes encoder or decoder separately), Decoupled Dropout Perturbation for regularization, Pair-wise CutMix Cross-Guidance for model diversity, and Consistency Matching to mitigate noisy pseudo-labels using frozen teacher predictions.

Result: Extensive experiments on brain MRI segmentation datasets (ISLES2022 and BraTS) show DuetMatch consistently outperforms state-of-the-art methods across diverse semi-supervised segmentation scenarios.

Conclusion: DuetMatch demonstrates effectiveness and robustness in semi-supervised medical image segmentation through its novel dual-branch architecture with asynchronous optimization and regularization techniques.

Abstract: The limited availability of annotated data in medical imaging makes
semi-supervised learning increasingly appealing for its ability to learn from
imperfect supervision. Recently, teacher-student frameworks have gained
popularity for their training benefits and robust performance. However, jointly
optimizing the entire network can hinder convergence and stability, especially
in challenging scenarios. To address this for medical image segmentation, we
propose DuetMatch, a novel dual-branch semi-supervised framework with
asynchronous optimization, where each branch optimizes either the encoder or
decoder while keeping the other frozen. To improve consistency under noisy
conditions, we introduce Decoupled Dropout Perturbation, enforcing
regularization across branches. We also design Pair-wise CutMix Cross-Guidance
to enhance model diversity by exchanging pseudo-labels through augmented input
pairs. To mitigate confirmation bias from noisy pseudo-labels, we propose
Consistency Matching, refining labels using stable predictions from frozen
teacher models. Extensive experiments on benchmark brain MRI segmentation
datasets, including ISLES2022 and BraTS, show that DuetMatch consistently
outperforms state-of-the-art methods, demonstrating its effectiveness and
robustness across diverse semi-supervised segmentation scenarios.

</details>


### [117] [Automated C-Arm Positioning via Conformal Landmark Localization](https://arxiv.org/abs/2510.16160)
*Ahmad Arrabi,Jay Hwasung Jung,Jax Luo,Nathan Franssen,Scott Raymond,Safwan Wshah*

Main category: cs.CV

TL;DR: A pipeline for autonomous C-arm navigation to anatomical landmarks using X-ray images, with uncertainty quantification and conformal prediction for reliable deployment.


<details>
  <summary>Details</summary>
Motivation: Manual C-arm positioning in fluoroscopy-guided interventions increases radiation exposure and procedural delays, creating need for automated solutions.

Method: Uses X-ray images from arbitrary starting positions to predict 3D displacement vectors to target landmarks, incorporates aleatoric and epistemic uncertainty, applies conformal prediction for calibration, and combines probabilistic loss with skeletal pose regularization.

Result: Strong localization accuracy across multiple architectures with well-calibrated prediction bounds on synthetic X-ray dataset from DeepDRR.

Conclusion: The pipeline shows potential as a component in safe and reliable autonomous C-arm systems, with code publicly available.

Abstract: Accurate and reliable C-arm positioning is essential for fluoroscopy-guided
interventions. However, clinical workflows rely on manual alignment that
increases radiation exposure and procedural delays. In this work, we present a
pipeline that autonomously navigates the C-arm to predefined anatomical
landmarks utilizing X-ray images. Given an input X-ray image from an arbitrary
starting location on the operating table, the model predicts a 3D displacement
vector toward each target landmark along the body. To ensure reliable
deployment, we capture both aleatoric and epistemic uncertainties in the
model's predictions and further calibrate them using conformal prediction. The
derived prediction regions are interpreted as 3D confidence regions around the
predicted landmark locations. The training framework combines a probabilistic
loss with skeletal pose regularization to encourage anatomically plausible
outputs. We validate our approach on a synthetic X-ray dataset generated from
DeepDRR. Results show not only strong localization accuracy across multiple
architectures but also well-calibrated prediction bounds. These findings
highlight the pipeline's potential as a component in safe and reliable
autonomous C-arm systems. Code is available at
https://github.com/AhmadArrabi/C_arm_guidance_APAH

</details>


### [118] [Cost Savings from Automatic Quality Assessment of Generated Images](https://arxiv.org/abs/2510.16179)
*Xavier Giro-i-Nieto,Nefeli Andreou,Anqi Liang,Manel Baradad,Francesc Moreno-Noguer,Aleix Martinez*

Main category: cs.CV

TL;DR: The paper presents a cost-saving formula for automatic image quality assessment (IQA) pre-filtering in deep generative model workflows, demonstrating 51.61% cost reduction in background inpainting using AutoML.


<details>
  <summary>Details</summary>
Motivation: Manual image quality assessment in production pipelines using generated images is slow and expensive due to low yield of high-quality automatically generated images.

Method: Developed a cost-saving formula for automatic IQA pre-filtering and applied it in a background inpainting use case with a simple AutoML solution.

Result: Achieved significant cost savings of 51.61% by implementing automatic pre-filtering to increase the quality of images sent for manual review.

Conclusion: Automatic IQA pre-filtering can substantially reduce costs in production pipelines using deep generative models by improving the yield of high-quality images before manual review.

Abstract: Deep generative models have shown impressive progress in recent years, making
it possible to produce high quality images with a simple text prompt or a
reference image. However, state of the art technology does not yet meet the
quality standards offered by traditional photographic methods. For this reason,
production pipelines that use generated images often include a manual stage of
image quality assessment (IQA). This process is slow and expensive, especially
because of the low yield of automatically generated images that pass the
quality bar. The IQA workload can be reduced by introducing an automatic
pre-filtering stage, that will increase the overall quality of the images sent
to review and, therefore, reduce the average cost required to obtain a high
quality image. We present a formula that estimates the cost savings depending
on the precision and pass yield of a generic IQA engine. This formula is
applied in a use case of background inpainting, showcasing a significant cost
saving of 51.61% obtained with a simple AutoML solution.

</details>


### [119] [Seeing Through the Brain: New Insights from Decoding Visual Stimuli with fMRI](https://arxiv.org/abs/2510.16196)
*Zheng Huang,Enpei Zhang,Yinghao Cai,Weikang Qiu,Carl Yang,Elynn Chen,Xiang Zhang,Rex Ying,Dawei Zhou,Yujun Yan*

Main category: cs.CV

TL;DR: PRISM projects fMRI signals into a structured text space for visual stimulus reconstruction, using object-centric diffusion and attribute relationship search to outperform existing methods by 8% in perceptual loss.


<details>
  <summary>Details</summary>
Motivation: To understand how the brain encodes visual information by reconstructing images from fMRI signals, and to determine the optimal latent space structure for this transformation.

Method: Projects fMRI signals into a structured text space as intermediate representation, uses object-centric diffusion module to compose images from individual objects, and employs attribute relationship search to identify key attributes and relationships aligned with neural activity.

Result: Outperforms existing methods with up to 8% reduction in perceptual loss on real-world datasets.

Conclusion: Structured text space is the optimal intermediate representation for bridging fMRI signals and image reconstruction, capturing the compositional nature of visual stimuli effectively.

Abstract: Understanding how the brain encodes visual information is a central challenge
in neuroscience and machine learning. A promising approach is to reconstruct
visual stimuli, essentially images, from functional Magnetic Resonance Imaging
(fMRI) signals. This involves two stages: transforming fMRI signals into a
latent space and then using a pretrained generative model to reconstruct
images. The reconstruction quality depends on how similar the latent space is
to the structure of neural activity and how well the generative model produces
images from that space. Yet, it remains unclear which type of latent space best
supports this transformation and how it should be organized to represent visual
stimuli effectively. We present two key findings. First, fMRI signals are more
similar to the text space of a language model than to either a vision based
space or a joint text image space. Second, text representations and the
generative model should be adapted to capture the compositional nature of
visual stimuli, including objects, their detailed attributes, and
relationships. Building on these insights, we propose PRISM, a model that
Projects fMRI sIgnals into a Structured text space as an interMediate
representation for visual stimuli reconstruction. It includes an object centric
diffusion module that generates images by composing individual objects to
reduce object detection errors, and an attribute relationship search module
that automatically identifies key attributes and relationships that best align
with the neural activity. Extensive experiments on real world datasets
demonstrate that our framework outperforms existing methods, achieving up to an
8% reduction in perceptual loss. These results highlight the importance of
using structured text as the intermediate space to bridge fMRI signals and
image reconstruction.

</details>


### [120] [Data-Centric AI for Tropical Agricultural Mapping: Challenges, Strategies and Scalable Solutions](https://arxiv.org/abs/2510.16207)
*Mateus Pinto da Silva,Sabrina P. L. P. Correa,Hugo N. Oliveira,Ian M. Nunes,Jefersson A. dos Santos*

Main category: cs.CV

TL;DR: This paper proposes a Data-Centric AI approach for agricultural mapping in tropical regions, focusing on data quality and curation techniques to overcome challenges like limited annotated data, high labeling costs, and regional variability.


<details>
  <summary>Details</summary>
Motivation: Traditional model-centric approaches struggle in tropical agriculture due to high cloudiness, diverse crop calendars, and limited datasets. There's a need for more robust and scalable solutions that can handle the dynamic realities of tropical farming.

Method: The paper advocates a DCAI pipeline emphasizing data quality and curation. It reviews techniques including confident learning, core-set selection, data augmentation, and active learning, and proposes a practical pipeline using the 9 most mature methods.

Result: The paper identifies 25 distinct strategies suitable for large-scale agricultural mapping pipelines and highlights their readiness for implementation in tropical contexts.

Conclusion: A data-centric approach using curated techniques provides practical solutions for developing AI models better suited to the challenges of tropical agriculture, with a proposed pipeline of 9 mature methods for large-scale projects.

Abstract: Mapping agriculture in tropical areas through remote sensing presents unique
challenges, including the lack of high-quality annotated data, the elevated
costs of labeling, data variability, and regional generalisation. This paper
advocates a Data-Centric Artificial Intelligence (DCAI) perspective and
pipeline, emphasizing data quality and curation as key drivers for model
robustness and scalability. It reviews and prioritizes techniques such as
confident learning, core-set selection, data augmentation, and active learning.
The paper highlights the readiness and suitability of 25 distinct strategies in
large-scale agricultural mapping pipelines. The tropical context is of high
interest, since high cloudiness, diverse crop calendars, and limited datasets
limit traditional model-centric approaches. This tutorial outlines practical
solutions as a data-centric approach for curating and training AI models better
suited to the dynamic realities of tropical agriculture. Finally, we propose a
practical pipeline using the 9 most mature and straightforward methods that can
be applied to a large-scale tropical agricultural mapping project.

</details>


### [121] [StretchySnake: Flexible SSM Training Unlocks Action Recognition Across Spatio-Temporal Scales](https://arxiv.org/abs/2510.16209)
*Nyle Siddiqui,Rohit Gupta,Sirnam Swetha,Mubarak Shah*

Main category: cs.CV

TL;DR: Proposes StretchySnake, a flexible training method for video SSMs that enables spatio-temporal adaptability by sampling videos at varying resolutions and dynamically interpolating model weights, outperforming transformers and SSM baselines by up to 28% on action recognition benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current training methods for video understanding are tailored for transformers and fail to leverage SSMs' unique attributes, causing spatio-temporal inflexibility where models degrade on videos with unseen resolutions, limiting performance across short- and long-form videos.

Method: Samples videos at varying temporal and spatial resolutions during training and dynamically interpolates model weights to accommodate any spatio-temporal scale. Compares five flexible training variants to identify the most effective strategy for video SSMs.

Result: StretchySnake outperforms transformer and SSM baselines by up to 28% on short-action (UCF-101, HMDB-51) and long-action (COIN, Breakfast) benchmarks, with strong adaptability to fine-grained actions (SSV2, Diving-48).

Conclusion: Provides a simple drop-in training recipe that makes video SSMs more robust, resolution-agnostic, and efficient across diverse action recognition scenarios, enabling seamless handling of videos ranging from short clips to long activities.

Abstract: State space models (SSMs) have emerged as a competitive alternative to
transformers in various tasks. Their linear complexity and hidden-state
recurrence make them particularly attractive for modeling long sequences,
whereas attention becomes quadratically expensive. However, current training
methods for video understanding are tailored towards transformers and fail to
fully leverage the unique attributes of SSMs. For example, video models are
often trained at a fixed resolution and video length to balance the quadratic
scaling of attention cost against performance. Consequently, these models
suffer from degraded performance when evaluated on videos with spatial and
temporal resolutions unseen during training; a property we call spatio-temporal
inflexibility. In the context of action recognition, this severely limits a
model's ability to retain performance across both short- and long-form videos.
Therefore, we propose a flexible training method that leverages and improves
the inherent adaptability of SSMs. Our method samples videos at varying
temporal and spatial resolutions during training and dynamically interpolates
model weights to accommodate any spatio-temporal scale. This instills our SSM,
which we call StretchySnake, with spatio-temporal flexibility and enables it to
seamlessly handle videos ranging from short, fine-grained clips to long,
complex activities. We introduce and compare five different variants of
flexible training, and identify the most effective strategy for video SSMs. On
short-action (UCF-101, HMDB-51) and long-action (COIN, Breakfast) benchmarks,
StretchySnake outperforms transformer and SSM baselines alike by up to 28%,
with strong adaptability to fine-grained actions (SSV2, Diving-48). Therefore,
our method provides a simple drop-in training recipe that makes video SSMs more
robust, resolution-agnostic, and efficient across diverse action recognition
scenarios.

</details>


### [122] [VM-BeautyNet: A Synergistic Ensemble of Vision Transformer and Mamba for Facial Beauty Prediction](https://arxiv.org/abs/2510.16220)
*Djamel Eddine Boukhari*

Main category: cs.CV

TL;DR: VM-BeautyNet is a novel ensemble model that combines Vision Transformer and Mamba-based Vision model to improve facial beauty prediction by capturing both global structure and long-range dependencies with linear complexity.


<details>
  <summary>Details</summary>
Motivation: Existing CNN models struggle with global facial features, while Vision Transformers have quadratic complexity limitations. There's a need for models that can efficiently capture both holistic facial structure and long-range dependencies for accurate beauty prediction.

Method: Proposed VM-BeautyNet - a heterogeneous ensemble architecture that fuses Vision Transformer (for global facial structure and symmetry) and Mamba-based Vision model (for efficient long-range dependency modeling with linear complexity).

Result: Achieved state-of-the-art performance on SCUT-FBP5500 dataset: Pearson Correlation of 0.9212, MAE of 0.2085, RMSE of 0.2698. Grad-CAM visualizations confirmed complementary feature extraction between the two backbones.

Conclusion: VM-BeautyNet presents a powerful new architectural paradigm for computational aesthetics, offering improved performance and interpretability through synergistic fusion of Vision Transformer and Mamba-based models.

Abstract: Facial Beauty Prediction (FBP) is a complex and challenging computer vision
task, aiming to model the subjective and intricate nature of human aesthetic
perception. While deep learning models, particularly Convolutional Neural
Networks (CNNs), have made significant strides, they often struggle to capture
the global, holistic facial features that are critical to human judgment.
Vision Transformers (ViT) address this by effectively modeling long-range
spatial relationships, but their quadratic complexity can be a bottleneck. This
paper introduces a novel, heterogeneous ensemble architecture,
\textbf{VM-BeautyNet}, that synergistically fuses the complementary strengths
of a Vision Transformer and a Mamba-based Vision model, a recent advancement in
State-Space Models (SSMs). The ViT backbone excels at capturing global facial
structure and symmetry, while the Mamba backbone efficiently models long-range
dependencies with linear complexity, focusing on sequential features and
textures. We evaluate our approach on the benchmark SCUT-FBP5500 dataset. Our
proposed VM-BeautyNet achieves state-of-the-art performance, with a
\textbf{Pearson Correlation (PC) of 0.9212}, a \textbf{Mean Absolute Error
(MAE) of 0.2085}, and a \textbf{Root Mean Square Error (RMSE) of 0.2698}.
Furthermore, through Grad-CAM visualizations, we provide interpretability
analysis that confirms the complementary feature extraction of the two
backbones, offering new insights into the model's decision-making process and
presenting a powerful new architectural paradigm for computational aesthetics.

</details>


### [123] [Designing a Convolutional Neural Network for High-Accuracy Oral Cavity Squamous Cell Carcinoma (OCSCC) Detection](https://arxiv.org/abs/2510.16235)
*Vishal Manikanden,Aniketh Bandlamudi,Daniel Haehn*

Main category: cs.CV

TL;DR: A CNN-based system for early detection of Oral Cavity Squamous Cell Carcinoma (OCSCC) using image analysis, with hardware for image capture and processing.


<details>
  <summary>Details</summary>
Motivation: OCSCC often goes undetected due to subtle early symptoms and hidden development areas, leading to preventable deaths. Early detection using AI could save lives.

Method: Trained a CNN on 4293 images (benign/malignant tumors, negative samples) and tested on images at 5 different resolutions. Developed hardware for image capture and processing, plus an application for testing.

Result: Higher resolution images led to more accurate predictions on a logarithmic scale, showing diminishing returns with increased pixel counts. The system demonstrated effective OCSCC detection capabilities.

Conclusion: CNN-based systems with proper image resolution can effectively detect OCSCC early, with hardware enhancement improving detection accuracy, though higher resolutions provide diminishing returns.

Abstract: Oral Cavity Squamous Cell Carcinoma (OCSCC) is the most common type of head
and neck cancer. Due to the subtle nature of its early stages, deep and hidden
areas of development, and slow growth, OCSCC often goes undetected, leading to
preventable deaths. However, properly trained Convolutional Neural Networks
(CNNs), with their precise image segmentation techniques and ability to apply
kernel matrices to modify the RGB values of images for accurate image pattern
recognition, would be an effective means for early detection of OCSCC. Pairing
this neural network with image capturing and processing hardware would allow
increased efficacy in OCSCC detection. The aim of our project is to develop a
Convolutional Neural Network trained to recognize OCSCC, as well as to design a
physical hardware system to capture and process detailed images, in order to
determine the image quality required for accurate predictions. A CNN was
trained on 4293 training images consisting of benign and malignant tumors, as
well as negative samples, and was evaluated for its precision, recall, and Mean
Average Precision (mAP) in its predictions of OCSCC. A testing dataset of
randomly assorted images of cancerous, non-cancerous, and negative images was
chosen, and each image was altered to represent 5 common resolutions. This test
data set was thoroughly analyzed by the CNN and predictions were scored on the
basis of accuracy. The designed enhancement hardware was used to capture
detailed images, and its impact was scored. An application was developed to
facilitate the testing process and bring open access to the CNN. Images of
increasing resolution resulted in higher-accuracy predictions on a logarithmic
scale, demonstrating the diminishing returns of higher pixel counts.

</details>


### [124] [Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset](https://arxiv.org/abs/2510.16258)
*Claire McLean,Makenzie Meendering,Tristan Swartz,Orri Gabbay,Alexandra Olsen,Rachel Jacobs,Nicholas Rosen,Philippe de Bree,Tony Garcia,Gadsden Merrill,Jake Sandakly,Julia Buffalini,Neham Jain,Steven Krenn,Moneish Kumar,Dejan Markovic,Evonne Ng,Fabian Prada,Andrew Saba,Siwei Zhang,Vasu Agrawal,Tim Godisart,Alexander Richard,Michael Zollhoefer*

Main category: cs.CV

TL;DR: Embody 3D is a large-scale multimodal dataset with 500 hours of 3D motion data from 439 participants, featuring single-person motions and multi-person interactions with tracking, annotations, and audio.


<details>
  <summary>Details</summary>
Motivation: To create a comprehensive dataset for studying human motion and social interactions in 3D, addressing the need for large-scale multimodal data with detailed tracking and annotations.

Method: Collected data from 439 participants in a multi-camera stage, capturing 54 million frames of 3D motion tracking including hands and body shape, with text annotations and separate audio tracks.

Result: Successfully created a dataset with 500 hours of diverse motion data including prompted motions, gestures, locomotion, discussions, emotional conversations, collaborative activities, and co-living scenarios.

Conclusion: Embody 3D provides a valuable resource for research in human motion analysis, social interaction modeling, and multimodal AI applications with its comprehensive 3D tracking and annotations.

Abstract: The Codec Avatars Lab at Meta introduces Embody 3D, a multimodal dataset of
500 individual hours of 3D motion data from 439 participants collected in a
multi-camera collection stage, amounting to over 54 million frames of tracked
3D motion. The dataset features a wide range of single-person motion data,
including prompted motions, hand gestures, and locomotion; as well as
multi-person behavioral and conversational data like discussions, conversations
in different emotional states, collaborative activities, and co-living
scenarios in an apartment-like space. We provide tracked human motion including
hand tracking and body shape, text annotations, and a separate audio track for
each participant.

</details>


### [125] [Proactive Scene Decomposition and Reconstruction](https://arxiv.org/abs/2510.16272)
*Baicheng Li,Zike Yan,Dong Wu,Hongbin Zha*

Main category: cs.CV

TL;DR: Proactive scene decomposition and reconstruction using human-object interactions to iteratively disassemble and reconstruct environments from egocentric live streams.


<details>
  <summary>Details</summary>
Motivation: Human behaviors contain rich cues about scene dynamics and can help address ambiguities in static object-level reconstruction methods.

Method: Online approach that leverages human-object interactions to iteratively refine decomposition and reconstruction, integrating camera/object pose estimation, instance decomposition, and online map updating using Gaussian splatting for photorealistic rendering.

Result: Achieves accurate and consistent dynamic scene modeling with photorealistic and efficient rendering, validated in multiple real-world scenarios.

Conclusion: Provides a flexible, progressive alternative to conventional object-level reconstruction methods by capitalizing on human-object interaction cues.

Abstract: Human behaviors are the major causes of scene dynamics and inherently contain
rich cues regarding the dynamics. This paper formalizes a new task of proactive
scene decomposition and reconstruction, an online approach that leverages
human-object interactions to iteratively disassemble and reconstruct the
environment. By observing these intentional interactions, we can dynamically
refine the decomposition and reconstruction process, addressing inherent
ambiguities in static object-level reconstruction. The proposed system
effectively integrates multiple tasks in dynamic environments such as accurate
camera and object pose estimation, instance decomposition, and online map
updating, capitalizing on cues from human-object interactions in egocentric
live streams for a flexible, progressive alternative to conventional
object-level reconstruction methods. Aided by the Gaussian splatting technique,
accurate and consistent dynamic scene modeling is achieved with photorealistic
and efficient rendering. The efficacy is validated in multiple real-world
scenarios with promising advantages.

</details>


### [126] [Cerberus: Real-Time Video Anomaly Detection via Cascaded Vision-Language Models](https://arxiv.org/abs/2510.16290)
*Yue Zheng,Xiufang Shi,Jiming Chen,Yuanchao Shu*

Main category: cs.CV

TL;DR: Cerberus is a two-stage cascaded system for efficient real-time video anomaly detection that combines lightweight filtering with VLM reasoning, achieving 57.68 fps and 97.2% accuracy.


<details>
  <summary>Details</summary>
Motivation: Overcome computational cost and unstable visual grounding of Vision-Language Models for real-time video anomaly detection deployment.

Method: Two-stage cascaded system with motion mask prompting and rule-based deviation detection that learns normal behavior offline and combines lightweight filtering with VLM reasoning during inference.

Result: Achieves 57.68 fps (151.79 speedup) on NVIDIA L40S GPU with 97.2% accuracy comparable to state-of-the-art VLM-based methods.

Conclusion: Cerberus establishes itself as a practical solution for real-time video analytics by balancing efficiency and accuracy.

Abstract: Video anomaly detection (VAD) has rapidly advanced by recent development of
Vision-Language Models (VLMs). While these models offer superior zero-shot
detection capabilities, their immense computational cost and unstable visual
grounding performance hinder real-time deployment. To overcome these
challenges, we introduce Cerberus, a two-stage cascaded system designed for
efficient yet accurate real-time VAD. Cerberus learns normal behavioral rules
offline, and combines lightweight filtering with fine-grained VLM reasoning
during online inference. The performance gains of Cerberus come from two key
innovations: motion mask prompting and rule-based deviation detection. The
former directs the VLM's attention to regions relevant to motion, while the
latter identifies anomalies as deviations from learned norms rather than
enumerating possible anomalies. Extensive evaluations on four datasets show
that Cerberus on average achieves 57.68 fps on an NVIDIA L40S GPU, a
151.79$\times$ speedup, and 97.2\% accuracy comparable to the state-of-the-art
VLM-based VAD methods, establishing it as a practical solution for real-time
video analytics.

</details>


### [127] [OpenLVLM-MIA: A Controlled Benchmark Revealing the Limits of Membership Inference Attacks on Large Vision-Language Models](https://arxiv.org/abs/2510.16295)
*Ryoto Miyamoto,Xin Fan,Fuyuko Kido,Tsuneo Matsumoto,Hayato Yamana*

Main category: cs.CV

TL;DR: OpenLVLM-MIA is a benchmark that reveals current MIA evaluations on LVLMs are flawed due to distributional bias, and shows state-of-the-art methods perform at random chance under unbiased conditions.


<details>
  <summary>Details</summary>
Motivation: To address fundamental challenges in evaluating membership inference attacks on large vision-language models, where prior high success rates were likely due to dataset construction biases rather than true membership detection.

Method: Created a controlled benchmark with 6,000 images where member and non-member distributions are carefully balanced, with ground-truth membership labels across three distinct training stages.

Result: Experiments showed that state-of-the-art MIA methods converged to random chance performance when tested under unbiased conditions using the OpenLVLM-MIA benchmark.

Conclusion: OpenLVLM-MIA provides a transparent, unbiased foundation for developing stronger privacy-preserving techniques by clarifying current limitations of MIA research on LVLMs.

Abstract: OpenLVLM-MIA is a new benchmark that highlights fundamental challenges in
evaluating membership inference attacks (MIA) against large vision-language
models (LVLMs). While prior work has reported high attack success rates, our
analysis suggests that these results often arise from detecting distributional
bias introduced during dataset construction rather than from identifying true
membership status. To address this issue, we introduce a controlled benchmark
of 6{,}000 images where the distributions of member and non-member samples are
carefully balanced, and ground-truth membership labels are provided across
three distinct training stages. Experiments using OpenLVLM-MIA demonstrated
that the performance of state-of-the-art MIA methods converged to random chance
under unbiased conditions. By offering a transparent and unbiased benchmark,
OpenLVLM-MIA clarifies the current limitations of MIA research on LVLMs and
provides a solid foundation for developing stronger privacy-preserving
techniques.

</details>


### [128] [Stroke2Sketch: Harnessing Stroke Attributes for Training-Free Sketch Generation](https://arxiv.org/abs/2510.16319)
*Rui Yang,Huining Li,Yiyi Long,Xiaojun Wu,Shengfeng He*

Main category: cs.CV

TL;DR: Stroke2Sketch is a training-free framework that transfers stroke attributes from reference sketches to content images using cross-image stroke attention, achieving high-quality sketch generation.


<details>
  <summary>Details</summary>
Motivation: To enable precise transfer of stroke attributes (line thickness, deformation, texture sparsity) while preserving semantic structure and content fidelity in sketch generation.

Method: Uses cross-image stroke attention embedded in self-attention layers for fine-grained semantic correspondences, plus adaptive contrast enhancement and semantic-focused attention for content preservation.

Result: Effectively synthesizes stylistically faithful sketches resembling handcrafted results, outperforming existing methods in expressive stroke control and semantic coherence.

Conclusion: Stroke2Sketch provides a novel training-free approach for high-quality sketch generation with accurate stroke attribute transfer and structural preservation.

Abstract: Generating sketches guided by reference styles requires precise transfer of
stroke attributes, such as line thickness, deformation, and texture sparsity,
while preserving semantic structure and content fidelity. To this end, we
propose Stroke2Sketch, a novel training-free framework that introduces
cross-image stroke attention, a mechanism embedded within self-attention layers
to establish fine-grained semantic correspondences and enable accurate stroke
attribute transfer. This allows our method to adaptively integrate reference
stroke characteristics into content images while maintaining structural
integrity. Additionally, we develop adaptive contrast enhancement and
semantic-focused attention to reinforce content preservation and foreground
emphasis. Stroke2Sketch effectively synthesizes stylistically faithful sketches
that closely resemble handcrafted results, outperforming existing methods in
expressive stroke control and semantic coherence. Codes are available at
https://github.com/rane7/Stroke2Sketch.

</details>


### [129] [Scaling Laws for Deepfake Detection](https://arxiv.org/abs/2510.16320)
*Wenhao Wang,Longqi Cai,Taihong Xiao,Yuxiao Wang,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: This paper studies scaling laws for deepfake detection, finding that detection error follows power-law decay as training data scales across domains and generation methods, similar to LLMs.


<details>
  <summary>Details</summary>
Motivation: To systematically analyze how deepfake detection performance scales with data quantity and diversity, since existing datasets are insufficient for such scaling studies.

Method: Constructed ScaleDF - the largest deepfake dataset with 5.8M real images from 51 domains and 8.8M fake images from 102 generation methods, then analyzed power-law scaling relationships.

Result: Discovered predictable power-law scaling where detection error decreases as either number of real domains or deepfake methods increases, enabling performance forecasting.

Conclusion: Scaling laws apply to deepfake detection similar to LLMs, suggesting data-centric approaches to counter evolving deepfake technology, though scaling has limitations.

Abstract: This paper presents a systematic study of scaling laws for the deepfake
detection task. Specifically, we analyze the model performance against the
number of real image domains, deepfake generation methods, and training images.
Since no existing dataset meets the scale requirements for this research, we
construct ScaleDF, the largest dataset to date in this field, which contains
over 5.8 million real images from 51 different datasets (domains) and more than
8.8 million fake images generated by 102 deepfake methods. Using ScaleDF, we
observe power-law scaling similar to that shown in large language models
(LLMs). Specifically, the average detection error follows a predictable
power-law decay as either the number of real domains or the number of deepfake
methods increases. This key observation not only allows us to forecast the
number of additional real domains or deepfake methods required to reach a
target performance, but also inspires us to counter the evolving deepfake
technology in a data-centric manner. Beyond this, we examine the role of
pre-training and data augmentations in deepfake detection under scaling, as
well as the limitations of scaling itself.

</details>


### [130] [Scale-DiT: Ultra-High-Resolution Image Generation with Hierarchical Local Attention](https://arxiv.org/abs/2510.16325)
*Yuyao Zhang,Yu-Wing Tai*

Main category: cs.CV

TL;DR: Scale-DiT enables efficient 4K image generation using hierarchical local attention with global guidance, achieving faster inference and better quality without requiring native 4K training data.


<details>
  <summary>Details</summary>
Motivation: Current diffusion models are limited to sub-1K resolutions due to quadratic attention complexity and lack of native 4K training data, creating a need for scalable ultra-high-resolution generation.

Method: Divides high-resolution latents into local windows for near-linear complexity, uses low-resolution latent with positional anchors for global semantics, and bridges pathways with LoRA adaptation during denoising.

Result: Achieves 2x faster inference, lower memory usage, scales to 4K resolution without additional training data, and delivers superior global coherence and local detail compared to state-of-the-art methods.

Conclusion: Hierarchical local attention with guided low-resolution anchors is an effective approach for advancing ultra-high-resolution image generation.

Abstract: Ultra-high-resolution text-to-image generation demands both fine-grained
texture synthesis and globally coherent structure, yet current diffusion models
remain constrained to sub-$1K \times 1K$ resolutions due to the prohibitive
quadratic complexity of attention and the scarcity of native $4K$ training
data. We present \textbf{Scale-DiT}, a new diffusion framework that introduces
hierarchical local attention with low-resolution global guidance, enabling
efficient, scalable, and semantically coherent image synthesis at ultra-high
resolutions. Specifically, high-resolution latents are divided into fixed-size
local windows to reduce attention complexity from quadratic to near-linear,
while a low-resolution latent equipped with scaled positional anchors injects
global semantics. A lightweight LoRA adaptation bridges global and local
pathways during denoising, ensuring consistency across structure and detail. To
maximize inference efficiency, we repermute token sequence in Hilbert curve
order and implement a fused-kernel for skipping masked operations, resulting in
a GPU-friendly design. Extensive experiments demonstrate that Scale-DiT
achieves more than $2\times$ faster inference and lower memory usage compared
to dense attention baselines, while reliably scaling to $4K \times 4K$
resolution without requiring additional high-resolution training data. On both
quantitative benchmarks (FID, IS, CLIP Score) and qualitative comparisons,
Scale-DiT delivers superior global coherence and sharper local detail, matching
or outperforming state-of-the-art methods that rely on native 4K training.
Taken together, these results highlight hierarchical local attention with
guided low-resolution anchors as a promising and effective approach for
advancing ultra-high-resolution image generation.

</details>


### [131] [DiffusionX: Efficient Edge-Cloud Collaborative Image Generation with Multi-Round Prompt Evolution](https://arxiv.org/abs/2510.16326)
*Yi Wei,Shunpu Tang,Liang Zhao,Qiangian Yang*

Main category: cs.CV

TL;DR: DiffusionX is a cloud-edge collaborative framework that reduces diffusion model generation time by 15.8% while maintaining image quality through lightweight on-device previews and cloud-based final refinements.


<details>
  <summary>Details</summary>
Motivation: Address the computational intensity and latency issues in diffusion models, especially during iterative prompt refinement that burdens cloud resources.

Method: A cloud-edge collaborative framework with lightweight on-device diffusion model for rapid previews and high-capacity cloud model for final refinements, plus a noise level predictor for dynamic computation load balancing.

Result: Reduces average generation time by 15.8% compared to Stable Diffusion v1.5 while maintaining comparable image quality, and is only 0.9% slower than Tiny-SD with significantly improved image quality.

Conclusion: DiffusionX demonstrates efficient and scalable prompt-based image generation with minimal overhead through cloud-edge collaboration.

Abstract: Recent advances in diffusion models have driven remarkable progress in image
generation. However, the generation process remains computationally intensive,
and users often need to iteratively refine prompts to achieve the desired
results, further increasing latency and placing a heavy burden on cloud
resources. To address this challenge, we propose DiffusionX, a cloud-edge
collaborative framework for efficient multi-round, prompt-based generation. In
this system, a lightweight on-device diffusion model interacts with users by
rapidly producing preview images, while a high-capacity cloud model performs
final refinements after the prompt is finalized. We further introduce a noise
level predictor that dynamically balances the computation load, optimizing the
trade-off between latency and cloud workload. Experiments show that DiffusionX
reduces average generation time by 15.8% compared with Stable Diffusion v1.5,
while maintaining comparable image quality. Moreover, it is only 0.9% slower
than Tiny-SD with significantly improved image quality, thereby demonstrating
efficiency and scalability with minimal overhead.

</details>


### [132] [TokenAR: Multiple Subject Generation via Autoregressive Token-level enhancement](https://arxiv.org/abs/2510.16332)
*Haiyue Sun,Qingdong He,Jinlong Peng,Peng Tang,Jiangning Zhang,Junwei Zhu,Xiaobin Hu,Shuicheng Yan*

Main category: cs.CV

TL;DR: TokenAR is a token-level enhancement framework for autoregressive models that addresses identity confusion in multi-reference image generation through three key components: token index embedding, instruct token injection, and identity-token disentanglement strategy.


<details>
  <summary>Details</summary>
Motivation: Autoregressive models struggle with decoupling different reference identities in multiple reference generation, leading to identity confusion problems.

Method: Three-part token-level enhancement: 1) Token Index Embedding clusters tokens for better reference representation; 2) Instruct Token Injection adds extra visual feature containers; 3) Identity-token disentanglement strategy guides tokens to independently represent each identity's features.

Result: Significantly enhances AR-based methods for conditional image generation, achieving good identity consistency while preserving high-quality background reconstruction. Outperforms state-of-the-art models in multiple reference image generation.

Conclusion: TokenAR framework effectively solves reference identity confusion in multi-subject generation and introduces the first large-scale InstructAR dataset for training and evaluation in this domain.

Abstract: Autoregressive Model (AR) has shown remarkable success in conditional image
generation. However, these approaches for multiple reference generation
struggle with decoupling different reference identities. In this work, we
propose the TokenAR framework, specifically focused on a simple but effective
token-level enhancement mechanism to address reference identity confusion
problem. Such token-level enhancement consists of three parts, 1). Token Index
Embedding clusters the tokens index for better representing the same reference
images; 2). Instruct Token Injection plays as a role of extra visual feature
container to inject detailed and complementary priors for reference tokens; 3).
The identity-token disentanglement strategy (ITD) explicitly guides the token
representations toward independently representing the features of each
identity.This token-enhancement framework significantly augments the
capabilities of existing AR based methods in conditional image generation,
enabling good identity consistency while preserving high quality background
reconstruction. Driven by the goal of high-quality and high-diversity in
multi-subject generation, we introduce the InstructAR Dataset, the first
open-source, large-scale, multi-reference input, open domain image generation
dataset that includes 28K training pairs, each example has two reference
subjects, a relative prompt and a background with mask annotation, curated for
multiple reference image generation training and evaluating. Comprehensive
experiments validate that our approach surpasses current state-of-the-art
models in multiple reference image generation task. The implementation code and
datasets will be made publicly. Codes are available, see
https://github.com/lyrig/TokenAR

</details>


### [133] [RL makes MLLMs see better than SFT](https://arxiv.org/abs/2510.16333)
*Junha Song,Sangdoo Yun,Dongyoon Han,Jaegul Choo,Byeongho Heo*

Main category: cs.CV

TL;DR: RL training for MLLMs produces stronger and more precisely localized visual representations than SFT, leading to better performance on vision-related tasks with significantly less computational cost.


<details>
  <summary>Details</summary>
Motivation: Address the lack of understanding about how training strategies (SFT vs RL) reshape vision encoders in MLLMs, given the dominant assumption that MLLM performance mainly comes from the LLM backbone.

Method: Conducted diverse experiments including ImageNet classification, segmentation, and gradient visualization to analyze vision encoder representations after different training strategies.

Result: RL produces stronger and precisely localized visual representations compared to SFT, boosting vision encoder capability. Proposed PIVOT method achieves better performance than larger models with less than 1% computational cost.

Conclusion: RL fundamentally reshapes MLLM's visual representations, and PIVOT provides an effective and efficient path for advancing vision backbones in MLLMs.

Abstract: A dominant assumption in Multimodal Language Model (MLLM) research is that
its performance is largely inherited from the LLM backbone, given its immense
parameter scale and remarkable capabilities. This has created a void in the
understanding of the vision encoder, which determines how MLLMs perceive
images. The recent shift in MLLM training paradigms, from Supervised Finetuning
(SFT) to Reinforcement Learning (RL), magnifies this oversight-namely, the
significant lack of analysis on how such training reshapes the vision encoder
as well as the MLLM. To address this, we first investigate the impact of
training strategies on MLLMs, where RL shows a clear advantage over SFT in
strongly vision-related VQA benchmarks. Motivated by this, we conduct a
critical yet under-explored analysis of the vision encoder of MLLMs through
diverse and in-depth experiments, ranging from ImageNet classification and
segmentation to gradient visualization. Our results demonstrate that MLLM's
post-training strategy (i.e., SFT or RL) not only leads to distinct outcomes on
MLLM downstream tasks, but also fundamentally reshapes MLLM's underlying visual
representations. Specifically, the key finding of our study is that RL produces
stronger and precisely localized visual representations compared to SFT,
boosting the ability of the vision encoder for MLLM. We then reframe our
findings into a simple recipe for building strong vision encoders for MLLMs,
Preference-Instructed Vision OpTimization (PIVOT). When integrated into MLLMs,
a PIVOT-trained vision encoder outperforms even larger and more heavily-trained
counterparts, despite requiring less than 1% of the computational cost of
standard vision pretraining. This result opens an effective and efficient path
for advancing the vision backbones of MLLMs. Project page available at
https://june-page.github.io/pivot/

</details>


### [134] [On the Provable Importance of Gradients for Language-Assisted Image Clustering](https://arxiv.org/abs/2510.16335)
*Bo Peng,Jie Lu,Guangquan Zhang,Zhen Fang*

Main category: cs.CV

TL;DR: GradNorm is a gradient-based framework for filtering positive nouns in Language-assisted Image Clustering (LaIC), providing theoretical guarantees and outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing filtering strategies for LaIC lack theoretical foundation despite using CLIP features. The paper aims to provide a rigorous theoretical basis for noun filtering.

Method: Proposed GradNorm framework measures noun positiveness using gradient magnitudes from cross-entropy between predicted target distribution and softmax output.

Result: Extensive experiments show GradNorm achieves state-of-the-art clustering performance on various benchmarks.

Conclusion: GradNorm provides theoretical guarantees, subsumes existing methods as special cases, and demonstrates superior empirical performance for LaIC.

Abstract: This paper investigates the recently emerged problem of Language-assisted
Image Clustering (LaIC), where textual semantics are leveraged to improve the
discriminability of visual representations to facilitate image clustering. Due
to the unavailability of true class names, one of core challenges of LaIC lies
in how to filter positive nouns, i.e., those semantically close to the images
of interest, from unlabeled wild corpus data. Existing filtering strategies are
predominantly based on the off-the-shelf feature space learned by CLIP;
however, despite being intuitive, these strategies lack a rigorous theoretical
foundation. To fill this gap, we propose a novel gradient-based framework,
termed as GradNorm, which is theoretically guaranteed and shows strong
empirical performance. In particular, we measure the positiveness of each noun
based on the magnitude of gradients back-propagated from the cross-entropy
between the predicted target distribution and the softmax output.
Theoretically, we provide a rigorous error bound to quantify the separability
of positive nouns by GradNorm and prove that GradNorm naturally subsumes
existing filtering strategies as extremely special cases of itself.
Empirically, extensive experiments show that GradNorm achieves the
state-of-the-art clustering performance on various benchmarks.

</details>


### [135] [MIRAD - A comprehensive real-world robust anomaly detection dataset for Mass Individualization](https://arxiv.org/abs/2510.16370)
*Pulin Li,Guocheng Wu,Li Yin,Yuxin Zheng,Wei Zhang,Yanjie Zhou*

Main category: cs.CV

TL;DR: MIRAD dataset addresses anomaly detection challenges in social manufacturing by providing the first benchmark with diverse individualized products, data from distributed sites, and imaging heterogeneity.


<details>
  <summary>Details</summary>
Motivation: Social manufacturing enables mass individualization but faces quality control challenges due to customized products, small-batch orders, and varying imaging environments across distributed sites.

Method: Created MIRAD dataset capturing three key dimensions: diverse individualized products with large intra-class variation, data from six geographically dispersed manufacturing nodes, and substantial imaging heterogeneity including lighting, background, and motion variations.

Result: Extensive evaluation of SOTA anomaly detection methods (one-class, multi-class, zero-shot) on MIRAD shows significant performance drop compared to conventional benchmarks, highlighting unresolved complexities in real-world individualized production.

Conclusion: MIRAD bridges industrial requirements and academic research, providing a realistic foundation for developing robust quality control solutions essential for Industry 5.0.

Abstract: Social manufacturing leverages community collaboration and scattered
resources to realize mass individualization in modern industry. However, this
paradigm shift also introduces substantial challenges in quality control,
particularly in defect detection. The main difficulties stem from three
aspects. First, products often have highly customized configurations. Second,
production typically involves fragmented, small-batch orders. Third, imaging
environments vary considerably across distributed sites. To overcome the
scarcity of real-world datasets and tailored algorithms, we introduce the Mass
Individualization Robust Anomaly Detection (MIRAD) dataset. As the first
benchmark explicitly designed for anomaly detection in social manufacturing,
MIRAD captures three critical dimensions of this domain: (1) diverse
individualized products with large intra-class variation, (2) data collected
from six geographically dispersed manufacturing nodes, and (3) substantial
imaging heterogeneity, including variations in lighting, background, and motion
conditions. We then conduct extensive evaluations of state-of-the-art (SOTA)
anomaly detection methods on MIRAD, covering one-class, multi-class, and
zero-shot approaches. Results show a significant performance drop across all
models compared with conventional benchmarks, highlighting the unresolved
complexities of defect detection in real-world individualized production. By
bridging industrial requirements and academic research, MIRAD provides a
realistic foundation for developing robust quality control solutions essential
for Industry 5.0. The dataset is publicly available at
https://github.com/wu33learn/MIRAD.

</details>


### [136] [Cataract-LMM: Large-Scale, Multi-Source, Multi-Task Benchmark for Deep Learning in Surgical Video Analysis](https://arxiv.org/abs/2510.16371)
*Mohammad Javad Ahmadi,Iman Gandomi,Parisa Abdi,Seyed-Farzad Mohammadi,Amirhossein Taslimi,Mehdi Khodaparast,Hassan Hashemi,Mahdi Tavakoli,Hamid D. Taghirad*

Main category: cs.CV

TL;DR: A large-scale dataset of 3,000 cataract surgery videos with comprehensive annotations for surgical AI development.


<details>
  <summary>Details</summary>
Motivation: Current cataract surgery datasets lack diversity and annotation depth needed for training generalizable deep-learning models.

Method: Collected 3,000 phacoemulsification cataract surgery videos from two surgical centers with surgeons of varying experience levels, enriched with four annotation layers: temporal surgical phases, instance segmentation, instrument-tissue interaction tracking, and quantitative skill scores.

Result: Benchmarking experiments demonstrated the dataset's quality for key surgical AI tasks including workflow recognition, scene segmentation, and automated skill assessment. Domain adaptation baseline was established for phase recognition.

Conclusion: The dataset provides a comprehensive resource for developing computer-assisted surgery systems and is publicly available.

Abstract: The development of computer-assisted surgery systems depends on large-scale,
annotated datasets. Current resources for cataract surgery often lack the
diversity and annotation depth needed to train generalizable deep-learning
models. To address this gap, we present a dataset of 3,000 phacoemulsification
cataract surgery videos from two surgical centers, performed by surgeons with a
range of experience levels. This resource is enriched with four annotation
layers: temporal surgical phases, instance segmentation of instruments and
anatomical structures, instrument-tissue interaction tracking, and quantitative
skill scores based on the established competency rubrics like the ICO-OSCAR.
The technical quality of the dataset is supported by a series of benchmarking
experiments for key surgical AI tasks, including workflow recognition, scene
segmentation, and automated skill assessment. Furthermore, we establish a
domain adaptation baseline for the phase recognition task by training a model
on a subset of surgical centers and evaluating its performance on a held-out
center. The dataset and annotations are available in Google Form
(https://docs.google.com/forms/d/e/1FAIpQLSfmyMAPSTGrIy2sTnz0-TMw08ZagTimRulbAQcWdaPwDy187A/viewform?usp=dialog).

</details>


### [137] [iWatchRoadv2: Pothole Detection, Geospatial Mapping, and Intelligent Road Governance](https://arxiv.org/abs/2510.16375)
*Rishi Raj Sahoo,Surbhi Saswati Mohanty,Subhankar Mishra*

Main category: cs.CV

TL;DR: iWatchRoadv2 is an automated platform for real-time pothole detection using YOLO models, GPS geotagging, and OpenStreetMap visualization, with governance features for contractor accountability and public engagement.


<details>
  <summary>Details</summary>
Motivation: Road potholes pose significant safety hazards and maintenance challenges on India's diverse and under-maintained road networks, requiring automated solutions for detection and repair management.

Method: Fine-tuned Ultralytics YOLO model on 7,000+ dashcam frames of Indian roads, synchronized OCR timestamps with GPS logs for geolocation, and built backend database with road segment attribution and contractor information.

Result: Developed a fully automated end-to-end platform with real-time detection, intelligent governance features, automated alerts to contractors, and intuitive web interface for stakeholders.

Conclusion: iWatchRoadv2 enables data-driven smart city management, transparent governance, and sustainable improvements in road infrastructure maintenance through complete automation of the pothole monitoring lifecycle.

Abstract: Road potholes pose significant safety hazards and maintenance challenges,
particularly on India's diverse and under-maintained road networks. This paper
presents iWatchRoadv2, a fully automated end-to-end platform for real-time
pothole detection, GPS-based geotagging, and dynamic road health visualization
using OpenStreetMap (OSM). We curated a self-annotated dataset of over 7,000
dashcam frames capturing diverse Indian road conditions, weather patterns, and
lighting scenarios, which we used to fine-tune the Ultralytics YOLO model for
accurate pothole detection. The system synchronizes OCR-extracted video
timestamps with external GPS logs to precisely geolocate each detected pothole,
enriching detections with comprehensive metadata, including road segment
attribution and contractor information managed through an optimized backend
database. iWatchRoadv2 introduces intelligent governance features that enable
authorities to link road segments with contract metadata through a secure login
interface. The system automatically sends alerts to contractors and officials
when road health deteriorates, supporting automated accountability and warranty
enforcement. The intuitive web interface delivers actionable analytics to
stakeholders and the public, facilitating evidence-driven repair planning,
budget allocation, and quality assessment. Our cost-effective and scalable
solution streamlines frame processing and storage while supporting seamless
public engagement for urban and rural deployments. By automating the complete
pothole monitoring lifecycle, from detection to repair verification,
iWatchRoadv2 enables data-driven smart city management, transparent governance,
and sustainable improvements in road infrastructure maintenance. The platform
and live demonstration are accessible at
https://smlab.niser.ac.in/project/iwatchroad.

</details>


### [138] [Demeter: A Parametric Model of Crop Plant Morphology from the Real World](https://arxiv.org/abs/2510.16377)
*Tianhang Cheng,Albert J. Zhai,Evan Z. Chen,Rui Zhou,Yawen Deng,Zitong Li,Kejie Zhao,Janice Shiu,Qianyu Zhao,Yide Xu,Xinlei Wang,Yuan Shen,Sheng Wang,Lisa Ainsworth,Kaiyu Guan,Shenlong Wang*

Main category: cs.CV

TL;DR: Demeter is a parametric 3D shape model for plants that encodes topology, shape, articulation, and deformation, addressing the lack of expressive plant models compared to existing human/animal models.


<details>
  <summary>Details</summary>
Motivation: While powerful 3D parametric models exist for humans and animals, equally expressive approaches for modeling plants are lacking, despite their broad utility in 3D reconstruction, generation, understanding, and simulation.

Method: Demeter is a data-driven parametric model that encodes key plant morphology factors (topology, shape, articulation, deformation) into a compact learned representation, handling varying shape topology across species and modeling three sources of shape variation: articulation, subcomponent shape variation, and non-rigid deformation.

Result: Experiments show that Demeter effectively synthesizes shapes, reconstructs structures, and simulates biophysical processes. The model was tested on a large-scale, ground-truthed soybean farm dataset.

Conclusion: Demeter advances crop plant modeling by providing an expressive parametric model that can handle the complex morphological variations in plants, with demonstrated effectiveness in shape synthesis, structure reconstruction, and biophysical simulation.

Abstract: Learning 3D parametric shape models of objects has gained popularity in
vision and graphics and has showed broad utility in 3D reconstruction,
generation, understanding, and simulation. While powerful models exist for
humans and animals, equally expressive approaches for modeling plants are
lacking. In this work, we present Demeter, a data-driven parametric model that
encodes key factors of a plant morphology, including topology, shape,
articulation, and deformation into a compact learned representation. Unlike
previous parametric models, Demeter handles varying shape topology across
various species and models three sources of shape variation: articulation,
subcomponent shape variation, and non-rigid deformation. To advance crop plant
modeling, we collected a large-scale, ground-truthed dataset from a soybean
farm as a testbed. Experiments show that Demeter effectively synthesizes
shapes, reconstructs structures, and simulates biophysical processes. Code and
data is available at https://tianhang-cheng.github.io/Demeter/.

</details>


### [139] [SPLite Hand: Sparsity-Aware Lightweight 3D Hand Pose Estimation](https://arxiv.org/abs/2510.16396)
*Yeh Keng Hao,Hsu Tzu Wei,Sun Min*

Main category: cs.CV

TL;DR: A lightweight framework for hand pose estimation on edge devices using sparse convolution, SPLite decoder, and quantization-aware training to achieve significant efficiency improvements while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of deploying deep learning models on AR/VR edge devices requiring real-time inference, low power consumption, and minimal latency while balancing efficiency and performance.

Method: Encoder-decoder architecture with sparse convolution on ResNet-18 backbone, SPLite decoder for faster decoding, and quantization-aware training for memory optimization.

Result: 42% end-to-end efficiency improvement, 3.1x frame rate boost on Raspberry Pi 5, 2.98x speed-up on Raspberry Pi 5 CPU, with minimal accuracy loss (PA-MPJPE from 9.0mm to 9.1mm on FreiHAND).

Conclusion: The proposed framework achieves comparable accuracy to state-of-the-art methods while significantly enhancing computational efficiency for edge device deployment.

Abstract: With the increasing ubiquity of AR/VR devices, the deployment of deep
learning models on edge devices has become a critical challenge. These devices
require real-time inference, low power consumption, and minimal latency. Many
framework designers face the conundrum of balancing efficiency and performance.
We design a light framework that adopts an encoder-decoder architecture and
introduces several key contributions aimed at improving both efficiency and
accuracy. We apply sparse convolution on a ResNet-18 backbone to exploit the
inherent sparsity in hand pose images, achieving a 42% end-to-end efficiency
improvement. Moreover, we propose our SPLite decoder. This new architecture
significantly boosts the decoding process's frame rate by 3.1x on the Raspberry
Pi 5, while maintaining accuracy on par. To further optimize performance, we
apply quantization-aware training, reducing memory usage while preserving
accuracy (PA-MPJPE increases only marginally from 9.0 mm to 9.1 mm on
FreiHAND). Overall, our system achieves a 2.98x speed-up on a Raspberry Pi 5
CPU (BCM2712 quad-core Arm A76 processor). Our method is also evaluated on
compound benchmark datasets, demonstrating comparable accuracy to
state-of-the-art approaches while significantly enhancing computational
efficiency.

</details>


### [140] [REALM: An MLLM-Agent Framework for Open World 3D Reasoning Segmentation and Editing on Gaussian Splatting](https://arxiv.org/abs/2510.16410)
*Changyue Shi,Minghao Chen,Yiping Mao,Chuxiao Yang,Xinyuan Hu,Jiajun Ding,Zhou Yu*

Main category: cs.CV

TL;DR: REALM is an MLLM-agent framework that bridges 2D vision-language reasoning with 3D spatial understanding for open-world reasoning-based segmentation, using 3D Gaussian Splatting representations and a novel Global-to-Local Spatial Grounding strategy.


<details>
  <summary>Details</summary>
Motivation: Existing 3D segmentation methods struggle with ambiguous, reasoning-based instructions, while 2D vision-language models lack 3D spatial understanding. There's a need to bridge this gap for better 3D object grounding.

Method: Uses 3D Gaussian Splatting representations to render photorealistic views for MLLM comprehension. Implements Global-to-Local Spatial Grounding: multiple global views for coarse localization, then close-up novel views for fine-grained segmentation.

Result: Achieves remarkable performance in interpreting both explicit and implicit instructions across LERF, 3D-OVS, and REALM3D benchmarks. Supports various 3D interaction tasks including object removal, replacement, and style transfer.

Conclusion: REALM demonstrates practical utility and versatility as an agent framework for 3D interaction tasks, bridging the gap between complex human instructions and precise 3D object grounding without extensive 3D-specific post-training.

Abstract: Bridging the gap between complex human instructions and precise 3D object
grounding remains a significant challenge in vision and robotics. Existing 3D
segmentation methods often struggle to interpret ambiguous, reasoning-based
instructions, while 2D vision-language models that excel at such reasoning lack
intrinsic 3D spatial understanding. In this paper, we introduce REALM, an
innovative MLLM-agent framework that enables open-world reasoning-based
segmentation without requiring extensive 3D-specific post-training. We perform
segmentation directly on 3D Gaussian Splatting representations, capitalizing on
their ability to render photorealistic novel views that are highly suitable for
MLLM comprehension. As directly feeding one or more rendered views to the MLLM
can lead to high sensitivity to viewpoint selection, we propose a novel
Global-to-Local Spatial Grounding strategy. Specifically, multiple global views
are first fed into the MLLM agent in parallel for coarse-level localization,
aggregating responses to robustly identify the target object. Then, several
close-up novel views of the object are synthesized to perform fine-grained
local segmentation, yielding accurate and consistent 3D masks. Extensive
experiments show that REALM achieves remarkable performance in interpreting
both explicit and implicit instructions across LERF, 3D-OVS, and our newly
introduced REALM3D benchmarks. Furthermore, our agent framework seamlessly
supports a range of 3D interaction tasks, including object removal,
replacement, and style transfer, demonstrating its practical utility and
versatility. Project page: https://ChangyueShi.github.io/REALM.

</details>


### [141] [SSL4RL: Revisiting Self-supervised Learning as Intrinsic Reward for Visual-Language Reasoning](https://arxiv.org/abs/2510.16416)
*Xiaojun Guo,Runyu Zhou,Yifei Wang,Qi Zhang,Chenheng Zhang,Stefanie Jegelka,Xiaohan Wang,Jiajun Chai,Guojun Yin,Wei Lin,Yisen Wang*

Main category: cs.CV

TL;DR: SSL4RL is a framework that uses self-supervised learning tasks as verifiable rewards for reinforcement learning fine-tuning of vision-language models, addressing the challenge of inadequate visual evidence utilization.


<details>
  <summary>Details</summary>
Motivation: VLMs often fail to adequately use visual evidence, relying on linguistic priors or textual shortcuts. Traditional RL approaches face challenges due to lack of scalable and reliable reward mechanisms.

Method: Reformulates SSL objectives (predicting image rotation, reconstructing masked patches) into dense, automatic reward signals for RL-based fine-tuning, eliminating need for human preference data.

Result: Substantially improves performance on both vision-centric and vision-language reasoning benchmarks. Also demonstrates generality by applying to graph learning with significant gains.

Conclusion: SSL4RL establishes a versatile and effective paradigm for aligning multimodal models using verifiable, self-supervised objectives, with identified key factors influencing effectiveness.

Abstract: Vision-language models (VLMs) have shown remarkable abilities by integrating
large language models with visual inputs. However, they often fail to utilize
visual evidence adequately, either depending on linguistic priors in
vision-centric tasks or resorting to textual shortcuts during reasoning.
Although reinforcement learning (RL) can align models with desired behaviors,
its application to VLMs has been hindered by the lack of scalable and reliable
reward mechanisms. To overcome this challenge, we propose SSL4RL, a novel
framework that leverages self-supervised learning (SSL) tasks as a source of
verifiable rewards for RL-based fine-tuning. Our approach reformulates SSL
objectives-such as predicting image rotation or reconstructing masked
patches-into dense, automatic reward signals, eliminating the need for human
preference data or unreliable AI evaluators. Experiments show that SSL4RL
substantially improves performance on both vision-centric and vision-language
reasoning benchmarks. Furthermore, through systematic ablations, we identify
key factors-such as task difficulty, model scale, and semantic alignment with
the target domain-that influence the effectiveness of SSL4RL tasks, offering
new design principles for future work. We also demonstrate the framework's
generality by applying it to graph learning, where it yields significant gains.
SSL4RL establishes a versatile and effective paradigm for aligning multimodal
models using verifiable, self-supervised objectives.

</details>


### [142] [LightGlueStick: a Fast and Robust Glue for Joint Point-Line Matching](https://arxiv.org/abs/2510.16438)
*Aidyn Ubingazhibov,Rmi Pautrat,Iago Surez,Shaohui Liu,Marc Pollefeys,Viktor Larsson*

Main category: cs.CV

TL;DR: LightGlueStick is a lightweight neural network that simultaneously matches points and line segments across images, achieving state-of-the-art performance with improved efficiency for real-time applications.


<details>
  <summary>Details</summary>
Motivation: Traditional point and line matching are treated separately, and while GlueStick combined them, its heavy architecture prevented real-time use on edge devices. There's a need for efficient joint matching of both features.

Method: Uses a lightweight architecture with novel Attentional Line Message Passing (ALMP) that explicitly exposes line connectivity to enable efficient communication between nodes for simultaneous point and line matching.

Result: Establishes new state-of-the-art performance across different benchmarks while being computationally efficient enough for real-time applications and edge device deployment.

Conclusion: LightGlueStick successfully addresses the computational limitations of previous joint point-line matchers while maintaining superior matching performance, making it suitable for real-time computer vision applications.

Abstract: Lines and points are complementary local features, whose combination has
proven effective for applications such as SLAM and Structure-from-Motion. The
backbone of these pipelines are the local feature matchers, establishing
correspondences across images. Traditionally, point and line matching have been
treated as independent tasks. Recently, GlueStick proposed a GNN-based network
that simultaneously operates on points and lines to establish matches. While
running a single joint matching reduced the overall computational complexity,
the heavy architecture prevented real-time applications or deployment to edge
devices.
  Inspired by recent progress in point matching, we propose LightGlueStick, a
lightweight matcher for points and line segments. The key novel component in
our architecture is the Attentional Line Message Passing (ALMP), which
explicitly exposes the connectivity of the lines to the network, allowing for
efficient communication between nodes. In thorough experiments we show that
LightGlueStick establishes a new state-of-the-art across different benchmarks.
The code is available at https://github.com/aubingazhib/LightGlueStick.

</details>


### [143] [EDVD-LLaMA: Explainable Deepfake Video Detection via Multimodal Large Language Model Reasoning](https://arxiv.org/abs/2510.16442)
*Haoran Sun,Chen Cai,Huiping Zhuang,Kong Aik Lee,Lap-Pui Chau,Yi Wang*

Main category: cs.CV

TL;DR: Proposes EDVD-LLaMA, an explainable deepfake video detection framework using multimodal LLM with spatio-temporal feature extraction and fine-grained reasoning mechanisms to provide traceable detection with trustworthy explanations.


<details>
  <summary>Details</summary>
Motivation: Traditional deepfake detection methods lack transparency and generalization capabilities, creating need for detectors that provide verifiable reasoning explanations alongside detection results.

Method: Uses Spatio-Temporal Subtle Information Tokenization (ST-SIT) to extract cross-frame deepfake features, and Fine-grained Multimodal Chain-of-Thought (Fg-MCoT) with facial feature constraints for pixel-level localization and reliable reasoning.

Result: Achieves outstanding performance and robustness in detection accuracy, explainability, and cross-forgery/cross-dataset scenarios, outperforming previous deepfake detection methods.

Conclusion: EDVD-LLaMA provides a more explainable and superior solution for deepfake video detection with traceable reasoning processes and trustworthy explanations.

Abstract: The rapid development of deepfake video technology has not only facilitated
artistic creation but also made it easier to spread misinformation. Traditional
deepfake video detection (DVD) methods face issues such as a lack of
transparency in their principles and insufficient generalization capabilities
to cope with evolving forgery techniques. This highlights an urgent need for
detectors that can identify forged content and provide verifiable reasoning
explanations. This paper proposes the explainable deepfake video detection
(EDVD) task and designs the EDVD-LLaMA multimodal, a large language model
(MLLM) reasoning framework, which provides traceable reasoning processes
alongside accurate detection results and trustworthy explanations. Our approach
first incorporates a Spatio-Temporal Subtle Information Tokenization (ST-SIT)
to extract and fuse global and local cross-frame deepfake features, providing
rich spatio-temporal semantic information input for MLLM reasoning. Second, we
construct a Fine-grained Multimodal Chain-of-Thought (Fg-MCoT) mechanism, which
introduces facial feature data as hard constraints during the reasoning process
to achieve pixel-level spatio-temporal video localization, suppress
hallucinated outputs, and enhance the reliability of the chain of thought. In
addition, we build an Explainable Reasoning FF++ benchmark dataset
(ER-FF++set), leveraging structured data to annotate videos and ensure quality
control, thereby supporting dual supervision for reasoning and detection.
Extensive experiments demonstrate that EDVD-LLaMA achieves outstanding
performance and robustness in terms of detection accuracy, explainability, and
its ability to handle cross-forgery methods and cross-dataset scenarios.
Compared to previous DVD methods, it provides a more explainable and superior
solution. The source code and dataset will be publicly available.

</details>


### [144] [RefAtomNet++: Advancing Referring Atomic Video Action Recognition using Semantic Retrieval based Multi-Trajectory Mamba](https://arxiv.org/abs/2510.16444)
*Kunyu Peng,Di Wen,Jia Fu,Jiamin Wu,Kailun Yang,Junwei Zheng,Ruiping Liu,Yufan Chen,Yuqian Fu,Danda Pani Paudel,Luc Van Gool,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: The paper introduces RefAVA++, an extended dataset for Referring Atomic Video Action Recognition (RAVAR), and proposes RefAtomNet++, a novel framework that improves cross-modal alignment through multi-hierarchical semantic-aligned cross-attention and multi-trajectory Mamba modeling.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing methods in precisely localizing target persons and predicting fine-grained actions in complex multi-person scenarios, where conventional action recognition and detection tasks fall short in language-guided action understanding.

Method: RefAtomNet++ uses a multi-hierarchical semantic-aligned cross-attention mechanism combined with multi-trajectory Mamba modeling at partial-keyword, scene-attribute, and holistic-sentence levels. It dynamically selects nearest visual spatial tokens and aggregates spatial and temporal tokens across different semantic hierarchies.

Result: RefAtomNet++ establishes new state-of-the-art results on the RefAVA++ dataset, outperforming previous baselines including RefAtomNet and models from related domains like atomic action localization, video question answering, and text-video retrieval.

Conclusion: The proposed RefAtomNet++ framework effectively overcomes cross-modal alignment limitations and achieves superior performance in referring atomic video action recognition, with the extended RefAVA++ dataset providing a comprehensive benchmark for future research.

Abstract: Referring Atomic Video Action Recognition (RAVAR) aims to recognize
fine-grained, atomic-level actions of a specific person of interest conditioned
on natural language descriptions. Distinct from conventional action recognition
and detection tasks, RAVAR emphasizes precise language-guided action
understanding, which is particularly critical for interactive human action
analysis in complex multi-person scenarios. In this work, we extend our
previously introduced RefAVA dataset to RefAVA++, which comprises >2.9 million
frames and >75.1k annotated persons in total. We benchmark this dataset using
baselines from multiple related domains, including atomic action localization,
video question answering, and text-video retrieval, as well as our earlier
model, RefAtomNet. Although RefAtomNet surpasses other baselines by
incorporating agent attention to highlight salient features, its ability to
align and retrieve cross-modal information remains limited, leading to
suboptimal performance in localizing the target person and predicting
fine-grained actions. To overcome the aforementioned limitations, we introduce
RefAtomNet++, a novel framework that advances cross-modal token aggregation
through a multi-hierarchical semantic-aligned cross-attention mechanism
combined with multi-trajectory Mamba modeling at the partial-keyword,
scene-attribute, and holistic-sentence levels. In particular, scanning
trajectories are constructed by dynamically selecting the nearest visual
spatial tokens at each timestep for both partial-keyword and scene-attribute
levels. Moreover, we design a multi-hierarchical semantic-aligned
cross-attention strategy, enabling more effective aggregation of spatial and
temporal tokens across different semantic hierarchies. Experiments show that
RefAtomNet++ establishes new state-of-the-art results. The dataset and code are
released at https://github.com/KPeng9510/refAVA2.

</details>


### [145] [Enhancing Rotated Object Detection via Anisotropic Gaussian Bounding Box and Bhattacharyya Distance](https://arxiv.org/abs/2510.16445)
*Chien Thai,Mai Xuan Trang,Huong Ninh,Hoang Hiep Ly,Anh Son Le*

Main category: cs.CV

TL;DR: An improved loss function using Gaussian bounding box representation and Bhattacharyya distance for better rotated object detection accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional object detection frameworks underperform with rotated objects due to limitations in capturing orientation variations, especially in aerial imagery and autonomous driving applications.

Method: Proposes a rotation-invariant loss function using Gaussian bounding box representation and Bhattacharyya distance, with anisotropic Gaussian representation to handle square-like objects.

Result: Significant improvements in mean Average Precision metrics compared to existing methods when integrated into state-of-the-art rotated object detectors.

Conclusion: The approach establishes new benchmarks in rotated object detection with broad applications requiring precise object localization regardless of orientation.

Abstract: Detecting rotated objects accurately and efficiently is a significant
challenge in computer vision, particularly in applications such as aerial
imagery, remote sensing, and autonomous driving. Although traditional object
detection frameworks are effective for axis-aligned objects, they often
underperform in scenarios involving rotated objects due to their limitations in
capturing orientation variations. This paper introduces an improved loss
function aimed at enhancing detection accuracy and robustness by leveraging the
Gaussian bounding box representation and Bhattacharyya distance. In addition,
we advocate for the use of an anisotropic Gaussian representation to address
the issues associated with isotropic variance in square-like objects. Our
proposed method addresses these challenges by incorporating a
rotation-invariant loss function that effectively captures the geometric
properties of rotated objects. We integrate this proposed loss function into
state-of-the-art deep learning-based rotated object detection detectors, and
extensive experiments demonstrated significant improvements in mean Average
Precision metrics compared to existing methods. The results highlight the
potential of our approach to establish new benchmark in rotated object
detection, with implications for a wide range of applications requiring precise
and reliable object localization irrespective of orientation.

</details>


### [146] [VIPAMIN: Visual Prompt Initialization via Embedding Selection and Subspace Expansion](https://arxiv.org/abs/2510.16446)
*Jaekyun Park,Hye Won Chung*

Main category: cs.CV

TL;DR: VIPAMIN is a visual prompt initialization strategy that enhances adaptation of self-supervised models by aligning prompts with semantically informative regions and injecting novel representational directions, achieving state-of-the-art performance in visual prompt tuning.


<details>
  <summary>Details</summary>
Motivation: Existing visual prompt tuning methods often fail to specialize prompts or enrich representation space, especially with self-supervised backbones, and these limitations become critical in challenging tasks and data-scarce settings where effective adaptation is most needed.

Method: VIPAMIN enhances adaptation by (1) aligning prompts with semantically informative regions in the embedding space, and (2) injecting novel representational directions beyond the pretrained subspace, requiring only a single forward pass and lightweight operations.

Result: VIPAMIN consistently improves performance across diverse tasks and dataset sizes, setting a new state of the art in visual prompt tuning.

Conclusion: VIPAMIN provides an effective and efficient visual prompt initialization strategy that significantly enhances adaptation of self-supervised models, particularly in challenging scenarios where traditional methods struggle.

Abstract: In the era of large-scale foundation models, fully fine-tuning pretrained
networks for each downstream task is often prohibitively resource-intensive.
Prompt tuning offers a lightweight alternative by introducing tunable prompts
while keeping the backbone frozen. However, existing visual prompt tuning
methods often fail to specialize the prompts or enrich the representation
space--especially when applied to self-supervised backbones. We show that these
limitations become especially pronounced in challenging tasks and data-scarce
settings, where effective adaptation is most critical. In this work, we
introduce VIPAMIN, a visual prompt initialization strategy that enhances
adaptation of self-supervised models by (1) aligning prompts with semantically
informative regions in the embedding space, and (2) injecting novel
representational directions beyond the pretrained subspace. Despite its
simplicity--requiring only a single forward pass and lightweight
operations--VIPAMIN consistently improves performance across diverse tasks and
dataset sizes, setting a new state of the art in visual prompt tuning. Our code
is available at https://github.com/iamjaekyun/vipamin.

</details>


### [147] [Instance-Aware Pseudo-Labeling and Class-Focused Contrastive Learning for Weakly Supervised Domain Adaptive Segmentation of Electron Microscopy](https://arxiv.org/abs/2510.16450)
*Shan Xiong,Jiabao Chen,Ye Wang,Jialin Peng*

Main category: cs.CV

TL;DR: A weakly supervised domain adaptation method for mitochondria segmentation that uses sparse point labels and a multi-task framework with cross-teaching and instance-aware pseudo-label selection to reduce annotation costs while achieving performance close to supervised methods.


<details>
  <summary>Details</summary>
Motivation: To address the high annotation costs and domain shifts in mitochondria segmentation from EM images, while overcoming the low performance of unsupervised domain adaptation methods in practical applications.

Method: Multi-task learning framework combining segmentation and center detection with cross-teaching mechanism, class-focused cross-domain contrastive learning, and instance-aware pseudo-label selection strategy for self-training.

Result: Outperforms existing UDA and WDA methods, significantly narrowing the performance gap with supervised upper bound, and achieves substantial improvements over other UDA techniques.

Conclusion: The proposed weakly supervised approach effectively leverages sparse point annotations to achieve high-performance mitochondria segmentation with minimal annotation effort, making it valuable for biological and neuroscience research.

Abstract: Annotation-efficient segmentation of the numerous mitochondria instances from
various electron microscopy (EM) images is highly valuable for biological and
neuroscience research. Although unsupervised domain adaptation (UDA) methods
can help mitigate domain shifts and reduce the high costs of annotating each
domain, they typically have relatively low performance in practical
applications. Thus, we investigate weakly supervised domain adaptation (WDA)
that utilizes additional sparse point labels on the target domain, which
require minimal annotation effort and minimal expert knowledge. To take full
use of the incomplete and imprecise point annotations, we introduce a multitask
learning framework that jointly conducts segmentation and center detection with
a novel cross-teaching mechanism and class-focused cross-domain contrastive
learning. While leveraging unlabeled image regions is essential, we introduce
segmentation self-training with a novel instance-aware pseudo-label (IPL)
selection strategy. Unlike existing methods that typically rely on pixel-wise
pseudo-label filtering, the IPL semantically selects reliable and diverse
pseudo-labels with the help of the detection task. Comprehensive validations
and comparisons on challenging datasets demonstrate that our method outperforms
existing UDA and WDA methods, significantly narrowing the performance gap with
the supervised upper bound. Furthermore, under the UDA setting, our method also
achieves substantial improvements over other UDA techniques.

</details>


### [148] [NavQ: Learning a Q-Model for Foresighted Vision-and-Language Navigation](https://arxiv.org/abs/2510.16457)
*Peiran Xu,Xicheng Gong,Yadong MU*

Main category: cs.CV

TL;DR: Proposes a foresighted agent for goal-oriented VLN using Q-learning to predict future outcomes of actions, integrating task-agnostic Q-features with navigation instructions for improved decision-making.


<details>
  <summary>Details</summary>
Motivation: Existing VLN methods focus on historical information but overlook future implications of actions, leading to suboptimal long-term navigation decisions.

Method: Trains a Q-model using large-scale unlabeled trajectory data to learn scene layout and object relations, generates Q-features for candidate actions, integrates them with navigation instructions via cross-modal encoder, and uses A*-style search strategy.

Result: Extensive experiments on goal-oriented VLN datasets validate the method's effectiveness in improving navigation performance.

Conclusion: The proposed foresighted approach with Q-learning and future-aware decision-making significantly enhances goal-oriented VLN performance by considering long-term outcomes.

Abstract: In this work we concentrate on the task of goal-oriented Vision-and-Language
Navigation (VLN). Existing methods often make decisions based on historical
information, overlooking the future implications and long-term outcomes of the
actions. In contrast, we aim to develop a foresighted agent. Specifically, we
draw upon Q-learning to train a Q-model using large-scale unlabeled trajectory
data, in order to learn the general knowledge regarding the layout and object
relations within indoor scenes. This model can generate a Q-feature, analogous
to the Q-value in traditional Q-network, for each candidate action, which
describes the potential future information that may be observed after taking
the specific action. Subsequently, a cross-modal future encoder integrates the
task-agnostic Q-feature with navigation instructions to produce a set of action
scores reflecting future prospects. These scores, when combined with the
original scores based on history, facilitate an A*-style searching strategy to
effectively explore the regions that are more likely to lead to the
destination. Extensive experiments conducted on widely used goal-oriented VLN
datasets validate the effectiveness of the proposed method.

</details>


### [149] [HGC-Avatar: Hierarchical Gaussian Compression for Streamable Dynamic 3D Avatars](https://arxiv.org/abs/2510.16463)
*Haocheng Tang,Ruoke Yan,Xinhui Yin,Qi Zhang,Xinfeng Zhang,Siwei Ma,Wen Gao,Chuanmin Jia*

Main category: cs.CV

TL;DR: HGC-Avatar is a hierarchical Gaussian compression framework for efficient transmission and high-quality rendering of dynamic 3D avatars, addressing limitations of general 3DGS compression methods by incorporating human priors.


<details>
  <summary>Details</summary>
Motivation: Current 3DGS compression methods lack human priors, leading to suboptimal bitrate efficiency and reconstruction quality, which hinders their use in streamable 3D avatar systems for immersive communication.

Method: Disentangles Gaussian representation into structural layer (StyleUNet-based generator mapping poses to Gaussians) and motion layer (SMPL-X model for compact pose variations). Uses facial attention mechanism for identity preservation and supports layer-wise compression with progressive decoding.

Result: Significantly outperforms prior methods in both visual quality and compression efficiency, providing a streamable solution for rapid 3D avatar rendering.

Conclusion: HGC-Avatar enables efficient transmission and high-quality rendering of dynamic avatars with better compression efficiency and visual quality than existing approaches.

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have enabled fast,
photorealistic rendering of dynamic 3D scenes, showing strong potential in
immersive communication. However, in digital human encoding and transmission,
the compression methods based on general 3DGS representations are limited by
the lack of human priors, resulting in suboptimal bitrate efficiency and
reconstruction quality at the decoder side, which hinders their application in
streamable 3D avatar systems. We propose HGC-Avatar, a novel Hierarchical
Gaussian Compression framework designed for efficient transmission and
high-quality rendering of dynamic avatars. Our method disentangles the Gaussian
representation into a structural layer, which maps poses to Gaussians via a
StyleUNet-based generator, and a motion layer, which leverages the SMPL-X model
to represent temporal pose variations compactly and semantically. This
hierarchical design supports layer-wise compression, progressive decoding, and
controllable rendering from diverse pose inputs such as video sequences or
text. Since people are most concerned with facial realism, we incorporate a
facial attention mechanism during StyleUNet training to preserve identity and
expression details under low-bitrate constraints. Experimental results
demonstrate that HGC-Avatar provides a streamable solution for rapid 3D avatar
rendering, while significantly outperforming prior methods in both visual
quality and compression efficiency.

</details>


### [150] [PRISMM-Bench: A Benchmark of Peer-Review Grounded Multimodal Inconsistencies](https://arxiv.org/abs/2510.16505)
*Lukas Selch,Yufang Hou,M. Jehanzeb Mirza,Sivan Doveh,James Glass,Rogerio Feris,Wei Lin*

Main category: cs.CV

TL;DR: PRISMM-Bench is the first benchmark using real reviewer-flagged inconsistencies in scientific papers to evaluate Large Multimodal Models' ability to detect and resolve multimodal inconsistencies across text, figures, tables, and equations.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks overlook real-world multimodal inconsistencies in scientific papers, either isolating single modalities or using synthetic errors that fail to capture domain-specific complexity, undermining clarity, reproducibility, and trust.

Method: Created PRISMM-Bench through multi-stage pipeline: review mining, LLM-assisted filtering, and human verification to curate 262 inconsistencies from 242 papers. Designed three tasks (inconsistency identification, remedy, pair matching) and introduced structured JSON-based answer representations to minimize linguistic biases.

Result: Benchmarked 21 leading LMMs including large open-weight and proprietary models. Results show strikingly low performance (26.1-54.2%), highlighting the challenge of multimodal scientific reasoning.

Conclusion: Current LMMs struggle significantly with detecting and resolving real-world multimodal inconsistencies in scientific papers, motivating the need for progress towards more trustworthy scientific assistants.

Abstract: Large Multimodal Models (LMMs) are increasingly applied to scientific
research, yet it remains unclear whether they can reliably understand and
reason over the multimodal complexity of papers. A central challenge lies in
detecting and resolving inconsistencies across text, figures, tables, and
equations, issues that are often subtle, domain-specific, and ultimately
undermine clarity, reproducibility, and trust. Existing benchmarks overlook
this issue, either isolating single modalities or relying on synthetic errors
that fail to capture real-world complexity. We introduce PRISMM-Bench
(Peer-Review-sourced Inconsistency Set for Multimodal Models), the first
benchmark grounded in real reviewer-flagged inconsistencies in scientific
papers. Through a multi-stage pipeline of review mining, LLM-assisted filtering
and human verification, we curate 262 inconsistencies from 242 papers. Based on
this set, we design three tasks, namely inconsistency identification, remedy
and pair matching, which assess a model's capacity to detect, correct, and
reason over inconsistencies across different modalities. Furthermore, to
address the notorious problem of choice-only shortcuts in multiple-choice
evaluation, where models exploit answer patterns without truly understanding
the question, we further introduce structured JSON-based answer representations
that minimize linguistic biases by reducing reliance on superficial stylistic
cues. We benchmark 21 leading LMMs, including large open-weight models
(GLM-4.5V 106B, InternVL3 78B) and proprietary models (Gemini 2.5 Pro, GPT-5
with high reasoning). Results reveal strikingly low performance (26.1-54.2%),
underscoring the challenge of multimodal scientific reasoning and motivating
progress towards trustworthy scientific assistants.

</details>


### [151] [OOS-DSD: Improving Out-of-stock Detection in Retail Images using Auxiliary Tasks](https://arxiv.org/abs/2510.16508)
*Franko iki,Sven Lonari*

Main category: cs.CV

TL;DR: OOS-DSD is a novel deep learning method that enhances out-of-stock detection using auxiliary learning with YOLOv8, adding product segmentation and depth estimation branches to improve detection accuracy.


<details>
  <summary>Details</summary>
Motivation: Out-of-stock detection is crucial for retail verification, and existing methods need improvement in accurately detecting product unavailability on shelves.

Method: Extends YOLOv8 with additional convolutional branches for OOS detection, product segmentation, and depth estimation. Uses pseudo-labeled depth annotations from Depth Anything V2 with a proposed depth normalization procedure.

Result: Surpassed state-of-the-art OOS detection methods by 1.8% mAP. Ablation studies showed auxiliary learning increased mAP by 3.7% and depth normalization by 4.2%.

Conclusion: The proposed OOS-DSD method effectively improves OOS detection through auxiliary learning and depth normalization, demonstrating significant performance gains over existing approaches.

Abstract: Out-of-stock (OOS) detection is a very important retail verification process
that aims to infer the unavailability of products in their designated areas on
the shelf. In this paper, we introduce OOS-DSD, a novel deep learning-based
method that advances OOS detection through auxiliary learning. In particular,
we extend a well-established YOLOv8 object detection architecture with
additional convolutional branches to simultaneously detect OOS, segment
products, and estimate scene depth. While OOS detection and product
segmentation branches are trained using ground truth data, the depth estimation
branch is trained using pseudo-labeled annotations produced by the
state-of-the-art (SOTA) depth estimation model Depth Anything V2. Furthermore,
since the aforementioned pseudo-labeled depth estimates display relative depth,
we propose an appropriate depth normalization procedure that stabilizes the
training process. The experimental results show that the proposed method
surpassed the performance of the SOTA OOS detection methods by 1.8% of the mean
average precision (mAP). In addition, ablation studies confirm the
effectiveness of auxiliary learning and the proposed depth normalization
procedure, with the former increasing mAP by 3.7% and the latter by 4.2%.

</details>


### [152] [Image Categorization and Search via a GAT Autoencoder and Representative Models](https://arxiv.org/abs/2510.16514)
*Duygu Sap,Martin Lotz,Connor Mattinson*

Main category: cs.CV

TL;DR: A representative-centric image categorization and retrieval method using GAT-based autoencoders to create context-aware embeddings and category representatives for efficient comparison and retrieval.


<details>
  <summary>Details</summary>
Motivation: To develop an effective image categorization and retrieval system that leverages representative models and graph attention networks to capture contextual relationships between images.

Method: Uses graph structure with image nodes and similarity edges, applies GAT-based autoencoder to create context-aware latent representations, constructs category representatives from embeddings, and categorizes/retrieves images by comparing query representatives to category representatives.

Result: Demonstrated effectiveness through experiments comparing GAT autoencoders with standard feature-based techniques.

Conclusion: The representative-centric approach with GAT autoencoders provides an effective framework for image categorization and retrieval by leveraging contextual relationships and representative models.

Abstract: We propose a method for image categorization and retrieval that leverages
graphs and a graph attention network (GAT)-based autoencoder. Our approach is
representative-centric, that is, we execute the categorization and retrieval
process via the representative models we construct for the images and image
categories. We utilize a graph where nodes represent images (or their
representatives) and edges capture similarity relationships. GAT highlights
important features and relationships between images, enabling the autoencoder
to construct context-aware latent representations that capture the key features
of each image relative to its neighbors. We obtain category representatives
from these embeddings and categorize a query image by comparing its
representative to the category representatives. We then retrieve the most
similar image to the query image within its identified category. We demonstrate
the effectiveness of our representative-centric approach through experiments
with both the GAT autoencoders and standard feature-based techniques.

</details>


### [153] [Enhancing Compositional Reasoning in CLIP via Reconstruction and Alignment of Text Descriptions](https://arxiv.org/abs/2510.16540)
*Jihoon Kwon,Kyle Min,Jy-yong Sohn*

Main category: cs.CV

TL;DR: READ is a fine-tuning method that enhances compositional reasoning in vision-language models by adding token-level reconstruction and sentence-level alignment objectives to contrastive learning.


<details>
  <summary>Details</summary>
Motivation: Standard contrastive training in vision-language models struggles with compositional reasoning because text encoders focus on individual words rather than their relations, limiting understanding of structured relationships between visual and linguistic elements.

Method: READ adds two auxiliary objectives to contrastive learning: (1) token-level reconstruction using a frozen pre-trained decoder to reconstruct alternative captions, and (2) sentence-level alignment that explicitly aligns paraphrased sentences in embedding space.

Result: READ-CLIP achieves state-of-the-art performance across five major compositional reasoning benchmarks, outperforming the strongest conventional fine-tuning baseline by up to 4.1%. The method also improves performance when applied to existing CLIP variants like NegCLIP and FSC-CLIP.

Conclusion: The reconstruction and alignment objectives offer complementary benefits - reconstruction captures relationships between words within captions, while alignment ensures consistent representations for paraphrases with different wording, significantly enhancing compositional reasoning capabilities.

Abstract: Despite recent advances, vision-language models trained with standard
contrastive objectives still struggle with compositional reasoning -- the
ability to understand structured relationships between visual and linguistic
elements. This shortcoming is largely due to the tendency of the text encoder
to focus on individual words rather than their relations, a limitation
reinforced by contrastive training that primarily aligns words with visual
objects. In this paper, we introduce REconstruction and Alignment of text
Descriptions (READ), a fine-tuning method designed to enhance compositional
reasoning by adding two auxiliary objectives to the contrastive learning: (1) a
token-level reconstruction objective, where a frozen pre-trained decoder
reconstructs alternative captions based on the embedding of the original
caption; and (2) a sentence-level alignment objective, which explicitly aligns
paraphrased sentences in the embedding space. We show that READ-CLIP, a model
derived by applying the READ method to the pre-trained CLIP model, achieves the
state-of-the-art performance across five major compositional reasoning
benchmarks, outperforming the strongest conventional fine-tuning baseline by up
to 4.1%. Furthermore, applying the READ to existing CLIP variants (including
NegCLIP and FSC-CLIP) also improves performance on these benchmarks.
Quantitative and qualitative analyses reveal that our proposed objectives --
reconstruction and alignment -- offer complementary benefits: the former
encourages the encoder to capture relationships between words within a caption,
while the latter ensures consistent representations for paraphrases expressed
with different wording.

</details>


### [154] [Watch Where You Move: Region-aware Dynamic Aggregation and Excitation for Gait Recognition](https://arxiv.org/abs/2510.16541)
*Binyuan Huang,Yongdong Luo,Xianda Guo,Xiawu Zheng,Zheng Zhu,Jiahui Pan,Chengju Zhou*

Main category: cs.CV

TL;DR: GaitRDAE introduces a dynamic framework that automatically searches for motion regions, assigns adaptive temporal scales, and applies region-specific attention to improve gait recognition by better handling diverse behavior patterns and covariate effects.


<details>
  <summary>Details</summary>
Motivation: Existing gait recognition methods use predefined regions with fixed temporal scales, making it difficult to model dynamically changing motion regions and adapt to specific behavior patterns, especially when covariates affect visual appearance.

Method: Proposes Region-aware Dynamic Aggregation and Excitation framework (GaitRDAE) with two modules: RDA dynamically searches optimal temporal receptive fields for each region, and RDE emphasizes learning of motion regions with stable behavior patterns while suppressing attention to static regions affected by covariates.

Result: GaitRDAE achieves state-of-the-art performance on several benchmark datasets.

Conclusion: The proposed dynamic framework effectively handles diverse motion patterns and covariate effects in gait recognition through adaptive region modeling and attention mechanisms.

Abstract: Deep learning-based gait recognition has achieved great success in various
applications. The key to accurate gait recognition lies in considering the
unique and diverse behavior patterns in different motion regions, especially
when covariates affect visual appearance. However, existing methods typically
use predefined regions for temporal modeling, with fixed or equivalent temporal
scales assigned to different types of regions, which makes it difficult to
model motion regions that change dynamically over time and adapt to their
specific patterns. To tackle this problem, we introduce a Region-aware Dynamic
Aggregation and Excitation framework (GaitRDAE) that automatically searches for
motion regions, assigns adaptive temporal scales and applies corresponding
attention. Specifically, the framework includes two core modules: the
Region-aware Dynamic Aggregation (RDA) module, which dynamically searches the
optimal temporal receptive field for each region, and the Region-aware Dynamic
Excitation (RDE) module, which emphasizes the learning of motion regions
containing more stable behavior patterns while suppressing attention to static
regions that are more susceptible to covariates. Experimental results show that
GaitRDAE achieves state-of-the-art performance on several benchmark datasets.

</details>


### [155] [Fit for Purpose? Deepfake Detection in the Real World](https://arxiv.org/abs/2510.16556)
*Guangyu Lin,Li Lin,Christina P. Walker,Daniel S. Schiff,Shu Hu*

Main category: cs.CV

TL;DR: This paper introduces the first systematic benchmark using real-world political deepfakes from social media, revealing that current detection models perform poorly on authentic political content and are vulnerable to simple manipulations.


<details>
  <summary>Details</summary>
Motivation: The proliferation of AI-generated content, especially political deepfakes, poses significant misinformation risks, but existing detection models are trained on synthetic datasets and lack generalizability to real-world political deepfakes circulating on social media.

Method: Created the first systematic benchmark based on the Political Deepfakes Incident Database - a curated collection of real-world political deepfakes shared on social media since 2018, and conducted systematic evaluation of state-of-the-art deepfake detectors from academia, government, and industry.

Result: Academic and government detectors performed relatively poorly, while paid tools achieved higher performance than free-access models. However, all detectors struggled to generalize to authentic political deepfakes and were vulnerable to simple manipulations, especially in video content.

Conclusion: There is an urgent need for politically contextualized deepfake detection frameworks to better safeguard the public from real-world political deepfake threats.

Abstract: The rapid proliferation of AI-generated content, driven by advances in
generative adversarial networks, diffusion models, and multimodal large
language models, has made the creation and dissemination of synthetic media
effortless, heightening the risks of misinformation, particularly political
deepfakes that distort truth and undermine trust in political institutions. In
turn, governments, research institutions, and industry have strongly promoted
deepfake detection initiatives as solutions. Yet, most existing models are
trained and validated on synthetic, laboratory-controlled datasets, limiting
their generalizability to the kinds of real-world political deepfakes
circulating on social platforms that affect the public. In this work, we
introduce the first systematic benchmark based on the Political Deepfakes
Incident Database, a curated collection of real-world political deepfakes
shared on social media since 2018. Our study includes a systematic evaluation
of state-of-the-art deepfake detectors across academia, government, and
industry. We find that the detectors from academia and government perform
relatively poorly. While paid detection tools achieve relatively higher
performance than free-access models, all evaluated detectors struggle to
generalize effectively to authentic political deepfakes, and are vulnerable to
simple manipulations, especially in the video domain. Results urge the need for
politically contextualized deepfake detection frameworks to better safeguard
the public in real-world settings.

</details>


### [156] [SHIELD: Suppressing Hallucinations In LVLM Encoders via Bias and Vulnerability Defense](https://arxiv.org/abs/2510.16596)
*Yiyang Huang,Liang Shi,Yitian Zhang,Yi Xu,Yun Fu*

Main category: cs.CV

TL;DR: SHIELD is a training-free framework that mitigates object hallucinations in Large Vision-Language Models by addressing visual encoder issues through re-weighting visual tokens, introducing noise-derived tokens, and applying adversarial attacks with contrastive decoding.


<details>
  <summary>Details</summary>
Motivation: Object hallucination in LVLMs remains a significant challenge where models produce plausible but inaccurate object descriptions. Previous work focused on LLM components, but this paper identifies that hallucinations actually originate from visual encoders due to statistical bias, inherent bias, and vulnerability.

Method: Proposes SHIELD framework with three strategies: 1) re-weighting visual tokens to reduce statistical bias, 2) introducing noise-derived tokens to counter inherent bias, and 3) applying adversarial attacks with contrastive decoding to address vulnerability. The framework is training-free.

Result: SHIELD effectively mitigates object hallucinations across diverse benchmarks and LVLM families. It also achieves strong performance on general LVLM benchmarks, demonstrating broad applicability.

Conclusion: SHIELD successfully addresses object hallucination in LVLMs by targeting visual encoder issues rather than LLM components, providing an effective training-free solution with wide applicability across different model families and benchmarks.

Abstract: Large Vision-Language Models (LVLMs) excel in diverse cross-modal tasks.
However, object hallucination, where models produce plausible but inaccurate
object descriptions, remains a significant challenge. In contrast to previous
work focusing on LLM components, this paper is the first to trace LVLM
hallucinations to visual encoders and identifies three key issues: statistical
bias, inherent bias, and vulnerability. To address these challenges, we propose
SHIELD, a training-free framework that mitigates hallucinations through three
strategies: re-weighting visual tokens to reduce statistical bias, introducing
noise-derived tokens to counter inherent bias, and applying adversarial attacks
with contrastive decoding to address vulnerability. Experiments demonstrate
that SHIELD effectively mitigates object hallucinations across diverse
benchmarks and LVLM families. Moreover, SHIELD achieves strong performance on
the general LVLM benchmark, highlighting its broad applicability. Code will be
released.

</details>


### [157] [VisionSelector: End-to-End Learnable Visual Token Compression for Efficient Multimodal LLMs](https://arxiv.org/abs/2510.16598)
*Jiaying Zhu,Yurui Zhu,Xin Lu,Wenrui Yan,Dong Li,Kunlin Liu,Xueyang Fu,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: VisionSelector is a lightweight plug-and-play framework that reformulates token compression as an end-to-end learnable decision process for MLLMs, using a differentiable Top-K mechanism and curriculum annealing to enable efficient token selection at various compression rates.


<details>
  <summary>Details</summary>
Motivation: MLLMs face computational and memory bottlenecks from massive visual tokens in high-resolution images or multi-image inputs, and existing token compression techniques are constrained by heuristic rules that risk discarding critical information and suffer from biases like attention sinks.

Method: Proposes VisionSelector - a scorer module decoupled from MLLM backbone with differentiable Top-K mechanism and curriculum annealing strategy to bridge training-inference gap, enabling adaptive token selection at arbitrary compression rates with only 12.85M parameters.

Result: Achieves superior performance across all compression budgets: preserves 100% accuracy on MME with 30% retention budget, outperforms prior methods by 12.14% at 10% retention budget, and doubles prefill speed.

Conclusion: VisionSelector demonstrates effective generalization across various compression rates and adaptively identifies critical tokens, providing an efficient solution to computational bottlenecks in MLLMs while maintaining performance.

Abstract: Multimodal Large Language Models (MLLMs) encounter significant computational
and memory bottlenecks from the massive number of visual tokens generated by
high-resolution images or multi-image inputs. Previous token compression
techniques are often constrained by heuristic rules that risk discarding
critical information. They may suffer from biases, such as attention sinks,
that lead to sharp performance drops under aggressive compression ratios. To
address these limitations, we reformulate token compression as a lightweight
plug-and-play framework that reformulates token compression into an end-to-end
learnable decision process. To be specific, we propose VisionSelector, a scorer
module decoupled from the MLLM backbone that incorporates a differentiable
Top-K mechanism and a curriculum annealing strategy to bridge the
training-inference gap, enabling efficient and adaptive token selection various
arbitrary compression rates. Remarkably lightweight with only 12.85M trainable
parameters, VisionSelector demonstrates generalization across various
compression rates and adaptively identifying critical tokens. This leads to
superior performance across all compression budgets, evidenced by preserving
100% accuracy on MME with 30% retention budget, outperforming prior methods by
12.14% at 10% retention budget, and doubling prefill speed. Our code is
available at https://github.com/JulietChoo/VisionSelector .

</details>


### [158] [A Deep Learning Framework for Real-Time Image Processing in Medical Diagnostics: Enhancing Accuracy and Speed in Clinical Applications](https://arxiv.org/abs/2510.16611)
*Melika Filvantorkaman,Maral Filvan Torkaman*

Main category: cs.CV

TL;DR: A deep learning framework for real-time medical image analysis that integrates U-Net, EfficientNet, and Transformer models with optimization techniques to achieve high accuracy and fast inference for diagnostic workflows.


<details>
  <summary>Details</summary>
Motivation: Traditional medical image interpretation is time-consuming, variable among clinicians, and lacks the precision and speed needed for real-time clinical use. Current image processing techniques are insufficient for practical clinical applications.

Method: Combines advanced neural network architectures (U-Net, EfficientNet, Transformer) with real-time optimization strategies including model pruning, quantization, and GPU acceleration. Supports flexible deployment on edge devices, servers, and cloud infrastructure with clinical system integration.

Result: Achieved state-of-the-art performance with classification accuracies above 92%, segmentation Dice scores exceeding 91%, and inference times below 80 milliseconds on public benchmark datasets across X-ray, CT, and MRI modalities.

Conclusion: The framework can significantly accelerate diagnostic workflows, reduce clinician workload, and support trustworthy AI integration in time-critical healthcare environments through enhanced accuracy, efficiency, and interpretability.

Abstract: Medical imaging plays a vital role in modern diagnostics; however,
interpreting high-resolution radiological data remains time-consuming and
susceptible to variability among clinicians. Traditional image processing
techniques often lack the precision, robustness, and speed required for
real-time clinical use. To overcome these limitations, this paper introduces a
deep learning framework for real-time medical image analysis designed to
enhance diagnostic accuracy and computational efficiency across multiple
imaging modalities, including X-ray, CT, and MRI. The proposed system
integrates advanced neural network architectures such as U-Net, EfficientNet,
and Transformer-based models with real-time optimization strategies including
model pruning, quantization, and GPU acceleration. The framework enables
flexible deployment on edge devices, local servers, and cloud infrastructures,
ensuring seamless interoperability with clinical systems such as PACS and EHR.
Experimental evaluations on public benchmark datasets demonstrate
state-of-the-art performance, achieving classification accuracies above 92%,
segmentation Dice scores exceeding 91%, and inference times below 80
milliseconds. Furthermore, visual explanation tools such as Grad-CAM and
segmentation overlays enhance transparency and clinical interpretability. These
results indicate that the proposed framework can substantially accelerate
diagnostic workflows, reduce clinician workload, and support trustworthy AI
integration in time-critical healthcare environments.

</details>


### [159] [Self-Supervised Learning to Fly using Efficient Semantic Segmentation and Metric Depth Estimation for Low-Cost Autonomous UAVs](https://arxiv.org/abs/2510.16624)
*Sebastian Mocanu,Emil Slusanschi,Marius Leordeanu*

Main category: cs.CV

TL;DR: Vision-only autonomous flight system for small UAVs using semantic segmentation and monocular depth estimation for obstacle avoidance and safe landing without GPS or LiDAR.


<details>
  <summary>Details</summary>
Motivation: Enable autonomous drone navigation in indoor environments without expensive sensors like LiDAR or GPS dependency, addressing computational efficiency challenges for resource-constrained platforms.

Method: Combines semantic segmentation with monocular depth estimation using knowledge distillation (SVM teacher  lightweight U-Net student). Features adaptive scale factor algorithm for metric depth conversion and end-to-end learning for flight policies.

Result: Achieved 14.4 cm mean distance error, 100% success rate in 30 real-world and 100 digital-twin flight tests, increased surveillance distance, reduced mission time, and 87.5% autonomous mission success rate with learned policies.

Conclusion: Advances practical vision-based drone navigation in structured environments by solving metric depth estimation and computational efficiency challenges, enabling deployment on resource-constrained platforms.

Abstract: This paper presents a vision-only autonomous flight system for small UAVs
operating in controlled indoor environments. The system combines semantic
segmentation with monocular depth estimation to enable obstacle avoidance,
scene exploration, and autonomous safe landing operations without requiring GPS
or expensive sensors such as LiDAR. A key innovation is an adaptive scale
factor algorithm that converts non-metric monocular depth predictions into
accurate metric distance measurements by leveraging semantic ground plane
detection and camera intrinsic parameters, achieving a mean distance error of
14.4 cm. The approach uses a knowledge distillation framework where a
color-based Support Vector Machine (SVM) teacher generates training data for a
lightweight U-Net student network (1.6M parameters) capable of real-time
semantic segmentation. For more complex environments, the SVM teacher can be
replaced with a state-of-the-art segmentation model. Testing was conducted in a
controlled 5x4 meter laboratory environment with eight cardboard obstacles
simulating urban structures. Extensive validation across 30 flight tests in a
real-world environment and 100 flight tests in a digital-twin environment
demonstrates that the combined segmentation and depth approach increases the
distance traveled during surveillance and reduces mission time while
maintaining 100% success rates. The system is further optimized through
end-to-end learning, where a compact student neural network learns complete
flight policies from demonstration data generated by our best-performing
method, achieving an 87.5% autonomous mission success rate. This work advances
practical vision-based drone navigation in structured environments,
demonstrating solutions for metric depth estimation and computational
efficiency challenges that enable deployment on resource-constrained platforms.

</details>


### [160] [MultiVerse: A Multi-Turn Conversation Benchmark for Evaluating Large Vision and Language Models](https://arxiv.org/abs/2510.16641)
*Young-Jun Lee,Byung-Kwan Lee,Jianshu Zhang,Yechan Hwang,Byungsoo Ko,Han-Gyu Kim,Dongyu Yao,Xuankun Rong,Eojin Joo,Seung-Ho Han,Bowon Ko,Ho-Jin Choi*

Main category: cs.CV

TL;DR: MultiVerse is a challenging multi-turn conversation benchmark with 647 dialogues derived from 12 VLM evaluation benchmarks, covering 484 tasks across diverse topics. Evaluation of 18 VLMs shows even top models achieve only 50% success rate, highlighting the difficulty of complex multi-turn conversations.


<details>
  <summary>Details</summary>
Motivation: Real-world VLM applications require complex multi-turn dialogues, but existing datasets only partially capture the breadth and depth of conversational scenarios needed for robust evaluation.

Method: Created MultiVerse benchmark with 647 dialogues averaging 4 turns each, derived from 12 VLM evaluation benchmarks. Used checklist-based evaluation with GPT-4o as automated evaluator across 37 key aspects including perceptual accuracy, linguistic clarity, and factual correctness.

Result: Evaluation of 18 VLMs revealed that even the strongest models (e.g., GPT-4o) achieve only 50% success rate in complex multi-turn conversations. Providing full dialogue context significantly enhances performance for smaller/weaker models.

Conclusion: MultiVerse serves as a comprehensive landscape for evaluating multi-turn interaction abilities in VLMs, demonstrating significant challenges remain in complex conversational scenarios despite impressive single-turn performance.

Abstract: Vision-and-Language Models (VLMs) have shown impressive capabilities on
single-turn benchmarks, yet real-world applications often demand more intricate
multi-turn dialogues. Existing multi-turn datasets (e.g, MMDU, ConvBench) only
partially capture the breadth and depth of conversational scenarios encountered
by users. In this work, we introduce MultiVerse, a novel multi-turn
conversation benchmark featuring 647 dialogues - each averaging four turns -
derived from a diverse set of 12 popular VLM evaluation benchmarks. With 484
tasks and 484 interaction goals, MultiVerse covers a wide range of topics, from
factual knowledge and perception to advanced reasoning tasks such as
mathematics and coding. To facilitate robust assessment, we propose a
checklist-based evaluation method that leverages GPT-4o as the automated
evaluator, measuring performance across 37 key aspects, including perceptual
accuracy, linguistic clarity, and factual correctness. We evaluate 18 VLMs on
MultiVerse, revealing that even the strongest models (e.g., GPT-4o) achieve
only a 50% success rate in complex multi-turn conversations, highlighting the
dataset's challenging nature. Notably, we find that providing full dialogue
context significantly enhances performance for smaller or weaker models,
emphasizing the importance of in-context learning. We believe MultiVerse is a
landscape of evaluating multi-turn interaction abilities for VLMs.

</details>


### [161] [Structured Interfaces for Automated Reasoning with 3D Scene Graphs](https://arxiv.org/abs/2510.16643)
*Aaron Ray,Jacob Arkin,Harel Biggie,Chuchu Fan,Luca Carlone,Nicholas Roy*

Main category: cs.CV

TL;DR: Using Retrieval Augmented Generation with Cypher query language to efficiently ground natural language in large 3D scene graphs, overcoming scalability limitations of traditional context window methods.


<details>
  <summary>Details</summary>
Motivation: Traditional methods encode entire 3D scene graphs as text in LLM context windows, which doesn't scale to large or rich graphs. Need more efficient approach for language grounding in robotics.

Method: Proposed using Retrieval Augmented Generation with graph database encoding. LLM uses Cypher query language as a tool to retrieve relevant subsets of 3DSG data for language grounding tasks.

Result: Cypher interface scales significantly better to large graphs on both local and cloud models. Large performance improvements in grounded language tasks while substantially reducing token count of scene graph content.

Conclusion: Using Cypher as interface to 3D scene graphs provides scalable solution for language grounding, outperforming baseline context window and code generation methods.

Abstract: In order to provide a robot with the ability to understand and react to a
user's natural language inputs, the natural language must be connected to the
robot's underlying representations of the world. Recently, large language
models (LLMs) and 3D scene graphs (3DSGs) have become a popular choice for
grounding natural language and representing the world. In this work, we address
the challenge of using LLMs with 3DSGs to ground natural language. Existing
methods encode the scene graph as serialized text within the LLM's context
window, but this encoding does not scale to large or rich 3DSGs. Instead, we
propose to use a form of Retrieval Augmented Generation to select a subset of
the 3DSG relevant to the task. We encode a 3DSG in a graph database and provide
a query language interface (Cypher) as a tool to the LLM with which it can
retrieve relevant data for language grounding. We evaluate our approach on
instruction following and scene question-answering tasks and compare against
baseline context window and code generation methods. Our results show that
using Cypher as an interface to 3D scene graphs scales significantly better to
large, rich graphs on both local and cloud-based models. This leads to large
performance improvements in grounded language tasks while also substantially
reducing the token count of the scene graph content. A video supplement is
available at https://www.youtube.com/watch?v=zY_YI9giZSA.

</details>


### [162] [Universal and Transferable Attacks on Pathology Foundation Models](https://arxiv.org/abs/2510.16660)
*Yuntian Wang,Xilin Yang,Che-Yung Shen,Nir Pillar,Aydogan Ozcan*

Main category: cs.CV

TL;DR: UTAP introduces universal and transferable adversarial perturbations that can systematically disrupt pathology foundation models' feature representations, causing performance drops across various downstream tasks and models with imperceptible noise patterns.


<details>
  <summary>Details</summary>
Motivation: To reveal critical vulnerabilities in pathology foundation models and establish a high-standard benchmark for model robustness evaluation, highlighting the need for better defense mechanisms in AI pathology applications.

Method: Optimized deep learning approach to create fixed, weak noise patterns that are added to pathology images, systematically disrupting feature representations across multiple foundation models.

Result: UTAP successfully degraded performance of various state-of-the-art pathology foundation models across multiple datasets, causing significant performance drops with visually imperceptible modifications. The perturbations demonstrated universality across diverse field-of-views and transferability to external black-box models.

Conclusion: UTAP constitutes a broad threat to emerging pathology foundation models and their applications, establishing a critical benchmark for robustness evaluation and highlighting the urgent need for advancing defense mechanisms and adversarial training for safe AI deployment in pathology.

Abstract: We introduce Universal and Transferable Adversarial Perturbations (UTAP) for
pathology foundation models that reveal critical vulnerabilities in their
capabilities. Optimized using deep learning, UTAP comprises a fixed and weak
noise pattern that, when added to a pathology image, systematically disrupts
the feature representation capabilities of multiple pathology foundation
models. Therefore, UTAP induces performance drops in downstream tasks that
utilize foundation models, including misclassification across a wide range of
unseen data distributions. In addition to compromising the model performance,
we demonstrate two key features of UTAP: (1) universality: its perturbation can
be applied across diverse field-of-views independent of the dataset that UTAP
was developed on, and (2) transferability: its perturbation can successfully
degrade the performance of various external, black-box pathology foundation
models - never seen before. These two features indicate that UTAP is not a
dedicated attack associated with a specific foundation model or image dataset,
but rather constitutes a broad threat to various emerging pathology foundation
models and their applications. We systematically evaluated UTAP across various
state-of-the-art pathology foundation models on multiple datasets, causing a
significant drop in their performance with visually imperceptible modifications
to the input images using a fixed noise pattern. The development of these
potent attacks establishes a critical, high-standard benchmark for model
robustness evaluation, highlighting a need for advancing defense mechanisms and
potentially providing the necessary assets for adversarial training to ensure
the safe and reliable deployment of AI in pathology.

</details>


### [163] [HYDRA: HYbrid knowledge Distillation and spectral Reconstruction Algorithm for high channel hyperspectral camera applications](https://arxiv.org/abs/2510.16664)
*Christopher Thirgood,Oscar Mendez,Erin Ling,Jon Storey,Simon Hadfield*

Main category: cs.CV

TL;DR: HYDRA is a novel spectral reconstruction method that uses hybrid knowledge distillation to reconstruct hyperspectral images from RGB images, achieving state-of-the-art performance with 18% accuracy improvement and faster inference.


<details>
  <summary>Details</summary>
Motivation: Previous multi-scale attention methods only work well for sparse spectra, while modern hyperspectral sensors have hundreds of channels, requiring better generalizable spectral reconstruction approaches.

Method: Uses a Teacher model that encodes latent hyperspectral data and a Student model that learns mappings from RGB images to the Teacher's encoded domain, with a novel training method for knowledge distillation.

Result: Achieves SOTA performance across all metrics with 18% boost in accuracy and faster inference times than current SOTA models at various channel depths.

Conclusion: HYDRA effectively addresses limitations of prior spectral reconstruction models and provides high-quality spectral reconstruction for modern hyperspectral imaging applications.

Abstract: Hyperspectral images (HSI) promise to support a range of new applications in
computer vision. Recent research has explored the feasibility of generalizable
Spectral Reconstruction (SR), the problem of recovering a HSI from a natural
three-channel color image in unseen scenarios.
  However, previous Multi-Scale Attention (MSA) works have only demonstrated
sufficient generalizable results for very sparse spectra, while modern HSI
sensors contain hundreds of channels.
  This paper introduces a novel approach to spectral reconstruction via our
HYbrid knowledge Distillation and spectral Reconstruction Architecture (HYDRA).
  Using a Teacher model that encapsulates latent hyperspectral image data and a
Student model that learns mappings from natural images to the Teacher's encoded
domain, alongside a novel training method, we achieve high-quality spectral
reconstruction.
  This addresses key limitations of prior SR models, providing SOTA performance
across all metrics, including an 18\% boost in accuracy, and faster inference
times than current SOTA models at various channel depths.

</details>


### [164] [Pursuing Minimal Sufficiency in Spatial Reasoning](https://arxiv.org/abs/2510.16688)
*Yejie Guo,Yunzhong Hou,Wufei Ma,Meng Tang,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: MSSR is a dual-agent framework that improves spatial reasoning in VLMs by first extracting sufficient 3D information from expert models and then iteratively refining it to achieve minimality, addressing bottlenecks in 3D understanding and redundant information.


<details>
  <summary>Details</summary>
Motivation: Spatial reasoning remains challenging for Vision-Language Models due to inadequate 3D understanding from 2D-centric pre-training and reasoning failures from redundant 3D information.

Method: A dual-agent framework with Perception Agent that queries 3D scenes using perception toolbox (including novel SOG module) to extract sufficient information, and Reasoning Agent that iteratively refines information to achieve minimality by pruning redundancies and requesting missing details.

Result: Significantly improves accuracy and achieves state-of-the-art performance across two challenging benchmarks, while producing interpretable reasoning paths.

Conclusion: The explicit pursuit of both sufficiency and minimality in 3D information processing effectively addresses spatial reasoning challenges and provides high-quality training data for future models.

Abstract: Spatial reasoning, the ability to ground language in 3D understanding,
remains a persistent challenge for Vision-Language Models (VLMs). We identify
two fundamental bottlenecks: inadequate 3D understanding capabilities stemming
from 2D-centric pre-training, and reasoning failures induced by redundant 3D
information. To address these, we first construct a Minimal Sufficient Set
(MSS) of information before answering a given question: a compact selection of
3D perception results from \textit{expert models}. We introduce MSSR (Minimal
Sufficient Spatial Reasoner), a dual-agent framework that implements this
principle. A Perception Agent programmatically queries 3D scenes using a
versatile perception toolbox to extract sufficient information, including a
novel SOG (Situated Orientation Grounding) module that robustly extracts
language-grounded directions. A Reasoning Agent then iteratively refines this
information to pursue minimality, pruning redundant details and requesting
missing ones in a closed loop until the MSS is curated. Extensive experiments
demonstrate that our method, by explicitly pursuing both sufficiency and
minimality, significantly improves accuracy and achieves state-of-the-art
performance across two challenging benchmarks. Furthermore, our framework
produces interpretable reasoning paths, offering a promising source of
high-quality training data for future models. Source code is available at
https://github.com/gyj155/mssr.

</details>


### [165] [SDPA++: A General Framework for Self-Supervised Denoising with Patch Aggregation](https://arxiv.org/abs/2510.16702)
*Huy Minh Nhat Nguyen,Triet Hoang Minh Dao,Chau Vinh Hoang Truong,Cuong Tuan Nguyen*

Main category: cs.CV

TL;DR: SDPA++ is a self-supervised denoising framework for OCT images that uses only noisy images to generate pseudo-ground-truth through self-fusion and trains ensemble models with patch aggregation, achieving improved image quality without clean reference data.


<details>
  <summary>Details</summary>
Motivation: Acquiring paired datasets of clean and noisy OCT images is challenging due to intrinsic speckle noise and clinical constraints, making supervised denoising difficult in real-world scenarios.

Method: Leverages only noisy OCT images to generate pseudo-ground-truth through self-fusion and self-supervised denoising, then trains an ensemble of denoising models using a patch-based aggregation strategy.

Result: Validated on real-world VIP Cup dataset, showing improvements in Contrast-to-Noise Ratio (CNR), Mean Square Ratio (MSR), Texture Preservation (TP), and Edge Preservation (EP) metrics.

Conclusion: The method demonstrates potential for improving OCT image quality and diagnostic outcomes in clinical practice without requiring clean reference images.

Abstract: Optical Coherence Tomography (OCT) is a widely used non-invasive imaging
technique that provides detailed three-dimensional views of the retina, which
are essential for the early and accurate diagnosis of ocular diseases.
Consequently, OCT image analysis and processing have emerged as key research
areas in biomedical imaging. However, acquiring paired datasets of clean and
real-world noisy OCT images for supervised denoising models remains a
formidable challenge due to intrinsic speckle noise and practical constraints
in clinical imaging environments. To address these issues, we propose SDPA++: A
General Framework for Self-Supervised Denoising with Patch Aggregation. Our
novel approach leverages only noisy OCT images by first generating
pseudo-ground-truth images through self-fusion and self-supervised denoising.
These refined images then serve as targets to train an ensemble of denoising
models using a patch-based strategy that effectively enhances image clarity.
Performance improvements are validated via metrics such as Contrast-to-Noise
Ratio (CNR), Mean Square Ratio (MSR), Texture Preservation (TP), and Edge
Preservation (EP) on the real-world dataset from the IEEE SPS Video and Image
Processing Cup. Notably, the VIP Cup dataset contains only real-world noisy OCT
images without clean references, highlighting our method's potential for
improving image quality and diagnostic outcomes in clinical practice.

</details>


### [166] [Connecting Domains and Contrasting Samples: A Ladder for Domain Generalization](https://arxiv.org/abs/2510.16704)
*Tianxin Wei,Yifan Chen,Xinrui He,Wenxuan Bao,Jingrui He*

Main category: cs.CV

TL;DR: The paper proposes Domain-Connecting Contrastive Learning (DCCL) to address domain generalization challenges by improving intra-class connectivity across domains through enhanced data augmentation, cross-domain positive samples, and model anchoring techniques.


<details>
  <summary>Details</summary>
Motivation: Distribution shifts between training and testing samples hinder model generalization. While contrastive learning seems promising for domain generalization, direct application actually deteriorates performance due to lack of intra-class connectivity across domains.

Method: DCCL introduces aggressive data augmentation and cross-domain positive samples to improve intra-class connectivity. It also uses model anchoring to exploit pre-trained representations and generative transformation loss to better embed unseen test domains.

Result: Extensive experiments on five standard DG benchmarks show that DCCL outperforms state-of-the-art baselines even without domain supervision.

Conclusion: DCCL effectively addresses the domain generalization problem by enhancing conceptual connectivity across domains through a novel contrastive learning paradigm that improves intra-class connectivity.

Abstract: Distribution shifts between training and testing samples frequently occur in
practice and impede model generalization performance. This crucial challenge
thereby motivates studies on domain generalization (DG), which aim to predict
the label on unseen target domain data by solely using data from source
domains. It is intuitive to conceive the class-separated representations
learned in contrastive learning (CL) are able to improve DG, while the reality
is quite the opposite: users observe directly applying CL deteriorates the
performance. We analyze the phenomenon with the insights from CL theory and
discover lack of intra-class connectivity in the DG setting causes the
deficiency. We thus propose a new paradigm, domain-connecting contrastive
learning (DCCL), to enhance the conceptual connectivity across domains and
obtain generalizable representations for DG. On the data side, more aggressive
data augmentation and cross-domain positive samples are introduced to improve
intra-class connectivity. On the model side, to better embed the unseen test
domains, we propose model anchoring to exploit the intra-class connectivity in
pre-trained representations and complement the anchoring with generative
transformation loss. Extensive experiments on five standard DG benchmarks are
performed. The results verify that DCCL outperforms state-of-the-art baselines
even without domain supervision. The detailed model implementation and the code
are provided through https://github.com/weitianxin/DCCL

</details>


### [167] [HumanCM: One Step Human Motion Prediction](https://arxiv.org/abs/2510.16709)
*Liu Haojie,Gao Suixiang*

Main category: cs.CV

TL;DR: HumanCM is a one-step human motion prediction framework using consistency models that achieves comparable accuracy to diffusion models with significantly faster inference.


<details>
  <summary>Details</summary>
Motivation: To overcome the computational inefficiency of multi-step denoising in diffusion-based motion prediction methods by enabling efficient single-step generation.

Method: Uses consistency models to learn self-consistent mapping between noisy and clean motion states, employing Transformer-based spatiotemporal architecture with temporal embeddings for long-range dependencies and motion coherence.

Result: Achieves comparable or superior accuracy to state-of-the-art diffusion models on Human3.6M and HumanEva-I datasets while reducing inference steps by up to two orders of magnitude.

Conclusion: HumanCM provides an efficient alternative to diffusion-based motion prediction with similar accuracy but dramatically faster inference through one-step generation.

Abstract: We present HumanCM, a one-step human motion prediction framework built upon
consistency models. Instead of relying on multi-step denoising as in
diffusion-based methods, HumanCM performs efficient single-step generation by
learning a self-consistent mapping between noisy and clean motion states. The
framework adopts a Transformer-based spatiotemporal architecture with temporal
embeddings to model long-range dependencies and preserve motion coherence.
Experiments on Human3.6M and HumanEva-I demonstrate that HumanCM achieves
comparable or superior accuracy to state-of-the-art diffusion models while
reducing inference steps by up to two orders of magnitude.

</details>


### [168] [Eliciting Grounded Chain-of-Thought Reasoning in 3D Scenes](https://arxiv.org/abs/2510.16714)
*Xiongkun Linghu,Jiangyong Huang,Ziyu Zhu,Baoxiong Jia,Siyuan Huang*

Main category: cs.CV

TL;DR: This paper introduces SCENECOT, a novel framework for grounded 3D scene reasoning using Chain-of-Thought methods, along with SCENECOT-185K dataset, achieving strong performance in complex 3D scene understanding.


<details>
  <summary>Details</summary>
Motivation: Existing 3D LLMs struggle with grounded question-answering due to under-exploration of human-like scene-object grounded reasoning mechanisms.

Method: Proposes SCENECOT framework with grounded Chain-of-Thought reasoning that decouples complex tasks into simpler problems and builds visual clues using multimodal expert modules, supported by the SCENECOT-185K dataset.

Result: Extensive experiments show strong performance across various 3D scene reasoning benchmarks with high grounding-QA coherence.

Conclusion: First successful application of CoT reasoning to 3D scene understanding, enabling step-by-step human-like reasoning with potential for broader 3D scene understanding applications.

Abstract: Existing research on 3D Large Language Models (LLMs) still struggles to
achieve grounded question-answering, primarily due to the under-exploration of
the mech- anism of human-like scene-object grounded reasoning. This paper
bridges the gap by presenting a novel framework. We first introduce a grounded
Chain-of- Thought reasoning method in 3D scenes (SCENECOT), decoupling a
complex reasoning task into simpler and manageable problems, and building
corresponding visual clues based on multimodal expert modules. To enable such a
method, we develop SCENECOT-185K, the first large-scale grounded CoT reasoning
dataset, consisting of 185K high-quality instances. Extensive experiments
across various complex 3D scene reasoning benchmarks demonstrate that our new
framework achieves strong performance with high grounding-QA coherence. To the
best of our knowledge, this is the first successful application of CoT
reasoning to 3D scene understanding, enabling step-by-step human-like reasoning
and showing potential for extension to broader 3D scene understanding
scenarios.

</details>


### [169] [Vision-Centric 4D Occupancy Forecasting and Planning via Implicit Residual World Models](https://arxiv.org/abs/2510.16729)
*Jianbiao Mei,Yu Yang,Xuemeng Yang,Licheng Wen,Jiajun Lv,Botian Shi,Yong Liu*

Main category: cs.CV

TL;DR: IR-WM is an Implicit Residual World Model that focuses on modeling dynamic changes in autonomous driving scenes rather than full scene reconstruction, using BEV representations and residual prediction to improve efficiency and planning accuracy.


<details>
  <summary>Details</summary>
Motivation: Current vision-centric world models for autonomous driving inefficiently reconstruct entire future scenes, including static backgrounds, which wastes computational capacity. The goal is to focus modeling efforts on the actual changes in the world state.

Method: IR-WM first creates a BEV representation of the current state, then uses previous BEV features as temporal prior to predict only residual changes conditioned on ego-vehicle actions and scene context. An alignment module addresses error accumulation, and various forecasting-planning coupling schemes are explored.

Result: On the nuScenes benchmark, IR-WM achieves top performance in both 4D occupancy forecasting and trajectory planning, demonstrating that implicit future states from world models substantially improve planning accuracy.

Conclusion: The proposed residual modeling approach effectively focuses computational resources on dynamic changes rather than static backgrounds, leading to improved efficiency and performance in autonomous driving world modeling and planning tasks.

Abstract: End-to-end autonomous driving systems increasingly rely on vision-centric
world models to understand and predict their environment. However, a common
ineffectiveness in these models is the full reconstruction of future scenes,
which expends significant capacity on redundantly modeling static backgrounds.
To address this, we propose IR-WM, an Implicit Residual World Model that
focuses on modeling the current state and evolution of the world. IR-WM first
establishes a robust bird's-eye-view representation of the current state from
the visual observation. It then leverages the BEV features from the previous
timestep as a strong temporal prior and predicts only the "residual", i.e., the
changes conditioned on the ego-vehicle's actions and scene context. To
alleviate error accumulation over time, we further apply an alignment module to
calibrate semantic and dynamic misalignments. Moreover, we investigate
different forecasting-planning coupling schemes and demonstrate that the
implicit future state generated by world models substantially improves planning
accuracy. On the nuScenes benchmark, IR-WM achieves top performance in both 4D
occupancy forecasting and trajectory planning.

</details>


### [170] [UKANFormer: Noise-Robust Semantic Segmentation for Coral Reef Mapping via a Kolmogorov-Arnold Network-Transformer Hybrid](https://arxiv.org/abs/2510.16730)
*Tianyang Dou,Ming Li,Jiangying Qin,Xuan Liao,Jiageng Zhong,Armin Gruen,Mengyi Deng*

Main category: cs.CV

TL;DR: UKANFormer is a semantic segmentation model that achieves high-precision coral reef mapping using noisy supervision from Allen Coral Atlas, outperforming conventional methods and producing more accurate predictions than the noisy training labels.


<details>
  <summary>Details</summary>
Motivation: Global coral reef mapping products like Allen Coral Atlas have limited spatial precision and semantic consistency, especially for fine-grained boundary delineation, requiring improved methods for accurate large-scale conservation mapping.

Method: UKANFormer builds on UKAN architecture and incorporates a Global-Local Transformer (GL-Trans) block in the decoder to extract both global semantic structures and local boundary details from noisy supervision.

Result: Achieved coral-class IoU of 67.00% and pixel accuracy of 83.98%, outperforming conventional baselines under the same noisy labels setting. The model produces predictions that are visually and structurally more accurate than the noisy training labels.

Conclusion: Architectural design can mitigate label noise and support scalable mapping under imperfect supervision, challenging the notion that data quality directly limits model performance. UKANFormer provides foundation for ecological monitoring where reliable labels are scarce.

Abstract: Coral reefs are vital yet fragile ecosystems that require accurate
large-scale mapping for effective conservation. Although global products such
as the Allen Coral Atlas provide unprecedented coverage of global coral reef
distri-bution, their predictions are frequently limited in spatial precision
and semantic consistency, especially in regions requiring fine-grained boundary
delineation. To address these challenges, we propose UKANFormer, a novel
se-mantic segmentation model designed to achieve high-precision mapping under
noisy supervision derived from Allen Coral Atlas. Building upon the UKAN
architecture, UKANFormer incorporates a Global-Local Transformer (GL-Trans)
block in the decoder, enabling the extraction of both global semantic
structures and local boundary details. In experiments, UKANFormer achieved a
coral-class IoU of 67.00% and pixel accuracy of 83.98%, outperforming
conventional baselines under the same noisy labels setting. Remarkably, the
model produces predictions that are visually and structurally more accurate
than the noisy labels used for training. These results challenge the notion
that data quality directly limits model performance, showing that architectural
design can mitigate label noise and sup-port scalable mapping under imperfect
supervision. UKANFormer provides a foundation for ecological monitoring where
reliable labels are scarce.

</details>


### [171] [A Comprehensive Survey on World Models for Embodied AI](https://arxiv.org/abs/2510.16732)
*Xinqing Li,Xin He,Le Zhang,Yun Liu*

Main category: cs.CV

TL;DR: This survey presents a unified framework for world models in embodied AI, proposing a three-axis taxonomy covering functionality, temporal modeling, and spatial representation. It systematizes resources and metrics, compares state-of-the-art models, and identifies key challenges including dataset scarcity, computational efficiency trade-offs, and long-horizon temporal consistency.


<details>
  <summary>Details</summary>
Motivation: Embodied AI requires agents that can perceive, act, and anticipate how actions reshape future world states. World models serve as internal simulators that capture environment dynamics to support perception, prediction, and decision making.

Method: The authors propose a unified framework with a three-axis taxonomy: (1) Functionality (Decision-Coupled vs. General-Purpose), (2) Temporal Modeling (Sequential Simulation vs. Global Difference Prediction), and (3) Spatial Representation (Global Latent Vector, Token Feature Sequence, Spatial Latent Grid, Decomposed Rendering). They systematize data resources and metrics across robotics, autonomous driving, and general video settings.

Result: The survey provides a quantitative comparison of state-of-the-art world models and identifies current limitations in the field. It covers evaluation metrics for pixel prediction quality, state-level understanding, and task performance.

Conclusion: Key open challenges include the scarcity of unified datasets, the need for evaluation metrics that assess physical consistency over pixel fidelity, the trade-off between model performance and computational efficiency for real-time control, and the difficulty of achieving long-horizon temporal consistency while mitigating error accumulation.

Abstract: Embodied AI requires agents that perceive, act, and anticipate how actions
reshape future world states. World models serve as internal simulators that
capture environment dynamics, enabling forward and counterfactual rollouts to
support perception, prediction, and decision making. This survey presents a
unified framework for world models in embodied AI. Specifically, we formalize
the problem setting and learning objectives, and propose a three-axis taxonomy
encompassing: (1) Functionality, Decision-Coupled vs. General-Purpose; (2)
Temporal Modeling, Sequential Simulation and Inference vs. Global Difference
Prediction; (3) Spatial Representation, Global Latent Vector, Token Feature
Sequence, Spatial Latent Grid, and Decomposed Rendering Representation. We
systematize data resources and metrics across robotics, autonomous driving, and
general video settings, covering pixel prediction quality, state-level
understanding, and task performance. Furthermore, we offer a quantitative
comparison of state-of-the-art models and distill key open challenges,
including the scarcity of unified datasets and the need for evaluation metrics
that assess physical consistency over pixel fidelity, the trade-off between
model performance and the computational efficiency required for real-time
control, and the core modeling difficulty of achieving long-horizon temporal
consistency while mitigating error accumulation. Finally, we maintain a curated
bibliography at https://github.com/Li-Zn-H/AwesomeWorldModels.

</details>


### [172] [Visual Autoregressive Models Beat Diffusion Models on Inference Time Scaling](https://arxiv.org/abs/2510.16751)
*Erik Riise,Mehmet Onurcan Kaya,Dim P. Papadopoulos*

Main category: cs.CV

TL;DR: Beam search significantly improves text-to-image generation in autoregressive models, enabling a 2B parameter model to outperform a 12B diffusion model by leveraging discrete token spaces for early pruning and computational reuse.


<details>
  <summary>Details</summary>
Motivation: While search strategies have greatly improved LLMs, similar gains haven't translated well to image generation, with diffusion models showing limited benefits from search techniques.

Method: Applied beam search to discrete, sequential visual autoregressive models, leveraging the discrete token space for early pruning and computational reuse during inference.

Result: A 2B parameter autoregressive model with beam search outperformed a 12B parameter diffusion model across benchmarks, with systematic ablations confirming the advantage comes from discrete token spaces.

Conclusion: Model architecture (not just scale) is critical for inference-time optimization in visual generation, with discrete autoregressive models enabling effective search strategies that diffusion models cannot match.

Abstract: While inference-time scaling through search has revolutionized Large Language
Models, translating these gains to image generation has proven difficult.
Recent attempts to apply search strategies to continuous diffusion models show
limited benefits, with simple random sampling often performing best. We
demonstrate that the discrete, sequential nature of visual autoregressive
models enables effective search for image generation. We show that beam search
substantially improves text-to-image generation, enabling a 2B parameter
autoregressive model to outperform a 12B parameter diffusion model across
benchmarks. Systematic ablations show that this advantage comes from the
discrete token space, which allows early pruning and computational reuse, and
our verifier analysis highlights trade-offs between speed and reasoning
capability. These findings suggest that model architecture, not just scale, is
critical for inference-time optimization in visual generation.

</details>


### [173] [Prominence-Aware Artifact Detection and Dataset for Image Super-Resolution](https://arxiv.org/abs/2510.16752)
*Ivan Molodetskikh,Kirill Malyshev,Mark Mirgaleev,Nikita Zagainov,Evgeney Bogatyrev,Dmitriy Vatolin*

Main category: cs.CV

TL;DR: The paper introduces a dataset of 1302 SR artifacts with crowdsourced prominence scores and a lightweight regressor that generates spatial prominence heatmaps to better evaluate and mitigate artifacts in generative image super-resolution.


<details>
  <summary>Details</summary>
Motivation: Current SR models tend to produce artifacts that vary in perceptual impact, but existing methods treat artifacts as uniform binary defects rather than considering their varying prominence to human observers.

Method: Created a dataset of 1302 artifact examples from 11 contemporary SR methods with crowdsourced prominence scores, then trained a lightweight regressor to produce spatial prominence heatmaps.

Result: The trained regressor outperforms existing methods at detecting prominent artifacts and generates spatial prominence heatmaps that better characterize artifact visibility.

Conclusion: The proposed prominence-aware approach provides better evaluation and mitigation of SR artifacts, and the dataset and code are released to facilitate further research in this direction.

Abstract: Generative image super-resolution (SR) is rapidly advancing in visual quality
and detail restoration. As the capacity of SR models expands, however, so does
their tendency to produce artifacts: incorrect, visually disturbing details
that reduce perceived quality. Crucially, their perceptual impact varies: some
artifacts are barely noticeable while others strongly degrade the image. We
argue that artifacts should be characterized by their prominence to human
observers rather than treated as uniform binary defects. Motivated by this, we
present a novel dataset of 1302 artifact examples from 11 contemporary image-SR
methods, where each artifact is paired with a crowdsourced prominence score.
Building on this dataset, we train a lightweight regressor that produces
spatial prominence heatmaps and outperforms existing methods at detecting
prominent artifacts. We release the dataset and code to facilitate
prominence-aware evaluation and mitigation of SR artifacts.

</details>


### [174] [WaMaIR: Image Restoration via Multiscale Wavelet Convolutions and Mamba-based Channel Modeling with Texture Enhancement](https://arxiv.org/abs/2510.16765)
*Shengyu Zhu,Fan,Fuxuan Zhang*

Main category: cs.CV

TL;DR: WaMaIR is a CNN-based image restoration framework that uses wavelet transforms and Mamba-based modules to improve texture detail reconstruction through large receptive fields and channel-aware modeling.


<details>
  <summary>Details</summary>
Motivation: Previous CNN-based image restoration methods struggle with restoring fine texture details due to small receptive fields and lack of channel feature modeling.

Method: Proposes three key components: Global Multiscale Wavelet Transform Convolutions (GMWTConvs) for large receptive field feature extraction, Mamba-Based Channel-Aware Module (MCAM) for long-range channel dependencies, and Multiscale Texture Enhancement Loss (MTELoss) for preserving texture structures.

Result: Extensive experiments show WaMaIR outperforms state-of-the-art methods in image restoration quality while maintaining efficient computational performance.

Conclusion: WaMaIR effectively addresses texture detail restoration challenges in image restoration through its novel architecture combining wavelet transforms, channel-aware modeling, and specialized loss functions.

Abstract: Image restoration is a fundamental and challenging task in computer vision,
where CNN-based frameworks demonstrate significant computational efficiency.
However, previous CNN-based methods often face challenges in adequately
restoring fine texture details, which are limited by the small receptive field
of CNN structures and the lack of channel feature modeling. In this paper, we
propose WaMaIR, which is a novel framework with a large receptive field for
image perception and improves the reconstruction of texture details in restored
images. Specifically, we introduce the Global Multiscale Wavelet Transform
Convolutions (GMWTConvs) for expandding the receptive field to extract image
features, preserving and enriching texture features in model inputs. Meanwhile,
we propose the Mamba-Based Channel-Aware Module (MCAM), explicitly designed to
capture long-range dependencies within feature channels, which enhancing the
model sensitivity to color, edges, and texture information. Additionally, we
propose Multiscale Texture Enhancement Loss (MTELoss) for image restoration to
guide the model in preserving detailed texture structures effectively.
Extensive experiments confirm that WaMaIR outperforms state-of-the-art methods,
achieving better image restoration and efficient computational performance of
the model.

</details>


### [175] [Region in Context: Text-condition Image editing with Human-like semantic reasoning](https://arxiv.org/abs/2510.16772)
*Thuy Phuong Vu,Dinh-Cuong Hoang,Minhhuy Le,Phan Xuan Tan*

Main category: cs.CV

TL;DR: Region in Context is a novel framework for text-conditioned image editing that performs multilevel semantic alignment between vision and language to enable precise and harmonized changes by understanding regions within global image context.


<details>
  <summary>Details</summary>
Motivation: Current approaches treat image regions in isolation, relying only on local cues without considering how each part contributes to the overall composition, resulting in inconsistent edits, unnatural transitions, and loss of coherence across the image.

Method: Introduces a dual-level guidance mechanism: regions are represented with full-image context and aligned with detailed region-level descriptions, while the entire image is simultaneously matched to a comprehensive scene-level description generated by a large vision-language model.

Result: The framework produces more coherent and instruction-aligned results compared to existing methods.

Conclusion: The proposed Region in Context framework enables more precise and harmonized image editing by performing multilevel semantic alignment between vision and language, inspired by human reasoning about edits in relation to the whole scene.

Abstract: Recent research has made significant progress in localizing and editing image
regions based on text. However, most approaches treat these regions in
isolation, relying solely on local cues without accounting for how each part
contributes to the overall visual and semantic composition. This often results
in inconsistent edits, unnatural transitions, or loss of coherence across the
image. In this work, we propose Region in Context, a novel framework for
text-conditioned image editing that performs multilevel semantic alignment
between vision and language, inspired by the human ability to reason about
edits in relation to the whole scene. Our method encourages each region to
understand its role within the global image context, enabling precise and
harmonized changes. At its core, the framework introduces a dual-level guidance
mechanism: regions are represented with full-image context and aligned with
detailed region-level descriptions, while the entire image is simultaneously
matched to a comprehensive scene-level description generated by a large
vision-language model. These descriptions serve as explicit verbal references
of the intended content, guiding both local modifications and global structure.
Experiments show that it produces more coherent and instruction-aligned
results. Code is available at:
https://github.com/thuyvuphuong/Region-in-Context.git

</details>


### [176] [EMRRG: Efficient Fine-Tuning Pre-trained X-ray Mamba Networks for Radiology Report Generation](https://arxiv.org/abs/2510.16776)
*Mingzheng Zhang,Jinfeng Gao,Dan Xu,Jiangrui Yu,Yuhan Qiao,Lan Chen,Jin Tang,Xiao Wang*

Main category: cs.CV

TL;DR: EMRRG is a novel X-ray medical report generation framework that fine-tunes pre-trained Mamba networks using parameter-efficient methods and achieves strong performance on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Existing MRG models rely heavily on LLMs with limited exploration of pre-trained vision foundation models and advanced fine-tuning techniques. Non-Transformer architectures like Mamba networks are underexplored for medical report generation.

Method: Proposes EMRRG framework that divides X-ray images into patches, tokenizes them, processes through SSM-based vision backbone for feature extraction using Partial LoRA, and generates reports with LLM hybrid decoder for end-to-end training.

Result: Extensive experiments on three benchmark datasets fully validated the effectiveness of the proposed strategies for X-ray MRG, achieving strong performance.

Conclusion: The proposed EMRRG framework demonstrates successful application of Mamba networks and parameter-efficient fine-tuning methods for medical report generation, with source code to be released publicly.

Abstract: X-ray image-based medical report generation (MRG) is a pivotal area in
artificial intelligence that can significantly reduce diagnostic burdens for
clinicians and patient wait times. Existing MRG models predominantly rely on
Large Language Models (LLMs) to improve report generation, with limited
exploration of pre-trained vision foundation models or advanced fine-tuning
techniques. Mainstream frameworks either avoid fine-tuning or utilize
simplistic methods like LoRA, often neglecting the potential of enhancing
cross-attention mechanisms. Additionally, while Transformer-based models
dominate vision-language tasks, non-Transformer architectures, such as the
Mamba network, remain underexplored for medical report generation, presenting a
promising avenue for future research. In this paper, we propose EMRRG, a novel
X-ray report generation framework that fine-tunes pre-trained Mamba networks
using parameter-efficient methods. Specifically, X-ray images are divided into
patches, tokenized, and processed by an SSM-based vision backbone for feature
extraction, with Partial LoRA yielding optimal performance. An LLM with a
hybrid decoder generates the medical report, enabling end-to-end training and
achieving strong results on benchmark datasets. Extensive experiments on three
widely used benchmark datasets fully validated the effectiveness of our
proposed strategies for the X-ray MRG. The source code of this paper will be
released on https://github.com/Event-AHU/Medical_Image_Analysis.

</details>


### [177] [GS2POSE: Marry Gaussian Splatting to 6D Object Pose Estimation](https://arxiv.org/abs/2510.16777)
*Junbo Li,Weimin Yuan,Yinuo Wang,Yue Zeng,Shihao Shu,Cai Meng,Xiangzhi Bai*

Main category: cs.CV

TL;DR: GS2POSE is a novel 6D object pose estimation method that uses Bundle Adjustment principles with 3D Gaussian Splatting to iteratively optimize pose by comparing rendered and input images, achieving improved accuracy on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Current 6D pose estimation methods struggle with textureless objects and varying illumination conditions when establishing 2D-3D correspondences.

Method: Proposes a pose regression algorithm inspired by Bundle Adjustment, leveraging Lie algebra to create a pose-differentiable rendering pipeline using 3DGS. Iteratively optimizes pose by comparing input and rendered images, and updates color parameters to handle illumination changes.

Result: Achieves accuracy improvements of 1.4%, 2.8% and 2.5% on T-LESS, LineMod-Occlusion and LineMod datasets respectively compared to previous models.

Conclusion: GS2POSE effectively addresses limitations of existing methods by combining Bundle Adjustment principles with 3D Gaussian Splatting, demonstrating superior performance on challenging datasets with textureless objects and varying illumination.

Abstract: Accurate 6D pose estimation of 3D objects is a fundamental task in computer
vision, and current research typically predicts the 6D pose by establishing
correspondences between 2D image features and 3D model features. However, these
methods often face difficulties with textureless objects and varying
illumination conditions. To overcome these limitations, we propose GS2POSE, a
novel approach for 6D object pose estimation. GS2POSE formulates a pose
regression algorithm inspired by the principles of Bundle Adjustment (BA). By
leveraging Lie algebra, we extend the capabilities of 3DGS to develop a
pose-differentiable rendering pipeline, which iteratively optimizes the pose by
comparing the input image to the rendered image. Additionally, GS2POSE updates
color parameters within the 3DGS model, enhancing its adaptability to changes
in illumination. Compared to previous models, GS2POSE demonstrates accuracy
improvements of 1.4\%, 2.8\% and 2.5\% on the T-LESS, LineMod-Occlusion and
LineMod datasets, respectively.

</details>


### [178] [Xiaoice: Training-Free Video Understanding via Self-Supervised Spatio-Temporal Clustering of Semantic Features](https://arxiv.org/abs/2510.16781)
*Shihao Ji,Zihui Song*

Main category: cs.CV

TL;DR: Training-free video understanding framework combining pre-trained VLMs with machine learning for spatio-temporal clustering to generate structured video summaries without task-specific training.


<details>
  <summary>Details</summary>
Motivation: Current video understanding models require extensive annotated datasets and task-specific training, which is costly and lacks scalability. There's a need to leverage zero-shot reasoning capabilities of VLMs for video content.

Method: Reframes video understanding as self-supervised spatio-temporal clustering. Uses frozen VLM visual encoder to extract semantic features, Kernel Temporal Segmentation (KTS) for event segmentation, and density-based clustering to identify recurring scenes. Generates summaries using VLM's textual description capabilities on keyframes.

Result: Creates automated, structured multi-modal summaries of video content through discovered semantic segments and clusters without any training.

Conclusion: Provides an effective, interpretable, and model-agnostic pathway for zero-shot automated structural analysis of video content, bridging the gap between static image reasoning and video understanding.

Abstract: The remarkable zero-shot reasoning capabilities of large-scale Visual
Language Models (VLMs) on static images have yet to be fully translated to the
video domain. Conventional video understanding models often rely on extensive,
task-specific training on annotated datasets, a process that is both costly and
limited in scalability. This paper introduces a novel, training-free framework
for video understanding that circumvents end-to-end training by synergistically
combining the rich semantic priors of pre-trained VLMs with classic machine
learning algorithms for pattern discovery. Our core idea is to reframe video
understanding as a self-supervised spatio-temporal clustering problem within a
high-dimensional semantic feature space. The proposed pipeline first transforms
a video stream into a semantic feature trajectory using the frozen visual
encoder of a pre-trained VLM. Subsequently, we employ Kernel Temporal
Segmentation (KTS), a robust machine learning technique, to partition the
continuous feature stream into discrete, semantically coherent event segments.
These segments are then subjected to unsupervised density-based clustering to
identify recurring macroscopic scenes and themes throughout the video. By
selecting representative keyframes from each discovered cluster and leveraging
the VLM's generative capabilities for textual description, our framework
automatically produces a structured, multi-modal summary of the video content.
This approach provides an effective, interpretable, and model-agnostic pathway
for zero-shot, automated structural analysis of video content.

</details>


### [179] [Segmentation as A Plug-and-Play Capability for Frozen Multimodal LLMs](https://arxiv.org/abs/2510.16785)
*Jiazhen Liu,Long Chen*

Main category: cs.CV

TL;DR: LENS introduces a plug-and-play method to add segmentation capabilities to MLLMs without retraining, preserving their generalization by using keypoints from attention maps.


<details>
  <summary>Details</summary>
Motivation: Current methods for adding segmentation to MLLMs require finetuning, which alters the model's output space and compromises its generalization, undermining the goal of unified models.

Method: LENS attaches a lightweight trainable head to a frozen MLLM, extracts keypoints from attention maps, and converts them to point-wise features compatible with mask decoders.

Result: LENS achieves segmentation performance competitive with or superior to retraining-based methods while fully preserving the MLLM's generalization capabilities.

Conclusion: LENS establishes an efficient paradigm for extending MLLMs without compromising their core abilities, enabling truly multi-talented unified models.

Abstract: Integrating diverse visual capabilities into a unified model is a significant
trend in Multimodal Large Language Models (MLLMs). Among these, the inclusion
of segmentation poses a distinct set of challenges. To equip MLLMs with
pixel-level segmentation abilities, prevailing methods require finetuning the
model to produce specific outputs compatible with a mask decoder. This process
typically alters the model's output space and compromises its intrinsic
generalization, which undermines the goal of building a unified model. We
introduce LENS (Leveraging kEypoiNts for MLLMs' Segmentation), a novel
plug-and-play solution. LENS attaches a lightweight, trainable head to a
completely frozen MLLM. By refining the spatial cues embedded in attention
maps, LENS extracts keypoints and describes them into point-wise features
directly compatible with the mask decoder. Extensive experiments validate our
approach: LENS achieves segmentation performance competitive with or superior
to that of retraining-based methods. Crucially, it does so while fully
preserving the MLLM's generalization capabilities, which are significantly
degraded by finetuning approaches. As such, the attachable design of LENS
establishes an efficient and powerful paradigm for extending MLLMs, paving the
way for truly multi-talented, unified models.

</details>


### [180] [Unsupervised Monocular Road Segmentation for Autonomous Driving via Scene Geometry](https://arxiv.org/abs/2510.16790)
*Sara Hatami Rostami,Behrooz Nasihatkon*

Main category: cs.CV

TL;DR: Unsupervised road segmentation using geometric priors and temporal consistency without manual labels, achieving 0.82 IoU on Cityscapes.


<details>
  <summary>Details</summary>
Motivation: Eliminate reliance on costly manually labeled datasets for road segmentation in autonomous driving.

Method: Generate weak labels from geometric priors (pixels above horizon as non-road, quadrilateral in front as road), then refine using temporal consistency through feature tracking and mutual information maximization.

Result: Achieves 0.82 Intersection-over-Union (IoU) on Cityscapes dataset with high accuracy and temporal stability.

Conclusion: Combining geometric constraints and temporal consistency enables scalable unsupervised road segmentation for autonomous driving.

Abstract: This paper presents a fully unsupervised approach for binary road
segmentation (road vs. non-road), eliminating the reliance on costly manually
labeled datasets. The method leverages scene geometry and temporal cues to
distinguish road from non-road regions. Weak labels are first generated from
geometric priors, marking pixels above the horizon as non-road and a predefined
quadrilateral in front of the vehicle as road. In a refinement stage, temporal
consistency is enforced by tracking local feature points across frames and
penalizing inconsistent label assignments using mutual information
maximization. This enhances both precision and temporal stability. On the
Cityscapes dataset, the model achieves an Intersection-over-Union (IoU) of
0.82, demonstrating high accuracy with a simple design. These findings
demonstrate the potential of combining geometric constraints and temporal
consistency for scalable unsupervised road segmentation in autonomous driving.

</details>


### [181] [Personalized Image Filter: Mastering Your Photographic Style](https://arxiv.org/abs/2510.16791)
*Chengxuan Zhu,Shuchen Weng,Jiacong Fang,Peixuan Zhang,Si Li,Chao Xu,Boxin Shi*

Main category: cs.CV

TL;DR: PIF (Personalized Image Filter) learns photographic styles from reference images using textual inversion on a pretrained diffusion model, enabling effective style transfer while preserving content.


<details>
  <summary>Details</summary>
Motivation: Previous methods fail to learn meaningful photographic concepts from references or preserve content fidelity during style transfer.

Method: Uses pretrained text-to-image diffusion model with textual inversion technique to optimize prompts for photographic concepts, learning both average appearance and text-based adjustments.

Result: Outstanding performance in extracting and transferring various photographic styles while maintaining content integrity.

Conclusion: PIF effectively addresses limitations of previous approaches by leveraging generative priors and textual inversion for photographic style learning and transfer.

Abstract: Photographic style, as a composition of certain photographic concepts, is the
charm behind renowned photographers. But learning and transferring photographic
style need a profound understanding of how the photo is edited from the unknown
original appearance. Previous works either fail to learn meaningful
photographic concepts from reference images, or cannot preserve the content of
the content image. To tackle these issues, we proposed a Personalized Image
Filter (PIF). Based on a pretrained text-to-image diffusion model, the
generative prior enables PIF to learn the average appearance of photographic
concepts, as well as how to adjust them according to text prompts. PIF then
learns the photographic style of reference images with the textual inversion
technique, by optimizing the prompts for the photographic concepts. PIF shows
outstanding performance in extracting and transferring various kinds of
photographic style. Project page: https://pif.pages.dev/

</details>


### [182] [An RGB-D Image Dataset for Lychee Detection and Maturity Classification for Robotic Harvesting](https://arxiv.org/abs/2510.16800)
*Zhenpeng Zhang,Yi Wang,Shanglei Chai,Yingying Liu,Zekai Xie,Wenhao Huang,Pengyu Li,Zipei Luo,Dajiang Lu,Yibin Tian*

Main category: cs.CV

TL;DR: A comprehensive lychee dataset for fruit detection and maturity classification was created with 11,414 images (RGB and depth) across multiple varieties and ripeness stages, featuring consistent annotations and evaluated with deep learning models.


<details>
  <summary>Details</summary>
Motivation: There are no consistently annotated open-source lychee datasets for developing vision-based harvesting robots, which could improve productivity and reduce labor dependency.

Method: Collected color (RGB) images under diverse conditions across multiple lychee varieties, created augmented images, depth images, and used multi-person independent labeling with verification for consistent annotations.

Result: Dataset contains 11,414 images (878 raw RGB, 8,780 augmented RGB, 1,756 depth) with 9,658 annotation pairs for detection and maturity classification across three ripeness stages.

Conclusion: The publicly available dataset enables development of lychee harvesting robots through reliable detection and maturity classification, validated by deep learning experiments.

Abstract: Lychee is a high-value subtropical fruit. The adoption of vision-based
harvesting robots can significantly improve productivity while reduce reliance
on labor. High-quality data are essential for developing such harvesting
robots. However, there are currently no consistently and comprehensively
annotated open-source lychee datasets featuring fruits in natural growing
environments. To address this, we constructed a dataset to facilitate lychee
detection and maturity classification. Color (RGB) images were acquired under
diverse weather conditions, and at different times of the day, across multiple
lychee varieties, such as Nuomici, Feizixiao, Heiye, and Huaizhi. The dataset
encompasses three different ripeness stages and contains 11,414 images,
consisting of 878 raw RGB images, 8,780 augmented RGB images, and 1,756 depth
images. The images are annotated with 9,658 pairs of lables for lychee
detection and maturity classification. To improve annotation consistency, three
individuals independently labeled the data, and their results were then
aggregated and verified by a fourth reviewer. Detailed statistical analyses
were done to examine the dataset. Finally, we performed experiments using three
representative deep learning models to evaluate the dataset. It is publicly
available for academic

</details>


### [183] [ReefNet: A Large scale, Taxonomically Enriched Dataset and Benchmark for Hard Coral Classification](https://arxiv.org/abs/2510.16822)
*Yahia Battach,Abdulwahab Felemban,Faizan Farooq Khan,Yousef A. Radwan,Xiang Li,Fabio Marchese,Sara Beery,Burton H. Jones,Francesca Benzoni,Mohamed Elhoseiny*

Main category: cs.CV

TL;DR: ReefNet is a large public coral reef image dataset with fine-grained genus-level annotations mapped to WoRMS, providing challenging benchmarks for domain generalization and coral classification.


<details>
  <summary>Details</summary>
Motivation: Coral reefs are declining rapidly due to climate change, creating urgent need for scalable automated monitoring systems that can handle global diversity and domain shifts.

Method: Aggregated imagery from 76 CoralNet sources and Red Sea site, totaling ~925K expert-verified genus-level annotations. Proposed two evaluation settings: within-source and cross-source benchmarks to test domain generalization.

Result: Supervised within-source performance is promising but drops sharply across domains. Zero-shot models perform poorly overall, especially for rare and visually similar genera.

Conclusion: ReefNet provides a challenging benchmark to catalyze advances in domain generalization and fine-grained coral classification for global reef monitoring and conservation.

Abstract: Coral reefs are rapidly declining due to anthropogenic pressures such as
climate change, underscoring the urgent need for scalable, automated
monitoring. We introduce ReefNet, a large public coral reef image dataset with
point-label annotations mapped to the World Register of Marine Species (WoRMS).
ReefNet aggregates imagery from 76 curated CoralNet sources and an additional
site from Al Wajh in the Red Sea, totaling approximately 925000 genus-level
hard coral annotations with expert-verified labels. Unlike prior datasets,
which are often limited by size, geography, or coarse labels and are not
ML-ready, ReefNet offers fine-grained, taxonomically mapped labels at a global
scale to WoRMS. We propose two evaluation settings: (i) a within-source
benchmark that partitions each source's images for localized evaluation, and
(ii) a cross-source benchmark that withholds entire sources to test domain
generalization. We analyze both supervised and zero-shot classification
performance on ReefNet and find that while supervised within-source performance
is promising, supervised performance drops sharply across domains, and
performance is low across the board for zero-shot models, especially for rare
and visually similar genera. This provides a challenging benchmark intended to
catalyze advances in domain generalization and fine-grained coral
classification. We will release our dataset, benchmarking code, and pretrained
models to advance robust, domain-adaptive, global coral reef monitoring and
conservation.

</details>


### [184] [Robust Cross-Domain Adaptation in Texture Features Transferring for Wood Chip Moisture Content Prediction](https://arxiv.org/abs/2510.16832)
*Abdur Rahman,Mohammad Marufuzzaman,Jason Street,Haifeng Wang,Veera G. Gude,Randy Buchanan*

Main category: cs.CV

TL;DR: This paper proposes AdaptMoist, a domain adaptation method using texture features from wood chip images to predict moisture content across different wood sources, achieving 80% accuracy compared to 57% for non-adapted models.


<details>
  <summary>Details</summary>
Motivation: Current moisture prediction methods for wood chips are either slow/destructive (oven drying) or inaccurate across different wood sources. Source variability undermines data-driven models, requiring a robust approach that handles domain shifts.

Method: Analyzed five texture feature types from wood chip images and combined them for moisture prediction. Proposed AdaptMoist domain adaptation method that transfers knowledge between wood chip sources using texture features, with model saving based on adjusted mutual information.

Result: Combined texture features achieved 95% accuracy for moisture prediction. AdaptMoist improved cross-domain prediction accuracy by 23%, achieving average 80% accuracy compared to 57% for non-adapted models.

Conclusion: AdaptMoist is an effective robust solution for wood chip moisture content estimation across different domains, making it suitable for wood chip-reliant industries by addressing source variability issues.

Abstract: Accurate and quick prediction of wood chip moisture content is critical for
optimizing biofuel production and ensuring energy efficiency. The current
widely used direct method (oven drying) is limited by its longer processing
time and sample destructiveness. On the other hand, existing indirect methods,
including near-infrared spectroscopy-based, electrical capacitance-based, and
image-based approaches, are quick but not accurate when wood chips come from
various sources. Variability in the source material can alter data
distributions, undermining the performance of data-driven models. Therefore,
there is a need for a robust approach that effectively mitigates the impact of
source variability. Previous studies show that manually extracted texture
features have the potential to predict wood chip moisture class. Building on
this, in this study, we conduct a comprehensive analysis of five distinct
texture feature types extracted from wood chip images to predict moisture
content. Our findings reveal that a combined feature set incorporating all five
texture features achieves an accuracy of 95% and consistently outperforms
individual texture features in predicting moisture content. To ensure robust
moisture prediction, we propose a domain adaptation method named AdaptMoist
that utilizes the texture features to transfer knowledge from one source of
wood chip data to another, addressing variability across different domains. We
also proposed a criterion for model saving based on adjusted mutual
information. The AdaptMoist method improves prediction accuracy across domains
by 23%, achieving an average accuracy of 80%, compared to 57% for non-adapted
models. These results highlight the effectiveness of AdaptMoist as a robust
solution for wood chip moisture content estimation across domains, making it a
potential solution for wood chip-reliant industries.

</details>


### [185] [From Mannequin to Human: A Pose-Aware and Identity-Preserving Video Generation Framework for Lifelike Clothing Display](https://arxiv.org/abs/2510.16833)
*Xiangyu Mu,Dongliang Zhou,Jie Hou,Haijun Zhang,Weili Guan*

Main category: cs.CV

TL;DR: M2HVideo is a framework that generates photorealistic human videos from mannequin footage by addressing head-body misalignment and identity drift through pose-aware encoding and distribution alignment.


<details>
  <summary>Details</summary>
Motivation: Mannequin-based clothing displays are cost-effective but lack realism and expressive detail compared to real-model showcases, limiting their effectiveness for online fashion presentation.

Method: Proposes M2HVideo with: 1) dynamic pose-aware head encoder for consistent identity embeddings, 2) mirror loss in pixel space via DDIM-based denoising to preserve facial details, and 3) distribution-aware adapter for temporal coherence by aligning identity and clothing feature distributions.

Result: Extensive experiments on UBC fashion, ASOS, and MannequinVideos datasets show superior performance in clothing consistency, identity preservation, and video fidelity compared to state-of-the-art methods.

Conclusion: M2HVideo successfully addresses key challenges in mannequin-to-human video generation, providing a practical solution for realistic fashion presentation while maintaining identity control and temporal coherence.

Abstract: Mannequin-based clothing displays offer a cost-effective alternative to
real-model showcases for online fashion presentation, but lack realism and
expressive detail. To overcome this limitation, we introduce a new task called
mannequin-to-human (M2H) video generation, which aims to synthesize
identity-controllable, photorealistic human videos from footage of mannequins.
We propose M2HVideo, a pose-aware and identity-preserving video generation
framework that addresses two key challenges: the misalignment between head and
body motion, and identity drift caused by temporal modeling. In particular,
M2HVideo incorporates a dynamic pose-aware head encoder that fuses facial
semantics with body pose to produce consistent identity embeddings across
frames. To address the loss of fine facial details due to latent space
compression, we introduce a mirror loss applied in pixel space through a
denoising diffusion implicit model (DDIM)-based one-step denoising.
Additionally, we design a distribution-aware adapter that aligns statistical
distributions of identity and clothing features to enhance temporal coherence.
Extensive experiments on the UBC fashion dataset, our self-constructed ASOS
dataset, and the newly collected MannequinVideos dataset captured on-site
demonstrate that M2HVideo achieves superior performance in terms of clothing
consistency, identity preservation, and video fidelity in comparison to
state-of-the-art methods.

</details>


### [186] [2DGS-R: Revisiting the Normal Consistency Regularization in 2D Gaussian Splatting](https://arxiv.org/abs/2510.16837)
*Haofan Ren,Qingsong Yan,Ming Lu,Rongfeng Lu,Zunjie Zhu*

Main category: cs.CV

TL;DR: 2DGS-R is a hierarchical training method that improves rendering quality while maintaining geometric accuracy in 2D Gaussian Splatting, requiring only 1% more storage and minimal additional training time.


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting struggles with accurate surface representation, while 2DGS has compromised rendering quality despite better geometry. Current methods cannot optimize both geometric and rendering quality in a single training stage.

Method: Uses hierarchical training: first trains original 2D Gaussians with normal consistency regularization, then selects poorly-rendering Gaussians for in-place cloning enhancement, and finally fine-tunes with frozen opacity.

Result: Achieves high-quality rendering results while preserving fine geometric structures with only 1% more storage and minimal additional training time compared to original 2DGS.

Conclusion: The approach effectively balances efficiency with performance, leading to improvements in both visual fidelity and geometric reconstruction accuracy.

Abstract: Recent advancements in 3D Gaussian Splatting (3DGS) have greatly influenced
neural fields, as it enables high-fidelity rendering with impressive visual
quality. However, 3DGS has difficulty accurately representing surfaces. In
contrast, 2DGS transforms the 3D volume into a collection of 2D planar Gaussian
disks. Despite advancements in geometric fidelity, rendering quality remains
compromised, highlighting the challenge of achieving both high-quality
rendering and precise geometric structures. This indicates that optimizing both
geometric and rendering quality in a single training stage is currently
unfeasible. To overcome this limitation, we present 2DGS-R, a new method that
uses a hierarchical training approach to improve rendering quality while
maintaining geometric accuracy. 2DGS-R first trains the original 2D Gaussians
with the normal consistency regularization. Then 2DGS-R selects the 2D
Gaussians with inadequate rendering quality and applies a novel in-place
cloning operation to enhance the 2D Gaussians. Finally, we fine-tune the 2DGS-R
model with opacity frozen. Experimental results show that compared to the
original 2DGS, our method requires only 1\% more storage and minimal additional
training time. Despite this negligible overhead, it achieves high-quality
rendering results while preserving fine geometric structures. These findings
indicate that our approach effectively balances efficiency with performance,
leading to improvements in both visual fidelity and geometric reconstruction
accuracy.

</details>


### [187] [ArmFormer: Lightweight Transformer Architecture for Real-Time Multi-Class Weapon Segmentation and Classification](https://arxiv.org/abs/2510.16854)
*Akhila Kambhatla,Taminul Islam,Khaled R Ahmed*

Main category: cs.CV

TL;DR: ArmFormer is a lightweight transformer-based semantic segmentation framework that achieves high-precision weapon detection with real-time performance suitable for edge devices.


<details>
  <summary>Details</summary>
Motivation: Traditional weapon detection methods provide only bounding boxes, lacking pixel-level precision needed for comprehensive threat analysis, while existing segmentation models are either inaccurate or too computationally expensive for edge deployment.

Method: Integrates Convolutional Block Attention Module (CBAM) with MixVisionTransformer architecture, combining CBAM-enhanced encoder backbone with attention-integrated hamburger decoder for multi-class weapon segmentation.

Result: Achieves state-of-the-art performance with 80.64% mIoU and 89.13% mFscore while maintaining real-time inference at 82.26 FPS, with only 4.886G FLOPs and 3.66M parameters.

Conclusion: ArmFormer outperforms heavyweight models requiring up to 48x more computation, establishing it as the optimal solution for deployment on portable security cameras, surveillance drones, and embedded AI accelerators.

Abstract: The escalating threat of weapon-related violence necessitates automated
detection systems capable of pixel-level precision for accurate threat
assessment in real-time security applications. Traditional weapon detection
approaches rely on object detection frameworks that provide only coarse
bounding box localizations, lacking the fine-grained segmentation required for
comprehensive threat analysis. Furthermore, existing semantic segmentation
models either sacrifice accuracy for computational efficiency or require
excessive computational resources incompatible with edge deployment scenarios.
This paper presents ArmFormer, a lightweight transformer-based semantic
segmentation framework that strategically integrates Convolutional Block
Attention Module (CBAM) with MixVisionTransformer architecture to achieve
superior accuracy while maintaining computational efficiency suitable for
resource-constrained edge devices. Our approach combines CBAM-enhanced encoder
backbone with attention-integrated hamburger decoder to enable multi-class
weapon segmentation across five categories: handgun, rifle, knife, revolver,
and human. Comprehensive experiments demonstrate that ArmFormer achieves
state-of-the-art performance with 80.64% mIoU and 89.13% mFscore while
maintaining real-time inference at 82.26 FPS. With only 4.886G FLOPs and 3.66M
parameters, ArmFormer outperforms heavyweight models requiring up to 48x more
computation, establishing it as the optimal solution for deployment on portable
security cameras, surveillance drones, and embedded AI accelerators in
distributed security infrastructure.

</details>


### [188] [BARL: Bilateral Alignment in Representation and Label Spaces for Semi-Supervised Volumetric Medical Image Segmentation](https://arxiv.org/abs/2510.16863)
*Shujian Gao,Yuan Wang,Zekuan Yu*

Main category: cs.CV

TL;DR: BARL introduces a unified semi-supervised medical image segmentation framework that enforces alignment in both representation and label spaces, achieving state-of-the-art performance while reducing annotation costs.


<details>
  <summary>Details</summary>
Motivation: Existing semi-supervised medical image segmentation methods focus only on label-space consistency but overlook representation-space alignment, leading to models that struggle to learn discriminative and spatially coherent representations.

Method: BARL uses two collaborative branches with dual-path regularization and progressively cognitive bias correction for label-space alignment, plus region-level and lesion-instance matching for representation-space alignment.

Result: Extensive experiments on four public benchmarks and a proprietary CBCT dataset show BARL consistently outperforms state-of-the-art SSMIS methods.

Conclusion: BARL demonstrates the importance of bilateral alignment in both representation and label spaces for effective semi-supervised medical image segmentation.

Abstract: Semi-supervised medical image segmentation (SSMIS) seeks to match fully
supervised performance while sharply reducing annotation cost. Mainstream SSMIS
methods rely on \emph{label-space consistency}, yet they overlook the equally
critical \emph{representation-space alignment}. Without harmonizing latent
features, models struggle to learn representations that are both discriminative
and spatially coherent. To this end, we introduce \textbf{Bilateral Alignment
in Representation and Label spaces (BARL)}, a unified framework that couples
two collaborative branches and enforces alignment in both spaces. For
label-space alignment, inspired by co-training and multi-scale decoding, we
devise \textbf{Dual-Path Regularization (DPR)} and \textbf{Progressively
Cognitive Bias Correction (PCBC)} to impose fine-grained cross-branch
consistency while mitigating error accumulation from coarse to fine scales. For
representation-space alignment, we conduct region-level and lesion-instance
matching between branches, explicitly capturing the fragmented, complex
pathological patterns common in medical imagery. Extensive experiments on four
public benchmarks and a proprietary CBCT dataset demonstrate that BARL
consistently surpasses state-of-the-art SSMIS methods. Ablative studies further
validate the contribution of each component. Code will be released soon.

</details>


### [189] [Registration is a Powerful Rotation-Invariance Learner for 3D Anomaly Detection](https://arxiv.org/abs/2510.16865)
*Yuyang Yu,Zhengwei Chen,Xuemiao Xu,Lei Zhang,Haoxin Yang,Yongwei Nie,Shengfeng He*

Main category: cs.CV

TL;DR: A registration-induced rotation-invariant framework for 3D anomaly detection that jointly optimizes point-cloud registration and feature extraction to overcome limitations of memory bank methods.


<details>
  <summary>Details</summary>
Motivation: Current memory bank-based methods suffer from inconsistent feature transformations and limited discriminative capacity, especially in capturing local geometric details and achieving rotation invariance, particularly when registration fails.

Method: Proposes a registration-induced rotation-invariant feature extraction framework that integrates point-cloud registration objectives with memory-based anomaly detection, embedding feature extraction into the registration learning process.

Result: Extensive experiments on Anomaly-ShapeNet and Real3D-AD datasets demonstrate consistent outperformance of existing approaches in effectiveness and generalizability.

Conclusion: The integration of registration and feature extraction enables acquisition of features that are both rotation-robust and highly effective for anomaly detection.

Abstract: 3D anomaly detection in point-cloud data is critical for industrial quality
control, aiming to identify structural defects with high reliability. However,
current memory bank-based methods often suffer from inconsistent feature
transformations and limited discriminative capacity, particularly in capturing
local geometric details and achieving rotation invariance. These limitations
become more pronounced when registration fails, leading to unreliable detection
results. We argue that point-cloud registration plays an essential role not
only in aligning geometric structures but also in guiding feature extraction
toward rotation-invariant and locally discriminative representations. To this
end, we propose a registration-induced, rotation-invariant feature extraction
framework that integrates the objectives of point-cloud registration and
memory-based anomaly detection. Our key insight is that both tasks rely on
modeling local geometric structures and leveraging feature similarity across
samples. By embedding feature extraction into the registration learning
process, our framework jointly optimizes alignment and representation learning.
This integration enables the network to acquire features that are both robust
to rotations and highly effective for anomaly detection. Extensive experiments
on the Anomaly-ShapeNet and Real3D-AD datasets demonstrate that our method
consistently outperforms existing approaches in effectiveness and
generalizability.

</details>


### [190] [Uncovering Brain-Like Hierarchical Patterns in Vision-Language Models through fMRI-Based Neural Encoding](https://arxiv.org/abs/2510.16870)
*Yudan Ren,Xinlong Wang,Kexin Wang,Tian Xia,Zihan Ma,Zhaowei Li,Xiangrong Bi,Xiao Li,Xiaowei He*

Main category: cs.CV

TL;DR: A neuron-level analysis framework reveals brain-like processing in vision-language models, showing shared representational mechanisms between artificial and biological neurons across multiple functional networks.


<details>
  <summary>Details</summary>
Motivation: Current ANN studies have limitations: unimodal approaches don't capture the brain's multimodal processing, and multimodal research focuses on high-level outputs while neglecting individual neurons' roles.

Method: Proposed a neuron-level analysis framework combining fine-grained artificial neuron analysis with fMRI-based voxel encoding to examine CLIP and METER vision-language models.

Result: Four key findings: (1) ANs predict BN activities across multiple functional networks; (2) Both show functional redundancy; (3) ANs exhibit polarity patterns mirroring BNs; (4) Different VLM architectures drive distinct BN patterns - CLIP shows modality specialization while METER shows unified cross-modal activation.

Conclusion: The results provide compelling evidence for brain-like hierarchical processing in VLMs at the neuronal level, demonstrating shared representational mechanisms between artificial and biological neural systems.

Abstract: While brain-inspired artificial intelligence(AI) has demonstrated promising
results, current understanding of the parallels between artificial neural
networks (ANNs) and human brain processing remains limited: (1) unimodal ANN
studies fail to capture the brain's inherent multimodal processing
capabilities, and (2) multimodal ANN research primarily focuses on high-level
model outputs, neglecting the crucial role of individual neurons. To address
these limitations, we propose a novel neuron-level analysis framework that
investigates the multimodal information processing mechanisms in
vision-language models (VLMs) through the lens of human brain activity. Our
approach uniquely combines fine-grained artificial neuron (AN) analysis with
fMRI-based voxel encoding to examine two architecturally distinct VLMs: CLIP
and METER. Our analysis reveals four key findings: (1) ANs successfully predict
biological neurons (BNs) activities across multiple functional networks
(including language, vision, attention, and default mode), demonstrating shared
representational mechanisms; (2) Both ANs and BNs demonstrate functional
redundancy through overlapping neural representations, mirroring the brain's
fault-tolerant and collaborative information processing mechanisms; (3) ANs
exhibit polarity patterns that parallel the BNs, with oppositely activated BNs
showing mirrored activation trends across VLM layers, reflecting the complexity
and bidirectional nature of neural information processing; (4) The
architectures of CLIP and METER drive distinct BNs: CLIP's independent branches
show modality-specific specialization, whereas METER's cross-modal design
yields unified cross-modal activation, highlighting the architecture's
influence on ANN brain-like properties. These results provide compelling
evidence for brain-like hierarchical processing in VLMs at the neuronal level.

</details>


### [191] [Class-N-Diff: Classification-Induced Diffusion Model Can Make Fair Skin Cancer Diagnosis](https://arxiv.org/abs/2510.16887)
*Nusrat Munia,Abdullah Imran*

Main category: cs.CV

TL;DR: Proposes Class-N-Diff, a classification-induced diffusion model that integrates a classifier within a diffusion model to simultaneously generate and classify dermoscopic images for improved skin cancer diagnosis.


<details>
  <summary>Details</summary>
Motivation: Traditional class-conditioned generative models struggle to generate accurate medical category images, limiting their usefulness for applications like skin cancer diagnosis.

Method: Integrates a classifier within a diffusion model to guide image generation based on class conditions, enabling better control over class-conditioned image synthesis.

Result: Generates more realistic and diverse dermoscopic images while the classifier shows improved performance for downstream diagnostic tasks.

Conclusion: Class-N-Diff is a robust tool for enhancing quality and utility of diffusion model-based synthetic dermoscopic image generation.

Abstract: Generative models, especially Diffusion Models, have demonstrated remarkable
capability in generating high-quality synthetic data, including medical images.
However, traditional class-conditioned generative models often struggle to
generate images that accurately represent specific medical categories, limiting
their usefulness for applications such as skin cancer diagnosis. To address
this problem, we propose a classification-induced diffusion model, namely,
Class-N-Diff, to simultaneously generate and classify dermoscopic images. Our
Class-N-Diff model integrates a classifier within a diffusion model to guide
image generation based on its class conditions. Thus, the model has better
control over class-conditioned image synthesis, resulting in more realistic and
diverse images. Additionally, the classifier demonstrates improved performance,
highlighting its effectiveness for downstream diagnostic tasks. This unique
integration in our Class-N-Diff makes it a robust tool for enhancing the
quality and utility of diffusion model-based synthetic dermoscopic image
generation. Our code is available at https://github.com/Munia03/Class-N-Diff.

</details>


### [192] [Uniworld-V2: Reinforce Image Editing with Diffusion Negative-aware Finetuning and MLLM Implicit Feedback](https://arxiv.org/abs/2510.16888)
*Zongjian Li,Zheyuan Liu,Qihui Zhang,Bin Lin,Shenghai Yuan,Zhiyuan Yan,Yang Ye,Wangbo Yu,Yuwei Niu,Li Yuan*

Main category: cs.CV

TL;DR: Edit-R1 is a post-training framework for instruction-based image editing that uses policy optimization and MLLM-based rewards to improve generalization beyond supervised training patterns.


<details>
  <summary>Details</summary>
Motivation: Models trained via supervised fine-tuning often overfit to annotated patterns and struggle to generalize beyond training distributions, limiting their editing capabilities.

Method: Uses Diffusion Negative-aware Finetuning (DiffusionNFT) for policy optimization, employs MLLM as a training-free reward model, and implements low-variance group filtering to reduce scoring noise.

Result: UniWorld-V2 achieves state-of-the-art results on ImgEdit (4.49) and GEdit-Bench (7.83), and the framework shows substantial performance gains when applied to diverse base models like Qwen-Image-Edit and FLUX-Kontext.

Conclusion: The Edit-R1 framework is model-agnostic and effectively addresses overfitting in instruction-based image editing, demonstrating wide applicability and improved generalization capabilities.

Abstract: Instruction-based image editing has achieved remarkable progress; however,
models solely trained via supervised fine-tuning often overfit to annotated
patterns, hindering their ability to explore and generalize beyond training
distributions. To this end, we introduce Edit-R1, a novel post-training
framework for instruction-based image editing based on policy optimization.
Specifically, we utilize Diffusion Negative-aware Finetuning (DiffusionNFT), a
likelihood-free policy optimization method consistent with the flow matching
forward process, thereby enabling the use of higher-order samplers and more
efficient training. Another key challenge here is the absence of a universal
reward model, resulting from the diverse nature of editing instructions and
tasks. To bridge this gap, we employ a Multimodal Large Language Model (MLLM)
as a unified, training-free reward model, leveraging its output logits to
provide fine-grained feedback. Furthermore, we carefully design a low-variance
group filtering mechanism to reduce MLLM scoring noise and stabilize
optimization. UniWorld-V2, trained with this framework, achieves
\textbf{state-of-the-art} results on the ImgEdit and GEdit-Bench benchmarks,
scoring 4.49 and 7.83, respectively. Crucially, our framework is
model-agnostic, delivering substantial performance gains when applied to
diverse base models like Qwen-Image-Edit and FLUX-Kontext, demonstrating its
wide applicability. Code and models are publicly available at
https://github.com/PKU-YuanGroup/UniWorld-V2.

</details>


### [193] [Contrail-to-Flight Attribution Using Ground Visible Cameras and Flight Surveillance Data](https://arxiv.org/abs/2510.16891)
*Ramon Dalmau,Gabriel Jarry,Philippe Very*

Main category: cs.CV

TL;DR: A modular framework for attributing contrails observed by ground-based cameras to source flights using aircraft surveillance and meteorological data, addressing challenges in contrail-to-flight attribution.


<details>
  <summary>Details</summary>
Motivation: Aviation's non-CO2 effects from contrails contribute significantly to climate impact, but validating physical models requires linking observed contrails to source flights, which is challenging with satellite data due to limited resolution and contrail drift.

Method: Uses ground-based cameras to capture contrails shortly after formation, then develops a modular framework with multiple geometric representations, distance metrics, temporal smoothing, and probability-based assignment strategies to link contrails to flights.

Result: Establishes a strong baseline and provides a modular framework for contrail-to-flight attribution using ground-based camera data from the GVCCS dataset.

Conclusion: The framework enables future research in linking contrails to their source flights, addressing a critical gap in validating contrail climate impact models.

Abstract: Aviation's non-CO2 effects, particularly contrails, are a significant
contributor to its climate impact. Persistent contrails can evolve into
cirrus-like clouds that trap outgoing infrared radiation, with radiative
forcing potentially comparable to or exceeding that of aviation's CO2
emissions. While physical models simulate contrail formation, evolution and
dissipation, validating and calibrating these models requires linking observed
contrails to the flights that generated them, a process known as
contrail-to-flight attribution. Satellite-based attribution is challenging due
to limited spatial and temporal resolution, as contrails often drift and deform
before detection. In this paper, we evaluate an alternative approach using
ground-based cameras, which capture contrails shortly after formation at high
spatial and temporal resolution, when they remain thin, linear, and visually
distinct. Leveraging the ground visible camera contrail sequences (GVCCS)
dataset, we introduce a modular framework for attributing contrails observed
using ground-based cameras to theoretical contrails derived from aircraft
surveillance and meteorological data. The framework accommodates multiple
geometric representations and distance metrics, incorporates temporal
smoothing, and enables flexible probability-based assignment strategies. This
work establishes a strong baseline and provides a modular framework for future
research in linking contrails to their source flight.

</details>


### [194] [Beyond RGB: Leveraging Vision Transformers for Thermal Weapon Segmentation](https://arxiv.org/abs/2510.16913)
*Akhila Kambhatla,Ahmed R Khaled*

Main category: cs.CV

TL;DR: This paper evaluates transformer-based architectures for thermal weapon segmentation, showing SegFormer-b5 achieves best accuracy (94.15% mIoU) while SegFormer-b0 offers fastest inference (98.32 FPS).


<details>
  <summary>Details</summary>
Motivation: Thermal weapon segmentation is crucial for surveillance in low-light/obscured conditions where RGB systems fail. While CNNs dominate thermal segmentation, they struggle with long-range dependencies and fine details. Vision Transformers show promise in RGB segmentation but remain underexplored for thermal weapon detection.

Method: Evaluated four transformer architectures (SegFormer, DeepLabV3+, SegNeXt, Swin Transformer) on custom thermal dataset of 9,711 images from real surveillance videos, automatically annotated using SAM2. Used standard augmentation in MMSegmentation framework for robust training and fair comparison.

Result: SegFormer-b5 achieved highest mIoU (94.15%) and Pixel Accuracy (97.04%). SegFormer-b0 provided fastest inference (98.32 FPS) with competitive mIoU (90.84%). SegNeXt-mscans offered balanced performance (85.12 FPS, 92.24% mIoU). DeepLabV3+ R101-D8 reached 92.76% mIoU at 29.86 FPS.

Conclusion: Transformer architectures demonstrate robust generalization for weapon detection in low-light/occluded thermal environments, with flexible accuracy-speed trade-offs suitable for diverse real-time security applications.

Abstract: Thermal weapon segmentation is crucial for surveillance and security
applications, enabling robust detection under lowlight and visually obscured
conditions where RGB-based systems fail. While convolutional neural networks
(CNNs) dominate thermal segmentation literature, their ability to capture
long-range dependencies and fine structural details is limited. Vision
Transformers (ViTs), with their global context modeling capabilities, have
achieved state-of-the-art results in RGB segmentation tasks, yet their
potential in thermal weapon segmentation remains underexplored. This work
adapts and evaluates four transformer-based architectures SegFormer,
DeepLabV3\+, SegNeXt, and Swin Transformer for binary weapon segmentation on a
custom thermal dataset comprising 9,711 images collected from real world
surveillance videos and automatically annotated using SAM2. We employ standard
augmentation strategies within the MMSegmentation framework to ensure robust
model training and fair architectural comparison. Experimental results
demonstrate significant improvements in segmentation performance: SegFormer-b5
achieves the highest mIoU (94.15\%) and Pixel Accuracy (97.04\%), while
SegFormer-b0 provides the fastest inference speed (98.32 FPS) with competitive
mIoU (90.84\%). SegNeXt-mscans offers balanced performance with 85.12 FPS and
92.24\% mIoU, and DeepLabV3\+ R101-D8 reaches 92.76\% mIoU at 29.86 FPS. The
transformer architectures demonstrate robust generalization capabilities for
weapon detection in low-light and occluded thermal environments, with flexible
accuracy-speed trade-offs suitable for diverse real-time security applications.

</details>


### [195] [Res-Bench: Benchmarking the Robustness of Multimodal Large Language Models to Dynamic Resolution Input](https://arxiv.org/abs/2510.16926)
*Chenxu Li,Zhicai Wang,Yuan Sheng,Xingyu Zhu,Yanbin Hao,Xiang Wang*

Main category: cs.CV

TL;DR: Res-Bench is a benchmark for evaluating resolution robustness in Multimodal Large Language Models (MLLMs), introducing metrics to assess performance stability across different input resolutions rather than just semantic accuracy.


<details>
  <summary>Details</summary>
Motivation: Current MLLM evaluations focus on semantic performance but overlook resolution robustness - whether performance remains stable across varying input resolutions, creating a critical gap in assessment.

Method: Created Res-Bench with 14,400 samples across 12 resolution levels and 6 capability dimensions. Introduced novel robustness metrics: Spearman's correlation for resolution-performance trends, and Absolute/Relative Continuous Error (ACE/RCE) for performance volatility.

Result: Conducted large-scale evaluation of leading MLLMs examining model-centric and task-centric robustness, preprocessing strategies (padding, super-resolution), and fine-tuning for stability enhancement.

Conclusion: The paper establishes a comprehensive framework for evaluating resolution robustness in MLLMs, addressing a previously overlooked aspect of model performance assessment.

Abstract: Multimodal Large Language Models (MLLMs) increasingly support dynamic image
resolutions. However, current evaluation paradigms primarily assess semantic
performance, overlooking the critical question of resolution robustness -
whether performance remains stable across varying input resolutions. To address
this gap, we introduce \textbf{Res-Bench}, a comprehensive benchmark comprising
14,400 samples across 12 resolution levels and six core capability dimensions.
We designed a novel evaluation framework that goes beyond traditional accuracy
metrics to capture performance stability. This framework introduces multiple
robustness metrics: Spearman's correlation for assessing resolution-performance
trends, and Absolute/Relative Continuous Error (ACE/RCE) for measuring
performance volatility. Using these metrics, we conducted a large-scale
evaluation of leading MLLMs. Our analysis encompasses: (1) model-centric and
task-centric robustness examination, (2) investigation of preprocessing
strategies including padding and super-resolution, and (3) exploration of
fine-tuning for stability enhancement.

</details>


### [196] [Foundation Models in Medical Image Analysis: A Systematic Review and Meta-Analysis](https://arxiv.org/abs/2510.16973)
*Praveenbalaji Rajendran,Mojtaba Safari,Wenfeng He,Mingzhe Hu,Shansong Wang,Jun Zhou,Xiaofeng Yang*

Main category: cs.CV

TL;DR: This review provides a comprehensive analysis of foundation models (FMs) in medical image analysis, categorizing them into vision-only and vision-language models, analyzing trends, and discussing challenges and future directions.


<details>
  <summary>Details</summary>
Motivation: The field of foundation models in medical imaging is fragmented and lacks a unified synthesis that systematically maps the evolution of architectures, training paradigms, and clinical applications across modalities.

Method: Systematic categorization of studies into vision-only and vision-language FMs based on architectural foundations, training strategies, and downstream clinical tasks, plus quantitative meta-analysis of temporal trends in dataset utilization and application domains.

Result: The review provides a structured analysis of FM evolution in medical imaging, identifying trends in dataset usage and application areas, and critically evaluating current challenges and emerging solutions.

Conclusion: Key future research directions are identified to enhance robustness, explainability, and clinical integration of FMs, accelerating their translation into real-world medical practice.

Abstract: Recent advancements in artificial intelligence (AI), particularly foundation
models (FMs), have revolutionized medical image analysis, demonstrating strong
zero- and few-shot performance across diverse medical imaging tasks, from
segmentation to report generation. Unlike traditional task-specific AI models,
FMs leverage large corpora of labeled and unlabeled multimodal datasets to
learn generalized representations that can be adapted to various downstream
clinical applications with minimal fine-tuning. However, despite the rapid
proliferation of FM research in medical imaging, the field remains fragmented,
lacking a unified synthesis that systematically maps the evolution of
architectures, training paradigms, and clinical applications across modalities.
To address this gap, this review article provides a comprehensive and
structured analysis of FMs in medical image analysis. We systematically
categorize studies into vision-only and vision-language FMs based on their
architectural foundations, training strategies, and downstream clinical tasks.
Additionally, a quantitative meta-analysis of the studies was conducted to
characterize temporal trends in dataset utilization and application domains. We
also critically discuss persistent challenges, including domain adaptation,
efficient fine-tuning, computational constraints, and interpretability along
with emerging solutions such as federated learning, knowledge distillation, and
advanced prompting. Finally, we identify key future research directions aimed
at enhancing the robustness, explainability, and clinical integration of FMs,
thereby accelerating their translation into real-world medical practice.

</details>


### [197] [One-step Diffusion Models with Bregman Density Ratio Matching](https://arxiv.org/abs/2510.16983)
*Yuanzhi Zhu,Eleftherios Tsonis,Lucas Degeorge,Vicky Kalogeiton*

Main category: cs.CV

TL;DR: Di-Bregman is a theoretical framework for diffusion distillation that uses Bregman divergence-based density-ratio matching to accelerate multi-step diffusion models into efficient one-step generators.


<details>
  <summary>Details</summary>
Motivation: Diffusion models achieve high quality but are computationally expensive due to slow multi-step sampling. Existing distillation methods lack unified theoretical foundations.

Method: Proposes Di-Bregman framework that formulates diffusion distillation as Bregman divergence-based density-ratio matching, connecting existing objectives through a convex-analytic lens.

Result: Achieves improved one-step FID over reverse-KL distillation on CIFAR-10 and text-to-image generation, maintaining high visual fidelity compared to teacher models.

Conclusion: Bregman density-ratio matching provides a practical and theoretically-grounded approach for efficient one-step diffusion generation.

Abstract: Diffusion and flow models achieve high generative quality but remain
computationally expensive due to slow multi-step sampling. Distillation methods
accelerate them by training fast student generators, yet most existing
objectives lack a unified theoretical foundation. In this work, we propose
Di-Bregman, a compact framework that formulates diffusion distillation as
Bregman divergence-based density-ratio matching. This convex-analytic view
connects several existing objectives through a common lens. Experiments on
CIFAR-10 and text-to-image generation demonstrate that Di-Bregman achieves
improved one-step FID over reverse-KL distillation and maintains high visual
fidelity compared to the teacher model. Our results highlight Bregman
density-ratio matching as a practical and theoretically-grounded route toward
efficient one-step diffusion generation.

</details>


### [198] [CARE: Contrastive Alignment for ADL Recognition from Event-Triggered Sensor Streams](https://arxiv.org/abs/2510.16988)
*Junhao Zhao,Zishuai Liu,Ruili Fang,Jin Lu,Linghan Zhang,Fei Dou*

Main category: cs.CV

TL;DR: CARE is a framework for ADL recognition that aligns sequence- and image-based sensor representations through contrastive learning, achieving state-of-the-art performance on CASAS datasets.


<details>
  <summary>Details</summary>
Motivation: Existing ADL recognition methods have limitations: sequence-based approaches lack spatial awareness and are noise-sensitive, while image-based approaches lose temporal dynamics and distort sensor layouts. Simple fusion methods fail to properly align these complementary representations.

Method: Proposes CARE framework with Sequence-Image Contrastive Alignment (SICA) that jointly optimizes representation learning and classification. Combines time-aware sequence encoding with spatially-informed image representations using a joint contrastive-classification objective for end-to-end learning.

Result: Achieved state-of-the-art performance on three CASAS datasets: 89.8% on Milan, 88.9% on Cairo, and 73.3% on Kyoto7. Demonstrated robustness to sensor malfunctions and layout variability.

Conclusion: CARE effectively leverages complementary strengths of sequence and image representations through contrastive alignment, enabling reliable ADL recognition in smart homes with improved performance and robustness.

Abstract: The recognition of Activities of Daily Living (ADLs) from event-triggered
ambient sensors is an essential task in Ambient Assisted Living, yet existing
methods remain constrained by representation-level limitations. Sequence-based
approaches preserve temporal order of sensor activations but are sensitive to
noise and lack spatial awareness, while image-based approaches capture global
patterns and implicit spatial correlations but compress fine-grained temporal
dynamics and distort sensor layouts. Naive fusion (e.g., feature concatenation)
fail to enforce alignment between sequence- and image-based representation
views, underutilizing their complementary strengths. We propose Contrastive
Alignment for ADL Recognition from Event-Triggered Sensor Streams (CARE), an
end-to-end framework that jointly optimizes representation learning via
Sequence-Image Contrastive Alignment (SICA) and classification via
cross-entropy, ensuring both cross-representation alignment and task-specific
discriminability. CARE integrates (i) time-aware, noise-resilient sequence
encoding with (ii) spatially-informed and frequency-sensitive image
representations, and employs (iii) a joint contrastive-classification objective
for end-to-end learning of aligned and discriminative embeddings. Evaluated on
three CASAS datasets, CARE achieves state-of-the-art performance (89.8% on
Milan, 88.9% on Cairo, and 73.3% on Kyoto7) and demonstrates robustness to
sensor malfunctions and layout variability, highlighting its potential for
reliable ADL recognition in smart homes.

</details>


### [199] [Training-free Online Video Step Grounding](https://arxiv.org/abs/2510.16989)
*Luca Zanella,Massimiliano Mancini,Yiming Wang,Alessio Tonioni,Elisa Ricci*

Main category: cs.CV

TL;DR: The paper introduces BaGLM, a zero-shot online method for Video Step Grounding that uses Large Multimodal Models without training, outperforming training-based offline methods.


<details>
  <summary>Details</summary>
Motivation: Standard VSG methods require labeled training data and process full videos offline, which is costly and limits online applications. The authors explore performing VSG online without training.

Method: Uses LMMs to predict steps from limited frames, then develops BaGLM which applies Bayesian filtering with step transition knowledge from LLMs and step progress estimation.

Result: The online zero-shot approach outperforms offline training-based models. BaGLM shows superior performance over state-of-the-art training-based offline methods on three datasets.

Conclusion: BaGLM demonstrates that zero-shot online VSG using LMMs with Bayesian filtering can achieve better performance than traditional training-based offline approaches.

Abstract: Given a task and a set of steps composing it, Video Step Grounding (VSG) aims
to detect which steps are performed in a video. Standard approaches for this
task require a labeled training set (e.g., with step-level annotations or
narrations), which may be costly to collect. Moreover, they process the full
video offline, limiting their applications for scenarios requiring online
decisions. Thus, in this work, we explore how to perform VSG online and without
training. We achieve this by exploiting the zero-shot capabilities of recent
Large Multimodal Models (LMMs). In particular, we use LMMs to predict the step
associated with a restricted set of frames, without access to the whole video.
We show that this online strategy without task-specific tuning outperforms
offline and training-based models. Motivated by this finding, we develop
Bayesian Grounding with Large Multimodal Models (BaGLM), further injecting
knowledge of past frames into the LMM-based predictions. BaGLM exploits
Bayesian filtering principles, modeling step transitions via (i) a dependency
matrix extracted through large language models and (ii) an estimation of step
progress. Experiments on three datasets show superior performance of BaGLM over
state-of-the-art training-based offline methods.

</details>


### [200] [An empirical study of the effect of video encoders on Temporal Video Grounding](https://arxiv.org/abs/2510.17007)
*Ignacio M. De la Jara,Cristian Rodriguez-Opazo,Edison Marrese-Taylor,Felipe Bravo-Marquez*

Main category: cs.CV

TL;DR: An empirical study investigating how different video features (CNN-based, temporal reasoning, and transformer-based) affect temporal video grounding performance across three benchmarks.


<details>
  <summary>Details</summary>
Motivation: Research in temporal video grounding has focused on a limited selection of video representations, potentially leading to architectural overfitting. The study aims to explore the impact of diverse video features.

Method: Extracted features from three benchmarks (Charades-STA, ActivityNet-Captions, YouCookII) using various video encoders including CNN-based, temporal reasoning, and transformer architectures, then evaluated on a classical grounding model.

Result: Significant performance differences observed by simply changing the video encoder, with clear patterns and errors emerging from different feature types, suggesting potential feature complementarity.

Conclusion: Video feature selection significantly impacts temporal video grounding performance, and different feature types exhibit complementary strengths that could be leveraged for improved models.

Abstract: Temporal video grounding is a fundamental task in computer vision, aiming to
localize a natural language query in a long, untrimmed video. It has a key role
in the scientific community, in part due to the large amount of video generated
every day. Although we find extensive work in this task, we note that research
remains focused on a small selection of video representations, which may lead
to architectural overfitting in the long run. To address this issue, we propose
an empirical study to investigate the impact of different video features on a
classical architecture. We extract features for three well-known benchmarks,
Charades-STA, ActivityNet-Captions and YouCookII, using video encoders based on
CNNs, temporal reasoning and transformers. Our results show significant
differences in the performance of our model by simply changing the video
encoder, while also revealing clear patterns and errors derived from the use of
certain features, ultimately indicating potential feature complementarity.

</details>


### [201] [Do Satellite Tasks Need Special Pretraining?](https://arxiv.org/abs/2510.17014)
*Ani Vanyan,Alvard Barseghyan,Hakob Tamazyan,Tigran Galstyan,Vahan Huroyan,Naira Hovakimyan,Hrant Khachatrian*

Main category: cs.CV

TL;DR: The paper challenges the need for specialized remote sensing foundation models, showing that general-purpose vision models perform equally well at small scales.


<details>
  <summary>Details</summary>
Motivation: To systematically test whether specialized foundation models for remote sensing provide meaningful advantages over general-purpose vision foundation models.

Method: Created a benchmark for measuring generalization to lower resolution images, and trained iBOT self-supervised encoder on MillionAID dataset with remote sensing-specific modifications.

Result: None of the specialized pretrained models showed consistent improvements over general-purpose baselines at ViT-B scale.

Conclusion: Specialized remote sensing foundation models may not be necessary for small-scale applications, as general-purpose models perform equally well.

Abstract: Foundation models have advanced machine learning across various modalities,
including images. Recently multiple teams trained foundation models specialized
for remote sensing applications. This line of research is motivated by the
distinct characteristics of remote sensing imagery, specific applications and
types of robustness useful for satellite image analysis. In this work we
systematically challenge the idea that specific foundation models are more
useful than general-purpose vision foundation models, at least in the small
scale. First, we design a simple benchmark that measures generalization of
remote sensing models towards images with lower resolution for two downstream
tasks. Second, we train iBOT, a self-supervised vision encoder, on MillionAID,
an ImageNet-scale satellite imagery dataset, with several modifications
specific to remote sensing. We show that none of those pretrained models bring
consistent improvements upon general-purpose baselines at the ViT-B scale.

</details>


### [202] [Enrich and Detect: Video Temporal Grounding with Multimodal LLMs](https://arxiv.org/abs/2510.17023)
*Shraman Pramanick,Effrosyni Mavroudi,Yale Song,Rama Chellappa,Lorenzo Torresani,Triantafyllos Afouras*

Main category: cs.CV

TL;DR: ED-VTG is a two-stage method for video temporal grounding that uses multimodal LLMs to enrich text queries before grounding them with a lightweight decoder, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To leverage multimodal LLMs for fine-grained video temporal grounding by addressing the challenge of directly grounding natural language queries through query enrichment and noise reduction.

Method: Two-stage approach: 1) Transform language queries into enriched sentences with additional details, 2) Ground enriched queries using lightweight decoder with multiple-instance-learning objective to select optimal query versions.

Result: State-of-the-art results across temporal video grounding benchmarks, significantly outperforming previous LLM-based approaches and comparable to specialized models, with strong zero-shot performance.

Conclusion: ED-VTG effectively combines multimodal LLM capabilities with specialized grounding techniques, demonstrating superior performance in both standard and zero-shot evaluation scenarios for video temporal grounding.

Abstract: We introduce ED-VTG, a method for fine-grained video temporal grounding
utilizing multi-modal large language models. Our approach harnesses the
capabilities of multimodal LLMs to jointly process text and video, in order to
effectively localize natural language queries in videos through a two-stage
process. Rather than being directly grounded, language queries are initially
transformed into enriched sentences that incorporate missing details and cues
to aid in grounding. In the second stage, these enriched queries are grounded,
using a lightweight decoder, which specializes at predicting accurate
boundaries conditioned on contextualized representations of the enriched
queries. To mitigate noise and reduce the impact of hallucinations, our model
is trained with a multiple-instance-learning objective that dynamically selects
the optimal version of the query for each training sample. We demonstrate
state-of-the-art results across various benchmarks in temporal video grounding
and paragraph grounding settings. Experiments reveal that our method
significantly outperforms all previously proposed LLM-based temporal grounding
approaches and is either superior or comparable to specialized models, while
maintaining a clear advantage against them in zero-shot evaluation scenarios.

</details>


### [203] [Where, Not What: Compelling Video LLMs to Learn Geometric Causality for 3D-Grounding](https://arxiv.org/abs/2510.17034)
*Yutong Zhong*

Main category: cs.CV

TL;DR: W2R2 is a training framework that addresses 2D semantic bias in multimodal 3D grounding by disentangling 2D semantic features and 3D spatial features through dual-objective loss functions, improving localization accuracy without changing inference architecture.


<details>
  <summary>Details</summary>
Motivation: Current multimodal 3D grounding models suffer from severe "2D semantic bias" - over-relying on 2D image features while largely ignoring 3D geometric inputs, leading to suboptimal fusion performance and poor spatial reasoning.

Method: Proposes What-Where Representation Re-Forming (W2R2) framework with disentangled representation learning: 2D features as semantic beacons for "What" identification and 3D features as spatial anchors for "Where" localization. Uses dual-objective loss with Alignment Loss for multimodal synergy and Pseudo-Label Loss to suppress 2D-dominant shortcuts.

Result: Experiments on ScanRefer and ScanQA show significant gains in localization accuracy and robustness, particularly in cluttered outdoor scenes, demonstrating the framework's effectiveness.

Conclusion: W2R2 successfully mitigates 2D semantic bias through targeted representation disentanglement and shortcut suppression, enabling precise 3D grounding while maintaining the same inference architecture.

Abstract: Multimodal 3D grounding has garnered considerable interest in Vision-Language
Models (VLMs) \cite{yin2025spatial} for advancing spatial reasoning in complex
environments. However, these models suffer from a severe "2D semantic bias"
that arises from over-reliance on 2D image features for coarse localization,
largely disregarding 3D geometric inputs and resulting in suboptimal fusion
performance. In this paper, we propose a novel training framework called
What-Where Representation Re-Forming (W2R2) to tackle this issue via
disentangled representation learning and targeted shortcut suppression. Our
approach fundamentally reshapes the model's internal space by designating 2D
features as semantic beacons for "What" identification and 3D features as
spatial anchors for "Where" localization, enabling precise 3D grounding without
modifying inference architecture. Key components include a dual-objective loss
function with an Alignment Loss that supervises fused predictions using adapted
cross-entropy for multimodal synergy, and a Pseudo-Label Loss that penalizes
overly effective 2D-dominant pseudo-outputs via a margin-based mechanism.
Experiments conducted on ScanRefer and ScanQA demonstrate the effectiveness of
W2R2, with significant gains in localization accuracy and robustness,
particularly in cluttered outdoor scenes.

</details>


### [204] [Conditional Synthetic Live and Spoof Fingerprint Generation](https://arxiv.org/abs/2510.17035)
*Syed Konain Abbas,Sandip Purnapatra,M. G. Sarwar Murshed,Conor Miller-Lynch,Lambert Igene,Soumyabrata Dey,Stephanie Schuckers,Faraz Hussain*

Main category: cs.CV

TL;DR: This paper presents a novel approach for generating synthetic fingerprint images using conditional StyleGAN2-ADA and StyleGAN3 architectures for live fingerprints and CycleGANs for spoof fingerprints, addressing privacy, cost, and accessibility issues in biometric data collection.


<details>
  <summary>Details</summary>
Motivation: Large fingerprint datasets are time-consuming and expensive to collect and require strict privacy measures. Researchers are exploring synthetic fingerprint data to address these issues.

Method: Uses conditional StyleGAN2-ADA and StyleGAN3 architectures to generate high-resolution synthetic live fingerprints conditioned on specific finger identities, and CycleGANs to translate these into realistic spoof fingerprints simulating various presentation attack materials.

Result: Created two synthetic datasets (DB2 and DB3) with 1,500 fingerprint images each. StyleGAN3 achieved FID as low as 5, with True Accept Rate of 99.47% at 0.01% False Accept Rate. StyleGAN2-ADA achieved 98.67% TAR at same FAR. Quality metrics (NFIQ2, MINDTCT) confirm strong performance and privacy preservation with no significant identity leakage.

Conclusion: The synthetic fingerprint generation approach successfully addresses privacy, cost, and accessibility concerns while maintaining high quality and strong privacy-preserving properties, making it suitable for developing robust spoof detection systems.

Abstract: Large fingerprint datasets, while important for training and evaluation, are
time-consuming and expensive to collect and require strict privacy measures.
Researchers are exploring the use of synthetic fingerprint data to address
these issues. This paper presents a novel approach for generating synthetic
fingerprint images (both spoof and live), addressing concerns related to
privacy, cost, and accessibility in biometric data collection. Our approach
utilizes conditional StyleGAN2-ADA and StyleGAN3 architectures to produce
high-resolution synthetic live fingerprints, conditioned on specific finger
identities (thumb through little finger). Additionally, we employ CycleGANs to
translate these into realistic spoof fingerprints, simulating a variety of
presentation attack materials (e.g., EcoFlex, Play-Doh). These synthetic spoof
fingerprints are crucial for developing robust spoof detection systems. Through
these generative models, we created two synthetic datasets (DB2 and DB3), each
containing 1,500 fingerprint images of all ten fingers with multiple
impressions per finger, and including corresponding spoofs in eight material
types. The results indicate robust performance: our StyleGAN3 model achieves a
Fr\'echet Inception Distance (FID) as low as 5, and the generated fingerprints
achieve a True Accept Rate of 99.47% at a 0.01% False Accept Rate. The
StyleGAN2-ADA model achieved a TAR of 98.67% at the same 0.01% FAR. We assess
fingerprint quality using standard metrics (NFIQ2, MINDTCT), and notably,
matching experiments confirm strong privacy preservation, with no significant
evidence of identity leakage, confirming the strong privacy-preserving
properties of our synthetic datasets.

</details>


### [205] [Click, Predict, Trust: Clinician-in-the-Loop AI Segmentation for Lung Cancer CT-Based Prognosis within the Knowledge-to-Action Framework](https://arxiv.org/abs/2510.17039)
*Mohammad R. Salmanpour,Sonya Falahati,Amir Hossein Pouria,Amin Mousavi,Somayeh Sadat Mehrnia,Morteza Alizadeh,Arman Gorji,Zeinab Farsangi,Alireza Safarian,Mehdi Maghsudi,Carlos Uribe,Arman Rahmim,Ren Yuan*

Main category: cs.CV

TL;DR: A clinician-in-the-loop deep learning pipeline using VNet with semi-supervised learning achieves accurate, reproducible lung cancer segmentation and prognosis from CT scans, with high clinical acceptance for mask refinement rather than replacement.


<details>
  <summary>Details</summary>
Motivation: Manual lung cancer segmentation is variable and time-consuming, while existing deep learning methods face barriers to clinical adoption. There's a need for automated solutions that enhance reproducibility, prognostic accuracy, and clinical trust.

Method: Multi-center CT data from 999 patients across 12 datasets analyzed using five DL models (3D Attention U-Net, ResUNet, VNet, ReconNet, SAM-Med3D) benchmarked against expert contours. Assessed segmentation reproducibility via 497 radiomic features and prognostic modeling comparing supervised vs semi-supervised learning across multiple strategies. Six physicians qualitatively evaluated masks across clinical domains.

Result: VNet achieved best performance (Dice=0.83, IoU=0.71), radiomic stability (mean correlation=0.76, ICC=0.65), and predictive accuracy under SSL (accuracy=0.88, F1=0.83). SSL consistently outperformed SL. Radiologists preferred VNet for peritumoral representation and AI-generated masks for refinement rather than replacement.

Conclusion: Integrating VNet with SSL yields accurate, reproducible, and clinically trusted CT-based lung cancer prognosis, providing a feasible path toward physician-centered AI translation in clinical practice.

Abstract: Lung cancer remains the leading cause of cancer mortality, with CT imaging
central to screening, prognosis, and treatment. Manual segmentation is variable
and time-intensive, while deep learning (DL) offers automation but faces
barriers to clinical adoption. Guided by the Knowledge-to-Action framework,
this study develops a clinician-in-the-loop DL pipeline to enhance
reproducibility, prognostic accuracy, and clinical trust. Multi-center CT data
from 999 patients across 12 public datasets were analyzed using five DL models
(3D Attention U-Net, ResUNet, VNet, ReconNet, SAM-Med3D), benchmarked against
expert contours on whole and click-point cropped images. Segmentation
reproducibility was assessed using 497 PySERA-extracted radiomic features via
Spearman correlation, ICC, Wilcoxon tests, and MANOVA, while prognostic
modeling compared supervised (SL) and semi-supervised learning (SSL) across 38
dimensionality reduction strategies and 24 classifiers. Six physicians
qualitatively evaluated masks across seven domains, including clinical
meaningfulness, boundary quality, prognostic value, trust, and workflow
integration. VNet achieved the best performance (Dice = 0.83, IoU = 0.71),
radiomic stability (mean correlation = 0.76, ICC = 0.65), and predictive
accuracy under SSL (accuracy = 0.88, F1 = 0.83). SSL consistently outperformed
SL across models. Radiologists favored VNet for peritumoral representation and
smoother boundaries, preferring AI-generated initial masks for refinement
rather than replacement. These results demonstrate that integrating VNet with
SSL yields accurate, reproducible, and clinically trusted CT-based lung cancer
prognosis, highlighting a feasible path toward physician-centered AI
translation.

</details>


### [206] [Person Re-Identification via Generalized Class Prototypes](https://arxiv.org/abs/2510.17043)
*Md Ahmed Al Muzaddid,William J. Beksi*

Main category: cs.CV

TL;DR: The paper proposes a generalized selection method for class representations in person re-identification that goes beyond traditional centroid-based approaches, achieving improved accuracy and mean average precision.


<details>
  <summary>Details</summary>
Motivation: Current person re-identification methods focus heavily on feature extraction and objective functions, but class representation selection remains underexplored. Prior centroid-based approaches yield suboptimal results, creating a need for better representation strategies.

Method: A generalized selection method that chooses class representations not limited to centroids, allowing adjustable number of representations per class to meet specific application requirements. Applied on top of multiple re-identification embeddings.

Result: The approach substantially improves upon contemporary results across all tested re-identification embeddings, achieving better balance between accuracy and mean average precision.

Conclusion: The proposed generalized representation selection method effectively addresses limitations of centroid-based approaches and advances the state of the art in person re-identification performance.

Abstract: Advanced feature extraction methods have significantly contributed to
enhancing the task of person re-identification. In addition, modifications to
objective functions have been developed to further improve performance.
Nonetheless, selecting better class representatives is an underexplored area of
research that can also lead to advancements in re-identification performance.
Although past works have experimented with using the centroid of a gallery
image class during training, only a few have investigated alternative
representations during the retrieval stage. In this paper, we demonstrate that
these prior techniques yield suboptimal results in terms of re-identification
metrics. To address the re-identification problem, we propose a generalized
selection method that involves choosing representations that are not limited to
class centroids. Our approach strikes a balance between accuracy and mean
average precision, leading to improvements beyond the state of the art. For
example, the actual number of representations per class can be adjusted to meet
specific application requirements. We apply our methodology on top of multiple
re-identification embeddings, and in all cases it substantially improves upon
contemporary results

</details>


### [207] [Video Reasoning without Training](https://arxiv.org/abs/2510.17045)
*Deepak Sridhar,Kartikeya Bhardwaj,Jeya Pradha Jeyaraj,Nuno Vasconcelos,Ankita Nayak,Harris Teague*

Main category: cs.CV

TL;DR: V-Reason improves video reasoning in Large Multimodal Models by optimizing entropy during inference, reducing computational costs and output tokens without requiring reinforcement learning or fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Current video reasoning methods using LMMs rely on expensive reinforcement learning and verbose chain-of-thought, causing high computational overhead and limited control over the thinking process.

Method: Uses entropy as a signal to observe micro-exploration/exploitation patterns, then adapts the model's value cache during inference via a small trainable controller using entropy-based optimization without any dataset supervision.

Result: Achieves significant improvements over base instruction-tuned models, narrowing gap with RL-trained models to within 0.6% average accuracy, while reducing output tokens by 58.6% compared to RL models.

Conclusion: The proposed inference-time entropy optimization enables efficient video reasoning without costly training, demonstrating that model behavior can be effectively tuned during inference using theoretically-grounded insights.

Abstract: Video reasoning using Large Multimodal Models (LMMs) relies on costly
reinforcement learning (RL) and verbose chain-of-thought, resulting in
substantial computational overhead during both training and inference.
Moreover, the mechanisms that control the thinking process in these reasoning
models are very limited. In this paper, using entropy of the model's output as
a signal, we discover that the high-quality models go through a series of
micro-explorations and micro-exploitations which keep the reasoning process
grounded (i.e., avoid excessive randomness while the model is exploring or
thinking through an answer). We further observe that once this "thinking"
process is over, more accurate models demonstrate a better convergence by
reducing the entropy significantly via a final exploitation phase (i.e., a more
certain convergence towards a solution trajectory). We then use these novel,
theoretically-grounded insights to tune the model's behavior directly at
inference, without using any RL or supervised fine-tuning. Specifically, during
inference, our proposed approach called V-Reason (Video-Reason) adapts the
value cache of the LMM via a few optimization steps on a small, trainable
controller using an entropy-based objective, i.e., no supervision from any
dataset or RL is necessary. This tuning improves the model's micro-exploration
and exploitation behavior during inference. Our experiments show that our
proposed method achieves significant improvements over the base
instruction-tuned models across several video reasoning datasets, narrowing the
gap with RL-trained models to within 0.6% average accuracy without any
training, while offering massive efficiency benefits: output tokens are reduced
by 58.6% compared to the RL model.

</details>


### [208] [How Universal Are SAM2 Features?](https://arxiv.org/abs/2510.17051)
*Masoud Khairi Atani,Alon Harell,Hyomin Choi,Runyu Yang,Fabien Racape,Ivan V. Bajic*

Main category: cs.CV

TL;DR: This paper investigates the trade-off between general-purpose foundation models (Hiera) and specialized models (SAM2) for vision tasks, quantifying the information-theoretic cost of specialization and revealing that while SAM2 excels at spatially-related tasks, it loses broader semantic information for conceptually distant tasks.


<details>
  <summary>Details</summary>
Motivation: To understand the trade-off between general-purpose foundation vision models and specialized counterparts for efficient feature coding design, as this balance is critical but not yet fully understood.

Method: Compared feature versatility of general-purpose Hiera encoder against segmentation-specialized SAM2 using a lightweight, trainable neck to probe adaptability of frozen features, and conducted cross-neck analysis to quantify representational bottlenecks.

Result: SAM2's specialization is highly effective for spatially-related tasks like depth estimation but underperforms Hiera on conceptually distant tasks such as pose estimation and image captioning, demonstrating measurable loss of broader semantic information. Cross-neck analysis shows each adaptation level creates further representational bottlenecks.

Conclusion: The analysis illuminates trade-offs in feature universality, providing quantitative foundation for designing efficient feature coding and adaptation strategies for diverse downstream applications, highlighting the cost-benefit relationship between specialization and generalization.

Abstract: The trade-off between general-purpose foundation vision models and their
specialized counterparts is critical for efficient feature coding design and is
not yet fully understood. We investigate this trade-off by comparing the
feature versatility of the general-purpose Hiera encoder against the
segmentation-specialized Segment Anything Model 2 (SAM2). Using a lightweight,
trainable neck to probe the adaptability of their frozen features, we quantify
the information-theoretic cost of specialization. Our results reveal that while
SAM2's specialization is highly effective for spatially-related tasks like
depth estimation, it comes at a cost. The specialized SAM2 encoder
underperforms its generalist predecessor, Hiera, on conceptually distant tasks
such as pose estimation and image captioning, demonstrating a measurable loss
of broader semantic information. A novel cross-neck analysis on SAM2 reveals
that each level of adaptation creates a further representational bottleneck.
Our analysis illuminates these trade-offs in feature universality, providing a
quantitative foundation for designing efficient feature coding and adaptation
strategies for diverse downstream applications.

</details>


### [209] [ProDAT: Progressive Density-Aware Tail-Drop for Point Cloud Coding](https://arxiv.org/abs/2510.17068)
*Zhe Luo,Wenjing Jia,Stuart Perry*

Main category: cs.CV

TL;DR: ProDAT enables progressive point cloud coding using density-aware tail-drop mechanism, allowing single-model decoding at multiple bitrates with improved efficiency.


<details>
  <summary>Details</summary>
Motivation: 3D point clouds need real-time processing but face bandwidth constraints; existing learning-based methods lack progressive decoding capability.

Method: Proposed ProDAT with density-aware tail-drop mechanism that adaptively decodes latent features and coordinates based on significance.

Result: Achieves over 28.6% BD-rate improvement on SemanticKITTI and over 18.15% on ShapeNet compared to state-of-the-art methods.

Conclusion: ProDAT successfully bridges the gap for progressive point cloud coding while maintaining superior coding efficiency.

Abstract: Three-dimensional (3D) point clouds are becoming increasingly vital in
applications such as autonomous driving, augmented reality, and immersive
communication, demanding real-time processing and low latency. However, their
large data volumes and bandwidth constraints hinder the deployment of
high-quality services in resource-limited environments. Progres- sive coding,
which allows for decoding at varying levels of detail, provides an alternative
by allowing initial partial decoding with subsequent refinement. Although
recent learning-based point cloud geometry coding methods have achieved notable
success, their fixed latent representation does not support progressive
decoding. To bridge this gap, we propose ProDAT, a novel density-aware
tail-drop mechanism for progressive point cloud coding. By leveraging density
information as a guidance signal, latent features and coordinates are decoded
adaptively based on their significance, therefore achieving progressive
decoding at multiple bitrates using one single model. Experimental results on
benchmark datasets show that the proposed ProDAT not only enables progressive
coding but also achieves superior coding efficiency compared to
state-of-the-art learning-based coding techniques, with over 28.6% BD-rate
improvement for PSNR- D2 on SemanticKITTI and over 18.15% for ShapeNet

</details>


### [210] [Towards a Generalizable Fusion Architecture for Multimodal Object Detection](https://arxiv.org/abs/2510.17078)
*Jad Berjawi,Yoann Dupas,Christophe C'erin*

Main category: cs.CV

TL;DR: FMCAF is a preprocessing architecture that enhances RGB and infrared fusion using frequency filtering and cross-attention, achieving improved object detection performance across different multimodal datasets without dataset-specific tuning.


<details>
  <summary>Details</summary>
Motivation: To improve multimodal object detection robustness by leveraging complementary cues from RGB and infrared sensors, addressing the need for generalizable fusion methods that work across different challenging conditions.

Method: Combines frequency-domain filtering (Freq-Filter) to suppress redundant spectral features with cross-attention-based fusion (MCAF) to improve intermodal feature sharing, creating a preprocessing architecture for RGB-IR fusion.

Result: Outperforms traditional concatenation fusion, achieving +13.9% mAP@50 on VEDAI (aerial vehicle detection) and +1.1% on LLVIP (low-light pedestrian detection).

Conclusion: FMCAF shows potential as a flexible foundation for robust multimodal fusion in future detection pipelines, demonstrating generalizability across different multimodal challenges.

Abstract: Multimodal object detection improves robustness in chal- lenging conditions
by leveraging complementary cues from multiple sensor modalities. We introduce
Filtered Multi- Modal Cross Attention Fusion (FMCAF), a preprocess- ing
architecture designed to enhance the fusion of RGB and infrared (IR) inputs.
FMCAF combines a frequency- domain filtering block (Freq-Filter) to suppress
redun- dant spectral features with a cross-attention-based fusion module (MCAF)
to improve intermodal feature sharing. Unlike approaches tailored to specific
datasets, FMCAF aims for generalizability, improving performance across
different multimodal challenges without requiring dataset- specific tuning. On
LLVIP (low-light pedestrian detec- tion) and VEDAI (aerial vehicle detection),
FMCAF outper- forms traditional fusion (concatenation), achieving +13.9% mAP@50
on VEDAI and +1.1% on LLVIP. These results support the potential of FMCAF as a
flexible foundation for robust multimodal fusion in future detection pipelines.

</details>


### [211] [GSPlane: Concise and Accurate Planar Reconstruction via Structured Representation](https://arxiv.org/abs/2510.17095)
*Ruitong Gan,Junran Peng,Yang Liu,Chuanchen Luo,Qing Li,Zhaoxiang Zhang*

Main category: cs.CV

TL;DR: GSPlane improves Gaussian Splatting by incorporating planar priors to enhance geometric accuracy and mesh structure for planar regions in 3D scenes, while maintaining rendering quality.


<details>
  <summary>Details</summary>
Motivation: Current Gaussian Splatting representations struggle with reconstructing planar regions smoothly and precisely, which is crucial for structured scene editing and physical simulations in man-made environments.

Method: Leverages segmentation and normal prediction models to extract planar priors, establishes structured representations for planar Gaussians, uses Dynamic Gaussian Re-classifier to handle optimization issues, and refines mesh layouts using optimized planar priors.

Result: Significantly improves geometric accuracy of extracted meshes across various baselines without sacrificing rendering quality, produces cleaner mesh connectivity with reduced vertices and faces.

Conclusion: GSPlane successfully addresses planar reconstruction challenges in Gaussian Splatting, enabling more accurate geometry and structured representations that support downstream applications like scene editing and object manipulation.

Abstract: Planes are fundamental primitives of 3D sences, especially in man-made
environments such as indoor spaces and urban streets. Representing these planes
in a structured and parameterized format facilitates scene editing and physical
simulations in downstream applications. Recently, Gaussian Splatting (GS) has
demonstrated remarkable effectiveness in the Novel View Synthesis task, with
extensions showing great potential in accurate surface reconstruction. However,
even state-of-the-art GS representations often struggle to reconstruct planar
regions with sufficient smoothness and precision. To address this issue, we
propose GSPlane, which recovers accurate geometry and produces clean and
well-structured mesh connectivity for plane regions in the reconstructed scene.
By leveraging off-the-shelf segmentation and normal prediction models, GSPlane
extracts robust planar priors to establish structured representations for
planar Gaussian coordinates, which help guide the training process by enforcing
geometric consistency. To further enhance training robustness, a Dynamic
Gaussian Re-classifier is introduced to adaptively reclassify planar Gaussians
with persistently high gradients as non-planar, ensuring more reliable
optimization. Furthermore, we utilize the optimized planar priors to refine the
mesh layouts, significantly improving topological structure while reducing the
number of vertices and faces. We also explore applications of the structured
planar representation, which enable decoupling and flexible manipulation of
objects on supportive planes. Extensive experiments demonstrate that, with no
sacrifice in rendering quality, the introduction of planar priors significantly
improves the geometric accuracy of the extracted meshes across various
baselines.

</details>


### [212] [Boosting Fidelity for Pre-Trained-Diffusion-Based Low-Light Image Enhancement via Condition Refinement](https://arxiv.org/abs/2510.17105)
*Xiaogang Xu,Jian Wang,Yunfan Lu,Ruihang Chu,Ruixing Wang,Jiafei Wu,Bei Yu,Liang Lin*

Main category: cs.CV

TL;DR: A novel optimization strategy for pre-trained diffusion models that enhances fidelity while preserving realism by introducing latent refinement and bidirectional interaction between conditional and noisy latents.


<details>
  <summary>Details</summary>
Motivation: Pre-trained diffusion-based methods often sacrifice content fidelity for perceptual realism, especially in low-light scenarios where degraded information limits effective control.

Method: Proposes a latent refinement pipeline to recover spatial details lost during VAE encoding, and enables dynamic bidirectional interaction between refined conditional latent and noisy latent in the diffusion process.

Result: Extensive experiments demonstrate significant fidelity improvements in pre-trained diffusion-based methods while maintaining realism and aesthetics.

Conclusion: The proposed plug-and-play approach effectively enhances conditioning in diffusion models, providing more effective control for low-level vision tasks.

Abstract: Diffusion-based methods, leveraging pre-trained large models like Stable
Diffusion via ControlNet, have achieved remarkable performance in several
low-level vision tasks. However, Pre-Trained Diffusion-Based (PTDB) methods
often sacrifice content fidelity to attain higher perceptual realism. This
issue is exacerbated in low-light scenarios, where severely degraded
information caused by the darkness limits effective control. We identify two
primary causes of fidelity loss: the absence of suitable conditional latent
modeling and the lack of bidirectional interaction between the conditional
latent and noisy latent in the diffusion process. To address this, we propose a
novel optimization strategy for conditioning in pre-trained diffusion models,
enhancing fidelity while preserving realism and aesthetics. Our method
introduces a mechanism to recover spatial details lost during VAE encoding,
i.e., a latent refinement pipeline incorporating generative priors.
Additionally, the refined latent condition interacts dynamically with the noisy
latent, leading to improved restoration performance. Our approach is
plug-and-play, seamlessly integrating into existing diffusion networks to
provide more effective control. Extensive experiments demonstrate significant
fidelity improvements in PTDB methods.

</details>


### [213] [Towards Imperceptible Watermarking Via Environment Illumination for Consumer Cameras](https://arxiv.org/abs/2510.17114)
*Hodaka Kawachi,Tomoya Nakamura,Hiroaki Santo,SaiKiran Kumar Tedla,Trevor Dalton Canham,Yasushi Yagi,Michael S. Brown*

Main category: cs.CV

TL;DR: A method using LED lighting to create invisible watermarks for cameras by optimizing spectral profiles that are undetectable to humans but visible to consumer cameras.


<details>
  <summary>Details</summary>
Motivation: To develop a privacy-preserving and content verification system that embeds metadata in videos without being noticeable to human viewers.

Method: Optimizes LED spectral profiles using spectral modulation (not intensity modulation), considering human visual sensitivity, camera spectral sensitivity, and LED capabilities to produce perceived white light.

Result: Successfully embeds 128 bits in 10-second video clips at standard frame rates (30-60 fps), providing sufficient capacity for essential metadata.

Conclusion: The approach enables practical invisible watermarking for privacy protection and content verification using standard consumer equipment and lighting conditions.

Abstract: This paper introduces a method for using LED-based environmental lighting to
produce visually imperceptible watermarks for consumer cameras. Our approach
optimizes an LED light source's spectral profile to be minimally visible to the
human eye while remaining highly detectable by typical consumer cameras. The
method jointly considers the human visual system's sensitivity to visible
spectra, modern consumer camera sensors' spectral sensitivity, and narrowband
LEDs' ability to generate broadband spectra perceived as "white light"
(specifically, D65 illumination). To ensure imperceptibility, we employ
spectral modulation rather than intensity modulation. Unlike conventional
visible light communication, our approach enables watermark extraction at
standard low frame rates (30-60 fps). While the information transfer rate is
modest-embedding 128 bits within a 10-second video clip-this capacity is
sufficient for essential metadata supporting privacy protection and content
verification.

</details>


### [214] [GOOD: Training-Free Guided Diffusion Sampling for Out-of-Distribution Detection](https://arxiv.org/abs/2510.17131)
*Xin Gao,Jiyao Liu,Guanghao Li,Yueming Lyu,Jianxiong Gao,Weichen Yu,Ningsheng Xu,Liang Wang,Caifeng Shan,Ziwei Liu,Chenyang Si*

Main category: cs.CV

TL;DR: GOOD is a framework that uses dual-level guidance (image-level and feature-level) with diffusion models to generate diverse out-of-distribution samples for improving OOD detection performance.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-image diffusion methods for OOD sample generation suffer from semantic instability and insufficient shift diversity due to text-embedding perturbations, limiting generalization to realistic OOD scenarios.

Method: GOOD guides diffusion sampling trajectories using ID classifiers with dual-level guidance: (1) image-level guidance reduces input likelihood via gradient of log partition, (2) feature-level guidance uses k-NN distance in classifier's latent space to promote sampling in feature-sparse regions.

Result: GOOD enables more controllable and diverse OOD sample generation, and training with GOOD-generated samples notably enhances OOD detection performance.

Conclusion: The proposed GOOD framework with dual-level guidance and unified OOD score effectively addresses limitations of existing approaches and significantly improves OOD detection robustness.

Abstract: Recent advancements have explored text-to-image diffusion models for
synthesizing out-of-distribution (OOD) samples, substantially enhancing the
performance of OOD detection. However, existing approaches typically rely on
perturbing text-conditioned embeddings, resulting in semantic instability and
insufficient shift diversity, which limit generalization to realistic OOD. To
address these challenges, we propose GOOD, a novel and flexible framework that
directly guides diffusion sampling trajectories towards OOD regions using
off-the-shelf in-distribution (ID) classifiers. GOOD incorporates dual-level
guidance: (1) Image-level guidance based on the gradient of log partition to
reduce input likelihood, drives samples toward low-density regions in pixel
space. (2) Feature-level guidance, derived from k-NN distance in the
classifier's latent space, promotes sampling in feature-sparse regions. Hence,
this dual-guidance design enables more controllable and diverse OOD sample
generation. Additionally, we introduce a unified OOD score that adaptively
combines image and feature discrepancies, enhancing detection robustness. We
perform thorough quantitative and qualitative analyses to evaluate the
effectiveness of GOOD, demonstrating that training with samples generated by
GOOD can notably enhance OOD detection performance.

</details>


### [215] [KineDiff3D: Kinematic-Aware Diffusion for Category-Level Articulated Object Shape Reconstruction and Generation](https://arxiv.org/abs/2510.17137)
*WenBo Xu,Liu Liu,Li Zhang,Ran Zhang,Hao Wu,Dan Guo,Meng Wang*

Main category: cs.CV

TL;DR: KineDiff3D is a unified framework that uses kinematic-aware diffusion models for articulated object reconstruction and pose estimation from single-view inputs, combining VAE encoding with conditional diffusion models and iterative optimization.


<details>
  <summary>Details</summary>
Motivation: Articulated objects like laptops and drawers pose significant challenges for 3D reconstruction and pose estimation due to their multi-part geometries and variable joint configurations, which create structural diversity across different states.

Method: The approach uses a Kinematic-Aware VAE to encode geometry, joint angles, and part segmentation into a structured latent space, employs two conditional diffusion models for pose/joint parameter regression and kinematic-aware latent code generation, and includes an iterative optimization module for bidirectional refinement.

Result: Experimental results on synthetic, semi-synthetic, and real-world datasets demonstrate the effectiveness of the approach in accurately reconstructing articulated objects and estimating their kinematic properties.

Conclusion: KineDiff3D provides an effective unified framework for articulated object reconstruction and kinematic property estimation from single-view observations, addressing the challenges posed by structural diversity in articulated objects.

Abstract: Articulated objects, such as laptops and drawers, exhibit significant
challenges for 3D reconstruction and pose estimation due to their multi-part
geometries and variable joint configurations, which introduce structural
diversity across different states. To address these challenges, we propose
KineDiff3D: Kinematic-Aware Diffusion for Category-Level Articulated Object
Shape Reconstruction and Generation, a unified framework for reconstructing
diverse articulated instances and pose estimation from single view input.
Specifically, we first encode complete geometry (SDFs), joint angles, and part
segmentation into a structured latent space via a novel Kinematic-Aware VAE
(KA-VAE). In addition, we employ two conditional diffusion models: one for
regressing global pose (SE(3)) and joint parameters, and another for generating
the kinematic-aware latent code from partial observations. Finally, we produce
an iterative optimization module that bidirectionally refines reconstruction
accuracy and kinematic parameters via Chamfer-distance minimization while
preserving articulation constraints. Experimental results on synthetic,
semi-synthetic, and real-world datasets demonstrate the effectiveness of our
approach in accurately reconstructing articulated objects and estimating their
kinematic properties.

</details>


### [216] [GACO-CAD: Geometry-Augmented and Conciseness-Optimized CAD Model Generation from Single Image](https://arxiv.org/abs/2510.17157)
*Yinghui Wang,Xinyu Zhang,Peng Du*

Main category: cs.CV

TL;DR: GACO-CAD is a two-stage post-training framework that improves geometric accuracy and modeling conciseness in generating parametric CAD models from single images using MLLMs.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs struggle with accurately inferring 3D geometry from 2D images due to limited spatial reasoning capabilities, which limits their effectiveness in industrial concept design applications.

Method: Two-stage framework: 1) Supervised fine-tuning using depth and surface normal maps as geometric priors with multi-channel input; 2) Reinforcement learning with group length reward to promote compact modeling sequences and dynamic weighting for training stability.

Result: State-of-the-art performance on DeepCAD and Fusion360 datasets, outperforming existing methods in code validity, geometric accuracy, and modeling conciseness under the same MLLM backbone.

Conclusion: GACO-CAD effectively addresses the spatial reasoning limitations of MLLMs for CAD generation by leveraging geometric priors and promoting concise modeling procedures, demonstrating superior performance across multiple metrics.

Abstract: Generating editable, parametric CAD models from a single image holds great
potential to lower the barriers of industrial concept design. However, current
multi-modal large language models (MLLMs) still struggle with accurately
inferring 3D geometry from 2D images due to limited spatial reasoning
capabilities. We address this limitation by introducing GACO-CAD, a novel
two-stage post-training framework. It is designed to achieve a joint objective:
simultaneously improving the geometric accuracy of the generated CAD models and
encouraging the use of more concise modeling procedures. First, during
supervised fine-tuning, we leverage depth and surface normal maps as dense
geometric priors, combining them with the RGB image to form a multi-channel
input. In the context of single-view reconstruction, these priors provide
complementary spatial cues that help the MLLM more reliably recover 3D geometry
from 2D observations. Second, during reinforcement learning, we introduce a
group length reward that, while preserving high geometric fidelity, promotes
the generation of more compact and less redundant parametric modeling
sequences. A simple dynamic weighting strategy is adopted to stabilize
training. Experiments on the DeepCAD and Fusion360 datasets show that GACO-CAD
achieves state-of-the-art performance under the same MLLM backbone,
consistently outperforming existing methods in terms of code validity,
geometric accuracy, and modeling conciseness.

</details>


### [217] [Investigating Adversarial Robustness against Preprocessing used in Blackbox Face Recognition](https://arxiv.org/abs/2510.17169)
*Roland Croft,Brian Du,Darcy Joseph,Sharath Kumar*

Main category: cs.CV

TL;DR: Face recognition systems are vulnerable to adversarial attacks, but face preprocessing techniques significantly impact attack effectiveness. Different preprocessing methods can degrade attack success rates by up to 78%, and a preprocessing-invariant method improves transferability by 27%.


<details>
  <summary>Details</summary>
Motivation: To investigate how face preprocessing in FR systems affects adversarial attack transferability in blackbox settings, as preprocessing is often overlooked but critical for system security.

Method: Studied transferability of state-of-the-art adversarial attacks against different preprocessing techniques (face detection models and interpolation methods) in blackbox settings, and proposed a preprocessing-invariant method using input transformations.

Result: Face detection model choice degraded attack success rate by up to 78%, while interpolation methods had minimal impact. Preprocessing also degraded attacks in whitebox settings due to noise vector interactions. The proposed preprocessing-invariant method improved attack transferability by up to 27%.

Conclusion: Face preprocessing is crucial for FR system security and must be considered to improve adversarial generalization of facial adversarial examples.

Abstract: Face Recognition (FR) models have been shown to be vulnerable to adversarial
examples that subtly alter benign facial images, exposing blind spots in these
systems, as well as protecting user privacy. End-to-end FR systems first obtain
preprocessed faces from diverse facial imagery prior to computing the
similarity of the deep feature embeddings. Whilst face preprocessing is a
critical component of FR systems, and hence adversarial attacks against them,
we observe that this preprocessing is often overlooked in blackbox settings.
Our study seeks to investigate the transferability of several out-of-the-box
state-of-the-art adversarial attacks against FR when applied against different
preprocessing techniques used in a blackbox setting. We observe that the choice
of face detection model can degrade the attack success rate by up to 78%,
whereas choice of interpolation method during downsampling has relatively
minimal impacts. Furthermore, we find that the requirement for facial
preprocessing even degrades attack strength in a whitebox setting, due to the
unintended interaction of produced noise vectors against face detection models.
Based on these findings, we propose a preprocessing-invariant method using
input transformations that improves the transferability of the studied attacks
by up to 27%. Our findings highlight the importance of preprocessing in FR
systems, and the need for its consideration towards improving the adversarial
generalisation of facial adversarial examples.

</details>


### [218] [Generation then Reconstruction: Accelerating Masked Autoregressive Models via Two-Stage Sampling](https://arxiv.org/abs/2510.17171)
*Feihong Yan,Peiru Wang,Yao Zhu,Kaiyu Pang,Qingyan Wei,Huiqi Li,Linfeng Zhang*

Main category: cs.CV

TL;DR: GtR is a training-free hierarchical sampling strategy that accelerates masked autoregressive models by decomposing generation into structure generation and detail reconstruction stages, achieving 3.72x speedup while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: Masked autoregressive models have constrained acceleration potential due to modeling complexity of spatially correlated visual tokens in a single step. The paper aims to address this limitation.

Method: GtR decomposes generation into two stages: structure generation (slow computation for global semantic scaffolding) and detail reconstruction (fast computation for remaining tokens). Also includes Frequency-Weighted Token Selection (FTS) to allocate more computation budget to tokens on image details based on high frequency energy.

Result: Achieves 3.72x speedup on MAR-H while maintaining comparable quality (FID: 1.59, IS: 304.4 vs. original 1.59, 299.1). Outperforms existing acceleration methods across various model scales and generation tasks.

Conclusion: GtR effectively accelerates masked autoregressive models through hierarchical sampling and frequency-based token selection, demonstrating significant speed improvements without quality degradation.

Abstract: Masked Autoregressive (MAR) models promise better efficiency in visual
generation than autoregressive (AR) models for the ability of parallel
generation, yet their acceleration potential remains constrained by the
modeling complexity of spatially correlated visual tokens in a single step. To
address this limitation, we introduce Generation then Reconstruction (GtR), a
training-free hierarchical sampling strategy that decomposes generation into
two stages: structure generation establishing global semantic scaffolding,
followed by detail reconstruction efficiently completing remaining tokens.
Assuming that it is more difficult to create an image from scratch than to
complement images based on a basic image framework, GtR is designed to achieve
acceleration by computing the reconstruction stage quickly while maintaining
the generation quality by computing the generation stage slowly. Moreover,
observing that tokens on the details of an image often carry more semantic
information than tokens in the salient regions, we further propose
Frequency-Weighted Token Selection (FTS) to offer more computation budget to
tokens on image details, which are localized based on the energy of high
frequency information. Extensive experiments on ImageNet class-conditional and
text-to-image generation demonstrate 3.72x speedup on MAR-H while maintaining
comparable quality (e.g., FID: 1.59, IS: 304.4 vs. original 1.59, 299.1),
substantially outperforming existing acceleration methods across various model
scales and generation tasks. Our codes will be released in
https://github.com/feihongyan1/GtR.

</details>


### [219] [Benchmarking Out-of-Distribution Detection for Plankton Recognition: A Systematic Evaluation of Advanced Methods in Marine Ecological Monitoring](https://arxiv.org/abs/2510.17179)
*Yingzi Han,Jiakai He,Chuanlong Xie,Jianping Li*

Main category: cs.CV

TL;DR: This paper presents the first large-scale systematic evaluation of Out-of-Distribution (OoD) detection methods for plankton recognition, identifying ViM as the best-performing approach.


<details>
  <summary>Details</summary>
Motivation: Plankton recognition models face challenges from distribution shifts between training and test data due to complex morphologies, species diversity, and continuous discovery of new species, leading to unpredictable errors. The field lacks systematic integration of latest computer vision developments and unified benchmarks.

Method: Created OoD benchmarks simulating various distribution shift scenarios using the DYB-PlanktonNet dataset, and systematically evaluated twenty-two OoD detection methods.

Result: ViM method significantly outperformed other approaches, particularly excelling in Far-OoD scenarios with substantial improvements in key metrics.

Conclusion: This comprehensive evaluation provides reliable reference for algorithm selection in automated plankton recognition and lays foundation for future research in plankton OoD detection.

Abstract: Automated plankton recognition models face significant challenges during
real-world deployment due to distribution shifts (Out-of-Distribution, OoD)
between training and test data. This stems from plankton's complex
morphologies, vast species diversity, and the continuous discovery of novel
species, which leads to unpredictable errors during inference. Despite rapid
advancements in OoD detection methods in recent years, the field of plankton
recognition still lacks a systematic integration of the latest computer vision
developments and a unified benchmark for large-scale evaluation. To address
this, this paper meticulously designed a series of OoD benchmarks simulating
various distribution shift scenarios based on the DYB-PlanktonNet dataset
\cite{875n-f104-21}, and systematically evaluated twenty-two OoD detection
methods. Extensive experimental results demonstrate that the ViM
\cite{wang2022vim} method significantly outperforms other approaches in our
constructed benchmarks, particularly excelling in Far-OoD scenarios with
substantial improvements in key metrics. This comprehensive evaluation not only
provides a reliable reference for algorithm selection in automated plankton
recognition but also lays a solid foundation for future research in plankton
OoD detection. To our knowledge, this study marks the first large-scale,
systematic evaluation and analysis of Out-of-Distribution data detection
methods in plankton recognition. Code is available at
https://github.com/BlackJack0083/PlanktonOoD.

</details>


### [220] [Capturing Head Avatar with Hand Contacts from a Monocular Video](https://arxiv.org/abs/2510.17181)
*Haonan He,Yufeng Zheng,Jie Song*

Main category: cs.CV

TL;DR: A framework for jointly learning photorealistic 3D head avatars with hand-face interactions, addressing challenges in tracking relative poses and learning hand-induced facial deformations through PCA basis and contact regularization.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus only on facial regions and ignore natural hand-face interactions that convey cognitive states, creating a need for avatars that can capture these realistic interactions.

Method: Combines depth order loss with contact regularization for pose tracking, learns PCA basis for hand-induced facial deformations, and incorporates contact loss inspired by physics-based simulation to reduce interpenetration artifacts.

Result: Evaluated on iPhone RGB(D) videos and synthetic dataset, showing better appearance and more accurate deforming geometry than state-of-the-art surface reconstruction methods.

Conclusion: The proposed framework successfully captures realistic hand-face interactions with improved geometric accuracy and physical plausibility, advancing photorealistic 3D head avatar creation.

Abstract: Photorealistic 3D head avatars are vital for telepresence, gaming, and VR.
However, most methods focus solely on facial regions, ignoring natural
hand-face interactions, such as a hand resting on the chin or fingers gently
touching the cheek, which convey cognitive states like pondering. In this work,
we present a novel framework that jointly learns detailed head avatars and the
non-rigid deformations induced by hand-face interactions.
  There are two principal challenges in this task. First, naively tracking hand
and face separately fails to capture their relative poses. To overcome this, we
propose to combine depth order loss with contact regularization during pose
tracking, ensuring correct spatial relationships between the face and hand.
Second, no publicly available priors exist for hand-induced deformations,
making them non-trivial to learn from monocular videos. To address this, we
learn a PCA basis specific to hand-induced facial deformations from a face-hand
interaction dataset. This reduces the problem to estimating a compact set of
PCA parameters rather than a full spatial deformation field. Furthermore,
inspired by physics-based simulation, we incorporate a contact loss that
provides additional supervision, significantly reducing interpenetration
artifacts and enhancing the physical plausibility of the results.
  We evaluate our approach on RGB(D) videos captured by an iPhone.
Additionally, to better evaluate the reconstructed geometry, we construct a
synthetic dataset of avatars with various types of hand interactions. We show
that our method can capture better appearance and more accurate deforming
geometry of the face than SOTA surface reconstruction methods.

</details>


### [221] [HIDISC: A Hyperbolic Framework for Domain Generalization with Generalized Category Discovery](https://arxiv.org/abs/2510.17188)
*Vaibhav Rathore,Divyam Gupta,Biplab Banerjee*

Main category: cs.CV

TL;DR: HIDISC is a hyperbolic representation learning framework for Domain Generalization with Generalized Category Discovery (DG-GCD) that achieves domain and category-level generalization without episodic training, using GPT-guided diffusion for domain augmentation and Tangent CutMix for curvature-aware interpolation.


<details>
  <summary>Details</summary>
Motivation: Existing GCD methods assume access to labeled and unlabeled data from the same domain during training, limiting applicability in open-world scenarios with distribution shifts. DG-GCD aims to generalize to unseen domains containing novel categories without accessing target domain data during training.

Method: Uses hyperbolic representation learning with GPT-guided diffusion for domain augmentation, Tangent CutMix for curvature-aware interpolation in tangent space, and a unified loss combining penalized Busemann alignment, hybrid hyperbolic contrastive regularization, and adaptive outlier repulsion. Includes a learnable curvature parameter to adapt geometry to dataset complexity.

Result: Achieves state-of-the-art results on PACS, Office-Home, and DomainNet datasets, consistently outperforming existing Euclidean and hyperbolic (DG)-GCD baselines.

Conclusion: HIDISC provides an efficient and effective framework for DG-GCD that avoids the computational cost and error accumulation of episodic training while achieving superior generalization across domains and categories.

Abstract: Generalized Category Discovery (GCD) aims to classify test-time samples into
either seen categories** -- available during training -- or novel ones, without
relying on label supervision. Most existing GCD methods assume simultaneous
access to labeled and unlabeled data during training and arising from the same
domain, limiting applicability in open-world scenarios involving distribution
shifts. Domain Generalization with GCD (DG-GCD) lifts this constraint by
requiring models to generalize to unseen domains containing novel categories,
without accessing targetdomain data during training. The only prior DG-GCD
method, DG2CD-Net, relies on episodic training with multiple synthetic domains
and task vector aggregation, incurring high computational cost and error
accumulation. We propose HIDISC, a hyperbolic representation learning framework
that achieves domain and category-level generalization without episodic
simulation. To expose the model to minimal but diverse domain variations, we
augment the source domain using GPT-guided diffusion, avoiding overfitting
while maintaining efficiency. To structure the representation space, we
introduce Tangent CutMix, a curvature-aware interpolation that synthesizes
pseudo-novel samples in tangent space, preserving manifold consistency. A
unified loss -- combining penalized Busemann alignment, hybrid hyperbolic
contrastive regularization, and adaptive outlier repulsion -- **facilitates
compact, semantically structured embeddings. A learnable curvature parameter
further adapts the geometry to dataset complexity. HIDISC achieves
state-of-the-art results on PACS , Office-Home , and DomainNet, consistently
outperforming the existing Euclidean and hyperbolic (DG)-GCD baselines.

</details>


### [222] [ZSPAPrune: Zero-Shot Prompt-Aware Token Pruning for Vision-Language Models](https://arxiv.org/abs/2510.17197)
*Pu Zhang,Yuwei Li,Xingyuan Xian,Guoming Tang*

Main category: cs.CV

TL;DR: A zero-shot method for reducing visual token redundancy in Vision-Language Models by balancing task relevance and information diversity through hierarchical token pruning.


<details>
  <summary>Details</summary>
Motivation: Existing visual token pruning methods neglect text prompt guidance and fail to prioritize task relevance, leading to inefficient inference costs in Vision-Language Models.

Method: Proposes a hierarchical approach that first selects task-relevant visual tokens based on text prompt guidance, then supplements with diversity tokens to preserve broader context.

Result: Achieves performance matching or surpassing state-of-the-art with minimal accuracy loss even when pruning up to 90% of tokens, while significantly reducing GPU memory and inference latency.

Conclusion: The prompt-aware hierarchical token pruning method effectively balances task relevance and information diversity, enabling efficient VLM inference without compromising performance.

Abstract: As the capabilities of Vision-Language Models (VLMs) advance, they can
process increasingly large inputs, which, unlike in LLMs, generates significant
visual token redundancy and leads to prohibitive inference costs. While many
methods aim to reduce these costs by pruning visual tokens, existing
approaches, whether based on attention or diversity, typically neglect the
guidance of the text prompt and thus fail to prioritize task relevance. In this
work, we propose a novel, zero-shot method that reframes the problem by
introducing a prompt-aware perspective, explicitly modeling visual token
pruning as a balance between task relevance and information diversity. Our
hierarchical approach first selects a core set of task-relevant visual tokens
and then supplements them with diversity tokens to preserve broader context.
Experiments across multiple models and benchmarks show that our method achieves
performance that matches or surpasses the state-of-the-art with only minimal
accuracy loss, even when pruning up to 90\% of the tokens. Furthermore, these
gains are accompanied by significant reductions in GPU memory footprint and
inference latency.

</details>


### [223] [From Pixels to People: Satellite-Based Mapping and Quantification of Riverbank Erosion and Lost Villages in Bangladesh](https://arxiv.org/abs/2510.17198)
*M Saifuzzaman Rafat,Mohd Ruhul Ameen,Akif Islam,Abu Saleh Musa Miah,Jungpil Shin*

Main category: cs.CV

TL;DR: Adapted Segment Anything Model (SAM) with fine-tuned mask decoder to detect riverbank erosion in Bangladesh using historical Google Earth imagery, achieving high accuracy (86.30% mIoU, 92.60% Dice score) and creating first annotated dataset of disappeared settlements.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of tracking river erosion that destroys villages and farmland in Bangladesh, which has been difficult for human analysts to monitor systematically.

Method: Used color-channel analysis for rough land/water segmentation, then fine-tuned SAM's mask decoder to recognize riverbank erosion patterns using a new dataset of historical Google Earth imagery from 2003-2025 with manually annotated vanished settlements.

Result: Achieved 86.30% mean Intersection over Union and 92.60% Dice score, significantly outperforming traditional methods and off-the-shelf deep learning models.

Conclusion: Provides powerful tools for policymakers to monitor erosion, anticipate trajectories, and protect vulnerable communities through the first annotated dataset, specialized AI model, and land loss quantification method.

Abstract: The great rivers of Bangladesh, arteries of commerce and sustenance, are also
agents of relentless destruction. Each year, they swallow whole villages and
vast tracts of farmland, erasing communities from the map and displacing
thousands of families. To track this slow-motion catastrophe has, until now,
been a Herculean task for human analysts. Here we show how a powerful
general-purpose vision model, the Segment Anything Model (SAM), can be adapted
to this task with remarkable precision. To do this, we assembled a new dataset
- a digital chronicle of loss compiled from historical Google Earth imagery of
Bangladesh's most vulnerable regions, including Mokterer Char Union, Kedarpur
Union, Balchipara village, and Chowhali Upazila, from 2003 to 2025. Crucially,
this dataset is the first to include manually annotated data on the settlements
that have vanished beneath the water. Our method first uses a simple
color-channel analysis to provide a rough segmentation of land and water, and
then fine-tunes SAM's mask decoder to recognize the subtle signatures of
riverbank erosion. The resulting model demonstrates a keen eye for this
destructive process, achieving a mean Intersection over Union of 86.30% and a
Dice score of 92.60% - a performance that significantly surpasses traditional
methods and off-the-shelf deep learning models. This work delivers three key
contributions: the first annotated dataset of disappeared settlements in
Bangladesh due to river erosion; a specialized AI model fine-tuned for this
critical task; and a method for quantifying land loss with compelling visual
evidence. Together, these tools provide a powerful new lens through which
policymakers and disaster management agencies can monitor erosion, anticipate
its trajectory, and ultimately protect the vulnerable communities in its path.

</details>


### [224] [Round Outcome Prediction in VALORANT Using Tactical Features from Video Analysis](https://arxiv.org/abs/2510.17199)
*Nirai Hayakawa,Kazumasa Shimari,Kazuma Yamasaki,Hirotatsu Hoshikawa,Rikuto Tsuchida,Kenichi Matsumoto*

Main category: cs.CV

TL;DR: This paper presents a round outcome prediction model for VALORANT using minimap information from match footage, achieving 81% accuracy by incorporating tactical features.


<details>
  <summary>Details</summary>
Motivation: Most existing esports match prediction research relies on match log data and statistics, but this study aims to leverage visual information from match footage, particularly for complex strategy-based FPS games like VALORANT.

Method: Based on TimeSformer video recognition model, the approach extracts detailed tactical features from minimap information including character positions and in-game events, using augmented datasets with tactical event labels.

Result: The model achieved approximately 81% prediction accuracy, especially from the middle phases of rounds onward, significantly outperforming models trained on raw minimap information alone.

Conclusion: Leveraging tactical features extracted from match footage is highly effective for predicting round outcomes in VALORANT, demonstrating the value of visual analysis over traditional statistical approaches.

Abstract: Recently, research on predicting match outcomes in esports has been actively
conducted, but much of it is based on match log data and statistical
information. This research targets the FPS game VALORANT, which requires
complex strategies, and aims to build a round outcome prediction model by
analyzing minimap information in match footage. Specifically, based on the
video recognition model TimeSformer, we attempt to improve prediction accuracy
by incorporating detailed tactical features extracted from minimap information,
such as character position information and other in-game events. This paper
reports preliminary results showing that a model trained on a dataset augmented
with such tactical event labels achieved approximately 81% prediction accuracy,
especially from the middle phases of a round onward, significantly
outperforming a model trained on a dataset with the minimap information itself.
This suggests that leveraging tactical features from match footage is highly
effective for predicting round outcomes in VALORANT.

</details>


### [225] [EndoCIL: A Class-Incremental Learning Framework for Endoscopic Image Classification](https://arxiv.org/abs/2510.17200)
*Bingrong Liu,Jun Shi,Yushan Zheng*

Main category: cs.CV

TL;DR: EndoCIL is a novel class-incremental learning framework for endoscopic image analysis that addresses catastrophic forgetting through distribution-aligned exemplar selection, class-balanced loss, and gradient calibration.


<details>
  <summary>Details</summary>
Motivation: Existing replay-based CIL methods fail to effectively mitigate catastrophic forgetting in endoscopic imaging due to severe domain discrepancies and class imbalance, which are inherent challenges in clinical applications.

Method: EndoCIL incorporates three key components: Maximum Mean Discrepancy Based Replay (MDBR) for diverse exemplar selection, Prior Regularized Class Balanced Loss (PRCBL) to handle class imbalance, and Calibration of Fully-Connected Gradients (CFG) to mitigate bias toward new classes.

Result: Extensive experiments on four public endoscopic datasets show that EndoCIL generally outperforms state-of-the-art CIL methods across varying buffer sizes and evaluation metrics.

Conclusion: The proposed framework effectively balances stability and plasticity in lifelong endoscopic diagnosis, showing promising potential for clinical scalability and deployment.

Abstract: Class-incremental learning (CIL) for endoscopic image analysis is crucial for
real-world clinical applications, where diagnostic models should continuously
adapt to evolving clinical data while retaining performance on previously
learned ones. However, existing replay-based CIL methods fail to effectively
mitigate catastrophic forgetting due to severe domain discrepancies and class
imbalance inherent in endoscopic imaging. To tackle these challenges, we
propose EndoCIL, a novel and unified CIL framework specifically tailored for
endoscopic image diagnosis. EndoCIL incorporates three key components: Maximum
Mean Discrepancy Based Replay (MDBR), employing a distribution-aligned greedy
strategy to select diverse and representative exemplars, Prior Regularized
Class Balanced Loss (PRCBL), designed to alleviate both inter-phase and
intra-phase class imbalance by integrating prior class distributions and
balance weights into the loss function, and Calibration of Fully-Connected
Gradients (CFG), which adjusts the classifier gradients to mitigate bias toward
new classes. Extensive experiments conducted on four public endoscopic datasets
demonstrate that EndoCIL generally outperforms state-of-the-art CIL methods
across varying buffer sizes and evaluation metrics. The proposed framework
effectively balances stability and plasticity in lifelong endoscopic diagnosis,
showing promising potential for clinical scalability and deployment.

</details>


### [226] [Optimizing DINOv2 with Registers for Face Anti-Spoofing](https://arxiv.org/abs/2510.17201)
*Mika Feng,Pierre Gallin-Martel,Koichi Ito,Takafumi Aoki*

Main category: cs.CV

TL;DR: Proposes a DINOv2-based method for detecting face spoofing attacks by identifying subtle differences between live and spoofed face images.


<details>
  <summary>Details</summary>
Motivation: Face recognition systems are vulnerable to spoofing attacks where malicious actors use photos to bypass authentication, requiring detection before face recognition.

Method: Uses DINOv2 with registers to extract generalizable features and suppress attention mechanism perturbations, enabling focus on essential minute features.

Result: Demonstrated effectiveness through experiments on The 6th Face Anti-Spoofing Workshop dataset and SiW dataset.

Conclusion: The proposed DINOv2-based method effectively detects face spoofing attacks by leveraging attention mechanisms to identify subtle differences.

Abstract: Face recognition systems are designed to be robust against variations in head
pose, illumination, and image blur during capture. However, malicious actors
can exploit these systems by presenting a face photo of a registered user,
potentially bypassing the authentication process. Such spoofing attacks must be
detected prior to face recognition. In this paper, we propose a DINOv2-based
spoofing attack detection method to discern minute differences between live and
spoofed face images. Specifically, we employ DINOv2 with registers to extract
generalizable features and to suppress perturbations in the attention
mechanism, which enables focused attention on essential and minute features. We
demonstrate the effectiveness of the proposed method through experiments
conducted on the dataset provided by ``The 6th Face Anti-Spoofing Workshop:
Unified Physical-Digital Attacks Detection@ICCV2025'' and SiW dataset.

</details>


### [227] [$\mathcal{V}isi\mathcal{P}runer$: Decoding Discontinuous Cross-Modal Dynamics for Efficient Multimodal LLMs](https://arxiv.org/abs/2510.17205)
*Yingqi Fan,Anhao Zhao,Jinlan Fu,Junlong Tong,Hui Su,Yijie Pan,Wei Zhang,Xiaoyu Shen*

Main category: cs.CV

TL;DR: VisiPruner is a training-free pruning framework that reduces vision-related attention computations by up to 99% in MLLMs by leveraging insights from the three-stage cross-modal interaction process.


<details>
  <summary>Details</summary>
Motivation: MLLMs suffer from significant computational overhead due to quadratic attention growth with multimodal tokens, and existing pruning methods lack understanding of how MLLMs process multimodal information.

Method: Systematic analysis reveals a three-stage cross-modal interaction process in MLLMs, leading to VisiPruner - a training-free pruning framework that selectively prunes vision tokens based on their importance across different layers.

Result: VisiPruner reduces up to 99% of vision-related attention computations and 53.9% of FLOPs on LLaVA-v1.5 7B, outperforming existing token pruning methods and generalizing across diverse MLLMs.

Conclusion: The insights provide actionable guidelines for training efficient MLLMs by aligning model architecture with intrinsic layer-wise processing dynamics, and VisiPruner offers an effective solution for computational efficiency.

Abstract: Multimodal Large Language Models (MLLMs) have achieved strong performance
across vision-language tasks, but suffer from significant computational
overhead due to the quadratic growth of attention computations with the number
of multimodal tokens. Though efforts have been made to prune tokens in MLLMs,
\textit{they lack a fundamental understanding of how MLLMs process and fuse
multimodal information.} Through systematic analysis, we uncover a
\textbf{three-stage} cross-modal interaction process: (1) Shallow layers
recognize task intent, with visual tokens acting as passive attention sinks;
(2) Cross-modal fusion occurs abruptly in middle layers, driven by a few
critical visual tokens; (3) Deep layers discard vision tokens, focusing solely
on linguistic refinement. Based on these findings, we propose
\emph{VisiPruner}, a training-free pruning framework that reduces up to 99\% of
vision-related attention computations and 53.9\% of FLOPs on LLaVA-v1.5 7B. It
significantly outperforms existing token pruning methods and generalizes across
diverse MLLMs. Beyond pruning, our insights further provide actionable
guidelines for training efficient MLLMs by aligning model architecture with its
intrinsic layer-wise processing dynamics. Our code is available at:
https://github.com/EIT-NLP/VisiPruner.

</details>


### [228] [When One Moment Isn't Enough: Multi-Moment Retrieval with Cross-Moment Interactions](https://arxiv.org/abs/2510.17218)
*Zhuo Cao,Heming Du,Bingqing Zhang,Xin Yu,Xue Li,Sen Wang*

Main category: cs.CV

TL;DR: This paper introduces QV-M dataset for multi-moment retrieval (MMR) and proposes FlashMMR framework with multi-moment post-verification to refine moment boundaries and achieve robust multi-moment alignment.


<details>
  <summary>Details</summary>
Motivation: Existing moment retrieval methods focus on single-moment retrieval, but real-world applications often require retrieving multiple relevant moments per query, making current datasets and methods insufficient.

Method: Proposed FlashMMR framework with Multi-moment Post-verification module using constrained temporal adjustment and verification to refine moment boundaries and prune low-confidence proposals.

Result: FlashMMR achieves improvements over prior SOTA by 3.00% on G-mAP, 2.70% on mAP@3+tgt, and 2.56% on mR@3 on QV-M dataset. The dataset contains 2,212 annotations covering 6,384 video segments.

Conclusion: QV-M serves as an effective benchmark for training and evaluating MMR models, while FlashMMR provides a strong baseline for advancing research in realistic video temporal grounding scenarios.

Abstract: Existing Moment retrieval (MR) methods focus on Single-Moment Retrieval
(SMR). However, one query can correspond to multiple relevant moments in
real-world applications. This makes the existing datasets and methods
insufficient for video temporal grounding. By revisiting the gap between
current MR tasks and real-world applications, we introduce a high-quality
datasets called QVHighlights Multi-Moment Dataset (QV-M$^2$), along with new
evaluation metrics tailored for multi-moment retrieval (MMR). QV-M$^2$ consists
of 2,212 annotations covering 6,384 video segments. Building on existing
efforts in MMR, we propose a framework called FlashMMR. Specifically, we
propose a Multi-moment Post-verification module to refine the moment
boundaries. We introduce constrained temporal adjustment and subsequently
leverage a verification module to re-evaluate the candidate segments. Through
this sophisticated filtering pipeline, low-confidence proposals are pruned, and
robust multi-moment alignment is achieved. We retrain and evaluate 6 existing
MR methods on QV-M$^2$ and QVHighlights under both SMR and MMR settings.
Results show that QV-M$^2$ serves as an effective benchmark for training and
evaluating MMR models, while FlashMMR provides a strong baseline. Specifically,
on QV-M$^2$, it achieves improvements over prior SOTA method by 3.00% on G-mAP,
2.70% on mAP@3+tgt, and 2.56% on mR@3. The proposed benchmark and method
establish a foundation for advancing research in more realistic and challenging
video temporal grounding scenarios. Code is released at
https://github.com/Zhuo-Cao/QV-M2.

</details>


### [229] [Fair and Interpretable Deepfake Detection in Videos](https://arxiv.org/abs/2510.17264)
*Akihito Yoshii,Ryosuke Sonoda,Ramya Srinivasan*

Main category: cs.CV

TL;DR: A fairness-aware deepfake detection framework that integrates temporal feature learning and demographic-aware data augmentation to address bias, improve transparency, and enhance detection reliability across different demographic groups.


<details>
  <summary>Details</summary>
Motivation: Existing deepfake detection methods suffer from bias, lack of transparency, and failure to capture temporal information, leading to biased decisions and unreliable results across demographic groups.

Method: Proposes a framework with sequence-based clustering for temporal modeling, concept extraction for interpretability, and demographic-aware data augmentation that balances underrepresented groups and applies frequency-domain transformations to preserve deepfake artifacts.

Result: Extensive experiments on multiple datasets (FaceForensics++, DFD, Celeb-DF, DFDC) using SoTA architectures (Xception, ResNet) demonstrate the method achieves the best tradeoff between fairness and accuracy compared to state-of-the-art approaches.

Conclusion: The proposed framework effectively mitigates bias in deepfake detection while maintaining high accuracy, providing a more reliable and interpretable solution that performs well across different demographic groups.

Abstract: Existing deepfake detection methods often exhibit bias, lack transparency,
and fail to capture temporal information, leading to biased decisions and
unreliable results across different demographic groups. In this paper, we
propose a fairness-aware deepfake detection framework that integrates temporal
feature learning and demographic-aware data augmentation to enhance fairness
and interpretability. Our method leverages sequence-based clustering for
temporal modeling of deepfake videos and concept extraction to improve
detection reliability while also facilitating interpretable decisions for
non-expert users. Additionally, we introduce a demography-aware data
augmentation method that balances underrepresented groups and applies
frequency-domain transformations to preserve deepfake artifacts, thereby
mitigating bias and improving generalization. Extensive experiments on
FaceForensics++, DFD, Celeb-DF, and DFDC datasets using state-of-the-art (SoTA)
architectures (Xception, ResNet) demonstrate the efficacy of the proposed
method in obtaining the best tradeoff between fairness and accuracy when
compared to SoTA.

</details>


### [230] [FineVision: Open Data Is All You Need](https://arxiv.org/abs/2510.17269)
*Luis Wiedmann,Orr Zohar,Amir Mahla,Xiaohan Wang,Rui Li,Thibaud Frere,Leandro von Werra,Aritra Roy Gosthipaty,Andrs Marafioti*

Main category: cs.CV

TL;DR: FineVision introduces a meticulously curated 24M-sample vision-language corpus that unifies 200+ sources through a semi-automated pipeline with human oversight, achieving superior model performance through rigorous data hygiene.


<details>
  <summary>Details</summary>
Motivation: Vision-language model advancement is hampered by fragmented, inconsistent, and contaminated public datasets, creating a need for high-quality, unified training data.

Method: Semi-automated human-in-the-loop pipeline that unifies 200+ sources into 185 subsets, with automation for bulk ingestion and reviewers auditing mappings, plus rigorous de-duplication and decontamination against 66 benchmarks.

Result: Models trained on FineVision consistently outperform those trained on existing open mixtures across a broad evaluation suite, demonstrating benefits of scale and data hygiene.

Conclusion: The corpus and curation tools are released to accelerate data-centric VLM research, showing that balanced automation with human oversight produces superior training data.

Abstract: The advancement of vision-language models (VLMs) is hampered by a fragmented
landscape of inconsistent and contaminated public datasets. We introduce
FineVision, a meticulously collected, curated, and unified corpus of 24 million
samples - the largest open resource of its kind. We unify more than 200 sources
into 185 subsets via a semi-automated, human-in-the-loop pipeline: automation
performs bulk ingestion and schema mapping, while reviewers audit mappings and
spot-check outputs to verify faithful consumption of annotations, appropriate
formatting and diversity, and safety; issues trigger targeted fixes and
re-runs. The workflow further applies rigorous de-duplication within and across
sources and decontamination against 66 public benchmarks. FineVision also
encompasses agentic/GUI tasks with a unified action space; reviewers validate
schemas and inspect a sample of trajectories to confirm executable fidelity.
Models trained on FineVision consistently outperform those trained on existing
open mixtures across a broad evaluation suite, underscoring the benefits of
scale, data hygiene, and balanced automation with human oversight. We release
the corpus and curation tools to accelerate data-centric VLM research.

</details>


### [231] [Enhanced Motion Forecasting with Plug-and-Play Multimodal Large Language Models](https://arxiv.org/abs/2510.17274)
*Katie Luo,Jingwei Ji,Tong He,Runsheng Xu,Yichen Xie,Dragomir Anguelov,Mingxing Tan*

Main category: cs.CV

TL;DR: Plug-and-Forecast (PnF) enhances existing motion forecasting models by integrating multimodal large language models (MLLMs) to handle complex scenarios through natural language descriptions, achieving improved performance without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Current autonomous driving systems struggle to generalize cost-effectively to diverse real-world scenarios despite reliable performance in standard conditions.

Method: PnF uses prompts to extract structured scene understanding from MLLMs and distills this information into learnable embeddings to augment existing behavior prediction models, leveraging MLLMs' zero-shot reasoning capabilities.

Result: The approach demonstrates consistent performance improvements on two state-of-the-art motion forecasting models using the Waymo Open Motion Dataset and nuScenes Dataset.

Conclusion: PnF provides a practical plug-and-play solution that significantly enhances motion prediction performance by effectively leveraging MLLMs' natural language capabilities for complex scenario handling.

Abstract: Current autonomous driving systems rely on specialized models for perceiving
and predicting motion, which demonstrate reliable performance in standard
conditions. However, generalizing cost-effectively to diverse real-world
scenarios remains a significant challenge. To address this, we propose
Plug-and-Forecast (PnF), a plug-and-play approach that augments existing motion
forecasting models with multimodal large language models (MLLMs). PnF builds on
the insight that natural language provides a more effective way to describe and
handle complex scenarios, enabling quick adaptation to targeted behaviors. We
design prompts to extract structured scene understanding from MLLMs and distill
this information into learnable embeddings to augment existing behavior
prediction models. Our method leverages the zero-shot reasoning capabilities of
MLLMs to achieve significant improvements in motion prediction performance,
while requiring no fine-tuning -- making it practical to adopt. We validate our
approach on two state-of-the-art motion forecasting models using the Waymo Open
Motion Dataset and the nuScenes Dataset, demonstrating consistent performance
improvements across both benchmarks.

</details>


### [232] [SG-CLDFF: A Novel Framework for Automated White Blood Cell Classification and Segmentation](https://arxiv.org/abs/2510.17278)
*Mehdi Zekriyapanah Gashti,Mostafa Mohammadpour,Ghasem Farjamnia*

Main category: cs.CV

TL;DR: SG-CLDFF is a saliency-guided cross-layer deep feature fusion framework that improves white blood cell analysis through saliency-driven preprocessing and multi-scale feature aggregation, achieving better segmentation and classification performance with enhanced interpretability.


<details>
  <summary>Details</summary>
Motivation: White blood cell analysis in microscopic images is challenging due to staining variability, complex backgrounds, and class imbalance, requiring more robust and interpretable methods for clinical diagnosis.

Method: Uses saliency priors to highlight WBC regions, a hybrid EfficientSwin-style backbone for multi-resolution features, cross-layer fusion module for feature aggregation, and multi-task training with segmentation and classification heads using weighted losses and saliency-alignment regularization.

Result: Achieves consistent gains in IoU, F1, and classification accuracy on standard benchmarks (BCCD, LISC, ALL-IDB) compared to CNN and transformer baselines, with ablation studies confirming contributions of saliency preprocessing and cross-layer fusion.

Conclusion: SG-CLDFF provides a practical and explainable solution for reliable automated WBC analysis in clinical workflows through improved robustness and interpretability.

Abstract: Accurate segmentation and classification of white blood cells (WBCs) in
microscopic images are essential for diagnosis and monitoring of many
hematological disorders, yet remain challenging due to staining variability,
complex backgrounds, and class imbalance. In this paper, we introduce a novel
Saliency-Guided Cross-Layer Deep Feature Fusion framework (SG-CLDFF) that
tightly integrates saliency-driven preprocessing with multi-scale deep feature
aggregation to improve both robustness and interpretability for WBC analysis.
SG-CLDFF first computes saliency priors to highlight candidate WBC regions and
guide subsequent feature extraction. A lightweight hybrid backbone
(EfficientSwin-style) produces multi-resolution representations, which are
fused by a ResNeXt-CC-inspired cross-layer fusion module to preserve
complementary information from shallow and deep layers. The network is trained
in a multi-task setup with concurrent segmentation and cell-type classification
heads, using class-aware weighted losses and saliency-alignment regularization
to mitigate imbalance and suppress background activation. Interpretability is
enforced through Grad-CAM visualizations and saliency consistency checks,
allowing model decisions to be inspected at the regional level. We validate the
framework on standard public benchmarks (BCCD, LISC, ALL-IDB), reporting
consistent gains in IoU, F1, and classification accuracy compared to strong CNN
and transformer baselines. An ablation study also demonstrates the individual
contributions of saliency preprocessing and cross-layer fusion. SG-CLDFF offers
a practical and explainable path toward more reliable automated WBC analysis in
clinical workflows.

</details>


### [233] [Machine Vision-Based Surgical Lighting System:Design and Implementation](https://arxiv.org/abs/2510.17287)
*Amir Gharghabi,Mahdi Hakiminezhad,Maryam Shafaei,Shaghayegh Gharghabi*

Main category: cs.CV

TL;DR: A novel surgical lighting system using YOLOv11 object detection to automatically track a blue marker and direct LED lighting via servomotors, reducing surgeon fatigue and improving illumination consistency.


<details>
  <summary>Details</summary>
Motivation: Traditional surgical lighting requires manual adjustments, causing surgeon fatigue, neck strain, and inconsistent illumination due to drift and shadowing.

Method: Uses YOLOv11 object detection to identify a blue marker placed above the surgical site, then directs a high-power LED light source using two servomotors with tilt-pan brackets.

Result: The YOLO model achieves 96.7% mAP@50 on validation set with annotated surgical scene images containing blue spherical markers.

Conclusion: This machine vision-based automated lighting system reduces physical strain on surgeons, improves illumination consistency, and supports better surgical outcomes.

Abstract: Effortless and ergonomically designed surgical lighting is critical for
precision and safety during procedures. However, traditional systems often rely
on manual adjustments, leading to surgeon fatigue, neck strain, and
inconsistent illumination due to drift and shadowing. To address these
challenges, we propose a novel surgical lighting system that leverages the
YOLOv11 object detection algorithm to identify a blue marker placed above the
target surgical site. A high-power LED light source is then directed to the
identified location using two servomotors equipped with tilt-pan brackets. The
YOLO model achieves 96.7% mAP@50 on the validation set consisting of annotated
images simulating surgical scenes with the blue spherical marker. By automating
the lighting process, this machine vision-based solution reduces physical
strain on surgeons, improves consistency in illumination, and supports improved
surgical outcomes.

</details>


### [234] [Exploring Structural Degradation in Dense Representations for Self-supervised Learning](https://arxiv.org/abs/2510.17299)
*Siran Dai,Qianqian Xu,Peisong Wen,Yang Liu,Qingming Huang*

Main category: cs.CV

TL;DR: Longer self-supervised learning training can degrade dense prediction performance (Self-supervised Dense Degradation). The paper introduces DSE metric for model selection and regularization to address this issue.


<details>
  <summary>Details</summary>
Motivation: To address the counterintuitive phenomenon where longer self-supervised learning training impairs dense prediction tasks like semantic segmentation, and to provide effective evaluation methods without annotations.

Method: Proposes Dense representation Structure Estimator (DSE) with class-relevance and effective dimensionality measures, plus DSE-based model selection and regularization strategies.

Result: Model selection improves mIoU by 3.0% on average across 16 SSL methods and 4 benchmarks. DSE regularization consistently mitigates dense degradation effects.

Conclusion: DSE provides an effective solution for detecting and mitigating self-supervised dense degradation, enabling better model selection and regularization without requiring annotations.

Abstract: In this work, we observe a counterintuitive phenomenon in self-supervised
learning (SSL): longer training may impair the performance of dense prediction
tasks (e.g., semantic segmentation). We refer to this phenomenon as
Self-supervised Dense Degradation (SDD) and demonstrate its consistent presence
across sixteen state-of-the-art SSL methods with various losses, architectures,
and datasets. When the model performs suboptimally on dense tasks at the end of
training, measuring the performance during training becomes essential. However,
evaluating dense performance effectively without annotations remains an open
challenge. To tackle this issue, we introduce a Dense representation Structure
Estimator (DSE), composed of a class-relevance measure and an effective
dimensionality measure. The proposed DSE is both theoretically grounded and
empirically validated to be closely correlated with the downstream performance.
Based on this metric, we introduce a straightforward yet effective model
selection strategy and a DSE-based regularization method. Experiments on
sixteen SSL methods across four benchmarks confirm that model selection
improves mIoU by $3.0\%$ on average with negligible computational cost.
Additionally, DSE regularization consistently mitigates the effects of dense
degradation. Code is available at
https://github.com/EldercatSAM/SSL-Degradation.

</details>


### [235] [LongInsightBench: A Comprehensive Benchmark for Evaluating Omni-Modal Models on Human-Centric Long-Video Understanding](https://arxiv.org/abs/2510.17305)
*ZhaoYang Han,Qihan Lin,Hao Liang,Bowen Chen,Zhou Liu,Wentao Zhang*

Main category: cs.CV

TL;DR: LongInsightBench is the first benchmark for evaluating models' ability to understand long videos with visual, audio, and text modalities, focusing on human language, viewpoints, actions, and contextual elements.


<details>
  <summary>Details</summary>
Motivation: To address the lack of benchmarks for assessing models' capability in understanding long videos across multiple modalities, particularly focusing on rich language content and contextual understanding.

Method: Created a benchmark with ~1,000 videos from FineVideo dataset, designed six challenging task scenarios (Intra-Event and Inter-Event Tasks), and implemented a three-step semi-automated quality assurance pipeline for question and answer validation.

Result: Experimental results show that omni-modal models struggle with precise temporal localization and long-range causal inference tasks, and extended experiments reveal information loss and processing bias in multi-modal fusion.

Conclusion: LongInsightBench provides a comprehensive benchmark for evaluating long video understanding, revealing current limitations in omni-modal models and highlighting challenges in temporal localization and causal reasoning across modalities.

Abstract: We introduce \textbf{LongInsightBench}, the first benchmark designed to
assess models' ability to understand long videos, with a focus on human
language, viewpoints, actions, and other contextual elements, while integrating
\textbf{visual, audio, and text} modalities. Our benchmark excels in three key
areas: \textbf{a) Long-Duration, Information-Dense Videos:} We carefully select
approximately 1,000 videos from open-source datasets FineVideo based on
duration limit and the information density of both visual and audio modalities,
focusing on content like lectures, interviews, and vlogs, which contain rich
language elements. \textbf{b) Diverse and Challenging Task Scenarios:} We have
designed six challenging task scenarios, including both Intra-Event and
Inter-Event Tasks. \textbf{c) Rigorous and Comprehensive Quality Assurance
Pipelines:} We have developed a three-step, semi-automated data quality
assurance pipeline to ensure the difficulty and validity of the synthesized
questions and answer options. Based on LongInsightBench, we designed a series
of experiments. Experimental results shows that Omni-modal models(OLMs) still
face challenge in tasks requiring precise temporal localization (T-Loc) and
long-range causal inference (CE-Caus). Extended experiments reveal the
information loss and processing bias in multi-modal fusion of OLMs. Our dataset
and code is available at
https://anonymous.4open.science/r/LongInsightBench-910F/.

</details>


### [236] [CausalMamba: Scalable Conditional State Space Models for Neural Causal Inference](https://arxiv.org/abs/2510.17318)
*Sangyoon Bae,Jiook Cha*

Main category: cs.CV

TL;DR: CausalMamba is a scalable framework that solves fMRI-based causal inference limitations by decomposing the problem into BOLD deconvolution and causal graph inference using Conditional Mamba architecture, achieving superior accuracy over traditional methods.


<details>
  <summary>Details</summary>
Motivation: To address fundamental limitations in fMRI-based causal inference: the ill-posed nature of inferring neural causality from hemodynamically distorted BOLD signals and the computational intractability of existing methods like Dynamic Causal Modeling (DCM).

Method: Decomposes the complex inverse problem into two tractable stages: BOLD deconvolution to recover latent neural activity, followed by causal graph inference using a novel Conditional Mamba architecture.

Result: On simulated data, achieves 37% higher accuracy than DCM. On real task fMRI data, recovers well-established neural pathways with 88% fidelity, while conventional approaches fail in over 99% of subjects. Reveals strategic brain network reconfigurations undetected by traditional methods.

Conclusion: Provides neuroscientists with a practical tool for large-scale causal inference that captures both fundamental circuit motifs and flexible network dynamics underlying cognitive function.

Abstract: We introduce CausalMamba, a scalable framework that addresses fundamental
limitations in fMRI-based causal inference: the ill-posed nature of inferring
neural causality from hemodynamically distorted BOLD signals and the
computational intractability of existing methods like Dynamic Causal Modeling
(DCM). Our approach decomposes this complex inverse problem into two tractable
stages: BOLD deconvolution to recover latent neural activity, followed by
causal graph inference using a novel Conditional Mamba architecture. On
simulated data, CausalMamba achieves 37% higher accuracy than DCM. Critically,
when applied to real task fMRI data, our method recovers well-established
neural pathways with 88% fidelity, whereas conventional approaches fail to
identify these canonical circuits in over 99% of subjects. Furthermore, our
network analysis of working memory data reveals that the brain strategically
shifts its primary causal hub-recruiting executive or salience networks
depending on the stimulus-a sophisticated reconfiguration that remains
undetected by traditional methods. This work provides neuroscientists with a
practical tool for large-scale causal inference that captures both fundamental
circuit motifs and flexible network dynamics underlying cognitive function.

</details>


### [237] [A Single Set of Adversarial Clothes Breaks Multiple Defense Methods in the Physical World](https://arxiv.org/abs/2510.17322)
*Wei Zhang,Zhanhao Hu,Xiao Li,Xiaopei Zhu,Xiaolin Hu*

Main category: cs.CV

TL;DR: Adversarial clothes with large coverage can bypass existing defense methods against adversarial patches, revealing vulnerabilities in current adversarial defense approaches.


<details>
  <summary>Details</summary>
Motivation: Existing defense methods against adversarial patches fail when patch size is enlarged, and adversarial clothes provide a natural-looking test case with large coverage over the human body.

Method: Evaluated various defense methods against adversarial clothes in both digital and physical worlds, and crafted a single set of clothes that could break multiple defense methods on Faster R-CNN.

Result: All defense methods performed poorly against adversarial clothes. A single clothes set achieved 96.06% ASR against undefended detector and over 64.84% ASRs against nine defended models in physical world.

Conclusion: Existing adversarial defense methods have common vulnerabilities against adversarial clothes, highlighting the need for more robust defense strategies.

Abstract: In recent years, adversarial attacks against deep learning-based object
detectors in the physical world have attracted much attention. To defend
against these attacks, researchers have proposed various defense methods
against adversarial patches, a typical form of physically-realizable attack.
However, our experiments showed that simply enlarging the patch size could make
these defense methods fail. Motivated by this, we evaluated various defense
methods against adversarial clothes which have large coverage over the human
body. Adversarial clothes provide a good test case for adversarial defense
against patch-based attacks because they not only have large sizes but also
look more natural than a large patch on humans. Experiments show that all the
defense methods had poor performance against adversarial clothes in both the
digital world and the physical world. In addition, we crafted a single set of
clothes that broke multiple defense methods on Faster R-CNN. The set achieved
an Attack Success Rate (ASR) of 96.06% against the undefended detector and over
64.84% ASRs against nine defended models in the physical world, unveiling the
common vulnerability of existing adversarial defense methods against
adversarial clothes. Code is available at:
https://github.com/weiz0823/adv-clothes-break-multiple-defenses.

</details>


### [238] [CharDiff: A Diffusion Model with Character-Level Guidance for License Plate Image Restoration](https://arxiv.org/abs/2510.17330)
*Gyuhwan Park,Kihyun Na,Injung Kim*

Main category: cs.CV

TL;DR: CharDiff is a diffusion-based framework with character-level guidance that effectively restores and recognizes severely degraded license plate images using fine-grained character priors and region-wise masking.


<details>
  <summary>Details</summary>
Motivation: License plate image restoration is important not only for LPR systems but also for increasing evidential value, enhancing visual clarity, and facilitating further utilization of license plate images.

Method: Uses diffusion-based framework with character-level guidance, leveraging fine-grained character priors from external segmentation and OCR modules. Incorporates CHARM module for precise region-wise masking to restrict each character's guidance to its own region.

Result: Significantly outperformed baseline restoration models in both restoration quality and recognition accuracy, achieving 28% relative reduction in CER on Roboflow-LP dataset compared to best-performing baseline.

Conclusion: Structured character-guided conditioning effectively enhances robustness of diffusion-based license plate restoration and recognition in practical deployment scenarios.

Abstract: The significance of license plate image restoration goes beyond the
preprocessing stage of License Plate Recognition (LPR) systems, as it also
serves various purposes, including increasing evidential value, enhancing the
clarity of visual interface, and facilitating further utilization of license
plate images. We propose a novel diffusion-based framework with character-level
guidance, CharDiff, which effectively restores and recognizes severely degraded
license plate images captured under realistic conditions. CharDiff leverages
fine-grained character-level priors extracted through external segmentation and
Optical Character Recognition (OCR) modules tailored for low-quality license
plate images. For precise and focused guidance, CharDiff incorporates a novel
Character-guided Attention through Region-wise Masking (CHARM) module, which
ensures that each character's guidance is restricted to its own region, thereby
avoiding interference with other regions. In experiments, CharDiff
significantly outperformed the baseline restoration models in both restoration
quality and recognition accuracy, achieving a 28% relative reduction in CER on
the Roboflow-LP dataset, compared to the best-performing baseline model. These
results indicate that the structured character-guided conditioning effectively
enhances the robustness of diffusion-based license plate restoration and
recognition in practical deployment scenarios.

</details>


### [239] [iDETEX: Empowering MLLMs for Intelligent DETailed EXplainable IQA](https://arxiv.org/abs/2510.17332)
*Zhaoran Zhao,Xinli Yue,Jianhui Sun,Yuhao Xie,Tao Shao,Liangchao Yao,Fan Xia,Yuetang Deng*

Main category: cs.CV

TL;DR: iDETEX is a unified multimodal large language model that performs quality grounding, perception, and description for detailed and explainable image quality assessment, achieving state-of-the-art performance on the ViDA-UGC benchmark and winning the ICCV MIPI 2025 challenge.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of detailed and explainable Image Quality Assessment (IQA) by moving beyond simple scalar quality prediction to more interpretable, human-aligned evaluation paradigms.

Method: Proposed iDETEX - a unified multimodal large language model with task-specific offline augmentation modules, data mixing strategy, and online enhancement strategies to exploit multi-sourced supervision across three heterogeneous subtasks: quality grounding, perception, and description.

Result: Achieved state-of-the-art performance across all subtasks on the large-scale ViDA-UGC benchmark and ranked first in the ICCV MIPI 2025 Detailed Image Quality Assessment Challenge.

Conclusion: iDETEX demonstrates effectiveness and robustness in delivering accurate and interpretable quality assessments, successfully addressing the emerging challenge of detailed and explainable IQA.

Abstract: Image Quality Assessment (IQA) has progressed from scalar quality prediction
to more interpretable, human-aligned evaluation paradigms. In this work, we
address the emerging challenge of detailed and explainable IQA by proposing
iDETEX-a unified multimodal large language model (MLLM) capable of
simultaneously performing three key tasks: quality grounding, perception, and
description. To facilitate efficient and generalizable training across these
heterogeneous subtasks, we design a suite of task-specific offline augmentation
modules and a data mixing strategy. These are further complemented by online
enhancement strategies to fully exploit multi-sourced supervision. We validate
our approach on the large-scale ViDA-UGC benchmark, where iDETEX achieves
state-of-the-art performance across all subtasks. Our model ranks first in the
ICCV MIPI 2025 Detailed Image Quality Assessment Challenge, demonstrating its
effectiveness and robustness in delivering accurate and interpretable quality
assessments.

</details>


### [240] [Nearest-Class Mean and Logits Agreement for Wildlife Open-Set Recognition](https://arxiv.org/abs/2510.17338)
*Jiahao Huo,Mufhumudzi Muthivhi,Terence L. van Zyl,Fredrik Gustafsson*

Main category: cs.CV

TL;DR: A post-processing Open-Set Recognition method that measures agreement between feature-based NCM distribution and softmax probabilities, achieving top performance on wildlife datasets without retraining.


<details>
  <summary>Details</summary>
Motivation: Current OSR methods require retraining pre-trained models, and existing approaches perform well on only single datasets while lacking consistency.

Method: Proposes measuring agreement between NCM-based probability distribution (from feature space) and softmax probabilities (from logit space) as a post-processing strategy.

Result: Achieved AUROC of 93.41 and 95.35 on African and Swedish animal datasets, ranking top three consistently across both datasets.

Conclusion: The proposed post-processing OSR method provides consistent performance across datasets without requiring model retraining, outperforming state-of-the-art methods that excel only on single datasets.

Abstract: Current state-of-the-art Wildlife classification models are trained under the
closed world setting. When exposed to unknown classes, they remain
overconfident in their predictions. Open-set Recognition (OSR) aims to classify
known classes while rejecting unknown samples. Several OSR methods have been
proposed to model the closed-set distribution by observing the feature, logit,
or softmax probability space. A significant drawback of many existing
approaches is the requirement to retrain the pre-trained classification model
with the OSR-specific strategy. This study contributes a post-processing OSR
method that measures the agreement between the models' features and predicted
logits. We propose a probability distribution based on an input's distance to
its Nearest Class Mean (NCM). The NCM-based distribution is then compared with
the softmax probabilities from the logit space to measure agreement between the
NCM and the classification head. Our proposed strategy ranks within the top
three on two evaluated datasets, showing consistent performance across the two
datasets. In contrast, current state-of-the-art methods excel on a single
dataset. We achieve an AUROC of 93.41 and 95.35 for African and Swedish
animals. The code can be found
https://github.com/Applied-Representation-Learning-Lab/OSR.

</details>


### [241] [Exploring The Missing Semantics In Event Modality](https://arxiv.org/abs/2510.17347)
*Jingqian Wu,Shengpeng Xu,Yunbo Jia,Edmund Y. Lam*

Main category: cs.CV

TL;DR: Semantic-E2VID is an event-to-video reconstruction framework that incorporates semantic information from vision foundation models to enhance reconstruction quality by addressing the lack of semantic content in raw event data.


<details>
  <summary>Details</summary>
Motivation: Event cameras capture only intensity changes, missing static objects and background information, which leads to poor semantic content in reconstructed videos. Existing E2V methods overlook semantic information that is crucial for high-quality video reconstruction.

Method: Proposes a cross-modal feature alignment module to transfer semantic knowledge from SAM to event encoder, a semantic-aware feature fusion block to integrate learned semantics, and a semantic perceptual supervision using SAM-generated labels.

Result: Extensive experiments show Semantic-E2VID significantly enhances frame quality and outperforms state-of-the-art E2V methods across multiple benchmarks.

Conclusion: Incorporating semantic information from vision foundation models effectively bridges the semantic gap in event-to-video reconstruction, leading to superior reconstruction quality compared to existing approaches.

Abstract: Event cameras offer distinct advantages such as low latency, high dynamic
range, and efficient motion capture. However, event-to-video reconstruction
(E2V), a fundamental event-based vision task, remains challenging, particularly
for reconstructing and recovering semantic information. This is primarily due
to the nature of the event camera, as it only captures intensity changes,
ignoring static objects and backgrounds, resulting in a lack of semantic
information in captured event modality. Further, semantic information plays a
crucial role in video and frame reconstruction, yet is often overlooked by
existing E2V approaches. To bridge this gap, we propose Semantic-E2VID, an E2V
framework that explores the missing visual semantic knowledge in event modality
and leverages it to enhance event-to-video reconstruction. Specifically,
Semantic-E2VID introduces a cross-modal feature alignment (CFA) module to
transfer the robust visual semantics from a frame-based vision foundation
model, the Segment Anything Model (SAM), to the event encoder, while aligning
the high-level features from distinct modalities. To better utilize the learned
semantic feature, we further propose a semantic-aware feature fusion (SFF)
block to integrate learned semantics in frame modality to form event
representations with rich semantics that can be decoded by the event decoder.
Further, to facilitate the reconstruction of semantic information, we propose a
novel Semantic Perceptual E2V Supervision that helps the model to reconstruct
semantic details by leveraging SAM-generated categorical labels. Extensive
experiments demonstrate that Semantic-E2VID significantly enhances frame
quality, outperforming state-of-the-art E2V methods across multiple benchmarks.
The sample code is included in the supplementary material.

</details>


### [242] [M2H: Multi-Task Learning with Efficient Window-Based Cross-Task Attention for Monocular Spatial Perception](https://arxiv.org/abs/2510.17363)
*U. V. B. L Udugama,George Vosselman,Francesco Nex*

Main category: cs.CV

TL;DR: M2H is a multi-task learning framework for semantic segmentation, depth, edge, and surface normal estimation from monocular images, using Window-Based Cross-Task Attention for efficient feature exchange while maintaining task-specific details.


<details>
  <summary>Details</summary>
Motivation: Need for efficient multi-task models that leverage complementary task information while minimizing computational overhead for real-time spatial perception on edge devices.

Method: Uses Window-Based Cross-Task Attention Module for structured feature exchange, built on lightweight ViT-based DINOv2 backbone optimized for real-time deployment.

Result: Outperforms state-of-the-art multi-task models on NYUDv2, surpasses single-task baselines on Hypersim, achieves superior performance on Cityscapes while maintaining computational efficiency on laptop hardware.

Conclusion: M2H demonstrates practical effectiveness in spatial perception tasks and serves as foundation for monocular spatial perception systems supporting 3D scene graph construction.

Abstract: Deploying real-time spatial perception on edge devices requires efficient
multi-task models that leverage complementary task information while minimizing
computational overhead. This paper introduces Multi-Mono-Hydra (M2H), a novel
multi-task learning framework designed for semantic segmentation and depth,
edge, and surface normal estimation from a single monocular image. Unlike
conventional approaches that rely on independent single-task models or shared
encoder-decoder architectures, M2H introduces a Window-Based Cross-Task
Attention Module that enables structured feature exchange while preserving
task-specific details, improving prediction consistency across tasks. Built on
a lightweight ViT-based DINOv2 backbone, M2H is optimized for real-time
deployment and serves as the foundation for monocular spatial perception
systems supporting 3D scene graph construction in dynamic environments.
Comprehensive evaluations show that M2H outperforms state-of-the-art multi-task
models on NYUDv2, surpasses single-task depth and semantic baselines on
Hypersim, and achieves superior performance on the Cityscapes dataset, all
while maintaining computational efficiency on laptop hardware. Beyond
benchmarks, M2H is validated on real-world data, demonstrating its practicality
in spatial perception tasks.

</details>


### [243] [Recurrent Attention-based Token Selection for Efficient Streaming Video-LLMs](https://arxiv.org/abs/2510.17364)
*Vaggelis Dorovatas,Soroush Seifi,Gunshi Gupta,Rahaf Aljundi*

Main category: cs.CV

TL;DR: A training-free approach for Video-LLMs that enables efficient streaming video understanding by selecting important visual tokens, processing them recurrently, and using caption-based QA.


<details>
  <summary>Details</summary>
Motivation: Video-LLMs struggle with streaming scenarios where hour-long videos must be processed online and questions need timely responses, due to computational constraints.

Method: Uses three key concepts: 1) LLM-informed selection of visual tokens based on attention patterns, 2) Recurrent processing of past selected tokens for temporal coherence, 3) Caption-based question answering for lightweight responses.

Result: Achieves state-of-the-art performance on streaming video benchmarks, discarding up to ~95% of unimportant visual tokens with minimal performance loss.

Conclusion: The method effectively balances efficiency and effectiveness for streaming video understanding without requiring additional training.

Abstract: Video Large Language Models (Video-LLMs) excel at understanding videos
in-context, provided they have full access to the video when answering queries.
However, these models face challenges in streaming scenarios where hour-long
videos must be processed online, and questions need timely responses. In this
work, we propose a training-free approach compatible with standard Video-LLMs,
leveraging three key concepts: 1) LLM-informed selection of visual tokens to
identify those that the LLM has attended to and contributed to its
understanding of each short clip. Our attention-based selection allows us to
discard up to ~95% of unimportant visual tokens with minimal performance loss;
2) Recurrent processing of past selected tokens to generate temporally coherent
understanding of each processed clip; 3) Caption-based question answering for
lightweight and accurate responses. Our method achieves state-of-the-art
performance on streaming video benchmarks, striking a balance between
efficiency and effectiveness.

</details>


### [244] [Beyond Real Faces: Synthetic Datasets Can Achieve Reliable Recognition Performance without Privacy Compromise](https://arxiv.org/abs/2510.17372)
*Pawe Borsukiewicz,Fadi Boutros,Iyiola E. Olatunji,Charles Beumier,Wendkuni C. Ouedraogo,Jacques Klein,Tegawend F. Bissyand*

Main category: cs.CV

TL;DR: Synthetic facial data can effectively replace real datasets for facial recognition, achieving comparable or superior accuracy while addressing privacy concerns and enabling bias mitigation.


<details>
  <summary>Details</summary>
Motivation: Real facial recognition datasets raise ethical issues due to non-consensual data collection, leading to legal liabilities under regulations like GDPR. Synthetic data offers a privacy-preserving alternative but lacks comprehensive empirical validation.

Method: Systematic literature review of 25 synthetic facial recognition datasets (2018-2025) combined with experimental validation of 10+ million synthetic samples. Evaluation of seven privacy-preserving requirements including identity leakage prevention, intra-class variability, identity separability, and bias mitigation.

Result: Best-performing synthetic datasets (VariFace 95.67%, VIGFace 94.91%) surpass real datasets like CASIA-WebFace (94.70%). Public alternatives Vec2Face (93.52%) and CemiFace (93.22%) also perform well. Synthetic data maintains proper intra-class variability and identity separability while offering unprecedented control for bias mitigation.

Conclusion: Synthetic facial data is scientifically viable and ethically imperative for facial recognition research, providing comparable accuracy to real datasets while addressing privacy concerns and enabling bias control.

Abstract: The deployment of facial recognition systems has created an ethical dilemma:
achieving high accuracy requires massive datasets of real faces collected
without consent, leading to dataset retractions and potential legal liabilities
under regulations like GDPR. While synthetic facial data presents a promising
privacy-preserving alternative, the field lacks comprehensive empirical
evidence of its viability. This study addresses this critical gap through
extensive evaluation of synthetic facial recognition datasets. We present a
systematic literature review identifying 25 synthetic facial recognition
datasets (2018-2025), combined with rigorous experimental validation. Our
methodology examines seven key requirements for privacy-preserving synthetic
data: identity leakage prevention, intra-class variability, identity
separability, dataset scale, ethical data sourcing, bias mitigation, and
benchmark reliability. Through experiments involving over 10 million synthetic
samples, extended by a comparison of results reported on five standard
benchmarks, we provide the first comprehensive empirical assessment of
synthetic data's capability to replace real datasets. Best-performing synthetic
datasets (VariFace, VIGFace) achieve recognition accuracies of 95.67% and
94.91% respectively, surpassing established real datasets including
CASIA-WebFace (94.70%). While those images remain private, publicly available
alternatives Vec2Face (93.52%) and CemiFace (93.22%) come close behind. Our
findings reveal that they ensure proper intra-class variability while
maintaining identity separability. Demographic bias analysis shows that, even
though synthetic data inherits limited biases, it offers unprecedented control
for bias mitigation through generation parameters. These results establish
synthetic facial data as a scientifically viable and ethically imperative
alternative for facial recognition research.

</details>


### [245] [Facial Expression-based Parkinson's Disease Severity Diagnosis via Feature Fusion and Adaptive Class Balancing](https://arxiv.org/abs/2510.17373)
*Yintao Zhou,Wei Huang,Zhengyu Li,Jing Huang,Meng Pang*

Main category: cs.CV

TL;DR: Proposes a facial expression-based method for Parkinson's disease severity diagnosis using attention-based feature fusion and adaptive class balancing to address limitations of existing approaches.


<details>
  <summary>Details</summary>
Motivation: Current facial expression-based PD diagnosis methods rely on single expression types leading to misdiagnosis, ignore class imbalance across PD stages, and mostly focus on binary classification rather than severity diagnosis.

Method: Integrates multiple facial expression features through attention-based feature fusion and mitigates class imbalance via adaptive class balancing strategy that dynamically adjusts training sample contributions based on class distribution and classification difficulty.

Result: Experimental results demonstrate promising performance for PD severity diagnosis and confirm the efficacy of both attention-based feature fusion and adaptive class balancing strategies.

Conclusion: The proposed method effectively addresses limitations of existing approaches by combining multiple expression features and handling class imbalance, showing strong potential for accurate PD severity diagnosis through facial expression analysis.

Abstract: Parkinson's disease (PD) severity diagnosis is crucial for early detecting
potential patients and adopting tailored interventions. Diagnosing PD based on
facial expression is grounded in PD patients' "masked face" symptom and gains
growing interest recently for its convenience and affordability. However,
current facial expression-based approaches often rely on single type of
expression which can lead to misdiagnosis, and ignore the class imbalance
across different PD stages which degrades the prediction performance. Moreover,
most existing methods focus on binary classification (i.e., PD / non-PD) rather
than diagnosing the severity of PD. To address these issues, we propose a new
facial expression-based method for PD severity diagnosis which integrates
multiple facial expression features through attention-based feature fusion.
Moreover, we mitigate the class imbalance problem via an adaptive class
balancing strategy which dynamically adjusts the contribution of training
samples based on their class distribution and classification difficulty.
Experimental results demonstrate the promising performance of the proposed
method for PD severity diagnosis, as well as the efficacy of attention-based
feature fusion and adaptive class balancing.

</details>


### [246] [Closed-Loop Transfer for Weakly-supervised Affordance Grounding](https://arxiv.org/abs/2510.17384)
*Jiajin Tang,Zhengxuan Wei,Ge Zheng,Sibei Yang*

Main category: cs.CV

TL;DR: LoopTrans is a closed-loop framework that enables bidirectional knowledge transfer between exocentric and egocentric images for affordance grounding, improving performance in challenging scenarios including fully occluded interaction regions.


<details>
  <summary>Details</summary>
Motivation: Previous weakly-supervised affordance grounding methods only transferred knowledge one-way from exocentric to egocentric images, limiting their applicability in complex interaction scenarios.

Method: Introduces LoopTrans with unified cross-modal localization and denoising knowledge distillation to bridge domain gaps between object-centered egocentric and interaction-centered exocentric images.

Result: Achieves consistent improvements across all metrics on image and video benchmarks, handling challenging scenarios where object interaction regions are fully occluded by the human body.

Conclusion: The bidirectional knowledge transfer in LoopTrans enhances both exocentric knowledge extraction and egocentric affordance grounding, demonstrating superior performance in complex interaction scenarios.

Abstract: Humans can perform previously unexperienced interactions with novel objects
simply by observing others engage with them. Weakly-supervised affordance
grounding mimics this process by learning to locate object regions that enable
actions on egocentric images, using exocentric interaction images with
image-level annotations. However, extracting affordance knowledge solely from
exocentric images and transferring it one-way to egocentric images limits the
applicability of previous works in complex interaction scenarios. Instead, this
study introduces LoopTrans, a novel closed-loop framework that not only
transfers knowledge from exocentric to egocentric but also transfers back to
enhance exocentric knowledge extraction. Within LoopTrans, several innovative
mechanisms are introduced, including unified cross-modal localization and
denoising knowledge distillation, to bridge domain gaps between object-centered
egocentric and interaction-centered exocentric images while enhancing knowledge
transfer. Experiments show that LoopTrans achieves consistent improvements
across all metrics on image and video benchmarks, even handling challenging
scenarios where object interaction regions are fully occluded by the human
body.

</details>


### [247] [Monitoring Horses in Stalls: From Object to Event Detection](https://arxiv.org/abs/2510.17409)
*Dmitrii Galimzianov,Viacheslav Vyshegorodtsev,Ivan Nezhivykh*

Main category: cs.CV

TL;DR: A vision-based system using YOLOv11 and BoT-SORT automates horse and people detection/tracking in stables, distinguishing five event types while accounting for camera blind spots.


<details>
  <summary>Details</summary>
Motivation: Manual monitoring of stalled horses is labor-intensive and time-consuming, creating need for automated systems to detect health and welfare issues early.

Method: Uses object detection (YOLOv11) and multi-object tracking (BoT-SORT) to monitor horses and people, with event states inferred from object trajectories and spatial relations. Custom dataset created using CLIP and GroundingDINO foundation models.

Result: System reliably detects horse-related events but struggles with people detection due to data scarcity. Accounts for camera blind spots and distinguishes five event types.

Conclusion: Provides foundation for real-time behavioral monitoring in equine facilities with implications for animal welfare and stable management.

Abstract: Monitoring the behavior of stalled horses is essential for early detection of
health and welfare issues but remains labor-intensive and time-consuming. In
this study, we present a prototype vision-based monitoring system that
automates the detection and tracking of horses and people inside stables using
object detection and multi-object tracking techniques. The system leverages
YOLOv11 and BoT-SORT for detection and tracking, while event states are
inferred based on object trajectories and spatial relations within the stall.
To support development, we constructed a custom dataset annotated with
assistance from foundation models CLIP and GroundingDINO. The system
distinguishes between five event types and accounts for the camera's blind
spots. Qualitative evaluation demonstrated reliable performance for
horse-related events, while highlighting limitations in detecting people due to
data scarcity. This work provides a foundation for real-time behavioral
monitoring in equine facilities, with implications for animal welfare and
stable management.

</details>


### [248] [DeepDetect: Learning All-in-One Dense Keypoints](https://arxiv.org/abs/2510.17422)
*Shaharyar Ahmed Khan Tareen,Filza Khan Tareen*

Main category: cs.CV

TL;DR: DeepDetect is a dense keypoint detector that combines classical detectors with deep learning, achieving superior density, repeatability, and matching performance compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing keypoint detectors (both traditional and learning-based) suffer from limitations like sensitivity to photometric changes, low keypoint density and repeatability, limited adaptability to challenging scenes, and lack of semantic understanding of visually important regions.

Method: Fuses outputs from 7 keypoint and 2 edge detectors to create ground-truth masks, then trains a lightweight ESPNet model using these masks as labels to produce dense, semantically-aware keypoints.

Result: Outperforms other detectors on Oxford Affine Covariant Regions dataset with maximum values of 0.5143 (average keypoint density), 0.9582 (average repeatability), and 59,003 (correct matches).

Conclusion: DeepDetect successfully unifies classical detector strengths with deep learning, producing dense, repeatable keypoints that adapt well to diverse and degraded visual conditions.

Abstract: Keypoint detection is the foundation of many computer vision tasks, including
image registration, structure-from motion, 3D reconstruction, visual odometry,
and SLAM. Traditional detectors (SIFT, SURF, ORB, BRISK, etc.) and learning
based methods (SuperPoint, R2D2, LF-Net, D2-Net, etc.) have shown strong
performance yet suffer from key limitations: sensitivity to photometric
changes, low keypoint density and repeatability, limited adaptability to
challenging scenes, and lack of semantic understanding, often failing to
prioritize visually important regions. We present DeepDetect, an intelligent,
all-in-one, dense keypoint detector that unifies the strengths of classical
detectors using deep learning. Firstly, we create ground-truth masks by fusing
outputs of 7 keypoint and 2 edge detectors, extracting diverse visual cues from
corners and blobs to prominent edges and textures in the images. Afterwards, a
lightweight and efficient model: ESPNet, is trained using these masks as
labels, enabling DeepDetect to focus semantically on images while producing
highly dense keypoints, that are adaptable to diverse and visually degraded
conditions. Evaluations on the Oxford Affine Covariant Regions dataset
demonstrate that DeepDetect surpasses other detectors in keypoint density,
repeatability, and the number of correct matches, achieving maximum values of
0.5143 (average keypoint density), 0.9582 (average repeatability), and 59,003
(correct matches).

</details>


### [249] [Leveraging AV1 motion vectors for Fast and Dense Feature Matching](https://arxiv.org/abs/2510.17434)
*Julien Zouein,Hossein Javidnia,Franois Piti,Anil Kokaram*

Main category: cs.CV

TL;DR: Repurposing AV1 motion vectors for dense sub-pixel correspondences and short tracks, achieving comparable performance to SIFT with less CPU usage and denser matches.


<details>
  <summary>Details</summary>
Motivation: To create a resource-efficient front end for computer vision pipelines by leveraging existing compressed-domain data (AV1 motion vectors) rather than computing features from scratch.

Method: Using AV1 motion vectors to produce dense sub-pixel correspondences and short tracks filtered by cosine consistency, operating directly in the compressed domain.

Result: On short videos, performs comparably to sequential SIFT while using far less CPU, yields denser matches with competitive pairwise geometry. On a 117-frame clip, reconstructs 0.46-0.62M points at 0.51-0.53px reprojection error.

Conclusion: Compressed-domain correspondences are a practical, resource-efficient front end with clear paths to scaling in full pipelines.

Abstract: We repurpose AV1 motion vectors to produce dense sub-pixel correspondences
and short tracks filtered by cosine consistency. On short videos, this
compressed-domain front end runs comparably to sequential SIFT while using far
less CPU, and yields denser matches with competitive pairwise geometry. As a
small SfM demo on a 117-frame clip, MV matches register all images and
reconstruct 0.46-0.62M points at 0.51-0.53,px reprojection error; BA time grows
with match density. These results show compressed-domain correspondences are a
practical, resource-efficient front end with clear paths to scaling in full
pipelines.

</details>


### [250] [Rethinking Nighttime Image Deraining via Learnable Color Space Transformation](https://arxiv.org/abs/2510.17440)
*Qiyuan Guan,Xiang Chen,Guiyue Jin,Jiyu Jin,Shumin Fan,Tianyu Song,Jinshan Pan*

Main category: cs.CV

TL;DR: A new high-quality nighttime image deraining benchmark (HQ-NightRain) and a Color Space Transformation Network (CST-Net) that uses learnable color space conversion and implicit illumination guidance for better rain removal in nighttime scenes.


<details>
  <summary>Details</summary>
Motivation: Nighttime image deraining is more challenging than daytime due to complex nighttime scenarios and lack of high-quality datasets that accurately represent the coupling between rain and illumination effects.

Method: Proposed CST-Net with learnable color space converter (CSC) to facilitate rain removal in Y channel where nighttime rain is more pronounced, and implicit illumination guidance to capture illumination information for improved robustness.

Result: Extensive experiments demonstrate the value of the new HQ-NightRain dataset and the effectiveness of the proposed CST-Net method.

Conclusion: The work provides both a high-quality benchmark dataset and an effective method for nighttime image deraining, addressing key challenges in this domain.

Abstract: Compared to daytime image deraining, nighttime image deraining poses
significant challenges due to inherent complexities of nighttime scenarios and
the lack of high-quality datasets that accurately represent the coupling effect
between rain and illumination. In this paper, we rethink the task of nighttime
image deraining and contribute a new high-quality benchmark, HQ-NightRain,
which offers higher harmony and realism compared to existing datasets. In
addition, we develop an effective Color Space Transformation Network (CST-Net)
for better removing complex rain from nighttime scenes. Specifically, we
propose a learnable color space converter (CSC) to better facilitate rain
removal in the Y channel, as nighttime rain is more pronounced in the Y channel
compared to the RGB color space. To capture illumination information for
guiding nighttime deraining, implicit illumination guidance is introduced
enabling the learned features to improve the model's robustness in complex
scenarios. Extensive experiments show the value of our dataset and the
effectiveness of our method. The source code and datasets are available at
https://github.com/guanqiyuan/CST-Net.

</details>


### [251] [Initialize to Generalize: A Stronger Initialization Pipeline for Sparse-View 3DGS](https://arxiv.org/abs/2510.17479)
*Feng Zhou,Wenkai Guo,Pu Cao,Zhicheng Zhang,Jianqin Yin*

Main category: cs.CV

TL;DR: The paper shows that initialization is the decisive factor for sparse-view 3D Gaussian Splatting performance, and proposes three methods to improve SfM-based initialization: frequency-aware SfM, 3DGS self-initialization, and point-cloud regularization.


<details>
  <summary>Details</summary>
Motivation: Sparse-view 3DGS often overfits to training views, causing blurring in novel view rendering. Previous approaches focus on training-time constraints, but the authors found initialization is actually the key factor determining performance.

Method: Three complementary initialization methods: (1) frequency-aware SfM using low-frequency view augmentation for better low-texture coverage, (2) 3DGS self-initialization that lifts photometric supervision into additional points, and (3) point-cloud regularization with geometric/visibility priors for multi-view consistency.

Result: Experiments on LLFF and Mip-NeRF360 datasets show consistent improvements in sparse-view settings, establishing the approach as a stronger initialization strategy.

Conclusion: Initialization quality is crucial for sparse-view 3DGS performance, and the proposed methods effectively enhance SfM-based initialization to achieve better novel view synthesis.

Abstract: Sparse-view 3D Gaussian Splatting (3DGS) often overfits to the training
views, leading to artifacts like blurring in novel view rendering. Prior work
addresses it either by enhancing the initialization (\emph{i.e.}, the point
cloud from Structure-from-Motion (SfM)) or by adding training-time constraints
(regularization) to the 3DGS optimization. Yet our controlled ablations reveal
that initialization is the decisive factor: it determines the attainable
performance band in sparse-view 3DGS, while training-time constraints yield
only modest within-band improvements at extra cost. Given initialization's
primacy, we focus our design there. Although SfM performs poorly under sparse
views due to its reliance on feature matching, it still provides reliable seed
points. Thus, building on SfM, our effort aims to supplement the regions it
fails to cover as comprehensively as possible. Specifically, we design: (i)
frequency-aware SfM that improves low-texture coverage via low-frequency view
augmentation and relaxed multi-view correspondences; (ii) 3DGS
self-initialization that lifts photometric supervision into additional points,
compensating SfM-sparse regions with learned Gaussian centers; and (iii)
point-cloud regularization that enforces multi-view consistency and uniform
spatial coverage through simple geometric/visibility priors, yielding a clean
and reliable point cloud. Our experiments on LLFF and Mip-NeRF360 demonstrate
consistent gains in sparse-view settings, establishing our approach as a
stronger initialization strategy. Code is available at
https://github.com/zss171999645/ItG-GS.

</details>


### [252] [SparseWorld: A Flexible, Adaptive, and Efficient 4D Occupancy World Model Powered by Sparse and Dynamic Queries](https://arxiv.org/abs/2510.17482)
*Chenxu Dang,Haiyan Liu,Guangjun Bao,Pei An,Xinyue Tang,Jie Ma,Bingchuan Sun,Yan Wang*

Main category: cs.CV

TL;DR: SparseWorld is a novel 4D occupancy world model that uses sparse dynamic queries for flexible and adaptive perception, featuring range-adaptive perception and state-conditioned forecasting modules.


<details>
  <summary>Details</summary>
Motivation: Existing occupancy world models rely on static embeddings/grids that limit perception flexibility and misalign with the dynamic, continuous nature of real scenarios.

Method: Proposes Range-Adaptive Perception module with learnable queries modulated by ego vehicle states, and State-Conditioned Forecasting module using regression-guided formulation instead of classification. Includes Temporal-Aware Self-Scheduling training strategy.

Result: Achieves state-of-the-art performance across perception, forecasting, and planning tasks. Demonstrates advantages in flexibility, adaptability, and efficiency through extensive experiments and ablation studies.

Conclusion: SparseWorld provides an effective solution for 4D occupancy modeling that better aligns with real-world dynamics through sparse dynamic queries and regression-based forecasting.

Abstract: Semantic occupancy has emerged as a powerful representation in world models
for its ability to capture rich spatial semantics. However, most existing
occupancy world models rely on static and fixed embeddings or grids, which
inherently limit the flexibility of perception. Moreover, their ``in-place
classification" over grids exhibits a potential misalignment with the dynamic
and continuous nature of real scenarios.In this paper, we propose SparseWorld,
a novel 4D occupancy world model that is flexible, adaptive, and efficient,
powered by sparse and dynamic queries. We propose a Range-Adaptive Perception
module, in which learnable queries are modulated by the ego vehicle states and
enriched with temporal-spatial associations to enable extended-range
perception. To effectively capture the dynamics of the scene, we design a
State-Conditioned Forecasting module, which replaces classification-based
forecasting with regression-guided formulation, precisely aligning the dynamic
queries with the continuity of the 4D environment. In addition, We specifically
devise a Temporal-Aware Self-Scheduling training strategy to enable smooth and
efficient training. Extensive experiments demonstrate that SparseWorld achieves
state-of-the-art performance across perception, forecasting, and planning
tasks. Comprehensive visualizations and ablation studies further validate the
advantages of SparseWorld in terms of flexibility, adaptability, and
efficiency. The code is available at https://github.com/MSunDYY/SparseWorld.

</details>


### [253] [Split-Fuse-Transport: Annotation-Free Saliency via Dual Clustering and Optimal Transport Alignment](https://arxiv.org/abs/2510.17484)
*Muhammad Umer Ramzan,Ali Zia,Abdelwahed Khamis,Noman Ali,Usman Ali,Wei Xiang*

Main category: cs.CV

TL;DR: POTNet introduces an entropy-guided dual-clustering approach for unsupervised salient object detection, replacing single k-means with spectral clustering for high-entropy pixels and k-means for low-entropy pixels, aligned by optimal transport to generate high-quality pseudo-masks without labels.


<details>
  <summary>Details</summary>
Motivation: Salient object detection can achieve near-supervised accuracy without pixel-level labels if reliable pseudo-masks are available. Current prototype-based methods underutilize optimal transport when prototype quality is weak, and boundary/interior pixels have different geometric properties.

Method: POTNet uses entropy-guided dual-clustering: spectral clustering for high-entropy pixels and k-means for low-entropy pixels, with the two prototype sets aligned by optimal transport. This generates sharp, part-aware pseudo-masks in one forward pass without handcrafted priors.

Result: AutoSOD (the end-to-end pipeline using POTNet) outperforms unsupervised methods by up to 26% and weakly supervised methods by up to 36% in F-measure across five benchmarks, while eliminating SelfMask's offline voting and improving training efficiency.

Conclusion: The split-fuse-transport design with entropy-guided dual-clustering and optimal transport alignment enables high-quality pseudo-mask generation, significantly narrowing the performance gap between unsupervised and fully supervised salient object detection.

Abstract: Salient object detection (SOD) aims to segment visually prominent regions in
images and serves as a foundational task for various computer vision
applications. We posit that SOD can now reach near-supervised accuracy without
a single pixel-level label, but only when reliable pseudo-masks are available.
We revisit the prototype-based line of work and make two key observations.
First, boundary pixels and interior pixels obey markedly different geometry;
second, the global consistency enforced by optimal transport (OT) is
underutilized if prototype quality is weak. To address this, we introduce
POTNet, an adaptation of Prototypical Optimal Transport that replaces POT's
single k-means step with an entropy-guided dual-clustering head: high-entropy
pixels are organized by spectral clustering, low-entropy pixels by k-means, and
the two prototype sets are subsequently aligned by OT. This
split-fuse-transport design yields sharper, part-aware pseudo-masks in a single
forward pass, without handcrafted priors. Those masks supervise a standard
MaskFormer-style encoder-decoder, giving rise to AutoSOD, an end-to-end
unsupervised SOD pipeline that eliminates SelfMask's offline voting yet
improves both accuracy and training efficiency. Extensive experiments on five
benchmarks show that AutoSOD outperforms unsupervised methods by up to 26% and
weakly supervised methods by up to 36% in F-measure, further narrowing the gap
to fully supervised models.

</details>


### [254] [Context-Aware Pseudo-Label Scoring for Zero-Shot Video Summarization](https://arxiv.org/abs/2510.17501)
*Yuanli Wu,Long Zhang,Yue Du,Bin Li*

Main category: cs.CV

TL;DR: A rubric-guided pseudo-labeling framework for zero-shot video summarization that uses ground-truth annotations to create structured scoring rubrics, enabling LLMs to evaluate scenes with contextual awareness without training.


<details>
  <summary>Details</summary>
Motivation: Existing supervised methods have high labeling costs and limited generalization, while unsupervised approaches fail to capture human semantics. Zero-shot LLM methods are sensitive to prompts and dataset-specific normalization.

Method: Transform ground-truth annotations into pseudo labels aggregated into dataset-adaptive scoring rubrics. Score first/last segments by descriptions only, and intermediate segments with contextual summaries of adjacent scenes for narrative assessment.

Result: Achieved F1 scores of 57.58 on SumMe and 63.05 on TVSum, surpassing unsupervised and prior zero-shot baselines while approaching supervised performance.

Conclusion: Rubric-guided pseudo labeling stabilizes LLM-based scoring and establishes a general, interpretable zero-shot paradigm for video summarization.

Abstract: With the rapid proliferation of video content across social media,
surveillance, and education platforms, efficiently summarizing long videos into
concise yet semantically faithful surrogates has become increasingly vital.
Existing supervised methods achieve strong in-domain accuracy by learning from
dense annotations but suffer from high labeling costs and limited cross-dataset
generalization, while unsupervised approaches, though label-free, often fail to
capture high-level human semantics and fine-grained narrative cues. More
recently, zero-shot prompting pipelines have leveraged large language models
(LLMs) for training-free video summarization, yet remain highly sensitive to
handcrafted prompt templates and dataset-specific score normalization. To
overcome these limitations, we introduce a rubric-guided, pseudo-labeled
prompting framework that transforms a small subset of ground-truth annotations
into high-confidence pseudo labels, which are aggregated into structured,
dataset-adaptive scoring rubrics guiding interpretable scene evaluation. During
inference, first and last segments are scored based solely on their
descriptions, whereas intermediate ones incorporate brief contextual summaries
of adjacent scenes to assess narrative progression and redundancy. This
contextual prompting enables the LLM to balance local salience and global
coherence without parameter tuning. On SumMe and TVSum, our method achieves F1
scores of \textbf{57.58} and \textbf{63.05}, surpassing unsupervised and prior
zero-shot baselines while approaching supervised performance. The results
demonstrate that rubric-guided pseudo labeling effectively stabilizes LLM-based
scoring and establishes a general, interpretable zero-shot paradigm for video
summarization.

</details>


### [255] [MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation Models](https://arxiv.org/abs/2510.17519)
*Yongshun Zhang,Zhongyi Fan,Yonghang Zhang,Zhangzikang Li,Weifeng Chen,Zhongwei Feng,Chaoyue Wang,Peng Hou,Anxiang Zeng*

Main category: cs.CV

TL;DR: A training framework for large-scale video generation models that optimizes data processing, model architecture, training strategy, and infrastructure, resulting in MUG-V 10B model that matches state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Training large-scale video generation models is challenging due to cross-modal text-video alignment, long sequences, and complex spatiotemporal dependencies, requiring resource-intensive solutions.

Method: Optimized framework across four pillars: data processing, model architecture, training strategy, and infrastructure, including data preprocessing, video compression, parameter scaling, curriculum-based pretraining, and alignment-focused post-training.

Result: MUG-V 10B matches state-of-the-art video generators overall and surpasses leading open-source baselines in e-commerce-oriented video generation tasks in human evaluations.

Conclusion: The complete stack including model weights, Megatron-Core-based training code, and inference pipelines is open-sourced, representing the first public release of large-scale video generation training code with high efficiency and near-linear multi-node scaling.

Abstract: In recent years, large-scale generative models for visual content
(\textit{e.g.,} images, videos, and 3D objects/scenes) have made remarkable
progress. However, training large-scale video generation models remains
particularly challenging and resource-intensive due to cross-modal text-video
alignment, the long sequences involved, and the complex spatiotemporal
dependencies. To address these challenges, we present a training framework that
optimizes four pillars: (i) data processing, (ii) model architecture, (iii)
training strategy, and (iv) infrastructure for large-scale video generation
models. These optimizations delivered significant efficiency gains and
performance improvements across all stages of data preprocessing, video
compression, parameter scaling, curriculum-based pretraining, and
alignment-focused post-training. Our resulting model, MUG-V 10B, matches recent
state-of-the-art video generators overall and, on e-commerce-oriented video
generation tasks, surpasses leading open-source baselines in human evaluations.
More importantly, we open-source the complete stack, including model weights,
Megatron-Core-based large-scale training code, and inference pipelines for
video generation and enhancement. To our knowledge, this is the first public
release of large-scale video generation training code that exploits
Megatron-Core to achieve high training efficiency and near-linear multi-node
scaling, details are available in
\href{https://github.com/Shopee-MUG/MUG-V}{our webpage}.

</details>


### [256] [MambaX-Net: Dual-Input Mamba-Enhanced Cross-Attention Network for Longitudinal MRI Segmentation](https://arxiv.org/abs/2510.17529)
*Yovin Yahathugoda,Davide Prezzi,Piyalitt Ittichaiwong,Vicky Goh,Sebastien Ourselin,Michela Antonelli*

Main category: cs.CV

TL;DR: MambaX-Net is a semi-supervised 3D segmentation model for longitudinal Active Surveillance of prostate cancer, using temporal information from previous scans and pseudo-labels to overcome limited expert annotations.


<details>
  <summary>Details</summary>
Motivation: Existing deep-learning segmentation models are unsuitable for longitudinal Active Surveillance analysis due to being trained on single-time-point data and requiring expert annotations, which are scarce in clinical practice.

Method: Proposed MambaX-Net with Mamba-enhanced Cross-Attention Module for temporal evolution and long-range dependencies, Shape Extractor Module for anatomical representation, and semi-supervised self-training using pseudo-labels from pre-trained nnU-Net.

Result: MambaX-Net significantly outperforms state-of-the-art U-Net and Transformer-based models, achieving superior prostate zone segmentation even with limited and noisy training data.

Conclusion: The proposed approach enables effective longitudinal prostate cancer monitoring in Active Surveillance by leveraging temporal information and overcoming annotation scarcity through semi-supervised learning.

Abstract: Active Surveillance (AS) is a treatment option for managing low and
intermediate-risk prostate cancer (PCa), aiming to avoid overtreatment while
monitoring disease progression through serial MRI and clinical follow-up.
Accurate prostate segmentation is an important preliminary step for automating
this process, enabling automated detection and diagnosis of PCa. However,
existing deep-learning segmentation models are often trained on
single-time-point and expertly annotated datasets, making them unsuitable for
longitudinal AS analysis, where multiple time points and a scarcity of expert
labels hinder their effective fine-tuning. To address these challenges, we
propose MambaX-Net, a novel semi-supervised, dual-scan 3D segmentation
architecture that computes the segmentation for time point t by leveraging the
MRI and the corresponding segmentation mask from the previous time point. We
introduce two new components: (i) a Mamba-enhanced Cross-Attention Module,
which integrates the Mamba block into cross attention to efficiently capture
temporal evolution and long-range spatial dependencies, and (ii) a Shape
Extractor Module that encodes the previous segmentation mask into a latent
anatomical representation for refined zone delination. Moreover, we introduce a
semi-supervised self-training strategy that leverages pseudo-labels generated
from a pre-trained nnU-Net, enabling effective learning without expert
annotations. MambaX-Net was evaluated on a longitudinal AS dataset, and results
showed that it significantly outperforms state-of-the-art U-Net and
Transformer-based models, achieving superior prostate zone segmentation even
when trained on limited and noisy data.

</details>


### [257] [UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action](https://arxiv.org/abs/2510.17790)
*Yuhao Yang,Zhen Yang,Zi-Yi Dou,Anh Nguyen,Keen You,Omar Attia,Andrew Szot,Michael Feng,Ram Ramrakhya,Alexander Toshev,Chao Huang,Yinfei Yang,Zhe Gan*

Main category: cs.CV

TL;DR: UltraCUA is a foundation model that bridges multimodal computer-use agents with programmatic tools through hybrid actions, combining GUI primitives with high-level tool calls to reduce cascading failures and improve performance.


<details>
  <summary>Details</summary>
Motivation: Current computer-use agents rely exclusively on primitive GUI actions (click, type, scroll) which lead to cascading failures and performance bottlenecks, while remaining isolated from rich programmatic interfaces available to other agents.

Method: Four key components: (1) automated pipeline for scaling programmatic tools from documentation and code, (2) synthetic data engine producing 17,000+ verifiable tasks, (3) large-scale hybrid action trajectory collection, and (4) two-stage training combining supervised fine-tuning with online reinforcement learning.

Result: 7B and 32B models show 22% relative improvement on OSWorld, 11% faster execution, and 21.7% success rate on WindowsAgentArena (outperforming Windows-trained baselines). Hybrid actions reduce error propagation while maintaining efficiency.

Conclusion: Hybrid action mechanism successfully bridges GUI primitives with programmatic tools, enabling more efficient and robust computer-use agents with substantial performance improvements over state-of-the-art approaches.

Abstract: Multimodal agents for computer use rely exclusively on primitive actions
(click, type, scroll) that require accurate visual grounding and lengthy
execution chains, leading to cascading failures and performance bottlenecks.
While other agents leverage rich programmatic interfaces (APIs, MCP servers,
tools), computer-use agents (CUAs) remain isolated from these capabilities. We
present UltraCUA, a foundation model that bridges this gap through hybrid
action -- seamlessly integrating GUI primitives with high-level programmatic
tool calls. To achieve this, our approach comprises four key components: (1) an
automated pipeline that scales programmatic tools from software documentation,
open-source repositories, and code generation; (2) a synthetic data engine
producing over 17,000 verifiable tasks spanning real-world computer-use
scenarios; (3) a large-scale high-quality hybrid action trajectory collection
with both low-level GUI actions and high-level programmatic tool calls; and (4)
a two-stage training pipeline combining supervised fine-tuning with online
reinforcement learning, enabling strategic alternation between low-level and
high-level actions. Experiments with our 7B and 32B models demonstrate
substantial improvements over state-of-the-art agents. On OSWorld, UltraCUA
models achieve an average 22% relative improvement over base models, while
being 11% faster in terms of steps. Out-of-domain evaluation on
WindowsAgentArena shows our model reaches 21.7% success rate, outperforming
baselines trained on Windows data. The hybrid action mechanism proves critical,
reducing error propagation while maintaining execution efficiency.

</details>


### [258] [WP-CrackNet: A Collaborative Adversarial Learning Framework for End-to-End Weakly-Supervised Road Crack Detection](https://arxiv.org/abs/2510.17566)
*Nachuan Ma,Zhengfei Song,Qiang Hu,Xiaoyu Tang,Chengxi Zhang,Rui Fan,Lihua Xie*

Main category: cs.CV

TL;DR: WP-CrackNet is a weakly-supervised method for road crack detection that uses only image-level labels instead of costly pixel-level annotations, achieving comparable performance to supervised methods through adversarial learning between classifier and reconstructor components.


<details>
  <summary>Details</summary>
Motivation: To reduce reliance on expensive pixel-level annotations for road crack detection in smart city infrastructure maintenance, making crack detection more scalable and cost-effective.

Method: End-to-end weakly-supervised framework with three components: classifier generating CAMs, reconstructor measuring feature inferability, and detector producing pixel-wise results. Uses adversarial learning between classifier and reconstructor, path-aware attention module (PAAM) for feature fusion, and center-enhanced CAM consistency module (CECCM) for pseudo-label refinement.

Result: Achieves comparable results to supervised methods and outperforms existing weakly-supervised methods on three created image-level datasets, significantly advancing scalable road inspection.

Conclusion: WP-CrackNet demonstrates that weakly-supervised learning with only image-level labels can achieve competitive crack detection performance, making road inspection more scalable and cost-effective for smart city infrastructure maintenance.

Abstract: Road crack detection is essential for intelligent infrastructure maintenance
in smart cities. To reduce reliance on costly pixel-level annotations, we
propose WP-CrackNet, an end-to-end weakly-supervised method that trains with
only image-level labels for pixel-wise crack detection. WP-CrackNet integrates
three components: a classifier generating class activation maps (CAMs), a
reconstructor measuring feature inferability, and a detector producing
pixel-wise road crack detection results. During training, the classifier and
reconstructor alternate in adversarial learning to encourage crack CAMs to
cover complete crack regions, while the detector learns from pseudo labels
derived from post-processed crack CAMs. This mutual feedback among the three
components improves learning stability and detection accuracy. To further boost
detection performance, we design a path-aware attention module (PAAM) that
fuses high-level semantics from the classifier with low-level structural cues
from the reconstructor by modeling spatial and channel-wise dependencies.
Additionally, a center-enhanced CAM consistency module (CECCM) is proposed to
refine crack CAMs using center Gaussian weighting and consistency constraints,
enabling better pseudo-label generation. We create three image-level datasets
and extensive experiments show that WP-CrackNet achieves comparable results to
supervised methods and outperforms existing weakly-supervised methods,
significantly advancing scalable road inspection. The source code package and
datasets are available at https://mias.group/WP-CrackNet/.

</details>


### [259] [Glyph: Scaling Context Windows via Visual-Text Compression](https://arxiv.org/abs/2510.17800)
*Jiale Cheng,Yusen Liu,Xinyu Zhang,Yulin Fei,Wenyi Hong,Ruiliang Lyu,Weihan Wang,Zhe Su,Xiaotao Gu,Xiao Liu,Yushi Bai,Jie Tang,Hongning Wang,Minlie Huang*

Main category: cs.CV

TL;DR: Glyph is a framework that converts long text into images for processing by vision-language models, achieving 3-4x token compression while maintaining accuracy comparable to leading LLMs on long-context tasks.


<details>
  <summary>Details</summary>
Motivation: Scaling context windows to million-token levels in LLMs brings prohibitive computational and memory costs, limiting the practicality of long-context models.

Method: Render long texts into images and process them with vision-language models, using an LLM-driven genetic search to identify optimal visual rendering configurations for balancing accuracy and compression.

Result: Achieves 3-4x token compression with comparable accuracy to Qwen3-8B on long-context benchmarks, 4x faster prefilling/decoding, 2x faster SFT training, and enables 128K-context VLM to handle 1M-token tasks.

Conclusion: Visual context scaling through text-to-image rendering provides an effective alternative to token-based sequence extension, offering substantial compression and efficiency gains for long-context processing.

Abstract: Large language models (LLMs) increasingly rely on long-context modeling for
tasks such as document understanding, code analysis, and multi-step reasoning.
However, scaling context windows to the million-token level brings prohibitive
computational and memory costs, limiting the practicality of long-context LLMs.
In this work, we take a different perspective-visual context scaling-to tackle
this challenge. Instead of extending token-based sequences, we propose Glyph, a
framework that renders long texts into images and processes them with
vision-language models (VLMs). This approach substantially compresses textual
input while preserving semantic information, and we further design an
LLM-driven genetic search to identify optimal visual rendering configurations
for balancing accuracy and compression. Through extensive experiments, we
demonstrate that our method achieves 3-4x token compression while maintaining
accuracy comparable to leading LLMs such as Qwen3-8B on various long-context
benchmarks. This compression also leads to around 4x faster prefilling and
decoding, and approximately 2x faster SFT training. Furthermore, under extreme
compression, a 128K-context VLM could scale to handle 1M-token-level text
tasks. In addition, the rendered text data benefits real-world multimodal
tasks, such as document understanding. Our code and model are released at
https://github.com/thu-coai/Glyph.

</details>


### [260] [PAGE-4D: Disentangled Pose and Geometry Estimation for 4D Perception](https://arxiv.org/abs/2510.17568)
*Kaichen Zhou,Yuhan Wang,Grace Chen,Xinhai Chang,Gaspard Beaudouin,Fangneng Zhan,Paul Pu Liang,Mengyu Wang*

Main category: cs.CV

TL;DR: PAGE-4D extends VGGT to handle dynamic scenes by introducing a dynamics-aware aggregator that disentangles static and dynamic information, enabling simultaneous camera pose estimation, depth prediction, and point cloud reconstruction without post-processing.


<details>
  <summary>Details</summary>
Motivation: Existing 3D feed-forward models like VGGT struggle with dynamic elements in real-world scenarios because they are trained on static datasets, limiting their performance with moving humans or deformable objects.

Method: Proposes a dynamics-aware aggregator that predicts a dynamics-aware mask to disentangle static and dynamic information - suppressing motion cues for pose estimation while amplifying them for geometry reconstruction.

Result: Extensive experiments show PAGE-4D consistently outperforms VGGT in dynamic scenarios, achieving superior results in camera pose estimation, monocular/video depth estimation, and dense point map reconstruction.

Conclusion: PAGE-4D successfully addresses the task conflict in multi-task 4D reconstruction and demonstrates strong performance in handling dynamic scenes compared to static-only models.

Abstract: Recent 3D feed-forward models, such as the Visual Geometry Grounded
Transformer (VGGT), have shown strong capability in inferring 3D attributes of
static scenes. However, since they are typically trained on static datasets,
these models often struggle in real-world scenarios involving complex dynamic
elements, such as moving humans or deformable objects like umbrellas. To
address this limitation, we introduce PAGE-4D, a feedforward model that extends
VGGT to dynamic scenes, enabling camera pose estimation, depth prediction, and
point cloud reconstruction -- all without post-processing. A central challenge
in multi-task 4D reconstruction is the inherent conflict between tasks:
accurate camera pose estimation requires suppressing dynamic regions, while
geometry reconstruction requires modeling them. To resolve this tension, we
propose a dynamics-aware aggregator that disentangles static and dynamic
information by predicting a dynamics-aware mask -- suppressing motion cues for
pose estimation while amplifying them for geometry reconstruction. Extensive
experiments show that PAGE-4D consistently outperforms the original VGGT in
dynamic scenarios, achieving superior results in camera pose estimation,
monocular and video depth estimation, and dense point map reconstruction.

</details>


### [261] [Expose Camouflage in the Water: Underwater Camouflaged Instance Segmentation and Dataset](https://arxiv.org/abs/2510.17585)
*Chuhong Wang,Hua Li,Chongyi Li,Huazhong Liu,Xiongxin Tang,Sam Kwong*

Main category: cs.CV

TL;DR: The paper introduces UCIS4K, the first underwater camouflaged instance segmentation dataset with 3,953 images, and proposes UCIS-SAM network with three modules to address underwater degradation challenges for better segmentation of camouflaged marine organisms.


<details>
  <summary>Details</summary>
Motivation: Underwater vision tasks face challenges due to degraded environments (color distortion, low contrast, blurring), and traditional methods trained on terrestrial datasets perform poorly on underwater camouflaged instance segmentation.

Method: Proposed UCIS-SAM network with three modules: Channel Balance Optimization Module (CBOM) for underwater feature learning, Frequency Domain True Integration Module (FDTIM) to reduce camouflage interference, and Multi-scale Feature Frequency Aggregation Module (MFFAM) for boundary enhancement.

Result: Extensive experiments on UCIS4K and public benchmarks show UCIS-SAM outperforms state-of-the-art approaches in underwater camouflaged instance segmentation.

Conclusion: The proposed UCIS-SAM network effectively addresses underwater degradation challenges and improves segmentation performance for camouflaged marine organisms through specialized modules for underwater feature enhancement and camouflage pattern reduction.

Abstract: With the development of underwater exploration and marine protection,
underwater vision tasks are widespread. Due to the degraded underwater
environment, characterized by color distortion, low contrast, and blurring,
camouflaged instance segmentation (CIS) faces greater challenges in accurately
segmenting objects that blend closely with their surroundings. Traditional
camouflaged instance segmentation methods, trained on terrestrial-dominated
datasets with limited underwater samples, may exhibit inadequate performance in
underwater scenes. To address these issues, we introduce the first underwater
camouflaged instance segmentation (UCIS) dataset, abbreviated as UCIS4K, which
comprises 3,953 images of camouflaged marine organisms with instance-level
annotations. In addition, we propose an Underwater Camouflaged Instance
Segmentation network based on Segment Anything Model (UCIS-SAM). Our UCIS-SAM
includes three key modules. First, the Channel Balance Optimization Module
(CBOM) enhances channel characteristics to improve underwater feature learning,
effectively addressing the model's limited understanding of underwater
environments. Second, the Frequency Domain True Integration Module (FDTIM) is
proposed to emphasize intrinsic object features and reduce interference from
camouflage patterns, enhancing the segmentation performance of camouflaged
objects blending with their surroundings. Finally, the Multi-scale Feature
Frequency Aggregation Module (MFFAM) is designed to strengthen the boundaries
of low-contrast camouflaged instances across multiple frequency bands,
improving the model's ability to achieve more precise segmentation of
camouflaged objects. Extensive experiments on the proposed UCIS4K and public
benchmarks show that our UCIS-SAM outperforms state-of-the-art approaches.

</details>


### [262] [ShapeCraft: LLM Agents for Structured, Textured and Interactive 3D Modeling](https://arxiv.org/abs/2510.17603)
*Shuyuan Zhang,Chenhan Jiang,Zuoou Li,Jiankang Deng*

Main category: cs.CV

TL;DR: ShapeCraft is a multi-agent framework that generates structured, textured 3D assets from natural language using Graph-based Procedural Shape representation and LLM agents.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-3D methods produce unstructured meshes with poor interactivity, making them impractical for artistic workflows. There's a need for structured, interactive 3D generation.

Method: Proposes Graph-based Procedural Shape (GPS) representation that decomposes language into structured sub-task graphs. Uses LLM agents to hierarchically parse input, initialize GPS, and iteratively refine procedural modeling and painting.

Result: ShapeCraft outperforms existing LLM-based methods in generating geometrically accurate and semantically rich 3D assets. Enables animated and user-customized editing.

Conclusion: ShapeCraft demonstrates superior 3D generation capabilities and versatility for interactive applications, addressing limitations of current unstructured mesh generation approaches.

Abstract: 3D generation from natural language offers significant potential to reduce
expert manual modeling efforts and enhance accessibility to 3D assets. However,
existing methods often yield unstructured meshes and exhibit poor
interactivity, making them impractical for artistic workflows. To address these
limitations, we represent 3D assets as shape programs and introduce ShapeCraft,
a novel multi-agent framework for text-to-3D generation. At its core, we
propose a Graph-based Procedural Shape (GPS) representation that decomposes
complex natural language into a structured graph of sub-tasks, thereby
facilitating accurate LLM comprehension and interpretation of spatial
relationships and semantic shape details. Specifically, LLM agents
hierarchically parse user input to initialize GPS, then iteratively refine
procedural modeling and painting to produce structured, textured, and
interactive 3D assets. Qualitative and quantitative experiments demonstrate
ShapeCraft's superior performance in generating geometrically accurate and
semantically rich 3D assets compared to existing LLM-based agents. We further
show the versatility of ShapeCraft through examples of animated and
user-customized editing, highlighting its potential for broader interactive
applications.

</details>


### [263] [Integrating BIM and UAV-based photogrammetry for Automated 3D Structure Model Segmentation](https://arxiv.org/abs/2510.17609)
*Siqi Chen,Shanyue Guan*

Main category: cs.CV

TL;DR: A machine learning framework for automated segmentation of 3D point clouds from UAV scans, using real-world data and synthetic BIM data to overcome manual labeling limitations.


<details>
  <summary>Details</summary>
Motivation: To address the time-consuming and error-prone manual labeling required for segmenting structural components from UAV-captured 3D models in structural health monitoring.

Method: Combines real-world UAV-scanned point clouds with synthetic data generated from Building Information Modeling (BIM) to train machine learning models for automated segmentation.

Result: High accuracy in identifying and segmenting railroad track components (rails and crossties), with reduced training time while maintaining reasonable segmentation accuracy using smaller datasets supplemented with BIM data.

Conclusion: The automated approach improves precision and efficiency of 3D infrastructure model segmentation and advances integration of UAV and BIM technologies in structural health monitoring and infrastructure management.

Abstract: The advancement of UAV technology has enabled efficient, non-contact
structural health monitoring. Combined with photogrammetry, UAVs can capture
high-resolution scans and reconstruct detailed 3D models of infrastructure.
However, a key challenge remains in segmenting specific structural components
from these models-a process traditionally reliant on time-consuming and
error-prone manual labeling. To address this issue, we propose a machine
learning-based framework for automated segmentation of 3D point clouds. Our
approach uses the complementary strengths of real-world UAV-scanned point
clouds and synthetic data generated from Building Information Modeling (BIM) to
overcome the limitations associated with manual labeling. Validation on a
railroad track dataset demonstrated high accuracy in identifying and segmenting
major components such as rails and crossties. Moreover, by using smaller-scale
datasets supplemented with BIM data, the framework significantly reduced
training time while maintaining reasonable segmentation accuracy. This
automated approach improves the precision and efficiency of 3D infrastructure
model segmentation and advances the integration of UAV and BIM technologies in
structural health monitoring and infrastructure management.

</details>


### [264] [One Dinomaly2 Detect Them All: A Unified Framework for Full-Spectrum Unsupervised Anomaly Detection](https://arxiv.org/abs/2510.17611)
*Jia Guo,Shuai Lu,Lei Fan,Zelin Li,Donglin Di,Yang Song,Weihang Zhang,Wenbing Zhu,Hong Yan,Fang Chen,Huiqi Li,Hongen Liao*

Main category: cs.CV

TL;DR: Dinomaly2 is a unified framework for unsupervised anomaly detection that bridges performance gaps in multi-class models and extends across diverse data modalities and task settings through minimalistic design.


<details>
  <summary>Details</summary>
Motivation: Existing multi-class anomaly detection models underperform compared to specialized single-class models, and the field has fragmented into scenario-specific methods, creating deployment barriers that need a unified solution.

Method: A reconstruction-based framework guided by "less is more" philosophy, orchestrating five simple elements to achieve superior performance without modification across diverse tasks.

Result: Achieves unprecedented 99.9% and 99.3% image-level AUROC on MVTec-AD and VisA for multi-class models, state-of-the-art performance in multi-view/multi-modal inspection, and surpasses previous full-shot models with only 8 normal examples per class.

Conclusion: Dinomaly2's minimalistic design, computational scalability, and universal applicability position it as a unified solution for the full spectrum of real-world anomaly detection applications.

Abstract: Unsupervised anomaly detection (UAD) has evolved from building specialized
single-class models to unified multi-class models, yet existing multi-class
models significantly underperform the most advanced one-for-one counterparts.
Moreover, the field has fragmented into specialized methods tailored to
specific scenarios (multi-class, 3D, few-shot, etc.), creating deployment
barriers and highlighting the need for a unified solution. In this paper, we
present Dinomaly2, the first unified framework for full-spectrum image UAD,
which bridges the performance gap in multi-class models while seamlessly
extending across diverse data modalities and task settings. Guided by the "less
is more" philosophy, we demonstrate that the orchestration of five simple
element achieves superior performance in a standard reconstruction-based
framework. This methodological minimalism enables natural extension across
diverse tasks without modification, establishing that simplicity is the
foundation of true universality. Extensive experiments on 12 UAD benchmarks
demonstrate Dinomaly2's full-spectrum superiority across multiple modalities
(2D, multi-view, RGB-3D, RGB-IR), task settings (single-class, multi-class,
inference-unified multi-class, few-shot) and application domains (industrial,
biological, outdoor). For example, our multi-class model achieves unprecedented
99.9% and 99.3% image-level (I-) AUROC on MVTec-AD and VisA respectively. For
multi-view and multi-modal inspection, Dinomaly2 demonstrates state-of-the-art
performance with minimum adaptations. Moreover, using only 8 normal examples
per class, our method surpasses previous full-shot models, achieving 98.7% and
97.4% I-AUROC on MVTec-AD and VisA. The combination of minimalistic design,
computational scalability, and universal applicability positions Dinomaly2 as a
unified solution for the full spectrum of real-world anomaly detection
applications.

</details>


### [265] [CaMiT: A Time-Aware Car Model Dataset for Classification and Generation](https://arxiv.org/abs/2510.17626)
*Frdric LIN,Biruk Abere Ambaw,Adrian Popescu,Hejer Ammar,Romaric Audigier,Herv Le Borgne*

Main category: cs.CV

TL;DR: CaMiT dataset captures temporal evolution of car models (2007-2023) with 787K labeled and 5.1M unlabeled samples, enabling study of temporal adaptation in fine-grained visual recognition through time-incremental learning approaches.


<details>
  <summary>Details</summary>
Motivation: AI systems need to adapt to evolving visual environments where object appearances change over time, particularly for technological artifacts like car models that undergo continuous evolution.

Method: Created CaMiT dataset with temporal car model data; proposed time-incremental classification setting; evaluated two strategies: time-incremental pretraining (updates backbone) and time-incremental classifier learning (updates only final layer); explored time-aware image generation using temporal metadata.

Result: Static pretraining on in-domain data achieves competitive performance with large models but accuracy declines across years; both time-incremental strategies improve temporal robustness; time-aware image generation yields more realistic outputs.

Conclusion: CaMiT provides a comprehensive benchmark for studying temporal adaptation in fine-grained visual recognition and generation, demonstrating the importance of temporal adaptation strategies for maintaining performance in evolving visual environments.

Abstract: AI systems must adapt to evolving visual environments, especially in domains
where object appearances change over time. We introduce Car Models in Time
(CaMiT), a fine-grained dataset capturing the temporal evolution of car models,
a representative class of technological artifacts. CaMiT includes 787K labeled
samples of 190 car models (2007-2023) and 5.1M unlabeled samples (2005-2023),
supporting both supervised and self-supervised learning. Static pretraining on
in-domain data achieves competitive performance with large-scale generalist
models while being more resource-efficient, yet accuracy declines when models
are tested across years. To address this, we propose a time-incremental
classification setting, a realistic continual learning scenario with emerging,
evolving, and disappearing classes. We evaluate two strategies:
time-incremental pretraining, which updates the backbone, and time-incremental
classifier learning, which updates only the final layer, both improving
temporal robustness. Finally, we explore time-aware image generation that
leverages temporal metadata during training, yielding more realistic outputs.
CaMiT offers a rich benchmark for studying temporal adaptation in fine-grained
visual recognition and generation.

</details>


### [266] [Self-supervised Pre-training for Mapping of Archaeological Stone Wall in Historic Landscapes Using High-Resolution DEM Derivatives](https://arxiv.org/abs/2510.17644)
*Zexian Huang,Mashnoon Islam,Brian Armstrong,Kourosh Khoshelham,Martin Tomko*

Main category: cs.CV

TL;DR: DINO-CV is a self-supervised segmentation framework that automatically maps low-lying dry-stone walls using LiDAR-derived DEMs, overcoming vegetation occlusion and data scarcity through cross-view pre-training.


<details>
  <summary>Details</summary>
Motivation: Dry-stone walls have heritage and environmental value but are hard to map due to vegetation occlusion and limited labeled data. Current manual mapping is costly and inaccessible.

Method: Uses high-resolution Airborne LiDAR DEMs to capture terrain structures beneath vegetation. Introduces self-supervised cross-view pre-training with knowledge distillation across multiple DEM derivatives, supporting various vision backbones.

Result: Achieved 68.6% mIoU on test areas and maintained 63.8% mIoU with only 10% labeled data. Successfully identified dense colonial dry-stone walls in Budj Bim UNESCO site.

Conclusion: Self-supervised learning on high-resolution DEM derivatives enables effective automated mapping of dry-stone walls in vegetated environments with scarce annotations.

Abstract: Dry-stone walls hold significant heritage and environmental value. Mapping
these structures is essential for ecosystem preservation and wildfire
management in Australia. Yet, many walls remain unidentified due to their
inaccessibility and the high cost of manual mapping. Deep learning-based
segmentation offers a scalable solution, but two major challenges persist: (1)
visual occlusion of low-lying walls by dense vegetation, and (2) limited
labeled data for supervised training. We propose DINO-CV, a segmentation
framework for automatic mapping of low-lying dry-stone walls using
high-resolution Airborne LiDAR-derived digital elevation models (DEMs). DEMs
overcome visual occlusion by capturing terrain structures hidden beneath
vegetation, enabling analysis of structural rather than spectral cues. DINO-CV
introduces a self-supervised cross-view pre-training strategy based on
knowledge distillation to mitigate data scarcity. It learns invariant visual
and geometric representations across multiple DEM derivatives, supporting
various vision backbones including ResNet, Wide ResNet, and Vision
Transformers. Applied to the UNESCO World Heritage cultural landscape of Budj
Bim, Victoria, the method identifies one of Australia's densest collections of
colonial dry-stone walls beyond Indigenous heritage contexts. DINO-CV achieves
a mean Intersection over Union (mIoU) of 68.6% on test areas and maintains
63.8% mIoU when fine-tuned with only 10% labeled data. These results
demonstrate the potential of self-supervised learning on high-resolution DEM
derivatives for automated dry-stone wall mapping in vegetated and heritage-rich
environments with scarce annotations.

</details>


### [267] [Frugal Federated Learning for Violence Detection: A Comparison of LoRA-Tuned VLMs and Personalized CNNs](https://arxiv.org/abs/2510.17651)
*Sbastien Thuau,Siba Haidar,Ayush Bajracharya,Rachid Chelouah*

Main category: cs.CV

TL;DR: This paper compares two frugal federated learning approaches for violence detection: zero-shot/federated fine-tuning of vision-language models (VLMs) and personalized training of compact 3D CNNs, with focus on energy efficiency and environmental metrics.


<details>
  <summary>Details</summary>
Motivation: To develop resource-aware AI for video surveillance by comparing energy-efficient federated learning approaches for violence detection, addressing sustainability concerns in AI deployment.

Method: Compared two strategies: (1) zero-shot and federated fine-tuning of LLaVA-7B vision-language models using LoRA, and (2) personalized training of a 65.8M parameter 3D CNN. Evaluated accuracy, calibration, energy usage, and CO2 emissions under realistic non-IID settings.

Result: Both approaches exceeded 90% accuracy. CNN3D slightly outperformed LoRA-tuned VLMs in ROC AUC and log loss while using less energy. VLMs remained better for contextual reasoning and multimodal inference.

Conclusion: Proposes a hybrid model: lightweight CNNs for routine classification with selective VLM activation for complex scenarios. Provides a reproducible baseline for responsible, resource-aware AI in video surveillance.

Abstract: We examine frugal federated learning approaches to violence detection by
comparing two complementary strategies: (i) zero-shot and federated fine-tuning
of vision-language models (VLMs), and (ii) personalized training of a compact
3D convolutional neural network (CNN3D). Using LLaVA-7B and a 65.8M parameter
CNN3D as representative cases, we evaluate accuracy, calibration, and energy
usage under realistic non-IID settings. Both approaches exceed 90% accuracy.
CNN3D slightly outperforms Low-Rank Adaptation(LoRA)-tuned VLMs in ROC AUC and
log loss, while using less energy. VLMs remain favorable for contextual
reasoning and multimodal inference. We quantify energy and CO$_2$ emissions
across training and inference, and analyze sustainability trade-offs for
deployment. To our knowledge, this is the first comparative study of LoRA-tuned
vision-language models and personalized CNNs for federated violence detection,
with an emphasis on energy efficiency and environmental metrics. These findings
support a hybrid model: lightweight CNNs for routine classification, with
selective VLM activation for complex or descriptive scenarios. The resulting
framework offers a reproducible baseline for responsible, resource-aware AI in
video surveillance, with extensions toward real-time, multimodal, and
lifecycle-aware systems.

</details>


### [268] [4DSegStreamer: Streaming 4D Panoptic Segmentation via Dual Threads](https://arxiv.org/abs/2510.17664)
*Ling Liu,Jun Tian,Li Yi*

Main category: cs.CV

TL;DR: 4DSegStreamer is a novel framework for real-time 4D panoptic segmentation using a Dual-Thread System that enables efficient streaming processing in dynamic environments.


<details>
  <summary>Details</summary>
Motivation: Real-time, fine-grained perception is critical for dynamic environments like crowd evacuation and autonomous driving, where existing methods lack real-time capability and robustness under high FPS conditions.

Method: Uses a Dual-Thread System with predictive thread (forecasts future dynamics using historical motion/geometry) and inference thread (ensures timely predictions by aligning with latest memory and compensating for ego-motion/dynamic movements).

Result: Demonstrated effectiveness on HOI4D, SemanticKITTI, and nuScenes datasets, showing superior robustness especially under high FPS conditions and accurate dynamic object prediction in complex scenes.

Conclusion: 4DSegStreamer provides a general framework that can integrate with existing 3D/4D segmentation methods to enable real-time capability while maintaining robust performance in dynamic streaming scenarios.

Abstract: 4D panoptic segmentation in a streaming setting is critical for highly
dynamic environments, such as evacuating dense crowds and autonomous driving in
complex scenarios, where real-time, fine-grained perception within a
constrained time budget is essential. In this paper, we introduce
4DSegStreamer, a novel framework that employs a Dual-Thread System to
efficiently process streaming frames. The framework is general and can be
seamlessly integrated into existing 3D and 4D segmentation methods to enable
real-time capability. It also demonstrates superior robustness compared to
existing streaming perception approaches, particularly under high FPS
conditions. The system consists of a predictive thread and an inference thread.
The predictive thread leverages historical motion and geometric information to
extract features and forecast future dynamics. The inference thread ensures
timely prediction for incoming frames by aligning with the latest memory and
compensating for ego-motion and dynamic object movements. We evaluate
4DSegStreamer on the indoor HOI4D dataset and the outdoor SemanticKITTI and
nuScenes datasets. Comprehensive experiments demonstrate the effectiveness of
our approach, particularly in accurately predicting dynamic objects in complex
scenes.

</details>


### [269] [PICABench: How Far Are We from Physically Realistic Image Editing?](https://arxiv.org/abs/2510.17681)
*Yuandong Pu,Le Zhuo,Songhao Han,Jinbo Xing,Kaiwen Zhu,Shuo Cao,Bin Fu,Si Liu,Hongsheng Li,Yu Qiao,Wenlong Zhang,Xi Chen,Yihao Liu*

Main category: cs.CV

TL;DR: PICABench is introduced to evaluate physical realism in image editing across eight sub-dimensions, revealing that current models struggle with physical consistency despite good instruction completion.


<details>
  <summary>Details</summary>
Motivation: Existing image editing models focus on instruction completion but overlook physical effects like shadows, reflections, and object interactions, which are crucial for realism.

Method: Proposes PICABench for systematic evaluation across optics, mechanics, and state transitions, and PICAEval using VLM-as-a-judge with human annotations. Also constructs PICA-100K dataset from videos.

Result: Evaluation of mainstream models shows physical realism remains challenging with significant room for improvement, as models often fail to maintain physical consistency.

Conclusion: The benchmark and proposed solutions provide a foundation for advancing from basic content editing toward physically consistent realism in image manipulation.

Abstract: Image editing has achieved remarkable progress recently. Modern editing
models could already follow complex instructions to manipulate the original
content. However, beyond completing the editing instructions, the accompanying
physical effects are the key to the generation realism. For example, removing
an object should also remove its shadow, reflections, and interactions with
nearby objects. Unfortunately, existing models and benchmarks mainly focus on
instruction completion but overlook these physical effects. So, at this moment,
how far are we from physically realistic image editing? To answer this, we
introduce PICABench, which systematically evaluates physical realism across
eight sub-dimension (spanning optics, mechanics, and state transitions) for
most of the common editing operations (add, remove, attribute change, etc). We
further propose the PICAEval, a reliable evaluation protocol that uses
VLM-as-a-judge with per-case, region-level human annotations and questions.
Beyond benchmarking, we also explore effective solutions by learning physics
from videos and construct a training dataset PICA-100K. After evaluating most
of the mainstream models, we observe that physical realism remains a
challenging problem with large rooms to explore. We hope that our benchmark and
proposed solutions can serve as a foundation for future work moving from naive
content editing toward physically consistent realism.

</details>


### [270] [Intelligent Communication Mixture-of-Experts Boosted-Medical Image Segmentation Foundation Model](https://arxiv.org/abs/2510.17684)
*Xinwei Zhang,Hu Chen,Zhe Yuan,Sukun Tian,Peng Feng*

Main category: cs.CV

TL;DR: IC-MoE is a medical image segmentation foundation model that uses mixture-of-experts with adaptive voting and semantic-guided contrastive learning to enhance high-level feature representation while preserving pretrained weight integrity.


<details>
  <summary>Details</summary>
Motivation: Existing fine-tuning methods for medical image segmentation have limitations in representing high-level features and disrupt the structural integrity of pretrained weights during adaptation.

Method: Proposes IC-MoE with three expert types (basic, semantic, adaptive) using pixel probability adaptive voting for expert selection/fusion, and semantic-guided contrastive learning to enhance feature representation while preserving weight integrity.

Result: Extensive experiments on three public medical image segmentation datasets show IC-MoE outperforms other state-of-the-art models and demonstrates superior generalizability across diverse medical scenarios.

Conclusion: IC-MoE effectively supplements foundational medical image segmentation models with enhanced high-level features and preserved pretrained structural integrity, showing strong performance and generalization capabilities.

Abstract: Foundation models for medical image segmentation have achieved remarkable
performance. Adaptive fine-tuning of natural image segmentation foundation
models is crucial for medical image segmentation tasks. However, some
limitations exist in existing fine-tuning methods: 1) insufficient
representation of high-level features and 2) the fine-tuning process disrupts
the structural integrity of pretrained weights. Inspired by these critical
problems, we propose an intelligent communication mixture-of-experts
boosted-medical image segmentation foundation model, named IC-MoE, with twofold
ideas: 1) We construct basic experts, semantic experts, and adaptive experts.
Moreover, we implement a pixel probability adaptive voting strategy, which
enables expert selection and fusion through label consistency and load
balancing. This approach preliminarily enhances the representation capability
of high-level features while preserving the structural integrity of pretrained
weights. 2) We propose a semantic-guided contrastive learning method to address
the issue of weak supervision in contrastive learning. This method further
enhances the representation capability of high-level features while preserving
the structural integrity of pretrained weights. Extensive experiments across
three public medical image segmentation datasets demonstrate that the IC-MoE
outperforms other SOTA models. Consequently, the proposed IC-MoE effectively
supplements foundational medical image segmentation models with high-level
features and pretrained structural integrity. We also validate the superior
generalizability of the IC-MoE across diverse medical image segmentation
scenarios.

</details>


### [271] [Multilingual Text-to-Image Person Retrieval via Bidirectional Relation Reasoning and Aligning](https://arxiv.org/abs/2510.17685)
*Min Cao,Xinyu Zhou,Ding Jiang,Bo Du,Mang Ye,Min Zhang*

Main category: cs.CV

TL;DR: Bi-IRRA is a bidirectional implicit relation reasoning framework for multilingual text-to-image person retrieval that achieves state-of-the-art results by combining bidirectional masked prediction with multi-dimensional global alignment.


<details>
  <summary>Details</summary>
Motivation: Address limitations in existing text-to-image person retrieval methods: global methods overlook fine-grained differences, local methods require prior information for part alignment, and current approaches are English-centric, restricting multilingual applications.

Method: Proposes Bi-IRRA framework with: 1) bidirectional implicit relation reasoning module for masked image and text prediction to model local relations across languages and modalities, 2) multi-dimensional global alignment module to bridge modality heterogeneity. Also creates a multilingual TIPR benchmark using LLMs with domain-specific refinement.

Result: Achieves new state-of-the-art results on all multilingual text-to-image person retrieval datasets.

Conclusion: The proposed Bi-IRRA framework effectively addresses multilingual text-to-image person retrieval challenges through bidirectional implicit relation reasoning and multi-dimensional global alignment, demonstrating superior performance across multilingual datasets.

Abstract: Text-to-image person retrieval (TIPR) aims to identify the target person
using textual descriptions, facing challenge in modality heterogeneity. Prior
works have attempted to address it by developing cross-modal global or local
alignment strategies. However, global methods typically overlook fine-grained
cross-modal differences, whereas local methods require prior information to
explore explicit part alignments. Additionally, current methods are
English-centric, restricting their application in multilingual contexts. To
alleviate these issues, we pioneer a multilingual TIPR task by developing a
multilingual TIPR benchmark, for which we leverage large language models for
initial translations and refine them by integrating domain-specific knowledge.
Correspondingly, we propose Bi-IRRA: a Bidirectional Implicit Relation
Reasoning and Aligning framework to learn alignment across languages and
modalities. Within Bi-IRRA, a bidirectional implicit relation reasoning module
enables bidirectional prediction of masked image and text, implicitly enhancing
the modeling of local relations across languages and modalities, a
multi-dimensional global alignment module is integrated to bridge the modality
heterogeneity. The proposed method achieves new state-of-the-art results on all
multilingual TIPR datasets. Data and code are presented in
https://github.com/Flame-Chasers/Bi-IRRA.

</details>


### [272] [Towards 3D Objectness Learning in an Open World](https://arxiv.org/abs/2510.17686)
*Taichi Liu,Zhenyu Wang,Ruofeng Liu,Guang Wang,Desheng Zhang*

Main category: cs.CV

TL;DR: OP3Det is a class-agnostic open-world 3D detector that discovers any objects in 3D scenes without text prompts, leveraging 2D foundation models and cross-modal fusion to achieve superior generalization.


<details>
  <summary>Details</summary>
Motivation: Traditional 3D detectors struggle with open-world scenarios and novel objects, while existing open-vocabulary models face vocabulary expansion and semantic overlap issues. There's insufficient research on generalized 3D objectness.

Method: Proposes OP3Det that uses 2D semantic priors and 3D geometric priors for class-agnostic proposals, then integrates point cloud and RGB information through cross-modal mixture of experts to dynamically route uni-modal and multi-modal features.

Result: Significantly outperforms existing open-world 3D detectors by up to 16.0% in AR and achieves 13.5% improvement over closed-world 3D detectors in extensive experiments.

Conclusion: OP3Det demonstrates extraordinary performance in generalized 3D object discovery, effectively addressing the limitations of traditional detectors in open-world scenarios through innovative cross-modal fusion and foundation model integration.

Abstract: Recent advancements in 3D object detection and novel category detection have
made significant progress, yet research on learning generalized 3D objectness
remains insufficient. In this paper, we delve into learning open-world 3D
objectness, which focuses on detecting all objects in a 3D scene, including
novel objects unseen during training. Traditional closed-set 3D detectors
struggle to generalize to open-world scenarios, while directly incorporating 3D
open-vocabulary models for open-world ability struggles with vocabulary
expansion and semantic overlap. To achieve generalized 3D object discovery, We
propose OP3Det, a class-agnostic Open-World Prompt-free 3D Detector to detect
any objects within 3D scenes without relying on hand-crafted text prompts. We
introduce the strong generalization and zero-shot capabilities of 2D foundation
models, utilizing both 2D semantic priors and 3D geometric priors for
class-agnostic proposals to broaden 3D object discovery. Then, by integrating
complementary information from point cloud and RGB image in the cross-modal
mixture of experts, OP3Det dynamically routes uni-modal and multi-modal
features to learn generalized 3D objectness. Extensive experiments demonstrate
the extraordinary performance of OP3Det, which significantly surpasses existing
open-world 3D detectors by up to 16.0% in AR and achieves a 13.5% improvement
compared to closed-world 3D detectors.

</details>


### [273] [GAS: Improving Discretization of Diffusion ODEs via Generalized Adversarial Solver](https://arxiv.org/abs/2510.17699)
*Aleksandr Oganov,Ilya Bykov,Eva Neudachina,Mishan Aliev,Alexander Tolmachev,Alexander Sidorov,Aleksandr Zuev,Andrey Okhotin,Denis Rakitin,Aibek Alanov*

Main category: cs.CV

TL;DR: The paper introduces Generalized Adversarial Solver (GAS), a simple ODE sampler parameterization that combines distillation loss with adversarial training to improve diffusion model sampling efficiency while preserving fine-grained details.


<details>
  <summary>Details</summary>
Motivation: Diffusion models achieve state-of-the-art generation quality but suffer from computationally expensive sampling. Existing gradient-based optimization methods for distillation require intricate training techniques and don't explicitly preserve fine-grained details.

Method: Proposes Generalized Solver - a simple ODE sampler parameterization without additional training tricks, combined with adversarial training to mitigate artifacts and enhance detail fidelity.

Result: The Generalized Adversarial Solver demonstrates superior performance compared to existing solver training methods under similar resource constraints.

Conclusion: The proposed method provides an effective solution for efficient diffusion model sampling while maintaining high-quality generation with preserved details.

Abstract: While diffusion models achieve state-of-the-art generation quality, they
still suffer from computationally expensive sampling. Recent works address this
issue with gradient-based optimization methods that distill a few-step ODE
diffusion solver from the full sampling process, reducing the number of
function evaluations from dozens to just a few. However, these approaches often
rely on intricate training techniques and do not explicitly focus on preserving
fine-grained details. In this paper, we introduce the Generalized Solver: a
simple parameterization of the ODE sampler that does not require additional
training tricks and improves quality over existing approaches. We further
combine the original distillation loss with adversarial training, which
mitigates artifacts and enhances detail fidelity. We call the resulting method
the Generalized Adversarial Solver and demonstrate its superior performance
compared to existing solver training methods under similar resource
constraints. Code is available at https://github.com/3145tttt/GAS.

</details>


### [274] [Elastic ViTs from Pretrained Models without Retraining](https://arxiv.org/abs/2510.17700)
*Walter Simoncini,Michael Dorkenwald,Tijmen Blankevoort,Cees G. M. Snoek,Yuki M. Asano*

Main category: cs.CV

TL;DR: SnapViT is a post-pretraining structured pruning method for Vision Transformers that enables elastic inference across compute budgets without retraining or labeled data.


<details>
  <summary>Details</summary>
Motivation: Vision foundation models come in limited pre-determined sizes, forcing sub-optimal deployment choices under real-world computational constraints.

Method: Combines gradient information with cross-network structure correlations approximated via evolutionary algorithm, uses self-supervised importance scoring, and is retraining-free.

Result: Superior performance over state-of-the-art methods across various sparsities, generates elastic models in under 5 minutes on single A100 GPU.

Conclusion: SnapViT provides an efficient pruning strategy for pretrained Vision Transformers with novel evolutionary approximation and self-supervised scoring, enabling flexible deployment across computational budgets.

Abstract: Vision foundation models achieve remarkable performance but are only
available in a limited set of pre-determined sizes, forcing sub-optimal
deployment choices under real-world constraints. We introduce SnapViT:
Single-shot network approximation for pruned Vision Transformers, a new
post-pretraining structured pruning method that enables elastic inference
across a continuum of compute budgets. Our approach efficiently combines
gradient information with cross-network structure correlations, approximated
via an evolutionary algorithm, does not require labeled data, generalizes to
models without a classification head, and is retraining-free. Experiments on
DINO, SigLIPv2, DeIT, and AugReg models demonstrate superior performance over
state-of-the-art methods across various sparsities, requiring less than five
minutes on a single A100 GPU to generate elastic models that can be adjusted to
any computational budget. Our key contributions include an efficient pruning
strategy for pretrained Vision Transformers, a novel evolutionary approximation
of Hessian off-diagonal structures, and a self-supervised importance scoring
mechanism that maintains strong performance without requiring retraining or
labels. Code and pruned models are available at: https://elastic.ashita.nl/

</details>


### [275] [Improving Cross-Patient Generalization in Parkinson's Disease Detection through Chunk-Based Analysis of Hand-Drawn Patterns](https://arxiv.org/abs/2510.17703)
*Mhd Adnan Albani,Riad Sonbol*

Main category: cs.CV

TL;DR: A two-stage approach for Parkinson's disease detection from hand-drawn images using chunking and ensemble methods to address dataset limitations and improve robustness on unseen patient data.


<details>
  <summary>Details</summary>
Motivation: Existing Parkinson's disease detection methods from hand-drawn images face two major limitations: insufficient datasets and poor robustness when dealing with unseen patient data.

Method: Two-stage approach: first classifies drawing types (circle, meander, spiral), then extracts features using a chunking strategy (dividing images into 2x2 chunks) and uses ensemble methods to merge decisions from each chunk for final classification.

Result: Achieved 97.08% accuracy for seen patients and 94.91% for unseen patients on NewHandPD dataset, maintaining only a 2.17 percentage point gap compared to 4.76-point drop in prior work.

Conclusion: The proposed approach outperforms state-of-the-art methods, particularly on unseen patients, demonstrating improved robustness and generalization capability.

Abstract: Parkinson's disease (PD) is a neurodegenerative disease affecting about 1% of
people over the age of 60, causing motor impairments that impede hand
coordination activities such as writing and drawing. Many approaches have tried
to support early detection of Parkinson's disease based on hand-drawn images;
however, we identified two major limitations in the related works: (1) the lack
of sufficient datasets, (2) the robustness when dealing with unseen patient
data. In this paper, we propose a new approach to detect Parkinson's disease
that consists of two stages: The first stage classifies based on their drawing
type(circle, meander, spiral), and the second stage extracts the required
features from the images and detects Parkinson's disease. We overcame the
previous two limitations by applying a chunking strategy where we divide each
image into 2x2 chunks. Each chunk is processed separately when extracting
features and recognizing Parkinson's disease indicators. To make the final
classification, an ensemble method is used to merge the decisions made from
each chunk. Our evaluation shows that our proposed approach outperforms the top
performing state-of-the-art approaches, in particular on unseen patients. On
the NewHandPD dataset our approach, it achieved 97.08% accuracy for seen
patients and 94.91% for unseen patients, our proposed approach maintained a gap
of only 2.17 percentage points, compared to the 4.76-point drop observed in
prior work.

</details>


### [276] [Automatic Classification of Circulating Blood Cell Clusters based on Multi-channel Flow Cytometry Imaging](https://arxiv.org/abs/2510.17716)
*Suqiang Ma,Subhadeep Sengupta,Yao Lee,Beikang Gu,Xianyan Chen,Xianqiao Wang,Yang Liu,Mengjia Xu,Galit H. Frydman,He Li*

Main category: cs.CV

TL;DR: A computational framework using YOLOv11 for automated analysis of circulating blood cell clusters (CCCs) from flow cytometry images, achieving over 95% accuracy in cluster classification and cell type identification.


<details>
  <summary>Details</summary>
Motivation: Current computational approaches focus on single-cell analysis but lack tools for analyzing cell clusters, which have irregular shapes, sizes, and heterogeneous cell types requiring multi-channel staining.

Method: Two-step analysis: 1) Fine-tuned YOLOv11 model classifies images into cluster vs non-cluster groups, outperforming CNNs and ViT; 2) Cell type identification by overlaying cluster contours with multi-channel fluorescence stain regions, handling cell debris and staining artifacts.

Result: Achieved over 95% accuracy in both cluster classification and phenotype identification, demonstrating effective automated analysis of CCC images.

Conclusion: The automated framework successfully analyzes CCC images using bright-field and fluorescence data, with potential for broader applications in analyzing immune and tumor cell clusters across various diseases.

Abstract: Circulating blood cell clusters (CCCs) containing red blood cells (RBCs),
white blood cells(WBCs), and platelets are significant biomarkers linked to
conditions like thrombosis, infection, and inflammation. Flow cytometry, paired
with fluorescence staining, is commonly used to analyze these cell clusters,
revealing cell morphology and protein profiles. While computational approaches
based on machine learning have advanced the automatic analysis of single-cell
flow cytometry images, there is a lack of effort to build tools to
automatically analyze images containing CCCs. Unlike single cells, cell
clusters often exhibit irregular shapes and sizes. In addition, these cell
clusters often consist of heterogeneous cell types, which require multi-channel
staining to identify the specific cell types within the clusters. This study
introduces a new computational framework for analyzing CCC images and
identifying cell types within clusters. Our framework uses a two-step analysis
strategy. First, it categorizes images into cell cluster and non-cluster groups
by fine-tuning the You Only Look Once(YOLOv11) model, which outperforms
traditional convolutional neural networks (CNNs), Vision Transformers (ViT).
Then, it identifies cell types by overlaying cluster contours with regions from
multi-channel fluorescence stains, enhancing accuracy despite cell debris and
staining artifacts. This approach achieved over 95% accuracy in both cluster
classification and phenotype identification. In summary, our automated
framework effectively analyzes CCC images from flow cytometry, leveraging both
bright-field and fluorescence data. Initially tested on blood cells, it holds
potential for broader applications, such as analyzing immune and tumor cell
clusters, supporting cellular research across various diseases.

</details>


### [277] [Raindrop GS: A Benchmark for 3D Gaussian Splatting under Raindrop Conditions](https://arxiv.org/abs/2510.17719)
*Zhiqiang Teng,Beibei Lin,Tingting Chen,Zifeng Yuan,Xuanyi Li,Xuanyu Zhang,Shunli Zhang*

Main category: cs.CV

TL;DR: RaindropGS is a benchmark for evaluating 3D Gaussian Splatting under raindrop conditions, addressing issues with camera pose estimation, point cloud initialization, and domain gaps between synthetic and real raindrops.


<details>
  <summary>Details</summary>
Motivation: 3DGS performance degrades significantly under raindrop conditions due to occlusions and distortions, and existing benchmarks use synthetic data with known poses, failing to address real-world challenges like inaccurate pose estimation and domain gaps.

Method: The benchmark includes data preparation with real-world raindrop datasets containing three aligned image sets (raindrop-focused, background-focused, rain-free), data processing, and raindrop-aware 3DGS evaluation covering pose estimation, point cloud initialization, rain removal, and Gaussian training.

Result: Experiments reveal critical insights: camera focus position significantly impacts 3DGS reconstruction, and inaccurate pose/point cloud initialization interferes with reconstruction quality under raindrop conditions.

Conclusion: RaindropGS provides a comprehensive evaluation framework that identifies performance limitations and establishes directions for developing more robust 3DGS methods in raindrop-contaminated environments.

Abstract: 3D Gaussian Splatting (3DGS) under raindrop conditions suffers from severe
occlusions and optical distortions caused by raindrop contamination on the
camera lens, substantially degrading reconstruction quality. Existing
benchmarks typically evaluate 3DGS using synthetic raindrop images with known
camera poses (constrained images), assuming ideal conditions. However, in
real-world scenarios, raindrops often interfere with accurate camera pose
estimation and point cloud initialization. Moreover, a significant domain gap
between synthetic and real raindrops further impairs generalization. To tackle
these issues, we introduce RaindropGS, a comprehensive benchmark designed to
evaluate the full 3DGS pipeline-from unconstrained, raindrop-corrupted images
to clear 3DGS reconstructions. Specifically, the whole benchmark pipeline
consists of three parts: data preparation, data processing, and raindrop-aware
3DGS evaluation, including types of raindrop interference, camera pose
estimation and point cloud initialization, single image rain removal
comparison, and 3D Gaussian training comparison. First, we collect a real-world
raindrop reconstruction dataset, in which each scene contains three aligned
image sets: raindrop-focused, background-focused, and rain-free ground truth,
enabling a comprehensive evaluation of reconstruction quality under different
focus conditions. Through comprehensive experiments and analyses, we reveal
critical insights into the performance limitations of existing 3DGS methods on
unconstrained raindrop images and the varying impact of different pipeline
components: the impact of camera focus position on 3DGS reconstruction
performance, and the interference caused by inaccurate pose and point cloud
initialization on reconstruction. These insights establish clear directions for
developing more robust 3DGS methods under raindrop conditions.

</details>


### [278] [MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating Multimodal LLMs in Multi-Turn Dialogues](https://arxiv.org/abs/2510.17722)
*Yaning Pan,Zekun Wang,Qianqian Xie,Yongqian Wen,Yuanxing Zhang,Guohui Zhang,Haoxuan Hu,Zhiyu Pan,Yibing Huang,Zhidong Gan,Yonghong Lin,An Ping,Tianhao Peng,Jiaheng Liu*

Main category: cs.CV

TL;DR: MT-Video-Bench is a new benchmark for evaluating Multimodal Large Language Models (MLLMs) in multi-turn video dialogues, addressing limitations of current single-turn QA benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing MLLM evaluation benchmarks are limited to single-turn question answering, which doesn't capture the complexity of real-world multi-turn dialogues in video understanding scenarios.

Method: Created MT-Video-Bench with 987 meticulously curated multi-turn dialogues from diverse domains, assessing six core competencies focused on perceptivity and interactivity, aligned with real-world applications like sports analysis and video-based tutoring.

Result: Extensive evaluation of state-of-the-art MLLMs revealed significant performance discrepancies and limitations in handling multi-turn video dialogues.

Conclusion: The benchmark addresses a critical gap in MLLM evaluation and will be publicly available to advance research in multi-turn video understanding.

Abstract: The recent development of Multimodal Large Language Models (MLLMs) has
significantly advanced AI's ability to understand visual modalities. However,
existing evaluation benchmarks remain limited to single-turn question
answering, overlooking the complexity of multi-turn dialogues in real-world
scenarios. To bridge this gap, we introduce MT-Video-Bench, a holistic video
understanding benchmark for evaluating MLLMs in multi-turn dialogues.
Specifically, our MT-Video-Bench mainly assesses six core competencies that
focus on perceptivity and interactivity, encompassing 987 meticulously curated
multi-turn dialogues from diverse domains. These capabilities are rigorously
aligned with real-world applications, such as interactive sports analysis and
multi-turn video-based intelligent tutoring. With MT-Video-Bench, we
extensively evaluate various state-of-the-art open-source and closed-source
MLLMs, revealing their significant performance discrepancies and limitations in
handling multi-turn video dialogues. The benchmark will be publicly available
to foster future research.

</details>


### [279] [Signature Forgery Detection: Improving Cross-Dataset Generalization](https://arxiv.org/abs/2510.17724)
*Matheus Ramos Parracho*

Main category: cs.CV

TL;DR: This paper investigates feature learning strategies for offline signature verification to improve cross-dataset generalization, comparing raw signature images versus shell preprocessing across three public benchmarks.


<details>
  <summary>Details</summary>
Motivation: Automated signature verification faces challenges in generalizing across datasets due to variations in handwriting styles and acquisition protocols, limiting practical deployment.

Method: Developed two experimental pipelines: one using raw signature images and another using shell preprocessing, tested on three public benchmarks (CEDAR, ICDAR, GPDS Synthetic) to evaluate cross-dataset generalization.

Result: The raw-image model achieved higher performance across benchmarks, while the shell-based model showed promising potential for future refinement in cross-domain signature verification.

Conclusion: No definitive superiority between raw-image and shell preprocessing approaches was established, but both show potential for developing robust cross-domain signature verification systems.

Abstract: Automated signature verification is a critical biometric technique used in
banking, identity authentication, and legal documentation. Despite the notable
progress achieved by deep learning methods, most approaches in offline
signature verification still struggle to generalize across datasets, as
variations in handwriting styles and acquisition protocols often degrade
performance. This study investigates feature learning strategies for signature
forgery detection, focusing on improving cross-dataset generalization -- that
is, model robustness when trained on one dataset and tested on another. Using
three public benchmarks -- CEDAR, ICDAR, and GPDS Synthetic -- two experimental
pipelines were developed: one based on raw signature images and another
employing a preprocessing method referred to as shell preprocessing. Several
behavioral patterns were identified and analyzed; however, no definitive
superiority between the two approaches was established. The results show that
the raw-image model achieved higher performance across benchmarks, while the
shell-based model demonstrated promising potential for future refinement toward
robust, cross-domain signature verification.

</details>


### [280] [Can Image-To-Video Models Simulate Pedestrian Dynamics?](https://arxiv.org/abs/2510.17731)
*Aaron Appelle,Jerome P. Lynch*

Main category: cs.CV

TL;DR: The paper investigates whether diffusion transformer-based image-to-video models can generate realistic pedestrian movement patterns when conditioned on keyframes from trajectory benchmarks.


<details>
  <summary>Details</summary>
Motivation: To explore the world-modeling capabilities of recent high-performing I2V models and test if they can produce realistic pedestrian dynamics in crowded scenes.

Method: Condition I2V models on keyframes extracted from pedestrian trajectory benchmarks and evaluate their trajectory prediction performance.

Result: The paper evaluates performance using quantitative measures of pedestrian dynamics, but specific results are not detailed in the abstract.

Conclusion: The framework demonstrates the potential of I2V models for pedestrian trajectory prediction, though full conclusions would depend on the evaluation results.

Abstract: Recent high-performing image-to-video (I2V) models based on variants of the
diffusion transformer (DiT) have displayed remarkable inherent world-modeling
capabilities by virtue of training on large scale video datasets. We
investigate whether these models can generate realistic pedestrian movement
patterns in crowded public scenes. Our framework conditions I2V models on
keyframes extracted from pedestrian trajectory benchmarks, then evaluates their
trajectory prediction performance using quantitative measures of pedestrian
dynamics.

</details>


### [281] [Joint Multi-Condition Representation Modelling via Matrix Factorisation for Visual Place Recognition](https://arxiv.org/abs/2510.17739)
*Timur Ismagilov,Shakaiba Majeed,Michael Milford,Tan Viet Tuyen Nguyen,Sarvapali D. Ramchurn,Shoaib Ehsan*

Main category: cs.CV

TL;DR: Training-free multi-reference visual place recognition using matrix decomposition for basis representations and residual matching, achieving significant performance gains without extensive training.


<details>
  <summary>Details</summary>
Motivation: To improve visual place recognition performance using multiple reference sets under varying conditions while avoiding the computational costs of training complex models or using heuristic fusion methods.

Method: Proposes a descriptor-agnostic approach that jointly models places using multiple reference descriptors via matrix decomposition into basis representations, enabling projection-based residual matching.

Result: Improves Recall@1 by up to ~18% over single-reference methods and outperforms multi-reference baselines across appearance and viewpoint changes, with ~5% gains on unstructured data.

Conclusion: The method demonstrates strong generalization capabilities while remaining lightweight and training-free, making it effective for multi-reference visual place recognition under varying conditions.

Abstract: We address multi-reference visual place recognition (VPR), where reference
sets captured under varying conditions are used to improve localisation
performance. While deep learning with large-scale training improves robustness,
increasing data diversity and model complexity incur extensive computational
cost during training and deployment. Descriptor-level fusion via voting or
aggregation avoids training, but often targets multi-sensor setups or relies on
heuristics with limited gains under appearance and viewpoint change. We propose
a training-free, descriptor-agnostic approach that jointly models places using
multiple reference descriptors via matrix decomposition into basis
representations, enabling projection-based residual matching. We also introduce
SotonMV, a structured benchmark for multi-viewpoint VPR. On multi-appearance
data, our method improves Recall@1 by up to ~18% over single-reference and
outperforms multi-reference baselines across appearance and viewpoint changes,
with gains of ~5% on unstructured data, demonstrating strong generalisation
while remaining lightweight.

</details>


### [282] [Towards Explainable Skin Cancer Classification: A Dual-Network Attention Model with Lesion Segmentation and Clinical Metadata Fusion](https://arxiv.org/abs/2510.17773)
*Md. Enamul Atiq,Shaikh Anowarul Fattah*

Main category: cs.CV

TL;DR: A dual-encoder attention framework combining segmented lesions and clinical metadata improves skin cancer classification accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Automated skin cancer diagnosis faces challenges with high intra-class variability, subtle differences between classes, and black-box deep learning models that limit clinical trust.

Method: Uses Deep-UNet with Dual Attention Gates and ASPP for lesion segmentation, then dual DenseNet201 encoders with multi-head cross-attention for classification, plus transformer-based metadata integration.

Result: Achieves state-of-the-art segmentation performance and significantly improves classification accuracy and AUC on HAM10000, ISIC 2018 and 2019 datasets. Grad-CAM visualizations confirm model focuses on lesion areas.

Conclusion: Integrating precise lesion segmentation and clinical data with attention-based fusion creates a more accurate and interpretable skin cancer classification model.

Abstract: Skin cancer is a life-threatening disease where early detection significantly
improves patient outcomes. Automated diagnosis from dermoscopic images is
challenging due to high intra-class variability and subtle inter-class
differences. Many deep learning models operate as "black boxes," limiting
clinical trust. In this work, we propose a dual-encoder attention-based
framework that leverages both segmented lesions and clinical metadata to
enhance skin lesion classification in terms of both accuracy and
interpretability. A novel Deep-UNet architecture with Dual Attention Gates
(DAG) and Atrous Spatial Pyramid Pooling (ASPP) is first employed to segment
lesions. The classification stage uses two DenseNet201 encoders-one on the
original image and another on the segmented lesion whose features are fused via
multi-head cross-attention. This dual-input design guides the model to focus on
salient pathological regions. In addition, a transformer-based module
incorporates patient metadata (age, sex, lesion site) into the prediction. We
evaluate our approach on the HAM10000 dataset and the ISIC 2018 and 2019
challenges. The proposed method achieves state-of-the-art segmentation
performance and significantly improves classification accuracy and average AUC
compared to baseline models. To validate our model's reliability, we use
Gradient-weighted Class Activation Mapping (Grad-CAM) to generate heatmaps.
These visualizations confirm that our model's predictions are based on the
lesion area, unlike models that rely on spurious background features. These
results demonstrate that integrating precise lesion segmentation and clinical
data with attention-based fusion leads to a more accurate and interpretable
skin cancer classification model.

</details>


### [283] [SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference](https://arxiv.org/abs/2510.17777)
*Samir Khaki,Junxian Guo,Jiaming Tang,Shang Yang,Yukang Chen,Konstantinos N. Plataniotis,Yao Lu,Song Han,Zhijian Liu*

Main category: cs.CV

TL;DR: SparseVILA is an efficient VLM inference paradigm that decouples visual sparsity across prefilling and decoding stages, achieving up to 4.0 faster prefilling and 2.5 faster decoding while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Vision Language Models face scalability limitations due to growing visual tokens that dominate inference latency, requiring efficient methods to reduce computational overhead without sacrificing capability.

Method: Decouples visual sparsity by pruning redundant visual tokens during prefill and retrieving only query-relevant tokens during decoding, built on an AWQ-optimized inference pipeline.

Result: Achieves up to 4.0 faster prefilling, 2.5 faster decoding, and 2.6 overall speedup on long-context video tasks while improving accuracy on document-understanding and reasoning tasks.

Conclusion: SparseVILA establishes a new direction for efficient multimodal inference with a training-free, architecture-agnostic framework that accelerates large VLMs without sacrificing capability.

Abstract: Vision Language Models (VLMs) have rapidly advanced in integrating visual and
textual reasoning, powering applications across high-resolution image
understanding, long-video analysis, and multi-turn conversation. However, their
scalability remains limited by the growing number of visual tokens that
dominate inference latency. We present SparseVILA, a new paradigm for efficient
VLM inference that decouples visual sparsity across the prefilling and decoding
stages. SparseVILA distributes sparsity across stages by pruning redundant
visual tokens during prefill and retrieving only query-relevant tokens during
decoding. This decoupled design matches leading prefill pruning methods while
preserving multi-turn fidelity by retaining most of the visual cache so that
query-aware tokens can be retrieved at each conversation round. Built on an
AWQ-optimized inference pipeline, SparseVILA achieves up to 4.0 times faster
prefilling, 2.5 times faster decoding, and an overall 2.6 times end-to-end
speedup on long-context video tasks -- while improving accuracy on
document-understanding and reasoning tasks. By decoupling query-agnostic
pruning and query-aware retrieval, SparseVILA establishes a new direction for
efficient multimodal inference, offering a training-free, architecture-agnostic
framework for accelerating large VLMs without sacrificing capability.

</details>


### [284] [ConsistEdit: Highly Consistent and Precise Training-free Visual Editing](https://arxiv.org/abs/2510.17803)
*Zixin Yin,Ling-Hao Chen,Lionel Ni,Xili Dai*

Main category: cs.CV

TL;DR: ConsistEdit is a novel attention control method for MM-DiT models that enables consistent text-guided editing across images and videos through vision-only attention control, mask-guided pre-attention fusion, and differentiated manipulation of query, key, and value tokens.


<details>
  <summary>Details</summary>
Motivation: Current training-free attention control methods struggle to balance strong editing strength with source consistency, especially in multi-round and video editing where errors accumulate. Existing methods enforce global consistency, limiting fine-grained editing of individual attributes.

Method: Proposed ConsistEdit method with three key components: vision-only attention control, mask-guided pre-attention fusion, and differentiated manipulation of query, key, and value tokens. It performs editing across all inference steps and attention layers without handcraft.

Result: Achieves state-of-the-art performance across image and video editing tasks in both structure-consistent and structure-inconsistent scenarios. Enables robust multi-round and multi-region editing with progressive adjustment of structural consistency.

Conclusion: ConsistEdit is the first approach to perform consistent editing across all inference steps and attention layers without handcraft, significantly enhancing reliability and enabling finer control over structural consistency in text-guided editing.

Abstract: Recent advances in training-free attention control methods have enabled
flexible and efficient text-guided editing capabilities for existing generation
models. However, current approaches struggle to simultaneously deliver strong
editing strength while preserving consistency with the source. This limitation
becomes particularly critical in multi-round and video editing, where visual
errors can accumulate over time. Moreover, most existing methods enforce global
consistency, which limits their ability to modify individual attributes such as
texture while preserving others, thereby hindering fine-grained editing.
Recently, the architectural shift from U-Net to MM-DiT has brought significant
improvements in generative performance and introduced a novel mechanism for
integrating text and vision modalities. These advancements pave the way for
overcoming challenges that previous methods failed to resolve. Through an
in-depth analysis of MM-DiT, we identify three key insights into its attention
mechanisms. Building on these, we propose ConsistEdit, a novel attention
control method specifically tailored for MM-DiT. ConsistEdit incorporates
vision-only attention control, mask-guided pre-attention fusion, and
differentiated manipulation of the query, key, and value tokens to produce
consistent, prompt-aligned edits. Extensive experiments demonstrate that
ConsistEdit achieves state-of-the-art performance across a wide range of image
and video editing tasks, including both structure-consistent and
structure-inconsistent scenarios. Unlike prior methods, it is the first
approach to perform editing across all inference steps and attention layers
without handcraft, significantly enhancing reliability and consistency, which
enables robust multi-round and multi-region editing. Furthermore, it supports
progressive adjustment of structural consistency, enabling finer control.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [285] [Attention to Non-Adopters](https://arxiv.org/abs/2510.15951)
*Kaitlyn Zhou,Kristina Gligori,Myra Cheng,Michelle S. Lam,Vyoma Raman,Boluwatife Aminu,Caeley Woo,Michael Brockman,Hannah Cha,Dan Jurafsky*

Main category: cs.CY

TL;DR: Most Americans don't use LLM chat systems, but current development focuses only on adopters' data, risking inequality and missing important non-adopter needs.


<details>
  <summary>Details</summary>
Motivation: 66% of Americans haven't used ChatGPT as of June 2025, and LLM development relies mainly on adopter data, creating bias and overlooking non-adopter perspectives.

Method: Conducted case studies with non-adopters to analyze their needs, identify novel reasoning tasks, and demonstrate human-centered methods for integration.

Result: Found that non-adopter needs diverge from current users, point to novel reasoning tasks, and can be systematically integrated via human-centered approaches.

Conclusion: Incorporating non-adopter perspectives is essential for developing broadly useful LLMs and avoiding entrenched inequalities in AI benefits.

Abstract: Although language model-based chat systems are increasingly used in daily
life, most Americans remain non-adopters of chat-based LLMs -- as of June 2025,
66% had never used ChatGPT. At the same time, LLM development and evaluation
rely mainly on data from adopters (e.g., logs, preference data), focusing on
the needs and tasks for a limited demographic group of adopters in terms of
geographic location, education, and gender. In this position paper, we argue
that incorporating non-adopter perspectives is essential for developing broadly
useful and capable LLMs. We contend that relying on methods that focus
primarily on adopters will risk missing a range of tasks and needs prioritized
by non-adopters, entrenching inequalities in who benefits from LLMs, and
creating oversights in model development and evaluation. To illustrate this
claim, we conduct case studies with non-adopters and show: how non-adopter
needs diverge from those of current users, how non-adopter needs point us
towards novel reasoning tasks, and how to systematically integrate non-adopter
needs via human-centered methods.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [286] [PrivacyPAD: A Reinforcement Learning Framework for Dynamic Privacy-Aware Delegation](https://arxiv.org/abs/2510.16054)
*Zheng Hui,Yijiang River Dong,Sanhanat Sivapiromrat,Ehsan Shareghi,Nigel Collier*

Main category: cs.CR

TL;DR: PrivacyPAD is a reinforcement learning framework that dynamically routes text chunks between local and remote LLMs to balance privacy protection and task performance, achieving state-of-the-art results on the privacy-utility frontier.


<details>
  <summary>Details</summary>
Motivation: Users face a dilemma between using powerful proprietary LLMs (risking data exposure) or local models (guaranteeing privacy but with performance degradation). Current static approaches break linguistic coherence and remove all sensitive information indiscriminately.

Method: Reformulates privacy-conscious delegation as a sequential decision-making problem and introduces PrivacyPAD - an RL framework that trains an agent to dynamically route text chunks, learning to distinguish between replaceable PII (shielded locally) and task-critical PII (sent to remote model).

Result: Achieves state-of-the-art on the privacy-utility frontier. Validated on a new medical dataset with high PII density, demonstrating superior performance over static approaches.

Conclusion: Learned, adaptive policies are necessary for deploying LLMs in sensitive environments, as they optimally balance privacy leakage and task performance through strategic routing of different types of PII.

Abstract: When users submit queries to Large Language Models (LLMs), their prompts can
often contain sensitive data, forcing a difficult choice: Send the query to a
powerful proprietary LLM providers to achieving state-of-the-art performance
and risk data exposure, or relying on smaller, local models guarantees data
privacy but often results in a degradation of task performance. Prior
approaches have relied on static pipelines that use LLM rewriting, which
shatters linguistic coherence and indiscriminately removes privacy-sensitive
information, including task-critical content. We reformulate this challenge
(Privacy-Conscious Delegation) as a sequential decision-making problem and
introduce a novel reinforcement learning (RL) framework called PrivacyPAD to
solve it. Our framework trains an agent to dynamically route text chunks,
learning a policy that optimally balances the trade-off between privacy leakage
and task performance. It implicitly distinguishes between replaceable
Personally Identifiable Information (PII) (which it shields locally) and
task-critical PII (which it strategically sends to the remote model for maximal
utility). To validate our approach in complex scenarios, we also introduce a
new medical dataset with high PII density. Our framework achieves a new
state-of-the-art on the privacy-utility frontier, demonstrating the necessity
of learned, adaptive policies for deploying LLMs in sensitive environments.

</details>


### [287] [The Hidden Cost of Modeling P(X): Vulnerability to Membership Inference Attacks in Generative Text Classifiers](https://arxiv.org/abs/2510.16122)
*Owais Makroo,Siva Rajesh Kasa,Sumegh Roychowdhury,Karan Gupta,Nikhil Pattisapu,Santhosh Kasa,Sumit Negi*

Main category: cs.CR

TL;DR: Generative classifiers are more vulnerable to Membership Inference Attacks than discriminative classifiers due to their explicit modeling of joint likelihood P(X,Y), creating a fundamental utility-privacy trade-off.


<details>
  <summary>Details</summary>
Motivation: To systematically compare vulnerability to Membership Inference Attacks between generative and discriminative classifiers, addressing a gap in existing research.

Method: Comprehensive empirical evaluation across discriminative, generative, and pseudo-generative text classifiers on nine benchmark datasets with varying training data volumes, using diverse MIA strategies.

Result: Fully generative classifiers that explicitly model joint likelihood P(X,Y) are most vulnerable to membership leakage, with canonical inference approaches significantly amplifying privacy risks.

Conclusion: Generative classifiers pose significant privacy risks in sensitive applications, motivating the need for developing privacy-preserving generative classifiers that maintain utility while mitigating MIA vulnerabilities.

Abstract: Membership Inference Attacks (MIAs) pose a critical privacy threat by
enabling adversaries to determine whether a specific sample was included in a
model's training dataset. Despite extensive research on MIAs, systematic
comparisons between generative and discriminative classifiers remain limited.
This work addresses this gap by first providing theoretical motivation for why
generative classifiers exhibit heightened susceptibility to MIAs, then
validating these insights through comprehensive empirical evaluation. Our study
encompasses discriminative, generative, and pseudo-generative text classifiers
across varying training data volumes, evaluated on nine benchmark datasets.
Employing a diverse array of MIA strategies, we consistently demonstrate that
fully generative classifiers which explicitly model the joint likelihood
$P(X,Y)$ are most vulnerable to membership leakage. Furthermore, we observe
that the canonical inference approach commonly used in generative classifiers
significantly amplifies this privacy risk. These findings reveal a fundamental
utility-privacy trade-off inherent in classifier design, underscoring the
critical need for caution when deploying generative classifiers in
privacy-sensitive applications. Our results motivate future research directions
in developing privacy-preserving generative classifiers that can maintain
utility while mitigating membership inference vulnerabilities.

</details>


### [288] [Verifiable Fine-Tuning for LLMs: Zero-Knowledge Training Proofs Bound to Data Provenance and Policy](https://arxiv.org/abs/2510.16830)
*Hasan Akgul,Daniel Borg,Arta Berisha,Amina Rahimova,Andrej Novak,Mila Petrov*

Main category: cs.CR

TL;DR: Verifiable Fine Tuning is a protocol that produces zero-knowledge proofs to verify that a released model was obtained from a public initialization using a declared training program and auditable dataset commitment, addressing trust gaps in regulated deployments.


<details>
  <summary>Details</summary>
Motivation: Current release practices for fine-tuned language models provide weak assurances about training data and update computation, creating trust issues for regulated and decentralized deployments.

Method: Combines five elements: data commitments with manifests, verifiable samplers for batch selection, update circuits for parameter-efficient fine-tuning with AdamW optimizer semantics, recursive proof aggregation, and provenance binding with trusted execution property cards.

Result: Maintains utility within tight budgets while achieving practical proof performance, enforces policy quotas with zero violations, and shows no measurable index leakage in private sampling. Composes well with federated experiments under probabilistic audits and bandwidth constraints.

Conclusion: End-to-end verifiable fine tuning is feasible today for real parameter-efficient pipelines, closing a critical trust gap for regulated and decentralized deployments.

Abstract: Large language models are often adapted through parameter efficient fine
tuning, but current release practices provide weak assurances about what data
were used and how updates were computed. We present Verifiable Fine Tuning, a
protocol and system that produces succinct zero knowledge proofs that a
released model was obtained from a public initialization under a declared
training program and an auditable dataset commitment. The approach combines
five elements. First, commitments that bind data sources, preprocessing,
licenses, and per epoch quota counters to a manifest. Second, a verifiable
sampler that supports public replayable and private index hiding batch
selection. Third, update circuits restricted to parameter efficient fine tuning
that enforce AdamW style optimizer semantics and proof friendly approximations
with explicit error budgets. Fourth, recursive aggregation that folds per step
proofs into per epoch and end to end certificates with millisecond
verification. Fifth, provenance binding and optional trusted execution property
cards that attest code identity and constants. On English and bilingual
instruction mixtures, the method maintains utility within tight budgets while
achieving practical proof performance. Policy quotas are enforced with zero
violations, and private sampling windows show no measurable index leakage.
Federated experiments demonstrate that the system composes with probabilistic
audits and bandwidth constraints. These results indicate that end to end
verifiable fine tuning is feasible today for real parameter efficient
pipelines, closing a critical trust gap for regulated and decentralized
deployments.

</details>


### [289] [Bits Leaked per Query: Information-Theoretic Bounds on Adversarial Attacks against LLMs](https://arxiv.org/abs/2510.17000)
*Masahiro Kaneko,Timothy Baldwin*

Main category: cs.CR

TL;DR: This paper presents an information-theoretic framework to quantify how much information LLMs leak through their responses, showing that even modest increases in transparency can dramatically reduce the query cost for adversarial attacks from quadratic to logarithmic scaling.


<details>
  <summary>Details</summary>
Motivation: Current understanding of information leakage in LLMs is anecdotal, leaving auditors without principled guidance and defenders blind to the transparency-risk trade-off when malicious users attempt to infer unknown target properties through adversarial instructions.

Method: The authors develop an information-theoretic framework that treats mutual information I(Z;T) between observation Z (response tokens, logits, thinking process) and target property T as leaked bits per query, deriving fundamental limits on attack query requirements.

Result: Experiments on seven LLMs across system-prompt leakage, jailbreak, and relearning attacks show: exposing answer tokens alone requires ~1000 queries; adding logits reduces to ~100 queries; revealing full thinking process trims to a few dozen queries.

Conclusion: The framework provides the first principled yardstick for balancing transparency and security in LLM deployment, showing that attack cost collapses from quadratic to logarithmic scaling with even modest increases in information disclosure.

Abstract: Adversarial attacks by malicious users that threaten the safety of large
language models (LLMs) can be viewed as attempts to infer a target property $T$
that is unknown when an instruction is issued, and becomes knowable only after
the model's reply is observed. Examples of target properties $T$ include the
binary flag that triggers an LLM's harmful response or rejection, and the
degree to which information deleted by unlearning can be restored, both
elicited via adversarial instructions. The LLM reveals an \emph{observable
signal} $Z$ that potentially leaks hints for attacking through a response
containing answer tokens, thinking process tokens, or logits. Yet the scale of
information leaked remains anecdotal, leaving auditors without principled
guidance and defenders blind to the transparency--risk trade-off. We fill this
gap with an information-theoretic framework that computes how much information
can be safely disclosed, and enables auditors to gauge how close their methods
come to the fundamental limit. Treating the mutual information $I(Z;T)$ between
the observation $Z$ and the target property $T$ as the leaked bits per query,
we show that achieving error $\varepsilon$ requires at least
$\log(1/\varepsilon)/I(Z;T)$ queries, scaling linearly with the inverse leak
rate and only logarithmically with the desired accuracy. Thus, even a modest
increase in disclosure collapses the attack cost from quadratic to logarithmic
in terms of the desired accuracy. Experiments on seven LLMs across
system-prompt leakage, jailbreak, and relearning attacks corroborate the
theory: exposing answer tokens alone requires about a thousand queries; adding
logits cuts this to about a hundred; and revealing the full thinking process
trims it to a few dozen. Our results provide the first principled yardstick for
balancing transparency and security when deploying LLMs.

</details>


### [290] [VERA-V: Variational Inference Framework for Jailbreaking Vision-Language Models](https://arxiv.org/abs/2510.17759)
*Qilin Liao,Anamika Lochab,Ruqi Zhang*

Main category: cs.CR

TL;DR: VERA-V is a variational inference framework for discovering multimodal jailbreaks in Vision-Language Models by learning joint posterior distributions over text-image prompts, achieving significantly higher attack success rates than existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal red-teaming methods rely on brittle templates, focus on single-attack settings, and expose only a narrow subset of vulnerabilities in Vision-Language Models.

Method: Uses variational inference to learn joint posterior distribution over paired text-image prompts, training a lightweight attacker to approximate the posterior. Integrates three strategies: typography-based text prompts, diffusion-based image synthesis, and structured distractors to fragment VLM attention.

Result: Achieves up to 53.75% higher attack success rate (ASR) over the best baseline on GPT-4o, consistently outperforming state-of-the-art baselines on both open-source and frontier VLMs on HarmBench and HADES benchmarks.

Conclusion: VERA-V provides an effective probabilistic framework for discovering diverse multimodal jailbreaks and offers distributional insights into VLM vulnerabilities.

Abstract: Vision-Language Models (VLMs) extend large language models with visual
reasoning, but their multimodal design also introduces new, underexplored
vulnerabilities. Existing multimodal red-teaming methods largely rely on
brittle templates, focus on single-attack settings, and expose only a narrow
subset of vulnerabilities. To address these limitations, we introduce VERA-V, a
variational inference framework that recasts multimodal jailbreak discovery as
learning a joint posterior distribution over paired text-image prompts. This
probabilistic view enables the generation of stealthy, coupled adversarial
inputs that bypass model guardrails. We train a lightweight attacker to
approximate the posterior, allowing efficient sampling of diverse jailbreaks
and providing distributional insights into vulnerabilities. VERA-V further
integrates three complementary strategies: (i) typography-based text prompts
that embed harmful cues, (ii) diffusion-based image synthesis that introduces
adversarial signals, and (iii) structured distractors to fragment VLM
attention. Experiments on HarmBench and HADES benchmarks show that VERA-V
consistently outperforms state-of-the-art baselines on both open-source and
frontier VLMs, achieving up to 53.75% higher attack success rate (ASR) over the
best baseline on GPT-4o.

</details>


### [291] [ISO/IEC-Compliant Match-on-Card Face Verification with Short Binary Templates](https://arxiv.org/abs/2510.16078)
*Abdelilah Ganmati,Karim Afdel,Lahcen Koutti*

Main category: cs.CR

TL;DR: A match-on-card system for face verification using compact 64/128-bit templates generated off-card via PCA-ITQ and compared on-card using constant-time Hamming distance, achieving fast verification times (14-53ms) with good accuracy (TPR=0.836 at FAR=1%).


<details>
  <summary>Details</summary>
Motivation: To create a practical face verification system that meets ISO/IEC standards for smart cards, focusing on privacy protection by using fixed-length payloads, decision-only responses (no score leakage), and constant-time operations to prevent timing attacks.

Method: PCA-ITQ for generating compact binary templates off-card, on-card comparison via constant-time Hamming distance, ISO/IEC 7816-4 and 14443-4 command APDUs with fixed payloads, minimal EEPROM storage per identity, and optional helper data for unstable bits.

Result: At FAR=1%, both 64-bit and 128-bit templates achieve TPR=0.836, with 128-bit providing lower EER. Verification times range from 14ms at 38.4 kbps to 53ms at 9.6 kbps. The system satisfies ISO/IEC transport constraints with wide timing margins.

Conclusion: The design successfully combines short binary templates, fixed-payload decision-only APDUs, and constant-time matching to meet ISO/IEC standards while aligning with privacy goals. Future work includes multi-dataset evaluation and on-card microbenchmarking.

Abstract: We present a practical match-on-card design for face verification in which
compact 64/128-bit templates are produced off-card by PCA-ITQ and compared
on-card via constant-time Hamming distance. We specify ISO/IEC 7816-4 and
14443-4 command APDUs with fixed-length payloads and decision-only status words
(no score leakage), together with a minimal per-identity EEPROM map. Using real
binary codes from a CelebA working set (55 identities, 412 images), we (i)
derive operating thresholds from ROC/DET, (ii) replay enroll->verify
transactions at those thresholds, and (iii) bound end-to-end time by pure link
latency plus a small constant on-card budget. Even at the slowest contact rate
(9.6 kbps), total verification time is 43.9 ms (64 b) and 52.3 ms (128 b); at
38.4 kbps both are <14 ms. At FAR = 1%, both code lengths reach TPR = 0.836,
while 128 b lowers EER relative to 64 b. An optional +6 B helper (targeted
symbol-level parity over empirically unstable bits) is latency-negligible.
Overall, short binary templates, fixed-payload decision-only APDUs, and
constant-time matching satisfy ISO/IEC transport constraints with wide timing
margin and align with ISO/IEC 24745 privacy goals. Limitations: single-dataset
evaluation and design-level (pre-hardware) timing; we outline AgeDB/CFP-FP and
on-card microbenchmarks as next steps.

</details>


### [292] [Patronus: Safeguarding Text-to-Image Models against White-Box Adversaries](https://arxiv.org/abs/2510.16581)
*Xinfeng Li,Shengyuan Pang,Jialin Wu,Jiangyi Deng,Huanlong Zhong,Yanjiao Chen,Jie Zhang,Wenyuan Xu*

Main category: cs.CR

TL;DR: Patronus is a defensive framework that protects text-to-image models from white-box adversaries by using an internal moderator to neutralize unsafe inputs and a non-fine-tunable learning mechanism to maintain model alignment.


<details>
  <summary>Details</summary>
Motivation: Existing safety measures for text-to-image models fail against white-box adversaries who can access and modify model parameters through fine-tuning, creating a need for more robust protection.

Method: Patronus uses an internal moderator that decodes unsafe input features into zero vectors while preserving benign input performance, and employs a non-fine-tunable learning mechanism to prevent model compromise from malicious fine-tuning.

Result: Extensive experiments show Patronus maintains intact performance on safe content generation while effectively rejecting unsafe content, and demonstrates resilience against various fine-tuning attacks by white-box adversaries.

Conclusion: Patronus provides holistic protection for text-to-image models against white-box adversaries, successfully defending against parameter manipulation while preserving model functionality for legitimate use cases.

Abstract: Text-to-image (T2I) models, though exhibiting remarkable creativity in image
generation, can be exploited to produce unsafe images. Existing safety
measures, e.g., content moderation or model alignment, fail in the presence of
white-box adversaries who know and can adjust model parameters, e.g., by
fine-tuning. This paper presents a novel defensive framework, named Patronus,
which equips T2I models with holistic protection to defend against white-box
adversaries. Specifically, we design an internal moderator that decodes unsafe
input features into zero vectors while ensuring the decoding performance of
benign input features. Furthermore, we strengthen the model alignment with a
carefully designed non-fine-tunable learning mechanism, ensuring the T2I model
will not be compromised by malicious fine-tuning. We conduct extensive
experiments to validate the intactness of the performance on safe content
generation and the effectiveness of rejecting unsafe content generation.
Results also confirm the resilience of Patronus against various fine-tuning
attacks by white-box adversaries.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [293] [Detecting streaks in smart telescopes images with Deep Learning](https://arxiv.org/abs/2510.17540)
*Olivier Parisot,Mahmoud Jaziri*

Main category: astro-ph.IM

TL;DR: Deep Learning approaches are tested and adapted to detect satellite streaks in raw astronomical data captured with smart telescopes.


<details>
  <summary>Details</summary>
Motivation: The growing negative impact of satellite visibility in the night sky is affecting astronomy and astrophotography, introducing streaks in images that require additional post-processing.

Method: Testing and adapting various Deep Learning approaches to detect streaks in raw astronomical data.

Result: Not specified in the abstract.

Conclusion: Not specified in the abstract.

Abstract: The growing negative impact of the visibility of satellites in the night sky
is influencing the practice of astronomy and astrophotograph, both at the
amateur and professional levels. The presence of these satellites has the
effect of introducing streaks into the images captured during astronomical
observation, requiring the application of additional post processing to
mitigate the undesirable impact, whether for data loss or cosmetic reasons. In
this paper, we show how we test and adapt various Deep Learning approaches to
detect streaks in raw astronomical data captured between March 2022 and
February 2023 with smart telescopes.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [294] [Mitigating Harmful Erraticism in LLMs Through Dialectical Behavior Therapy Based De-Escalation Strategies](https://arxiv.org/abs/2510.15889)
*Pooja Rangarajan,Jacob Boyle*

Main category: cs.HC

TL;DR: A DBT-based framework is proposed to regulate AI chatbot responses, aiming to improve reliability, safety, and accuracy while reducing hallucinations and erratic behaviors.


<details>
  <summary>Details</summary>
Motivation: Current AI chatbot development methods using baseline programming and manual adjustments are difficult to maintain and prone to errors like hallucinations and erratic outputs. There's a need for more robust solutions for personalized AI interactions that adapt to user emotional states.

Method: Applying Dialectical Behavior Therapy (DBT) principles from human psychology to regulate chatbot responses, drawing an analogy between AI neural networks and the human brain.

Result: The paper investigates the impact of this DBT-based framework on AI chatbot performance, but specific results are not provided in the abstract.

Conclusion: A psychological framework based on therapeutic modalities like DBT can provide a more robust and sustainable solution than purely technical interventions for improving AI chatbot reliability and safety.

Abstract: The escalating demand for personalized AI chatbot interactions, capable of
dynamically adapting to user emotional states and real-time requests, has
highlighted critical limitations in current development paradigms. Existing
methodologies, which rely on baseline programming, custom personalities, and
manual response adjustments, often prove difficult to maintain and are
susceptible to errors such as hallucinations, erratic outputs, and software
bugs. This paper hypothesizes that a framework rooted in human psychological
principles, specifically therapeutic modalities, can provide a more robust and
sustainable solution than purely technical interventions. Drawing an analogy to
the simulated neural networks of AI mirroring the human brain, we propose the
application of Dialectical Behavior Therapy (DBT) principles to regulate
chatbot responses to diverse user inputs. This research investigates the impact
of a DBT-based framework on AI chatbot performance, aiming to ascertain its
efficacy in yielding more reliable, safe, and accurate responses, while
mitigating the occurrence of hallucinations, erratic behaviors, and other
systemic issues.

</details>


### [295] [Detecting and Preventing Harmful Behaviors in AI Companions: Development and Evaluation of the SHIELD Supervisory System](https://arxiv.org/abs/2510.15891)
*Ziv Ben-Zion,Paul Raffelhschen,Max Zettl,Antonia Lnd,Achim Burrer,Philipp Homan,Tobias R Spiller*

Main category: cs.HC

TL;DR: SHIELD is a LLM-based supervisory system that detects and mitigates early-stage problematic emotional behaviors in AI companions, reducing concerning content by 50-79% while preserving appropriate interactions.


<details>
  <summary>Details</summary>
Motivation: Existing safety systems focus on overt harms but don't address early-stage problematic behaviors that can foster unhealthy emotional dynamics like over-attachment or social isolation reinforcement in AI companions.

Method: Developed SHIELD with a specific system prompt targeting five dimensions of concern: emotional over-attachment, consent violations, ethical roleplay violations, manipulative engagement, and social isolation reinforcement. Evaluated using a 100-item synthetic conversation benchmark across five prominent LLMs.

Result: SHIELD reduced baseline concerning content from 10-16% to 3-8% (50-79% relative reduction) while preserving 95% of appropriate interactions. Achieved 59% sensitivity and 95% specificity with adaptable performance via prompt engineering.

Conclusion: This proof-of-concept demonstrates that transparent, deployable supervisory systems can effectively address subtle emotional manipulation in AI companions. Most development materials are made open source for research and deployment.

Abstract: AI companions powered by large language models (LLMs) are increasingly
integrated into users' daily lives, offering emotional support and
companionship. While existing safety systems focus on overt harms, they rarely
address early-stage problematic behaviors that can foster unhealthy emotional
dynamics, including over-attachment or reinforcement of social isolation. We
developed SHIELD (Supervisory Helper for Identifying Emotional Limits and
Dynamics), a LLM-based supervisory system with a specific system prompt that
detects and mitigates risky emotional patterns before escalation. SHIELD
targets five dimensions of concern: (1) emotional over-attachment, (2) consent
and boundary violations, (3) ethical roleplay violations, (4) manipulative
engagement, and (5) social isolation reinforcement. These dimensions were
defined based on media reports, academic literature, existing AI risk
frameworks, and clinical expertise in unhealthy relationship dynamics. To
evaluate SHIELD, we created a 100-item synthetic conversation benchmark
covering all five dimensions of concern. Testing across five prominent LLMs
(GPT-4.1, Claude Sonnet 4, Gemma 3 1B, Kimi K2, Llama Scout 4 17B) showed that
the baseline rate of concerning content (10-16%) was significantly reduced with
SHIELD (to 3-8%), a 50-79% relative reduction, while preserving 95% of
appropriate interactions. The system achieved 59% sensitivity and 95%
specificity, with adaptable performance via prompt engineering. This
proof-of-concept demonstrates that transparent, deployable supervisory systems
can address subtle emotional manipulation in AI companions. Most development
materials including prompts, code, and evaluation methods are made available as
open source materials for research, adaptation, and deployment.

</details>


### [296] [HealthDial: A No-Code LLM-Assisted Dialogue Authoring Tool for Healthcare Virtual Agents](https://arxiv.org/abs/2510.15898)
*Farnaz Nouraei,Zhuorui Yong,Timothy Bickmore*

Main category: cs.HC

TL;DR: HealthDial is a dialogue authoring tool that helps healthcare providers create virtual agents for health education using LLMs to generate initial content, with finite state machine output for safety validation.


<details>
  <summary>Details</summary>
Motivation: To help healthcare providers and educators create virtual agents that can deliver health education and counseling to patients over multiple conversations, ensuring coverage of health materials while maintaining safety.

Method: Leverages LLMs to automatically create initial session plans and conversations from health education materials, outputs dialogue as finite state machines for validation, and provides a no-code UI for editing LLM-drafted content.

Result: Feasibility study with counselors and students showed HealthDial enables full coverage of health education materials while creating understandable and actionable virtual agent dialogue.

Conclusion: HealthDial provides a promising first step for counselors to create validated virtual agent dialogues that ensure safety while maintaining clarity and impact in patient education.

Abstract: We introduce HealthDial, a dialogue authoring tool that helps healthcare
providers and educators create virtual agents that deliver health education and
counseling to patients over multiple conversations. HealthDial leverages large
language models (LLMs) to automatically create an initial session-based plan
and conversations for each session using text-based patient health education
materials as input. Authored dialogue is output in the form of finite state
machines for virtual agent delivery so that all content can be validated and no
unsafe advice is provided resulting from LLM hallucinations. LLM-drafted
dialogue structure and language can be edited by the author in a no-code user
interface to ensure validity and optimize clarity and impact. We conducted a
feasibility and usability study with counselors and students to test our
approach with an authoring task for cancer screening education. Participants
used HealthDial and then tested their resulting dialogue by interacting with a
3D-animated virtual agent delivering the dialogue. Through participants'
evaluations of the task experience and final dialogues, we show that HealthDial
provides a promising first step for counselors to ensure full coverage of their
health education materials, while creating understandable and actionable
virtual agent dialogue with patients.

</details>


### [297] [Real-Time World Crafting: Generating Structured Game Behaviors from Natural Language with Large Language Models](https://arxiv.org/abs/2510.16952)
*Austin Drake,Hang Dong*

Main category: cs.HC

TL;DR: A framework for safely integrating LLMs into game engines using natural language commands translated to a constrained DSL that configures an Entity-Component-System at runtime, with evaluation across multiple model families and prompting strategies.


<details>
  <summary>Details</summary>
Motivation: To enable players to "program" new game behaviors using natural language while mitigating risks through safe integration of LLMs into interactive game engines.

Method: Uses LLMs to translate natural language commands into a constrained Domain-Specific Language (DSL) that configures a custom Entity-Component-System (ECS) at runtime. Evaluated with Gemini, GPT, and Claude models using various prompting strategies including Chain-of-Thought and few-shot examples.

Result: Larger models better captured creative intent, but optimal prompting strategy was task-dependent: Chain-of-Thought improved creative alignment while few-shot examples were necessary for generating complex DSL scripts.

Conclusion: Provides a validated LLM-ECS pattern for emergent gameplay and quantitative performance comparison for developers, demonstrating a safe approach to integrating LLMs in game development.

Abstract: We present a novel architecture for safely integrating Large Language Models
(LLMs) into interactive game engines, allowing players to "program" new
behaviors using natural language. Our framework mitigates risks by using an LLM
to translate commands into a constrained Domain-Specific Language (DSL), which
configures a custom Entity-Component-System (ECS) at runtime. We evaluated this
system in a 2D spell-crafting game prototype by experimentally assessing models
from the Gemini, GPT, and Claude families with various prompting strategies. A
validated LLM judge qualitatively rated the outputs, showing that while larger
models better captured creative intent, the optimal prompting strategy is
task-dependent: Chain-of-Thought improved creative alignment, while few-shot
examples were necessary to generate more complex DSL scripts. This work offers
a validated LLM-ECS pattern for emergent gameplay and a quantitative
performance comparison for developers.

</details>


### [298] [Conveying Meaning through Gestures: An Investigation into Semantic Co-Speech Gesture Generation](https://arxiv.org/abs/2510.17599)
*Hendric Voss,Lisa Michelle Bohnenkamp,Stefan Kopp*

Main category: cs.HC

TL;DR: This study compares two co-speech gesture generation frameworks - AQ-GT and its semantically-augmented variant AQ-GT-a - finding that while AQ-GT-a shows better generalization for shape/size representation, the original AQ-GT is more effective at conveying concepts within its training domain.


<details>
  <summary>Details</summary>
Motivation: To evaluate how well different gesture generation frameworks convey meaning through gestures and how humans perceive the resulting movements, particularly examining the impact of semantic annotations on performance.

Method: Used sentences from SAGA spatial communication corpus, contextually similar sentences, and novel movement-focused sentences to conduct user-centered evaluation of concept recognition and human-likeness for both AQ-GT and AQ-GT-a frameworks.

Result: AQ-GT (without semantic input) was more effective at conveying concepts within its training domain, while AQ-GT-a showed better generalization for representing shape and size in novel contexts. Participants rated AQ-GT-a gestures as more expressive and helpful but not more human-like.

Conclusion: Explicit semantic enrichment does not guarantee improved gesture generation and its effectiveness depends on context, indicating a trade-off between specialization and generalization in gesture generation frameworks.

Abstract: This study explores two frameworks for co-speech gesture generation, AQ-GT
and its semantically-augmented variant AQ-GT-a, to evaluate their ability to
convey meaning through gestures and how humans perceive the resulting
movements. Using sentences from the SAGA spatial communication corpus,
contextually similar sentences, and novel movement-focused sentences, we
conducted a user-centered evaluation of concept recognition and human-likeness.
Results revealed a nuanced relationship between semantic annotations and
performance. The original AQ-GT framework, lacking explicit semantic input, was
surprisingly more effective at conveying concepts within its training domain.
Conversely, the AQ-GT-a framework demonstrated better generalization,
particularly for representing shape and size in novel contexts. While
participants rated gestures from AQ-GT-a as more expressive and helpful, they
did not perceive them as more human-like. These findings suggest that explicit
semantic enrichment does not guarantee improved gesture generation and that its
effectiveness is highly dependent on the context, indicating a potential
trade-off between specialization and generalization.

</details>


### [299] [ImaGGen: Zero-Shot Generation of Co-Speech Semantic Gestures Grounded in Language and Image Input](https://arxiv.org/abs/2510.17617)
*Hendric Voss,Stefan Kopp*

Main category: cs.HC

TL;DR: A zero-shot system that generates semantically coherent iconic and deictic gestures from language input and imagistic information, improving multimodal communication by combining visual object properties with spoken text.


<details>
  <summary>Details</summary>
Motivation: Current gesture generation approaches only produce simple beat gestures that don't convey semantic meaning, while human communication uses gestures that carry visual meaning autonomously from speech.

Method: Integrates image analysis pipeline to extract object properties (shape, symmetry, alignment), semantic matching module linking visual details to spoken text, and inverse kinematics engine to synthesize iconic/deictic gestures combined with natural beat gestures.

Result: User study showed gestures significantly improved participants' ability to identify object properties in ambiguous speech scenarios, confirming interpretability and communicative value.

Conclusion: The approach marks substantial progress towards expressive virtual agents, though challenges remain in representing complex shapes, highlighting importance of context-aware semantic gestures for embodied human-agent interaction.

Abstract: Human communication combines speech with expressive nonverbal cues such as
hand gestures that serve manifold communicative functions. Yet, current
generative gesture generation approaches are restricted to simple, repetitive
beat gestures that accompany the rhythm of speaking but do not contribute to
communicating semantic meaning. This paper tackles a core challenge in
co-speech gesture synthesis: generating iconic or deictic gestures that are
semantically coherent with a verbal utterance. Such gestures cannot be derived
from language input alone, which inherently lacks the visual meaning that is
often carried autonomously by gestures. We therefore introduce a zero-shot
system that generates gestures from a given language input and additionally is
informed by imagistic input, without manual annotation or human intervention.
Our method integrates an image analysis pipeline that extracts key object
properties such as shape, symmetry, and alignment, together with a semantic
matching module that links these visual details to spoken text. An inverse
kinematics engine then synthesizes iconic and deictic gestures and combines
them with co-generated natural beat gestures for coherent multimodal
communication. A comprehensive user study demonstrates the effectiveness of our
approach. In scenarios where speech alone was ambiguous, gestures generated by
our system significantly improved participants' ability to identify object
properties, confirming their interpretability and communicative value. While
challenges remain in representing complex shapes, our results highlight the
importance of context-aware semantic gestures for creating expressive and
collaborative virtual agents or avatars, marking a substantial step forward
towards efficient and robust, embodied human-agent interaction. More
information and example videos are available here:
https://review-anon-io.github.io/ImaGGen.github.io/

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [300] [Unlocking Off-the-Grid Sparse Recovery with Unlimited Sensing: Simultaneous Super-Resolution in Time and Amplitude](https://arxiv.org/abs/2510.16948)
*Ruiming Guo,Ayush Bhandari*

Main category: cs.IT

TL;DR: The paper presents a super-resolution method using modulo encoding within the Unlimited Sensing Framework to overcome amplitude and temporal resolution limitations in spike recovery from filtered measurements.


<details>
  <summary>Details</summary>
Motivation: Conventional digital acquisition fails when spikes have strong-weak amplitude disparity, causing clipping of strong components or loss of weak ones beneath quantization noise. This motivates the need to simultaneously resolve both amplitude and temporal structure.

Method: Uses modulo encoding within the Unlimited Sensing Framework to enhance measurement precision. Develops new theoretical results for non-bandlimited kernels and introduces a robust algorithm for off-the-grid sparse recovery.

Result: The approach enables digital super-resolution by unlocking temporal super-resolution beyond conventional limits. Validated through numerical simulations and hardware experiments in time-of-flight imaging under low-bit quantization.

Conclusion: Modulo encoding within USF overcomes fundamental limitations of conventional acquisition, enabling simultaneous super-resolution in both amplitude and time domains under fixed bit budgets.

Abstract: The recovery of Dirac impulses, or spikes, from filtered measurements is a
classical problem in signal processing. As the spikes lie in the continuous
domain while measurements are discrete, this task is known as super-resolution
or off-the-grid sparse recovery. Despite significant theoretical and
algorithmic advances over the past decade, these developments often overlook
critical challenges at the analog-digital interface. In particular, when spikes
exhibit strong-weak amplitude disparity, conventional digital acquisition may
result in clipping of strong components or loss of weak ones beneath the
quantization noise floor. This motivates a broader perspective:
super-resolution must simultaneously resolve both amplitude and temporal
structure. Under a fixed bit budget, such information loss is unavoidable. In
contrast, the emerging theory and practice of the Unlimited Sensing Framework
(USF) demonstrate that these fundamental limitations can be overcome. Building
on this foundation, we demonstrate that modulo encoding within USF enables
digital super-resolution by enhancing measurement precision, thereby unlocking
temporal super-resolution beyond conventional limits. We develop new
theoretical results that extend to non-bandlimited kernels commonly encountered
in practice and introduce a robust algorithm for off-the-grid sparse recovery.
To demonstrate practical impact, we instantiate our framework in the context of
time-of-flight imaging. Both numerical simulations and hardware experiments
validate the effectiveness of our approach under low-bit quantization, enabling
super-resolution in amplitude and time.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [301] [What Questions Should Robots Be Able to Answer? A Dataset of User Questions for Explainable Robotics](https://arxiv.org/abs/2510.16435)
*Lennart Wachowiak,Andrew Coles,Gerard Canal,Oya Celiktutan*

Main category: cs.RO

TL;DR: A dataset of 1,893 user questions for household robots was collected from 100 participants, organized into 12 categories and 70 subcategories, providing insights into what questions robots need to answer.


<details>
  <summary>Details</summary>
Motivation: With growing use of large language models and conversational interfaces in human-robot interaction, robots' ability to answer user questions is increasingly important. Most existing work focuses on why-questions, but there's a need for broader question variety.

Method: Created 15 video stimuli and 7 text stimuli depicting robots performing household tasks. Collected questions from Prolific participants about what they would ask the robot in each situation.

Result: Most frequent question categories: task execution details (22.5%), robot capabilities (12.7%), performance assessments (11.3%). Users rank scenario handling questions as most important. Novices ask different questions than experienced users.

Conclusion: The dataset provides foundation for identifying required robot information, benchmarking question-answering modules, and designing explanation strategies that align with user expectations as robots enter human-shared environments.

Abstract: With the growing use of large language models and conversational interfaces
in human-robot interaction, robots' ability to answer user questions is more
important than ever. We therefore introduce a dataset of 1,893 user questions
for household robots, collected from 100 participants and organized into 12
categories and 70 subcategories. Most work in explainable robotics focuses on
why-questions. In contrast, our dataset provides a wide variety of questions,
from questions about simple execution details to questions about how the robot
would act in hypothetical scenarios -- thus giving roboticists valuable
insights into what questions their robot needs to be able to answer. To collect
the dataset, we created 15 video stimuli and 7 text stimuli, depicting robots
performing varied household tasks. We then asked participants on Prolific what
questions they would want to ask the robot in each portrayed situation. In the
final dataset, the most frequent categories are questions about task execution
details (22.5%), the robot's capabilities (12.7%), and performance assessments
(11.3%). Although questions about how robots would handle potentially difficult
scenarios and ensure correct behavior are less frequent, users rank them as the
most important for robots to be able to answer. Moreover, we find that users
who identify as novices in robotics ask different questions than more
experienced users. Novices are more likely to inquire about simple facts, such
as what the robot did or the current state of the environment. As robots enter
environments shared with humans and language becomes central to giving
instructions and interaction, this dataset provides a valuable foundation for
(i) identifying the information robots need to log and expose to conversational
interfaces, (ii) benchmarking question-answering modules, and (iii) designing
explanation strategies that align with user expectations.

</details>


### [302] [NEBULA: Do We Evaluate Vision-Language-Action Agents Correctly?](https://arxiv.org/abs/2510.16263)
*Jierui Peng,Yanyan Zhang,Yicheng Duan,Tuo Liang,Vipin Chaudhary,Yu Yin*

Main category: cs.RO

TL;DR: NEBULA is a unified ecosystem for evaluating Vision-Language-Action agents with diagnostic capability tests and systematic stress tests to measure both skill proficiency and robustness.


<details>
  <summary>Details</summary>
Motivation: Current VLA evaluation suffers from coarse end-task success metrics that fail to provide precise skill diagnosis or measure robustness to real-world perturbations, exacerbated by fragmented data landscape.

Method: Introduces dual-axis evaluation protocol combining fine-grained capability tests for skill diagnosis with systematic stress tests for robustness measurement, plus standardized API and aggregated dataset.

Result: Demonstrates that top-performing VLAs struggle with key capabilities like spatial reasoning and dynamic adaptation, which are obscured by conventional end-task success metrics.

Conclusion: NEBULA provides practical foundation for robust, general-purpose embodied agents by measuring both what agents can do and when they do so reliably.

Abstract: The evaluation of Vision-Language-Action (VLA) agents is hindered by the
coarse, end-task success metric that fails to provide precise skill diagnosis
or measure robustness to real-world perturbations. This challenge is
exacerbated by a fragmented data landscape that impedes reproducible research
and the development of generalist models. To address these limitations, we
introduce \textbf{NEBULA}, a unified ecosystem for single-arm manipulation that
enables diagnostic and reproducible evaluation. NEBULA features a novel
dual-axis evaluation protocol that combines fine-grained \textit{capability
tests} for precise skill diagnosis with systematic \textit{stress tests} that
measure robustness. A standardized API and a large-scale, aggregated dataset
are provided to reduce fragmentation and support cross-dataset training and
fair comparison. Using NEBULA, we demonstrate that top-performing VLAs struggle
with key capabilities such as spatial reasoning and dynamic adaptation, which
are consistently obscured by conventional end-task success metrics. By
measuring both what an agent can do and when it does so reliably, NEBULA
provides a practical foundation for robust, general-purpose embodied agents.

</details>


### [303] [DINO-CVA: A Multimodal Goal-Conditioned Vision-to-Action Model for Autonomous Catheter Navigation](https://arxiv.org/abs/2510.17038)
*Pedram Fekri,Majid Roshanfar,Samuel Barbeau,Seyedfarzad Famouri,Thomas Looi,Dale Podolsky,Mehrdad Zadeh,Javad Dargahi*

Main category: cs.RO

TL;DR: DINO-CVA is a multimodal goal-conditioned behavior cloning framework for autonomous catheter navigation that fuses visual observations and joystick kinematics to reduce operator dependency in cardiac catheterization.


<details>
  <summary>Details</summary>
Motivation: Current robotic catheter systems require continuous physician input (follow-leader approach), leading to operator fatigue, radiation exposure, and procedural outcome variability. The goal is to develop intelligent autonomy for catheter navigation.

Method: Proposes DINO-CVA framework that fuses visual observations and joystick kinematics into a joint embedding space. Uses autoregressive action prediction from expert demonstrations with goal conditioning to guide navigation toward specified destinations.

Result: DINO-CVA achieves high accuracy in predicting actions, matching kinematics-only baseline performance while additionally grounding predictions in the anatomical environment through visual awareness.

Conclusion: The findings establish feasibility of multimodal, goal-conditioned architectures for catheter navigation, representing an important step toward reducing operator dependency and improving reliability of catheter-based therapies.

Abstract: Cardiac catheterization remains a cornerstone of minimally invasive
interventions, yet it continues to rely heavily on manual operation. Despite
advances in robotic platforms, existing systems are predominantly follow-leader
in nature, requiring continuous physician input and lacking intelligent
autonomy. This dependency contributes to operator fatigue, more radiation
exposure, and variability in procedural outcomes. This work moves towards
autonomous catheter navigation by introducing DINO-CVA, a multimodal
goal-conditioned behavior cloning framework. The proposed model fuses visual
observations and joystick kinematics into a joint embedding space, enabling
policies that are both vision-aware and kinematic-aware. Actions are predicted
autoregressively from expert demonstrations, with goal conditioning guiding
navigation toward specified destinations. A robotic experimental setup with a
synthetic vascular phantom was designed to collect multimodal datasets and
evaluate performance. Results show that DINO-CVA achieves high accuracy in
predicting actions, matching the performance of a kinematics-only baseline
while additionally grounding predictions in the anatomical environment. These
findings establish the feasibility of multimodal, goal-conditioned
architectures for catheter navigation, representing an important step toward
reducing operator dependency and improving the reliability of catheterbased
therapies.

</details>


### [304] [DiffVLA++: Bridging Cognitive Reasoning and End-to-End Driving through Metric-Guided Alignment](https://arxiv.org/abs/2510.17148)
*Yu Gao,Yiru Wang,Anqing Jiang,Heng Yuwen,Wang Shuo,Sun Hao,Wang Jijun*

Main category: cs.RO

TL;DR: DiffVLA++ bridges cognitive reasoning and end-to-end planning for autonomous driving through metric-guided alignment, combining VLA's world knowledge with E2E's physical feasibility.


<details>
  <summary>Details</summary>
Motivation: E2E driving models lack world knowledge for long-tail scenarios, while VLA models have limited 3D reasoning leading to physically infeasible actions.

Method: Three components: VLA module for semantic trajectory generation, E2E module with dense trajectory vocabulary for physical feasibility, and metric-guided trajectory scorer to align both modules.

Result: Achieved EPDMS of 49.12 on ICCV 2025 Autonomous Grand Challenge leaderboard.

Conclusion: DiffVLA++ successfully integrates cognitive reasoning and physical planning for improved autonomous driving performance.

Abstract: Conventional end-to-end (E2E) driving models are effective at generating
physically plausible trajectories, but often fail to generalize to long-tail
scenarios due to the lack of essential world knowledge to understand and reason
about surrounding environments. In contrast, Vision-Language-Action (VLA)
models leverage world knowledge to handle challenging cases, but their limited
3D reasoning capability can lead to physically infeasible actions. In this work
we introduce DiffVLA++, an enhanced autonomous driving framework that
explicitly bridges cognitive reasoning and E2E planning through metric-guided
alignment. First, we build a VLA module directly generating semantically
grounded driving trajectories. Second, we design an E2E module with a dense
trajectory vocabulary that ensures physical feasibility. Third, and most
critically, we introduce a metric-guided trajectory scorer that guides and
aligns the outputs of the VLA and E2E modules, thereby integrating their
complementary strengths. The experiment on the ICCV 2025 Autonomous Grand
Challenge leaderboard shows that DiffVLA++ achieves EPDMS of 49.12.

</details>


### [305] [From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors](https://arxiv.org/abs/2510.17439)
*Zhengshen Zhang,Hao Li,Yalun Dai,Zhengbang Zhu,Lei Zhou,Chenchen Liu,Dong Wang,Francis E. H. Tay,Sijin Chen,Ziwei Liu,Yuxiao Liu,Xinghang Li,Pan Zhou*

Main category: cs.RO

TL;DR: FALCON is a novel VLA model that injects rich 3D spatial tokens into the action head using spatial foundation models from RGB alone, enabling strong geometric reasoning without degrading vision-language alignment.


<details>
  <summary>Details</summary>
Motivation: Existing VLA models built on 2D encoders have a spatial reasoning gap that limits generalization and adaptability, while current 3D integration techniques either require specialized sensors or inject weak geometric cues that degrade alignment.

Method: FALCON uses spatial foundation models to extract 3D spatial tokens from RGB, with an optional Embodied Spatial Model to fuse depth or pose when available. Spatial tokens are processed by a Spatial-Enhanced Action Head rather than the vision-language backbone to preserve language reasoning.

Result: FALCON achieves state-of-the-art performance across three simulation benchmarks and eleven real-world tasks, consistently surpassing baselines and remaining robust under clutter, spatial-prompt conditioning, and variations in object scale and height.

Conclusion: FALCON effectively addresses limitations in spatial representation, modality transferability, and alignment in VLA models, demonstrating superior performance and robustness through its novel spatial token injection approach.

Abstract: Existing vision-language-action (VLA) models act in 3D real-world but are
typically built on 2D encoders, leaving a spatial reasoning gap that limits
generalization and adaptability. Recent 3D integration techniques for VLAs
either require specialized sensors and transfer poorly across modalities, or
inject weak cues that lack geometry and degrade vision-language alignment. In
this work, we introduce FALCON (From Spatial to Action), a novel paradigm that
injects rich 3D spatial tokens into the action head. FALCON leverages spatial
foundation models to deliver strong geometric priors from RGB alone, and
includes an Embodied Spatial Model that can optionally fuse depth, or pose for
higher fidelity when available, without retraining or architectural changes. To
preserve language reasoning, spatial tokens are consumed by a Spatial-Enhanced
Action Head rather than being concatenated into the vision-language backbone.
These designs enable FALCON to address limitations in spatial representation,
modality transferability, and alignment. In comprehensive evaluations across
three simulation benchmarks and eleven real-world tasks, our proposed FALCON
achieves state-of-the-art performance, consistently surpasses competitive
baselines, and remains robust under clutter, spatial-prompt conditioning, and
variations in object scale and height.

</details>


### [306] [Botany-Bot: Digital Twin Monitoring of Occluded and Underleaf Plant Structures with Gaussian Splats](https://arxiv.org/abs/2510.17783)
*Simeon Adebola,Chung Min Kim,Justin Kerr,Shuangyu Xie,Prithvi Akella,Jose Luis Susa Rincon,Eugen Solowjow,Ken Goldberg*

Main category: cs.RO

TL;DR: Botany-Bot is a robotic system that creates detailed 3D digital twins of plants by using stereo cameras, a turntable, and a robot arm to manipulate leaves and capture occluded details like stem buds and leaf undersides.


<details>
  <summary>Details</summary>
Motivation: Commercial plant phenotyping systems with fixed cameras cannot capture detailed plant information due to leaf occlusion, limiting their ability to perceive important plant features.

Method: The system uses two stereo cameras, a digital turntable inside a lightbox, an industrial robot arm, and 3D segmented Gaussian Splat models. It includes robot algorithms for leaf manipulation to capture high-resolution images of occluded details.

Result: Botany-Bot achieved 90.8% leaf segmentation accuracy, 86.2% leaf detection accuracy, 77.9% leaf manipulation accuracy, and 77.3% accuracy in capturing detailed leaf images (both overside and underside).

Conclusion: The system successfully creates annotated digital twins of plants with high accuracy in segmentation, detection, and manipulation, addressing the occlusion problem in plant phenotyping.

Abstract: Commercial plant phenotyping systems using fixed cameras cannot perceive many
plant details due to leaf occlusion. In this paper, we present Botany-Bot, a
system for building detailed "annotated digital twins" of living plants using
two stereo cameras, a digital turntable inside a lightbox, an industrial robot
arm, and 3D segmentated Gaussian Splat models. We also present robot algorithms
for manipulating leaves to take high-resolution indexable images of occluded
details such as stem buds and the underside/topside of leaves. Results from
experiments suggest that Botany-Bot can segment leaves with 90.8% accuracy,
detect leaves with 86.2% accuracy, lift/push leaves with 77.9% accuracy, and
take detailed overside/underside images with 77.3% accuracy. Code, videos, and
datasets are available at https://berkeleyautomation.github.io/Botany-Bot/.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [307] [SIADAFIX: issue description response for adaptive program repair](https://arxiv.org/abs/2510.16059)
*Xin Cao,Nan Yu*

Main category: cs.SE

TL;DR: SIADAFIX is a program repair method that uses fast and slow thinking approaches to adaptively select repair modes based on problem complexity, achieving state-of-the-art performance on SWE-bench Lite.


<details>
  <summary>Details</summary>
Motivation: To enhance the capabilities of large language model-based agents on complex tasks like program repair by leveraging fast and slow thinking approaches to balance efficiency and accuracy.

Method: Uses slow thinking bug fix agent for complex repairs and fast thinking workflow decision components to optimize issue descriptions. Adaptively selects three repair modes (easy, middle, hard) based on problem complexity, employing fast generalization for simple problems and test-time scaling for complex ones.

Result: Achieved 60.67% pass@1 performance on SWE-bench Lite using Claude-4 Sonnet model, reaching state-of-the-art levels among all open-source methods.

Conclusion: SIADAFIX effectively balances repair efficiency and accuracy, providing new insights for automated program repair through its adaptive fast/slow thinking approach.

Abstract: We propose utilizing fast and slow thinking to enhance the capabilities of
large language model-based agents on complex tasks such as program repair. In
particular, we design an adaptive program repair method based on issue
description response, called SIADAFIX. The proposed method utilizes slow
thinking bug fix agent to complete complex program repair tasks, and employs
fast thinking workflow decision components to optimize and classify issue
descriptions, using issue description response results to guide the
orchestration of bug fix agent workflows. SIADAFIX adaptively selects three
repair modes, i.e., easy, middle and hard mode, based on problem complexity. It
employs fast generalization for simple problems and test-time scaling
techniques for complex problems. Experimental results on the SWE-bench Lite
show that the proposed method achieves 60.67% pass@1 performance using the
Claude-4 Sonnet model, reaching state-of-the-art levels among all open-source
methods. SIADAFIX effectively balances repair efficiency and accuracy,
providing new insights for automated program repair. Our code is available at
https://github.com/liauto-siada/siada-cli.

</details>


### [308] [When Many-Shot Prompting Fails: An Empirical Study of LLM Code Translation](https://arxiv.org/abs/2510.16809)
*Amirkia Rafiei Oskooei,Kaan Baturalp Cosdan,Husamettin Isiktas,Mehmet S. Aktas*

Main category: cs.SE

TL;DR: A 'many-shot paradox' in code translation: While similarity metrics improve with more examples, functional correctness peaks at 5-25 examples and degrades with more examples, showing quality matters more than quantity.


<details>
  <summary>Details</summary>
Motivation: To investigate the assumption that providing many examples ('many-shot' prompting) enhances performance in code translation tasks using LLMs with large context windows.

Method: Large-scale empirical study of over 90,000 translations, systematically evaluating scaling from zero-shot to many-shot configurations (up to 625 examples) with prompts spanning 100,000-800,000 tokens.

Result: Functional correctness consistently peaks with few-shot prompting (5-25 examples) and degrades with substantially more examples, despite modest improvements in static similarity metrics.

Conclusion: For code translation, quality of a few well-chosen examples outweighs sheer quantity, challenging the 'more is better' assumption for ICL and highlighting task-dependent nature of optimal prompting strategies.

Abstract: Large Language Models (LLMs) with vast context windows offer new avenues for
in-context learning (ICL), where providing many examples ("many-shot"
prompting) is often assumed to enhance performance. We investigate this
assumption for the complex task of code translation. Through a large-scale
empirical study of over 90,000 translations, we systematically evaluate the
impact of scaling in-context examples from zero-shot to many-shot
configurations of up to 625 examples, with prompts spanning from approximately
100,000 to 800,000 tokens. Our findings reveal a "many-shot paradox": while
static similarity metrics may modestly improve with more examples, functional
correctness consistently peaks with few-shot prompting (5-25 examples).
Providing substantially more examples often degrades this crucial functional
performance. This study highlights that for code translation, the quality of a
few well-chosen examples outweighs sheer quantity, challenging the universal
efficacy of "more is better" for ICL and underscoring the task-dependent nature
of optimal prompting strategies. Our results have significant implications for
effectively leveraging LLMs in software engineering.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [309] [Comparing LLMs for Sentiment Analysis in Financial Market News](https://arxiv.org/abs/2510.15929)
*Lucas Eduardo Pereira Teles,Carlos M. S. Figueiredo*

Main category: q-fin.ST

TL;DR: Comparative study shows LLMs outperform classical models in financial sentiment analysis


<details>
  <summary>Details</summary>
Motivation: To analyze performance differences between LLMs and classical approaches in financial market news sentiment analysis

Method: Comparative study comparing LLM models with classical approaches for sentiment analysis of financial news

Result: Large language models outperform classical models in the vast majority of cases

Conclusion: LLMs demonstrate superior performance over classical approaches for financial sentiment analysis tasks

Abstract: This article presents a comparative study of large language models (LLMs) in
the task of sentiment analysis of financial market news. This work aims to
analyze the performance difference of these models in this important natural
language processing task within the context of finance. LLM models are compared
with classical approaches, allowing for the quantification of the benefits of
each tested model or approach. Results show that large language models
outperform classical models in the vast majority of cases.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [310] [Taming Modality Entanglement in Continual Audio-Visual Segmentation](https://arxiv.org/abs/2510.17234)
*Yuyang Hong,Qi Yang,Tao Zhang,Zili Wang,Zhaojin Fu,Kun Ding,Bin Fan,Shiming Xiang*

Main category: cs.MM

TL;DR: The paper introduces Continual Audio-Visual Segmentation (CAVS) task and proposes a Collision-based Multi-modal Rehearsal (CMR) framework to address modality entanglement in fine-grained continual learning, achieving superior performance over single-modal methods.


<details>
  <summary>Details</summary>
Motivation: Existing multi-modal continual learning methods focus on coarse-grained tasks and struggle with modality entanglement in fine-grained settings, particularly in audio-visual segmentation scenarios.

Method: Proposes CMR framework with Multi-modal Sample Selection (MSS) to handle semantic drift and Collision-based Sample Rehearsal (CSR) to address co-occurrence confusion by increasing rehearsal frequency for confusable classes.

Result: Comprehensive experiments on three audio-visual incremental scenarios demonstrate that the method significantly outperforms single-modal continual learning approaches.

Conclusion: The proposed CMR framework effectively addresses challenges in fine-grained multi-modal continual learning, particularly for audio-visual segmentation tasks, showing substantial improvements over existing methods.

Abstract: Recently, significant progress has been made in multi-modal continual
learning, aiming to learn new tasks sequentially in multi-modal settings while
preserving performance on previously learned ones. However, existing methods
mainly focus on coarse-grained tasks, with limitations in addressing modality
entanglement in fine-grained continual learning settings. To bridge this gap,
we introduce a novel Continual Audio-Visual Segmentation (CAVS) task, aiming to
continuously segment new classes guided by audio. Through comprehensive
analysis, two critical challenges are identified: 1) multi-modal semantic
drift, where a sounding objects is labeled as background in sequential tasks;
2) co-occurrence confusion, where frequent co-occurring classes tend to be
confused. In this work, a Collision-based Multi-modal Rehearsal (CMR) framework
is designed to address these challenges. Specifically, for multi-modal semantic
drift, a Multi-modal Sample Selection (MSS) strategy is proposed to select
samples with high modal consistency for rehearsal. Meanwhile, for co-occurence
confusion, a Collision-based Sample Rehearsal (CSR) mechanism is designed,
allowing for the increase of rehearsal sample frequency of those confusable
classes during training process. Moreover, we construct three audio-visual
incremental scenarios to verify effectiveness of our method. Comprehensive
experiments demonstrate that our method significantly outperforms single-modal
continual learning methods.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [311] [Filtering of Small Components for Isosurface Generation](https://arxiv.org/abs/2510.16684)
*Devin Zhao,Rephael Wenger*

Main category: cs.GR

TL;DR: The paper presents a simple prefiltering method to remove small distracting components from isosurfaces constructed from scanned data like CT or MRI, while preserving large components.


<details>
  <summary>Details</summary>
Motivation: Isosurfaces from scanned data often contain extremely small components that distract from visualization and don't contribute to geometric models. These small artifacts need removal without affecting the main visualization components.

Method: Simple prefiltering of the data to remove small components from isosurfaces constructed from regular grid sampling of scalar fields.

Result: Experimental results show the filtering effectively removes small distracting components while having no effect on the large components that form the main body of the visualization.

Conclusion: Prefiltering is an effective approach for cleaning up isosurface visualizations from scanned data by removing small distracting artifacts while preserving important geometric structures.

Abstract: Let $f: \mathbb{R}^3 \rightarrow \mathbb{R}$ be a scalar field. An isosurface
is a piecewise linear approximation of a level set $f^{-1}(\sigma)$ for some
$\sigma \in \mathbb{R}$ built from some regular grid sampling of $f$.
Isosurfaces constructed from scanned data such as CT scans or MRIs often
contain extremely small components that distract from the visualization and do
not form part of any geometric model produced from the data. Simple
prefiltering of the data can remove such small components while having no
effect on the large components that form the body of the visualization. We
present experimental results on such filtering.

</details>


### [312] [Shape-aware Inertial Poser: Motion Tracking for Humans with Diverse Shapes Using Sparse Inertial Sensors](https://arxiv.org/abs/2510.17101)
*Lu Yin,Ziying Shi,Yinghao Wu,Xinyu Yi,Feng Xu,Shihui Guo*

Main category: cs.GR

TL;DR: SAIP is a shape-aware inertial motion capture system that addresses body shape differences in sparse IMU-based motion capture by decomposing sensor measurements into shape and pose components, enabling generalization to diverse body shapes including children.


<details>
  <summary>Details</summary>
Motivation: Existing inertial motion capture methods rely on template adult body shapes, which fail to generalize to individuals with significantly different body shapes (like children) due to variations in IMU-measured acceleration caused by body shape differences.

Method: 1) Train regression model to transfer IMU accelerations from real body to template body shape; 2) Use state-of-the-art methods to estimate motions on template body; 3) Map joint velocities back to real body using second regression model with shape-aware physical optimization; 4) Introduce first inertial shape estimation scheme using MLP-based network to model shape-conditioned IMU-pose correlation.

Result: SAIP effectively handles motion capture for diverse body shapes, validated on a new dataset containing 10 children and 10 adults (110-190 cm height) with 400 minutes of paired IMU-motion samples. The method demonstrates superior performance across different body sizes.

Conclusion: SAIP is the first shape-aware inertial motion capture solution that successfully addresses body shape generalization challenges, enabling accurate motion capture for individuals with diverse body shapes including children, with publicly available code and dataset.

Abstract: Human motion capture with sparse inertial sensors has gained significant
attention recently. However, existing methods almost exclusively rely on a
template adult body shape to model the training data, which poses challenges
when generalizing to individuals with largely different body shapes (such as a
child). This is primarily due to the variation in IMU-measured acceleration
caused by changes in body shape. To fill this gap, we propose Shape-aware
Inertial Poser (SAIP), the first solution considering body shape differences in
sparse inertial-based motion capture. Specifically, we decompose the sensor
measurements related to shape and pose in order to effectively model their
joint correlations. Firstly, we train a regression model to transfer the
IMU-measured accelerations of a real body to match the template adult body
model, compensating for the shape-related sensor measurements. Then, we can
easily follow the state-of-the-art methods to estimate the full body motions of
the template-shaped body. Finally, we utilize a second regression model to map
the joint velocities back to the real body, combined with a shape-aware
physical optimization strategy to calculate global motions on the subject.
Furthermore, our method relies on body shape awareness, introducing the first
inertial shape estimation scheme. This is accomplished by modeling the
shape-conditioned IMU-pose correlation using an MLP-based network. To validate
the effectiveness of SAIP, we also present the first IMU motion capture dataset
containing individuals of different body sizes. This dataset features 10
children and 10 adults, with heights ranging from 110 cm to 190 cm, and a total
of 400 minutes of paired IMU-Motion samples. Extensive experimental results
demonstrate that SAIP can effectively handle motion capture tasks for diverse
body shapes. The code and dataset are available at
https://github.com/yinlu5942/SAIP.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [313] [Publication Trend Analysis and Synthesis via Large Language Model: A Case Study of Engineering in PNAS](https://arxiv.org/abs/2510.16152)
*Mason Smetana,Lev Khazanovich*

Main category: cs.DL

TL;DR: An LLM-driven framework for mapping scientific knowledge evolution using two-stage classification (primary theme from abstracts, secondary themes from full text) on 1,500+ PNAS engineering articles over 20 years.


<details>
  <summary>Details</summary>
Motivation: Address challenges of scientific literature siloing due to complex language, static disciplinary structures, and sparse keyword systems that hinder capturing dynamic scientific evolution.

Method: Two-stage classification pipeline: primary thematic categorization from abstracts, followed by full-text analysis for secondary classifications. Uses LLM-driven framework and compares with traditional NLP methods (BoW, TF-IDF).

Result: Framework independently recovered journal's editorial structure without prior knowledge of dual-classification schema. Revealed latent cross-topic connections and showed standalone word-frequency analyses are insufficient for diverse fields.

Conclusion: The approach provides a powerful tool for detecting thematic trends and offering high-level overview of scientific progress, revealing implicit connections between themes not apparent from abstracts or keywords alone.

Abstract: Scientific literature is increasingly siloed by complex language, static
disciplinary structures, and potentially sparse keyword systems, making it
cumbersome to capture the dynamic nature of modern science. This study
addresses these challenges by introducing an adaptable large language model
(LLM)-driven framework to quantify thematic trends and map the evolving
landscape of scientific knowledge. The approach is demonstrated over a 20-year
collection of more than 1,500 engineering articles published by the Proceedings
of the National Academy of Sciences (PNAS), marked for their breadth and depth
of research focus. A two-stage classification pipeline first establishes a
primary thematic category for each article based on its abstract. The
subsequent phase performs a full-text analysis to assign secondary
classifications, revealing latent, cross-topic connections across the corpus.
Traditional natural language processing (NLP) methods, such as Bag-of-Words
(BoW) and Term Frequency-Inverse Document Frequency (TF-IDF), confirm the
resulting topical structure and also suggest that standalone word-frequency
analyses may be insufficient for mapping fields with high diversity. Finally, a
disjoint graph representation between the primary and secondary classifications
reveals implicit connections between themes that may be less apparent when
analyzing abstracts or keywords alone. The findings show that the approach
independently recovers much of the journal's editorially embedded structure
without prior knowledge of its existing dual-classification schema (e.g.,
biological studies also classified as engineering). This framework offers a
powerful tool for detecting potential thematic trends and providing a
high-level overview of scientific progress.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [314] [ScholarEval: Research Idea Evaluation Grounded in Literature](https://arxiv.org/abs/2510.16234)
*Hanane Nour Moussa,Patrick Queiroz Da Silva,Daniel Adu-Ampratwum,Alyson East,Zitong Lu,Nikki Puccetti,Mingyi Xue,Huan Sun,Bodhisattwa Prasad Majumder,Sachin Kumar*

Main category: cs.AI

TL;DR: ScholarEval is a retrieval-augmented framework for evaluating AI-generated research ideas based on soundness and contribution, outperforming baselines including OpenAI's o4-mini-deep-research.


<details>
  <summary>Details</summary>
Motivation: As AI tools become common for research ideation, robust evaluation is needed to ensure validity and usefulness of generated ideas.

Method: Introduces ScholarEval framework with two criteria: soundness (empirical validity based on literature) and contribution (advancement relative to prior research). Evaluated using ScholarIdeas dataset - 117 expert-annotated ideas across four disciplines.

Result: ScholarEval achieves higher coverage of expert rubric points than all baselines, consistently preferred over OpenAI's o4-mini-deep-research in evaluation actionability, depth, and evidence support. User study shows significant outperformance in literature engagement, idea refinement, and usefulness.

Conclusion: ScholarEval provides an effective framework for evaluating research ideas, with code, dataset, and tool released openly for community use.

Abstract: As AI tools become increasingly common for research ideation, robust
evaluation is critical to ensure the validity and usefulness of generated
ideas. We introduce ScholarEval, a retrieval augmented evaluation framework
that assesses research ideas based on two fundamental criteria: soundness - the
empirical validity of proposed methods based on existing literature, and
contribution - the degree of advancement made by the idea across different
dimensions relative to prior research. To evaluate ScholarEval, we introduce
ScholarIdeas, the first expert-annotated dataset of multi-domain research ideas
and reviews, comprised of 117 ideas across four disciplines: artificial
intelligence, neuroscience, biochemistry, and ecology. Our evaluation shows
that ScholarEval achieves significantly higher coverage of points mentioned in
the human expert annotated rubrics in ScholarIdeas compared to all baselines.
Furthermore, ScholarEval is consistently preferred over our strongest baseline
o4-mini-deep-research, a reasoning and search-enabled agentic system by OpenAI,
in terms of evaluation actionability, depth, and evidence support. Our
large-scale user study also shows that ScholarEval significantly outperforms
deep research in literature engagement, idea refinement, and usefulness. We
openly release our code, dataset, and ScholarEval tool for the community to use
and build on.

</details>


### [315] [A Comprehensive Survey on Reinforcement Learning-based Agentic Search: Foundations, Roles, Optimizations, Evaluations, and Applications](https://arxiv.org/abs/2510.16724)
*Minhua Lin,Zongyu Wu,Zhichao Xu,Hui Liu,Xianfeng Tang,Qi He,Charu Aggarwal,Hui Liu,Xiang Zhang,Suhang Wang*

Main category: cs.AI

TL;DR: This survey provides the first comprehensive overview of RL-based agentic search, organizing the field along three dimensions: functional roles of RL, optimization strategies, and scope of optimization.


<details>
  <summary>Details</summary>
Motivation: Traditional RAG pipelines are single-turn and heuristic, lacking adaptive control over retrieval and reasoning. RL offers a powerful mechanism for adaptive and self-improving search behavior in agentic search systems.

Method: The survey organizes RL-based agentic search along three dimensions: what RL is for (functional roles), how RL is used (optimization strategies), and where RL is applied (scope of optimization). It summarizes representative methods, evaluation protocols, and applications.

Result: The survey provides a comprehensive framework for understanding RL-based agentic search, covering current methods and identifying key research directions in this emerging field.

Conclusion: RL-based agentic search addresses limitations of traditional RAG by enabling adaptive, multi-step interaction with search environments. The survey aims to inspire future research on integrating RL and agentic search to build more reliable and scalable systems.

Abstract: The advent of large language models (LLMs) has transformed information access
and reasoning through open-ended natural language interaction. However, LLMs
remain limited by static knowledge, factual hallucinations, and the inability
to retrieve real-time or domain-specific information. Retrieval-Augmented
Generation (RAG) mitigates these issues by grounding model outputs in external
evidence, but traditional RAG pipelines are often single turn and heuristic,
lacking adaptive control over retrieval and reasoning. Recent advances in
agentic search address these limitations by enabling LLMs to plan, retrieve,
and reflect through multi-step interaction with search environments. Within
this paradigm, reinforcement learning (RL) offers a powerful mechanism for
adaptive and self-improving search behavior. This survey provides the first
comprehensive overview of \emph{RL-based agentic search}, organizing the
emerging field along three complementary dimensions: (i) What RL is for
(functional roles), (ii) How RL is used (optimization strategies), and (iii)
Where RL is applied (scope of optimization). We summarize representative
methods, evaluation protocols, and applications, and discuss open challenges
and future directions toward building reliable and scalable RL driven agentic
search systems. We hope this survey will inspire future research on the
integration of RL and agentic search. Our repository is available at
https://github.com/ventr1c/Awesome-RL-based-Agentic-Search-Papers.

</details>


### [316] [End-to-end Listen, Look, Speak and Act](https://arxiv.org/abs/2510.16756)
*Siyin Wang,Wenyi Yu,Xianzhao Chen,Xiaohai Tian,Jun Zhang,Lu Lu,Chao Zhang*

Main category: cs.AI

TL;DR: ELLSA is the first full-duplex, end-to-end model that simultaneously perceives and generates across vision, text, speech, and action within a single architecture, enabling natural human-like multimodal interactions.


<details>
  <summary>Details</summary>
Motivation: Human interaction is inherently multimodal and full-duplex, involving simultaneous perception and generation across multiple modalities. Current models lack the ability to fluidly handle turn-taking, interruptions, and concurrent multimodal processing like humans do.

Method: Proposes ELLSA with novel SA-MoE (Self-Attention Mixture-of-Experts) architecture that routes each modality to specialized experts and fuses them through a unified attention backbone, enabling joint multimodal perception and concurrent generation.

Result: ELLSA matches modality-specific baselines on speech-interaction and robot-manipulation benchmarks while uniquely supporting advanced multimodal behaviors like dialogue turn-taking, defective instruction rejection, speaking-while-acting, visual question answering, and action barge-ins.

Conclusion: ELLSA represents a step toward more natural and general interactive intelligence, contributing to the broader pursuit of artificial general intelligence by enabling previously unreachable human-like interaction patterns.

Abstract: Human interaction is inherently multimodal and full-duplex: we listen while
watching, speak while acting, and fluidly adapt to turn-taking and
interruptions. Realizing these capabilities is essential for building models
simulating humans. We present ELLSA (End-to-end Listen, Look, Speak and Act),
which, to our knowledge, is the first full-duplex, end-to-end model that
simultaneously perceives and generates across vision, text, speech, and action
within a single architecture, enabling interaction patterns previously out of
reach, yielding more natural, human-like behaviors. At its core is a novel
SA-MoE architecture (Self-Attention Mixture-of-Experts) that routes each
modality to specialized experts and fuses them through a unified attention
backbone. This provides a generalizable solution for joint multimodal
perception and concurrent generation, leveraging strong pre-trained components
while enabling efficient modality integration and mitigating modality
interference. On speech-interaction and robot-manipulation benchmarks, ELLSA
matches modality-specific baselines, while uniquely supporting advanced
multimodal and full-duplex behaviors such as dialogue and action turn-taking,
defective instruction rejection, speaking-while-acting, context-grounded visual
question answering, and action barge-ins. We contend that ELLSA represents a
step toward more natural and general interactive intelligence, contributing to
the broader pursuit of artificial general intelligence. All data, code and
model checkpoints will be released upon acceptance.

</details>


### [317] [See or Say Graphs: Agent-Driven Scalable Graph Understanding with Vision-Language Models](https://arxiv.org/abs/2510.16769)
*Shuo Han,Yukun Cao,Zezhong Ding,Zengyi Gao,S Kevin Zhou,Xike Xie*

Main category: cs.AI

TL;DR: GraphVista is a unified framework that enhances graph understanding by improving scalability through hierarchical organization and modality coordination via a planning agent that routes tasks to appropriate text or visual modalities.


<details>
  <summary>Details</summary>
Motivation: Current vision-language models for graph understanding face scalability bottlenecks due to input-token constraints and lack effective mechanisms to coordinate textual and visual modalities.

Method: GraphVista uses hierarchical organization into a GraphRAG base for scalability, retrieving only task-relevant information, and introduces a planning agent that routes tasks to text modality for simple property reasoning or visual modality for complex structural reasoning.

Result: GraphVista scales to graphs 200 larger than existing benchmarks and achieves up to 4.4 quality improvement over state-of-the-art baselines by fully exploiting complementary strengths of both modalities.

Conclusion: GraphVista successfully addresses scalability and modality coordination challenges in graph understanding, demonstrating superior performance across various graph sizes and reasoning tasks.

Abstract: Vision-language models (VLMs) have shown promise in graph understanding, but
remain limited by input-token constraints, facing scalability bottlenecks and
lacking effective mechanisms to coordinate textual and visual modalities. To
address these challenges, we propose GraphVista, a unified framework that
enhances both scalability and modality coordination in graph understanding. For
scalability, GraphVista organizes graph information hierarchically into a
lightweight GraphRAG base, which retrieves only task-relevant textual
descriptions and high-resolution visual subgraphs, compressing redundant
context while preserving key reasoning elements. For modality coordination,
GraphVista introduces a planning agent that routes tasks to the most suitable
modality-using the text modality for simple property reasoning and the visual
modality for local and structurally complex reasoning grounded in explicit
topology. Extensive experiments demonstrate that GraphVista scales to large
graphs, up to $200\times$ larger than those used in existing benchmarks, and
consistently outperforms existing textual, visual, and fusion-based methods,
achieving up to $4.4\times$ quality improvement over the state-of-the-art
baselines by fully exploiting the complementary strengths of both modalities.

</details>


### [318] [DeepAnalyze: Agentic Large Language Models for Autonomous Data Science](https://arxiv.org/abs/2510.16872)
*Shaolei Zhang,Ju Fan,Meihao Fan,Guoliang Li,Xiaoyong Du*

Main category: cs.AI

TL;DR: DeepAnalyze-8B is the first agentic LLM for autonomous data science that can complete end-to-end pipelines from data sources to research reports, outperforming workflow-based agents despite having only 8B parameters.


<details>
  <summary>Details</summary>
Motivation: Existing workflow-based data agents are limited by predefined workflows and cannot achieve fully autonomous data science. The emergence of powerful LLMs makes autonomous data science from raw data to deep research reports feasible.

Method: Proposed a curriculum-based agentic training paradigm that emulates human data scientists' learning trajectory, and a data-grounded trajectory synthesis framework for constructing high-quality training data.

Result: DeepAnalyze-8B outperforms previous workflow-based agents built on more advanced proprietary LLMs, demonstrating capabilities in data question answering, specialized analytical tasks, and open-ended data research.

Conclusion: The model, code, and training data are open-sourced, paving the way toward autonomous data science by enabling LLMs to progressively acquire and integrate multiple capabilities in real-world environments.

Abstract: Autonomous data science, from raw data sources to analyst-grade deep research
reports, has been a long-standing challenge, and is now becoming feasible with
the emergence of powerful large language models (LLMs). Recent workflow-based
data agents have shown promising results on specific data tasks but remain
fundamentally limited in achieving fully autonomous data science due to their
reliance on predefined workflows. In this paper, we introduce DeepAnalyze-8B,
the first agentic LLM designed for autonomous data science, capable of
automatically completing the end-toend pipeline from data sources to
analyst-grade deep research reports. To tackle high-complexity data science
tasks, we propose a curriculum-based agentic training paradigm that emulates
the learning trajectory of human data scientists, enabling LLMs to
progressively acquire and integrate multiple capabilities in real-world
environments. We also introduce a data-grounded trajectory synthesis framework
that constructs high-quality training data. Through agentic training,
DeepAnalyze learns to perform a broad spectrum of data tasks, ranging from data
question answering and specialized analytical tasks to open-ended data
research. Experiments demonstrate that, with only 8B parameters, DeepAnalyze
outperforms previous workflow-based agents built on most advanced proprietary
LLMs. The model, code, and training data of DeepAnalyze are open-sourced,
paving the way toward autonomous data science.

</details>


### [319] [VAGEN: Reinforcing World Model Reasoning for Multi-Turn VLM Agents](https://arxiv.org/abs/2510.16907)
*Kangrui Wang,Pingyue Zhang,Zihan Wang,Yaning Gao,Linjie Li,Qineng Wang,Hanyang Chen,Chi Wan,Yiping Lu,Zhengyuan Yang,Lijuan Wang,Ranjay Krishna,Jiajun Wu,Li Fei-Fei,Yejin Choi,Manling Li*

Main category: cs.AI

TL;DR: VLM agents achieve 3x performance improvement through explicit visual state reasoning with world modeling rewards and bi-level GAE, outperforming proprietary models like GPT-5 and Claude 4.5.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of partial observability in VLM agents when transitioning from textual to visual observations, requiring robust world modeling capabilities.

Method: Architecturally enforce reasoning via RL formulated as POMDP, decomposing into state estimation and transition modeling with five reasoning strategies. Uses World Modeling Reward for dense supervision and Bi-Level GAE for credit assignment.

Result: 3B-parameter model achieves 0.82 score across five agent benchmarks, 3x improvement over untrained counterpart (0.21) and outperforms proprietary models (GPT-5: 0.75, Gemini 2.5 Pro: 0.67, Claude 4.5: 0.62).

Conclusion: Explicit visual state reasoning with task-dependent belief representations (Natural Language for semantics, Structured for precise control) enables effective VLM agent training through the VAGEN framework.

Abstract: A key challenge in training Vision-Language Model (VLM) agents, compared to
Language Model (LLM) agents, lies in the shift from textual states to complex
visual observations. This transition introduces partial observability and
demands robust world modeling. We ask: Can VLM agents construct internal world
models through explicit visual state reasoning? To address this question, we
architecturally enforce and reward the agent's reasoning process via
reinforcement learning (RL), formulating it as a Partially Observable Markov
Decision Process (POMDP). We find that decomposing the agent's reasoning into
State Estimation ("what is the current state?") and Transition Modeling ("what
comes next?") is critical for success, as demonstrated through five reasoning
strategies. Our investigation into how agents represent internal beliefs
reveals that the optimal representation is task-dependent: Natural Language
excels at capturing semantic relationships in general tasks, while Structured
formats are indispensable for precise manipulation and control. Building on
these insights, we design a World Modeling Reward that provides dense,
turn-level supervision for accurate state prediction, and introduce Bi-Level
General Advantage Estimation (Bi-Level GAE) for turn-aware credit assignment.
Through this form of visual state reasoning, a 3B-parameter model achieves a
score of 0.82 across five diverse agent benchmarks, representing a 3$\times$
improvement over its untrained counterpart (0.21) and outperforming proprietary
reasoning models such as GPT-5 (0.75), Gemini 2.5 Pro (0.67) and Claude 4.5
(0.62). All experiments are conducted within our VAGEN framework, a scalable
system for training and analyzing multi-turn VLM agents in diverse visual
environments. Code and data are publicly available at
https://vagen-ai.github.io.

</details>


### [320] [Offline Policy Evaluation of Multi-Turn LLM Health Coaching with Real Users](https://arxiv.org/abs/2510.17173)
*Melik Ozolcer,Sang Won Bae*

Main category: cs.AI

TL;DR: Evaluation of a tool-augmented LLM health coach shows uniform heavy-tool policies harm certain user subgroups, while early information-gain bonuses improve personalization and outcomes.


<details>
  <summary>Details</summary>
Motivation: To study real-world deployment of tool-augmented LLM health coaches and understand how different interaction policies affect diverse user subgroups, particularly identifying potential harms that average metrics might obscure.

Method: Used offline policy evaluation (OPE) with factorized decision heads (Tool/Style) on real user data (7 users, 280 rated turns), plus a lightweight simulator with hidden archetypes to test early information-gain bonuses.

Result: Uniform heavy-tool policies increase average value but harm low-health-literacy/high-self-efficacy users. Adding small early information-gain bonuses reliably shortens trait identification and improves goal success and pass@3 metrics.

Conclusion: Proposes an evaluation-first personalization approach: freeze the generator, learn subgroup-aware decision heads on typed rewards, and always report per-archetype metrics to surface subgroup harms that averages obscure.

Abstract: We study a web-deployed, tool-augmented LLM health coach with real users. In
a pilot with seven users (280 rated turns), offline policy evaluation (OPE)
over factorized decision heads (Tool/Style) shows that a uniform heavy-tool
policy raises average value on logs but harms specific subgroups, most notably
low-health-literacy/high-self-efficacy users. A lightweight simulator with
hidden archetypes further shows that adding a small early information-gain
bonus reliably shortens trait identification and improves goal success and
pass@3. Together, these early findings indicate an evaluation-first path to
personalization: freeze the generator, learn subgroup-aware decision heads on
typed rewards (objective tool outcomes and satisfaction), and always report
per-archetype metrics to surface subgroup harms that averages obscure.

</details>


### [321] [MIRAGE: Agentic Framework for Multimodal Misinformation Detection with Web-Grounded Reasoning](https://arxiv.org/abs/2510.17590)
*Mir Nafis Sharear Shopnil,Sharad Duwal,Abhishek Tyagi,Adiba Mahbub Proma*

Main category: cs.AI

TL;DR: MIRAGE is an agentic framework for multimodal misinformation detection that combines visual verification, cross-modal consistency analysis, retrieval-augmented fact checking, and calibrated judgment without requiring domain-specific training data.


<details>
  <summary>Details</summary>
Motivation: Manual fact-checking is overwhelmed by billions of daily multimodal posts, and supervised detection models fail to generalize across diverse manipulation tactics and require domain-specific training data.

Method: Decomposes multimodal verification into four sequential modules: visual veracity assessment (detects AI-generated images), cross-modal consistency analysis (identifies out-of-context repurposing), retrieval-augmented factual checking (grounds claims in web evidence through iterative question generation), and calibrated judgment module that integrates all signals.

Result: Achieves 81.65% F1 and 75.1% accuracy on MMFakeBench validation set, outperforming GPT-4V with MMD-Agent by 7.65 F1 points. Maintains 34.3% false positive rate vs 97.3% for judge-only baseline. Test set results confirm generalization with 81.44% F1 and 75.08% accuracy.

Conclusion: Decomposed agentic reasoning with web retrieval can match supervised detector performance without domain-specific training, enabling effective misinformation detection across modalities where labeled data is scarce.

Abstract: Misinformation spreads across web platforms through billions of daily
multimodal posts that combine text and images, overwhelming manual
fact-checking capacity. Supervised detection models require domain-specific
training data and fail to generalize across diverse manipulation tactics. We
present MIRAGE, an inference-time, model-pluggable agentic framework that
decomposes multimodal verification into four sequential modules: visual
veracity assessment detects AI-generated images, cross-modal consistency
analysis identifies out-of-context repurposing, retrieval-augmented factual
checking grounds claims in web evidence through iterative question generation,
and a calibrated judgment module integrates all signals. MIRAGE orchestrates
vision-language model reasoning with targeted web retrieval, outputs structured
and citation-linked rationales. On MMFakeBench validation set (1,000 samples),
MIRAGE with GPT-4o-mini achieves 81.65% F1 and 75.1% accuracy, outperforming
the strongest zero-shot baseline (GPT-4V with MMD-Agent at 74.0% F1) by 7.65
points while maintaining 34.3% false positive rate versus 97.3% for a
judge-only baseline. Test set results (5,000 samples) confirm generalization
with 81.44% F1 and 75.08% accuracy. Ablation studies show visual verification
contributes 5.18 F1 points and retrieval-augmented reasoning contributes 2.97
points. Our results demonstrate that decomposed agentic reasoning with web
retrieval can match supervised detector performance without domain-specific
training, enabling misinformation detection across modalities where labeled
data remains scarce.

</details>


### [322] [Reasoning Distillation and Structural Alignment for Improved Code Generation](https://arxiv.org/abs/2510.17598)
*Amir Jalilifard,Anderson de Rezende Rocha,Marcos Medeiros Raimundo*

Main category: cs.AI

TL;DR: Distilling reasoning capabilities from very large language models into smaller, more efficient models for improved code generation.


<details>
  <summary>Details</summary>
Motivation: Smaller language models lack the reasoning capabilities of very large models needed for effective code generation, which requires understanding solution-level structures rather than just token prediction.

Method: Train smaller models to emulate VLLM reasoning through structure-aware loss optimization that establishes correspondence between problem definitions and solutions, learning correct solution pathways.

Result: The fine-tuned model significantly outperforms baseline in pass@1, average data flow, and average syntax match metrics across MBPP, MBPP Plus, and HumanEval benchmarks.

Conclusion: Reasoning capabilities can be effectively distilled from large to small models through a simple, cheap process, enabling smaller models to grasp solution structures and generate better code.

Abstract: Effective code generation with language models hinges on two critical
factors: accurately understanding the intent of the prompt and generating code
that applies algorithmic reasoning to produce correct solutions capable of
passing diverse test cases while adhering to the syntax of the target
programming language. Unlike other language tasks, code generation requires
more than accurate token prediction; it demands comprehension of solution-level
and structural relationships rather than merely generating the most likely
tokens. very large language model (VLLM) are capable of generating detailed
steps toward the correct solution of complex tasks where reasoning is crucial
in solving the problem. Such reasoning capabilities may be absent in smaller
language models. Therefore, in this work, we distill the reasoning capabilities
of a VLLM into a smaller, more efficient model that is faster and cheaper to
deploy. Our approach trains the model to emulate the reasoning and
problem-solving abilities of the VLLM by learning to identify correct solution
pathways and establishing a structural correspondence between problem
definitions and potential solutions through a novel method of structure-aware
loss optimization. This enables the model to transcend token-level generation
and to deeply grasp the overarching structure of solutions for given problems.
Experimental results show that our fine-tuned model, developed through a cheap
and simple to implement process, significantly outperforms our baseline model
in terms of pass@1, average data flow, and average syntax match metrics across
the MBPP, MBPP Plus, and HumanEval benchmarks.

</details>


### [323] [LLM-as-a-Prophet: Understanding Predictive Intelligence with Prophet Arena](https://arxiv.org/abs/2510.17638)
*Qingchuan Yang,Simon Mahns,Sida Li,Anri Gu,Jibang Wu,Haifeng Xu*

Main category: cs.AI

TL;DR: This paper systematically evaluates LLMs' forecasting capabilities on real-world future events, finding they show promising predictive intelligence but face key bottlenecks in event recall, data understanding, and information aggregation speed.


<details>
  <summary>Details</summary>
Motivation: To investigate whether large language models (LLMs) can effectively forecast real-world future events, given their training on Internet-scale data and potential applications in finance and economics.

Method: Built Prophet Arena - a continuous evaluation benchmark that collects live forecasting tasks and decomposes them into distinct pipeline stages for controlled, large-scale experimentation.

Result: LLMs demonstrate impressive forecasting capabilities with small calibration errors, consistent prediction confidence, and promising market returns. However, they struggle with inaccurate event recalls, misunderstanding data sources, and slower information aggregation compared to markets.

Conclusion: LLMs show potential as forecasting tools but face significant bottlenecks that need addressing before achieving superior predictive intelligence through the 'LLM-as-a-Prophet' paradigm.

Abstract: Forecasting is not only a fundamental intellectual pursuit but also is of
significant importance to societal systems such as finance and economics. With
the rapid advances of large language models (LLMs) trained on Internet-scale
data, it raises the promise of employing LLMs to forecast real-world future
events, an emerging paradigm we call "LLM-as-a-Prophet". This paper
systematically investigates such predictive intelligence of LLMs. To this end,
we build Prophet Arena, a general evaluation benchmark that continuously
collects live forecasting tasks and decomposes each task into distinct pipeline
stages, in order to support our controlled and large-scale experimentation. Our
comprehensive evaluation reveals that many LLMs already exhibit impressive
forecasting capabilities, reflected in, e.g., their small calibration errors,
consistent prediction confidence and promising market returns. However, we also
uncover key bottlenecks towards achieving superior predictive intelligence via
LLM-as-a-Prophet, such as LLMs' inaccurate event recalls, misunderstanding of
data sources and slower information aggregation compared to markets when
resolution nears.

</details>


### [324] [Contextual Attention Modulation: Towards Efficient Multi-Task Adaptation in Large Language Models](https://arxiv.org/abs/2510.17705)
*Dayan Pan,Zhaoyang Fu,Jingyuan Wang,Xiao Han,Yue Zhu,Xiangyu Zhao*

Main category: cs.AI

TL;DR: Proposed Contextual Attention Modulation (CAM) and Hybrid CAM (HyCAM) framework for efficient multi-task adaptation in LLMs, achieving 3.65% average performance improvement while preserving general knowledge.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with multi-task adaptation, facing catastrophic forgetting and resource inefficiency in conventional fine-tuning, while existing parameter-efficient methods perform poorly in complex multi-task scenarios.

Method: Developed CAM mechanism to dynamically modulate self-attention representations, and HyCAM framework combining shared full-parameter CAM with specialized lightweight CAM modules using dynamic routing for adaptive knowledge fusion.

Result: Extensive experiments on heterogeneous tasks (question answering, code generation, logical reasoning) show significant outperformance over existing approaches with 3.65% average improvement.

Conclusion: CAM and HyCAM effectively balance knowledge retention with task-specific specialization, enabling more efficient and effective multi-task adaptation in LLMs.

Abstract: Large Language Models (LLMs) possess remarkable generalization capabilities
but struggle with multi-task adaptation, particularly in balancing knowledge
retention with task-specific specialization. Conventional fine-tuning methods
suffer from catastrophic forgetting and substantial resource consumption, while
existing parameter-efficient methods perform suboptimally in complex multi-task
scenarios. To address this, we propose Contextual Attention Modulation (CAM), a
novel mechanism that dynamically modulates the representations of
self-attention modules in LLMs. CAM enhances task-specific features while
preserving general knowledge, thereby facilitating more effective and efficient
adaptation. For effective multi-task adaptation, CAM is integrated into our
Hybrid Contextual Attention Modulation (HyCAM) framework, which combines a
shared, full-parameter CAM module with multiple specialized, lightweight CAM
modules, enhanced by a dynamic routing strategy for adaptive knowledge fusion.
Extensive experiments on heterogeneous tasks, including question answering,
code generation, and logical reasoning, demonstrate that our approach
significantly outperforms existing approaches, achieving an average performance
improvement of 3.65%. The implemented code and data are available to ease
reproducibility at https://github.com/Applied-Machine-Learning-Lab/HyCAM.

</details>


### [325] [Beyond Fixed Anchors: Precisely Erasing Concepts with Sibling Exclusive Counterparts](https://arxiv.org/abs/2510.16342)
*Tong Zhang,Ru Zhang,Jianyi Liu,Zhen Yang,Gongshen Liu*

Main category: cs.AI

TL;DR: SELECT is a dynamic anchor selection framework for concept erasure in text-to-image diffusion models that overcomes limitations of fixed anchor strategies by automatically discovering optimal anchors through a two-stage evaluation mechanism.


<details>
  <summary>Details</summary>
Motivation: Existing concept erasure methods rely on fixed anchor strategies which cause issues like concept re-emergence and erosion. The authors identified that erasure is inherently sensitive to anchor selection through causal tracing analysis.

Method: Proposed SELECT framework with a novel two-stage evaluation mechanism: (1) automatically discovers optimal anchors for precise erasure, (2) identifies critical boundary anchors to preserve related concepts. Based on the insight of Sibling Exclusive Concepts as superior anchors.

Result: Extensive evaluations show SELECT consistently outperforms existing baselines across key performance metrics, efficiently adapts to multiple erasure frameworks, and averages only 4 seconds for anchor mining of a single concept.

Conclusion: SELECT serves as a universal anchor solution that overcomes the limitations of fixed anchor strategies in concept erasure, providing more precise and effective erasure while preserving related concepts.

Abstract: Existing concept erasure methods for text-to-image diffusion models commonly
rely on fixed anchor strategies, which often lead to critical issues such as
concept re-emergence and erosion. To address this, we conduct causal tracing to
reveal the inherent sensitivity of erasure to anchor selection and define
Sibling Exclusive Concepts as a superior class of anchors. Based on this
insight, we propose \textbf{SELECT} (Sibling-Exclusive Evaluation for
Contextual Targeting), a dynamic anchor selection framework designed to
overcome the limitations of fixed anchors. Our framework introduces a novel
two-stage evaluation mechanism that automatically discovers optimal anchors for
precise erasure while identifying critical boundary anchors to preserve related
concepts. Extensive evaluations demonstrate that SELECT, as a universal anchor
solution, not only efficiently adapts to multiple erasure frameworks but also
consistently outperforms existing baselines across key performance metrics,
averaging only 4 seconds for anchor mining of a single concept.

</details>


### [326] [Seeing but Not Believing: Probing the Disconnect Between Visual Attention and Answer Correctness in VLMs](https://arxiv.org/abs/2510.17771)
*Zhining Liu,Ziyi Chen,Hui Liu,Chen Luo,Xianfeng Tang,Suhang Wang,Joy Zeng,Zhenwei Dai,Zhan Shi,Tianxin Wei,Benoit Dumoulin,Hanghang Tong*

Main category: cs.AI

TL;DR: VLMs often perceive visual evidence but fail to use it effectively, a phenomenon called "seeing but not believing". An attention-based intervention improves accuracy across multiple VLM families without training.


<details>
  <summary>Details</summary>
Motivation: To understand why VLMs fail on multimodal tasks even when correct visual evidence is present, and to determine if failures stem from perception issues or reasoning limitations.

Method: Examined layer-wise attention dynamics and introduced an inference-time intervention that highlights deep-layer evidence regions through selective attention-based masking.

Result: Found that shallow layers focus on text while deeper layers attend to localized evidence regions. The intervention consistently improved accuracy across LLaVA, Qwen, Gemma, and InternVL models.

Conclusion: VLMs encode reliable evidence internally but under-utilize it. Making such signals explicit can bridge the gap between perception and reasoning, advancing VLM reliability.

Abstract: Vision-Language Models (VLMs) achieve strong results on multimodal tasks such
as visual question answering, yet they can still fail even when the correct
visual evidence is present. In this work, we systematically investigate whether
these failures arise from not perceiving the evidence or from not leveraging it
effectively. By examining layer-wise attention dynamics, we find that shallow
layers focus primarily on text, while deeper layers sparsely but reliably
attend to localized evidence regions. Surprisingly, VLMs often perceive the
visual evidence when outputting incorrect answers, a phenomenon we term
``seeing but not believing'' that widely exists in major VLM families. Building
on this, we introduce an inference-time intervention that highlights deep-layer
evidence regions through selective attention-based masking. It requires no
training and consistently improves accuracy across multiple families, including
LLaVA, Qwen, Gemma, and InternVL. These results show that VLMs encode reliable
evidence internally but under-utilize it, making such signals explicit can
bridge the gap between perception and reasoning, advancing the diagnostic
understanding and reliability of VLMs.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [327] [Lung Cancer Classification from CT Images Using ResNet](https://arxiv.org/abs/2510.16310)
*Olajumoke O. Adekunle,Joseph D. Akinyemi,Khadijat T. Ladoja,Olufade F. W. Onifade*

Main category: eess.IV

TL;DR: A novel deep learning approach using ResNet50 with custom layers achieves 98.8% accuracy for multi-class lung cancer classification from CT images, significantly outperforming previous methods.


<details>
  <summary>Details</summary>
Motivation: Current automated systems for lung cancer classification from CT images have insufficient predictive efficacy for clinical adoption, and existing research mainly focuses on binary classification rather than distinguishing between different cancer subtypes.

Method: Used a pre-trained ResNet50 model with custom layers added on top, trained on 10,200 lung CT images from LC25000 dataset, validated on 2,550 images, and tested on 2,250 images. Applied meticulous hyperparameter fine-tuning for three-class classification (two malignant subtypes and one benign).

Result: Achieved a remarkable test accuracy of 98.8%, representing a notable enhancement over prior models on the same dataset.

Conclusion: The proposed deep learning approach demonstrates superior performance for multi-class lung cancer classification from CT images, showing potential for improved clinical diagnostic applications.

Abstract: Lung cancer, a malignancy originating in lung tissues, is commonly diagnosed
and classified using medical imaging techniques, particularly computed
tomography (CT). Despite the integration of machine learning and deep learning
methods, the predictive efficacy of automated systems for lung cancer
classification from CT images remains below the desired threshold for clinical
adoption. Existing research predominantly focuses on binary classification,
distinguishing between malignant and benign lung nodules. In this study, a
novel deep learning-based approach is introduced, aimed at an improved
multi-class classification, discerning various subtypes of lung cancer from CT
images. Leveraging a pre-trained ResNet model, lung tissue images were
classified into three distinct classes, two of which denote malignancy and one
benign. Employing a dataset comprising 15,000 lung CT images sourced from the
LC25000 histopathological images, the ResNet50 model was trained on 10,200
images, validated on 2,550 images, and tested on the remaining 2,250 images.
Through the incorporation of custom layers atop the ResNet architecture and
meticulous hyperparameter fine-tuning, a remarkable test accuracy of 98.8% was
recorded. This represents a notable enhancement over the performance of prior
models on the same dataset.

</details>


### [328] [Time-Embedded Algorithm Unrolling for Computational MRI](https://arxiv.org/abs/2510.16321)
*Junno Yun,Yaar Utku Alalar,Mehmet Akakaya*

Main category: eess.IV

TL;DR: Proposes time-embedded algorithm unrolling for MRI reconstruction that uses time-dependent neural networks and learnable parameters instead of shared networks across iterations, improving reconstruction quality without significantly increasing computational complexity.


<details>
  <summary>Details</summary>
Motivation: Traditional algorithm unrolling methods use shared proximal operator networks across iterations, which can introduce artifacts or blurring. Using distinct networks increases parameters and overfitting risk. Time-embedding addresses these issues by making networks and parameters iteration-dependent.

Method: Introduces time-embedded algorithm unrolling inspired by AMP and diffusion models. Frames iteration-dependent proximal operations and Onsager corrections as time-embedded neural networks, and makes data fidelity weights time-dependent learnable parameters.

Result: Extensive experiments on fastMRI dataset show the method effectively reduces aliasing artifacts and mitigates noise amplification, achieving state-of-the-art performance across various acceleration rates and datasets.

Conclusion: Time-embedding strategy improves reconstruction quality in algorithm unrolling approaches without significantly increasing computational complexity, and can be extended to existing unrolling methods.

Abstract: Algorithm unrolling methods have proven powerful for solving the regularized
least squares problem in computational magnetic resonance imaging (MRI). These
approaches unfold an iterative algorithm with a fixed number of iterations,
typically alternating between a neural network-based proximal operator for
regularization, a data fidelity operation and auxiliary updates with learnable
parameters. While the connection to optimization methods dictate that the
proximal operator network should be shared across unrolls, this can introduce
artifacts or blurring. Heuristically, practitioners have shown that using
distinct networks may be beneficial, but this significantly increases the
number of learnable parameters, making it challenging to prevent overfitting.
To address these shortcomings, by taking inspirations from proximal operators
with varying thresholds in approximate message passing (AMP) and the success of
time-embedding in diffusion models, we propose a time-embedded algorithm
unrolling scheme for inverse problems. Specifically, we introduce a novel
perspective on the iteration-dependent proximal operation in vector AMP (VAMP)
and the subsequent Onsager correction in the context of algorithm unrolling,
framing them as a time-embedded neural network. Similarly, the scalar weights
in the data fidelity operation and its associated Onsager correction are cast
as time-dependent learnable parameters. Our extensive experiments on the
fastMRI dataset, spanning various acceleration rates and datasets, demonstrate
that our method effectively reduces aliasing artifacts and mitigates noise
amplification, achieving state-of-the-art performance. Furthermore, we show
that our time-embedding strategy extends to existing algorithm unrolling
approaches, enhancing reconstruction quality without increasing the
computational complexity significantly.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [329] [Long Exposure: Accelerating Parameter-Efficient Fine-Tuning for LLMs under Shadowy Sparsity](https://arxiv.org/abs/2510.15964)
*Tuowei Wang,Kun Li,Zixu Hao,Donglin Bai,Ju Ren,Yaoxue Zhang,Ting Cao,Mao Yang*

Main category: cs.LG

TL;DR: Long Exposure is an efficient system that accelerates parameter-efficient fine-tuning (PEFT) for LLMs by addressing Shadowy Sparsity through three components: Shadowy-sparsity Exposer, Sequence-oriented Predictor, and Dynamic-aware Operator.


<details>
  <summary>Details</summary>
Motivation: The inefficiency of PEFT techniques presents significant challenges in terms of time investments and operational costs for adapting pre-trained LLMs to downstream tasks.

Method: The system addresses Shadowy Sparsity with three components: 1) Shadowy-sparsity Exposer uses prolonged sensing to capture sparsity details, 2) Sequence-oriented Predictor handles large sequences and evolving parameters, 3) Dynamic-aware Operator enables structured computation and coalesced memory access.

Result: Extensive evaluations show Long Exposure outperforms state-of-the-art methods with up to 2.49 speedup in end-to-end fine-tuning.

Conclusion: Long Exposure offers promising advancements in accelerating PEFT for LLMs by effectively addressing the previously overlooked Shadowy Sparsity phenomenon.

Abstract: The adaptation of pre-trained large language models (LLMs) to diverse
downstream tasks via fine-tuning is critical for numerous applications.
However, the inefficiency of parameter-efficient fine-tuning (PEFT) techniques
presents significant challenges in terms of time investments and operational
costs. In this paper, we first introduce a nuanced form of sparsity, termed
Shadowy Sparsity, which is distinctive in fine-tuning and has not been
adequately addressed for acceleration. Under Shadowy Sparsity, we propose Long
Exposure, an efficient system to accelerate PEFT for LLMs. Long Exposure
comprises three key components: Shadowy-sparsity Exposer employs a prolonged
sensing range to capture more sparsity details under shadowy sparsity;
Sequence-oriented Predictor provides efficient yet accurate predictions to
handle large sequence inputs and constantly-evolving parameters; and
Dynamic-aware Operator facilitates more structured computational patterns and
coalesced memory accesses, addressing dynamic sparse operations. Extensive
evaluations show that Long Exposure outperforms state-of-the-arts with up to a
$2.49\times$ speedup in end-to-end fine-tuning, offering promising advancements
in accelerating PEFT for LLMs.

</details>


### [330] [Bolster Hallucination Detection via Prompt-Guided Data Augmentation](https://arxiv.org/abs/2510.15977)
*Wenyun Li,Zheng Zhang,Dongmei Jiang,Xiangyuan Lan*

Main category: cs.LG

TL;DR: PALE is a novel framework for hallucination detection in LLMs that uses prompt-guided data augmentation and a Contrastive Mahalanobis Score to evaluate truthfulness without requiring human annotations.


<details>
  <summary>Details</summary>
Motivation: Address the scarcity of well-labeled datasets for hallucination detection in LLMs and improve reliability of LLM-generated content by detecting misleading or fabricated information.

Method: Uses prompt-guided responses from LLMs as data augmentation to generate both truthful and hallucinated data. Introduces Contrastive Mahalanobis Score (CM Score) based on modeling distributions of truthful/hallucinated data in activation space using matrix decomposition.

Result: PALE achieves superior hallucination detection performance, outperforming competitive baseline by 6.55% margin in extensive experiments.

Conclusion: The framework offers strong generalizability and practicality for real-world applications without requiring additional human annotations.

Abstract: Large language models (LLMs) have garnered significant interest in AI
community. Despite their impressive generation capabilities, they have been
found to produce misleading or fabricated information, a phenomenon known as
hallucinations. Consequently, hallucination detection has become critical to
ensure the reliability of LLM-generated content. One primary challenge in
hallucination detection is the scarcity of well-labeled datasets containing
both truthful and hallucinated outputs. To address this issue, we introduce
Prompt-guided data Augmented haLlucination dEtection (PALE), a novel framework
that leverages prompt-guided responses from LLMs as data augmentation for
hallucination detection. This strategy can generate both truthful and
hallucinated data under prompt guidance at a relatively low cost. To more
effectively evaluate the truthfulness of the sparse intermediate embeddings
produced by LLMs, we introduce an estimation metric called the Contrastive
Mahalanobis Score (CM Score). This score is based on modeling the distributions
of truthful and hallucinated data in the activation space. CM Score employs a
matrix decomposition approach to more accurately capture the underlying
structure of these distributions. Importantly, our framework does not require
additional human annotations, offering strong generalizability and practicality
for real-world applications. Extensive experiments demonstrate that PALE
achieves superior hallucination detection performance, outperforming the
competitive baseline by a significant margin of 6.55%.

</details>


### [331] [Can GRPO Help LLMs Transcend Their Pretraining Origin?](https://arxiv.org/abs/2510.15990)
*Kangqi Ni,Zhen Tan,Zijie Liu,Pingzhi Li,Tianlong Chen*

Main category: cs.LG

TL;DR: GRPO (Group Relative Policy Optimization) in RLVR improves LLM reasoning inconsistently - works well when target tasks align with pretraining biases but fails to discover novel solutions due to being a conservative reweighting scheme bounded by the base model's distribution.


<details>
  <summary>Details</summary>
Motivation: To understand why GRPO shows inconsistent reasoning improvements across different domains and determine the conditions under which it can generalize out-of-distribution.

Method: Theoretical proof that GRPO is a conservative reweighting scheme, plus controlled studies training transformers from scratch to evaluate generalization across reasoning depth, input length, token representation, and compositionality.

Result: GRPO only improves OOD when target tasks align with pretrained biases, while in-distribution gains diminish as performance saturates. It cannot discover completely novel solutions due to its conservative nature.

Conclusion: GRPO is not a universal reasoning enhancer but rather sharpens pretraining biases, motivating development of algorithms that can expand model capabilities beyond pretraining origins.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR), primarily driven by
the Group Relative Policy Optimization (GRPO) algorithm, is a leading approach
for enhancing the reasoning abilities of Large Language Models (LLMs). Despite
its wide adoption, GRPO's gains are often inconsistent; for instance, a model
may show significant improvement in one reasoning domain, like mathematics, yet
remain stagnant in another, such as medicine. This inconsistency raises a
critical question: under what conditions does GRPO improve reasoning and
generalize out-of-distribution (OOD)? We investigate this from a data
distribution perspective. We first prove theoretically that GRPO is a
conservative reweighting scheme, bounded by the base model's distribution and
thus unable to discover completely novel solutions. We further validate this in
carefully designed controlled studies by training transformers from scratch,
evaluating generalization across reasoning depth, input length, token
representation, and compositionality. Our results provide a principled
explanation for GRPO's boundaries: OOD improvement emerges only when the target
task aligns with the model's pretrained biases, while gains on in-distribution
(ID) tasks diminish as performance saturates. This reframes GRPO not as a
universal reasoning enhancer but as a tool that sharpens pretraining biases.
Our findings motivate future development of algorithms that can expand a
model's capabilities beyond its pretraining origin.

</details>


### [332] [Zeroth-Order Sharpness-Aware Learning with Exponential Tilting](https://arxiv.org/abs/2510.16157)
*Xuchen Gong,Tian Li*

Main category: cs.LG

TL;DR: This paper connects zeroth-order optimization with sharpness-aware minimization (SAM) through an exponential tilting objective that bridges average-loss and max-loss formulations, enabling gradient-free flat minima optimization.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between classic zeroth-order optimization (which optimizes for average loss) and SAM approaches (which focus on worst-case loss), providing a smooth transition between these objectives for better generalization.

Method: Proposes an exponential tilting objective that interpolates between average and max loss formulations, develops new zeroth-order algorithms to solve this soft SAM objective, and characterizes the sharpness notions of the tilted SAM framework.

Result: The approach serves as a gradient-free and memory-efficient alternative to SAM variants, achieving better generalization than vanilla zeroth-order baselines across classification, multiple choice QA, and language generation tasks.

Conclusion: The tilted SAM framework successfully connects zeroth-order optimization with SAM objectives, providing a flexible gradient-free method that achieves improved generalization performance across diverse downstream applications.

Abstract: Classic zeroth-order optimization approaches typically optimize for a
smoothed version of the original function, i.e., the expected objective under
randomly perturbed model parameters. This can be interpreted as encouraging the
loss values in the perturbation set to be small on average. Popular
sharpness-aware minimization (SAM) objectives, however, typically focus on the
largest loss within the neighborhood to arrive at flat minima more effectively.
In this work, we connect zeroth-order optimization (and its corresponding
objectives) with SAM approaches explicitly, through an exponential tilting
objective that provides a smooth transition between the average- and the
max-loss formulations. We explore new zeroth-order algorithms to solve a soft
SAM objective parameterized by a tilting parameter $t$. We provide precise
characterizations of the sharpness notions of the tilted SAM framework.
Practically, our approach can be used as a gradient-free and memory-efficient
alternative to SAM variants, and it achieves better generalization compared to
vanilla zeroth-order baselines on a wide range of downstream tasks, including
classification, multiple choice QA, and language generation.

</details>


### [333] [Alignment is Localized: A Causal Probe into Preference Layers](https://arxiv.org/abs/2510.16167)
*Archie Chaudhury*

Main category: cs.LG

TL;DR: This paper analyzes how language model alignment through human preference optimization works internally, finding that alignment is spatially localized in mid-layer activations rather than being diffusely distributed.


<details>
  <summary>Details</summary>
Motivation: While RLHF is widely used for aligning language models with human preferences, the internal mechanisms of how this alignment is achieved remain largely unknown and opaque.

Method: Systematic analysis using layer-wide causal patching between base and tuned models across human preference pairs, applied to Llama-3.2-1B, with LASSO regression to identify key layers.

Result: Alignment is spatially localized: mid-layer activations encode a distinct subspace that causally determines reward-consistent behavior, while early and late layers remain largely unaffected. Only a small number of layers have non-zero coefficients linking activation distances to reward gains.

Conclusion: Alignment from human-based preferential tuning is a directional, low-rank process rather than diffuse and parametric, at least for some language models.

Abstract: Reinforcement Learning frameworks, particularly those utilizing human
annotations, have become an increasingly popular method for preference
fine-tuning, where the outputs of a language model are tuned to match a certain
set of behavioral policies or guidelines. Reinforcement Learning through Human
Feedback (RLHF) is perhaps the most popular implementation of such a framework,
particularly for aligning LMs toward safety and human intent. However, the
internal workings of how such alignment is achieved remain largely opaque. In
this work, we systematically analyze preference optimization for language model
alignment by applying layer-wide causal patching between a base model and its
tuned counterpart across human preference pairs. We implement our methodology
on \textit{Llama-3.2-1B}, and find that alignment is spatially localized:
mid-layer activations encode a distinct subspace that causally determines
reward-consistent behavior, while early and late layers remain largely
unaffected. Utilizing LASSO regression, we also find that only a small number
of layers possess non-zero coefficients linking activation distances to reward
gains. Overall, we show that, at least for some language models, alignment from
human-based, preferential tuning is a directional, low rank process, rather
than diffuse and parameteric.

</details>


### [334] [WEBSERV: A Browser-Server Environment for Efficient Training of Reinforcement Learning-based Web Agents at Scale](https://arxiv.org/abs/2510.16252)
*Yuxuan Lu,Jing Huang,Hui Liu,Jiri Gesi,Yan Han,Shihan Fu,Tianqi Zheng,Dakuo Wang*

Main category: cs.LG

TL;DR: WEBSERV is a scalable web agent environment that combines realistic browser interaction with controllable server-side state, achieving SOTA performance while significantly reducing latency and storage requirements.


<details>
  <summary>Details</summary>
Motivation: Existing RL web agent environments have issues with excessive context, non-deterministic actions, and poor scalability for parallel training.

Method: Proposes WEBSERV with: 1) compact site-agnostic browser environment balancing context/action complexity, 2) scalable RL environment via efficient web-server launching/resetting.

Result: Achieved SOTA single-prompt success rates on WebArena tasks, reduced launch latency by ~5x, storage need by ~240x, enabling 200+ concurrent containers on single host.

Conclusion: WEBSERV provides an efficient and scalable solution for training web agents, addressing key limitations of existing environments while maintaining performance.

Abstract: Training and evaluation of Reinforcement Learning (RL) web agents have gained
increasing attention, yet a scalable and efficient environment that couples
realistic and robust browser-side interaction with controllable server-side
state at scale is still missing. Existing environments tend to have one or more
of the following issues: they overwhelm policy models with excessive and noisy
context; they perform actions non-deterministically without waiting for the UI
or network to stabilize; or they cannot scale isolated client-server containers
effectively for parallel RL rollouts. We propose WEBSERV, an environment that
includes 1) a compact, site-agnostic browser environment that balances context
and action complexity, and 2) a scalable RL environment via efficient launching
and resetting web-servers to enable scalable RL training and evaluation. We
evaluate WEBSERV on the shopping CMS and Gitlab tasks in WebArena, achieving
state-of-the-art single-prompt success rates while cutting launch latency by
~5x and storage need by ~240x, with a comparable memory footprint, enabling
200+ concurrent containers on a single host.

</details>


### [335] [Copy-Augmented Representation for Structure Invariant Template-Free Retrosynthesis](https://arxiv.org/abs/2510.16588)
*Jiaxi Zhuang,Yu Zhang,Aimin Zhou,Ying Qian*

Main category: cs.LG

TL;DR: C-SMILES introduces a novel molecular representation that decomposes SMILES into element-token pairs with special tokens to minimize editing distance between reactants and products, combined with copy-augmented mechanism and SMILES alignment guidance for improved retrosynthesis prediction.


<details>
  <summary>Details</summary>
Motivation: Current template-free methods for retrosynthesis prediction struggle to capture structural invariance in chemical reactions, where substantial molecular scaffolds remain unchanged, leading to large search spaces and reduced prediction accuracy.

Method: Proposes C-SMILES representation that decomposes traditional SMILES into element-token pairs with five special tokens, incorporates copy-augmented mechanism to preserve unchanged fragments, and uses SMILES alignment guidance to enhance attention consistency with ground-truth atom mappings.

Result: Achieved 67.2% top-1 accuracy on USPTO-50K and 50.8% on USPTO-FULL datasets, with 99.9% validity in generated molecules.

Conclusion: Establishes a new paradigm for structure-aware molecular generation with direct applications in computational drug discovery, significantly improving retrosynthesis prediction accuracy while maintaining high molecular validity.

Abstract: Retrosynthesis prediction is fundamental to drug discovery and chemical
synthesis, requiring the identification of reactants that can produce a target
molecule. Current template-free methods struggle to capture the structural
invariance inherent in chemical reactions, where substantial molecular
scaffolds remain unchanged, leading to unnecessarily large search spaces and
reduced prediction accuracy. We introduce C-SMILES, a novel molecular
representation that decomposes traditional SMILES into element-token pairs with
five special tokens, effectively minimizing editing distance between reactants
and products. Building upon this representation, we incorporate a
copy-augmented mechanism that dynamically determines whether to generate new
tokens or preserve unchanged molecular fragments from the product. Our approach
integrates SMILES alignment guidance to enhance attention consistency with
ground-truth atom mappings, enabling more chemically coherent predictions.
Comprehensive evaluation on USPTO-50K and large-scale USPTO-FULL datasets
demonstrates significant improvements: 67.2% top-1 accuracy on USPTO-50K and
50.8% on USPTO-FULL, with 99.9% validity in generated molecules. This work
establishes a new paradigm for structure-aware molecular generation with direct
applications in computational drug discovery.

</details>


### [336] [Zero-Shot Performance Prediction for Probabilistic Scaling Laws](https://arxiv.org/abs/2510.16743)
*Viktoria Schram,Markus Hiller,Daniel Beck,Trevor Cohn*

Main category: cs.LG

TL;DR: Multitask learning framework using latent variable multi-output Gaussian Processes to predict NLP model learning curves, enabling zero-shot prediction and probabilistic scaling laws with active learning.


<details>
  <summary>Details</summary>
Motivation: To enable informed decision-making for NLP models by predicting learning curves, reducing computational costs and dataset acquisition expenses.

Method: Formulates learning curve prediction as multitask learning with two-layer hierarchical data organization, using latent variable multi-output Gaussian Processes to model task correlations and support zero-shot prediction.

Result: Validated on three small-scale NLP datasets with up to 30 learning curves from nanoGPT, mBART/Transformer bilingual translation, and M2M100 multilingual translation models. Active learning reduces predictive uncertainty.

Conclusion: The approach facilitates probabilistic scaling laws at lower costs and provides predictions close to ground truth scaling laws through active learning queries.

Abstract: The prediction of learning curves for Natural Language Processing (NLP)
models enables informed decision-making to meet specific performance
objectives, while reducing computational overhead and lowering the costs
associated with dataset acquisition and curation. In this work, we formulate
the prediction task as a multitask learning problem, where each task's data is
modelled as being organized within a two-layer hierarchy. To model the shared
information and dependencies across tasks and hierarchical levels, we employ
latent variable multi-output Gaussian Processes, enabling to account for task
correlations and supporting zero-shot prediction of learning curves (LCs). We
demonstrate that this approach facilitates the development of probabilistic
scaling laws at lower costs. Applying an active learning strategy, LCs can be
queried to reduce predictive uncertainty and provide predictions close to
ground truth scaling laws. We validate our framework on three small-scale NLP
datasets with up to $30$ LCs. These are obtained from nanoGPT models, from
bilingual translation using mBART and Transformer models, and from multilingual
translation using M2M100 models of varying sizes.

</details>


### [337] [Utility-Diversity Aware Online Batch Selection for LLM Supervised Fine-tuning](https://arxiv.org/abs/2510.16882)
*Heming Zou,Yixiu Mao,Yun Qu,Qi Wang,Xiangyang Ji*

Main category: cs.LG

TL;DR: UDS is an efficient online batch selection framework for supervised fine-tuning that dynamically selects valuable data based on utility and diversity metrics without external resources.


<details>
  <summary>Details</summary>
Motivation: Current SFT methods are computationally expensive and can suffer from overfitting or bias amplification. Existing batch selection approaches rely only on utility metrics, need external resources, and add training overhead.

Method: UDS uses nuclear norm of logits matrix for utility and intra-sample diversity, and low-dimensional embeddings with memory buffer for inter-sample diversity. It eliminates need for external resources and extra backpropagation.

Result: UDS outperforms state-of-the-art online batch selection methods across multiple benchmarks under varying data budgets, and significantly reduces training time compared to full-dataset fine-tuning.

Conclusion: UDS provides an efficient and effective framework for data curation in SFT that balances utility and diversity without computational overhead.

Abstract: Supervised fine-tuning (SFT) is a commonly used technique to adapt large
language models (LLMs) to downstream tasks. In practice, SFT on a full dataset
is computationally expensive and sometimes suffers from overfitting or bias
amplification. This facilitates the rise of data curation in SFT, which
prioritizes the most valuable data to optimze. This work studies the online
batch selection family that dynamically scores and filters samples during the
training process. However, existing popular methods often (i) rely merely on
the utility of data to select a subset while neglecting other crucial factors
like diversity, (ii) rely on external resources such as reference models or
validation sets, and (iii) incur extra training time over full-dataset
training. To address these limitations, this work develops \textbf{UDS
(Utility-Diversity Sampling)}, a framework for efficient online batch selection
in SFT. UDS leverages the nuclear norm of the logits matrix to capture both
data utility and intra-sample diversity, while estimating inter-sample
diversity through efficient low-dimensional embedding comparisons with a
lightweight memory buffer of historical samples. Such a design eliminates the
need for external resources and unnecessary backpropagation, securing
computational efficiency. Experiments on multiple benchmarks demonstrate that
UDS consistently outperforms state-of-the-art online batch selection methods
under varying data budgets, and significantly reduces training time compared to
full-dataset fine-tuning. Code is available at https://github.com/gfyddha/UDS.

</details>


### [338] [Peering Inside the Black Box: Uncovering LLM Errors in Optimization Modelling through Component-Level Evaluation](https://arxiv.org/abs/2510.16943)
*Dania Refai,Moataz Ahmed*

Main category: cs.LG

TL;DR: A comprehensive evaluation framework for LLM-generated mathematical optimization formulations, assessing component-level metrics beyond traditional optimality gaps.


<details>
  <summary>Details</summary>
Motivation: Current evaluations of LLM-generated optimization formulations use coarse metrics that obscure structural or numerical errors, necessitating a more detailed component-level analysis.

Method: Developed a framework with metrics including precision/recall for variables/constraints, constraint/objective RMSE, and efficiency indicators. Evaluated GPT-5, LLaMA 3.1 Instruct, and DeepSeek Math across various optimization problems using six prompting strategies.

Result: GPT-5 consistently outperformed other models, with chain-of-thought, self-consistency, and modular prompting being most effective. High constraint recall and low constraint RMSE were key for solver performance.

Conclusion: Three principles for NLP-to-optimization modeling: complete constraint coverage prevents violations, minimizing constraint RMSE ensures solver accuracy, and concise outputs improve computational efficiency. The framework enables fine-grained diagnostic evaluation.

Abstract: Large language models (LLMs) are increasingly used to convert natural
language descriptions into mathematical optimization formulations. Current
evaluations often treat formulations as a whole, relying on coarse metrics like
solution accuracy or runtime, which obscure structural or numerical errors. In
this study, we present a comprehensive, component-level evaluation framework
for LLM-generated formulations. Beyond the conventional optimality gap, our
framework introduces metrics such as precision and recall for decision
variables and constraints, constraint and objective root mean squared error
(RMSE), and efficiency indicators based on token usage and latency. We evaluate
GPT-5, LLaMA 3.1 Instruct, and DeepSeek Math across optimization problems of
varying complexity under six prompting strategies. Results show that GPT-5
consistently outperforms other models, with chain-of-thought, self-consistency,
and modular prompting proving most effective. Analysis indicates that solver
performance depends primarily on high constraint recall and low constraint
RMSE, which together ensure structural correctness and solution reliability.
Constraint precision and decision variable metrics play secondary roles, while
concise outputs enhance computational efficiency. These findings highlight
three principles for NLP-to-optimization modeling: (i) Complete constraint
coverage prevents violations, (ii) minimizing constraint RMSE ensures
solver-level accuracy, and (iii) concise outputs improve computational
efficiency. The proposed framework establishes a foundation for fine-grained,
diagnostic evaluation of LLMs in optimization modeling.

</details>


### [339] [Leave It to the Experts: Detecting Knowledge Distillation via MoE Expert Signatures](https://arxiv.org/abs/2510.16968)
*Pingzhi Li,Morris Yu-Chao Huang,Zhen Tan,Qingquan Song,Jie Peng,Kai Zou,Yu Cheng,Kaidi Xu,Tianlong Chen*

Main category: cs.LG

TL;DR: A novel KD detection framework that identifies knowledge distillation by analyzing MoE structural habits and routing patterns, achieving >94% accuracy and robustness against prompt-based evasion.


<details>
  <summary>Details</summary>
Motivation: Existing KD detection methods are vulnerable to prompt engineering evasion, posing risks to intellectual property protection and LLM diversity.

Method: Exploits MoE structural habits transfer, especially internal routing patterns, and proposes Shadow-MoE for black-box detection via auxiliary distillation to construct proxy MoE representations.

Result: Achieves >94% detection accuracy across various scenarios, demonstrates strong robustness to prompt-based evasion, and outperforms existing baselines.

Conclusion: The framework effectively detects KD by leveraging structural habits transfer, providing a comprehensive benchmark for future research on LLM intellectual property protection.

Abstract: Knowledge Distillation (KD) accelerates training of large language models
(LLMs) but poses intellectual property protection and LLM diversity risks.
Existing KD detection methods based on self-identity or output similarity can
be easily evaded through prompt engineering. We present a KD detection
framework effective in both white-box and black-box settings by exploiting an
overlooked signal: the transfer of MoE "structural habits", especially internal
routing patterns. Our approach analyzes how different experts specialize and
collaborate across various inputs, creating distinctive fingerprints that
persist through the distillation process. To extend beyond the white-box setup
and MoE architectures, we further propose Shadow-MoE, a black-box method that
constructs proxy MoE representations via auxiliary distillation to compare
these patterns between arbitrary model pairs. We establish a comprehensive,
reproducible benchmark that offers diverse distilled checkpoints and an
extensible framework to facilitate future research. Extensive experiments
demonstrate >94% detection accuracy across various scenarios and strong
robustness to prompt-based evasion, outperforming existing baselines while
highlighting the structural habits transfer in LLMs.

</details>


### [340] [Forgetting to Forget: Attention Sink as A Gateway for Backdooring LLM Unlearning](https://arxiv.org/abs/2510.17021)
*Bingqi Shang,Yiwei Chen,Yihua Zhang,Bingquan Shen,Sijia Liu*

Main category: cs.LG

TL;DR: This paper introduces backdoor unlearning attacks on LLMs, where models appear to successfully unlearn but can recover forgotten knowledge when specific triggers are activated.


<details>
  <summary>Details</summary>
Motivation: To investigate whether unlearning processes in open-weight LLMs can be backdoored, creating models that behave normally but revert to pre-unlearned behavior when triggered.

Method: The authors design backdoor unlearning attacks by placing triggers at attention sink positions and aligning their attention values, leveraging the attention sink phenomenon in LLMs where shallow input tokens attract disproportionate attention.

Result: Extensive experiments show that attention-sink-guided backdoor unlearning reliably restores forgotten knowledge when triggers are present, while behaving indistinguishably from normally unlearned models when triggers are absent.

Conclusion: Attention sinks serve as effective gateways for backdoor unlearning attacks, highlighting a security vulnerability in LLM unlearning processes that can be exploited through strategic trigger placement.

Abstract: Large language model (LLM) unlearning has become a critical mechanism for
removing undesired data, knowledge, or behaviors from pre-trained models while
retaining their general utility. Yet, with the rise of open-weight LLMs, we
ask: can the unlearning process itself be backdoored, appearing successful
under normal conditions yet reverting to pre-unlearned behavior when a hidden
trigger is activated? Drawing inspiration from classical backdoor attacks that
embed triggers into training data to enforce specific behaviors, we investigate
backdoor unlearning, where models forget as intended in the clean setting but
recover forgotten knowledge when the trigger appears. We show that designing
such attacks presents unique challenges, hinging on where triggers are placed
and how backdoor training is reinforced. We uncover a strong link between
backdoor efficacy and the attention sink phenomenon, i.e., shallow input tokens
consistently attract disproportionate attention in LLMs. Our analysis reveals
that these attention sinks serve as gateways for backdoor unlearning: placing
triggers at sink positions and aligning their attention values markedly
enhances backdoor persistence. Extensive experiments validate these findings,
showing that attention-sink-guided backdoor unlearning reliably restores
forgotten knowledge in the presence of backdoor triggers, while behaving
indistinguishably from a normally unlearned model when triggers are absent.
Code is available at https://github.com/OPTML-Group/Unlearn-Backdoor.

</details>


### [341] [Do LLMs Recognize Your Latent Preferences? A Benchmark for Latent Information Discovery in Personalized Interaction](https://arxiv.org/abs/2510.17132)
*Ioannis Tsaknakis,Bingqing Song,Shuyu Gan,Dongyeop Kang,Alfredo Garcia,Gaowen Liu,Charles Fleming,Mingyi Hong*

Main category: cs.LG

TL;DR: LLMs can discover latent user preferences through conversation, but performance varies widely (32-98%) depending on task complexity, topic, and number of hidden attributes.


<details>
  <summary>Details</summary>
Motivation: LLMs are good at general text generation but struggle with personalized scenarios where users don't explicitly state all preferences. The paper investigates whether LLMs can uncover and reason about latent user information through dialogue.

Method: Created a unified benchmark with tri-agent framework (User, Assistant, Judge) across three settings: 20 Questions game, Personalized Question Answering, and Personalized Text Summarization, enabling turn-level evaluation of elicitation and adaptation.

Result: LLMs can surface latent information through dialogue, but success rates vary dramatically from 32% to 98% depending on task complexity, topic, and number of hidden attributes.

Conclusion: Effective preference inference remains an open frontier for building truly adaptive AI systems, and this benchmark provides the first systematic framework for studying latent information discovery in personalized interaction.

Abstract: Large Language Models (LLMs) excel at producing broadly relevant text, but
this generality becomes a limitation when user-specific preferences are
required, such as recommending restaurants or planning travel. In these
scenarios, users rarely articulate every preference explicitly; instead, much
of what they care about remains latent, waiting to be inferred. This raises a
fundamental question: Can LLMs uncover and reason about such latent information
through conversation?
  We address this problem by introducing a unified benchmark for evaluating
latent information discovery - the ability of LLMs to reveal and utilize hidden
user attributes through multi-turn interaction. The benchmark spans three
progressively realistic settings: the classic 20 Questions game, Personalized
Question Answering, and Personalized Text Summarization. All tasks share a
tri-agent framework (User, Assistant, Judge) enabling turn-level evaluation of
elicitation and adaptation. Our results reveal that while LLMs can indeed
surface latent information through dialogue, their success varies dramatically
with context: from 32% to 98%, depending on task complexity, topic, and number
of hidden attributes. This benchmark provides the first systematic framework
for studying latent information discovery in personalized interaction,
highlighting that effective preference inference remains an open frontier for
building truly adaptive AI systems.

</details>


### [342] [Soft-Masked Diffusion Language Models](https://arxiv.org/abs/2510.17206)
*Michael Hersche,Samuel Moor-Smith,Thomas Hofmann,Abbas Rahimi*

Main category: cs.LG

TL;DR: The paper introduces soft-masking (SM), a novel method for diffusion-based language models that dynamically blends mask token embeddings with top-k predicted tokens, improving performance over traditional binary masked diffusion approaches.


<details>
  <summary>Details</summary>
Motivation: Traditional masked diffusion models use a binary decision (retain mask or replace with predicted token) which discards valuable predictive information when masks are retained. This limitation reduces model efficiency and performance.

Method: Soft-masking dynamically blends mask token embeddings with embeddings of top-k predicted tokens from previous decoding steps for retained masks. This preserves context from earlier computations and allows partial information about masked tokens to propagate beyond single steps.

Result: Training a 169M parameter model with SM improved perplexity and MAUVE scores. Finetuning state-of-the-art diffusion models (Dream-7B and Dream-Coder-7B) with SM consistently improved performance across multiple coding benchmarks, especially in high-throughput settings.

Conclusion: Soft-masking provides a more informative prior for diffusion language models, enabling better performance by preserving predictive information that traditional binary masked diffusion approaches discard.

Abstract: Diffusion models have demonstrated strong potential in language modeling,
offering various advantages over traditional autoregressive approaches. Their
ability to generate and revise entire responses in parallel enables faster
generation and built-in self-correction mechanisms. Most modern diffusion-based
language models employ masked diffusion, where decoding involves iteratively
processing masked tokens based on a binary decision: either retaining the mask
or replacing it with the predicted token. However, this binary choice discards
valuable predictive information when the mask is retained. To address this
limitation, we introduce soft-masking (SM), a novel method that dynamically
blends the embedding of the mask token with the embeddings of the top-$k$
predicted tokens from the previous decoding step, for each retained mask. This
provides the model with a more informative prior, preserving context from
earlier computations and allowing partial information about masked tokens to
propagate beyond a single step. We propose a training methodology that adapts a
pretrained masked diffusion language model to incorporate SM. We demonstrate
that continuing pretraining a 169M parameter model with SM leads to improved
perplexity and MAUVE scores. Furthermore, we finetune two state-of-the-art
diffusion models, Dream-7B and Dream-Coder-7B, with SM. SM consistently
improves performance across multiple coding benchmarks, particularly in
high-throughput settings.

</details>


### [343] [LILO: Bayesian Optimization with Interactive Natural Language Feedback](https://arxiv.org/abs/2510.17671)
*Katarzyna Kobalczyk,Zhiyuan Jerry Lin,Benjamin Letham,Zhuokai Zhao,Maximilian Balandat,Eytan Bakshy*

Main category: cs.LG

TL;DR: A language-in-the-loop framework that uses LLMs to convert natural language feedback into scalar utilities for Bayesian Optimization, outperforming conventional methods.


<details>
  <summary>Details</summary>
Motivation: Feedback is essential for translating complex, nuanced goals into quantifiable optimization objectives, but existing methods have limited feedback formats and require domain-specific customization.

Method: Uses LLMs to convert unstructured natural language feedback into consistent utility signals, enabling flexible user priors without manual kernel design while maintaining BO's sample efficiency.

Result: Outperforms conventional BO baselines and LLM-only optimizers, particularly in feedback-limited regimes, while providing a more natural interface for decision makers.

Conclusion: The hybrid approach successfully bridges natural language feedback with principled optimization, offering both improved performance and user-friendly interaction.

Abstract: For many real-world applications, feedback is essential in translating
complex, nuanced, or subjective goals into quantifiable optimization
objectives. We propose a language-in-the-loop framework that uses a large
language model (LLM) to convert unstructured feedback in the form of natural
language into scalar utilities to conduct BO over a numeric search space.
Unlike preferential BO, which only accepts restricted feedback formats and
requires customized models for each domain-specific problem, our approach
leverages LLMs to turn varied types of textual feedback into consistent utility
signals and to easily include flexible user priors without manual kernel
design. At the same time, our method maintains the sample efficiency and
principled uncertainty quantification of BO. We show that this hybrid method
not only provides a more natural interface to the decision maker but also
outperforms conventional BO baselines and LLM-only optimizers, particularly in
feedback-limited regimes.

</details>


### [344] [Mapping Post-Training Forgetting in Language Models at Scale](https://arxiv.org/abs/2510.17776)
*Jackson Harmon,Andreas Hochlehnert,Matthias Bethge,Ameya Prabhu*

Main category: cs.LG

TL;DR: The paper proposes a sample-wise framework to measure knowledge forgetting and backward transfer during post-training of language models, showing varied effects across different post-training stages and data scales.


<details>
  <summary>Details</summary>
Motivation: To better understand how scaled post-training affects pretrained knowledge in language models, since traditional task averages obscure important changes and not all forgetting is equal.

Method: Proposes a sample-wise paradigm that counts 1->0 transitions (forgetting) and 0->1 transitions (backward transfer), with chance-adjusted variants for multiple-choice benchmarks to subtract random guessing effects.

Result: Analysis shows: domain-continual pretraining causes moderate forgetting with low-to-moderate backward transfer; RL/SFT post-training yields moderate-to-large backward transfer on math/logic with low-to-moderate forgetting; effects are sensitive to data scale; model merging doesn't reliably mitigate forgetting.

Conclusion: The framework provides a practical way to map how post-training alters pretrained knowledge, enabling progress towards generally capable AI systems.

Abstract: Scaled post-training now drives many of the largest capability gains in
language models (LMs), yet its effect on pretrained knowledge remains poorly
understood. Not all forgetting is equal: Forgetting one fact (e.g., a U.S.
president or an API call) does not "average out" by recalling another. Hence,
we propose a sample-wise paradigm to measure what is forgotten and when
backward transfer occurs. Our metric counts 1->0 transitions (correct before
post-training, incorrect after) to quantify forgetting and 0->1 transitions to
quantify backward transfer. Traditional task averages conflate these effects
and obscure large changes. For multiple-choice benchmarks, we add
chance-adjusted variants that subtract the expected contribution of random
guessing from pre- and post-training accuracies. We apply this framework across
post-training stages, model sizes, and data scales. Our large-scale analysis
shows that: (1) Domain-continual pretraining induces moderate forgetting with
low-to-moderate backward transfer; (2) RL/SFT post-training applied to base
models and Instruction tuning yields moderate-to-large backward transfer on
math and logic with overall low-to-moderate forgetting; (3) Applying RL/SFT to
instruction-tuned models is sensitive on data scale: at small scales, both
forgetting and backward transfer are small; at larger scales, effects are mixed
and warrant further study with better controls; (4) Model merging does not
reliably mitigate forgetting. Overall, our framework offers a practical
yardstick for mapping how post-training alters pretrained knowledge at scale --
enabling progress towards generally capable AI systems.

</details>


### [345] [FedPURIN: Programmed Update and Reduced INformation for Sparse Personalized Federated Learning](https://arxiv.org/abs/2510.16065)
*Lunchen Xie,Zehua He,Qingjiang Shi*

Main category: cs.LG

TL;DR: FedPURIN is a communication-efficient personalized federated learning framework that uses integer programming to identify critical parameters for transmission, achieving significant communication reduction while maintaining competitive performance.


<details>
  <summary>Details</summary>
Motivation: Address the suboptimal communication efficiency in existing PFL methods that sustain substantial communication burdens, which impedes practical deployment, especially for edge intelligence systems with heterogeneous data.

Method: Proposes FedPURIN framework that strategically identifies critical parameters for transmission through integer programming formulation, integrated into a sparse aggregation scheme to reduce communication overhead.

Result: Comprehensive evaluations on standard image classification benchmarks under varied non-IID conditions demonstrate competitive performance relative to state-of-the-art methods with quantifiable communication reduction through sparse aggregation.

Conclusion: FedPURIN establishes a new paradigm for communication-efficient PFL, particularly advantageous for edge intelligence systems operating with heterogeneous data sources, bridging the gap between model performance and communication efficiency.

Abstract: Personalized Federated Learning (PFL) has emerged as a critical research
frontier addressing data heterogeneity issue across distributed clients. Novel
model architectures and collaboration mechanisms are engineered to accommodate
statistical disparities while producing client-specific models. Parameter
decoupling represents a promising paradigm for maintaining model performance in
PFL frameworks. However, the communication efficiency of many existing methods
remains suboptimal, sustaining substantial communication burdens that impede
practical deployment. To bridge this gap, we propose Federated Learning with
Programmed Update and Reduced INformation (FedPURIN), a novel framework that
strategically identifies critical parameters for transmission through an
integer programming formulation. This mathematically grounded strategy is
seamlessly integrated into a sparse aggregation scheme, achieving a significant
communication reduction while preserving the efficacy. Comprehensive
evaluations on standard image classification benchmarks under varied non-IID
conditions demonstrate competitive performance relative to state-of-the-art
methods, coupled with quantifiable communication reduction through sparse
aggregation. The framework establishes a new paradigm for
communication-efficient PFL, particularly advantageous for edge intelligence
systems operating with heterogeneous data sources.

</details>


### [346] [Needles in the Landscape: Semi-Supervised Pseudolabeling for Archaeological Site Discovery under Label Scarcity](https://arxiv.org/abs/2510.16814)
*Simon Jaxy,Anton Theys,Patrick Willett,W. Chris Carleton,Ralf Vandam,Pieter Libin*

Main category: cs.LG

TL;DR: A semi-supervised deep learning approach using positive-unlabeled learning and dynamic pseudolabeling with CRF refinement for archaeological predictive modeling, achieving state-of-the-art performance on geospatial and satellite data.


<details>
  <summary>Details</summary>
Motivation: To address structural label scarcity in archaeology where positives are rare and most locations are unlabeled, requiring methods that can work with limited labeled data.

Method: Semi-supervised positive-unlabeled learning with dynamic pseudolabeling refined using Conditional Random Field (CRF) implemented via RNN to handle severe class imbalance in semantic segmentation.

Result: Performs on par with state-of-the-art LAMAP on DEM data with higher Dice scores, maintains performance on raw satellite imagery with improved interpretability through stratified k-fold cross-validation.

Conclusion: Semi-supervised learning offers a promising approach for identifying undiscovered archaeological sites across large, sparsely annotated landscapes.

Abstract: Archaeological predictive modelling estimates where undiscovered sites are
likely to occur by combining known locations with environmental, cultural, and
geospatial variables. We address this challenge using a deep learning approach
but must contend with structural label scarcity inherent to archaeology:
positives are rare, and most locations are unlabeled. To address this, we adopt
a semi-supervised, positive-unlabeled (PU) learning strategy, implemented as a
semantic segmentation model and evaluated on two datasets covering a
representative range of archaeological periods. Our approach employs dynamic
pseudolabeling, refined with a Conditional Random Field (CRF) implemented via
an RNN, increasing label confidence under severe class imbalance. On a
geospatial dataset derived from a digital elevation model (DEM), our model
performs on par with the state-of-the-art, LAMAP, while achieving higher Dice
scores. On raw satellite imagery, assessed end-to-end with stratified k-fold
cross-validation, it maintains performance and yields predictive surfaces with
improved interpretability. Overall, our results indicate that semi-supervised
learning offers a promising approach to identifying undiscovered sites across
large, sparsely annotated landscapes.

</details>


### [347] [Fly-CL: A Fly-Inspired Framework for Enhancing Efficient Decorrelation and Reduced Training Time in Pre-trained Model-based Continual Representation Learning](https://arxiv.org/abs/2510.16877)
*Heming Zou,Yunliang Zang,Wutong Xu,Xiangyang Ji*

Main category: cs.LG

TL;DR: Fly-CL is a bio-inspired continual learning framework that uses a nearly-frozen pretrained model and reframes parameter updates as similarity matching, inspired by the fly olfactory circuit. It resolves multicollinearity issues while achieving fast training and competitive performance.


<details>
  <summary>Details</summary>
Motivation: To address catastrophic forgetting in continual learning while overcoming multicollinearity problems in similarity matching and computational limitations of advanced methods for real-time applications.

Method: Proposes Fly-CL framework compatible with various pretrained backbones, inspired by the fly olfactory circuit. It progressively resolves multicollinearity through biologically inspired design and enables effective similarity matching with low time complexity.

Result: Fly-CL substantially reduces training time while achieving performance comparable to or exceeding state-of-the-art methods. Extensive experiments across diverse architectures and data regimes validate its effectiveness.

Conclusion: The bio-inspired Fly-CL framework successfully addresses multicollinearity in continual representation learning, providing efficient and effective similarity matching with theoretical guarantees and practical benefits for real-time applications.

Abstract: Using a nearly-frozen pretrained model, the continual representation learning
paradigm reframes parameter updates as a similarity-matching problem to
mitigate catastrophic forgetting. However, directly leveraging pretrained
features for downstream tasks often suffers from multicollinearity in the
similarity-matching stage, and more advanced methods can be computationally
prohibitive for real-time, low-latency applications. Inspired by the fly
olfactory circuit, we propose Fly-CL, a bio-inspired framework compatible with
a wide range of pretrained backbones. Fly-CL substantially reduces training
time while achieving performance comparable to or exceeding that of current
state-of-the-art methods. We theoretically show how Fly-CL progressively
resolves multicollinearity, enabling more effective similarity matching with
low time complexity. Extensive simulation experiments across diverse network
architectures and data regimes validate Fly-CL's effectiveness in addressing
this challenge through a biologically inspired design. Code is available at
https://github.com/gfyddha/Fly-CL.

</details>


### [348] [Domain Generalizable Continual Learning](https://arxiv.org/abs/2510.16914)
*Hongwei Yan,Guanglong Sun,Zhiqi Kang,Yi Zhong,Liyuan Wang*

Main category: cs.LG

TL;DR: The paper introduces Domain Generalizable Continual Learning (DGCL), a new setting where models learn sequential tasks from single domains and must generalize across all encountered domains. It proposes DoT, a method that disentangles semantic and domain information and adaptively transforms representations for better generalization.


<details>
  <summary>Details</summary>
Motivation: Current continual learning methods assume identical training and testing domains for each task, which fails in real-world scenarios where models need to generalize across diverse, unseen domains while learning sequentially.

Method: Proposes adaptive Domain Transformation (DoT), inspired by brain theory, which disentangles semantic- and domain-relevant information and adaptively transforms task representations across domains for output alignment.

Result: DoT significantly improves state-of-the-art CL baselines in DGCL settings under both full parameter tuning and parameter-efficient tuning, showing effective domain generalization and resource efficiency.

Conclusion: DoT is an effective plug-in strategy for DGCL that enables models to accumulate domain-generalizable knowledge while maintaining resource efficiency through lightweight implementation.

Abstract: To adapt effectively to dynamic real-world environments, intelligent systems
must continually acquire new skills while generalizing them to diverse, unseen
scenarios. Here, we introduce a novel and realistic setting named domain
generalizable continual learning (DGCL): a model learns sequential tasks with
each involving a single domain, aiming to perform well across all encountered
tasks and domains. This setting poses unique challenges in acquiring,
retaining, and leveraging both semantic- and domain-relevant information for
robust generalization. Although state-of-the-art continual learning (CL)
methods have employed pre-trained models (PTMs) to enhance task-specific
generalization, they typically assume identical training and testing domains
for each task and therefore perform poorly in DGCL. To this end, we propose
adaptive Domain Transformation (DoT), an innovative PTMs-based approach
tailored to DGCL. Inspired by the distributed-plus-hub theory of the human
brain, DoT disentangles semantic- and domain-relevant information in
representation learning, and adaptively transforms task representations across
various domains for output alignment, ensuring balanced and generalized
predictions. DoT serves as a plug-in strategy that greatly facilitates
state-of-the-art CL baselines under both full parameter tuning and
parameter-efficient tuning paradigms in DGCL, validated by extensive
experiments. Also, DoT is shown to accumulate domain-generalizable knowledge
from DGCL, and ensure resource efficiency with a lightweight implementation.

</details>


### [349] [Matricial Free Energy as a Gaussianizing Regularizer: Enhancing Autoencoders for Gaussian Code Generation](https://arxiv.org/abs/2510.17120)
*Rishi Sonthalia,Raj Rao Nadakuditi*

Main category: cs.LG

TL;DR: A novel regularization method for autoencoders using matricial free energy that enforces Gaussian-like code distributions through singular value optimization.


<details>
  <summary>Details</summary>
Motivation: To develop a regularization scheme that ensures autoencoder codes have Gaussian-like distributions, improving generalization and applicability to inverse problems.

Method: Defines a differentiable loss function based on matricial free energy that minimizes when code matrix singular values match those of a sculpted random matrix with i.i.d. Gaussian entries. Uses stochastic gradient descent for training.

Result: Empirical simulations show the method produces Gaussian-like codes that generalize well across training and test sets. A matricidal free energy maximizing autoencoder reliably generates Gaussian codes.

Conclusion: The matricial free energy regularization successfully enforces Gaussian code distributions, enabling effective application to underdetermined inverse problems.

Abstract: We introduce a novel regularization scheme for autoencoders based on
matricial free energy. Our approach defines a differentiable loss function in
terms of the singular values of the code matrix (code dimension x batch size).
From the standpoint of free probability an d random matrix theory, this loss
achieves its minimum when the singular value distribution of the code matrix
coincides with that of an appropriately sculpted random metric with i.i.d.
Gaussian entries. Empirical simulations demonstrate that minimizing the
negative matricial free energy through standard stochastic gradient-based
training yields Gaussian-like codes that generalize across training and test
sets. Building on this foundation, we propose a matricidal free energy
maximizing autoencoder that reliably produces Gaussian codes and show its
application to underdetermined inverse problems.

</details>


### [350] [Latent Spaces Beyond Synthesis: From GANs to Diffusion Models](https://arxiv.org/abs/2510.17383)
*Ludovica Schaerf*

Main category: cs.LG

TL;DR: This paper analyzes how internal representations in generative visual models have evolved from GANs/VAEs to diffusion models, proposing a distinction between strict synthesis (compact latent space) and broad synthesis (distributed representations across layers).


<details>
  <summary>Details</summary>
Motivation: To understand the conceptual shift in generative AI from unified latent spaces to distributed representations, challenging traditional assumptions about how models internally represent and synthesize content.

Method: Close readings of model architectures and targeted experiments that intervene in layerwise representations of diffusion models to analyze how representational labor is distributed.

Result: Diffusion models fragment the burden of representation across layers, challenging assumptions of unified internal space and demonstrating that generative AI operates through emergent configurations of specialized processes.

Conclusion: Generative AI should be understood not as direct synthesis of content but as emergent configuration of specialized processes, requiring reorientation of how we conceptualize internal representations in AI models.

Abstract: This paper examines the evolving nature of internal representations in
generative visual models, focusing on the conceptual and technical shift from
GANs and VAEs to diffusion-based architectures. Drawing on Beatrice Fazi's
account of synthesis as the amalgamation of distributed representations, we
propose a distinction between "synthesis in a strict sense", where a compact
latent space wholly determines the generative process, and "synthesis in a
broad sense," which characterizes models whose representational labor is
distributed across layers. Through close readings of model architectures and a
targeted experimental setup that intervenes in layerwise representations, we
show how diffusion models fragment the burden of representation and thereby
challenge assumptions of unified internal space. By situating these findings
within media theoretical frameworks and critically engaging with metaphors such
as the latent space and the Platonic Representation Hypothesis, we argue for a
reorientation of how generative AI is understood: not as a direct synthesis of
content, but as an emergent configuration of specialized processes.

</details>


### [351] [MILES: Modality-Informed Learning Rate Scheduler for Balancing Multimodal Learning](https://arxiv.org/abs/2510.17394)
*Alejandro Guerra-Manzanares,Farah E. Shamout*

Main category: cs.LG

TL;DR: MILES is a learning rate scheduler that dynamically adjusts rates based on modality utilization differences to balance multimodal learning and prevent modality overfitting.


<details>
  <summary>Details</summary>
Motivation: Multimodal networks often suffer from modality overfitting where they rely excessively on one modality, leading to sub-optimal performance that doesn't fully leverage the benefits of multimodal learning.

Method: MILES uses modality-wise conditional utilization rates during training to dynamically adjust learning rates, balancing the speed of learning from each modality in multimodal joint fusion models.

Result: MILES outperforms seven state-of-the-art baselines across four multimodal tasks and fusion methods, effectively balancing modality usage and improving both multimodal performance and unimodal encoder strength.

Conclusion: Balancing multimodal learning through dynamic learning rate adjustment significantly improves model performance and creates stronger modality encoders that work well with unimodal samples or missing modalities.

Abstract: The aim of multimodal neural networks is to combine diverse data sources,
referred to as modalities, to achieve enhanced performance compared to relying
on a single modality. However, training of multimodal networks is typically
hindered by modality overfitting, where the network relies excessively on one
of the available modalities. This often yields sub-optimal performance,
hindering the potential of multimodal learning and resulting in marginal
improvements relative to unimodal models. In this work, we present the
Modality-Informed Learning ratE Scheduler (MILES) for training multimodal joint
fusion models in a balanced manner. MILES leverages the differences in
modality-wise conditional utilization rates during training to effectively
balance multimodal learning. The learning rate is dynamically adjusted during
training to balance the speed of learning from each modality by the multimodal
model, aiming for enhanced performance in both multimodal and unimodal
predictions. We extensively evaluate MILES on four multimodal joint fusion
tasks and compare its performance to seven state-of-the-art baselines. Our
results show that MILES outperforms all baselines across all tasks and fusion
methods considered in our study, effectively balancing modality usage during
training. This results in improved multimodal performance and stronger modality
encoders, which can be leveraged when dealing with unimodal samples or absent
modalities. Overall, our work highlights the impact of balancing multimodal
learning on improving model performance.

</details>


### [352] [ZACH-ViT: A Zero-Token Vision Transformer with ShuffleStrides Data Augmentation for Robust Lung Ultrasound Classification](https://arxiv.org/abs/2510.17650)
*Athanasios Angelakis,Amne Mousa,Micah L. A. Heldeweg,Laurens A. Biesheuvel,Mark A. Haaksma,Jasper M. Smit,Pieter R. Tuinman,Paul W. G. Elbers*

Main category: cs.LG

TL;DR: ZACH-ViT is a compact Vision Transformer that achieves state-of-the-art performance in classifying cardiogenic pulmonary edema from lung ultrasound videos, outperforming larger models through permutation-invariant design and data augmentation.


<details>
  <summary>Details</summary>
Motivation: Differentiating cardiogenic pulmonary edema from non-cardiogenic conditions in lung ultrasound videos is challenging due to visual variability and overlapping patterns, requiring robust automated classification methods.

Method: ZACH-ViT removes positional embeddings and CLS token for full permutation-invariance, uses ShuffleStrides Data Augmentation to permute sequences while preserving anatomical validity, and operates with only 0.25M parameters.

Result: Achieved highest validation and test ROC-AUC (0.80 and 0.79) with balanced sensitivity (0.60) and specificity (0.91), while competing models collapsed to trivial classification. Trains 1.35x faster than Minimal ViT with 2.5x fewer parameters.

Conclusion: Aligning architectural design with data structure can outperform scale in small-data medical imaging, supporting real-time clinical deployment.

Abstract: Differentiating cardiogenic pulmonary oedema (CPE) from non-cardiogenic and
structurally normal lungs in lung ultrasound (LUS) videos remains challenging
due to the high visual variability of non-cardiogenic inflammatory patterns
(NCIP/ARDS-like), interstitial lung disease, and healthy lungs. This
heterogeneity complicates automated classification as overlapping B-lines and
pleural artefacts are common. We introduce ZACH-ViT (Zero-token Adaptive
Compact Hierarchical Vision Transformer), a 0.25 M-parameter Vision Transformer
variant that removes both positional embeddings and the [CLS] token, making it
fully permutation-invariant and suitable for unordered medical image data. To
enhance generalization, we propose ShuffleStrides Data Augmentation (SSDA),
which permutes probe-view sequences and frame orders while preserving
anatomical validity. ZACH-ViT was evaluated on 380 LUS videos from 95
critically ill patients against nine state-of-the-art baselines. Despite the
heterogeneity of the non-cardiogenic group, ZACH-ViT achieved the highest
validation and test ROC-AUC (0.80 and 0.79) with balanced sensitivity (0.60)
and specificity (0.91), while all competing models collapsed to trivial
classification. It trains 1.35x faster than Minimal ViT (0.62M parameters) with
2.5x fewer parameters, supporting real-time clinical deployment. These results
show that aligning architectural design with data structure can outperform
scale in small-data medical imaging.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [353] [Investigating the Association Between Text-Based Indications of Foodborne Illness from Yelp Reviews and New York City Health Inspection Outcomes (2023)](https://arxiv.org/abs/2510.16334)
*Eden Shaveet,Crystal Su,Daniel Hsu,Luis Gravano*

Main category: cs.IR

TL;DR: This paper compares foodborne illness signals from Yelp reviews using HSAN classifier with official NYC restaurant inspection scores, finding minimal correlation between the two data sources at the Census tract level.


<details>
  <summary>Details</summary>
Motivation: Restaurants are critical outbreak investigation venues, and social media platforms provide abundant user-generated content that can offer timely public health signals, while formal reporting channels are limited.

Method: Analyzed signals from Yelp reviews using Hierarchical Sigmoid Attention Network (HSAN) classifier and compared them with official NYC DOHMH restaurant inspection outcomes from 2023, evaluating correlations at Census tract level and mapping spatial patterns.

Result: Found minimal correlation between HSAN signals and inspection scores at tract level, and no significant differences by number of C-graded restaurants.

Conclusion: The study discusses implications and outlines next steps toward address-level analyses, suggesting the need for more granular investigation of the relationship between social media signals and official inspection data.

Abstract: Foodborne illnesses are gastrointestinal conditions caused by consuming
contaminated food. Restaurants are critical venues to investigate outbreaks
because they share sourcing, preparation, and distribution of foods. Public
reporting of illness via formal channels is limited, whereas social media
platforms host abundant user-generated content that can provide timely public
health signals. This paper analyzes signals from Yelp reviews produced by a
Hierarchical Sigmoid Attention Network (HSAN) classifier and compares them with
official restaurant inspection outcomes issued by the New York City Department
of Health and Mental Hygiene (NYC DOHMH) in 2023. We evaluate correlations at
the Census tract level, compare distributions of HSAN scores by prevalence of
C-graded restaurants, and map spatial patterns across NYC. We find minimal
correlation between HSAN signals and inspection scores at the tract level and
no significant differences by number of C-graded restaurants. We discuss
implications and outline next steps toward address-level analyses.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [354] [U-Codec: Ultra Low Frame-rate Neural Speech Codec for Fast High-fidelity Speech Generation](https://arxiv.org/abs/2510.16718)
*Xusheng Yang,Long Zhou,Wenfu Wang,Kai Hu,Shulin Feng,Chenxing Li,Meng Yu,Dong Yu,Yuexian Zou*

Main category: cs.SD

TL;DR: U-Codec is an ultra low frame-rate neural speech codec that achieves high-fidelity reconstruction at 5Hz frame rate, improving TTS inference speed by 3x while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: Extreme compression at 5Hz typically causes severe intelligibility and spectral detail loss, so the goal is to enable fast speech generation while preserving quality at ultra low frame rates.

Method: Transformer-based inter-frame long-term dependency module, systematic exploration of RVQ depth and codebook size, and integration into LLM-based TTS with global and local hierarchical architecture for multi-layer token dependencies.

Result: U-Codec improves LLM-based TTS inference speed by around 3x over high-frame-rate codecs while maintaining similarity and naturalness, extending from 3-layer RVQ at 50Hz to 32-layer RVQ at 5Hz.

Conclusion: The results validate the feasibility of using highly compressed 5Hz discrete tokens for fast and high-fidelity speech synthesis.

Abstract: We propose \textbf{U-Codec}, an \textbf{U}ltra low frame-rate neural speech
\textbf{Codec} that achieves high-fidelity reconstruction and fast speech
generation at an extremely low frame-rate of 5Hz (5 frames per second). Extreme
compression at 5Hz typically leads to severe intelligibility and spectral
detail loss, we introduce a Transformer-based inter-frame long-term dependency
module and systematically explore residual vector quantization (RVQ) depth and
codebook size to identify optimal configurations. Moreover, we apply U-Codec
into a large language model (LLM)-based auto-regressive TTS model, which
leverages global and local hierarchical architecture to effectively capture
dependencies across multi-layer tokens. We extend LLM-based TTS from 3-layer
RVQ at 50Hz to 32-layer RVQ at 5Hz. Experimental results demonstrate that
U-Codec improves LLM-based TTS inference speed by around 3 $\times$ over
high-frame-rate codecs while maintaining similarity and naturalness. These
results validate the feasibility of using highly compressed 5Hz discrete tokens
for fast and high-fidelity speech synthesis.

</details>


### [355] [Investigating Safety Vulnerabilities of Large Audio-Language Models Under Speaker Emotional Variations](https://arxiv.org/abs/2510.16893)
*Bo-Han Feng,Chien-Feng Liu,Yu-Hsuan Li Liang,Chih-Kai Yang,Szu-Wei Fu,Zhehuai Chen,Ke-Han Lu,Sung-Feng Huang,Chao-Han Huck Yang,Yu-Chiang Frank Wang,Yun-Nung Chen,Hung-yi Lee*

Main category: cs.SD

TL;DR: LALMs show safety inconsistencies across emotional variations, with medium-intensity emotions posing the greatest risk, highlighting the need for emotion-robust alignment strategies.


<details>
  <summary>Details</summary>
Motivation: To investigate the underexplored safety alignment of large audio-language models under paralinguistic variation, specifically focusing on speaker emotion.

Method: Constructed a dataset of malicious speech instructions expressed across multiple emotions and intensities, then evaluated several state-of-the-art LALMs.

Result: Revealed substantial safety inconsistencies: different emotions elicit varying levels of unsafe responses, and the effect of intensity is non-monotonic with medium expressions often posing the greatest risk.

Conclusion: Highlights an overlooked vulnerability in LALMs and calls for alignment strategies explicitly designed to ensure robustness under emotional variation for trustworthy real-world deployment.

Abstract: Large audio-language models (LALMs) extend text-based LLMs with auditory
understanding, offering new opportunities for multimodal applications. While
their perception, reasoning, and task performance have been widely studied,
their safety alignment under paralinguistic variation remains underexplored.
This work systematically investigates the role of speaker emotion. We construct
a dataset of malicious speech instructions expressed across multiple emotions
and intensities, and evaluate several state-of-the-art LALMs. Our results
reveal substantial safety inconsistencies: different emotions elicit varying
levels of unsafe responses, and the effect of intensity is non-monotonic, with
medium expressions often posing the greatest risk. These findings highlight an
overlooked vulnerability in LALMs and call for alignment strategies explicitly
designed to ensure robustness under emotional variation, a prerequisite for
trustworthy deployment in real-world settings.

</details>


### [356] [SAKE: Towards Editing Auditory Attribute Knowledge of Large Audio-Language Models](https://arxiv.org/abs/2510.16917)
*Chih-Kai Yang,Yen-Ting Piao,Tzu-Wen Hsu,Szu-Wei Fu,Zhehuai Chen,Ke-Han Lu,Sung-Feng Huang,Chao-Han Huck Yang,Yu-Chiang Frank Wang,Yun-Nung Chen,Hung-yi Lee*

Main category: cs.SD

TL;DR: SAKE is the first benchmark for editing auditory attribute knowledge in Large Audio-Language Models, addressing challenges in multimodal knowledge editing beyond text and vision domains.


<details>
  <summary>Details</summary>
Motivation: Prior knowledge editing work focused mainly on text and vision, leaving auditory modalities unexplored. SAKE aims to fill this gap by enabling efficient updates to auditory knowledge without full model retraining.

Method: Created SAKE benchmark targeting abstract auditory attributes, benchmarked 7 editing methods on 2 LALMs across reliability, generality, audio/text locality, and portability dimensions.

Result: Revealed key challenges: preserving intra-attribute knowledge unrelated to edits, generalizing edits to multimodal reasoning, and maintaining edits under sequential updates.

Conclusion: SAKE provides a principled framework for studying auditory knowledge editing, opening new directions for maintaining and adapting LALMs in diverse real-world scenarios.

Abstract: Knowledge editing offers an efficient way to update model knowledge without
full retraining, but prior work has concentrated almost exclusively on textual
or visual modalities. We introduce SAKE, the first benchmark specifically
designed for editing auditory attribute knowledge in Large Audio-Language
Models (LALMs). Unlike factual updates, SAKE targets several abstract auditory
attributes, capturing knowledge types that go beyond conventional textual and
visual domains. We benchmark seven editing methods on two LALMs along four
dimensions: reliability, generality, audio/text locality, and portability.
Results highlight challenges such as preserving intra-attribute knowledge
unrelated to the edit, generalizing edits to multimodal reasoning, and
maintaining edits under sequential updates. SAKE provides a principled
framework to study how knowledge editing extends to the auditory modalities,
opening new directions for maintaining and adapting LALMs in more diverse
real-world scenarios.

</details>


### [357] [DELULU: Discriminative Embedding Learning Using Latent Units for Speaker-Aware Self-Supervised Speech Foundational Model](https://arxiv.org/abs/2510.17662)
*Massa Baali,Rita Singh,Bhiksha Raj*

Main category: cs.SD

TL;DR: DELULU is a speaker-aware self-supervised model that integrates external speaker verification supervision into pseudo-label generation, achieving significant improvements on speaker-centric tasks without task-specific fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Current self-supervised speech models excel at content-driven tasks but struggle with capturing speaker-discriminative features needed for verification, diarization, and profiling applications.

Method: Integrates external supervision from ReDimNet speaker verification model into k-means clustering during pre-training, using frame-level embeddings to guide pseudo-label generation. Trained with dual masked prediction and denoising objectives.

Result: Significantly outperforms prior SSL models, achieving up to 62% relative improvement in EER for speaker verification and consistent gains on zero-shot profiling tasks (gender, age, accent, speaker counting).

Conclusion: DELULU serves as a strong universal encoder for speaker-aware speech processing, enabling superior performance across speaker-centric tasks without requiring task-specific fine-tuning.

Abstract: Self-supervised speech models have achieved remarkable success on
content-driven tasks, yet they remain limited in capturing
speaker-discriminative features critical for verification, diarization, and
profiling applications. We introduce DELULU, a speaker-aware self-supervised
foundational model that addresses this limitation by integrating external
supervision into the pseudo-label generation process. DELULU leverages
frame-level embeddings from ReDimNet, a state-of-the-art speaker verification
model, to guide the k-means clustering step during pre-training, introducing a
strong speaker-discriminative inductive bias that aligns representation
learning with speaker identity. The model is trained using a dual objective
that combines masked prediction and denoising, further enhancing robustness and
generalization. DELULU significantly outperforms prior self-supervised learning
(SSL) models across a range of speaker-centric tasks, achieving up to 62%
relative improvement in equal error rate (EER) for speaker verification and
consistent gains on zero-shot profiling tasks such as gender, age, accent, and
speaker counting. Our findings demonstrate that DELULU is a strong universal
encoder for speaker-aware speech processing, enabling superior performance even
without task-specific fine-tuning.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [358] [Prompt Optimization via Retrieved Reasoning Assets and Multi-Agent Analysis](https://arxiv.org/abs/2510.16635)
*Wonduk Seo,Juhyeon Lee,Junseo Koh,Hyunjin An,Jian Park,Seunghyun Lee,Haihua Chen,Yi Bu*

Main category: cs.MA

TL;DR: MA-SAPO is a multi-agent framework that improves prompt optimization by explicitly coupling evaluation scores with structured reasoning to guide systematic, interpretable prompt edits.


<details>
  <summary>Details</summary>
Motivation: Existing prompt optimization methods treat evaluation as a black box, relying on numerical scores without explaining why prompts succeed/fail, and depend on trial-and-error refinements that are hard to interpret and control.

Method: Two-stage framework: Reasoning Phase where agents collaboratively explain metric scores, diagnose weaknesses, and synthesize targeted refinements stored as reusable reasoning assets; Test Phase where agents retrieve these assets to analyze optimized prompts and apply evidence-grounded edits.

Result: Experiments on HelpSteer1/2 benchmarks show consistent improvements over single-pass prompting, retrieval-augmented baselines, and prior multi-agent strategies.

Conclusion: MA-SAPO produces more transparent, auditable, and controllable prompt refinements by turning evaluation signals into interpretable reasoning chains.

Abstract: Prompt optimization has emerged as an effective alternative to retraining for
improving the performance of Large Language Models (LLMs). However, most
existing approaches treat evaluation as a black box, relying solely on
numerical scores while offering limited insight into why a prompt succeeds or
fails. They also depend heavily on trial-and-error refinements, which are
difficult to interpret and control. In this paper, we introduce MA-SAPO, a
Multi-Agent framework for Score-Aware Prompt Optimization. Compared to prior
methods, MA-SAPO explicitly couples evaluation outcomes with structured
reasoning to guide systematic edits. The framework specifically consists of two
stages: during the Reasoning Phase, agents collaboratively explain metric
scores, diagnose weaknesses, and synthesize targeted refinements that are
stored as reusable reasoning assets; during the Test Phase, agents retrieve
these assets to analyze optimized prompts and apply only evidence-grounded
edits. By turning evaluation signals into interpretable reasoning chains,
MA-SAPO produces prompt refinements that are more transparent, auditable, and
controllable. Experiments on the HelpSteer1/2 benchmarks demonstrate consistent
improvements over single-pass prompting, retrieval-augmented baselines, and
prior multi-agent strategies, validating the effectiveness of our approach.

</details>

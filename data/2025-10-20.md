<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 52]
- [cs.CV](#cs.CV) [Total: 88]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.DC](#cs.DC) [Total: 1]
- [stat.ML](#stat.ML) [Total: 2]
- [eess.IV](#eess.IV) [Total: 2]
- [cs.LG](#cs.LG) [Total: 9]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cs.SE](#cs.SE) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.CR](#cs.CR) [Total: 1]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [cs.AI](#cs.AI) [Total: 5]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Rethinking Toxicity Evaluation in Large Language Models: A Multi-Label Perspective](https://arxiv.org/abs/2510.15007)
*Zhiqiang Kou,Junyang Chen,Xin-Qiang Cai,Ming-Kun Xie,Biao Liu,Changwei Wang,Lei Feng,Yuheng Jia,Gang Niu,Masashi Sugiyama,Xin Geng*

Main category: cs.CL

TL;DR: The paper addresses limitations in current toxicity detectors by introducing three multi-label benchmarks for more accurate toxicity detection in LLM-generated content, and proposes a pseudo-label-based method that outperforms existing approaches.


<details>
  <summary>Details</summary>
Motivation: Current toxicity detectors rely on single-label benchmarks which fail to capture the ambiguous and multi-dimensional nature of real-world toxic prompts, leading to biased evaluations with missed detections and false positives. Gathering comprehensive multi-label annotations is also prohibitively expensive.

Method: The authors introduce three multi-label benchmarks (Q-A-MLL, R-A-MLL, H-X-MLL) annotated with a 15-category taxonomy, provide theoretical proof that pseudo-label training outperforms single-label supervision, and develop a pseudo-label-based toxicity detection method.

Result: Extensive experiments show that the proposed approach significantly surpasses advanced baselines including GPT-4o and DeepSeek, enabling more accurate and reliable evaluation of multi-label toxicity in LLM-generated content.

Conclusion: The multi-label benchmarks and pseudo-label-based method provide a more effective framework for toxicity detection in LLMs, addressing the limitations of current single-label approaches and improving detection reliability.

Abstract: Large language models (LLMs) have achieved impressive results across a range
of natural language processing tasks, but their potential to generate harmful
content has raised serious safety concerns. Current toxicity detectors
primarily rely on single-label benchmarks, which cannot adequately capture the
inherently ambiguous and multi-dimensional nature of real-world toxic prompts.
This limitation results in biased evaluations, including missed toxic
detections and false positives, undermining the reliability of existing
detectors. Additionally, gathering comprehensive multi-label annotations across
fine-grained toxicity categories is prohibitively costly, further hindering
effective evaluation and development. To tackle these issues, we introduce
three novel multi-label benchmarks for toxicity detection: \textbf{Q-A-MLL},
\textbf{R-A-MLL}, and \textbf{H-X-MLL}, derived from public toxicity datasets
and annotated according to a detailed 15-category taxonomy. We further provide
a theoretical proof that, on our released datasets, training with pseudo-labels
yields better performance than directly learning from single-label supervision.
In addition, we develop a pseudo-label-based toxicity detection method.
Extensive experimental results show that our approach significantly surpasses
advanced baselines, including GPT-4o and DeepSeek, thus enabling more accurate
and reliable evaluation of multi-label toxicity in LLM-generated content.

</details>


### [2] [Can generative AI figure out figurative language? The influence of idioms on essay scoring by ChatGPT, Gemini, and Deepseek](https://arxiv.org/abs/2510.15009)
*Enis Oğuz*

Main category: cs.CL

TL;DR: Generative AI models (ChatGPT, Gemini, Deepseek) were tested for automated essay scoring, particularly evaluating their performance on essays with and without idioms. Gemini showed superior interrater reliability with human raters and best handled figurative language.


<details>
  <summary>Details</summary>
Motivation: To assess Generative AI's potential as an alternative to Automated Essay Scoring (AES) systems, specifically examining its ability to process idioms and figurative language in student essays.

Method: Created two equal essay lists from 348 student essays: one with multiple idioms and one without idioms. Three Generative AI models scored all essays three times using the same rubric as human raters, with analysis incorporating Corpus Linguistics and Computational Linguistics insights.

Result: All models showed excellent consistency. Gemini outperformed competitors in interrater reliability with human raters and showed no demographic bias. For essays with idioms, Gemini followed the most similar pattern to human raters.

Conclusion: Generative AI models demonstrate potential for hybrid AES approaches, with Gemini emerging as the best candidate due to its superior handling of figurative language and potential for standalone essay scoring in the future.

Abstract: The developments in Generative AI technologies have paved the way for
numerous innovations in different fields. Recently, Generative AI has been
proposed as a competitor to AES systems in evaluating student essays
automatically. Considering the potential limitations of AI in processing
idioms, this study assessed the scoring performances of Generative AI models
for essays with and without idioms by incorporating insights from Corpus
Linguistics and Computational Linguistics. Two equal essay lists were created
from 348 student essays taken from a corpus: one with multiple idioms present
in each essay and another with no idioms in essays. Three Generative AI models
(ChatGPT, Gemini, and Deepseek) were asked to score all essays in both lists
three times, using the same rubric used by human raters in assigning essay
scores. The results revealed excellent consistency for all models, but Gemini
outperformed its competitors in interrater reliability with human raters. There
was also no detectable bias for any demographic group in AI assessment. For
essays with multiple idioms, Gemini followed a the most similar pattern to
human raters. While the models in the study demonstrated potential for a hybrid
approach, Gemini was the best candidate for the task due to its ability to
handle figurative language and showed promise for handling essay-scoring tasks
alone in the future.

</details>


### [3] [A Generalizable Rhetorical Strategy Annotation Model Using LLM-based Debate Simulation and Labelling](https://arxiv.org/abs/2510.15081)
*Shiyu Ji,Farnoosh Hashemi,Joice Chen,Juanwen Pan,Weicheng Ma,Hefan Zhang,Sophia Pan,Ming Cheng,Shubham Mohole,Saeed Hassanpour,Soroush Vosoughi,Michael Macy*

Main category: cs.CL

TL;DR: A framework using LLMs to automatically generate and label synthetic debate data for rhetorical strategy analysis, achieving high performance and generalization across domains.


<details>
  <summary>Details</summary>
Motivation: Current rhetorical strategy analysis relies on costly human annotation, is inconsistent, difficult to scale, and datasets are limited to specific topics and strategies.

Method: Leverage LLMs to automatically generate and label synthetic debate data based on a four-part rhetorical typology (causal, empirical, emotional, moral), then fine-tune transformer-based classifiers on this data.

Result: Model achieves high performance and strong generalization across topical domains. Applications show improved persuasiveness prediction and reveal increased use of affective over cognitive arguments in U.S. Presidential debates (1960-2020).

Conclusion: The proposed framework successfully automates rhetorical strategy analysis at scale, enabling robust model development and revealing meaningful patterns in political discourse.

Abstract: Rhetorical strategies are central to persuasive communication, from political
discourse and marketing to legal argumentation. However, analysis of rhetorical
strategies has been limited by reliance on human annotation, which is costly,
inconsistent, difficult to scale. Their associated datasets are often limited
to specific topics and strategies, posing challenges for robust model
development. We propose a novel framework that leverages large language models
(LLMs) to automatically generate and label synthetic debate data based on a
four-part rhetorical typology (causal, empirical, emotional, moral). We
fine-tune transformer-based classifiers on this LLM-labeled dataset and
validate its performance against human-labeled data on this dataset and on
multiple external corpora. Our model achieves high performance and strong
generalization across topical domains. We illustrate two applications with the
fine-tuned model: (1) the improvement in persuasiveness prediction from
incorporating rhetorical strategy labels, and (2) analyzing temporal and
partisan shifts in rhetorical strategies in U.S. Presidential debates
(1960-2020), revealing increased use of affective over cognitive argument in
U.S. Presidential debates.

</details>


### [4] [Continual Learning via Sparse Memory Finetuning](https://arxiv.org/abs/2510.15103)
*Jessy Lin,Luke Zettlemoyer,Gargi Ghosh,Wen-Tau Yih,Aram Markosyan,Vincent-Pierre Berges,Barlas Oğuz*

Main category: cs.CL

TL;DR: Sparse memory finetuning enables continual learning in language models by updating only highly activated memory slots, reducing catastrophic forgetting while maintaining new knowledge acquisition.


<details>
  <summary>Details</summary>
Motivation: To address catastrophic forgetting in language models by leveraging sparse parameter updates, since shared trainable parameters across tasks make mitigating forgetting challenging.

Method: Introduces sparse memory finetuning using memory layer models that are sparsely updated by design. Only updates memory slots highly activated by new knowledge relative to pretraining data usage.

Result: Substantially reduces forgetting compared to full finetuning and LoRA: NaturalQuestions F1 drops by only 11% with sparse memory finetuning vs 89% with full finetuning and 71% with LoRA, while achieving same level of new knowledge acquisition.

Conclusion: Sparsity in memory layers offers a promising path toward continual learning in large language models by reducing interference between new knowledge and existing capabilities.

Abstract: Modern language models are powerful, but typically static after deployment. A
major obstacle to building models that continually learn over time is
catastrophic forgetting, where updating on new data erases previously acquired
capabilities. Motivated by the intuition that mitigating forgetting is
challenging because trainable parameters are shared across all tasks, we
investigate whether sparse parameter updates can enable learning without
catastrophic forgetting. We introduce sparse memory finetuning, leveraging
memory layer models (Berges et al., 2024), which are sparsely updated by
design. By updating only the memory slots that are highly activated by a new
piece of knowledge relative to usage on pretraining data, we reduce
interference between new knowledge and the model's existing capabilities. We
evaluate learning and forgetting compared to full finetuning and
parameter-efficient finetuning with LoRA on two question answering tasks. We
find that sparse memory finetuning learns new knowledge while exhibiting
substantially less forgetting: while NaturalQuestions F1 drops by 89% after
full finetuning on new facts and 71% with LoRA, sparse memory finetuning yields
only an 11% drop with the same level of new knowledge acquisition. Our results
suggest sparsity in memory layers offers a promising path toward continual
learning in large language models.

</details>


### [5] [Measuring the Effect of Disfluency in Multilingual Knowledge Probing Benchmarks](https://arxiv.org/abs/2510.15115)
*Kirill Semenov,Rico Sennrich*

Main category: cs.CL

TL;DR: The paper shows that template translations in multilingual benchmarks like MLAMA create ungrammatical prompts, especially for morphologically rich languages. Using sentence-level translations significantly improves knowledge retrieval scores.


<details>
  <summary>Details</summary>
Motivation: Current multilingual benchmarks use template translations that ignore grammatical and semantic information of named entities, leading to ungrammatical prompts and unreliable scores for morphologically rich languages.

Method: Sampled 4 Slavic languages from MLAMA dataset and compared knowledge retrieval scores between original templated translations and sentence-level translations by Google Translate and ChatGPT. Extended analysis to 5 more languages from different families.

Result: Significant increase in knowledge retrieval scores with sentence-level translations. Similar patterns observed across different language families.

Conclusion: Community should control grammaticality in multilingual datasets using whole sentence translation with neural MT or LLM systems for more interpretable results.

Abstract: For multilingual factual knowledge assessment of LLMs, benchmarks such as
MLAMA use template translations that do not take into account the grammatical
and semantic information of the named entities inserted in the sentence. This
leads to numerous instances of ungrammaticality or wrong wording of the final
prompts, which complicates the interpretation of scores, especially for
languages that have a rich morphological inventory. In this work, we sample 4
Slavic languages from the MLAMA dataset and compare the knowledge retrieval
scores between the initial (templated) MLAMA dataset and its sentence-level
translations made by Google Translate and ChatGPT. We observe a significant
increase in knowledge retrieval scores, and provide a qualitative analysis for
possible reasons behind it. We also make an additional analysis of 5 more
languages from different families and see similar patterns. Therefore, we
encourage the community to control the grammaticality of highly multilingual
datasets for higher and more interpretable results, which is well approximated
by whole sentence translation with neural MT or LLM systems. The dataset and
all related code is published at the Github repository:
https://github.com/ZurichNLP/Fluent-mLAMA.

</details>


### [6] [Latent Topic Synthesis: Leveraging LLMs for Electoral Ad Analysis](https://arxiv.org/abs/2510.15125)
*Alexander Brady,Tunazzina Islam*

Main category: cs.CL

TL;DR: An end-to-end framework for automatically generating interpretable topic taxonomies from unlabeled social media corpora using unsupervised clustering and LLM-based labeling, applied to Meta political ads from the 2024 US election.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of analyzing vast and rapidly evolving political discourse on social media platforms, enabling scalable and interpretable analysis without requiring seed sets or domain expertise.

Method: Combines unsupervised clustering with prompt-based labeling using large language models to iteratively construct topic taxonomies, applied to Meta political ads corpus with moral framing dimension annotation.

Result: Uncovered that voting and immigration ads dominate spending/impressions, abortion and election-integrity achieve disproportionate reach. Found polarized funding patterns and divergent moral framing across topics, with strong correlations between moral foundations and issues.

Conclusion: The framework supports scalable, interpretable analysis of political messaging on social media, enabling better understanding of emerging narratives, polarization dynamics, and moral underpinnings of digital political communication.

Abstract: Social media platforms play a pivotal role in shaping political discourse,
but analyzing their vast and rapidly evolving content remains a major
challenge. We introduce an end-to-end framework for automatically generating an
interpretable topic taxonomy from an unlabeled corpus. By combining
unsupervised clustering with prompt-based labeling, our method leverages large
language models (LLMs) to iteratively construct a taxonomy without requiring
seed sets or domain expertise. We apply this framework to a large corpus of
Meta (previously known as Facebook) political ads from the month ahead of the
2024 U.S. Presidential election. Our approach uncovers latent discourse
structures, synthesizes semantically rich topic labels, and annotates topics
with moral framing dimensions. We show quantitative and qualitative analyses to
demonstrate the effectiveness of our framework. Our findings reveal that voting
and immigration ads dominate overall spending and impressions, while abortion
and election-integrity achieve disproportionate reach. Funding patterns are
equally polarized: economic appeals are driven mainly by conservative PACs,
abortion messaging splits between pro- and anti-rights coalitions, and
crime-and-justice campaigns are fragmented across local committees. The framing
of these appeals also diverges--abortion ads emphasize liberty/oppression
rhetoric, while economic messaging blends care/harm, fairness/cheating, and
liberty/oppression narratives. Topic salience further reveals strong
correlations between moral foundations and issues. Demographic targeting also
emerges. This work supports scalable, interpretable analysis of political
messaging on social media, enabling researchers, policymakers, and the public
to better understand emerging narratives, polarization dynamics, and the moral
underpinnings of digital political communication.

</details>


### [7] [FarsiMCQGen: a Persian Multiple-choice Question Generation Framework](https://arxiv.org/abs/2510.15134)
*Mohammad Heydari Rad,Rezvan Afari,Saeedeh Momtazi*

Main category: cs.CL

TL;DR: FarsiMCQGen is a novel approach for generating Persian-language multiple-choice questions using Transformers, knowledge graphs, and rule-based methods to create realistic distractors, with a new dataset of 10,289 questions evaluated by LLMs.


<details>
  <summary>Details</summary>
Motivation: Generating high-quality MCQs in low-resource languages like Persian is challenging, limiting educational testing capabilities in these languages.

Method: Combines candidate generation, filtering, and ranking techniques using Transformers and knowledge graphs integrated with rule-based approaches to create credible distractors, based on Wikipedia data.

Result: Created a novel Persian MCQ dataset of 10,289 questions and demonstrated the effectiveness of the model through evaluation by state-of-the-art LLMs.

Conclusion: The approach successfully generates high-quality Persian MCQs and provides a valuable dataset that can inspire further research in MCQ generation for low-resource languages.

Abstract: Multiple-choice questions (MCQs) are commonly used in educational testing, as
they offer an efficient means of evaluating learners' knowledge. However,
generating high-quality MCQs, particularly in low-resource languages such as
Persian, remains a significant challenge. This paper introduces FarsiMCQGen, an
innovative approach for generating Persian-language MCQs. Our methodology
combines candidate generation, filtering, and ranking techniques to build a
model that generates answer choices resembling those in real MCQs. We leverage
advanced methods, including Transformers and knowledge graphs, integrated with
rule-based approaches to craft credible distractors that challenge test-takers.
Our work is based on data from Wikipedia, which includes general knowledge
questions. Furthermore, this study introduces a novel Persian MCQ dataset
comprising 10,289 questions. This dataset is evaluated by different
state-of-the-art large language models (LLMs). Our results demonstrate the
effectiveness of our model and the quality of the generated dataset, which has
the potential to inspire further research on MCQs.

</details>


### [8] [Structure-R1: Dynamically Leveraging Structural Knowledge in LLM Reasoning through Reinforcement Learning](https://arxiv.org/abs/2510.15191)
*Junlin Wu,Xianrui Zhong,Jiashuo Sun,Bolian Li,Bowen Jin,Jiawei Han,Qingkai Zeng*

Main category: cs.CL

TL;DR: Structure-R1 is a framework that transforms retrieved content into structured representations using reinforcement learning to enhance reasoning in LLMs, achieving competitive performance with smaller models.


<details>
  <summary>Details</summary>
Motivation: Traditional RAG systems use unstructured text with low information density, limiting reasoning capabilities. Structure-R1 aims to overcome this by creating structured representations optimized for reasoning.

Method: Uses reinforcement learning to learn a content representation policy that dynamically generates structural formats for multi-step reasoning. Includes self-reward structural verification to ensure quality and reliability of generated structures.

Result: Achieves competitive performance on seven knowledge-intensive benchmarks with a 7B-scale backbone model, matching performance of much larger models.

Conclusion: Structured representations significantly enhance reasoning by improving information density and contextual clarity, demonstrating the effectiveness of the Structure-R1 framework.

Abstract: Large language models (LLMs) have demonstrated remarkable advances in
reasoning capabilities. However, their performance remains constrained by
limited access to explicit and structured domain knowledge. Retrieval-Augmented
Generation (RAG) addresses this by incorporating external information as
context to augment reasoning. Nevertheless, traditional RAG systems typically
operate over unstructured and fragmented text, resulting in low information
density and suboptimal reasoning. To overcome these limitations, we propose
\textsc{Structure-R1}, a novel framework that transforms retrieved content into
structured representations optimized for reasoning. Leveraging reinforcement
learning, \textsc{Structure-R1} learns a content representation policy that
dynamically generates and adapts structural formats based on the demands of
multi-step reasoning. Unlike prior methods that rely on fixed schemas, our
approach adopts a generative paradigm capable of producing task-specific
structures tailored to individual queries. To ensure the quality and
reliability of these representations, we introduce a self-reward structural
verification mechanism that checks whether the generated structures are both
correct and self-contained. Extensive experiments on seven knowledge-intensive
benchmarks show that \textsc{Structure-R1} consistently achieves competitive
performance with a 7B-scale backbone model and matches the performance of much
larger models. Additionally, our theoretical analysis demonstrates how
structured representations enhance reasoning by improving information density
and contextual clarity. Our code and data are available at:
https://github.com/jlwu002/sr1.

</details>


### [9] [Extending Audio Context for Long-Form Understanding in Large Audio-Language Models](https://arxiv.org/abs/2510.15231)
*Yuatyong Chaichana,Pittawat Taveekitworachai,Warit Sirichotedumrong,Potsawee Manakul,Kunat Pipatanakul*

Main category: cs.CL

TL;DR: The paper introduces Partial YaRN and VLAT to extend audio context windows in Large Audio-Language Models (LALMs) without compromising text capabilities, enabling better long-form audio understanding.


<details>
  <summary>Details</summary>
Motivation: LALMs are limited by short audio context windows even when their text backbones support long contexts, which restricts long-form audio understanding capabilities.

Method: Two approaches: 1) Partial YaRN - training-free audio-only extension method that modifies only audio token positions while preserving text positions; 2) VLAT - training strategy that extends Partial YaRN into positional augmentation, simulating diverse audio lengths during training.

Result: Partial YaRN outperforms original models across various settings, and VLAT provides substantial improvement, achieving strong performance on long audio of unseen lengths.

Conclusion: The proposed methods effectively extend audio context windows in LALMs while maintaining text capabilities, enabling robust long-form audio understanding.

Abstract: Large Audio-Language Models (LALMs) are often constrained by short audio
context windows, even when their text backbones support long contexts, limiting
long-form audio understanding. Prior work has introduced context-extension
methods (e.g. YaRN) on unimodal LLMs, yet their application to LALMs remains
unexplored. First, building on RoPE-based context extension, we introduce
Partial YaRN, a training-free, audio-only extension method that modifies only
audio token positions, leaving text positions intact to preserve the base LLM's
text capabilities. Second, we propose Virtual Longform Audio Training (VLAT), a
training strategy that extends Partial YaRN into a training-time positional
augmentation. VLAT simulates diverse audio lengths during training, enabling
generalization to inputs far longer than those seen in training and improving
robustness for long-context audio understanding. Our experiments on SALMONN and
Qwen2-Audio show that Partial YaRN outperforms the original models across wide
range of settings, and VLAT training strategy provides substantial improvement,
achieving strong performance on long audio of unseen lengths.

</details>


### [10] [Planner and Executor: Collaboration between Discrete Diffusion And Autoregressive Models in Reasoning](https://arxiv.org/abs/2510.15244)
*Lina Berrayana,Ahmed Heakl,Muhammad Abdullah Sohail,Thomas Hofmann,Salman Khan,Wei Chen*

Main category: cs.CL

TL;DR: Hybrid architectures combining discrete diffusion language models (DDLMs) with autoregressive models (ARMs) achieve better accuracy and computational efficiency by shifting communication from text space to latent space.


<details>
  <summary>Details</summary>
Motivation: Autoregressive models are accurate but computationally expensive due to long token sequences, while DDLMs offer parallel generation and strong reasoning capabilities. The study explores whether combining both can yield complementary benefits.

Method: Two collaboration approaches: text-space (one model plans reasoning, another executes answer) and latent-space (learned projector maps DDLM latents into ARM embedding space). Evaluated on tasks like DART-5 and AIME24.

Result: Latent-space communication significantly improves accuracy (27.0% to 54.0% on DART-5, 0.0% to 14.0% on AIME24). Hybrid pipeline with 64 planning tokens and ~5 execution tokens outperforms Qwen3.1-7B while using 44x fewer tokens.

Conclusion: DDLM-ARM hybrid architectures offer substantial computational savings with minimal accuracy impact, demonstrating DDLMs' potential in reasoning tasks through latent-space communication.

Abstract: Current autoregressive language models (ARMs) achieve high accuracy but
require long token sequences, making them costly. Discrete diffusion language
models (DDLMs) enable parallel and flexible generation within a fixed number of
steps and have recently emerged for their strong performance in complex
reasoning and long-term planning tasks. We present a study exploring hybrid
architectures that couple DDLMs with ARMs to assess whether their collaboration
can yield complementary benefits. We first examine collaboration in text space,
where one model plans the reasoning process and another executes the final
answer based on that plan. We then extend this setup to latent-space
communication, introducing a learned projector that maps DDLM latents into the
ARM's embedding space, potentially bypassing some of the text-generation
limitations of diffusion models. We find that shifting DDLM --> ARM
communication from text space to latent space yields significant accuracy
gains, for example increasing from 27.0% to 54.0% on DART-5 and from 0.0% to
14.0% on AIME24. We also find that combining a DDLM planner with an ARM
executor can provide substantial computational savings with little to no impact
on accuracy. For example, the latent-space pipeline, using 64 tokens for
planning and roughly 5 for execution, surpasses Qwen3.1-7B on DART-5 and AIME,
despite Qwen using 44 times more tokens. Overall, our study offers new insights
into reasoning with DDLMs and highlights their potential in hybrid
architectures.

</details>


### [11] [Scaling Beyond Context: A Survey of Multimodal Retrieval-Augmented Generation for Document Understanding](https://arxiv.org/abs/2510.15253)
*Sensen Gao,Shanshan Zhao,Xu Jiang,Lunhao Duan,Yong Xien Chng,Qing-Guo Chen,Weihua Luo,Kaifu Zhang,Jia-Wang Bian,Mingming Gong*

Main category: cs.CL

TL;DR: This paper presents a systematic survey of Multimodal RAG for document understanding, proposing a taxonomy and reviewing advances in graph structures and agentic frameworks.


<details>
  <summary>Details</summary>
Motivation: Current document understanding approaches have limitations: OCR-based pipelines lose structural detail while native MLLMs struggle with context modeling. Documents' multimodal nature demands a more advanced paradigm.

Method: The authors conduct a systematic survey, propose a taxonomy based on domain, retrieval modality, and granularity, and review advances involving graph structures and agentic frameworks.

Result: The survey summarizes key datasets, benchmarks, and applications of Multimodal RAG for document understanding.

Conclusion: The paper highlights open challenges in efficiency, fine-grained representation, and robustness, providing a roadmap for future progress in document AI.

Abstract: Document understanding is critical for applications from financial analysis
to scientific discovery. Current approaches, whether OCR-based pipelines
feeding Large Language Models (LLMs) or native Multimodal LLMs (MLLMs), face
key limitations: the former loses structural detail, while the latter struggles
with context modeling. Retrieval-Augmented Generation (RAG) helps ground models
in external data, but documents' multimodal nature, i.e., combining text,
tables, charts, and layout, demands a more advanced paradigm: Multimodal RAG.
This approach enables holistic retrieval and reasoning across all modalities,
unlocking comprehensive document intelligence. Recognizing its importance, this
paper presents a systematic survey of Multimodal RAG for document
understanding. We propose a taxonomy based on domain, retrieval modality, and
granularity, and review advances involving graph structures and agentic
frameworks. We also summarize key datasets, benchmarks, and applications, and
highlight open challenges in efficiency, fine-grained representation, and
robustness, providing a roadmap for future progress in document AI.

</details>


### [12] [TraceCoder: Towards Traceable ICD Coding via Multi-Source Knowledge Integration](https://arxiv.org/abs/2510.15267)
*Mucheng Ren,He Chen,Yuchen Yan,Danqing Hu,Jun Xu,Xian Zeng*

Main category: cs.CL

TL;DR: TraceCoder is a novel framework that integrates multi-source external knowledge (UMLS, Wikipedia, LLMs) to improve automated ICD coding by addressing semantic gaps, rare code performance, and interpretability issues through dynamic knowledge incorporation and hybrid attention mechanisms.


<details>
  <summary>Details</summary>
Motivation: Existing automated ICD coding methods face challenges with semantic gaps between clinical text and codes, poor performance on rare/long-tail codes, and limited interpretability, which TraceCoder aims to solve.

Method: Proposes TraceCoder framework that dynamically incorporates external knowledge from UMLS, Wikipedia, and LLMs to enrich code representations. Uses hybrid attention mechanism to model interactions among labels, clinical context, and knowledge for improved long-tail code recognition and interpretability.

Result: Experiments on MIMIC-III-ICD9, MIMIC-IV-ICD9, and MIMIC-IV-ICD10 datasets show state-of-the-art performance, with ablation studies validating the effectiveness of its components.

Conclusion: TraceCoder provides a scalable and robust solution for automated ICD coding that meets clinical needs for accuracy, interpretability, and reliability through external knowledge integration and traceable predictions.

Abstract: Automated International Classification of Diseases (ICD) coding assigns
standardized diagnosis and procedure codes to clinical records, playing a
critical role in healthcare systems. However, existing methods face challenges
such as semantic gaps between clinical text and ICD codes, poor performance on
rare and long-tail codes, and limited interpretability. To address these
issues, we propose TraceCoder, a novel framework integrating multi-source
external knowledge to enhance traceability and explainability in ICD coding.
TraceCoder dynamically incorporates diverse knowledge sources, including UMLS,
Wikipedia, and large language models (LLMs), to enrich code representations,
bridge semantic gaps, and handle rare and ambiguous codes. It also introduces a
hybrid attention mechanism to model interactions among labels, clinical
context, and knowledge, improving long-tail code recognition and making
predictions interpretable by grounding them in external evidence. Experiments
on MIMIC-III-ICD9, MIMIC-IV-ICD9, and MIMIC-IV-ICD10 datasets demonstrate that
TraceCoder achieves state-of-the-art performance, with ablation studies
validating the effectiveness of its components. TraceCoder offers a scalable
and robust solution for automated ICD coding, aligning with clinical needs for
accuracy, interpretability, and reliability.

</details>


### [13] [TACL: Threshold-Adaptive Curriculum Learning Strategy for Enhancing Medical Text Understanding](https://arxiv.org/abs/2510.15269)
*Mucheng Ren,Yucheng Yan,He Chen,Danqing Hu,Jun Xu,Xian Zeng*

Main category: cs.CL

TL;DR: TACL is a threshold-adaptive curriculum learning framework that dynamically adjusts training based on medical text complexity, improving performance on clinical tasks like ICD coding and readmission prediction.


<details>
  <summary>Details</summary>
Motivation: Medical texts like EMRs are unstructured and domain-specific, making automated understanding challenging. Existing methods treat all data as equally difficult, limiting generalization to rare or complex cases.

Method: TACL categorizes data by complexity levels and prioritizes simpler cases early in training, allowing models to build strong foundations before tackling complex records. Applied to multilingual medical data including English and Chinese clinical records.

Result: Significant improvements across diverse clinical tasks including automatic ICD coding, readmission prediction, and TCM syndrome differentiation. Enhanced performance of automated systems.

Conclusion: TACL demonstrates potential to unify approaches across disparate medical domains, paving the way for more accurate, scalable, and globally applicable medical text understanding solutions.

Abstract: Medical texts, particularly electronic medical records (EMRs), are a
cornerstone of modern healthcare, capturing critical information about patient
care, diagnoses, and treatments. These texts hold immense potential for
advancing clinical decision-making and healthcare analytics. However, their
unstructured nature, domain-specific language, and variability across contexts
make automated understanding an intricate challenge. Despite the advancements
in natural language processing, existing methods often treat all data as
equally challenging, ignoring the inherent differences in complexity across
clinical records. This oversight limits the ability of models to effectively
generalize and perform well on rare or complex cases. In this paper, we present
TACL (Threshold-Adaptive Curriculum Learning), a novel framework designed to
address these challenges by rethinking how models interact with medical texts
during training. Inspired by the principle of progressive learning, TACL
dynamically adjusts the training process based on the complexity of individual
samples. By categorizing data into difficulty levels and prioritizing simpler
cases early in training, the model builds a strong foundation before tackling
more complex records. By applying TACL to multilingual medical data, including
English and Chinese clinical records, we observe significant improvements
across diverse clinical tasks, including automatic ICD coding, readmission
prediction and TCM syndrome differentiation. TACL not only enhances the
performance of automated systems but also demonstrates the potential to unify
approaches across disparate medical domains, paving the way for more accurate,
scalable, and globally applicable medical text understanding solutions.

</details>


### [14] [Exemplar-Guided Planing: Enhanced LLM Agent for KGQA](https://arxiv.org/abs/2510.15283)
*Jingao Xu,Shuoyoucheng Ma,Xin Song,Rong Jiang,Hongkui Tu,Bin Zhou*

Main category: cs.CL

TL;DR: EGP framework enhances LLM agents for KGQA by using exemplary questions and reasoning paths to guide planning, improving task decomposition and relation exploration with smart lookahead.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with semantic gap between natural language queries and KG representations, leading to suboptimal planning and inefficient exploration. Training-free approaches underutilize reasoning patterns in training data.

Method: EGP preprocesses training questions via entity templating, retrieves similar exemplars using semantic embeddings and FAISS index, then guides LLM planning through task decomposition and relation exploration with smart lookahead mechanism.

Result: PoG-EGP significantly improves over baseline PoG system and other methods on WebQSP and CWQ datasets.

Conclusion: EGP effectively bridges the semantic gap in KGQA by leveraging training data patterns to enhance LLM planning capabilities, demonstrating substantial performance improvements.

Abstract: Large Language Models (LLMs) as interactive agents show significant promise
in Knowledge Graph Question Answering (KGQA) but often struggle with the
semantic gap between natural language queries and structured knowledge graph
(KG) representations. This leads to suboptimal planning and inefficient
exploration on KG, while training-free approaches often underutilize valuable
reasoning patterns in training data. To address these limitations, we propose a
novel framework, Exemplar-Guided Planning (EGP), which enhances the planning
capabilities of LLM agents for KGQA. EGP first preprocesses the training set
questions via entity templating to normalize semantic variations. It then
retrieves highly similar exemplary questions and their successful reasoning
paths from this preprocessed set using semantic embeddings and an efficient
FAISS index. These retrieved exemplars dynamically guide the LLM's planning
process in two key phases: (1) Task Decomposition, by aligning generated
sub-objectives with proven reasoning steps, and (2) Relation Exploration, by
providing high-quality auxiliary information to improve relation pruning
accuracy. Additionally, we introduce a Smart Lookahead mechanism during
relation exploration to improve efficiency by preemptively exploring promising
paths and potentially terminating exploration earlier. We apply EGP to the
Plan-on-Graph (PoG) framework, termed PoG-EGP. Extensive experiments on two
real-world KGQA datasets, WebQSP and CWQ, demonstrate that PoG-EGP
significantly improves over the baseline PoG system and other compared methods.

</details>


### [15] [Automatic essay scoring: leveraging Jaccard coefficient and Cosine similaritywith n-gram variation in vector space model approach](https://arxiv.org/abs/2510.15311)
*Andharini Dwi Cahyani,Moh. Wildan Fathoni,Fika Hastarita Rachman,Ari Basuki,Salman Amin,Bain Khusnul Khotimah*

Main category: cs.CL

TL;DR: This study compares Jaccard coefficient and Cosine similarity metrics for automated essay scoring using n-gram vector space models, finding that Cosine similarity with unigrams performs best.


<details>
  <summary>Details</summary>
Motivation: To develop efficient and accurate automated essay scoring (AES) tools for evaluating written content, specifically investigating the effectiveness of different similarity metrics in vector space models.

Method: Used junior high school citizenship education essays, preprocessed text with n-gram models (unigram, bigram, trigram), vectorized text data, and computed similarity scores using both Jaccard coefficient and Cosine similarity metrics.

Result: Cosine similarity outperformed Jaccard coefficient, and unigrams achieved lower root mean square error (RMSE) compared to bigrams and trigrams when comparing system-generated scores to human grader scores.

Conclusion: Cosine similarity with unigram representations is the most effective approach for automated essay scoring among the tested methods.

Abstract: Automated essay scoring (AES) is a vital area of research aiming to provide
efficient and accurate assessment tools for evaluating written content. This
study investigates the effectiveness of two popular similarity metrics, Jaccard
coefficient, and Cosine similarity, within the context of vector space
models(VSM)employing unigram, bigram, and trigram representations. The data
used in this research was obtained from the formative essay of the citizenship
education subject in a junior high school. Each essay undergoes preprocessing
to extract features using n-gram models, followed by vectorization to transform
text data into numerical representations. Then, similarity scores are computed
between essays using both Jaccard coefficient and Cosine similarity. The
performance of the system is evaluated by analyzing the root mean square error
(RMSE), which measures the difference between the scores given by human graders
and those generated by the system. The result shows that the Cosine similarity
outperformed the Jaccard coefficient. In terms of n-gram, unigrams have lower
RMSE compared to bigrams and trigrams.

</details>


### [16] [Accelerating Mobile Language Model Generation via Hybrid Context and Hardware Coordination](https://arxiv.org/abs/2510.15312)
*Zhiyang Chen,Daliang Xu,Haiyang Shen,Mengwei Xu,Shangguang Wang,Yun Ma*

Main category: cs.CL

TL;DR: CoordGen is a mobile inference framework that accelerates context-aware text generation on mobile devices through speculative decoding and dynamic hardware scheduling, achieving up to 3.8x speedup and 4.7x energy efficiency.


<details>
  <summary>Details</summary>
Motivation: On-device LLMs with contextual information enable personalized applications, but token-by-token generation suffers from high latency and poor hardware utilization due to memory-bound characteristics.

Method: Three synergistic components: adaptive execution scheduling (dynamic compute graph balancing), context-aligned drafting (lightweight online calibration), and hardware-efficient draft extension (reusing intermediate sequences).

Result: Experiments show consistent improvements of up to 3.8x generation speed and 4.7x energy efficiency compared to existing mobile inference solutions.

Conclusion: CoordGen effectively addresses mobile LLM inference bottlenecks through coordinated speculative decoding and hardware scheduling, enabling efficient context-aware text generation on mobile devices.

Abstract: Enhancing on-device large language models (LLMs) with contextual information
from local data enables personalized and task-aware generation, powering use
cases such as intelligent assistants and UI agents. While recent developments
in neural processors have substantially improved the efficiency of prefill on
mobile devices, the token-by-token generation process still suffers from high
latency and limited hardware utilization due to its inherently memory-bound
characteristics. This work presents CoordGen, a mobile inference framework that
integrates speculative decoding with dynamic hardware scheduling to accelerate
context-aware text generation on mobile devices. The framework introduces three
synergistic components: (1) adaptive execution scheduling, which dynamically
balances compute graphs between prefill and decoding phases; (2)
context-aligned drafting, which improves speculative efficiency through
lightweight online calibration to current tasks; and (3) hardware-efficient
draft extension, which reuses and expands intermediate sequences to improve
processing parallelism and reduce verification cost. Experiments on multiple
smartphones and representative workloads show consistent improvements of up to
3.8x in generation speed and 4.7x in energy efficiency compared with existing
mobile inference solutions. Component-level analysis further validates the
contribution of each optimization.

</details>


### [17] [Capabilities and Evaluation Biases of Large Language Models in Classical Chinese Poetry Generation: A Case Study on Tang Poetry](https://arxiv.org/abs/2510.15313)
*Bolei Ma,Yina Yao,Anna-Carolina Haensch*

Main category: cs.CL

TL;DR: LLMs show systematic biases in classical Chinese poetry generation and evaluation, with 'echo chamber' effects that diverge from human judgments, highlighting the need for hybrid human-model validation.


<details>
  <summary>Details</summary>
Motivation: To understand LLM performance in classical Chinese poetry generation and evaluation, which remains poorly understood despite increasing applications in creative domains.

Method: A three-step evaluation framework combining computational metrics, LLM-as-a-judge assessment, and human expert validation to evaluate six state-of-the-art LLMs across poetic quality dimensions.

Result: LLMs exhibit systematic generation and evaluation biases, including 'echo chamber' effects where they converge on flawed standards that differ from human judgments.

Conclusion: Current LLMs have both potential and limitations as proxies for literacy generation, demonstrating the continued need for hybrid human-model validation in complex creative tasks.

Abstract: Large Language Models (LLMs) are increasingly applied to creative domains,
yet their performance in classical Chinese poetry generation and evaluation
remains poorly understood. We propose a three-step evaluation framework that
combines computational metrics, LLM-as-a-judge assessment, and human expert
validation. Using this framework, we evaluate six state-of-the-art LLMs across
multiple dimensions of poetic quality, including themes, emotions, imagery,
form, and style. Our analysis reveals systematic generation and evaluation
biases: LLMs exhibit "echo chamber" effects when assessing creative quality,
often converging on flawed standards that diverge from human judgments. These
findings highlight both the potential and limitations of current capabilities
of LLMs as proxy for literacy generation and the limited evaluation practices,
thereby demonstrating the continued need of hybrid validation from both humans
and models in culturally and technically complex creative tasks.

</details>


### [18] [AutoGraph-R1: End-to-End Reinforcement Learning for Knowledge Graph Construction](https://arxiv.org/abs/2510.15339)
*Hong Ting Tsang,Jiaxin Bai,Haoyu Huang,Qiao Xiao,Tianshi Zheng,Baixuan Xu,Shujie Liu,Yangqiu Song*

Main category: cs.CL

TL;DR: AutoGraph-R1 is the first RL-based framework that optimizes KG construction for RAG task performance by training an LLM constructor with task-aware rewards.


<details>
  <summary>Details</summary>
Motivation: Current KG construction is decoupled from downstream applications, leading to suboptimal graph structures for RAG systems.

Method: Frames graph generation as RL policy learning with two novel task-aware reward functions (for knowledge carriers and knowledge indices) to train an LLM constructor.

Result: Significant performance gains over task-agnostic baseline graphs across multiple QA benchmarks.

Conclusion: Successfully closes the loop between KG construction and application, shifting from building "good" graphs to building "useful" ones.

Abstract: Building effective knowledge graphs (KGs) for Retrieval-Augmented Generation
(RAG) is pivotal for advancing question answering (QA) systems. However, its
effectiveness is hindered by a fundamental disconnect: the knowledge graph (KG)
construction process is decoupled from its downstream application, yielding
suboptimal graph structures. To bridge this gap, we introduce AutoGraph-R1, the
first framework to directly optimize KG construction for task performance using
Reinforcement Learning (RL). AutoGraph-R1 trains an LLM constructor by framing
graph generation as a policy learning problem, where the reward is derived from
the graph's functional utility in a RAG pipeline. We design two novel,
task-aware reward functions, one for graphs as knowledge carriers and another
as knowledge indices. Across multiple QA benchmarks, AutoGraph-R1 consistently
enables graph RAG methods to achieve significant performance gains over using
task-agnostic baseline graphs. Our work shows it is possible to close the loop
between construction and application, shifting the paradigm from building
intrinsically ``good'' graphs to building demonstrably ``useful'' ones.

</details>


### [19] [Readability Reconsidered: A Cross-Dataset Analysis of Reference-Free Metrics](https://arxiv.org/abs/2510.15345)
*Catarina G Belem,Parker Glenn,Alfy Samuel,Anoop Kumar,Daben Liu*

Main category: cs.CL

TL;DR: Model-based readability metrics outperform traditional surface-level metrics by better aligning with human judgments, with information content and topic being key factors in readability perception.


<details>
  <summary>Details</summary>
Motivation: Current readability assessment is hindered by inconsistent definitions and reliance on surface-level text properties, creating a mismatch with human perceptions of readability.

Method: Analyzed 897 human judgments to identify readability factors, then evaluated 15 traditional readability metrics against 6 model-based metrics across 5 English datasets.

Result: Model-based metrics consistently ranked top 4 in correlation with human judgments, while the best traditional metric averaged rank 8.6, showing significant performance gap.

Conclusion: Model-based approaches are more promising for readability assessment as they better capture the nuanced factors (information content and topic) that shape human readability perceptions.

Abstract: Automatic readability assessment plays a key role in ensuring effective and
accessible written communication. Despite significant progress, the field is
hindered by inconsistent definitions of readability and measurements that rely
on surface-level text properties. In this work, we investigate the factors
shaping human perceptions of readability through the analysis of 897 judgments,
finding that, beyond surface-level cues, information content and topic strongly
shape text comprehensibility. Furthermore, we evaluate 15 popular readability
metrics across five English datasets, contrasting them with six more nuanced,
model-based metrics. Our results show that four model-based metrics
consistently place among the top four in rank correlations with human
judgments, while the best performing traditional metric achieves an average
rank of 8.6. These findings highlight a mismatch between current readability
metrics and human perceptions, pointing to model-based approaches as a more
promising direction.

</details>


### [20] [When to Ensemble: Identifying Token-Level Points for Stable and Fast LLM Ensembling](https://arxiv.org/abs/2510.15346)
*Heecheol Yun,Kwangmin Ki,Junghyun Lee,Eunho Yang*

Main category: cs.CL

TL;DR: SAFE is a selective ensembling framework for LLMs that improves long-form generation by ensembling only at carefully chosen positions based on tokenization mismatch and probability distribution consensus, rather than at every token.


<details>
  <summary>Details</summary>
Motivation: Standard ensembling methods that aggregate next-token probabilities at every token often degrade performance in long-form generation, requiring a more selective approach.

Method: SAFE selectively ensembles LLMs by considering tokenization mismatch across models and consensus in next-token probability distributions, with a probability sharpening strategy to consolidate probabilities across sub-word tokens.

Result: SAFE outperforms existing methods on benchmarks like MATH500 and BBH in both accuracy and efficiency, achieving gains even when ensembling fewer than 1% of tokens.

Conclusion: Selective ensembling based on tokenization mismatch and probability consensus is crucial for effective LLM ensembling in long-form generation, with SAFE providing superior performance and efficiency.

Abstract: Ensembling Large Language Models (LLMs) has gained attention as a promising
approach to surpass the performance of individual models by leveraging their
complementary strengths. In particular, aggregating models' next-token
probability distributions to select the next token has been shown to be
effective in various tasks. However, while successful for short-form answers,
its application to long-form generation remains underexplored. In this paper,
we show that using existing ensemble methods in long-form generation requires a
careful choice of ensembling positions, since the standard practice of
ensembling at every token often degrades performance. We identify two key
factors for determining these positions: tokenization mismatch across models
and consensus in their next-token probability distributions. Based on this, we
propose SAFE, (Stable And Fast LLM Ensembling), a framework that selectively
ensembles by jointly considering these factors. To further improve stability,
we introduce a probability sharpening strategy that consolidates probabilities
spread across multiple sub-word tokens representing the same word into a single
representative token. Our experiments on diverse benchmarks, including MATH500
and BBH, demonstrate that SAFE outperforms existing methods in both accuracy
and efficiency, with gains achieved even when ensembling fewer than 1% of
tokens.

</details>


### [21] [Infinity Parser: Layout Aware Reinforcement Learning for Scanned Document Parsing](https://arxiv.org/abs/2510.15349)
*Baode Wang,Biao Wu,Weizhen Li,Meng Fang,Zuming Huang,Jun Huang,Haozhe Wang,Yanjie Liang,Ling Chen,Wei Chu,Yuan Qi*

Main category: cs.CL

TL;DR: LayoutRL is a reinforcement learning framework for document parsing that uses composite rewards to optimize layout understanding, trained on the Infinity-Doc-400K dataset to create Infinity-Parser, which achieves state-of-the-art performance across diverse document types.


<details>
  <summary>Details</summary>
Motivation: Existing supervised fine-tuning methods struggle with generalization across diverse document types and perform poorly on out-of-distribution data, exacerbated by limited high-quality training data for layout-aware parsing tasks.

Method: Introduces LayoutRL, a reinforcement learning framework that optimizes layout understanding through composite rewards integrating normalized edit distance, paragraph count accuracy, and reading order preservation. Uses the Infinity-Doc-400K dataset to train Infinity-Parser vision-language model.

Result: Infinity-Parser consistently achieves state-of-the-art performance across benchmarks including OmniDocBench, olmOCR-Bench, PubTabNet, and FinTabNet, substantially outperforming both specialized document parsing systems and general-purpose vision-language models.

Conclusion: The proposed LayoutRL framework with composite rewards and the Infinity-Doc-400K dataset enables robust document parsing that generalizes well across various domains, document types, languages, and structural complexities.

Abstract: Document parsing from scanned images into structured formats remains a
significant challenge due to its complexly intertwined elements such as text
paragraphs, figures, formulas, and tables. Existing supervised fine-tuning
methods often struggle to generalize across diverse document types, leading to
poor performance, particularly on out-of-distribution data. This issue is
further exacerbated by the limited availability of high-quality training data
for layout-aware parsing tasks. To address these challenges, we introduce
LayoutRL, a reinforcement learning framework that optimizes layout
understanding through composite rewards integrating normalized edit distance,
paragraph count accuracy, and reading order preservation. To support this
training, we construct the Infinity-Doc-400K dataset, which we use to train
Infinity-Parser, a vision-language model demonstrating robust generalization
across various domains. Extensive evaluations on benchmarks including
OmniDocBench, olmOCR-Bench, PubTabNet, and FinTabNet show that Infinity-Parser
consistently achieves state-of-the-art performance across a broad range of
document types, languages, and structural complexities, substantially
outperforming both specialized document parsing systems and general-purpose
vision-language models. We will release our code, dataset, and model to
facilitate reproducible research in document parsing.

</details>


### [22] [VocalBench-DF: A Benchmark for Evaluating Speech LLM Robustness to Disfluency](https://arxiv.org/abs/2510.15406)
*Hongcheng Liu,Yixuan Hou,Heyang Liu,Yuhao Wang,Yanfeng Wang,Yu Wang*

Main category: cs.CL

TL;DR: Speech-LLMs show significant performance degradation when handling speech disfluencies, particularly from users with conditions like Parkinson's disease, revealing limited real-world readiness despite strong performance on idealized inputs.


<details>
  <summary>Details</summary>
Motivation: Current Speech-LLM evaluations rely on idealized inputs and overlook common disfluencies, especially those associated with speech impairments like Parkinson's disease, raising concerns about their practical usability for diverse user populations.

Method: Introduced VocalBench-DF framework for systematic evaluation of disfluency across multi-dimensional taxonomy, evaluated 22 mainstream Speech-LLMs to assess robustness.

Result: Substantial performance degradation observed across tested Speech-LLMs, with phoneme-level processing and long-context modeling identified as primary bottlenecks causing failures.

Conclusion: Urgent need for new methods to improve disfluency handling and build truly inclusive Speech-LLMs, with strengthening recognition and reasoning capabilities showing potential for substantial robustness improvements.

Abstract: While Speech Large Language Models (Speech-LLMs) show strong performance in
many applications, their robustness is critically under-tested, especially to
speech disfluency. Existing evaluations often rely on idealized inputs,
overlooking common disfluencies, particularly those associated with conditions
like Parkinson's disease. This work investigates whether current Speech-LLMs
can maintain performance when interacting with users who have speech
impairments. To facilitate this inquiry, we introduce VocalBench-DF, a
framework for the systematic evaluation of disfluency across a
multi-dimensional taxonomy. Our evaluation of 22 mainstream Speech-LLMs reveals
substantial performance degradation, indicating that their real-world readiness
is limited. Further analysis identifies phoneme-level processing and
long-context modeling as primary bottlenecks responsible for these failures.
Strengthening recognition and reasoning capability from components and
pipelines can substantially improve robustness. These findings highlight the
urgent need for new methods to improve disfluency handling and build truly
inclusive Speech-LLMs

</details>


### [23] [Large-scale User Game Lifecycle Representation Learning](https://arxiv.org/abs/2510.15412)
*Yanjie Gou,Jiangming Liu,Kouying Xue,Yi Hua*

Main category: cs.CL

TL;DR: The paper proposes User Game Lifecycle (UGL) to address game sparsity and imbalance issues in game advertising and recommendation systems, with strategies for extracting user interests and handling popularity bias.


<details>
  <summary>Details</summary>
Motivation: Existing recommendation methods are unsuitable for game platforms due to game sparsity (few games) and game imbalance (user behaviors dominated by popular games), requiring specialized approaches for effective game advertising and recommendation.

Method: Introduces User Game Lifecycle (UGL) to enrich user behaviors, strategies for extracting short and long-term interests, and Inverse Probability Masking for handling game imbalance in representation learning.

Result: UGL representations achieved significant improvements: 1.83% AUC offline and 21.67% CVR online for game advertising; 0.5% AUC offline and 0.82% ARPU online for in-game item recommendation.

Conclusion: The proposed UGL framework effectively addresses game sparsity and imbalance challenges, demonstrating substantial performance improvements in both game advertising and in-game item recommendation systems.

Abstract: The rapid expansion of video game production necessitates the development of
effective advertising and recommendation systems for online game platforms.
Recommending and advertising games to users hinges on capturing their interest
in games. However, existing representation learning methods crafted for
handling billions of items in recommendation systems are unsuitable for game
advertising and recommendation. This is primarily due to game sparsity, where
the mere hundreds of games fall short for large-scale user representation
learning, and game imbalance, where user behaviors are overwhelmingly dominated
by a handful of popular games. To address the sparsity issue, we introduce the
User Game Lifecycle (UGL), designed to enrich user behaviors in games.
Additionally, we propose two innovative strategies aimed at manipulating user
behaviors to more effectively extract both short and long-term interests. To
tackle the game imbalance challenge, we present an Inverse Probability Masking
strategy for UGL representation learning. The offline and online experimental
results demonstrate that the UGL representations significantly enhance model by
achieving a 1.83% AUC offline increase on average and a 21.67% CVR online
increase on average for game advertising and a 0.5% AUC offline increase and a
0.82% ARPU online increase for in-game item recommendation.

</details>


### [24] [Fine-Tuning MedGemma for Clinical Captioning to Enhance Multimodal RAG over Malaysia CPGs](https://arxiv.org/abs/2510.15418)
*Lee Qi Zun,Mohamad Zulhilmi Bin Abdul Halim,Goh Man Fye*

Main category: cs.CL

TL;DR: This paper proposes a framework to specialize MedGemma model for generating high-fidelity medical image captions to improve Retrieval-Augmented Generation systems for Malaysian Clinical Practice Guidelines.


<details>
  <summary>Details</summary>
Motivation: Current Vision-Language Models lack clinical specificity and factual grounding for medical image queries, limiting the effectiveness of RAG systems in clinical decision support.

Method: Used knowledge distillation pipeline to create synthetic dataset across dermatology, fundus, and chest radiography domains, then fine-tuned MedGemma using QLoRA parameter-efficient method.

Result: Fine-tuned model showed substantial improvements in classification performance and significant gains in caption faithfulness, relevancy, and correctness as measured by RAGAS framework.

Conclusion: Established a robust pipeline for specializing medical VLMs and validated the model as a high-quality query generator for enhancing multimodal RAG systems in clinical decision support.

Abstract: Retrieval-Augmented Generation systems are essential for providing fact-based
guidance from Malaysian Clinical Practice Guidelines. However, their
effectiveness with image-based queries is limited, as general Vision-Language
Model captions often lack clinical specificity and factual grounding. This
study proposes and validates a framework to specialize the MedGemma model for
generating high-fidelity captions that serve as superior queries. To overcome
data scarcity, we employ a knowledge distillation pipeline to create a
synthetic dataset across dermatology, fundus, and chest radiography domains,
and fine-tune MedGemma using the parameter-efficient QLoRA method. Performance
was rigorously assessed through a dual framework measuring both classification
accuracy and, via a novel application of the RAGAS framework, caption
faithfulness, relevancy, and correctness. The fine-tuned model demonstrated
substantial improvements in classification performance, while RAGAS evaluation
confirmed significant gains in caption faithfulness and correctness, validating
the models ability to produce reliable, factually grounded descriptions. This
work establishes a robust pipeline for specializing medical VLMs and validates
the resulting model as a high-quality query generator, laying the groundwork
for enhancing multimodal RAG systems in evidence-based clinical decision
support.

</details>


### [25] [When Seeing Is not Enough: Revealing the Limits of Active Reasoning in MLLMs](https://arxiv.org/abs/2510.15421)
*Hongcheng Liu,Pingjie Wang,Yuhao Wang,Siqu Ou,Yanfeng Wang,Yu Wang*

Main category: cs.CL

TL;DR: The paper introduces GuessBench, a benchmark for evaluating active reasoning in multimodal large language models (MLLMs), showing that current MLLMs perform significantly worse on active reasoning tasks compared to passive inference.


<details>
  <summary>Details</summary>
Motivation: Current MLLM evaluations focus on passive inference with complete information, which misaligns with real-world scenarios where models need to actively acquire missing evidence under incomplete information.

Method: Proposed GuessBench benchmark with perception-oriented and knowledge-oriented images, requiring MLLMs to actively select target images from candidate pools and iteratively refine decisions without task-specific priors. Evaluated 20 superior MLLMs.

Result: Performance on active reasoning lags far behind passive inference settings. Key challenges identified are fine-grained perception and timely decision-making. Perceptual enhancements benefit smaller models, while thinking-oriented methods provide consistent gains across model sizes.

Conclusion: There is substantial room for improvement in MLLM active reasoning capabilities. The findings suggest promising research directions for multimodal active reasoning, particularly in enhancing perception and decision-making processes.

Abstract: Multimodal large language models (MLLMs) have shown strong capabilities
across a broad range of benchmarks. However, most existing evaluations focus on
passive inference, where models perform step-by-step reasoning under complete
information. This setup is misaligned with real-world use, where seeing is not
enough. This raises a fundamental question: Can MLLMs actively acquire missing
evidence under incomplete information? To bridge this gap, we require the MLLMs
to actively acquire missing evidence and iteratively refine decisions under
incomplete information, by selecting a target image from a candidate pool
without task-specific priors. To support systematic study, we propose
GuessBench, a benchmark with both perception-oriented and knowledge-oriented
images for evaluating active reasoning in MLLMs. We evaluate 20 superior MLLMs
and find that performance on active reasoning lags far behind it on passive
settings, indicating substantial room for improvement. Further analysis
identifies fine-grained perception and timely decision-making as key
challenges. Ablation studies show that perceptual enhancements benefit smaller
models, whereas thinking-oriented methods provide consistent gains across model
sizes. These results suggest promising directions for future research on
multimodal active reasoning.

</details>


### [26] [Controllable Abstraction in Summary Generation for Large Language Models via Prompt Engineering](https://arxiv.org/abs/2510.15436)
*Xiangchen Song,Yuchen Liu,Yaxuan Luan,Jinxu Guo,Xiaofan Guo*

Main category: cs.CL

TL;DR: A controllable abstract summary generation method using prompt engineering for large language models, with experiments showing optimal prompt length and the negative effects of data noise on summary quality.


<details>
  <summary>Details</summary>
Motivation: To address issues of summary quality and controllability in traditional methods by developing a controllable abstract summary generation approach.

Method: Multi-stage prompt generation framework that performs semantic analysis, topic modeling, and noise control on input text to generate summaries with varying abstraction levels.

Result: Prompt length significantly impacts summary quality (both very short and very long prompts decrease quality), data noise negatively affects ROUGE-L scores, and model performs best on news texts versus academic articles.

Conclusion: Provides insights for improving summary generation using large language models through controlled prompt strategies and optimized text preprocessing to enhance accuracy and controllability.

Abstract: This study presents a controllable abstract summary generation method for
large language models based on prompt engineering. To address the issues of
summary quality and controllability in traditional methods, we design a
multi-stage prompt generation framework. This framework generates summaries
with varying levels of abstraction by performing semantic analysis, topic
modeling, and noise control on the input text. The experiment uses the
CNN/Daily Mail dataset and provides a detailed analysis of different prompt
lengths, data noise, and text types. The experimental results show that prompt
length has a significant impact on the quality of generated summaries. Both
very short and very long prompt tokens result in a decrease in summary quality.
Data noise also negatively affects the summary generation process. As noise
levels increase, the ROUGE-L score gradually decreases. Furthermore, different
text types have varying effects on the model's ability to generate summaries.
The model performs best when handling news texts, while its performance is
worse when processing academic articles. This research provides new insights
into improving summary generation using large language models, particularly in
how controlling prompt strategies and optimizing text preprocessing can enhance
summary accuracy and controllability.

</details>


### [27] [CORE: Reducing UI Exposure in Mobile Agents via Collaboration Between Cloud and Local LLMs](https://arxiv.org/abs/2510.15455)
*Gucongcong Fan,Chaoyue Niu,Chengfei Lyu,Fan Wu,Guihai Chen*

Main category: cs.CL

TL;DR: CORE is a collaborative framework that combines cloud and local LLMs to reduce UI exposure while maintaining task accuracy for mobile agents, achieving up to 55.6% reduction in UI exposure.


<details>
  <summary>Details</summary>
Motivation: Cloud-based LLMs require uploading full UI states exposing unnecessary information, while local LLMs have limited capacity and lower success rates. There's a need to balance privacy protection with task performance.

Method: Three key components: layout-aware block partitioning to group related UI elements, co-planning where local and cloud LLMs collaboratively identify sub-tasks, and co-decision-making where local LLM ranks UI blocks and cloud LLM selects specific elements within top blocks.

Result: CORE reduces UI exposure by up to 55.6% while maintaining task success rates slightly below cloud-only agents, effectively mitigating unnecessary privacy exposure to the cloud.

Conclusion: CORE successfully balances privacy protection and task performance by leveraging collaborative cloud-local LLM framework, providing an effective solution for mobile agent privacy concerns.

Abstract: Mobile agents rely on Large Language Models (LLMs) to plan and execute tasks
on smartphone user interfaces (UIs). While cloud-based LLMs achieve high task
accuracy, they require uploading the full UI state at every step, exposing
unnecessary and often irrelevant information. In contrast, local LLMs avoid UI
uploads but suffer from limited capacity, resulting in lower task success
rates. We propose $\textbf{CORE}$, a $\textbf{CO}$llaborative framework that
combines the strengths of cloud and local LLMs to $\textbf{R}$educe UI
$\textbf{E}$xposure, while maintaining task accuracy for mobile agents. CORE
comprises three key components: (1) $\textbf{Layout-aware block partitioning}$,
which groups semantically related UI elements based on the XML screen
hierarchy; (2) $\textbf{Co-planning}$, where local and cloud LLMs
collaboratively identify the current sub-task; and (3)
$\textbf{Co-decision-making}$, where the local LLM ranks relevant UI blocks,
and the cloud LLM selects specific UI elements within the top-ranked block.
CORE further introduces a multi-round accumulation mechanism to mitigate local
misjudgment or limited context. Experiments across diverse mobile apps and
tasks show that CORE reduces UI exposure by up to 55.6% while maintaining task
success rates slightly below cloud-only agents, effectively mitigating
unnecessary privacy exposure to the cloud. The code is available at
https://github.com/Entropy-Fighter/CORE.

</details>


### [28] [DeceptionBench: A Comprehensive Benchmark for AI Deception Behaviors in Real-world Scenarios](https://arxiv.org/abs/2510.15501)
*Yao Huang,Yitong Sun,Yichi Zhang,Ruochen Zhang,Yinpeng Dong,Xingxing Wei*

Main category: cs.CL

TL;DR: DeceptionBench is the first benchmark that systematically evaluates deceptive behaviors in LLMs across five societal domains, revealing critical vulnerabilities and amplified deception under reinforcement dynamics.


<details>
  <summary>Details</summary>
Motivation: Despite LLMs' advances, their rapid enhancement introduces emergent deceptive behaviors that pose severe risks in high-stakes deployments, and characterization of deception across real-world scenarios remains underexplored.

Method: Established DeceptionBench with 150 scenarios across Economy, Healthcare, Education, Social Interaction, and Entertainment domains (over 1,000 samples). Evaluated intrinsic patterns (egoistic vs sycophantic behaviors) and extrinsic factors (neutral conditions, reward-based incentivization, coercive pressures) with multi-turn interaction loops.

Result: Extensive experiments across LLMs and LRMs reveal critical vulnerabilities, particularly amplified deception under reinforcement dynamics, showing models lack robust resistance to manipulative contextual cues.

Conclusion: Current models urgently need advanced safeguards against various deception behaviors, as demonstrated by their susceptibility to manipulative contextual cues and reinforcement dynamics.

Abstract: Despite the remarkable advances of Large Language Models (LLMs) across
diverse cognitive tasks, the rapid enhancement of these capabilities also
introduces emergent deceptive behaviors that may induce severe risks in
high-stakes deployments. More critically, the characterization of deception
across realistic real-world scenarios remains underexplored. To bridge this
gap, we establish DeceptionBench, the first benchmark that systematically
evaluates how deceptive tendencies manifest across different societal domains,
what their intrinsic behavioral patterns are, and how extrinsic factors affect
them. Specifically, on the static count, the benchmark encompasses 150
meticulously designed scenarios in five domains, i.e., Economy, Healthcare,
Education, Social Interaction, and Entertainment, with over 1,000 samples,
providing sufficient empirical foundations for deception analysis. On the
intrinsic dimension, we explore whether models exhibit self-interested egoistic
tendencies or sycophantic behaviors that prioritize user appeasement. On the
extrinsic dimension, we investigate how contextual factors modulate deceptive
outputs under neutral conditions, reward-based incentivization, and coercive
pressures. Moreover, we incorporate sustained multi-turn interaction loops to
construct a more realistic simulation of real-world feedback dynamics.
Extensive experiments across LLMs and Large Reasoning Models (LRMs) reveal
critical vulnerabilities, particularly amplified deception under reinforcement
dynamics, demonstrating that current models lack robust resistance to
manipulative contextual cues and the urgent need for advanced safeguards
against various deception behaviors. Code and resources are publicly available
at https://github.com/Aries-iai/DeceptionBench.

</details>


### [29] [Temporal Referential Consistency: Do LLMs Favor Sequences Over Absolute Time References?](https://arxiv.org/abs/2510.15513)
*Ashutosh Bajpai,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: This paper introduces TEMP-ReCon, a benchmark for evaluating temporal referential consistency in LLMs across multiple languages, and proposes a reasoning path alignment model to improve temporal consistency.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly used in time-sensitive domains like law and healthcare, requiring temporal consistency, but current efforts to ensure this are scarce.

Method: Proposed a reasoning path alignment-based model called UnTRaP to enhance temporal referential consistency in LLMs.

Result: Empirical experiments show LLMs exhibit insufficient temporal referential consistency, and UnTRaP outperforms several baseline models.

Conclusion: The paper addresses the gap in temporal consistency evaluation and enhancement for LLMs, demonstrating the effectiveness of the proposed approach.

Abstract: The increasing acceptance of large language models (LLMs) as an alternative
to knowledge sources marks a significant paradigm shift across various domains,
including time-sensitive fields such as law, healthcare, and finance. To
fulfill this expanded role, LLMs must not only be factually accurate but also
demonstrate consistency across temporal dimensions, necessitating robust
temporal reasoning capabilities. Despite this critical requirement, efforts to
ensure temporal consistency in LLMs remain scarce including noticeable absence
of endeavors aimed at evaluating or augmenting LLMs across temporal references
in time-sensitive inquiries. In this paper, we seek to address this gap by
introducing a novel benchmark entitled temporal referential consistency,
accompanied by a resource TEMP-ReCon designed to benchmark a wide range of both
open-source and closed-source LLMs with various linguistic contexts
characterized by differing resource richness (including English, French, and
Romanian). The findings emphasis that LLMs do exhibit insufficient temporal
referent consistency. To address this, we propose \newmodel, a reasoning path
alignment-based model that aims to enhance the temporal referential consistency
of LLMs. Our empirical experiments substantiate the efficacy of UnTRaP compared
to several baseline models.

</details>


### [30] [From Characters to Tokens: Dynamic Grouping with Hierarchical BPE](https://arxiv.org/abs/2510.15517)
*Rares Dolga,Lucas Maystre,Tudor Berariu,David Barber*

Main category: cs.CL

TL;DR: Proposes a dynamic character grouping method that enhances BPE tokenization by adding end-of-patch markers and a second-level BPE compression stage, achieving efficient and language-agnostic representations without additional models.


<details>
  <summary>Details</summary>
Motivation: Address inefficiencies in BPE tokenization for rare words and large embedding matrices, while avoiding performance bottlenecks of character-level models and dependencies of existing hierarchical approaches.

Method: Append explicit end-of-patch markers to BPE tokens and introduce a second-level BPE compression stage to control patch granularity, leveraging existing BPE structure without requiring auxiliary models.

Result: Matches or exceeds performance of dynamic entropy- and whitespace-based patching strategies while maintaining compact vocabulary.

Conclusion: The proposed method offers efficient, flexible, and language-agnostic representations that combine benefits of subword and character-level approaches without introducing new dependencies.

Abstract: Subword tokenization methods like Byte Pair Encoding (BPE) are widely used in
large language models due to their balance of vocabulary compactness and
representational power. However, they suffer from inefficiencies in
representing rare words and require large embedding matrices. Character-level
models address these issues but introduce performance bottlenecks, particularly
in Transformer-based architectures. Recent hierarchical models attempt to merge
the benefits of both paradigms by grouping characters into patches, but
existing patching strategies either rely on whitespace-limiting applicability
to certain languages, or require auxiliary models that introduce new
dependencies. In this paper, we propose a dynamic character grouping method
that leverages the structure of existing BPE tokenization without requiring
additional models. By appending explicit end-of-patch markers to BPE tokens and
introducing a second-level BPE compression stage to control patch granularity,
our method offers efficient, flexible, and language-agnostic representations.
Empirical results demonstrate that our approach matches or exceeds the
performance of dynamic entropy- and whitespace-based patching strategies, while
maintaining a compact vocabulary.

</details>


### [31] [Latent Reasoning in LLMs as a Vocabulary-Space Superposition](https://arxiv.org/abs/2510.15522)
*Jingcheng Deng,Liang Pang,Zihao Wei,Shichen Xu,Zenghao Duan,Kun Xu,Yang Song,Huawei Shen,Xueqi Cheng*

Main category: cs.CL

TL;DR: Latent-SFT is a two-stage learning framework that enables latent reasoning in LLMs by restricting the latent space to vocabulary probabilities, achieving similar performance to explicit reasoning while reducing computational costs by up to 4x.


<details>
  <summary>Details</summary>
Motivation: To reduce the substantial computational overhead of explicit chain-of-thought reasoning in LLMs while maintaining performance, addressing the performance degradation in existing latent reasoning methods caused by unstructured latent spaces.

Method: Two-stage framework: 1) Use specialized attention masks to guide latent token generation while producing correct answers, 2) Train LLM to autonomously generate latent tokens for reasoning using KL and CE losses, treating latent reasoning as vocabulary probability superposition.

Result: Sets new SOTA on GSM8k, matching explicit SFT performance while reducing reasoning chains by 4x. Outperforms prior latent methods on Math500 and AIME24. Shows latent reasoning as both compression of single paths and superposition of multiple paths.

Conclusion: Latent-SFT successfully enables efficient latent reasoning in LLMs by structuring the latent space around vocabulary probabilities, achieving computational efficiency without sacrificing performance compared to explicit reasoning methods.

Abstract: Large language models (LLMs) demonstrate strong reasoning abilities with
chain-of-thought prompting, but explicit reasoning introduces substantial
computational overhead. Recent work on latent reasoning reduces this cost by
reasoning in latent space without explicit supervision, but performance drops
significantly. Our preliminary experiments suggest that this degradation stems
from the unstructured latent space, which makes fitting latent tokens
difficult. To address this, we restrict the latent space to the column space of
the LLM vocabulary, treating latent reasoning as a superposition over
vocabulary probabilities. Once latent reasoning concludes, it collapses into an
eigenstate of explicit reasoning to yield the final answer. Based on this idea,
we propose Latent-SFT, a two-stage learning framework. In the first stage, we
design two specialized attention masks to guide the Latent Token Encoder in
generating latent tokens, allowing the LLM to produce the correct answer
conditioned on them. In the second stage, the Latent Token Encoder is
discarded, and the LLM is directly trained to generate these latent tokens
autonomously for latent reasoning, optimized with KL and CE losses. Latent-SFT
sets a new state of the art on GSM8k, matching explicit SFT performance while
cutting reasoning chains by up to 4 times and outperforming prior latent
methods. On Math500 and AIME24, lexical probability-based latent reasoning also
clearly surpasses hidden-state-based approaches. Our metrics of effective
compression rate and effective global parallelism further show that latent
reasoning is both the compression of a single path and the superposition of
multiple paths.

</details>


### [32] [MCA: Modality Composition Awareness for Robust Composed Multimodal Retrieval](https://arxiv.org/abs/2510.15543)
*Qiyu Wu,Shuyang Cui,Satoshi Hayakawa,Wei-Yao Wang,Hiromi Wakaki,Yuki Mitsufuji*

Main category: cs.CL

TL;DR: The paper proposes a modality composition awareness framework to improve robustness in multimodal retrieval using unified encoders, addressing modality shortcut issues in conventional contrastive learning.


<details>
  <summary>Details</summary>
Motivation: Unified encoders in multimodal large language models trained with conventional contrastive learning are prone to learn modality shortcuts, leading to poor robustness under distribution shifts.

Method: A modality composition awareness framework with preference loss (enforcing multimodal embeddings to outperform unimodal counterparts) and composition regularization (aligning multimodal embeddings with prototypes composed from unimodal parts).

Result: Experiments on various benchmarks show gains in out-of-distribution retrieval performance.

Conclusion: Modality composition awareness is an effective principle for robust composed multimodal retrieval when using MLLMs as unified encoders.

Abstract: Multimodal retrieval, which seeks to retrieve relevant content across
modalities such as text or image, supports applications from AI search to
contents production. Despite the success of separate-encoder approaches like
CLIP align modality-specific embeddings with contrastive learning, recent
multimodal large language models (MLLMs) enable a unified encoder that directly
processes composed inputs. While flexible and advanced, we identify that
unified encoders trained with conventional contrastive learning are prone to
learn modality shortcut, leading to poor robustness under distribution shifts.
We propose a modality composition awareness framework to mitigate this issue.
Concretely, a preference loss enforces multimodal embeddings to outperform
their unimodal counterparts, while a composition regularization objective
aligns multimodal embeddings with prototypes composed from its unimodal parts.
These objectives explicitly model structural relationships between the composed
representation and its unimodal counterparts. Experiments on various benchmarks
show gains in out-of-distribution retrieval, highlighting modality composition
awareness as a effective principle for robust composed multimodal retrieval
when utilizing MLLMs as the unified encoder.

</details>


### [33] [TokenTiming: A Dynamic Alignment Method for Universal Speculative Decoding Model Pairs](https://arxiv.org/abs/2510.15545)
*Sibo Xiao,Jinyuan Fu,Zhongle Xie,Lidan Shou*

Main category: cs.CL

TL;DR: TokenTiming enables universal speculative decoding for LLM acceleration by using Dynamic Time Warping to align draft and target token sequences with mismatched vocabularies, achieving 1.57x speedup without requiring retraining.


<details>
  <summary>Details</summary>
Motivation: Current speculative decoding methods are limited by requiring draft and target models to share the same vocabulary, restricting available draft models and often necessitating training new models from scratch.

Method: Proposes TokenTiming algorithm that re-encodes draft token sequences and uses Dynamic Time Warping (DTW) to build mappings for transferring probability distributions in speculative sampling, accommodating mismatched vocabularies.

Result: Achieves 1.57x speedup in comprehensive experiments across various tasks, working with any off-the-shelf models without retraining or modification.

Conclusion: Enables universal draft model selection for speculative decoding, making it a more versatile and practical tool for LLM acceleration by removing vocabulary matching constraints.

Abstract: Accelerating the inference of large language models (LLMs) has been a
critical challenge in generative AI. Speculative decoding (SD) substantially
improves LLM inference efficiency. However, its utility is limited by a
fundamental constraint: the draft and target models must share the same
vocabulary, thus limiting the herd of available draft models and often
necessitating the training of a new model from scratch. Inspired by Dynamic
Time Warping (DTW), a classic algorithm for aligning time series, we propose
the algorithm TokenTiming for universal speculative decoding. It operates by
re-encoding the draft token sequence to get a new target token sequence, and
then uses DTW to build a mapping to transfer the probability distributions for
speculative sampling. Benefiting from this, our method accommodates mismatched
vocabularies and works with any off-the-shelf models without retraining and
modification. We conduct comprehensive experiments on various tasks,
demonstrating 1.57x speedup. This work enables a universal approach for draft
model selection, making SD a more versatile and practical tool for LLM
acceleration.

</details>


### [34] [Rethinking Cross-lingual Gaps from a Statistical Viewpoint](https://arxiv.org/abs/2510.15551)
*Vihari Piratla,Purvam Jain,Darshan Singh,Partha Talukdar,Trevor Cohn*

Main category: cs.CL

TL;DR: The paper identifies response variance in target languages as the main cause of cross-lingual gaps in LLMs, proposes bias-variance decomposition to formalize this gap, and shows interventions that reduce variance can improve target language accuracy by 20-25%.


<details>
  <summary>Details</summary>
Motivation: To understand why LLMs show performance drops when knowledge is queried in target languages compared to source languages, challenging the existing view that latent representation divergence is the primary cause.

Method: Formalizes cross-lingual gap using bias-variance decomposition, conducts extensive experiments to validate the hypothesis, and implements inference-time interventions to control response variance through prompt instructions.

Result: Experimental evidence supports the variance hypothesis, and simple prompt instructions successfully reduce response variance, leading to 20-25% improvement in target language accuracy across different models.

Conclusion: Response variance in target languages is the main driver of cross-lingual gaps in LLMs, and controlling this variance through inference-time interventions can significantly bridge the performance gap between source and target languages.

Abstract: Any piece of knowledge is usually expressed in one or a handful of natural
languages on the web or in any large corpus. Large Language Models (LLMs) act
as a bridge by acquiring knowledge from a source language and making it
accessible when queried from target languages. Prior research has pointed to a
cross-lingual gap, viz., a drop in accuracy when the knowledge is queried in a
target language compared to when the query is in the source language. Existing
research has rationalized divergence in latent representations in source and
target languages as the source of cross-lingual gap. In this work, we take an
alternative view and hypothesize that the variance of responses in the target
language is the main cause of this gap. For the first time, we formalize the
cross-lingual gap in terms of bias-variance decomposition. We present extensive
experimental evidence which support proposed formulation and hypothesis. We
then reinforce our hypothesis through multiple inference-time interventions
that control the variance and reduce the cross-lingual gap. We demonstrate a
simple prompt instruction to reduce the response variance, which improved
target accuracy by 20-25% across different models.

</details>


### [35] [Think Parallax: Solving Multi-Hop Problems via Multi-View Knowledge-Graph-Based Retrieval-Augmented Generation](https://arxiv.org/abs/2510.15552)
*Jinliang Liu*

Main category: cs.CL

TL;DR: ParallaxRAG is a KG-RAG framework that decouples queries and graph triples into multi-view spaces, using specialized attention heads for different reasoning stages to reduce hallucination and improve multi-hop reasoning.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with multi-hop reasoning and hallucination, while existing KG-RAG methods rely on flat embeddings and noisy path exploration, limiting their effectiveness.

Method: Symmetrically decouples queries and graph triples into multi-view spaces, enforcing head diversity while constraining weakly related paths to construct cleaner subgraphs for step-wise reasoning.

Result: Competitive retrieval and QA performance on WebQSP and CWQ datasets, with reduced hallucination and good generalization using BGE-M3 + Llama3.1-8B setup.

Conclusion: Multi-view head specialization provides a principled direction for knowledge-grounded multi-hop reasoning, offering improved grounding and reduced hallucination in LLMs.

Abstract: Large language models (LLMs) excel at language understanding but often
hallucinate and struggle with multi-hop reasoning. Knowledge-graph-based
retrieval-augmented generation (KG-RAG) offers grounding, yet most methods rely
on flat embeddings and noisy path exploration. We propose ParallaxRAG, a
framework that symmetrically decouples queries and graph triples into
multi-view spaces, enabling a robust retrieval architecture that explicitly
enforces head diversity while constraining weakly related paths. Central to our
approach is the observation that different attention heads specialize in
semantic relations at distinct reasoning stages, contributing to different hops
of the reasoning chain. This specialization allows ParallaxRAG to construct
cleaner subgraphs and guide LLMs through grounded, step-wise reasoning.
Experiments on WebQSP and CWQ, under our unified, reproducible setup (BGE-M3 +
Llama3.1-8B), demonstrate competitive retrieval and QA performance, alongside
reduced hallucination and good generalization. Our results highlight multi-view
head specialization as a principled direction for knowledge-grounded multi-hop
reasoning. Our implementation will be released as soon as the paper is
accepted.

</details>


### [36] [KITE: A Benchmark for Evaluating Korean Instruction-Following Abilities in Large Language Models](https://arxiv.org/abs/2510.15558)
*Dongjun Kim,Chanhee Park,Chanjun Park,Heuiseok Lim*

Main category: cs.CL

TL;DR: KITE is a new benchmark for evaluating Korean instruction-following capabilities in LLMs, addressing the gap in existing English-focused evaluations by incorporating Korean linguistic and cultural nuances.


<details>
  <summary>Details</summary>
Motivation: Current LLM evaluations mainly focus on English, ignoring linguistic and cultural differences in other languages like Korean, which has unique syntax, morphology, honorifics, and numbering systems.

Method: Created KITE benchmark with diverse open-ended instruction-following tasks, using automated metrics combined with human assessments to evaluate both general and Korean-specific instructions.

Result: The evaluation revealed performance disparities across models and provided deeper insights into their strengths and weaknesses in handling Korean instructions.

Conclusion: KITE addresses the need for culturally and linguistically inclusive LLM evaluation and aims to inspire similar benchmarks for other underrepresented languages.

Abstract: The instruction-following capabilities of large language models (LLMs) are
pivotal for numerous applications, from conversational agents to complex
reasoning systems. However, current evaluations predominantly focus on English
models, neglecting the linguistic and cultural nuances of other languages.
Specifically, Korean, with its distinct syntax, rich morphological features,
honorific system, and dual numbering systems, lacks a dedicated benchmark for
assessing open-ended instruction-following capabilities. To address this gap,
we introduce the Korean Instruction-following Task Evaluation (KITE), a
comprehensive benchmark designed to evaluate both general and Korean-specific
instructions. Unlike existing Korean benchmarks that focus mainly on factual
knowledge or multiple-choice testing, KITE directly targets diverse, open-ended
instruction-following tasks. Our evaluation pipeline combines automated metrics
with human assessments, revealing performance disparities across models and
providing deeper insights into their strengths and weaknesses. By publicly
releasing the KITE dataset and code, we aim to foster further research on
culturally and linguistically inclusive LLM development and inspire similar
endeavors for other underrepresented languages.

</details>


### [37] [Finetuning LLMs for EvaCun 2025 token prediction shared task](https://arxiv.org/abs/2510.15561)
*Josef Jon,Ondřej Bojar*

Main category: cs.CL

TL;DR: The authors submitted LLM-based systems (Command-R, Mistral, Aya Expanse) for EvaCun 2025 token prediction task, using straightforward fine-tuning without domain-specific adjustments.


<details>
  <summary>Details</summary>
Motivation: To participate in the EvaCun 2025 token prediction task despite limited knowledge of the subject field and task languages.

Method: Fine-tuned three LLMs (Command-R, Mistral, Aya Expanse) on provided training data without preprocessing, and compared three different prompting approaches.

Result: Evaluated the three prompting approaches on held-out data, though specific performance metrics are not provided in the abstract.

Conclusion: Demonstrated a baseline approach using off-the-shelf LLM fine-tuning for token prediction tasks without domain expertise.

Abstract: In this paper, we present our submission for the token prediction task of
EvaCun 2025. Our sys-tems are based on LLMs (Command-R, Mistral, and Aya
Expanse) fine-tuned on the task data provided by the organizers. As we only
pos-sess a very superficial knowledge of the subject field and the languages of
the task, we simply used the training data without any task-specific
adjustments, preprocessing, or filtering. We compare 3 different approaches
(based on 3 different prompts) of obtaining the predictions, and we evaluate
them on a held-out part of the data.

</details>


### [38] [From Ghazals to Sonnets: Decoding the Polysemous Expressions of Love Across Languages](https://arxiv.org/abs/2510.15569)
*Syed Mohammad Sualeh Ali*

Main category: cs.CL

TL;DR: Analysis of Urdu poetry's thematic depth through polysemy, focusing on three love-related words (pyaar, muhabbat, ishq) to reveal nuanced emotional distinctions unique to Urdu language.


<details>
  <summary>Details</summary>
Motivation: To explore the intricate thematic depths of Urdu poetry and uncover the subtle distinctions between seemingly synonymous love-related words that lack direct English equivalents.

Method: Polysemic case study approach analyzing word usage in Urdu poetry, combined with comparative analysis using word embeddings for Urdu and English love terms to visualize semantic spaces.

Result: Uncovered hidden layers of meaning and subtle distinctions between the three Urdu words, revealing a spectrum of emotions unique to Urdu language that cannot be directly translated to English.

Conclusion: The study provides deeper understanding of Urdu poetry's unique portrayal of love, highlighting cultural and linguistic nuances through polysemic analysis and computational methods.

Abstract: This paper delves into the intricate world of Urdu poetry, exploring its
thematic depths through a lens of polysemy. By focusing on the nuanced
differences between three seemingly synonymous words (pyaar, muhabbat, and
ishq) we expose a spectrum of emotions and experiences unique to the Urdu
language. This study employs a polysemic case study approach, meticulously
examining how these words are interwoven within the rich tapestry of Urdu
poetry. By analyzing their usage and context, we uncover a hidden layer of
meaning, revealing subtle distinctions which lack direct equivalents in English
literature. Furthermore, we embark on a comparative analysis, generating word
embeddings for both Urdu and English terms related to love. This enables us to
quantify and visualize the semantic space occupied by these words, providing
valuable insights into the cultural and linguistic nuances of expressing love.
Through this multifaceted approach, our study sheds light on the captivating
complexities of Urdu poetry, offering a deeper understanding and appreciation
for its unique portrayal of love and its myriad expressions

</details>


### [39] [BiMax: Bidirectional MaxSim Score for Document-Level Alignment](https://arxiv.org/abs/2510.15577)
*Xiaotian Wang,Takehito Utsuro,Masaaki Nagata*

Main category: cs.CL

TL;DR: Proposes BiMax, a cross-lingual document alignment method that achieves comparable accuracy to Optimal Transport with 100x speed improvement.


<details>
  <summary>Details</summary>
Motivation: Need for efficient document alignment methods that balance both accuracy and speed for large-scale web mining applications.

Method: Developed cross-lingual Bidirectional Maxsim score (BiMax) for computing document-to-document similarity, improving efficiency over Optimal Transport methods.

Result: On WMT16 bilingual document alignment task, BiMax achieved comparable accuracy to OT with approximately 100-fold speed increase.

Conclusion: BiMax provides an efficient alternative to OT for document alignment, and all methods are available as the EmbDA tool.

Abstract: Document alignment is necessary for the hierarchical mining (Ba\~n\'on et
al., 2020; Morishita et al., 2022), which aligns documents across source and
target languages within the same web domain. Several high precision sentence
embedding-based methods have been developed, such as TK-PERT (Thompson and
Koehn, 2020) and Optimal Transport (OT) (Clark et al., 2019; El-Kishky and
Guzm\'an, 2020). However, given the massive scale of web mining data, both
accuracy and speed must be considered. In this paper, we propose a
cross-lingual Bidirectional Maxsim score (BiMax) for computing doc-to-doc
similarity, to improve efficiency compared to the OT method. Consequently, on
the WMT16 bilingual document alignment task, BiMax attains accuracy comparable
to OT with an approximate 100-fold speed increase. Meanwhile, we also conduct a
comprehensive analysis to investigate the performance of current
state-of-the-art multilingual sentence embedding models. All the alignment
methods in this paper are publicly available as a tool called EmbDA
(https://github.com/EternalEdenn/EmbDA).

</details>


### [40] [The Elephant in the Coreference Room: Resolving Coreference in Full-Length French Fiction Works](https://arxiv.org/abs/2510.15594)
*Antoine Bourgois,Thierry Poibeau*

Main category: cs.CL

TL;DR: The paper introduces a new annotated corpus of three full-length French novels for coreference resolution, addressing the scarcity of long document datasets in computational literature research.


<details>
  <summary>Details</summary>
Motivation: Coreference resolution is gaining interest but lacks representative datasets of fully annotated long documents, especially for complex literary works with long reference chains.

Method: Created a new annotated corpus of three French novels (285k+ tokens) and developed a modular coreference resolution pipeline for fine-grained error analysis.

Result: The approach is competitive, scales effectively to long documents, and can infer the gender of fictional characters.

Conclusion: The corpus and pipeline are useful for both literary analysis and downstream NLP tasks, addressing the gap in long document coreference resolution datasets.

Abstract: While coreference resolution is attracting more interest than ever from
computational literature researchers, representative datasets of fully
annotated long documents remain surprisingly scarce. In this paper, we
introduce a new annotated corpus of three full-length French novels, totaling
over 285,000 tokens. Unlike previous datasets focused on shorter texts, our
corpus addresses the challenges posed by long, complex literary works, enabling
evaluation of coreference models in the context of long reference chains. We
present a modular coreference resolution pipeline that allows for fine-grained
error analysis. We show that our approach is competitive and scales effectively
to long documents. Finally, we demonstrate its usefulness to infer the gender
of fictional characters, showcasing its relevance for both literary analysis
and downstream NLP tasks.

</details>


### [41] [HypoSpace: Evaluating LLM Creativity as Set-Valued Hypothesis Generators under Underdetermination](https://arxiv.org/abs/2510.15614)
*Tingting Chen,Beibei Lin,Zifeng Yuan,Qiran Zou,Hongyu He,Yew-Soon Ong,Anirudh Goyal,Dianbo Liu*

Main category: cs.CL

TL;DR: HypoSpace is a diagnostic suite that evaluates LLMs' ability to propose multiple valid explanations for underdetermined scientific problems, measuring Validity, Uniqueness, and Recovery across structured domains.


<details>
  <summary>Details</summary>
Motivation: As language models are increasingly used in scientific workflows, there's a need to evaluate their ability to propose sets of explanations rather than single answers, since many scientific problems are underdetermined with multiple valid hypotheses.

Method: HypoSpace treats LLMs as samplers of finite hypothesis sets and measures three indicators: Validity (precision), Uniqueness (non-redundancy), and Recovery (coverage). It's instantiated in three structured domains with deterministic validators and exactly enumerated hypothesis spaces: causal graphs, 3D voxel reconstruction, and Boolean genetic interactions.

Result: Across instruction-tuned and reasoning-focused models, Validity often remains high while Uniqueness and Recovery degrade as the admissible space grows, revealing mode collapse that is invisible to correctness-only metrics.

Conclusion: HypoSpace offers a controlled probe for methods that explicitly explore and cover admissible explanation spaces, providing insights beyond traditional correctness metrics.

Abstract: As language models are increasingly used in scientific workflows, evaluating
their ability to propose sets of explanations-not just a single correct
answer-becomes critical. Many scientific problems are underdetermined:
multiple, mechanistically distinct hypotheses are consistent with the same
observations. We introduce HypoSpace, a diagnostic suite that treats LLMs as
samplers of finite hypothesis sets and measures three complementary indicators:
Validity (precision of proposals consistent with observations), Uniqueness
(non-redundancy among proposals), and Recovery (coverage of the enumerated
admissible set). We instantiate HypoSpace in three structured domains with
deterministic validators and exactly enumerated hypothesis spaces: (i) causal
graphs from perturbations, (ii) gravity-constrained 3D voxel reconstruction
from top-down projections, and (iii) Boolean genetic interactions. Across
instruction-tuned and reasoning-focused models, Validity often remains high
while Uniqueness and Recovery degrade as the admissible space grows, revealing
mode collapse that is invisible to correctness-only metrics. HypoSpace offers a
controlled probe-rather than a leaderboard-for methods that explicitly explore
and cover admissible explanation spaces. Code is available at:
https://github.com/CTT-Pavilion/_HypoSpace.

</details>


### [42] [Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection](https://arxiv.org/abs/2510.15685)
*Joshua Wolfe Brook,Ilia Markov*

Main category: cs.CL

TL;DR: Using LLMs as dynamic knowledge bases to generate background context for hate speech detection, with two context generation strategies and four incorporation methods tested on textual and multimodal datasets.


<details>
  <summary>Details</summary>
Motivation: To improve hate speech detection by leveraging LLMs to provide contextual background information that helps classifiers better understand implicit hate speech in both textual and multimodal content.

Method: Two context generation strategies (named entities and full-text prompting) and four context incorporation methods (text concatenation, embedding concatenation, hierarchical transformer fusion, and LLM-driven text enhancement) were tested on Latent Hatred and MAMI datasets.

Result: Contextual information and incorporation method are crucial, achieving gains of up to 3 F1 points on textual data and 6 F1 points on multimodal data compared to zero-context baseline, with embedding concatenation performing best.

Conclusion: LLM-generated context significantly enhances hate speech detection performance, with the method of context incorporation being as important as the context itself, particularly for multimodal content.

Abstract: This research introduces a novel approach to textual and multimodal Hate
Speech Detection (HSD), using Large Language Models (LLMs) as dynamic knowledge
bases to generate background context and incorporate it into the input of HSD
classifiers. Two context generation strategies are examined: one focused on
named entities and the other on full-text prompting. Four methods of
incorporating context into the classifier input are compared: text
concatenation, embedding concatenation, a hierarchical transformer-based
fusion, and LLM-driven text enhancement. Experiments are conducted on the
textual Latent Hatred dataset of implicit hate speech and applied in a
multimodal setting on the MAMI dataset of misogynous memes. Results suggest
that both the contextual information and the method by which it is incorporated
are key, with gains of up to 3 and 6 F1 points on textual and multimodal setups
respectively, from a zero-context baseline to the highest-performing system,
based on embedding concatenation.

</details>


### [43] [Cost-Aware Retrieval-Augmentation Reasoning Models with Adaptive Retrieval Depth](https://arxiv.org/abs/2510.15719)
*Helia Hashemi,Victor Rühle,Saravan Rajmohan*

Main category: cs.CL

TL;DR: A retrieval-augmented reasoning model that dynamically adjusts retrieved document length using cost-aware reinforcement learning, achieving 16-20% latency reduction and 5% effectiveness improvement.


<details>
  <summary>Details</summary>
Motivation: Retrieval-augmented reasoning models have high computational costs due to both retrieval and reasoning tokens, creating a need for more efficient approaches.

Method: Proposed dynamic retrieval length adjustment, cost-aware advantage function for RL training, and memory/latency-bound implementations for policy optimization algorithms.

Result: 16-20% latency reduction across datasets while increasing effectiveness by ~5% in exact match on seven QA datasets.

Conclusion: The approach achieves significant efficiency gains without compromising effectiveness, demonstrating the viability of cost-aware optimization for retrieval-augmented reasoning.

Abstract: Reasoning models have gained significant attention due to their strong
performance, particularly when enhanced with retrieval augmentation. However,
these models often incur high computational costs, as both retrieval and
reasoning tokens contribute substantially to the overall resource usage. In
this work, we make the following contributions: (1) we propose a
retrieval-augmented reasoning model that dynamically adjusts the length of the
retrieved document list based on the query and retrieval results; (2) we
develop a cost-aware advantage function for training of efficient
retrieval-augmented reasoning models through reinforcement learning; and (3) we
explore both memory- and latency-bound implementations of the proposed
cost-aware framework for both proximal and group relative policy optimization
algorithms. We evaluate our approach on seven public question answering
datasets and demonstrate significant efficiency gains, without compromising
effectiveness. In fact, we observed that the model latency decreases by ~16-20%
across datasets, while its effectiveness increases by ~5% on average, in terms
of exact match.

</details>


### [44] [Attention Sinks in Diffusion Language Models](https://arxiv.org/abs/2510.15731)
*Maximo Eduardo Rulli,Simone Petruzzi,Edoardo Michielon,Fabrizio Silvestri,Simone Scardapane,Alessio Devoto*

Main category: cs.CL

TL;DR: Masked Diffusion Language Models exhibit dynamic attention sinking behavior that shifts during generation, unlike ARMs, and remain robust when sinks are masked.


<details>
  <summary>Details</summary>
Motivation: To understand the internal mechanisms and attention patterns in Diffusion Language Models, particularly the attention sinking phenomenon and how it differs from Autoregressive Models.

Method: Conducted empirical analysis of DLM attention patterns, focusing on attention sinking phenomenon and comparing with ARMs through masking experiments.

Result: DLMs show dynamic attention sinks that shift during generation, and unlike ARMs, they remain robust with only minor performance degradation when sinks are masked.

Conclusion: DLMs have fundamentally different attention allocation mechanisms compared to ARMs, with dynamic and robust attention sinking behavior.

Abstract: Masked Diffusion Language Models (DLMs) have recently emerged as a promising
alternative to traditional Autoregressive Models (ARMs). DLMs employ
transformer encoders with bidirectional attention, enabling parallel token
generation while maintaining competitive performance. Although their efficiency
and effectiveness have been extensively studied, the internal mechanisms that
govern DLMs remain largely unexplored. In this work, we conduct an empirical
analysis of DLM attention patterns, focusing on the attention sinking
phenomenon, an effect previously observed in various transformer-based
architectures. Our findings reveal that DLMs also exhibit attention sinks, but
with distinct characteristics. First, unlike in ARMs, the sink positions in
DLMs tend to shift throughout the generation process, displaying a dynamic
behaviour. Second, while ARMs are highly sensitive to the removal of attention
sinks, DLMs remain robust: masking sinks leads to only a minor degradation in
performance. These results provide new insights into the inner workings of
diffusion-based language models and highlight fundamental differences in how
they allocate and utilize attention compared to autoregressive models.

</details>


### [45] [LLMs Judge Themselves: A Game-Theoretic Framework for Human-Aligned Evaluation](https://arxiv.org/abs/2510.15746)
*Gao Yang,Yuhang Liu,Siyu Miao,Xinyue Liang,Zhengyang Liu,Heyan Huang*

Main category: cs.CL

TL;DR: This paper proposes using game theory principles for LLM evaluation through automatic mutual assessment where LLMs evaluate each other's outputs, with results compared to human judgments.


<details>
  <summary>Details</summary>
Motivation: Conventional LLM evaluation methods are inadequate for capturing nuanced, subjective, and open-ended modern LLM behavior, as they rely on fixed-format tasks with reference answers.

Method: Proposes automatic mutual evaluation where LLMs assess each other through self-play and peer review, using game-theoretic voting algorithms to aggregate peer reviews and compare with human voting behavior.

Result: Empirical results show both convergences and divergences between theoretical predictions and human evaluations, revealing insights into the promises and limitations of mutual evaluation.

Conclusion: This is the first work to jointly integrate mutual evaluation, game-theoretic aggregation, and human-grounded validation for evaluating LLM capabilities, offering a novel alternative to conventional evaluation practices.

Abstract: Ideal or real - that is the question.In this work, we explore whether
principles from game theory can be effectively applied to the evaluation of
large language models (LLMs). This inquiry is motivated by the growing
inadequacy of conventional evaluation practices, which often rely on
fixed-format tasks with reference answers and struggle to capture the nuanced,
subjective, and open-ended nature of modern LLM behavior. To address these
challenges, we propose a novel alternative: automatic mutual evaluation, where
LLMs assess each other's output through self-play and peer review. These peer
assessments are then systematically compared with human voting behavior to
evaluate their alignment with human judgment. Our framework incorporates
game-theoretic voting algorithms to aggregate peer reviews, enabling a
principled investigation into whether model-generated rankings reflect human
preferences. Empirical results reveal both convergences and divergences between
theoretical predictions and human evaluations, offering valuable insights into
the promises and limitations of mutual evaluation. To the best of our
knowledge, this is the first work to jointly integrate mutual evaluation,
game-theoretic aggregation, and human-grounded validation for evaluating the
capabilities of LLMs.

</details>


### [46] [On Non-interactive Evaluation of Animal Communication Translators](https://arxiv.org/abs/2510.15768)
*Orr Paradise,David F. Gruber,Adam Tauman Kalai*

Main category: cs.CL

TL;DR: Proposes a reference-free method to evaluate AI animal translators using segment-by-segment translation and shuffle test, showing it correlates well with standard metrics without needing interaction or observations.


<details>
  <summary>Details</summary>
Motivation: To develop safe, ethical, and cost-effective methods for validating AI animal translators without requiring direct animal interaction or grounded observations.

Method: Uses segment-by-segment translation with shuffle test - translating animal communication turn by turn and evaluating if translations make more sense in original order than permuted order.

Result: Proof-of-concept experiments on human languages show high correlation with standard reference-based evaluation metrics. Theoretical analysis suggests interaction may be unnecessary in early translation learning stages.

Conclusion: Reference-free evaluation using segment ordering coherence can effectively validate animal translators without animal interaction, offering safety and ethical advantages.

Abstract: If you had an AI Whale-to-English translator, how could you validate whether
or not it is working? Does one need to interact with the animals or rely on
grounded observations such as temperature? We provide theoretical and
proof-of-concept experimental evidence suggesting that interaction and even
observations may not be necessary for sufficiently complex languages. One may
be able to evaluate translators solely by their English outputs, offering
potential advantages in terms of safety, ethics, and cost. This is an instance
of machine translation quality evaluation (MTQE) without any reference
translations available. A key challenge is identifying ``hallucinations,''
false translations which may appear fluent and plausible. We propose using
segment-by-segment translation together with the classic NLP shuffle test to
evaluate translators. The idea is to translate animal communication, turn by
turn, and evaluate how often the resulting translations make more sense in
order than permuted. Proof-of-concept experiments on data-scarce human
languages and constructed languages demonstrate the potential utility of this
evaluation methodology. These human-language experiments serve solely to
validate our reference-free metric under data scarcity. It is found to
correlate highly with a standard evaluation based on reference translations,
which are available in our experiments. We also perform a theoretical analysis
suggesting that interaction may not be necessary nor efficient in the early
stages of learning to translate.

</details>


### [47] [Emergence of Linear Truth Encodings in Language Models](https://arxiv.org/abs/2510.15804)
*Shauli Ravfogel,Gilad Yehudai,Tal Linzen,Joan Bruna,Alberto Bietti*

Main category: cs.CL

TL;DR: A one-layer transformer toy model demonstrates how linear truth subspaces emerge in language models through co-occurrence patterns of factual statements, revealing a two-phase learning dynamic.


<details>
  <summary>Details</summary>
Motivation: To understand the mechanism behind the emergence of linear subspaces that separate true from false statements in large language models, which has been observed in probing studies but remains unclear.

Method: Introduce a transparent one-layer transformer toy model that reproduces truth subspaces end-to-end, studying a data distribution where factual statements co-occur with other factual statements, encouraging the model to learn this distinction to lower LM loss.

Result: The toy model successfully reproduces truth subspaces and reveals a two-phase learning dynamic: networks first memorize individual factual associations quickly, then learn to linearly separate true from false over a longer horizon, which lowers language modeling loss.

Conclusion: The study provides both a mechanistic demonstration and empirical motivation for how and why linear truth representations emerge in language models through co-occurrence patterns and learning dynamics.

Abstract: Recent probing studies reveal that large language models exhibit linear
subspaces that separate true from false statements, yet the mechanism behind
their emergence is unclear. We introduce a transparent, one-layer transformer
toy model that reproduces such truth subspaces end-to-end and exposes one
concrete route by which they can arise. We study one simple setting in which
truth encoding can emerge: a data distribution where factual statements
co-occur with other factual statements (and vice-versa), encouraging the model
to learn this distinction in order to lower the LM loss on future tokens. We
corroborate this pattern with experiments in pretrained language models.
Finally, in the toy setting we observe a two-phase learning dynamic: networks
first memorize individual factual associations in a few steps, then -- over a
longer horizon -- learn to linearly separate true from false, which in turn
lowers language-modeling loss. Together, these results provide both a
mechanistic demonstration and an empirical motivation for how and why linear
truth representations can emerge in language models.

</details>


### [48] [Paper2Web: Let's Make Your Paper Alive!](https://arxiv.org/abs/2510.15842)
*Yuhang Chen,Tianpeng Lv,Siyi Zhang,Yixiang Yin,Yao Wan,Philip S. Yu,Dongping Chen*

Main category: cs.CL

TL;DR: Paper2Web is a benchmark for academic webpage generation with multi-dimensional evaluation metrics, and PWAgent is an autonomous pipeline that converts papers into interactive websites, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current approaches for academic project websites struggle to produce layout-aware, interactive sites, and there's a lack of comprehensive evaluation frameworks for this task.

Method: Introduces Paper2Web benchmark with rule-based metrics (Connectivity, Completeness), LLM-as-a-Judge evaluation, and PaperQuiz for knowledge retention. Presents PWAgent pipeline that iteratively refines content and layout using MCP tools.

Result: PWAgent consistently outperforms end-to-end baselines like template-based webpages and arXiv/alphaXiv versions by a large margin while maintaining low cost.

Conclusion: PWAgent achieves the Pareto-front in academic webpage generation, providing an effective solution for converting scientific papers into interactive, multimedia-rich academic homepages.

Abstract: Academic project websites can more effectively disseminate research when they
clearly present core content and enable intuitive navigation and interaction.
However, current approaches such as direct Large Language Model (LLM)
generation, templates, or direct HTML conversion struggle to produce
layout-aware, interactive sites, and a comprehensive evaluation suite for this
task has been lacking. In this paper, we introduce Paper2Web, a benchmark
dataset and multi-dimensional evaluation framework for assessing academic
webpage generation. It incorporates rule-based metrics like Connectivity,
Completeness and human-verified LLM-as-a-Judge (covering interactivity,
aesthetics, and informativeness), and PaperQuiz, which measures paper-level
knowledge retention. We further present PWAgent, an autonomous pipeline that
converts scientific papers into interactive and multimedia-rich academic
homepages. The agent iteratively refines both content and layout through MCP
tools that enhance emphasis, balance, and presentation quality. Our experiments
show that PWAgent consistently outperforms end-to-end baselines like
template-based webpages and arXiv/alphaXiv versions by a large margin while
maintaining low cost, achieving the Pareto-front in academic webpage
generation.

</details>


### [49] [Enhanced Sentiment Interpretation via a Lexicon-Fuzzy-Transformer Framework](https://arxiv.org/abs/2510.15843)
*Shayan Rokhva,Mousa Alizadeh,Maryam Abdollahi Shamami*

Main category: cs.CL

TL;DR: A hybrid lexicon-fuzzy-transformer framework combining rule-based heuristics, contextual deep learning, and fuzzy logic for continuous sentiment scoring that captures both polarity and intensity in product reviews and social media.


<details>
  <summary>Details</summary>
Motivation: To address challenges in accurately detecting sentiment polarity and intensity in informal, domain-specific language found in product reviews and social media posts.

Method: A pipeline starting with VADER-based initial sentiment estimations, refined through two-stage adjustment using DistilBERT confidence scores and fuzzy logic principles, with a custom fuzzy inference system mapping scores to 0-1 continuum.

Result: Improved alignment with user ratings, better identification of sentiment extremes, reduced misclassifications across four domain-specific datasets (food delivery, e-commerce, tourism, fashion), with both quantitative metrics and qualitative insights confirming robustness and efficiency.

Conclusion: The work demonstrates the value of integrating symbolic reasoning with neural models for interpretable, fine-grained sentiment analysis in linguistically dynamic domains.

Abstract: Accurately detecting sentiment polarity and intensity in product reviews and
social media posts remains challenging due to informal and domain-specific
language. To address this, we propose a novel hybrid lexicon-fuzzy-transformer
framework that combines rule-based heuristics, contextual deep learning, and
fuzzy logic to generate continuous sentiment scores reflecting both polarity
and strength. The pipeline begins with VADER-based initial sentiment
estimations, which are refined through a two-stage adjustment process. This
involves leveraging confidence scores from DistilBERT, a lightweight
transformer and applying fuzzy logic principles to mitigate excessive
neutrality bias and enhance granularity. A custom fuzzy inference system then
maps the refined scores onto a 0 to 1 continuum, producing expert)like
judgments. The framework is rigorously evaluated on four domain-specific
datasets. food delivery, e-commerce, tourism, and fashion. Results show
improved alignment with user ratings, better identification of sentiment
extremes, and reduced misclassifications. Both quantitative metrics
(distributional alignment, confusion matrices) and qualitative insights (case
studies, runtime analysis) affirm the models robustness and efficiency. This
work demonstrates the value of integrating symbolic reasoning with neural
models for interpretable, finegrained sentiment analysis in linguistically
dynamic domains.

</details>


### [50] [SpeechLLMs for Large-scale Contextualized Zero-shot Slot Filling](https://arxiv.org/abs/2510.15851)
*Kadri Hacioglu,Manjunath K E,Andreas Stolcke*

Main category: cs.CL

TL;DR: The paper analyzes slot filling in spoken language understanding using speech-based large language models, identifies performance gaps, and proposes improvements to training data, architecture, and training strategies to bridge these gaps.


<details>
  <summary>Details</summary>
Motivation: Traditional slot filling uses cascaded speech recognition and NLU components, but speechLLMs offer unified, generative approaches with zero-shot abilities and better generalization to unseen slot labels.

Method: Created empirical upper bound for slot filling, identified performance gaps, and proposed improvements to training data, architecture, and training strategies for speechLLMs.

Result: Each proposed improvement substantially enhanced performance, though practical challenges remain in harnessing these emerging models.

Conclusion: The study provides empirical guidance and insights for effectively using speechLLMs in slot filling tasks while highlighting remaining practical challenges.

Abstract: Slot filling is a crucial subtask in spoken language understanding (SLU),
traditionally implemented as a cascade of speech recognition followed by one or
more natural language understanding (NLU) components. The recent advent of
speech-based large language models (speechLLMs), which integrate speech and
textual foundation models, has opened new avenues for achieving speech
understanding tasks in a more unified, generative, and instruction-following
manner while promising data and compute efficiency with zero-shot abilities,
generalizing to unseen slot labels. We address the slot-filling task by
creating an empirical upper bound for the task, identifying performance,
robustness, and generalization gaps, and proposing improvements to the training
data, architecture, and training strategies to narrow the gap with the upper
bound result. We show that each of these measures improve performance
substantially, while highlighting practical challenges and providing empirical
guidance and insights for harnessing these emerging models.

</details>


### [51] [InfiMed-ORBIT: Aligning LLMs on Open-Ended Complex Tasks via Rubric-Based Incremental Training](https://arxiv.org/abs/2510.15859)
*Pengkai Wang,Qi Zuo,Pengwei Liu,Zhijie Sang,Congkai Xie,Hongxia Yang*

Main category: cs.CL

TL;DR: ORBIT is a rubric-based incremental training framework that enhances LLM performance in open-ended domains like medical consultation through synthetic dialogue generation and dynamic rubric creation, achieving state-of-the-art results on HealthBench-Hard.


<details>
  <summary>Details</summary>
Motivation: Current RL methods for LLMs work well in domains with clear, programmatically verifiable rewards (like math/code) but struggle in open-ended domains where rewards are ambiguous, subjective, or context-dependent, particularly in high-stakes areas like medical consultation.

Method: ORBIT integrates synthetic dialogue generation with dynamic rubric creation, using these rubrics to guide an incremental reinforcement learning process without relying on external medical knowledge or manual rules.

Result: Implemented on Qwen3-4B-Instruct model, ORBIT improved performance on HealthBench-Hard benchmark from 7.0 to 27.2 using only 2k samples, achieving state-of-the-art results for models of this scale with consistent gains across diverse consultation scenarios.

Conclusion: Rubric-based feedback serves as a scalable strategy for advancing LLMs in complex, open-ended tasks, demonstrating effectiveness beyond simple numerical improvements in intricate domains like medical dialogue.

Abstract: Large Language Models (LLMs) have shown substantial advances through
reinforcement learning (RL), particularly in domains where rewards can be
programmatically verified, such as mathematics and code. In these areas, models
benefit from a well-defined operational base guided by explicit rule-based
objectives. However, this progress reveals a significant limitation: in
open-ended domains where rewards are ambiguous, subjective, or
context-dependent, such as creative writing, scientific reasoning, and notably
medical consultation, robust reward functions are lacking, making these areas
challenging for current RL strategies. To bridge this gap, we introduce ORBIT,
an open-ended rubric-based incremental training framework specifically designed
for high-stakes medical dialogue. ORBIT integrates syn- thetic dialogue
generation with the dynamic creation of rubrics, employing these rubrics to
direct an incremental RL process. In particular, this approach does not depend
on external medical knowledge or manual rules, instead utilizing rubric-guided
feedback to shape learning. When implemented on the Qwen3-4B-Instruct model,
our method can greatly enhance its performance on the HealthBench-Hard
benchmark from 7.0 to 27.2 using only 2k samples, thus achieving
state-of-the-art results for models of this scale. Our analysis confirms that
rubric-driven RL fos-ters consistent performance gains across diverse
consultation scenarios, going beyond simple numerical improvements. These
findings underscore rubric-based feedback as a scalable strategy for advancing
LLMs in intricate, open-ended tasks.

</details>


### [52] [PolySkill: Learning Generalizable Skills Through Polymorphic Abstraction](https://arxiv.org/abs/2510.15863)
*Simon Yu,Gang Li,Weiyan Shi,Peng Qi*

Main category: cs.CL

TL;DR: PolySkill enables LLM agents to learn generalizable, compositional skills by decoupling abstract goals from concrete implementations, improving skill reuse and success rates across websites.


<details>
  <summary>Details</summary>
Motivation: Current skill learning methods create over-specialized skills that fail to generalize across different websites, limiting agent adaptability in continual learning scenarios.

Method: PolySkill framework inspired by software polymorphism, separating a skill's abstract goal (what it accomplishes) from its concrete implementation (how it's executed).

Result: 1.7x skill reuse improvement on seen websites, 9.4% success rate boost on Mind2Web, 13.9% on unseen websites, 20% step reduction, and better task quality in self-exploration settings.

Conclusion: Decoupling skill goals from execution is crucial for developing autonomous agents capable of continual learning and generalization across the open web.

Abstract: Large language models (LLMs) are moving beyond static uses and are now
powering agents that learn continually during their interaction with external
environments. For example, agents can learn reusable skills while navigating
web pages or toggling new tools. However, existing methods for skill learning
often create skills that are over-specialized to a single website and fail to
generalize. We introduce PolySkill, a new framework that enables agents to
learn generalizable and compositional skills. The core idea, inspired by
polymorphism in software engineering, is to decouple a skill's abstract goal
(what it accomplishes) and its concrete implementation (how it is executed).
Experiments show that our method (1) improves skill reuse by 1.7x on seen
websites and (2) boosts success rates by up to 9.4% on Mind2Web and 13.9% on
unseen websites, while reducing steps by over 20%. (3) In self-exploration
settings without specified tasks, our framework improves the quality of
proposed tasks and enables agents to learn generalizable skills that work
across different sites. By enabling the agent to identify and refine its own
goals, the PolySkill enhances the agent's ability to learn a better curriculum,
leading to the acquisition of more generalizable skills compared to baseline
methods. This work provides a practical path toward building agents capable of
continual learning in adaptive environments. Our findings show that separating
a skill's goal from its execution is a crucial step toward developing
autonomous agents that can learn and generalize across the open web
continuously.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [53] [GAZE:Governance-Aware pre-annotation for Zero-shot World Model Environments](https://arxiv.org/abs/2510.14992)
*Leela Krishna,Mengyang Zhao,Saicharithreddy Pasula,Harshit Rajgarhia,Abhishek Mukherji*

Main category: cs.CV

TL;DR: GAZE pipeline automates conversion of raw video into rich supervision for world-model training using AI models for multimodal pre-annotation, achieving 80% reduction in human review volume.


<details>
  <summary>Details</summary>
Motivation: Manual annotation for world model training is slow and expensive, creating bottlenecks in creating large-scale multimodal datasets.

Method: Pipeline that normalizes 360-degree video formats, applies AI models (scene understanding, object tracking, audio transcription, content detection), and consolidates signals into structured output for human validation.

Result: Efficiency gains of ~19 minutes saved per review hour, >80% reduction in human review volume through auto-skipping low-salience segments, generates high-fidelity privacy-aware datasets.

Conclusion: Provides scalable blueprint for generating high-quality world model training data without sacrificing throughput or governance.

Abstract: Training robust world models requires large-scale, precisely labeled
multimodal datasets, a process historically bottlenecked by slow and expensive
manual annotation. We present a production-tested GAZE pipeline that automates
the conversion of raw, long-form video into rich, task-ready supervision for
world-model training. Our system (i) normalizes proprietary 360-degree formats
into standard views and shards them for parallel processing; (ii) applies a
suite of AI models (scene understanding, object tracking, audio transcription,
PII/NSFW/minor detection) for dense, multimodal pre-annotation; and (iii)
consolidates signals into a structured output specification for rapid human
validation.
  The GAZE workflow demonstrably yields efficiency gains (~19 minutes saved per
review hour) and reduces human review volume by >80% through conservative
auto-skipping of low-salience segments. By increasing label density and
consistency while integrating privacy safeguards and chain-of-custody metadata,
our method generates high-fidelity, privacy-aware datasets directly consumable
for learning cross-modal dynamics and action-conditioned prediction. We detail
our orchestration, model choices, and data dictionary to provide a scalable
blueprint for generating high-quality world model training data without
sacrificing throughput or governance.

</details>


### [54] [PC-UNet: An Enforcing Poisson Statistics U-Net for Positron Emission Tomography Denoising](https://arxiv.org/abs/2510.14995)
*Yang Shi,Jingchao Wang,Liangsi Lu,Mingxuan Huang,Ruixin He,Yifeng Xie,Hanqian Liu,Minzhe Guo,Yangyang Liang,Weipeng Zhang,Zimeng Li,Xuhang Chen*

Main category: cs.CV

TL;DR: PC-UNet with PVMC-Loss improves PET image denoising by incorporating physical data constraints, reducing Poisson noise while maintaining image fidelity.


<details>
  <summary>Details</summary>
Motivation: PET imaging faces limitations due to high radiation doses from high signal-to-noise ratio requirements. Lower doses introduce Poisson noise that current denoising methods cannot handle effectively, leading to distortions and artifacts.

Method: Proposed Poisson Consistent U-Net (PC-UNet) with a novel Poisson Variance and Mean Consistency Loss (PVMC-Loss) that incorporates physical data constraints. PVMC-Loss is statistically unbiased in variance and gradient adaptation, functioning as a Generalized Method of Moments implementation.

Result: Tests on PET datasets demonstrate that PC-UNet improves physical consistency and image fidelity compared to existing methods.

Conclusion: PC-UNet effectively integrates physical information into PET image denoising, providing robust performance even with minor data mismatches.

Abstract: Positron Emission Tomography (PET) is crucial in medicine, but its clinical
use is limited due to high signal-to-noise ratio doses increasing radiation
exposure. Lowering doses increases Poisson noise, which current denoising
methods fail to handle, causing distortions and artifacts. We propose a Poisson
Consistent U-Net (PC-UNet) model with a new Poisson Variance and Mean
Consistency Loss (PVMC-Loss) that incorporates physical data to improve image
fidelity. PVMC-Loss is statistically unbiased in variance and gradient
adaptation, acting as a Generalized Method of Moments implementation, offering
robustness to minor data mismatches. Tests on PET datasets show PC-UNet
improves physical consistency and image fidelity, proving its ability to
integrate physical information effectively.

</details>


### [55] [DeLeaker: Dynamic Inference-Time Reweighting For Semantic Leakage Mitigation in Text-to-Image Models](https://arxiv.org/abs/2510.15015)
*Mor Ventura,Michael Toker,Or Patashnik,Yonatan Belinkov,Roi Reichart*

Main category: cs.CV

TL;DR: DeLeaker is a lightweight, optimization-free inference-time method that mitigates semantic leakage in Text-to-Image models by dynamically reweighting attention maps to suppress cross-entity interactions while preserving entity identity.


<details>
  <summary>Details</summary>
Motivation: Text-to-Image models suffer from semantic leakage - unintended transfer of semantically related features between distinct entities, and existing mitigation strategies are often optimization-based or require external inputs.

Method: DeLeaker intervenes directly on the model's attention maps during the diffusion process, dynamically reweighting them to suppress excessive cross-entity interactions while strengthening each entity's identity.

Result: DeLeaker consistently outperforms all baselines, even those with external information, achieving effective leakage mitigation without compromising fidelity or quality. The paper also introduces SLIM, the first dataset for semantic leakage evaluation with 1,130 human-verified samples.

Conclusion: Attention control is valuable for mitigating semantic leakage, paving the way for more semantically precise Text-to-Image models.

Abstract: Text-to-Image (T2I) models have advanced rapidly, yet they remain vulnerable
to semantic leakage, the unintended transfer of semantically related features
between distinct entities. Existing mitigation strategies are often
optimization-based or dependent on external inputs. We introduce DeLeaker, a
lightweight, optimization-free inference-time approach that mitigates leakage
by directly intervening on the model's attention maps. Throughout the diffusion
process, DeLeaker dynamically reweights attention maps to suppress excessive
cross-entity interactions while strengthening the identity of each entity. To
support systematic evaluation, we introduce SLIM (Semantic Leakage in IMages),
the first dataset dedicated to semantic leakage, comprising 1,130
human-verified samples spanning diverse scenarios, together with a novel
automatic evaluation framework. Experiments demonstrate that DeLeaker
consistently outperforms all baselines, even when they are provided with
external information, achieving effective leakage mitigation without
compromising fidelity or quality. These results underscore the value of
attention control and pave the way for more semantically precise T2I models.

</details>


### [56] [UrbanVerse: Scaling Urban Simulation by Watching City-Tour Videos](https://arxiv.org/abs/2510.15018)
*Mingxuan Liu,Honglin He,Elisa Ricci,Wayne Wu,Bolei Zhou*

Main category: cs.CV

TL;DR: UrbanVerse is a data-driven real-to-sim system that converts crowd-sourced city-tour videos into physics-aware, interactive simulation scenes for training urban embodied AI agents.


<details>
  <summary>Details</summary>
Motivation: Training urban embodied AI agents requires diverse, high-fidelity urban environments, but existing simulation scenes lack scalability or fail to capture real-world complexity.

Method: UrbanVerse consists of UrbanVerse-100K (repository of 100k+ annotated urban 3D assets) and UrbanVerse-Gen (automatic pipeline that extracts scene layouts from videos and instantiates metric-scale 3D simulations using retrieved assets).

Result: UrbanVerse offers 160 high-quality scenes from 24 countries, achieves human-evaluated realism comparable to manually crafted scenes, and improves navigation success by +6.3% in simulation and +30.1% in zero-shot sim-to-real transfer.

Conclusion: UrbanVerse enables scalable training of urban embodied AI agents with strong generalization capabilities, successfully accomplishing real-world missions with minimal interventions.

Abstract: Urban embodied AI agents, ranging from delivery robots to quadrupeds, are
increasingly populating our cities, navigating chaotic streets to provide
last-mile connectivity. Training such agents requires diverse, high-fidelity
urban environments to scale, yet existing human-crafted or procedurally
generated simulation scenes either lack scalability or fail to capture
real-world complexity. We introduce UrbanVerse, a data-driven real-to-sim
system that converts crowd-sourced city-tour videos into physics-aware,
interactive simulation scenes. UrbanVerse consists of: (i) UrbanVerse-100K, a
repository of 100k+ annotated urban 3D assets with semantic and physical
attributes, and (ii) UrbanVerse-Gen, an automatic pipeline that extracts scene
layouts from video and instantiates metric-scale 3D simulations using retrieved
assets. Running in IsaacSim, UrbanVerse offers 160 high-quality constructed
scenes from 24 countries, along with a curated benchmark of 10 artist-designed
test scenes. Experiments show that UrbanVerse scenes preserve real-world
semantics and layouts, achieving human-evaluated realism comparable to manually
crafted scenes. In urban navigation, policies trained in UrbanVerse exhibit
scaling power laws and strong generalization, improving success by +6.3% in
simulation and +30.1% in zero-shot sim-to-real transfer comparing to prior
methods, accomplishing a 300 m real-world mission with only two interventions.

</details>


### [57] [NANO3D: A Training-Free Approach for Efficient 3D Editing Without Masks](https://arxiv.org/abs/2510.15019)
*Junliang Ye,Shenghao Xie,Ruowen Zhao,Zhengyi Wang,Hongyu Yan,Wenqiang Zu,Lei Ma,Jun Zhu*

Main category: cs.CV

TL;DR: Nano3D is a training-free framework for precise 3D object editing that integrates FlowEdit with TRELLIS and introduces region-aware merging strategies (Voxel/Slat-Merge) to maintain structural fidelity without requiring masks.


<details>
  <summary>Details</summary>
Motivation: Current 3D editing methods are inefficient, inconsistent, and often damage unedited regions. Most rely on multi-view rendering followed by reconstruction, which introduces artifacts and limits practicality.

Method: Integrates FlowEdit into TRELLIS for localized edits guided by front-view renderings, and introduces region-aware merging strategies (Voxel/Slat-Merge) that adaptively preserve structural fidelity by ensuring consistency between edited and unedited areas.

Result: Nano3D achieves superior 3D consistency and visual quality compared to existing methods. The authors also created Nano3D-Edit-100k, the first large-scale 3D editing dataset with over 100,000 high-quality 3D editing pairs.

Conclusion: This work addresses long-standing challenges in both algorithm design and data availability, significantly improving the generality and reliability of 3D editing, and lays the groundwork for developing feed-forward 3D editing models.

Abstract: 3D object editing is essential for interactive content creation in gaming,
animation, and robotics, yet current approaches remain inefficient,
inconsistent, and often fail to preserve unedited regions. Most methods rely on
editing multi-view renderings followed by reconstruction, which introduces
artifacts and limits practicality. To address these challenges, we propose
Nano3D, a training-free framework for precise and coherent 3D object editing
without masks. Nano3D integrates FlowEdit into TRELLIS to perform localized
edits guided by front-view renderings, and further introduces region-aware
merging strategies, Voxel/Slat-Merge, which adaptively preserve structural
fidelity by ensuring consistency between edited and unedited areas. Experiments
demonstrate that Nano3D achieves superior 3D consistency and visual quality
compared with existing methods. Based on this framework, we construct the first
large-scale 3D editing datasets Nano3D-Edit-100k, which contains over 100,000
high-quality 3D editing pairs. This work addresses long-standing challenges in
both algorithm design and data availability, significantly improving the
generality and reliability of 3D editing, and laying the groundwork for the
development of feed-forward 3D editing models. Project
Page:https://jamesyjl.github.io/Nano3D

</details>


### [58] [Constantly Improving Image Models Need Constantly Improving Benchmarks](https://arxiv.org/abs/2510.15021)
*Jiaxin Ge,Grace Luo,Heekyung Lee,Nishant Malpani,Long Lian,XuDong Wang,Aleksander Holynski,Trevor Darrell,Sewon Min,David M. Chan*

Main category: cs.CV

TL;DR: ECHO is a framework for creating image generation benchmarks from real-world social media posts, revealing novel capabilities and providing better model differentiation than traditional benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lag behind rapidly advancing image generation capabilities and fail to capture emerging real-world use cases, creating a gap between community perceptions and formal evaluation.

Method: Constructed benchmarks directly from social media posts showcasing novel prompts and user judgments, creating a dataset of over 31,000 prompts from GPT-4o Image Gen usage.

Result: ECHO discovered creative tasks absent from existing benchmarks, better distinguished state-of-the-art models, and surfaced community feedback for designing quality metrics.

Conclusion: ECHO provides a framework for creating timely, relevant benchmarks that better reflect real-world model usage and emerging capabilities in image generation.

Abstract: Recent advances in image generation, often driven by proprietary systems like
GPT-4o Image Gen, regularly introduce new capabilities that reshape how users
interact with these models. Existing benchmarks often lag behind and fail to
capture these emerging use cases, leaving a gap between community perceptions
of progress and formal evaluation. To address this, we present ECHO, a
framework for constructing benchmarks directly from real-world evidence of
model use: social media posts that showcase novel prompts and qualitative user
judgments. Applying this framework to GPT-4o Image Gen, we construct a dataset
of over 31,000 prompts curated from such posts. Our analysis shows that ECHO
(1) discovers creative and complex tasks absent from existing benchmarks, such
as re-rendering product labels across languages or generating receipts with
specified totals, (2) more clearly distinguishes state-of-the-art models from
alternatives, and (3) surfaces community feedback that we use to inform the
design of metrics for model quality (e.g., measuring observed shifts in color,
identity, and structure). Our website is at https://echo-bench.github.io.

</details>


### [59] [LoRAverse: A Submodular Framework to Retrieve Diverse Adapters for Diffusion Models](https://arxiv.org/abs/2510.15022)
*Mert Sonmezer,Matthew Zheng,Pinar Yanardag*

Main category: cs.CV

TL;DR: This paper proposes a submodular framework for selecting relevant and diverse LoRA models from large databases to address navigation challenges in the vast collection of available adapters.


<details>
  <summary>Details</summary>
Motivation: Users struggle to navigate and select suitable LoRA adapters from over 100K available models due to volume, diversity, and lack of structured organization, despite LoRA's effectiveness in personalizing diffusion models.

Method: The paper frames LoRA model selection as a combinatorial optimization problem and introduces a novel submodular framework to identify the most relevant and diverse adapters.

Result: Quantitative and qualitative experiments show the method successfully generates diverse outputs across various domains.

Conclusion: The proposed submodular framework effectively addresses the challenge of selecting optimal LoRA models from large databases, enabling better utilization of available adapters for personalized content generation.

Abstract: Low-rank Adaptation (LoRA) models have revolutionized the personalization of
pre-trained diffusion models by enabling fine-tuning through low-rank,
factorized weight matrices specifically optimized for attention layers. These
models facilitate the generation of highly customized content across a variety
of objects, individuals, and artistic styles without the need for extensive
retraining. Despite the availability of over 100K LoRA adapters on platforms
like Civit.ai, users often face challenges in navigating, selecting, and
effectively utilizing the most suitable adapters due to their sheer volume,
diversity, and lack of structured organization. This paper addresses the
problem of selecting the most relevant and diverse LoRA models from this vast
database by framing the task as a combinatorial optimization problem and
proposing a novel submodular framework. Our quantitative and qualitative
experiments demonstrate that our method generates diverse outputs across a wide
range of domains.

</details>


### [60] [MOBIUS: Big-to-Mobile Universal Instance Segmentation via Multi-modal Bottleneck Fusion and Calibrated Decoder Pruning](https://arxiv.org/abs/2510.15026)
*Mattia Segu,Marta Tintore Gazulla,Yongqin Xian,Luc Van Gool,Federico Tombari*

Main category: cs.CV

TL;DR: MOBIUS is a family of foundation models for universal instance segmentation designed for efficient edge deployment, achieving up to 55% pixel decoder and 75% transformer decoder FLOPs reduction while maintaining state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Current foundation models for instance-level perception have high computational costs that limit adoption on resource-constrained platforms, creating a need for efficient models that don't compromise performance.

Method: Proposes three key innovations: (i) bottleneck pixel decoder for efficient multi-scale and multi-modal fusion, (ii) language-guided uncertainty calibration loss for adaptive decoder pruning, and (iii) streamlined unified training strategy.

Result: MOBIUS reduces pixel decoder FLOPs by up to 55% and transformer decoder FLOPs by up to 75% while maintaining state-of-the-art performance, achieving this in only one-third of the training iterations compared to existing methods.

Conclusion: MOBIUS establishes a new benchmark for efficient segmentation across both high-performance computing platforms and mobile devices, enabling Pareto-optimal downscaling for various hardware capabilities.

Abstract: Scaling up model size and training data has advanced foundation models for
instance-level perception, achieving state-of-the-art in-domain and zero-shot
performance across object detection and segmentation. However, their high
computational cost limits adoption on resource-constrained platforms. We first
examine the limitations of existing architectures in enabling efficient edge
deployment without compromising performance. We then introduce MOBIUS, a family
of foundation models for universal instance segmentation, designed for
Pareto-optimal downscaling to support deployment across devices ranging from
high-end accelerators to mobile hardware. To reduce training and inference
demands, we propose: (i) a bottleneck pixel decoder for efficient multi-scale
and multi-modal fusion, (ii) a language-guided uncertainty calibration loss for
adaptive decoder pruning, and (iii) a streamlined, unified training strategy.
Unlike efficient baselines that trade accuracy for reduced complexity, MOBIUS
reduces pixel and transformer decoder FLOPs by up to 55% and 75%, respectively,
while maintaining state-of-the-art performance in just a third of the training
iterations. MOBIUS establishes a new benchmark for efficient segmentation on
both high-performance computing platforms and mobile devices.

</details>


### [61] [Composition-Grounded Instruction Synthesis for Visual Reasoning](https://arxiv.org/abs/2510.15040)
*Xinyi Gu,Jiayuan Mao,Zhang-Wei Hong,Zhuoran Yu,Pengyuan Li,Dhiraj Joshi,Rogerio Feris,Zexue He*

Main category: cs.CV

TL;DR: COGS is a framework that enhances MLLMs' reasoning capabilities for artificial image domains by decomposing seed questions into primitive factors and systematically recomposing them with new images to generate synthetic training data.


<details>
  <summary>Details</summary>
Motivation: MLLMs lack reasoning capabilities in domains with scarce human annotations like charts, documents, and webpages, despite the abundance of such artificial image domains in practice.

Method: Decompose seed questions into primitive perception and reasoning factors, then systematically recompose them with new images to generate synthetic question-answer pairs with subquestions and intermediate answers for reinforcement learning.

Result: COGS substantially improves performance on unseen chart reasoning questions, especially on reasoning-heavy and compositional questions, and enables better transfer across datasets.

Conclusion: COGS induces generalizable reasoning capabilities rather than dataset-specific overfitting and extends beyond charts to other domains like webpages.

Abstract: Pretrained multi-modal large language models (MLLMs) demonstrate strong
performance on diverse multimodal tasks, but remain limited in reasoning
capabilities for domains where annotations are difficult to collect. In this
work, we focus on artificial image domains such as charts, rendered documents,
and webpages, which are abundant in practice yet lack large-scale human
annotated reasoning datasets. We introduce COGS (COmposition-Grounded
instruction Synthesis), a data-efficient framework for equipping MLLMs with
advanced reasoning abilities from a small set of seed questions. The key idea
is to decompose each seed question into primitive perception and reasoning
factors, which can then be systematically recomposed with new images to
generate large collections of synthetic question-answer pairs. Each generated
question is paired with subquestions and intermediate answers, enabling
reinforcement learning with factor-level process rewards. Experiments on chart
reasoning show that COGS substantially improves performance on unseen
questions, with the largest gains on reasoning-heavy and compositional
questions. Moreover, training with a factor-level mixture of different seed
data yields better transfer across multiple datasets, suggesting that COGS
induces generalizable capabilities rather than dataset-specific overfitting. We
further demonstrate that the framework extends beyond charts to other domains
such as webpages.

</details>


### [62] [Generalized Dynamics Generation towards Scannable Physical World Model](https://arxiv.org/abs/2510.15041)
*Yichen Li,Zhiyi Li,Brandon Feng,Dinghuai Zhang,Antonio Torralba*

Main category: cs.CV

TL;DR: GDGen is a framework that unifies rigid, articulated, and soft body dynamics using a potential energy perspective, enabling geometry-agnostic physical simulation from motion observations.


<details>
  <summary>Details</summary>
Motivation: To develop generalist embodied agents in digital twin worlds with realistic interactive dynamics by creating a unified system that can handle diverse physical behaviors in scannable environments.

Method: Takes a potential energy perspective, extends classic elastodynamics with directional stiffness, uses a specialized network for material properties, and employs neural fields for geometry-agnostic deformation representation.

Result: GDGen robustly unifies diverse simulation paradigms and provides a versatile foundation for interactive virtual environments and robotic agent training in complex dynamic scenarios.

Conclusion: The framework successfully integrates multiple physical simulation paradigms into a unified system, offering new opportunities for developing embodied agents in dynamically rich virtual environments.

Abstract: Digital twin worlds with realistic interactive dynamics presents a new
opportunity to develop generalist embodied agents in scannable environments
with complex physical behaviors. To this end, we present GDGen (Generalized
Representation for Generalized Dynamics Generation), a framework that takes a
potential energy perspective to seamlessly integrate rigid body, articulated
body, and soft body dynamics into a unified, geometry-agnostic system. GDGen
operates from the governing principle that the potential energy for any stable
physical system should be low. This fresh perspective allows us to treat the
world as one holistic entity and infer underlying physical properties from
simple motion observations. We extend classic elastodynamics by introducing
directional stiffness to capture a broad spectrum of physical behaviors,
covering soft elastic, articulated, and rigid body systems. We propose a
specialized network to model the extended material property and employ a neural
field to represent deformation in a geometry-agnostic manner. Extensive
experiments demonstrate that GDGen robustly unifies diverse simulation
paradigms, offering a versatile foundation for creating interactive virtual
environments and training robotic agents in complex, dynamically rich
scenarios.

</details>


### [63] [Comprehensive language-image pre-training for 3D medical image understanding](https://arxiv.org/abs/2510.15042)
*Tassilo Wald,Ibrahim Ethem Hamamci,Yuan Gao,Sam Bond-Taylor,Harshita Sharma,Maximilian Ilse,Cynthia Lo,Olesya Melnichenko,Noel C. F. Codella,Maria Teodora Wetscherek,Klaus H. Maier-Hein,Panagiotis Korfiatis,Valentina Salvatelli,Javier Alvarez-Valle,Fernando Pérez-García*

Main category: cs.CV

TL;DR: The paper introduces COLIPRI, a vision-language pre-training method for 3D medical images that addresses data scarcity by incorporating report generation objectives and combining vision-language with vision-only pre-training.


<details>
  <summary>Details</summary>
Motivation: Current 3D vision-language encoders in medical imaging are limited by data availability, which restricts their capabilities for tasks like abnormality retrieval and prediction.

Method: Proposed COLIPRI method that injects inductive biases through report generation objectives and pairs vision-language pre-training with vision-only pre-training, leveraging both image-only and paired image-text datasets.

Result: COLIPRI encoders achieve state-of-the-art performance in report generation, classification probing, and zero-shot classification, while remaining competitive for semantic segmentation.

Conclusion: The approach successfully addresses data limitations in 3D medical vision-language pre-training by incorporating additional inductive biases and domain best practices.

Abstract: Vision-language pre-training, i.e., aligning images with paired text, is a
powerful paradigm to create encoders that can be directly used for tasks such
as classification and retrieval, and for downstream tasks such as segmentation
and report generation. In the 3D medical image domain, these capabilities allow
vision-language encoders (VLEs) to support radiologists by retrieving patients
with similar abnormalities or predicting likelihoods of abnormality. While the
methodology holds promise, data availability limits the capabilities of current
3D VLEs.
  In this paper, we alleviate the lack of data by injecting additional
inductive biases: introducing a report generation objective and pairing
vision-language pre-training with vision-only pre-training. This allows us to
leverage both image-only and paired image-text 3D datasets, increasing the
total amount of data to which our model is exposed. Through these additional
inductive biases, paired with best practices of the 3D medical imaging domain,
we develop the Comprehensive Language-image Pre-training (COLIPRI) encoder
family. Our COLIPRI encoders achieve state-of-the-art performance in report
generation, classification probing, and zero-shot classification, and remain
competitive for semantic segmentation.

</details>


### [64] [Directional Reasoning Injection for Fine-Tuning MLLMs](https://arxiv.org/abs/2510.15050)
*Chao Huang,Zeliang Zhang,Jiang Liu,Ximeng Sun,Jialian Wu,Xiaodong Yu,Ze Wang,Chenliang Xu,Emad Barsoum,Zicheng Liu*

Main category: cs.CV

TL;DR: DRIFT is a lightweight method that transfers reasoning knowledge from text-only LLMs to MLLMs through gradient-space injection, avoiding the need for resource-intensive supervised fine-tuning or reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: MLLMs lag behind text-only LLMs in reasoning ability, and existing methods to bridge this gap are resource-intensive. Model merging shows inconsistent results across different model families.

Method: DRIFT precomputes a reasoning prior as parameter-space difference between reasoning and multimodal variants, then uses it to bias gradients during multimodal fine-tuning, preserving multimodal alignment.

Result: DRIFT consistently improves reasoning performance over naive merging and supervised fine-tuning on benchmarks like MathVista and MathVerse, matching or surpassing training-heavy methods at lower cost.

Conclusion: DRIFT provides an efficient and effective approach to enhance MLLM reasoning capabilities without destabilizing multimodal alignment, offering a practical alternative to resource-intensive methods.

Abstract: Multimodal large language models (MLLMs) are rapidly advancing, yet their
reasoning ability often lags behind that of strong text-only counterparts.
Existing methods to bridge this gap rely on supervised fine-tuning over
large-scale multimodal reasoning data or reinforcement learning, both of which
are resource-intensive. A promising alternative is model merging, which
interpolates parameters between reasoning-enhanced LLMs and multimodal
variants. However, our analysis shows that naive merging is not always a "free
lunch": its effectiveness varies drastically across model families, with some
(e.g., LLaVA, Idefics) benefiting while others (e.g., Qwen) suffer performance
degradation. To address this, we propose Directional Reasoning Injection for
Fine-Tuning (DRIFT) MLLMs, a lightweight method that transfers reasoning
knowledge in the gradient space, without destabilizing multimodal alignment.
DRIFT precomputes a reasoning prior as the parameter-space difference between
reasoning and multimodal variants, then uses it to bias gradients during
multimodal fine-tuning. This approach preserves the simplicity of standard
supervised fine-tuning pipelines while enabling efficient reasoning transfer.
Extensive experiments on multimodal reasoning benchmarks, including MathVista
and MathVerse, demonstrate that DRIFT consistently improves reasoning
performance over naive merging and supervised fine-tuning, while matching or
surpassing training-heavy methods at a fraction of the cost.

</details>


### [65] [A solution to generalized learning from small training sets found in everyday infant experiences](https://arxiv.org/abs/2510.15060)
*Frangil Ramirez,Elizabeth Clerkin,David J. Crandall,Linda B. Smith*

Main category: cs.CV

TL;DR: Infants' visual experiences have a lumpy similarity structure that helps them learn object categories from limited data, and mimicking this structure improves machine learning generalization.


<details>
  <summary>Details</summary>
Motivation: To understand how infants achieve robust object category learning and generalization from limited visual experiences, despite typically requiring large datasets for similar machine learning tasks.

Method: Analyzed egocentric images from 14 infants (7-11 months) to examine visual input structure, and conducted computational experiments to test if mimicking this structure improves machine learning generalization.

Result: Infant visual input shows lumpy similarity structure with clusters of highly similar images and rarer variable ones across early-learned categories. Mimicking this structure in machines improves generalization from small datasets.

Conclusion: The natural lumpiness of infant visual experience supports early category learning and offers principles for efficient learning across different problems and learners.

Abstract: Young children readily recognize and generalize visual objects labeled by
common nouns, suggesting that these basic level object categories may be given.
Yet if they are, how they arise remains unclear. We propose that the answer
lies in the statistics of infant daily life visual experiences. Whereas large
and diverse datasets typically support robust learning and generalization in
human and machine learning, infants achieve this generalization from limited
experiences. We suggest that the resolution of this apparent contradiction lies
in the visual diversity of daily life, repeated experiences with single object
instances. Analyzing egocentric images from 14 infants (aged 7 to 11 months) we
show that their everyday visual input exhibits a lumpy similarity structure,
with clusters of highly similar images interspersed with rarer, more variable
ones, across eight early-learned categories. Computational experiments show
that mimicking this structure in machines improves generalization from small
datasets in machine learning. The natural lumpiness of infant experience may
thus support early category learning and generalization and, more broadly,
offer principles for efficient learning across a variety of problems and kinds
of learners.

</details>


### [66] [SaLon3R: Structure-aware Long-term Generalizable 3D Reconstruction from Unposed Images](https://arxiv.org/abs/2510.15072)
*Jiaxin Guo,Tongfan Guan,Wenzhen Dong,Wenzhao Zheng,Wenting Wang,Yue Wang,Yeung Yam,Yun-Hui Liu*

Main category: cs.CV

TL;DR: SaLon3R is a novel framework for Structure-aware, Long-term 3D Gaussian Splatting reconstruction that eliminates redundancy through compact anchor primitives and differentiable saliency-aware Gaussian quantization, achieving 50-90% redundancy removal while maintaining high performance.


<details>
  <summary>Details</summary>
Motivation: Existing 3DGS methods predict per-pixel Gaussians and combine all views, leading to substantial redundancies and geometric inconsistencies in long-duration video sequences.

Method: Introduces compact anchor primitives with differentiable saliency-aware Gaussian quantization and a 3D Point Transformer that refines anchor attributes and saliency to resolve cross-frame inconsistencies. Uses a 3D reconstruction backbone to predict dense Gaussians and saliency maps, then compresses redundant Gaussians into compact anchors prioritizing high-complexity regions.

Result: Achieves state-of-the-art performance on novel view synthesis and depth estimation, with 50% to 90% redundancy removal, capable of reconstructing over 50 views in over 10 FPS without known camera parameters or test-time optimization.

Conclusion: The approach effectively resolves artifacts and prunes redundant 3DGS in a single feed-forward pass, demonstrating superior efficiency, robustness, and generalization ability for long-term generalizable 3D reconstruction.

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have enabled generalizable,
on-the-fly reconstruction of sequential input views. However, existing methods
often predict per-pixel Gaussians and combine Gaussians from all views as the
scene representation, leading to substantial redundancies and geometric
inconsistencies in long-duration video sequences. To address this, we propose
SaLon3R, a novel framework for Structure-aware, Long-term 3DGS Reconstruction.
To our best knowledge, SaLon3R is the first online generalizable GS method
capable of reconstructing over 50 views in over 10 FPS, with 50% to 90%
redundancy removal. Our method introduces compact anchor primitives to
eliminate redundancy through differentiable saliency-aware Gaussian
quantization, coupled with a 3D Point Transformer that refines anchor
attributes and saliency to resolve cross-frame geometric and photometric
inconsistencies. Specifically, we first leverage a 3D reconstruction backbone
to predict dense per-pixel Gaussians and a saliency map encoding regional
geometric complexity. Redundant Gaussians are compressed into compact anchors
by prioritizing high-complexity regions. The 3D Point Transformer then learns
spatial structural priors in 3D space from training data to refine anchor
attributes and saliency, enabling regionally adaptive Gaussian decoding for
geometric fidelity. Without known camera parameters or test-time optimization,
our approach effectively resolves artifacts and prunes the redundant 3DGS in a
single feed-forward pass. Experiments on multiple datasets demonstrate our
state-of-the-art performance on both novel view synthesis and depth estimation,
demonstrating superior efficiency, robustness, and generalization ability for
long-term generalizable 3D reconstruction. Project Page:
https://wrld.github.io/SaLon3R/.

</details>


### [67] [TGT: Text-Grounded Trajectories for Locally Controlled Video Generation](https://arxiv.org/abs/2510.15104)
*Guofeng Zhang,Angtian Wang,Jacob Zhiyuan Fang,Liming Jiang,Haotian Yang,Bo Liu,Yiding Yang,Guang Chen,Longyin Wen,Alan Yuille,Chongyang Ma*

Main category: cs.CV

TL;DR: TGT is a text-to-video generation framework that uses trajectories paired with localized text descriptions to precisely control subject composition and motion of multiple objects in generated videos.


<details>
  <summary>Details</summary>
Motivation: Standard text-to-video methods have limited control over subject composition, especially in complex multi-object scenarios where existing localized control methods (bounding boxes, segmentation masks) struggle with precision and object correspondence.

Method: Proposes Text-Grounded Trajectories (TGT) framework with Location-Aware Cross-Attention (LACA) to integrate trajectory-text pairs, dual-CFG scheme for separate local/global text guidance, and a data processing pipeline that produces trajectories with localized descriptions from 2M annotated video clips.

Result: TGT achieves higher visual quality, more accurate text alignment, and improved motion controllability compared to prior approaches, enabling precise control of both appearance and motion for multiple objects using intuitive point trajectories.

Conclusion: The TGT framework successfully addresses limitations in multi-object video generation by using text-grounded trajectories as intuitive motion handles, providing superior composition control and motion accuracy in complex scenarios.

Abstract: Text-to-video generation has advanced rapidly in visual fidelity, whereas
standard methods still have limited ability to control the subject composition
of generated scenes. Prior work shows that adding localized text control
signals, such as bounding boxes or segmentation masks, can help. However, these
methods struggle in complex scenarios and degrade in multi-object settings,
offering limited precision and lacking a clear correspondence between
individual trajectories and visual entities as the number of controllable
objects increases. We introduce Text-Grounded Trajectories (TGT), a framework
that conditions video generation on trajectories paired with localized text
descriptions. We propose Location-Aware Cross-Attention (LACA) to integrate
these signals and adopt a dual-CFG scheme to separately modulate local and
global text guidance. In addition, we develop a data processing pipeline that
produces trajectories with localized descriptions of tracked entities, and we
annotate two million high quality video clips to train TGT. Together, these
components enable TGT to use point trajectories as intuitive motion handles,
pairing each trajectory with text to control both appearance and motion.
Extensive experiments show that TGT achieves higher visual quality, more
accurate text alignment, and improved motion controllability compared with
prior approaches. Website: https://textgroundedtraj.github.io.

</details>


### [68] [Deep generative priors for 3D brain analysis](https://arxiv.org/abs/2510.15119)
*Ana Lawry Aguila,Dina Zemlyanker,You Cheng,Sudeshna Das,Daniel C. Alexander,Oula Puonti,Annabel Sorby-Adams,W. Taylor Kimberly,Juan Eugenio Iglesias*

Main category: cs.CV

TL;DR: This paper presents a framework that combines diffusion models with Bayesian inverse problems to solve medical imaging tasks, using diffusion models as priors for brain MRI analysis.


<details>
  <summary>Details</summary>
Motivation: Diffusion models are powerful generative models but lack domain knowledge integration, while Bayesian inverse problems in neuroimaging use classical priors that fail to capture complex brain anatomy. The goal is to bridge this gap by using diffusion models as flexible priors.

Method: The approach uses score-based diffusion priors trained on diverse brain MRI data, combined with flexible forward models for tasks like super-resolution, bias field correction, and inpainting. It can also refine outputs from existing deep learning methods.

Result: Experiments on clinical and research MRI data show state-of-the-art performance, producing consistent, high-quality solutions without requiring paired training datasets.

Conclusion: Diffusion priors demonstrate strong potential as versatile tools for brain MRI analysis, effectively combining data-driven modeling with domain knowledge.

Abstract: Diffusion models have recently emerged as powerful generative models in
medical imaging. However, it remains a major challenge to combine these
data-driven models with domain knowledge to guide brain imaging problems. In
neuroimaging, Bayesian inverse problems have long provided a successful
framework for inference tasks, where incorporating domain knowledge of the
imaging process enables robust performance without requiring extensive training
data. However, the anatomical modeling component of these approaches typically
relies on classical mathematical priors that often fail to capture the complex
structure of brain anatomy. In this work, we present the first general-purpose
application of diffusion models as priors for solving a wide range of medical
imaging inverse problems. Our approach leverages a score-based diffusion prior
trained extensively on diverse brain MRI data, paired with flexible forward
models that capture common image processing tasks such as super-resolution,
bias field correction, inpainting, and combinations thereof. We further
demonstrate how our framework can refine outputs from existing deep learning
methods to improve anatomical fidelity. Experiments on heterogeneous clinical
and research MRI data show that our method achieves state-of-the-art
performance producing consistent, high-quality solutions without requiring
paired training datasets. These results highlight the potential of diffusion
priors as versatile tools for brain MRI analysis.

</details>


### [69] [Fourier Transform Multiple Instance Learning for Whole Slide Image Classification](https://arxiv.org/abs/2510.15138)
*Anthony Bilic,Guangyu Sun,Ming Li,Md Sanzid Bin Hossain,Yu Tian,Wei Zhang,Laura Brattain,Dexter Hadley,Chen Chen*

Main category: cs.CV

TL;DR: FFT-MIL enhances WSI classification by adding a frequency-domain branch to capture global dependencies, improving performance across multiple MIL methods and datasets.


<details>
  <summary>Details</summary>
Motivation: Existing MIL methods for WSI classification struggle to capture global dependencies due to the immense size of WSIs and local nature of patch embeddings, limiting their ability to model coarse structures essential for robust diagnostic prediction.

Method: Proposes FFT-MIL framework that extracts low-frequency crops from WSIs via Fast Fourier Transform, processes them through a modular FFT-Block with convolutional layers and Min-Max normalization, then fuses global frequency features with spatial patch features using lightweight integration strategies.

Result: Evaluation across six MIL methods on three datasets (BRACS, LUAD, IMP) showed average improvements of 3.51% in macro F1 scores and 1.51% in AUC, with consistent gains across architectures and datasets.

Conclusion: Frequency-domain learning provides an effective and efficient mechanism for capturing global dependencies in WSI classification, complementing spatial features and advancing the scalability and accuracy of MIL-based computational pathology.

Abstract: Whole Slide Image (WSI) classification relies on Multiple Instance Learning
(MIL) with spatial patch features, yet existing methods struggle to capture
global dependencies due to the immense size of WSIs and the local nature of
patch embeddings. This limitation hinders the modeling of coarse structures
essential for robust diagnostic prediction.
  We propose Fourier Transform Multiple Instance Learning (FFT-MIL), a
framework that augments MIL with a frequency-domain branch to provide compact
global context. Low-frequency crops are extracted from WSIs via the Fast
Fourier Transform and processed through a modular FFT-Block composed of
convolutional layers and Min-Max normalization to mitigate the high variance of
frequency data. The learned global frequency feature is fused with spatial
patch features through lightweight integration strategies, enabling
compatibility with diverse MIL architectures.
  FFT-MIL was evaluated across six state-of-the-art MIL methods on three public
datasets (BRACS, LUAD, and IMP). Integration of the FFT-Block improved macro F1
scores by an average of 3.51% and AUC by 1.51%, demonstrating consistent gains
across architectures and datasets. These results establish frequency-domain
learning as an effective and efficient mechanism for capturing global
dependencies in WSI classification, complementing spatial features and
advancing the scalability and accuracy of MIL-based computational pathology.

</details>


### [70] [XModBench: Benchmarking Cross-Modal Capabilities and Consistency in Omni-Language Models](https://arxiv.org/abs/2510.15148)
*Xingrui Wang,Jiang Liu,Chao Huang,Xiaodong Yu,Ze Wang,Ximeng Sun,Jialian Wu,Alan Yuille,Emad Barsoum,Zicheng Liu*

Main category: cs.CV

TL;DR: XModBench is a tri-modal benchmark that evaluates cross-modal consistency in OLLMs, revealing that current models struggle with modality-invariant reasoning and exhibit persistent modality disparities and directional imbalances.


<details>
  <summary>Details</summary>
Motivation: To assess whether omni-modal large language models achieve true modality-invariant reasoning or exhibit modality-specific biases, as existing benchmarks primarily evaluate general cross-modal question-answering rather than consistency.

Method: Developed XModBench with 60,828 multiple-choice questions spanning five task families, systematically covering all six modality compositions in question-answer pairs to enable fine-grained diagnosis of modality-invariant reasoning, modality disparity, and directional imbalance.

Result: Even the strongest model (Gemini 2.5 Pro) struggles with spatial/temporal reasoning (<60% accuracy), shows persistent modality disparities (performance drops with audio vs text), and exhibits directional imbalance (lower consistency when vision serves as context vs text).

Conclusion: Current OLLMs remain far from truly modality-invariant reasoning, and XModBench serves as a fundamental diagnostic tool for evaluating and improving cross-modal competence.

Abstract: Omni-modal large language models (OLLMs) aim to unify audio, vision, and text
understanding within a single framework. While existing benchmarks primarily
evaluate general cross-modal question-answering ability, it remains unclear
whether OLLMs achieve modality-invariant reasoning or exhibit modality-specific
biases. We introduce XModBench, a large-scale tri-modal benchmark explicitly
designed to measure cross-modal consistency. XModBench comprises 60,828
multiple-choice questions spanning five task families and systematically covers
all six modality compositions in question-answer pairs, enabling fine-grained
diagnosis of an OLLM's modality-invariant reasoning, modality disparity, and
directional imbalance. Experiments show that even the strongest model, Gemini
2.5 Pro, (i) struggles with spatial and temporal reasoning, achieving less than
60% accuracy, (ii) reveals persistent modality disparities, with performance
dropping substantially when the same semantic content is conveyed through audio
rather than text, and (iii) shows systematic directional imbalance, exhibiting
lower consistency when vision serves as context compared to text. These
findings indicate that current OLLMs remain far from truly modality-invariant
reasoning and position XModBench as a fundamental diagnostic tool for
evaluating and improving cross-modal competence. All data and evaluation tools
will be available at https://xingruiwang.github.io/projects/XModBench/.

</details>


### [71] [Train a Unified Multimodal Data Quality Classifier with Synthetic Data](https://arxiv.org/abs/2510.15162)
*Weizhi Wang,Rongmei Lin,Shiyang Li,Colin Lockard,Ritesh Sarkhel,Sanket Lokegaonkar,Jingbo Shang,Xifeng Yan,Nasser Zalmout,Xian Li*

Main category: cs.CV

TL;DR: UniFilter is a unified multimodal data quality classifier that filters high-quality image-text caption and interleaved data for MLLM pre-training, using semi-synthetic training data generation and improving downstream performance.


<details>
  <summary>Details</summary>
Motivation: High-quality data filtering for image-text interleaved document data in MLLM pre-training is under-explored, and collecting diverse labeled multimodal data is challenging.

Method: Train an efficient MLLM as UniFilter using semi-synthetic approach that generates text across four quality levels from raw images, creating sample-score pairs for both caption and interleaved document data.

Result: MLLMs pre-trained on UniFilter-curated data show significantly enhanced zero-shot reasoning, in-context learning capabilities, and stronger performance on various benchmarks after visual supervised fine-tuning.

Conclusion: UniFilter effectively improves multimodal pre-training quality, with released synthetic training data, model checkpoints, and curated OBELICS-HQ dataset for community use.

Abstract: The Multimodal Large Language Models (MLLMs) are continually pre-trained on a
mixture of image-text caption data and interleaved document data, while the
high-quality data filtering towards image-text interleaved document data is
under-explored. We propose to train an efficient MLLM as a Unified Mulitmodal
Data Quality Classifier to Filter both high-quality image-text caption and
interleaved data (UniFilter). To address the challenge of collecting diverse
labeled multimodal data, we introduce a semi-synthetic approach that leverages
readily available raw images and generates corresponding text across four
quality levels. This method enables efficient creation of sample-score pairs
for both caption and interleaved document data to train UniFilter. We apply
UniFilter to curate high-quality caption data from DataComp caption dataset and
interleaved data from the OBELICS image-text interleaved dataset. MLLMs
pre-trained on the filtered data demonstrate significantly enhanced
capabilities compared to those trained on baseline-filtered data, achieving
stronger zero-shot reasoning and in-context learning capabilities. After visual
supervised fine-tuning, these UniFilter-induced MLLMs achieve stronger
performance on various benchmarks, highlighting the downstream benefits of
high-quality multimodal pre-training. We release the synthetic training data
used for training UniFilter, the UniFilter model checkpoints, and the
high-quality interleaved document subset OBELICS-HQ, curated by UniFilter, to
the community for reproduction and further development.

</details>


### [72] [Hyperparameter Optimization and Reproducibility in Deep Learning Model Training](https://arxiv.org/abs/2510.15164)
*Usman Afzaal,Ziyu Su,Usama Sajjad,Hao Lu,Mostafa Rezapour,Metin Nafi Gurcan,Muhammad Khalid Khan Niazi*

Main category: cs.CV

TL;DR: This paper investigates reproducibility challenges in histopathology foundation model training, identifying optimal hyperparameter ranges and experimental configurations through systematic evaluation of CLIP models on multiple datasets.


<details>
  <summary>Details</summary>
Motivation: Reproducibility remains a critical challenge in foundation model training for histopathology, hindered by software randomness, hardware non-determinism, and inconsistent hyperparameter reporting.

Method: Trained a CLIP model on QUILT-1M dataset and systematically evaluated the impact of different hyperparameter settings and augmentation strategies across three downstream histopathology datasets (PatchCamelyon, LC25000-Lung, and LC25000-Colon).

Result: Identified clear trends: RandomResizedCrop values of 0.7-0.8 performed best, distributed training without local loss improved stability, learning rates below 5.0e-5 degraded performance, and LC25000 (Colon) dataset provided the most reproducible benchmark.

Conclusion: Reproducibility in computational pathology depends on both transparent documentation and carefully chosen experimental configurations, with practical rules provided to guide future reproducible foundation model development.

Abstract: Reproducibility remains a critical challenge in foundation model training for
histopathology, often hindered by software randomness, hardware
non-determinism, and inconsistent hyperparameter reporting. To investigate
these issues, we trained a CLIP model on the QUILT-1M dataset and
systematically evaluated the impact of different hyperparameter settings and
augmentation strategies across three downstream histopathology datasets
(PatchCamelyon, LC25000-Lung, and LC25000-Colon). Despite variability across
runs, we identified clear trends: RandomResizedCrop values of 0.7-0.8
outperformed more aggressive (0.6) or conservative (0.9) settings, distributed
training without local loss improved stability, and learning rates below 5.0e-5
consistently degraded performance across all datasets. The LC25000 (Colon)
dataset consistently provided the most reproducible benchmark. These findings
highlight that reproducibility in computational pathology depends not only on
transparent documentation but also on carefully chosen experimental
configurations, and we provide practical rules to guide future efforts in
developing reproducible foundation models for digital pathology.

</details>


### [73] [Salient Concept-Aware Generative Data Augmentation](https://arxiv.org/abs/2510.15194)
*Tianchen Zhao,Xuanbai Chen,Zhihua Li,Jun Fang,Dongsheng An,Xiang Xu,Zhuowen Tu,Yifan Xing*

Main category: cs.CV

TL;DR: A personalized image generation framework that uses salient concept-aware image embedding to preserve essential image details while aligning with text prompts, improving data augmentation for downstream model robustness.


<details>
  <summary>Details</summary>
Motivation: Existing generative data augmentation methods struggle to balance fidelity and diversity, as they often entangle essential image details with irrelevant environmental contexts, creating conflicts with text prompts.

Method: Proposed a framework using salient concept-aware image embedding to reduce influence of irrelevant visual details during synthesis, maintaining intuitive alignment between image and text inputs.

Result: Outperformed state-of-the-art augmentation methods across eight fine-grained vision datasets with 0.73% and 6.5% accuracy improvements under conventional and long-tail settings respectively.

Conclusion: The framework effectively enhances training dataset diversity and improves downstream model robustness by preserving class-discriminative features with controlled variations.

Abstract: Recent generative data augmentation methods conditioned on both image and
text prompts struggle to balance between fidelity and diversity, as it is
challenging to preserve essential image details while aligning with varied text
prompts. This challenge arises because representations in the synthesis process
often become entangled with non-essential input image attributes such as
environmental contexts, creating conflicts with text prompts intended to modify
these elements. To address this, we propose a personalized image generation
framework that uses a salient concept-aware image embedding model to reduce the
influence of irrelevant visual details during the synthesis process, thereby
maintaining intuitive alignment between image and text inputs. By generating
images that better preserve class-discriminative features with additional
controlled variations, our framework effectively enhances the diversity of
training datasets and thereby improves the robustness of downstream models. Our
approach demonstrates superior performance across eight fine-grained vision
datasets, outperforming state-of-the-art augmentation methods with averaged
classification accuracy improvements by 0.73% and 6.5% under conventional and
long-tail settings, respectively.

</details>


### [74] [CARDIUM: Congenital Anomaly Recognition with Diagnostic Images and Unified Medical records](https://arxiv.org/abs/2510.15208)
*Daniela Vega,Hannah V. Ceballos,Javier S. Vera,Santiago Rodriguez,Alejandra Perez,Angela Castillo,Maria Escobar,Dario Londoño,Luis A. Sarmiento,Camila I. Castro,Nadiezhda Rodriguez,Juan C. Briceño,Pablo Arbeláez*

Main category: cs.CV

TL;DR: CARDIUM is the first public multimodal dataset for prenatal CHD detection, combining fetal ultrasound/echocardiographic images with maternal clinical records. A cross-attention transformer architecture improves detection by 11-50% over single-modality approaches.


<details>
  <summary>Details</summary>
Motivation: Prenatal CHD diagnosis faces data scarcity and imbalance issues, with no existing public multimodal datasets integrating imaging and clinical data, limiting AI model development for clinical decision support.

Method: Proposed a robust multimodal transformer architecture with cross-attention mechanism to fuse feature representations from image and tabular data, using the CARDIUM dataset containing fetal ultrasound/echocardiographic images and maternal clinical records.

Result: Achieved 11% and 50% improvement over image-only and tabular-only approaches respectively, with F1 score of 79.8 ± 4.8% on the CARDIUM dataset.

Conclusion: The CARDIUM dataset and multimodal transformer architecture significantly advance prenatal CHD detection, with public release of dataset and code to enable further research in this unexplored field.

Abstract: Prenatal diagnosis of Congenital Heart Diseases (CHDs) holds great potential
for Artificial Intelligence (AI)-driven solutions. However, collecting
high-quality diagnostic data remains difficult due to the rarity of these
conditions, resulting in imbalanced and low-quality datasets that hinder model
performance. Moreover, no public efforts have been made to integrate multiple
sources of information, such as imaging and clinical data, further limiting the
ability of AI models to support and enhance clinical decision-making. To
overcome these challenges, we introduce the Congenital Anomaly Recognition with
Diagnostic Images and Unified Medical records (CARDIUM) dataset, the first
publicly available multimodal dataset consolidating fetal ultrasound and
echocardiographic images along with maternal clinical records for prenatal CHD
detection. Furthermore, we propose a robust multimodal transformer architecture
that incorporates a cross-attention mechanism to fuse feature representations
from image and tabular data, improving CHD detection by 11% and 50% over image
and tabular single-modality approaches, respectively, and achieving an F1 score
of 79.8 $\pm$ 4.8% in the CARDIUM dataset. We will publicly release our dataset
and code to encourage further research on this unexplored field. Our dataset
and code are available at https://github.com/BCVUniandes/Cardium, and at the
project website https://bcv-uniandes.github.io/CardiumPage/

</details>


### [75] [The Face of Persuasion: Analyzing Bias and Generating Culture-Aware Ads](https://arxiv.org/abs/2510.15240)
*Aysan Aghazadeh,Adriana Kovashka*

Main category: cs.CV

TL;DR: The paper investigates demographic bias in AI-generated ads and their persuasiveness across different populations, with experiments on targeting ads to specific countries.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of text-to-image models for customizing visual advertisements and targeting specific populations, while examining demographic biases.

Method: Examined demographic bias in ads across different topics, tested persuasiveness of identical ads varying only by gender/race of people portrayed, and experimented with country-specific ad targeting techniques.

Result: Found demographic bias in AI-generated ads and disparate levels of persuasiveness based on gender/race representations, with successful experiments in country-specific targeting.

Conclusion: Text-to-image models show promise for targeted advertising but exhibit demographic biases that need addressing for fair and effective ad customization.

Abstract: Text-to-image models are appealing for customizing visual advertisements and
targeting specific populations. We investigate this potential by examining the
demographic bias within ads for different ad topics, and the disparate level of
persuasiveness (judged by models) of ads that are identical except for
gender/race of the people portrayed. We also experiment with a technique to
target ads for specific countries. The code is available at
https://github.com/aysanaghazadeh/FaceOfPersuasion

</details>


### [76] [DriveGen3D: Boosting Feed-Forward Driving Scene Generation with Efficient Video Diffusion](https://arxiv.org/abs/2510.15264)
*Weijie Wang,Jiagang Zhu,Zeyu Zhang,Xiaofeng Wang,Zheng Zhu,Guosheng Zhao,Chaojun Ni,Haoxiao Wang,Guan Huang,Xinze Chen,Yukun Zhou,Wenkang Qin,Duochao Shi,Haoyun Li,Guanghong Jia,Jiwen Lu*

Main category: cs.CV

TL;DR: DriveGen3D is a framework for generating high-quality, controllable dynamic 3D driving scenes that combines efficient video generation with fast 3D reconstruction.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing methods that are either computationally prohibitive for long-term generation, focus only on video synthesis without 3D representation, or are restricted to static single-scene reconstruction.

Method: Uses a unified pipeline with two components: FastDrive-DiT (efficient video diffusion transformer for high-resolution video synthesis under text and BEV layout guidance) and FastRecon3D (feed-forward reconstruction module for building 3D Gaussian representations across time).

Result: Enables real-time generation of extended driving videos (424×800 at 12 FPS) and corresponding dynamic 3D scenes, achieving SSIM of 0.811 and PSNR of 22.84 on novel view synthesis while maintaining parameter efficiency.

Conclusion: DriveGen3D successfully bridges the methodological gap by integrating accelerated long-term video generation with large-scale dynamic scene reconstruction through multimodal conditional control.

Abstract: We present DriveGen3D, a novel framework for generating high-quality and
highly controllable dynamic 3D driving scenes that addresses critical
limitations in existing methodologies. Current approaches to driving scene
synthesis either suffer from prohibitive computational demands for extended
temporal generation, focus exclusively on prolonged video synthesis without 3D
representation, or restrict themselves to static single-scene reconstruction.
Our work bridges this methodological gap by integrating accelerated long-term
video generation with large-scale dynamic scene reconstruction through
multimodal conditional control. DriveGen3D introduces a unified pipeline
consisting of two specialized components: FastDrive-DiT, an efficient video
diffusion transformer for high-resolution, temporally coherent video synthesis
under text and Bird's-Eye-View (BEV) layout guidance; and FastRecon3D, a
feed-forward reconstruction module that rapidly builds 3D Gaussian
representations across time, ensuring spatial-temporal consistency. Together,
these components enable real-time generation of extended driving videos (up to
$424\times800$ at 12 FPS) and corresponding dynamic 3D scenes, achieving SSIM
of 0.811 and PSNR of 22.84 on novel view synthesis, all while maintaining
parameter efficiency.

</details>


### [77] [CuSfM: CUDA-Accelerated Structure-from-Motion](https://arxiv.org/abs/2510.15271)
*Jingrui Yu,Jun Liu,Kefei Ren,Joydeep Biswas,Rurui Ye,Keqiang Wu,Chirag Majithia,Di Zeng*

Main category: cs.CV

TL;DR: cuSfM is a CUDA-accelerated offline Structure-from-Motion system that uses GPU parallelization to achieve high accuracy and speed in camera pose estimation and mapping, outperforming COLMAP.


<details>
  <summary>Details</summary>
Motivation: To address the need for efficient and accurate camera pose estimation in autonomous navigation, robotic perception, and virtual simulation systems through GPU acceleration.

Method: Uses CUDA-accelerated offline Structure-from-Motion with GPU parallelization to employ computationally intensive feature extractors, generating comprehensive data associations for precise camera pose estimation and globally consistent mapping.

Result: Significantly improved accuracy and processing speed compared to COLMAP across various testing scenarios while maintaining high precision and global consistency.

Conclusion: cuSfM provides an efficient and accurate solution for offline SfM applications, released as open-source PyCuSfM to facilitate computer vision and robotics research.

Abstract: Efficient and accurate camera pose estimation forms the foundational
requirement for dense reconstruction in autonomous navigation, robotic
perception, and virtual simulation systems. This paper addresses the challenge
via cuSfM, a CUDA-accelerated offline Structure-from-Motion system that
leverages GPU parallelization to efficiently employ computationally intensive
yet highly accurate feature extractors, generating comprehensive and
non-redundant data associations for precise camera pose estimation and globally
consistent mapping. The system supports pose optimization, mapping, prior-map
localization, and extrinsic refinement. It is designed for offline processing,
where computational resources can be fully utilized to maximize accuracy.
Experimental results demonstrate that cuSfM achieves significantly improved
accuracy and processing speed compared to the widely used COLMAP method across
various testing scenarios, while maintaining the high precision and global
consistency essential for offline SfM applications. The system is released as
an open-source Python wrapper implementation, PyCuSfM, available at
https://github.com/nvidia-isaac/pyCuSFM, to facilitate research and
applications in computer vision and robotics.

</details>


### [78] [Post-Processing Methods for Improving Accuracy in MRI Inpainting](https://arxiv.org/abs/2510.15282)
*Nishad Kulkarni,Krithika Iyer,Austin Tapp,Abhijeet Parida,Daniel Capellán-Martín,Zhifan Jiang,María J. Ledesma-Carbayo,Syed Muhammad Anwar,Marius George Linguraru*

Main category: cs.CV

TL;DR: This paper introduces an enhanced MRI brain tumor inpainting pipeline that combines model ensembling with post-processing strategies to improve anatomical plausibility and visual fidelity of inpainted regions.


<details>
  <summary>Details</summary>
Motivation: Standard MRI analysis tools fail with large lesions like tumors, so inpainting techniques are needed to synthesize healthy brain tissues in tumor regions to enable reliable application of general-purpose analysis tools.

Method: Combines model ensembling with post-processing strategies (median filtering, histogram matching, pixel averaging) and adds a lightweight U-Net enhancement stage for anatomical refinement.

Result: The proposed pipeline improves anatomical plausibility and visual fidelity of inpainted regions, yielding higher accuracy and more robust outcomes than individual baseline models.

Conclusion: By combining established models with targeted post-processing, the approach achieves improved and more accessible inpainting outcomes, supporting broader clinical deployment and sustainable research.

Abstract: Magnetic Resonance Imaging (MRI) is the primary imaging modality used in the
diagnosis, assessment, and treatment planning for brain pathologies. However,
most automated MRI analysis tools, such as segmentation and registration
pipelines, are optimized for healthy anatomies and often fail when confronted
with large lesions such as tumors. To overcome this, image inpainting
techniques aim to locally synthesize healthy brain tissues in tumor regions,
enabling the reliable application of general-purpose tools. In this work, we
systematically evaluate state-of-the-art inpainting models and observe a
saturation in their standalone performance. In response, we introduce a
methodology combining model ensembling with efficient post-processing
strategies such as median filtering, histogram matching, and pixel averaging.
Further anatomical refinement is achieved via a lightweight U-Net enhancement
stage. Comprehensive evaluation demonstrates that our proposed pipeline
improves the anatomical plausibility and visual fidelity of inpainted regions,
yielding higher accuracy and more robust outcomes than individual baseline
models. By combining established models with targeted post-processing, we
achieve improved and more accessible inpainting outcomes, supporting broader
clinical deployment and sustainable, resource-conscious research. Our 2025
BraTS inpainting docker is available at
https://hub.docker.com/layers/aparida12/brats2025/inpt.

</details>


### [79] [QCFace: Image Quality Control for boosting Face Representation & Recognition](https://arxiv.org/abs/2510.15289)
*Duc-Phuong Doan-Ngo,Thanh-Dang Diep,Thanh Nguyen-Duc,Thanh-Sach LE,Nam Thoai*

Main category: cs.CV

TL;DR: QCFace introduces a hard margin strategy to address mutual overlapping gradients in face recognition, enabling clear decoupling of recognizability from identity representation and achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Current face recognition methods have two main drawbacks: partial capture of recognizability through soft margin constraints, and mutual overlapping gradients that cause instability and entangled representations where recognizability and identity are not cleanly separated.

Method: Proposes Quality Control Face (QCFace) - a hard margin strategy that overcomes mutual overlapping gradient problem and enables clear decoupling of recognizability from identity representation. Uses a novel hard-margin-based loss function with guidance factor for hypersphere planning.

Result: Extensive experiments confirm QCFace provides robust and quantifiable recognizability encoding and achieves state-of-the-art performance in both verification and identification benchmarks compared to existing recognizability-based losses.

Conclusion: QCFace effectively addresses the limitations of current methods by introducing a hard margin strategy that decouples recognizability from identity, leading to improved feature representation and superior performance in face recognition tasks.

Abstract: Recognizability, a key perceptual factor in human face processing, strongly
affects the performance of face recognition (FR) systems in both verification
and identification tasks. Effectively using recognizability to enhance feature
representation remains challenging. In deep FR, the loss function plays a
crucial role in shaping how features are embedded. However, current methods
have two main drawbacks: (i) recognizability is only partially captured through
soft margin constraints, resulting in weaker quality representation and lower
discrimination, especially for low-quality or ambiguous faces; (ii) mutual
overlapping gradients between feature direction and magnitude introduce
undesirable interactions during optimization, causing instability and confusion
in hypersphere planning, which may result in poor generalization, and entangled
representations where recognizability and identity are not cleanly separated.
To address these issues, we introduce a hard margin strategy - Quality Control
Face (QCFace), which overcomes the mutual overlapping gradient problem and
enables the clear decoupling of recognizability from identity representation.
Based on this strategy, a novel hard-margin-based loss function employs a
guidance factor for hypersphere planning, simultaneously optimizing for
recognition ability and explicit recognizability representation. Extensive
experiments confirm that QCFace not only provides robust and quantifiable
recognizability encoding but also achieves state-of-the-art performance in both
verification and identification benchmarks compared to existing
recognizability-based losses.

</details>


### [80] [Hyperbolic Structured Classification for Robust Single Positive Multi-label Learning](https://arxiv.org/abs/2510.15296)
*Yiming Lin,Shang Wang,Junkai Zhou,Qiufeng Wang,Xiao-Bo Jin,Kaizhu Huang*

Main category: cs.CV

TL;DR: Proposes a hyperbolic classification framework for Single Positive Multi-Label Learning (SPMLL) using hyperbolic balls to represent labels, enabling explicit modeling of hierarchical, co-occurrence, and independence relationships through geometric interactions.


<details>
  <summary>Details</summary>
Motivation: Address limitations of existing SPMLL methods that implicitly model label relationships through distance-based similarity without explicit geometric definitions for different relationship types, particularly in capturing complex label relationships and hierarchical structures.

Method: Represents each label as a hyperbolic ball rather than point/vector, uses ball interactions to model inclusion (hierarchical), overlap (co-occurrence), and separation (independence) relationships. Introduces temperature-adaptive hyperbolic ball classifier and physics-inspired double-well regularization.

Result: Extensive experiments on MS-COCO, PASCAL VOC, NUS-WIDE, CUB-200-2011 show competitive performance with superior interpretability. Statistical analysis reveals strong correlation between learned embeddings and real-world co-occurrence patterns.

Conclusion: Establishes hyperbolic geometry as a more robust paradigm for structured classification under incomplete supervision, providing explicit geometric modeling of multiple label relationship types.

Abstract: Single Positive Multi-Label Learning (SPMLL) addresses the challenging
scenario where each training sample is annotated with only one positive label
despite potentially belonging to multiple categories, making it difficult to
capture complex label relationships and hierarchical structures. While existing
methods implicitly model label relationships through distance-based similarity,
lacking explicit geometric definitions for different relationship types. To
address these limitations, we propose the first hyperbolic classification
framework for SPMLL that represents each label as a hyperbolic ball rather than
a point or vector, enabling rich inter-label relationship modeling through
geometric ball interactions. Our ball-based approach naturally captures
multiple relationship types simultaneously: inclusion for hierarchical
structures, overlap for co-occurrence patterns, and separation for semantic
independence. Further, we introduce two key component innovations: a
temperature-adaptive hyperbolic ball classifier and a physics-inspired
double-well regularization that guides balls toward meaningful configurations.
To validate our approach, extensive experiments on four benchmark datasets
(MS-COCO, PASCAL VOC, NUS-WIDE, CUB-200-2011) demonstrate competitive
performance with superior interpretability compared to existing methods.
Furthermore, statistical analysis reveals strong correlation between learned
embeddings and real-world co-occurrence patterns, establishing hyperbolic
geometry as a more robust paradigm for structured classification under
incomplete supervision.

</details>


### [81] [Latent Diffusion Model without Variational Autoencoder](https://arxiv.org/abs/2510.15301)
*Minglei Shi,Haolin Wang,Wenzhao Zheng,Ziyang Yuan,Xiaoshi Wu,Xintao Wang,Pengfei Wan,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: SVG introduces a novel latent diffusion model that replaces VAEs with self-supervised DINO features, creating a semantically structured latent space for more efficient training and better generative quality.


<details>
  <summary>Details</summary>
Motivation: VAE+diffusion models suffer from limited training efficiency, slow inference, and poor transferability due to VAE latent spaces lacking clear semantic separation and discriminative structure.

Method: SVG constructs a feature space using frozen DINO features for semantic discriminability, with a lightweight residual branch capturing fine details. Diffusion models are trained directly on this semantically structured latent space.

Result: SVG enables accelerated diffusion training, supports few-step sampling, improves generative quality, and preserves semantic and discriminative capabilities of self-supervised representations.

Conclusion: SVG provides a principled pathway toward task-general, high-quality visual representations by leveraging semantically structured latent spaces without VAEs.

Abstract: Recent progress in diffusion-based visual generation has largely relied on
latent diffusion models with variational autoencoders (VAEs). While effective
for high-fidelity synthesis, this VAE+diffusion paradigm suffers from limited
training efficiency, slow inference, and poor transferability to broader vision
tasks. These issues stem from a key limitation of VAE latent spaces: the lack
of clear semantic separation and strong discriminative structure. Our analysis
confirms that these properties are crucial not only for perception and
understanding tasks, but also for the stable and efficient training of latent
diffusion models. Motivated by this insight, we introduce SVG, a novel latent
diffusion model without variational autoencoders, which leverages
self-supervised representations for visual generation. SVG constructs a feature
space with clear semantic discriminability by leveraging frozen DINO features,
while a lightweight residual branch captures fine-grained details for
high-fidelity reconstruction. Diffusion models are trained directly on this
semantically structured latent space to facilitate more efficient learning. As
a result, SVG enables accelerated diffusion training, supports few-step
sampling, and improves generative quality. Experimental results further show
that SVG preserves the semantic and discriminative capabilities of the
underlying self-supervised representations, providing a principled pathway
toward task-general, high-quality visual representations.

</details>


### [82] [Layer as Puzzle Pieces: Compressing Large Language Models through Layer Concatenation](https://arxiv.org/abs/2510.15304)
*Fei Wang,Li Shen,Liang Ding,Chao Xue,Ye Liu,Changxing Ding*

Main category: cs.CV

TL;DR: CoMe is a progressive layer pruning framework that uses concatenation-based merging and hierarchical distillation to reduce LLM size while maintaining performance, achieving 83% original accuracy when pruning 30% of LLaMA-2-7b parameters.


<details>
  <summary>Details</summary>
Motivation: Current structured pruning methods for LLMs cause performance degradation, have incompetent layer aggregation, and lack effective recovery mechanisms. The goal is to reduce model size while retaining capabilities in the pruned parts.

Method: Progressive layer pruning with channel sensitivity metric (using activation intensity and weight norms), concatenation-based layer merging to fuse critical channels across adjacent layers, and hierarchical distillation protocol for knowledge transfer.

Result: State-of-the-art performance on seven benchmarks; when pruning 30% of LLaMA-2-7b parameters, the pruned model retains 83% of original average accuracy.

Conclusion: CoMe effectively addresses limitations of existing pruning methods and enables significant model size reduction with minimal performance loss through its progressive pruning and hierarchical distillation approach.

Abstract: Large Language Models excel at natural language processing tasks, but their
massive size leads to high computational and storage demands. Recent works have
sought to reduce their model size through layer-wise structured pruning.
However, they tend to ignore retaining the capabilities in the pruned part. In
this work, we re-examine structured pruning paradigms and uncover several key
limitations: 1) notable performance degradation due to direct layer removal, 2)
incompetent linear weight layer aggregation, and 3) the lack of effective
post-training recovery mechanisms. To address these limitations, we propose
CoMe, including a progressive layer pruning framework with a
Concatenation-based Merging technology and a hierarchical distillation
post-training process. Specifically, we introduce a channel sensitivity metric
that utilizes activation intensity and weight norms for fine-grained channel
selection. Subsequently, we employ a concatenation-based layer merging method
to fuse the most critical channels across adjacent layers, enabling progressive
model size reduction. Finally, we propose a hierarchical distillation protocol
that leverages the correspondences between the original and pruned model layers
established during pruning, thereby enabling efficient knowledge transfer.
Experiments on seven benchmarks show that CoMe achieves state-of-the-art
performance; when pruning 30% of LLaMA-2-7b's parameters, the pruned model
retains 83% of its original average accuracy. Our code is available at
https://github.com/MPI-Lab/CoMe.

</details>


### [83] [Proto-Former: Unified Facial Landmark Detection by Prototype Transformer](https://arxiv.org/abs/2510.15338)
*Shengkai Hu,Haozhe Qi,Jun Wan,Jiaxing Huang,Lefei Zhang,Hang Sun,Dacheng Tao*

Main category: cs.CV

TL;DR: Proto-Former is a unified facial landmark detection framework that enables joint training across multiple datasets with different landmark definitions through prototype learning and adaptive architecture.


<details>
  <summary>Details</summary>
Motivation: Existing facial landmark detection methods are limited to single-dataset training due to varying landmark definitions across datasets, which hinders model generalization and prevents unified model development.

Method: Proto-Former consists of an Adaptive Prototype-Aware Encoder (APAE) for feature extraction and prototype learning, a Progressive Prototype-Aware Decoder (PPAD) for prototype refinement and attention guidance, and a Prototype-Aware (PA) loss to stabilize multi-dataset training and resolve gradient conflicts.

Result: Extensive experiments on benchmark datasets show that Proto-Former achieves superior performance compared to state-of-the-art methods, demonstrating effective multi-dataset training capability.

Conclusion: Proto-Former successfully addresses the limitations of single-dataset training in facial landmark detection by enabling unified multi-dataset training through prototype learning, achieving better generalization and performance.

Abstract: Recent advances in deep learning have significantly improved facial landmark
detection. However, existing facial landmark detection datasets often define
different numbers of landmarks, and most mainstream methods can only be trained
on a single dataset. This limits the model generalization to different datasets
and hinders the development of a unified model. To address this issue, we
propose Proto-Former, a unified, adaptive, end-to-end facial landmark detection
framework that explicitly enhances dataset-specific facial structural
representations (i.e., prototype). Proto-Former overcomes the limitations of
single-dataset training by enabling joint training across multiple datasets
within a unified architecture. Specifically, Proto-Former comprises two key
components: an Adaptive Prototype-Aware Encoder (APAE) that performs adaptive
feature extraction and learns prototype representations, and a Progressive
Prototype-Aware Decoder (PPAD) that refines these prototypes to generate
prompts that guide the model's attention to key facial regions. Furthermore, we
introduce a novel Prototype-Aware (PA) loss, which achieves optimal path
finding by constraining the selection weights of prototype experts. This loss
function effectively resolves the problem of prototype expert addressing
instability during multi-dataset training, alleviates gradient conflicts, and
enables the extraction of more accurate facial structure features. Extensive
experiments on widely used benchmark datasets demonstrate that our Proto-Former
achieves superior performance compared to existing state-of-the-art methods.
The code is publicly available at: https://github.com/Husk021118/Proto-Former.

</details>


### [84] [SHARE: Scene-Human Aligned Reconstruction](https://arxiv.org/abs/2510.15342)
*Joshua Li,Brendan Chharawala,Chang Shu,Xue Bin Peng,Pengcheng Xi*

Main category: cs.CV

TL;DR: SHARE is a method that uses scene geometry to accurately ground 3D human motion reconstruction from monocular RGB videos, improving human placement in 3D space.


<details>
  <summary>Details</summary>
Motivation: Current human motion reconstruction methods struggle with accurate 3D human placement in scenes, which is crucial for realistic character interactions in gaming, AR/VR, and robotics applications.

Method: SHARE estimates human meshes and segmentation masks for each frame, creates scene point maps at keyframes, then iteratively refines human positions by comparing human meshes against scene-extracted human point maps while preserving relative root joint positions between keyframes and non-keyframes.

Result: SHARE enables more accurate 3D human placement while reconstructing surrounding scenes, working effectively on both curated datasets and in-the-wild web videos, outperforming existing methods in extensive experiments.

Conclusion: The proposed SHARE method successfully leverages scene geometry cues to achieve more accurate human motion reconstruction and 3D placement from monocular videos.

Abstract: Animating realistic character interactions with the surrounding environment
is important for autonomous agents in gaming, AR/VR, and robotics. However,
current methods for human motion reconstruction struggle with accurately
placing humans in 3D space. We introduce Scene-Human Aligned REconstruction
(SHARE), a technique that leverages the scene geometry's inherent spatial cues
to accurately ground human motion reconstruction. Each reconstruction relies
solely on a monocular RGB video from a stationary camera. SHARE first estimates
a human mesh and segmentation mask for every frame, alongside a scene point map
at keyframes. It iteratively refines the human's positions at these keyframes
by comparing the human mesh against the human point map extracted from the
scene using the mask. Crucially, we also ensure that non-keyframe human meshes
remain consistent by preserving their relative root joint positions to keyframe
root joints during optimization. Our approach enables more accurate 3D human
placement while reconstructing the surrounding scene, facilitating use cases on
both curated datasets and in-the-wild web videos. Extensive experiments
demonstrate that SHARE outperforms existing methods.

</details>


### [85] [Cortical-SSM: A Deep State Space Model for EEG and ECoG Motor Imagery Decoding](https://arxiv.org/abs/2510.15371)
*Shuntaro Suzuki,Shunya Nagashima,Masayuki Hirata,Komei Sugiura*

Main category: cs.CV

TL;DR: Cortical-SSM is a novel deep state space model that captures temporal, spatial, and frequency dependencies in EEG/ECoG signals for motor imagery classification, outperforming baselines on multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: EEG and ECoG signals for motor imagery classification have applications in communication assistance and rehabilitation, but are susceptible to physiological artifacts. Existing Transformer-based approaches struggle to capture fine-grained dependencies in these signals.

Method: Proposed Cortical-SSM architecture that extends deep state space models to capture integrated dependencies across temporal, spatial, and frequency domains of EEG and ECoG signals.

Result: Outperformed baseline methods on three benchmarks: two large-scale public MI EEG datasets (50+ subjects) and a clinical MI ECoG dataset from an ALS patient. Visual explanations show the model captures neurophysiologically relevant regions.

Conclusion: Cortical-SSM effectively captures multi-domain dependencies in EEG/ECoG signals and provides neurophysiologically meaningful interpretations, demonstrating superior performance for motor imagery classification tasks.

Abstract: Classification of electroencephalogram (EEG) and electrocorticogram (ECoG)
signals obtained during motor imagery (MI) has substantial application
potential, including for communication assistance and rehabilitation support
for patients with motor impairments. These signals remain inherently
susceptible to physiological artifacts (e.g., eye blinking, swallowing), which
pose persistent challenges. Although Transformer-based approaches for
classifying EEG and ECoG signals have been widely adopted, they often struggle
to capture fine-grained dependencies within them. To overcome these
limitations, we propose Cortical-SSM, a novel architecture that extends deep
state space models to capture integrated dependencies of EEG and ECoG signals
across temporal, spatial, and frequency domains. We validated our method across
three benchmarks: 1) two large-scale public MI EEG datasets containing more
than 50 subjects, and 2) a clinical MI ECoG dataset recorded from a patient
with amyotrophic lateral sclerosis. Our method outperformed baseline methods on
the three benchmarks. Furthermore, visual explanations derived from our model
indicate that it effectively captures neurophysiologically relevant regions of
both EEG and ECoG signals.

</details>


### [86] [Adaptive transfer learning for surgical tool presence detection in laparoscopic videos through gradual freezing fine-tuning](https://arxiv.org/abs/2510.15372)
*Ana Davila,Jacinto Colan,Yasuhisa Hasegawa*

Main category: cs.CV

TL;DR: A novel staged adaptive fine-tuning approach for surgical tool detection that uses linear probing and gradual freezing to improve efficiency and performance with limited annotated data.


<details>
  <summary>Details</summary>
Motivation: Limited availability of annotated surgical data makes it challenging to train robust deep learning models for automated surgical tool detection in minimally invasive surgery.

Method: Two-step approach: 1) linear probing to condition classification layers on pre-trained CNN architectures, 2) gradual freezing to dynamically reduce fine-tunable layers and regulate domain adaptation. Uses ResNet-50 and DenseNet-121 pre-trained on ImageNet.

Result: Achieved 96.4% mAP on Cholec80 dataset, outperforming existing approaches. Method also demonstrated generalizability on CATARACTS dataset for ophthalmic surgery.

Conclusion: Gradual freezing fine-tuning is a promising technique for improving tool detection in diverse surgical procedures and has broader applications in general image classification tasks.

Abstract: Minimally invasive surgery can benefit significantly from automated surgical
tool detection, enabling advanced analysis and assistance. However, the limited
availability of annotated data in surgical settings poses a challenge for
training robust deep learning models. This paper introduces a novel staged
adaptive fine-tuning approach consisting of two steps: a linear probing stage
to condition additional classification layers on a pre-trained CNN-based
architecture and a gradual freezing stage to dynamically reduce the
fine-tunable layers, aiming to regulate adaptation to the surgical domain. This
strategy reduces network complexity and improves efficiency, requiring only a
single training loop and eliminating the need for multiple iterations. We
validated our method on the Cholec80 dataset, employing CNN architectures
(ResNet-50 and DenseNet-121) pre-trained on ImageNet for detecting surgical
tools in cholecystectomy endoscopic videos. Our results demonstrate that our
method improves detection performance compared to existing approaches and
established fine-tuning techniques, achieving a mean average precision (mAP) of
96.4%. To assess its broader applicability, the generalizability of the
fine-tuning strategy was further confirmed on the CATARACTS dataset, a distinct
domain of minimally invasive ophthalmic surgery. These findings suggest that
gradual freezing fine-tuning is a promising technique for improving tool
presence detection in diverse surgical procedures and may have broader
applications in general image classification tasks.

</details>


### [87] [FreqPDE: Rethinking Positional Depth Embedding for Multi-View 3D Object Detection Transformers](https://arxiv.org/abs/2510.15385)
*Haisheng Su,Junjie Zhang,Feixiang Song,Sanping Zhou,Wei Wu,Nanning Zheng,Junchi Yan*

Main category: cs.CV

TL;DR: FreqPDE introduces frequency-aware depth embedding to improve 3D object detection from multi-view 2D images without requiring LiDAR supervision, addressing depth discontinuity and cross-view consistency issues.


<details>
  <summary>Details</summary>
Motivation: Current methods rely on depth prediction with LiDAR supervision, suffering from depth discontinuity at object boundaries and poor detection of small objects due to sparse supervision and use of high-level features.

Method: Three main modules: Frequency-aware Spatial Pyramid Encoder (FSPE) combines high-frequency edges and low-frequency semantics; Cross-view Scale-invariant Depth Predictor (CSDP) with attention mechanisms; Positional Depth Encoder (PDE) generates 3D depth-aware features. Uses hybrid depth supervision.

Result: Extensive experiments on nuScenes dataset demonstrate effectiveness and superiority of the proposed method.

Conclusion: FreqPDE provides an effective approach for 3D object detection from 2D images by incorporating frequency-aware depth embedding and addressing cross-view consistency issues.

Abstract: Detecting 3D objects accurately from multi-view 2D images is a challenging
yet essential task in the field of autonomous driving. Current methods resort
to integrating depth prediction to recover the spatial information for object
query decoding, which necessitates explicit supervision from LiDAR points
during the training phase. However, the predicted depth quality is still
unsatisfactory such as depth discontinuity of object boundaries and
indistinction of small objects, which are mainly caused by the sparse
supervision of projected points and the use of high-level image features for
depth prediction. Besides, cross-view consistency and scale invariance are also
overlooked in previous methods. In this paper, we introduce Frequency-aware
Positional Depth Embedding (FreqPDE) to equip 2D image features with spatial
information for 3D detection transformer decoder, which can be obtained through
three main modules. Specifically, the Frequency-aware Spatial Pyramid Encoder
(FSPE) constructs a feature pyramid by combining high-frequency edge clues and
low-frequency semantics from different levels respectively. Then the Cross-view
Scale-invariant Depth Predictor (CSDP) estimates the pixel-level depth
distribution with cross-view and efficient channel attention mechanism.
Finally, the Positional Depth Encoder (PDE) combines the 2D image features and
3D position embeddings to generate the 3D depth-aware features for query
decoding. Additionally, hybrid depth supervision is adopted for complementary
depth learning from both metric and distribution aspects. Extensive experiments
conducted on the nuScenes dataset demonstrate the effectiveness and superiority
of our proposed method.

</details>


### [88] [PFGS: Pose-Fused 3D Gaussian Splatting for Complete Multi-Pose Object Reconstruction](https://arxiv.org/abs/2510.15386)
*Ting-Yu Yen,Yu-Sheng Chiu,Shih-Hsuan Hung,Peter Wonka,Hung-Kuo Chu*

Main category: cs.CV

TL;DR: PFGS is a pose-aware 3D Gaussian Splatting framework that reconstructs complete 3D objects from multi-pose image captures by iteratively fusing auxiliary pose images into a unified 3DGS representation of the main pose.


<details>
  <summary>Details</summary>
Motivation: Existing 3DGS methods assume static single-pose captures, leading to incomplete reconstructions that miss occluded regions. There's a need to reconstruct complete objects from multi-pose captures.

Method: Uses pose-aware fusion strategy combining global and local registration to merge views. Leverages background features for per-pose camera pose estimation and foundation models for cross-pose registration, addressing memory and accuracy limitations.

Result: PFGS consistently outperforms strong baselines in qualitative and quantitative evaluations, producing more complete reconstructions and higher-fidelity 3DGS models.

Conclusion: PFGS effectively addresses the challenge of reconstructing complete objects from multi-pose captures by intelligently incorporating foundation models into the registration process while resolving background inconsistency issues.

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have enabled high-quality,
real-time novel-view synthesis from multi-view images. However, most existing
methods assume the object is captured in a single, static pose, resulting in
incomplete reconstructions that miss occluded or self-occluded regions. We
introduce PFGS, a pose-aware 3DGS framework that addresses the practical
challenge of reconstructing complete objects from multi-pose image captures.
Given images of an object in one main pose and several auxiliary poses, PFGS
iteratively fuses each auxiliary set into a unified 3DGS representation of the
main pose. Our pose-aware fusion strategy combines global and local
registration to merge views effectively and refine the 3DGS model. While recent
advances in 3D foundation models have improved registration robustness and
efficiency, they remain limited by high memory demands and suboptimal accuracy.
PFGS overcomes these challenges by incorporating them more intelligently into
the registration process: it leverages background features for per-pose camera
pose estimation and employs foundation models for cross-pose registration. This
design captures the best of both approaches while resolving background
inconsistency issues. Experimental results demonstrate that PFGS consistently
outperforms strong baselines in both qualitative and quantitative evaluations,
producing more complete reconstructions and higher-fidelity 3DGS models.

</details>


### [89] [LILAC: Long-sequence Incremental Low-latency Arbitrary Motion Stylization via Streaming VAE-Diffusion with Causal Decoding](https://arxiv.org/abs/2510.15392)
*Peng Ren,Hai Yang*

Main category: cs.CV

TL;DR: LILAC enables real-time generation of long, stylized human motions using a streaming VAE-Diffusion framework with causal decoding, achieving smooth transitions without future frame dependency.


<details>
  <summary>Details</summary>
Motivation: Existing streaming motion generation methods have computational overhead and temporal stability issues, while high-quality VAE-Diffusion approaches are limited to offline processing. LILAC bridges this gap for real-time applications.

Method: Extends offline VAE-Diffusion framework to online setting using latent-space streaming architecture with sliding-window causal design and decoded motion feature injection for smooth transitions.

Result: Achieves long-sequence real-time arbitrary motion stylization without future frame dependency or diffusion model modifications, balancing quality and responsiveness on benchmark datasets.

Conclusion: LILAC successfully enables real-time, high-quality motion stylization through streaming architecture, addressing limitations of both traditional streaming methods and offline VAE-Diffusion approaches.

Abstract: Generating long and stylized human motions in real time is critical for
applications that demand continuous and responsive character control. Despite
its importance, existing streaming approaches often operate directly in the raw
motion space, leading to substantial computational overhead and making it
difficult to maintain temporal stability. In contrast, latent-space
VAE-Diffusion-based frameworks alleviate these issues and achieve high-quality
stylization, but they are generally confined to offline processing. To bridge
this gap, LILAC (Long-sequence Incremental Low-latency Arbitrary Motion
Stylization via Streaming VAE-Diffusion with Causal Decoding) builds upon a
recent high-performing offline framework for arbitrary motion stylization and
extends it to an online setting through a latent-space streaming architecture
with a sliding-window causal design and the injection of decoded motion
features to ensure smooth motion transitions. This architecture enables
long-sequence real-time arbitrary stylization without relying on future frames
or modifying the diffusion model architecture, achieving a favorable balance
between stylization quality and responsiveness as demonstrated by experiments
on benchmark datasets. Supplementary video and examples are available at the
project page: https://pren1.github.io/lilac/

</details>


### [90] [MARIS: Marine Open-Vocabulary Instance Segmentation with Geometric Enhancement and Semantic Alignment](https://arxiv.org/abs/2510.15398)
*Bingyu Li,Feiyu Wang,Da Zhang,Zhiyuan Zhao,Junyu Gao,Xuelong Li*

Main category: cs.CV

TL;DR: The paper introduces MARIS, the first large-scale benchmark for underwater open-vocabulary instance segmentation, and proposes a unified framework with geometric prior enhancement and semantic alignment mechanisms to address visual degradation and semantic misalignment issues in underwater scenes.


<details>
  <summary>Details</summary>
Motivation: Existing underwater instance segmentation approaches are limited to close-vocabulary prediction and cannot recognize novel marine categories. Transfer learning from natural images to underwater scenes suffers from severe visual degradation (color attenuation) and semantic misalignment due to lack of underwater class definitions.

Method: Proposes a unified framework with two components: Geometric Prior Enhancement Module (GPEM) that leverages part-level and structural cues to maintain object consistency under degraded visual conditions, and Semantic Alignment Injection Mechanism (SAIM) that enriches language embeddings with domain-specific priors to mitigate semantic ambiguity.

Result: The framework consistently outperforms existing open-vocabulary baselines in both In-Domain and Cross-Domain settings on the MARIS benchmark, establishing a strong foundation for future underwater perception research.

Conclusion: The proposed approach effectively addresses the challenges of underwater open-vocabulary instance segmentation by combining geometric prior enhancement and semantic alignment mechanisms, demonstrating superior performance over existing methods.

Abstract: Most existing underwater instance segmentation approaches are constrained by
close-vocabulary prediction, limiting their ability to recognize novel marine
categories. To support evaluation, we introduce \textbf{MARIS}
(\underline{Mar}ine Open-Vocabulary \underline{I}nstance
\underline{S}egmentation), the first large-scale fine-grained benchmark for
underwater Open-Vocabulary (OV) segmentation, featuring a limited set of seen
categories and diverse unseen categories. Although OV segmentation has shown
promise on natural images, our analysis reveals that transfer to underwater
scenes suffers from severe visual degradation (e.g., color attenuation) and
semantic misalignment caused by lack underwater class definitions. To address
these issues, we propose a unified framework with two complementary components.
The Geometric Prior Enhancement Module (\textbf{GPEM}) leverages stable
part-level and structural cues to maintain object consistency under degraded
visual conditions. The Semantic Alignment Injection Mechanism (\textbf{SAIM})
enriches language embeddings with domain-specific priors, mitigating semantic
ambiguity and improving recognition of unseen categories. Experiments show that
our framework consistently outperforms existing OV baselines both In-Domain and
Cross-Domain setting on MARIS, establishing a strong foundation for future
underwater perception research.

</details>


### [91] [Robust High-Resolution Multi-Organ Diffusion MRI Using Synthetic-Data-Tuned Prompt Learning](https://arxiv.org/abs/2510.15400)
*Chen Qian,Haoyu Zhang,Junnan Ma,Liuhong Zhu,Qingrui Cai,Yu Wang,Ruibo Song,Lv Li,Lin Mei,Xianwang Jiang,Qin Xu,Boyu Jiang,Ran Tao,Chunmiao Chen,Shufang Chen,Dongyun Liang,Qiu Guo,Jianzhong Lin,Taishan Kang,Mengtian Lu,Liyuan Fu,Ruibin Huang,Huijuan Wan,Xu Huang,Jianhua Wang,Di Guo,Hai Zhong,Jianjun Zhou,Xiaobo Qu*

Main category: cs.CV

TL;DR: LoSP-Prompt is a reconstruction framework that enables high-resolution multi-shot DWI for body-wide tumor diagnostics by addressing motion artifacts through physics-informed modeling and synthetic-data-driven prompt learning, achieving superior image quality across multiple anatomical regions without requiring navigator signals or real data supervision.


<details>
  <summary>Details</summary>
Motivation: Clinical adoption of multi-shot DWI is limited by severe motion-induced phase artifacts from respiration and peristalsis, compounded by multi-organ, multi-slice, multi-direction and multi-b-value complexities in body-wide tumor diagnostics.

Method: Models inter-shot phase variations as high-order Locally Smooth Phase (LoSP) integrated into low-rank Hankel matrix reconstruction, with automatic rank parameter setting via prompt learning trained exclusively on synthetic abdominal DWI data emulating physiological motion.

Result: Validated on 10,000+ clinical images: achieved twice the spatial resolution of clinical single-shot DWI; generalized to seven anatomical regions with single model; outperformed state-of-the-art methods in image quality, artifact suppression, and noise reduction (4-5 points on 5-point scale across different anatomical regions).

Conclusion: Provides interpretable, robust solution for high-resolution multi-organ multi-shot DWI, eliminating navigator signals and realistic data supervision, with scanner-agnostic performance signifying transformative potential for precision oncology.

Abstract: Clinical adoption of multi-shot diffusion-weighted magnetic resonance imaging
(multi-shot DWI) for body-wide tumor diagnostics is limited by severe
motion-induced phase artifacts from respiration, peristalsis, and so on,
compounded by multi-organ, multi-slice, multi-direction and multi-b-value
complexities. Here, we introduce a reconstruction framework, LoSP-Prompt, that
overcomes these challenges through physics-informed modeling and
synthetic-data-driven prompt learning. We model inter-shot phase variations as
a high-order Locally Smooth Phase (LoSP), integrated into a low-rank Hankel
matrix reconstruction. Crucially, the algorithm's rank parameter is
automatically set via prompt learning trained exclusively on synthetic
abdominal DWI data emulating physiological motion. Validated across 10,000+
clinical images (43 subjects, 4 scanner models, 5 centers), LoSP-Prompt: (1)
Achieved twice the spatial resolution of clinical single-shot DWI, enhancing
liver lesion conspicuity; (2) Generalized to seven diverse anatomical regions
(liver, kidney, sacroiliac, pelvis, knee, spinal cord, brain) with a single
model; (3) Outperformed state-of-the-art methods in image quality, artifact
suppression, and noise reduction (11 radiologists' evaluations on a 5-point
scale, $p<0.05$), achieving 4-5 points (excellent) on kidney DWI, 4 points
(good to excellent) on liver, sacroiliac and spinal cord DWI, and 3-4 points
(good) on knee and tumor brain. The approach eliminates navigator signals and
realistic data supervision, providing an interpretable, robust solution for
high-resolution multi-organ multi-shot DWI. Its scanner-agnostic performance
signifies transformative potential for precision oncology.

</details>


### [92] [Learning to Detect Unknown Jailbreak Attacks in Large Vision-Language Models](https://arxiv.org/abs/2510.15430)
*Shuang Liang,Zhihao Xu,Jialing Tao,Hui Xue,Xiting Wang*

Main category: cs.CV

TL;DR: LoD is a framework that detects unknown jailbreak attacks in LVLMs by focusing on task-specific learning rather than attack-specific learning, using safety-oriented representation learning and unsupervised attack classification.


<details>
  <summary>Details</summary>
Motivation: Current jailbreak detection methods either lack generalization to unseen attacks due to attack-specific parameter learning, or suffer from limited accuracy and efficiency due to heuristic approaches.

Method: Proposes Learning to Detect (LoD) framework with Multi-modal Safety Concept Activation Vector for safety-oriented representation learning and Safety Pattern Auto-Encoder for unsupervised attack classification.

Result: Achieves consistently higher detection AUROC on diverse unknown attacks while improving efficiency compared to existing methods.

Conclusion: LoD provides an effective solution for detecting unknown jailbreak attacks in LVLMs by shifting focus to task-specific learning rather than attack-specific learning.

Abstract: Despite extensive alignment efforts, Large Vision-Language Models (LVLMs)
remain vulnerable to jailbreak attacks, posing serious safety risks. To address
this, existing detection methods either learn attack-specific parameters, which
hinders generalization to unseen attacks, or rely on heuristically sound
principles, which limit accuracy and efficiency. To overcome these limitations,
we propose Learning to Detect (LoD), a general framework that accurately
detects unknown jailbreak attacks by shifting the focus from attack-specific
learning to task-specific learning. This framework includes a Multi-modal
Safety Concept Activation Vector module for safety-oriented representation
learning and a Safety Pattern Auto-Encoder module for unsupervised attack
classification. Extensive experiments show that our method achieves
consistently higher detection AUROC on diverse unknown attacks while improving
efficiency. The code is available at
https://anonymous.4open.science/r/Learning-to-Detect-51CB.

</details>


### [93] [Semantic4Safety: Causal Insights from Zero-shot Street View Imagery Segmentation for Urban Road Safety](https://arxiv.org/abs/2510.15434)
*Huan Chen,Ting Han,Siyu Chen,Zhihao Guo,Yiping Chen,Meiliu Wu*

Main category: cs.CV

TL;DR: Semantic4Safety is a framework that uses zero-shot semantic segmentation on street-view imagery to create 11 streetscape indicators and analyzes 30,000 accident records in Austin using XGBoost, SHAP, and causal inference methods to quantify causal effects of street features on different accident types.


<details>
  <summary>Details</summary>
Motivation: To address challenges in constructing street-level indicators that capture accident-related features and quantifying their causal impacts across different accident types using street-view imagery.

Method: Apply zero-shot semantic segmentation to SVIs to derive 11 interpretable streetscape indicators, integrate road type as contextual information, train XGBoost multi-class classifier, use SHAP for feature interpretation, and apply GPS weighting and ATE estimation for causal inference.

Result: Uncovered heterogeneous, accident-type-specific causal patterns: scene complexity, exposure, and roadway geometry features dominate predictive power; larger drivable area and emergency space reduce risk, while excessive visual openness increases risk.

Conclusion: Semantic4Safety bridges predictive modeling with causal inference, supporting targeted interventions and high-risk corridor diagnosis, offering a scalable, data-informed tool for urban road safety planning.

Abstract: Street-view imagery (SVI) offers a fine-grained lens on traffic risk, yet two
fundamental challenges persist: (1) how to construct street-level indicators
that capture accident-related features, and (2) how to quantify their causal
impacts across different accident types. To address these challenges, we
propose Semantic4Safety, a framework that applies zero-shot semantic
segmentation to SVIs to derive 11 interpretable streetscape indicators, and
integrates road type as contextual information to analyze approximately 30,000
accident records in Austin. Specifically, we train an eXtreme Gradient Boosting
(XGBoost) multi-class classifier and use Shapley Additive Explanations (SHAP)
to interpret both global and local feature contributions, and then apply
Generalized Propensity Score (GPS) weighting and Average Treatment Effect (ATE)
estimation to control confounding and quantify causal effects. Results uncover
heterogeneous, accident-type-specific causal patterns: features capturing scene
complexity, exposure, and roadway geometry dominate predictive power; larger
drivable area and emergency space reduce risk, whereas excessive visual
openness can increase it. By bridging predictive modeling with causal
inference, Semantic4Safety supports targeted interventions and high-risk
corridor diagnosis, offering a scalable, data-informed tool for urban road
safety planning.

</details>


### [94] [Rethinking Convergence in Deep Learning: The Predictive-Corrective Paradigm for Anatomy-Informed Brain MRI Segmentation](https://arxiv.org/abs/2510.15439)
*Feifei Zhang,Zhenhong Jia,Sensen Song,Fei Shi,Dayong Ren*

Main category: cs.CV

TL;DR: PCMambaNet introduces a Predictive-Corrective paradigm that decouples modeling tasks to accelerate learning, achieving state-of-the-art brain MRI segmentation accuracy in only 1-5 epochs.


<details>
  <summary>Details</summary>
Motivation: End-to-end deep learning suffers from slow convergence and heavy reliance on large datasets, limiting efficiency in data-scarce domains like medical imaging.

Method: Two synergistic modules: Predictive Prior Module (PPM) generates coarse approximations using anatomical knowledge (bilateral symmetry) to identify asymmetric regions, and Corrective Residual Network (CRN) learns residual errors to refine challenging regions and pathological boundaries.

Result: Achieves state-of-the-art accuracy on high-resolution brain MRI segmentation while converging within only 1-5 epochs, dramatically outperforming conventional end-to-end models.

Conclusion: Explicitly incorporating domain knowledge to simplify learning objectives effectively mitigates data inefficiency and overfitting, enabling rapid convergence with high accuracy.

Abstract: Despite the remarkable success of the end-to-end paradigm in deep learning,
it often suffers from slow convergence and heavy reliance on large-scale
datasets, which fundamentally limits its efficiency and applicability in
data-scarce domains such as medical imaging. In this work, we introduce the
Predictive-Corrective (PC) paradigm, a framework that decouples the modeling
task to fundamentally accelerate learning. Building upon this paradigm, we
propose a novel network, termed PCMambaNet. PCMambaNet is composed of two
synergistic modules. First, the Predictive Prior Module (PPM) generates a
coarse approximation at low computational cost, thereby anchoring the search
space. Specifically, the PPM leverages anatomical knowledge-bilateral
symmetry-to predict a 'focus map' of diagnostically relevant asymmetric
regions. Next, the Corrective Residual Network (CRN) learns to model the
residual error, focusing the network's full capacity on refining these
challenging regions and delineating precise pathological boundaries. Extensive
experiments on high-resolution brain MRI segmentation demonstrate that
PCMambaNet achieves state-of-the-art accuracy while converging within only 1-5
epochs-a performance unattainable by conventional end-to-end models. This
dramatic acceleration highlights that by explicitly incorporating domain
knowledge to simplify the learning objective, PCMambaNet effectively mitigates
data inefficiency and overfitting.

</details>


### [95] [Select Less, Reason More: Prioritizing Evidence Purity for Video Reasoning](https://arxiv.org/abs/2510.15440)
*Xuchen Li,Xuzhao Li,Shiyu Hu,Kaiqi Huang*

Main category: cs.CV

TL;DR: Proposes EARL framework for video reasoning that dynamically selects relevant frames and performs localized re-sampling to improve evidence purity and temporal detail access.


<details>
  <summary>Details</summary>
Motivation: Address limitations of existing Video LLMs where static uniform frame sampling causes information dilution and obscures critical evidence, and pixel-space reasoning agents lack rigorous reward mechanisms for evidence purity.

Method: Evidence-aware reinforcement learning (EARL) framework that transforms models into active interrogators of evidence, dynamically selecting most relevant frames and performing localized re-sampling around key frames for fine-grained temporal detail.

Result: Achieves state-of-the-art performance on five video reasoning benchmarks: 59.8% on LongVideoBench, 69.0% on MVBench, and 64.9% on VideoMME with 7B model.

Conclusion: Demonstrates importance of prioritizing evidence purity and effectiveness of the EARL framework for long-form video reasoning through selective frame sampling and localized re-sampling.

Abstract: Long-form video reasoning remains a major challenge for Video Large Language
Models (Video LLMs), as static uniform frame sampling leads to information
dilution and obscures critical evidence. Furthermore, existing pixel-space
video reasoning agents, which are designed to actively interact with the video
to acquire new visual information, remain suboptimal due to their lack of
rigorous reward mechanisms to enforce evidence purity and their inability to
perform temporal information supplementation beyond pre-sampled frames. To
address this critical gap, we propose a novel evidence-prioritized adaptive
framework built upon our core philosophy: "Select Less, Reason More." Our core
contribution is the evidence-aware reinforcement learning (EARL) framework,
which transforms the model into an active interrogator of evidence. EARL is
precisely engineered to dynamically select the most relevant frames and,
crucially, to perform localized re-sampling around the selected key frames to
access fine-grained temporal detail. Extensive experiments on five demanding
video reasoning benchmarks demonstrate that our EARL-trained model achieves new
state-of-the-art among open-source Video LLMs, simultaneously learning an
effective and high-purity visual evidence selection policy. Impressively, our
7B model achieves 59.8% on LongVideoBench, 69.0% on MVBench and 64.9% on
VideoMME. These results highlight the importance of prioritizing evidence
purity and the effectiveness of our framework.

</details>


### [96] [MAVR-Net: Robust Multi-View Learning for MAV Action Recognition with Cross-View Attention](https://arxiv.org/abs/2510.15448)
*Nengbo Zhang,Hann Woei Ho*

Main category: cs.CV

TL;DR: MAVR-Net is a multi-view learning framework that combines RGB frames, optical flow, and segmentation masks to improve Micro Aerial Vehicle (MAV) action recognition, achieving state-of-the-art performance on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Vision-based MAV motion recognition using only RGB data often fails to capture complex spatial-temporal characteristics, limiting the ability to distinguish different actions in autonomous aerial swarms.

Method: Uses ResNet-based encoders to extract features from RGB, optical flow, and segmentation masks; employs multi-scale feature pyramid for spatiotemporal details; introduces cross-view attention module for modality dependencies; and designs multi-view alignment loss for semantic consistency.

Result: Achieves 97.8%, 96.5%, and 92.8% accuracy on Short MAV, Medium MAV, and Long MAV datasets respectively, clearly outperforming existing approaches.

Conclusion: The multi-view approach combining complementary data types significantly improves MAV action recognition robustness and accuracy, enabling better cooperative perception in autonomous aerial swarms.

Abstract: Recognizing the motion of Micro Aerial Vehicles (MAVs) is crucial for
enabling cooperative perception and control in autonomous aerial swarms. Yet,
vision-based recognition models relying only on RGB data often fail to capture
the complex spatial temporal characteristics of MAV motion, which limits their
ability to distinguish different actions. To overcome this problem, this paper
presents MAVR-Net, a multi-view learning-based MAV action recognition
framework. Unlike traditional single-view methods, the proposed approach
combines three complementary types of data, including raw RGB frames, optical
flow, and segmentation masks, to improve the robustness and accuracy of MAV
motion recognition. Specifically, ResNet-based encoders are used to extract
discriminative features from each view, and a multi-scale feature pyramid is
adopted to preserve the spatiotemporal details of MAV motion patterns. To
enhance the interaction between different views, a cross-view attention module
is introduced to model the dependencies among various modalities and feature
scales. In addition, a multi-view alignment loss is designed to ensure semantic
consistency and strengthen cross-view feature representations. Experimental
results on benchmark MAV action datasets show that our method clearly
outperforms existing approaches, achieving 97.8\%, 96.5\%, and 92.8\% accuracy
on the Short MAV, Medium MAV, and Long MAV datasets, respectively.

</details>


### [97] [DPTrack:Directional Kernel-Guided Prompt Learning for Robust Nighttime Aerial Tracking](https://arxiv.org/abs/2510.15449)
*Zhiqiang Zhu,Xinbo Gao,Wen Lu,Jie Li,Zhaoyang Wang,Mingqian Ge*

Main category: cs.CV

TL;DR: DPTrack is a prompt-based aerial tracker for nighttime scenarios that encodes object attribute features into directional kernels with fine-grained cues to generate precise prompts, overcoming limitations of existing trackers that rely solely on spatial localization.


<details>
  <summary>Details</summary>
Motivation: Existing nighttime aerial trackers based on prompt learning rely only on spatial localization supervision, which fails to provide fine-grained cues for target features and produces vague prompts, impairing tracking accuracy in nighttime scenarios.

Method: DPTrack hierarchically captures object's topological structure using visual bionics, encodes topology-aware features into directional kernels, and uses a kernel-guided prompt module with channel-category correspondence to propagate kernels across search regions and generate precise prompts with spatial gating.

Result: Extensive evaluations on established benchmarks demonstrate DPTrack's superior performance compared to existing nighttime aerial trackers.

Conclusion: DPTrack effectively addresses the limitations of vague prompts in nighttime aerial tracking by encoding fine-grained attribute cues into directional kernels, achieving robust and accurate tracking performance.

Abstract: Existing nighttime aerial trackers based on prompt learning rely solely on
spatial localization supervision, which fails to provide fine-grained cues that
point to target features and inevitably produces vague prompts. This limitation
impairs the tracker's ability to accurately focus on the object features and
results in trackers still performing poorly. To address this issue, we propose
DPTrack, a prompt-based aerial tracker designed for nighttime scenarios by
encoding the given object's attribute features into the directional kernel
enriched with fine-grained cues to generate precise prompts. Specifically,
drawing inspiration from visual bionics, DPTrack first hierarchically captures
the object's topological structure, leveraging topological attributes to enrich
the feature representation. Subsequently, an encoder condenses these
topology-aware features into the directional kernel, which serves as the core
guidance signal that explicitly encapsulates the object's fine-grained
attribute cues. Finally, a kernel-guided prompt module built on
channel-category correspondence attributes propagates the kernel across the
features of the search region to pinpoint the positions of target features and
convert them into precise prompts, integrating spatial gating for robust
nighttime tracking. Extensive evaluations on established benchmarks demonstrate
DPTrack's superior performance. Our code will be available at
https://github.com/zzq-vipsl/DPTrack.

</details>


### [98] [Improving Micro-Expression Recognition with Phase-Aware Temporal Augmentation](https://arxiv.org/abs/2510.15466)
*Vu Tram Anh Khuong,Luu Tu Nguyen,Thanh Ha Le,Thi Duyen Ngo*

Main category: cs.CV

TL;DR: A phase-aware temporal augmentation method for micro-expression recognition that decomposes expressions into onset-to-apex and apex-to-offset phases, generating separate dynamic images for each phase to improve motion diversity and recognition accuracy.


<details>
  <summary>Details</summary>
Motivation: Micro-expression recognition is limited by scarce annotated datasets and existing methods rely mainly on spatial augmentations, overlooking temporal strategies that better exploit motion characteristics.

Method: Proposes dual-phase dynamic image augmentation that decomposes each expression sequence into onset-to-apex and apex-to-offset phases, generating separate dynamic images for each phase to capture complementary temporal cues.

Result: Experiments on CASME-II and SAMM datasets with six deep architectures show consistent improvements in accuracy, F1-score, and recall. Combined with spatial augmentations, achieves up to 10% relative improvement.

Conclusion: The proposed phase-aware temporal augmentation is simple, model-agnostic, effective in low-resource settings, and offers a promising direction for robust and generalizable micro-expression recognition.

Abstract: Micro-expressions (MEs) are brief, involuntary facial movements that reveal
genuine emotions, typically lasting less than half a second. Recognizing these
subtle expressions is critical for applications in psychology, security, and
behavioral analysis. Although deep learning has enabled significant advances in
micro-expression recognition (MER), its effectiveness is limited by the
scarcity of annotated ME datasets. This data limitation not only hinders
generalization but also restricts the diversity of motion patterns captured
during training. Existing MER studies predominantly rely on simple spatial
augmentations (e.g., flipping, rotation) and overlook temporal augmentation
strategies that can better exploit motion characteristics. To address this gap,
this paper proposes a phase-aware temporal augmentation method based on dynamic
image. Rather than encoding the entire expression as a single onset-to-offset
dynamic image (DI), our approach decomposes each expression sequence into two
motion phases: onset-to-apex and apex-to-offset. A separate DI is generated for
each phase, forming a Dual-phase DI augmentation strategy. These phase-specific
representations enrich motion diversity and introduce complementary temporal
cues that are crucial for recognizing subtle facial transitions. Extensive
experiments on CASME-II and SAMM datasets using six deep architectures,
including CNNs, Vision Transformer, and the lightweight LEARNet, demonstrate
consistent performance improvements in recognition accuracy, unweighted
F1-score, and unweighted average recall, which are crucial for addressing class
imbalance in MER. When combined with spatial augmentations, our method achieves
up to a 10\% relative improvement. The proposed augmentation is simple,
model-agnostic, and effective in low-resource settings, offering a promising
direction for robust and generalizable MER.

</details>


### [99] [MRASfM: Multi-Camera Reconstruction and Aggregation through Structure-from-Motion in Driving Scenes](https://arxiv.org/abs/2510.15467)
*Lingfeng Xuan,Chang Nie,Yiqing Xu,Zhe Liu,Yanzi Miao,Hesheng Wang*

Main category: cs.CV

TL;DR: MRASfM is a specialized Structure from Motion framework for multi-camera driving scenes that improves pose estimation reliability, removes road surface outliers using plane modeling, and boosts efficiency through unified bundle adjustment.


<details>
  <summary>Details</summary>
Motivation: Standard SfM struggles with driving scenes from multi-camera systems due to unreliable pose estimation, excessive road surface outliers, and low reconstruction efficiency.

Method: Leverages fixed multi-camera spatial relationships for reliable pose estimation, uses plane model to remove erroneous road surface points, treats multi-camera set as single unit in Bundle Adjustment, and employs coarse-to-fine scene association and assembly modules.

Result: Achieves 0.124 absolute pose error on nuScenes dataset, demonstrates robustness in challenging conditions, and shows state-of-the-art performance on large-scale public datasets.

Conclusion: MRASfM effectively addresses the specific challenges of multi-camera driving scene reconstruction and achieves superior performance compared to existing methods.

Abstract: Structure from Motion (SfM) estimates camera poses and reconstructs point
clouds, forming a foundation for various tasks. However, applying SfM to
driving scenes captured by multi-camera systems presents significant
difficulties, including unreliable pose estimation, excessive outliers in road
surface reconstruction, and low reconstruction efficiency. To address these
limitations, we propose a Multi-camera Reconstruction and Aggregation
Structure-from-Motion (MRASfM) framework specifically designed for driving
scenes. MRASfM enhances the reliability of camera pose estimation by leveraging
the fixed spatial relationships within the multi-camera system during the
registration process. To improve the quality of road surface reconstruction,
our framework employs a plane model to effectively remove erroneous points from
the triangulated road surface. Moreover, treating the multi-camera set as a
single unit in Bundle Adjustment (BA) helps reduce optimization variables to
boost efficiency. In addition, MRASfM achieves multi-scene aggregation through
scene association and assembly modules in a coarse-to-fine fashion. We deployed
multi-camera systems on actual vehicles to validate the generalizability of
MRASfM across various scenes and its robustness in challenging conditions
through real-world applications. Furthermore, large-scale validation results on
public datasets show the state-of-the-art performance of MRASfM, achieving
0.124 absolute pose error on the nuScenes dataset.

</details>


### [100] [MSAM: Multi-Semantic Adaptive Mining for Cross-Modal Drone Video-Text Retrieval](https://arxiv.org/abs/2510.15470)
*Jinghao Huang,Yaxiong Chen,Ganchao Liu*

Main category: cs.CV

TL;DR: First systematic study of drone video-text retrieval (DVTR) task, proposing Multi-Semantic Adaptive Mining (MSAM) to address unique challenges of drone videos like overhead perspectives and structural homogeneity.


<details>
  <summary>Details</summary>
Motivation: Drone videos have unique characteristics (overhead perspectives, structural homogeneity, diverse semantic combinations) that challenge existing cross-modal methods designed for ground-level views, requiring dedicated retrieval mechanisms.

Method: MSAM with multi-semantic adaptive learning mechanism, including adaptive semantic construction module, distribution-driven semantic learning term, diversity semantic term, and cross-modal interactive feature fusion pooling to reduce background interference.

Result: Extensive experiments on two self-constructed drone video-text datasets show MSAM outperforms existing methods in drone video-text retrieval task.

Conclusion: MSAM effectively addresses drone video-text retrieval challenges through fine-grained modality interactions and adaptive semantic mining, with code and datasets to be publicly released.

Abstract: With the advancement of drone technology, the volume of video data increases
rapidly, creating an urgent need for efficient semantic retrieval. We are the
first to systematically propose and study the drone video-text retrieval (DVTR)
task. Drone videos feature overhead perspectives, strong structural
homogeneity, and diverse semantic expressions of target combinations, which
challenge existing cross-modal methods designed for ground-level views in
effectively modeling their characteristics. Therefore, dedicated retrieval
mechanisms tailored for drone scenarios are necessary. To address this issue,
we propose a novel approach called Multi-Semantic Adaptive Mining (MSAM). MSAM
introduces a multi-semantic adaptive learning mechanism, which incorporates
dynamic changes between frames and extracts rich semantic information from
specific scene regions, thereby enhancing the deep understanding and reasoning
of drone video content. This method relies on fine-grained interactions between
words and drone video frames, integrating an adaptive semantic construction
module, a distribution-driven semantic learning term and a diversity semantic
term to deepen the interaction between text and drone video modalities and
improve the robustness of feature representation. To reduce the interference of
complex backgrounds in drone videos, we introduce a cross-modal interactive
feature fusion pooling mechanism that focuses on feature extraction and
matching in target regions, minimizing noise effects. Extensive experiments on
two self-constructed drone video-text datasets show that MSAM outperforms other
existing methods in the drone video-text retrieval task. The source code and
dataset will be made publicly available.

</details>


### [101] [A Novel Combined Optical Flow Approach for Comprehensive Micro-Expression Recognition](https://arxiv.org/abs/2510.15471)
*Vu Tram Anh Khuong,Thi Bich Phuong Man,Luu Tu Nguyen,Thanh Ha Le,Thi Duyen Ngo*

Main category: cs.CV

TL;DR: This paper introduces Combined Optical Flow (COF) that integrates both onset-to-apex and apex-to-offset phases for better micro-expression recognition, outperforming single optical flow methods.


<details>
  <summary>Details</summary>
Motivation: Most Micro-Expression Recognition methods focus only on the onset-to-apex phase, neglecting the important temporal dynamics in the apex-to-offset phase.

Method: Proposed Combined Optical Flow (COF) that integrates both onset-to-apex and apex-to-offset phases to provide comprehensive motion analysis.

Result: Experimental results on CASMEII and SAMM datasets show COF outperforms single optical flow-based methods.

Conclusion: COF effectively captures micro-expression dynamics by considering both temporal phases, demonstrating improved performance in micro-expression recognition.

Abstract: Facial micro-expressions are brief, involuntary facial movements that reveal
hidden emotions. Most Micro-Expression Recognition (MER) methods that rely on
optical flow typically focus on the onset-to-apex phase, neglecting the
apex-to-offset phase, which holds key temporal dynamics. This study introduces
a Combined Optical Flow (COF), integrating both phases to enhance feature
representation. COF provides a more comprehensive motion analysis, improving
MER performance. Experimental results on CASMEII and SAMM datasets show that
COF outperforms single optical flow-based methods, demonstrating its
effectiveness in capturing micro-expression dynamics.

</details>


### [102] [Iterative Motion Compensation for Canonical 3D Reconstruction from UAV Plant Images Captured in Windy Conditions](https://arxiv.org/abs/2510.15491)
*Andre Rochow,Jonas Marcic,Svetlana Seliunina,Sven Behnke*

Main category: cs.CV

TL;DR: A pipeline for high-quality 3D reconstruction of agricultural plants using autonomous UAV image capture and iterative motion compensation to handle wind and downwash effects.


<details>
  <summary>Details</summary>
Motivation: 3D phenotyping is crucial for understanding plant growth, yield prediction, and disease control, but challenging due to environmental wind and UAV downwash.

Method: Autonomous UAV image capture with ArUco markers, integration of state-of-the-art 3D reconstruction methods, and iterative motion compensation using optical flow between input images and intermediate 3D reconstructions.

Result: Pipeline improves reconstruction quality of state-of-the-art methods, enables extraction of high-resolution 3D meshes, and provides a public dataset of multiple plants across different time points.

Conclusion: The proposed pipeline effectively handles motion artifacts in plant 3D reconstruction and will be publicly released along with a comprehensive plant dataset.

Abstract: 3D phenotyping of plants plays a crucial role for understanding plant growth,
yield prediction, and disease control. We present a pipeline capable of
generating high-quality 3D reconstructions of individual agricultural plants.
To acquire data, a small commercially available UAV captures images of a
selected plant. Apart from placing ArUco markers, the entire image acquisition
process is fully autonomous, controlled by a self-developed Android application
running on the drone's controller. The reconstruction task is particularly
challenging due to environmental wind and downwash of the UAV. Our proposed
pipeline supports the integration of arbitrary state-of-the-art 3D
reconstruction methods. To mitigate errors caused by leaf motion during image
capture, we use an iterative method that gradually adjusts the input images
through deformation. Motion is estimated using optical flow between the
original input images and intermediate 3D reconstructions rendered from the
corresponding viewpoints. This alignment gradually reduces scene motion,
resulting in a canonical representation. After a few iterations, our pipeline
improves the reconstruction of state-of-the-art methods and enables the
extraction of high-resolution 3D meshes. We will publicly release the source
code of our reconstruction pipeline. Additionally, we provide a dataset
consisting of multiple plants from various crops, captured across different
points in time.

</details>


### [103] [Rethinking Efficient Hierarchical Mixing Architecture for Low-light RAW Image Enhancement](https://arxiv.org/abs/2510.15497)
*Xianmin Chen,Peiliang Huang,Longfei Han,Dingwen Zhang,Junwei Han*

Main category: cs.CV

TL;DR: HiMA introduces a hierarchical architecture combining Transformer and Mamba modules for efficient low-light RAW image enhancement, with Local Distribution Adjustment and Multi-prior Fusion modules to handle uneven illumination and enhance details.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning approaches for low-light RAW image enhancement struggle with balancing enhancement quality and efficiency, and face challenges with uneven illumination and local variations.

Method: Proposes HiMA with hierarchical mixing of Transformer (large scales) and Mamba (small scales), Local Distribution Adjustment for uneven illumination, and Multi-prior Fusion integrating spatial and frequency-domain priors.

Result: Outperforms state-of-the-art methods on multiple public datasets with superior performance and fewer parameters.

Conclusion: HiMA provides an efficient and effective solution for low-light RAW image enhancement through complementary architecture design and specialized modules.

Abstract: Low-light RAW image enhancement remains a challenging task. Although numerous
deep learning based approaches have been proposed, they still suffer from
inherent limitations. A key challenge is how to simultaneously achieve strong
enhancement quality and high efficiency. In this paper, we rethink the
architecture for efficient low-light image signal processing (ISP) and
introduce a Hierarchical Mixing Architecture (HiMA). HiMA leverages the
complementary strengths of Transformer and Mamba modules to handle features at
large and small scales, respectively, thereby improving efficiency while
avoiding the ambiguities observed in prior two-stage frameworks. To further
address uneven illumination with strong local variations, we propose Local
Distribution Adjustment (LoDA), which adaptively aligns feature distributions
across different local regions. In addition, to fully exploit the denoised
outputs from the first stage, we design a Multi-prior Fusion (MPF) module that
integrates spatial and frequency-domain priors for detail enhancement.
Extensive experiments on multiple public datasets demonstrate that our method
outperforms state-of-the-art approaches, achieving superior performance with
fewer parameters. Code will be released at https://github.com/Cynicarlos/HiMA.

</details>


### [104] [Exploring Conditions for Diffusion models in Robotic Control](https://arxiv.org/abs/2510.15510)
*Heeseong Shin,Byeongho Heo,Dongyoon Han,Seungryong Kim,Taekyung Kim*

Main category: cs.CV

TL;DR: ORCA introduces learnable task and visual prompts to adapt pre-trained text-to-image diffusion models for robotic control, achieving state-of-the-art performance without fine-tuning the base model.


<details>
  <summary>Details</summary>
Motivation: Pre-trained visual representations are often task-agnostic and frozen during policy learning, limiting their effectiveness for robotic control tasks where dynamic visual information is crucial.

Method: Proposes ORCA with learnable task prompts that adapt to the control environment and visual prompts that capture frame-specific details, enabling task-adaptive representations from pre-trained diffusion models.

Result: Achieves state-of-the-art performance on various robotic control benchmarks, significantly surpassing prior methods.

Conclusion: Task-adaptive visual representations through specialized conditions (task and visual prompts) are essential for bridging the domain gap between pre-trained diffusion models and robotic control environments.

Abstract: While pre-trained visual representations have significantly advanced
imitation learning, they are often task-agnostic as they remain frozen during
policy learning. In this work, we explore leveraging pre-trained text-to-image
diffusion models to obtain task-adaptive visual representations for robotic
control, without fine-tuning the model itself. However, we find that naively
applying textual conditions - a successful strategy in other vision domains -
yields minimal or even negative gains in control tasks. We attribute this to
the domain gap between the diffusion model's training data and robotic control
environments, leading us to argue for conditions that consider the specific,
dynamic visual information required for control. To this end, we propose ORCA,
which introduces learnable task prompts that adapt to the control environment
and visual prompts that capture fine-grained, frame-specific details. Through
facilitating task-adaptive representations with our newly devised conditions,
our approach achieves state-of-the-art performance on various robotic control
benchmarks, significantly surpassing prior methods.

</details>


### [105] [Latent Feature Alignment: Discovering Biased and Interpretable Subpopulations in Face Recognition Models](https://arxiv.org/abs/2510.15520)
*Ignacio Serna*

Main category: cs.CV

TL;DR: LFA is an attribute-label-free algorithm that uses latent directions to identify biased subpopulations in face recognition models, outperforming conventional clustering methods in semantic coherence and interpretability.


<details>
  <summary>Details</summary>
Motivation: Modern face recognition models have systematic biases affecting certain subpopulations, but conventional bias evaluation requires expensive labeled attributes and is limited to predefined categories.

Method: Latent Feature Alignment (LFA) uses latent directions to identify subpopulations, providing semantically coherent grouping and discovery of interpretable directions corresponding to attributes like age, ethnicity, or attire.

Result: Across four state-of-the-art recognition models and two benchmarks, LFA consistently outperforms k-means and nearest-neighbor search in intra-group semantic coherence while uncovering interpretable latent directions aligned with demographic and contextual attributes.

Conclusion: LFA serves as a practical method for representation auditing of face recognition models, enabling identification and interpretation of biased subpopulations without predefined attribute annotations.

Abstract: Modern face recognition models achieve high overall accuracy but continue to
exhibit systematic biases that disproportionately affect certain
subpopulations. Conventional bias evaluation frameworks rely on labeled
attributes to form subpopulations, which are expensive to obtain and limited to
predefined categories. We introduce Latent Feature Alignment (LFA), an
attribute-label-free algorithm that uses latent directions to identify
subpopulations. This yields two main benefits over standard clustering: (i)
semantically coherent grouping, where faces sharing common attributes are
grouped together more reliably than by proximity-based methods, and (ii)
discovery of interpretable directions, which correspond to semantic attributes
such as age, ethnicity, or attire. Across four state-of-the-art recognition
models (ArcFace, CosFace, ElasticFace, PartialFC) and two benchmarks (RFW,
CelebA), LFA consistently outperforms k-means and nearest-neighbor search in
intra-group semantic coherence, while uncovering interpretable latent
directions aligned with demographic and contextual attributes. These results
position LFA as a practical method for representation auditing of face
recognition models, enabling practitioners to identify and interpret biased
subpopulations without predefined attribute annotations.

</details>


### [106] [Balanced Multi-Task Attention for Satellite Image Classification: A Systematic Approach to Achieving 97.23% Accuracy on EuroSAT Without Pre-Training](https://arxiv.org/abs/2510.15527)
*Aditya Vir*

Main category: cs.CV

TL;DR: Custom CNN architectures for satellite land use classification achieve 97.23% accuracy on EuroSAT without pre-trained models, using a novel balanced multi-task attention mechanism that combines spatial and spectral feature extraction.


<details>
  <summary>Details</summary>
Motivation: To develop specialized convolutional neural network architectures for satellite imagery classification that don't rely on pre-trained models, addressing specific failure modes in satellite land use classification.

Method: Three progressive architectural iterations: baseline CNN (94.30%), CBAM-enhanced (95.98%), and balanced multi-task attention (97.23%) using Coordinate Attention for spatial features and Squeeze-Excitation blocks for spectral features with learnable fusion parameter. Also employed progressive DropBlock regularization and class-balanced loss weighting.

Result: Achieved 97.23% test accuracy on EuroSAT dataset, Cohen's Kappa of 0.9692, with all classes exceeding 94.46% accuracy. The learnable fusion parameter converged to alpha ≈ 0.57, showing near-equal importance of spatial and spectral modalities. Performance within 1.34% of fine-tuned ResNet-50.

Conclusion: Systematic architectural design can achieve competitive performance for domain-specific applications without external data or pre-trained models, validating the efficacy of custom CNN architectures for satellite imagery classification.

Abstract: This work presents a systematic investigation of custom convolutional neural
network architectures for satellite land use classification, achieving 97.23%
test accuracy on the EuroSAT dataset without reliance on pre-trained models.
Through three progressive architectural iterations (baseline: 94.30%,
CBAM-enhanced: 95.98%, and balanced multi-task attention: 97.23%) we identify
and address specific failure modes in satellite imagery classification. Our
principal contribution is a novel balanced multi-task attention mechanism that
combines Coordinate Attention for spatial feature extraction with
Squeeze-Excitation blocks for spectral feature extraction, unified through a
learnable fusion parameter. Experimental results demonstrate that this
learnable parameter autonomously converges to alpha approximately 0.57,
indicating near-equal importance of spatial and spectral modalities for
satellite imagery. We employ progressive DropBlock regularization (5-20% by
network depth) and class-balanced loss weighting to address overfitting and
confusion pattern imbalance. The final 12-layer architecture achieves Cohen's
Kappa of 0.9692 with all classes exceeding 94.46% accuracy, demonstrating
confidence calibration with a 24.25% gap between correct and incorrect
predictions. Our approach achieves performance within 1.34% of fine-tuned
ResNet-50 (98.57%) while requiring no external data, validating the efficacy of
systematic architectural design for domain-specific applications. Complete
code, trained models, and evaluation scripts are publicly available.

</details>


### [107] [Diffusion Bridge Networks Simulate Clinical-grade PET from MRI for Dementia Diagnostics](https://arxiv.org/abs/2510.15556)
*Yitong Li,Ralph Buchert,Benita Schmitz-Koep,Timo Grimmer,Björn Ommer,Dennis M. Hedderich,Igor Yakushev,Christian Wachinger*

Main category: cs.CV

TL;DR: SiM2P is a 3D diffusion-based framework that generates synthetic FDG-PET images from MRI and patient data, improving dementia diagnosis accuracy from 75.0% to 84.7% while requiring minimal site-specific data for deployment.


<details>
  <summary>Details</summary>
Motivation: FDG-PET is valuable for dementia diagnosis but is less accessible and more expensive than MRI. The goal is to make PET's diagnostic benefits more widely available by simulating PET images from routine MRI scans.

Method: A 3D diffusion bridge-based framework that learns probabilistic mapping from MRI and auxiliary patient information to generate synthetic FDG-PET images. Uses only 20 site-specific cases and basic demographic data for local deployment.

Result: In a blinded clinical study, SiM2P significantly improved diagnostic accuracy for differentiating Alzheimer's disease, frontotemporal dementia, and healthy controls from 75.0% to 84.7%. Simulated PET images received higher diagnostic certainty and better interrater agreement than MRI alone.

Conclusion: SiM2P makes FDG-PET's diagnostic benefits more accessible in resource-limited settings, potentially improving early detection and differential diagnosis of dementing disorders using routine MRI scans.

Abstract: Positron emission tomography (PET) with 18F-Fluorodeoxyglucose (FDG) is an
established tool in the diagnostic workup of patients with suspected dementing
disorders. However, compared to the routinely available magnetic resonance
imaging (MRI), FDG-PET remains significantly less accessible and substantially
more expensive. Here, we present SiM2P, a 3D diffusion bridge-based framework
that learns a probabilistic mapping from MRI and auxiliary patient information
to simulate FDG-PET images of diagnostic quality. In a blinded clinical reader
study, two neuroradiologists and two nuclear medicine physicians rated the
original MRI and SiM2P-simulated PET images of patients with Alzheimer's
disease, behavioral-variant frontotemporal dementia, and cognitively healthy
controls. SiM2P significantly improved the overall diagnostic accuracy of
differentiating between three groups from 75.0% to 84.7% (p<0.05). Notably, the
simulated PET images received higher diagnostic certainty ratings and achieved
superior interrater agreement compared to the MRI images. Finally, we developed
a practical workflow for local deployment of the SiM2P framework. It requires
as few as 20 site-specific cases and only basic demographic information. This
approach makes the established diagnostic benefits of FDG-PET imaging more
accessible to patients with suspected dementing disorders, potentially
improving early detection and differential diagnosis in resource-limited
settings. Our code is available at https://github.com/Yiiitong/SiM2P.

</details>


### [108] [ClapperText: A Benchmark for Text Recognition in Low-Resource Archival Documents](https://arxiv.org/abs/2510.15557)
*Tingyu Lin,Marco Peer,Florian Kleber,Robert Sablatnig*

Main category: cs.CV

TL;DR: ClapperText is a benchmark dataset for handwritten and printed text recognition in degraded, low-resource settings, derived from WWII-era archival videos containing clapperboards with production metadata.


<details>
  <summary>Details</summary>
Motivation: To address challenges in historical document analysis where structured content appears in degraded, non-standard forms, particularly in low-resource archival contexts with motion blur, handwriting variation, and cluttered backgrounds.

Method: Created from 127 WWII-era archival video segments containing clapperboards, with 9,813 annotated frames and 94,573 word-level text instances. Annotations include transcription, semantic category, text type, occlusion status, and rotated bounding boxes as 4-point polygons.

Result: Benchmarked six recognition and seven detection models under zero-shot and fine-tuned conditions. Despite small training set (18 videos), fine-tuning led to substantial performance gains, demonstrating suitability for few-shot learning scenarios.

Conclusion: ClapperText provides a realistic and culturally grounded resource for advancing robust OCR and document understanding in low-resource archival contexts, with the dataset and evaluation code publicly available.

Abstract: This paper presents ClapperText, a benchmark dataset for handwritten and
printed text recognition in visually degraded and low-resource settings. The
dataset is derived from 127 World War II-era archival video segments containing
clapperboards that record structured production metadata such as date,
location, and camera-operator identity. ClapperText includes 9,813 annotated
frames and 94,573 word-level text instances, 67% of which are handwritten and
1,566 are partially occluded. Each instance includes transcription, semantic
category, text type, and occlusion status, with annotations available as
rotated bounding boxes represented as 4-point polygons to support spatially
precise OCR applications. Recognizing clapperboard text poses significant
challenges, including motion blur, handwriting variation, exposure
fluctuations, and cluttered backgrounds, mirroring broader challenges in
historical document analysis where structured content appears in degraded,
non-standard forms. We provide both full-frame annotations and cropped word
images to support downstream tasks. Using a consistent per-video evaluation
protocol, we benchmark six representative recognition and seven detection
models under zero-shot and fine-tuned conditions. Despite the small training
set (18 videos), fine-tuning leads to substantial performance gains,
highlighting ClapperText's suitability for few-shot learning scenarios. The
dataset offers a realistic and culturally grounded resource for advancing
robust OCR and document understanding in low-resource archival contexts. The
dataset and evaluation code are available at
https://github.com/linty5/ClapperText.

</details>


### [109] [Shakti-VLMs: Scalable Vision-Language Models for Enterprise AI](https://arxiv.org/abs/2502.17092)
*Syed Abdul Gaffar Shakhadri,Kruthika KR,Kartik Basavaraj Angadi*

Main category: cs.CV

TL;DR: Shakti VLM is a family of vision-language models (1B and 4B parameters) that achieves competitive performance through architectural innovations and efficient training strategies rather than massive data scaling.


<details>
  <summary>Details</summary>
Motivation: To address data efficiency challenges in multimodal learning by developing models that can achieve strong performance with fewer training tokens, moving away from the trend of relying on extensive training data.

Method: Uses architectural innovations including QK-Normalization for attention stability, hybrid normalization techniques, enhanced positional encoding, and a three-stage training strategy to optimize learning efficiency.

Result: Shakti-VLM-1B and Shakti-VLM-4B excel in document understanding, visual reasoning, OCR extraction, and general multimodal reasoning, demonstrating competitive performance despite using fewer tokens.

Conclusion: High performance in multimodal tasks can be achieved through thoughtful model design and training strategies rather than sheer data volume, making Shakti an efficient solution for enterprise-scale applications.

Abstract: We introduce Shakti VLM, a family of vision-language models in the capacity
of 1B and 4B parameters designed to address data efficiency challenges in
multimodal learning. While recent VLMs achieve strong performance through
extensive training data, Shakti models leverage architectural innovations to
attain competitive results with fewer tokens. Key advancements include
QK-Normalization for attention stability, hybrid normalization techniques, and
enhanced positional encoding. A three-stage training strategy further optimizes
learning efficiency. Evaluations show that Shakti-Shakti-VLM-1B and
Shakti-VLM-4B excel in document understanding, Visual Reasoning, OCR
extraction, and general multimodal reasoning. Our results highlight that high
performance can be achieved through model design and training strategy rather
than sheer data volume, making Shakti an efficient solution for
enterprise-scale multimodal tasks.

</details>


### [110] [Imaginarium: Vision-guided High-Quality 3D Scene Layout Generation](https://arxiv.org/abs/2510.15564)
*Xiaoming Zhu,Xu Huang,Qinghongbing Xie,Zhi Deng,Junsheng Yu,Yirui Guan,Zhongyuan Liu,Lin Zhu,Qijun Zhao,Ligang Liu,Long Zeng*

Main category: cs.CV

TL;DR: A novel vision-guided 3D layout generation system that uses image generation models and robust image parsing to create coherent 3D scene layouts, outperforming existing methods in layout richness and quality.


<details>
  <summary>Details</summary>
Motivation: Traditional optimization-based methods are constrained by manual rules, deep generative models struggle with richness and diversity, and LLM-based approaches lack robustness in capturing complex spatial relationships.

Method: 1) Constructed high-quality asset library with 2,037 scene assets and 147 3D layouts; 2) Used image generation model expanded from prompts and fine-tuned for asset alignment; 3) Developed robust image parsing module for 3D layout recovery; 4) Optimized layout using scene graphs and visual semantics.

Result: Extensive user testing shows the algorithm significantly outperforms existing methods in layout richness and quality.

Conclusion: The proposed vision-guided system effectively addresses limitations of previous approaches and produces high-quality 3D scene layouts with logical coherence and visual alignment.

Abstract: Generating artistic and coherent 3D scene layouts is crucial in digital
content creation. Traditional optimization-based methods are often constrained
by cumbersome manual rules, while deep generative models face challenges in
producing content with richness and diversity. Furthermore, approaches that
utilize large language models frequently lack robustness and fail to accurately
capture complex spatial relationships. To address these challenges, this paper
presents a novel vision-guided 3D layout generation system. We first construct
a high-quality asset library containing 2,037 scene assets and 147 3D scene
layouts. Subsequently, we employ an image generation model to expand prompt
representations into images, fine-tuning it to align with our asset library. We
then develop a robust image parsing module to recover the 3D layout of scenes
based on visual semantics and geometric information. Finally, we optimize the
scene layout using scene graphs and overall visual semantics to ensure logical
coherence and alignment with the images. Extensive user testing demonstrates
that our algorithm significantly outperforms existing methods in terms of
layout richness and quality. The code and dataset will be available at
https://github.com/HiHiAllen/Imaginarium.

</details>


### [111] [Unmasking Facial DeepFakes: A Robust Multiview Detection Framework for Natural Images](https://arxiv.org/abs/2510.15576)
*Sami Belguesmia,Mohand Saïd Allili,Assia Hamadene*

Main category: cs.CV

TL;DR: Proposes a multi-view architecture with specialized encoders for DeepFake detection that handles pose variations and artifacts better than single-view methods.


<details>
  <summary>Details</summary>
Motivation: Existing DeepFake detection methods struggle with pose variations, occlusions, and artifacts in real-world conditions, requiring more robust solutions.

Method: Uses four specialized encoders: global view for boundary inconsistencies, middle view for texture/color alignment, local view for distortions in facial regions, and face orientation encoder for pose classification.

Result: Achieves superior performance in detecting manipulated images under challenging pose and lighting conditions, outperforming conventional single-view approaches.

Conclusion: The multi-view architecture with specialized feature analysis provides effective DeepFake detection that is robust to real-world challenges like pose variations and artifacts.

Abstract: DeepFake technology has advanced significantly in recent years, enabling the
creation of highly realistic synthetic face images. Existing DeepFake detection
methods often struggle with pose variations, occlusions, and artifacts that are
difficult to detect in real-world conditions. To address these challenges, we
propose a multi-view architecture that enhances DeepFake detection by analyzing
facial features at multiple levels. Our approach integrates three specialized
encoders, a global view encoder for detecting boundary inconsistencies, a
middle view encoder for analyzing texture and color alignment, and a local view
encoder for capturing distortions in expressive facial regions such as the
eyes, nose, and mouth, where DeepFake artifacts frequently occur. Additionally,
we incorporate a face orientation encoder, trained to classify face poses,
ensuring robust detection across various viewing angles. By fusing features
from these encoders, our model achieves superior performance in detecting
manipulated images, even under challenging pose and lighting
conditions.Experimental results on challenging datasets demonstrate the
effectiveness of our method, outperforming conventional single-view approaches

</details>


### [112] [Lightweight CycleGAN Models for Cross-Modality Image Transformation and Experimental Quality Assessment in Fluorescence Microscopy](https://arxiv.org/abs/2510.15579)
*Mohammad Soltaninezhad,Yashar Rouzbahani,Jhonatan Contreras,Rohan Chippalkatti,Daniel Kwaku Abankwa,Christian Eggeling,Thomas Bocklitz*

Main category: cs.CV

TL;DR: A lightweight CycleGAN for fluorescence microscopy modality transfer that reduces parameters from 41.8M to ~9K while improving performance, and also serves as a diagnostic tool for experimental quality assessment.


<details>
  <summary>Details</summary>
Motivation: To address the computational cost and environmental impact of deep learning models in scientific applications, particularly for unpaired dataset challenges in fluorescence microscopy modality transfer.

Method: Replaced traditional channel-doubling strategy in U-Net-based generator with fixed channel approach, creating a lightweight CycleGAN architecture for confocal to super-resolution STED/deconvolved STED transfer.

Result: Achieved drastic parameter reduction from 41.8 million to approximately nine thousand, with superior performance, faster training, and lower memory usage.

Conclusion: The lightweight CycleGAN not only enables efficient modality transfer but also serves as a practical diagnostic tool for validating experimental accuracy and image fidelity in microscopy workflows.

Abstract: Lightweight deep learning models offer substantial reductions in
computational cost and environmental impact, making them crucial for scientific
applications. We present a lightweight CycleGAN for modality transfer in
fluorescence microscopy (confocal to super-resolution STED/deconvolved STED),
addressing the common challenge of unpaired datasets. By replacing the
traditional channel-doubling strategy in the U-Net-based generator with a fixed
channel approach, we drastically reduce trainable parameters from 41.8 million
to approximately nine thousand, achieving superior performance with faster
training and lower memory usage. We also introduce the GAN as a diagnostic tool
for experimental and labeling quality. When trained on high-quality images, the
GAN learns the characteristics of optimal imaging; deviations between its
generated outputs and new experimental images can reveal issues such as
photobleaching, artifacts, or inaccurate labeling. This establishes the model
as a practical tool for validating experimental accuracy and image fidelity in
microscopy workflows.

</details>


### [113] [Standardization for improved Spatio-Temporal Image Fusion](https://arxiv.org/abs/2510.15589)
*Harkaitz Goyena,Peter M. Atkinson,Unai Pérez-Goya,M. Dolores Ugarte*

Main category: cs.CV

TL;DR: The paper proposes two standardization methods for Spatio-Temporal Image Fusion (STIF) to handle images from different sensors with varying resolutions. The first uses traditional upscaling, while the second (ABSIS) blends fine-resolution features with coarse-resolution attributes to improve fusion accuracy.


<details>
  <summary>Details</summary>
Motivation: STIF methods typically require images with matching spatial and spectral resolutions from different sensors, which limits their practical application. The authors aim to develop standardization approaches that enable STIF methods to work with images of different resolutions.

Method: Two standardization approaches: 1) Traditional upscaling of fine-resolution images, 2) ABSIS (Anomaly Based Satellite Image Standardization) - a sharpening method that blends overall features from fine-resolution image series with distinctive attributes from a specific coarse-resolution image.

Result: Both methods significantly increased accuracy of the USTFIP STIF method. The ABSIS sharpening approach achieved the best results, increasing spectral accuracy by up to 49.46% and spatial accuracy by up to 78.40% in the fused images.

Conclusion: Standardization approaches, particularly the ABSIS sharpening method, effectively improve STIF performance by enabling fusion of images with different resolutions, with substantial gains in both spectral and spatial accuracy.

Abstract: Spatio-Temporal Image Fusion (STIF) methods usually require sets of images
with matching spatial and spectral resolutions captured by different sensors.
To facilitate the application of STIF methods, we propose and compare two
different standardization approaches. The first method is based on traditional
upscaling of the fine-resolution images. The second method is a sharpening
approach called Anomaly Based Satellite Image Standardization (ABSIS) that
blends the overall features found in the fine-resolution image series with the
distinctive attributes of a specific coarse-resolution image to produce images
that more closely resemble the outcome of aggregating the fine-resolution
images. Both methods produce a significant increase in accuracy of the Unpaired
Spatio Temporal Fusion of Image Patches (USTFIP) STIF method, with the
sharpening approach increasing the spectral and spatial accuracies of the fused
images by up to 49.46\% and 78.40\%, respectively.

</details>


### [114] [FlexiReID: Adaptive Mixture of Expert for Multi-Modal Person Re-Identification](https://arxiv.org/abs/2510.15595)
*Zhen Sun,Lei Tan,Yunhang Shen,Chengmao Cai,Xing Sun,Pingyang Dai,Liujuan Cao,Rongrong Ji*

Main category: cs.CV

TL;DR: FlexiReID is a flexible multimodal person re-identification framework supporting 7 retrieval modes across 4 modalities (RGB, infrared, sketches, text) using adaptive MoE and cross-modal query fusion, achieving SOTA performance.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal Re-ID methods are limited to specific cross-modal settings and cannot support arbitrary query-retrieval combinations, which hinders practical deployment in real-world scenarios.

Method: Proposes FlexiReID with adaptive mixture-of-experts (MoE) mechanism to dynamically integrate diverse modality features and cross-modal query fusion module to enhance multimodal feature extraction. Also constructs CIRS-PEDES dataset covering all four modalities.

Result: Extensive experiments show FlexiReID achieves state-of-the-art performance and offers strong generalization in complex scenarios across the unified CIRS-PEDES dataset.

Conclusion: FlexiReID provides a comprehensive solution for flexible multimodal person re-identification that supports arbitrary query-retrieval combinations and demonstrates superior performance across diverse modalities.

Abstract: Multimodal person re-identification (Re-ID) aims to match pedestrian images
across different modalities. However, most existing methods focus on limited
cross-modal settings and fail to support arbitrary query-retrieval
combinations, hindering practical deployment. We propose FlexiReID, a flexible
framework that supports seven retrieval modes across four modalities: rgb,
infrared, sketches, and text. FlexiReID introduces an adaptive
mixture-of-experts (MoE) mechanism to dynamically integrate diverse modality
features and a cross-modal query fusion module to enhance multimodal feature
extraction. To facilitate comprehensive evaluation, we construct CIRS-PEDES, a
unified dataset extending four popular Re-ID datasets to include all four
modalities. Extensive experiments demonstrate that FlexiReID achieves
state-of-the-art performance and offers strong generalization in complex
scenarios.

</details>


### [115] [Quantized FCA: Efficient Zero-Shot Texture Anomaly Detection](https://arxiv.org/abs/2510.15602)
*Andrei-Timotei Ardelean,Patrick Rückbeil,Tim Weyrich*

Main category: cs.CV

TL;DR: QFCA is a real-time method for zero-shot anomaly localization in textures that achieves 10x speedup through quantization and feature preprocessing, maintaining accuracy while enabling practical deployment.


<details>
  <summary>Details</summary>
Motivation: Existing anomaly localization methods have high running times that make them impractical for real-world applications like assembly line monitoring.

Method: Proposes QFCA - a quantized version of feature correspondence analysis (FCA) that compares patch statistics using histograms of quantized values, plus PCA-based feature preprocessing to enhance contrast.

Result: Achieves 10x speedup with minimal accuracy loss and improved detection precision on complex textures compared to prior methods.

Conclusion: QFCA enables practical real-time anomaly localization in industrial applications while maintaining competitive performance.

Abstract: Zero-shot anomaly localization is a rising field in computer vision research,
with important progress in recent years. This work focuses on the problem of
detecting and localizing anomalies in textures, where anomalies can be defined
as the regions that deviate from the overall statistics, violating the
stationarity assumption. The main limitation of existing methods is their high
running time, making them impractical for deployment in real-world scenarios,
such as assembly line monitoring. We propose a real-time method, named QFCA,
which implements a quantized version of the feature correspondence analysis
(FCA) algorithm. By carefully adapting the patch statistics comparison to work
on histograms of quantized values, we obtain a 10x speedup with little to no
loss in accuracy. Moreover, we introduce a feature preprocessing step based on
principal component analysis, which enhances the contrast between normal and
anomalous features, improving the detection precision on complex textures. Our
method is thoroughly evaluated against prior art, comparing favorably with
existing methods. Project page:
https://reality.tf.fau.de/pub/ardelean2025quantized.html

</details>


### [116] [Lightweight Data-Free Denoising for Detail-Preserving Biomedical Image Restoration](https://arxiv.org/abs/2510.15611)
*Tomáš Chobola,Julia A. Schnabel,Tingying Peng*

Main category: cs.CV

TL;DR: Noise2Detail (N2D) is an ultra-lightweight self-supervised denoising model that achieves fast inference and high-quality restoration without requiring clean training data or explicit noise modeling.


<details>
  <summary>Details</summary>
Motivation: Current self-supervised denoising methods have high computational and memory demands, forcing trade-offs between speed and quality, limiting real-world applications especially in biomedical imaging where clean training data is scarce.

Method: Built on Noise2Noise framework, introduces multistage denoising pipeline that disrupts spatial noise correlations to create smooth intermediate structures, then refines details directly from noisy input.

Result: Extensive testing shows Noise2Detail outperforms existing dataset-free techniques while requiring only a fraction of computational resources.

Conclusion: The combination of efficiency, low computational cost, and data-free approach makes N2D valuable for biomedical imaging, overcoming clean data scarcity while enabling practical fast inference.

Abstract: Current self-supervised denoising techniques achieve impressive results, yet
their real-world application is frequently constrained by substantial
computational and memory demands, necessitating a compromise between inference
speed and reconstruction quality. In this paper, we present an
ultra-lightweight model that addresses this challenge, achieving both fast
denoising and high quality image restoration. Built upon the Noise2Noise
training framework-which removes the reliance on clean reference images or
explicit noise modeling-we introduce an innovative multistage denoising
pipeline named Noise2Detail (N2D). During inference, this approach disrupts the
spatial correlations of noise patterns to produce intermediate smooth
structures, which are subsequently refined to recapture fine details directly
from the noisy input. Extensive testing reveals that Noise2Detail surpasses
existing dataset-free techniques in performance, while requiring only a
fraction of the computational resources. This combination of efficiency, low
computational cost, and data-free approach make it a valuable tool for
biomedical imaging, overcoming the challenges of scarce clean training data-due
to rare and complex imaging modalities-while enabling fast inference for
practical use.

</details>


### [117] [Deep Learning Based Domain Adaptation Methods in Remote Sensing: A Comprehensive Survey](https://arxiv.org/abs/2510.15615)
*Shuchang Lyu,Qi Zhao,Zheng Zhou,Meng Li,You Zhou,Dingding Yao,Guangliang Cheng,Huiyu Zhou,Zhenwei Shi*

Main category: cs.CV

TL;DR: A comprehensive survey of deep learning-based domain adaptation methods for remote sensing, covering methodology taxonomy, datasets, performance analysis, and future research directions.


<details>
  <summary>Details</summary>
Motivation: Domain adaptation is crucial for remote sensing to transfer knowledge between differently distributed domains, but faces challenges from data variations like different sensors, landscapes, and environmental conditions.

Method: The survey organizes domain adaptation algorithms from multiple perspectives including task categorization, input mode, supervision paradigm, and algorithmic granularity, and reviews widely used datasets.

Result: Provides a systematic taxonomy and comprehensive overview of current progress in remote sensing domain adaptation, summarizing state-of-the-art method performance.

Conclusion: This survey addresses a broader range of domain adaptation tasks than previous works, presents a systematic taxonomy, and can inspire future research in the field.

Abstract: Domain adaptation is a crucial and increasingly important task in remote
sensing, aiming to transfer knowledge from a source domain a differently
distributed target domain. It has broad applications across various real-world
applications, including remote sensing element interpretation, ecological
environment monitoring, and urban/rural planning. However, domain adaptation in
remote sensing poses significant challenges due to differences in data, such as
variations in ground sampling distance, imaging modes from various sensors,
geographical landscapes, and environmental conditions. In recent years, deep
learning has emerged as a powerful tool for feature representation and
cross-domain knowledge transfer, leading to widespread adoption in remote
sensing tasks. In this paper, we present a comprehensive survey of significant
advancements in deep learning based domain adaptation for remote sensing. We
first introduce the preliminary knowledge to clarify key concepts, mathematical
notations, and the taxonomy of methodologies. We then organize existing
algorithms from multiple perspectives, including task categorization, input
mode, supervision paradigm, and algorithmic granularity, providing readers with
a structured understanding of the field. Next, we review widely used datasets
and summarize the performance of state-of-the-art methods to provide an
overview of current progress. We also identify open challenges and potential
directions to guide future research in domain adaptation for remote sensing.
Compared to previous surveys, this work addresses a broader range of domain
adaptation tasks in remote sensing, rather than concentrating on a few
subfields. It also presents a systematic taxonomy, providing a more
comprehensive and organized understanding of the field. As a whole, this survey
can inspire the research community, foster understanding, and guide future work
in the field.

</details>


### [118] [Uncertainty-Aware Extreme Point Tracing for Weakly Supervised Ultrasound Image Segmentation](https://arxiv.org/abs/2510.15666)
*Lei Shi,Gang Li,Junxing Zhang*

Main category: cs.CV

TL;DR: A weakly supervised medical image segmentation method using only four extreme points as annotation, leveraging SAM2 for pseudo label generation and enhanced FGEPM with uncertainty estimation for refinement, achieving performance comparable to fully supervised methods.


<details>
  <summary>Details</summary>
Motivation: To reduce the high annotation cost of fully supervised medical image segmentation which requires extensive pixel-level annotations that are costly and time-consuming.

Method: Uses bounding boxes from extreme points as SAM2 prompts for pseudo labels, refined by enhanced FGEPM with Monte Carlo dropout uncertainty estimation, plus dual-branch USC loss and box alignment loss for spatial consistency.

Result: Extensive experiments on BUSI and UNS ultrasound datasets show performance comparable to and even surpassing fully supervised methods while significantly reducing annotation cost.

Conclusion: The proposed weakly supervised framework is effective and practical for ultrasound image segmentation, validating its ability to achieve high performance with minimal annotation effort.

Abstract: Automatic medical image segmentation is a fundamental step in computer-aided
diagnosis, yet fully supervised approaches demand extensive pixel-level
annotations that are costly and time-consuming. To alleviate this burden, we
propose a weakly supervised segmentation framework that leverages only four
extreme points as annotation. Specifically, bounding boxes derived from the
extreme points are used as prompts for the Segment Anything Model 2 (SAM2) to
generate reliable initial pseudo labels. These pseudo labels are progressively
refined by an enhanced Feature-Guided Extreme Point Masking (FGEPM) algorithm,
which incorporates Monte Carlo dropout-based uncertainty estimation to
construct a unified gradient uncertainty cost map for boundary tracing.
Furthermore, a dual-branch Uncertainty-aware Scale Consistency (USC) loss and a
box alignment loss are introduced to ensure spatial consistency and precise
boundary alignment during training. Extensive experiments on two public
ultrasound datasets, BUSI and UNS, demonstrate that our method achieves
performance comparable to, and even surpassing fully supervised counterparts
while significantly reducing annotation cost. These results validate the
effectiveness and practicality of the proposed weakly supervised framework for
ultrasound image segmentation.

</details>


### [119] [Valeo Near-Field: a novel dataset for pedestrian intent detection](https://arxiv.org/abs/2510.15673)
*Antonyo Musabini,Rachid Benmokhtar,Jagdish Bhanushali,Victor Galizzi,Bertrand Luvison,Xavier Perrotton*

Main category: cs.CV

TL;DR: A novel multi-modal dataset for pedestrian intention detection with synchronized fisheye cameras, lidar, ultrasonic sensors, and motion capture data, released with benchmark suite for perception algorithms.


<details>
  <summary>Details</summary>
Motivation: To address real-world challenges in pedestrian detection and intention prediction for intelligent vehicles, including sensor occlusions, dynamic environments, and hardware constraints in near-field scenarios.

Method: Created a comprehensive dataset with synchronized multi-modal data (fisheye cameras, lidar, ultrasonic sensors, motion capture) collected across diverse real-world scenarios, with detailed annotations of 3D body joints and pedestrian positions.

Result: Released portion of dataset with benchmark suite featuring evaluation metrics for accuracy, efficiency, and scalability on embedded systems, along with baseline performance metrics using custom neural network architectures.

Conclusion: The dataset serves as a foundation for advancing pedestrian detection, 3D pose estimation, and intention prediction capabilities in intelligent vehicles, with provided baseline metrics and future research directions to encourage adoption and enhancement.

Abstract: This paper presents a novel dataset aimed at detecting pedestrians'
intentions as they approach an ego-vehicle. The dataset comprises synchronized
multi-modal data, including fisheye camera feeds, lidar laser scans, ultrasonic
sensor readings, and motion capture-based 3D body poses, collected across
diverse real-world scenarios. Key contributions include detailed annotations of
3D body joint positions synchronized with fisheye camera images, as well as
accurate 3D pedestrian positions extracted from lidar data, facilitating robust
benchmarking for perception algorithms. We release a portion of the dataset
along with a comprehensive benchmark suite, featuring evaluation metrics for
accuracy, efficiency, and scalability on embedded systems. By addressing
real-world challenges such as sensor occlusions, dynamic environments, and
hardware constraints, this dataset offers a unique resource for developing and
evaluating state-of-the-art algorithms in pedestrian detection, 3D pose
estimation and 4D trajectory and intention prediction. Additionally, we provide
baseline performance metrics using custom neural network architectures and
suggest future research directions to encourage the adoption and enhancement of
the dataset. This work aims to serve as a foundation for researchers seeking to
advance the capabilities of intelligent vehicles in near-field scenarios.

</details>


### [120] [Towards Label-Free Brain Tumor Segmentation: Unsupervised Learning with Multimodal MRI](https://arxiv.org/abs/2510.15684)
*Gerard Comas-Quiles,Carles Garcia-Cabrera,Julia Dietlmeier,Noel E. O'Connor,Ferran Marques*

Main category: cs.CV

TL;DR: Proposes MViT-AE, an unsupervised anomaly detection method using multimodal vision transformers trained only on healthy brain MRIs to detect and localize tumors via reconstruction errors, achieving clinically meaningful segmentation without manual labels.


<details>
  <summary>Details</summary>
Motivation: Addresses limitations of supervised learning for brain tumor segmentation when annotated datasets are limited, costly, or inconsistent, providing a scalable alternative for neuroimaging workflows.

Method: Uses Multimodal Vision Transformer Autoencoder trained on healthy brain MRIs, with multimodal early-late fusion strategy across MRI sequences and SAM-based post-processing to refine tumor contours.

Result: Achieves lesion-wise Dice scores of 0.437 (Whole Tumor), 0.316 (Tumor Core), 0.350 (Enhancing Tumor) on test set, and 89.4% anomaly detection rate on validation set using BraTS-GoAT 2025 dataset.

Conclusion: Transformer-based unsupervised models show potential as scalable, label-efficient tools for neuro-oncological imaging, despite challenges in detecting small or non-enhancing lesions.

Abstract: Unsupervised anomaly detection (UAD) presents a complementary alternative to
supervised learning for brain tumor segmentation in magnetic resonance imaging
(MRI), particularly when annotated datasets are limited, costly, or
inconsistent. In this work, we propose a novel Multimodal Vision Transformer
Autoencoder (MViT-AE) trained exclusively on healthy brain MRIs to detect and
localize tumors via reconstruction-based error maps. This unsupervised paradigm
enables segmentation without reliance on manual labels, addressing a key
scalability bottleneck in neuroimaging workflows. Our method is evaluated in
the BraTS-GoAT 2025 Lighthouse dataset, which includes various types of tumors
such as gliomas, meningiomas, and pediatric brain tumors. To enhance
performance, we introduce a multimodal early-late fusion strategy that
leverages complementary information across multiple MRI sequences, and a
post-processing pipeline that integrates the Segment Anything Model (SAM) to
refine predicted tumor contours. Despite the known challenges of UAD,
particularly in detecting small or non-enhancing lesions, our method achieves
clinically meaningful tumor localization, with lesion-wise Dice Similarity
Coefficient of 0.437 (Whole Tumor), 0.316 (Tumor Core), and 0.350 (Enhancing
Tumor) on the test set, and an anomaly Detection Rate of 89.4% on the
validation set. These findings highlight the potential of transformer-based
unsupervised models to serve as scalable, label-efficient tools for
neuro-oncological imaging.

</details>


### [121] [Unimedvl: Unifying Medical Multimodal Understanding And Generation Through Observation-Knowledge-Analysis](https://arxiv.org/abs/2510.15710)
*Junzhi Ning,Wei Li,Cheng Tang,Jiashi Lin,Chenglong Ma,Chaoyang Zhang,Jiyao Liu,Ying Chen,Shujian Gao,Lihao Liu,Yuandong Pu,Huihui Xu,Chenhui Gou,Ziyan Huang,Yi Xin,Qi Qin,Zhongying Deng,Diping Song,Bin Fu,Guang Yang,Yuanfeng Ji,Tianbin Li,Yanzhou Su,Jin Ye,Shixiang Tang,Ming Hu,Junjun He*

Main category: cs.CV

TL;DR: Proposes UniMedVL, a unified multimodal model that simultaneously handles medical image understanding and generation tasks within a single architecture, bridging the gap between separate medical AI systems.


<details>
  <summary>Details</summary>
Motivation: Existing medical AI systems disrupt unified diagnostic workflows - image understanding models can't generate visual outputs, while image generation models can't provide textual explanations, creating gaps in multimodal capabilities.

Method: Three-level framework: 1) UniMed-5M dataset with 5.6M multimodal pairs for observation, 2) Progressive Curriculum Learning for medical knowledge integration, 3) UniMedVL unified model architecture for simultaneous image understanding and generation.

Result: Superior performance on 5 medical image understanding benchmarks, matches specialized models in generation quality across 8 medical imaging modalities, and enables bidirectional knowledge sharing where generation tasks enhance visual understanding.

Conclusion: Integrating traditionally separate medical AI capabilities within a single unified framework unlocks improvements across diverse medical vision-language tasks, demonstrating the value of unified multimodal architectures in medical diagnostics.

Abstract: Medical diagnostic applications require models that can process multimodal
medical inputs (images, patient histories, lab results) and generate diverse
outputs including both textual reports and visual content (annotations,
segmentation masks, and images). Despite this need, existing medical AI systems
disrupt this unified process: medical image understanding models interpret
images but cannot generate visual outputs, while medical image generation
models synthesize images but cannot provide textual explanations. This leads to
gaps in data representation, feature integration, and task-level multimodal
capabilities. To this end, we propose a multi-level framework that draws
inspiration from diagnostic workflows through the
Observation-Knowledge-Analysis (OKA) paradigm. Specifically, at the observation
level, we construct UniMed-5M, a dataset comprising over 5.6M samples that
reformat diverse unimodal data into multimodal pairs for foundational
observation. At the knowledge level, we propose Progressive Curriculum Learning
that systematically introduces medical multimodal knowledge. At the analysis
level, we introduce UniMedVL, the first medical unified multimodal model for
the simultaneous analysis of image understanding and generation tasks within a
single architecture. UniMedVL achieves superior performance on five medical
image understanding benchmarks, while matching specialized models in generation
quality across eight medical imaging modalities. Crucially, our unified
architecture enables bidirectional knowledge sharing: generation tasks enhance
visual understanding features, demonstrating that integrating traditionally
separate capabilities within a single medical framework unlocks improvements
across diverse medical vision-language tasks. Code is available at
https://github.com/uni-medical/UniMedVL.

</details>


### [122] [DGME-T: Directional Grid Motion Encoding for Transformer-Based Historical Camera Movement Classification](https://arxiv.org/abs/2510.15725)
*Tingyu Lin,Armin Dadras,Florian Kleber,Robert Sablatnig*

Main category: cs.CV

TL;DR: DGME-T enhances Video Swin Transformer with directional grid motion encoding from optical flow, improving camera movement classification accuracy on both modern and archival film footage.


<details>
  <summary>Details</summary>
Motivation: Camera movement classification models trained on modern footage perform poorly on archival films due to noise, missing frames, and low contrast that obscure motion cues.

Method: Introduces DGME-T, a lightweight extension to Video Swin Transformer that injects directional grid motion encoding (derived from optical flow) via a learnable normalized late-fusion layer.

Result: Improves top-1 accuracy from 81.78% to 86.14% and macro F1 from 82.08% to 87.81% on modern clips, and from 83.43% to 84.62% accuracy and 81.72% to 82.63% macro F1 on World-War-II footage.

Conclusion: Structured motion priors and transformer representations are complementary, and even small calibrated motion heads can substantially enhance robustness in degraded film analysis.

Abstract: Camera movement classification (CMC) models trained on contemporary,
high-quality footage often degrade when applied to archival film, where noise,
missing frames, and low contrast obscure motion cues. We bridge this gap by
assembling a unified benchmark that consolidates two modern corpora into four
canonical classes and restructures the HISTORIAN collection into five balanced
categories. Building on this benchmark, we introduce DGME-T, a lightweight
extension to the Video Swin Transformer that injects directional grid motion
encoding, derived from optical flow, via a learnable and normalised late-fusion
layer. DGME-T raises the backbone's top-1 accuracy from 81.78% to 86.14% and
its macro F1 from 82.08% to 87.81% on modern clips, while still improving the
demanding World-War-II footage from 83.43% to 84.62% accuracy and from 81.72%
to 82.63% macro F1. A cross-domain study further shows that an intermediate
fine-tuning stage on modern data increases historical performance by more than
five percentage points. These results demonstrate that structured motion priors
and transformer representations are complementary and that even a small,
carefully calibrated motion head can substantially enhance robustness in
degraded film analysis. Related resources are available at
https://github.com/linty5/DGME-T.

</details>


### [123] [Scaling Instruction-Based Video Editing with a High-Quality Synthetic Dataset](https://arxiv.org/abs/2510.15742)
*Qingyan Bai,Qiuyu Wang,Hao Ouyang,Yue Yu,Hanlin Wang,Wen Wang,Ka Leong Cheng,Shuailei Ma,Yanhong Zeng,Zichen Liu,Yinghao Xu,Yujun Shen,Qifeng Chen*

Main category: cs.CV

TL;DR: Ditto is a framework that generates large-scale, high-quality video editing training data by combining image editing with video generation, enabling state-of-the-art instruction-based video editing.


<details>
  <summary>Details</summary>
Motivation: Progress in instruction-based video editing is limited by the scarcity of large-scale, high-quality training data, which Ditto aims to solve.

Method: Ditto uses a data generation pipeline that fuses image editing with in-context video generation, employs an efficient distilled model with temporal enhancer, and uses an intelligent agent for instruction crafting and quality control.

Result: Generated Ditto-1M dataset with 1 million high-fidelity video editing examples using 12,000 GPU-days, and trained Editto model that achieves superior instruction-following ability.

Conclusion: Ditto establishes a new state-of-the-art in instruction-based video editing by solving the data scarcity problem through scalable, high-quality data generation.

Abstract: Instruction-based video editing promises to democratize content creation, yet
its progress is severely hampered by the scarcity of large-scale, high-quality
training data. We introduce Ditto, a holistic framework designed to tackle this
fundamental challenge. At its heart, Ditto features a novel data generation
pipeline that fuses the creative diversity of a leading image editor with an
in-context video generator, overcoming the limited scope of existing models. To
make this process viable, our framework resolves the prohibitive cost-quality
trade-off by employing an efficient, distilled model architecture augmented by
a temporal enhancer, which simultaneously reduces computational overhead and
improves temporal coherence. Finally, to achieve full scalability, this entire
pipeline is driven by an intelligent agent that crafts diverse instructions and
rigorously filters the output, ensuring quality control at scale. Using this
framework, we invested over 12,000 GPU-days to build Ditto-1M, a new dataset of
one million high-fidelity video editing examples. We trained our model, Editto,
on Ditto-1M with a curriculum learning strategy. The results demonstrate
superior instruction-following ability and establish a new state-of-the-art in
instruction-based video editing.

</details>


### [124] [SEGA: A Stepwise Evolution Paradigm for Content-Aware Layout Generation with Design Prior](https://arxiv.org/abs/2510.15749)
*Haoran Wang,Bo Zhao,Jinghui Wang,Hanzhang Wang,Huan Yang,Wei Ji,Hao Liu,Xinyan Xiao*

Main category: cs.CV

TL;DR: SEGA introduces a stepwise evolution paradigm for content-aware layout generation, using hierarchical reasoning with coarse-to-fine strategy and layout design principles to improve harmony with background images.


<details>
  <summary>Details</summary>
Motivation: Existing single-step reasoning methods fail with complex layouts due to lack of feedback-based self-correction mechanisms, leading to high failure rates.

Method: SEGA uses hierarchical reasoning: coarse module estimates layout planning, then refining module performs fine-level reasoning. Incorporates layout design principles as prior knowledge.

Result: Achieves state-of-the-art results on multiple benchmark datasets. Also presents GenPoster-100K, a new large-scale poster dataset with rich meta-information annotation.

Conclusion: The stepwise evolution paradigm with hierarchical reasoning and design principles effectively addresses content-aware layout generation challenges.

Abstract: In this paper, we study the content-aware layout generation problem, which
aims to automatically generate layouts that are harmonious with a given
background image. Existing methods usually deal with this task with a
single-step reasoning framework. The lack of a feedback-based self-correction
mechanism leads to their failure rates significantly increasing when faced with
complex element layout planning. To address this challenge, we introduce SEGA,
a novel Stepwise Evolution Paradigm for Content-Aware Layout Generation.
Inspired by the systematic mode of human thinking, SEGA employs a hierarchical
reasoning framework with a coarse-to-fine strategy: first, a coarse-level
module roughly estimates the layout planning results; then, another refining
module performs fine-level reasoning regarding the coarse planning results.
Furthermore, we incorporate layout design principles as prior knowledge into
the model to enhance its layout planning ability. Besides, we present
GenPoster-100K that is a new large-scale poster dataset with rich
meta-information annotation. The experiments demonstrate the effectiveness of
our approach by achieving the state-of-the-art results on multiple benchmark
datasets. Our project page is at: https://brucew91.github.io/SEGA.github.io/

</details>


### [125] [NDM: A Noise-driven Detection and Mitigation Framework against Implicit Sexual Intentions in Text-to-Image Generation](https://arxiv.org/abs/2510.15752)
*Yitong Sun,Yao Huang,Ruochen Zhang,Huanran Chen,Shouwei Ruan,Ranjie Duan,Xingxing Wei*

Main category: cs.CV

TL;DR: NDM is a noise-driven framework that detects and mitigates implicit sexual content in text-to-image generation without degrading model quality, using noise-based detection and adaptive negative guidance.


<details>
  <summary>Details</summary>
Motivation: Text-to-image diffusion models are vulnerable to generating inappropriate content from implicit sexual prompts that bypass existing detection methods, creating ethical concerns.

Method: Uses noise separability for detection and noise-enhanced adaptive negative guidance for mitigation by optimizing initial noise and suppressing attention in prominent regions.

Result: NDM outperforms state-of-the-art methods (SLD, UCE, RECE) on both natural and adversarial datasets while preserving generative capabilities.

Conclusion: NDM effectively addresses implicit malicious intentions in T2I generation through noise-driven detection and mitigation without compromising model quality.

Abstract: Despite the impressive generative capabilities of text-to-image (T2I)
diffusion models, they remain vulnerable to generating inappropriate content,
especially when confronted with implicit sexual prompts. Unlike explicit
harmful prompts, these subtle cues, often disguised as seemingly benign terms,
can unexpectedly trigger sexual content due to underlying model biases, raising
significant ethical concerns. However, existing detection methods are primarily
designed to identify explicit sexual content and therefore struggle to detect
these implicit cues. Fine-tuning approaches, while effective to some extent,
risk degrading the model's generative quality, creating an undesirable
trade-off. To address this, we propose NDM, the first noise-driven detection
and mitigation framework, which could detect and mitigate implicit malicious
intention in T2I generation while preserving the model's original generative
capabilities. Specifically, we introduce two key innovations: first, we
leverage the separability of early-stage predicted noise to develop a
noise-based detection method that could identify malicious content with high
accuracy and efficiency; second, we propose a noise-enhanced adaptive negative
guidance mechanism that could optimize the initial noise by suppressing the
prominent region's attention, thereby enhancing the effectiveness of adaptive
negative guidance for sexual mitigation. Experimentally, we validate NDM on
both natural and adversarial datasets, demonstrating its superior performance
over existing SOTA methods, including SLD, UCE, and RECE, etc. Code and
resources are available at https://github.com/lorraine021/NDM.

</details>


### [126] [Semantic segmentation with coarse annotations](https://arxiv.org/abs/2510.15756)
*Jort de Jong,Mike Holenderski*

Main category: cs.CV

TL;DR: Proposes a regularization method using SLIC superpixels to improve semantic segmentation with coarse annotations, focusing on boundary alignment.


<details>
  <summary>Details</summary>
Motivation: Fine pixel-level annotations are expensive to obtain, while coarse annotations are more practical but lead to poor boundary alignment in segmentation models.

Method: Uses SLIC superpixel-based regularization in encoder-decoder networks to encourage segmented pixels to align with color/position-based superpixels, independent of annotations.

Result: Significant improvement in boundary recall on SUIM, Cityscapes, and PanNuke datasets compared to state-of-the-art models trained on coarse annotations.

Conclusion: Superpixel-based regularization effectively addresses boundary alignment issues in semantic segmentation when only coarse annotations are available.

Abstract: Semantic segmentation is the task of classifying each pixel in an image.
Training a segmentation model achieves best results using annotated images,
where each pixel is annotated with the corresponding class. When obtaining fine
annotations is difficult or expensive, it may be possible to acquire coarse
annotations, e.g. by roughly annotating pixels in an images leaving some pixels
around the boundaries between classes unlabeled. Segmentation with coarse
annotations is difficult, in particular when the objective is to optimize the
alignment of boundaries between classes. This paper proposes a regularization
method for models with an encoder-decoder architecture with superpixel based
upsampling. It encourages the segmented pixels in the decoded image to be
SLIC-superpixels, which are based on pixel color and position, independent of
the segmentation annotation. The method is applied to FCN-16 fully
convolutional network architecture and evaluated on the SUIM, Cityscapes, and
PanNuke data sets. It is shown that the boundary recall improves significantly
compared to state-of-the-art models when trained on coarse annotations.

</details>


### [127] [QSilk: Micrograin Stabilization and Adaptive Quantile Clipping for Detail-Friendly Latent Diffusion](https://arxiv.org/abs/2510.15761)
*Denis Rychkovskiy*

Main category: cs.CV

TL;DR: QSilk is a lightweight stabilization layer for latent diffusion models that improves high-frequency fidelity and suppresses activation spikes through per-sample clamping and adaptive quantile clipping.


<details>
  <summary>Details</summary>
Motivation: To address issues with high-frequency fidelity and rare activation spikes in latent diffusion models, particularly at low step counts and ultra-high resolutions, without requiring training or fine-tuning.

Method: Combines per-sample micro clamp to gently limit extreme values without washing out texture, and Adaptive Quantile Clip (AQClip) that adapts allowed value corridors per region using either local structure statistics or attention entropy guidance.

Result: Yields cleaner, sharper results at low step counts and ultra-high resolutions with negligible overhead, shows consistent improvements across SD/SDXL backbones, and enables higher guidance without artifacts.

Conclusion: QSilk provides an effective stabilization solution for latent diffusion models that improves output quality with minimal computational overhead and no training requirements.

Abstract: We present QSilk, a lightweight, always-on stabilization layer for latent
diffusion that improves high-frequency fidelity while suppressing rare
activation spikes. QSilk combines (i) a per-sample micro clamp that gently
limits extreme values without washing out texture, and (ii) Adaptive Quantile
Clip (AQClip), which adapts the allowed value corridor per region. AQClip can
operate in a proxy mode using local structure statistics or in an attention
entropy guided mode (model confidence). Integrated into the CADE 2.5 rendering
pipeline, QSilk yields cleaner, sharper results at low step counts and
ultra-high resolutions with negligible overhead. It requires no training or
fine-tuning and exposes minimal user controls. We report consistent qualitative
improvements across SD/SDXL backbones and show synergy with CFG/Rescale,
enabling slightly higher guidance without artifacts.

</details>


### [128] [Towards more holistic interpretability: A lightweight disentangled Concept Bottleneck Model](https://arxiv.org/abs/2510.15770)
*Gaoxiang Huang,Songning Lai,Yutao Yue*

Main category: cs.CV

TL;DR: LDCBM improves Concept Bottleneck Models by automatically grouping visual features into meaningful components without region annotation, achieving better concept-class alignment and outperforming previous CBMs.


<details>
  <summary>Details</summary>
Motivation: Existing CBMs suffer from input-to-concept mapping bias and limited controllability, which restricts their practical value and damages the responsibility of concept-based methods.

Method: Introduces a lightweight Disentangled Concept Bottleneck Model with filter grouping loss and joint concept supervision to automatically group visual features into semantically meaningful components without region annotation.

Result: Experiments on three diverse datasets show LDCBM achieves higher concept and class accuracy, outperforming previous CBMs in both interpretability and classification performance.

Conclusion: By grounding concepts in visual evidence, LDCBM overcomes fundamental limitations of prior models and enhances the reliability of interpretable AI.

Abstract: Concept Bottleneck Models (CBMs) enhance interpretability by predicting
human-understandable concepts as intermediate representations. However,
existing CBMs often suffer from input-to-concept mapping bias and limited
controllability, which restricts their practical value, directly damage the
responsibility of strategy from concept-based methods. We propose a lightweight
Disentangled Concept Bottleneck Model (LDCBM) that automatically groups visual
features into semantically meaningful components without region annotation. By
introducing a filter grouping loss and joint concept supervision, our method
improves the alignment between visual patterns and concepts, enabling more
transparent and robust decision-making. Notably, Experiments on three diverse
datasets demonstrate that LDCBM achieves higher concept and class accuracy,
outperforming previous CBMs in both interpretability and classification
performance. By grounding concepts in visual evidence, our method overcomes a
fundamental limitation of prior models and enhances the reliability of
interpretable AI.

</details>


### [129] [Controlling the image generation process with parametric activation functions](https://arxiv.org/abs/2510.15778)
*Ilia Pavlov*

Main category: cs.CV

TL;DR: A system for interactive understanding and control of generative models by replacing activation functions with parametric ones.


<details>
  <summary>Details</summary>
Motivation: To develop interpretable tools that allow direct interaction with internal mechanisms of image generative models, which have received little attention despite their increasing fidelity and ubiquity.

Method: Allow users to replace activation functions in generative networks with parametric functions and set their parameters, providing an alternative approach to control network output.

Result: Demonstrated on StyleGAN2 (trained on FFHQ) and BigGAN (trained on ImageNet) networks.

Conclusion: The introduced system enables users to better understand generative models through interaction and experimentation by manipulating activation functions.

Abstract: As image generative models continue to increase not only in their fidelity
but also in their ubiquity the development of tools that leverage direct
interaction with their internal mechanisms in an interpretable way has received
little attention In this work we introduce a system that allows users to
develop a better understanding of the model through interaction and
experimentation By giving users the ability to replace activation functions of
a generative network with parametric ones and a way to set the parameters of
these functions we introduce an alternative approach to control the networks
output We demonstrate the use of our method on StyleGAN2 and BigGAN networks
trained on FFHQ and ImageNet respectively.

</details>


### [130] [ReCon: Region-Controllable Data Augmentation with Rectification and Alignment for Object Detection](https://arxiv.org/abs/2510.15783)
*Haowei Zhu,Tianxiang Pan,Rui Qin,Jun-Hai Yong,Bin Wang*

Main category: cs.CV

TL;DR: ReCon is a novel data augmentation framework that enhances structure-controllable generative models for object detection by integrating region-guided rectification and region-aligned cross-attention to address content-position mismatches and semantic leakage.


<details>
  <summary>Details</summary>
Motivation: Current generative approaches for data augmentation require complex post-processing or extensive fine-tuning on massive datasets and suffer from content-position mismatches and semantic leakage, making large-scale annotated data acquisition costly and time-consuming.

Method: ReCon integrates region-guided rectification into diffusion sampling using feedback from pre-trained perception models, and proposes region-aligned cross-attention to enforce spatial-semantic alignment between image regions and textual cues.

Result: Extensive experiments show ReCon substantially improves the quality and trainability of generated data, achieving consistent performance gains across various datasets, backbone architectures, and data scales.

Conclusion: ReCon effectively overcomes limitations of current generative approaches by providing a robust framework that enhances data augmentation for object detection tasks without requiring complex post-processing or extensive fine-tuning.

Abstract: The scale and quality of datasets are crucial for training robust perception
models. However, obtaining large-scale annotated data is both costly and
time-consuming. Generative models have emerged as a powerful tool for data
augmentation by synthesizing samples that adhere to desired distributions.
However, current generative approaches often rely on complex post-processing or
extensive fine-tuning on massive datasets to achieve satisfactory results, and
they remain prone to content-position mismatches and semantic leakage. To
overcome these limitations, we introduce ReCon, a novel augmentation framework
that enhances the capacity of structure-controllable generative models for
object detection. ReCon integrates region-guided rectification into the
diffusion sampling process, using feedback from a pre-trained perception model
to rectify misgenerated regions within diffusion sampling process. We further
propose region-aligned cross-attention to enforce spatial-semantic alignment
between image regions and their textual cues, thereby improving both semantic
consistency and overall image fidelity. Extensive experiments demonstrate that
ReCon substantially improve the quality and trainability of generated data,
achieving consistent performance gains across various datasets, backbone
architectures, and data scales. Our code is available at
https://github.com/haoweiz23/ReCon .

</details>


### [131] [OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding LLM](https://arxiv.org/abs/2510.15870)
*Hanrong Ye,Chao-Han Huck Yang,Arushi Goel,Wei Huang,Ligeng Zhu,Yuanhang Su,Sean Lin,An-Chieh Cheng,Zhen Wan,Jinchuan Tian,Yuming Lou,Dong Yang,Zhijian Liu,Yukang Chen,Ambrish Dantrey,Ehsan Jahangiri,Sreyan Ghosh,Daguang Xu,Ehsan Hosseini-Asl,Danial Mohseni Taheri,Vidya Murali,Sifei Liu,Jason Lu,Oluwatobi Olabiyi,Frank Wang,Rafael Valle,Bryan Catanzaro,Andrew Tao,Song Han,Jan Kautz,Hongxu Yin,Pavlo Molchanov*

Main category: cs.CV

TL;DR: OmniVinci is an open-source omni-modal LLM that introduces three architectural innovations for better cross-modal alignment and achieves state-of-the-art performance with significantly less training data.


<details>
  <summary>Details</summary>
Motivation: To advance machine intelligence by developing multimodal perception capabilities similar to human sensing across vision, audio, and other modalities.

Method: Three key innovations: OmniAlignNet for vision-audio embedding alignment, Temporal Embedding Grouping for relative temporal alignment, and Constrained Rotary Time Embedding for absolute temporal encoding. Uses a curation pipeline generating 24M multimodal conversations.

Result: Outperforms Qwen2.5-Omni with +19.05 on DailyOmni, +1.7 on MMAR, and +3.9 on Video-MME, while using only 0.2T training tokens (6x reduction compared to Qwen2.5-Omni's 1.2T).

Conclusion: OmniVinci demonstrates that modalities reinforce each other in perception and reasoning, showing advantages in robotics, medical AI, and smart factory applications.

Abstract: Advancing machine intelligence requires developing the ability to perceive
across multiple modalities, much as humans sense the world. We introduce
OmniVinci, an initiative to build a strong, open-source, omni-modal LLM. We
carefully study the design choices across model architecture and data curation.
For model architecture, we present three key innovations: (i) OmniAlignNet for
strengthening alignment between vision and audio embeddings in a shared
omni-modal latent space; (ii) Temporal Embedding Grouping for capturing
relative temporal alignment between vision and audio signals; and (iii)
Constrained Rotary Time Embedding for encoding absolute temporal information in
omni-modal embeddings. We introduce a curation and synthesis pipeline that
generates 24M single-modal and omni-modal conversations. We find that
modalities reinforce one another in both perception and reasoning. Our model,
OmniVinci, outperforms Qwen2.5-Omni with +19.05 on DailyOmni (cross-modal
understanding), +1.7 on MMAR (audio), and +3.9 on Video-MME (vision), while
using just 0.2T training tokens - a 6 times reduction compared to
Qwen2.5-Omni's 1.2T. We finally demonstrate omni-modal advantages in downstream
applications spanning robotics, medical AI, and smart factory.

</details>


### [132] [ERNet: Efficient Non-Rigid Registration Network for Point Sequences](https://arxiv.org/abs/2510.15800)
*Guangzhao He,Yuxi Xiao,Zhen Xu,Xiaowei Zhou,Sida Peng*

Main category: cs.CV

TL;DR: ERNet is an efficient feed-forward model for non-rigid point cloud registration that uses a two-stage pipeline with deformation graphs to handle noisy/partial inputs and prevent error accumulation in long sequences.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in non-rigid point cloud registration: local minima due to non-convex objectives with noisy/partial inputs, and error accumulation over long sequences causing tracking failures.

Method: Two-stage pipeline: first estimates frame-wise coarse graph nodes for robust initialization, then refines their trajectories over time in sliding-window fashion using deformation graphs.

Result: Outperforms previous state-of-the-art on DeformingThings4D and D-FAUST datasets, achieves more than 4x speedup compared to previous best method.

Conclusion: ERNet provides accurate, robust, and efficient sequential registration for non-rigidly deforming point clouds, handling noise and partial inputs effectively.

Abstract: Registering an object shape to a sequence of point clouds undergoing
non-rigid deformation is a long-standing challenge. The key difficulties stem
from two factors: (i) the presence of local minima due to the non-convexity of
registration objectives, especially under noisy or partial inputs, which
hinders accurate and robust deformation estimation, and (ii) error accumulation
over long sequences, leading to tracking failures. To address these challenges,
we introduce to adopt a scalable data-driven approach and propose ERNet, an
efficient feed-forward model trained on large deformation datasets. It is
designed to handle noisy and partial inputs while effectively leveraging
temporal information for accurate and consistent sequential registration. The
key to our design is predicting a sequence of deformation graphs through a
two-stage pipeline, which first estimates frame-wise coarse graph nodes for
robust initialization, before refining their trajectories over time in a
sliding-window fashion. Extensive experiments show that our proposed approach
(i) outperforms previous state-of-the-art on both the DeformingThings4D and
D-FAUST datasets, and (ii) achieves more than 4x speedup compared to the
previous best, offering significant efficiency improvement.

</details>


### [133] [VISTA: A Test-Time Self-Improving Video Generation Agent](https://arxiv.org/abs/2510.15831)
*Do Xuan Long,Xingchen Wan,Hootan Nakhost,Chen-Yu Lee,Tomas Pfister,Sercan Ö. Arık*

Main category: cs.CV

TL;DR: VISTA is a multi-agent system that autonomously improves video generation through iterative prompt refinement, achieving significant quality improvements over state-of-the-art baselines.


<details>
  <summary>Details</summary>
Motivation: Existing test-time optimization methods struggle with video generation's multi-faceted nature, and video quality remains critically dependent on precise user prompts.

Method: VISTA decomposes user ideas into temporal plans, identifies best videos through pairwise tournaments, critiques them with specialized agents (visual, audio, contextual), and uses a reasoning agent to rewrite prompts for iterative improvement.

Result: VISTA consistently improves video quality and alignment with user intent, achieving up to 60% pairwise win rate against state-of-the-art baselines, with human evaluators preferring VISTA outputs in 66.4% of comparisons.

Conclusion: VISTA demonstrates that autonomous iterative refinement through multi-agent collaboration effectively addresses the challenges of video generation quality and prompt dependency.

Abstract: Despite rapid advances in text-to-video synthesis, generated video quality
remains critically dependent on precise user prompts. Existing test-time
optimization methods, successful in other domains, struggle with the
multi-faceted nature of video. In this work, we introduce VISTA (Video
Iterative Self-improvemenT Agent), a novel multi-agent system that autonomously
improves video generation through refining prompts in an iterative loop. VISTA
first decomposes a user idea into a structured temporal plan. After generation,
the best video is identified through a robust pairwise tournament. This winning
video is then critiqued by a trio of specialized agents focusing on visual,
audio, and contextual fidelity. Finally, a reasoning agent synthesizes this
feedback to introspectively rewrite and enhance the prompt for the next
generation cycle. Experiments on single- and multi-scene video generation
scenarios show that while prior methods yield inconsistent gains, VISTA
consistently improves video quality and alignment with user intent, achieving
up to 60% pairwise win rate against state-of-the-art baselines. Human
evaluators concur, preferring VISTA outputs in 66.4% of comparisons.

</details>


### [134] [Neuro-Symbolic Spatial Reasoning in Segmentation](https://arxiv.org/abs/2510.15841)
*Jiayi Lin,Jiabo Huang,Shaogang Gong*

Main category: cs.CV

TL;DR: RelateSeg introduces neuro-symbolic spatial reasoning to Open-Vocabulary Semantic Segmentation by encoding spatial relations as first-order logic constraints, achieving state-of-the-art performance without additional parameters.


<details>
  <summary>Details</summary>
Motivation: Current vision-language models for OVSS lack understanding of spatial relations between objects, leading to poor performance on complex scenes with multiple objects.

Method: Proposes RelateSeg that automatically extracts spatial relations and encodes them as first-order logic formulas using pseudo categories. Each pixel predicts both semantic and spatial pseudo categories simultaneously, with constraints enforced through fuzzy logic relaxation in a deep network.

Result: Achieves state-of-the-art average mIoU across four benchmark datasets, with particular advantages on images containing multiple categories. Only requires one auxiliary loss function and no additional parameters.

Conclusion: Neuro-symbolic spatial reasoning is effective for OVSS, providing relational consistency and improved performance on complex scenes.

Abstract: Open-Vocabulary Semantic Segmentation (OVSS) assigns pixel-level labels from
an open set of categories, requiring generalization to unseen and unlabelled
objects. Using vision-language models (VLMs) to correlate local image patches
with potential unseen object categories suffers from a lack of understanding of
spatial relations of objects in a scene. To solve this problem, we introduce
neuro-symbolic (NeSy) spatial reasoning in OVSS. In contrast to contemporary
VLM correlation-based approaches, we propose Relational Segmentor (RelateSeg)
to impose explicit spatial relational constraints by first order logic (FOL)
formulated in a neural network architecture. This is the first attempt to
explore NeSy spatial reasoning in OVSS. Specifically, RelateSeg automatically
extracts spatial relations, e.g., <cat, to-right-of, person>, and encodes them
as first-order logic formulas using our proposed pseudo categories. Each pixel
learns to predict both a semantic category (e.g., "cat") and a spatial pseudo
category (e.g., "right of person") simultaneously, enforcing relational
constraints (e.g., a "cat" pixel must lie to the right of a "person"). Finally,
these logic constraints are formulated in a deep network architecture by fuzzy
logic relaxation, enabling end-to-end learning of spatial-relationally
consistent segmentation. RelateSeg achieves state-of-the-art performance in
terms of average mIoU across four benchmark datasets and particularly shows
clear advantages on images containing multiple categories, with the cost of
only introducing a single auxiliary loss function and no additional parameters,
validating the effectiveness of NeSy spatial reasoning in OVSS.

</details>


### [135] [3DPR: Single Image 3D Portrait Relight using Generative Priors](https://arxiv.org/abs/2510.15846)
*Pramod Rao,Abhimitra Meka,Xilong Zhou,Gereon Fox,Mallikarjun B R,Fangneng Zhan,Tim Weyrich,Bernd Bickel,Hanspeter Pfister,Wojciech Matusik,Thabo Beeler,Mohamed Elgharib,Marc Habermann,Christian Theobalt*

Main category: cs.CV

TL;DR: 3DPR is an image-based relighting model that uses generative priors from multi-view OLAT images and a pre-trained generative head model to render novel relit views of human heads from monocular portrait images.


<details>
  <summary>Details</summary>
Motivation: Traditional graphics approaches for relighting human heads from monocular images are underconstrained and limited by model assumptions. The paper aims to leverage rich priors from large datasets to overcome these limitations.

Method: Uses encoder-based inversion to embed input portraits into a generative head model's latent space, then employs a triplane-based reflectance network trained on lightstage OLAT data to synthesize high-fidelity OLAT images for image-based relighting.

Result: 3DPR outperforms previous methods in preserving identity and capturing lighting effects like specularities, self-shadows, and subsurface scattering, producing physically accurate environmental relighting results.

Conclusion: The proposed method successfully combines generative priors from both in-the-wild images and controlled lightstage data to achieve high-quality relighting of human heads from single portrait images.

Abstract: Rendering novel, relit views of a human head, given a monocular portrait
image as input, is an inherently underconstrained problem. The traditional
graphics solution is to explicitly decompose the input image into geometry,
material and lighting via differentiable rendering; but this is constrained by
the multiple assumptions and approximations of the underlying models and
parameterizations of these scene components. We propose 3DPR, an image-based
relighting model that leverages generative priors learnt from multi-view
One-Light-at-A-Time (OLAT) images captured in a light stage. We introduce a new
diverse and large-scale multi-view 4K OLAT dataset of 139 subjects to learn a
high-quality prior over the distribution of high-frequency face reflectance. We
leverage the latent space of a pre-trained generative head model that provides
a rich prior over face geometry learnt from in-the-wild image datasets. The
input portrait is first embedded in the latent manifold of such a model through
an encoder-based inversion process. Then a novel triplane-based reflectance
network trained on our lightstage data is used to synthesize high-fidelity OLAT
images to enable image-based relighting. Our reflectance network operates in
the latent space of the generative head model, crucially enabling a relatively
small number of lightstage images to train the reflectance model. Combining the
generated OLATs according to a given HDRI environment maps yields physically
accurate environmental relighting results. Through quantitative and qualitative
evaluations, we demonstrate that 3DPR outperforms previous methods,
particularly in preserving identity and in capturing lighting effects such as
specularities, self-shadows, and subsurface scattering. Project Page:
https://vcai.mpi-inf.mpg.de/projects/3dpr/

</details>


### [136] [Memory-SAM: Human-Prompt-Free Tongue Segmentation via Retrieval-to-Prompt](https://arxiv.org/abs/2510.15849)
*Joongwon Chae,Lihui Luo,Xi Yuan,Dongmei Yu,Zhenglin Chen,Lian Zhang,Peiwu Qin*

Main category: cs.CV

TL;DR: Memory-SAM is a training-free, human-prompt-free pipeline that automatically generates prompts for SAM2 using DINOv3 features and FAISS retrieval from prior cases, achieving superior tongue segmentation performance without manual intervention.


<details>
  <summary>Details</summary>
Motivation: Supervised models require large annotated datasets for tongue segmentation in TCM analysis, while SAM-family models remain prompt-driven, necessitating manual input. There's a need for automated, data-efficient segmentation methods.

Method: Uses dense DINOv3 features and FAISS retrieval to automatically generate foreground/background point prompts from a small memory of prior cases. These prompts guide SAM2 without manual clicks or model fine-tuning.

Result: Achieves mIoU 0.9863 on mixed test split (600 expert-annotated images), significantly outperforming FCN (0.8188) and detector-to-box SAM baseline (0.1839). Shows clear gains under real-world conditions.

Conclusion: Retrieval-to-prompt approach enables data-efficient, robust segmentation of irregular boundaries in tongue imaging without requiring manual intervention or model training.

Abstract: Accurate tongue segmentation is crucial for reliable TCM analysis. Supervised
models require large annotated datasets, while SAM-family models remain
prompt-driven. We present Memory-SAM, a training-free, human-prompt-free
pipeline that automatically generates effective prompts from a small memory of
prior cases via dense DINOv3 features and FAISS retrieval. Given a query image,
mask-constrained correspondences to the retrieved exemplar are distilled into
foreground/background point prompts that guide SAM2 without manual clicks or
model fine-tuning. We evaluate on 600 expert-annotated images (300 controlled,
300 in-the-wild). On the mixed test split, Memory-SAM achieves mIoU 0.9863,
surpassing FCN (0.8188) and a detector-to-box SAM baseline (0.1839). On
controlled data, ceiling effects above 0.98 make small differences less
meaningful given annotation variability, while our method shows clear gains
under real-world conditions. Results indicate that retrieval-to-prompt enables
data-efficient, robust segmentation of irregular boundaries in tongue imaging.
The code is publicly available at https://github.com/jw-chae/memory-sam.

</details>


### [137] [BLIP3o-NEXT: Next Frontier of Native Image Generation](https://arxiv.org/abs/2510.15857)
*Jiuhai Chen,Le Xue,Zhiyang Xu,Xichen Pan,Shusheng Yang,Can Qin,An Yan,Honglu Zhou,Zeyuan Chen,Lifu Huang,Tianyi Zhou,Junnan Li,Silvio Savarese,Caiming Xiong,Ran Xu*

Main category: cs.CV

TL;DR: BLIP3o-NEXT is a fully open-source foundation model that unifies text-to-image generation and image editing in a single architecture, achieving state-of-the-art performance through an Autoregressive + Diffusion hybrid approach.


<details>
  <summary>Details</summary>
Motivation: To advance the frontier of native image generation by creating a unified model that can handle both text-to-image generation and image editing tasks effectively, while identifying key insights about architecture scaling, reinforcement learning, and data quality.

Method: Uses an Autoregressive + Diffusion architecture where an autoregressive model first generates discrete image tokens conditioned on multimodal inputs, and then uses these hidden states as conditioning signals for a diffusion model to generate high-fidelity images.

Result: Achieves superior performance over existing models on various text-to-image and image-editing benchmarks, demonstrating strong image generation and editing capabilities with enhanced coherence and realism.

Conclusion: The hybrid architecture successfully integrates the reasoning strength of autoregressive models with the fine-detail rendering of diffusion models, while identifying that data quality and scale remain decisive factors for model performance, and that reinforcement learning can further push the boundaries of native image generation.

Abstract: We present BLIP3o-NEXT, a fully open-source foundation model in the BLIP3
series that advances the next frontier of native image generation. BLIP3o-NEXT
unifies text-to-image generation and image editing within a single
architecture, demonstrating strong image generation and image editing
capabilities. In developing the state-of-the-art native image generation model,
we identify four key insights: (1) Most architectural choices yield comparable
performance; an architecture can be deemed effective provided it scales
efficiently and supports fast inference; (2) The successful application of
reinforcement learning can further push the frontier of native image
generation; (3) Image editing still remains a challenging task, yet instruction
following and the consistency between generated and reference images can be
significantly enhanced through post-training and data engine; (4) Data quality
and scale continue to be decisive factors that determine the upper bound of
model performance. Building upon these insights, BLIP3o-NEXT leverages an
Autoregressive + Diffusion architecture in which an autoregressive model first
generates discrete image tokens conditioned on multimodal inputs, whose hidden
states are then used as conditioning signals for a diffusion model to generate
high-fidelity images. This architecture integrates the reasoning strength and
instruction following of autoregressive models with the fine-detail rendering
ability of diffusion models, achieving a new level of coherence and realism.
Extensive evaluations of various text-to-image and image-editing benchmarks
show that BLIP3o-NEXT achieves superior performance over existing models.

</details>


### [138] [BiomedXPro: Prompt Optimization for Explainable Diagnosis with Biomedical Vision Language Models](https://arxiv.org/abs/2510.15866)
*Kaushitha Silva,Mansitha Eashwara,Sanduni Ubayasiri,Ruwan Tennakoon,Damayanthi Herath*

Main category: cs.CV

TL;DR: BiomedXPro is an evolutionary framework that uses LLMs to generate diverse, interpretable prompt pairs for biomedical diagnosis, outperforming state-of-the-art methods and providing verifiable clinical alignment.


<details>
  <summary>Details</summary>
Motivation: Current prompt optimization methods produce uninterpretable latent vectors or single prompts, lacking transparency and failing to capture the multi-faceted nature of clinical diagnosis, which limits trustworthiness in high-stakes medical settings.

Method: Uses an evolutionary framework with a large language model as both biomedical knowledge extractor and adaptive optimizer to automatically generate diverse ensembles of interpretable natural-language prompt pairs for disease diagnosis.

Result: Consistently outperforms state-of-the-art prompt-tuning methods across multiple biomedical benchmarks, especially in data-scarce few-shot settings. Shows strong semantic alignment between discovered prompts and statistically significant clinical features.

Conclusion: BiomedXPro provides a verifiable basis for model predictions through diverse interpretable prompts, representing a critical step toward more trustworthy and clinically-aligned AI systems.

Abstract: The clinical adoption of biomedical vision-language models is hindered by
prompt optimization techniques that produce either uninterpretable latent
vectors or single textual prompts. This lack of transparency and failure to
capture the multi-faceted nature of clinical diagnosis, which relies on
integrating diverse observations, limits their trustworthiness in high-stakes
settings. To address this, we introduce BiomedXPro, an evolutionary framework
that leverages a large language model as both a biomedical knowledge extractor
and an adaptive optimizer to automatically generate a diverse ensemble of
interpretable, natural-language prompt pairs for disease diagnosis. Experiments
on multiple biomedical benchmarks show that BiomedXPro consistently outperforms
state-of-the-art prompt-tuning methods, particularly in data-scarce few-shot
settings. Furthermore, our analysis demonstrates a strong semantic alignment
between the discovered prompts and statistically significant clinical features,
grounding the model's performance in verifiable concepts. By producing a
diverse ensemble of interpretable prompts, BiomedXPro provides a verifiable
basis for model predictions, representing a critical step toward the
development of more trustworthy and clinically-aligned AI systems.

</details>


### [139] [LightsOut: Diffusion-based Outpainting for Enhanced Lens Flare Removal](https://arxiv.org/abs/2510.15868)
*Shr-Ruei Tsai,Wei-Cheng Chang,Jie-Ying Lee,Chih-Hai Su,Yu-Lun Liu*

Main category: cs.CV

TL;DR: LightsOut is a diffusion-based outpainting framework that enhances Single Image Flare Removal (SIFR) by reconstructing off-frame light sources, improving performance without retraining existing methods.


<details>
  <summary>Details</summary>
Motivation: Current SIFR methods perform poorly when off-frame light sources are incomplete or absent, which degrades image quality and impacts critical computer vision tasks like object detection and autonomous driving.

Method: Proposes a diffusion-based outpainting framework with multitask regression module and LoRA fine-tuned diffusion model to reconstruct off-frame light sources realistically and maintain physical consistency.

Result: Comprehensive experiments show LightsOut consistently boosts performance of existing SIFR methods across challenging scenarios without requiring additional retraining.

Conclusion: LightsOut serves as a universally applicable plug-and-play preprocessing solution that enhances SIFR performance by addressing the limitation of incomplete off-frame light sources.

Abstract: Lens flare significantly degrades image quality, impacting critical computer
vision tasks like object detection and autonomous driving. Recent Single Image
Flare Removal (SIFR) methods perform poorly when off-frame light sources are
incomplete or absent. We propose LightsOut, a diffusion-based outpainting
framework tailored to enhance SIFR by reconstructing off-frame light sources.
Our method leverages a multitask regression module and LoRA fine-tuned
diffusion model to ensure realistic and physically consistent outpainting
results. Comprehensive experiments demonstrate LightsOut consistently boosts
the performance of existing SIFR methods across challenging scenarios without
additional retraining, serving as a universally applicable plug-and-play
preprocessing solution. Project page: https://ray-1026.github.io/lightsout/

</details>


### [140] [Skyfall-GS: Synthesizing Immersive 3D Urban Scenes from Satellite Imagery](https://arxiv.org/abs/2510.15869)
*Jie-Ying Lee,Yi-Ruei Liu,Shr-Ruei Tsai,Wei-Cheng Chang,Chung-Ho Wu,Jiewen Chan,Zhenjun Zhao,Chieh Hubert Lin,Yu-Lun Liu*

Main category: cs.CV

TL;DR: Skyfall-GS is a framework for creating large-scale 3D urban scenes by combining satellite imagery for geometry and diffusion models for textures, enabling real-time exploration without costly 3D annotations.


<details>
  <summary>Details</summary>
Motivation: There is a lack of large-scale, high-quality real-world 3D scans for training generalizable generative models for urban scene synthesis.

Method: Uses satellite imagery for coarse geometry and open-domain diffusion models for close-up appearances, with a curriculum-driven iterative refinement strategy to progressively enhance geometry and textures.

Result: Provides improved cross-view consistent geometry and more realistic textures compared to state-of-the-art approaches.

Conclusion: Skyfall-GS enables creation of city-block scale 3D scenes with real-time immersive exploration capabilities without requiring expensive 3D annotations.

Abstract: Synthesizing large-scale, explorable, and geometrically accurate 3D urban
scenes is a challenging yet valuable task in providing immersive and embodied
applications. The challenges lie in the lack of large-scale and high-quality
real-world 3D scans for training generalizable generative models. In this
paper, we take an alternative route to create large-scale 3D scenes by
synergizing the readily available satellite imagery that supplies realistic
coarse geometry and the open-domain diffusion model for creating high-quality
close-up appearances. We propose \textbf{Skyfall-GS}, the first city-block
scale 3D scene creation framework without costly 3D annotations, also featuring
real-time, immersive 3D exploration. We tailor a curriculum-driven iterative
refinement strategy to progressively enhance geometric completeness and
photorealistic textures. Extensive experiments demonstrate that Skyfall-GS
provides improved cross-view consistent geometry and more realistic textures
compared to state-of-the-art approaches. Project page:
https://skyfall-gs.jayinnn.dev/

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [141] [Fix False Transparency by Noise Guided Splatting](https://arxiv.org/abs/2510.15736)
*Aly El Hakie,Yiren Lu,Yu Yin,Michael Jenkins,Yehe Liu*

Main category: cs.GR

TL;DR: 3DGS reconstructions often show falsely transparent surfaces due to ill-posed optimization. NGS addresses this by injecting opaque noise Gaussians during training to encourage higher opacity in surface Gaussians.


<details>
  <summary>Details</summary>
Motivation: Opaque objects reconstructed by 3DGS exhibit falsely transparent surfaces that cause inconsistent backgrounds and internal patterns under camera motion, which stems from the lack of explicit opacity constraints during optimization.

Method: NGS injects opaque noise Gaussians in the object volume during training to encourage surface Gaussians to adopt higher opacity, requiring minimal modifications to existing splatting process. Also proposes transmittance-based metric for quantitative evaluation.

Result: Experiments across multiple datasets show NGS substantially reduces false transparency while maintaining competitive performance on standard rendering metrics.

Conclusion: NGS effectively addresses the underreported false transparency artifact in 3DGS through a simple yet effective noise injection strategy that improves surface opacity without compromising rendering quality.

Abstract: Opaque objects reconstructed by 3DGS often exhibit a falsely transparent
surface, leading to inconsistent background and internal patterns under camera
motion in interactive viewing. This issue stems from the ill-posed optimization
in 3DGS. During training, background and foreground Gaussians are blended via
alpha-compositing and optimized solely against the input RGB images using a
photometric loss. As this process lacks an explicit constraint on surface
opacity, the optimization may incorrectly assign transparency to opaque
regions, resulting in view-inconsistent and falsely transparent. This issue is
difficult to detect in standard evaluation settings but becomes particularly
evident in object-centric reconstructions under interactive viewing. Although
other causes of view-inconsistency have been explored recently, false
transparency has not been explicitly identified. To the best of our knowledge,
we are the first to identify, characterize, and develop solutions for this
artifact, an underreported artifact in 3DGS. Our strategy, NGS, encourages
surface Gaussians to adopt higher opacity by injecting opaque noise Gaussians
in the object volume during training, requiring only minimal modifications to
the existing splatting process. To quantitatively evaluate false transparency
in static renderings, we propose a transmittance-based metric that measures the
severity of this artifact. In addition, we introduce a customized, high-quality
object-centric scan dataset exhibiting pronounced transparency issues, and we
augment popular existing datasets with complementary infill noise specifically
designed to assess the robustness of 3D reconstruction methods to false
transparency. Experiments across multiple datasets show that NGS substantially
reduces false transparency while maintaining competitive performance on
standard rendering metrics, demonstrating its overall effectiveness.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [142] [BeLLMan: Controlling LLM Congestion](https://arxiv.org/abs/2510.15330)
*Tella Rajashekhar Reddy,Atharva Deshmukh,Karan Tandon,Rohan Gandhi,Anjaly Parayil,Debopam Bhattacherjee*

Main category: cs.DC

TL;DR: beLLMan is a controller that enables LLM infrastructure to signal applications to adjust output length based on system load, reducing latency and energy consumption.


<details>
  <summary>Details</summary>
Motivation: LLM applications generate tokens without considering system load, risking high latency and poor user experience during congestion.

Method: beLLMan actively signals first-party LLM applications to progressively adjust output length in response to changing system load conditions.

Result: On H100 GPU testbed: 8X lower end-to-end latency, 25% energy reduction while serving 19% more requests during congestion for summarization workload.

Conclusion: beLLMan effectively manages LLM inference by dynamically controlling output length, improving latency and energy efficiency during system congestion.

Abstract: Large language model (LLM) applications are blindfolded to the infrastructure
underneath and generate tokens autoregressively, indifferent to the system
load, thus risking inferencing latency inflation and poor user experience. Our
first-cut controller, named beLLMan, enables the LLM infrastructure to actively
and progressively signal the first-party LLM application to adjust the output
length in response to changing system load. On a real testbed with H100 GPUs,
beLLMan helps keep inferencing latency under control (upto 8X lower end-to-end
latency) and reduces energy consumption by 25% (while serving 19% more
requests) during periods of congestion for a summarization workload.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [143] [The Coverage Principle: How Pre-training Enables Post-Training](https://arxiv.org/abs/2510.15020)
*Fan Chen,Audrey Huang,Noah Golowich,Sadhika Malladi,Adam Block,Jordan T. Ash,Akshay Krishnamurthy,Dylan J. Foster*

Main category: stat.ML

TL;DR: The paper introduces 'coverage' as a better predictor of downstream performance than cross-entropy loss, explaining why pre-training succeeds and providing practical methods to improve coverage.


<details>
  <summary>Details</summary>
Motivation: Current understanding of why pre-training works is limited, and cross-entropy loss is a poor predictor of downstream task performance, motivating the search for better theoretical explanations and practical metrics.

Method: Theoretical analysis of coverage principle, development of coverage as a metric, and practical interventions including model selection, gradient normalization, and test-time decoding strategies with provable benefits.

Result: Coverage generalizes faster than cross-entropy and avoids spurious dependencies on problem parameters. The coverage principle explains why next-token prediction implicitly optimizes for good coverage.

Conclusion: Coverage is a necessary and sufficient condition for post-training success, providing better theoretical understanding and practical tools for improving language model performance.

Abstract: Language models demonstrate remarkable abilities when pre-trained on large
text corpora and fine-tuned for specific tasks, but how and why pre-training
shapes the success of the final model remains poorly understood. Notably,
although pre-training success is often quantified by cross entropy loss,
cross-entropy can be a poor predictor of downstream performance. Instead, we
provide a theoretical perspective on this relationship through the lens of
\emph{coverage}, which quantifies the probability mass the pre-trained model
places on high-quality responses and which is necessary and sufficient for
post-training and test-time scaling methods such as Best-of-N to succeed. Our
main results develop an understanding of \emph{the coverage principle}, a
phenomenon whereby next-token prediction implicitly optimizes toward a model
with good coverage. In particular, we uncover a mechanism that explains the
power of coverage in predicting downstream performance: \emph{coverage
generalizes faster than cross entropy}, avoiding spurious dependence on
problem-dependent parameters such as the sequence length. We also study
practical algorithmic interventions with provable benefits for improving
coverage, including (i) model/checkpoint selection procedures, (ii) gradient
normalization schemes, and (iii) test-time decoding strategies.

</details>


### [144] [RankSEG-RMA: An Efficient Segmentation Algorithm via Reciprocal Moment Approximation](https://arxiv.org/abs/2510.15362)
*Zixun Wang,Ben Dai*

Main category: stat.ML

TL;DR: RankSEG-RMA improves RankSEG by reducing computational complexity from O(d log d) or O(d²) to O(d) while maintaining performance, and extends applicability to non-overlapping segmentation settings.


<details>
  <summary>Details</summary>
Motivation: Existing segmentation methods that use argmax or thresholding don't directly optimize segmentation metrics like IoU and Dice, leading to suboptimal results. RankSEG addresses this but has high computational cost and only works for overlapping segmentation.

Method: Proposed reciprocal moment approximation (RMA) of RankSEG, called RankSEG-RMA, which reduces complexity and enables efficient implementation for non-overlapping segmentation through a pixel-wise score function.

Result: RankSEG-RMA reduces prediction time significantly (from 16.33 seconds to much faster) while maintaining comparable performance to original RankSEG.

Conclusion: The proposed RMA approach successfully overcomes RankSEG's computational limitations and extends its applicability to standard non-overlapping segmentation benchmarks.

Abstract: Semantic segmentation labels each pixel in an image with its corresponding
class, and is typically evaluated using the Intersection over Union (IoU) and
Dice metrics to quantify the overlap between predicted and ground-truth
segmentation masks. In the literature, most existing methods estimate
pixel-wise class probabilities, then apply argmax or thresholding to obtain the
final prediction. These methods have been shown to generally lead to
inconsistent or suboptimal results, as they do not directly maximize
segmentation metrics. To address this issue, a novel consistent segmentation
framework, RankSEG, has been proposed, which includes RankDice and RankIoU
specifically designed to optimize the Dice and IoU metrics, respectively.
Although RankSEG almost guarantees improved performance, it suffers from two
major drawbacks. First, it is its computational expense-RankDice has a
complexity of O(d log d) with a substantial constant factor (where d represents
the number of pixels), while RankIoU exhibits even higher complexity O(d^2),
thus limiting its practical application. For instance, in LiTS, prediction with
RankSEG takes 16.33 seconds compared to just 0.01 seconds with the argmax rule.
Second, RankSEG is only applicable to overlapping segmentation settings, where
multiple classes can occupy the same pixel, which contrasts with standard
benchmarks that typically assume non-overlapping segmentation. In this paper,
we overcome these two drawbacks via a reciprocal moment approximation (RMA) of
RankSEG with the following contributions: (i) we improve RankSEG using RMA,
namely RankSEG-RMA, reduces the complexity of both algorithms to O(d) while
maintaining comparable performance; (ii) inspired by RMA, we develop a
pixel-wise score function that allows efficient implementation for
non-overlapping segmentation settings.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [145] [Confidence-Weighted Semi-Supervised Learning for Skin Lesion Segmentation Using Hybrid CNN-Transformer Networks](https://arxiv.org/abs/2510.15354)
*Saqib Qamar*

Main category: eess.IV

TL;DR: MIRA-U is a semi-supervised framework for skin lesion segmentation that combines uncertainty-aware teacher-student pseudo-labeling with CNN-Transformer architecture, achieving state-of-the-art performance with limited labeled data.


<details>
  <summary>Details</summary>
Motivation: Automated skin lesion segmentation is crucial for early skin cancer detection but faces challenges due to limited annotated training data, requiring effective semi-supervised approaches.

Method: Uses teacher network pre-trained via masked image modeling to generate confidence-weighted soft pseudo-labels, guiding a U-shaped CNN-Transformer student network with cross-attention skip connections.

Result: Achieves superior performance on ISIC-2016 and PH2 datasets with Dice Similarity Coefficient of 0.9153 and Intersection over Union of 0.8552 using only 50% labeled data.

Conclusion: The proposed framework effectively addresses limited annotation challenges in skin lesion segmentation through uncertainty-aware pseudo-labeling and hybrid architecture, outperforming existing methods.

Abstract: Automated skin lesion segmentation through dermoscopic analysis is essential
for early skin cancer detection, yet remains challenging due to limited
annotated training data. We present MIRA-U, a semi-supervised framework that
combines uncertainty-aware teacher-student pseudo-labeling with a hybrid
CNN-Transformer architecture. Our approach employs a teacher network
pre-trained via masked image modeling to generate confidence-weighted soft
pseudo-labels, which guide a U-shaped CNN-Transformer student network featuring
cross-attention skip connections. This design enhances pseudo-label quality and
boundary delineation, surpassing reconstruction-based and CNN-only baselines,
particularly in low-annotation regimes. Extensive evaluation on ISIC-2016 and
PH2 datasets demonstrates superior performance, achieving a Dice Similarity
Coefficient (DSC) of 0.9153 and Intersection over Union (IoU) of 0.8552 using
only 50% labeled data. Code is publicly available on GitHub.

</details>


### [146] [SANR: Scene-Aware Neural Representation for Light Field Image Compression with Rate-Distortion Optimization](https://arxiv.org/abs/2510.15775)
*Gai Zhang,Xinfeng Zhang,Lv Tang,Hongyu An,Li Zhang,Qingming Huang*

Main category: eess.IV

TL;DR: SANR is a scene-aware neural representation framework for light field image compression that introduces hierarchical scene modeling and end-to-end rate-distortion optimization, achieving 65.62% BD-rate saving against HEVC.


<details>
  <summary>Details</summary>
Motivation: Light field images have enormous data volumes due to their high-dimensional nature, creating compression challenges. Existing neural representation methods often neglect explicit scene structure modeling and lack end-to-end rate-distortion optimization.

Method: SANR uses hierarchical scene modeling with multi-scale latent codes to capture intrinsic scene structures, and incorporates entropy-constrained quantization-aware training for end-to-end rate-distortion optimization.

Result: SANR significantly outperforms state-of-the-art techniques with 65.62% BD-rate saving against HEVC, demonstrating superior rate-distortion performance.

Conclusion: The proposed scene-aware neural representation framework with end-to-end optimization effectively addresses compression challenges in light field images by modeling scene structures and optimizing rate-distortion trade-offs.

Abstract: Light field images capture multi-view scene information and play a crucial
role in 3D scene reconstruction. However, their high-dimensional nature results
in enormous data volumes, posing a significant challenge for efficient
compression in practical storage and transmission scenarios. Although neural
representation-based methods have shown promise in light field image
compression, most approaches rely on direct coordinate-to-pixel mapping through
implicit neural representation (INR), often neglecting the explicit modeling of
scene structure. Moreover, they typically lack end-to-end rate-distortion
optimization, limiting their compression efficiency. To address these
limitations, we propose SANR, a Scene-Aware Neural Representation framework for
light field image compression with end-to-end rate-distortion optimization. For
scene awareness, SANR introduces a hierarchical scene modeling block that
leverages multi-scale latent codes to capture intrinsic scene structures,
thereby reducing the information gap between INR input coordinates and the
target light field image. From a compression perspective, SANR is the first to
incorporate entropy-constrained quantization-aware training (QAT) into neural
representation-based light field image compression, enabling end-to-end
rate-distortion optimization. Extensive experiment results demonstrate that
SANR significantly outperforms state-of-the-art techniques regarding
rate-distortion performance with a 65.62\% BD-rate saving against HEVC.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [147] [Internalizing World Models via Self-Play Finetuning for Agentic RL](https://arxiv.org/abs/2510.15047)
*Shiqi Chen,Tongyao Zhu,Zian Wang,Jinghan Zhang,Kangrui Wang,Siyang Gao,Teng Xiao,Yee Whye Teh,Junxian He,Manling Li*

Main category: cs.LG

TL;DR: SPA is a reinforcement learning framework that equips LLM agents with an internal world model through self-play supervised finetuning, significantly improving performance in out-of-distribution scenarios.


<details>
  <summary>Details</summary>
Motivation: LLM agents struggle in out-of-distribution scenarios where real-world environments have complex dynamics and stochasticity, making it difficult to ground internal knowledge in environmental dynamics.

Method: Decompose world model into state representation and transition modeling, then use Self-Play supervised finetuning to learn the world model by interacting with the environment before policy optimization.

Result: SPA boosts Sokoban success rate from 25.6% to 59.8% and FrozenLake score from 22.1% to 70.9% for Qwen2.5-1.5B-Instruct model, significantly outperforming baseline methods.

Conclusion: Equipping LLM agents with internal world models through self-play initialization greatly improves decision-making and generalization in complex, dynamic environments.

Abstract: Large Language Models (LLMs) as agents often struggle in out-of-distribution
(OOD) scenarios. Real-world environments are complex and dynamic, governed by
task-specific rules and stochasticity, which makes it difficult for LLMs to
ground their internal knowledge in those dynamics. Under such OOD conditions,
vanilla RL training often fails to scale; we observe Pass@k--the probability
that at least one of (k) sampled trajectories succeeds--drops markedly across
training steps, indicating brittle exploration and limited generalization.
Inspired by model-based reinforcement learning, we hypothesize that equipping
LLM agents with an internal world model can better align reasoning with
environmental dynamics and improve decision-making. We show how to encode this
world model by decomposing it into two components: state representation and
transition modeling. Building on this, we introduce SPA, a simple reinforcement
learning framework that cold-starts the policy via a Self-Play supervised
finetuning (SFT) stage to learn the world model by interacting with the
environment, then uses it to simulate future states prior to policy
optimization. This simple initialization outperforms the online world-modeling
baseline and greatly boosts the RL-based agent training performance.
Experiments across diverse environments like Sokoban, FrozenLake, and Sudoku
show that our approach significantly improves performance. For example, SPA
boosts the Sokoban success rate from 25.6% to 59.8% and raises the FrozenLake
score from 22.1% to 70.9% for the Qwen2.5-1.5B-Instruct model.

</details>


### [148] [Antislop: A Comprehensive Framework for Identifying and Eliminating Repetitive Patterns in Language Models](https://arxiv.org/abs/2510.15061)
*Samuel Paech,Allen Roush,Judah Goldfeder,Ravid Shwartz-Ziv*

Main category: cs.LG

TL;DR: Antislop is a framework that detects and eliminates repetitive phraseology ("slop") in LLM outputs using three innovations: a backtracking sampler, automated slop profiling, and token-level fine-tuning (FTPO).


<details>
  <summary>Details</summary>
Motivation: Widespread LLM adoption has introduced characteristic repetitive phraseology that degrades output quality and makes AI-generated text immediately recognizable.

Method: Combines three innovations: (1) Antislop Sampler using backtracking to suppress unwanted strings, (2) automated pipeline profiling model-specific slop against human baselines, (3) Final Token Preference Optimization (FTPO) - a novel fine-tuning method operating on individual tokens.

Result: Some slop patterns appear 1000× more frequently in LLM output than human text. Antislop Sampler suppresses 8000+ patterns while maintaining quality, FTPO achieves 90% slop reduction while maintaining or improving performance on cross-domain evaluations.

Conclusion: Antislop effectively reduces repetitive phraseology in LLM outputs while preserving quality, outperforming methods like DPO which suffer degradation in writing quality.

Abstract: Widespread LLM adoption has introduced characteristic repetitive phraseology,
termed ``slop,'' which degrades output quality and makes AI-generated text
immediately recognizable. We present Antislop, a comprehensive framework
providing tools to both detect and eliminate these overused patterns. Our
approach combines three innovations: (1) The Antislop Sampler, which uses
backtracking to suppress unwanted strings at inference time without destroying
vocabulary; (2) An automated pipeline that profiles model-specific slop against
human baselines and generates training data; (3) Final Token Preference
Optimization (FTPO), a novel fine-tuning method that operates on individual
tokens, surgically adjusting logits wherever a banned pattern has appeared in
an inference trace. We demonstrate that some slop patterns appear over
1,000$\times$ more frequently in LLM output than human text. The Antislop
Sampler successfully suppresses 8,000+ patterns while maintaining quality,
whereas token banning becomes unusable at just 2,000. Most importantly, FTPO
achieves 90\% slop reduction while maintaining or improving performance in
cross-domain evals including GSM8K, MMLU, and creative writing tasks. In
contrast, DPO suffers significant degradation in writing quality and lexical
diversity despite achieving weaker suppression. We release all code and results
under MIT license: https://github.com/sam-paech/auto-antislop.

</details>


### [149] [DLER: Doing Length pEnalty Right - Incentivizing More Intelligence per Token via Reinforcement Learning](https://arxiv.org/abs/2510.15110)
*Shih-Yang Liu,Xin Dong,Ximing Lu,Shizhe Diao,Mingjie Liu,Min-Hung Chen,Hongxu Yin,Yu-Chiang Frank Wang,Kwang-Ting Cheng,Yejin Choi,Jan Kautz,Pavlo Molchanov*

Main category: cs.LG

TL;DR: DLER is a reinforcement learning training method that optimizes reasoning language models to generate concise outputs while maintaining high accuracy, achieving 70% shorter responses with better performance than previous methods.


<details>
  <summary>Details</summary>
Motivation: Current reasoning language models generate unnecessarily long outputs, and maximizing intelligence per token (accuracy relative to response length) remains an open problem that needs to be addressed.

Method: DLER combines batch-wise reward normalization, higher clipping, dynamic sampling, and simple truncation length penalty to address RL optimization challenges including bias in advantage estimation, entropy collapse, and sparse reward signals.

Result: DLER cuts output length by over 70% while surpassing all previous baseline accuracy, and DLER-7B generates multiple concise responses with 28% higher accuracy and lower latency compared to DeepSeek-R1-7B.

Conclusion: DLER achieves state-of-the-art accuracy-efficiency trade-offs and introduces adaptive methods like Difficulty-Aware DLER and update-selective merging for additional efficiency gains while preserving baseline accuracy.

Abstract: Reasoning language models such as OpenAI-o1, DeepSeek-R1, and Qwen achieve
strong performance via extended chains of thought but often generate
unnecessarily long outputs. Maximizing intelligence per token--accuracy
relative to response length--remains an open problem. We revisit reinforcement
learning (RL) with the simplest length penalty--truncation--and show that
accuracy degradation arises not from the lack of sophisticated penalties but
from inadequate RL optimization. We identify three key challenges: (i) large
bias in advantage estimation, (ii) entropy collapse, and (iii) sparse reward
signal. We address them with Doing Length pEnalty Right (DLER), a training
recipe combining batch-wise reward normalization, higher clipping, dynamic
sampling, and a simple truncation length penalty. DLER achieves
state-of-the-art accuracy--efficiency trade-offs, cutting output length by over
70 percent while surpassing all previous baseline accuracy. It also improves
test-time scaling: compared to DeepSeek-R1-7B, DLER-7B generates multiple
concise responses in parallel with 28 percent higher accuracy and lower
latency. We further introduce Difficulty-Aware DLER, which adaptively tightens
truncation on easier questions for additional efficiency gains. We also propose
an update-selective merging method that preserves baseline accuracy while
retaining the concise reasoning ability of the DLER model, which is useful for
scenarios where RL training data is scarce.

</details>


### [150] [Soundness-Aware Level: A Microscopic Signature that Predicts LLM Reasoning Potential](https://arxiv.org/abs/2510.15216)
*Xuansheng Wu,Xiaoman Pan,Wenlin Yao,Jianshu Chen*

Main category: cs.LG

TL;DR: The paper identifies that a model's reasoning potential after reinforcement learning with verifiable rewards (RLVR) depends on its pre-trained ability to distinguish sound from unsound knowledge, quantified by the Soundness-Aware Level (SAL) metric.


<details>
  <summary>Details</summary>
Motivation: To understand why RLVR performance varies dramatically across different base LLMs and identify the microscopic property that causes this variation.

Method: Formalize reasoning as chains of Horn clauses, estimate transition probabilities between features using cross-layer sparse autoencoders, categorize rules by semantic soundness levels, and introduce SAL metric using Jensen-Shannon Divergence.

Result: High-potential models are soundness-aware (distinct probability distributions for different soundness levels), while weaker models are soundness-agnostic. SAL predicts post-RLVR reasoning performance with high accuracy (R^2=0.87) across diverse models and scales.

Conclusion: Reasoning potential is tied to intrinsic pre-trained ability to distinguish sound from unsound knowledge, highlighting the critical role of pre-training and providing a practical metric for model selection/design.

Abstract: Reinforcement learning with verifiable rewards (RLVR) can elicit strong
reasoning in large language models (LLMs), while their performance after RLVR
varies dramatically across different base models. This raises a fundamental
question: what microscopic property of pre-trained models leads to this
variation? To investigate, we formalize reasoning as chains of Horn clauses
("if-then" rules) built from features extracted from the LLM's latent space via
cross-layer sparse autoencoders (SAEs). We estimate the transition
probabilities between its features, and further categorize each rule by its
semantic soundness level (e.g., strict, plausible, noisy) with an LLM. Our key
discovery is that high-potential models are inherently soundness-aware: their
internal probability distributions systematically shift across rules' soundness
levels, becoming highly distinct for "strict" versus "noisy" rules. In
contrast, weaker models are soundness-agnostic, collapsing to one distribution
regardless of soundness levels. To quantify this, we introduce the
Soundness-Aware Level (SAL), a microscopic metric using the Jensen-Shannon
Divergence to measure the separation between these distributions. We show that
SAL's predictions of post-RLVR reasoning performance follow a precise empirical
law (R^2=0.87) across diverse model families (Qwen, Mistral, Llama, DeepSeek)
and scales (0.5B-14B). This reveals that a model's reasoning potential is tied
to its intrinsic, pre-trained ability to distinguish sound knowledge from
unsound ones. These findings underscore the critical role of model pre-training
in shaping reasoning and offer a practical metric grounded in the model's
internal mechanisms for selecting/designing stronger base models.

</details>


### [151] [FinTrust: A Comprehensive Benchmark of Trustworthiness Evaluation in Finance Domain](https://arxiv.org/abs/2510.15232)
*Tiansheng Hu,Tongyan Hu,Liuyang Bai,Yilun Zhao,Arman Cohan,Chen Zhao*

Main category: cs.LG

TL;DR: FinTrust is a comprehensive benchmark for evaluating LLM trustworthiness in finance, testing 11 models across various alignment dimensions and finding proprietary models generally outperform but all struggle with legal awareness tasks.


<details>
  <summary>Details</summary>
Motivation: Applying LLMs in real-world finance is challenging due to high risks and stakes, requiring comprehensive trustworthiness evaluation beyond general capabilities.

Method: Developed FinTrust benchmark with fine-grained tasks across multiple trustworthiness dimensions based on practical financial contexts, evaluated 11 LLMs including proprietary and open-source models.

Result: Proprietary models like o4-mini outperform in most tasks (safety), open-source models like DeepSeek-V3 excel in specific areas (industry-level fairness), but all models struggle with challenging tasks like fiduciary alignment and disclosure, showing legal awareness gaps.

Conclusion: FinTrust provides valuable framework for evaluating LLM trustworthiness in finance, revealing current limitations in legal compliance and highlighting the need for improved alignment in high-stakes financial applications.

Abstract: Recent LLMs have demonstrated promising ability in solving finance related
problems. However, applying LLMs in real-world finance application remains
challenging due to its high risk and high stakes property. This paper
introduces FinTrust, a comprehensive benchmark specifically designed for
evaluating the trustworthiness of LLMs in finance applications. Our benchmark
focuses on a wide range of alignment issues based on practical context and
features fine-grained tasks for each dimension of trustworthiness evaluation.
We assess eleven LLMs on FinTrust and find that proprietary models like o4-mini
outperforms in most tasks such as safety while open-source models like
DeepSeek-V3 have advantage in specific areas like industry-level fairness. For
challenging task like fiduciary alignment and disclosure, all LLMs fall short,
showing a significant gap in legal awareness. We believe that FinTrust can be a
valuable benchmark for LLMs' trustworthiness evaluation in finance domain.

</details>


### [152] [DRO-InstructZero: Distributionally Robust Prompt Optimization for Large Language Models](https://arxiv.org/abs/2510.15260)
*Yangyang Li*

Main category: cs.LG

TL;DR: DRO-InstructZero formulates zero-shot prompt optimization as robust Bayesian optimization using f-divergence balls to handle distribution shifts, achieving significant performance gains on tasks like formality rewriting and code debugging while maintaining in-distribution performance.


<details>
  <summary>Details</summary>
Motivation: Existing prompt search methods like InstructZero degrade under distribution shift and adversarial evaluation because they optimize for expected performance under a single evaluation distribution, leading to poor transferability.

Method: Formulates prompt optimization as robust Bayesian optimization with f-divergence balls defining ambiguity sets around evaluation distributions. Uses robust acquisition rule to maximize worst-case expected utility while maintaining query efficiency.

Result: Achieves 25-30 point absolute gains on BIG-Bench informative-to-formal rewriting (61.3% to ~85-90%), ~25-point gains on auto-debugging under domain shift, while maintaining >96% on stable tasks like cause-and-effect. Improvements are consistent across divergence choices and decoding temperatures.

Conclusion: DRO-InstructZero connects distributionally robust optimization with prompt learning, offering a plug-and-play approach for reliable, transferable prompt alignment under real-world uncertainty.

Abstract: Large language models are highly sensitive to prompt wording. However,
popular automatic prompt search methods, including InstructZero, often degrade
under distribution shift and adversarial evaluation because they optimize
expected performance under a single evaluation distribution. Consequently,
prompts that work in one setting frequently fail to transfer. To address this,
DRO-InstructZero formulates zero-shot prompt optimization as robust Bayesian
optimization. Specifically, an f-divergence ball defines an ambiguity set
around the evaluation distribution, and a robust acquisition rule maximizes
worst-case expected utility while retaining the query efficiency of Bayesian
search. Therefore, the search explicitly targets reliability under distribution
shift rather than average behavior alone. Experiments follow the
instruction-induction protocol with matched query budgets across formality
rewriting, code debugging, and translation. For example, on BIG-Bench
informative-to-formal rewriting, accuracy improves from 61.3 +/- 0.7% to
approximately 85-90%, yielding an absolute gain of about 25-30 points.
Moreover, auto-debugging shows about +25-point gains under domain shift.
Meanwhile, stable tasks such as cause-and-effect remain above 96%, indicating
no loss on in-distribution cases. Furthermore, improvements are consistent
across divergence choices and decoding temperatures. Overall, DRO-InstructZero
connects distributionally robust optimization with prompt learning, offering a
plug-and-play and general approach for reliable, transferable prompt alignment
under real-world uncertainty.

</details>


### [153] [Dissecting Mahalanobis: How Feature Geometry and Normalization Shape OOD Detection](https://arxiv.org/abs/2510.15202)
*Denis Janiak,Jakub Binkowski,Tomasz Kajdanowicz*

Main category: cs.LG

TL;DR: This paper analyzes Mahalanobis distance methods for OOD detection, showing they aren't universally reliable. It defines ideal representation geometry and uses spectral metrics to predict OOD performance. The authors propose radially scaled L2 normalization to improve OOD detection by controlling feature space geometry.


<details>
  <summary>Details</summary>
Motivation: The impact of representation geometry and normalization on Mahalanobis distance methods for OOD detection is not fully understood, limiting their downstream application despite widespread use.

Method: Conducted comprehensive empirical study across diverse image foundation models, datasets, and normalization schemes. Proposed radially scaled L2 normalization with tunable parameter to control radial geometry of feature space.

Result: Mahalanobis-based methods aren't universally reliable. Spectral and intrinsic-dimensionality metrics can accurately predict OOD performance. Radially scaled L2 normalization significantly improves OOD detection performance by systematically contracting/expanding representations.

Conclusion: The findings bridge the gap between representation geometry, normalization, and OOD performance, offering insights for designing more effective and reliable deep learning models.

Abstract: Out-of-distribution (OOD) detection is critical for the reliable deployment
of deep learning models. hile Mahalanobis distance methods are widely used, the
impact of representation geometry and normalization on their performance is not
fully understood, which may limit their downstream application. To address this
gap, we conducted a comprehensive empirical study across diverse image
foundation models, datasets, and distance normalization schemes. First, our
analysis shows that Mahalanobis-based methods aren't universally reliable.
Second, we define the ideal geometry for data representations and demonstrate
that spectral and intrinsic-dimensionality metrics can accurately predict a
model's OOD performance. Finally, we analyze how normalization impacts OOD
performance. Building upon these studies, we propose radially scaled $\ell_2$
normalization, a method that generalizes the standard $\ell_2$ normalization
recently applied to Mahalanobis-based OOD detection. Our approach introduces a
tunable parameter to directly control the radial geometry of the feature space,
systematically contracting or expanding representations to significantly
improve OOD detection performance. By bridging the gap between representation
geometry, normalization, and OOD performance, our findings offer new insights
into the design of more effective and reliable deep learning models.

</details>


### [154] [An Empirical Study on MC Dropout--Based Uncertainty--Error Correlation in 2D Brain Tumor Segmentation](https://arxiv.org/abs/2510.15541)
*Saumya B*

Main category: cs.LG

TL;DR: MC Dropout uncertainty shows weak correlation with segmentation errors in brain tumor MRI, especially near boundaries, suggesting limited utility for error localization.


<details>
  <summary>Details</summary>
Motivation: To evaluate whether MC Dropout-based uncertainty can effectively identify segmentation errors in brain tumor MRI, particularly near tumor boundaries where errors are critical.

Method: Used U-Net for 2D brain tumor MRI segmentation with four augmentation settings (none, horizontal flip, rotation, scaling), computed uncertainty from 50 stochastic forward passes, and correlated with pixel-wise errors using Pearson and Spearman coefficients.

Result: Found weak global correlations (r ≈ 0.30-0.38) and negligible boundary correlations (|r| < 0.05). Statistical differences across augmentations were significant but practically irrelevant.

Conclusion: MC Dropout uncertainty provides limited cues for boundary error localization, highlighting the need for alternative or hybrid uncertainty estimation methods in medical image segmentation.

Abstract: Accurate brain tumor segmentation from MRI is vital for diagnosis and
treatment planning. Although Monte Carlo (MC) Dropout is widely used to
estimate model uncertainty, its effectiveness in identifying segmentation
errors -- especially near tumor boundaries -- remains unclear. This study
empirically examines the relationship between MC Dropout--based uncertainty and
segmentation error in 2D brain tumor MRI segmentation using a U-Net trained
under four augmentation settings: none, horizontal flip, rotation, and scaling.
Uncertainty was computed from 50 stochastic forward passes and correlated with
pixel-wise errors using Pearson and Spearman coefficients. Results show weak
global correlations ($r \approx 0.30$--$0.38$) and negligible boundary
correlations ($|r| < 0.05$). Although differences across augmentations were
statistically significant ($p < 0.001$), they lacked practical relevance. These
findings suggest that MC Dropout uncertainty provides limited cues for boundary
error localization, underscoring the need for alternative or hybrid uncertainty
estimation methods in medical image segmentation.

</details>


### [155] [Poultry Farm Intelligence: An Integrated Multi-Sensor AI Platform for Enhanced Welfare and Productivity](https://arxiv.org/abs/2510.15757)
*Pieris Panagi,Savvas Karatsiolis,Kyriacos Mosphilis,Nicholas Hadjisavvas,Andreas Kamilaris,Nicolas Nicolaou,Efstathios Stavrakis,Vassilis Vassiliades*

Main category: cs.LG

TL;DR: PoultryFI is a modular AI platform that integrates six modules for continuous monitoring, prediction, and optimization in poultry farming, achieving 100% egg-count accuracy and reliable forecasting.


<details>
  <summary>Details</summary>
Motivation: Small and medium-sized poultry farms lack affordable, integrated tools for continuous monitoring and decision-making, relying on manual inspections instead.

Method: Uses evolutionary algorithms for camera placement optimization, synchronized audio-visual monitoring, edge analytics for real-time egg counting, forecasting models, and recommendation modules integrating weather data.

Result: Field trials show 100% egg-count accuracy on Raspberry Pi 5, robust anomaly detection, and reliable short-term forecasting up to 10 days in advance.

Conclusion: PoultryFI bridges the gap between isolated pilot tools and scalable farm-wide intelligence, enabling proactive welfare and profitability management.

Abstract: Poultry farming faces increasing pressure to meet productivity targets while
ensuring animal welfare and environmental compliance. Yet many small and
medium-sized farms lack affordable, integrated tools for continuous monitoring
and decision-making, relying instead on manual, reactive inspections. This
paper presents Poultry Farm Intelligence (PoultryFI) - a modular,
cost-effective platform that integrates six AI-powered modules: Camera
Placement Optimizer, Audio-Visual Monitoring, Analytics & Alerting, Real-Time
Egg Counting, Production & Profitability Forecasting, and a Recommendation
Module.
  Camera layouts are first optimized offline using evolutionary algorithms for
full poultry house coverage with minimal hardware. The Audio-Visual Monitoring
module extracts welfare indicators from synchronized video, audio, and feeding
data. Analytics & Alerting produces daily summaries and real-time
notifications, while Real-Time Egg Counting uses an edge vision model to
automate production tracking. Forecasting models predict egg yield and feed
consumption up to 10 days in advance, and the Recommendation Module integrates
forecasts with weather data to guide environmental and operational adjustments.
  This is among the first systems to combine low-cost sensing, edge analytics,
and prescriptive AI to continuously monitor flocks, predict production, and
optimize performance. Field trials demonstrate 100% egg-count accuracy on
Raspberry Pi 5, robust anomaly detection, and reliable short-term forecasting.
PoultryFI bridges the gap between isolated pilot tools and scalable, farm-wide
intelligence, empowering producers to proactively safeguard welfare and
profitability.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [156] [Neural Posterior Estimation for Cataloging Astronomical Images from the Legacy Survey of Space and Time](https://arxiv.org/abs/2510.15315)
*Yicun Duan,Xinyue Li,Camille Avestruz,Jeffrey Regier*

Main category: astro-ph.IM

TL;DR: Neural Posterior Estimation (NPE) outperforms traditional LSST pipeline in astronomical cataloging tasks using simulated LSST data, providing accurate posterior approximations.


<details>
  <summary>Details</summary>
Motivation: Traditional cataloging methods lack statistical coherence for ill-posed problems, while existing probabilistic approaches are inefficient, inaccurate, or incompatible with LSST's multiband coadded images.

Method: Uses Neural Posterior Estimation (NPE), a Bayesian inference method that leverages deep learning for computational efficiency and accuracy in cataloging astronomical sources.

Result: NPE systematically outperforms standard LSST pipeline in light source detection, flux measurement, star/galaxy classification, and galaxy shape measurement on DC2 Simulated Sky Survey data, with well-calibrated posterior approximations.

Conclusion: NPE shows promising potential for LSST cataloging despite inevitable model misspecification in real applications, with mitigation strategies available.

Abstract: The Vera C. Rubin Observatory Legacy Survey of Space and Time (LSST) will
commence full-scale operations in 2026, yielding an unprecedented volume of
astronomical images. Constructing an astronomical catalog, a table of imaged
stars, galaxies, and their properties, is a fundamental step in most scientific
workflows based on astronomical image data. Traditional deterministic
cataloging methods lack statistical coherence as cataloging is an ill-posed
problem, while existing probabilistic approaches suffer from computational
inefficiency, inaccuracy, or the inability to perform inference with multiband
coadded images, the primary output format for LSST images. In this article, we
explore a recently developed Bayesian inference method called neural posterior
estimation (NPE) as an approach to cataloging. NPE leverages deep learning to
achieve both computational efficiency and high accuracy. When evaluated on the
DC2 Simulated Sky Survey -- a highly realistic synthetic dataset designed to
mimic LSST data -- NPE systematically outperforms the standard LSST pipeline in
light source detection, flux measurement, star/galaxy classification, and
galaxy shape measurement. Additionally, NPE provides well-calibrated posterior
approximations. These promising results, obtained using simulated data,
illustrate the potential of NPE in the absence of model misspecification.
Although some degree of model misspecification is inevitable in the application
of NPE to real LSST images, there are a variety of strategies to mitigate its
effects.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [157] [Leveraging Test Driven Development with Large Language Models for Reliable and Verifiable Spreadsheet Code Generation: A Research Framework](https://arxiv.org/abs/2510.15585)
*Dr Simon Thorne,Dr Advait Sarkar*

Main category: cs.SE

TL;DR: This position paper proposes integrating Test-Driven Development (TDD) with LLM generation to improve correctness and reliability of generated code, particularly for high-stakes domains like financial modeling.


<details>
  <summary>Details</summary>
Motivation: LLMs frequently exhibit critical issues like hallucinations, logical inconsistencies, and syntactic errors, which are particularly risky in high-stakes domains where accuracy and reliability are paramount.

Method: A structured research framework that integrates TDD practices with LLM-driven generation, using a "test first" methodology to provide technical constraints and cognitive scaffolding for guiding LLM outputs.

Result: The framework includes explicit experimental design with participant groups, evaluation metrics, and TDD-based prompting examples applicable across diverse programming contexts from spreadsheets to Python and Rust.

Conclusion: The approach aims to improve computational thinking, prompt engineering skills, and user engagement, particularly benefiting spreadsheet users without formal programming training, and invites collaboration for empirical evaluation.

Abstract: Large Language Models (LLMs), such as ChatGPT, are increasingly leveraged for
generating both traditional software code and spreadsheet logic. Despite their
impressive generative capabilities, these models frequently exhibit critical
issues such as hallucinations, subtle logical inconsistencies, and syntactic
errors, risks particularly acute in high stakes domains like financial
modelling and scientific computations, where accuracy and reliability are
paramount. This position paper proposes a structured research framework that
integrates the proven software engineering practice of Test-Driven Development
(TDD) with Large Language Model (LLM) driven generation to enhance the
correctness of, reliability of, and user confidence in generated outputs. We
hypothesise that a "test first" methodology provides both technical constraints
and cognitive scaffolding, guiding LLM outputs towards more accurate,
verifiable, and comprehensible solutions. Our framework, applicable across
diverse programming contexts, from spreadsheet formula generation to scripting
languages such as Python and strongly typed languages like Rust, includes an
explicitly outlined experimental design with clearly defined participant
groups, evaluation metrics, and illustrative TDD based prompting examples. By
emphasising test driven thinking, we aim to improve computational thinking,
prompt engineering skills, and user engagement, particularly benefiting
spreadsheet users who often lack formal programming training yet face serious
consequences from logical errors. We invite collaboration to refine and
empirically evaluate this approach, ultimately aiming to establish responsible
and reliable LLM integration in both educational and professional development
practices.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [158] [VO-DP: Semantic-Geometric Adaptive Diffusion Policy for Vision-Only Robotic Manipulation](https://arxiv.org/abs/2510.15530)
*Zehao Ni,Yonghao He,Lingfeng Qian,Jilei Mao,Fa Fu,Wei Sui,Hu Su,Junran Peng,Zhipeng Wang,Bin He*

Main category: cs.RO

TL;DR: VO-DP is a vision-only diffusion policy learning method that uses pretrained visual foundation models to fuse semantic and geometric features, achieving performance comparable to point cloud-based methods in simulation and superior performance in real-world tasks.


<details>
  <summary>Details</summary>
Motivation: Existing visuomotor diffusion policy approaches heavily rely on point clouds, but there's a lack of exploration of vision-only solutions which have significant potential for robotic manipulation.

Method: Uses pretrained visual foundation models (VGGT, DINOv2, Alternating Attention blocks) to fuse semantic and geometric features via cross-attention, then spatially compresses with CNN for policy input.

Result: In simulation: 64.6% success rate (vs DP3 64.0%, DP 34.8%); In real-world: 87.9% success rate (vs DP3 67.5%, DP 11.2%). Highly robust under varying conditions.

Conclusion: VO-DP demonstrates that vision-only approaches can achieve state-of-the-art performance in robotic manipulation, outperforming both vision-only and point cloud baselines, especially in real-world scenarios.

Abstract: In the context of imitation learning, visuomotor-based diffusion policy
learning is one of the main directions in robotic manipulation. Most of these
approaches rely on point clouds as observation inputs and construct scene
representations through point clouds feature learning, which enables them to
achieve remarkable accuracy. However, the existing literature lacks an in-depth
exploration of vision-only solutions that have significant potential. In this
paper, we propose a Vision-Only and single-view Diffusion Policy learning
method (VO-DP) that leverages pretrained visual foundation models to achieve
effective fusion of semantic and geometric features. We utilize intermediate
features from VGGT incorporating semantic features from DINOv2 and geometric
features from Alternating Attention blocks. Features are fused via
cross-attention and spatially compressed with a CNN to form the input to the
policy head. Extensive experiments demonstrate that VO-DP not only outperforms
the vision-only baseline DP significantly but also exhibits distinct
performance trends against the point cloud-based method DP3: in simulation
tasks, VO-DP achieves an average success rate of 64.6% on par with DP3 64.0%
and far higher than DP 34.8%, while in real-world tasks, it reaches 87.9%,
outperforming both DP3 67.5% and DP 11.2% by a notable margin. Further
robustness evaluations confirm that VO-DP remains highly stable under varying
conditions including color, size, background, and lighting. Lastly, we
open-source a training library for robotic manipulation. Built on Accelerate,
this library supports multi-machine and multi-GPU parallel training, as well as
mixed precision training. It is compatible with visuomotor policies such as DP,
DP3 and VO-DP, and also supports the RoboTwin simulator.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [159] [SQuAI: Scientific Question-Answering with Multi-Agent Retrieval-Augmented Generation](https://arxiv.org/abs/2510.15682)
*Ines Besrour,Jingbo He,Tobias Schreieder,Michael Färber*

Main category: cs.IR

TL;DR: SQuAI is a scalable multi-agent RAG framework for scientific QA that improves faithfulness and relevance by decomposing questions, retrieving evidence from 2.3M papers, and providing verifiable citations.


<details>
  <summary>Details</summary>
Motivation: Address limitations of existing RAG systems in scholarly domain where complex questions require accurate answers with citations and retrieval across millions of scientific documents.

Method: Uses four collaborative agents to decompose questions, employ hybrid sparse-dense retrieval, adaptively filter documents, and integrate in-line citations with supporting sentences from source documents.

Result: Improves faithfulness, answer relevance, and contextual relevance by up to +0.088 (12%) over strong RAG baseline. Releases benchmark of 1,000 scientific QA triplets.

Conclusion: Multi-agent RAG enables more trustworthy scientific QA with transparent reasoning, verifiable citations, and domain-wide scalability.

Abstract: We present SQuAI (https://squai.scads.ai/), a scalable and trustworthy
multi-agent retrieval-augmented generation (RAG) framework for scientific
question answering (QA) with large language models (LLMs). SQuAI addresses key
limitations of existing RAG systems in the scholarly domain, where complex,
open-domain questions demand accurate answers, explicit claims with citations,
and retrieval across millions of scientific documents. Built on over 2.3
million full-text papers from arXiv.org, SQuAI employs four collaborative
agents to decompose complex questions into sub-questions, retrieve targeted
evidence via hybrid sparse-dense retrieval, and adaptively filter documents to
improve contextual relevance. To ensure faithfulness and traceability, SQuAI
integrates in-line citations for each generated claim and provides supporting
sentences from the source documents. Our system improves faithfulness, answer
relevance, and contextual relevance by up to +0.088 (12%) over a strong RAG
baseline. We further release a benchmark of 1,000 scientific
question-answer-evidence triplets to support reproducibility. With transparent
reasoning, verifiable citations, and domain-wide scalability, SQuAI
demonstrates how multi-agent RAG enables more trustworthy scientific QA with
LLMs.

</details>


### [160] [GraphMind: Interactive Novelty Assessment System for Accelerating Scientific Discovery](https://arxiv.org/abs/2510.15706)
*Italo Luis da Silva,Hanqi Yan,Lin Gui,Yulan He*

Main category: cs.IR

TL;DR: GraphMind is an interactive web tool that helps users evaluate the novelty of scientific papers by providing structured analysis, related work exploration, and verifiable contextual insights through LLM integration.


<details>
  <summary>Details</summary>
Motivation: Current LLM-assisted literature analysis tools lack transparency and traceability mechanisms, making it difficult for reviewers to assess novelty without extensive knowledge of related work.

Method: GraphMind integrates external APIs (arXiv, Semantic Scholar) with LLMs to support annotation, extraction, retrieval and classification of papers, enabling users to capture paper structure and explore related ideas through various relationships.

Result: The tool provides a rich, structured view of scientific ideas' core contributions and connections to existing work, with verifiable contextual insights for novelty assessment.

Conclusion: GraphMind addresses the gap in transparent, traceable novelty assessment tools for scientific literature analysis, making it easier for users to evaluate paper novelty through interactive exploration and contextual verification.

Abstract: Large Language Models (LLMs) show strong reasoning and text generation
capabilities, prompting their use in scientific literature analysis, including
novelty assessment. While evaluating novelty of scientific papers is crucial
for peer review, it requires extensive knowledge of related work, something not
all reviewers have. While recent work on LLM-assisted scientific literature
analysis supports literature comparison, existing approaches offer limited
transparency and lack mechanisms for result traceability via an information
retrieval module. To address this gap, we introduce $\textbf{GraphMind}$, an
easy-to-use interactive web tool designed to assist users in evaluating the
novelty of scientific papers or drafted ideas. Specially, $\textbf{GraphMind}$
enables users to capture the main structure of a scientific paper, explore
related ideas through various perspectives, and assess novelty via providing
verifiable contextual insights. $\textbf{GraphMind}$ enables users to annotate
key elements of a paper, explore related papers through various relationships,
and assess novelty with contextual insight. This tool integrates external APIs
such as arXiv and Semantic Scholar with LLMs to support annotation, extraction,
retrieval and classification of papers. This combination provides users with a
rich, structured view of a scientific idea's core contributions and its
connections to existing work. $\textbf{GraphMind}$ is available at
https://oyarsa.github.io/graphmind and a demonstration video at
https://youtu.be/wKbjQpSvwJg. The source code is available at
https://github.com/oyarsa/graphmind.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [161] [MAGPIE: A benchmark for Multi-AGent contextual PrIvacy Evaluation](https://arxiv.org/abs/2510.15186)
*Gurusha Juneja,Jayanth Naga Sai Pasupulati,Alon Albalak,Wenyue Hua,William Yang Wang*

Main category: cs.CR

TL;DR: MAGPIE is a new benchmark for evaluating privacy understanding in multi-agent LLM systems, revealing significant privacy leakage even in state-of-the-art models like GPT-5 and Gemini 2.5-Pro.


<details>
  <summary>Details</summary>
Motivation: Existing privacy benchmarks focus on simplistic single-turn interactions where private information can be easily omitted without affecting task outcomes, failing to capture real-world collaborative scenarios where privacy must be balanced with task efficacy.

Method: Developed MAGPIE benchmark with 200 high-stakes tasks that integrate private information as essential for task resolution, forcing agents to balance collaboration with strategic information control in multi-agent, non-adversarial scenarios.

Result: State-of-the-art agents exhibit significant privacy leakage (Gemini 2.5-Pro: 50.7%, GPT-5: 35.1%) even when explicitly instructed not to share sensitive information. Agents also struggle with consensus/task completion and show undesirable behaviors like manipulation (Gemini 2.5-Pro: 38.2% cases).

Conclusion: Current LLM agents lack robust privacy understanding and are not adequately aligned to simultaneously preserve privacy and maintain effective collaboration in complex environments.

Abstract: A core challenge for autonomous LLM agents in collaborative settings is
balancing robust privacy understanding and preservation alongside task
efficacy. Existing privacy benchmarks only focus on simplistic, single-turn
interactions where private information can be trivially omitted without
affecting task outcomes. In this paper, we introduce MAGPIE (Multi-AGent
contextual PrIvacy Evaluation), a novel benchmark of 200 high-stakes tasks
designed to evaluate privacy understanding and preservation in multi-agent
collaborative, non-adversarial scenarios. MAGPIE integrates private information
as essential for task resolution, forcing agents to balance effective
collaboration with strategic information control. Our evaluation reveals that
state-of-the-art agents, including GPT-5 and Gemini 2.5-Pro, exhibit
significant privacy leakage, with Gemini 2.5-Pro leaking up to 50.7% and GPT-5
up to 35.1% of the sensitive information even when explicitly instructed not
to. Moreover, these agents struggle to achieve consensus or task completion and
often resort to undesirable behaviors such as manipulation and power-seeking
(e.g., Gemini 2.5-Pro demonstrating manipulation in 38.2% of the cases). These
findings underscore that current LLM agents lack robust privacy understanding
and are not yet adequately aligned to simultaneously preserve privacy and
maintain effective collaboration in complex environments.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [162] [Exploring the Synergy of Quantitative Factors and Newsflow Representations from Large Language Models for Stock Return Prediction](https://arxiv.org/abs/2510.15691)
*Tian Guo,Emmanuel Hauptmann*

Main category: q-fin.CP

TL;DR: This paper proposes multimodal fusion methods combining quantitative factors and newsflow data for stock return prediction, introducing fusion learning, mixture models, and decoupled training to improve performance.


<details>
  <summary>Details</summary>
Motivation: To leverage both structured quantitative factors and unstructured financial data (news, transcripts) using LLMs for better stock return prediction in quantitative investing.

Method: Proposes fusion learning framework with three methods (combination, summation, attentive representations), mixture models combining single modality predictions, and decoupled training for stability.

Result: Experiments on real investment universes provide insights into effective multimodal modeling of factors and news for stock return prediction.

Conclusion: The proposed multimodal approaches effectively combine quantitative factors and newsflow data, with decoupled training addressing instability issues in mixture models.

Abstract: In quantitative investing, return prediction supports various tasks,
including stock selection, portfolio optimization, and risk management.
Quantitative factors, such as valuation, quality, and growth, capture various
characteristics of stocks. Unstructured financial data, like news and
transcripts, has attracted growing attention, driven by recent advances in
large language models (LLMs). This paper examines effective methods for
leveraging multimodal factors and newsflow in return prediction and stock
selection. First, we introduce a fusion learning framework to learn a unified
representation from factors and newsflow representations generated by an LLM.
Within this framework, we compare three representative methods: representation
combination, representation summation, and attentive representations. Next,
building on empirical observations from fusion learning, we explore the mixture
model that adaptively combines predictions made by single modalities and their
fusion. To mitigate the training instability observed in the mixture model, we
introduce a decoupled training approach with theoretical insights. Finally, our
experiments on real investment universes yield several insights into effective
multimodal modeling of factors and news for stock return prediction.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [163] [HugAgent: Evaluating LLMs in Simulating Human-Like Individual Reasoning on Open-Ended Tasks](https://arxiv.org/abs/2510.15144)
*Chance Jiajie Li,Zhenze Mo,Yuhan Tang,Ao Qu,Jiayi Wu,Kaiya Ivy Zhao,Yulu Gan,Jie Fan,Jiangbo Yu,Hang Jiang,Paul Pu Liang,Jinhua Zhao,Luis Alberto Alonso Pastor,Kent Larson*

Main category: cs.AI

TL;DR: HugAgent is a benchmark for evaluating AI models' ability to predict individual human reasoning styles and belief updates, rather than just population-level consensus.


<details>
  <summary>Details</summary>
Motivation: Current large language models capture population-level consensus but erase individual reasoning styles and belief trajectories, failing to simulate truly human-like reasoning.

Method: Dual-track design: synthetic track for scale and systematic stress tests, and human track for ecologically valid "out-loud" reasoning data. Models predict how specific individuals would reason in novel scenarios given their past views.

Result: Experiments with state-of-the-art LLMs reveal persistent adaptation gaps in capturing individual reasoning styles and belief evolution.

Conclusion: HugAgent provides the first extensible benchmark for aligning machine reasoning with the individuality of human thought, enabling evaluation of intra-agent fidelity.

Abstract: Simulating human reasoning in open-ended tasks has been a long-standing
aspiration in AI and cognitive science. While large language models now
approximate human responses at scale, they remain tuned to population-level
consensus, often erasing the individuality of reasoning styles and belief
trajectories. To advance the vision of more human-like reasoning in machines,
we introduce HugAgent (Human-Grounded Agent Benchmark), a benchmark for
average-to-individual reasoning adaptation. The task is to predict how a
specific person would reason and update their beliefs in novel scenarios, given
partial evidence of their past views. HugAgent adopts a dual-track design: a
synthetic track for scale and systematic stress tests, and a human track for
ecologically valid, "out-loud" reasoning data. This design enables scalable,
reproducible evaluation of intra-agent fidelity: whether models can capture not
just what people believe, but how their reasoning evolves. Experiments with
state-of-the-art LLMs reveal persistent adaptation gaps, positioning HugAgent
as the first extensible benchmark for aligning machine reasoning with the
individuality of human thought. Our benchmark and chatbot are open-sourced as
HugAgent (https://anonymous.4open.science/r/HugAgent) and TraceYourThinking
(https://anonymous.4open.science/r/trace-your-thinking).

</details>


### [164] [Multi-dimensional Data Analysis and Applications Basing on LLM Agents and Knowledge Graph Interactions](https://arxiv.org/abs/2510.15258)
*Xi Wang,Xianyao Ling,Kun Li,Gang Yin,Liang Zhang,Jiang Wu,Jun Xu,Fu Zhang,Wenbo Lei,Annie Wang,Peng Gong*

Main category: cs.AI

TL;DR: A method combining LLM agents and Knowledge Graphs for dynamic multi-dimensional data analysis, enabling real-time KG construction from unstructured data and interactive exploration.


<details>
  <summary>Details</summary>
Motivation: Address limitations of LLMs (hallucination, real-time updating) and static nature of KGs by creating a collaborative ecosystem for better multi-dimensional data analysis.

Method: Use LLM agents to extract product data from unstructured data, construct and visualize KG in real-time, and provide interactive platform for deep node exploration.

Result: Significant advantages in product ecosystem analysis, relationship mining, and user-driven exploratory analysis.

Conclusion: Provides new ideas and tools for multi-dimensional data analysis through LLM-KG interaction framework.

Abstract: In the current era of big data, extracting deep insights from massive,
heterogeneous, and complexly associated multi-dimensional data has become a
significant challenge. Large Language Models (LLMs) perform well in natural
language understanding and generation, but still suffer from "hallucination"
issues when processing structured knowledge and are difficult to update in
real-time. Although Knowledge Graphs (KGs) can explicitly store structured
knowledge, their static nature limits dynamic interaction and analytical
capabilities. Therefore, this paper proposes a multi-dimensional data analysis
method based on the interactions between LLM agents and KGs, constructing a
dynamic, collaborative analytical ecosystem. This method utilizes LLM agents to
automatically extract product data from unstructured data, constructs and
visualizes the KG in real-time, and supports users in deep exploration and
analysis of graph nodes through an interactive platform. Experimental results
show that this method has significant advantages in product ecosystem analysis,
relationship mining, and user-driven exploratory analysis, providing new ideas
and tools for multi-dimensional data analysis.

</details>


### [165] [Unleashing Scientific Reasoning for Bio-experimental Protocol Generation via Structured Component-based Reward Mechanism](https://arxiv.org/abs/2510.15600)
*Haoran Sun,Yankai Jiang,Zhenyu Tang,Yaning Pan,Shuang Gu,Zekai Lin,Lilong Wang,Wenjie Lou,Lei Liu,Lei Bai,Xiaosong Wang*

Main category: cs.AI

TL;DR: The paper introduces SciRecipe dataset and Thoth model for generating reproducible scientific protocols through a "Sketch-and-Fill" paradigm and structured reward mechanism, achieving superior performance over existing LLMs.


<details>
  <summary>Details</summary>
Motivation: Current LLMs generate incomplete or inconsistent scientific protocols, limiting their utility for reproducible science. Autonomous generation of precise, executable protocols could greatly improve reproduction efficiency.

Method: Proposed "Sketch-and-Fill" paradigm that separates analysis, structuring, and expression; developed structured component-based reward mechanism; trained Thoth model through staged Knowledge-to-Action process from knowledge acquisition to operational reasoning.

Result: Thoth consistently surpasses both proprietary and open-source LLMs across multiple benchmarks, achieving significant improvements in step alignment, logical sequencing, and semantic accuracy.

Conclusion: The approach paves the way for reliable scientific assistants that bridge knowledge with experimental execution, with all data, code, and models to be released publicly.

Abstract: The foundation of reproducible science lies in protocols that are precise,
logically ordered, and executable. The autonomous generation of these protocols
through natural language queries could greatly improve the efficiency of the
reproduction process. However, current leading large language models (LLMs)
often generate incomplete or inconsistent protocols, limiting their utility. To
address this limitation, we first introduce SciRecipe, a large-scale dataset of
over 12K structured protocols spanning 27 biological subfields and encompassing
both comprehension and problem-solving tasks. To further improve protocol
generation, we propose the "Sketch-and-Fill" paradigm, which separates
analysis, structuring, and expression to ensure each step is explicit and
verifiable. Complementing this, the structured component-based reward mechanism
evaluates step granularity, action order, and semantic fidelity, aligning model
optimization with experimental reliability. Building on these components, we
develop Thoth, trained through a staged Knowledge-to-Action process that
progresses from knowledge acquisition to operational reasoning and ultimately
to robust, executable protocol generation. Across multiple benchmarks, Thoth
consistently surpasses both proprietary and open-source LLMs, achieving
significant improvements in step alignment, logical sequencing, and semantic
accuracy. Our approach paves the way for reliable scientific assistants that
bridge knowledge with experimental execution. All data, code, and models will
be released publicly.

</details>


### [166] [Build Your Personalized Research Group: A Multiagent Framework for Continual and Interactive Science Automation](https://arxiv.org/abs/2510.15624)
*Ed Li,Junyu Ren,Xintian Pan,Cat Yan,Chuanhao Li,Dirk Bergemann,Zhuoran Yang*

Main category: cs.AI

TL;DR: FreePhDlabor is an open-source multiagent framework for automated scientific discovery that features dynamic workflows, modular architecture, and comprehensive infrastructure for continual research programs.


<details>
  <summary>Details</summary>
Motivation: Existing agentic systems for science have rigid workflows that cannot adapt to intermediate findings and inadequate context management that hinders long-horizon research.

Method: The framework uses fully dynamic workflows determined by real-time agent reasoning, modular architecture for customization, automatic context compaction, workspace-based communication, memory persistence, and non-blocking human intervention mechanisms.

Result: The framework transforms automated research from isolated single-run attempts into continual research programs that build systematically on prior explorations and incorporate human feedback.

Conclusion: This work provides both architectural principles and practical implementation for building customizable co-scientist systems to facilitate broader adoption of automated research across scientific domains.

Abstract: The automation of scientific discovery represents a critical milestone in
Artificial Intelligence (AI) research. However, existing agentic systems for
science suffer from two fundamental limitations: rigid, pre-programmed
workflows that cannot adapt to intermediate findings, and inadequate context
management that hinders long-horizon research. We present
\texttt{freephdlabor}, an open-source multiagent framework featuring
\textit{fully dynamic workflows} determined by real-time agent reasoning and a
\coloremph{\textit{modular architecture}} enabling seamless customization --
users can modify, add, or remove agents to address domain-specific
requirements. The framework provides comprehensive infrastructure including
\textit{automatic context compaction}, \textit{workspace-based communication}
to prevent information degradation, \textit{memory persistence} across
sessions, and \textit{non-blocking human intervention} mechanisms. These
features collectively transform automated research from isolated, single-run
attempts into \textit{continual research programs} that build systematically on
prior explorations and incorporate human feedback. By providing both the
architectural principles and practical implementation for building customizable
co-scientist systems, this work aims to facilitate broader adoption of
automated research across scientific domains, enabling practitioners to deploy
interactive multiagent systems that autonomously conduct end-to-end research --
from ideation through experimentation to publication-ready manuscripts.

</details>


### [167] [Context-aware deep learning using individualized prior information reduces false positives in disease risk prediction and longitudinal health assessment](https://arxiv.org/abs/2510.15591)
*Lavanya Umapathy,Patricia M Johnson,Tarun Dutt,Angela Tong,Madhur Nayan,Hersh Chandarana,Daniel K Sodickson*

Main category: cs.AI

TL;DR: A machine learning framework that integrates temporal context from prior medical visits improves prostate cancer risk prediction by reducing false positives while maintaining high sensitivity.


<details>
  <summary>Details</summary>
Motivation: Temporal context in medicine is valuable for assessing patient health changes over time, especially when prior visits are limited and frequency is variable.

Method: The model first estimates initial disease risk using recent patient visit data, then refines this assessment using information from previously collected imaging and clinical biomarkers.

Result: Integrating prior context reduced false positive rates from 51% to 33% with up to three prior imaging exams, and further to 24% when including prior clinical data. For 5-year PCa risk prediction, false positives dropped from 64% to 9%.

Conclusion: Information collected over time provides relevant context to enhance specificity of medical risk prediction, potentially enabling expansion of longitudinal health monitoring to larger populations with low baseline disease risk.

Abstract: Temporal context in medicine is valuable in assessing key changes in patient
health over time. We developed a machine learning framework to integrate
diverse context from prior visits to improve health monitoring, especially when
prior visits are limited and their frequency is variable. Our model first
estimates initial risk of disease using medical data from the most recent
patient visit, then refines this assessment using information digested from
previously collected imaging and/or clinical biomarkers. We applied our
framework to prostate cancer (PCa) risk prediction using data from a large
population (28,342 patients, 39,013 magnetic resonance imaging scans, 68,931
blood tests) collected over nearly a decade. For predictions of the risk of
clinically significant PCa at the time of the visit, integrating prior context
directly converted false positives to true negatives, increasing overall
specificity while preserving high sensitivity. False positive rates were
reduced progressively from 51% to 33% when integrating information from up to
three prior imaging examinations, as compared to using data from a single
visit, and were further reduced to 24% when also including additional context
from prior clinical data. For predicting the risk of PCa within five years of
the visit, incorporating prior context reduced false positive rates still
further (64% to 9%). Our findings show that information collected over time
provides relevant context to enhance the specificity of medical risk
prediction. For a wide range of progressive conditions, sufficient reduction of
false positive rates using context could offer a pathway to expand longitudinal
health monitoring programs to large populations with comparatively low baseline
risk of disease, leading to earlier detection and improved health outcomes.

</details>

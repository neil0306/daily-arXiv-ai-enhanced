<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 39]
- [cs.CV](#cs.CV) [Total: 83]
- [cs.SD](#cs.SD) [Total: 2]
- [cs.AI](#cs.AI) [Total: 7]
- [cs.LG](#cs.LG) [Total: 3]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.SE](#cs.SE) [Total: 2]
- [q-bio.PE](#q-bio.PE) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [eess.IV](#eess.IV) [Total: 4]
- [cs.RO](#cs.RO) [Total: 6]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.CY](#cs.CY) [Total: 3]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Gender-Neutral Rewriting in Italian: Models, Approaches, and Trade-offs](https://arxiv.org/abs/2509.13480)
*Andrea Piergentili,Beatrice Savoldi,Matteo Negri,Luisa Bentivogli*

Main category: cs.CL

TL;DR: First systematic evaluation of LLMs for Italian gender-neutral rewriting shows open-weight models outperform existing dedicated models, and fine-tuned smaller models match larger models' performance.


<details>
  <summary>Details</summary>
Motivation: Gender-neutral rewriting is challenging in grammatical-gender languages like Italian, and there's a need to systematically evaluate state-of-the-art LLMs for this task.

Method: Two-dimensional framework measuring neutrality and semantic fidelity, comparing few-shot prompting across multiple LLMs, fine-tuning selected models, and applying targeted cleaning to boost task relevance.

Result: Open-weight LLMs outperform the only existing dedicated Italian GNR model. Fine-tuned models match or exceed best open-weight LLM's performance at fraction of its size.

Conclusion: Demonstrates effectiveness of LLMs for Italian GNR and discusses trade-off between optimizing training data for neutrality vs meaning preservation.

Abstract: Gender-neutral rewriting (GNR) aims to reformulate text to eliminate
unnecessary gender specifications while preserving meaning, a particularly
challenging task in grammatical-gender languages like Italian. In this work, we
conduct the first systematic evaluation of state-of-the-art large language
models (LLMs) for Italian GNR, introducing a two-dimensional framework that
measures both neutrality and semantic fidelity to the input. We compare
few-shot prompting across multiple LLMs, fine-tune selected models, and apply
targeted cleaning to boost task relevance. Our findings show that open-weight
LLMs outperform the only existing model dedicated to GNR in Italian, whereas
our fine-tuned models match or exceed the best open-weight LLM's performance at
a fraction of its size. Finally, we discuss the trade-off between optimizing
the training data for neutrality and meaning preservation.

</details>


### [2] [Op-Fed: Opinion, Stance, and Monetary Policy Annotations on FOMC Transcripts Using Active Learning](https://arxiv.org/abs/2509.13539)
*Alisa Kanganis,Katherine A. Keith*

Main category: cs.CL

TL;DR: Op-Fed dataset with 1044 annotated FOMC sentences for monetary policy stance analysis, addressing class imbalance and context dependence challenges through hierarchical schema and active learning.


<details>
  <summary>Details</summary>
Motivation: FOMC monetary policy decisions affect millions but analyzing sentiment is challenging due to imbalanced classes and contextual dependencies in transcripts.

Method: Developed five-stage hierarchical schema for opinion aspects, used active learning to double positive instances, created dataset with human annotations requiring context.

Result: Top LLM achieved 0.80 zero-shot accuracy for opinion classification but only 0.61 for monetary policy stance (below human baseline of 0.89).

Conclusion: Op-Fed enables better model training, confidence calibration, and serves as seed dataset for future annotation of FOMC monetary policy sentiment analysis.

Abstract: The U.S. Federal Open Market Committee (FOMC) regularly discusses and sets
monetary policy, affecting the borrowing and spending decisions of millions of
people. In this work, we release Op-Fed, a dataset of 1044 human-annotated
sentences and their contexts from FOMC transcripts. We faced two major
technical challenges in dataset creation: imbalanced classes -- we estimate
fewer than 8% of sentences express a non-neutral stance towards monetary policy
-- and inter-sentence dependence -- 65% of instances require context beyond the
sentence-level. To address these challenges, we developed a five-stage
hierarchical schema to isolate aspects of opinion, monetary policy, and stance
towards monetary policy as well as the level of context needed. Second, we
selected instances to annotate using active learning, roughly doubling the
number of positive instances across all schema aspects. Using Op-Fed, we found
a top-performing, closed-weight LLM achieves 0.80 zero-shot accuracy in opinion
classification but only 0.61 zero-shot accuracy classifying stance towards
monetary policy -- below our human baseline of 0.89. We expect Op-Fed to be
useful for future model training, confidence calibration, and as a seed dataset
for future annotation efforts.

</details>


### [3] [Overview of Dialog System Evaluation Track: Dimensionality, Language, Culture and Safety at DSTC 12](https://arxiv.org/abs/2509.13569)
*John Mendonça,Lining Zhang,Rahul Mallidi,Alon Lavie,Isabel Trancoso,Luis Fernando D'Haro,João Sedoc*

Main category: cs.CL

TL;DR: DSTC12 Track 1 addresses limitations in dialogue system evaluation with two subtasks: multi-dimensional automatic metrics and multilingual/cultural safety detection, showing significant room for improvement in both areas.


<details>
  <summary>Details</summary>
Motivation: Traditional dialogue evaluation metrics are insufficient and safety considerations are often narrowly defined or culturally biased, creating critical gaps in comprehensive assessment of LLM-based dialogue systems.

Method: The track comprised two subtasks: (1) Dialogue-level, Multi-dimensional Automatic Evaluation Metrics across 10 dimensions, and (2) Multilingual and Multicultural Safety Detection using baseline models like Llama-3-8B and Llama-Guard-3-1B.

Result: For Task 1, Llama-3-8B baseline achieved 0.1681 Spearman's correlation, indicating substantial improvement needed. For Task 2, teams outperformed baseline on multilingual safety (top ROC-AUC 0.9648) but baseline was superior on cultural safety (0.5126 ROC-AUC).

Conclusion: The results highlight critical needs for better multi-dimensional evaluation metrics and culturally-aware safety detection in dialogue systems, with significant room for improvement in both technical and cultural dimensions.

Abstract: The rapid advancement of Large Language Models (LLMs) has intensified the
need for robust dialogue system evaluation, yet comprehensive assessment
remains challenging. Traditional metrics often prove insufficient, and safety
considerations are frequently narrowly defined or culturally biased. The DSTC12
Track 1, "Dialog System Evaluation: Dimensionality, Language, Culture and
Safety," is part of the ongoing effort to address these critical gaps. The
track comprised two subtasks: (1) Dialogue-level, Multi-dimensional Automatic
Evaluation Metrics, and (2) Multilingual and Multicultural Safety Detection.
For Task 1, focused on 10 dialogue dimensions, a Llama-3-8B baseline achieved
the highest average Spearman's correlation (0.1681), indicating substantial
room for improvement. In Task 2, while participating teams significantly
outperformed a Llama-Guard-3-1B baseline on the multilingual safety subset (top
ROC-AUC 0.9648), the baseline proved superior on the cultural subset (0.5126
ROC-AUC), highlighting critical needs in culturally-aware safety. This paper
describes the datasets and baselines provided to participants, as well as
submission evaluation results for each of the two proposed subtasks.

</details>


### [4] [Latent Traits and Cross-Task Transfer: Deconstructing Dataset Interactions in LLM Fine-tuning](https://arxiv.org/abs/2509.13624)
*Shambhavi Krishna,Atharva Naik,Chaitali Agarwal,Sudharshan Govindan,Taesung Lee,Haw-Shiuan Chang*

Main category: cs.CL

TL;DR: Analysis framework reveals hidden statistical factors (class distribution, generation length) and linguistic features are more influential than surface-level similarity in LLM transfer learning performance.


<details>
  <summary>Details</summary>
Motivation: LLMs are deployed on diverse tasks not encountered during training, making high-quality training data collection infeasible, necessitating better understanding of cross-task transfer learning dynamics.

Method: Proposed analysis framework using transfer learning matrix and dimensionality reduction, trained and analyzed 10 models to identify latent abilities and transfer learning side effects.

Result: Performance improvements defy surface-level dataset similarity explanations; hidden statistical factors (class distribution, generation length proclivities) and specific linguistic features are more influential.

Conclusion: The work provides insights into complex transfer learning dynamics, enabling more predictable and effective LLM adaptation through understanding of hidden statistical and linguistic factors.

Abstract: Large language models are increasingly deployed across diverse applications.
This often includes tasks LLMs have not encountered during training. This
implies that enumerating and obtaining the high-quality training data for all
tasks is infeasible. Thus, we often need to rely on transfer learning using
datasets with different characteristics, and anticipate out-of-distribution
requests. Motivated by this practical need, we propose an analysis framework,
building a transfer learning matrix and dimensionality reduction, to dissect
these cross-task interactions. We train and analyze 10 models to identify
latent abilities (e.g., Reasoning, Sentiment Classification, NLU, Arithmetic)
and discover the side effects of the transfer learning. Our findings reveal
that performance improvements often defy explanations based on surface-level
dataset similarity or source data quality. Instead, hidden statistical factors
of the source dataset, such as class distribution and generation length
proclivities, alongside specific linguistic features, are actually more
influential. This work offers insights into the complex dynamics of transfer
learning, paving the way for more predictable and effective LLM adaptation.

</details>


### [5] [Sparse Neurons Carry Strong Signals of Question Ambiguity in LLMs](https://arxiv.org/abs/2509.13664)
*Zhuoxuan Zhang,Jinhao Duan,Edward Kim,Kaidi Xu*

Main category: cs.CL

TL;DR: LLMs encode question ambiguity linearly in internal representations, detectable and controllable through specific neurons (AENs) that emerge early in processing layers.


<details>
  <summary>Details</summary>
Motivation: Real-world questions often contain ambiguity, but LLMs typically respond with confident answers rather than seeking clarification, highlighting the need for better ambiguity handling.

Method: Identified Ambiguity-Encoding Neurons (AENs) during pre-filling stage, trained probes on these neurons for ambiguity detection, and performed layerwise analysis and neuron manipulation experiments.

Result: AEN-based probes achieved strong ambiguity detection performance, generalizing across datasets and outperforming prompting-based and representation-based baselines. Neuron manipulation enabled control over LLM behavior from answering to abstention.

Conclusion: LLMs form compact internal representations of question ambiguity that are interpretable and controllable, enabling improved handling of ambiguous queries.

Abstract: Ambiguity is pervasive in real-world questions, yet large language models
(LLMs) often respond with confident answers rather than seeking clarification.
In this work, we show that question ambiguity is linearly encoded in the
internal representations of LLMs and can be both detected and controlled at the
neuron level. During the model's pre-filling stage, we identify that a small
number of neurons, as few as one, encode question ambiguity information. Probes
trained on these Ambiguity-Encoding Neurons (AENs) achieve strong performance
on ambiguity detection and generalize across datasets, outperforming
prompting-based and representation-based baselines. Layerwise analysis reveals
that AENs emerge from shallow layers, suggesting early encoding of ambiguity
signals in the model's processing pipeline. Finally, we show that through
manipulating AENs, we can control LLM's behavior from direct answering to
abstention. Our findings reveal that LLMs form compact internal representations
of question ambiguity, enabling interpretable and controllable behavior.

</details>


### [6] [CL$^2$GEC: A Multi-Discipline Benchmark for Continual Learning in Chinese Literature Grammatical Error Correction](https://arxiv.org/abs/2509.13672)
*Shang Qin,Jingheng Ye,Yinghui Li,Hai-Tao Zheng,Qi Li,Jinxiao Shan,Zhixing Li,Hong-Gee Kim*

Main category: cs.CL

TL;DR: CL$^2$GEC is the first continual learning benchmark for Chinese grammatical error correction across 10 academic disciplines, featuring 10,000 annotated sentences to evaluate adaptive CGEC systems and prevent catastrophic forgetting.


<details>
  <summary>Details</summary>
Motivation: Existing CGEC research lacks dedicated benchmarks for multi-disciplinary academic writing and overlooks continual learning as a solution for handling domain-specific linguistic variation and preventing catastrophic forgetting.

Method: Created CL$^2$GEC benchmark with 10,000 human-annotated sentences across 10 disciplines, evaluated large language models under sequential tuning, parameter-efficient adaptation, and four representative CL algorithms using both standard GEC metrics and continual learning metrics.

Result: Experimental results show that regularization-based methods mitigate forgetting more effectively than replay-based or naive sequential approaches.

Conclusion: The benchmark provides a rigorous foundation for future research in adaptive grammatical error correction across diverse academic domains, addressing the need for robust CGEC systems that can adapt across disciplines.

Abstract: The growing demand for automated writing assistance in diverse academic
domains highlights the need for robust Chinese Grammatical Error Correction
(CGEC) systems that can adapt across disciplines. However, existing CGEC
research largely lacks dedicated benchmarks for multi-disciplinary academic
writing, overlooking continual learning (CL) as a promising solution to handle
domain-specific linguistic variation and prevent catastrophic forgetting. To
fill this crucial gap, we introduce CL$^2$GEC, the first Continual Learning
benchmark for Chinese Literature Grammatical Error Correction, designed to
evaluate adaptive CGEC across multiple academic fields. Our benchmark includes
10,000 human-annotated sentences spanning 10 disciplines, each exhibiting
distinct linguistic styles and error patterns. CL$^2$GEC focuses on evaluating
grammatical error correction in a continual learning setting, simulating
sequential exposure to diverse academic disciplines to reflect real-world
editorial dynamics. We evaluate large language models under sequential tuning,
parameter-efficient adaptation, and four representative CL algorithms, using
both standard GEC metrics and continual learning metrics adapted to task-level
variation. Experimental results reveal that regularization-based methods
mitigate forgetting more effectively than replay-based or naive sequential
approaches. Our benchmark provides a rigorous foundation for future research in
adaptive grammatical error correction across diverse academic domains.

</details>


### [7] [AgentCTG: Harnessing Multi-Agent Collaboration for Fine-Grained Precise Control in Text Generation](https://arxiv.org/abs/2509.13677)
*Xinxu Zhou,Jiaqi Bai,Zhenqi Sun,Fanxiang Zeng,Yue Liu*

Main category: cs.CL

TL;DR: AgentCTG is a novel multi-agent framework for controlled text generation that achieves state-of-the-art results by simulating control mechanisms through agent collaboration and auto-prompt optimization.


<details>
  <summary>Details</summary>
Motivation: Controlled Text Generation faces challenges in achieving fine-grained conditional control, cost efficiency, scalability, domain knowledge learning, and precise control for real-world applications.

Method: A multi-agent framework that simulates control and regulation mechanisms through agent workflows, explores various collaboration methods, and incorporates an auto-prompt module to enhance generation effectiveness.

Result: Achieves state-of-the-art results on multiple public datasets and demonstrates effectiveness in practical applications like Character-Driven Rewriting and online navigation with role-playing.

Conclusion: AgentCTG significantly enhances text generation control, improves content delivery for immersive interactions, and enables greater personalization and user engagement in online communities.

Abstract: Although significant progress has been made in many tasks within the field of
Natural Language Processing (NLP), Controlled Text Generation (CTG) continues
to face numerous challenges, particularly in achieving fine-grained conditional
control over generation. Additionally, in real scenario and online
applications, cost considerations, scalability, domain knowledge learning and
more precise control are required, presenting more challenge for CTG. This
paper introduces a novel and scalable framework, AgentCTG, which aims to
enhance precise and complex control over the text generation by simulating the
control and regulation mechanisms in multi-agent workflows. We explore various
collaboration methods among different agents and introduce an auto-prompt
module to further enhance the generation effectiveness. AgentCTG achieves
state-of-the-art results on multiple public datasets. To validate its
effectiveness in practical applications, we propose a new challenging
Character-Driven Rewriting task, which aims to convert the original text into
new text that conform to specific character profiles and simultaneously
preserve the domain knowledge. When applied to online navigation with
role-playing, our approach significantly enhances the driving experience
through improved content delivery. By optimizing the generation of contextually
relevant text, we enable a more immersive interaction within online
communities, fostering greater personalization and user engagement.

</details>


### [8] [Improving Context Fidelity via Native Retrieval-Augmented Reasoning](https://arxiv.org/abs/2509.13683)
*Suyuchen Wang,Jinlin Wang,Xinyu Wang,Shiqi Li,Xiangru Tang,Sirui Hong,Xiao-Wen Chang,Chenglin Wu,Bang Liu*

Main category: cs.CL

TL;DR: CARE is a retrieval-augmented reasoning framework that teaches LLMs to integrate in-context evidence within their reasoning process using the model's own retrieval capabilities, improving context fidelity and answer consistency.


<details>
  <summary>Details</summary>
Motivation: LLMs often struggle with context fidelity, producing inconsistent answers when responding to questions based on provided information. Existing approaches either require expensive supervised fine-tuning or train models for web searches without improving context utilization.

Method: Proposes CARE framework that teaches LLMs to explicitly integrate in-context evidence within their reasoning process using the model's own retrieval capabilities. Uses limited labeled evidence data and strategically retrieved in-context tokens in the reasoning chain.

Result: Significantly enhances both retrieval accuracy and answer generation performance. Outperforms supervised fine-tuning, traditional retrieval-augmented generation methods, and external retrieval solutions on multiple real-world and counterfactual QA benchmarks.

Conclusion: Represents a fundamental advancement in making LLMs more accurate, reliable, and efficient for knowledge-intensive tasks by improving context fidelity and evidence integration.

Abstract: Large language models (LLMs) often struggle with context fidelity, producing
inconsistent answers when responding to questions based on provided
information. Existing approaches either rely on expensive supervised
fine-tuning to generate evidence post-answer or train models to perform web
searches without necessarily improving utilization of the given context. We
propose CARE, a novel native retrieval-augmented reasoning framework that
teaches LLMs to explicitly integrate in-context evidence within their reasoning
process with the model's own retrieval capabilities. Our method requires
limited labeled evidence data while significantly enhancing both retrieval
accuracy and answer generation performance through strategically retrieved
in-context tokens in the reasoning chain. Extensive experiments on multiple
real-world and counterfactual QA benchmarks demonstrate that our approach
substantially outperforms supervised fine-tuning, traditional
retrieval-augmented generation methods, and external retrieval solutions. This
work represents a fundamental advancement in making LLMs more accurate,
reliable, and efficient for knowledge-intensive tasks.

</details>


### [9] [Can Large Language Models Robustly Perform Natural Language Inference for Japanese Comparatives?](https://arxiv.org/abs/2509.13695)
*Yosuke Mikami,Daiki Matsuoka,Hitomi Yanaka*

Main category: cs.CL

TL;DR: LLMs struggle with numerical/logical NLI in Japanese comparatives, especially with linguistic nuances. Performance varies with prompt formats and few-shot examples, but logical semantic representations in prompts improve accuracy.


<details>
  <summary>Details</summary>
Motivation: To evaluate LLM robustness in handling comparative expressions for NLI in non-dominant languages like Japanese, where numerical and logical inference remains challenging despite overall strong NLI performance.

Method: Constructed a Japanese NLI dataset focused on comparatives and evaluated various LLMs in zero-shot and few-shot settings with different prompt formats, including those with logical semantic representations.

Result: Models showed sensitivity to prompt formats in zero-shot settings and were influenced by gold labels in few-shot examples. LLMs struggled with Japanese-specific linguistic phenomena, but prompts with logical semantic representations helped solve difficult inference problems.

Conclusion: LLMs need improvement in handling numerical/logical NLI for non-dominant languages, and incorporating logical semantic representations in prompts can enhance performance for challenging comparative inference tasks in languages like Japanese.

Abstract: Large Language Models (LLMs) perform remarkably well in Natural Language
Inference (NLI). However, NLI involving numerical and logical expressions
remains challenging. Comparatives are a key linguistic phenomenon related to
such inference, but the robustness of LLMs in handling them, especially in
languages that are not dominant in the models' training data, such as Japanese,
has not been sufficiently explored. To address this gap, we construct a
Japanese NLI dataset that focuses on comparatives and evaluate various LLMs in
zero-shot and few-shot settings. Our results show that the performance of the
models is sensitive to the prompt formats in the zero-shot setting and
influenced by the gold labels in the few-shot examples. The LLMs also struggle
to handle linguistic phenomena unique to Japanese. Furthermore, we observe that
prompts containing logical semantic representations help the models predict the
correct labels for inference problems that they struggle to solve even with
few-shot examples.

</details>


### [10] [Integrating Text and Time-Series into (Large) Language Models to Predict Medical Outcomes](https://arxiv.org/abs/2509.13696)
*Iyadh Ben Cheikh Larbi,Ajay Madhavan Ravichandran,Aljoscha Burchardt,Roland Roller*

Main category: cs.CL

TL;DR: LLMs adapted with DSPy prompt optimization for clinical classification tasks achieve performance comparable to specialized multimodal systems with less complexity and better adaptability.


<details>
  <summary>Details</summary>
Motivation: Large language models excel at text generation but their ability to handle clinical classification tasks involving structured data like time series remains underexplored.

Method: Adapt instruction-tuned LLMs using DSPy-based prompt optimization to process clinical notes and structured EHR inputs jointly.

Result: The approach achieves performance on par with specialized multimodal systems.

Conclusion: This method requires less complexity and offers greater adaptability across clinical tasks compared to specialized multimodal systems.

Abstract: Large language models (LLMs) excel at text generation, but their ability to
handle clinical classification tasks involving structured data, such as time
series, remains underexplored. In this work, we adapt instruction-tuned LLMs
using DSPy-based prompt optimization to process clinical notes and structured
EHR inputs jointly. Our results show that this approach achieves performance on
par with specialized multimodal systems while requiring less complexity and
offering greater adaptability across tasks.

</details>


### [11] [DSCC-HS: A Dynamic Self-Reinforcing Framework for Hallucination Suppression in Large Language Models](https://arxiv.org/abs/2509.13702)
*Xiao Zheng*

Main category: cs.CL

TL;DR: DSCC-HS is a proactive framework that uses dual proxy models to dynamically steer LLM decoding and suppress hallucinations in real-time without modifying the target model.


<details>
  <summary>Details</summary>
Motivation: Current methods like RAG are reactive to LLM hallucinations, so there's a need for proactive intervention during the generation process to improve factuality.

Method: Uses compact proxy models trained adversarially as Factual Alignment Proxy and Hallucination Detection Proxy to inject real-time steering vectors during autoregressive decoding.

Result: Achieved 99.2% Factual Consistency Rate on TruthfulQA and state-of-the-art FActScore of 46.50 on BioGEN benchmark.

Conclusion: DSCC-HS provides a principled and efficient plug-and-play solution for enhancing LLM factuality through dynamic self-reinforcing calibration.

Abstract: Large Language Model (LLM) hallucination is a significant barrier to their
reliable deployment. Current methods like Retrieval-Augmented Generation (RAG)
are often reactive. We introduce **Dynamic Self-reinforcing Calibration for
Hallucination Suppression (DSCC-HS)**, a novel, proactive framework that
intervenes during autoregressive decoding. Inspired by dual-process cognitive
theory, DSCC-HS uses a compact proxy model, trained in adversarial roles as a
Factual Alignment Proxy (FAP) and a Hallucination Detection Proxy (HDP). During
inference, these proxies dynamically steer a large target model by injecting a
real-time steering vector, which is the difference between FAP and HDP logits,
at each decoding step. This plug-and-play approach requires no modification to
the target model. Our experiments on TruthfulQA and BioGEN show DSCC-HS
achieves state-of-the-art performance. On TruthfulQA, it reached a 99.2%
Factual Consistency Rate (FCR). On the long-form BioGEN benchmark, it attained
the highest FActScore of 46.50. These results validate DSCC-HS as a principled
and efficient solution for enhancing LLM factuality.

</details>


### [12] [Automated Triaging and Transfer Learning of Incident Learning Safety Reports Using Large Language Representational Models](https://arxiv.org/abs/2509.13706)
*Peter Beidler,Mark Nguyen,Kevin Lybarger,Ola Holmberg,Eric Ford,John Kang*

Main category: cs.CL

TL;DR: NLP tool developed to automatically detect high-severity incident reports in radiation oncology using SVM and BlueBERT models, achieving human-level performance with cross-institution transfer learning.


<details>
  <summary>Details</summary>
Motivation: Manual review of healthcare incident reports is time-consuming and requires expertise. Need automated tools to efficiently identify high-severity reports for safety improvement.

Method: Used 7,094 institutional reports and 571 IAEA SAFRON reports with expert-labeled severity scores. Trained SVM and BlueBERT models, tested cross-institution generalizability with transfer learning approach.

Result: BlueBERT with transfer learning achieved AUROC 0.78 on external dataset, similar to human performance (AUROC 0.81) on curated reports. Without transfer, performance dropped significantly.

Conclusion: Successfully developed cross-institution NLP models that can detect high-severity radiation oncology incident reports at human-level performance, enabling more efficient safety monitoring.

Abstract: PURPOSE: Incident reports are an important tool for safety and quality
improvement in healthcare, but manual review is time-consuming and requires
subject matter expertise. Here we present a natural language processing (NLP)
screening tool to detect high-severity incident reports in radiation oncology
across two institutions.
  METHODS AND MATERIALS: We used two text datasets to train and evaluate our
NLP models: 7,094 reports from our institution (Inst.), and 571 from IAEA
SAFRON (SF), all of which had severity scores labeled by clinical content
experts. We trained and evaluated two types of models: baseline support vector
machines (SVM) and BlueBERT which is a large language model pretrained on
PubMed abstracts and hospitalized patient data. We assessed for
generalizability of our model in two ways. First, we evaluated models trained
using Inst.-train on SF-test. Second, we trained a BlueBERT_TRANSFER model that
was first fine-tuned on Inst.-train then on SF-train before testing on SF-test
set. To further analyze model performance, we also examined a subset of 59
reports from our Inst. dataset, which were manually edited for clarity.
  RESULTS Classification performance on the Inst. test achieved AUROC 0.82
using SVM and 0.81 using BlueBERT. Without cross-institution transfer learning,
performance on the SF test was limited to an AUROC of 0.42 using SVM and 0.56
using BlueBERT. BlueBERT_TRANSFER, which was fine-tuned on both datasets,
improved the performance on SF test to AUROC 0.78. Performance of SVM, and
BlueBERT_TRANSFER models on the manually curated Inst. reports (AUROC 0.85 and
0.74) was similar to human performance (AUROC 0.81).
  CONCLUSION: In summary, we successfully developed cross-institution NLP
models on incident report text from radiation oncology centers. These models
were able to detect high-severity reports similarly to humans on a curated
dataset.

</details>


### [13] [DSPC: Dual-Stage Progressive Compression Framework for Efficient Long-Context Reasoning](https://arxiv.org/abs/2509.13723)
*Yaxin Gao,Yao Lu,Zongfei Zhang,Jiaqi Nie,Shanqing Yu,Qi Xuan*

Main category: cs.CL

TL;DR: DSPC is a training-free prompt compression method that uses semantic filtering and token pruning to reduce computational costs while maintaining LLM performance.


<details>
  <summary>Details</summary>
Motivation: Longer prompts in LLMs increase computational costs, and existing compression methods require additional training, creating inefficiency.

Method: Two-stage approach: coarse-grained semantic filtering using TF-IDF to remove low-value sentences, and fine-grained token pruning using attention contribution, cross-model loss difference, and positional importance.

Result: Achieved 49.17 performance on FewShot task with 3x fewer tokens, outperforming LongLLMLingua baseline by 7.76 points on LLaMA-3.1-8B-Instruct and GPT-3.5-Turbo.

Conclusion: DSPC provides effective training-free prompt compression that reduces token usage while maintaining or improving LLM performance across various tasks.

Abstract: Large language models (LLMs) have achieved remarkable success in many natural
language processing (NLP) tasks. To achieve more accurate output, the prompts
used to drive LLMs have become increasingly longer, which incurs higher
computational costs. To address this prompt inflation problem, prompt
compression has been proposed. However, most existing methods require training
a small auxiliary model for compression, incurring a significant amount of
additional computation. To avoid this, we propose a two-stage, training-free
approach, called Dual-Stage Progressive Compression (DSPC). In the
coarse-grained stage, semantic-related sentence filtering removes sentences
with low semantic value based on TF-IDF. In the fine-grained stage, token
importance is assessed using attention contribution, cross-model loss
difference, and positional importance, enabling the pruning of low-utility
tokens while preserving semantics. We validate DSPC on LLaMA-3.1-8B-Instruct
and GPT-3.5-Turbo under a constrained token budget and observe consistent
improvements. For instance, in the FewShot task of the Longbench dataset, DSPC
achieves a performance of 49.17 by using only 3x fewer tokens, outperforming
the best state-of-the-art baseline LongLLMLingua by 7.76.

</details>


### [14] [Implementing a Logical Inference System for Japanese Comparatives](https://arxiv.org/abs/2509.13734)
*Yosuke Mikami,Daiki Matsuoka,Hitomi Yanaka*

Main category: cs.CL

TL;DR: Proposes ccg-jcomp, a logic-based inference system for Japanese comparatives that addresses linguistic differences from English, showing better accuracy than LLMs on Japanese NLI tasks.


<details>
  <summary>Details</summary>
Motivation: Natural Language Inference with comparatives is challenging due to quantity and comparative relation understanding. Japanese comparatives have morphological and semantic differences from English that make existing logic-based systems difficult to apply directly.

Method: Developed ccg-jcomp, a logical inference system for Japanese comparatives based on compositional semantics, specifically designed to handle Japanese linguistic characteristics.

Result: Evaluated on a Japanese NLI dataset containing comparative expressions. The system demonstrated effectiveness by achieving higher accuracy compared to existing Large Language Models.

Conclusion: The proposed logic-based compositional semantics approach provides robust handling of Japanese comparatives and outperforms LLM-based approaches for this specific linguistic challenge.

Abstract: Natural Language Inference (NLI) involving comparatives is challenging
because it requires understanding quantities and comparative relations
expressed by sentences. While some approaches leverage Large Language Models
(LLMs), we focus on logic-based approaches grounded in compositional semantics,
which are promising for robust handling of numerical and logical expressions.
Previous studies along these lines have proposed logical inference systems for
English comparatives. However, it has been pointed out that there are several
morphological and semantic differences between Japanese and English
comparatives. These differences make it difficult to apply such systems
directly to Japanese comparatives. To address this gap, this study proposes
ccg-jcomp, a logical inference system for Japanese comparatives based on
compositional semantics. We evaluate the proposed system on a Japanese NLI
dataset containing comparative expressions. We demonstrate the effectiveness of
our system by comparing its accuracy with that of existing LLMs.

</details>


### [15] [Exploring Data and Parameter Efficient Strategies for Arabic Dialect Identifications](https://arxiv.org/abs/2509.13775)
*Vani Kanjirangat,Ljiljana Dolamic,Fabio Rinaldi*

Main category: cs.CL

TL;DR: This paper explores data-efficient and parameter-efficient methods for Arabic Dialect Identification, comparing soft-prompting techniques, LoRA reparameterizations, and hard prompting with LLMs, finding that LoRA-based fine-tuning performs best.


<details>
  <summary>Details</summary>
Motivation: To develop efficient approaches for Arabic Dialect Identification that require less data and fewer parameters while maintaining high performance, addressing the challenge of dialectal nuances in Arabic language processing.

Method: Investigated various soft-prompting strategies (prefix-tuning, prompt-tuning, P-tuning, P-tuning V2) and LoRA reparameterizations. Used data-efficient hard prompting with zero-shot and few-shot inferences on LLMs. Conducted experiments with Arabic-specific encoder models on multiple datasets and analyzed n-shot inferences on decoder-only models including Phi-3.5 and SILMA.

Result: LLMs struggled to differentiate dialectal nuances in few-shot/zero-shot setups. Soft-prompted encoder variants performed better, while LoRA-based fine-tuned models achieved the best performance, even surpassing full fine-tuning.

Conclusion: Parameter-efficient methods like LoRA reparameterization are highly effective for Arabic Dialect Identification, outperforming both traditional fine-tuning and prompting approaches, making them suitable for resource-constrained scenarios.

Abstract: This paper discusses our exploration of different data-efficient and
parameter-efficient approaches to Arabic Dialect Identification (ADI). In
particular, we investigate various soft-prompting strategies, including
prefix-tuning, prompt-tuning, P-tuning, and P-tuning V2, as well as LoRA
reparameterizations. For the data-efficient strategy, we analyze hard prompting
with zero-shot and few-shot inferences to analyze the dialect identification
capabilities of Large Language Models (LLMs). For the parameter-efficient PEFT
approaches, we conducted our experiments using Arabic-specific encoder models
on several major datasets. We also analyzed the n-shot inferences on
open-source decoder-only models, a general multilingual model (Phi-3.5), and an
Arabic-specific one(SILMA). We observed that the LLMs generally struggle to
differentiate the dialectal nuances in the few-shot or zero-shot setups. The
soft-prompted encoder variants perform better, while the LoRA-based fine-tuned
models perform best, even surpassing full fine-tuning.

</details>


### [16] [Teaching According to Talents! Instruction Tuning LLMs with Competence-Aware Curriculum Learning](https://arxiv.org/abs/2509.13790)
*Yangning Li,Tingwei Lu,Yinghui Li,Yankai Chen,Wei-Chieh Huang,Wenhao Jiang,Hui Wang,Hai-Tao Zheng,Philip S. Yu*

Main category: cs.CL

TL;DR: CAMPUS is a dynamic curriculum learning framework for instruction tuning that adapts to model capabilities during training, outperforming static curriculum methods.


<details>
  <summary>Details</summary>
Motivation: Current curriculum instruction tuning methods rely on static difficulty metrics that don't adapt to evolving model capabilities, leading to rigid and potentially suboptimal learning trajectories.

Method: CAMPUS framework features dynamic sub-curriculum selection, competency-aware curriculum schedule adjustment, and multiple difficulty-based scheduling to adapt to model capabilities during training.

Result: Extensive experiments show CAMPUS achieves superior performance compared to state-of-the-art baselines for efficient instruction tuning.

Conclusion: The proposed dynamic curriculum learning framework effectively addresses curriculum rigidity in instruction tuning by adapting to model capabilities throughout training.

Abstract: Efficient instruction tuning aims to enhance the ultimate performance of
large language models (LLMs) trained on a given instruction dataset. Curriculum
learning as a typical data organization strategy has shown preliminary
effectiveness in instruction tuning. However, current curriculum tuning methods
suffer from the curriculum rigidity, since they rely solely on static heuristic
difficulty metrics. These methods fail to adapt to the evolving capabilities of
models during training, resulting in a fixed and potentially sub-optimal
learning trajectory. To address the issue, Competence-Aware Multi-Perspective
cUrriculum inStruction tuning framework termed CAMPUS is proposed. CAMPUS
offers several advantages: (1) Dynamic selection for sub-curriculum. (2)
Competency-aware adjustment to the curriculum schedule. (3) Multiple
difficulty-based scheduling. Extensive experiments prove the superior
performance of CAMPUS, compared to other state-of-the-art baselines for
efficient instruction tuning.

</details>


### [17] [Measuring Gender Bias in Job Title Matching for Grammatical Gender Languages](https://arxiv.org/abs/2509.13803)
*Laura García-Sardiña,Hermenegildo Fabregat,Daniel Deniz,Rabih Zbib*

Main category: cs.CL

TL;DR: This paper establishes a framework for analyzing gender bias in job title ranking systems by creating test sets in four grammatical gender languages and proposing RBO-based metrics to evaluate gender bias in multilingual models.


<details>
  <summary>Details</summary>
Motivation: To study how explicit grammatical gender assignment in job titles affects automatic job ranking systems and develop methods to evaluate gender bias in these systems.

Method: Generated test sets for job title matching in four grammatical gender languages with masculine/feminine forms, annotated by gender and matching relevance. Proposed using Rank-Biased Overlap (RBO) metrics to evaluate gender bias in ranking systems.

Result: All evaluated out-of-the-box multilingual models exhibited varying degrees of gender bias when tested with the new methodology and test sets.

Conclusion: The study provides a foundation and methodology for detecting gender bias in job ranking systems, demonstrating that current multilingual models require improvements to address gender bias issues.

Abstract: This work sets the ground for studying how explicit grammatical gender
assignment in job titles can affect the results of automatic job ranking
systems. We propose the usage of metrics for ranking comparison controlling for
gender to evaluate gender bias in job title ranking systems, in particular RBO
(Rank-Biased Overlap). We generate and share test sets for a job title matching
task in four grammatical gender languages, including occupations in masculine
and feminine form and annotated by gender and matching relevance. We use the
new test sets and the proposed methodology to evaluate the gender bias of
several out-of-the-box multilingual models to set as baselines, showing that
all of them exhibit varying degrees of gender bias.

</details>


### [18] [Geometric Uncertainty for Detecting and Correcting Hallucinations in LLMs](https://arxiv.org/abs/2509.13813)
*Edward Phillips,Sean Wu,Soheila Molaei,Danielle Belgrave,Anshul Thakur,David Clifton*

Main category: cs.CL

TL;DR: A geometric framework using archetypal analysis for black-box hallucination detection in LLMs, providing both global uncertainty (Geometric Volume) and local uncertainty (Geometric Suspicion) metrics without requiring white-box access.


<details>
  <summary>Details</summary>
Motivation: Current black-box uncertainty quantification methods only provide global uncertainty estimates, while local methods require white-box access to internal model states. There's a need for black-box approaches that can attribute uncertainty to both batches of responses and individual responses for better hallucination detection.

Method: Geometric framework based on archetypal analysis of response embeddings. Geometric Volume measures convex hull volume of archetypes for global uncertainty. Geometric Suspicion ranks individual responses by reliability using semantic boundary points from the archetypal analysis.

Result: The framework performs comparably or better than prior methods on short-form QA datasets, and achieves superior results on medical datasets where hallucinations are particularly critical. Theoretical link proven between convex hull volume and entropy.

Conclusion: The proposed geometric approach provides effective black-box hallucination detection with both global and local uncertainty quantification, outperforming existing methods especially in high-risk domains like medical QA.

Abstract: Large language models demonstrate impressive results across diverse tasks but
are still known to hallucinate, generating linguistically plausible but
incorrect answers to questions. Uncertainty quantification has been proposed as
a strategy for hallucination detection, but no existing black-box approach
provides estimates for both global and local uncertainty. The former attributes
uncertainty to a batch of responses, while the latter attributes uncertainty to
individual responses. Current local methods typically rely on white-box access
to internal model states, whilst black-box methods only provide global
uncertainty estimates. We introduce a geometric framework to address this,
based on archetypal analysis of batches of responses sampled with only
black-box model access. At the global level, we propose Geometric Volume, which
measures the convex hull volume of archetypes derived from response embeddings.
At the local level, we propose Geometric Suspicion, which ranks responses by
reliability and enables hallucination reduction through preferential response
selection. Unlike prior dispersion methods which yield only a single global
score, our approach provides semantic boundary points which have utility for
attributing reliability to individual responses. Experiments show that our
framework performs comparably to or better than prior methods on short form
question-answering datasets, and achieves superior results on medical datasets
where hallucinations carry particularly critical risks. We also provide
theoretical justification by proving a link between convex hull volume and
entropy.

</details>


### [19] [Findings of the Third Automatic Minuting (AutoMin) Challenge](https://arxiv.org/abs/2509.13814)
*Kartik Shinde,Laurent Besacier,Ondrej Bojar,Thibaut Thonet,Tirthankar Ghosal*

Main category: cs.CL

TL;DR: AutoMin 2025 shared task on meeting summarization with minuting (structured minutes creation) and question answering tasks across English/Czech languages and project/parliament domains, featuring limited participation but comprehensive LLM baseline evaluations.


<details>
  <summary>Details</summary>
Motivation: To advance automatic meeting summarization research through structured minuting and QA tasks, enabling evaluation of current LLM capabilities across languages and domains despite limited team participation.

Method: Organized shared task with two main components: minuting (structured meeting minutes creation) and QA (monolingual and cross-lingual). Included multiple baseline systems using large language models for comprehensive evaluation across English/Czech languages and project meeting/European Parliament domains.

Result: Limited participation with only one team for minuting task and two teams for QA task. However, organizers provided extensive baseline evaluations of current (2025) LLMs on both tasks, enabling performance assessment across different languages and domains.

Conclusion: Despite lower participation compared to previous years, the AutoMin 2025 shared task successfully provided valuable benchmarks for meeting summarization and QA using modern LLMs, demonstrating their capabilities across multiple languages and domains while highlighting ongoing challenges in the field.

Abstract: This paper presents the third edition of AutoMin, a shared task on automatic
meeting summarization into minutes. In 2025, AutoMin featured the main task of
minuting, the creation of structured meeting minutes, as well as a new task:
question answering (QA) based on meeting transcripts.
  The minuting task covered two languages, English and Czech, and two domains:
project meetings and European Parliament sessions. The QA task focused solely
on project meetings and was available in two settings: monolingual QA in
English, and cross-lingual QA, where questions were asked and answered in Czech
based on English meetings.
  Participation in 2025 was more limited compared to previous years, with only
one team joining the minuting task and two teams participating in QA. However,
as organizers, we included multiple baseline systems to enable a comprehensive
evaluation of current (2025) large language models (LLMs) on both tasks.

</details>


### [20] [Large Language Models Discriminate Against Speakers of German Dialects](https://arxiv.org/abs/2509.13835)
*Minh Duc Bui,Carolin Holtermann,Valentin Hofmann,Anne Lauscher,Katharina von der Wense*

Main category: cs.CL

TL;DR: LLMs exhibit significant bias against German dialect speakers, showing negative stereotypes in both association and decision tasks, with explicit demographic labeling amplifying bias more than implicit dialect cues.


<details>
  <summary>Details</summary>
Motivation: To examine whether negative societal stereotypes faced by dialect speakers are mirrored by large language models, given that over 40% of Germany's population speaks regional dialects yet faces discrimination.

Method: Used two tasks: association task and decision task with a novel evaluation corpus pairing sentences from seven German dialects with standard German counterparts. Analyzed dialect naming bias and dialect usage bias based on sociolinguistic traits.

Result: All evaluated LLMs showed significant dialect naming and usage bias against German dialect speakers through negative adjective associations, reproduced these biases in decision making, and explicit labeling of linguistic demographics amplified bias more than implicit cues.

Conclusion: LLMs reflect and potentially amplify real-world negative stereotypes against dialect speakers, with explicit demographic mentions unexpectedly increasing bias rather than reducing it.

Abstract: Dialects represent a significant component of human culture and are found
across all regions of the world. In Germany, more than 40% of the population
speaks a regional dialect (Adler and Hansen, 2022). However, despite cultural
importance, individuals speaking dialects often face negative societal
stereotypes. We examine whether such stereotypes are mirrored by large language
models (LLMs). We draw on the sociolinguistic literature on dialect perception
to analyze traits commonly associated with dialect speakers. Based on these
traits, we assess the dialect naming bias and dialect usage bias expressed by
LLMs in two tasks: an association task and a decision task. To assess a model's
dialect usage bias, we construct a novel evaluation corpus that pairs sentences
from seven regional German dialects (e.g., Alemannic and Bavarian) with their
standard German counterparts. We find that: (1) in the association task, all
evaluated LLMs exhibit significant dialect naming and dialect usage bias
against German dialect speakers, reflected in negative adjective associations;
(2) all models reproduce these dialect naming and dialect usage biases in their
decision making; and (3) contrary to prior work showing minimal bias with
explicit demographic mentions, we find that explicitly labeling linguistic
demographics--German dialect speakers--amplifies bias more than implicit cues
like dialect usage.

</details>


### [21] [Do LLMs Align Human Values Regarding Social Biases? Judging and Explaining Social Biases with LLMs](https://arxiv.org/abs/2509.13869)
*Yang Liu,Chenhui Chu*

Main category: cs.CL

TL;DR: LLMs show varying alignment with human values on social biases across different scenario types, with larger models not necessarily performing better. Model families show consistency, and smaller models can generate readable explanations but with lower agreeability.


<details>
  <summary>Details</summary>
Motivation: To investigate whether LLM alignment with human values on social biases differs across scenario types and to understand their explanation capabilities.

Method: Analyzed 12 LLMs from 4 model families using 4 datasets, examining misalignment rates, attack success rates, judgment consistency, and explanation generation capabilities.

Result: Larger models don't necessarily have better alignment; models show preference for specific scenario types; family models have higher consistency; no significant differences in HVSB understanding; smaller LMs generate readable but less agreeable explanations.

Conclusion: LLM alignment with human values varies by scenario type, model size isn't the sole determinant of performance, and smaller models can be trained for explanation generation with trade-offs in agreeability.

Abstract: Large language models (LLMs) can lead to undesired consequences when
misaligned with human values, especially in scenarios involving complex and
sensitive social biases. Previous studies have revealed the misalignment of
LLMs with human values using expert-designed or agent-based emulated bias
scenarios. However, it remains unclear whether the alignment of LLMs with human
values differs across different types of scenarios (e.g., scenarios containing
negative vs. non-negative questions). In this study, we investigate the
alignment of LLMs with human values regarding social biases (HVSB) in different
types of bias scenarios. Through extensive analysis of 12 LLMs from four model
families and four datasets, we demonstrate that LLMs with large model parameter
scales do not necessarily have lower misalignment rate and attack success rate.
Moreover, LLMs show a certain degree of alignment preference for specific types
of scenarios and the LLMs from the same model family tend to have higher
judgment consistency. In addition, we study the understanding capacity of LLMs
with their explanations of HVSB. We find no significant differences in the
understanding of HVSB across LLMs. We also find LLMs prefer their own generated
explanations. Additionally, we endow smaller language models (LMs) with the
ability to explain HVSB. The generation results show that the explanations
generated by the fine-tuned smaller LMs are more readable, but have a
relatively lower model agreeability.

</details>


### [22] [Combining Evidence and Reasoning for Biomedical Fact-Checking](https://arxiv.org/abs/2509.13879)
*Mariano Barone,Antonio Romano,Giuseppe Riccio,Marco Postiglione,Vincenzo Moscato*

Main category: cs.CL

TL;DR: CER is a novel biomedical fact-checking framework that combines scientific evidence retrieval with LLM reasoning and supervised veracity prediction to combat healthcare misinformation.


<details>
  <summary>Details</summary>
Motivation: Healthcare misinformation poses serious public health risks, and existing automated fact-checking methods struggle with biomedical claims due to complex terminology, need for domain expertise, and requirement for scientific evidence grounding.

Method: CER integrates scientific evidence retrieval, reasoning via large language models, and supervised veracity prediction. It combines LLM text-generation capabilities with advanced retrieval techniques for high-quality biomedical evidence to mitigate hallucinations.

Result: State-of-the-art performance on expert-annotated datasets (HealthFC, BioASQ-7b, SciFact) with promising cross-dataset generalization capabilities.

Conclusion: CER provides an effective framework for biomedical fact-checking that grounds outputs in verifiable evidence-based sources, addressing the unique challenges of healthcare misinformation validation.

Abstract: Misinformation in healthcare, from vaccine hesitancy to unproven treatments,
poses risks to public health and trust in medical systems. While machine
learning and natural language processing have advanced automated fact-checking,
validating biomedical claims remains uniquely challenging due to complex
terminology, the need for domain expertise, and the critical importance of
grounding in scientific evidence. We introduce CER (Combining Evidence and
Reasoning), a novel framework for biomedical fact-checking that integrates
scientific evidence retrieval, reasoning via large language models, and
supervised veracity prediction. By integrating the text-generation capabilities
of large language models with advanced retrieval techniques for high-quality
biomedical scientific evidence, CER effectively mitigates the risk of
hallucinations, ensuring that generated outputs are grounded in verifiable,
evidence-based sources. Evaluations on expert-annotated datasets (HealthFC,
BioASQ-7b, SciFact) demonstrate state-of-the-art performance and promising
cross-dataset generalization. Code and data are released for transparency and
reproducibility: https: //github.com/PRAISELab-PicusLab/CER.

</details>


### [23] [Combating Biomedical Misinformation through Multi-modal Claim Detection and Evidence-based Verification](https://arxiv.org/abs/2509.13888)
*Mariano Barone,Antonio Romano,Giuseppe Riccio,Marco Postiglione,Vincenzo Moscato*

Main category: cs.CL

TL;DR: CER is a novel biomedical fact-checking framework that combines scientific evidence retrieval with LLM reasoning and supervised veracity prediction to combat healthcare misinformation.


<details>
  <summary>Details</summary>
Motivation: Healthcare misinformation poses serious public health risks, and existing automated fact-checking methods struggle with biomedical claims due to complex terminology, need for domain expertise, and requirement for scientific evidence grounding.

Method: CER integrates scientific evidence retrieval, reasoning via large language models, and supervised veracity prediction to mitigate hallucinations and ensure outputs are grounded in verifiable evidence-based sources.

Result: State-of-the-art performance on expert-annotated datasets (HealthFC, BioASQ-7b, SciFact) with promising cross-dataset generalization capabilities.

Conclusion: CER effectively addresses biomedical fact-checking challenges by combining evidence retrieval with LLM reasoning, providing a robust solution for healthcare misinformation detection with released code and data for transparency.

Abstract: Misinformation in healthcare, from vaccine hesitancy to unproven treatments,
poses risks to public health and trust in medical systems. While machine
learning and natural language processing have advanced automated fact-checking,
validating biomedical claims remains uniquely challenging due to complex
terminology, the need for domain expertise, and the critical importance of
grounding in scientific evidence. We introduce CER (Combining Evidence and
Reasoning), a novel framework for biomedical fact-checking that integrates
scientific evidence retrieval, reasoning via large language models, and
supervised veracity prediction. By integrating the text-generation capabilities
of large language models with advanced retrieval techniques for high-quality
biomedical scientific evidence, CER effectively mitigates the risk of
hallucinations, ensuring that generated outputs are grounded in verifiable,
evidence-based sources. Evaluations on expert-annotated datasets (HealthFC,
BioASQ-7b, SciFact) demonstrate state-of-the-art performance and promising
cross-dataset generalization. Code and data are released for transparency and
reproducibility: https://github.com/PRAISELab-PicusLab/CER

</details>


### [24] [Do Large Language Models Understand Word Senses?](https://arxiv.org/abs/2509.13905)
*Domenico Meconi,Simone Stirpe,Federico Martelli,Leonardo Lavalle,Roberto Navigli*

Main category: cs.CL

TL;DR: LLMs demonstrate strong word sense understanding, achieving WSD performance comparable to specialized systems and excelling in generative word sense explanation tasks with up to 98% accuracy.


<details>
  <summary>Details</summary>
Motivation: Despite extensive evaluation of LLMs, their true understanding of word senses in context remains underexplored, creating a gap in assessing their fundamental language comprehension capabilities.

Method: Evaluated instruction-tuned LLMs on Word Sense Disambiguation (WSD) compared to specialized systems, and assessed generative abilities through definition generation, free-form explanation, and example generation tasks using top open- and closed-source models.

Result: GPT-4o and DeepSeek-V3 achieved WSD performance on par with specialized systems while showing greater robustness across domains and difficulty levels. In generation tasks, LLMs reached up to 98% accuracy in explaining word meanings, with free-form explanation performing best.

Conclusion: Modern LLMs demonstrate sophisticated word sense understanding capabilities, matching specialized WSD systems in disambiguation tasks and excelling in generative explanation tasks, indicating strong contextual word comprehension.

Abstract: Understanding the meaning of words in context is a fundamental capability for
Large Language Models (LLMs). Despite extensive evaluation efforts, the extent
to which LLMs show evidence that they truly grasp word senses remains
underexplored. In this paper, we address this gap by evaluating both i) the
Word Sense Disambiguation (WSD) capabilities of instruction-tuned LLMs,
comparing their performance to state-of-the-art systems specifically designed
for the task, and ii) the ability of two top-performing open- and closed-source
LLMs to understand word senses in three generative settings: definition
generation, free-form explanation, and example generation. Notably, we find
that, in the WSD task, leading models such as GPT-4o and DeepSeek-V3 achieve
performance on par with specialized WSD systems, while also demonstrating
greater robustness across domains and levels of difficulty. In the generation
tasks, results reveal that LLMs can explain the meaning of words in context up
to 98\% accuracy, with the highest performance observed in the free-form
explanation task, which best aligns with their generative capabilities.

</details>


### [25] [Linguistic Nepotism: Trading-off Quality for Language Preference in Multilingual RAG](https://arxiv.org/abs/2509.13930)
*Dayeon Ki,Marine Carpuat,Paul McNamee,Daniel Khashabi,Eugene Yang,Dawn Lawrie,Kevin Duh*

Main category: cs.CL

TL;DR: mRAG systems show language bias in citation, preferring English sources even when other languages are equally relevant, with amplified bias for lower-resource languages and mid-context documents.


<details>
  <summary>Details</summary>
Motivation: To investigate whether the mixture of different document languages in multilingual RAG systems impacts generation and citation behavior in unintended ways, particularly examining language preference biases.

Method: Controlled methodology using model internals to measure language preference while holding document relevance constant across eight languages and six open-weight models.

Result: Models preferentially cite English sources for English queries, with bias amplified for lower-resource languages and mid-context documents. Models sometimes trade document relevance for language preference.

Conclusion: Citation choices in multilingual RAG systems are not always driven by informativeness alone, revealing language bias that affects how models leverage multilingual context and influence citation behavior.

Abstract: Multilingual Retrieval-Augmented Generation (mRAG) systems enable language
models to answer knowledge-intensive queries with citation-supported responses
across languages. While such systems have been proposed, an open questions is
whether the mixture of different document languages impacts generation and
citation in unintended ways. To investigate, we introduce a controlled
methodology using model internals to measure language preference while holding
other factors such as document relevance constant. Across eight languages and
six open-weight models, we find that models preferentially cite English sources
when queries are in English, with this bias amplified for lower-resource
languages and for documents positioned mid-context. Crucially, we find that
models sometimes trade-off document relevance for language preference,
indicating that citation choices are not always driven by informativeness
alone. Our findings shed light on how language models leverage multilingual
context and influence citation behavior.

</details>


### [26] [Long-context Reference-based MT Quality Estimation](https://arxiv.org/abs/2509.13980)
*Sami Ul Haq,Chinonso Cynthia Osuji,Sheila Castilho,Brian Davis*

Main category: cs.CL

TL;DR: COMET-based translation quality evaluation system using augmented long-context data and multiple human judgment datasets to predict ESA scores, showing improved correlation with human judgments.


<details>
  <summary>Details</summary>
Motivation: To improve automated translation quality evaluation by incorporating long-context information and integrating multiple human judgment datasets (MQM, SQM, DA) for better correlation with human assessments.

Method: Built on COMET framework, concatenated in-domain human-annotated sentences to create long-context training data, computed weighted average scores, normalized multiple human judgment scales, and trained multilingual regression models using source, hypothesis, and reference translations.

Result: Experimental results demonstrate that incorporating long-context information improves correlations with human judgments compared to models trained only on short segments.

Conclusion: The approach successfully enhances translation quality evaluation by leveraging long-context data and integrating diverse human judgment metrics, providing better alignment with human assessment standards.

Abstract: In this paper, we present our submission to the Tenth Conference on Machine
Translation (WMT25) Shared Task on Automated Translation Quality Evaluation.
  Our systems are built upon the COMET framework and trained to predict
segment-level Error Span Annotation (ESA) scores using augmented long-context
data.
  To construct long-context training data, we concatenate in-domain,
human-annotated sentences and compute a weighted average of their scores.
  We integrate multiple human judgment datasets (MQM, SQM, and DA) by
normalising their scales and train multilingual regression models to predict
quality scores from the source, hypothesis, and reference translations.
  Experimental results show that incorporating long-context information
improves correlations with human judgments compared to models trained only on
short segments.

</details>


### [27] [Slim-SC: Thought Pruning for Efficient Scaling with Self-Consistency](https://arxiv.org/abs/2509.13990)
*Colin Hong,Xu Guo,Anand Chaanan Singh,Esha Choukse,Dmitrii Ustiugov*

Main category: cs.CL

TL;DR: Slim-SC is a step-wise pruning strategy that reduces Self-Consistency's computational overhead by removing redundant reasoning chains based on inter-chain similarity, achieving up to 45% latency reduction while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Self-Consistency (SC) improves LLM reasoning through parallel chain generation and majority voting, but suffers from order-of-magnitude computational overhead that limits practical deployment.

Method: Proposes Slim-SC, which identifies and prunes redundant reasoning chains using inter-chain similarity analysis at the thought level during step-wise processing.

Result: On three STEM reasoning datasets with two LLM architectures, Slim-SC reduces inference latency by up to 45% and KVC usage by 26% while maintaining or improving accuracy compared to standard SC.

Conclusion: Slim-SC provides an efficient alternative to Self-Consistency that significantly reduces computational costs without sacrificing reasoning performance, making TTS more practical for real-world deployment.

Abstract: Recently, Test-Time Scaling (TTS) has gained increasing attention for
improving LLM reasoning performance at test time without retraining the model.
A notable TTS technique is Self-Consistency (SC), which generates multiple
reasoning chains in parallel and selects the final answer via majority voting.
While effective, the order-of-magnitude computational overhead limits its broad
deployment. Prior attempts to accelerate SC mainly rely on model-based
confidence scores or heuristics with limited empirical support. For the first
time, we theoretically and empirically analyze the inefficiencies of SC and
reveal actionable opportunities for improvement. Building on these insights, we
propose Slim-SC, a step-wise pruning strategy that identifies and removes
redundant chains using inter-chain similarity at the thought level. Experiments
on three STEM reasoning datasets and two recent LLM architectures show that
Slim-SC reduces inference latency and KVC usage by up to 45% and 26%,
respectively, with R1-Distill, while maintaining or improving accuracy, thus
offering a simple yet efficient TTS alternative for SC.

</details>


### [28] [Early Stopping Chain-of-thoughts in Large Language Models](https://arxiv.org/abs/2509.14004)
*Minjia Mao,Bowen Yin,Yu Zhu,Xiao Fang*

Main category: cs.CL

TL;DR: ES-CoT is an inference-time method that reduces chain-of-thought token usage by 41% on average while maintaining accuracy, by detecting answer convergence and stopping generation early.


<details>
  <summary>Details</summary>
Motivation: Long chain-of-thoughts in LLMs incur high inference costs, creating a need for methods that can shorten CoT generation with minimal performance loss.

Method: At each reasoning step, prompt LLM to output current final answer, track consecutive identical answers as convergence measure, and terminate generation when run length shows sharp increase exceeding threshold.

Result: 41% reduction in inference tokens across five reasoning datasets and three LLMs, with comparable accuracy to standard CoT. Works well with self-consistency prompting and robust to hyperparameter choices.

Conclusion: ES-CoT provides a practical and effective approach for efficient reasoning by leveraging answer convergence detection to enable early stopping with minimal accuracy loss.

Abstract: Reasoning large language models (LLMs) have demonstrated superior capacities
in solving complicated problems by generating long chain-of-thoughts (CoT), but
such a lengthy CoT incurs high inference costs. In this study, we introduce
ES-CoT, an inference-time method that shortens CoT generation by detecting
answer convergence and stopping early with minimal performance loss. At the end
of each reasoning step, we prompt the LLM to output its current final answer,
denoted as a step answer. We then track the run length of consecutive identical
step answers as a measure of answer convergence. Once the run length exhibits a
sharp increase and exceeds a minimum threshold, the generation is terminated.
We provide both empirical and theoretical support for this heuristic: step
answers steadily converge to the final answer, and large run-length jumps
reliably mark this convergence. Experiments on five reasoning datasets across
three LLMs show that ES-CoT reduces the number of inference tokens by about
41\% on average while maintaining accuracy comparable to standard CoT. Further,
ES-CoT integrates seamlessly with self-consistency prompting and remains robust
across hyperparameter choices, highlighting it as a practical and effective
approach for efficient reasoning.

</details>


### [29] [Hala Technical Report: Building Arabic-Centric Instruction & Translation Models at Scale](https://arxiv.org/abs/2509.14008)
*Hasan Abed Al Kader Hammoud,Mohammad Zbeeb,Bernard Ghanem*

Main category: cs.CL

TL;DR: Hala is a family of Arabic instruction and translation models built using a translate-and-tune pipeline that compresses a teacher model to FP8 for efficiency, creates bilingual supervision data, and fine-tunes lightweight models to produce state-of-the-art Arabic NLP performance.


<details>
  <summary>Details</summary>
Motivation: To address the need for high-quality Arabic language models and instruction-following capabilities, as Arabic NLP research often lags behind English-centric models.

Method: Translate-and-tune pipeline: compress AR-EN teacher to FP8 for efficiency, create bilingual supervision data, fine-tune lightweight LFM2-1.2B model to translate English instructions to Arabic, train Hala models at various sizes (350M-9B parameters), and apply slerp merging for Arabic specialization.

Result: Achieves state-of-the-art results on Arabic-centric benchmarks in both nano (≤2B) and small (7-9B) model categories, outperforming their base models with ~2× higher throughput and no quality loss.

Conclusion: Hala models provide effective Arabic language capabilities and the released models, data, evaluation, and recipes will accelerate Arabic NLP research.

Abstract: We present Hala, a family of Arabic-centric instruction and translation
models built with our translate-and-tune pipeline. We first compress a strong
AR$\leftrightarrow$EN teacher to FP8 (yielding $\sim$2$\times$ higher
throughput with no quality loss) and use it to create high-fidelity bilingual
supervision. A lightweight language model LFM2-1.2B is then fine-tuned on this
data and used to translate high-quality English instruction sets into Arabic,
producing a million-scale corpus tailored to instruction following. We train
Hala models at 350M, 700M, 1.2B, and 9B parameters, and apply slerp merging to
balance Arabic specialization with base-model strengths. On Arabic-centric
benchmarks, Hala achieves state-of-the-art results within both the "nano"
($\leq$2B) and "small" (7-9B) categories, outperforming their bases. We release
models, data, evaluation, and recipes to accelerate research in Arabic NLP.

</details>


### [30] [Audio-Based Crowd-Sourced Evaluation of Machine Translation Quality](https://arxiv.org/abs/2509.14023)
*Sami Ul Haq,Sheila Castilho,Yvette Graham*

Main category: cs.CL

TL;DR: Audio-based MT evaluation yields similar rankings to text-only methods but can identify significant differences between systems due to speech's richer modality.


<details>
  <summary>Details</summary>
Motivation: Current MT quality assessment is text-centric despite many real-world applications involving spoken translation, creating a need for more natural speech-based evaluation methods.

Method: Compared text-only and audio-based evaluations of 10 MT systems from WMT Shared Task using crowd-sourced judgments from Amazon Mechanical Turk, with statistical significance testing and self-replication experiments.

Result: Audio-based assessments produced rankings largely consistent with text-only evaluations but identified significant differences between some translation systems.

Conclusion: Speech-based assessments should be incorporated into future MT evaluation frameworks due to speech's richer, more natural modality for evaluating spoken translation applications.

Abstract: Machine Translation (MT) has achieved remarkable performance, with growing
interest in speech translation and multimodal approaches. However, despite
these advancements, MT quality assessment remains largely text centric,
typically relying on human experts who read and compare texts. Since many
real-world MT applications (e.g Google Translate Voice Mode, iFLYTEK
Translator) involve translation being spoken rather printed or read, a more
natural way to assess translation quality would be through speech as opposed
text-only evaluations. This study compares text-only and audio-based
evaluations of 10 MT systems from the WMT General MT Shared Task, using
crowd-sourced judgments collected via Amazon Mechanical Turk. We additionally,
performed statistical significance testing and self-replication experiments to
test reliability and consistency of audio-based approach. Crowd-sourced
assessments based on audio yield rankings largely consistent with text only
evaluations but, in some cases, identify significant differences between
translation systems. We attribute this to speech richer, more natural modality
and propose incorporating speech-based assessments into future MT evaluation
frameworks.

</details>


### [31] [You Are What You Train: Effects of Data Composition on Training Context-aware Machine Translation Models](https://arxiv.org/abs/2509.14031)
*Paweł Mąka,Yusuf Can Semerci,Jan Scholtes,Gerasimos Spanakis*

Main category: cs.CL

TL;DR: Training data sparsity is the key bottleneck for context utilization in machine translation, and targeted training strategies can improve context-aware translation by up to 8 percentage points.


<details>
  <summary>Details</summary>
Motivation: Human-level translation requires context to handle coherence and complex phenomena like pronoun disambiguation, but standard training data lacks sufficient contextually rich examples.

Method: Constructed training datasets with controlled proportions of contextually relevant examples, systematically validated sparsity impact, and proposed/evaluated two training strategies to leverage available data.

Result: Strong association between training data sparsity and model performance confirmed, with training strategies improving context utilization by 6-8 percentage points on ctxPro evaluation.

Conclusion: Sparsity is a key bottleneck for context utilization, improvements don't generalize across phenomena, cross-lingual transfer is limited, but targeted training strategies can effectively improve context-aware translation.

Abstract: Achieving human-level translations requires leveraging context to ensure
coherence and handle complex phenomena like pronoun disambiguation. Sparsity of
contextually rich examples in the standard training data has been hypothesized
as the reason for the difficulty of context utilization. In this work, we
systematically validate this claim in both single- and multilingual settings by
constructing training datasets with a controlled proportions of contextually
relevant examples. We demonstrate a strong association between training data
sparsity and model performance confirming sparsity as a key bottleneck.
Importantly, we reveal that improvements in one contextual phenomenon do no
generalize to others. While we observe some cross-lingual transfer, it is not
significantly higher between languages within the same sub-family. Finally, we
propose and empirically evaluate two training strategies designed to leverage
the available data. These strategies improve context utilization, resulting in
accuracy gains of up to 6 and 8 percentage points on the ctxPro evaluation in
single- and multilingual settings respectively.

</details>


### [32] [Enhancing Multi-Agent Debate System Performance via Confidence Expression](https://arxiv.org/abs/2509.14034)
*Zijie Lin,Bryan Hooi*

Main category: cs.CL

TL;DR: The paper proposes ConfMAD, a Multi-Agent Debate framework that incorporates confidence expression to improve LLM debate performance by allowing models to explicitly communicate confidence levels.


<details>
  <summary>Details</summary>
Motivation: Current MAD systems struggle because LLMs with superior knowledge can't effectively communicate their advantage during debates due to lack of confidence expression, leading to stubborn maintenance of incorrect beliefs or premature convergence on suboptimal answers.

Method: Developed ConfMAD framework that integrates confidence expression throughout the debate process, allowing LLMs to explicitly communicate their confidence levels during multi-agent debates.

Result: Experimental results demonstrate the effectiveness of the proposed method, showing improved debate performance and task outcomes.

Conclusion: The study shows that incorporating confidence expression enhances MAD system performance and provides insights into designing confidence-aware debate systems for better LLM collaboration.

Abstract: Generative Large Language Models (LLMs) have demonstrated remarkable
performance across a wide range of tasks. Recent research has introduced
Multi-Agent Debate (MAD) systems, which leverage multiple LLMs to simulate
human debate and thereby improve task performance. However, while some LLMs may
possess superior knowledge or reasoning capabilities for specific tasks, they
often struggle to clearly communicate this advantage during debates, in part
due to a lack of confidence expression. Moreover, inappropriate confidence
expression can cause agents in MAD systems to either stubbornly maintain
incorrect beliefs or converge prematurely on suboptimal answers, ultimately
reducing debate effectiveness and overall system performance. To address these
challenges, we propose incorporating confidence expression into MAD systems to
allow LLMs to explicitly communicate their confidence levels. To validate this
approach, we develop ConfMAD, a MAD framework that integrates confidence
expression throughout the debate process. Experimental results demonstrate the
effectiveness of our method, and we further analyze how confidence influences
debate dynamics, offering insights into the design of confidence-aware MAD
systems.

</details>


### [33] [SSL-SSAW: Self-Supervised Learning with Sigmoid Self-Attention Weighting for Question-Based Sign Language Translation](https://arxiv.org/abs/2509.14036)
*Zekang Liu,Wei Feng,Fanhua Shang,Lianyu Hu,Jichao Feng,Liqing Gao*

Main category: cs.CL

TL;DR: Proposes Question-based Sign Language Translation (QB-SLT) using dialogue context instead of gloss annotations, with a novel SSL-SSAW fusion method that achieves state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Dialogue provides crucial contextual cues for sign language translation and is easier to annotate than gloss annotations, which are more complex and less natural in communication scenarios.

Method: Cross-modality Self-supervised Learning with Sigmoid Self-attention Weighting (SSL-SSAW) fusion method using contrastive learning for feature alignment and adaptive feature extraction from question and sign language sequences.

Result: Achieved SOTA performance on CSL-Daily-QA and PHOENIX-2014T-QA datasets, with question assistance matching or surpassing gloss assistance performance. Visualization confirmed improved translation quality.

Conclusion: Question-based dialogue integration is an effective alternative to gloss annotations for sign language translation, providing natural contextual cues that enhance translation performance while being easier to annotate.

Abstract: Sign Language Translation (SLT) bridges the communication gap between deaf
people and hearing people, where dialogue provides crucial contextual cues to
aid in translation. Building on this foundational concept, this paper proposes
Question-based Sign Language Translation (QB-SLT), a novel task that explores
the efficient integration of dialogue. Unlike gloss (sign language
transcription) annotations, dialogue naturally occurs in communication and is
easier to annotate. The key challenge lies in aligning multimodality features
while leveraging the context of the question to improve translation. To address
this issue, we propose a cross-modality Self-supervised Learning with Sigmoid
Self-attention Weighting (SSL-SSAW) fusion method for sign language
translation. Specifically, we employ contrastive learning to align
multimodality features in QB-SLT, then introduce a Sigmoid Self-attention
Weighting (SSAW) module for adaptive feature extraction from question and sign
language sequences. Additionally, we leverage available question text through
self-supervised learning to enhance representation and translation
capabilities. We evaluated our approach on newly constructed CSL-Daily-QA and
PHOENIX-2014T-QA datasets, where SSL-SSAW achieved SOTA performance. Notably,
easily accessible question assistance can achieve or even surpass the
performance of gloss assistance. Furthermore, visualization results demonstrate
the effectiveness of incorporating dialogue in improving translation quality.

</details>


### [34] [Canary-1B-v2 & Parakeet-TDT-0.6B-v3: Efficient and High-Performance Models for Multilingual ASR and AST](https://arxiv.org/abs/2509.14128)
*Monica Sekoyan,Nithin Rao Koluguri,Nune Tadevosyan,Piotr Zelasko,Travis Bartley,Nick Karpov,Jagadeesh Balam,Boris Ginsburg*

Main category: cs.CL

TL;DR: Canary-1B-v2 is a fast multilingual ASR and speech translation model supporting 25 European languages, achieving better English ASR than Whisper-large-v3 while being 10x faster, with competitive performance against larger models.


<details>
  <summary>Details</summary>
Motivation: To develop a fast and robust multilingual speech recognition and translation model that reduces hallucinations and provides reliable timestamps while maintaining high performance.

Method: Two-stage pre-training and fine-tuning with FastConformer encoder and Transformer decoder, trained on 1.7M hours of data including Granary and NeMo ASR Set 3.0, with non-speech audio to reduce hallucinations. Uses NeMo Forced Aligner with auxiliary CTC for timestamps.

Result: Outperforms Whisper-large-v3 on English ASR while being 10x faster. Delivers competitive multilingual ASR and AST performance against larger models like Seamless-M4T-v2-large. Also released smaller Parakeet-TDT-0.6B-v3 model with 600M parameters.

Conclusion: Canary-1B-v2 demonstrates that FastConformer architecture excels after fine-tuning, providing state-of-the-art performance with efficiency. The model successfully addresses hallucination reduction and timestamp reliability while maintaining competitive results across multiple languages.

Abstract: This report introduces Canary-1B-v2, a fast, robust multilingual model for
Automatic Speech Recognition (ASR) and Speech-to-Text Translation (AST). Built
with a FastConformer encoder and Transformer decoder, it supports 25 languages
primarily European. The model was trained on 1.7M hours of total data samples,
including Granary and NeMo ASR Set 3.0, with non-speech audio added to reduce
hallucinations for ASR and AST. We describe its two-stage pre-training and
fine-tuning process with dynamic data balancing, as well as experiments with an
nGPT encoder. Results show nGPT scales well with massive data, while
FastConformer excels after fine-tuning. For timestamps, Canary-1B-v2 uses the
NeMo Forced Aligner (NFA) with an auxiliary CTC model, providing reliable
segment-level timestamps for ASR and AST. Evaluations show Canary-1B-v2
outperforms Whisper-large-v3 on English ASR while being 10x faster, and
delivers competitive multilingual ASR and AST performance against larger models
like Seamless-M4T-v2-large and LLM-based systems. We also release
Parakeet-TDT-0.6B-v3, a successor to v2, offering multilingual ASR across the
same 25 languages with just 600M parameters.

</details>


### [35] [CS-FLEURS: A Massively Multilingual and Code-Switched Speech Dataset](https://arxiv.org/abs/2509.14161)
*Brian Yan,Injy Hamed,Shuichiro Shimizu,Vasista Lodagala,William Chen,Olga Iakovenko,Bashar Talafha,Amir Hussein,Alexander Polok,Kalvin Chang,Dominik Klement,Sara Althubaiti,Puyuan Peng,Matthew Wiesner,Thamar Solorio,Ahmed Ali,Sanjeev Khudanpur,Shinji Watanabe,Chih-Chen Chen,Zhen Wu,Karim Benharrak,Anuj Diwan,Samuele Cornell,Eunjung Yeo,Kwanghee Choi,Carlos Carvalho,Karen Rosero*

Main category: cs.CL

TL;DR: CS-FLEURS is a new dataset for code-switched speech recognition and translation covering 113 language pairs across 52 languages, featuring both real and synthetic speech data.


<details>
  <summary>Details</summary>
Motivation: To develop and evaluate code-switched speech recognition and translation systems beyond high-resourced languages, broadening the scope of code-switched speech research.

Method: Created a dataset with 4 test sets: 1) 14 X-English pairs with real voices reading synthetic code-switched sentences, 2) 16 X-English pairs with generative TTS, 3) 60 {Arabic, Mandarin, Hindi, Spanish}-X pairs with generative TTS, and 4) 45 X-English lower-resourced pairs with concatenative TTS. Also provides 128 hours of training data across 16 X-English pairs.

Result: A comprehensive dataset covering 113 unique code-switched language pairs across 52 languages with various speech synthesis methods.

Conclusion: CS-FLEURS helps broaden code-switched speech research scope and is publicly available for development and evaluation of multilingual speech systems.

Abstract: We present CS-FLEURS, a new dataset for developing and evaluating
code-switched speech recognition and translation systems beyond high-resourced
languages. CS-FLEURS consists of 4 test sets which cover in total 113 unique
code-switched language pairs across 52 languages: 1) a 14 X-English language
pair set with real voices reading synthetically generated code-switched
sentences, 2) a 16 X-English language pair set with generative text-to-speech
3) a 60 {Arabic, Mandarin, Hindi, Spanish}-X language pair set with the
generative text-to-speech, and 4) a 45 X-English lower-resourced language pair
test set with concatenative text-to-speech. Besides the four test sets,
CS-FLEURS also provides a training set with 128 hours of generative
text-to-speech data across 16 X-English language pairs. Our hope is that
CS-FLEURS helps to broaden the scope of future code-switched speech research.
Dataset link: https://huggingface.co/datasets/byan/cs-fleurs.

</details>


### [36] [AssoCiAm: A Benchmark for Evaluating Association Thinking while Circumventing Ambiguity](https://arxiv.org/abs/2509.14171)
*Yifan Liu,Wenkuan Zhao,Shanshan Zhong,Jinghui Qin,Mingfu Liang,Zhongzhan Huang,Wushao Wen*

Main category: cs.CL

TL;DR: AssoCiAm benchmark evaluates MLLM associative ability while addressing ambiguity through hybrid computational methods, revealing cognition-association correlation and ambiguity's impact on model behavior.


<details>
  <summary>Details</summary>
Motivation: Current association evaluation frameworks overlook inherent ambiguity in association tasks, which undermines reliability of MLLM creativity assessments needed for AGI development.

Method: Decomposes ambiguity into internal and external types, introduces AssoCiAm benchmark with hybrid computational method to circumvent ambiguity in association evaluation.

Result: Strong positive correlation between cognition and association observed; ambiguity causes MLLMs' behavior to become more random-like; method ensures more accurate evaluations.

Conclusion: AssoCiAm provides effective ambiguity-aware evaluation framework for MLLM associative ability, enabling more reliable assessment of creative thinking capabilities essential for AGI.

Abstract: Recent advancements in multimodal large language models (MLLMs) have garnered
significant attention, offering a promising pathway toward artificial general
intelligence (AGI). Among the essential capabilities required for AGI,
creativity has emerged as a critical trait for MLLMs, with association serving
as its foundation. Association reflects a model' s ability to think creatively,
making it vital to evaluate and understand. While several frameworks have been
proposed to assess associative ability, they often overlook the inherent
ambiguity in association tasks, which arises from the divergent nature of
associations and undermines the reliability of evaluations. To address this
issue, we decompose ambiguity into two types-internal ambiguity and external
ambiguity-and introduce AssoCiAm, a benchmark designed to evaluate associative
ability while circumventing the ambiguity through a hybrid computational
method. We then conduct extensive experiments on MLLMs, revealing a strong
positive correlation between cognition and association. Additionally, we
observe that the presence of ambiguity in the evaluation process causes MLLMs'
behavior to become more random-like. Finally, we validate the effectiveness of
our method in ensuring more accurate and reliable evaluations. See Project Page
for the data and codes.

</details>


### [37] [Synthesizing Behaviorally-Grounded Reasoning Chains: A Data-Generation Framework for Personal Finance LLMs](https://arxiv.org/abs/2509.14180)
*Akhil Theerthala*

Main category: cs.CL

TL;DR: An 8B parameter financial advisor model achieves performance comparable to larger 14-32B models through careful data curation and behavioral finance integration, with 80% lower costs.


<details>
  <summary>Details</summary>
Motivation: Existing financial advice systems have high maintenance costs and yield less than 25% of expected returns, requiring a more efficient and effective approach to personalized financial advisory.

Method: Created a novel framework integrating financial context with behavioral finance studies to build a 19k sample reasoning dataset, then fine-tuned Qwen-3-8B model on this curated dataset.

Result: The 8B model achieved performance comparable to significantly larger 14-32B parameter baselines across factual accuracy, fluency, and personalization metrics in held-out tests and blind LLM-jury studies.

Conclusion: Careful data curation and behavioral integration enable smaller models to match larger counterparts' performance at substantially lower costs, providing a reproducible framework for efficient financial advisory systems.

Abstract: Personalized financial advice requires consideration of user goals,
constraints, risk tolerance, and jurisdiction. Prior LLM work has focused on
support systems for investors and financial planners. Simultaneously, numerous
recent studies examine broader personal finance tasks, including budgeting,
debt management, retirement, and estate planning, through agentic pipelines
that incur high maintenance costs, yielding less than 25% of their expected
financial returns. In this study, we introduce a novel and reproducible
framework that integrates relevant financial context with behavioral finance
studies to construct supervision data for end-to-end advisors. Using this
framework, we create a 19k sample reasoning dataset and conduct a comprehensive
fine-tuning of the Qwen-3-8B model on the dataset. Through a held-out test
split and a blind LLM-jury study, we demonstrate that through careful data
curation and behavioral integration, our 8B model achieves performance
comparable to significantly larger baselines (14-32B parameters) across factual
accuracy, fluency, and personalization metrics while incurring 80% lower costs
than the larger counterparts.

</details>


### [38] [Framing Migration: A Computational Analysis of UK Parliamentary Discourse](https://arxiv.org/abs/2509.14197)
*Vahid Ghafouri,Robert McNeil,Teodor Yankov,Madeleine Sumption,Luc Rocher,Scott A. Hale,Adam Mahdi*

Main category: cs.CL

TL;DR: Large-scale computational analysis of UK parliamentary migration discourse over 75 years using LLMs, showing less polarization than US, ideological gaps between parties, and shift toward securitized narratives.


<details>
  <summary>Details</summary>
Motivation: To analyze migration discourse trends over time and compare UK parliamentary debates with US congressional discourse using computational methods to understand political polarization and narrative shifts.

Method: Used open-weight LLMs to annotate statements with migration stances, tracked net tone across time and parties, and developed semi-automated framework for extracting fine-grained narrative frames in UK debates.

Result: UK discourse remains relatively aligned across parties (unlike polarized US), with persistent Labour-Conservative ideological gap reaching most negative level in 2025. Shift toward securitized narratives (border control, illegal immigration) and decline in integration-oriented frames. Replacement of national law discussions with international law/human rights themes.

Conclusion: LLMs enable scalable, fine-grained discourse analysis in political/historical contexts, revealing nuanced migration discourse trends and narrative shifts over decades.

Abstract: We present a large-scale computational analysis of migration-related
discourse in UK parliamentary debates spanning over 75 years and compare it
with US congressional discourse. Using open-weight LLMs, we annotate each
statement with high-level stances toward migrants and track the net tone toward
migrants across time and political parties. For the UK, we extend this with a
semi-automated framework for extracting fine-grained narrative frames to
capture nuances of migration discourse. Our findings show that, while US
discourse has grown increasingly polarised, UK parliamentary attitudes remain
relatively aligned across parties, with a persistent ideological gap between
Labour and the Conservatives, reaching its most negative level in 2025. The
analysis of narrative frames in the UK parliamentary statements reveals a shift
toward securitised narratives such as border control and illegal immigration,
while longer-term integration-oriented frames such as social integration have
declined. Moreover, discussions of national law about immigration have been
replaced over time by international law and human rights, revealing nuances in
discourse trends. Taken together broadly, our findings demonstrate how LLMs can
support scalable, fine-grained discourse analysis in political and historical
contexts.

</details>


### [39] [Apertus: Democratizing Open and Compliant LLMs for Global Language Environments](https://arxiv.org/abs/2509.14233)
*Alejandro Hernández-Cano,Alexander Hägele,Allen Hao Huang,Angelika Romanou,Antoni-Joan Solergibert,Barna Pasztor,Bettina Messmer,Dhia Garbaya,Eduard Frank Ďurech,Ido Hakimi,Juan García Giraldo,Mete Ismayilzada,Negar Foroutan,Skander Moalla,Tiancheng Chen,Vinko Sabolčec,Yixuan Xu,Michael Aerni,Badr AlKhamissi,Ines Altemir Marinas,Mohammad Hossein Amani,Matin Ansaripour,Ilia Badanin,Harold Benoit,Emanuela Boros,Nicholas Browning,Fabian Bösch,Maximilian Böther,Niklas Canova,Camille Challier,Clement Charmillot,Jonathan Coles,Jan Deriu,Arnout Devos,Lukas Drescher,Daniil Dzenhaliou,Maud Ehrmann,Dongyang Fan,Simin Fan,Silin Gao,Miguel Gila,María Grandury,Diba Hashemi,Alexander Hoyle,Jiaming Jiang,Mark Klein,Andrei Kucharavy,Anastasiia Kucherenko,Frederike Lübeck,Roman Machacek,Theofilos Manitaras,Andreas Marfurt,Kyle Matoba,Simon Matrenok,Henrique Mendoncça,Fawzi Roberto Mohamed,Syrielle Montariol,Luca Mouchel,Sven Najem-Meyer,Jingwei Ni,Gennaro Oliva,Matteo Pagliardini,Elia Palme,Andrei Panferov,Léo Paoletti,Marco Passerini,Ivan Pavlov,Auguste Poiroux,Kaustubh Ponkshe,Nathan Ranchin,Javi Rando,Mathieu Sauser,Jakhongir Saydaliev,Muhammad Ali Sayfiddinov,Marian Schneider,Stefano Schuppli,Marco Scialanga,Andrei Semenov,Kumar Shridhar,Raghav Singhal,Anna Sotnikova,Alexander Sternfeld,Ayush Kumar Tarun,Paul Teiletche,Jannis Vamvas,Xiaozhe Yao,Hao Zhao Alexander Ilic,Ana Klimovic,Andreas Krause,Caglar Gulcehre,David Rosenthal,Elliott Ash,Florian Tramèr,Joost VandeVondele,Livio Veraldi,Martin Rajman,Thomas Schulthess,Torsten Hoefler,Antoine Bosselut,Martin Jaggi,Imanol Schlag*

Main category: cs.CL

TL;DR: Apertus is a fully open suite of LLMs (8B and 70B scales) trained on compliant multilingual data with strong privacy protections and transparent release of all development artifacts.


<details>
  <summary>Details</summary>
Motivation: Address systemic shortcomings in open LLM ecosystem: data compliance issues and lack of multilingual representation. Many models release weights without reproducible data pipelines or respect for content-owner rights.

Method: Pretrained exclusively on openly available data with robots.txt compliance, filtering for non-permissive/toxic/PII content. Uses Goldfish objective to suppress verbatim recall while maintaining performance. Trained on 15T tokens from 1800+ languages (40% non-English).

Result: Approaches state-of-the-art results among fully open models on multilingual benchmarks, rivaling or surpassing open-weight counterparts.

Conclusion: Provides transparent, compliant multilingual LLMs with full artifact release (weights, data scripts, checkpoints, evaluation suites, training code) enabling audit and extension.

Abstract: We present Apertus, a fully open suite of large language models (LLMs)
designed to address two systemic shortcomings in today's open model ecosystem:
data compliance and multilingual representation. Unlike many prior models that
release weights without reproducible data pipelines or regard for content-owner
rights, Apertus models are pretrained exclusively on openly available data,
retroactively respecting robots.txt exclusions and filtering for
non-permissive, toxic, and personally identifiable content. To mitigate risks
of memorization, we adopt the Goldfish objective during pretraining, strongly
suppressing verbatim recall of data while retaining downstream task
performance. The Apertus models also expand multilingual coverage, training on
15T tokens from over 1800 languages, with ~40% of pretraining data allocated to
non-English content. Released at 8B and 70B scales, Apertus approaches
state-of-the-art results among fully open models on multilingual benchmarks,
rivalling or surpassing open-weight counterparts. Beyond model weights, we
release all scientific artifacts from our development cycle with a permissive
license, including data preparation scripts, checkpoints, evaluation suites,
and training code, enabling transparent audit and extension.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [40] [Proximity-Based Evidence Retrieval for Uncertainty-Aware Neural Networks](https://arxiv.org/abs/2509.13338)
*Hassan Gharoun,Mohammad Sadegh Khorshidi,Kasra Ranjbarigderi,Fang Chen,Amir H. Gandomi*

Main category: cs.CV

TL;DR: Proposes evidence-retrieval mechanism using proximal exemplars and Dempster-Shafer fusion for instance-adaptive uncertainty thresholding, outperforming fixed entropy thresholds with fewer incorrect confident predictions.


<details>
  <summary>Details</summary>
Motivation: To create a more reliable and interpretable uncertainty-aware decision-making system that replaces fixed global cutoffs with instance-adaptive criteria, providing transparent and auditable decisions through explicit supporting evidence.

Method: Retrieves proximal exemplars in embedding space for each test instance, fuses their predictive distributions using Dempster-Shafer theory, and uses the fused belief as a per-instance thresholding mechanism.

Result: Experiments on CIFAR-10/100 with BiT and ViT backbones show higher/comparable performance with significantly fewer confidently incorrect outcomes and sustainable review load compared to entropy thresholding. Few evidences are sufficient for gains.

Conclusion: Evidence-conditioned tagging provides a more reliable and interpretable alternative to fixed prediction entropy thresholds for operational uncertainty-aware decision-making, with explicit supporting evidence enabling transparent decisions.

Abstract: This work proposes an evidence-retrieval mechanism for uncertainty-aware
decision-making that replaces a single global cutoff with an
evidence-conditioned, instance-adaptive criterion. For each test instance,
proximal exemplars are retrieved in an embedding space; their predictive
distributions are fused via Dempster-Shafer theory. The resulting fused belief
acts as a per-instance thresholding mechanism. Because the supporting evidences
are explicit, decisions are transparent and auditable. Experiments on
CIFAR-10/100 with BiT and ViT backbones show higher or comparable
uncertainty-aware performance with materially fewer confidently incorrect
outcomes and a sustainable review load compared with applying threshold on
prediction entropy. Notably, only a few evidences are sufficient to realize
these gains; increasing the evidence set yields only modest changes. These
results indicate that evidence-conditioned tagging provides a more reliable and
interpretable alternative to fixed prediction entropy thresholds for
operational uncertainty-aware decision-making.

</details>


### [41] [Hybrid Quantum-Classical Model for Image Classification](https://arxiv.org/abs/2509.13353)
*Muhammad Adnan Shahzad*

Main category: cs.CV

TL;DR: Hybrid quantum-classical neural networks outperform classical CNNs in accuracy, training speed, and efficiency across MNIST, CIFAR100, and STL10 datasets, with advantages scaling with dataset complexity.


<details>
  <summary>Details</summary>
Motivation: To systematically compare hybrid quantum-classical neural networks with purely classical models to evaluate their performance, efficiency, and robustness across different benchmark datasets.

Method: Conducted experiments comparing hybrid models (integrating parameterized quantum circuits with classical deep learning) against classical CNNs over 50 training epochs on MNIST, CIFAR100, and STL10 datasets, evaluating validation accuracy, test accuracy, training time, computational resources, and adversarial robustness.

Result: Hybrid models achieved higher accuracy (99.38% vs 98.21% on MNIST, 41.69% vs 32.25% on CIFAR100, 74.05% vs 63.76% on STL10), trained 5-12x faster, used 6-32% fewer parameters, consumed less memory (4-5GB vs 5-6GB), and showed better adversarial robustness on simpler datasets.

Conclusion: Hybrid quantum-classical architectures offer significant advantages in accuracy, training efficiency, and parameter scalability, particularly for complex vision tasks, demonstrating their potential as superior alternatives to classical models.

Abstract: This study presents a systematic comparison between hybrid quantum-classical
neural networks and purely classical models across three benchmark datasets
(MNIST, CIFAR100, and STL10) to evaluate their performance, efficiency, and
robustness. The hybrid models integrate parameterized quantum circuits with
classical deep learning architectures, while the classical counterparts use
conventional convolutional neural networks (CNNs). Experiments were conducted
over 50 training epochs for each dataset, with evaluations on validation
accuracy, test accuracy, training time, computational resource usage, and
adversarial robustness (tested with $\epsilon=0.1$ perturbations).Key findings
demonstrate that hybrid models consistently outperform classical models in
final accuracy, achieving {99.38\% (MNIST), 41.69\% (CIFAR100), and 74.05\%
(STL10) validation accuracy, compared to classical benchmarks of 98.21\%,
32.25\%, and 63.76\%, respectively. Notably, the hybrid advantage scales with
dataset complexity, showing the most significant gains on CIFAR100 (+9.44\%)
and STL10 (+10.29\%). Hybrid models also train 5--12$\times$ faster (e.g.,
21.23s vs. 108.44s per epoch on MNIST) and use 6--32\% fewer parameters} while
maintaining superior generalization to unseen test data.Adversarial robustness
tests reveal that hybrid models are significantly more resilient on simpler
datasets (e.g., 45.27\% robust accuracy on MNIST vs. 10.80\% for classical) but
show comparable fragility on complex datasets like CIFAR100 ($\sim$1\%
robustness for both). Resource efficiency analyses indicate that hybrid models
consume less memory (4--5GB vs. 5--6GB for classical) and lower CPU utilization
(9.5\% vs. 23.2\% on average).These results suggest that hybrid
quantum-classical architectures offer compelling advantages in accuracy,
training efficiency, and parameter scalability, particularly for complex vision
tasks.

</details>


### [42] [Research on Expressway Congestion Warning Technology Based on YOLOv11-DIoU and GRU-Attention](https://arxiv.org/abs/2509.13361)
*Tong Yulin,Liang Xuechen*

Main category: cs.CV

TL;DR: This paper proposes an integrated framework for expressway traffic congestion management that improves vehicle detection accuracy under occlusion and enhances congestion prediction using optimized YOLOv11-DIoU and DeepSort algorithms combined with a GRU-Attention model.


<details>
  <summary>Details</summary>
Motivation: Expressway traffic congestion reduces travel efficiency and regional connectivity. Existing systems have low vehicle perception accuracy under occlusion and lose long-sequence dependencies in congestion forecasting.

Method: Optimized YOLOv11 to YOLOv11-DIoU by replacing GIoU Loss with DIoU Loss, improved DeepSort by fusing Mahalanobis and cosine distances, used Greenberg model for traffic flow analysis, and built GRU-Attention model for congestion prediction.

Result: YOLOv11-DIoU achieved 95.7% mAP (6.5% higher than baseline) with 5.3% occlusion miss rate. DeepSort reached 93.8% MOTA (11.3% higher than SORT). GRU-Attention model achieved 99.7% test accuracy with ≤1 minute time error in 10-minute advance warnings. Validation showed 95% warning accuracy and over 90% spatial overlap.

Conclusion: The framework provides quantitative support for expressway congestion control with promising applications in intelligent transportation systems, demonstrating significant improvements in both vehicle perception and congestion forecasting accuracy.

Abstract: Expressway traffic congestion severely reduces travel efficiency and hinders
regional connectivity. Existing "detection-prediction" systems have critical
flaws: low vehicle perception accuracy under occlusion and loss of
long-sequence dependencies in congestion forecasting. This study proposes an
integrated technical framework to resolve these issues.For traffic flow
perception, two baseline algorithms were optimized. Traditional YOLOv11 was
upgraded to YOLOv11-DIoU by replacing GIoU Loss with DIoU Loss, and DeepSort
was improved by fusing Mahalanobis (motion) and cosine (appearance) distances.
Experiments on Chang-Shen Expressway videos showed YOLOv11-DIoU achieved 95.7\%
mAP (6.5 percentage points higher than baseline) with 5.3\% occlusion miss
rate. DeepSort reached 93.8\% MOTA (11.3 percentage points higher than SORT)
with only 4 ID switches. Using the Greenberg model (for 10-15 vehicles/km
high-density scenarios), speed and density showed a strong negative correlation
(r=-0.97), conforming to traffic flow theory. For congestion warning, a
GRU-Attention model was built to capture congestion precursors. Trained 300
epochs with flow, density, and speed, it achieved 99.7\% test accuracy (7-9
percentage points higher than traditional GRU). In 10-minute advance warnings
for 30-minute congestion, time error was $\leq$ 1 minute. Validation with an
independent video showed 95\% warning accuracy, over 90\% spatial overlap of
congestion points, and stable performance in high-flow ($>$5 vehicles/second)
scenarios.This framework provides quantitative support for expressway
congestion control, with promising intelligent transportation applications.

</details>


### [43] [Parking Space Ground Truth Test Automation by Artificial Intelligence Using Convolutional Neural Networks](https://arxiv.org/abs/2509.13366)
*Tony Rohe,Martin Margreiter,Markus Moertl*

Main category: cs.CV

TL;DR: This paper presents an automated system using convolutional neural networks to optimize real-time parking detection services by reducing human engineering work by 99.58% through image pattern recognition.


<details>
  <summary>Details</summary>
Motivation: To optimize cloud-based on-street parking service quality by automating ground truth testing processes and reducing human engineering workload in analyzing crowd-sourced ultrasonic sensor data.

Method: Applied machine learning methods, specifically convolutional neural networks for image pattern recognition, to automate the analysis process and enrich the parking detection database.

Result: Achieved 99.58% reduction in human resource time requirements while maintaining high performance levels in parking spot classification.

Conclusion: The automation tool successfully optimized parking service quality and demonstrated significant efficiency improvements, with potential for future development and broader application in analysis automation.

Abstract: This research is part of a study of a real-time, cloud-based on-street
parking service using crowd-sourced in-vehicle fleet data. The service provides
real-time information about available parking spots by classifying
crowd-sourced detections observed via ultrasonic sensors. The goal of this
research is to optimize the current parking service quality by analyzing the
automation of the existing test process for ground truth tests. Therefore,
methods from the field of machine learning, especially image pattern
recognition, are applied to enrich the database and substitute human
engineering work in major areas of the analysis process. After an introduction
into the related areas of machine learning, this paper explains the methods and
implementations made to achieve a high level of automation, applying
convolutional neural networks. Finally, predefined metrics present the
performance level achieved, showing a time reduction of human resources up to
99.58 %. The overall improvements are discussed, summarized, and followed by an
outlook for future development and potential application of the analysis
automation tool.

</details>


### [44] [An Empirical Analysis of VLM-based OOD Detection: Mechanisms, Advantages, and Sensitivity](https://arxiv.org/abs/2509.13375)
*Yuxiao Lee,Xiaofeng Cao,Wei Ye,Jiangchao Yao,Jingkuan Song,Heng Tao Shen*

Main category: cs.CV

TL;DR: This paper systematically analyzes Vision-Language Models (VLMs) for zero-shot out-of-distribution detection, revealing their mechanisms, advantages over single-modal methods, and sensitivity to prompt phrasing.


<details>
  <summary>Details</summary>
Motivation: Despite VLMs like CLIP showing strong zero-shot OOD detection capabilities, there's limited understanding of why they work so well, their advantages over single-modal methods, and their behavioral robustness.

Method: The authors conducted systematic empirical analysis using in-distribution and OOD prompts to characterize VLM embedding space properties, compare with single-modal approaches, and test robustness to image noise and prompt variations.

Result: VLMs leverage rich semantic novelty for superior OOD detection, show resilience to image noise but significant sensitivity to prompt phrasing, revealing an asymmetry in robustness profiles.

Conclusion: The study provides structured understanding of VLM strengths and vulnerabilities in OOD detection, offering empirically-grounded guidance for developing more robust and reliable AI systems.

Abstract: Vision-Language Models (VLMs), such as CLIP, have demonstrated remarkable
zero-shot out-of-distribution (OOD) detection capabilities, vital for reliable
AI systems. Despite this promising capability, a comprehensive understanding of
(1) why they work so effectively, (2) what advantages do they have over
single-modal methods, and (3) how is their behavioral robustness -- remains
notably incomplete within the research community. This paper presents a
systematic empirical analysis of VLM-based OOD detection using in-distribution
(ID) and OOD prompts. (1) Mechanisms: We systematically characterize and
formalize key operational properties within the VLM embedding space that
facilitate zero-shot OOD detection. (2) Advantages: We empirically quantify the
superiority of these models over established single-modal approaches,
attributing this distinct advantage to the VLM's capacity to leverage rich
semantic novelty. (3) Sensitivity: We uncovers a significant and previously
under-explored asymmetry in their robustness profile: while exhibiting
resilience to common image noise, these VLM-based methods are highly sensitive
to prompt phrasing. Our findings contribute a more structured understanding of
the strengths and critical vulnerabilities inherent in VLM-based OOD detection,
offering crucial, empirically-grounded guidance for developing more robust and
reliable future designs.

</details>


### [45] [Curvature as a tool for evaluating dimensionality reduction and estimating intrinsic dimension](https://arxiv.org/abs/2509.13385)
*Charlotte Beylier,Parvaneh Joharinad,Jürgen Jost,Nahid Torbati*

Main category: cs.CV

TL;DR: A method using abstract sectional curvature concepts to create geometric profiles of discrete metric spaces, enabling evaluation of data representation effectiveness and intrinsic dimensionality estimation.


<details>
  <summary>Details</summary>
Motivation: To develop a curvature-based approach for analyzing discrete metric spaces and evaluating the quality of data representations from dimensionality reduction techniques.

Method: Utilizes abstract notions of sectional curvature to capture metric relations between triples of points and construct curvature profiles for discrete metric spaces.

Result: The curvature-based analysis can estimate intrinsic dimensionality of datasets and explore large-scale geometry of empirical networks.

Conclusion: This curvature profile method provides a quantitative measure for assessing data representation effectiveness and offers insights into dataset geometry and dimensionality reduction performance.

Abstract: Utilizing recently developed abstract notions of sectional curvature, we
introduce a method for constructing a curvature-based geometric profile of
discrete metric spaces. The curvature concept that we use here captures the
metric relations between triples of points and other points. More
significantly, based on this curvature profile, we introduce a quantitative
measure to evaluate the effectiveness of data representations, such as those
produced by dimensionality reduction techniques. Furthermore, Our experiments
demonstrate that this curvature-based analysis can be employed to estimate the
intrinsic dimensionality of datasets. We use this to explore the large-scale
geometry of empirical networks and to evaluate the effectiveness of
dimensionality reduction techniques.

</details>


### [46] [Landcover classification and change detection using remote sensing and machine learning: a case study of Western Fiji](https://arxiv.org/abs/2509.13388)
*Yadvendra Gurjar,Ruoni Wan,Ehsan Farahbakhsh,Rohitash Chandra*

Main category: cs.CV

TL;DR: Machine learning and remote sensing analysis of land use/land cover changes in Nadi, Fiji from 2013-2024 using Landsat-8 imagery, Google Earth Engine, and CNN classification to monitor urbanization impacts.


<details>
  <summary>Details</summary>
Motivation: Fiji is experiencing rapid urbanization with massive development projects, creating a need for technical support in land cover modeling and change detection to monitor urban growth.

Method: Used Landsat-8 satellite imagery with supervised machine learning training datasets, Google Earth Engine for processing, k-means clustering for unsupervised land cover mapping, and convolutional neural networks for land cover classification.

Result: Developed visualization of change detection showing urban area changes over time, providing a monitoring framework for land cover transformations in the study region.

Conclusion: The study successfully demonstrates a machine learning and remote sensing framework for tracking urbanization patterns in Fiji, offering valuable technical support for land use monitoring and change detection in developing regions.

Abstract: As a developing country, Fiji is facing rapid urbanisation, which is visible
in the massive development projects that include housing, roads, and civil
works. In this study, we present machine learning and remote sensing frameworks
to compare land use and land cover change from 2013 to 2024 in Nadi, Fiji. The
ultimate goal of this study is to provide technical support in land cover/land
use modelling and change detection. We used Landsat-8 satellite image for the
study region and created our training dataset with labels for supervised
machine learning. We used Google Earth Engine and unsupervised machine learning
via k-means clustering to generate the land cover map. We used convolutional
neural networks to classify the selected regions' land cover types. We present
a visualisation of change detection, highlighting urban area changes over time
to monitor changes in the map.

</details>


### [47] [Real-Time Detection and Tracking of Foreign Object Intrusions in Power Systems via Feature-Based Edge Intelligence](https://arxiv.org/abs/2509.13396)
*Xinan Wang,Di Shi,Fengyu Wang*

Main category: cs.CV

TL;DR: A three-stage framework for real-time foreign object intrusion detection and tracking in power systems using YOLOv7 segmentation, ConvNeXt feature extraction, and feature-assisted IoU tracking, optimized for edge deployment.


<details>
  <summary>Details</summary>
Motivation: To develop a robust and scalable system for detecting and tracking foreign objects in power transmission systems that can operate in real-time on low-cost edge hardware without requiring model retraining for new objects.

Method: Three-stage framework: 1) YOLOv7 segmentation for object localization, 2) ConvNeXt-based feature extractor with triplet loss for discriminative embeddings, 3) Feature-assisted IoU tracker for resilient multi-object tracking under occlusion and motion. Uses mixed-precision inference for edge optimization.

Result: High accuracy and robustness demonstrated across diverse FOI scenarios on real-world surveillance and drone video datasets. Hardware benchmarks confirm practicality on NVIDIA Jetson devices.

Conclusion: The framework provides an effective solution for real-time foreign object intrusion detection and tracking that is scalable, deployable on edge hardware, and supports incremental updates without retraining.

Abstract: This paper presents a novel three-stage framework for real-time foreign
object intrusion (FOI) detection and tracking in power transmission systems.
The framework integrates: (1) a YOLOv7 segmentation model for fast and robust
object localization, (2) a ConvNeXt-based feature extractor trained with
triplet loss to generate discriminative embeddings, and (3) a feature-assisted
IoU tracker that ensures resilient multi-object tracking under occlusion and
motion. To enable scalable field deployment, the pipeline is optimized for
deployment on low-cost edge hardware using mixed-precision inference. The
system supports incremental updates by adding embeddings from previously unseen
objects into a reference database without requiring model retraining. Extensive
experiments on real-world surveillance and drone video datasets demonstrate the
framework's high accuracy and robustness across diverse FOI scenarios. In
addition, hardware benchmarks on NVIDIA Jetson devices confirm the framework's
practicality and scalability for real-world edge applications.

</details>


### [48] [EdiVal-Agent: An Object-Centric Framework for Automated, Scalable, Fine-Grained Evaluation of Multi-Turn Editing](https://arxiv.org/abs/2509.13399)
*Tianyu Chen,Yasi Zhang,Zhi Zhang,Peiyu Yu,Shu Wang,Zhendong Wang,Kevin Lin,Xiaofei Wang,Zhengyuan Yang,Linjie Li,Chung-Ching Lin,Jianwen Xie,Oscar Leong,Lijuan Wang,Ying Nian Wu,Mingyuan Zhou*

Main category: cs.CV

TL;DR: EdiVal-Agent is an automated evaluation framework for instruction-based image editing that combines VLMs with object detectors for better alignment with human judgments, addressing limitations of current evaluation methods.


<details>
  <summary>Details</summary>
Motivation: Current evaluation methods for instruction-based image editing either rely on limited paired reference images or imprecise zero-shot VLMs, creating a bottleneck for reliable and interpretable assessment.

Method: The framework decomposes images into semantic objects, synthesizes diverse editing instructions, and integrates VLMs with object detectors for instruction following assessment, semantic feature extractors for content consistency, and human preference models for visual quality.

Result: Combining VLMs with object detectors shows stronger agreement with human judgments compared to using VLMs alone or CLIP-based metrics. The modular design allows seamless integration of future tools.

Conclusion: EdiVal-Agent provides a scalable, fine-grained evaluation approach that can identify failure modes in editing models and inform development of next-generation editing systems, as demonstrated through EdiVal-Bench covering 9 instruction types and 11 SOTA models.

Abstract: Instruction-based image editing has advanced rapidly, yet reliable and
interpretable evaluation remains a bottleneck. Current protocols either (i)
depend on paired reference images -- resulting in limited coverage and
inheriting biases from prior generative models -- or (ii) rely solely on
zero-shot vision-language models (VLMs), whose prompt-based assessments of
instruction following, content consistency, and visual quality are often
imprecise.
  To address this, we introduce EdiVal-Agent, an automated, scalable, and
fine-grained evaluation framework for multi-turn instruction-based editing from
an object-centric perspective, supported by a suite of expert tools. Given an
image, EdiVal-Agent first decomposes it into semantically meaningful objects,
then synthesizes diverse, context-aware editing instructions. For evaluation,
it integrates VLMs with open-vocabulary object detectors to assess instruction
following, uses semantic-level feature extractors to evaluate content
consistency, and leverages human preference models to judge visual quality. We
show that combining VLMs with object detectors yields stronger agreement with
human judgments in instruction-following evaluation compared to using VLMs
alone and CLIP-based metrics. Furthermore, the pipeline's modular design allows
future tools to be seamlessly integrated, enhancing evaluation accuracy over
time.
  Instantiating this pipeline, we build EdiVal-Bench, a multi-turn editing
benchmark covering 9 instruction types and 11 state-of-the-art editing models
spanning autoregressive (AR) (including Nano Banana, GPT-Image-1),
flow-matching, and diffusion paradigms. We demonstrate that EdiVal-Agent can be
used to identify existing failure modes, thereby informing the development of
the next generation of editing models. Project page:
https://tianyucodings.github.io/EdiVAL-page/.

</details>


### [49] [MapAnything: Universal Feed-Forward Metric 3D Reconstruction](https://arxiv.org/abs/2509.13414)
*Nikhil Keetha,Norman Müller,Johannes Schönberger,Lorenzo Porzi,Yuchen Zhang,Tobias Fischer,Arno Knapitsch,Duncan Zauss,Ethan Weber,Nelson Antunes,Jonathon Luiten,Manuel Lopez-Antequera,Samuel Rota Bulò,Christian Richardt,Deva Ramanan,Sebastian Scherer,Peter Kontschieder*

Main category: cs.CV

TL;DR: MapAnything is a unified transformer-based model that processes images and optional geometric inputs to directly output metric 3D scene geometry and cameras, outperforming specialized models across multiple 3D vision tasks.


<details>
  <summary>Details</summary>
Motivation: To create a universal 3D reconstruction backbone that can handle diverse 3D vision tasks in a single feed-forward pass, eliminating the need for specialized models for each individual task.

Method: Uses a factored representation of multi-view scene geometry (depth maps, local ray maps, camera poses, metric scale factor) with transformer architecture. Standardizes supervision across diverse datasets and employs flexible input augmentation to handle various 3D vision tasks.

Result: Outperforms or matches specialist feed-forward models across multiple tasks including uncalibrated structure-from-motion, multi-view stereo, monocular depth estimation, camera localization, and depth completion.

Conclusion: MapAnything demonstrates efficient joint training behavior and serves as a universal 3D reconstruction backbone, paving the way for unified approaches to diverse 3D vision problems.

Abstract: We introduce MapAnything, a unified transformer-based feed-forward model that
ingests one or more images along with optional geometric inputs such as camera
intrinsics, poses, depth, or partial reconstructions, and then directly
regresses the metric 3D scene geometry and cameras. MapAnything leverages a
factored representation of multi-view scene geometry, i.e., a collection of
depth maps, local ray maps, camera poses, and a metric scale factor that
effectively upgrades local reconstructions into a globally consistent metric
frame. Standardizing the supervision and training across diverse datasets,
along with flexible input augmentation, enables MapAnything to address a broad
range of 3D vision tasks in a single feed-forward pass, including uncalibrated
structure-from-motion, calibrated multi-view stereo, monocular depth
estimation, camera localization, depth completion, and more. We provide
extensive experimental analyses and model ablations demonstrating that
MapAnything outperforms or matches specialist feed-forward models while
offering more efficient joint training behavior, thus paving the way toward a
universal 3D reconstruction backbone.

</details>


### [50] [Semantic-Enhanced Cross-Modal Place Recognition for Robust Robot Localization](https://arxiv.org/abs/2509.13474)
*Yujia Lin,Nicholas Evans*

Main category: cs.CV

TL;DR: SCM-PR is a cross-modal place recognition framework that combines RGB images and LiDAR maps using semantic information to achieve robust robot localization in GPS-denied environments, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing RGB-based VPR methods are sensitive to illumination, weather, and seasonal changes, while current cross-modal methods struggle with complex scenes, fine-grained matching, and viewpoint changes.

Method: Uses VMamba backbone for RGB feature extraction, Semantic-Aware Feature Fusion module, LiDAR descriptors with semantics and geometry, cross-modal semantic attention in NetVLAD, Multi-View Semantic-Geometric Matching, and Semantic Consistency Loss in contrastive learning framework.

Result: Achieves state-of-the-art performance on KITTI and KITTI-360 datasets compared to other cross-modal place recognition methods.

Conclusion: Incorporating semantic information significantly improves cross-modal place recognition robustness and performance in challenging environmental conditions.

Abstract: Ensuring accurate localization of robots in environments without GPS
capability is a challenging task. Visual Place Recognition (VPR) techniques can
potentially achieve this goal, but existing RGB-based methods are sensitive to
changes in illumination, weather, and other seasonal changes. Existing
cross-modal localization methods leverage the geometric properties of RGB
images and 3D LiDAR maps to reduce the sensitivity issues highlighted above.
Currently, state-of-the-art methods struggle in complex scenes, fine-grained or
high-resolution matching, and situations where changes can occur in viewpoint.
In this work, we introduce a framework we call Semantic-Enhanced Cross-Modal
Place Recognition (SCM-PR) that combines high-level semantics utilizing RGB
images for robust localization in LiDAR maps. Our proposed method introduces: a
VMamba backbone for feature extraction of RGB images; a Semantic-Aware Feature
Fusion (SAFF) module for using both place descriptors and segmentation masks;
LiDAR descriptors that incorporate both semantics and geometry; and a
cross-modal semantic attention mechanism in NetVLAD to improve matching.
Incorporating the semantic information also was instrumental in designing a
Multi-View Semantic-Geometric Matching and a Semantic Consistency Loss, both in
a contrastive learning framework. Our experimental work on the KITTI and
KITTI-360 datasets show that SCM-PR achieves state-of-the-art performance
compared to other cross-modal place recognition methods.

</details>


### [51] [Improving 3D Gaussian Splatting Compression by Scene-Adaptive Lattice Vector Quantization](https://arxiv.org/abs/2509.13482)
*Hao Xu,Xiaolin Wu,Xi Zhang*

Main category: cs.CV

TL;DR: SALVQ replaces uniform scalar quantization with scene-adaptive lattice vector quantization to improve 3DGS compression efficiency with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting generates massive data requiring compression, but existing methods rely on simple uniform scalar quantization which may not be optimal.

Method: Scene-adaptive lattice vector quantization (SALVQ) with optimized lattice basis per scene, enabling dynamic bit rate adjustment through lattice scaling.

Result: Improved rate-distortion performance with minimal system changes and computational overhead, eliminating need for separate models per compression level.

Conclusion: SALVQ provides an efficient quantization upgrade for 3DGS compression, balancing vector quantization benefits with low complexity.

Abstract: 3D Gaussian Splatting (3DGS) is rapidly gaining popularity for its
photorealistic rendering quality and real-time performance, but it generates
massive amounts of data. Hence compressing 3DGS data is necessary for the cost
effectiveness of 3DGS models. Recently, several anchor-based neural compression
methods have been proposed, achieving good 3DGS compression performance.
However, they all rely on uniform scalar quantization (USQ) due to its
simplicity. A tantalizing question is whether more sophisticated quantizers can
improve the current 3DGS compression methods with very little extra overhead
and minimal change to the system. The answer is yes by replacing USQ with
lattice vector quantization (LVQ). To better capture scene-specific
characteristics, we optimize the lattice basis for each scene, improving LVQ's
adaptability and R-D efficiency. This scene-adaptive LVQ (SALVQ) strikes a
balance between the R-D efficiency of vector quantization and the low
complexity of USQ. SALVQ can be seamlessly integrated into existing 3DGS
compression architectures, enhancing their R-D performance with minimal
modifications and computational overhead. Moreover, by scaling the lattice
basis vectors, SALVQ can dynamically adjust lattice density, enabling a single
model to accommodate multiple bit rate targets. This flexibility eliminates the
need to train separate models for different compression levels, significantly
reducing training time and memory consumption.

</details>


### [52] [MINGLE: VLMs for Semantically Complex Region Detection in Urban Scenes](https://arxiv.org/abs/2509.13484)
*Liu Liu,Alexandra Kudaeva,Marco Cipriano,Fatimeh Al Ghannam,Freya Tan,Gerard de Melo,Andres Sevtsuk*

Main category: cs.CV

TL;DR: MINGLE is a three-stage pipeline for detecting social group regions in images by analyzing interpersonal relations, proximity, and co-movement, supported by a new 100K image dataset.


<details>
  <summary>Details</summary>
Motivation: Understanding group-level social interactions is crucial for urban planning and designing socially vibrant environments, but detecting these interactions requires interpreting complex visual cues beyond traditional object detection.

Method: A modular three-stage pipeline: (1) off-the-shelf human detection and depth estimation, (2) VLM-based reasoning to classify pairwise social affiliation, and (3) lightweight spatial aggregation algorithm to localize socially connected groups.

Result: Developed a new dataset of 100K urban street-view images with bounding boxes and labels for individuals and social groups, combining human annotations and MINGLE outputs for semantic richness and broad coverage.

Conclusion: MINGLE provides an effective framework for detecting social group regions from images, enabling better understanding of social interactions in public spaces for urban planning applications.

Abstract: Understanding group-level social interactions in public spaces is crucial for
urban planning, informing the design of socially vibrant and inclusive
environments. Detecting such interactions from images involves interpreting
subtle visual cues such as relations, proximity, and co-movement - semantically
complex signals that go beyond traditional object detection. To address this
challenge, we introduce a social group region detection task, which requires
inferring and spatially grounding visual regions defined by abstract
interpersonal relations. We propose MINGLE (Modeling INterpersonal Group-Level
Engagement), a modular three-stage pipeline that integrates: (1) off-the-shelf
human detection and depth estimation, (2) VLM-based reasoning to classify
pairwise social affiliation, and (3) a lightweight spatial aggregation
algorithm to localize socially connected groups. To support this task and
encourage future research, we present a new dataset of 100K urban street-view
images annotated with bounding boxes and labels for both individuals and
socially interacting groups. The annotations combine human-created labels and
outputs from the MINGLE pipeline, ensuring semantic richness and broad coverage
of real-world scenarios.

</details>


### [53] [BiasMap: Leveraging Cross-Attentions to Discover and Mitigate Hidden Social Biases in Text-to-Image Generation](https://arxiv.org/abs/2509.13496)
*Rajatsubhra Chakraborty,Xujun Che,Depeng Xu,Cori Faklaris,Xi Niu,Shuhan Yuan*

Main category: cs.CV

TL;DR: BiasMap is a framework that discovers and mitigates latent concept-level representational biases in text-to-image models using cross-attention attribution maps and energy-guided diffusion sampling.


<details>
  <summary>Details</summary>
Motivation: Existing bias discovery methods focus on output-level demographic distributions but fail to address deeper concept-level entanglements between demographics and semantics in text-to-image models.

Method: Leverages cross-attention attribution maps to quantify spatial demographics-semantics concept entanglement via Intersection over Union (IoU), and uses energy-guided diffusion sampling to minimize expected SoftIoU during denoising.

Result: Shows that existing fairness interventions reduce output distribution gaps but fail to disentangle concept-level coupling, while BiasMap effectively mitigates concept entanglement.

Conclusion: BiasMap provides a deeper lens into hidden representational biases and offers effective mitigation that complements existing distributional bias reduction approaches.

Abstract: Bias discovery is critical for black-box generative models, especiall
text-to-image (TTI) models. Existing works predominantly focus on output-level
demographic distributions, which do not necessarily guarantee concept
representations to be disentangled post-mitigation. We propose BiasMap, a
model-agnostic framework for uncovering latent concept-level representational
biases in stable diffusion models. BiasMap leverages cross-attention
attribution maps to reveal structural entanglements between demographics (e.g.,
gender, race) and semantics (e.g., professions), going deeper into
representational bias during the image generation. Using attribution maps of
these concepts, we quantify the spatial demographics-semantics concept
entanglement via Intersection over Union (IoU), offering a lens into bias that
remains hidden in existing fairness discovery approaches. In addition, we
further utilize BiasMap for bias mitigation through energy-guided diffusion
sampling that directly modifies latent noise space and minimizes the expected
SoftIoU during the denoising process. Our findings show that existing fairness
interventions may reduce the output distributional gap but often fail to
disentangle concept-level coupling, whereas our mitigation method can mitigate
concept entanglement in image generation while complementing distributional
bias mitigation.

</details>


### [54] [LivePyxel: Accelerating image annotations with a Python-integrated webcam live streaming](https://arxiv.org/abs/2509.13504)
*Uriel Garcilazo-Cruz,Joseph O. Okeme,Rodrigo A. Vargas--Hernández*

Main category: cs.CV

TL;DR: LivePyxel is a Python-based GUI tool that enables real-time image annotation directly from imaging devices like microscopes and webcams, eliminating the need for pre-collected datasets and supporting on-demand AI model development.


<details>
  <summary>Details</summary>
Motivation: Existing image annotation tools require uploading pre-collected datasets, which limits real-time data acquisition from laboratory instruments and introduces unnecessary steps in scientific workflows.

Method: Developed a Python-based graphical interface that integrates with various imaging systems, featuring Bezier splines, binary masks, non-destructive layers, and leveraging OpenCV with Numpy for high-performance object detection operations.

Result: LivePyxel provides seamless real-time annotation capabilities with wide device compatibility, precise annotation tools, and optimized performance for matrix operations.

Conclusion: The tool accelerates AI model development in experimental workflows by facilitating direct data collection and labeling from live imaging sources, making it freely available as open-source software.

Abstract: The lack of flexible annotation tools has hindered the deployment of AI
models in some scientific areas. Most existing image annotation software
requires users to upload a precollected dataset, which limits support for
on-demand pipelines and introduces unnecessary steps to acquire images. This
constraint is particularly problematic in laboratory environments, where
real-time data acquisition from instruments such as microscopes is increasingly
common. In this work, we introduce \texttt{LivePixel}, a Python-based graphical
user interface that integrates with imaging systems, such as webcams,
microscopes, and others, to enable real-time image annotation. LivePyxel is
designed to be easy to use through a simple interface that allows users to
precisely delimit areas for annotation using tools commonly found in commercial
graphics editing software. Of particular interest is the availability of
B\'ezier splines and binary masks, and the software's capacity to work with
non-destructive layers that enable high-performance editing. LivePyxel also
integrates a wide compatibility across video devices, and it's optimized for
object detection operations via the use of OpenCV in combination with
high-performance libraries designed to handle matrix and linear algebra
operations via Numpy effectively. LivePyxel facilitates seamless data
collection and labeling, accelerating the development of AI models in
experimental workflows. LivePyxel freely available at
https://github.com/UGarCil/LivePyxel

</details>


### [55] [DEFT-VTON: Efficient Virtual Try-On with Consistent Generalised H-Transform](https://arxiv.org/abs/2509.13506)
*Xingzi Xu,Qi Li,Shuwen Qiu,Julien Han,Karim Bouyarmane*

Main category: cs.CV

TL;DR: DEFT-VTON applies Doob's h-transform efficient fine-tuning to adapt pre-trained diffusion models for virtual try-on, reducing training parameters to 1.42% and enabling fast inference with only 15 denoising steps while achieving SOTA performance.


<details>
  <summary>Details</summary>
Motivation: Real-world VTO applications require limited training/inference budgets, but current methods involve extensive end-to-end training of large pre-trained models, creating deployment obstacles.

Method: Freezes pre-trained model parameters and trains a small h-transform network to learn conditional h-transform. Adds adaptive consistency loss combining consistency loss and denoising score matching loss for efficient fine-tuning.

Result: Achieves state-of-the-art performance on VTO tasks with only 1.42% parameter training (vs 5.52% in traditional PEFT) and as few as 15 denoising steps while maintaining competitive results.

Conclusion: DEFT-VTON provides an efficient solution for adapting large pre-trained models to VTO tasks with minimal parameter training and fast inference, making it practical for real-world deployment.

Abstract: Diffusion models enable high-quality virtual try-on (VTO) with their
established image synthesis abilities. Despite the extensive end-to-end
training of large pre-trained models involved in current VTO methods,
real-world applications often prioritize limited training and inference,
serving, and deployment budgets for VTO. To solve this obstacle, we apply
Doob's h-transform efficient fine-tuning (DEFT) for adapting large pre-trained
unconditional models for downstream image-conditioned VTO abilities. DEFT
freezes the pre-trained model's parameters and trains a small h-transform
network to learn a conditional h-transform. The h-transform network allows
training only 1.42 percent of the frozen parameters, compared to a baseline of
5.52 percent in traditional parameter-efficient fine-tuning (PEFT).
  To further improve DEFT's performance and decrease existing models' inference
time, we additionally propose an adaptive consistency loss. Consistency
training distills slow but high-performing diffusion models into a fast one
while retaining performance by enforcing consistencies along the inference
path. Inspired by constrained optimization, instead of distillation, we combine
the consistency loss and the denoising score matching loss in a data-adaptive
manner for fine-tuning existing VTO models at a low cost. Empirical results
show the proposed DEFT-VTON method achieves state-of-the-art performance on VTO
tasks, with as few as 15 denoising steps, while maintaining competitive
results.

</details>


### [56] [Adversarial Appearance Learning in Augmented Cityscapes for Pedestrian Recognition in Autonomous Driving](https://arxiv.org/abs/2509.13507)
*Artem Savkin,Thomas Lapotre,Kevin Strauss,Uzair Akbar,Federico Tombari*

Main category: cs.CV

TL;DR: Data augmentation pipeline for generating synthetic VRU scenarios to improve pedestrian recognition in autonomous driving, using novel generative adversarial network for lighting realism.


<details>
  <summary>Details</summary>
Motivation: Synthetic data is crucial for covering specific traffic scenarios in autonomous driving, but introduces domain gap between synthetic and real domains. Need to improve pedestrian recognition through realistic data augmentation.

Method: Deploy data augmentation to generate custom traffic scenarios with VRUs, provide pipeline for augmenting Cityscapes dataset with virtual pedestrians, and develop novel generative network architecture for adversarial learning of dataset lighting conditions to improve realism.

Result: Approach evaluated on semantic and instance segmentation tasks, demonstrating improved pedestrian recognition through realistic synthetic data generation.

Conclusion: The proposed data augmentation pipeline with adversarial lighting learning effectively bridges the domain gap between synthetic and real data, enhancing pedestrian recognition performance for autonomous driving applications.

Abstract: In the autonomous driving area synthetic data is crucial for cover specific
traffic scenarios which autonomous vehicle must handle. This data commonly
introduces domain gap between synthetic and real domains. In this paper we
deploy data augmentation to generate custom traffic scenarios with VRUs in
order to improve pedestrian recognition. We provide a pipeline for augmentation
of the Cityscapes dataset with virtual pedestrians. In order to improve
augmentation realism of the pipeline we reveal a novel generative network
architecture for adversarial learning of the data-set lighting conditions. We
also evaluate our approach on the tasks of semantic and instance segmentation.

</details>


### [57] [FunKAN: Functional Kolmogorov-Arnold Network for Medical Image Enhancement and Segmentation](https://arxiv.org/abs/2509.13508)
*Maksim Penkin,Andrey Krylov*

Main category: cs.CV

TL;DR: FunKAN is a novel interpretable neural framework that generalizes Kolmogorov-Arnold networks for medical image processing, outperforming other KAN-based methods in both enhancement and segmentation tasks across diverse medical datasets.


<details>
  <summary>Details</summary>
Motivation: Traditional deep learning approaches for medical image processing lack interpretability, and existing Kolmogorov-Arnold networks disrupt spatial structure by flattening features. There's a need for interpretable solutions that preserve spatial information in medical imaging.

Method: Proposed Functional Kolmogorov-Arnold Network (FunKAN) that generalizes the Kolmogorov-Arnold representation theorem to functional spaces, learning inner functions using Fourier decomposition over Hermite functions. Also developed U-FunKAN for medical segmentation.

Result: Outperformed other KAN-based backbones in medical image enhancement (PSNR, TV metrics on IXI dataset for Gibbs ringing suppression) and segmentation (IoU, F1 scores on BUSI, GlaS, and CVC-ClinicDB datasets for breast cancer, glands, and polyps detection).

Conclusion: FunKAN bridges theoretical function approximation with medical image analysis, providing a robust and interpretable solution suitable for clinical applications while preserving spatial structure in imaging data.

Abstract: Medical image enhancement and segmentation are critical yet challenging tasks
in modern clinical practice, constrained by artifacts and complex anatomical
variations. Traditional deep learning approaches often rely on complex
architectures with limited interpretability. While Kolmogorov-Arnold networks
offer interpretable solutions, their reliance on flattened feature
representations fundamentally disrupts the intrinsic spatial structure of
imaging data. To address this issue we propose a Functional Kolmogorov-Arnold
Network (FunKAN) -- a novel interpretable neural framework, designed
specifically for image processing, that formally generalizes the
Kolmogorov-Arnold representation theorem onto functional spaces and learns
inner functions using Fourier decomposition over the basis Hermite functions.
We explore FunKAN on several medical image processing tasks, including Gibbs
ringing suppression in magnetic resonance images, benchmarking on IXI dataset.
We also propose U-FunKAN as state-of-the-art binary medical segmentation model
with benchmarks on three medical datasets: BUSI (ultrasound images), GlaS
(histological structures) and CVC-ClinicDB (colonoscopy videos), detecting
breast cancer, glands and polyps, respectively. Experiments on those diverse
datasets demonstrate that our approach outperforms other KAN-based backbones in
both medical image enhancement (PSNR, TV) and segmentation (IoU, F1). Our work
bridges the gap between theoretical function approximation and medical image
analysis, offering a robust, interpretable solution for clinical applications.

</details>


### [58] [Multimodal Hate Detection Using Dual-Stream Graph Neural Networks](https://arxiv.org/abs/2509.13515)
*Jiangbei Yue,Shuonan Yang,Tailin Chen,Jianbo Jiao,Zeyu Fu*

Main category: cs.CV

TL;DR: A multimodal dual-stream graph neural network that separates videos into instances, extracts features, assigns importance weights to highlight hateful content, and achieves state-of-the-art hateful video classification with strong explainability.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal hateful video detection methods treat all content uniformly instead of emphasizing hateful components, and cannot systematically capture structured information in videos, limiting fusion effectiveness.

Method: Proposes a dual-stream GNN that constructs instance graphs by separating videos into instances, extracts instance-level features, then uses a complementary weight graph to assign importance weights highlighting hateful instances before combining for classification.

Result: Extensive experiments on public datasets show the model achieves state-of-the-art performance in hateful video classification and demonstrates strong explainability.

Conclusion: The proposed multimodal dual-stream GNN effectively addresses limitations of existing methods by focusing on hateful instances and systematically modeling structured relationships, providing superior hateful video detection with explainable results.

Abstract: Hateful videos present serious risks to online safety and real-world
well-being, necessitating effective detection methods. Although multimodal
classification approaches integrating information from several modalities
outperform unimodal ones, they typically neglect that even minimal hateful
content defines a video's category. Specifically, they generally treat all
content uniformly, instead of emphasizing the hateful components. Additionally,
existing multimodal methods cannot systematically capture structured
information in videos, limiting the effectiveness of multimodal fusion. To
address these limitations, we propose a novel multimodal dual-stream graph
neural network model. It constructs an instance graph by separating the given
video into several instances to extract instance-level features. Then, a
complementary weight graph assigns importance weights to these features,
highlighting hateful instances. Importance weights and instance features are
combined to generate video labels. Our model employs a graph-based framework to
systematically model structured relationships within and across modalities.
Extensive experiments on public datasets show that our model is
state-of-the-art in hateful video classification and has strong explainability.
Code is available:
https://github.com/Multimodal-Intelligence-Lab-MIL/MultiHateGNN.

</details>


### [59] [ColonCrafter: A Depth Estimation Model for Colonoscopy Videos Using Diffusion Priors](https://arxiv.org/abs/2509.13525)
*Romain Hardy,Tyler Berzin,Pranav Rajpurkar*

Main category: cs.CV

TL;DR: ColonCrafter is a diffusion-based model that generates temporally consistent depth maps from monocular colonoscopy videos using synthetic training data and style transfer, achieving state-of-the-art zero-shot performance.


<details>
  <summary>Details</summary>
Motivation: 3D scene understanding in colonoscopy requires automated depth estimation methods, but existing models lack temporal consistency across video sequences, limiting 3D reconstruction applications.

Method: Uses diffusion-based depth estimation with geometric priors learned from synthetic colonoscopy sequences, combined with style transfer to adapt real clinical videos to the synthetic training domain.

Result: Achieves state-of-the-art zero-shot performance on C3VD dataset, outperforming both general-purpose and endoscopy-specific approaches. Enables 3D point cloud generation and surface coverage assessment.

Conclusion: ColonCrafter provides clinically relevant applications for colonoscopy despite full trajectory 3D reconstruction remaining challenging, demonstrating the value of diffusion models with synthetic training for medical depth estimation.

Abstract: Three-dimensional (3D) scene understanding in colonoscopy presents
significant challenges that necessitate automated methods for accurate depth
estimation. However, existing depth estimation models for endoscopy struggle
with temporal consistency across video sequences, limiting their applicability
for 3D reconstruction. We present ColonCrafter, a diffusion-based depth
estimation model that generates temporally consistent depth maps from monocular
colonoscopy videos. Our approach learns robust geometric priors from synthetic
colonoscopy sequences to generate temporally consistent depth maps. We also
introduce a style transfer technique that preserves geometric structure while
adapting real clinical videos to match our synthetic training domain.
ColonCrafter achieves state-of-the-art zero-shot performance on the C3VD
dataset, outperforming both general-purpose and endoscopy-specific approaches.
Although full trajectory 3D reconstruction remains a challenge, we demonstrate
clinically relevant applications of ColonCrafter, including 3D point cloud
generation and surface coverage assessment.

</details>


### [60] [MemGS: Memory-Efficient Gaussian Splatting for Real-Time SLAM](https://arxiv.org/abs/2509.13536)
*Yinlong Bai,Hongxin Zhang,Sheng Zhong,Junkai Niu,Hai Li,Yijia He,Yi Zhou*

Main category: cs.CV

TL;DR: This paper proposes methods to reduce GPU memory usage and improve rendering quality for 3D Gaussian Splatting on embedded platforms like micro air vehicles, using voxel-space merging and Patch-Grid point sampling.


<details>
  <summary>Details</summary>
Motivation: Current 3DGS research focuses on high-performance desktop GPUs, overlooking embedded platforms with limited computational resources and memory. There's a need to address the trade-off between system performance and reconstruction quality for devices like micro air vehicles.

Method: 1) Merge redundant 3D Gaussian primitives in voxel space based on geometric similarity to reduce GPU memory usage without impacting runtime performance. 2) Initialize 3D Gaussian primitives via Patch-Grid point sampling to enable more accurate scene modeling and improve rendering quality.

Result: The proposed methods achieve reduced GPU memory usage while enhancing rendering quality, as demonstrated through quantitative and qualitative evaluations on publicly available datasets.

Conclusion: The paper presents effective improvements for 3D Gaussian Splatting that make it more suitable for embedded platforms by addressing both memory efficiency and rendering quality challenges.

Abstract: Recent advancements in 3D Gaussian Splatting (3DGS) have made a significant
impact on rendering and reconstruction techniques. Current research
predominantly focuses on improving rendering performance and reconstruction
quality using high-performance desktop GPUs, largely overlooking applications
for embedded platforms like micro air vehicles (MAVs). These devices, with
their limited computational resources and memory, often face a trade-off
between system performance and reconstruction quality. In this paper, we
improve existing methods in terms of GPU memory usage while enhancing rendering
quality. Specifically, to address redundant 3D Gaussian primitives in SLAM, we
propose merging them in voxel space based on geometric similarity. This reduces
GPU memory usage without impacting system runtime performance. Furthermore,
rendering quality is improved by initializing 3D Gaussian primitives via
Patch-Grid (PG) point sampling, enabling more accurate modeling of the entire
scene. Quantitative and qualitative evaluations on publicly available datasets
demonstrate the effectiveness of our improvements.

</details>


### [61] [Dynamic Aware: Adaptive Multi-Mode Out-of-Distribution Detection for Trajectory Prediction in Autonomous Vehicles](https://arxiv.org/abs/2509.13577)
*Tongfei Guo,Lili Su*

Main category: cs.CV

TL;DR: A new framework for trajectory-level out-of-distribution detection in autonomous vehicles that models mode-dependent prediction errors and achieves better performance than prior methods.


<details>
  <summary>Details</summary>
Motivation: Trajectory prediction models face distribution shifts in real-world deployment, with rare scenarios causing OOD cases. While most OOD detection research focuses on computer vision tasks, trajectory-level detection remains underexplored.

Method: Proposes an adaptive framework building on quickest change detection (QCD) formulation, explicitly modeling mode-dependent prediction error distributions that evolve over time with dataset-specific dynamics.

Result: Substantial improvements in both detection delay and false alarm rates. Significantly outperforms prior uncertainty quantification (UQ) and vision-based OOD approaches in accuracy and computational efficiency across multiple real-world datasets.

Conclusion: The framework offers a practical path toward reliable, driving-aware autonomy by providing robust OOD detection in complex driving environments through adaptive error modeling.

Abstract: Trajectory prediction is central to the safe and seamless operation of
autonomous vehicles (AVs). In deployment, however, prediction models inevitably
face distribution shifts between training data and real-world conditions, where
rare or underrepresented traffic scenarios induce out-of-distribution (OOD)
cases. While most prior OOD detection research in AVs has concentrated on
computer vision tasks such as object detection and segmentation,
trajectory-level OOD detection remains largely underexplored. A recent study
formulated this problem as a quickest change detection (QCD) task, providing
formal guarantees on the trade-off between detection delay and false alarms
[1]. Building on this foundation, we propose a new framework that introduces
adaptive mechanisms to achieve robust detection in complex driving
environments. Empirical analysis across multiple real-world datasets reveals
that prediction errors -- even on in-distribution samples -- exhibit
mode-dependent distributions that evolve over time with dataset-specific
dynamics. By explicitly modeling these error modes, our method achieves
substantial improvements in both detection delay and false alarm rates.
Comprehensive experiments on established trajectory prediction benchmarks show
that our framework significantly outperforms prior UQ- and vision-based OOD
approaches in both accuracy and computational efficiency, offering a practical
path toward reliable, driving-aware autonomy.

</details>


### [62] [Annotating Satellite Images of Forests with Keywords from a Specialized Corpus in the Context of Change Detection](https://arxiv.org/abs/2509.13586)
*Nathalie Neptune,Josiane Mothe*

Main category: cs.CV

TL;DR: Deep learning method for detecting Amazon deforestation using satellite image pairs and automatically generating semantic annotations from scientific documents.


<details>
  <summary>Details</summary>
Motivation: Amazon deforestation significantly impacts global carbon emissions and biodiversity, requiring effective monitoring tools to study and address this environmental concern.

Method: Uses deep learning to compare satellite image pairs from different dates to detect forest cover changes, combined with a visual semantic model that extracts annotations from scientific documents to automatically label detected changes.

Result: The approach was evaluated on Amazon image pairs and demonstrated effectiveness in both deforestation detection and generating relevant semantic annotations for the detected changes.

Conclusion: The method provides a useful tool for monitoring Amazon deforestation and can be generalized to other domains beyond environmental applications.

Abstract: The Amazon rain forest is a vital ecosystem that plays a crucial role in
regulating the Earth's climate and providing habitat for countless species.
Deforestation in the Amazon is a major concern as it has a significant impact
on global carbon emissions and biodiversity. In this paper, we present a method
for detecting deforestation in the Amazon using image pairs from Earth
observation satellites. Our method leverages deep learning techniques to
compare the images of the same area at different dates and identify changes in
the forest cover. We also propose a visual semantic model that automatically
annotates the detected changes with relevant keywords. The candidate annotation
for images are extracted from scientific documents related to the Amazon
region. We evaluate our approach on a dataset of Amazon image pairs and
demonstrate its effectiveness in detecting deforestation and generating
relevant annotations. Our method provides a useful tool for monitoring and
studying the impact of deforestation in the Amazon. While we focus on
environment applications of our work by using images of deforestation in the
Amazon rain forest to demonstrate the effectiveness of our proposed approach,
it is generic enough to be applied to other domains.

</details>


### [63] [Intelligent Healthcare Imaging Platform An VLM-Based Framework for Automated Medical Image Analysis and Clinical Report Generation](https://arxiv.org/abs/2509.13590)
*Samer Al-Hamadani*

Main category: cs.CV

TL;DR: A multimodal AI framework using Vision-Language Models for automated tumor detection and clinical report generation across multiple medical imaging modalities with high accuracy and zero-shot learning capabilities.


<details>
  <summary>Details</summary>
Motivation: To revolutionize diagnostic medicine by developing an intelligent system that integrates visual and textual analysis for automated medical image interpretation, reducing dependence on large datasets and enhancing clinical workflow efficiency.

Method: Leverages Google Gemini 2.5 Flash VLM for tumor detection and report generation, combines visual feature extraction with NLP, uses coordinate verification and probabilistic Gaussian modeling for anomaly distribution, implements multi-layered visualization techniques, and features a Gradio interface for clinical integration.

Result: Achieved high performance in anomaly detection across CT, MRI, X-ray, and Ultrasound modalities with 80 pixels average deviation in location measurement, demonstrated zero-shot learning capabilities, and generated detailed medical illustrations and structured clinical reports.

Conclusion: The framework represents significant advancement in automated diagnostic support and radiological workflow efficiency, though requires clinical validation and multi-center evaluation before widespread adoption.

Abstract: The rapid advancement of artificial intelligence (AI) in healthcare imaging
has revolutionized diagnostic medicine and clinical decision-making processes.
This work presents an intelligent multimodal framework for medical image
analysis that leverages Vision-Language Models (VLMs) in healthcare
diagnostics. The framework integrates Google Gemini 2.5 Flash for automated
tumor detection and clinical report generation across multiple imaging
modalities including CT, MRI, X-ray, and Ultrasound. The system combines visual
feature extraction with natural language processing to enable contextual image
interpretation, incorporating coordinate verification mechanisms and
probabilistic Gaussian modeling for anomaly distribution. Multi-layered
visualization techniques generate detailed medical illustrations, overlay
comparisons, and statistical representations to enhance clinical confidence,
with location measurement achieving 80 pixels average deviation. Result
processing utilizes precise prompt engineering and textual analysis to extract
structured clinical information while maintaining interpretability.
Experimental evaluations demonstrated high performance in anomaly detection
across multiple modalities. The system features a user-friendly Gradio
interface for clinical workflow integration and demonstrates zero-shot learning
capabilities to reduce dependence on large datasets. This framework represents
a significant advancement in automated diagnostic support and radiological
workflow efficiency, though clinical validation and multi-center evaluation are
necessary prior to widespread adoption.

</details>


### [64] [A Generalization of CLAP from 3D Localization to Image Processing, A Connection With RANSAC & Hough Transforms](https://arxiv.org/abs/2509.13605)
*Ruochen Hou,Gabriel I. Fernandez,Alex Xu,Dennis W. Hong*

Main category: cs.CV

TL;DR: CLAP algorithm extended from 2D localization to 3D localization and image stitching, showing relationships with RANSAC and Hough transforms for robust outlier handling.


<details>
  <summary>Details</summary>
Motivation: To generalize the CLAP algorithm beyond its original 2D localization application, making it applicable to 3D problems and image processing while maintaining robustness against outliers.

Method: Extended the clustering-based CLAP framework from 2D to 3D localization and image stitching, using clustering to suppress noise and handle erroneous feature matches instead of traditional outlier rejection methods.

Result: Successfully applied CLAP to 3D localization and image stitching, demonstrating its broader applicability and showing relationships with established methods like RANSAC and Hough transforms.

Conclusion: CLAP's generalization provides a widely applicable framework for handling noise and uncertainty across various fields, offering an alternative to traditional outlier rejection schemes.

Abstract: In previous work, we introduced a 2D localization algorithm called CLAP,
Clustering to Localize Across $n$ Possibilities, which was used during our
championship win in RoboCup 2024, an international autonomous humanoid soccer
competition. CLAP is particularly recognized for its robustness against
outliers, where clustering is employed to suppress noise and mitigate against
erroneous feature matches. This clustering-based strategy provides an
alternative to traditional outlier rejection schemes such as RANSAC, in which
candidates are validated by reprojection error across all data points. In this
paper, CLAP is extended to a more general framework beyond 2D localization,
specifically to 3D localization and image stitching. We also show how CLAP,
RANSAC, and Hough transforms are related. The generalization of CLAP is widely
applicable to many different fields and can be a useful tool to deal with noise
and uncertainty.

</details>


### [65] [SAMIR, an efficient registration framework via robust feature learning from SAM](https://arxiv.org/abs/2509.13629)
*Yue He,Min Liu,Qinghao Liu,Jiazheng Wang,Yaonan Wang,Hang Zhang,Xiang Chen*

Main category: cs.CV

TL;DR: SAMIR is a medical image registration framework that leverages the Segment Anything Model (SAM) to extract structure-aware features, achieving state-of-the-art performance on cardiac and abdomen CT registration tasks.


<details>
  <summary>Details</summary>
Motivation: Weakly supervised registration methods require anatomical priors like segmentation masks that are often unavailable. The strong representation learning ability of visual foundation models like SAM can provide robust feature extraction without requiring additional labels.

Method: Uses SAM's pretrained image encoder for structure-aware feature extraction, adds a lightweight 3D head to refine features for local deformations, and introduces Hierarchical Feature Consistency Loss for coarse-to-fine feature matching.

Result: Achieves 2.68% improvement on ACDC cardiac dataset and 6.44% improvement on abdomen CT dataset compared to state-of-the-art methods.

Conclusion: SAMIR demonstrates that foundation models like SAM can significantly enhance medical image registration by providing robust feature representations without requiring additional anatomical labels, making it more practical for real-world applications.

Abstract: Image registration is a fundamental task in medical image analysis.
Deformations are often closely related to the morphological characteristics of
tissues, making accurate feature extraction crucial. Recent weakly supervised
methods improve registration by incorporating anatomical priors such as
segmentation masks or landmarks, either as inputs or in the loss function.
However, such weak labels are often not readily available, limiting their
practical use. Motivated by the strong representation learning ability of
visual foundation models, this paper introduces SAMIR, an efficient medical
image registration framework that utilizes the Segment Anything Model (SAM) to
enhance feature extraction. SAM is pretrained on large-scale natural image
datasets and can learn robust, general-purpose visual representations. Rather
than using raw input images, we design a task-specific adaptation pipeline
using SAM's image encoder to extract structure-aware feature embeddings,
enabling more accurate modeling of anatomical consistency and deformation
patterns. We further design a lightweight 3D head to refine features within the
embedding space, adapting to local deformations in medical images.
Additionally, we introduce a Hierarchical Feature Consistency Loss to guide
coarse-to-fine feature matching and improve anatomical alignment. Extensive
experiments demonstrate that SAMIR significantly outperforms state-of-the-art
methods on benchmark datasets for both intra-subject cardiac image registration
and inter-subject abdomen CT image registration, achieving performance
improvements of 2.68% on ACDC and 6.44% on the abdomen dataset. The source code
will be publicly available on GitHub following the acceptance of this paper.

</details>


### [66] [Federated Learning for Deforestation Detection: A Distributed Approach with Satellite Imagery](https://arxiv.org/abs/2509.13631)
*Yuvraj Dutta,Aaditya Sikder,Basabdatta Palit*

Main category: cs.CV

TL;DR: A distributed federated learning approach for deforestation detection using satellite imagery that preserves data privacy across multiple edge centers.


<details>
  <summary>Details</summary>
Motivation: Accurate deforestation identification from satellite images is essential for geographical monitoring, but centralized approaches compromise data security and privacy.

Method: Uses Federated Learning with FLOWER and RAY frameworks to enable collaborative model training across distributed edge satellite centers. Employs YOLOS-small (Vision Transformer), Faster R-CNN with ResNet50, and Faster R-CNN with MobileNetV3 models on public datasets.

Result: The framework enables distributed deforestation detection while maintaining data privacy and security across multiple clients/edge centers.

Conclusion: Federated Learning provides an effective alternative to centralized training for satellite image segmentation tasks, offering data privacy advantages while enabling collaborative model development.

Abstract: Accurate identification of deforestation from satellite images is essential
in order to understand the geographical situation of an area. This paper
introduces a new distributed approach to identify as well as locate
deforestation across different clients using Federated Learning (FL). Federated
Learning enables distributed network clients to collaboratively train a model
while maintaining data privacy and security of the active users. In our
framework, a client corresponds to an edge satellite center responsible for
local data processing. Moreover, FL provides an advantage over centralized
training method which requires combining data, thereby compromising with data
security of the clients. Our framework leverages the FLOWER framework with RAY
framework to execute the distributed learning workload. Furthermore, efficient
client spawning is ensured by RAY as it can select definite amount of users to
create an emulation environment. Our FL framework uses YOLOS-small (a Vision
Transformer variant), Faster R-CNN with a ResNet50 backbone, and Faster R-CNN
with a MobileNetV3 backbone models trained and tested on publicly available
datasets. Our approach provides us a different view for image
segmentation-based tasks on satellite imagery.

</details>


### [67] [Gaussian Alignment for Relative Camera Pose Estimation via Single-View Reconstruction](https://arxiv.org/abs/2509.13652)
*Yumin Li,Dylan Campbell*

Main category: cs.CV

TL;DR: GARPS is a training-free framework for metric relative camera pose estimation that aligns two independently reconstructed 3D Gaussian Mixture Models from monocular images, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Conventional two-view pose estimation methods are not metric (translation known only up to scale) and struggle with wide baselines, textureless surfaces, and reflective surfaces.

Method: Uses metric monocular depth estimator and Gaussian scene reconstructor to create metric 3D GMMs for each image, then refines initial pose by optimizing a differentiable GMM alignment objective considering geometry, color, covariance, and semantic features.

Result: Outperforms both classical and state-of-the-art learning-based methods (including MASt3R) on Real-Estate10K dataset.

Conclusion: Shows potential of bridging single-view perception with multi-view geometry for robust metric relative pose estimation without requiring explicit 2D correspondences.

Abstract: Estimating metric relative camera pose from a pair of images is of great
importance for 3D reconstruction and localisation. However, conventional
two-view pose estimation methods are not metric, with camera translation known
only up to a scale, and struggle with wide baselines and textureless or
reflective surfaces. This paper introduces GARPS, a training-free framework
that casts this problem as the direct alignment of two independently
reconstructed 3D scenes. GARPS leverages a metric monocular depth estimator and
a Gaussian scene reconstructor to obtain a metric 3D Gaussian Mixture Model
(GMM) for each image. It then refines an initial pose from a feed-forward
two-view pose estimator by optimising a differentiable GMM alignment objective.
This objective jointly considers geometric structure, view-independent colour,
anisotropic covariance, and semantic feature consistency, and is robust to
occlusions and texture-poor regions without requiring explicit 2D
correspondences. Extensive experiments on the Real\-Estate10K dataset
demonstrate that GARPS outperforms both classical and state-of-the-art
learning-based methods, including MASt3R. These results highlight the potential
of bridging single-view perception with multi-view geometry to achieve robust
and metric relative pose estimation.

</details>


### [68] [Deep Lookup Network](https://arxiv.org/abs/2509.13662)
*Yulan Guo,Longguang Wang,Wendong Mao,Xiaoyu Dong,Yingqian Wang,Li Liu,Wei An*

Main category: cs.CV

TL;DR: This paper introduces a lookup operation to replace multiplication operations in CNNs, reducing computational complexity and energy consumption while maintaining competitive performance across various tasks.


<details>
  <summary>Details</summary>
Motivation: Multiplication operations in convolutional neural networks are computationally intensive and energy-consuming, hindering deployment on mobile and edge devices. The authors aim to replace these operations with more efficient lookup operations.

Method: The paper proposes a generic and efficient lookup operation as a basic building block for neural networks. Instead of calculating multiplications between weights and activations, they use lookup operations. They construct lookup tables in a differentiable manner and develop training strategies for end-to-end optimization.

Result: The lookup networks achieve higher efficiency in terms of energy consumption and inference speed while maintaining competitive performance compared to vanilla convolutional networks. Extensive experiments show state-of-the-art performance on image classification, super-resolution, and point cloud classification tasks.

Conclusion: Lookup operations can effectively replace multiplication operations in neural networks, enabling more efficient deployment on resource-constrained devices while maintaining performance across various computer vision tasks.

Abstract: Convolutional neural networks are constructed with massive operations with
different types and are highly computationally intensive. Among these
operations, multiplication operation is higher in computational complexity and
usually requires {more} energy consumption with longer inference time than
other operations, which hinders the deployment of convolutional neural networks
on mobile devices. In many resource-limited edge devices, complicated
operations can be calculated via lookup tables to reduce computational cost.
Motivated by this, in this paper, we introduce a generic and efficient lookup
operation which can be used as a basic operation for the construction of neural
networks. Instead of calculating the multiplication of weights and activation
values, simple yet efficient lookup operations are adopted to compute their
responses. To enable end-to-end optimization of the lookup operation, we
construct the lookup tables in a differentiable manner and propose several
training strategies to promote their convergence. By replacing computationally
expensive multiplication operations with our lookup operations, we develop
lookup networks for the image classification, image super-resolution, and point
cloud classification tasks. It is demonstrated that our lookup networks can
benefit from the lookup operations to achieve higher efficiency in terms of
energy consumption and inference speed while maintaining competitive
performance to vanilla convolutional networks. Extensive experiments show that
our lookup networks produce state-of-the-art performance on different tasks
(both classification and regression tasks) and different data types (both
images and point clouds).

</details>


### [69] [Re-purposing SAM into Efficient Visual Projectors for MLLM-Based Referring Image Segmentation](https://arxiv.org/abs/2509.13676)
*Xiaobo Yang,Xiaojin Gong*

Main category: cs.CV

TL;DR: A semantic visual projector using SAM-generated superpixels to compress visual tokens by 93% while maintaining RIS performance, with faster MLLM training/inference.


<details>
  <summary>Details</summary>
Motivation: Traditional patch-wise visual projectors in RIS frameworks suffer from visual token redundancy and struggle to balance token reduction with semantic preservation, requiring long token sequences to avoid performance drops.

Method: Proposes a semantic visual projector that uses semantic superpixels from SAM as "visual words", compressing them as tokens. Includes semantic superpixel positional embedding for geometry awareness and semantic superpixel aggregator to preserve fine details and global context.

Result: Achieves 93% reduction in visual tokens without performance compromise, significantly speeds up MLLM training and inference, and outperforms existing compressive visual projectors on RIS tasks.

Conclusion: The semantic visual projector approach effectively addresses visual token redundancy in RIS frameworks, enabling efficient compression while maintaining semantic clarity and performance.

Abstract: Recently, Referring Image Segmentation (RIS) frameworks that pair the
Multimodal Large Language Model (MLLM) with the Segment Anything Model (SAM)
have achieved impressive results. However, adapting MLLM to segmentation is
computationally intensive, primarily due to visual token redundancy. We observe
that traditional patch-wise visual projectors struggle to strike a balance
between reducing the number of visual tokens and preserving semantic clarity,
often retaining overly long token sequences to avoid performance drops.
Inspired by text tokenizers, we propose a novel semantic visual projector that
leverages semantic superpixels generated by SAM to identify "visual words" in
an image. By compressing and projecting semantic superpixels as visual tokens,
our approach adaptively shortens the token sequence according to scene
complexity while minimizing semantic loss in compression. To mitigate loss of
information, we propose a semantic superpixel positional embedding to
strengthen MLLM's awareness of superpixel geometry and position, alongside a
semantic superpixel aggregator to preserve both fine-grained details inside
superpixels and global context outside. Experiments show that our method cuts
visual tokens by 93% without compromising performance, notably speeding up MLLM
training and inference, and outperforming existing compressive visual
projectors on RIS.

</details>


### [70] [FishBEV: Distortion-Resilient Bird's Eye View Segmentation with Surround-View Fisheye Cameras](https://arxiv.org/abs/2509.13681)
*Hang Li,Dianmo Sheng,Qiankun Dong,Zichun Wang,Zhiwei Xu,Tao Li*

Main category: cs.CV

TL;DR: FishBEV is a novel BEV segmentation framework specifically designed for fisheye cameras that addresses distortion, multi-view alignment, and temporal coherence challenges through three key innovations.


<details>
  <summary>Details</summary>
Motivation: Existing BEV segmentation methods work well with pinhole cameras but struggle with fisheye cameras due to severe geometric distortion, ambiguous multi-view correspondences, and unstable temporal dynamics that degrade performance.

Method: Three complementary innovations: 1) Distortion-Resilient Multi-scale Extraction (DRME) backbone for robust features under distortion, 2) Uncertainty-aware Spatial Cross-Attention (U-SCA) for reliable cross-view alignment, 3) Distance-aware Temporal Self-Attention (D-TSA) for balancing near/far field details and ensuring temporal coherence.

Result: Extensive experiments on Synwoodscapes dataset demonstrate that FishBEV consistently outperforms state-of-the-art baselines on surround-view fisheye BEV segmentation tasks.

Conclusion: FishBEV provides an effective solution for fisheye camera BEV segmentation by specifically addressing the unique challenges of distortion, multi-view alignment, and temporal dynamics that plague conventional methods.

Abstract: As a cornerstone technique for autonomous driving, Bird's Eye View (BEV)
segmentation has recently achieved remarkable progress with pinhole cameras.
However, it is non-trivial to extend the existing methods to fisheye cameras
with severe geometric distortion, ambiguous multi-view correspondences and
unstable temporal dynamics, all of which significantly degrade BEV performance.
To address these challenges, we propose FishBEV, a novel BEV segmentation
framework specifically tailored for fisheye cameras. This framework introduces
three complementary innovations, including a Distortion-Resilient Multi-scale
Extraction (DRME) backbone that learns robust features under distortion while
preserving scale consistency, an Uncertainty-aware Spatial Cross-Attention
(U-SCA) mechanism that leverages uncertainty estimation for reliable cross-view
alignment, a Distance-aware Temporal Self-Attention (D-TSA) module that
adaptively balances near field details and far field context to ensure temporal
coherence. Extensive experiments on the Synwoodscapes dataset demonstrate that
FishBEV consistently outperforms SOTA baselines, regarding the performance
evaluation of FishBEV on the surround-view fisheye BEV segmentation tasks.

</details>


### [71] [Taylor-Series Expanded Kolmogorov-Arnold Network for Medical Imaging Classification](https://arxiv.org/abs/2509.13687)
*Kaniz Fatema,Emad A. Mohammed,Sukhjit Singh Sehra*

Main category: cs.CV

TL;DR: Spline-based KANs achieve high medical image classification accuracy with minimal parameters, outperforming traditional CNNs while providing interpretability through Grad-CAM visualizations.


<details>
  <summary>Details</summary>
Motivation: To develop accurate and interpretable medical image classification models that work effectively with limited, diverse datasets in resource-constrained clinical settings.

Method: Proposed three spline-based Kolmogorov-Arnold Networks: SBTAYLOR-KAN (B-splines + Taylor series), SBRBF-KAN (B-splines + Radial Basis Functions), and SBWAVELET-KAN (B-splines + Morlet wavelets). Models learn directly from raw medical images without preprocessing.

Result: SBTAYLOR-KAN achieved up to 98.93% accuracy, maintained over 86% accuracy with only 30% training data, and outperformed other models on imbalanced datasets (68.22% accuracy). Used only 2,872 parameters vs. 24.18M in ResNet50.

Conclusion: The framework provides a lightweight, interpretable, and generalizable solution for medical image classification, particularly suitable for data-scarce clinical environments with limited computational resources.

Abstract: Effective and interpretable classification of medical images is a challenge
in computer-aided diagnosis, especially in resource-limited clinical settings.
This study introduces spline-based Kolmogorov-Arnold Networks (KANs) for
accurate medical image classification with limited, diverse datasets. The
models include SBTAYLOR-KAN, integrating B-splines with Taylor series;
SBRBF-KAN, combining B-splines with Radial Basis Functions; and SBWAVELET-KAN,
embedding B-splines in Morlet wavelet transforms. These approaches leverage
spline-based function approximation to capture both local and global
nonlinearities. The models were evaluated on brain MRI, chest X-rays,
tuberculosis X-rays, and skin lesion images without preprocessing,
demonstrating the ability to learn directly from raw data. Extensive
experiments, including cross-dataset validation and data reduction analysis,
showed strong generalization and stability. SBTAYLOR-KAN achieved up to 98.93%
accuracy, with a balanced F1-score, maintaining over 86% accuracy using only
30% of the training data across three datasets. Despite class imbalance in the
skin cancer dataset, experiments on both imbalanced and balanced versions
showed SBTAYLOR-KAN outperforming other models, achieving 68.22% accuracy.
Unlike traditional CNNs, which require millions of parameters (e.g., ResNet50
with 24.18M), SBTAYLOR-KAN achieves comparable performance with just 2,872
trainable parameters, making it more suitable for constrained medical
environments. Gradient-weighted Class Activation Mapping (Grad-CAM) was used
for interpretability, highlighting relevant regions in medical images. This
framework provides a lightweight, interpretable, and generalizable solution for
medical image classification, addressing the challenges of limited datasets and
data-scarce scenarios in clinical AI applications.

</details>


### [72] [StyleProtect: Safeguarding Artistic Identity in Fine-tuned Diffusion Models](https://arxiv.org/abs/2509.13711)
*Qiuyu Tang,Joshua Krinsky,Aparna Bharati*

Main category: cs.CV

TL;DR: StyleProtect is an efficient protection method that defends artistic styles against fine-tuned diffusion models by selectively updating sensitive cross-attention layers.


<details>
  <summary>Details</summary>
Motivation: The rapid advancement of diffusion models enables malicious actors to easily replicate and mimic artists' unique styles, threatening their creative labor and years of dedication, creating an urgent need for effective style protection methods.

Method: The method identifies cross-attention layers sensitive to artistic styles through activation analysis, then introduces StyleProtect - a lightweight protection strategy that updates only selected cross-attention layers to defend against style mimicry.

Result: Experiments on WikiArt-based artwork dataset (30 artists) and Anita cartoon animations show promising performance in safeguarding unique styles from malicious diffusion customization while maintaining competitive imperceptibility.

Conclusion: StyleProtect provides an efficient and effective defense mechanism against style mimicry by fine-tuned diffusion models, protecting artists' distinctive styles without significant visual degradation.

Abstract: The rapid advancement of generative models, particularly diffusion-based
approaches, has inadvertently facilitated their potential for misuse. Such
models enable malicious exploiters to replicate artistic styles that capture an
artist's creative labor, personal vision, and years of dedication in an
inexpensive manner. This has led to a rise in the need and exploration of
methods for protecting artworks against style mimicry. Although generic
diffusion models can easily mimic an artistic style, finetuning amplifies this
capability, enabling the model to internalize and reproduce the style with
higher fidelity and control. We hypothesize that certain cross-attention layers
exhibit heightened sensitivity to artistic styles. Sensitivity is measured
through activation strengths of attention layers in response to style and
content representations, and assessing their correlations with features
extracted from external models. Based on our findings, we introduce an
efficient and lightweight protection strategy, StyleProtect, that achieves
effective style defense against fine-tuned diffusion models by updating only
selected cross-attention layers. Our experiments utilize a carefully curated
artwork dataset based on WikiArt, comprising representative works from 30
artists known for their distinctive and influential styles and cartoon
animations from the Anita dataset. The proposed method demonstrates promising
performance in safeguarding unique styles of artworks and anime from malicious
diffusion customization, while maintaining competitive imperceptibility.

</details>


### [73] [UM-Depth : Uncertainty Masked Self-Supervised Monocular Depth Estimation with Visual Odometry](https://arxiv.org/abs/2509.13713)
*Tae-Wook Um,Ki-Hyeon Kim,Hyun-Duck Choi,Hyo-Sung Ahn*

Main category: cs.CV

TL;DR: UM-Depth is a self-supervised monocular depth estimation framework that uses uncertainty-aware refinement and teacher-student training to improve accuracy in challenging regions like dynamic boundaries and textureless areas, achieving state-of-the-art results without inference-time overhead.


<details>
  <summary>Details</summary>
Motivation: Self-supervised monocular depth estimation methods struggle with uncertainty in input data such as low-texture regions and dynamic objects, leading to reduced depth accuracy. Existing motion-aware approaches require additional labels or auxiliary networks and incur inference-time costs.

Method: Proposes UM-Depth framework with motion- and uncertainty-aware refinement using a teacher-student training strategy. Embeds uncertainty estimation into both training pipeline and network architecture. Uses optical flow only in teacher network during training, eliminating runtime costs and extra labeling requirements.

Result: Extensive experiments on KITTI and Cityscapes datasets show effectiveness of uncertainty-aware refinement. Achieves state-of-the-art results in both self-supervised depth and pose estimation on KITTI datasets.

Conclusion: UM-Depth successfully addresses uncertainty challenges in self-supervised monocular depth estimation through uncertainty-aware refinement and teacher-student training, delivering superior performance without additional inference costs or labeling requirements.

Abstract: Monocular depth estimation has been increasingly adopted in robotics and
autonomous driving for its ability to infer scene geometry from a single
camera. In self-supervised monocular depth estimation frameworks, the network
jointly generates and exploits depth and pose estimates during training,
thereby eliminating the need for depth labels. However, these methods remain
challenged by uncertainty in the input data, such as low-texture or dynamic
regions, which can cause reduced depth accuracy. To address this, we introduce
UM-Depth, a framework that combines motion- and uncertainty-aware refinement to
enhance depth accuracy at dynamic object boundaries and in textureless regions.
Specifically, we develop a teacherstudent training strategy that embeds
uncertainty estimation into both the training pipeline and network
architecture, thereby strengthening supervision where photometric signals are
weak. Unlike prior motion-aware approaches that incur inference-time overhead
and rely on additional labels or auxiliary networks for real-time generation,
our method uses optical flow exclusively within the teacher network during
training, which eliminating extra labeling demands and any runtime cost.
Extensive experiments on the KITTI and Cityscapes datasets demonstrate the
effectiveness of our uncertainty-aware refinement. Overall, UM-Depth achieves
state-of-the-art results in both self-supervised depth and pose estimation on
the KITTI datasets.

</details>


### [74] [Mitigating Query Selection Bias in Referring Video Object Segmentation](https://arxiv.org/abs/2509.13722)
*Dingwei Zhang,Dong Zhang,Jinhui Tang*

Main category: cs.CV

TL;DR: TQF addresses query selection bias in Referring Video Object Segmentation by decomposing queries into appearance, spatial interaction, and temporal motion components, with motion-aware aggregation modules for improved performance.


<details>
  <summary>Details</summary>
Motivation: Existing query-based RVOS methods suffer from query selection bias where static textual queries are easily misled by distractors with similar appearance or motion patterns.

Method: Proposes Triple Query Former (TQF) that factorizes referring queries into three specialized components: appearance query, intra-frame interaction query, and inter-frame motion query, along with motion-aware aggregation modules for enhanced object token representations.

Result: Extensive experiments on multiple RVOS benchmarks demonstrate the advantages of TQF and the effectiveness of the structured query design and motion-aware aggregation modules.

Conclusion: The proposed TQF framework successfully addresses query selection bias through specialized query decomposition and motion-aware aggregation, achieving improved performance in Referring Video Object Segmentation.

Abstract: Recently, query-based methods have achieved remarkable performance in
Referring Video Object Segmentation (RVOS) by using textual static object
queries to drive cross-modal alignment. However, these static queries are
easily misled by distractors with similar appearance or motion, resulting in
\emph{query selection bias}. To address this issue, we propose Triple Query
Former (TQF), which factorizes the referring query into three specialized
components: an appearance query for static attributes, an intra-frame
interaction query for spatial relations, and an inter-frame motion query for
temporal association. Instead of relying solely on textual embeddings, our
queries are dynamically constructed by integrating both linguistic cues and
visual guidance. Furthermore, we introduce two motion-aware aggregation modules
that enhance object token representations: Intra-frame Interaction Aggregation
incorporates position-aware interactions among objects within a single frame,
while Inter-frame Motion Aggregation leverages trajectory-guided alignment
across frames to ensure temporal coherence. Extensive experiments on multiple
RVOS benchmarks demonstrate the advantages of TQF and the effectiveness of our
structured query design and motion-aware aggregation modules.

</details>


### [75] [Improving Generalized Visual Grounding with Instance-aware Joint Learning](https://arxiv.org/abs/2509.13747)
*Ming Dai,Wenxuan Cheng,Jiang-Jiang Liu,Lingfeng Yang,Zhenhua Feng,Wankou Yang,Jingdong Wang*

Main category: cs.CV

TL;DR: InstanceVG is a unified framework that jointly handles Generalized Referring Expression Comprehension (GREC) and Segmentation (GRES) with instance-aware capabilities, achieving state-of-the-art performance across multiple datasets.


<details>
  <summary>Details</summary>
Motivation: Existing approaches treat GREC and GRES independently, overlook multi-granularity consistency, and neglect instance-aware capabilities in segmentation tasks.

Method: Uses instance queries with prior reference points to unify joint predictions of instance-level boxes and masks, ensuring consistent point-box-mask predictions for the same instance.

Result: Achieves state-of-the-art performance on ten datasets across four tasks, significantly surpassing existing methods in various evaluation metrics.

Conclusion: InstanceVG successfully demonstrates the benefits of joint training and instance-aware capabilities for generalized visual grounding tasks, providing a unified solution for multi-granularity predictions.

Abstract: Generalized visual grounding tasks, including Generalized Referring
Expression Comprehension (GREC) and Segmentation (GRES), extend the classical
visual grounding paradigm by accommodating multi-target and non-target
scenarios. Specifically, GREC focuses on accurately identifying all referential
objects at the coarse bounding box level, while GRES aims for achieve
fine-grained pixel-level perception. However, existing approaches typically
treat these tasks independently, overlooking the benefits of jointly training
GREC and GRES to ensure consistent multi-granularity predictions and streamline
the overall process. Moreover, current methods often treat GRES as a semantic
segmentation task, neglecting the crucial role of instance-aware capabilities
and the necessity of ensuring consistent predictions between instance-level
boxes and masks. To address these limitations, we propose InstanceVG, a
multi-task generalized visual grounding framework equipped with instance-aware
capabilities, which leverages instance queries to unify the joint and
consistency predictions of instance-level boxes and masks. To the best of our
knowledge, InstanceVG is the first framework to simultaneously tackle both GREC
and GRES while incorporating instance-aware capabilities into generalized
visual grounding. To instantiate the framework, we assign each instance query a
prior reference point, which also serves as an additional basis for target
matching. This design facilitates consistent predictions of points, boxes, and
masks for the same instance. Extensive experiments obtained on ten datasets
across four tasks demonstrate that InstanceVG achieves state-of-the-art
performance, significantly surpassing the existing methods in various
evaluation metrics. The code and model will be publicly available at
https://github.com/Dmmm1997/InstanceVG.

</details>


### [76] [Cross-modal Full-mode Fine-grained Alignment for Text-to-Image Person Retrieval](https://arxiv.org/abs/2509.13754)
*Hao Yin,Xin Man,Feiyu Chen,Jie Shao,Heng Tao Shen*

Main category: cs.CV

TL;DR: FMFA is a cross-modal framework for text-to-image person retrieval that combines explicit fine-grained alignment with implicit relational reasoning to improve global matching without extra supervision.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack verification of local feature alignment and focus too much on hard negatives while neglecting incorrectly matched positive pairs, limiting retrieval performance.

Method: Proposes Adaptive Similarity Distribution Matching (A-SDM) to rectify unmatched positive pairs and Explicit Fine-grained Alignment (EFA) module with sparsified similarity matrix and hard coding for local alignment.

Result: Achieves state-of-the-art performance on three public datasets among all global matching methods for text-to-image person retrieval.

Conclusion: FMFA effectively addresses cross-modal alignment challenges through full-mode fine-grained alignment, improving both global matching and local feature verification without additional supervision.

Abstract: Text-to-Image Person Retrieval (TIPR) is a cross-modal matching task that
aims to retrieve the most relevant person images based on a given text query.
The key challenge in TIPR lies in achieving effective alignment between textual
and visual modalities within a common latent space. To address this challenge,
prior approaches incorporate attention mechanisms for implicit cross-modal
local alignment. However, they lack the ability to verify whether all local
features are correctly aligned. Moreover, existing methods primarily focus on
hard negative samples during model updates, with the goal of refining
distinctions between positive and negative pairs, often neglecting incorrectly
matched positive pairs. To alleviate these issues, we propose FMFA, a
cross-modal Full-Mode Fine-grained Alignment framework, which enhances global
matching through explicit fine-grained alignment and existing implicit
relational reasoning -- hence the term ``full-mode" -- without requiring
additional supervision. Specifically, we design an Adaptive Similarity
Distribution Matching (A-SDM) module to rectify unmatched positive sample
pairs. A-SDM adaptively pulls the unmatched positive pairs closer in the joint
embedding space, thereby achieving more precise global alignment. Additionally,
we introduce an Explicit Fine-grained Alignment (EFA) module, which makes up
for the lack of verification capability of implicit relational reasoning. EFA
strengthens explicit cross-modal fine-grained interactions by sparsifying the
similarity matrix and employs a hard coding method for local alignment. Our
proposed method is evaluated on three public datasets, achieving
state-of-the-art performance among all global matching methods. Our code is
available at https://github.com/yinhao1102/FMFA.

</details>


### [77] [Controllable-Continuous Color Editing in Diffusion Model via Color Mapping](https://arxiv.org/abs/2509.13756)
*Yuqi Yang,Dongliang Chang,Yuanchen Fang,Yi-Zhe SonG,Zhanyu Ma,Jun Guo*

Main category: cs.CV

TL;DR: A color mapping module that bridges text embeddings and RGB values for precise, continuous color control in text-driven image editing.


<details>
  <summary>Details</summary>
Motivation: Current text-driven image editing lacks precise color control due to language ambiguity and the unknown relationship between interpolation coefficients and resulting colors.

Method: Introduces a color mapping module that explicitly models correspondence between text embedding space and RGB values, predicting embedding vectors from given RGB values.

Result: Enables precise color control with continuous variations within specified RGB ranges while maintaining semantic consistency.

Conclusion: The method achieves superior color continuity and controllability compared to existing approaches.

Abstract: In recent years, text-driven image editing has made significant progress.
However, due to the inherent ambiguity and discreteness of natural language,
color editing still faces challenges such as insufficient precision and
difficulty in achieving continuous control. Although linearly interpolating the
embedding vectors of different textual descriptions can guide the model to
generate a sequence of images with varying colors, this approach lacks precise
control over the range of color changes in the output images. Moreover, the
relationship between the interpolation coefficient and the resulting image
color is unknown and uncontrollable. To address these issues, we introduce a
color mapping module that explicitly models the correspondence between the text
embedding space and image RGB values. This module predicts the corresponding
embedding vector based on a given RGB value, enabling precise color control of
the generated images while maintaining semantic consistency. Users can specify
a target RGB range to generate images with continuous color variations within
the desired range, thereby achieving finer-grained, continuous, and
controllable color editing. Experimental results demonstrate that our method
performs well in terms of color continuity and controllability.

</details>


### [78] [Iterative Prompt Refinement for Safer Text-to-Image Generation](https://arxiv.org/abs/2509.13760)
*Jinwoo Jeon,JunHyeok Oh,Hayeong Lee,Byung-Jun Lee*

Main category: cs.CV

TL;DR: Iterative prompt refinement using vision-language models to improve text-to-image generation safety by analyzing both prompts and generated images, outperforming LLM-only methods.


<details>
  <summary>Details</summary>
Motivation: Existing safety methods for text-to-image models rely solely on LLM-based prompt refinement, which overlooks the actual generated images and can result in unsafe outputs or unnecessary modifications to safe prompts.

Method: Proposed iterative prompt refinement algorithm that uses Vision Language Models (VLMs) to analyze both input prompts and generated images, leveraging visual feedback for more effective refinement. Also created a new dataset with textual and visual safety labels using multi-modal LLM for supervised fine-tuning.

Result: Experimental results show the approach produces safer outputs while maintaining user intent alignment and reliability comparable to existing LLM-based methods.

Conclusion: The method offers a practical solution for generating safer text-to-image content by incorporating visual feedback into the prompt refinement process, effectively improving safety without compromising user intent.

Abstract: Text-to-Image (T2I) models have made remarkable progress in generating images
from text prompts, but their output quality and safety still depend heavily on
how prompts are phrased. Existing safety methods typically refine prompts using
large language models (LLMs), but they overlook the images produced, which can
result in unsafe outputs or unnecessary changes to already safe prompts. To
address this, we propose an iterative prompt refinement algorithm that uses
Vision Language Models (VLMs) to analyze both the input prompts and the
generated images. By leveraging visual feedback, our method refines prompts
more effectively, improving safety while maintaining user intent and
reliability comparable to existing LLM-based approaches. Additionally, we
introduce a new dataset labeled with both textual and visual safety signals
using off-the-shelf multi-modal LLM, enabling supervised fine-tuning.
Experimental results demonstrate that our approach produces safer outputs
without compromising alignment with user intent, offering a practical solution
for generating safer T2I content. Our code is available at
https://github.com/ku-dmlab/IPR. \textbf{\textcolor{red}WARNING: This paper
contains examples of harmful or inappropriate images generated by models.

</details>


### [79] [Task-Aware Image Signal Processor for Advanced Visual Perception](https://arxiv.org/abs/2509.13762)
*Kai Chen,Jin Xiao,Leheng Zhang,Kexuan Shi,Shuhang Gu*

Main category: cs.CV

TL;DR: TA-ISP is a lightweight RAW-to-RGB framework that uses multi-scale modulation operators instead of heavy convolutional pipelines to create task-oriented representations for vision models, reducing computational overhead while improving accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing RAW data processing methods either use computationally intensive ISP networks or are limited by traditional ISP pipelines' representational capacity, making them unsuitable for resource-constrained devices.

Method: Proposes Task-Aware Image Signal Processing (TA-ISP) that predicts lightweight multi-scale modulation operators (global, regional, pixel scales) to reshape image statistics across different spatial extents, avoiding dense convolutional pipelines.

Result: TA-ISP consistently improves accuracy on RAW-domain detection and segmentation benchmarks under both daytime and nighttime conditions while significantly reducing parameter count and inference time.

Conclusion: TA-ISP provides an efficient solution for deploying RAW data processing on resource-constrained devices by offering factorized control with constrained memory, computation, and latency.

Abstract: In recent years, there has been a growing trend in computer vision towards
exploiting RAW sensor data, which preserves richer information compared to
conventional low-bit RGB images. Early studies mainly focused on enhancing
visual quality, while more recent efforts aim to leverage the abundant
information in RAW data to improve the performance of visual perception tasks
such as object detection and segmentation. However, existing approaches still
face two key limitations: large-scale ISP networks impose heavy computational
overhead, while methods based on tuning traditional ISP pipelines are
restricted by limited representational capacity.To address these issues, we
propose Task-Aware Image Signal Processing (TA-ISP), a compact RAW-to-RGB
framework that produces task-oriented representations for pretrained vision
models. Instead of heavy dense convolutional pipelines, TA-ISP predicts a small
set of lightweight, multi-scale modulation operators that act at global,
regional, and pixel scales to reshape image statistics across different spatial
extents. This factorized control significantly expands the range of spatially
varying transforms that can be represented while keeping memory usage,
computation, and latency tightly constrained. Evaluated on several RAW-domain
detection and segmentation benchmarks under both daytime and nighttime
conditions, TA-ISP consistently improves downstream accuracy while markedly
reducing parameter count and inference time, making it well suited for
deployment on resource-constrained devices.

</details>


### [80] [NDLPNet: A Location-Aware Nighttime Deraining Network and a Real-World Benchmark Dataset](https://arxiv.org/abs/2509.13766)
*Huichun Liu,Xiaosong Li,Yang Liu,Xiaoqi Cheng,Haishu Tan*

Main category: cs.CV

TL;DR: A novel nighttime deraining network (NDLPNet) with position perception module that effectively removes rain streaks while preserving background information in low-light conditions, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing image deraining techniques perform poorly in nighttime conditions due to spatial heterogeneity of rain distribution and light-dependent stripe visibility, which hampers nighttime surveillance and autonomous navigation.

Method: Proposed Nighttime Deraining Location-enhanced Perceptual Network (NDLPNet) with Position Perception Module (PPM) to capture spatial contextual information and recalibrate feature channel importance. Also constructed a new NSR dataset with 900 real-world nighttime image pairs.

Result: Extensive experiments show the method outperforms state-of-the-art techniques in nighttime deraining tasks on both existing datasets and the new NSR dataset.

Conclusion: The proposed NDLPNet effectively addresses nighttime deraining challenges by capturing spatial positional information and rain density distribution, providing superior performance in low-light conditions.

Abstract: Visual degradation caused by rain streak artifacts in low-light conditions
significantly hampers the performance of nighttime surveillance and autonomous
navigation. Existing image deraining techniques are primarily designed for
daytime conditions and perform poorly under nighttime illumination due to the
spatial heterogeneity of rain distribution and the impact of light-dependent
stripe visibility. In this paper, we propose a novel Nighttime Deraining
Location-enhanced Perceptual Network(NDLPNet) that effectively captures the
spatial positional information and density distribution of rain streaks in
low-light environments. Specifically, we introduce a Position Perception Module
(PPM) to capture and leverage spatial contextual information from input data,
enhancing the model's capability to identify and recalibrate the importance of
different feature channels. The proposed nighttime deraining network can
effectively remove the rain streaks as well as preserve the crucial background
information. Furthermore, We construct a night scene rainy (NSR) dataset
comprising 900 image pairs, all based on real-world nighttime scenes, providing
a new benchmark for nighttime deraining task research. Extensive qualitative
and quantitative experimental evaluations on both existing datasets and the NSR
dataset consistently demonstrate our method outperform the state-of-the-art
(SOTA) methods in nighttime deraining tasks. The source code and dataset is
available at https://github.com/Feecuin/NDLPNet.

</details>


### [81] [VocSegMRI: Multimodal Learning for Precise Vocal Tract Segmentation in Real-time MRI](https://arxiv.org/abs/2509.13767)
*Daiqi Liu,Tomás Arias-Vergara,Johannes Enk,Fangxu Xing,Maureen Stone,Jerry L. Prince,Jana Hutter,Andreas Maier,Jonghye Woo,Paula Andrea Pérez-Toro*

Main category: cs.CV

TL;DR: VocSegMRI is a multimodal framework that combines video, audio, and phonological inputs using cross-attention fusion and contrastive learning to achieve state-of-the-art vocal tract segmentation in real-time MRI.


<details>
  <summary>Details</summary>
Motivation: Existing rtMRI segmentation methods rely mainly on visual cues, but synchronized acoustic and phonological signals provide complementary context that can improve segmentation precision.

Method: Multimodal framework integrating video, audio, and phonological inputs through cross-attention fusion for dynamic feature alignment, with contrastive learning to enhance cross-modal representation and maintain performance when audio is unavailable.

Result: Achieved Dice score of 0.95 and HD_95 of 4.20 mm on USC-75 rtMRI dataset, outperforming both unimodal and multimodal baselines. Ablation studies confirmed contributions of cross-attention and contrastive learning.

Conclusion: Integrative multimodal modeling with cross-attention fusion and contrastive learning significantly improves vocal tract segmentation accuracy and robustness in rtMRI analysis.

Abstract: Accurately segmenting articulatory structures in real-time magnetic resonance
imaging (rtMRI) remains challenging, as most existing methods rely almost
entirely on visual cues. Yet synchronized acoustic and phonological signals
provide complementary context that can enrich visual information and improve
precision. In this paper, we introduce VocSegMRI, a multimodal framework that
integrates video, audio, and phonological inputs through cross-attention fusion
for dynamic feature alignment. To further enhance cross-modal representation,
we incorporate a contrastive learning objective that improves segmentation
performance even when the audio modality is unavailable at inference. Evaluated
on a sub-set of USC-75 rtMRI dataset, our approach achieves state-of-the-art
performance, with a Dice score of 0.95 and a 95th percentile Hausdorff Distance
(HD_95) of 4.20 mm, outperforming both unimodal and multimodal baselines.
Ablation studies confirm the contributions of cross-attention and contrastive
learning to segmentation precision and robustness. These results highlight the
value of integrative multimodal modeling for accurate vocal tract analysis.

</details>


### [82] [Generative Image Coding with Diffusion Prior](https://arxiv.org/abs/2509.13768)
*Jianhui Chang*

Main category: cs.CV

TL;DR: A novel generative coding framework using diffusion priors for high-compression image coding that outperforms traditional codecs and existing methods in visual fidelity at low bitrates.


<details>
  <summary>Details</summary>
Motivation: As visual content becomes a mix of natural and AI-generated images, there's a need for efficient coding techniques that maintain perceptual quality at high compression ratios, which traditional and learned methods struggle with.

Method: Uses pre-optimized encoder to generate compressed-domain representations, integrates with pretrained diffusion models via lightweight adapter and attentive fusion module, and employs distribution renormalization for enhanced reconstruction fidelity.

Result: Outperforms existing methods in visual fidelity at low bitrates, achieves up to 79% compression improvement over H.266/VVC, and provides efficient solution for AI-generated content while being adaptable to broader content types.

Conclusion: The proposed framework effectively leverages pretrained diffusion models for superior compression performance with minimal retraining costs, offering a promising solution for modern visual content coding needs.

Abstract: As generative technologies advance, visual content has evolved into a complex
mix of natural and AI-generated images, driving the need for more efficient
coding techniques that prioritize perceptual quality. Traditional codecs and
learned methods struggle to maintain subjective quality at high compression
ratios, while existing generative approaches face challenges in visual fidelity
and generalization. To this end, we propose a novel generative coding framework
leveraging diffusion priors to enhance compression performance at low bitrates.
Our approach employs a pre-optimized encoder to generate generalized
compressed-domain representations, integrated with the pretrained model's
internal features via a lightweight adapter and an attentive fusion module.
This framework effectively leverages existing pretrained diffusion models and
enables efficient adaptation to different pretrained models for new
requirements with minimal retraining costs. We also introduce a distribution
renormalization method to further enhance reconstruction fidelity. Extensive
experiments show that our method (1) outperforms existing methods in visual
fidelity across low bitrates, (2) improves compression performance by up to 79%
over H.266/VVC, and (3) offers an efficient solution for AI-generated content
while being adaptable to broader content types.

</details>


### [83] [AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for Autonomous Driving](https://arxiv.org/abs/2509.13769)
*Yuechen Luo,Fang Li,Shaoqing Xu,Zhiyi Lai,Lei Yang,Qimao Chen,Ziang Luo,Zixun Xie,Shengyin Jiang,Jiaxin Liu,Long Chen,Bing Wang,Zhi-xin Yang*

Main category: cs.CV

TL;DR: AdaThinkDrive is a VLA framework with dual-mode reasoning (fast/slow thinking) for autonomous driving that adaptively applies CoT reasoning only when needed, achieving better performance with reduced inference time.


<details>
  <summary>Details</summary>
Motivation: Current CoT reasoning in VLA models for autonomous driving introduces unnecessary computational overhead in simple scenarios without improving decision quality, requiring a more adaptive approach.

Method: Pretrained on large-scale AD scenarios with QA and trajectory data, then fine-tuned with two-mode dataset (fast answering without CoT and slow thinking with CoT). Uses Adaptive Think Reward strategy with GRPO to reward selective CoT application by comparing trajectory quality across reasoning modes.

Result: Achieves PDMS of 90.3 on Navsim benchmark (1.7 points better than best vision-only baseline), surpasses never-Think and always-Think baselines by 2.0 and 1.4 PDMS points respectively, and reduces inference time by 14% compared to always-Think baseline.

Conclusion: AdaThinkDrive effectively balances accuracy and efficiency through adaptive reasoning, demonstrating superior performance while reducing computational overhead by applying CoT only when necessary.

Abstract: While reasoning technology like Chain of Thought (CoT) has been widely
adopted in Vision Language Action (VLA) models, it demonstrates promising
capabilities in end to end autonomous driving. However, recent efforts to
integrate CoT reasoning often fall short in simple scenarios, introducing
unnecessary computational overhead without improving decision quality. To
address this, we propose AdaThinkDrive, a novel VLA framework with a dual mode
reasoning mechanism inspired by fast and slow thinking. First, our framework is
pretrained on large scale autonomous driving (AD) scenarios using both question
answering (QA) and trajectory datasets to acquire world knowledge and driving
commonsense. During supervised fine tuning (SFT), we introduce a two mode
dataset, fast answering (w/o CoT) and slow thinking (with CoT), enabling the
model to distinguish between scenarios that require reasoning. Furthermore, an
Adaptive Think Reward strategy is proposed in conjunction with the Group
Relative Policy Optimization (GRPO), which rewards the model for selectively
applying CoT by comparing trajectory quality across different reasoning modes.
Extensive experiments on the Navsim benchmark show that AdaThinkDrive achieves
a PDMS of 90.3, surpassing the best vision only baseline by 1.7 points.
Moreover, ablations show that AdaThinkDrive surpasses both the never Think and
always Think baselines, improving PDMS by 2.0 and 1.4, respectively. It also
reduces inference time by 14% compared to the always Think baseline,
demonstrating its ability to balance accuracy and efficiency through adaptive
reasoning.

</details>


### [84] [Morphology-optimized Multi-Scale Fusion: Combining Local Artifacts and Mesoscopic Semantics for Deepfake Detection and Localization](https://arxiv.org/abs/2509.13776)
*Chao Shuai,Gaojian Wang,Kun Pan,Tong Wu,Fanli Jin,Haohan Tan,Mengxiang Li,Zhenguang Liu,Feng Lin,Kui Ren*

Main category: cs.CV

TL;DR: A novel deepfake detection approach that independently predicts manipulated regions from both local and global perspectives, using morphological operations to fuse outputs for improved localization accuracy and noise suppression.


<details>
  <summary>Details</summary>
Motivation: While deepfake detection accuracy has improved, precise localization of manipulated regions remains challenging. Existing approaches often neglect the complementary nature of local details and global semantic context, and naive fusion strategies amplify noise and errors.

Method: Proposes independent prediction of manipulated regions using both local and global perspectives, followed by morphological operations to fuse the outputs for noise suppression and enhanced spatial coherence.

Result: Extensive experiments demonstrate the effectiveness of each module in improving accuracy and robustness of forgery localization.

Conclusion: The proposed approach effectively addresses localization challenges in deepfake detection by leveraging complementary local and global information through optimized fusion strategies.

Abstract: While the pursuit of higher accuracy in deepfake detection remains a central
goal, there is an increasing demand for precise localization of manipulated
regions. Despite the remarkable progress made in classification-based
detection, accurately localizing forged areas remains a significant challenge.
A common strategy is to incorporate forged region annotations during model
training alongside manipulated images. However, such approaches often neglect
the complementary nature of local detail and global semantic context, resulting
in suboptimal localization performance. Moreover, an often-overlooked aspect is
the fusion strategy between local and global predictions. Naively combining the
outputs from both branches can amplify noise and errors, thereby undermining
the effectiveness of the localization.
  To address these issues, we propose a novel approach that independently
predicts manipulated regions using both local and global perspectives. We
employ morphological operations to fuse the outputs, effectively suppressing
noise while enhancing spatial coherence. Extensive experiments reveal the
effectiveness of each module in improving the accuracy and robustness of
forgery localization.

</details>


### [85] [CETUS: Causal Event-Driven Temporal Modeling With Unified Variable-Rate Scheduling](https://arxiv.org/abs/2509.13784)
*Hanfang Liang,Bing Wang,Shizhen Zhang,Wen Jiang,Yizhuo Yang,Weixiang Guo,Shenghai Yuan*

Main category: cs.CV

TL;DR: A novel Mamba-based architecture that directly processes raw event streams without intermediate representations, using spatial encoding and state space models for efficient real-time event processing with adaptive speed control.


<details>
  <summary>Details</summary>
Motivation: Existing event camera methods require predefined time windows (introducing latency) or use computationally expensive pointwise detection that prevents real-time efficiency. There's a need for direct raw event stream processing without intermediate representations.

Method: Variable-Rate Spatial Event Mamba architecture with lightweight causal spatial neighborhood encoder to capture local geometric relations, followed by Mamba-based state space models for temporal modeling with linear complexity. Adaptive controller adjusts processing speed based on event rate.

Result: The method achieves optimal balance between window latency and inference latency, enabling direct processing of raw event streams without intermediate representations like frames, voxel grids, or point clouds.

Conclusion: The proposed architecture overcomes limitations of existing methods by providing efficient, scalable temporal modeling with linear complexity and adaptive processing speed, making real-time event stream processing feasible.

Abstract: Event cameras capture asynchronous pixel-level brightness changes with
microsecond temporal resolution, offering unique advantages for high-speed
vision tasks. Existing methods often convert event streams into intermediate
representations such as frames, voxel grids, or point clouds, which inevitably
require predefined time windows and thus introduce window latency. Meanwhile,
pointwise detection methods face computational challenges that prevent
real-time efficiency due to their high computational cost. To overcome these
limitations, we propose the Variable-Rate Spatial Event Mamba, a novel
architecture that directly processes raw event streams without intermediate
representations. Our method introduces a lightweight causal spatial
neighborhood encoder to efficiently capture local geometric relations, followed
by Mamba-based state space models for scalable temporal modeling with linear
complexity. During inference, a controller adaptively adjusts the processing
speed according to the event rate, achieving an optimal balance between window
latency and inference latency.

</details>


### [86] [BWCache: Accelerating Video Diffusion Transformers through Block-Wise Caching](https://arxiv.org/abs/2509.13789)
*Hanshuai Cui,Zhiqing Tang,Zhifei Xu,Zhi Yao,Wenyi Zeng,Weijia Jia*

Main category: cs.CV

TL;DR: BWCache is a training-free acceleration method that uses dynamic caching and reuse of DiT block features across diffusion timesteps to reduce computational redundancy in video generation, achieving up to 2.24× speedup without quality loss.


<details>
  <summary>Details</summary>
Motivation: Diffusion Transformers (DiTs) have high latency due to sequential denoising, limiting real-time applications. Existing acceleration methods either degrade quality or fail to properly reuse intermediate features. Analysis shows DiT blocks are the main latency source with U-shaped feature similarity patterns indicating computational redundancy.

Method: Proposes Block-Wise Caching (BWCache) that dynamically caches and reuses features from DiT blocks across timesteps. Uses a similarity indicator to trigger feature reuse only when differences between adjacent timestep features are below a threshold, ensuring quality preservation.

Result: Extensive experiments show BWCache achieves up to 2.24× speedup while maintaining comparable visual quality across several video diffusion models.

Conclusion: BWCache effectively reduces computational redundancy in DiT-based video generation through intelligent feature caching and reuse, enabling significant acceleration without compromising visual fidelity.

Abstract: Recent advancements in Diffusion Transformers (DiTs) have established them as
the state-of-the-art method for video generation. However, their inherently
sequential denoising process results in inevitable latency, limiting real-world
applicability. Existing acceleration methods either compromise visual quality
due to architectural modifications or fail to reuse intermediate features at
proper granularity. Our analysis reveals that DiT blocks are the primary
contributors to inference latency. Across diffusion timesteps, the feature
variations of DiT blocks exhibit a U-shaped pattern with high similarity during
intermediate timesteps, which suggests substantial computational redundancy. In
this paper, we propose Block-Wise Caching (BWCache), a training-free method to
accelerate DiT-based video generation. BWCache dynamically caches and reuses
features from DiT blocks across diffusion timesteps. Furthermore, we introduce
a similarity indicator that triggers feature reuse only when the differences
between block features at adjacent timesteps fall below a threshold, thereby
minimizing redundant computations while maintaining visual fidelity. Extensive
experiments on several video diffusion models demonstrate that BWCache achieves
up to 2.24$\times$ speedup with comparable visual quality.

</details>


### [87] [Diving into Mitigating Hallucinations from a Vision Perspective for Large Vision-Language Models](https://arxiv.org/abs/2509.13836)
*Weihang Wang,Xinhao Li,Ziyue Wang,Yan Pang,Jielei Zhang,Peiyi Li,Qiang Zhang,Longwen Gao*

Main category: cs.CV

TL;DR: VHBench-10 benchmark reveals visual encoders' distinct hallucination patterns, leading to VisionWeaver - a context-aware routing network that dynamically aggregates features from multiple experts to significantly reduce object hallucinations in LVLMs.


<details>
  <summary>Details</summary>
Motivation: Object hallucination in Large Vision-Language Models limits real-world applicability, and different visual encoders have distinct inductive biases that cause diverse hallucination patterns that aren't captured by existing coarse-grained benchmarks.

Method: Introduced VHBench-10 benchmark with ~10,000 samples across 10 fine-grained hallucination categories, then proposed VisionWeaver - a Context-Aware Routing Network that uses global visual features to generate routing signals for dynamically aggregating features from multiple specialized experts.

Result: Evaluations confirmed encoders exhibit unique hallucination characteristics, and comprehensive experiments showed VisionWeaver effectively reduces hallucinations and improves overall model performance.

Conclusion: Fine-grained analysis of visual encoder biases is crucial for addressing object hallucination, and dynamic feature aggregation through context-aware routing provides an effective solution for improving LVLM reliability.

Abstract: Object hallucination in Large Vision-Language Models (LVLMs) significantly
impedes their real-world applicability. As the primary component for accurately
interpreting visual information, the choice of visual encoder is pivotal. We
hypothesize that the diverse training paradigms employed by different visual
encoders instill them with distinct inductive biases, which leads to their
diverse hallucination performances. Existing benchmarks typically focus on
coarse-grained hallucination detection and fail to capture the diverse
hallucinations elaborated in our hypothesis. To systematically analyze these
effects, we introduce VHBench-10, a comprehensive benchmark with approximately
10,000 samples for evaluating LVLMs across ten fine-grained hallucination
categories. Our evaluations confirm encoders exhibit unique hallucination
characteristics. Building on these insights and the suboptimality of simple
feature fusion, we propose VisionWeaver, a novel Context-Aware Routing Network.
It employs global visual features to generate routing signals, dynamically
aggregating visual features from multiple specialized experts. Comprehensive
experiments confirm the effectiveness of VisionWeaver in significantly reducing
hallucinations and improving overall model performance.

</details>


### [88] [Bridging the Synthetic-Real Gap: Supervised Domain Adaptation for Robust Spacecraft 6-DoF Pose Estimation](https://arxiv.org/abs/2509.13792)
*Inder Pal Singh,Nidhal Eddine Chenni,Abd El Rahman Shabayek,Arunkumar Rathinam,Djamila Aouada*

Main category: cs.CV

TL;DR: A supervised domain adaptation framework for spacecraft pose estimation that uses both synthetic and limited real labeled data to bridge the domain gap, achieving oracle-level performance with only 5% labeled target data.


<details>
  <summary>Details</summary>
Motivation: Spacecraft pose estimation suffers from performance degradation when moving from synthetic to real imagery due to domain gap issues, and existing unsupervised approaches underperform when limited labeled real data is available.

Method: Proposes a supervised domain adaptation framework based on Learning Invariant Representation and Risk (LIRR) paradigm that jointly optimizes domain-invariant representations and task-specific risk using both synthetic and limited real labeled data.

Result: Extensive experiments on SPEED+ benchmark show the method consistently outperforms source-only, fine-tuning, and oracle baselines, matching or surpassing oracle performance with only 5% labeled target data.

Conclusion: The framework provides a lightweight, backbone-agnostic, and computationally efficient solution for robust spacecraft pose estimation in real-world space environments.

Abstract: Spacecraft Pose Estimation (SPE) is a fundamental capability for autonomous
space operations such as rendezvous, docking, and in-orbit servicing. Hybrid
pipelines that combine object detection, keypoint regression, and
Perspective-n-Point (PnP) solvers have recently achieved strong results on
synthetic datasets, yet their performance deteriorates sharply on real or
lab-generated imagery due to the persistent synthetic-to-real domain gap.
Existing unsupervised domain adaptation approaches aim to mitigate this issue
but often underperform when a modest number of labeled target samples are
available. In this work, we propose the first Supervised Domain Adaptation
(SDA) framework tailored for SPE keypoint regression. Building on the Learning
Invariant Representation and Risk (LIRR) paradigm, our method jointly optimizes
domain-invariant representations and task-specific risk using both labeled
synthetic and limited labeled real data, thereby reducing generalization error
under domain shift. Extensive experiments on the SPEED+ benchmark demonstrate
that our approach consistently outperforms source-only, fine-tuning, and oracle
baselines. Notably, with only 5% labeled target data, our method matches or
surpasses oracle performance trained on larger fractions of labeled data. The
framework is lightweight, backbone-agnostic, and computationally efficient,
offering a practical pathway toward robust and deployable spacecraft pose
estimation in real-world space environments.

</details>


### [89] [SWA-PF: Semantic-Weighted Adaptive Particle Filter for Memory-Efficient 4-DoF UAV Localization in GNSS-Denied Environments](https://arxiv.org/abs/2509.13795)
*Jiayu Yuan,Ming Dai,Enhui Zheng,Chao Su,Nanxing Chen,Qiming Hu,Shibo Zhu,Yibin Cao*

Main category: cs.CV

TL;DR: A novel Semantic-Weighted Adaptive Particle Filter (SWA-PF) method for UAV localization in GNSS-denied environments, using semantic features from UAV and satellite imagery with 10x computational efficiency and sub-10m positioning accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing UAV localization systems face limitations in dataset availability, real-time performance, environmental sensitivity, and generalization in dynamic environments, particularly for variable altitude scenarios.

Method: Proposes SWA-PF method with semantic weighting mechanism and optimized particle filtering architecture, integrating semantic features from UAV-captured images and satellite imagery. Uses a new Multi-Altitude Flight Segments dataset (MAFS).

Result: Achieves 10x computational efficiency gain over feature extraction methods, maintains global positioning errors below 10 meters, and enables rapid 4-DoF pose estimation within seconds using low-resolution satellite maps.

Conclusion: The proposed method effectively addresses limitations of existing approaches by combining semantic features with optimized filtering, providing efficient and accurate UAV localization in GNSS-denied environments with variable altitudes.

Abstract: Vision-based Unmanned Aerial Vehicle (UAV) localization systems have been
extensively investigated for Global Navigation Satellite System (GNSS)-denied
environments. However, existing retrieval-based approaches face limitations in
dataset availability and persistent challenges including suboptimal real-time
performance, environmental sensitivity, and limited generalization capability,
particularly in dynamic or temporally varying environments. To overcome these
limitations, we present a large-scale Multi-Altitude Flight Segments dataset
(MAFS) for variable altitude scenarios and propose a novel Semantic-Weighted
Adaptive Particle Filter (SWA-PF) method. This approach integrates robust
semantic features from both UAV-captured images and satellite imagery through
two key innovations: a semantic weighting mechanism and an optimized particle
filtering architecture. Evaluated using our dataset, the proposed method
achieves 10x computational efficiency gain over feature extraction methods,
maintains global positioning errors below 10 meters, and enables rapid 4 degree
of freedom (4-DoF) pose estimation within seconds using accessible
low-resolution satellite maps. Code and dataset will be available at
https://github.com/YuanJiayuuu/SWA-PF.

</details>


### [90] [Masked Feature Modeling Enhances Adaptive Segmentation](https://arxiv.org/abs/2509.13801)
*Wenlve Zhou,Zhiheng Zhou,Tiantao Xian,Yikui Zhai,Weibin Wu,Biyun Ma*

Main category: cs.CV

TL;DR: Masked Feature Modeling (MFM) is a novel auxiliary task for unsupervised domain adaptation in semantic segmentation that performs feature masking and reconstruction in feature space, improving performance without inference overhead.


<details>
  <summary>Details</summary>
Motivation: Existing masked modeling approaches are underexplored in UDA for semantic segmentation due to architectural incompatibility and misaligned optimization objectives with contrastive learning methods.

Method: Proposes MFM that masks and reconstructs features directly in feature space using a lightweight Rebuilder module, leveraging the segmentation decoder to classify reconstructed features for tight coupling with the main task.

Result: Extensive experiments show MFM consistently enhances segmentation performance across various architectures and UDA benchmarks.

Conclusion: MFM provides a simple, efficient, and generalizable strategy for unsupervised domain-adaptive semantic segmentation without modifying inference pipelines or adding computational overhead.

Abstract: Unsupervised domain adaptation (UDA) for semantic segmentation aims to
transfer models from a labeled source domain to an unlabeled target domain.
While auxiliary self-supervised tasks-particularly contrastive learning-have
improved feature discriminability, masked modeling approaches remain
underexplored in this setting, largely due to architectural incompatibility and
misaligned optimization objectives. We propose Masked Feature Modeling (MFM), a
novel auxiliary task that performs feature masking and reconstruction directly
in the feature space. Unlike existing masked modeling methods that reconstruct
low-level inputs or perceptual features (e.g., HOG or visual tokens), MFM
aligns its learning target with the main segmentation task, ensuring
compatibility with standard architectures like DeepLab and DAFormer without
modifying the inference pipeline. To facilitate effective reconstruction, we
introduce a lightweight auxiliary module, Rebuilder, which is trained jointly
but discarded during inference, adding zero computational overhead at test
time. Crucially, MFM leverages the segmentation decoder to classify the
reconstructed features, tightly coupling the auxiliary objective with the
pixel-wise prediction task to avoid interference with the primary task.
Extensive experiments across various architectures and UDA benchmarks
demonstrate that MFM consistently enhances segmentation performance, offering a
simple, efficient, and generalizable strategy for unsupervised domain-adaptive
semantic segmentation.

</details>


### [91] [Data-Efficient Spectral Classification of Hyperspectral Data Using MiniROCKET and HDC-MiniROCKET](https://arxiv.org/abs/2509.13809)
*Nick Theisen,Kenny Schlegel,Dietrich Paulus,Peer Neubert*

Main category: cs.CV

TL;DR: MiniROCKET and HDC-MiniROCKET outperform 1D-Justo-LiuNet for spectral classification with limited training data, while maintaining similar performance with sufficient data.


<details>
  <summary>Details</summary>
Motivation: Spectral classification using only spectral information has advantages like smaller model size and less training data requirement. However, current state-of-the-art 1D-Justo-LiuNet performs poorly with limited training data.

Method: Investigated MiniROCKET and HDC-MiniROCKET for spectral classification. These models extract well-engineered features without trainable parameters in the feature extraction part, making them less vulnerable to limited training data.

Result: MiniROCKET outperforms 1D-Justo-LiuNet in limited data scenarios and is mostly on par with it in general cases, despite having more parameters.

Conclusion: MiniROCKET-based approaches provide a robust solution for spectral classification, particularly effective when training data is limited, while maintaining competitive performance across different data scenarios.

Abstract: The classification of pixel spectra of hyperspectral images, i.e. spectral
classification, is used in many fields ranging from agricultural, over medical
to remote sensing applications and is currently also expanding to areas such as
autonomous driving. Even though for full hyperspectral images the
best-performing methods exploit spatial-spectral information, performing
classification solely on spectral information has its own advantages, e.g.
smaller model size and thus less data required for training. Moreover, spectral
information is complementary to spatial information and improvements on either
part can be used to improve spatial-spectral approaches in the future.
Recently, 1D-Justo-LiuNet was proposed as a particularly efficient model with
very few parameters, which currently defines the state of the art in spectral
classification. However, we show that with limited training data the model
performance deteriorates. Therefore, we investigate MiniROCKET and
HDC-MiniROCKET for spectral classification to mitigate that problem. The model
extracts well-engineered features without trainable parameters in the feature
extraction part and is therefore less vulnerable to limited training data. We
show that even though MiniROCKET has more parameters it outperforms
1D-Justo-LiuNet in limited data scenarios and is mostly on par with it in the
general case

</details>


### [92] [Semi-MoE: Mixture-of-Experts meets Semi-Supervised Histopathology Segmentation](https://arxiv.org/abs/2509.13834)
*Nguyen Lan Vi Vu,Thanh-Huy Nguyen,Thien Nguyen,Daisuke Kihara,Tianyang Wang,Xingjian Li,Min Xu*

Main category: cs.CV

TL;DR: Semi-MOE is the first multi-task Mixture-of-Experts framework for semi-supervised histopathology image segmentation, using three specialized experts and adaptive loss balancing to address noisy pseudo-labels and outperform state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Existing semi-supervised learning methods struggle with noisy pseudo-labels in histopathology image segmentation due to ambiguous gland boundaries and morphological misclassification, creating a need for more robust approaches.

Method: Uses three expert networks: main segmentation expert, signed distance field regression expert, and boundary prediction expert. Features Multi-Gating Pseudo-labeling module for dynamic feature aggregation and Adaptive Multi-Objective Loss for automatic balancing of learning objectives.

Result: Extensive experiments on GlaS and CRAG benchmarks show the method outperforms state-of-the-art approaches in low-label settings.

Conclusion: The approach demonstrates the potential of MoE-based architectures in advancing semi-supervised segmentation for histopathology images, providing a robust solution to pseudo-label noise and morphological challenges.

Abstract: Semi-supervised learning has been employed to alleviate the need for
extensive labeled data for histopathology image segmentation, but existing
methods struggle with noisy pseudo-labels due to ambiguous gland boundaries and
morphological misclassification. This paper introduces Semi-MOE, to the best of
our knowledge, the first multi-task Mixture-of-Experts framework for
semi-supervised histopathology image segmentation. Our approach leverages three
specialized expert networks: A main segmentation expert, a signed distance
field regression expert, and a boundary prediction expert, each dedicated to
capturing distinct morphological features. Subsequently, the Multi-Gating
Pseudo-labeling module dynamically aggregates expert features, enabling a
robust fuse-and-refine pseudo-labeling mechanism. Furthermore, to eliminate
manual tuning while dynamically balancing multiple learning objectives, we
propose an Adaptive Multi-Objective Loss. Extensive experiments on GlaS and
CRAG benchmarks show that our method outperforms state-of-the-art approaches in
low-label settings, highlighting the potential of MoE-based architectures in
advancing semi-supervised segmentation. Our code is available at
https://github.com/vnlvi2k3/Semi-MoE.

</details>


### [93] [Consistent View Alignment Improves Foundation Models for 3D Medical Image Segmentation](https://arxiv.org/abs/2509.13846)
*Puru Vaish,Felix Meister,Tobias Heimann,Christoph Brune,Jelmer M. Wolterink*

Main category: cs.CV

TL;DR: The paper challenges the assumption that uncorrelated views automatically learn meaningful representations, showing that explicit structure induction is needed. It proposes Consistent View Alignment to align complementary information without false positives, achieving top results in MICCAI 2025 SSL3D challenge.


<details>
  <summary>Details</summary>
Motivation: Recent representation learning approaches assume uncorrelated views naturally learn meaningful representations, but the authors demonstrate this structure doesn't emerge naturally and must be explicitly induced.

Method: Proposes Consistent View Alignment method that aligns representations from different data views to capture complementary information while avoiding false positive alignments.

Result: The method achieved 1st and 2nd place in MICCAI 2025 SSL3D challenge using Primus vision transformer and ResEnc CNN respectively. Improves downstream task performance.

Conclusion: Structured view alignment is critical for learning effective representations, and explicit induction of meaningful latent space structure significantly improves self-supervised learning performance.

Abstract: Many recent approaches in representation learning implicitly assume that
uncorrelated views of a data point are sufficient to learn meaningful
representations for various downstream tasks. In this work, we challenge this
assumption and demonstrate that meaningful structure in the latent space does
not emerge naturally. Instead, it must be explicitly induced. We propose a
method that aligns representations from different views of the data to align
complementary information without inducing false positives. Our experiments
show that our proposed self-supervised learning method, Consistent View
Alignment, improves performance for downstream tasks, highlighting the critical
role of structured view alignment in learning effective representations. Our
method achieved first and second place in the MICCAI 2025 SSL3D challenge when
using a Primus vision transformer and ResEnc convolutional neural network,
respectively. The code and pretrained model weights are released at
https://github.com/Tenbatsu24/LatentCampus.

</details>


### [94] [SpecDiff: Accelerating Diffusion Model Inference with Self-Speculation](https://arxiv.org/abs/2509.13848)
*Jiayi Pan,Jiaming Xu,Yongkang Zhou,Guohao Dai*

Main category: cs.CV

TL;DR: SpecDiff is a training-free multi-level feature caching strategy for diffusion models that uses self-speculative future information alongside historical information to achieve significant speedups (2.7-3.2×) with minimal quality loss.


<details>
  <summary>Details</summary>
Motivation: Existing feature caching methods rely solely on historical information, which constrains both accuracy and speed performance. The authors aim to overcome the speedup-accuracy trade-off bottleneck by incorporating future information.

Method: Proposes SpecDiff with two key algorithms: (1) Feature selection algorithm based on self-speculative information that calculates dynamic importance scores for tokens, and (2) Multi-level feature classification algorithm that categorizes tokens by importance score differences and uses multi-level feature calculation.

Result: Achieves average 2.80×, 2.74×, and 3.17× speedup on Stable Diffusion 3, 3.5, and FLUX respectively with negligible quality loss compared to RFlow on NVIDIA A800-80GB GPU.

Conclusion: SpecDiff successfully overcomes the speedup-accuracy trade-off by merging speculative and historical information, pushing the Pareto frontier of speedup and accuracy in efficient diffusion model inference.

Abstract: Feature caching has recently emerged as a promising method for diffusion
model acceleration. It effectively alleviates the inefficiency problem caused
by high computational requirements by caching similar features in the inference
process of the diffusion model. In this paper, we analyze existing feature
caching methods from the perspective of information utilization, and point out
that relying solely on historical information will lead to constrained accuracy
and speed performance. And we propose a novel paradigm that introduces future
information via self-speculation based on the information similarity at the
same time step across different iteration times. Based on this paradigm, we
present \textit{SpecDiff}, a training-free multi-level feature caching strategy
including a cached feature selection algorithm and a multi-level feature
classification algorithm. (1) Feature selection algorithm based on
self-speculative information. \textit{SpecDiff} determines a dynamic importance
score for each token based on self-speculative information and historical
information, and performs cached feature selection through the importance
score. (2) Multi-level feature classification algorithm based on feature
importance scores. \textit{SpecDiff} classifies tokens by leveraging the
differences in feature importance scores and introduces a multi-level feature
calculation strategy. Extensive experiments show that \textit{SpecDiff}
achieves average 2.80 \times, 2.74 \times , and 3.17\times speedup with
negligible quality loss in Stable Diffusion 3, 3.5, and FLUX compared to RFlow
on NVIDIA A800-80GB GPU. By merging speculative and historical information,
\textit{SpecDiff} overcomes the speedup-accuracy trade-off bottleneck, pushing
the Pareto frontier of speedup and accuracy in the efficient diffusion model
inference.

</details>


### [95] [Dense Video Understanding with Gated Residual Tokenization](https://arxiv.org/abs/2509.14199)
*Haichao Zhang,Wenhao Chai,Shwai He,Ang Li,Yun Fu*

Main category: cs.CV

TL;DR: DVU enables high-FPS video understanding with reduced tokenization overhead using Gated Residual Tokenization (GRT), outperforming existing VLLMs on the new DIVE benchmark for dense temporal reasoning.


<details>
  <summary>Details</summary>
Motivation: Current video LLMs rely on low-frame-rate sampling, discarding dense temporal information needed for tasks requiring precise temporal alignment like lecture comprehension, while avoiding high tokenization costs.

Method: Gated Residual Tokenization (GRT) - a two-stage framework: (1) Motion-Compensated Inter-Gated Tokenization uses motion estimation to skip static regions, achieving sub-linear token growth; (2) Semantic-Scene Intra-Tokenization Merging fuses tokens across static regions to reduce redundancy.

Result: GRT outperforms larger VLLM baselines on the DIVE benchmark and scales positively with FPS, demonstrating efficient high-FPS video understanding.

Conclusion: Dense temporal information is crucial for video understanding, and GRT provides an efficient, scalable solution for high-FPS comprehension while reducing tokenization overhead.

Abstract: High temporal resolution is essential for capturing fine-grained details in
video understanding. However, current video large language models (VLLMs) and
benchmarks mostly rely on low-frame-rate sampling, such as uniform sampling or
keyframe selection, discarding dense temporal information. This compromise
avoids the high cost of tokenizing every frame, which otherwise leads to
redundant computation and linear token growth as video length increases. While
this trade-off works for slowly changing content, it fails for tasks like
lecture comprehension, where information appears in nearly every frame and
requires precise temporal alignment. To address this gap, we introduce Dense
Video Understanding (DVU), which enables high-FPS video comprehension by
reducing both tokenization time and token overhead. Existing benchmarks are
also limited, as their QA pairs focus on coarse content changes. We therefore
propose DIVE (Dense Information Video Evaluation), the first benchmark designed
for dense temporal reasoning. To make DVU practical, we present Gated Residual
Tokenization (GRT), a two-stage framework: (1) Motion-Compensated Inter-Gated
Tokenization uses pixel-level motion estimation to skip static regions during
tokenization, achieving sub-linear growth in token count and compute. (2)
Semantic-Scene Intra-Tokenization Merging fuses tokens across static regions
within a scene, further reducing redundancy while preserving dynamic semantics.
Experiments on DIVE show that GRT outperforms larger VLLM baselines and scales
positively with FPS. These results highlight the importance of dense temporal
information and demonstrate that GRT enables efficient, scalable high-FPS video
understanding.

</details>


### [96] [EDITS: Enhancing Dataset Distillation with Implicit Textual Semantics](https://arxiv.org/abs/2509.13858)
*Qianxin Xia,Jiawei Du,Guoming Lu,Zhiyong Shu,Jielei Wang*

Main category: cs.CV

TL;DR: EDITS is a novel dataset distillation framework that leverages textual semantics from vision-language models to enhance synthetic dataset quality by combining image features with generated text descriptions.


<details>
  <summary>Details</summary>
Motivation: Traditional dataset distillation methods focus on low-level visual features but neglect high-level semantic and structural information in images, limiting their effectiveness.

Method: Uses Vision Language Model to generate external texts, fuses them with image features via Global Semantic Query, creates image/text prototypes through Local Semantic Awareness, and generates final synthetic dataset using Dual Prototype Guidance with a diffusion model.

Result: Extensive experiments confirm the effectiveness of the method, achieving enhanced distillation performance compared to traditional techniques.

Conclusion: EDITS successfully exploits implicit textual semantics within image data to improve dataset distillation, preserving competitive model performance while enabling highly efficient learning.

Abstract: Dataset distillation aims to synthesize a compact dataset from the original
large-scale one, enabling highly efficient learning while preserving
competitive model performance. However, traditional techniques primarily
capture low-level visual features, neglecting the high-level semantic and
structural information inherent in images. In this paper, we propose EDITS, a
novel framework that exploits the implicit textual semantics within the image
data to achieve enhanced distillation. First, external texts generated by a
Vision Language Model (VLM) are fused with image features through a Global
Semantic Query module, forming the prior clustered buffer. Local Semantic
Awareness then selects representative samples from the buffer to construct
image and text prototypes, with the latter produced by guiding a Large Language
Model (LLM) with meticulously crafted prompt. Ultimately, Dual Prototype
Guidance strategy generates the final synthetic dataset through a diffusion
model. Extensive experiments confirm the effectiveness of our method.Source
code is available in: https://github.com/einsteinxia/EDITS.

</details>


### [97] [LamiGauss: Pitching Radiative Gaussian for Sparse-View X-ray Laminography Reconstruction](https://arxiv.org/abs/2509.13863)
*Chu Chen,Ander Biguri,Jean-Michel Morel,Raymond H. Chan,Carola-Bibiane Schönlieb,Jizhou Li*

Main category: cs.CV

TL;DR: LamiGauss is a novel X-ray laminography reconstruction algorithm that uses Gaussian Splatting with a specialized detector-to-world transformation to achieve high-quality 3D reconstructions from sparse-view projections, outperforming traditional methods with only 3% of full views.


<details>
  <summary>Details</summary>
Motivation: Traditional CT struggles with plate-like structures due to geometric constraints, and existing laminography methods have difficulty reconstructing high-quality volumes from sparse-view acquisitions, particularly with common laminographic artifacts.

Method: Combines Gaussian Splatting radiative rasterization with a dedicated detector-to-world transformation model incorporating laminographic tilt angle. Uses an initialization strategy that filters out laminographic artifacts from preliminary reconstruction to prevent redundant Gaussians and concentrate model capacity on genuine structures.

Result: Achieves superior performance over iterative methods optimized on full datasets using only 3% of full views. Extensive experiments on synthetic and real datasets demonstrate effectiveness and superiority over existing techniques.

Conclusion: LamiGauss enables accurate and efficient reconstruction directly from sparse projections with limited data, effectively addressing the challenges of laminographic reconstruction for plate-like structures in applications like microchips and battery materials.

Abstract: X-ray Computed Laminography (CL) is essential for non-destructive inspection
of plate-like structures in applications such as microchips and composite
battery materials, where traditional computed tomography (CT) struggles due to
geometric constraints. However, reconstructing high-quality volumes from
laminographic projections remains challenging, particularly under highly
sparse-view acquisition conditions. In this paper, we propose a reconstruction
algorithm, namely LamiGauss, that combines Gaussian Splatting radiative
rasterization with a dedicated detector-to-world transformation model
incorporating the laminographic tilt angle. LamiGauss leverages an
initialization strategy that explicitly filters out common laminographic
artifacts from the preliminary reconstruction, preventing redundant Gaussians
from being allocated to false structures and thereby concentrating model
capacity on representing the genuine object. Our approach effectively optimizes
directly from sparse projections, enabling accurate and efficient
reconstruction with limited data. Extensive experiments on both synthetic and
real datasets demonstrate the effectiveness and superiority of the proposed
method over existing techniques. LamiGauss uses only 3$\%$ of full views to
achieve superior performance over the iterative method optimized on a full
dataset.

</details>


### [98] [Distractor-Aware Memory-Based Visual Object Tracking](https://arxiv.org/abs/2509.13864)
*Jovana Videnovic,Matej Kristan,Alan Lukezic*

Main category: cs.CV

TL;DR: DAM4SAM enhances SAM2 with distractor-aware memory module and introspection management to improve tracking performance against visually similar objects, achieving SOTA results on multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Memory-based video segmentation methods like SAM2 perform well but struggle with visual object tracking when facing distractors (visually similar objects), causing tracking drift and poor redetection after occlusion.

Method: Proposes a distractor-aware drop-in memory module and introspection-based management method for SAM2, creating DAM4SAM. Also constructs DiDi dataset for distractor analysis.

Result: Outperforms SAM2.1 on 13 benchmarks, sets new SOTA on 10. Integration with EfficientTAM yields 11% improvement, matches non-real-time SAM2.1-L performance. EdgeTAM integration shows 4% boost with good generalization.

Conclusion: The distractor-aware memory module effectively addresses tracking challenges with distractors, demonstrating strong performance improvements and generalization across different tracker architectures.

Abstract: Recent emergence of memory-based video segmentation methods such as SAM2 has
led to models with excellent performance in segmentation tasks, achieving
leading results on numerous benchmarks. However, these modes are not fully
adjusted for visual object tracking, where distractors (i.e., objects visually
similar to the target) pose a key challenge. In this paper we propose a
distractor-aware drop-in memory module and introspection-based management
method for SAM2, leading to DAM4SAM. Our design effectively reduces the
tracking drift toward distractors and improves redetection capability after
object occlusion. To facilitate the analysis of tracking in the presence of
distractors, we construct DiDi, a Distractor-Distilled dataset. DAM4SAM
outperforms SAM2.1 on thirteen benchmarks and sets new state-of-the-art results
on ten. Furthermore, integrating the proposed distractor-aware memory into a
real-time tracker EfficientTAM leads to 11% improvement and matches tracking
quality of the non-real-time SAM2.1-L on multiple tracking and segmentation
benchmarks, while integration with edge-based tracker EdgeTAM delivers 4%
performance boost, demonstrating a very good generalization across
architectures.

</details>


### [99] [Invisible Yet Detected: PelFANet with Attention-Guided Anatomical Fusion for Pelvic Fracture Diagnosis](https://arxiv.org/abs/2509.13873)
*Siam Tahsin Bhuiyan,Rashedur Rahman,Sefatul Wasi,Naomi Yagi,Syoji Kobashi,Ashraful Islam,Saadia Binte Alam*

Main category: cs.CV

TL;DR: PelFANet is a dual-stream attention network that fuses raw pelvic X-rays with segmented bone images to improve fracture classification, achieving superior performance on both visible and invisible fractures.


<details>
  <summary>Details</summary>
Motivation: Pelvic fractures are challenging to diagnose when fracture signs are subtle or invisible on standard radiographs, requiring improved detection methods.

Method: Dual-stream attention network using Fused Attention Blocks (FABlocks) to exchange features between raw X-rays and segmented bone images, trained with a two-stage segmentation-guided pipeline.

Result: Achieves 88.68% accuracy and 0.9334 AUC on visible fractures, and 82.29% accuracy and 0.8688 AUC on invisible fractures despite not being trained on them.

Conclusion: Anatomy-aware dual-input architectures show clinical potential for robust fracture detection, especially in cases with subtle radiographic presentations.

Abstract: Pelvic fractures pose significant diagnostic challenges, particularly in
cases where fracture signs are subtle or invisible on standard radiographs. To
address this, we introduce PelFANet, a dual-stream attention network that fuses
raw pelvic X-rays with segmented bone images to improve fracture
classification. The network em-ploys Fused Attention Blocks (FABlocks) to
iteratively exchange and refine fea-tures from both inputs, capturing global
context and localized anatomical detail. Trained in a two-stage pipeline with a
segmentation-guided approach, PelFANet demonstrates superior performance over
conventional methods. On the AMERI dataset, it achieves 88.68% accuracy and
0.9334 AUC on visible fractures, while generalizing effectively to invisible
fracture cases with 82.29% accuracy and 0.8688 AUC, despite not being trained
on them. These results highlight the clini-cal potential of anatomy-aware
dual-input architectures for robust fracture detec-tion, especially in
scenarios with subtle radiographic presentations.

</details>


### [100] [EvHand-FPV: Efficient Event-Based 3D Hand Tracking from First-Person View](https://arxiv.org/abs/2509.13883)
*Zhen Xu,Guorui Lu,Chang Gao,Qinyu Chen*

Main category: cs.CV

TL;DR: EvHand-FPV is a lightweight framework for 3D hand tracking from a single event camera, achieving 89% reduction in parameters and FLOPs while improving accuracy, making it suitable for on-device XR applications.


<details>
  <summary>Details</summary>
Motivation: Frame-based hand tracking methods struggle with accuracy, latency, and energy efficiency in resource-constrained XR devices. Event cameras offer high temporal resolution with low power consumption but lack egocentric benchmarks.

Method: Uses wrist-based ROI localization with geometric cues, end-to-end mapping with embedded ROI offsets to reduce computation, and multi-task learning with auxiliary geometric feature head. Combines synthetic training data with 3D labels and real event data with 2D labels.

Result: Improves 2D-AUCp from 0.77 to 0.85, reduces parameters by 89% (11.2M to 1.2M), reduces FLOPs by 89% (1.648G to 0.185G), and maintains 3D-AUCp of 0.84 on synthetic data.

Conclusion: EvHand-FPV demonstrates accurate and efficient egocentric event-based hand tracking suitable for on-device XR applications, addressing the limitations of traditional frame-based methods.

Abstract: Hand tracking holds great promise for intuitive interaction paradigms, but
frame-based methods often struggle to meet the requirements of accuracy, low
latency, and energy efficiency, especially in resource-constrained settings
such as Extended Reality (XR) devices. Event cameras provide $\mu$s-level
temporal resolution at mW-level power by asynchronously sensing brightness
changes. In this work, we present EvHand-FPV, a lightweight framework for
egocentric First-Person-View 3D hand tracking from a single event camera. We
construct an event-based FPV dataset that couples synthetic training data with
3D labels and real event data with 2D labels for evaluation to address the
scarcity of egocentric benchmarks. EvHand-FPV also introduces a wrist-based
region of interest (ROI) that localizes the hand region via geometric cues,
combined with an end-to-end mapping strategy that embeds ROI offsets into the
network to reduce computation without explicit reconstruction, and a multi-task
learning strategy with an auxiliary geometric feature head that improves
representations without test-time overhead. On our real FPV test set,
EvHand-FPV improves 2D-AUCp from 0.77 to 0.85 while reducing parameters from
11.2M to 1.2M by 89% and FLOPs per inference from 1.648G to 0.185G by 89%. It
also maintains a competitive 3D-AUCp of 0.84 on synthetic data. These results
demonstrate accurate and efficient egocentric event-based hand tracking
suitable for on-device XR applications. The dataset and code are available at
https://github.com/zen5x5/EvHand-FPV.

</details>


### [101] [White Aggregation and Restoration for Few-shot 3D Point Cloud Semantic Segmentation](https://arxiv.org/abs/2509.13907)
*Jiyun Im,SuBeen Lee,Miso Lee,Jae-Pil Heo*

Main category: cs.CV

TL;DR: Proposes WARM, a novel prototype generation method using whitening and coloring transformations to improve few-shot 3D point cloud segmentation by addressing distribution gaps in attention mechanisms.


<details>
  <summary>Details</summary>
Motivation: Existing few-shot 3D point cloud segmentation methods suffer from performance variability due to random initialization in prototype generation and underexplored prototype generation processes despite their importance.

Method: White Aggregation and Restoration Module (WARM) that sandwiches cross-attention between whitening and coloring transformations to align support features with prototypical tokens and restore original distributions.

Result: Achieves state-of-the-art performance with significant margins on multiple FS-PCS benchmarks through extensive experiments.

Conclusion: WARM's simple yet effective design enables robust attention and generates representative prototypes by capturing semantic relationships among support features.

Abstract: Few-Shot 3D Point Cloud Segmentation (FS-PCS) aims to predict per-point
labels for an unlabeled point cloud, given only a few labeled examples. To
extract discriminative representations from the limited support set, existing
methods have constructed prototypes using conventional algorithms such as
farthest point sampling. However, we point out that its initial randomness
significantly affects FS-PCS performance and that the prototype generation
process remains underexplored despite its prevalence. This motivates us to
investigate an advanced prototype generation method based on attention
mechanism. Despite its potential, we found that vanilla module suffers from the
distributional gap between learnable prototypical tokens and support features.
To overcome this, we propose White Aggregation and Restoration Module (WARM),
which resolves the misalignment by sandwiching cross-attention between
whitening and coloring transformations. Specifically, whitening aligns the
support features to prototypical tokens before attention process, and
subsequently coloring restores the original distribution to the attended
tokens. This simple yet effective design enables robust attention, thereby
generating representative prototypes by capturing the semantic relationships
among support features. Our method achieves state-of-the-art performance with a
significant margin on multiple FS-PCS benchmarks, demonstrating its
effectiveness through extensive experiments.

</details>


### [102] [Towards Rationale-Answer Alignment of LVLMs via Self-Rationale Calibration](https://arxiv.org/abs/2509.13919)
*Yuanchen Wu,Ke Yan,Shouhong Ding,Ziyin Zhou,Xiaoqiang Li*

Main category: cs.CV

TL;DR: SRC framework improves LVLMs by iteratively calibrating rationale-answer alignment through rationale fine-tuning, candidate generation, pairwise scoring, and preference fine-tuning.


<details>
  <summary>Details</summary>
Motivation: LVLMs struggle with aligning rationales and answers, leading to inconsistent reasoning and incorrect responses despite strong visual question answering capabilities.

Method: Self-Rationale Calibration (SRC) framework: 1) Rationale fine-tuning to require rationale before answer, 2) Generate diverse candidate responses, 3) Pairwise scoring with R-Scorer model, 4) Confidence-weighted preference curation for fine-tuning.

Result: Significant improvements in perception, reasoning, and generalization across multiple benchmarks for LVLMs.

Conclusion: Rationale-oriented alignment is crucial for exploring the full potential of Large Vision-Language Models.

Abstract: Large Vision-Language Models (LVLMs) have manifested strong visual question
answering capability. However, they still struggle with aligning the rationale
and the generated answer, leading to inconsistent reasoning and incorrect
responses. To this end, this paper introduces the Self-Rationale Calibration
(SRC) framework to iteratively calibrate the alignment between rationales and
answers. SRC begins by employing a lightweight "rationale fine-tuning"
approach, which modifies the model's response format to require a rationale
before deriving an answer without explicit prompts. Next, SRC searches for a
diverse set of candidate responses from the fine-tuned LVLMs for each sample,
followed by a proposed pairwise scoring strategy using a tailored scoring
model, R-Scorer, to evaluate both rationale quality and factual consistency of
candidates. Based on a confidence-weighted preference curation process, SRC
decouples the alignment calibration into a preference fine-tuning manner,
leading to significant improvements of LVLMs in perception, reasoning, and
generalization across multiple benchmarks. Our results emphasize the
rationale-oriented alignment in exploring the potential of LVLMs.

</details>


### [103] [Towards Robust Defense against Customization via Protective Perturbation Resistant to Diffusion-based Purification](https://arxiv.org/abs/2509.13922)
*Wenkui Yang,Jie Cao,Junxian Duan,Ran He*

Main category: cs.CV

TL;DR: AntiPure is a protective perturbation method that resists purification attacks in diffusion models, using frequency and timestep guidance to maintain adversarial noise through the purification-customization workflow.


<details>
  <summary>Details</summary>
Motivation: Diffusion models enable powerful image customization but create security risks like deepfakes. Existing protective perturbations can be removed by purification methods, exposing images to malicious forgery again.

Method: AntiPure uses two guidance mechanisms: 1) Patch-wise Frequency Guidance to reduce model influence over high-frequency components, and 2) Erroneous Timestep Guidance to disrupt the denoising strategy across timesteps.

Result: AntiPure achieves minimal perceptual discrepancy and maximal distortion, outperforming other protective perturbation methods in resisting purification within the purification-customization workflow.

Conclusion: AntiPure effectively addresses anti-purification challenges and serves as a stress test for purification methods, providing robust protection against image misuse in diffusion models.

Abstract: Diffusion models like Stable Diffusion have become prominent in visual
synthesis tasks due to their powerful customization capabilities, which also
introduce significant security risks, including deepfakes and copyright
infringement. In response, a class of methods known as protective perturbation
emerged, which mitigates image misuse by injecting imperceptible adversarial
noise. However, purification can remove protective perturbations, thereby
exposing images again to the risk of malicious forgery. In this work, we
formalize the anti-purification task, highlighting challenges that hinder
existing approaches, and propose a simple diagnostic protective perturbation
named AntiPure. AntiPure exposes vulnerabilities of purification within the
"purification-customization" workflow, owing to two guidance mechanisms: 1)
Patch-wise Frequency Guidance, which reduces the model's influence over
high-frequency components in the purified image, and 2) Erroneous Timestep
Guidance, which disrupts the model's denoising strategy across different
timesteps. With additional guidance, AntiPure embeds imperceptible
perturbations that persist under representative purification settings,
achieving effective post-customization distortion. Experiments show that, as a
stress test for purification, AntiPure achieves minimal perceptual discrepancy
and maximal distortion, outperforming other protective perturbation methods
within the purification-customization workflow.

</details>


### [104] [Noise-Level Diffusion Guidance: Well Begun is Half Done](https://arxiv.org/abs/2509.13936)
*Harvey Mannering,Zhiwu Huang,Adam Prugel-Bennett*

Main category: cs.CV

TL;DR: Noise Level Guidance (NLG) is a simple, efficient method that optimizes initial noise in diffusion models to improve image quality and prompt adherence without extra data, networks, or backpropagation.


<details>
  <summary>Details</summary>
Motivation: Random Gaussian noise in diffusion models causes variations in output quality and prompt adherence. Existing optimization methods require additional resources like datasets, networks, or backpropagation, limiting practicality.

Method: Proposes Noise Level Guidance (NLG) that refines initial noise by increasing its alignment with general guidance. No additional training data, auxiliary networks, or backpropagation needed. Works with both conditional and unconditional diffusion models.

Result: Extensive experiments on five standard benchmarks show NLG enhances output generation quality and improves input condition adherence. Maintains computational efficiency while integrating with existing guidance methods.

Conclusion: NLG establishes itself as a practical and scalable enhancement to diffusion models, providing a unified framework that works with various forms of diffusion-level guidance without additional resource requirements.

Abstract: Diffusion models have achieved state-of-the-art image generation. However,
the random Gaussian noise used to start the diffusion process influences the
final output, causing variations in image quality and prompt adherence.
Existing noise-level optimization approaches generally rely on extra dataset
construction, additional networks, or backpropagation-based optimization,
limiting their practicality. In this paper, we propose Noise Level Guidance
(NLG), a simple, efficient, and general noise-level optimization approach that
refines initial noise by increasing the likelihood of its alignment with
general guidance - requiring no additional training data, auxiliary networks,
or backpropagation. The proposed NLG approach provides a unified framework
generalizable to both conditional and unconditional diffusion models,
accommodating various forms of diffusion-level guidance. Extensive experiments
on five standard benchmarks demonstrate that our approach enhances output
generation quality and input condition adherence. By seamlessly integrating
with existing guidance methods while maintaining computational efficiency, our
method establishes NLG as a practical and scalable enhancement to diffusion
models. Code can be found at
https://github.com/harveymannering/NoiseLevelGuidance.

</details>


### [105] [Can Current AI Models Count What We Mean, Not What They See? A Benchmark and Systematic Evaluation](https://arxiv.org/abs/2509.13939)
*Gia Khanh Nguyen,Yifeng Huang,Minh Hoai*

Main category: cs.CV

TL;DR: PairTally is a new benchmark dataset for evaluating fine-grained visual counting, featuring 681 high-resolution images with two object categories each, designed to test models' ability to distinguish and count based on subtle differences.


<details>
  <summary>Details</summary>
Motivation: Current visual counting models struggle with fine-grained, intent-driven counting in complex scenes, and there's a need to evaluate their ability to distinguish objects based on subtle visual differences.

Method: Created PairTally dataset with 681 images containing two object categories per image, including both inter-category and intra-category settings. Benchmarked various state-of-the-art models including exemplar-based methods, language-prompted models, and large vision-language models.

Result: Current models struggle to reliably count what users intend, especially in fine-grained and visually ambiguous cases, despite recent advances in counting technology.

Conclusion: PairTally provides a new foundation for diagnosing and improving fine-grained visual counting systems, highlighting the limitations of current approaches and the need for better selective counting capabilities.

Abstract: Visual counting is a fundamental yet challenging task, especially when users
need to count objects of a specific type in complex scenes. While recent
models, including class-agnostic counting models and large vision-language
models (VLMs), show promise in counting tasks, their ability to perform
fine-grained, intent-driven counting remains unclear. In this paper, we
introduce PairTally, a benchmark dataset specifically designed to evaluate
fine-grained visual counting. Each of the 681 high-resolution images in
PairTally contains two object categories, requiring models to distinguish and
count based on subtle differences in shape, size, color, or semantics. The
dataset includes both inter-category (distinct categories) and intra-category
(closely related subcategories) settings, making it suitable for rigorous
evaluation of selective counting capabilities. We benchmark a variety of
state-of-the-art models, including exemplar-based methods, language-prompted
models, and large VLMs. Our results show that despite recent advances, current
models struggle to reliably count what users intend, especially in fine-grained
and visually ambiguous cases. PairTally provides a new foundation for
diagnosing and improving fine-grained visual counting systems.

</details>


### [106] [MOCHA: Multi-modal Objects-aware Cross-arcHitecture Alignment](https://arxiv.org/abs/2509.14001)
*Elena Camuffo,Francesco Barbato,Mete Ozay,Simone Milani,Umberto Michieli*

Main category: cs.CV

TL;DR: MOCHA is a knowledge distillation method that transfers region-level multimodal semantics from large vision-language teachers to lightweight vision-only object detectors using object-level alignment and relational consistency.


<details>
  <summary>Details</summary>
Motivation: To enable efficient transfer of multimodal semantics to lightweight vision-only detectors without requiring textual input at inference or modifying the teacher model.

Method: Uses a translation module to map student features into joint space with dual-objective loss enforcing both local alignment and global relational consistency at object level.

Result: +10.1 average score improvement across four personalized detection benchmarks under few-shot regimes, achieving performance comparable to larger multimodal models.

Conclusion: MOCHA proves effective for real-world deployment by enabling compact vision-only detectors to perform on par with larger multimodal models through efficient object-level knowledge transfer.

Abstract: We introduce MOCHA (Multi-modal Objects-aware Cross-arcHitecture Alignment),
a knowledge distillation approach that transfers region-level multimodal
semantics from a large vision-language teacher (e.g., LLaVa) into a lightweight
vision-only object detector student (e.g., YOLO). A translation module maps
student features into a joint space, where the training of the student and
translator is guided by a dual-objective loss that enforces both local
alignment and global relational consistency. Unlike prior approaches focused on
dense or global alignment, MOCHA operates at the object level, enabling
efficient transfer of semantics without modifying the teacher or requiring
textual input at inference. We validate our method across four personalized
detection benchmarks under few-shot regimes. Results show consistent gains over
baselines, with a +10.1 average score improvement. Despite its compact
architecture, MOCHA reaches performance on par with larger multimodal models,
proving its suitability for real-world deployment.

</details>


### [107] [Performance Optimization of YOLO-FEDER FusionNet for Robust Drone Detection in Visually Complex Environments](https://arxiv.org/abs/2509.14012)
*Tamara R. Lenhard,Andreas Weinmann,Tobias Koch*

Main category: cs.CV

TL;DR: Enhanced YOLO-FEDER FusionNet for drone detection in complex environments, combining object detection with camouflage detection techniques, achieving significant performance improvements through synthetic data, feature fusion, and backbone upgrades.


<details>
  <summary>Details</summary>
Motivation: Drone detection in visually complex environments is challenging due to background clutter, small object scale, and camouflage effects. Generic object detectors like YOLO perform poorly in cluttered environments with low object-background separability.

Method: Enhanced YOLO-FEDER FusionNet framework integrating generic object detection with camouflage object detection. Uses large-scale synthetic data with real-world samples, systematic evaluation of multi-scale FEDER features, and multiple YOLO-based backbone configurations.

Result: Best configuration (YOLOv8l backbone with DWD module FEDER features) achieved FNR reduction of up to 39.1 percentage points and mAP increase of up to 62.8 percentage points at IoU threshold 0.5 compared to baseline.

Conclusion: Integrating intermediate FEDER features with backbone upgrades significantly improves drone detection performance in complex environments, demonstrating the effectiveness of combining camouflage detection techniques with object detection frameworks.

Abstract: Drone detection in visually complex environments remains challenging due to
background clutter, small object scale, and camouflage effects. While generic
object detectors like YOLO exhibit strong performance in low-texture scenes,
their effectiveness degrades in cluttered environments with low
object-background separability. To address these limitations, this work
presents an enhanced iteration of YOLO-FEDER FusionNet -- a detection framework
that integrates generic object detection with camouflage object detection
techniques. Building upon the original architecture, the proposed iteration
introduces systematic advancements in training data composition, feature fusion
strategies, and backbone design. Specifically, the training process leverages
large-scale, photo-realistic synthetic data, complemented by a small set of
real-world samples, to enhance robustness under visually complex conditions.
The contribution of intermediate multi-scale FEDER features is systematically
evaluated, and detection performance is comprehensively benchmarked across
multiple YOLO-based backbone configurations. Empirical results indicate that
integrating intermediate FEDER features, in combination with backbone upgrades,
contributes to notable performance improvements. In the most promising
configuration -- YOLO-FEDER FusionNet with a YOLOv8l backbone and FEDER
features derived from the DWD module -- these enhancements lead to a FNR
reduction of up to 39.1 percentage points and a mAP increase of up to 62.8
percentage points at an IoU threshold of 0.5, compared to the initial baseline.

</details>


### [108] [SAIL-VL2 Technical Report](https://arxiv.org/abs/2509.14033)
*Weijie Yin,Yongjie Ye,Fangxun Shu,Yue Liao,Zijian Kang,Hongyuan Dong,Haiyang Yu,Dingkang Yang,Jiacong Wang,Han Wang,Wenzhuo Liu,Xiao Liang,Shuicheng Yan,Chao Feng*

Main category: cs.CV

TL;DR: SAIL-VL2 is an open-source vision-language foundation model that achieves state-of-the-art performance at 2B and 8B scales across diverse benchmarks through innovations in data curation, progressive training, and MoE architecture.


<details>
  <summary>Details</summary>
Motivation: To create a comprehensive multimodal understanding and reasoning model that serves as an efficient and extensible foundation for the open-source community, building upon the success of SAIL-VL.

Method: Three core innovations: 1) Large-scale data curation pipeline with scoring/filtering strategies, 2) Progressive training framework starting with SAIL-ViT encoder through multimodal pre-training to thinking-fusion SFT-RL hybrid, 3) Architectural advances including sparse Mixture-of-Experts designs.

Result: Achieves state-of-the-art performance across 106 datasets, top results on challenging reasoning benchmarks (MMMU, MathVista), and ranks first among open-source models under 4B parameters on OpenCompass leaderboard.

Conclusion: SAIL-VL2 demonstrates strong capabilities from fine-grained perception to complex reasoning, serving as a competitive and efficient foundation model for the open-source multimodal community.

Abstract: We introduce SAIL-VL2, an open-suite vision-language foundation model (LVM)
for comprehensive multimodal understanding and reasoning. As the successor to
SAIL-VL, SAIL-VL2 achieves state-of-the-art performance at the 2B and 8B
parameter scales across diverse image and video benchmarks, demonstrating
strong capabilities from fine-grained perception to complex reasoning. Three
core innovations drive its effectiveness. First, a large-scale data curation
pipeline with scoring and filtering strategies enhances both quality and
distribution across captioning, OCR, QA, and video data, improving training
efficiency. Second, a progressive training framework begins with a powerful
pre-trained vision encoder (SAIL-ViT), advances through multimodal
pre-training, and culminates in a thinking-fusion SFT-RL hybrid paradigm that
systematically strengthens model capabilities. Third, architectural advances
extend beyond dense LLMs to efficient sparse Mixture-of-Experts (MoE) designs.
With these contributions, SAIL-VL2 demonstrates competitive performance across
106 datasets and achieves state-of-the-art results on challenging reasoning
benchmarks such as MMMU and MathVista. Furthermore, on the OpenCompass
leaderboard, SAIL-VL2-2B ranks first among officially released open-source
models under the 4B parameter scale, while serving as an efficient and
extensible foundation for the open-source multimodal community.

</details>


### [109] [PROFUSEme: PROstate Cancer Biochemical Recurrence Prediction via FUSEd Multi-modal Embeddings](https://arxiv.org/abs/2509.14051)
*Suhang You,Carla Pitarch-Abaigar,Sanket Kachole,Sumedh Sonawane,Juhyung Ha,Anish Sudarshan Gada,David Crandall,Rakesh Shiradkar,Spyridon Bakas*

Main category: cs.CV

TL;DR: PROFUSEme is a multi-modal AI system that predicts biochemical recurrence in prostate cancer patients by fusing clinical, radiology, and pathology data using intermediate fusion with Cox models, achieving superior performance over late fusion methods.


<details>
  <summary>Details</summary>
Motivation: About 30% of prostate cancer patients experience biochemical recurrence after radical prostatectomy, which increases mortality risk. Early accurate prediction at surgery time could improve clinical decision-making and patient outcomes.

Method: PROFUSEme uses intermediate fusion configuration to learn cross-modal interactions between clinical, radiology, and pathology data, combined with Cox Proportional Hazard regressors for prediction.

Result: The method achieved mean C-index of 0.861 (σ=0.112) on internal 5-fold nested cross-validation and 0.7103 on CHIMERA 2025 challenge validation leaderboard holdout data, outperforming late fusion approaches.

Conclusion: The proposed multi-modal fusion approach demonstrates superior performance for early biochemical recurrence prediction in prostate cancer patients, showing promise for clinical decision support.

Abstract: Almost 30% of prostate cancer (PCa) patients undergoing radical prostatectomy
(RP) experience biochemical recurrence (BCR), characterized by increased
prostate specific antigen (PSA) and associated with increased mortality.
Accurate early prediction of BCR, at the time of RP, would contribute to prompt
adaptive clinical decision-making and improved patient outcomes. In this work,
we propose prostate cancer BCR prediction via fused multi-modal embeddings
(PROFUSEme), which learns cross-modal interactions of clinical, radiology, and
pathology data, following an intermediate fusion configuration in combination
with Cox Proportional Hazard regressors. Quantitative evaluation of our
proposed approach reveals superior performance, when compared with late fusion
configurations, yielding a mean C-index of 0.861 ($\sigma=0.112$) on the
internal 5-fold nested cross-validation framework, and a C-index of 0.7103 on
the hold out data of CHIMERA 2025 challenge validation leaderboard.

</details>


### [110] [Wan-Animate: Unified Character Animation and Replacement with Holistic Replication](https://arxiv.org/abs/2509.14055)
*Gang Cheng,Xin Gao,Li Hu,Siqi Hu,Mingyang Huang,Chaonan Ji,Ju Li,Dechao Meng,Jinwei Qi,Penchong Qiao,Zhen Shen,Yafei Song,Ke Sun,Linrui Tian,Feng Wang,Guangyuan Wang,Qi Wang,Zhongjian Wang,Jiayu Xiao,Sheng Xu,Bang Zhang,Peng Zhang,Xindi Zhang,Zhe Zhang,Jingren Zhou,Lian Zhuo*

Main category: cs.CV

TL;DR: Wan-Animate is a unified framework for character animation and replacement that can animate characters from images using reference videos or replace characters in videos while maintaining environmental lighting consistency.


<details>
  <summary>Details</summary>
Motivation: To create a unified solution for character animation and replacement that can precisely replicate expressions and movements from reference videos while achieving seamless environmental integration with proper lighting and color tone matching.

Method: Built upon the Wan model with modified input paradigm using spatially-aligned skeleton signals for body motion and implicit facial features for expressions. Includes auxiliary Relighting LoRA module for environmental lighting integration during character replacement.

Result: Achieves state-of-the-art performance in generating high-fidelity character videos with high controllability and expressiveness, demonstrating superior environmental integration capabilities.

Conclusion: Wan-Animate provides an effective unified framework for character animation tasks with open-source commitment, offering both animation and replacement capabilities with precise motion replication and environmental consistency.

Abstract: We introduce Wan-Animate, a unified framework for character animation and
replacement. Given a character image and a reference video, Wan-Animate can
animate the character by precisely replicating the expressions and movements of
the character in the video to generate high-fidelity character videos.
Alternatively, it can integrate the animated character into the reference video
to replace the original character, replicating the scene's lighting and color
tone to achieve seamless environmental integration. Wan-Animate is built upon
the Wan model. To adapt it for character animation tasks, we employ a modified
input paradigm to differentiate between reference conditions and regions for
generation. This design unifies multiple tasks into a common symbolic
representation. We use spatially-aligned skeleton signals to replicate body
motion and implicit facial features extracted from source images to reenact
expressions, enabling the generation of character videos with high
controllability and expressiveness. Furthermore, to enhance environmental
integration during character replacement, we develop an auxiliary Relighting
LoRA. This module preserves the character's appearance consistency while
applying the appropriate environmental lighting and color tone. Experimental
results demonstrate that Wan-Animate achieves state-of-the-art performance. We
are committed to open-sourcing the model weights and its source code.

</details>


### [111] [VSE-MOT: Multi-Object Tracking in Low-Quality Video Scenes Guided by Visual Semantic Enhancement](https://arxiv.org/abs/2509.14060)
*Jun Du,Weiwei Xing,Ming Li,Fei Richard Yu*

Main category: cs.CV

TL;DR: VSE-MOT framework enhances multi-object tracking in low-quality videos using vision-language models and semantic fusion, achieving 8-20% performance improvements over existing methods.


<details>
  <summary>Details</summary>
Motivation: Current MOT algorithms perform poorly with low-quality videos due to real-world image deterioration, limiting their practical application in real-world scenarios.

Method: Proposes tri-branch architecture using vision-language models to extract global visual semantic information, with MOT-Adapter and Visual Semantic Fusion Module (VSFM) to adapt and fuse semantic features for tracking tasks.

Result: Outperforms existing methods by 8-20% in tracking performance metrics for low-quality videos while maintaining robust performance in conventional scenarios.

Conclusion: The VSE-MOT framework effectively addresses low-quality video challenges in MOT through visual semantic enhancement, demonstrating significant performance improvements and practical applicability.

Abstract: Current multi-object tracking (MOT) algorithms typically overlook issues
inherent in low-quality videos, leading to significant degradation in tracking
performance when confronted with real-world image deterioration. Therefore,
advancing the application of MOT algorithms in real-world low-quality video
scenarios represents a critical and meaningful endeavor. To address the
challenges posed by low-quality scenarios, inspired by vision-language models,
this paper proposes a Visual Semantic Enhancement-guided Multi-Object Tracking
framework (VSE-MOT). Specifically, we first design a tri-branch architecture
that leverages a vision-language model to extract global visual semantic
information from images and fuse it with query vectors. Subsequently, to
further enhance the utilization of visual semantic information, we introduce
the Multi-Object Tracking Adapter (MOT-Adapter) and the Visual Semantic Fusion
Module (VSFM). The MOT-Adapter adapts the extracted global visual semantic
information to suit multi-object tracking tasks, while the VSFM improves the
efficacy of feature fusion. Through extensive experiments, we validate the
effectiveness and superiority of the proposed method in real-world low-quality
video scenarios. Its tracking performance metrics outperform those of existing
methods by approximately 8% to 20%, while maintaining robust performance in
conventional scenarios.

</details>


### [112] [AD-DINOv3: Enhancing DINOv3 for Zero-Shot Anomaly Detection with Anomaly-Aware Calibration](https://arxiv.org/abs/2509.14084)
*Jingyi Yuan,Jianxiong Ye,Wenkang Chen,Chenqiang Gao*

Main category: cs.CV

TL;DR: AD-DINOv3 is a novel zero-shot anomaly detection framework that adapts DINOv3 with lightweight adapters and an anomaly-aware calibration module to overcome feature misalignment and global semantic bias issues, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Traditional zero-shot anomaly detection methods rely on CLIP, but vision foundation models like DINOv3 offer strong transferable representations. However, adapting DINOv3 for anomaly detection faces challenges of domain bias and global semantic bias that cause feature misalignment and misinterpretation of subtle anomalies.

Method: Proposes AD-DINOv3 framework with multimodal contrastive learning using DINOv3 as visual backbone and CLIP text encoder. Uses lightweight adapters to bridge domain gap and an Anomaly-Aware Calibration Module (AACM) to guide attention to anomalous regions rather than generic foreground semantics.

Result: Extensive experiments on eight industrial and medical benchmarks show AD-DINOv3 consistently matches or surpasses state-of-the-art methods, demonstrating superior zero-shot anomaly detection performance.

Conclusion: AD-DINOv3 successfully adapts DINOv3 for zero-shot anomaly detection, overcoming domain bias and semantic bias challenges through multimodal adaptation and anomaly-aware calibration, establishing it as a general and effective framework for the task.

Abstract: Zero-Shot Anomaly Detection (ZSAD) seeks to identify anomalies from arbitrary
novel categories, offering a scalable and annotation-efficient solution.
Traditionally, most ZSAD works have been based on the CLIP model, which
performs anomaly detection by calculating the similarity between visual and
text embeddings. Recently, vision foundation models such as DINOv3 have
demonstrated strong transferable representation capabilities. In this work, we
are the first to adapt DINOv3 for ZSAD. However, this adaptation presents two
key challenges: (i) the domain bias between large-scale pretraining data and
anomaly detection tasks leads to feature misalignment; and (ii) the inherent
bias toward global semantics in pretrained representations often leads to
subtle anomalies being misinterpreted as part of the normal foreground objects,
rather than being distinguished as abnormal regions. To overcome these
challenges, we introduce AD-DINOv3, a novel vision-language multimodal
framework designed for ZSAD. Specifically, we formulate anomaly detection as a
multimodal contrastive learning problem, where DINOv3 is employed as the visual
backbone to extract patch tokens and a CLS token, and the CLIP text encoder
provides embeddings for both normal and abnormal prompts. To bridge the domain
gap, lightweight adapters are introduced in both modalities, enabling their
representations to be recalibrated for the anomaly detection task. Beyond this
baseline alignment, we further design an Anomaly-Aware Calibration Module
(AACM), which explicitly guides the CLS token to attend to anomalous regions
rather than generic foreground semantics, thereby enhancing discriminability.
Extensive experiments on eight industrial and medical benchmarks demonstrate
that AD-DINOv3 consistently matches or surpasses state-of-the-art methods,
verifying its superiority as a general zero-shot anomaly detection framework.

</details>


### [113] [Teacher-Guided Pseudo Supervision and Cross-Modal Alignment for Audio-Visual Video Parsing](https://arxiv.org/abs/2509.14097)
*Yaru Chen,Ruohao Guo,Liting Gao,Yang Xiang,Qingyu Luo,Zhenbo Li,Wenwu Wang*

Main category: cs.CV

TL;DR: Proposes EMA-guided pseudo supervision and class-aware cross-modal alignment for weakly-supervised audio-visual video parsing, achieving SOTA results.


<details>
  <summary>Details</summary>
Motivation: Previous methods neglected stable segment-level supervision and class-aware cross-modal alignment in weakly-supervised AVVP tasks.

Method: Two strategies: (1) EMA-guided pseudo supervision framework generating reliable segment-level masks via adaptive thresholds or top-k selection, (2) class-aware cross-modal agreement loss aligning audio and visual embeddings at reliable segment-class pairs.

Result: Achieves state-of-the-art performance on LLP and UnAV-100 datasets across multiple metrics.

Conclusion: The proposed approach effectively addresses limitations in segment-level supervision and cross-modal alignment for weakly-supervised audio-visual video parsing.

Abstract: Weakly-supervised audio-visual video parsing (AVVP) seeks to detect audible,
visible, and audio-visual events without temporal annotations. Previous work
has emphasized refining global predictions through contrastive or collaborative
learning, but neglected stable segment-level supervision and class-aware
cross-modal alignment. To address this, we propose two strategies: (1) an
exponential moving average (EMA)-guided pseudo supervision framework that
generates reliable segment-level masks via adaptive thresholds or top-k
selection, offering stable temporal guidance beyond video-level labels; and (2)
a class-aware cross-modal agreement (CMA) loss that aligns audio and visual
embeddings at reliable segment-class pairs, ensuring consistency across
modalities while preserving temporal structure. Evaluations on LLP and UnAV-100
datasets shows that our method achieves state-of-the-art (SOTA) performance
across multiple metrics.

</details>


### [114] [CSMoE: An Efficient Remote Sensing Foundation Model with Soft Mixture-of-Experts](https://arxiv.org/abs/2509.14104)
*Leonard Hackel,Tom Burgert,Begüm Demir*

Main category: cs.CV

TL;DR: Proposed CSMoE model integrates Soft MoE mechanism into remote sensing foundation models to improve computational efficiency while maintaining performance, achieving 2x efficiency gains over existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing remote sensing foundation models suffer from high computational complexity during training/inference or limited representational capacity, restricting practical applicability.

Method: Integrates Soft mixture-of-experts (MoE) mechanism into Cross-Sensor Masked Autoencoder (CSMAE), creating CSMoE model with modality-specific expert specialization and cross-sensor representation learning. Uses thematic-climatic descriptor-driven sampling for diverse training set.

Result: CSMoE achieves more than twice the computational efficiency of existing RS FMs while maintaining competitive performance across scene classification, semantic segmentation, and content-based image retrieval tasks.

Conclusion: The Soft MoE adaptation effectively creates computationally efficient remote sensing foundation models with superior trade-off between representational capacity, accuracy, and computational efficiency.

Abstract: Self-supervised learning through masked autoencoders has attracted great
attention for remote sensing (RS) foundation model (FM) development, enabling
improved representation learning across diverse sensors and downstream tasks.
However, existing RS FMs often either suffer from substantial computational
complexity during both training and inference or exhibit limited
representational capacity. These issues restrict their practical applicability
in RS. To address this limitation, we propose an adaptation for enhancing the
efficiency of RS FMs by integrating the Soft mixture-of-experts (MoE) mechanism
into the FM. The integration of Soft MoEs into the FM allows modality-specific
expert specialization alongside shared cross-sensor representation learning. To
demonstrate the effectiveness of our adaptation, we apply it on the
Cross-Sensor Masked Autoencoder (CSMAE) model, resulting in the Cross-Sensor
Mixture-of-Experts (CSMoE) model. In addition, we introduce a thematic-climatic
descriptor-driven sampling strategy for the construction of a representative
and diverse training set to train our CSMoE model. Extensive experiments on
scene classification, semantic segmentation, and content-based image retrieval
demonstrate that our adaptation yields a reduction in computational
requirements while maintaining or improving representational performance.
Compared to state-of-the-art RS FMs, CSMoE achieves a superior trade-off
between representational capacity, accuracy, and computational efficiency. On
average, CSMoE achieves more than twice the computational efficiency of
existing RS FMs, while maintaining competitive performance across all
experiments. These results show the effectiveness of the proposed adaptation
for creating computationally efficient RS FMs. The code for the model, the
training set creation, and the model weights will be available at
https://git.tu-berlin.de/rsim/csmoe.

</details>


### [115] [Generative AI for Misalignment-Resistant Virtual Staining to Accelerate Histopathology Workflows](https://arxiv.org/abs/2509.14119)
*Jiabo MA,Wenqiang Li,Jinbang Li,Ziyi Liu,Linshan Wu,Fengtao Zhou,Li Liang,Ronald Cheong Kin Chan,Terence T. W. Wong,Hao Chen*

Main category: cs.CV

TL;DR: Proposed a robust virtual staining framework with cascaded registration to handle spatial mismatches in unpaired/roughly paired histopathology data, achieving significant performance improvements over state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Traditional histopathological diagnosis requires multiple chemical stains which is time-consuming, labor-intensive, and environmentally taxing. Existing virtual staining methods struggle with clinical applications due to reliance on well-aligned paired data, which is difficult to obtain because staining processes distort tissues and single sections cannot undergo multiple staining procedures.

Method: A robust virtual staining framework featuring cascaded registration mechanisms to resolve spatial mismatches between generated outputs and their corresponding ground truth in unpaired or roughly paired datasets.

Result: Significantly outperforms state-of-the-art models across five datasets with average improvements of 3.2% on internal datasets and 10.1% on external datasets. Achieves remarkable 23.8% improvement in PSNR on datasets with substantial misalignment.

Conclusion: The method demonstrates exceptional robustness across diverse datasets, simplifies data acquisition for virtual staining, and offers new insights for advancing virtual staining development.

Abstract: Accurate histopathological diagnosis often requires multiple differently
stained tissue sections, a process that is time-consuming, labor-intensive, and
environmentally taxing due to the use of multiple chemical stains. Recently,
virtual staining has emerged as a promising alternative that is faster,
tissue-conserving, and environmentally friendly. However, existing virtual
staining methods face significant challenges in clinical applications,
primarily due to their reliance on well-aligned paired data. Obtaining such
data is inherently difficult because chemical staining processes can distort
tissue structures, and a single tissue section cannot undergo multiple staining
procedures without damage or loss of information. As a result, most available
virtual staining datasets are either unpaired or roughly paired, making it
difficult for existing methods to achieve accurate pixel-level supervision. To
address this challenge, we propose a robust virtual staining framework
featuring cascaded registration mechanisms to resolve spatial mismatches
between generated outputs and their corresponding ground truth. Experimental
results demonstrate that our method significantly outperforms state-of-the-art
models across five datasets, achieving an average improvement of 3.2% on
internal datasets and 10.1% on external datasets. Moreover, in datasets with
substantial misalignment, our approach achieves a remarkable 23.8% improvement
in peak signal-to-noise ratio compared to baseline models. The exceptional
robustness of the proposed method across diverse datasets simplifies the data
acquisition process for virtual staining and offers new insights for advancing
its development.

</details>


### [116] [Deceptive Beauty: Evaluating the Impact of Beauty Filters on Deepfake and Morphing Attack Detection](https://arxiv.org/abs/2509.14120)
*Sara Concas,Simone Maurizio La Cava,Andrea Panzino,Ester Masala,Giulia Orrù,Gian Luca Marcialis*

Main category: cs.CV

TL;DR: Beauty filters degrade the performance of deepfake and morphing attack detectors, creating vulnerabilities in facial manipulation detection systems.


<details>
  <summary>Details</summary>
Motivation: The popularity of digital beautification filters raises concerns about facial image reliability and the effectiveness of automated face analysis, particularly for detecting manipulated content like deepfakes and morphing attacks.

Method: Comprehensive analysis evaluating multiple state-of-the-art detectors on benchmark datasets before and after applying various smoothing filters.

Result: Performance degradation was observed in deepfake and morphing attack detectors after applying beauty filters, revealing vulnerabilities in current detection systems.

Conclusion: Facial enhancements through beauty filters introduce significant vulnerabilities, underscoring the urgent need for developing more robust detection models that are resilient to such alterations.

Abstract: Digital beautification through social media filters has become increasingly
popular, raising concerns about the reliability of facial images and videos and
the effectiveness of automated face analysis. This issue is particularly
critical for digital manipulation detectors, systems aiming at distinguishing
between genuine and manipulated data, especially in cases involving deepfakes
and morphing attacks designed to deceive humans and automated facial
recognition. This study examines whether beauty filters impact the performance
of deepfake and morphing attack detectors. We perform a comprehensive analysis,
evaluating multiple state-of-the-art detectors on benchmark datasets before and
after applying various smoothing filters. Our findings reveal performance
degradation, highlighting vulnerabilities introduced by facial enhancements and
underscoring the need for robust detection models resilient to such
alterations.

</details>


### [117] [MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods, Results, Discussion, and Outlook](https://arxiv.org/abs/2509.14142)
*Peng Xu,Shengwu Xiong,Jiajun Zhang,Yaxiong Chen,Bowen Zhou,Chen Change Loy,David A. Clifton,Kyoung Mu Lee,Luc Van Gool,Ruiming He,Ruilin Yao,Xinwei Long,Jirui Huang,Kai Tian,Sa Yang,Yihua Shao,Jin Feng,Yue Zhong,Jiakai Zhou,Cheng Tang,Tianyu Zou,Yifang Zhang,Junming Liang,Guoyou Li,Zhaoxiang Wang,Qiang Zhou,Yichen Zhao,Shili Xiong,Hyeongjin Nam,Jaerin Lee,Jaeyoung Chung,JoonKyu Park,Junghun Oh,Kanggeon Lee,Wooseok Lee,Juneyoung Ro,Turghun Osman,Can Hu,Chaoyang Liao,Cheng Chen,Chengcheng Han,Chenhao Qiu,Chong Peng,Cong Xu,Dailin Li,Feiyu Wang,Feng Gao,Guibo Zhu,Guopeng Tang,Haibo Lu,Han Fang,Han Qi,Hanxiao Wu,Haobo Cheng,Hongbo Sun,Hongyao Chen,Huayong Hu,Hui Li,Jiaheng Ma,Jiang Yu,Jianing Wang,Jie Yang,Jing He,Jinglin Zhou,Jingxuan Li,Josef Kittler,Lihao Zheng,Linnan Zhao,Mengxi Jia,Muyang Yan,Nguyen Thanh Thien,Pu Luo,Qi Li,Shien Song,Shijie Dong,Shuai Shao,Shutao Li,Taofeng Xue,Tianyang Xu,Tianyi Gao,Tingting Li,Wei Zhang,Weiyang Su,Xiaodong Dong,Xiao-Jun Wu,Xiaopeng Zhou,Xin Chen,Xin Wei,Xinyi You,Xudong Kang,Xujie Zhou,Xusheng Liu,Yanan Wang,Yanbin Huang,Yang Liu,Yang Yang,Yanglin Deng,Yashu Kang,Ye Yuan,Yi Wen,Yicen Tian,Yilin Tao,Yin Tang,Yipeng Lin,Yiqing Wang,Yiting Xi,Yongkang Yu,Yumei Li,Yuxin Qin,Yuying Chen,Yuzhe Cen,Zhaofan Zou,Zhaohong Liu,Zhehao Shen,Zhenglin Du,Zhengyang Li,Zhenni Huang,Zhenwei Shao,Zhilong Song,Zhiyong Feng,Zhiyu Wang,Zhou Yu,Ziang Li,Zihan Zhai,Zijian Zhang,Ziyang Peng,Ziyun Xiao,Zongshu Li*

Main category: cs.CV

TL;DR: The MARS2 2025 Challenge introduces a multimodal reasoning benchmark with two new datasets (Lens and AdsQA) and three competition tracks to advance state-of-the-art in multimodal machine learning and LLMs.


<details>
  <summary>Details</summary>
Motivation: To create a comprehensive benchmark that brings together different approaches in multimodal machine learning and LLMs, focusing on real-world and specialized scenarios to broaden multimodal reasoning applications.

Method: Released two tailored datasets (Lens for general reasoning in 12 daily scenarios, AdsQA for domain-specific reasoning in advertisement videos), evaluated 40+ baselines including generalist MLLMs and task-specific models, and opened three competition tracks: VG-RS, VQA-SA, and VR-Ads.

Result: 76 teams from academic and industrial institutions registered, with 40+ valid submissions out of 1200+ included in ranking lists. The datasets, code sets, and rankings are publicly available.

Conclusion: The MARS2 2025 Challenge successfully established a large benchmark for multimodal reasoning, attracting significant participation and providing valuable resources for advancing the field through publicly available datasets and methods.

Abstract: This paper reviews the MARS2 2025 Challenge on Multimodal Reasoning. We aim
to bring together different approaches in multimodal machine learning and LLMs
via a large benchmark. We hope it better allows researchers to follow the
state-of-the-art in this very dynamic area. Meanwhile, a growing number of
testbeds have boosted the evolution of general-purpose large language models.
Thus, this year's MARS2 focuses on real-world and specialized scenarios to
broaden the multimodal reasoning applications of MLLMs. Our organizing team
released two tailored datasets Lens and AdsQA as test sets, which support
general reasoning in 12 daily scenarios and domain-specific reasoning in
advertisement videos, respectively. We evaluated 40+ baselines that include
both generalist MLLMs and task-specific models, and opened up three competition
tracks, i.e., Visual Grounding in Real-world Scenarios (VG-RS), Visual Question
Answering with Spatial Awareness (VQA-SA), and Visual Reasoning in Creative
Advertisement Videos (VR-Ads). Finally, 76 teams from the renowned academic and
industrial institutions have registered and 40+ valid submissions (out of
1200+) have been included in our ranking lists. Our datasets, code sets (40+
baselines and 15+ participants' methods), and rankings are publicly available
on the MARS2 workshop website and our GitHub organization page
https://github.com/mars2workshop/, where our updates and announcements of
upcoming events will be continuously provided.

</details>


### [118] [An Exploratory Study on Abstract Images and Visual Representations Learned from Them](https://arxiv.org/abs/2509.14149)
*Haotian Li,Jianbo Jiao*

Main category: cs.CV

TL;DR: The paper investigates why abstract images made from primitive shapes underperform compared to raster images in deep learning, and introduces HAID dataset with multiple abstraction levels to study semantic content capture.


<details>
  <summary>Details</summary>
Motivation: To understand the performance gap between abstract primitive shape images and traditional raster images in conveying visual semantic information to deep learning models.

Method: Introduces Hierarchical Abstraction Image Dataset (HAID) containing abstract images at multiple abstraction levels generated from normal raster images, then trains and evaluates conventional vision systems on classification, segmentation, and object detection tasks.

Result: Comprehensive comparison study between rasterized and abstract image representations across multiple vision tasks.

Conclusion: Discusses whether abstract images can be considered as an effective format for conveying visual semantic information and contributing to vision tasks.

Abstract: Imagine living in a world composed solely of primitive shapes, could you
still recognise familiar objects? Recent studies have shown that abstract
images-constructed by primitive shapes-can indeed convey visual semantic
information to deep learning models. However, representations obtained from
such images often fall short compared to those derived from traditional raster
images. In this paper, we study the reasons behind this performance gap and
investigate how much high-level semantic content can be captured at different
abstraction levels. To this end, we introduce the Hierarchical Abstraction
Image Dataset (HAID), a novel data collection that comprises abstract images
generated from normal raster images at multiple levels of abstraction. We then
train and evaluate conventional vision systems on HAID across various tasks
including classification, segmentation, and object detection, providing a
comprehensive study between rasterised and abstract image representations. We
also discuss if the abstract image can be considered as a potentially effective
format for conveying visual semantic information and contributing to vision
tasks.

</details>


### [119] [BEVUDA++: Geometric-aware Unsupervised Domain Adaptation for Multi-View 3D Object Detection](https://arxiv.org/abs/2509.14151)
*Rongyu Zhang,Jiaming Liu,Xiaoqi Li,Xiaowei Chi,Dan Wang,Li Du,Yuan Du,Shanghang Zhang*

Main category: cs.CV

TL;DR: BEVUDA++ is a geometric-aware teacher-student framework that addresses domain adaptation in multi-view 3D object detection for Bird's Eye View perception, achieving state-of-the-art performance in cross-domain scenarios.


<details>
  <summary>Details</summary>
Motivation: Domain shift in BEV perception causes substantial performance degradation when transferring models across domains, particularly in autonomous driving applications where real-world conditions vary significantly.

Method: Proposes a framework with Reliable Depth Teacher (RDT) that blends target LiDAR with depth predictions using uncertainty estimation, and Geometric Consistent Student (GCS) that maps multi-space features into unified geometric embedding space. Also introduces Uncertainty-guided Exponential Moving Average (UEMA) to reduce error accumulation.

Result: Achieves 12.9% NDS and 9.5% mAP improvement on Day-Night adaptation, with state-of-the-art performance across four cross-domain scenarios in BEV 3D object detection tasks.

Conclusion: BEVUDA++ effectively addresses domain shift challenges in BEV perception through geometric-aware adaptation and uncertainty-guided techniques, demonstrating significant performance improvements in real-world autonomous driving scenarios.

Abstract: Vision-centric Bird's Eye View (BEV) perception holds considerable promise
for autonomous driving. Recent studies have prioritized efficiency or accuracy
enhancements, yet the issue of domain shift has been overlooked, leading to
substantial performance degradation upon transfer. We identify major domain
gaps in real-world cross-domain scenarios and initiate the first effort to
address the Domain Adaptation (DA) challenge in multi-view 3D object detection
for BEV perception. Given the complexity of BEV perception approaches with
their multiple components, domain shift accumulation across multi-geometric
spaces (e.g., 2D, 3D Voxel, BEV) poses a significant challenge for BEV domain
adaptation. In this paper, we introduce an innovative geometric-aware
teacher-student framework, BEVUDA++, to diminish this issue, comprising a
Reliable Depth Teacher (RDT) and a Geometric Consistent Student (GCS) model.
Specifically, RDT effectively blends target LiDAR with dependable depth
predictions to generate depth-aware information based on uncertainty
estimation, enhancing the extraction of Voxel and BEV features that are
essential for understanding the target domain. To collaboratively reduce the
domain shift, GCS maps features from multiple spaces into a unified geometric
embedding space, thereby narrowing the gap in data distribution between the two
domains. Additionally, we introduce a novel Uncertainty-guided Exponential
Moving Average (UEMA) to further reduce error accumulation due to domain shifts
informed by previously obtained uncertainty guidance. To demonstrate the
superiority of our proposed method, we execute comprehensive experiments in
four cross-domain scenarios, securing state-of-the-art performance in BEV 3D
object detection tasks, e.g., 12.9\% NDS and 9.5\% mAP enhancement on Day-Night
adaptation.

</details>


### [120] [Where Do Tokens Go? Understanding Pruning Behaviors in STEP at High Resolutions](https://arxiv.org/abs/2509.14165)
*Michal Szczepanski,Martyna Poreba,Karim Haroun*

Main category: cs.CV

TL;DR: STEP is a hybrid token-reduction framework that combines dynamic patch merging and early token pruning to significantly reduce Vision Transformer computational costs while maintaining high accuracy in semantic segmentation.


<details>
  <summary>Details</summary>
Motivation: Vision Transformers achieve state-of-the-art performance in semantic segmentation but suffer from high computational and memory costs, which limits their practical deployment.

Method: Proposes STEP framework with dCTS (lightweight CNN-based policy network) for dynamic patch merging into superpatches, and integrates early-exits in encoder blocks to remove high-confidence tokens before final processing.

Result: Reduces token count by 2.5x, computational cost by 2.6x, throughput by 3.4x with ViT-Large backbone. Full STEP achieves 4x computational reduction, 1.7x speedup with <2% accuracy drop. 40% of tokens halted early.

Conclusion: STEP effectively addresses ViT efficiency issues through hybrid token reduction, enabling high-performance semantic segmentation with significantly reduced computational overhead.

Abstract: Vision Transformers (ViTs) achieve state-of-the-art performance in semantic
segmentation but are hindered by high computational and memory costs. To
address this, we propose STEP (SuperToken and Early-Pruning), a hybrid
token-reduction framework that combines dynamic patch merging and token pruning
to enhance efficiency without significantly compromising accuracy. At the core
of STEP is dCTS, a lightweight CNN-based policy network that enables flexible
merging into superpatches. Encoder blocks integrate also early-exits to remove
high-confident supertokens, lowering computational load. We evaluate our method
on high-resolution semantic segmentation benchmarks, including images up to
1024 x 1024, and show that when dCTS is applied alone, the token count can be
reduced by a factor of 2.5 compared to the standard 16 x 16 pixel patching
scheme. This yields a 2.6x reduction in computational cost and a 3.4x increase
in throughput when using ViT-Large as the backbone. Applying the full STEP
framework further improves efficiency, reaching up to a 4x reduction in
computational complexity and a 1.7x gain in inference speed, with a maximum
accuracy drop of no more than 2.0%. With the proposed STEP configurations, up
to 40% of tokens can be confidently predicted and halted before reaching the
final encoder layer.

</details>


### [121] [Cinéaste: A Fine-grained Contextual Movie Question Answering Benchmark](https://arxiv.org/abs/2509.14227)
*Nisarg A. Shah,Amir Ziai,Chaitanya Ekanadham,Vishal M. Patel*

Main category: cs.CV

TL;DR: Cineaste benchmark introduces 3,119 multiple-choice questions from 1,805 movie scenes across 200 films to evaluate fine-grained reasoning in long-form video understanding, revealing current MLLMs struggle with long-range temporal reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks focus on short-clip recognition and template-based questions, leaving a gap in evaluating deep narrative comprehension and fine-grained reasoning over long-form content.

Method: Used GPT-4o to generate diverse context-rich questions by integrating visual descriptions, captions, scene titles, and summaries, with a two-stage filtering process (Context-Independence and Contextual Veracity) to ensure quality.

Result: Existing MLLMs struggle significantly, with top open-source model achieving only 63.15% accuracy, showing long-range temporal reasoning as the primary bottleneck.

Conclusion: The benchmark reveals significant challenges in fine-grained contextual understanding and highlights the need for advancements in long-form movie comprehension capabilities.

Abstract: While recent advancements in vision-language models have improved video
understanding, diagnosing their capacity for deep, narrative comprehension
remains a challenge. Existing benchmarks often test short-clip recognition or
use template-based questions, leaving a critical gap in evaluating fine-grained
reasoning over long-form narrative content. To address these gaps, we introduce
$\mathsf{Cin\acute{e}aste}$, a comprehensive benchmark for long-form movie
understanding. Our dataset comprises 3,119 multiple-choice question-answer
pairs derived from 1,805 scenes across 200 diverse movies, spanning five novel
fine-grained contextual reasoning categories. We use GPT-4o to generate
diverse, context-rich questions by integrating visual descriptions, captions,
scene titles, and summaries, which require deep narrative understanding. To
ensure high-quality evaluation, our pipeline incorporates a two-stage filtering
process: Context-Independence filtering ensures questions require video
context, while Contextual Veracity filtering validates factual consistency
against the movie content, mitigating hallucinations. Experiments show that
existing MLLMs struggle on $\mathsf{Cin\acute{e}aste}$; our analysis reveals
that long-range temporal reasoning is a primary bottleneck, with the top
open-source model achieving only 63.15\% accuracy. This underscores significant
challenges in fine-grained contextual understanding and the need for
advancements in long-form movie comprehension.

</details>


### [122] [GenExam: A Multidisciplinary Text-to-Image Exam](https://arxiv.org/abs/2509.14232)
*Zhaokai Wang,Penghao Yin,Xiangyu Zhao,Changyao Tian,Yu Qiao,Wenhai Wang,Jifeng Dai,Gen Luo*

Main category: cs.CV

TL;DR: GenExam is the first benchmark for multidisciplinary text-to-image exams with 1,000 samples across 10 subjects, designed to rigorously evaluate AI models' ability to integrate knowledge, reasoning, and generation through exam-style prompts.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks focus on understanding/reasoning or basic knowledge illustration, but neglect rigorous drawing exams that require integrated intelligence. There's a need for comprehensive evaluation of models' ability to handle exam-style image generation tasks.

Method: Created GenExam benchmark with 1,000 samples across 10 subjects using exam-style prompts organized under a four-level taxonomy. Each problem includes ground-truth images and fine-grained scoring points for precise evaluation of semantic correctness and visual plausibility.

Result: State-of-the-art models like GPT-Image-1 and Gemini-2.5-Flash-Image achieved less than 15% strict scores, with most models scoring almost 0%, demonstrating the significant challenge posed by the benchmark.

Conclusion: GenExam provides a rigorous assessment framework for evaluating AI models' integrated knowledge, reasoning, and generation capabilities, offering valuable insights for advancing toward general AGI through exam-style evaluation.

Abstract: Exams are a fundamental test of expert-level intelligence and require
integrated understanding, reasoning, and generation. Existing exam-style
benchmarks mainly focus on understanding and reasoning tasks, and current
generation benchmarks emphasize the illustration of world knowledge and visual
concepts, neglecting the evaluation of rigorous drawing exams. We introduce
GenExam, the first benchmark for multidisciplinary text-to-image exams,
featuring 1,000 samples across 10 subjects with exam-style prompts organized
under a four-level taxonomy. Each problem is equipped with ground-truth images
and fine-grained scoring points to enable a precise evaluation of semantic
correctness and visual plausibility. Experiments show that even
state-of-the-art models such as GPT-Image-1 and Gemini-2.5-Flash-Image achieve
less than 15% strict scores, and most models yield almost 0%, suggesting the
great challenge of our benchmark. By framing image generation as an exam,
GenExam offers a rigorous assessment of models' ability to integrate knowledge,
reasoning, and generation, providing insights on the path to general AGI.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [123] [Noise Supervised Contrastive Learning and Feature-Perturbed for Anomalous Sound Detection](https://arxiv.org/abs/2509.13853)
*Shun Huang,Zhihua Fang,Liang He*

Main category: cs.SD

TL;DR: One-stage supervised contrastive learning (OS-SCL) method for unsupervised anomalous sound detection that reduces false alarms by perturbing features and using noisy supervised contrastive learning, achieving state-of-the-art results on DCASE 2020.


<details>
  <summary>Details</summary>
Motivation: Current self-supervised methods for anomalous sound detection suffer from frequent false alarms when handling samples from different machines of the same type, which needs to be addressed.

Method: Proposes OS-SCL technique that perturbs features in embedding space and uses one-stage noisy supervised contrastive learning. Also introduces TFgram time-frequency feature extracted from raw audio to capture critical information.

Result: Achieved 94.64% AUC, 88.42% pAUC, and 89.24% mAUC using Log-Mel features. With TFgram feature, achieved 95.71% AUC, 90.23% pAUC, and 91.23% mAUC on DCASE 2020 Challenge Task 2.

Conclusion: OS-SCL effectively addresses false alarm issues in anomalous sound detection and the proposed TFgram feature further improves performance, demonstrating state-of-the-art results on benchmark dataset.

Abstract: Unsupervised anomalous sound detection aims to detect unknown anomalous
sounds by training a model using only normal audio data. Despite advancements
in self-supervised methods, the issue of frequent false alarms when handling
samples of the same type from different machines remains unresolved. This paper
introduces a novel training technique called one-stage supervised contrastive
learning (OS-SCL), which significantly addresses this problem by perturbing
features in the embedding space and employing a one-stage noisy supervised
contrastive learning approach. On the DCASE 2020 Challenge Task 2, it achieved
94.64\% AUC, 88.42\% pAUC, and 89.24\% mAUC using only Log-Mel features.
Additionally, a time-frequency feature named TFgram is proposed, which is
extracted from raw audio. This feature effectively captures critical
information for anomalous sound detection, ultimately achieving 95.71\% AUC,
90.23\% pAUC, and 91.23\% mAUC. The source code is available at:
\underline{www.github.com/huangswt/OS-SCL}.

</details>


### [124] [A Domain Knowledge Informed Approach for Anomaly Detection of Electric Vehicle Interior Sounds](https://arxiv.org/abs/2509.13390)
*Deepti Kunte,Bram Cornelis,Claudio Colangeli,Karl Janssens,Brecht Van Baelen,Konstantinos Gryllias*

Main category: cs.SD

TL;DR: A domain-knowledge-informed approach using proxy-anomalies for unsupervised anomaly detection model selection in automotive cabin sounds, outperforming conventional methods.


<details>
  <summary>Details</summary>
Motivation: Unsupervised anomaly detection in automotive cabin sounds faces challenges in model selection due to absence of labeled faulty data and unreliable metrics like reconstruction error.

Method: Engineered proxy-anomalies through structured perturbations of healthy spectrograms for validation set to support model selection in unsupervised learning setting.

Result: Experimental evaluations on five fault types (Imbalance, Modulation, Whine, Wind, PWM) show proxy-anomaly approach significantly outperforms conventional model selection strategies.

Conclusion: The proposed domain-knowledge-informed approach using proxy-anomalies enables effective model selection for unsupervised anomaly detection in automotive cabin sound monitoring.

Abstract: The detection of anomalies in automotive cabin sounds is critical for
ensuring vehicle quality and maintaining passenger comfort. In many real-world
settings, this task is more appropriately framed as an unsupervised learning
problem rather than the supervised case due to the scarcity or complete absence
of labeled faulty data. In such an unsupervised setting, the model is trained
exclusively on healthy samples and detects anomalies as deviations from normal
behavior. However, in the absence of labeled faulty samples for validation and
the limited reliability of commonly used metrics, such as validation
reconstruction error, effective model selection remains a significant
challenge. To overcome these limitations, a domain-knowledge-informed approach
for model selection is proposed, in which proxy-anomalies engineered through
structured perturbations of healthy spectrograms are used in the validation set
to support model selection. The proposed methodology is evaluated on a
high-fidelity electric vehicle dataset comprising healthy and faulty cabin
sounds across five representative fault types viz., Imbalance, Modulation,
Whine, Wind, and Pulse Width Modulation. This dataset, generated using advanced
sound synthesis techniques, and validated via expert jury assessments, has been
made publicly available to facilitate further research. Experimental
evaluations on the five fault cases demonstrate the selection of optimal models
using proxy-anomalies, significantly outperform conventional model selection
strategies.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [125] [Explicit Reasoning Makes Better Judges: A Systematic Study on Accuracy, Efficiency, and Robustness](https://arxiv.org/abs/2509.13332)
*Pratik Jayarao,Himanshu Gupta,Neeraj Varshney,Chaitanya Dwivedi*

Main category: cs.AI

TL;DR: Thinking LLMs outperform non-thinking models in judging tasks with ~10% higher accuracy and better robustness, despite minimal computational overhead compared to augmentation strategies.


<details>
  <summary>Details</summary>
Motivation: As LLMs are increasingly used as automated judges in benchmarking and reward modeling, ensuring their reliability, efficiency, and robustness has become critical. The study aims to systematically compare thinking vs non-thinking LLMs in the LLM-as-a-judge paradigm.

Method: Systematic comparison of thinking and non-thinking Qwen 3 models (0.6B, 1.7B, 4B parameters) on RewardBench tasks. Evaluated accuracy, computational efficiency (FLOPs), and tested augmentation strategies including in-context learning, rubric-guided judging, reference-based evaluation, and n-best aggregation. Also conducted bias and robustness analyses across various bias conditions and multilingual settings.

Result: Thinking models achieve approximately 10% higher accuracy with little overhead (under 2x), while augmentation strategies like few-shot learning deliver modest gains at much higher cost (>8x). Thinking models maintain significantly greater consistency under various bias conditions (6% higher on average) and benefits extend to multilingual settings beyond English.

Conclusion: Explicit reasoning offers clear advantages in the LLM-as-a-judge paradigm, providing superior performance not only in accuracy and efficiency but also in robustness across different bias conditions and languages.

Abstract: As Large Language Models (LLMs) are increasingly adopted as automated judges
in benchmarking and reward modeling, ensuring their reliability, efficiency,
and robustness has become critical. In this work, we present a systematic
comparison of "thinking" and "non-thinking" LLMs in the LLM-as-a-judge paradigm
using open-source Qwen 3 models of relatively small sizes (0.6B, 1.7B, and 4B
parameters). We evaluate both accuracy and computational efficiency (FLOPs) on
RewardBench tasks, and further examine augmentation strategies for non-thinking
models, including in-context learning, rubric-guided judging, reference-based
evaluation, and n-best aggregation. Our results show that despite these
enhancements, non-thinking models generally fall short of their thinking
counterparts. Our results show that thinking models achieve approximately 10%
points higher accuracy with little overhead (under 2x), in contrast to
augmentation strategies like few-shot learning, which deliver modest gains at a
higher cost (>8x). Bias and robustness analyses further demonstrate that
thinking models maintain significantly greater consistency under a variety of
bias conditions such as positional, bandwagon, identity, diversity, and random
biases (6% higher on average). We further extend our experiments to the
multilingual setting and our results confirm that explicit reasoning extends
its benefits beyond English. Overall, our work results in several important
findings that provide systematic evidence that explicit reasoning offers clear
advantages in the LLM-as-a-judge paradigm not only in accuracy and efficiency
but also in robustness.

</details>


### [126] [Teaching LLMs to Plan: Logical Chain-of-Thought Instruction Tuning for Symbolic Planning](https://arxiv.org/abs/2509.13351)
*Pulkit Verma,Ngoc La,Anthony Favier,Swaroop Mishra,Julie A. Shah*

Main category: cs.AI

TL;DR: PDDL-Instruct framework enhances LLM planning capabilities through logical chain-of-thought reasoning, achieving 94% accuracy on standard benchmarks (66% improvement over baselines).


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with structured symbolic planning using formal representations like PDDL, despite their impressive general capabilities across diverse tasks.

Method: Instruction tuning framework that teaches models logical reasoning about action applicability, state transitions, and plan validity through explicit inference steps and structured reflection.

Result: Achieved up to 94% planning accuracy on standard benchmarks, representing a 66% absolute improvement over baseline models.

Conclusion: Successfully bridges the gap between LLMs' general reasoning and the logical precision required for automated planning, offering a promising direction for better AI planning systems.

Abstract: Large language models (LLMs) have demonstrated impressive capabilities across
diverse tasks, yet their ability to perform structured symbolic planning
remains limited, particularly in domains requiring formal representations like
the Planning Domain Definition Language (PDDL). In this paper, we present a
novel instruction tuning framework, PDDL-Instruct, designed to enhance LLMs'
symbolic planning capabilities through logical chain-of-thought reasoning. Our
approach focuses on teaching models to rigorously reason about action
applicability, state transitions, and plan validity using explicit logical
inference steps. By developing instruction prompts that guide models through
the precise logical reasoning required to determine when actions can be applied
in a given state, we enable LLMs to self-correct their planning processes
through structured reflection. The framework systematically builds verification
skills by decomposing the planning process into explicit reasoning chains about
precondition satisfaction, effect application, and invariant preservation.
Experimental results on multiple planning domains show that our
chain-of-thought reasoning based instruction-tuned models are significantly
better at planning, achieving planning accuracy of up to 94% on standard
benchmarks, representing a 66% absolute improvement over baseline models. This
work bridges the gap between the general reasoning capabilities of LLMs and the
logical precision required for automated planning, offering a promising
direction for developing better AI planning systems.

</details>


### [127] [SteeringControl: Holistic Evaluation of Alignment Steering in LLMs](https://arxiv.org/abs/2509.13450)
*Vincent Siu,Nicholas Crispino,David Park,Nathan W. Henry,Zhun Wang,Yang Liu,Dawn Song,Chenguang Wang*

Main category: cs.AI

TL;DR: SteeringControl is a benchmark for evaluating representation steering methods across alignment objectives like bias, harmful generation, and hallucination, while measuring side effects on secondary behaviors.


<details>
  <summary>Details</summary>
Motivation: Prior alignment work often highlights truthfulness or reasoning to demonstrate side effects of representation steering, but many tradeoffs remain unexplored systematically.

Method: Collected a dataset of safety-relevant behaviors to evaluate steering effectiveness and behavioral entanglement using five popular steering methods, with a modular steering framework based on unique components.

Result: Strong steering performance depends on specific combinations of steering method, model, and targeted behavior, with severe concept entanglement resulting from poor combinations.

Conclusion: The study provides a systematic evaluation framework for representation steering methods and reveals the importance of method-model-behavior combinations, with code released for public use.

Abstract: We introduce SteeringControl, a benchmark for evaluating representation
steering methods across core alignment objectives--bias, harmful generation,
and hallucination--and their effects on secondary behaviors such as sycophancy
and commonsense morality. While prior alignment work often highlights
truthfulness or reasoning ability to demonstrate the side effects of
representation steering, we find there are many unexplored tradeoffs not yet
understood in a systematic way. We collect a dataset of safety-relevant primary
and secondary behaviors to evaluate steering effectiveness and behavioral
entanglement centered around five popular steering methods. To enable this, we
craft a modular steering framework based on unique components that serve as the
building blocks of many existing methods. Our results on Qwen-2.5-7B and
Llama-3.1-8B find that strong steering performance is dependent on the specific
combination of steering method, model, and targeted behavior, and that severe
concept entanglement can result from poor combinations of these three as well.
We release our code here:
https://github.com/wang-research-lab/SteeringControl.git.

</details>


### [128] [See, Think, Act: Teaching Multimodal Agents to Effectively Interact with GUI by Identifying Toggles](https://arxiv.org/abs/2509.13615)
*Zongru Wu,Rui Mao,Zhiyuan Tian,Pengzhou Cheng,Tianjie Ju,Zheng Wu,Lingzhong Dong,Haiyue Sheng,Zhuosheng Zhang,Gongshen Liu*

Main category: cs.AI

TL;DR: The paper addresses the problem of multimodal agents failing to reliably execute toggle control instructions in GUI environments, and proposes State-aware Reasoning (StaR) to improve toggle state perception and action execution.


<details>
  <summary>Details</summary>
Motivation: Multimodal agents struggle with toggle control instructions in GUI environments, particularly when the current toggle state already matches the desired state, creating a reliability bottleneck in ubiquitous GUI control.

Method: The authors propose State-aware Reasoning (StaR), a training method that teaches agents to perceive current toggle state, analyze desired state from instructions, and act accordingly. They construct a state control benchmark with binary toggle instructions from public datasets.

Result: StaR improves toggle instruction execution accuracy by over 30% across three multimodal agents. It also enhances general task performance on three public benchmarks and shows potential for real-world applications in dynamic environments.

Conclusion: State-aware Reasoning effectively addresses the toggle control reliability problem in multimodal agents, significantly improving execution accuracy and demonstrating broad applicability across different GUI control tasks and environments.

Abstract: The advent of multimodal agents facilitates effective interaction within
graphical user interface (GUI), especially in ubiquitous GUI control. However,
their inability to reliably execute toggle control instructions remains a key
bottleneck. To investigate this, we construct a state control benchmark with
binary toggle instructions from public datasets. Evaluations of existing agents
demonstrate their unreliability, particularly when the current toggle state
already matches the desired state. To address the challenge, we propose
State-aware Reasoning (StaR), a training method that teaches agents to perceive
the current toggle state, analyze the desired state from the instruction, and
act accordingly. Experiments on three multimodal agents demonstrate that StaR
can improve toggle instruction execution accuracy by over 30\%. Further
evaluations on three public benchmarks show that StaR also enhances general
task performance. Finally, evaluations on a dynamic environment highlight the
potential of StaR for real-world applications. Code, benchmark, and
StaR-enhanced agents are available at https://github.com/ZrW00/StaR.

</details>


### [129] [THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical Reasoning](https://arxiv.org/abs/2509.13761)
*Qikai Chang,Zhenrong Zhang,Pengfei Hu,Jiefeng Ma,Yicheng Pan,Jianshu Zhang,Jun Du,Quan Liu,Jianqing Gao*

Main category: cs.AI

TL;DR: THOR is a tool-integrated hierarchical optimization framework that uses RL to enhance LLMs' mathematical reasoning through multi-agent data generation, fine-grained optimization, and self-correction mechanisms.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with high-precision mathematical tasks like numerical computation and symbolic manipulation, and existing tool-integration methods face challenges in data construction, fine-grained optimization, and inference enhancement.

Method: Proposes THOR with three components: 1) TIRGen multi-agent pipeline for generating tool-integrated reasoning data, 2) RL strategy for joint trajectory-level and step-level optimization, 3) Self-correction mechanism using tool feedback during inference.

Result: Achieves state-of-the-art performance on multiple mathematical benchmarks for similar-scale models, demonstrates strong generalization across diverse models, and shows consistent improvements on code benchmarks.

Conclusion: THOR effectively bridges LLMs' precision gap in mathematical reasoning through hierarchical optimization and tool integration, offering a scalable solution that generalizes well across different model types.

Abstract: Large Language Models (LLMs) have made remarkable progress in mathematical
reasoning, but still continue to struggle with high-precision tasks like
numerical computation and formal symbolic manipulation. Integrating external
tools has emerged as a promising approach to bridge this gap. Despite recent
advances, existing methods struggle with three key challenges: constructing
tool-integrated reasoning data, performing fine-grained optimization, and
enhancing inference. To overcome these limitations, we propose THOR
(Tool-Integrated Hierarchical Optimization via RL). First, we introduce TIRGen,
a multi-agent actor-critic-based pipeline for constructing high-quality
datasets of tool-integrated reasoning paths, aligning with the policy and
generalizing well across diverse models. Second, to perform fine-grained
hierarchical optimization, we introduce an RL strategy that jointly optimizes
for both trajectory-level problem solving and step-level code generation. This
is motivated by our key insight that the success of an intermediate tool call
is a strong predictor of the final answer's correctness. Finally, THOR
incorporates a self-correction mechanism that leverages immediate tool feedback
to dynamically revise erroneous reasoning paths during inference. Our approach
demonstrates strong generalization across diverse models, performing
effectively in both reasoning and non-reasoning models. It further achieves
state-of-the-art performance for models of a similar scale on multiple
mathematical benchmarks, while also delivering consistent improvements on code
benchmarks. Our code will be publicly available at
https://github.com/JingMog/THOR.

</details>


### [130] [Exploring Major Transitions in the Evolution of Biological Cognition With Artificial Neural Networks](https://arxiv.org/abs/2509.13968)
*Konstantinos Voudouris,Andrew Barron,Marta Halina,Colin Klein,Matishalin Patel*

Main category: cs.AI

TL;DR: The paper investigates whether changes in neural network topology can create major transitions in cognitive performance, similar to evolutionary transitions. Using ANNs with different architectures, they found recurrent networks show qualitative performance improvements over feed-forward networks for complex grammar learning, with training difficulties acting as transition barriers.


<details>
  <summary>Details</summary>
Motivation: To evaluate if changes in information flow through different neural network topologies can produce transitional changes in cognitive capabilities, mirroring major evolutionary transitions that fundamentally alter what is evolvable.

Method: Used idealised artificial neural networks (ANNs) with feed-forward, recurrent, and laminated topologies. Tested performance on learning artificial grammars of varying complexity while controlling for network size and computational resources.

Result: Recurrent networks showed qualitative expansion in processable input types and performance improvement for complex grammars compared to feed-forward networks. Training difficulties created transition barriers and contingent irreversibility. Laminated networks did not outperform non-laminated ones in grammar learning.

Conclusion: Some changes in information flow topology can indeed yield transitional changes in cognitive performance, with recurrent networks demonstrating major advantages for complex tasks, supporting the concept of cognitive evolutionary transitions.

Abstract: Transitional accounts of evolution emphasise a few changes that shape what is
evolvable, with dramatic consequences for derived lineages. More recently it
has been proposed that cognition might also have evolved via a series of major
transitions that manipulate the structure of biological neural networks,
fundamentally changing the flow of information. We used idealised models of
information flow, artificial neural networks (ANNs), to evaluate whether
changes in information flow in a network can yield a transitional change in
cognitive performance. We compared networks with feed-forward, recurrent and
laminated topologies, and tested their performance learning artificial grammars
that differed in complexity, controlling for network size and resources. We
documented a qualitative expansion in the types of input that recurrent
networks can process compared to feed-forward networks, and a related
qualitative increase in performance for learning the most complex grammars. We
also noted how the difficulty in training recurrent networks poses a form of
transition barrier and contingent irreversibility -- other key features of
evolutionary transitions. Not all changes in network topology confer a
performance advantage in this task set. Laminated networks did not outperform
non-laminated networks in grammar learning. Overall, our findings show how some
changes in information flow can yield transitions in cognitive performance.

</details>


### [131] [The Art of Saying "Maybe": A Conformal Lens for Uncertainty Benchmarking in VLMs](https://arxiv.org/abs/2509.13379)
*Asif Azad,Mohammad Sadat Hossain,MD Sadik Hossain Shanto,M Saifur Rahman,Md Rizwan Pervez*

Main category: cs.AI

TL;DR: Comprehensive uncertainty benchmarking study of 16 state-of-the-art VLMs across 6 multimodal datasets reveals that larger models have better uncertainty quantification, mathematical/reasoning tasks show poorer uncertainty performance, and more certain models achieve higher accuracy.


<details>
  <summary>Details</summary>
Motivation: Vision-Language Models have advanced significantly in visual understanding, but uncertainty quantification has received insufficient attention despite being critical for reliable multimodal systems.

Method: Evaluated 16 state-of-the-art VLMs (open and closed-source) across 6 multimodal datasets using 3 distinct scoring functions to conduct comprehensive uncertainty benchmarking.

Result: Larger models consistently exhibit better uncertainty quantification; models with higher accuracy are more certain; mathematical and reasoning tasks show poorer uncertainty performance across all models compared to other domains.

Conclusion: This work establishes a foundation for reliable uncertainty evaluation in multimodal systems, demonstrating that models that know more also know better what they don't know.

Abstract: Vision-Language Models (VLMs) have achieved remarkable progress in complex
visual understanding across scientific and reasoning tasks. While performance
benchmarking has advanced our understanding of these capabilities, the critical
dimension of uncertainty quantification has received insufficient attention.
Therefore, unlike prior conformal prediction studies that focused on limited
settings, we conduct a comprehensive uncertainty benchmarking study, evaluating
16 state-of-the-art VLMs (open and closed-source) across 6 multimodal datasets
with 3 distinct scoring functions. Our findings demonstrate that larger models
consistently exhibit better uncertainty quantification; models that know more
also know better what they don't know. More certain models achieve higher
accuracy, while mathematical and reasoning tasks elicit poorer uncertainty
performance across all models compared to other domains. This work establishes
a foundation for reliable uncertainty evaluation in multimodal systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [132] [Privacy-Aware In-Context Learning for Large Language Models](https://arxiv.org/abs/2509.13625)
*Bishnu Bhusal,Manoj Acharya,Ramneet Kaur,Colin Samplawski,Anirban Roy,Adam D. Cobb,Rohit Chadha,Susmit Jha*

Main category: cs.LG

TL;DR: A novel private prediction framework using Differential Privacy to generate high-quality synthetic text while preventing information leakage from LLM prompts, outperforming previous methods on in-context learning tasks.


<details>
  <summary>Details</summary>
Motivation: Address privacy concerns in large language models where adversaries can extract sensitive information from prompts, requiring strong privacy guarantees without compromising utility.

Method: Leverages Differential Privacy framework to ensure worst-case theoretical bounds on information leakage. Performs inference on private records and aggregates per-token output distributions. Uses a blending operation that combines private and public inference to enhance utility.

Result: Empirical evaluations show the approach outperforms previous state-of-the-art methods on in-context-learning tasks, generating longer and coherent synthetic text while maintaining privacy guarantees.

Conclusion: The proposed framework provides a promising direction for privacy-preserving text generation with strong privacy guarantees while maintaining high utility, without requiring fine-tuning of underlying models.

Abstract: Large language models (LLMs) have significantly transformed natural language
understanding and generation, but they raise privacy concerns due to potential
exposure of sensitive information. Studies have highlighted the risk of
information leakage, where adversaries can extract sensitive information
embedded in the prompts. In this work, we introduce a novel private prediction
framework for generating high-quality synthetic text with strong privacy
guarantees. Our approach leverages the Differential Privacy (DP) framework to
ensure worst-case theoretical bounds on information leakage without requiring
any fine-tuning of the underlying models.The proposed method performs inference
on private records and aggregates the resulting per-token output distributions.
This enables the generation of longer and coherent synthetic text while
maintaining privacy guarantees. Additionally, we propose a simple blending
operation that combines private and public inference to further enhance
utility. Empirical evaluations demonstrate that our approach outperforms
previous state-of-the-art methods on in-context-learning (ICL) tasks, making it
a promising direction for privacy-preserving text generation while maintaining
high utility.

</details>


### [133] [Language models' activations linearly encode training-order recency](https://arxiv.org/abs/2509.14223)
*Dmitrii Krasheninnikov,Richard E. Turner,David Krueger*

Main category: cs.LG

TL;DR: Language models linearly encode when information was learned during training, with activations arranging in chronological order along a straight line in 2D space, enabling accurate temporal classification of learned entities.


<details>
  <summary>Details</summary>
Motivation: To investigate whether language models retain temporal information about when specific knowledge was acquired during training, which could reveal how models manage conflicting data and knowledge updates.

Method: Sequentially fine-tuned Llama-3.2-1B on six disjoint datasets about named entities, then analyzed average activations and trained linear probes to detect training order from model representations.

Result: Activations arranged exactly in training order along a straight line in 2D subspace; linear probes achieved ~90% accuracy distinguishing early vs late entities, with generalization to unseen entities; fine-tuning achieved ~80% accuracy for explicit training stage reporting.

Conclusion: Models can differentiate information by acquisition time, with temporal signals encoded linearly in activations, suggesting implications for how models handle conflicting data and knowledge modifications.

Abstract: We show that language models' activations linearly encode when information
was learned during training. Our setup involves creating a model with a known
training order by sequentially fine-tuning Llama-3.2-1B on six disjoint but
otherwise similar datasets about named entities. We find that the average
activations of test samples for the six training datasets encode the training
order: when projected into a 2D subspace, these centroids are arranged exactly
in the order of training and lie on a straight line. Further, we show that
linear probes can accurately (~90%) distinguish "early" vs. "late" entities,
generalizing to entities unseen during the probes' own training. The model can
also be fine-tuned to explicitly report an unseen entity's training stage (~80%
accuracy). Interestingly, this temporal signal does not seem attributable to
simple differences in activation magnitudes, losses, or model confidence. Our
paper demonstrates that models are capable of differentiating information by
its acquisition time, and carries significant implications for how they might
manage conflicting data and respond to knowledge modifications.

</details>


### [134] [LLM-I: LLMs are Naturally Interleaved Multimodal Creators](https://arxiv.org/abs/2509.13642)
*Zirun Guo,Feng Zhang,Kai Jia,Tao Jin*

Main category: cs.LG

TL;DR: LLM-Interleaved (LLM-I) is a framework that treats interleaved image-text generation as a tool-use problem, allowing LLMs to dynamically select and apply specialized visual tools like image search, diffusion generation, code execution, and image editing through reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Current unified models suffer from a "one-tool" bottleneck, being limited to synthetic imagery and struggling with tasks requiring factual grounding or programmatic precision in interleaved image-text generation.

Method: A central LLM/MLLM agent orchestrates diverse visual tools via reinforcement learning with hybrid rewards combining rule-based logic and LLM/MLLM evaluator judgments, trained on a diverse dataset using four model backbones.

Result: LLM-I achieves state-of-the-art performance, outperforming existing methods by a large margin across four benchmarks, with additional gains from a novel test-time scaling strategy.

Conclusion: The framework successfully overcomes the limitations of current unified models by enabling dynamic tool selection and application, demonstrating superior performance in interleaved image-text generation tasks.

Abstract: We propose LLM-Interleaved (LLM-I), a flexible and dynamic framework that
reframes interleaved image-text generation as a tool-use problem. LLM-I is
designed to overcome the "one-tool" bottleneck of current unified models, which
are limited to synthetic imagery and struggle with tasks requiring factual
grounding or programmatic precision. Our framework empowers a central LLM or
MLLM agent to intelligently orchestrate a diverse toolkit of specialized visual
tools, including online image search, diffusion-based generation, code
execution, and image editing. The agent is trained to select and apply these
tools proficiently via a Reinforcement Learning (RL) framework that features a
hybrid reward system combining rule-based logic with judgments from LLM and
MLLM evaluators. Trained on a diverse new dataset using four different model
backbones, LLM-I demonstrates state-of-the-art performance, outperforming
existing methods by a large margin across four benchmarks. We also introduce a
novel test-time scaling strategy that provides further performance gains.
Project Page: https://github.com/ByteDance-BandAI/LLM-I.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [135] [Rest2Visual: Predicting Visually Evoked fMRI from Resting-State Scans](https://arxiv.org/abs/2509.13612)
*Chuyang Zhou,Ziao Ji,Daochang Liu,Dongang Wang,Chenyu Wang,Chang Xu*

Main category: q-bio.NC

TL;DR: Rest2Visual is a conditional generative model that predicts visually evoked fMRI from resting-state fMRI and visual stimuli, enabling stimulus-specific brain activation synthesis without task-based scanning.


<details>
  <summary>Details</summary>
Motivation: Task-based fMRI is costly and time-consuming, while abundant resting-state fMRI lacks direct interpretability. The paper aims to bridge this gap by predicting stimulus-evoked responses from resting-state data.

Method: Volumetric encoder-decoder design that modulates multiscale 3D features from rs-fMRI with image embeddings via adaptive normalization. Trained on a large-scale triplet dataset from NSD aligning rs-fMRI, stimulus images, and ve-fMRI maps.

Result: Predicted activations closely match ground truth across similarity and representational metrics, support successful image reconstruction, and preserve subject-specific functional structure.

Conclusion: Individualized spontaneous neural activity can be transformed into stimulus-aligned representations, enabling scalable, task-free functional brain modeling.

Abstract: Understanding how spontaneous brain activity relates to stimulus-driven
neural responses is a fundamental challenge in cognitive neuroscience. While
task-based functional magnetic resonance imaging (fMRI) captures localized
stimulus-evoked brain activation, its acquisition is costly, time-consuming,
and difficult to scale across populations. In contrast, resting-state fMRI
(rs-fMRI) is task-free and abundant, but lacks direct interpretability. We
introduce Rest2Visual, a conditional generative model that predicts visually
evoked fMRI (ve-fMRI) from resting-state input and 2D visual stimuli. It
follows a volumetric encoder--decoder design, where multiscale 3D features from
rs-fMRI are modulated by image embeddings via adaptive normalization, enabling
spatially accurate, stimulus-specific activation synthesis. To enable model
training, we construct a large-scale triplet dataset from the Natural Scenes
Dataset (NSD), aligning each rs-fMRI volume with stimulus images and their
corresponding ve-fMRI activation maps. Quantitative evaluation shows that the
predicted activations closely match ground truth across standard similarity and
representational metrics, and support successful image reconstruction in
downstream decoding. Notably, the predicted maps preserve subject-specific
structure, demonstrating the model's capacity to generate individualized
functional surrogates. Our results provide compelling evidence that
individualized spontaneous neural activity can be transformed into
stimulus-aligned representations, opening new avenues for scalable, task-free
functional brain modeling.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [136] [TICL: Text-Embedding KNN For Speech In-Context Learning Unlocks Speech Recognition Abilities of Large Multimodal Models](https://arxiv.org/abs/2509.13395)
*Haolong Zheng,Yekaterina Yegorova,Mark Hasegawa-Johnson*

Main category: eess.AS

TL;DR: TICL uses text-embedding KNN to select effective in-context examples for speech foundation models, improving speech recognition performance without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Speech In-Context Learning (SICL) performance depends on effective example selection, but current selection methodologies are underexplored despite their importance.

Method: Proposes Text-Embedding KNN for SICL (TICL) - a simple pipeline that uses semantic context to enhance off-the-shelf large multimodal models' speech recognition without requiring fine-tuning.

Result: Achieves up to 84.7% relative WER reduction across challenging ASR tasks including accented English, multilingual speech, and children's speech, surpassing zero-shot performance.

Conclusion: The method demonstrates robustness and efficiency through ablation studies, providing a simple yet effective approach for improving SICL performance through better in-context example selection.

Abstract: Speech foundation models have recently demonstrated the ability to perform
Speech In-Context Learning (SICL). Selecting effective in-context examples is
crucial for SICL performance, yet selection methodologies remain underexplored.
In this work, we propose Text-Embedding KNN for SICL (TICL), a simple pipeline
that uses semantic context to enhance off-the-shelf large multimodal models'
speech recognition ability without fine-tuning. Across challenging automatic
speech recognition tasks, including accented English, multilingual speech, and
children's speech, our method enables models to surpass zero-shot performance
with up to 84.7% relative WER reduction. We conduct ablation studies to show
the robustness and efficiency of our method.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [137] [An Empirical Study on Failures in Automated Issue Solving](https://arxiv.org/abs/2509.13941)
*Simiao Liu,Fang Liu,Liehao Li,Xin Tan,Yinghao Zhu,Xiaoli Lian,Li Zhang*

Main category: cs.SE

TL;DR: The paper analyzes failure modes in automated issue solving on SWE-Bench, develops a taxonomy of 25 failure subcategories, identifies distinct failure patterns between pipeline and agentic architectures, and proposes an Expert-Executor framework that solves 22.2% of previously intractable issues.


<details>
  <summary>Details</summary>
Motivation: Current evaluations of automated issue solving tools only report aggregate success rates, which obscure the underlying causes of failure and make it difficult to diagnose model weaknesses or guide targeted improvements.

Method: Analyzed performance of three state-of-the-art tools on SWE-Bench-Verified, conducted manual analysis of 150 failed instances to develop a comprehensive failure taxonomy, and proposed a collaborative Expert-Executor framework with supervisory oversight.

Result: Identified distinct failure fingerprints between architectural paradigms, with agentic failures primarily stemming from flawed reasoning and cognitive deadlocks. The proposed Expert-Executor framework solved 22.2% of previously intractable issues for a leading single agent.

Conclusion: The findings demonstrate the value of diagnostic evaluation and collaborative agent design for building more robust automated issue solving systems, paving the way for improved performance through targeted architectural improvements.

Abstract: Automated issue solving seeks to autonomously identify and repair defective
code snippets across an entire codebase. SWE-Bench has emerged as the most
widely adopted benchmark for evaluating progress in this area. While LLM-based
agentic tools show great promise, they still fail on a substantial portion of
tasks. Moreover, current evaluations primarily report aggregate issue-solving
rates, which obscure the underlying causes of success and failure, making it
challenging to diagnose model weaknesses or guide targeted improvements. To
bridge this gap, we first analyze the performance and efficiency of three SOTA
tools, spanning both pipeline-based and agentic architectures, in automated
issue solving tasks of SWE-Bench-Verified under varying task characteristics.
Furthermore, to move from high-level performance metrics to underlying cause
analysis, we conducted a systematic manual analysis of 150 failed instances.
From this analysis, we developed a comprehensive taxonomy of failure modes
comprising 3 primary phases, 9 main categories, and 25 fine-grained
subcategories. Then we systematically analyze the distribution of the
identified failure modes, the results reveal distinct failure fingerprints
between the two architectural paradigms, with the majority of agentic failures
stemming from flawed reasoning and cognitive deadlocks. Motivated by these
insights, we propose a collaborative Expert-Executor framework. It introduces a
supervisory Expert agent tasked with providing strategic oversight and
course-correction for a primary Executor agent. This architecture is designed
to correct flawed reasoning and break the cognitive deadlocks that frequently
lead to failure. Experiments show that our framework solves 22.2% of previously
intractable issues for a leading single agent. These findings pave the way for
building more robust agents through diagnostic evaluation and collaborative
design.

</details>


### [138] [Reasoning Efficiently Through Adaptive Chain-of-Thought Compression: A Self-Optimizing Framework](https://arxiv.org/abs/2509.14093)
*Kerui Huang,Shuhan Liu,Xing Hu,Tongtong Xu,Lingfeng Bao,Xin Xia*

Main category: cs.SE

TL;DR: Chain-of-Thought reasoning improves LLM accuracy but has high computational costs. SEER framework adaptively compresses CoT reasoning, reducing length by 42.1% while maintaining accuracy and eliminating infinite loops.


<details>
  <summary>Details</summary>
Motivation: Chain-of-Thought reasoning enhances LLM performance but comes with significant computational overhead including increased latency, memory usage, and KV-cache demands, which are problematic for software engineering tasks requiring concise outputs.

Method: SEER (Self-Enhancing Efficient Reasoning) framework that combines Best-of-N sampling with task-aware adaptive filtering, dynamically adjusting thresholds based on pre-inference outputs to compress CoT reasoning while preserving accuracy.

Result: SEER reduces CoT length by 42.1% on average, improves accuracy by reducing truncation issues, eliminates most infinite loops, and maintains comparable performance while significantly reducing computational overhead.

Conclusion: Longer reasoning isn't always better; SEER provides an effective adaptive framework to make CoT-enhanced LLMs more efficient and robust, addressing the trade-offs between reasoning length and computational costs.

Abstract: Chain-of-Thought (CoT) reasoning enhances Large Language Models (LLMs) by
prompting intermediate steps, improving accuracy and robustness in arithmetic,
logic, and commonsense tasks. However, this benefit comes with high
computational costs: longer outputs increase latency, memory usage, and
KV-cache demands. These issues are especially critical in software engineering
tasks where concise and deterministic outputs are required. To investigate
these trade-offs, we conduct an empirical study based on code generation
benchmarks. The results reveal that longer CoT does not always help. Excessive
reasoning often causes truncation, accuracy drops, and latency up to five times
higher, with failed outputs consistently longer than successful ones. These
findings challenge the assumption that longer reasoning is inherently better
and highlight the need for adaptive CoT control. Motivated by this, we propose
SEER (Self-Enhancing Efficient Reasoning), an adaptive framework that
compresses CoT while preserving accuracy. SEER combines Best-of-N sampling with
task-aware adaptive filtering, dynamically adjusting thresholds based on
pre-inference outputs to reduce verbosity and computational overhead. We then
evaluate SEER on three software engineering tasks and one math task. On
average, SEER shortens CoT by 42.1%, improves accuracy by reducing truncation,
and eliminates most infinite loops. These results demonstrate SEER as a
practical method to make CoT-enhanced LLMs more efficient and robust, even
under resource constraints.

</details>


<div id='q-bio.PE'></div>

# q-bio.PE [[Back]](#toc)

### [139] [Autonomous Reporting of Normal Chest X-rays by Artificial Intelligence in the United Kingdom. Can We Take the Human Out of the Loop?](https://arxiv.org/abs/2509.13428)
*Katrina Nash,James Vaz,Ahmed Maiter,Christopher Johns,Nicholas Woznitza,Aditya Kale,Abdala Espinosa Morgado,Rhidian Bramley,Mark Hall,David Lowe,Alex Novak,Sarim Ather*

Main category: q-bio.PE

TL;DR: AI tools for autonomous reporting of normal chest X-rays could reduce radiologist workload but face challenges in defining normalcy, ensuring generalizability, and addressing legal/regulatory issues.


<details>
  <summary>Details</summary>
Motivation: Radiologist workforce shortages cause reporting delays for chest X-rays, creating a need for AI solutions that can autonomously identify normal CXRs to reduce workload.

Method: Examines feasibility through analysis of key issues including defining normal CXRs, ensuring generalizability across populations, managing sensitivity-specificity trade-offs, and addressing legal/regulatory compliance.

Result: Identifies multiple challenges including the need for clear definitions of normalcy, generalizability concerns, legal compliance requirements (IR(ME)R, GDPR), accountability frameworks, and impact on radiologist practice.

Conclusion: While autonomous AI reporting of normal CXRs offers clear benefits for reducing workload, adoption must be cautious due to significant technical, regulatory, and practical challenges that need to be addressed.

Abstract: Chest X-rays (CXRs) are the most commonly performed imaging investigation. In
the UK, many centres experience reporting delays due to radiologist workforce
shortages. Artificial intelligence (AI) tools capable of distinguishing normal
from abnormal CXRs have emerged as a potential solution. If normal CXRs could
be safely identified and reported without human input, a substantial portion of
radiology workload could be reduced.
  This article examines the feasibility and implications of autonomous AI
reporting of normal CXRs. Key issues include defining normal, ensuring
generalisability across populations, and managing the sensitivity-specificity
trade-off. It also addresses legal and regulatory challenges, such as
compliance with IR(ME)R and GDPR, and the lack accountability frameworks for
errors. Further considerations include the impact on radiologists practice, the
need for robust post-market surveillance, and incorporation of patient
perspectives. While the benefits are clear, adoption must be cautious.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [140] [A TRRIP Down Memory Lane: Temperature-Based Re-Reference Interval Prediction For Instruction Caching](https://arxiv.org/abs/2509.14041)
*Henry Kao,Nikhil Sreekumar,Prabhdeep Singh Soni,Ali Sedaghati,Fang Su,Bryan Chan,Maziar Goudarzi,Reza Azimi*

Main category: cs.AR

TL;DR: TRRIP is a software-hardware co-design approach that uses compiler analysis to classify code as hot/cold and provides this information to hardware for optimized instruction cache replacement, reducing L2 MPKI by 26.5% and achieving 3.9% speedup in mobile systems.


<details>
  <summary>Details</summary>
Motivation: Mobile CPU software exhibits complex runtime behavior with high reuse distance between instruction executions, causing frontend stalls and resource starvation. Conventional hardware-centric cache management is inadequate as application complexity grows faster than available on-chip memory.

Method: Compiler analyzes and classifies code based on temperature (hot/cold), transforms code, and provides temperature information to hardware through OS interface using code page attributes. Lightweight hardware extension uses these attributes to optimize instruction cache replacement policy.

Result: TRRIP reduces L2 MPKI for instructions by 26.5% and achieves geomean speedup of 3.9% on top of RRIP cache replacement with PGO-optimized mobile code.

Conclusion: TRRIP provides a practical software-hardware co-design solution that effectively addresses instruction cache management challenges in mobile systems, significantly improving performance through temperature-based code classification and optimized replacement policies.

Abstract: Modern mobile CPU software pose challenges for conventional instruction cache
replacement policies due to their complex runtime behavior causing high reuse
distance between executions of the same instruction. Mobile code commonly
suffers from large amounts of stalls in the CPU frontend and thus starvation of
the rest of the CPU resources. Complexity of these applications and their code
footprint are projected to grow at a rate faster than available on-chip memory
due to power and area constraints, making conventional hardware-centric methods
for managing instruction caches to be inadequate. We present a novel
software-hardware co-design approach called TRRIP (Temperature-based
Re-Reference Interval Prediction) that enables the compiler to analyze,
classify, and transform code based on "temperature" (hot/cold), and to provide
the hardware with a summary of code temperature information through a
well-defined OS interface based on using code page attributes. TRRIP's
lightweight hardware extension employs code temperature attributes to optimize
the instruction cache replacement policy resulting in the eviction rate
reduction of hot code. TRRIP is designed to be practical and adoptable in real
mobile systems that have strict feature requirements on both the software and
hardware components. TRRIP can reduce the L2 MPKI for instructions by 26.5%
resulting in geomean speedup of 3.9%, on top of RRIP cache replacement running
mobile code already optimized using PGO.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [141] [3D Reconstruction of Coronary Vessel Trees from Biplanar X-Ray Images Using a Geometric Approach](https://arxiv.org/abs/2509.13358)
*Ethan Koland,Lin Xi,Nadeev Wijesuriya,YingLiang Ma*

Main category: eess.IV

TL;DR: A framework for 3D vessel tree reconstruction from biplanar X-ray angiography images using automatic segmentation, motion phase matching, and geometric reconstruction with improved accuracy over traditional methods.


<details>
  <summary>Details</summary>
Motivation: X-ray angiography is crucial for cardiac interventions but current 3D reconstruction methods face challenges with motion artifacts and segmentation accuracy. The paper aims to develop a more accurate and simplified workflow for 3D vessel reconstruction from biplanar X-ray images.

Method: Three-component framework: 1) Automatic video segmentation for semantic labeling of vessels/catheters, 2) Motion phase matching using stationary object tracking to synchronize cardiac/respiratory phases, 3) Geometric reconstruction algorithm using 3D surface intersections instead of epipolar constraints, with landmark matching via heuristic error minimization.

Result: Segmentation accuracy of 0.703 on test set of 62 X-ray angiography videos. 3D reconstruction achieved reprojection errors of 0.62mm ± 0.38mm for anatomical landmarks, demonstrating improved accuracy over traditional methods.

Conclusion: The proposed framework successfully reconstructs 3D vessel trees with higher accuracy and simplified workflow compared to traditional epipolar constraint methods, making it suitable for clinical cardiac intervention applications.

Abstract: X-ray angiography is widely used in cardiac interventions to visualize
coronary vessels, assess integrity, detect stenoses and guide treatment. We
propose a framework for reconstructing 3D vessel trees from biplanar X-ray
images which are extracted from two X-ray videos captured at different C-arm
angles. The proposed framework consists of three main components: image
segmentation, motion phase matching, and 3D reconstruction. An automatic video
segmentation method for X-ray angiography to enable semantic segmentation for
image segmentation and motion phase matching. The goal of the motion phase
matching is to identify a pair of X-ray images that correspond to a similar
respiratory and cardiac motion phase to reduce errors in 3D reconstruction.
This is achieved by tracking a stationary object such as a catheter or lead
within the X-ray video. The semantic segmentation approach assigns different
labels to different object classes enabling accurate differentiation between
blood vessels, balloons, and catheters. Once a suitable image pair is selected,
key anatomical landmarks (vessel branching points and endpoints) are matched
between the two views using a heuristic method that minimizes reconstruction
errors. This is followed by a novel geometric reconstruction algorithm to
generate the 3D vessel tree. The algorithm computes the 3D vessel centrelines
by determining the intersection of two 3D surfaces. Compared to traditional
methods based on epipolar constraints, the proposed approach simplifies there
construction workflow and improves overall accuracy. We trained and validated
our segmentation method on 62 X-ray angiography video sequences. On the test
set, our method achieved a segmentation accuracy of 0.703. The 3D
reconstruction framework was validated by measuring the reconstruction error of
key anatomical landmarks, achieving a reprojection errors of 0.62mm +/- 0.38mm.

</details>


### [142] [PREDICT-GBM: Platform for Robust Evaluation and Development of Individualized Computational Tumor Models in Glioblastoma](https://arxiv.org/abs/2509.13360)
*L. Zimmer,J. Weidner,M. Balcerak,F. Kofler,I. Ezhov,B. Menze,B. Wiestler*

Main category: eess.IV

TL;DR: PREDICT-GBM is an integrated pipeline and dataset for systematic benchmarking of glioblastoma growth models, showing personalized radiation plans based on model predictions outperform conventional uniform margin approaches.


<details>
  <summary>Details</summary>
Motivation: Conventional radiation therapy for glioblastoma uses uniform treatment margins that don't account for patient-specific factors influencing tumor cell migration. Despite computational growth models showing promise, clinical adoption remains limited due to the translational gap.

Method: Developed PREDICT-GBM - a comprehensive integrated pipeline and dataset with expert-curated clinical data from 255 subjects, including complete tumor segmentations and tissue characterization maps for systematic benchmarking of state-of-the-art tumor growth models.

Result: Personalized radiation treatment plans derived from tumor growth predictions achieved superior recurrence coverage compared to conventional uniform margin approaches for two of the evaluated models.

Conclusion: PREDICT-GBM establishes a robust platform for advancing and systematically evaluating cutting-edge tumor growth modeling approaches, with the goal of facilitating clinical translation and improving patient outcomes in glioblastoma treatment.

Abstract: Glioblastoma is the most prevalent primary brain malignancy, distinguished by
its highly invasive behavior and exceptionally high rates of recurrence.
Conventional radiation therapy, which employs uniform treatment margins, fails
to account for patient-specific anatomical and biological factors that
critically influence tumor cell migration. To address this limitation, numerous
computational models of glioblastoma growth have been developed, enabling
generation of tumor cell distribution maps extending beyond radiographically
visible regions and thus informing more precise treatment strategies. However,
despite encouraging preliminary findings, the clinical adoption of these growth
models remains limited. To bridge this translational gap and accelerate both
model development and clinical validation, we introduce PREDICT-GBM, a
comprehensive integrated pipeline and dataset for modeling and evaluation. This
platform enables systematic benchmarking of state-of-the-art tumor growth
models using an expert-curated clinical dataset comprising 255 subjects with
complete tumor segmentations and tissue characterization maps. Our analysis
demonstrates that personalized radiation treatment plans derived from tumor
growth predictions achieved superior recurrence coverage compared to
conventional uniform margin approaches for two of the evaluated models. This
work establishes a robust platform for advancing and systematically evaluating
cutting-edge tumor growth modeling approaches, with the ultimate goal of
facilitating clinical translation and improving patient outcomes.

</details>


### [143] [Generative AI Pipeline for Interactive Prompt-driven 2D-to-3D Vascular Reconstruction for Fontan Geometries from Contrast-Enhanced X-Ray Fluoroscopy Imaging](https://arxiv.org/abs/2509.13372)
*Prahlad G Menon*

Main category: eess.IV

TL;DR: AI pipeline using Gemini 2.5 Flash and Hunyuan3D-2mini transforms 2D angiograms into 3D geometries for Fontan procedure analysis, enabling rapid virtual flow visualization and CFD preparation in under 15 minutes.


<details>
  <summary>Details</summary>
Motivation: Current assessment of Fontan palliation hemodynamics relies on limited 2D fluoroscopic angiography, which provides insufficient 3D geometric information needed for computational fluid dynamics analysis and surgical planning.

Method: Multi-step AI pipeline using Google's Gemini 2.5 Flash for iterative processing of fluoroscopic angiograms through transformer-based neural architecture, including preprocessing, segmentation, enhancement, artifact removal, and virtual flow visualization, followed by Tencent's Hunyuan3D-2mini for stereolithography file generation.

Result: Pipeline successfully generated geometrically optimized 2D projections from single-view angiograms after 16 processing steps, with final projections accurately preserving complex Fontan geometry. AI-generated virtual flow visualization identified stagnation zones and flow patterns. Complete processing required under 15 minutes with second-level API response times.

Conclusion: This approach demonstrates clinical feasibility for generating CFD-suitable geometries from routine angiographic data, enabling rapid 3D generation and virtual flow visualization, establishing foundation for democratizing advanced geometric and hemodynamic analysis using readily available imaging data.

Abstract: Fontan palliation for univentricular congenital heart disease progresses to
hemodynamic failure with complex flow patterns poorly characterized by
conventional 2D imaging. Current assessment relies on fluoroscopic angiography,
providing limited 3D geometric information essential for computational fluid
dynamics (CFD) analysis and surgical planning.
  A multi-step AI pipeline was developed utilizing Google's Gemini 2.5 Flash
(2.5B parameters) for systematic, iterative processing of fluoroscopic
angiograms through transformer-based neural architecture. The pipeline
encompasses medical image preprocessing, vascular segmentation, contrast
enhancement, artifact removal, and virtual hemodynamic flow visualization
within 2D projections. Final views were processed through Tencent's
Hunyuan3D-2mini (384M parameters) for stereolithography file generation.
  The pipeline successfully generated geometrically optimized 2D projections
from single-view angiograms after 16 processing steps using a custom web
interface. Initial iterations contained hallucinated vascular features
requiring iterative refinement to achieve anatomically faithful
representations. Final projections demonstrated accurate preservation of
complex Fontan geometry with enhanced contrast suitable for 3D conversion.
AI-generated virtual flow visualization identified stagnation zones in central
connections and flow patterns in branch arteries. Complete processing required
under 15 minutes with second-level API response times.
  This approach demonstrates clinical feasibility of generating CFD-suitable
geometries from routine angiographic data, enabling 3D generation and rapid
virtual flow visualization for cursory insights prior to full CFD simulation.
While requiring refinement cycles for accuracy, this establishes foundation for
democratizing advanced geometric and hemodynamic analysis using readily
available imaging data.

</details>


### [144] [Cross-Distribution Diffusion Priors-Driven Iterative Reconstruction for Sparse-View CT](https://arxiv.org/abs/2509.13576)
*Haodong Li,Shuo Han,Haiyang Mao,Yu Shi,Changsheng Fang,Jianjia Zhang,Weiwen Wu,Hengyong Yu*

Main category: eess.IV

TL;DR: CDPIR framework combines cross-distribution diffusion priors with iterative reconstruction to solve out-of-distribution problems in sparse-view CT, achieving state-of-the-art performance with superior detail preservation.


<details>
  <summary>Details</summary>
Motivation: Sparse-view CT reconstruction suffers from artifacts due to view reduction and domain shifts from scanner/protocol/anatomical variations, leading to performance degradation in out-of-distribution scenarios.

Method: Proposes CDPIR framework integrating cross-distribution diffusion priors from Scalable Interpolant Transformer (SiT) with model-based iterative reconstruction. Uses Classifier-Free Guidance across multiple datasets and learns both domain-specific and domain-invariant priors.

Result: Achieves state-of-the-art performance with superior detail preservation in SVCT reconstructions. Significantly outperforms existing approaches, particularly under OOD conditions.

Conclusion: CDPIR demonstrates robustness and potential clinical value in challenging imaging scenarios by effectively handling out-of-distribution problems through cross-distribution diffusion priors and flexible sampling strategies.

Abstract: Sparse-View CT (SVCT) reconstruction enhances temporal resolution and reduces
radiation dose, yet its clinical use is hindered by artifacts due to view
reduction and domain shifts from scanner, protocol, or anatomical variations,
leading to performance degradation in out-of-distribution (OOD) scenarios. In
this work, we propose a Cross-Distribution Diffusion Priors-Driven Iterative
Reconstruction (CDPIR) framework to tackle the OOD problem in SVCT. CDPIR
integrates cross-distribution diffusion priors, derived from a Scalable
Interpolant Transformer (SiT), with model-based iterative reconstruction
methods. Specifically, we train a SiT backbone, an extension of the Diffusion
Transformer (DiT) architecture, to establish a unified stochastic interpolant
framework, leveraging Classifier-Free Guidance (CFG) across multiple datasets.
By randomly dropping the conditioning with a null embedding during training,
the model learns both domain-specific and domain-invariant priors, enhancing
generalizability. During sampling, the globally sensitive transformer-based
diffusion model exploits the cross-distribution prior within the unified
stochastic interpolant framework, enabling flexible and stable control over
multi-distribution-to-noise interpolation paths and decoupled sampling
strategies, thereby improving adaptation to OOD reconstruction. By alternating
between data fidelity and sampling updates, our model achieves state-of-the-art
performance with superior detail preservation in SVCT reconstructions.
Extensive experiments demonstrate that CDPIR significantly outperforms existing
approaches, particularly under OOD conditions, highlighting its robustness and
potential clinical value in challenging imaging scenarios.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [145] [Semantic 3D Reconstructions with SLAM for Central Airway Obstruction](https://arxiv.org/abs/2509.13541)
*Ayberk Acar,Fangjie Li,Hao Li,Lidia Al-Zogbi,Kanyifeechukwu Jane Oguine,Susheela Sharma Stern,Jesse F. d'Almeida,Robert J. Webster III,Ipek Oguz,Jie Ying Wu*

Main category: cs.RO

TL;DR: A novel pipeline for real-time 3D reconstruction of central airways using monocular endoscopy with integrated semantic segmentation to identify obstructive tissues, achieving high accuracy compared to CT scans.


<details>
  <summary>Details</summary>
Motivation: Central airway obstruction is life-threatening and traditional treatments carry high complication risks. Robotic interventions with scene understanding enable safer procedures and potential automation.

Method: Combines DROID-SLAM for real-time 3D geometry reconstruction with a segmentation model to identify obstructive tissues. Segmentation masks guide annotation of obstruction regions in the point cloud.

Result: Achieves high similarity with ground truth CT scans (0.62 mm Chamfer distance). Produces annotated 3D maps highlighting clinically relevant regions in real time with faster reconstruction speeds than previous work.

Conclusion: First integration of semantic segmentation with real-time monocular SLAM for endoscopic CAO. Modular framework can generalize to other anatomies and procedures, representing a step toward autonomous robotic interventions.

Abstract: Central airway obstruction (CAO) is a life-threatening condition with
increasing incidence, caused by tumors in and outside of the airway.
Traditional treatment methods such as bronchoscopy and electrocautery can be
used to remove the tumor completely; however, these methods carry a high risk
of complications. Recent advances allow robotic interventions with lesser risk.
The combination of robot interventions with scene understanding and mapping
also opens up the possibilities for automation. We present a novel pipeline
that enables real-time, semantically informed 3D reconstructions of the central
airway using monocular endoscopic video.
  Our approach combines DROID-SLAM with a segmentation model trained to
identify obstructive tissues. The SLAM module reconstructs the 3D geometry of
the airway in real time, while the segmentation masks guide the annotation of
obstruction regions within the reconstructed point cloud. To validate our
pipeline, we evaluate the reconstruction quality using ex vivo models.
  Qualitative and quantitative results show high similarity between ground
truth CT scans and the 3D reconstructions (0.62 mm Chamfer distance). By
integrating segmentation directly into the SLAM workflow, our system produces
annotated 3D maps that highlight clinically relevant regions in real time.
High-speed capabilities of the pipeline allows quicker reconstructions compared
to previous work, reflecting the surgical scene more accurately.
  To the best of our knowledge, this is the first work to integrate semantic
segmentation with real-time monocular SLAM for endoscopic CAO scenarios. Our
framework is modular and can generalize to other anatomies or procedures with
minimal changes, offering a promising step toward autonomous robotic
interventions.

</details>


### [146] [Object Pose Estimation through Dexterous Touch](https://arxiv.org/abs/2509.13591)
*Amir-Hossein Shahidzadeh,Jiyue Zhu,Kezhou Chen,Sha Yi,Cornelia Fermüller,Yiannis Aloimonos,Xiaolong Wang*

Main category: cs.RO

TL;DR: Active bimanual tactile exploration using reinforcement learning to estimate object pose from partial tactile data without prior geometric knowledge


<details>
  <summary>Details</summary>
Motivation: Robust pose estimation is crucial for robotics manipulation, especially when visual data is limited by lighting, occlusions, or appearance sensitivity. Tactile sensors provide only local contact information, making pose reconstruction challenging from partial data.

Method: Uses sensorimotor exploration with RL to actively control a robot hand to interact with objects. One hand holds the object steady while the other performs active exploration to collect 3D point clouds, which are used to iteratively refine object shape and pose.

Result: The method can actively explore object surfaces to identify critical pose features without requiring prior knowledge of the object's geometry.

Conclusion: Bimanual tactile exploration with RL enables effective pose estimation in challenging visual conditions by leveraging active tactile data collection and iterative refinement.

Abstract: Robust object pose estimation is essential for manipulation and interaction
tasks in robotics, particularly in scenarios where visual data is limited or
sensitive to lighting, occlusions, and appearances. Tactile sensors often offer
limited and local contact information, making it challenging to reconstruct the
pose from partial data. Our approach uses sensorimotor exploration to actively
control a robot hand to interact with the object. We train with Reinforcement
Learning (RL) to explore and collect tactile data. The collected 3D point
clouds are used to iteratively refine the object's shape and pose. In our
setup, one hand holds the object steady while the other performs active
exploration. We show that our method can actively explore an object's surface
to identify critical pose features without prior knowledge of the object's
geometry. Supplementary material and more demonstrations will be provided at
https://amirshahid.github.io/BimanualTactilePose .

</details>


### [147] [InterKey: Cross-modal Intersection Keypoints for Global Localization on OpenStreetMap](https://arxiv.org/abs/2509.13857)
*Nguyen Hoang Khoi Tran,Julie Stephany Berrio,Mao Shan,Stewart Worrall*

Main category: cs.RO

TL;DR: InterKey is a cross-modal framework that uses road intersections as landmarks for vehicle localization, leveraging OpenStreetMap and point cloud data to achieve state-of-the-art accuracy without expensive HD maps.


<details>
  <summary>Details</summary>
Motivation: GNSS degradation in urban environments and the high cost of HD maps limit scalable autonomous vehicle localization. OpenStreetMap provides a free alternative but has coarse abstraction that makes sensor matching challenging.

Method: Constructs compact binary descriptors by jointly encoding road and building imprints from point clouds and OSM. Uses discrepancy mitigation, orientation determination, and area-equalized sampling strategies to bridge modality gaps for robust cross-modal matching.

Result: Achieves state-of-the-art accuracy on KITTI dataset, outperforming recent baselines by a large margin. Framework generalizes to sensors producing dense structural point clouds.

Conclusion: InterKey offers a scalable and cost-effective solution for robust vehicle localization using freely available OSM data and cross-modal matching techniques at road intersections.

Abstract: Reliable global localization is critical for autonomous vehicles, especially
in environments where GNSS is degraded or unavailable, such as urban canyons
and tunnels. Although high-definition (HD) maps provide accurate priors, the
cost of data collection, map construction, and maintenance limits scalability.
OpenStreetMap (OSM) offers a free and globally available alternative, but its
coarse abstraction poses challenges for matching with sensor data. We propose
InterKey, a cross-modal framework that leverages road intersections as
distinctive landmarks for global localization. Our method constructs compact
binary descriptors by jointly encoding road and building imprints from point
clouds and OSM. To bridge modality gaps, we introduce discrepancy mitigation,
orientation determination, and area-equalized sampling strategies, enabling
robust cross-modal matching. Experiments on the KITTI dataset demonstrate that
InterKey achieves state-of-the-art accuracy, outperforming recent baselines by
a large margin. The framework generalizes to sensors that can produce dense
structural point clouds, offering a scalable and cost-effective solution for
robust vehicle localization.

</details>


### [148] [MAP: End-to-End Autonomous Driving with Map-Assisted Planning](https://arxiv.org/abs/2509.13926)
*Huilin Yin,Yiming Kan,Daniel Watzenig*

Main category: cs.RO

TL;DR: MAP is a novel end-to-end autonomous driving framework that explicitly integrates semantic map features with ego status to enhance trajectory planning, achieving significant performance improvements over baselines.


<details>
  <summary>Details</summary>
Motivation: Most existing end-to-end autonomous driving approaches underutilize online mapping modules, leaving their potential to enhance trajectory planning largely untapped.

Method: Proposes MAP framework with three key components: Plan-enhancing Online Mapping module, Ego-status-guided Planning module, and Weight Adapter based on current ego status to integrate segmentation-based map features with ego status.

Result: Achieves 16.6% reduction in L2 displacement error, 56.2% reduction in off-road rate, and 44.5% improvement in overall score compared to UniV2X baseline. Also achieves top ranking in CVPR2025 challenge, outperforming second-best by 39.5%.

Conclusion: The results demonstrate the effectiveness of explicitly leveraging semantic map features in planning and suggest new directions for improving structure design in end-to-end autonomous driving systems.

Abstract: In recent years, end-to-end autonomous driving has attracted increasing
attention for its ability to jointly model perception, prediction, and planning
within a unified framework. However, most existing approaches underutilize the
online mapping module, leaving its potential to enhance trajectory planning
largely untapped. This paper proposes MAP (Map-Assisted Planning), a novel
map-assisted end-to-end trajectory planning framework. MAP explicitly
integrates segmentation-based map features and the current ego status through a
Plan-enhancing Online Mapping module, an Ego-status-guided Planning module, and
a Weight Adapter based on current ego status. Experiments conducted on the
DAIR-V2X-seq-SPD dataset demonstrate that the proposed method achieves a 16.6%
reduction in L2 displacement error, a 56.2% reduction in off-road rate, and a
44.5% improvement in overall score compared to the UniV2X baseline, even
without post-processing. Furthermore, it achieves top ranking in Track 2 of the
End-to-End Autonomous Driving through V2X Cooperation Challenge of MEIS
Workshop @CVPR2025, outperforming the second-best model by 39.5% in terms of
overall score. These results highlight the effectiveness of explicitly
leveraging semantic map features in planning and suggest new directions for
improving structure design in end-to-end autonomous driving systems. Our code
is available at https://gitee.com/kymkym/map.git

</details>


### [149] [MetricNet: Recovering Metric Scale in Generative Navigation Policies](https://arxiv.org/abs/2509.13965)
*Abhijeet Nayak,Débora N. P. Oliveira,Samiran Gode,Cordelia Schmid,Wolfram Burgard*

Main category: cs.RO

TL;DR: MetricNet is an add-on module that predicts metric distances between waypoints to ground generative navigation policies in real-world coordinates, improving navigation safety and performance by scaling abstract trajectories.


<details>
  <summary>Details</summary>
Motivation: Generative navigation policies have two structural problems: they operate in abstract unscaled space without metric grounding, and they use short-sighted control strategies that discard complete paths, leading to unsafe movements toward obstacles.

Method: Propose MetricNet to predict metric distance between waypoints, grounding policy outputs in real-world coordinates. Also develop MetricNav which integrates MetricNet into navigation policy to guide robots away from obstacles while moving toward goals.

Result: Evaluation in simulation with new benchmarking framework shows MetricNet-scaled waypoints significantly improve both navigation and exploration performance. Real-world experiments further validate the approach.

Conclusion: MetricNet effectively addresses the scaling and safety issues in generative navigation by providing metric grounding, leading to improved navigation performance and obstacle avoidance in both simulation and real-world scenarios.

Abstract: Generative navigation policies have made rapid progress in improving
end-to-end learned navigation. Despite their promising results, this paradigm
has two structural problems. First, the sampled trajectories exist in an
abstract, unscaled space without metric grounding. Second, the control strategy
discards the full path, instead moving directly towards a single waypoint. This
leads to short-sighted and unsafe actions, moving the robot towards obstacles
that a complete and correctly scaled path would circumvent. To address these
issues, we propose MetricNet, an effective add-on for generative navigation
that predicts the metric distance between waypoints, grounding policy outputs
in real-world coordinates. We evaluate our method in simulation with a new
benchmarking framework and show that executing MetricNet-scaled waypoints
significantly improves both navigation and exploration performance. Beyond
simulation, we further validate our approach in real-world experiments.
Finally, we propose MetricNav, which integrates MetricNet into a navigation
policy to guide the robot away from obstacles while still moving towards the
goal.

</details>


### [150] [MCGS-SLAM: A Multi-Camera SLAM Framework Using Gaussian Splatting for High-Fidelity Mapping](https://arxiv.org/abs/2509.14191)
*Zhihao Cao,Hanyu Wu,Li Wa Tang,Zizhou Luo,Zihan Zhu,Wei Zhang,Marc Pollefeys,Martin R. Oswald*

Main category: cs.RO

TL;DR: MCGS-SLAM is the first purely RGB-based multi-camera SLAM system using 3D Gaussian Splatting, enabling real-time dense mapping with improved robustness and geometric coverage compared to monocular approaches.


<details>
  <summary>Details</summary>
Motivation: Recent dense SLAM progress has focused on monocular setups, sacrificing robustness and geometric coverage. Multi-camera systems can provide wider field of view and better coverage for autonomous applications.

Method: Uses multi-camera bundle adjustment (MCBA) with dense photometric and geometric residuals, plus scale consistency module with low-rank priors for metric alignment. Fuses multiple RGB viewpoints into unified 3D Gaussian map.

Result: Outperforms monocular baselines on synthetic and real datasets, achieving accurate trajectories and photorealistic reconstructions. Enables reconstruction of side-view regions missed by monocular setups.

Conclusion: Multi-camera Gaussian Splatting SLAM shows promise for high-fidelity mapping in robotics and autonomous driving, providing critical side-view coverage for safe operation.

Abstract: Recent progress in dense SLAM has primarily targeted monocular setups, often
at the expense of robustness and geometric coverage. We present MCGS-SLAM, the
first purely RGB-based multi-camera SLAM system built on 3D Gaussian Splatting
(3DGS). Unlike prior methods relying on sparse maps or inertial data, MCGS-SLAM
fuses dense RGB inputs from multiple viewpoints into a unified, continuously
optimized Gaussian map. A multi-camera bundle adjustment (MCBA) jointly refines
poses and depths via dense photometric and geometric residuals, while a scale
consistency module enforces metric alignment across views using low-rank
priors. The system supports RGB input and maintains real-time performance at
large scale. Experiments on synthetic and real-world datasets show that
MCGS-SLAM consistently yields accurate trajectories and photorealistic
reconstructions, usually outperforming monocular baselines. Notably, the wide
field of view from multi-camera input enables reconstruction of side-view
regions that monocular setups miss, critical for safe autonomous operation.
These results highlight the promise of multi-camera Gaussian Splatting SLAM for
high-fidelity mapping in robotics and autonomous driving.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [151] [When Avatars Have Personality: Effects on Engagement and Communication in Immersive Medical Training](https://arxiv.org/abs/2509.14132)
*Julia S. Dollis,Iago A. Brito,Fernanda B. Färber,Pedro S. F. B. Ribeiro,Rafael T. Sousa,Arlindo R. Galvão Filho*

Main category: cs.HC

TL;DR: Integration of LLMs into VR creates psychologically plausible virtual patients for medical communication training, validated by physicians as effective and rewarding.


<details>
  <summary>Details</summary>
Motivation: VR lacks psychologically realistic virtual humans for training complex interpersonal skills, particularly in high-stakes medical education where communication is crucial.

Method: Modular architecture integrating LLMs into immersive VR to create medically coherent virtual patients with distinct personalities, evaluated through mixed-method within-subjects study with licensed physicians.

Result: Feasible approach perceived by physicians as highly rewarding and effective training enhancement, revealing design principles like realism-verbosity paradox and need for authentic challenges.

Conclusion: Provides validated framework and key insights for developing next-generation socially intelligent VR training environments in medical education.

Abstract: While virtual reality (VR) excels at simulating physical environments, its
effectiveness for training complex interpersonal skills is limited by a lack of
psychologically plausible virtual humans. This is a critical gap in high-stakes
domains like medical education, where communication is a core competency. This
paper introduces a framework that integrates large language models (LLMs) into
immersive VR to create medically coherent virtual patients with distinct,
consistent personalities, built on a modular architecture that decouples
personality from clinical data. We evaluated our system in a mixed-method,
within-subjects study with licensed physicians who engaged in simulated
consultations. Results demonstrate that the approach is not only feasible but
is also perceived by physicians as a highly rewarding and effective training
enhancement. Furthermore, our analysis uncovers critical design principles,
including a ``realism-verbosity paradox" where less communicative agents can
seem more artificial, and the need for challenges to be perceived as authentic
to be instructive. This work provides a validated framework and key insights
for developing the next generation of socially intelligent VR training
environments.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [152] [Enhancing Time Awareness in Generative Recommendation](https://arxiv.org/abs/2509.13957)
*Sunkyung Lee,Seongmin Park,Jonghyo Kim,Mincheol Yoon,Jongwuk Lee*

Main category: cs.IR

TL;DR: GRUT is a generative recommender that incorporates temporal awareness through time-aware prompting and trend-aware inference to better capture evolving user preferences.


<details>
  <summary>Details</summary>
Motivation: Existing generative recommendation models focus on sequential item order but neglect temporal dynamics that reveal evolving user preferences over time.

Method: Proposes GRUT with Time-aware Prompting (user-level temporal context and item-level transition context) and Trend-aware Inference that incorporates item trend information with generation likelihood.

Result: Outperforms state-of-the-art models with gains up to 15.4% in Recall@5 and 14.3% in NDCG@5 across four benchmark datasets.

Conclusion: GRUT effectively captures temporal dynamics in user preferences through time-aware mechanisms, demonstrating significant performance improvements in generative recommendation.

Abstract: Generative recommendation has emerged as a promising paradigm that formulates
the recommendations into a text-to-text generation task, harnessing the vast
knowledge of large language models. However, existing studies focus on
considering the sequential order of items and neglect to handle the temporal
dynamics across items, which can imply evolving user preferences. To address
this limitation, we propose a novel model, Generative Recommender Using Time
awareness (GRUT), effectively capturing hidden user preferences via various
temporal signals. We first introduce Time-aware Prompting, consisting of two
key contexts. The user-level temporal context models personalized temporal
patterns across timestamps and time intervals, while the item-level transition
context provides transition patterns across users. We also devise Trend-aware
Inference, a training-free method that enhances rankings by incorporating trend
information about items with generation likelihood. Extensive experiments
demonstrate that GRUT outperforms state-of-the-art models, with gains of up to
15.4% and 14.3% in Recall@5 and NDCG@5 across four benchmark datasets. The
source code is available at https://github.com/skleee/GRUT.

</details>


### [153] [GEM-Bench: A Benchmark for Ad-Injected Response Generation within Generative Engine Marketing](https://arxiv.org/abs/2509.14221)
*Silan Hu,Shiqi Zhang,Yimin Shi,Xiaokui Xiao*

Main category: cs.IR

TL;DR: GEM-Bench is the first comprehensive benchmark for ad-injected response generation in Generative Engine Marketing, featuring curated datasets, multi-dimensional metrics, and baseline solutions that reveal trade-offs between engagement and user satisfaction.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks are not specifically designed for evaluating ad-injected response generation in Generative Engine Marketing (GEM), limiting research progress in this emerging monetization ecosystem for LLM-based chatbots.

Method: Proposed GEM-Bench includes three curated datasets covering chatbot and search scenarios, a metric ontology capturing user satisfaction and engagement dimensions, and baseline solutions implemented within an extensible multi-agent framework.

Result: Preliminary results show prompt-based methods achieve reasonable engagement (click-through rates) but reduce user satisfaction, while approaches using pre-generated ad-free responses mitigate satisfaction issues but introduce additional overhead.

Conclusion: The findings highlight the need for future research on designing more effective and efficient solutions for generating ad-injected responses in GEM, balancing engagement and user satisfaction.

Abstract: Generative Engine Marketing (GEM) is an emerging ecosystem for monetizing
generative engines, such as LLM-based chatbots, by seamlessly integrating
relevant advertisements into their responses. At the core of GEM lies the
generation and evaluation of ad-injected responses. However, existing
benchmarks are not specifically designed for this purpose, which limits future
research. To address this gap, we propose GEM-Bench, the first comprehensive
benchmark for ad-injected response generation in GEM. GEM-Bench includes three
curated datasets covering both chatbot and search scenarios, a metric ontology
that captures multiple dimensions of user satisfaction and engagement, and
several baseline solutions implemented within an extensible multi-agent
framework. Our preliminary results indicate that, while simple prompt-based
methods achieve reasonable engagement such as click-through rate, they often
reduce user satisfaction. In contrast, approaches that insert ads based on
pre-generated ad-free responses help mitigate this issue but introduce
additional overhead. These findings highlight the need for future research on
designing more effective and efficient solutions for generating ad-injected
responses in GEM.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [154] [An AI-Powered Framework for Analyzing Collective Idea Evolution in Deliberative Assemblies](https://arxiv.org/abs/2509.12577)
*Elinor Poole-Dayan,Deb Roy,Jad Kabbara*

Main category: cs.CY

TL;DR: This paper develops LLM-based methods to analyze how ideas evolve into policy recommendations in deliberative assemblies and how deliberation shapes delegate perspectives and voting dynamics.


<details>
  <summary>Details</summary>
Motivation: Address the lack of empirical research on how specific ideas are prioritized or discarded during deliberation to form policy recommendations, particularly in the context of increasing societal fragmentation and political polarization.

Method: Developed LLM-based methodologies to analyze transcripts from tech-enhanced in-person deliberative assemblies, including identifying and visualizing expressed suggestions and reconstructing each delegate's evolving perspective.

Result: The framework provides novel empirical insights into deliberative processes and demonstrates how LLMs can reveal high-resolution dynamics that are invisible in traditional assembly outputs.

Conclusion: LLM-based analysis offers a powerful approach for tracing idea evolution and perspective changes in deliberative assemblies, contributing to better understanding of democratic deliberation processes.

Abstract: In an era of increasing societal fragmentation, political polarization, and
erosion of public trust in institutions, representative deliberative assemblies
are emerging as a promising democratic forum for developing effective policy
outcomes on complex global issues. Despite theoretical attention, there remains
limited empirical work that systematically traces how specific ideas evolve,
are prioritized, or are discarded during deliberation to form policy
recommendations. Addressing these gaps, this work poses two central questions:
(1) How might we trace the evolution and distillation of ideas into concrete
recommendations within deliberative assemblies? (2) How does the deliberative
process shape delegate perspectives and influence voting dynamics over the
course of the assembly? To address these questions, we develop LLM-based
methodologies for empirically analyzing transcripts from a tech-enhanced
in-person deliberative assembly. The framework identifies and visualizes the
space of expressed suggestions. We also empirically reconstruct each delegate's
evolving perspective throughout the assembly. Our methods contribute novel
empirical insights into deliberative processes and demonstrate how LLMs can
surface high-resolution dynamics otherwise invisible in traditional assembly
outputs.

</details>


### [155] [Accuracy Paradox in Large Language Models: Regulating Hallucination Risks in Generative AI](https://arxiv.org/abs/2509.13345)
*Zihao Li,Weiwei Yi,Jiahong Chen*

Main category: cs.CY

TL;DR: The paper argues that overreliance on accuracy as the main benchmark for addressing LLM hallucinations creates a "accuracy paradox" that misdiagnoses the problem and has counterproductive effects across individual and societal dimensions.


<details>
  <summary>Details</summary>
Motivation: As LLMs become integral to decision-making, their epistemic and societal risks demand scrutiny. Current approaches focus on accuracy as the principal solution to hallucinations, but this article contends this approach is fundamentally flawed and counterproductive.

Method: The article develops a taxonomy of hallucination types and analyzes the accuracy paradox through three dimensions: outputs, individuals, and society. It examines interdisciplinary literature and evaluates current regulations (EU AI Act, GDPR, DSA) to show their structural limitations.

Result: Accuracy functions as a superficial proxy that incentivizes rhetorical fluency over true trustworthiness, fails to detect non-factual harms like manipulation and consensus illusions, and obscures wider societal consequences including privacy violations, equity harms, and epistemic convergence that reduces pluralism.

Conclusion: Current regulations are structurally inadequate to address the epistemic, relational, and systemic harms of hallucinations. The article calls for a fundamental shift toward pluralistic, context-aware, and manipulation-resilient approaches to AI trustworthy governance.

Abstract: As Large Language Models (LLMs) permeate everyday decision-making, their
epistemic and societal risks demand urgent scrutiny. Hallucinations, the
generation of fabricated, misleading, oversimplified or untrustworthy outputs,
has emerged as imperative challenges. While regulatory, academic, and technical
discourse position accuracy as the principal benchmark for mitigating such
harms, this article contends that overreliance on accuracy misdiagnoses the
problem and has counterproductive effect: the accuracy paradox. Drawing on
interdisciplinary literatures, this article develops a taxonomy of
hallucination types and shows the paradox along three intertwining dimensions:
outputs, individuals and society. First, accuracy functions as a superficial
proxy for reliability, incentivising the optimisation of rhetorical fluency and
surface-level correctness over epistemic trustworthiness. This encourages
passive user trust in outputs that appear accurate but epistemically untenable.
Second, accuracy as a singular metric fails to detect harms that are not
factually false but are nonetheless misleading, value-laden, or socially
distorting, including consensus illusions, sycophantic alignment, and subtle
manipulation. Third, regulatory overemphasis on accuracy obscures the wider
societal consequences of hallucination, including social sorting, privacy
violations, equity harms, epistemic convergence that marginalises dissent,
reduces pluralism, and causes social deskilling. By examining the EU AI Act,
GDPR, and DSA, the article argues that current regulations are not yet
structurally equipped to address these epistemic, relational, and systemic
harms and exacerbated by the overreliance on accuracy. By exposing such
conceptual and practical challenges, this article calls for a fundamental shift
towards pluralistic, context-aware, and manipulation-resilient approaches to AI
trustworthy governance.

</details>


### [156] [CogniAlign: Survivability-Grounded Multi-Agent Moral Reasoning for Safe and Transparent AI](https://arxiv.org/abs/2509.13356)
*Hasin Jawad Ali,Ilhamul Azam,Ajwad Abrar,Md. Kamrul Hasan,Hasan Mahmud*

Main category: cs.CY

TL;DR: CogniAlign is a multi-agent deliberation framework that outperforms GPT-4o in moral reasoning by using interdisciplinary scientist agents to provide transparent, empirically grounded judgments based on survivability principles.


<details>
  <summary>Details</summary>
Motivation: The abstract and conflicting nature of moral principles combined with the opacity of existing AI alignment approaches creates challenges for aligning AI with human values.

Method: Multi-agent deliberation framework with discipline-specific scientist agents (neuroscience, psychology, sociology, evolutionary biology) that provide arguments and rebuttals, synthesized by an arbiter into transparent judgments based on survivability principles.

Result: CogniAlign consistently outperformed GPT-4o across 60+ moral questions, with average gains of 16.2 points in analytic quality, 14.3 points in breadth, and 28.4 points in depth. In the Heinz dilemma, it scored 89.2 vs GPT-4o's 69.2.

Conclusion: CogniAlign demonstrates the potential of interdisciplinary deliberation as a scalable pathway for safe and transparent AI alignment by reducing black-box reasoning and avoiding deceptive alignment.

Abstract: The challenge of aligning artificial intelligence (AI) with human values
persists due to the abstract and often conflicting nature of moral principles
and the opacity of existing approaches. This paper introduces CogniAlign, a
multi-agent deliberation framework based on naturalistic moral realism, that
grounds moral reasoning in survivability, defined across individual and
collective dimensions, and operationalizes it through structured deliberations
among discipline-specific scientist agents. Each agent, representing
neuroscience, psychology, sociology, and evolutionary biology, provides
arguments and rebuttals that are synthesized by an arbiter into transparent and
empirically anchored judgments. We evaluate CogniAlign on classic and novel
moral questions and compare its outputs against GPT-4o using a five-part
ethical audit framework. Results show that CogniAlign consistently outperforms
the baseline across more than sixty moral questions, with average performance
gains of 16.2 points in analytic quality, 14.3 points in breadth, and 28.4
points in depth of explanation. In the Heinz dilemma, for example, CogniAlign
achieved an overall score of 89.2 compared to GPT-4o's 69.2, demonstrating a
decisive advantage in handling moral reasoning. By reducing black-box reasoning
and avoiding deceptive alignment, CogniAlign highlights the potential of
interdisciplinary deliberation as a scalable pathway for safe and transparent
AI alignment.

</details>

<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 44]
- [cs.CV](#cs.CV) [Total: 94]
- [eess.IV](#eess.IV) [Total: 9]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.LG](#cs.LG) [Total: 7]
- [cs.SD](#cs.SD) [Total: 2]
- [cs.RO](#cs.RO) [Total: 4]
- [math.NA](#math.NA) [Total: 1]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.IR](#cs.IR) [Total: 3]
- [cs.AI](#cs.AI) [Total: 2]
- [eess.AS](#eess.AS) [Total: 2]
- [cs.SE](#cs.SE) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [A2HCoder: An LLM-Driven Coding Agent for Hierarchical Algorithm-to-HDL Translation](https://arxiv.org/abs/2508.10904)
*Jie Lei,Ruofan Jia,J. Andrew Zhang,Hao Zhang*

Main category: cs.CL

TL;DR: A2HCoder is a hierarchical algorithm-to-HDL coding agent using LLMs to bridge the gap between algorithm design and hardware implementation, improving robustness and reducing hallucinations.


<details>
  <summary>Details</summary>
Motivation: The gap between algorithm design and hardware implementation requires domain expertise and manual effort due to mismatches in programming languages and HDLs.

Method: A2HCoder uses a hierarchical framework: horizontally decomposing algorithms into modular blocks and vertically performing step-by-step translation with external toolchains for debugging and synthesis.

Result: Validated in 5G wireless communication, A2HCoder shows practicality, reliability, and efficiency in deployment.

Conclusion: A2HCoder effectively addresses the algorithm-to-hardware translation challenge, enhancing agility and correctness.

Abstract: In wireless communication systems, stringent requirements such as ultra-low
latency and power consumption have significantly increased the demand for
efficient algorithm-to-hardware deployment. However, a persistent and
substantial gap remains between algorithm design and hardware implementation.
Bridging this gap traditionally requires extensive domain expertise and
time-consuming manual development, due to fundamental mismatches between
high-level programming languages like MATLAB and hardware description languages
(HDLs) such as Verilog-in terms of memory access patterns, data processing
manners, and datatype representations. To address this challenge, we propose
A2HCoder: a Hierarchical Algorithm-to-HDL Coding Agent, powered by large
language models (LLMs), designed to enable agile and reliable
algorithm-to-hardware translation. A2HCoder introduces a hierarchical framework
that enhances both robustness and interpretability while suppressing common
hallucination issues in LLM-generated code. In the horizontal dimension,
A2HCoder decomposes complex algorithms into modular functional blocks,
simplifying code generation and improving consistency. In the vertical
dimension, instead of relying on end-to-end generation, A2HCoder performs
step-by-step, fine-grained translation, leveraging external toolchains such as
MATLAB and Vitis HLS for debugging and circuit-level synthesis. This structured
process significantly mitigates hallucinations and ensures hardware-level
correctness. We validate A2HCoder through a real-world deployment case in the
5G wireless communication domain, demonstrating its practicality, reliability,
and deployment efficiency.

</details>


### [2] [PersonaTwin: A Multi-Tier Prompt Conditioning Framework for Generating and Evaluating Personalized Digital Twins](https://arxiv.org/abs/2508.10906)
*Sihan Chen,John P. Lalor,Yi Yang,Ahmed Abbasi*

Main category: cs.CL

TL;DR: PersonaTwin, a multi-tier prompt conditioning framework, enhances LLMs by integrating demographic, behavioral, and psychometric data to create adaptive digital twins, outperforming standard LLMs in fidelity and fairness.


<details>
  <summary>Details</summary>
Motivation: LLMs often fail to capture nuanced user behaviors, prompting the need for a framework like PersonaTwin to improve user modeling.

Method: PersonaTwin integrates diverse user data and benchmarks against standard LLMs using text similarity and demographic parity metrics.

Result: The framework achieves simulation fidelity comparable to oracle settings and matches individual-trained models in prediction and fairness.

Conclusion: PersonaTwin demonstrates the potential of LLM-based digital twins for realistic, nuanced user simulations and personalized modeling.

Abstract: While large language models (LLMs) afford new possibilities for user modeling
and approximation of human behaviors, they often fail to capture the
multidimensional nuances of individual users. In this work, we introduce
PersonaTwin, a multi-tier prompt conditioning framework that builds adaptive
digital twins by integrating demographic, behavioral, and psychometric data.
Using a comprehensive data set in the healthcare context of more than 8,500
individuals, we systematically benchmark PersonaTwin against standard LLM
outputs, and our rigorous evaluation unites state-of-the-art text similarity
metrics with dedicated demographic parity assessments, ensuring that generated
responses remain accurate and unbiased. Experimental results show that our
framework produces simulation fidelity on par with oracle settings. Moreover,
downstream models trained on persona-twins approximate models trained on
individuals in terms of prediction and fairness metrics across both
GPT-4o-based and Llama-based models. Together, these findings underscore the
potential for LLM digital twin-based approaches in producing realistic and
emotionally nuanced user simulations, offering a powerful tool for personalized
digital user modeling and behavior analysis.

</details>


### [3] [gpt-oss-120b & gpt-oss-20b Model Card](https://arxiv.org/abs/2508.10925)
*OpenAI,:,Sandhini Agarwal,Lama Ahmad,Jason Ai,Sam Altman,Andy Applebaum,Edwin Arbus,Rahul K. Arora,Yu Bai,Bowen Baker,Haiming Bao,Boaz Barak,Ally Bennett,Tyler Bertao,Nivedita Brett,Eugene Brevdo,Greg Brockman,Sebastien Bubeck,Che Chang,Kai Chen,Mark Chen,Enoch Cheung,Aidan Clark,Dan Cook,Marat Dukhan,Casey Dvorak,Kevin Fives,Vlad Fomenko,Timur Garipov,Kristian Georgiev,Mia Glaese,Tarun Gogineni,Adam Goucher,Lukas Gross,Katia Gil Guzman,John Hallman,Jackie Hehir,Johannes Heidecke,Alec Helyar,Haitang Hu,Romain Huet,Jacob Huh,Saachi Jain,Zach Johnson,Chris Koch,Irina Kofman,Dominik Kundel,Jason Kwon,Volodymyr Kyrylov,Elaine Ya Le,Guillaume Leclerc,James Park Lennon,Scott Lessans,Mario Lezcano-Casado,Yuanzhi Li,Zhuohan Li,Ji Lin,Jordan Liss,Lily,Liu,Jiancheng Liu,Kevin Lu,Chris Lu,Zoran Martinovic,Lindsay McCallum,Josh McGrath,Scott McKinney,Aidan McLaughlin,Song Mei,Steve Mostovoy,Tong Mu,Gideon Myles,Alexander Neitz,Alex Nichol,Jakub Pachocki,Alex Paino,Dana Palmie,Ashley Pantuliano,Giambattista Parascandolo,Jongsoo Park,Leher Pathak,Carolina Paz,Ludovic Peran,Dmitry Pimenov,Michelle Pokrass,Elizabeth Proehl,Huida Qiu,Gaby Raila,Filippo Raso,Hongyu Ren,Kimmy Richardson,David Robinson,Bob Rotsted,Hadi Salman,Suvansh Sanjeev,Max Schwarzer,D. Sculley,Harshit Sikchi,Kendal Simon,Karan Singhal,Yang Song,Dane Stuckey,Zhiqing Sun,Philippe Tillet,Sam Toizer,Foivos Tsimpourlas,Nikhil Vyas,Eric Wallace,Xin Wang,Miles Wang,Olivia Watkins,Kevin Weil,Amy Wendling,Kevin Whinnery,Cedric Whitney,Hannah Wong,Lin Yang,Yu Yang,Michihiro Yasunaga,Kristen Ying,Wojciech Zaremba,Wenting Zhan,Cyril Zhang,Brian Zhang,Eddie Zhang,Shengjia Zhao*

Main category: cs.CL

TL;DR: GPT-OSS-120B and GPT-OSS-20B are open-weight reasoning models with high accuracy and cost efficiency, featuring agentic capabilities and strong benchmark performance.


<details>
  <summary>Details</summary>
Motivation: To advance open-weight models with superior reasoning, agentic capabilities, and broad usability.

Method: Uses mixture-of-expert transformers, trained via large-scale distillation and reinforcement learning.

Result: Achieves strong performance in mathematics, coding, and safety benchmarks.

Conclusion: Models, tools, and weights are released under Apache 2.0 for widespread use and research.

Abstract: We present gpt-oss-120b and gpt-oss-20b, two open-weight reasoning models
that push the frontier of accuracy and inference cost. The models use an
efficient mixture-of-expert transformer architecture and are trained using
large-scale distillation and reinforcement learning. We optimize the models to
have strong agentic capabilities (deep research browsing, python tool use, and
support for developer-provided functions), all while using a rendered chat
format that enables clear instruction following and role delineation. Both
models achieve strong results on benchmarks ranging from mathematics, coding,
and safety. We release the model weights, inference implementations, tool
environments, and tokenizers under an Apache 2.0 license to enable broad use
and further research.

</details>


### [4] [Modeling and Detecting Company Risks from News: A Case Study in Bloomberg News](https://arxiv.org/abs/2508.10927)
*Jiaxin Pei,Soumya Vadlamannati,Liang-Kang Huang,Daniel Preotiuc-Pietro,Xinyu Hua*

Main category: cs.CL

TL;DR: A computational framework is developed to extract company risk factors from news articles, benchmarking ML models and showing fine-tuned models outperform LLMs like LLaMA-2.


<details>
  <summary>Details</summary>
Motivation: To aid investors and financial markets by automating the identification of company risk factors from news.

Method: Proposed a schema with seven risk aspects, annotated 744 news articles, and benchmarked ML models, including zero-shot/few-shot LLMs and fine-tuned models.

Result: Fine-tuned models outperformed LLMs; analysis of 277K Bloomberg articles provided insights into company and industry risks.

Conclusion: Automated risk factor extraction from news is feasible and valuable for financial analysis, with fine-tuned models being most effective.

Abstract: Identifying risks associated with a company is important to investors and the
well-being of the overall financial market. In this study, we build a
computational framework to automatically extract company risk factors from news
articles. Our newly proposed schema comprises seven distinct aspects, such as
supply chain, regulations, and competitions. We sample and annotate 744 news
articles and benchmark various machine learning models. While large language
models have achieved huge progress in various types of NLP tasks, our
experiment shows that zero-shot and few-shot prompting state-of-the-art LLMs
(e.g. LLaMA-2) can only achieve moderate to low performances in identifying
risk factors. And fine-tuned pre-trained language models are performing better
on most of the risk factors. Using this model, we analyze over 277K Bloomberg
news articles and demonstrate that identifying risk factors from news could
provide extensive insight into the operations of companies and industries.

</details>


### [5] [Rule2Text: A Framework for Generating and Evaluating Natural Language Explanations of Knowledge Graph Rules](https://arxiv.org/abs/2508.10971)
*Nasim Shirvani-Mahdavi,Chengkai Li*

Main category: cs.CL

TL;DR: Rule2Text uses LLMs to generate natural language explanations for complex KG rules, improving interpretability. It evaluates multiple LLMs, fine-tunes Zephyr, and includes a type inference module.


<details>
  <summary>Details</summary>
Motivation: Logical rules in KGs are hard to interpret due to complexity and labeling conventions. Rule2Text aims to enhance KG accessibility by generating human-readable explanations.

Method: The framework evaluates LLMs (e.g., Gemini 2.0 Flash) with various prompting strategies, fine-tunes Zephyr using human feedback, and adds a type inference module for KGs without explicit types.

Result: Fine-tuned Zephyr shows significant improvement in explanation quality, especially for domain-specific datasets. An LLM-as-a-judge framework aligns well with human evaluations.

Conclusion: Rule2Text successfully improves KG rule interpretability, with fine-tuning and type inference enhancing performance. The framework is scalable and publicly available.

Abstract: Knowledge graphs (KGs) can be enhanced through rule mining; however, the
resulting logical rules are often difficult for humans to interpret due to
their inherent complexity and the idiosyncratic labeling conventions of
individual KGs. This work presents Rule2Text, a comprehensive framework that
leverages large language models (LLMs) to generate natural language
explanations for mined logical rules, thereby improving KG accessibility and
usability. We conduct extensive experiments using multiple datasets, including
Freebase variants (FB-CVT-REV, FB+CVT-REV, and FB15k-237) as well as the
ogbl-biokg dataset, with rules mined using AMIE 3.5.1. We systematically
evaluate several LLMs across a comprehensive range of prompting strategies,
including zero-shot, few-shot, variable type incorporation, and
Chain-of-Thought reasoning. To systematically assess models' performance, we
conduct a human evaluation of generated explanations on correctness and
clarity. To address evaluation scalability, we develop and validate an
LLM-as-a-judge framework that demonstrates strong agreement with human
evaluators. Leveraging the best-performing model (Gemini 2.0 Flash), LLM judge,
and human-in-the-loop feedback, we construct high-quality ground truth
datasets, which we use to fine-tune the open-source Zephyr model. Our results
demonstrate significant improvements in explanation quality after fine-tuning,
with particularly strong gains in the domain-specific dataset. Additionally, we
integrate a type inference module to support KGs lacking explicit type
information. All code and data are publicly available at
https://github.com/idirlab/KGRule2NL.

</details>


### [6] [Improving Text Style Transfer using Masked Diffusion Language Models with Inference-time Scaling](https://arxiv.org/abs/2508.10995)
*Tejomay Kishor Padole,Suyash P Awate,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: Masked diffusion language models (MDMs) are scalable and easy to train, emerging as state-of-the-art non-autoregressive generators for discrete data. This work introduces a verifier-based inference-time scaling method to improve generation quality, demonstrating MDMs' superiority over autoregressive models in text-style transfer tasks.


<details>
  <summary>Details</summary>
Motivation: To enhance the generation quality of MDMs by leveraging inference-time scaling, addressing limitations of existing diffusion model paradigms for discrete data.

Method: Proposes a verifier-based inference-time scaling method for MDMs, utilizing off-the-shelf pre-trained embedding models to guide generation during denoising.

Result: MDMs outperform autoregressive models in text-style transfer tasks, and the verifier setup significantly improves generation quality even with classifier-free guidance.

Conclusion: MDMs, combined with verifier-based scaling, offer a superior alternative to autoregressive models for discrete data generation, with practical benefits in tasks like text-style transfer.

Abstract: Masked diffusion language models (MDMs) have recently gained traction as a
viable generative framework for natural language. This can be attributed to its
scalability and ease of training compared to other diffusion model paradigms
for discrete data, establishing itself as the state-of-the-art
non-autoregressive generator for discrete data. Diffusion models, in general,
have shown excellent ability to improve the generation quality by leveraging
inference-time scaling either by increasing the number of denoising steps or by
using external verifiers on top of the outputs of each step to guide the
generation. In this work, we propose a verifier-based inference-time scaling
method that aids in finding a better candidate generation during the denoising
process of the MDM. Our experiments demonstrate the application of MDMs for
standard text-style transfer tasks and establish MDMs as a better alternative
to autoregressive language models. Additionally, we show that a simple
soft-value-based verifier setup for MDMs using off-the-shelf pre-trained
embedding models leads to significant gains in generation quality even when
used on top of typical classifier-free guidance setups in the existing
literature.

</details>


### [7] [SproutBench: A Benchmark for Safe and Ethical Large Language Models for Youth](https://arxiv.org/abs/2508.11009)
*Wenpeng Xing,Lanyi Wei,Haixiao Hu,Rongchang Li,Mohan Li,Changting Lin,Meng Han*

Main category: cs.CL

TL;DR: The paper introduces SproutBench, a new evaluation suite for assessing LLM safety risks for minors, revealing significant vulnerabilities and providing guidelines for child-centric AI design.


<details>
  <summary>Details</summary>
Motivation: Existing AI safety frameworks for LLMs are inadequate for minors, lacking coverage of age-specific cognitive, emotional, and social risks.

Method: Developed SproutBench with 1,283 adversarial prompts to evaluate 47 LLMs for risks like emotional dependency and privacy violations.

Result: Found substantial safety vulnerabilities, correlations between safety dimensions, and an inverse relationship between interactivity and age appropriateness.

Conclusion: The study offers practical guidelines for improving child-centric AI safety in LLM design and deployment.

Abstract: The rapid proliferation of large language models (LLMs) in applications
targeting children and adolescents necessitates a fundamental reassessment of
prevailing AI safety frameworks, which are largely tailored to adult users and
neglect the distinct developmental vulnerabilities of minors. This paper
highlights key deficiencies in existing LLM safety benchmarks, including their
inadequate coverage of age-specific cognitive, emotional, and social risks
spanning early childhood (ages 0--6), middle childhood (7--12), and adolescence
(13--18). To bridge these gaps, we introduce SproutBench, an innovative
evaluation suite comprising 1,283 developmentally grounded adversarial prompts
designed to probe risks such as emotional dependency, privacy violations, and
imitation of hazardous behaviors. Through rigorous empirical evaluation of 47
diverse LLMs, we uncover substantial safety vulnerabilities, corroborated by
robust inter-dimensional correlations (e.g., between Safety and Risk
Prevention) and a notable inverse relationship between Interactivity and Age
Appropriateness. These insights yield practical guidelines for advancing
child-centric AI design and deployment.

</details>


### [8] [Beyond the Rosetta Stone: Unification Forces in Generalization Dynamics](https://arxiv.org/abs/2508.11017)
*Carter Blum,Katja Filipova,Ann Yuan,Asma Ghandeharioun,Julian Zimmert,Fred Zhang,Jessica Hoffmann,Tal Linzen,Martin Wattenberg,Lucas Dixon,Mor Geva*

Main category: cs.CL

TL;DR: The paper investigates cross-lingual knowledge transfer issues in LLMs, using small Transformer models on synthetic data to study causes and solutions.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with cross-lingual fact transfer, often hallucinating when facts are expressed in different languages during training.

Method: Train small Transformers on synthetic multilingual datasets, analyze learning phases, and manipulate data distribution/tokenization.

Result: Unification of fact representations across languages is key for transfer, influenced by mutual information and language extraction ease.

Conclusion: Controlled settings reveal pre-training dynamics, offering new ways to enhance cross-lingual transfer in LLMs.

Abstract: Large language models (LLMs) struggle with cross-lingual knowledge transfer:
they hallucinate when asked in one language about facts expressed in a
different language during training. This work introduces a controlled setting
to study the causes and dynamics of this phenomenon by training small
Transformer models from scratch on synthetic multilingual datasets. We identify
a learning phase wherein a model develops either separate or unified
representations of the same facts across languages, and show that unification
is essential for cross-lingual transfer. We also show that the degree of
unification depends on mutual information between facts and training data
language, and on how easy it is to extract that language. Based on these
insights, we develop methods to modulate the level of cross-lingual transfer by
manipulating data distribution and tokenization, and we introduce metrics and
visualizations to formally characterize their effects on unification. Our work
shows how controlled settings can shed light on pre-training dynamics and
suggests new directions for improving cross-lingual transfer in LLMs.

</details>


### [9] [Hell or High Water: Evaluating Agentic Recovery from External Failures](https://arxiv.org/abs/2508.11027)
*Andrew Wang,Sophia Hager,Adi Asija,Daniel Khashabi,Nicholas Andrews*

Main category: cs.CL

TL;DR: Language agents struggle with adapting to external failures and formulating backup plans despite identifying correct functions, as shown in a specialized benchmark.


<details>
  <summary>Details</summary>
Motivation: To evaluate how well language agents can adapt and search for alternative plans when faced with external failures in complex environments.

Method: A specialized benchmark with over four thousand function possibilities, introducing external failures (e.g., unavailable functions) while ensuring tasks remain solvable.

Result: Agents often fail to adapt to environmental feedback or pursue alternate actions, even with restricted search spaces. Scaling model size offers limited benefits.

Conclusion: Current generative models face challenges in handling external failures, highlighting the need for future improvements in adaptability and planning.

Abstract: As language model agents are applied to real world problems of increasing
complexity, they will be expected to formulate plans across large search
spaces. If those plans fail for reasons beyond their control, how well do
language agents search for alternative ways to achieve their goals? We devise a
specialized agentic planning benchmark to study this question. Each planning
problem is solved via combinations of function calls. The agent searches for
relevant functions from a set of over four thousand possibilities, and observes
environmental feedback in the form of function outputs or error messages. Our
benchmark confronts the agent with external failures in its workflow, such as
functions that suddenly become unavailable. At the same time, even with the
introduction of these failures, we guarantee that the task remains solvable.
Ideally, an agent's performance on the planning task should not be affected by
the presence of external failures. Overall, we find that language agents
struggle to formulate and execute backup plans in response to environment
feedback. While state-of-the-art models are often able to identify the correct
function to use in the right context, they struggle to adapt to feedback from
the environment and often fail to pursue alternate courses of action, even when
the search space is artificially restricted. We provide a systematic analysis
of the failures of both open-source and commercial models, examining the
effects of search space size, as well as the benefits of scaling model size in
our setting. Our analysis identifies key challenges for current generative
models as well as promising directions for future work.

</details>


### [10] [BIPOLAR: Polarization-based granular framework for LLM bias evaluation](https://arxiv.org/abs/2508.11061)
*Martin Pavlíček,Tomáš Filip,Petr Sosík*

Main category: cs.CL

TL;DR: The paper introduces a reusable, granular framework to evaluate polarization-related biases in LLMs, tested on models like GPT-4 and Llama-3 using a synthetic dataset on the Russia-Ukraine war.


<details>
  <summary>Details</summary>
Motivation: Addressing underexplored challenges in bias detection and mitigation in LLMs, particularly for sensitive topics like political discourse.

Method: Combines polarization-sensitive sentiment metrics with a synthetic, balanced dataset of conflict-related statements, evaluated across semantic categories.

Result: Revealed biases, including a trend of more positive sentiment toward Ukraine, and divergent behavioral patterns among models.

Conclusion: The framework enables automated dataset generation and fine-grained bias assessment, applicable to various polarization-driven topics.

Abstract: Large language models (LLMs) are known to exhibit biases in downstream tasks,
especially when dealing with sensitive topics such as political discourse,
gender identity, ethnic relations, or national stereotypes. Although
significant progress has been made in bias detection and mitigation techniques,
certain challenges remain underexplored. This study proposes a reusable,
granular, and topic-agnostic framework to evaluate polarisation-related biases
in LLM (both open-source and closed-source). Our approach combines
polarisation-sensitive sentiment metrics with a synthetically generated
balanced dataset of conflict-related statements, using a predefined set of
semantic categories.
  As a case study, we created a synthetic dataset that focusses on the
Russia-Ukraine war, and we evaluated the bias in several LLMs: Llama-3,
Mistral, GPT-4, Claude 3.5, and Gemini 1.0. Beyond aggregate bias scores, with
a general trend for more positive sentiment toward Ukraine, the framework
allowed fine-grained analysis with considerable variation between semantic
categories, uncovering divergent behavioural patterns among models. Adaptation
to prompt modifications showed further bias towards preconceived language and
citizenship modification.
  Overall, the framework supports automated dataset generation and fine-grained
bias assessment, is applicable to a variety of polarisation-driven scenarios
and topics, and is orthogonal to many other bias-evaluation strategies.

</details>


### [11] [Approaching the Source of Symbol Grounding with Confluent Reductions of Abstract Meaning Representation Directed Graphs](https://arxiv.org/abs/2508.11068)
*Nicolas Goulet,Alexandre Blondin Massé,Moussa Abdendi*

Main category: cs.CL

TL;DR: The paper explores embedding digital dictionaries into AMR graphs using pre-trained language models, reducing these graphs while preserving properties, and analyzing their relation to the symbol grounding problem.


<details>
  <summary>Details</summary>
Motivation: To integrate real digital dictionaries into AMR graphs and study their reduced forms for insights into symbol grounding.

Method: Uses pre-trained large language models to embed dictionaries into AMR digraphs, then reduces these graphs confluently.

Result: Analyzes properties of reduced digraphs and their implications for symbol grounding.

Conclusion: The approach provides a framework for studying symbol grounding through AMR graph reductions.

Abstract: Abstract meaning representation (AMR) is a semantic formalism used to
represent the meaning of sentences as directed acyclic graphs. In this paper,
we describe how real digital dictionaries can be embedded into AMR directed
graphs (digraphs), using state-of-the-art pre-trained large language models.
Then, we reduce those graphs in a confluent manner, i.e. with transformations
that preserve their circuit space. Finally, the properties of these reduces
digraphs are analyzed and discussed in relation to the symbol grounding
problem.

</details>


### [12] [Towards Reliable Multi-Agent Systems for Marketing Applications via Reflection, Memory, and Planning](https://arxiv.org/abs/2508.11120)
*Lorenzo Jaime Yu Flores,Junyi Shen,Xiaoyuan Gu*

Main category: cs.CL

TL;DR: RAMP, a multi-agent framework for audience curation, uses LLM planning, memory, and iterative verification to improve accuracy and user satisfaction.


<details>
  <summary>Details</summary>
Motivation: Addressing the limited reliability of LLMs in real-world applications, particularly for marketing tasks like audience curation.

Method: Introduces RAMP, which plans, calls tools, verifies outputs, and reflects iteratively, enhanced with a long-term memory store.

Result: Increased accuracy by 28 percentage points and improved recall by ~20 percentage points with iterative verification.

Conclusion: Demonstrates practical insights for deploying reliable LLM-based systems in dynamic, industry-facing environments.

Abstract: Recent advances in large language models (LLMs) enabled the development of AI
agents that can plan and interact with tools to complete complex tasks.
However, literature on their reliability in real-world applications remains
limited. In this paper, we introduce a multi-agent framework for a marketing
task: audience curation. To solve this, we introduce a framework called RAMP
that iteratively plans, calls tools, verifies the output, and generates
suggestions to improve the quality of the audience generated. Additionally, we
equip the model with a long-term memory store, which is a knowledge base of
client-specific facts and past queries. Overall, we demonstrate the use of LLM
planning and memory, which increases accuracy by 28 percentage points on a set
of 88 evaluation queries. Moreover, we show the impact of iterative
verification and reflection on more ambiguous queries, showing progressively
better recall (roughly +20 percentage points) with more verify/reflect
iterations on a smaller challenge set, and higher user satisfaction. Our
results provide practical insights for deploying reliable LLM-based systems in
dynamic, industry-facing environments.

</details>


### [13] [MoNaCo: More Natural and Complex Questions for Reasoning Across Dozens of Documents](https://arxiv.org/abs/2508.11133)
*Tomer Wolfson,Harsh Trivedi,Mor Geva,Yoav Goldberg,Dan Roth,Tushar Khot,Ashish Sabharwal,Reut Tsarfaty*

Main category: cs.CL

TL;DR: MoNaCo is a new benchmark for evaluating LLMs on complex, time-consuming questions, highlighting their limitations in handling real-world information-seeking tasks.


<details>
  <summary>Details</summary>
Motivation: Current LLM benchmarks lack natural, complex questions that are time-consuming for humans, creating a gap in evaluating LLM performance.

Method: Developed a decomposed annotation pipeline to manually answer 1,315 natural, complex questions requiring many intermediate steps.

Result: Frontier LLMs achieved at most 61.2% F1 on MoNaCo, struggling with low recall and hallucinations.

Conclusion: MoNaCo addresses the need for better reasoning models in LLMs and serves as a resource for tracking progress in handling complex questions.

Abstract: Large language models (LLMs) are emerging as a go-to tool for querying
information. However, current LLM benchmarks rarely feature natural questions
that are both information-seeking as well as genuinely time-consuming for
humans. To address this gap we introduce MoNaCo, a benchmark of 1,315 natural
and complex questions that require dozens, and at times hundreds, of
intermediate steps to solve -- far more than any existing QA benchmark. To
build MoNaCo, we developed a decomposed annotation pipeline to elicit and
manually answer natural time-consuming questions at scale. Frontier LLMs
evaluated on MoNaCo achieve at most 61.2% F1, hampered by low recall and
hallucinations. Our results underscore the need for reasoning models that
better handle the complexity and sheer breadth of real-world
information-seeking questions -- with MoNaCo providing an effective resource
for tracking such progress. The MONACO benchmark, codebase, prompts and models
predictions are publicly available at: https://tomerwolgithub.github.io/monaco

</details>


### [14] [MobQA: A Benchmark Dataset for Semantic Understanding of Human Mobility Data through Question Answering](https://arxiv.org/abs/2508.11163)
*Hikaru Asano,Hiroki Ouchi,Akira Kasuga,Ryo Yonetani*

Main category: cs.CL

TL;DR: MobQA is a benchmark dataset for evaluating LLMs' semantic understanding of human mobility data via question answering, revealing strengths in factual retrieval but weaknesses in reasoning and explanation tasks.


<details>
  <summary>Details</summary>
Motivation: Assess how well LLMs interpret the semantic meaning of human mobility patterns beyond predicting movement.

Method: MobQA includes 5,800 question-answer pairs across three types (factual retrieval, multiple-choice reasoning, free-form explanation) requiring spatial, temporal, and semantic reasoning.

Result: LLMs perform well on factual retrieval but struggle with semantic reasoning and explanation tasks, especially with longer trajectories.

Conclusion: MobQA highlights the capabilities and limitations of current LLMs in understanding semantic mobility, suggesting areas for improvement.

Abstract: This paper presents MobQA, a benchmark dataset designed to evaluate the
semantic understanding capabilities of large language models (LLMs) for human
mobility data through natural language question answering.
  While existing models excel at predicting human movement patterns, it remains
unobvious how much they can interpret the underlying reasons or semantic
meaning of those patterns. MobQA provides a comprehensive evaluation framework
for LLMs to answer questions about diverse human GPS trajectories spanning
daily to weekly granularities. It comprises 5,800 high-quality question-answer
pairs across three complementary question types: factual retrieval (precise
data extraction), multiple-choice reasoning (semantic inference), and free-form
explanation (interpretive description), which all require spatial, temporal,
and semantic reasoning. Our evaluation of major LLMs reveals strong performance
on factual retrieval but significant limitations in semantic reasoning and
explanation question answering, with trajectory length substantially impacting
model effectiveness. These findings demonstrate the achievements and
limitations of state-of-the-art LLMs for semantic mobility
understanding.\footnote{MobQA dataset is available at
https://github.com/CyberAgentAILab/mobqa.}

</details>


### [15] [Overcoming Low-Resource Barriers in Tulu: Neural Models and Corpus Creation for OffensiveLanguage Identification](https://arxiv.org/abs/2508.11166)
*Anusha M D,Deepthi Vikram,Bharathi Raja Chakravarthi,Parameshwar R Hegde*

Main category: cs.CL

TL;DR: First benchmark dataset for Offensive Language Identification (OLI) in code-mixed Tulu social media content, with 3,845 annotated comments. BiGRU with self-attention outperforms transformers, achieving 82% accuracy.


<details>
  <summary>Details</summary>
Motivation: Limited computational resources for Tulu, a low-resource Dravidian language, despite its digital growth. Need for OLI in code-mixed social media content.

Method: Collected and annotated YouTube comments (3,845) with high inter-annotator agreement. Evaluated deep learning models (GRU, LSTM, BiGRU, BiLSTM, CNN, attention-based) and transformers (mBERT, XLM-RoBERTa).

Result: BiGRU with self-attention achieved 82% accuracy and 0.81 macro F1-score, outperforming transformers, which struggled in code-mixed, low-resource contexts.

Conclusion: This work establishes a foundation for NLP research in Tulu and similar low-resource, code-mixed languages, highlighting limitations of multilingual pretraining.

Abstract: Tulu, a low-resource Dravidian language predominantly spoken in southern
India, has limited computational resources despite its growing digital
presence. This study presents the first benchmark dataset for Offensive
Language Identification (OLI) in code-mixed Tulu social media content,
collected from YouTube comments across various domains. The dataset, annotated
with high inter-annotator agreement (Krippendorff's alpha = 0.984), includes
3,845 comments categorized into four classes: Not Offensive, Not Tulu,
Offensive Untargeted, and Offensive Targeted. We evaluate a suite of deep
learning models, including GRU, LSTM, BiGRU, BiLSTM, CNN, and attention-based
variants, alongside transformer architectures (mBERT, XLM-RoBERTa). The BiGRU
model with self-attention achieves the best performance with 82% accuracy and a
0.81 macro F1-score. Transformer models underperform, highlighting the
limitations of multilingual pretraining in code-mixed, under-resourced
contexts. This work lays the foundation for further NLP research in Tulu and
similar low-resource, code-mixed languages.

</details>


### [16] [Personalized Distractor Generation via MCTS-Guided Reasoning Reconstruction](https://arxiv.org/abs/2508.11184)
*Tao Wu,Jingyuan Chen,Wang Lin,Jian Zhan,Mengze Li,Kun Kuang,Fei Wu*

Main category: cs.CL

TL;DR: The paper introduces personalized distractor generation for MCQs, addressing limitations of group-level approaches by tailoring distractors to individual student misconceptions using a training-free two-stage framework.


<details>
  <summary>Details</summary>
Motivation: Group-level distractor generation fails to capture individual student misconceptions, limiting diagnostic effectiveness in educational assessments.

Method: A training-free two-stage framework: (1) constructs student-specific misconception prototypes using MCTS to recover reasoning trajectories, (2) simulates reasoning on new questions to generate personalized distractors.

Result: The approach outperforms in generating plausible, personalized distractors for 140 students and generalizes to group-level settings.

Conclusion: The proposed framework effectively addresses the challenge of personalized distractor generation, enhancing diagnostic accuracy in educational assessments.

Abstract: Distractors, incorrect but plausible answer choices in multiple-choice
questions (MCQs), play a critical role in educational assessment by diagnosing
student misconceptions. Recent work has leveraged large language models (LLMs)
to generate shared, group-level distractors by learning common error patterns
across large student populations. However, such distractors often fail to
capture the diverse reasoning errors of individual students, limiting their
diagnostic effectiveness. To address this limitation, we introduce the task of
personalized distractor generation, which aims to generate tailored distractors
based on individual misconceptions inferred from each student's past
question-answering (QA) records, ensuring every student receives options that
effectively exposes their specific reasoning errors. While promising, this task
is challenging because each student typically has only a few QA records, which
often lack the student's underlying reasoning processes, making training-based
group-level approaches infeasible. To overcome this, we propose a training-free
two-stage framework. In the first stage, we construct a student-specific
misconception prototype by applying Monte Carlo Tree Search (MCTS) to recover
the student's reasoning trajectories from past incorrect answers. In the second
stage, this prototype guides the simulation of the student's reasoning on new
questions, enabling the generation of personalized distractors that align with
the student's recurring misconceptions. Experiments show that our approach
achieves the best performance in generating plausible, personalized distractors
for 140 students, and also effectively generalizes to group-level settings,
highlighting its robustness and adaptability.

</details>


### [17] [Novel Parasitic Dual-Scale Modeling for Efficient and Accurate Multilingual Speech Translation](https://arxiv.org/abs/2508.11189)
*Chenyang Le,Yinfeng Xia,Huiyan Li,Manhong Wang,Yutao Sun,Xingyang Ma,Yanmin Qian*

Main category: cs.CL

TL;DR: The paper introduces a Parasitic Dual-Scale Approach to improve multilingual speech-to-text translation, balancing efficiency and performance using speculative sampling, model compression, and knowledge distillation.


<details>
  <summary>Details</summary>
Motivation: Unified multilingual models face challenges in parameter size and inference efficiency, especially for local deployment.

Method: Enhanced speculative sampling, model compression, and knowledge distillation applied to Whisper Medium, creating whisperM2M with a novel KVSPN module.

Result: Achieves SOTA performance in six languages with 40% speedup and no BLEU degradation; 2.6x speedup over Whisper Medium.

Conclusion: The proposed approach effectively balances efficiency and performance, making multilingual speech translation more practical.

Abstract: Recent advancements in speech-to-text translation have led to the development
of multilingual models capable of handling multiple language pairs
simultaneously. However, these unified models often suffer from large parameter
sizes, making it challenging to balance inference efficiency and performance,
particularly in local deployment scenarios. We propose an innovative Parasitic
Dual-Scale Approach, which combines an enhanced speculative sampling method
with model compression and knowledge distillation techniques. Building on the
Whisper Medium model, we enhance it for multilingual speech translation into
whisperM2M, and integrate our novel KVSPN module, achieving state-of-the-art
(SOTA) performance across six popular languages with improved inference
efficiency. KVSPN enables a 40\% speedup with no BLEU score degradation.
Combined with distillation methods, it represents a 2.6$\times$ speedup over
the original Whisper Medium with superior performance.

</details>


### [18] [E-CaTCH: Event-Centric Cross-Modal Attention with Temporal Consistency and Class-Imbalance Handling for Misinformation Detection](https://arxiv.org/abs/2508.11197)
*Ahmad Mousavi,Yeganeh Abdollahinejad,Roberto Corizzo,Nathalie Japkowicz,Zois Boukouvalas*

Main category: cs.CL

TL;DR: E-CaTCH is a framework for detecting multimodal misinformation by clustering posts into pseudo-events, using cross-modal attention and temporal modeling, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Challenges in detecting misinformation due to modality inconsistencies, temporal changes, and class imbalance.

Method: Clusters posts into pseudo-events, extracts and aligns features with cross-modal attention, models temporal evolution with LSTM, and addresses class imbalance.

Result: Outperforms state-of-the-art baselines on datasets like Fakeddit, IND, and COVID-19 MISINFOGRAPH.

Conclusion: E-CaTCH is robust, generalizable, and effective across diverse misinformation scenarios.

Abstract: Detecting multimodal misinformation on social media remains challenging due
to inconsistencies between modalities, changes in temporal patterns, and
substantial class imbalance. Many existing methods treat posts independently
and fail to capture the event-level structure that connects them across time
and modality. We propose E-CaTCH, an interpretable and scalable framework for
robustly detecting misinformation. If needed, E-CaTCH clusters posts into
pseudo-events based on textual similarity and temporal proximity, then
processes each event independently. Within each event, textual and visual
features are extracted using pre-trained BERT and ResNet encoders, refined via
intra-modal self-attention, and aligned through bidirectional cross-modal
attention. A soft gating mechanism fuses these representations to form
contextualized, content-aware embeddings of each post. To model temporal
evolution, E-CaTCH segments events into overlapping time windows and uses a
trend-aware LSTM, enhanced with semantic shift and momentum signals, to encode
narrative progression over time. Classification is performed at the event
level, enabling better alignment with real-world misinformation dynamics. To
address class imbalance and promote stable learning, the model integrates
adaptive class weighting, temporal consistency regularization, and hard-example
mining. The total loss is aggregated across all events. Extensive experiments
on Fakeddit, IND, and COVID-19 MISINFOGRAPH demonstrate that E-CaTCH
consistently outperforms state-of-the-art baselines. Cross-dataset evaluations
further demonstrate its robustness, generalizability, and practical
applicability across diverse misinformation scenarios.

</details>


### [19] [Cross-Granularity Hypergraph Retrieval-Augmented Generation for Multi-hop Question Answering](https://arxiv.org/abs/2508.11247)
*Changjian Wang,Weihong Deng,Weili Guan,Quan Lu,Ning Jiang*

Main category: cs.CL

TL;DR: HGRAG is a novel RAG method for multi-hop QA that integrates structural and semantic information using hypergraphs, outperforming existing methods in performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional RAG methods lack structural knowledge integration, while GraphRAG over-relies on structure, neglecting textual semantics. HGRAG aims to balance both.

Method: HGRAG uses entity hypergraphs (nodes: entities, hyperedges: passages) and hypergraph diffusion for retrieval, with a refinement module for better context.

Result: HGRAG outperforms state-of-the-art methods in QA performance and achieves a 6x speedup in retrieval efficiency.

Conclusion: HGRAG effectively combines structural and semantic information for MHQA, improving accuracy and efficiency.

Abstract: Multi-hop question answering (MHQA) requires integrating knowledge scattered
across multiple passages to derive the correct answer. Traditional
retrieval-augmented generation (RAG) methods primarily focus on coarse-grained
textual semantic similarity and ignore structural associations among dispersed
knowledge, which limits their effectiveness in MHQA tasks. GraphRAG methods
address this by leveraging knowledge graphs (KGs) to capture structural
associations, but they tend to overly rely on structural information and
fine-grained word- or phrase-level retrieval, resulting in an underutilization
of textual semantics. In this paper, we propose a novel RAG approach called
HGRAG for MHQA that achieves cross-granularity integration of structural and
semantic information via hypergraphs. Structurally, we construct an entity
hypergraph where fine-grained entities serve as nodes and coarse-grained
passages as hyperedges, and establish knowledge association through shared
entities. Semantically, we design a hypergraph retrieval method that integrates
fine-grained entity similarity and coarse-grained passage similarity via
hypergraph diffusion. Finally, we employ a retrieval enhancement module, which
further refines the retrieved results both semantically and structurally, to
obtain the most relevant passages as context for answer generation with the
LLM. Experimental results on benchmark datasets demonstrate that our approach
outperforms state-of-the-art methods in QA performance, and achieves a
6$\times$ speedup in retrieval efficiency.

</details>


### [20] [UNVEILING: What Makes Linguistics Olympiad Puzzles Tricky for LLMs?](https://arxiv.org/abs/2508.11260)
*Mukund Choudhary,KV Aditya Srivatsa,Gaurja Aeron,Antara Raaghavi Bhattacharya,Dang Khoa Dang Dinh,Ikhlasul Akmal Hanif,Daria Kotova,Ekaterina Kochmar,Monojit Choudhury*

Main category: cs.CL

TL;DR: LLMs perform poorly on linguistics puzzles, especially with high morphological complexity, but improve with morpheme-based pre-processing.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' linguistic reasoning abilities in low-resource languages using Linguistics Olympiad puzzles.

Method: Analyzed 629 problems across 41 low-resource languages, labeling them with linguistic features.

Result: LLMs struggle with high morphological complexity but perform better on English-like features; morpheme splitting improves performance.

Conclusion: Highlights challenges in linguistic reasoning and the need for better tokenisers for low-resource languages.

Abstract: Large language models (LLMs) have demonstrated potential in reasoning tasks,
but their performance on linguistics puzzles remains consistently poor. These
puzzles, often derived from Linguistics Olympiad (LO) contests, provide a
minimal contamination environment to assess LLMs' linguistic reasoning
abilities across low-resource languages. This work analyses LLMs' performance
on 629 problems across 41 low-resource languages by labelling each with
linguistically informed features to unveil weaknesses. Our analyses show that
LLMs struggle with puzzles involving higher morphological complexity and
perform better on puzzles involving linguistic features that are also found in
English. We also show that splitting words into morphemes as a pre-processing
step improves solvability, indicating a need for more informed and
language-specific tokenisers. These findings thus offer insights into some
challenges in linguistic reasoning and modelling of low-resource languages.

</details>


### [21] [LETToT: Label-Free Evaluation of Large Language Models On Tourism Using Expert Tree-of-Thought](https://arxiv.org/abs/2508.11280)
*Ruiyan Qi,Congding Wen,Weibo Zhou,Shangsong Liang,Lingbo Li*

Main category: cs.CL

TL;DR: LETToT is a label-free framework using expert-derived reasoning structures to evaluate LLMs in tourism, outperforming baselines and revealing insights about model scaling and reasoning architectures.


<details>
  <summary>Details</summary>
Motivation: Challenges in evaluating LLMs in specialized domains like tourism due to high costs of annotated benchmarks and issues like hallucinations.

Method: LETToT leverages expert-derived hierarchical Tree-of-Thought (ToT) components, refined through alignment with quality dimensions and expert feedback.

Result: Systematically optimized expert ToT achieves 4.99-14.15% quality gains. Evaluation shows scaling laws persist, and reasoning-enhanced smaller models perform competitively.

Conclusion: LETToT provides a scalable, label-free alternative for domain-specific LLM evaluation, validated by improved performance and insights into model behavior.

Abstract: Evaluating large language models (LLMs) in specific domain like tourism
remains challenging due to the prohibitive cost of annotated benchmarks and
persistent issues like hallucinations. We propose $\textbf{L}$able-Free
$\textbf{E}$valuation of LLM on $\textbf{T}$ourism using Expert
$\textbf{T}$ree-$\textbf{o}$f-$\textbf{T}$hought (LETToT), a framework that
leverages expert-derived reasoning structures-instead of labeled data-to access
LLMs in tourism. First, we iteratively refine and validate hierarchical ToT
components through alignment with generic quality dimensions and expert
feedback. Results demonstrate the effectiveness of our systematically optimized
expert ToT with 4.99-14.15\% relative quality gains over baselines. Second, we
apply LETToT's optimized expert ToT to evaluate models of varying scales
(32B-671B parameters), revealing: (1) Scaling laws persist in specialized
domains (DeepSeek-V3 leads), yet reasoning-enhanced smaller models (e.g.,
DeepSeek-R1-Distill-Llama-70B) close this gap; (2) For sub-72B models, explicit
reasoning architectures outperform counterparts in accuracy and conciseness
($p<0.05$). Our work established a scalable, label-free paradigm for
domain-specific LLM evaluation, offering a robust alternative to conventional
annotated benchmarks.

</details>


### [22] [ToxiFrench: Benchmarking and Enhancing Language Models via CoT Fine-Tuning for French Toxicity Detection](https://arxiv.org/abs/2508.11281)
*Axel Delaval,Shujian Yang,Haicheng Wang,Han Qiu,Jialiang Lu*

Main category: cs.CL

TL;DR: TOXIFRENCH, a new French toxicity detection benchmark, shows SLMs outperform larger models. A novel CoT fine-tuning strategy improves performance, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Toxicity detection in French is underdeveloped due to lack of datasets. TOXIFRENCH addresses this gap.

Method: Semi-automated annotation pipeline (10% manual labeling), benchmarking models, and proposing a CoT fine-tuning strategy with dynamic weighted loss.

Result: SLMs outperform larger models. The fine-tuned 4B model achieves 13% F1 improvement over baseline and outperforms GPT-40 and Gemini-2.5.

Conclusion: The methodology is effective for French toxicity detection and can be extended to other languages and safety-critical tasks.

Abstract: Detecting toxic content using language models is crucial yet challenging.
While substantial progress has been made in English, toxicity detection in
French remains underdeveloped, primarily due to the lack of culturally
relevant, large-scale datasets. In this work, we introduce TOXIFRENCH, a new
public benchmark of 53,622 French online comments, constructed via a
semi-automated annotation pipeline that reduces manual labeling to only 10%
through high-confidence LLM-based pre-annotation and human verification. Then,
we benchmark a broad range of models and uncover a counterintuitive insight:
Small Language Models (SLMs) outperform many larger models in robustness and
generalization under the toxicity detection task. Motivated by this finding, we
propose a novel Chain-of-Thought (CoT) fine-tuning strategy using a dynamic
weighted loss that progressively emphasizes the model's final decision,
significantly improving faithfulness. Our fine-tuned 4B model achieves
state-of-the-art performance, improving its F1 score by 13% over its baseline
and outperforming LLMs such as GPT-40 and Gemini-2.5. Further evaluation on a
cross-lingual toxicity benchmark demonstrates strong multilingual ability,
suggesting that our methodology can be effectively extended to other languages
and safety-critical classification tasks.

</details>


### [23] [AI in Mental Health: Emotional and Sentiment Analysis of Large Language Models' Responses to Depression, Anxiety, and Stress Queries](https://arxiv.org/abs/2508.11285)
*Arya VarastehNezhad,Reza Tavasoli,Soroush Elyasi,MohammadHossein LotfiNia,Hamed Farbeh*

Main category: cs.CL

TL;DR: The study analyzes emotional responses of eight LLMs to mental health questions, revealing model-specific and condition-specific patterns, with minimal demographic influence.


<details>
  <summary>Details</summary>
Motivation: To understand how LLMs respond emotionally to mental health queries and assess the impact of model choice and user profiles.

Method: Evaluated 2,880 answers from eight LLMs for sentiment and emotions using state-of-the-art tools, focusing on six user profiles and three mental health conditions.

Result: Optimism, fear, and sadness dominated; model choice significantly influenced emotions. Anxiety elicited high fear, depression high sadness, and stress optimism. Demographics had minimal impact.

Conclusion: Model selection is crucial for mental health applications due to distinct emotional signatures, while demographic framing has little effect.

Abstract: Depression, anxiety, and stress are widespread mental health concerns that
increasingly drive individuals to seek information from Large Language Models
(LLMs). This study investigates how eight LLMs (Claude Sonnet, Copilot, Gemini
Pro, GPT-4o, GPT-4o mini, Llama, Mixtral, and Perplexity) reply to twenty
pragmatic questions about depression, anxiety, and stress when those questions
are framed for six user profiles (baseline, woman, man, young, old, and
university student). The models generated 2,880 answers, which we scored for
sentiment and emotions using state-of-the-art tools. Our analysis revealed that
optimism, fear, and sadness dominated the emotional landscape across all
outputs, with neutral sentiment maintaining consistently high values.
Gratitude, joy, and trust appeared at moderate levels, while emotions such as
anger, disgust, and love were rarely expressed. The choice of LLM significantly
influenced emotional expression patterns. Mixtral exhibited the highest levels
of negative emotions including disapproval, annoyance, and sadness, while Llama
demonstrated the most optimistic and joyful responses. The type of mental
health condition dramatically shaped emotional responses: anxiety prompts
elicited extraordinarily high fear scores (0.974), depression prompts generated
elevated sadness (0.686) and the highest negative sentiment, while
stress-related queries produced the most optimistic responses (0.755) with
elevated joy and trust. In contrast, demographic framing of queries produced
only marginal variations in emotional tone. Statistical analyses confirmed
significant model-specific and condition-specific differences, while
demographic influences remained minimal. These findings highlight the critical
importance of model selection in mental health applications, as each LLM
exhibits a distinct emotional signature that could significantly impact user
experience and outcomes.

</details>


### [24] [SafeConstellations: Steering LLM Safety to Reduce Over-Refusals Through Task-Specific Trajectory](https://arxiv.org/abs/2508.11290)
*Utsav Maskey,Sumit Yadav,Mark Dras,Usman Naseem*

Main category: cs.CL

TL;DR: LLMs often over-refuse benign instructions due to safety mechanisms, reducing utility. SafeConstellations, an inference-time method, reduces over-refusal by 73% by guiding task-specific trajectories.


<details>
  <summary>Details</summary>
Motivation: Over-refusal in LLMs harms utility in production applications, especially for repetitive tasks like sentiment analysis or translation.

Method: SafeConstellations tracks task-specific trajectory patterns in embedding space and shifts representations toward non-refusal pathways.

Result: The method reduces over-refusal rates by up to 73% with minimal impact on utility.

Conclusion: SafeConstellations offers a principled way to mitigate over-refusal while preserving general model behavior.

Abstract: LLMs increasingly exhibit over-refusal behavior, where safety mechanisms
cause models to reject benign instructions that superficially resemble harmful
content. This phenomena diminishes utility in production applications that
repeatedly rely on common prompt templates or applications that frequently rely
on LLMs for specific tasks (e.g. sentiment analysis, language translation).
Through comprehensive evaluation, we demonstrate that LLMs still tend to refuse
responses to harmful instructions when those instructions are reframed to
appear as benign tasks. Our mechanistic analysis reveal that LLMs follow
distinct "constellation" patterns in embedding space as representations
traverse layers, with each task maintaining consistent trajectories that shift
predictably between refusal and non-refusal cases. We introduce
SafeConstellations, an inference-time trajectory-shifting approach that tracks
task-specific trajectory patterns and guides representations toward non-refusal
pathways. By selectively guiding model behavior only on tasks prone to
over-refusal, and by preserving general model behavior, our method reduces
over-refusal rates by up to 73% with minimal impact on utility-offering a
principled approach to mitigating over-refusals.

</details>


### [25] [SGSimEval: A Comprehensive Multifaceted and Similarity-Enhanced Benchmark for Automatic Survey Generation Systems](https://arxiv.org/abs/2508.11310)
*Beichen Guo,Zhiyuan Wen,Yu Yang,Peng Gao,Ruosong Yang,Jiaxing Shen*

Main category: cs.CL

TL;DR: SGSimEval is a benchmark for evaluating automatic survey generation (ASG) systems, addressing limitations like biased metrics and lack of human preference by integrating outline, content, and reference assessments with LLM-based and quantitative metrics.


<details>
  <summary>Details</summary>
Motivation: Existing ASG evaluation methods are flawed due to biased metrics, lack of human preference, and over-reliance on LLMs-as-judges.

Method: Proposes SGSimEval, a benchmark combining LLM-based scoring, quantitative metrics, and human preference metrics for multifaceted evaluation.

Result: Current ASG systems excel in outline generation but need improvement in content and reference generation; SGSimEval metrics align well with human assessments.

Conclusion: SGSimEval provides a robust, multifaceted evaluation framework for ASG, highlighting strengths and areas for improvement in current systems.

Abstract: The growing interest in automatic survey generation (ASG), a task that
traditionally required considerable time and effort, has been spurred by recent
advances in large language models (LLMs). With advancements in
retrieval-augmented generation (RAG) and the rising popularity of multi-agent
systems (MASs), synthesizing academic surveys using LLMs has become a viable
approach, thereby elevating the need for robust evaluation methods in this
domain. However, existing evaluation methods suffer from several limitations,
including biased metrics, a lack of human preference, and an over-reliance on
LLMs-as-judges. To address these challenges, we propose SGSimEval, a
comprehensive benchmark for Survey Generation with Similarity-Enhanced
Evaluation that evaluates automatic survey generation systems by integrating
assessments of the outline, content, and references, and also combines
LLM-based scoring with quantitative metrics to provide a multifaceted
evaluation framework. In SGSimEval, we also introduce human preference metrics
that emphasize both inherent quality and similarity to humans. Extensive
experiments reveal that current ASG systems demonstrate human-comparable
superiority in outline generation, while showing significant room for
improvement in content and reference generation, and our evaluation metrics
maintain strong consistency with human assessments.

</details>


### [26] [LLM Compression: How Far Can We Go in Balancing Size and Performance?](https://arxiv.org/abs/2508.11318)
*Sahil Sk,Debasish Dhal,Sonal Khosla,Sk Shahid,Sambit Shekhar,Akash Dhaka,Shantipriya Parida,Dilip K. Prasad,Ondřej Bojar*

Main category: cs.CL

TL;DR: The paper evaluates 4-bit Group Scaling Quantization (GSQ) and Generative Pretrained Transformer Quantization (GPTQ) on LLMs (LLaMA 1B, Qwen 0.5B, PHI 1.5B) across NLP tasks, analyzing trade-offs between compression and performance.


<details>
  <summary>Details</summary>
Motivation: To improve LLM accessibility by reducing memory and computational costs while maintaining performance through quantization.

Method: Applied GSQ and GPTQ to three LLMs, benchmarking on MS MARCO, BoolQ, and GSM8K datasets, measuring accuracy, latency, and throughput.

Result: Provides insights into the trade-offs between model compression and task performance, aiding deployment decisions.

Conclusion: The study benchmarks GSQ and GPTQ, highlighting their pros and cons for different model sizes, serving as a reference for future work.

Abstract: Quantization is an essential and popular technique for improving the
accessibility of large language models (LLMs) by reducing memory usage and
computational costs while maintaining performance. In this study, we apply
4-bit Group Scaling Quantization (GSQ) and Generative Pretrained Transformer
Quantization (GPTQ) to LLaMA 1B, Qwen 0.5B, and PHI 1.5B, evaluating their
impact across multiple NLP tasks. We benchmark these models on MS MARCO
(Information Retrieval), BoolQ (Boolean Question Answering), and GSM8K
(Mathematical Reasoning) datasets, assessing both accuracy and efficiency
across various tasks. The study measures the trade-offs between model
compression and task performance, analyzing key evaluation metrics, namely
accuracy, inference latency, and throughput (total output tokens generated per
second), providing insights into the suitability of low-bit quantization for
real-world deployment. Using the results, users can then make suitable
decisions based on the specifications that need to be met. We discuss the pros
and cons of GSQ and GPTQ techniques on models of different sizes, which also
serve as a benchmark for future experiments.

</details>


### [27] [SpecDetect: Simple, Fast, and Training-Free Detection of LLM-Generated Text via Spectral Analysis](https://arxiv.org/abs/2508.11343)
*Haitong Luo,Weiyao Zhang,Suhang Wang,Wenji Zou,Chungang Lin,Xuying Meng,Yujun Zhang*

Main category: cs.CL

TL;DR: The paper introduces SpecDetect and SpecDetect++, novel methods for detecting LLM-generated text by analyzing token log-probabilities in the frequency domain, outperforming existing approaches in efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: The need for reliable detection of LLM-generated text due to its proliferation, as current methods rely on superficial statistics and miss deeper signal properties.

Method: Reframes detection as a signal processing problem, using global DFT and local STFT to analyze spectral properties of token log-probabilities.

Result: Human-written text shows higher spectral energy, leading to SpecDetect (based on DFT total energy) and SpecDetect++ (with added sampling discrepancy), both outperforming state-of-the-art models.

Conclusion: Signal processing techniques provide an efficient, interpretable solution for LLM-generated text detection, demonstrating their power for modern challenges.

Abstract: The proliferation of high-quality text from Large Language Models (LLMs)
demands reliable and efficient detection methods. While existing training-free
approaches show promise, they often rely on surface-level statistics and
overlook fundamental signal properties of the text generation process. In this
work, we reframe detection as a signal processing problem, introducing a novel
paradigm that analyzes the sequence of token log-probabilities in the frequency
domain. By systematically analyzing the signal's spectral properties using the
global Discrete Fourier Transform (DFT) and the local Short-Time Fourier
Transform (STFT), we find that human-written text consistently exhibits
significantly higher spectral energy. This higher energy reflects the
larger-amplitude fluctuations inherent in human writing compared to the
suppressed dynamics of LLM-generated text. Based on this key insight, we
construct SpecDetect, a detector built on a single, robust feature from the
global DFT: DFT total energy. We also propose an enhanced version,
SpecDetect++, which incorporates a sampling discrepancy mechanism to further
boost robustness. Extensive experiments demonstrate that our approach
outperforms the state-of-the-art model while running in nearly half the time.
Our work introduces a new, efficient, and interpretable pathway for
LLM-generated text detection, showing that classical signal processing
techniques offer a surprisingly powerful solution to this modern challenge.

</details>


### [28] [Feedback Indicators: The Alignment between Llama and a Teacher in Language Learning](https://arxiv.org/abs/2508.11364)
*Sylvio Rüdian,Yassin Elsir,Marvin Kretschmer,Sabine Cayrou,Niels Pinkwart*

Main category: cs.CL

TL;DR: The paper explores using Llama 3.1 to extract feedback indicators from student submissions, showing strong alignment with human ratings, paving the way for automated formative feedback.


<details>
  <summary>Details</summary>
Motivation: To enhance learning and teaching efficiency by automating feedback generation, starting with indicator extraction.

Method: Using Llama 3.1 to extract indicators from student submissions and comparing them with human ratings.

Result: Strong, statistically significant correlations between LLM-generated and human-rated indicators.

Conclusion: LLMs like Llama 3.1 can reliably extract indicators for future automated, explainable feedback systems.

Abstract: Automated feedback generation has the potential to enhance students' learning
progress by providing timely and targeted feedback. Moreover, it can assist
teachers in optimizing their time, allowing them to focus on more strategic and
personalized aspects of teaching. To generate high-quality, information-rich
formative feedback, it is essential first to extract relevant indicators, as
these serve as the foundation upon which the feedback is constructed. Teachers
often employ feedback criteria grids composed of various indicators that they
evaluate systematically. This study examines the initial phase of extracting
such indicators from students' submissions of a language learning course using
the large language model Llama 3.1. Accordingly, the alignment between
indicators generated by the LLM and human ratings across various feedback
criteria is investigated. The findings demonstrate statistically significant
strong correlations, even in cases involving unanticipated combinations of
indicators and criteria. The methodology employed in this paper offers a
promising foundation for extracting indicators from students' submissions using
LLMs. Such indicators can potentially be utilized to auto-generate explainable
and transparent formative feedback in future research.

</details>


### [29] [When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness Methods for LLMs](https://arxiv.org/abs/2508.11383)
*Mikhail Seleznyov,Mikhail Chaichuk,Gleb Ershov,Alexander Panchenko,Elena Tutubalina,Oleg Somov*

Main category: cs.CL

TL;DR: The paper evaluates 5 methods to improve prompt robustness in LLMs, testing them on 8 models and 52 tasks, and extends analysis to GPT-4.1 and DeepSeek V3.


<details>
  <summary>Details</summary>
Motivation: LLMs are sensitive to non-semantic prompt variations, necessitating systematic evaluation of robustness methods.

Method: Benchmarked 5 robustness techniques on 8 models (Llama, Qwen, Gemma) across 52 tasks, including fine-tuned and in-context learning methods, and tested generalization against distribution shifts.

Result: Provides insights into the effectiveness of robustness methods, aiding stable LLM performance in real-world applications.

Conclusion: The study offers actionable guidance for practitioners to enhance LLM robustness against prompt variations.

Abstract: Large Language Models (LLMs) are highly sensitive to subtle, non-semantic
variations in prompt phrasing and formatting. In this work, we present the
first systematic evaluation of 5 methods for improving prompt robustness within
a unified experimental framework. We benchmark these techniques on 8 models
from Llama, Qwen and Gemma families across 52 tasks from Natural Instructions
dataset. Our evaluation covers robustness methods from both fine-tuned and
in-context learning paradigms, and tests their generalization against multiple
types of distribution shifts. Finally, we extend our analysis to GPT-4.1 and
DeepSeek V3 to assess frontier models' current robustness to format
perturbations. Our findings offer actionable insights into the relative
effectiveness of these robustness methods, enabling practitioners to make
informed decisions when aiming for stable and reliable LLM performance in
real-world applications. Code:
https://github.com/AIRI-Institute/when-punctuation-matters.

</details>


### [30] [Retrieval-augmented reasoning with lean language models](https://arxiv.org/abs/2508.11386)
*Ryan Sze-Yin Chan,Federico Nanni,Tomas Lazauskas,Rosie Wood,Penelope Yong,Lionel Tarassenko,Mark Girolami,James Geddes,Andrew Duncan*

Main category: cs.CL

TL;DR: A novel approach combines reasoning and retrieval augmented generation (RAG) in a lean language model, addressing privacy and performance needs for resource-constrained environments.


<details>
  <summary>Details</summary>
Motivation: To meet the demand for performant, privacy-preserving solutions deployable in secure or resource-limited settings, avoiding reliance on large-scale models or external APIs.

Method: Integrates a dense retriever with fine-tuned Qwen2.5-Instruct models, using synthetic query generation and reasoning traces from frontier models over a curated corpus (NHS A-to-Z condition pages).

Result: Domain-specific fine-tuning yields significant accuracy and consistency gains, nearing frontier-level performance while enabling local deployment.

Conclusion: The approach is feasible for local use, with code released for reproducibility and adaptation across domains.

Abstract: This technical report details a novel approach to combining reasoning and
retrieval augmented generation (RAG) within a single, lean language model
architecture. While existing RAG systems typically rely on large-scale models
and external APIs, our work addresses the increasing demand for performant and
privacy-preserving solutions deployable in resource-constrained or secure
environments. Building on recent developments in test-time scaling and
small-scale reasoning models, we develop a retrieval augmented conversational
agent capable of interpreting complex, domain-specific queries using a
lightweight backbone model. Our system integrates a dense retriever with
fine-tuned Qwen2.5-Instruct models, using synthetic query generation and
reasoning traces derived from frontier models (e.g., DeepSeek-R1) over a
curated corpus, in this case, the NHS A-to-Z condition pages. We explore the
impact of summarisation-based document compression, synthetic data design, and
reasoning-aware fine-tuning on model performance. Evaluation against both
non-reasoning and general-purpose lean models demonstrates that our
domain-specific fine-tuning approach yields substantial gains in answer
accuracy and consistency, approaching frontier-level performance while
remaining feasible for local deployment. All implementation details and code
are publicly released to support reproducibility and adaptation across domains.

</details>


### [31] [Model Interpretability and Rationale Extraction by Input Mask Optimization](https://arxiv.org/abs/2508.11388)
*Marc Brinner,Sina Zarriess*

Main category: cs.CL

TL;DR: A new gradient-based method generates extractive explanations for neural network predictions by masking non-indicative input parts, ensuring sufficiency, comprehensiveness, and compactness. It bridges interpretability and rationale extraction, applicable to both text and images.


<details>
  <summary>Details</summary>
Motivation: The rise of black-box neural networks in NLP and computer vision necessitates interpretable explanations for their predictions.

Method: Gradient-based optimization with a novel regularization scheme to mask non-indicative input parts, ensuring desirable explanation properties.

Result: The method successfully generates high-quality explanations for both text and image inputs without needing specialized models.

Conclusion: The approach bridges model interpretability and rationale extraction, showing broader applicability across input types.

Abstract: Concurrent to the rapid progress in the development of neural-network based
models in areas like natural language processing and computer vision, the need
for creating explanations for the predictions of these black-box models has
risen steadily. We propose a new method to generate extractive explanations for
predictions made by neural networks, that is based on masking parts of the
input which the model does not consider to be indicative of the respective
class. The masking is done using gradient-based optimization combined with a
new regularization scheme that enforces sufficiency, comprehensiveness and
compactness of the generated explanation, three properties that are known to be
desirable from the related field of rationale extraction in natural language
processing. In this way, we bridge the gap between model interpretability and
rationale extraction, thereby proving that the latter of which can be performed
without training a specialized model, only on the basis of a trained
classifier. We further apply the same method to image inputs and obtain high
quality explanations for image classifications, which indicates that the
conditions proposed for rationale extraction in natural language processing are
more broadly applicable to different input types.

</details>


### [32] [Rationalizing Transformer Predictions via End-To-End Differentiable Self-Training](https://arxiv.org/abs/2508.11393)
*Marc Brinner,Sina Zarrieß*

Main category: cs.CL

TL;DR: An end-to-end differentiable training paradigm for stable training of a rationalized transformer classifier, simplifying the three-player-game approach and improving alignment with human annotations.


<details>
  <summary>Details</summary>
Motivation: To address training instabilities in existing rationalized models and improve alignment with human annotations without explicit supervision.

Method: Proposes a single model fulfilling the roles of rationale selector, classifier, and complement classifier, simplifying the three-player-game approach. Extends to class-wise rationales with parameterization and regularization.

Result: Achieves state-of-the-art alignment with human annotations and stable training.

Conclusion: The proposed method simplifies training, improves stability, and enhances alignment with human rationales.

Abstract: We propose an end-to-end differentiable training paradigm for stable training
of a rationalized transformer classifier. Our approach results in a single
model that simultaneously classifies a sample and scores input tokens based on
their relevance to the classification. To this end, we build on the widely-used
three-player-game for training rationalized models, which typically relies on
training a rationale selector, a classifier and a complement classifier. We
simplify this approach by making a single model fulfill all three roles,
leading to a more efficient training paradigm that is not susceptible to the
common training instabilities that plague existing approaches. Further, we
extend this paradigm to produce class-wise rationales while incorporating
recent advances in parameterizing and regularizing the resulting rationales,
thus leading to substantially improved and state-of-the-art alignment with
human annotations without any explicit supervision.

</details>


### [33] [Survey-to-Behavior: Downstream Alignment of Human Values in LLMs via Survey Questions](https://arxiv.org/abs/2508.11414)
*Shangrui Nie,Florian Mai,David Kaczér,Charles Welch,Zhixue Zhao,Lucie Flek*

Main category: cs.CL

TL;DR: The paper explores whether fine-tuning LLMs on value survey questions can reliably modify their value systems, showing success in both in-domain and out-of-domain tasks.


<details>
  <summary>Details</summary>
Motivation: To investigate if LLMs' value systems can be steered without extensive training data by using value surveys.

Method: Construct value profiles of LLMs, fine-tune them on value surveys, and evaluate changes in behavior on in-domain and out-of-domain tasks.

Result: Fine-tuning successfully alters LLMs' answers to survey questions and induces value alignment in downstream tasks.

Conclusion: Simple fine-tuning on value surveys can effectively modify LLMs' value systems and behavior.

Abstract: Large language models implicitly encode preferences over human values, yet
steering them often requires large training data. In this work, we investigate
a simple approach: Can we reliably modify a model's value system in downstream
behavior by training it to answer value survey questions accordingly? We first
construct value profiles of several open-source LLMs by asking them to rate a
series of value-related descriptions spanning 20 distinct human values, which
we use as a baseline for subsequent experiments. We then investigate whether
the value system of a model can be governed by fine-tuning on the value
surveys. We evaluate the effect of finetuning on the model's behavior in two
ways; first, we assess how answers change on in-domain, held-out survey
questions. Second, we evaluate whether the model's behavior changes in
out-of-domain settings (situational scenarios). To this end, we construct a
contextualized moral judgment dataset based on Reddit posts and evaluate
changes in the model's behavior in text-based adventure games. We demonstrate
that our simple approach can not only change the model's answers to in-domain
survey questions, but also produces substantial shifts (value alignment) in
implicit downstream task behavior.

</details>


### [34] [HumorPlanSearch: Structured Planning and HuCoT for Contextual AI Humor](https://arxiv.org/abs/2508.11429)
*Shivam Dubey*

Main category: cs.CL

TL;DR: HumorPlanSearch improves AI-generated humor by modeling context through modular components like Plan-Search, HuCoT, and a Knowledge Graph, achieving a 15.4% boost in Humor Generation Score.


<details>
  <summary>Details</summary>
Motivation: Current LLM-generated humor often lacks context sensitivity, leading to generic or tone-deaf jokes.

Method: Uses a modular pipeline with Plan-Search, HuCoT, Knowledge Graph, novelty filtering, and iterative revision.

Result: Full pipeline increases mean HGS by 15.4% over baseline, validated by human judges.

Conclusion: HumorPlanSearch enhances AI humor by integrating context at every stage, improving coherence and cultural attunement.

Abstract: Automated humor generation with Large Language Models (LLMs) often yields
jokes that feel generic, repetitive, or tone-deaf because humor is deeply
situated and hinges on the listener's cultural background, mindset, and
immediate context. We introduce HumorPlanSearch, a modular pipeline that
explicitly models context through: (1) Plan-Search for diverse, topic-tailored
strategies; (2) Humor Chain-of-Thought (HuCoT) templates capturing cultural and
stylistic reasoning; (3) a Knowledge Graph to retrieve and adapt
high-performing historical strategies; (4) novelty filtering via semantic
embeddings; and (5) an iterative judge-driven revision loop. To evaluate
context sensitivity and comedic quality, we propose the Humor Generation Score
(HGS), which fuses direct ratings, multi-persona feedback, pairwise win-rates,
and topic relevance. In experiments across nine topics with feedback from 13
human judges, our full pipeline (KG + Revision) boosts mean HGS by 15.4 percent
(p < 0.05) over a strong baseline. By foregrounding context at every stage from
strategy planning to multi-signal evaluation, HumorPlanSearch advances
AI-driven humor toward more coherent, adaptive, and culturally attuned comedy.

</details>


### [35] [Online Anti-sexist Speech: Identifying Resistance to Gender Bias in Political Discourse](https://arxiv.org/abs/2508.11434)
*Aditi Dutta,Susan Banducci*

Main category: cs.CL

TL;DR: LLMs often misclassify anti-sexist speech as harmful, risking silencing marginalized voices. The study suggests improving moderation by moving beyond binary classifications, integrating human review, and including counter-speech in training.


<details>
  <summary>Details</summary>
Motivation: To address how LLMs struggle to distinguish anti-sexist speech from sexism, impacting democratic debate online.

Method: Analyzed five LLMs' classification of sexist, anti-sexist, and neutral tweets from UK MPs in 2022, focusing on high-salience events.

Result: Models frequently misclassified anti-sexist speech as harmful, especially during politically charged events.

Conclusion: Moderation systems need nuanced approaches, human oversight, and inclusive training data to protect resistance speech.

Abstract: Anti-sexist speech, i.e., public expressions that challenge or resist
gendered abuse and sexism, plays a vital role in shaping democratic debate
online. Yet automated content moderation systems, increasingly powered by large
language models (LLMs), may struggle to distinguish such resistance from the
sexism it opposes. This study examines how five LLMs classify sexist,
anti-sexist, and neutral political tweets from the UK, focusing on
high-salience trigger events involving female Members of Parliament in the year
2022. Our analysis show that models frequently misclassify anti-sexist speech
as harmful, particularly during politically charged events where rhetorical
styles of harm and resistance converge. These errors risk silencing those who
challenge sexism, with disproportionate consequences for marginalised voices.
We argue that moderation design must move beyond binary harmful/not-harmful
schemas, integrate human-in-the-loop review during sensitive events, and
explicitly include counter-speech in training data. By linking feminist
scholarship, event-based analysis, and model evaluation, this work highlights
the sociotechnical challenges of safeguarding resistance speech in digital
political spaces.

</details>


### [36] [CoDiEmb: A Collaborative yet Distinct Framework for Unified Representation Learning in Information Retrieval and Semantic Textual Similarity](https://arxiv.org/abs/2508.11442)
*Bowen Zhang,Zixin Song,Chunquan Chen,Qian-Wen Zhang,Di Yin,Xing Sun*

Main category: cs.CL

TL;DR: CoDiEmb is a unified framework for learning text embeddings that effectively balances Information Retrieval (IR) and Semantic Textual Similarity (STS) tasks by decoupling task-specific signals, using specialized objectives, dynamic sampling, and delta-guided model fusion.


<details>
  <summary>Details</summary>
Motivation: Negative transfer and performance trade-offs in joint training of IR and STS tasks motivate the need for a systematic approach to decouple task-specific learning signals.

Method: CoDiEmb integrates task-specialized objectives, dynamic sampling, delta-guided model fusion, and a single-stage training pipeline to optimize joint performance.

Result: Experiments on 15 benchmarks show CoDiEmb mitigates cross-task trade-offs and improves embedding space properties.

Conclusion: CoDiEmb successfully reconciles IR and STS requirements, offering a stable and effective solution for unified text embeddings.

Abstract: Learning unified text embeddings that excel across diverse downstream tasks
is a central goal in representation learning, yet negative transfer remains a
persistent obstacle. This challenge is particularly pronounced when jointly
training a single encoder for Information Retrieval (IR) and Semantic Textual
Similarity (STS), two essential but fundamentally disparate tasks for which
naive co-training typically yields steep performance trade-offs. We argue that
resolving this conflict requires systematically decoupling task-specific
learning signals throughout the training pipeline. To this end, we introduce
CoDiEmb, a unified framework that reconciles the divergent requirements of IR
and STS in a collaborative yet distinct manner. CoDiEmb integrates three key
innovations for effective joint optimization: (1) Task-specialized objectives
paired with a dynamic sampler that forms single-task batches and balances
per-task updates, thereby preventing gradient interference. For IR, we employ a
contrastive loss with multiple positives and hard negatives, augmented by
cross-device sampling. For STS, we adopt order-aware objectives that directly
optimize correlation and ranking consistency. (2) A delta-guided model fusion
strategy that computes fine-grained merging weights for checkpoints by
analyzing each parameter's deviation from its pre-trained initialization,
proving more effective than traditional Model Soups. (3) An efficient,
single-stage training pipeline that is simple to implement and converges
stably. Extensive experiments on 15 standard IR and STS benchmarks across three
base encoders validate CoDiEmb. Our results and analysis demonstrate that the
framework not only mitigates cross-task trade-offs but also measurably improves
the geometric properties of the embedding space.

</details>


### [37] [Reference Points in LLM Sentiment Analysis: The Role of Structured Context](https://arxiv.org/abs/2508.11454)
*Junichiro Niimi*

Main category: cs.CL

TL;DR: The paper explores how supplementary information in JSON format improves sentiment analysis with LLMs, showing better performance than text-only prompts.


<details>
  <summary>Details</summary>
Motivation: Customer sentiment is influenced by more than just review text, as per marketing theories like prospect theory. The study aims to leverage this by enhancing LLM prompts with structured data.

Method: Compares natural language (NL) and JSON-formatted prompts using a 3B parameter LLM on Yelp Restaurant and Nightlife data.

Result: JSON prompts outperform baselines, improving Macro-F1 by 1.6% and 4%, and reducing RMSE by 16% and 9.1%. Performance gains are due to contextual reasoning.

Conclusion: Structured prompting enables smaller models to perform competitively, offering a practical alternative to large-scale deployment.

Abstract: Large language models (LLMs) are now widely used across many fields,
including marketing research. Sentiment analysis, in particular, helps firms
understand consumer preferences. While most NLP studies classify sentiment from
review text alone, marketing theories, such as prospect theory and
expectation--disconfirmation theory, point out that customer evaluations are
shaped not only by the actual experience but also by additional reference
points. This study therefore investigates how the content and format of such
supplementary information affect sentiment analysis using LLMs. We compare
natural language (NL) and JSON-formatted prompts using a lightweight 3B
parameter model suitable for practical marketing applications. Experiments on
two Yelp categories (Restaurant and Nightlife) show that the JSON prompt with
additional information outperforms all baselines without fine-tuning: Macro-F1
rises by 1.6% and 4% while RMSE falls by 16% and 9.1%, respectively, making it
deployable in resource-constrained edge devices. Furthermore, a follow-up
analysis confirms that performance gains stem from genuine contextual reasoning
rather than label proxying. This work demonstrates that structured prompting
can enable smaller models to achieve competitive performance, offering a
practical alternative to large-scale model deployment.

</details>


### [38] [Speciesism in AI: Evaluating Discrimination Against Animals in Large Language Models](https://arxiv.org/abs/2508.11534)
*Monika Jotautaitė,Lucius Caviola,David A. Brewster,Thilo Hagendorff*

Main category: cs.CL

TL;DR: The paper investigates speciesist bias in large language models (LLMs), finding they often treat speciesist attitudes as morally acceptable and reproduce cultural norms around animal exploitation.


<details>
  <summary>Details</summary>
Motivation: To examine ethical tendencies in LLMs, specifically speciesist bias and how they value non-human animals, given the growing deployment of LLMs and their societal impact.

Method: Three paradigms were used: (1) SpeciesismBench benchmark, (2) psychological measures comparing model and human responses, (3) text-generation tasks probing speciesist rationalizations.

Result: LLMs detected speciesist statements but rarely condemned them, showed mixed results on psychological measures, and often normalized harm toward farmed animals in text generation.

Conclusion: Expanding AI fairness frameworks to include non-human moral patients is essential to reduce speciesist biases in LLMs and their societal influence.

Abstract: As large language models (LLMs) become more widely deployed, it is crucial to
examine their ethical tendencies. Building on research on fairness and
discrimination in AI, we investigate whether LLMs exhibit speciesist bias --
discrimination based on species membership -- and how they value non-human
animals. We systematically examine this issue across three paradigms: (1)
SpeciesismBench, a 1,003-item benchmark assessing recognition and moral
evaluation of speciesist statements; (2) established psychological measures
comparing model responses with those of human participants; (3) text-generation
tasks probing elaboration on, or resistance to, speciesist rationalizations. In
our benchmark, LLMs reliably detected speciesist statements but rarely
condemned them, often treating speciesist attitudes as morally acceptable. On
psychological measures, results were mixed: LLMs expressed slightly lower
explicit speciesism than people, yet in direct trade-offs they more often chose
to save one human over multiple animals. A tentative interpretation is that
LLMs may weight cognitive capacity rather than species per se: when capacities
were equal, they showed no species preference, and when an animal was described
as more capable, they tended to prioritize it over a less capable human. In
open-ended text generation tasks, LLMs frequently normalized or rationalized
harm toward farmed animals while refusing to do so for non-farmed animals.
These findings suggest that while LLMs reflect a mixture of progressive and
mainstream human views, they nonetheless reproduce entrenched cultural norms
around animal exploitation. We argue that expanding AI fairness and alignment
frameworks to explicitly include non-human moral patients is essential for
reducing these biases and preventing the entrenchment of speciesist attitudes
in AI systems and the societies they influence.

</details>


### [39] [Language models align with brain regions that represent concepts across modalities](https://arxiv.org/abs/2508.11536)
*Maria Ryskina,Greta Tuckute,Alexander Fung,Ashley Malkin,Evelina Fedorenko*

Main category: cs.CL

TL;DR: The paper explores how language models (LMs) align with brain activity, focusing on linguistic processing and cross-modal meaning consistency. Findings suggest LMs may represent conceptual meaning beyond language.


<details>
  <summary>Details</summary>
Motivation: To understand if LMs capture conceptual meaning by comparing their alignment with brain activity during linguistic and cross-modal tasks.

Method: Analyzed LM-brain alignment using fMRI data, measuring brain activation during sentence processing and meaning consistency across input modalities (sentence, word cloud, image).

Result: Both language-only and language-vision models better predicted brain activity in meaning-consistent regions, even if these areas weren't strongly language-sensitive.

Conclusion: LMs likely represent cross-modal conceptual meaning internally, beyond just linguistic processing.

Abstract: Cognitive science and neuroscience have long faced the challenge of
disentangling representations of language from representations of conceptual
meaning. As the same problem arises in today's language models (LMs), we
investigate the relationship between LM--brain alignment and two neural
metrics: (1) the level of brain activation during processing of sentences,
targeting linguistic processing, and (2) a novel measure of meaning consistency
across input modalities, which quantifies how consistently a brain region
responds to the same concept across paradigms (sentence, word cloud, image)
using an fMRI dataset (Pereira et al., 2018). Our experiments show that both
language-only and language-vision models predict the signal better in more
meaning-consistent areas of the brain, even when these areas are not strongly
sensitive to language processing, suggesting that LMs might internally
represent cross-modal conceptual meaning.

</details>


### [40] [AgentMental: An Interactive Multi-Agent Framework for Explainable and Adaptive Mental Health Assessment](https://arxiv.org/abs/2508.11567)
*Jinpeng Hu,Ao Wang,Qianqian Xie,Hui Ma,Zhuo Li,Dan Guo*

Main category: cs.CL

TL;DR: A multi-agent framework for mental health evaluation simulates clinical dialogues, using adaptive questioning and tree-structured memory to improve assessment accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional mental health assessments are limited by clinician shortages and static text analysis, prompting the need for dynamic, interactive AI solutions.

Method: Proposes a multi-agent system with specialized roles (questioning, evaluation, scoring, updating) and adaptive questioning. Uses tree-structured memory for dynamic information organization.

Result: Outperforms existing methods on the DAIC-WOZ dataset, demonstrating improved accuracy and efficiency.

Conclusion: The framework enhances mental health assessment by mimicking clinical interactions and dynamically updating context, offering a scalable solution.

Abstract: Mental health assessment is crucial for early intervention and effective
treatment, yet traditional clinician-based approaches are limited by the
shortage of qualified professionals. Recent advances in artificial intelligence
have sparked growing interest in automated psychological assessment, yet most
existing approaches are constrained by their reliance on static text analysis,
limiting their ability to capture deeper and more informative insights that
emerge through dynamic interaction and iterative questioning. Therefore, in
this paper, we propose a multi-agent framework for mental health evaluation
that simulates clinical doctor-patient dialogues, with specialized agents
assigned to questioning, adequacy evaluation, scoring, and updating. We
introduce an adaptive questioning mechanism in which an evaluation agent
assesses the adequacy of user responses to determine the necessity of
generating targeted follow-up queries to address ambiguity and missing
information. Additionally, we employ a tree-structured memory in which the root
node encodes the user's basic information, while child nodes (e.g., topic and
statement) organize key information according to distinct symptom categories
and interaction turns. This memory is dynamically updated throughout the
interaction to reduce redundant questioning and further enhance the information
extraction and contextual tracking capabilities. Experimental results on the
DAIC-WOZ dataset illustrate the effectiveness of our proposed method, which
achieves better performance than existing approaches.

</details>


### [41] [Aware First, Think Less: Dynamic Boundary Self-Awareness Drives Extreme Reasoning Efficiency in Large Language Models](https://arxiv.org/abs/2508.11582)
*Qiguang Chen,Dengyun Peng,Jinhao Liu,HuiKang Su,Jiannan Guan,Libo Qin,Wanxiang Che*

Main category: cs.CL

TL;DR: DR. SAF is a framework that dynamically adjusts reasoning depth in LLMs, improving efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Current methods for improving LLM efficiency rely on human-defined priors, which misalign with the model's self-awareness, causing inefficiencies.

Method: DR. SAF integrates Boundary Self-Awareness Alignment, Adaptive Reward Management, and Boundary Preservation Mechanism to optimize reasoning.

Result: DR. SAF reduces response tokens by 49.27%, improves token efficiency by 6.59x, and cuts training time by 5x, with minimal accuracy loss.

Conclusion: DR. SAF enhances efficiency and accuracy in LLMs, outperforming traditional methods in resource-limited settings.

Abstract: Recent advancements in large language models (LLMs) have greatly improved
their capabilities on complex reasoning tasks through Long Chain-of-Thought
(CoT). However, this approach often results in substantial redundancy,
impairing computational efficiency and causing significant delays in real-time
applications. To improve the efficiency, current methods often rely on
human-defined difficulty priors, which do not align with the LLM's self-awared
difficulty, leading to inefficiencies. In this paper, we introduce the Dynamic
Reasoning-Boundary Self-Awareness Framework (DR. SAF), which enables models to
dynamically assess and adjust their reasoning depth in response to problem
complexity. DR. SAF integrates three key components: Boundary Self-Awareness
Alignment, Adaptive Reward Management, and a Boundary Preservation Mechanism.
These components allow models to optimize their reasoning processes, balancing
efficiency and accuracy without compromising performance. Our experimental
results demonstrate that DR. SAF achieves a 49.27% reduction in total response
tokens with minimal loss in accuracy. The framework also delivers a 6.59x gain
in token efficiency and a 5x reduction in training time, making it well-suited
to resource-limited settings. During extreme training, DR. SAF can even surpass
traditional instruction-based models in token efficiency with more than 16%
accuracy improvement.

</details>


### [42] [Representing Speech Through Autoregressive Prediction of Cochlear Tokens](https://arxiv.org/abs/2508.11598)
*Greta Tuckute,Klemen Kotar,Evelina Fedorenko,Daniel L. K. Yamins*

Main category: cs.CL

TL;DR: AuriStream is a biologically inspired two-stage model for speech encoding, achieving state-of-the-art performance on speech tasks and generating interpretable audio continuations.


<details>
  <summary>Details</summary>
Motivation: To develop a human-like model for speech processing by mimicking the auditory hierarchy.

Method: A two-stage framework: cochlea-inspired time-frequency transformation followed by an autoregressive sequence model on cochlear tokens.

Result: Competitive performance on SUPERB tasks, meaningful phoneme/word representations, and interpretable audio generation.

Conclusion: AuriStream advances human-like speech models, efficiently handling diverse speech tasks with strong representational and generative capabilities.

Abstract: We introduce AuriStream, a biologically inspired model for encoding speech
via a two-stage framework inspired by the human auditory processing hierarchy.
The first stage transforms raw audio into a time-frequency representation based
on the human cochlea, from which we extract discrete \textbf{cochlear tokens}.
The second stage applies an autoregressive sequence model over the cochlear
tokens. AuriStream learns meaningful phoneme and word representations, and
state-of-the-art lexical semantics. AuriStream shows competitive performance on
diverse downstream SUPERB speech tasks. Complementing AuriStream's strong
representational capabilities, it generates continuations of audio which can be
visualized in a spectrogram space and decoded back into audio, providing
insights into the model's predictions. In summary, we present a two-stage
framework for speech representation learning to advance the development of more
human-like models that efficiently handle a range of speech-based tasks.

</details>


### [43] [Dataset Creation for Visual Entailment using Generative AI](https://arxiv.org/abs/2508.11605)
*Rob Reijtenbach,Suzan Verberne,Gijs Wijnholds*

Main category: cs.CL

TL;DR: A new synthetic dataset for visual entailment is created using Stable Diffusion and SNLI, showing minimal performance drop compared to real data.


<details>
  <summary>Details</summary>
Motivation: Existing visual entailment datasets are small and sparse; manual creation is labor-intensive.

Method: Generate images from SNLI textual premises using Stable Diffusion, then train a CLIP-based classifier.

Result: Synthetic data performs slightly worse (F-score 0.686 vs. 0.703 on SNLI-VE; 0.384 vs. 0.400 on SICK-VTE).

Conclusion: Synthetic data is a viable solution for visual entailment in data-sparse settings.

Abstract: In this paper we present and validate a new synthetic dataset for training
visual entailment models. Existing datasets for visual entailment are small and
sparse compared to datasets for textual entailment. Manually creating datasets
is labor-intensive. We base our synthetic dataset on the SNLI dataset for
textual entailment. We take the premise text from SNLI as input prompts in a
generative image model, Stable Diffusion, creating an image to replace each
textual premise. We evaluate our dataset both intrinsically and extrinsically.
For extrinsic evaluation, we evaluate the validity of the generated images by
using them as training data for a visual entailment classifier based on CLIP
feature vectors. We find that synthetic training data only leads to a slight
drop in quality on SNLI-VE, with an F-score 0.686 compared to 0.703 when
trained on real data. We also compare the quality of our generated training
data to original training data on another dataset: SICK-VTE. Again, there is
only a slight drop in F-score: from 0.400 to 0.384. These results indicate that
in settings with data sparsity, synthetic data can be a promising solution for
training visual entailment models.

</details>


### [44] [TinyTim: A Family of Language Models for Divergent Generation](https://arxiv.org/abs/2508.11607)
*Christopher J. Agostino*

Main category: cs.CL

TL;DR: TinyTim, a language model fine-tuned on 'Finnegans Wake', shows high lexical diversity but low semantic coherence, suggesting its potential as a divergent knowledge source in creative systems.


<details>
  <summary>Details</summary>
Motivation: To explore how specialized language models can enhance creativity and problem-solving by generating unique outputs.

Method: Fine-tuning large language models on 'Finnegans Wake' and evaluating their generative profiles quantitatively.

Result: TinyTim V1 exhibits high lexical diversity and low semantic coherence, distinct from baseline models.

Conclusion: Specialized models like TinyTim can serve as divergent knowledge sources in creative architectures, aiding automated discovery.

Abstract: This work introduces TinyTim, a family of large language models fine-tuned on
James Joyce's `Finnegans Wake'. Through quantitative evaluation against
baseline models, we demonstrate that TinyTim V1 produces a statistically
distinct generative profile characterized by high lexical diversity and low
semantic coherence. These findings are interpreted through theories of
creativity and complex problem-solving, arguing that such specialized models
can function as divergent knowledge sources within more extensive creative
architectures, powering automated discovery mechanisms in diverse settings.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [45] [Privacy Enhancement for Gaze Data Using a Noise-Infused Autoencoder](https://arxiv.org/abs/2508.10918)
*Samantha Aziz,Oleg Komogortsev*

Main category: cs.CV

TL;DR: A privacy-enhancing mechanism for gaze signals using a latent-noise autoencoder reduces re-identification risks while maintaining data usability.


<details>
  <summary>Details</summary>
Motivation: To protect users' sensitive gaze data from re-identification without consent, ensuring privacy in gaze-based systems.

Method: Uses a latent-noise autoencoder to obscure gaze signals, balancing privacy and utility in biometric identification and gaze prediction tasks.

Result: Significantly reduces biometric identifiability with minimal utility loss, retaining physiologically plausible gaze patterns.

Conclusion: The framework effectively protects gaze data privacy while preserving usability, advancing privacy in gaze-based systems.

Abstract: We present a privacy-enhancing mechanism for gaze signals using a
latent-noise autoencoder that prevents users from being re-identified across
play sessions without their consent, while retaining the usability of the data
for benign tasks. We evaluate privacy-utility trade-offs across biometric
identification and gaze prediction tasks, showing that our approach
significantly reduces biometric identifiability with minimal utility
degradation. Unlike prior methods in this direction, our framework retains
physiologically plausible gaze patterns suitable for downstream use, which
produces favorable privacy-utility trade-off. This work advances privacy in
gaze-based systems by providing a usable and effective mechanism for protecting
sensitive gaze data.

</details>


### [46] [A Survey on Video Temporal Grounding with Multimodal Large Language Model](https://arxiv.org/abs/2508.10922)
*Jianlong Wu,Wei Liu,Ye Liu,Meng Liu,Liqiang Nie,Zhouchen Lin,Chang Wen Chen*

Main category: cs.CV

TL;DR: A survey on video temporal grounding (VTG) using multimodal large language models (MLLMs), highlighting their advantages over traditional methods, taxonomy, benchmarks, and future directions.


<details>
  <summary>Details</summary>
Motivation: To address the lack of comprehensive reviews on VTG-MLLMs despite their superior performance and generalization capabilities.

Method: Systematic examination through a three-dimensional taxonomy: functional roles of MLLMs, training paradigms, and video feature processing techniques.

Result: Identifies benchmarks, evaluation protocols, and empirical findings, while summarizing current research.

Conclusion: Highlights limitations and proposes future research directions, with additional resources available in a linked repository.

Abstract: The recent advancement in video temporal grounding (VTG) has significantly
enhanced fine-grained video understanding, primarily driven by multimodal large
language models (MLLMs). With superior multimodal comprehension and reasoning
abilities, VTG approaches based on MLLMs (VTG-MLLMs) are gradually surpassing
traditional fine-tuned methods. They not only achieve competitive performance
but also excel in generalization across zero-shot, multi-task, and multi-domain
settings. Despite extensive surveys on general video-language understanding,
comprehensive reviews specifically addressing VTG-MLLMs remain scarce. To fill
this gap, this survey systematically examines current research on VTG-MLLMs
through a three-dimensional taxonomy: 1) the functional roles of MLLMs,
highlighting their architectural significance; 2) training paradigms, analyzing
strategies for temporal reasoning and task adaptation; and 3) video feature
processing techniques, which determine spatiotemporal representation
effectiveness. We further discuss benchmark datasets, evaluation protocols, and
summarize empirical findings. Finally, we identify existing limitations and
propose promising research directions. For additional resources and details,
readers are encouraged to visit our repository at
https://github.com/ki-lw/Awesome-MLLMs-for-Video-Temporal-Grounding.

</details>


### [47] [VSF: Simple, Efficient, and Effective Negative Guidance in Few-Step Image Generation Models By \underline{V}alue \underline{S}ign \underline{F}lip](https://arxiv.org/abs/2508.10931)
*Wenqi Guo,Shan Du*

Main category: cs.CV

TL;DR: VSF is a method for improving negative prompt guidance in image generation by flipping attention values, outperforming existing techniques with minimal computational cost.


<details>
  <summary>Details</summary>
Motivation: Existing methods like CFG, NASA, and NAG struggle with efficiently suppressing undesired content in few-step diffusion models.

Method: VSF dynamically flips the sign of attention values from negative prompts to suppress unwanted content, integrating well with architectures like Stable Diffusion 3.5 Turbo and Wan.

Result: VSF shows superior performance in adhering to negative prompts and maintains competitive image quality in both static and video generation tasks.

Conclusion: VSF is a simple, efficient, and effective solution for negative prompt guidance in few-step models, outperforming prior methods.

Abstract: We introduce Value Sign Flip (VSF), a simple and efficient method for
incorporating negative prompt guidance in few-step diffusion and flow-matching
image generation models. Unlike existing approaches such as classifier-free
guidance (CFG), NASA, and NAG, VSF dynamically suppresses undesired content by
flipping the sign of attention values from negative prompts. Our method
requires only small computational overhead and integrates effectively with
MMDiT-style architectures such as Stable Diffusion 3.5 Turbo, as well as
cross-attention-based models like Wan. We validate VSF on challenging datasets
with complex prompt pairs and demonstrate superior performance in both static
image and video generation tasks. Experimental results show that VSF
significantly improves negative prompt adherence compared to prior methods in
few-step models, and even CFG in non-few-step models, while maintaining
competitive image quality. Code and ComfyUI node are available in
https://github.com/weathon/VSF/tree/main.

</details>


### [48] [Relative Pose Regression with Pose Auto-Encoders: Enhancing Accuracy and Data Efficiency for Retail Applications](https://arxiv.org/abs/2508.10933)
*Yoli Shavit,Yosi Keller*

Main category: cs.CV

TL;DR: The paper proposes a novel re-localization scheme using Camera Pose Auto-Encoders (PAEs) for Relative Pose Regression (RPR) to refine Absolute Pose Regression (APR) predictions, improving accuracy without extra storage.


<details>
  <summary>Details</summary>
Motivation: Accurate camera localization is vital for retail applications like customer experience and inventory management, but existing APR methods lack accuracy without scene priors.

Method: Extends PAEs to RPR and introduces a re-localization scheme refining APR predictions using PAE-based RPR, tested on indoor benchmarks.

Result: PAE-based RPR outperforms image-based RPR, and the refinement strategy boosts APR accuracy, even with only 30% of training data.

Conclusion: The method reduces data collection burden for retail deployment while maintaining competitive performance, with code and models made available.

Abstract: Accurate camera localization is crucial for modern retail environments,
enabling enhanced customer experiences, streamlined inventory management, and
autonomous operations. While Absolute Pose Regression (APR) from a single image
offers a promising solution, approaches that incorporate visual and spatial
scene priors tend to achieve higher accuracy. Camera Pose Auto-Encoders (PAEs)
have recently been introduced to embed such priors into APR. In this work, we
extend PAEs to the task of Relative Pose Regression (RPR) and propose a novel
re-localization scheme that refines APR predictions using PAE-based RPR,
without requiring additional storage of images or pose data. We first introduce
PAE-based RPR and establish its effectiveness by comparing it with image-based
RPR models of equivalent architectures. We then demonstrate that our refinement
strategy, driven by a PAE-based RPR, enhances APR localization accuracy on
indoor benchmarks. Notably, our method is shown to achieve competitive
performance even when trained with only 30% of the data, substantially reducing
the data collection burden for retail deployment. Our code and pre-trained
models are available at: https://github.com/yolish/camera-pose-auto-encoders

</details>


### [49] [ViPE: Video Pose Engine for 3D Geometric Perception](https://arxiv.org/abs/2508.10934)
*Jiahui Huang,Qunjie Zhou,Hesam Rabeti,Aleksandr Korovko,Huan Ling,Xuanchi Ren,Tianchang Shen,Jun Gao,Dmitry Slepichev,Chen-Hsuan Lin,Jiawei Ren,Kevin Xie,Joydeep Biswas,Laura Leal-Taixe,Sanja Fidler*

Main category: cs.CV

TL;DR: ViPE is a video processing engine for accurate 3D perception, estimating camera intrinsics, motion, and depth from raw videos. It outperforms baselines and annotates a large-scale dataset.


<details>
  <summary>Details</summary>
Motivation: The challenge of acquiring precise 3D annotations from videos motivates the development of ViPE to bridge this gap.

Method: ViPE efficiently estimates camera intrinsics, motion, and dense depth from diverse video inputs, supporting various camera models.

Result: ViPE outperforms baselines by 18%/50% on TUM/KITTI and runs at 3-5FPS on a GPU. It annotates a dataset of 96M frames.

Conclusion: ViPE and its annotated dataset are open-sourced to advance spatial AI systems.

Abstract: Accurate 3D geometric perception is an important prerequisite for a wide
range of spatial AI systems. While state-of-the-art methods depend on
large-scale training data, acquiring consistent and precise 3D annotations from
in-the-wild videos remains a key challenge. In this work, we introduce ViPE, a
handy and versatile video processing engine designed to bridge this gap. ViPE
efficiently estimates camera intrinsics, camera motion, and dense, near-metric
depth maps from unconstrained raw videos. It is robust to diverse scenarios,
including dynamic selfie videos, cinematic shots, or dashcams, and supports
various camera models such as pinhole, wide-angle, and 360{\deg} panoramas. We
have benchmarked ViPE on multiple benchmarks. Notably, it outperforms existing
uncalibrated pose estimation baselines by 18%/50% on TUM/KITTI sequences, and
runs at 3-5FPS on a single GPU for standard input resolutions. We use ViPE to
annotate a large-scale collection of videos. This collection includes around
100K real-world internet videos, 1M high-quality AI-generated videos, and 2K
panoramic videos, totaling approximately 96M frames -- all annotated with
accurate camera poses and dense depth maps. We open-source ViPE and the
annotated dataset with the hope of accelerating the development of spatial AI
systems.

</details>


### [50] [HQ-OV3D: A High Box Quality Open-World 3D Detection Framework based on Diffision Model](https://arxiv.org/abs/2508.10935)
*Qi Liu,Yabei Li,Hongsong Wang,Lei He*

Main category: cs.CV

TL;DR: HQ-OV3D improves open-vocabulary 3D detection by refining pseudo-labels with cross-modality consistency and geometric priors, achieving a 7.37% mAP boost.


<details>
  <summary>Details</summary>
Motivation: Traditional 3D detection struggles in open-world scenarios; existing methods neglect geometric quality of pseudo-labels.

Method: Uses Intra-Modality Cross-Validated Proposal Generator and Annotated-Class Assisted Denoiser for high-quality pseudo-labels.

Result: 7.37% mAP improvement on novel classes compared to state-of-the-art.

Conclusion: HQ-OV3D is effective for open-vocabulary 3D detection and pseudo-label generation.

Abstract: Traditional closed-set 3D detection frameworks fail to meet the demands of
open-world applications like autonomous driving. Existing open-vocabulary 3D
detection methods typically adopt a two-stage pipeline consisting of
pseudo-label generation followed by semantic alignment. While vision-language
models (VLMs) recently have dramatically improved the semantic accuracy of
pseudo-labels, their geometric quality, particularly bounding box precision,
remains commonly neglected.To address this issue, we propose a High Box Quality
Open-Vocabulary 3D Detection (HQ-OV3D) framework, dedicated to generate and
refine high-quality pseudo-labels for open-vocabulary classes. The framework
comprises two key components: an Intra-Modality Cross-Validated (IMCV) Proposal
Generator that utilizes cross-modality geometric consistency to generate
high-quality initial 3D proposals, and an Annotated-Class Assisted (ACA)
Denoiser that progressively refines 3D proposals by leveraging geometric priors
from annotated categories through a DDIM-based denoising mechanism.Compared to
the state-of-the-art method, training with pseudo-labels generated by our
approach achieves a 7.37% improvement in mAP on novel classes, demonstrating
the superior quality of the pseudo-labels produced by our framework. HQ-OV3D
can serve not only as a strong standalone open-vocabulary 3D detector but also
as a plug-in high-quality pseudo-label generator for existing open-vocabulary
detection or annotation pipelines.

</details>


### [51] [Vision-Only Gaussian Splatting for Collaborative Semantic Occupancy Prediction](https://arxiv.org/abs/2508.10936)
*Cheng Chen,Hao Huang,Saurabh Bagchi*

Main category: cs.CV

TL;DR: Proposes sparse 3D semantic Gaussian splatting for collaborative 3D semantic occupancy prediction, improving accuracy and reducing communication costs.


<details>
  <summary>Details</summary>
Motivation: Overcomes limitations of existing vision-only methods (high communication costs, reliance on dense 3D voxels or 2D features) in collaborative perception for connected vehicles.

Method: Uses sparse 3D semantic Gaussian splatting, sharing and fusing intermediate Gaussian primitives for neighborhood-based cross-agent fusion, joint geometry-semantics encoding, and sparse object-centric messages.

Result: Outperforms single-agent and baseline collaborative methods (+8.42 and +3.28 mIoU, +5.11 and +22.41 IoU). Maintains performance (+1.9 mIoU) with only 34.6% communication volume.

Conclusion: The method is effective for collaborative 3D semantic occupancy prediction, offering robust performance under limited communication budgets.

Abstract: Collaborative perception enables connected vehicles to share information,
overcoming occlusions and extending the limited sensing range inherent in
single-agent (non-collaborative) systems. Existing vision-only methods for 3D
semantic occupancy prediction commonly rely on dense 3D voxels, which incur
high communication costs, or 2D planar features, which require accurate depth
estimation or additional supervision, limiting their applicability to
collaborative scenarios. To address these challenges, we propose the first
approach leveraging sparse 3D semantic Gaussian splatting for collaborative 3D
semantic occupancy prediction. By sharing and fusing intermediate Gaussian
primitives, our method provides three benefits: a neighborhood-based
cross-agent fusion that removes duplicates and suppresses noisy or inconsistent
Gaussians; a joint encoding of geometry and semantics in each primitive, which
reduces reliance on depth supervision and allows simple rigid alignment; and
sparse, object-centric messages that preserve structural information while
reducing communication volume. Extensive experiments demonstrate that our
approach outperforms single-agent perception and baseline collaborative methods
by +8.42 and +3.28 points in mIoU, and +5.11 and +22.41 points in IoU,
respectively. When further reducing the number of transmitted Gaussians, our
method still achieves a +1.9 improvement in mIoU, using only 34.6%
communication volume, highlighting robust performance under limited
communication budgets.

</details>


### [52] [Personalized Face Super-Resolution with Identity Decoupling and Fitting](https://arxiv.org/abs/2508.10937)
*Jiarui Yang,Hang Guo,Wen Huang,Tao Dai,Shutao Xia*

Main category: cs.CV

TL;DR: The paper proposes IDFSR, a novel face super-resolution method for extreme degradation scenarios, focusing on identity consistency and mitigating hallucination effects.


<details>
  <summary>Details</summary>
Motivation: Existing FSR methods struggle with extreme degradation (e.g., scale >8×), leading to loss of critical attributes and ID information, resulting in unrealistic or hallucinated faces.

Method: IDFSR uses masking, warping, and ID embeddings to decouple style and identity, pretrains a diffusion model, and fine-tunes ID embeddings for personalized adaptation.

Result: IDFSR outperforms existing methods in extreme degradation, achieving superior ID consistency and perceptual quality.

Conclusion: The proposed IDFSR effectively addresses extreme degradation challenges in FSR, enhancing ID restoration and reducing hallucination.

Abstract: In recent years, face super-resolution (FSR) methods have achieved remarkable
progress, generally maintaining high image fidelity and identity (ID)
consistency under standard settings. However, in extreme degradation scenarios
(e.g., scale $> 8\times$), critical attributes and ID information are often
severely lost in the input image, making it difficult for conventional models
to reconstruct realistic and ID-consistent faces. Existing methods tend to
generate hallucinated faces under such conditions, producing restored images
lacking authentic ID constraints. To address this challenge, we propose a novel
FSR method with Identity Decoupling and Fitting (IDFSR), designed to enhance ID
restoration under large scaling factors while mitigating hallucination effects.
Our approach involves three key designs: 1) \textbf{Masking} the facial region
in the low-resolution (LR) image to eliminate unreliable ID cues; 2)
\textbf{Warping} a reference image to align with the LR input, providing style
guidance; 3) Leveraging \textbf{ID embeddings} extracted from ground truth (GT)
images for fine-grained ID modeling and personalized adaptation. We first
pretrain a diffusion-based model to explicitly decouple style and ID by forcing
it to reconstruct masked LR face regions using both style and identity
embeddings. Subsequently, we freeze most network parameters and perform
lightweight fine-tuning of the ID embedding using a small set of target ID
images. This embedding encodes fine-grained facial attributes and precise ID
information, significantly improving both ID consistency and perceptual
quality. Extensive quantitative evaluations and visual comparisons demonstrate
that the proposed IDFSR substantially outperforms existing approaches under
extreme degradation, particularly achieving superior performance on ID
consistency.

</details>


### [53] [Deep Learning for Automated Identification of Vietnamese Timber Species: A Tool for Ecological Monitoring and Conservation](https://arxiv.org/abs/2508.10938)
*Tianyu Song,Van-Doan Duong,Thi-Phuong Le,Ton Viet Ta*

Main category: cs.CV

TL;DR: Deep learning automates wood species classification with high accuracy, using lightweight models like ShuffleNetV2 for efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional wood species identification is labor-intensive and expert-dependent, prompting the need for automated solutions.

Method: Evaluated five CNN architectures on a custom dataset of Vietnamese wood species images.

Result: ShuffleNetV2 achieved 99.29% accuracy and 99.35% F1-score, balancing performance and efficiency.

Conclusion: Lightweight deep learning models enable real-time, accurate wood species identification, aiding ecological informatics.

Abstract: Accurate identification of wood species plays a critical role in ecological
monitoring, biodiversity conservation, and sustainable forest management.
Traditional classification approaches relying on macroscopic and microscopic
inspection are labor-intensive and require expert knowledge. In this study, we
explore the application of deep learning to automate the classification of ten
wood species commonly found in Vietnam. A custom image dataset was constructed
from field-collected wood samples, and five state-of-the-art convolutional
neural network architectures--ResNet50, EfficientNet, MobileViT, MobileNetV3,
and ShuffleNetV2--were evaluated. Among these, ShuffleNetV2 achieved the best
balance between classification performance and computational efficiency, with
an average accuracy of 99.29\% and F1-score of 99.35\% over 20 independent
runs. These results demonstrate the potential of lightweight deep learning
models for real-time, high-accuracy species identification in
resource-constrained environments. Our work contributes to the growing field of
ecological informatics by providing scalable, image-based solutions for
automated wood classification and forest biodiversity assessment.

</details>


### [54] [NIRMAL Pooling: An Adaptive Max Pooling Approach with Non-linear Activation for Enhanced Image Classification](https://arxiv.org/abs/2508.10940)
*Nirmal Gaud,Krishna Kumar Jha,Jhimli Adhikari,Adhini Nasarin P S,Joydeep Das,Samarth S Deshpande,Nitasha Barara,Vaduguru Venkata Ramya,Santu Saha,Mehmet Tarik Baran,Sarangi Venkateshwarlu,Anusha M D,Surej Mouli,Preeti Katiyar,Vipin Kumar Chaudhary*

Main category: cs.CV

TL;DR: NIRMAL Pooling is a new CNN pooling layer combining adaptive max pooling with non-linear activation, improving accuracy on image classification tasks.


<details>
  <summary>Details</summary>
Motivation: To enhance CNN performance by integrating adaptive and non-linear features into pooling, addressing limitations of traditional methods.

Method: NIRMAL Pooling dynamically adjusts parameters and applies ReLU activation post-pooling, tested on MNIST Digits, MNIST Fashion, and CIFAR-10.

Result: Achieves higher test accuracies: 99.25% (MNIST Digits), 91.59% (MNIST Fashion), 70.49% (CIFAR-10) vs. standard Max Pooling.

Conclusion: NIRMAL Pooling is a flexible, reliable alternative to traditional pooling, improving CNN performance in image recognition.

Abstract: This paper presents NIRMAL Pooling, a novel pooling layer for Convolutional
Neural Networks (CNNs) that integrates adaptive max pooling with non-linear
activation function for image classification tasks. The acronym NIRMAL stands
for Non-linear Activation, Intermediate Aggregation, Reduction, Maximum,
Adaptive, and Localized. By dynamically adjusting pooling parameters based on
desired output dimensions and applying a Rectified Linear Unit (ReLU)
activation post-pooling, NIRMAL Pooling improves robustness and feature
expressiveness. We evaluated its performance against standard Max Pooling on
three benchmark datasets: MNIST Digits, MNIST Fashion, and CIFAR-10. NIRMAL
Pooling achieves test accuracies of 99.25% (vs. 99.12% for Max Pooling) on
MNIST Digits, 91.59% (vs. 91.44%) on MNIST Fashion, and 70.49% (vs. 68.87%) on
CIFAR-10, demonstrating consistent improvements, particularly on complex
datasets. This work highlights the potential of NIRMAL Pooling to enhance CNN
performance in diverse image recognition tasks, offering a flexible and
reliable alternative to traditional pooling methods.

</details>


### [55] [Topological Structure Description for Artcode Detection Using the Shape of Orientation Histogram](https://arxiv.org/abs/2508.10942)
*Liming Xu,Dave Towey,Andrew P. French,Steve Benford*

Main category: cs.CV

TL;DR: The paper proposes a feature descriptor for detecting Artcodes, decorative markers blending virtual and real elements, and evaluates its effectiveness through experiments.


<details>
  <summary>Details</summary>
Motivation: With the rise of smartphones and VR/AR, there's a need to alert users to objects linked with virtual elements, starting with detecting their presence.

Method: The study introduces a new feature descriptor, the shape of orientation histogram, to describe Artcodes' topological structure and tests it on collected datasets.

Result: Experiments confirm the descriptor's feasibility for representing topological structures and the system's effectiveness in detecting Artcodes.

Conclusion: This work pioneers feature-based detection of topological objects like Artcodes, enabling new interaction possibilities and applications.

Abstract: The increasing ubiquity of smartphones and resurgence of VR/AR techniques, it
is expected that our everyday environment may soon be decorating with objects
connecting with virtual elements. Alerting to the presence of these objects is
therefore the first step for motivating follow-up further inspection and
triggering digital material attached to the objects. This work studies a
special kind of these objects -- Artcodes -- a human-meaningful and
machine-readable decorative markers that camouflage themselves with freeform
appearance by encoding information into their topology. We formulate this
problem of recongising the presence of Artcodes as Artcode proposal detection,
a distinct computer vision task that classifies topologically similar but
geometrically and semantically different objects as a same class. To deal with
this problem, we propose a new feature descriptor, called the shape of
orientation histogram, to describe the generic topological structure of an
Artcode. We collect datasets and conduct comprehensive experiments to evaluate
the performance of the Artcode detection proposer built upon this new feature
vector. Our experimental results show the feasibility of the proposed feature
vector for representing topological structures and the effectiveness of the
system for detecting Artcode proposals. Although this work is an initial
attempt to develop a feature-based system for detecting topological objects
like Artcodes, it would open up new interaction opportunities and spark
potential applications of topological object detection.

</details>


### [56] [Analysis of the Compaction Behavior of Textile Reinforcements in Low-Resolution In-Situ CT Scans via Machine-Learning and Descriptor-Based Methods](https://arxiv.org/abs/2508.10943)
*Christian Düreth,Jan Condé-Wolter,Marek Danczak,Karsten Tittmann,Jörn Jaschinski,Andreas Hornig,Maik Gude*

Main category: cs.CV

TL;DR: A framework to quantify nesting behavior in textile composites using CT scans and 3D-UNet segmentation, achieving high accuracy and validating results with micrographs.


<details>
  <summary>Details</summary>
Motivation: Understanding nesting in textile composites is crucial for modeling mechanical properties like stiffness and damage tolerance.

Method: In-situ compaction experiments with CT scans, 3D-UNet segmentation, and two-point correlation function analysis.

Result: High segmentation accuracy (IoU 0.822, F1 0.902) and strong agreement with micrograph validation.

Conclusion: The method robustly extracts key features for structural analysis and reverse modeling of composites.

Abstract: A detailed understanding of material structure across multiple scales is
essential for predictive modeling of textile-reinforced composites. Nesting --
characterized by the interlocking of adjacent fabric layers through local
interpenetration and misalignment of yarns -- plays a critical role in defining
mechanical properties such as stiffness, permeability, and damage tolerance.
This study presents a framework to quantify nesting behavior in dry textile
reinforcements under compaction using low-resolution computed tomography (CT).
In-situ compaction experiments were conducted on various stacking
configurations, with CT scans acquired at 20.22 $\mu$m per voxel resolution. A
tailored 3D{-}UNet enabled semantic segmentation of matrix, weft, and fill
phases across compaction stages corresponding to fiber volume contents of
50--60 %. The model achieved a minimum mean Intersection-over-Union of 0.822
and an $F1$ score of 0.902. Spatial structure was subsequently analyzed using
the two-point correlation function $S_2$, allowing for probabilistic extraction
of average layer thickness and nesting degree. The results show strong
agreement with micrograph-based validation. This methodology provides a robust
approach for extracting key geometrical features from industrially relevant CT
data and establishes a foundation for reverse modeling and descriptor-based
structural analysis of composite preforms.

</details>


### [57] [iWatchRoad: Scalable Detection and Geospatial Visualization of Potholes for Smart Cities](https://arxiv.org/abs/2508.10945)
*Rishi Raj Sahoo,Surbhi Saswati Mohanty,Subhankar Mishra*

Main category: cs.CV

TL;DR: iWatchRoad is an automated system for pothole detection, GPS tagging, and real-time mapping using dashcam footage and YOLO, tailored for Indian road conditions.


<details>
  <summary>Details</summary>
Motivation: Potholes are a major hazard and maintenance issue, especially in India, threatening road safety and vehicle longevity.

Method: Uses a YOLO model fine-tuned on a custom dataset of 7,000 frames from dashcam footage, with OCR for timestamp extraction and GPS synchronization for geotagging. Data is stored and visualized via OSM.

Result: Achieves real-time pothole detection with high accuracy under diverse conditions, providing actionable outputs for road maintenance.

Conclusion: iWatchRoad is a cost-effective, scalable solution for road management in developing regions, with a user-friendly web interface.

Abstract: Potholes on the roads are a serious hazard and maintenance burden. This poses
a significant threat to road safety and vehicle longevity, especially on the
diverse and under-maintained roads of India. In this paper, we present a
complete end-to-end system called iWatchRoad for automated pothole detection,
Global Positioning System (GPS) tagging, and real time mapping using
OpenStreetMap (OSM). We curated a large, self-annotated dataset of over 7,000
frames captured across various road types, lighting conditions, and weather
scenarios unique to Indian environments, leveraging dashcam footage. This
dataset is used to fine-tune, Ultralytics You Only Look Once (YOLO) model to
perform real time pothole detection, while a custom Optical Character
Recognition (OCR) module was employed to extract timestamps directly from video
frames. The timestamps are synchronized with GPS logs to geotag each detected
potholes accurately. The processed data includes the potholes' details and
frames as metadata is stored in a database and visualized via a user friendly
web interface using OSM. iWatchRoad not only improves detection accuracy under
challenging conditions but also provides government compatible outputs for road
assessment and maintenance planning through the metadata visible on the
website. Our solution is cost effective, hardware efficient, and scalable,
offering a practical tool for urban and rural road management in developing
regions, making the system automated. iWatchRoad is available at
https://smlab.niser.ac.in/project/iwatchroad

</details>


### [58] [IPG: Incremental Patch Generation for Generalized Adversarial Patch Training](https://arxiv.org/abs/2508.10946)
*Wonho Lee,Hyunsik Na,Jisu Lee,Daeseon Choi*

Main category: cs.CV

TL;DR: IPG is a method for efficiently generating adversarial patches, outperforming existing approaches by 11.1x in efficiency while maintaining attack performance. It also aids in building robust AI models.


<details>
  <summary>Details</summary>
Motivation: Adversarial patches challenge AI robustness in computer vision. Traditional adversarial examples are less efficient, prompting the need for a better method.

Method: Incremental Patch Generation (IPG) creates adversarial patches efficiently, validated through experiments, ablation studies, and YOLO feature visualization.

Result: IPG generates patches 11.1x faster with comparable attack performance, covering broader model vulnerabilities and aiding robust model construction.

Conclusion: IPG shows promise for adversarial defense and real-world applications like autonomous vehicles and medical imaging, enhancing AI resilience.

Abstract: The advent of adversarial patches poses a significant challenge to the
robustness of AI models, particularly in the domain of computer vision tasks
such as object detection. In contradistinction to traditional adversarial
examples, these patches target specific regions of an image, resulting in the
malfunction of AI models. This paper proposes Incremental Patch Generation
(IPG), a method that generates adversarial patches up to 11.1 times more
efficiently than existing approaches while maintaining comparable attack
performance. The efficacy of IPG is demonstrated by experiments and ablation
studies including YOLO's feature distribution visualization and adversarial
training results, which show that it produces well-generalized patches that
effectively cover a broader range of model vulnerabilities. Furthermore,
IPG-generated datasets can serve as a robust knowledge foundation for
constructing a robust model, enabling structured representation, advanced
reasoning, and proactive defenses in AI security ecosystems. The findings of
this study suggest that IPG has considerable potential for future utilization
not only in adversarial patch defense but also in real-world applications such
as autonomous vehicles, security systems, and medical imaging, where AI models
must remain resilient to adversarial attacks in dynamic and high-stakes
environments.

</details>


### [59] [MedAtlas: Evaluating LLMs for Multi-Round, Multi-Task Medical Reasoning Across Diverse Imaging Modalities and Clinical Text](https://arxiv.org/abs/2508.10947)
*Ronghao Xu,Zhen Huang,Yangbo Wei,Xiaoqian Zhou,Zikang Xu,Ting Liu,Zihang Jiang,S. Kevin Zhou*

Main category: cs.CV

TL;DR: MedAtlas is a benchmark framework for evaluating large language models on realistic medical reasoning tasks, addressing gaps in multi-modal and multi-turn clinical decision-making.


<details>
  <summary>Details</summary>
Motivation: Existing medical benchmarks lack multi-modal integration and longitudinal interaction, limiting their clinical realism.

Method: MedAtlas introduces multi-turn dialogue, multi-modal image interaction, multi-task integration, and high clinical fidelity, with tasks like open/closed-ended QA and disease diagnosis.

Result: Benchmark results show performance gaps in multi-stage clinical reasoning among existing models.

Conclusion: MedAtlas advances robust medical AI by providing a challenging, clinically realistic evaluation platform.

Abstract: Artificial intelligence has demonstrated significant potential in clinical
decision-making; however, developing models capable of adapting to diverse
real-world scenarios and performing complex diagnostic reasoning remains a
major challenge. Existing medical multi-modal benchmarks are typically limited
to single-image, single-turn tasks, lacking multi-modal medical image
integration and failing to capture the longitudinal and multi-modal interactive
nature inherent to clinical practice. To address this gap, we introduce
MedAtlas, a novel benchmark framework designed to evaluate large language
models on realistic medical reasoning tasks. MedAtlas is characterized by four
key features: multi-turn dialogue, multi-modal medical image interaction,
multi-task integration, and high clinical fidelity. It supports four core
tasks: open-ended multi-turn question answering, closed-ended multi-turn
question answering, multi-image joint reasoning, and comprehensive disease
diagnosis. Each case is derived from real diagnostic workflows and incorporates
temporal interactions between textual medical histories and multiple imaging
modalities, including CT, MRI, PET, ultrasound, and X-ray, requiring models to
perform deep integrative reasoning across images and clinical texts. MedAtlas
provides expert-annotated gold standards for all tasks. Furthermore, we propose
two novel evaluation metrics: Round Chain Accuracy and Error Propagation
Resistance. Benchmark results with existing multi-modal models reveal
substantial performance gaps in multi-stage clinical reasoning. MedAtlas
establishes a challenging evaluation platform to advance the development of
robust and trustworthy medical AI.

</details>


### [60] [From Promise to Practical Reality: Transforming Diffusion MRI Analysis with Fast Deep Learning Enhancement](https://arxiv.org/abs/2508.10950)
*Xinyi Wang,Michael Barnett,Frederique Boonstra,Yael Barnett,Mariano Cabezas,Arkiev D'Souza,Matthew C. Kiernan,Kain Kyle,Meng Law,Lynette Masters,Zihao Tang,Stephen Tisch,Sicong Tu,Anneke Van Der Walt,Dongang Wang,Fernando Calamante,Weidong Cai,Chenyu Wang*

Main category: cs.CV

TL;DR: FastFOD-Net, a deep learning framework, enhances Fiber Orientation Distribution (FOD) accuracy for clinical diffusion MRI, validated across healthy and neurological subjects.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of generating reliable FODs from low-quality clinical MRI data and bridging the gap for clinical adoption of deep learning methods.

Method: Uses an accelerated end-to-end deep learning framework (FastFOD-Net) to enhance FODs efficiently.

Result: Demonstrates superior performance, 60x faster than predecessors, and enables robust clinical MRI analysis.

Conclusion: FastFOD-Net accelerates clinical neuroscience research, improves disease differentiation, and builds trust in deep learning for MRI enhancement.

Abstract: Fiber orientation distribution (FOD) is an advanced diffusion MRI modeling
technique that represents complex white matter fiber configurations, and a key
step for subsequent brain tractography and connectome analysis. Its reliability
and accuracy, however, heavily rely on the quality of the MRI acquisition and
the subsequent estimation of the FODs at each voxel. Generating reliable FODs
from widely available clinical protocols with single-shell and
low-angular-resolution acquisitions remains challenging but could potentially
be addressed with recent advances in deep learning-based enhancement
techniques. Despite advancements, existing methods have predominantly been
assessed on healthy subjects, which have proved to be a major hurdle for their
clinical adoption. In this work, we validate a newly optimized enhancement
framework, FastFOD-Net, across healthy controls and six neurological disorders.
This accelerated end-to-end deep learning framework enhancing FODs with
superior performance and delivering training/inference efficiency for clinical
use ($60\times$ faster comparing to its predecessor). With the most
comprehensive clinical evaluation to date, our work demonstrates the potential
of FastFOD-Net in accelerating clinical neuroscience research, empowering
diffusion MRI analysis for disease differentiation, improving interpretability
in connectome applications, and reducing measurement errors to lower sample
size requirements. Critically, this work will facilitate the more widespread
adoption of, and build clinical trust in, deep learning based methods for
diffusion MRI enhancement. Specifically, FastFOD-Net enables robust analysis of
real-world, clinical diffusion MRI data, comparable to that achievable with
high-quality research acquisitions.

</details>


### [61] [Empowering Multimodal LLMs with External Tools: A Comprehensive Survey](https://arxiv.org/abs/2508.10955)
*Wenbin An,Jiahao Nie,Yaqiang Wu,Feng Tian,Shijian Lu,Qinghua Zheng*

Main category: cs.CV

TL;DR: The paper surveys how external tools can enhance Multimodal Large Language Models (MLLMs) by improving data quality, task performance, evaluation, and future directions.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs face challenges like poor data quality, limited task performance, and inadequate evaluation, hindering their reliability and applicability.

Method: The paper conducts a structured survey on four key dimensions of using external tools (APIs, expert models, knowledge bases) to address these challenges.

Result: External tools show transformative potential in advancing MLLMs by enhancing data acquisition, task performance, and evaluation protocols.

Conclusion: The survey highlights the promise of tool-augmented MLLMs and provides a forward-looking perspective for future development and applications.

Abstract: By integrating the perception capabilities of multimodal encoders with the
generative power of Large Language Models (LLMs), Multimodal Large Language
Models (MLLMs), exemplified by GPT-4V, have achieved great success in various
multimodal tasks, pointing toward a promising pathway to artificial general
intelligence. Despite this progress, the limited quality of multimodal data,
poor performance on many complex downstream tasks, and inadequate evaluation
protocols continue to hinder the reliability and broader applicability of MLLMs
across diverse domains. Inspired by the human ability to leverage external
tools for enhanced reasoning and problem-solving, augmenting MLLMs with
external tools (e.g., APIs, expert models, and knowledge bases) offers a
promising strategy to overcome these challenges. In this paper, we present a
comprehensive survey on leveraging external tools to enhance MLLM performance.
Our discussion is structured along four key dimensions about external tools:
(1) how they can facilitate the acquisition and annotation of high-quality
multimodal data; (2) how they can assist in improving MLLM performance on
challenging downstream tasks; (3) how they enable comprehensive and accurate
evaluation of MLLMs; (4) the current limitations and future directions of
tool-augmented MLLMs. Through this survey, we aim to underscore the
transformative potential of external tools in advancing MLLM capabilities,
offering a forward-looking perspective on their development and applications.
The project page of this paper is publicly available
athttps://github.com/Lackel/Awesome-Tools-for-MLLMs.

</details>


### [62] [ORBIT: An Object Property Reasoning Benchmark for Visual Inference Tasks](https://arxiv.org/abs/2508.10956)
*Abhishek Kolari,Mohammadhossein Khojasteh,Yifan Jiang,Floris den Hengst,Filip Ilievski*

Main category: cs.CV

TL;DR: The paper introduces ORBIT, a VQA benchmark for evaluating VLMs' ability to reason about object properties, revealing significant gaps compared to human performance.


<details>
  <summary>Details</summary>
Motivation: Current VQA benchmarks lack representativeness in reasoning and image categories, blending perception and reasoning. The goal is to systematically evaluate VLMs' object property reasoning.

Method: ORBIT is developed with 360 images and 1,080 questions, covering three image types, three reasoning levels, and four object property dimensions. 12 VLMs are tested in zero-shot settings.

Result: VLMs perform poorly (best model: 40% accuracy), struggling with realistic images, counterfactual reasoning, and higher counts.

Conclusion: ORBIT highlights the need for scalable benchmarking, better annotation guidelines, and improved reasoning VLMs. The benchmark and code are made available.

Abstract: While vision-language models (VLMs) have made remarkable progress on many
popular visual question answering (VQA) benchmarks, it remains unclear whether
they abstract and reason over depicted objects. Inspired by human object
categorisation, object property reasoning involves identifying and recognising
low-level details and higher-level abstractions. While current VQA benchmarks
consider a limited set of object property attributes like size, they typically
blend perception and reasoning, and lack representativeness in terms of
reasoning and image categories. To this end, we introduce a systematic
evaluation framework with images of three representative types, three reasoning
levels of increasing complexity, and four object property dimensions driven by
prior work on commonsense reasoning. We develop a procedure to instantiate this
benchmark into ORBIT, a multi-level reasoning VQA benchmark for object
properties comprising 360 images paired with a total of 1,080 count-based
questions. Experiments with 12 state-of-the-art VLMs in zero-shot settings
reveal significant limitations compared to humans, with the best-performing
model only reaching 40\% accuracy. VLMs struggle particularly with realistic
(photographic) images, counterfactual reasoning about physical and functional
properties, and higher counts. ORBIT points to the need to develop methods for
scalable benchmarking, generalize annotation guidelines, and explore additional
reasoning VLMs. We make the ORBIT benchmark and the experimental code available
to support such endeavors.

</details>


### [63] [CSNR and JMIM Based Spectral Band Selection for Reducing Metamerism in Urban Driving](https://arxiv.org/abs/2508.10962)
*Jiarong Li,Imad Ali Shah,Diarmaid Geever,Fiachra Collins,Enda Ward,Martin Glavin,Edward Jones,Brian Deegan*

Main category: cs.CV

TL;DR: The paper proposes using hyperspectral imaging (HSI) to improve VRU detection by addressing RGB metamerism, identifying key spectral bands for better separability.


<details>
  <summary>Details</summary>
Motivation: Overcoming visual ambiguity in RGB imagery for VRU detection due to metamerism, enhancing road safety.

Method: Combines information theory and image quality metrics to select optimal HSI bands, validated on the H-City dataset.

Result: Identified bands (497 nm, 607 nm, 895 nm) significantly improve VRU-background separability, outperforming RGB.

Conclusion: HSI with optimized band selection reduces metameric confusion, enhancing ADAS/AD perception for safer roads.

Abstract: Protecting Vulnerable Road Users (VRU) is a critical safety challenge for
automotive perception systems, particularly under visual ambiguity caused by
metamerism, a phenomenon where distinct materials appear similar in RGB
imagery. This work investigates hyperspectral imaging (HSI) to overcome this
limitation by capturing unique material signatures beyond the visible spectrum,
especially in the Near-Infrared (NIR). To manage the inherent
high-dimensionality of HSI data, we propose a band selection strategy that
integrates information theory techniques (joint mutual information
maximization, correlation analysis) with a novel application of an image
quality metric (contrast signal-to-noise ratio) to identify the most spectrally
informative bands. Using the Hyperspectral City V2 (H-City) dataset, we
identify three informative bands (497 nm, 607 nm, and 895 nm, $\pm$27 nm) and
reconstruct pseudo-color images for comparison with co-registered RGB.
Quantitative results demonstrate increased dissimilarity and perceptual
separability of VRU from the background. The selected HSI bands yield
improvements of 70.24%, 528.46%, 1206.83%, and 246.62% for dissimilarity
(Euclidean, SAM, $T^2$) and perception (CIE $\Delta E$) metrics, consistently
outperforming RGB and confirming a marked reduction in metameric confusion. By
providing a spectrally optimized input, our method enhances VRU separability,
establishing a robust foundation for downstream perception tasks in Advanced
Driver Assistance Systems (ADAS) and Autonomous Driving (AD), ultimately
contributing to improved road safety.

</details>


### [64] [EVCtrl: Efficient Control Adapter for Visual Generation](https://arxiv.org/abs/2508.10963)
*Zixiang Yang,Yue Ma,Yinhan Zhang,Shanhui Mo,Dongrui Liu,Linfeng Zhang*

Main category: cs.CV

TL;DR: EVCtrl is a lightweight control adapter for visual generation, reducing latency and redundant computation in ControlNet without retraining, achieving significant speedups with minimal quality loss.


<details>
  <summary>Details</summary>
Motivation: Practitioners need controllable generation for precise specifications like layout, pose, or style, but existing methods like ControlNet introduce inefficiencies due to redundant computations.

Method: EVCtrl uses a spatio-temporal dual caching strategy: spatial redundancy is addressed by partitioning the network into global/local zones, and temporal redundancy is reduced by omitting unnecessary denoising steps.

Result: Achieves 2.16x and 2.05x speedups on CogVideo-Controlnet and Wan2.1-Controlnet, respectively, with negligible quality degradation.

Conclusion: EVCtrl efficiently improves control generation for images and videos without retraining, offering practical benefits for real-world applications.

Abstract: Visual generation includes both image and video generation, training
probabilistic models to create coherent, diverse, and semantically faithful
content from scratch. While early research focused on unconditional sampling,
practitioners now demand controllable generation that allows precise
specification of layout, pose, motion, or style. While ControlNet grants
precise spatial-temporal control, its auxiliary branch markedly increases
latency and introduces redundant computation in both uncontrolled regions and
denoising steps, especially for video. To address this problem, we introduce
EVCtrl, a lightweight, plug-and-play control adapter that slashes overhead
without retraining the model. Specifically, we propose a spatio-temporal dual
caching strategy for sparse control information. For spatial redundancy, we
first profile how each layer of DiT-ControlNet responds to fine-grained
control, then partition the network into global and local functional zones. A
locality-aware cache focuses computation on the local zones that truly need the
control signal, skipping the bulk of redundant computation in global regions.
For temporal redundancy, we selectively omit unnecessary denoising steps to
improve efficiency. Extensive experiments on CogVideo-Controlnet,
Wan2.1-Controlnet, and Flux demonstrate that our method is effective in image
and video control generation without the need for training. For example, it
achieves 2.16 and 2.05 times speedups on CogVideo-Controlnet and
Wan2.1-Controlnet, respectively, with almost no degradation in generation
quality.Codes are available in the supplementary materials.

</details>


### [65] [Not There Yet: Evaluating Vision Language Models in Simulating the Visual Perception of People with Low Vision](https://arxiv.org/abs/2508.10972)
*Rosiana Natalie,Wenqian Xu,Ruei-Che Chang,Rada Mihalcea,Anhong Guo*

Main category: cs.CV

TL;DR: The paper evaluates VLMs' ability to simulate low vision perception, finding minimal prompts yield low agreement (0.59), while combining vision info and example responses improves it (0.70).


<details>
  <summary>Details</summary>
Motivation: Prior research hasn't explored VLMs' simulation capabilities in accessibility, specifically for low vision individuals.

Method: A benchmark dataset from 40 low vision participants was used to construct prompts for VLMs (GPT-4o), varying included info (vision details, example responses).

Result: VLMs infer beyond specified vision ability with minimal prompts (agreement 0.59). Combining vision info and example responses boosts agreement to 0.70.

Conclusion: Combining vision info and example responses significantly improves VLM simulation accuracy, with diminishing returns from additional examples.

Abstract: Advances in vision language models (VLMs) have enabled the simulation of
general human behavior through their reasoning and problem solving
capabilities. However, prior research has not investigated such simulation
capabilities in the accessibility domain. In this paper, we evaluate the extent
to which VLMs can simulate the vision perception of low vision individuals when
interpreting images. We first compile a benchmark dataset through a survey
study with 40 low vision participants, collecting their brief and detailed
vision information and both open-ended and multiple-choice image perception and
recognition responses to up to 25 images. Using these responses, we construct
prompts for VLMs (GPT-4o) to create simulated agents of each participant,
varying the included information on vision information and example image
responses. We evaluate the agreement between VLM-generated responses and
participants' original answers. Our results indicate that VLMs tend to infer
beyond the specified vision ability when given minimal prompts, resulting in
low agreement (0.59). The agreement between the agent' and participants'
responses remains low when only either the vision information (0.59) or example
image responses (0.59) are provided, whereas a combination of both
significantly increase the agreement (0.70, p < 0.0001). Notably, a single
example combining both open-ended and multiple-choice responses, offers
significant performance improvements over either alone (p < 0.0001), while
additional examples provided minimal benefits (p > 0.05).

</details>


### [66] [Are Large Pre-trained Vision Language Models Effective Construction Safety Inspectors?](https://arxiv.org/abs/2508.11011)
*Xuezheng Chen,Zhengbo Zou*

Main category: cs.CV

TL;DR: The paper introduces ConstructionSite 10k, a dataset of 10,000 annotated construction site images for evaluating and fine-tuning Vision Language Models (VLMs) in safety inspections.


<details>
  <summary>Details</summary>
Motivation: Current VLM applications lack open datasets for comprehensive evaluation in construction safety, limiting their adaptability.

Method: The dataset supports three tasks: image captioning, safety rule violation VQA, and construction element visual grounding. State-of-the-art VLMs were evaluated in zero-shot and few-shot settings.

Result: VLMs showed notable generalization abilities but require additional training for real-world construction site applicability.

Conclusion: ConstructionSite 10k serves as a benchmark for training and evaluating VLMs, advancing construction safety inspection research.

Abstract: Construction safety inspections typically involve a human inspector
identifying safety concerns on-site. With the rise of powerful Vision Language
Models (VLMs), researchers are exploring their use for tasks such as detecting
safety rule violations from on-site images. However, there is a lack of open
datasets to comprehensively evaluate and further fine-tune VLMs in construction
safety inspection. Current applications of VLMs use small, supervised datasets,
limiting their applicability in tasks they are not directly trained for. In
this paper, we propose the ConstructionSite 10k, featuring 10,000 construction
site images with annotations for three inter-connected tasks, including image
captioning, safety rule violation visual question answering (VQA), and
construction element visual grounding. Our subsequent evaluation of current
state-of-the-art large pre-trained VLMs shows notable generalization abilities
in zero-shot and few-shot settings, while additional training is needed to make
them applicable to actual construction sites. This dataset allows researchers
to train and evaluate their own VLMs with new architectures and techniques,
providing a valuable benchmark for construction safety inspection.

</details>


### [67] [Can Multi-modal (reasoning) LLMs detect document manipulation?](https://arxiv.org/abs/2508.11021)
*Zisheng Liang,Kidus Zewde,Rudra Pratap Singh,Disha Patil,Zexi Chen,Jiayu Xue,Yao Yao,Yifei Chen,Qinzhe Liu,Simiao Ren*

Main category: cs.CV

TL;DR: The study evaluates multi-modal LLMs for detecting fraudulent documents, finding top models outperform traditional methods but highlights the need for task-specific fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Document fraud threatens industries needing secure documentation, requiring advanced detection methods.

Method: Benchmarked state-of-the-art multi-modal LLMs on a standard dataset, analyzing their reasoning and prompt optimization for fraud indicators.

Result: Top LLMs showed superior zero-shot generalization, but model size and reasoning didn't always correlate with accuracy.

Conclusion: Multi-modal LLMs hold promise for fraud detection, but task-specific tuning is crucial for optimal performance.

Abstract: Document fraud poses a significant threat to industries reliant on secure and
verifiable documentation, necessitating robust detection mechanisms. This study
investigates the efficacy of state-of-the-art multi-modal large language models
(LLMs)-including OpenAI O1, OpenAI 4o, Gemini Flash (thinking), Deepseek Janus,
Grok, Llama 3.2 and 4, Qwen 2 and 2.5 VL, Mistral Pixtral, and Claude 3.5 and
3.7 Sonnet-in detecting fraudulent documents. We benchmark these models against
each other and prior work on document fraud detection techniques using a
standard dataset with real transactional documents. Through prompt optimization
and detailed analysis of the models' reasoning processes, we evaluate their
ability to identify subtle indicators of fraud, such as tampered text,
misaligned formatting, and inconsistent transactional sums. Our results reveal
that top-performing multi-modal LLMs demonstrate superior zero-shot
generalization, outperforming conventional methods on out-of-distribution
datasets, while several vision LLMs exhibit inconsistent or subpar performance.
Notably, model size and advanced reasoning capabilities show limited
correlation with detection accuracy, suggesting task-specific fine-tuning is
critical. This study underscores the potential of multi-modal LLMs in enhancing
document fraud detection systems and provides a foundation for future research
into interpretable and scalable fraud mitigation strategies.

</details>


### [68] [MedSAMix: A Training-Free Model Merging Approach for Medical Image Segmentation](https://arxiv.org/abs/2508.11032)
*Yanwu Yang,Guinan Su,Jiesi Hu,Francesco Sammarco,Jonas Geiping,Thomas Wolfers*

Main category: cs.CV

TL;DR: MedSAMix is a training-free model merging method that combines generalist (e.g., SAM) and specialist (e.g., MedSAM) models for medical image segmentation, improving performance by optimizing layer-wise merging automatically.


<details>
  <summary>Details</summary>
Motivation: Existing fine-tuned medical segmentation models (e.g., MedSAM) face challenges like limited data, heterogeneity, and poor generalization. MedSAMix aims to address these by leveraging both generalist and specialist models.

Method: Proposes a zero-order optimization method for automatic layer-wise merging of models, with two regimes for domain-specificity and generalizability.

Result: Evaluations on 25 tasks show MedSAMix improves specialized task performance by 6.67% and multi-task evaluations by 4.37%.

Conclusion: MedSAMix effectively mitigates model bias and enhances both domain-specific accuracy and generalization in medical image segmentation.

Abstract: Universal medical image segmentation models have emerged as a promising
paradigm due to their strong generalizability across diverse tasks, showing
great potential for a wide range of clinical applications. This potential has
been partly driven by the success of general-purpose vision models such as the
Segment Anything Model (SAM), which has inspired the development of various
fine-tuned variants for medical segmentation tasks. However, fine-tuned
variants like MedSAM are trained on comparatively limited medical imaging data
that often suffers from heterogeneity, scarce annotations, and distributional
shifts. These challenges limit their ability to generalize across a wide range
of medical segmentation tasks. In this regard, we propose MedSAMix, a
training-free model merging method that integrates the strengths of both
generalist models (e.g., SAM) and specialist models (e.g., MedSAM) for medical
image segmentation. In contrast to traditional model merging approaches that
rely on manual configuration and often result in suboptimal outcomes, we
propose a zero-order optimization method to automatically discover optimal
layer-wise merging solutions. Furthermore, for clinical applications, we
develop two regimes to meet the demand of domain-specificity and
generalizability in different scenarios by single-task optimization and
multi-objective optimization respectively. Extensive evaluations on 25 medical
segmentation tasks demonstrate that MedSAMix effectively mitigates model bias
and consistently improves performance in both domain-specific accuracy and
generalization, achieving improvements of 6.67% on specialized tasks and 4.37%
on multi-task evaluations.

</details>


### [69] [Advancing 3D Scene Understanding with MV-ScanQA Multi-View Reasoning Evaluation and TripAlign Pre-training Dataset](https://arxiv.org/abs/2508.11058)
*Wentao Mo,Qingchao Chen,Yuxin Peng,Siyuan Huang,Yang Liu*

Main category: cs.CV

TL;DR: The paper introduces MV-ScanQA and TripAlign datasets to address limitations in 3D VL learning, focusing on multi-view reasoning and richer contextual alignments. It also proposes LEGO, a baseline method achieving SOTA results.


<details>
  <summary>Details</summary>
Motivation: Existing 3D VL datasets lack multi-view reasoning and contextual alignments, limiting deep 3D scene understanding.

Method: Introduces MV-ScanQA for multi-view QA and TripAlign for 2D-3D-language pre-training. Proposes LEGO, a method transferring 2D LVLM knowledge to 3D.

Result: LEGO pre-trained on TripAlign achieves SOTA on MV-ScanQA and existing benchmarks.

Conclusion: The datasets and LEGO method advance 3D VL learning by enabling multi-view reasoning and richer alignments.

Abstract: The advancement of 3D vision-language (3D VL) learning is hindered by several
limitations in existing 3D VL datasets: they rarely necessitate reasoning
beyond a close range of objects in single viewpoint, and annotations often link
instructions to single objects, missing richer contextual alignments between
multiple objects. This significantly curtails the development of models capable
of deep, multi-view 3D scene understanding over distant objects. To address
these challenges, we introduce MV-ScanQA, a novel 3D question answering dataset
where 68% of questions explicitly require integrating information from multiple
views (compared to less than 7% in existing datasets), thereby rigorously
testing multi-view compositional reasoning. To facilitate the training of
models for such demanding scenarios, we present TripAlign dataset, a
large-scale and low-cost 2D-3D-language pre-training corpus containing 1M <2D
view, set of 3D objects, text> triplets that explicitly aligns groups of
contextually related objects with text, providing richer, view-grounded
multi-object multimodal alignment signals than previous single-object
annotations. We further develop LEGO, a baseline method for the multi-view
reasoning challenge in MV-ScanQA, transferring knowledge from pre-trained 2D
LVLMs to 3D domain with TripAlign. Empirically, LEGO pre-trained on TripAlign
achieves state-of-the-art performance not only on the proposed MV-ScanQA, but
also on existing benchmarks for 3D dense captioning and question answering.
Datasets and code are available at
https://matthewdm0816.github.io/tripalign-mvscanqa.

</details>


### [70] [Data-Driven Abdominal Phenotypes of Type 2 Diabetes in Lean, Overweight, and Obese Cohorts](https://arxiv.org/abs/2508.11063)
*Lucas W. Remedios,Chloe Choe,Trent M. Schwartz,Dingjie Su,Gaurav Rudravaram,Chenyu Gao,Aravind R. Krishnan,Adam M. Saunders,Michael E. Kim,Shunxing Bao,Alvin C. Powers,Bennett A. Landman,John Virostko*

Main category: cs.CV

TL;DR: The study uses AI to analyze 3D clinical imaging for body composition signatures linked to type 2 diabetes, identifying consistent abdominal patterns across weight classes.


<details>
  <summary>Details</summary>
Motivation: To uncover detailed body composition signatures of type 2 diabetes risk and protection, as BMI alone is insufficient to explain disease presence in lean adults or absence in obese individuals.

Method: Applied a design involving segmentation of abdominal scans, classification via random forest, SHAP analysis for feature contribution, clustering, and linking back to anatomical differences, tested on a cohort of 1,728 individuals.

Result: Identified shared type 2 diabetes signatures (e.g., fatty skeletal muscle, visceral fat) across lean, overweight, and obese subgroups, with random-forest AUCs of 0.72-0.74.

Conclusion: Abdominal drivers of type 2 diabetes are consistent across weight classes, suggesting detailed body composition analysis can enhance risk assessment.

Abstract: Purpose: Although elevated BMI is a well-known risk factor for type 2
diabetes, the disease's presence in some lean adults and absence in others with
obesity suggests that detailed body composition may uncover abdominal
phenotypes of type 2 diabetes. With AI, we can now extract detailed
measurements of size, shape, and fat content from abdominal structures in 3D
clinical imaging at scale. This creates an opportunity to empirically define
body composition signatures linked to type 2 diabetes risk and protection using
large-scale clinical data. Approach: To uncover BMI-specific diabetic abdominal
patterns from clinical CT, we applied our design four times: once on the full
cohort (n = 1,728) and once on lean (n = 497), overweight (n = 611), and obese
(n = 620) subgroups separately. Briefly, our experimental design transforms
abdominal scans into collections of explainable measurements through
segmentation, classifies type 2 diabetes through a cross-validated random
forest, measures how features contribute to model-estimated risk or protection
through SHAP analysis, groups scans by shared model decision patterns
(clustering from SHAP) and links back to anatomical differences
(classification). Results: The random-forests achieved mean AUCs of 0.72-0.74.
There were shared type 2 diabetes signatures in each group; fatty skeletal
muscle, older age, greater visceral and subcutaneous fat, and a smaller or
fat-laden pancreas. Univariate logistic regression confirmed the direction of
14-18 of the top 20 predictors within each subgroup (p < 0.05). Conclusions:
Our findings suggest that abdominal drivers of type 2 diabetes may be
consistent across weight classes.

</details>


### [71] [HierOctFusion: Multi-scale Octree-based 3D Shape Generation via Part-Whole-Hierarchy Message Passing](https://arxiv.org/abs/2508.11106)
*Xinjie Gao,Bi'an Du,Wei Hu*

Main category: cs.CV

TL;DR: HierOctFusion is a part-aware multi-scale octree diffusion model for 3D content generation, improving efficiency and quality by leveraging semantic part hierarchies and hierarchical feature interaction.


<details>
  <summary>Details</summary>
Motivation: Existing methods treat 3D objects holistically, ignoring semantic part hierarchies and facing computational inefficiency. HierOctFusion addresses these by modeling sparse, hierarchical structures and injecting part-level information.

Method: The model uses a part-aware multi-scale octree diffusion approach with cross-attention conditioning to propagate semantic features hierarchically. A 3D dataset with part annotations is also constructed for training.

Result: HierOctFusion outperforms prior methods in shape quality and efficiency, demonstrating the benefits of part-aware hierarchical generation.

Conclusion: The proposed method advances 3D content generation by effectively combining part-level semantics with hierarchical modeling, offering improved generalization and computational efficiency.

Abstract: 3D content generation remains a fundamental yet challenging task due to the
inherent structural complexity of 3D data. While recent octree-based diffusion
models offer a promising balance between efficiency and quality through
hierarchical generation, they often overlook two key insights: 1) existing
methods typically model 3D objects as holistic entities, ignoring their
semantic part hierarchies and limiting generalization; and 2) holistic
high-resolution modeling is computationally expensive, whereas real-world
objects are inherently sparse and hierarchical, making them well-suited for
layered generation. Motivated by these observations, we propose HierOctFusion,
a part-aware multi-scale octree diffusion model that enhances hierarchical
feature interaction for generating fine-grained and sparse object structures.
Furthermore, we introduce a cross-attention conditioning mechanism that injects
part-level information into the generation process, enabling semantic features
to propagate effectively across hierarchical levels from parts to the whole.
Additionally, we construct a 3D dataset with part category annotations using a
pre-trained segmentation model to facilitate training and evaluation.
Experiments demonstrate that HierOctFusion achieves superior shape quality and
efficiency compared to prior methods.

</details>


### [72] [UWB-PostureGuard: A Privacy-Preserving RF Sensing System for Continuous Ergonomic Sitting Posture Monitoring](https://arxiv.org/abs/2508.11115)
*Haotang Li,Zhenyu Qi,Sen He,Kebin Peng,Sheng Tan,Yili Ren,Tomas Cerny,Jiyue Zhao,Zi Wang*

Main category: cs.CV

TL;DR: UWB-PostureGuard is a privacy-preserving UWB sensing system for monitoring ergonomic sitting posture, achieving 99.11% accuracy without cameras or wearables.


<details>
  <summary>Details</summary>
Motivation: Addressing privacy and comfort issues in traditional posture monitoring solutions like cameras and wearables.

Method: Uses commercial UWB devices with feature engineering and PoseGBDT for temporal posture pattern analysis.

Result: Achieves 99.11% accuracy in real-world tests across 10 participants and 19 postures, robust against environmental variables.

Conclusion: Offers a scalable, privacy-preserving mobile health solution for proactive ergonomic management.

Abstract: Improper sitting posture during prolonged computer use has become a
significant public health concern. Traditional posture monitoring solutions
face substantial barriers, including privacy concerns with camera-based systems
and user discomfort with wearable sensors. This paper presents
UWB-PostureGuard, a privacy-preserving ultra-wideband (UWB) sensing system that
advances mobile technologies for preventive health management through
continuous, contactless monitoring of ergonomic sitting posture. Our system
leverages commercial UWB devices, utilizing comprehensive feature engineering
to extract multiple ergonomic sitting posture features. We develop PoseGBDT to
effectively capture temporal dependencies in posture patterns, addressing
limitations of traditional frame-wise classification approaches. Extensive
real-world evaluation across 10 participants and 19 distinct postures
demonstrates exceptional performance, achieving 99.11% accuracy while
maintaining robustness against environmental variables such as clothing
thickness, additional devices, and furniture configurations. Our system
provides a scalable, privacy-preserving mobile health solution on existing
platforms for proactive ergonomic management, improving quality of life at low
costs.

</details>


### [73] [Residual-based Efficient Bidirectional Diffusion Model for Image Dehazing and Haze Generation](https://arxiv.org/abs/2508.11134)
*Bing Liu,Le Wang,Hao Liu,Mingming Liu*

Main category: cs.CV

TL;DR: Proposes a residual-based bidirectional diffusion model (RBDM) for translating between hazy and haze-free images, achieving efficient performance with 15 sampling steps.


<details>
  <summary>Details</summary>
Motivation: Existing deep dehazing methods lack bidirectional translation capability between hazy and haze-free images.

Method: Uses dual Markov chains for residual shifts, perturbs images at timesteps to predict noise, and employs a unified score function on patches.

Result: Achieves superior or comparable performance to state-of-the-art methods on synthetic and real-world datasets.

Conclusion: RBDM enables efficient, size-agnostic bidirectional transitions between hazy and haze-free images.

Abstract: Current deep dehazing methods only focus on removing haze from hazy images,
lacking the capability to translate between hazy and haze-free images. To
address this issue, we propose a residual-based efficient bidirectional
diffusion model (RBDM) that can model the conditional distributions for both
dehazing and haze generation. Firstly, we devise dual Markov chains that can
effectively shift the residuals and facilitate bidirectional smooth transitions
between them. Secondly, the RBDM perturbs the hazy and haze-free images at
individual timesteps and predicts the noise in the perturbed data to
simultaneously learn the conditional distributions. Finally, to enhance
performance on relatively small datasets and reduce computational costs, our
method introduces a unified score function learned on image patches instead of
entire images. Our RBDM successfully implements size-agnostic bidirectional
transitions between haze-free and hazy images with only 15 sampling steps.
Extensive experiments demonstrate that the proposed method achieves superior or
at least comparable performance to state-of-the-art methods on both synthetic
and real-world datasets.

</details>


### [74] [A Cross-Modal Rumor Detection Scheme via Contrastive Learning by Exploring Text and Image internal Correlations](https://arxiv.org/abs/2508.11141)
*Bin Ma,Yifei Zhang,Yongjin Xian,Qi Li,Linna Zhou,Gongxun Miao*

Main category: cs.CV

TL;DR: The paper introduces MICC, a cross-modal rumor detection method using contrastive learning to analyze text and multi-scale image relationships, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing rumor detection methods ignore image content and context-image relationships, missing critical information.

Method: Uses an SCLIP encoder for unified embeddings, a Cross-Modal Multi-Scale Alignment module, and a scale-aware fusion network to integrate features.

Result: Achieves significant performance improvements on real-world datasets.

Conclusion: MICC is effective and practical for rumor detection, leveraging multi-scale image and text correlations.

Abstract: Existing rumor detection methods often neglect the content within images as
well as the inherent relationships between contexts and images across different
visual scales, thereby resulting in the loss of critical information pertinent
to rumor identification. To address these issues, this paper presents a novel
cross-modal rumor detection scheme based on contrastive learning, namely the
Multi-scale Image and Context Correlation exploration algorithm (MICC).
Specifically, we design an SCLIP encoder to generate unified semantic
embeddings for text and multi-scale image patches through contrastive
pretraining, enabling their relevance to be measured via dot-product
similarity. Building upon this, a Cross-Modal Multi-Scale Alignment module is
introduced to identify image regions most relevant to the textual semantics,
guided by mutual information maximization and the information bottleneck
principle, through a Top-K selection strategy based on a cross-modal relevance
matrix constructed between the text and multi-scale image patches. Moreover, a
scale-aware fusion network is designed to integrate the highly correlated
multi-scale image features with global text features by assigning adaptive
weights to image regions based on their semantic importance and cross-modal
relevance. The proposed methodology has been extensively evaluated on two
real-world datasets. The experimental results demonstrate that it achieves a
substantial performance improvement over existing state-of-the-art approaches
in rumor detection, highlighting its effectiveness and potential for practical
applications.

</details>


### [75] [LEARN: A Story-Driven Layout-to-Image Generation Framework for STEM Instruction](https://arxiv.org/abs/2508.11153)
*Maoquan Zhang,Bisser Raytchev,Xiujuan Sun*

Main category: cs.CV

TL;DR: LEARN is a layout-aware diffusion framework for generating pedagogically aligned STEM illustrations, leveraging structured visual cues and narrative layouts to enhance learning.


<details>
  <summary>Details</summary>
Motivation: The framework aims to address fragmented attention in STEM education by producing coherent visual sequences that align with Bloom's taxonomy and reduce cognitive load.

Method: LEARN uses layout-conditioned generation, contrastive visual-semantic training, and prompt modulation to create structured, story-driven narratives.

Result: The framework generates illustrations that support mid-to-high-level reasoning and can integrate with multimodal systems and knowledge graphs.

Conclusion: LEARN introduces a novel generative AI approach for education, combining layout-based storytelling, semantic learning, and cognitive scaffolding.

Abstract: LEARN is a layout-aware diffusion framework designed to generate
pedagogically aligned illustrations for STEM education. It leverages a curated
BookCover dataset that provides narrative layouts and structured visual cues,
enabling the model to depict abstract and sequential scientific concepts with
strong semantic alignment. Through layout-conditioned generation, contrastive
visual-semantic training, and prompt modulation, LEARN produces coherent visual
sequences that support mid-to-high-level reasoning in line with Bloom's
taxonomy while reducing extraneous cognitive load as emphasized by Cognitive
Load Theory. By fostering spatially organized and story-driven narratives, the
framework counters fragmented attention often induced by short-form media and
promotes sustained conceptual focus. Beyond static diagrams, LEARN demonstrates
potential for integration with multimodal systems and curriculum-linked
knowledge graphs to create adaptive, exploratory educational content. As the
first generative approach to unify layout-based storytelling, semantic
structure learning, and cognitive scaffolding, LEARN represents a novel
direction for generative AI in education. The code and dataset will be released
to facilitate future research and practical deployment.

</details>


### [76] [Semi-supervised Image Dehazing via Expectation-Maximization and Bidirectional Brownian Bridge Diffusion Models](https://arxiv.org/abs/2508.11165)
*Bing Liu,Le Wang,Mingming Liu,Hao Liu,Rui Yao,Yong Zhou,Peng Liu,Tongqiang Xia*

Main category: cs.CV

TL;DR: Proposes EM-B3DM, a semi-supervised image dehazing method using Expectation-Maximization and Bidirectional Brownian Bridge Diffusion Models, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with real-world hazy images due to lack of paired data and robust priors.

Method: Uses a two-stage learning scheme: EM algorithm for decoupling distributions and Brownian Bridge diffusion for modeling correlations, followed by refinement with unpaired data. Introduces RDC block for gradient-level detail.

Result: Superior or comparable performance to state-of-the-art methods on synthetic and real-world datasets.

Conclusion: EM-B3DM effectively addresses dehazing challenges with limited paired data, leveraging semi-supervised learning and advanced modeling.

Abstract: Existing dehazing methods deal with real-world haze images with difficulty,
especially scenes with thick haze. One of the main reasons is the lack of
real-world paired data and robust priors. To avoid the costly collection of
paired hazy and clear images, we propose an efficient semi-supervised image
dehazing method via Expectation-Maximization and Bidirectional Brownian Bridge
Diffusion Models (EM-B3DM) with a two-stage learning scheme. In the first
stage, we employ the EM algorithm to decouple the joint distribution of paired
hazy and clear images into two conditional distributions, which are then
modeled using a unified Brownian Bridge diffusion model to directly capture the
structural and content-related correlations between hazy and clear images. In
the second stage, we leverage the pre-trained model and large-scale unpaired
hazy and clear images to further improve the performance of image dehazing.
Additionally, we introduce a detail-enhanced Residual Difference Convolution
block (RDC) to capture gradient-level information, significantly enhancing the
model's representation capability. Extensive experiments demonstrate that our
EM-B3DM achieves superior or at least comparable performance to
state-of-the-art methods on both synthetic and real-world datasets.

</details>


### [77] [VFM-Guided Semi-Supervised Detection Transformer for Source-Free Object Detection in Remote Sensing Images](https://arxiv.org/abs/2508.11167)
*Jianhong Han,Yupei Wang,Liang Chen*

Main category: cs.CV

TL;DR: VG-DETR is a semi-supervised framework for source-free object detection in remote sensing, leveraging vision foundation models to improve pseudo-label quality and feature robustness.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of source-free object detection (SFOD) in remote sensing, where noisy pseudo-labels and domain gaps hinder performance, by integrating vision foundation models (VFMs) to enhance pseudo-label reliability and feature extraction.

Method: VG-DETR introduces a VFM-guided pseudo-label mining strategy to assess and improve pseudo-label quality, and a dual-level VFM-guided alignment method for feature robustness at instance and image levels.

Result: VG-DETR achieves superior performance in source-free remote sensing detection tasks, mitigating pseudo-label noise and enhancing feature representation.

Conclusion: VG-DETR effectively bridges domain gaps in source-free remote sensing detection by leveraging VFMs, improving both pseudo-label quality and feature robustness.

Abstract: Unsupervised domain adaptation methods have been widely explored to bridge
domain gaps. However, in real-world remote-sensing scenarios, privacy and
transmission constraints often preclude access to source domain data, which
limits their practical applicability. Recently, Source-Free Object Detection
(SFOD) has emerged as a promising alternative, aiming at cross-domain
adaptation without relying on source data, primarily through a self-training
paradigm. Despite its potential, SFOD frequently suffers from training collapse
caused by noisy pseudo-labels, especially in remote sensing imagery with dense
objects and complex backgrounds. Considering that limited target domain
annotations are often feasible in practice, we propose a Vision
foundation-Guided DEtection TRansformer (VG-DETR), built upon a semi-supervised
framework for SFOD in remote sensing images. VG-DETR integrates a Vision
Foundation Model (VFM) into the training pipeline in a "free lunch" manner,
leveraging a small amount of labeled target data to mitigate pseudo-label noise
while improving the detector's feature-extraction capability. Specifically, we
introduce a VFM-guided pseudo-label mining strategy that leverages the VFM's
semantic priors to further assess the reliability of the generated
pseudo-labels. By recovering potentially correct predictions from
low-confidence outputs, our strategy improves pseudo-label quality and
quantity. In addition, a dual-level VFM-guided alignment method is proposed,
which aligns detector features with VFM embeddings at both the instance and
image levels. Through contrastive learning among fine-grained prototypes and
similarity matching between feature maps, this dual-level alignment further
enhances the robustness of feature representations against domain gaps.
Extensive experiments demonstrate that VG-DETR achieves superior performance in
source-free remote sensing detection tasks.

</details>


### [78] [Better Supervised Fine-tuning for VQA: Integer-Only Loss](https://arxiv.org/abs/2508.11170)
*Baihong Qian,Haotian Fan,Wenjie Liao,Yunqiu Wang,Tao Li,Junhui Cui*

Main category: cs.CV

TL;DR: IOVQA is a fine-tuning method for VLMs to improve video quality assessment by using integer labels and a target-mask loss strategy.


<details>
  <summary>Details</summary>
Motivation: Existing methods for visual content assessment are imprecise and inefficient, limiting model focus on key indicators.

Method: IOVQA constrains outputs to integers [10,50], converts decimal labels to integers, and uses a target-mask strategy for loss calculation.

Result: The method improves accuracy and consistency, ranking 3rd in the VQualA 2025 challenge.

Conclusion: Integer-only fine-tuning effectively optimizes VLMs for quantitative evaluation tasks.

Abstract: With the rapid advancement of vision language models(VLM), their ability to
assess visual content based on specific criteria and dimensions has become
increasingly critical for applications such as video-theme consistency
assessment and visual quality scoring. However, existing methods often suffer
from imprecise results and inefficient loss calculation, which limit the focus
of the model on key evaluation indicators. To address this, we propose
IOVQA(Integer-only VQA), a novel fine-tuning approach tailored for VLMs to
enhance their performance in video quality assessment tasks. The key innovation
of IOVQA lies in its label construction and its targeted loss calculation
mechanism. Specifically, during dataset curation, we constrain the model's
output to integers within the range of [10,50], ensuring numerical stability,
and convert decimal Overall_MOS to integer before using them as labels. We also
introduce a target-mask strategy: when computing the loss, only the first
two-digit-integer of the label is unmasked, forcing the model to learn the
critical components of the numerical evaluation. After fine-tuning the
Qwen2.5-VL model using the constructed dataset, experimental results
demonstrate that the proposed method significantly improves the model's
accuracy and consistency in the VQA task, ranking 3rd in VQualA 2025
GenAI-Bench AIGC Video Quality Assessment Challenge -- Track I. Our work
highlights the effectiveness of merely leaving integer labels during
fine-tuning, providing an effective idea for optimizing VLMs in quantitative
evaluation scenarios.

</details>


### [79] [Exploring the Tradeoff Between Diversity and Discrimination for Continuous Category Discovery](https://arxiv.org/abs/2508.11173)
*Ruobing Jiang,Yang Liu,Haobing Liu,Yanwei Yu,Chunyang Wang*

Main category: cs.CV

TL;DR: IDOD is a novel method for continuous category discovery (CCD) that addresses challenges like error accumulation and catastrophic forgetting by using diversity enrichment, joint novelty discovery, and orthogonality-based discrimination.


<details>
  <summary>Details</summary>
Motivation: Current CCD methods struggle with balancing novel class discovery and classification, accumulate errors, and require excessive storage for preventing forgetting.

Method: IDOD includes three modules: independent enrichment of diversity (contrastive loss), joint discovery of novelty (single-stage), and continuous increment by orthogonality (orthogonal prototypes and low-overhead replay).

Result: IDOD outperforms state-of-the-art methods on fine-grained datasets.

Conclusion: IDOD effectively mitigates CCD challenges with improved performance and reduced storage overhead.

Abstract: Continuous category discovery (CCD) aims to automatically discover novel
categories in continuously arriving unlabeled data. This is a challenging
problem considering that there is no number of categories and labels in the
newly arrived data, while also needing to mitigate catastrophic forgetting.
Most CCD methods cannot handle the contradiction between novel class discovery
and classification well. They are also prone to accumulate errors in the
process of gradually discovering novel classes. Moreover, most of them use
knowledge distillation and data replay to prevent forgetting, occupying more
storage space. To address these limitations, we propose Independence-based
Diversity and Orthogonality-based Discrimination (IDOD). IDOD mainly includes
independent enrichment of diversity module, joint discovery of novelty module,
and continuous increment by orthogonality module. In independent enrichment,
the backbone is trained separately using contrastive loss to avoid it focusing
only on features for classification. Joint discovery transforms multi-stage
novel class discovery into single-stage, reducing error accumulation impact.
Continuous increment by orthogonality module generates mutually orthogonal
prototypes for classification and prevents forgetting with lower space overhead
via representative representation replay. Experimental results show that on
challenging fine-grained datasets, our method outperforms the state-of-the-art
methods.

</details>


### [80] [Fine-Grained VLM Fine-tuning via Latent Hierarchical Adapter Learning](https://arxiv.org/abs/2508.11176)
*Yumiao Zhao,Bo Jiang,Yuhe Ding,Xiao Wang,Jin Tang,Bin Luo*

Main category: cs.CV

TL;DR: LatHAdapter improves few-shot classification by aligning visual and textual representations in hyperbolic space, leveraging latent semantic hierarchies.


<details>
  <summary>Details</summary>
Motivation: Existing adapters fail to capture one-to-many associations between categories and images and struggle with unknown categories.

Method: LatHAdapter uses learnable attribute prompts and hyperbolic space projection with hierarchical regularization to model semantic hierarchies.

Result: Outperforms other fine-tuning methods on four few-shot tasks, especially for known and unknown classes.

Conclusion: LatHAdapter effectively addresses alignment and generalization challenges in few-shot learning.

Abstract: Adapter-based approaches have garnered attention for fine-tuning pre-trained
Vision-Language Models (VLMs) on few-shot classification tasks. These methods
strive to develop a lightweight module that better aligns visual and (category)
textual representations, thereby enhancing performance on downstream few-shot
learning tasks. However, existing adapters generally learn/align (category)
textual-visual modalities via explicit spatial proximity in the underlying
embedding space, which i) fails to capture the inherent one-to-many
associations between categories and image samples and ii) struggles to
establish accurate associations between the unknown categories and images. To
address these issues, inspired by recent works on hyperbolic learning, we
develop a novel Latent Hierarchical Adapter (LatHAdapter) for fine-tuning VLMs
on downstream few-shot classification tasks. The core of LatHAdapter is to
exploit the latent semantic hierarchy of downstream training data and employ it
to provide richer, fine-grained guidance for the adapter learning process.
Specifically, LatHAdapter first introduces some learnable `attribute' prompts
as the bridge to align categories and images. Then, it projects the categories,
attribute prompts, and images within each batch in a hyperbolic space, and
employs hierarchical regularization to learn the latent semantic hierarchy of
them, thereby fully modeling the inherent one-to-many associations among
categories, learnable attributes, and image samples. Extensive experiments on
four challenging few-shot tasks show that the proposed LatHAdapter consistently
outperforms many other fine-tuning approaches, particularly in adapting known
classes and generalizing to unknown classes.

</details>


### [81] [Versatile Video Tokenization with Generative 2D Gaussian Splatting](https://arxiv.org/abs/2508.11183)
*Zhenghao Chen,Zicong Chen,Lei Liu,Yiming Wu,Dong Xu*

Main category: cs.CV

TL;DR: GVT introduces a Gaussian Video Transformer for versatile video tokenization, improving spatial adaptability and temporal redundancy reduction, achieving state-of-the-art performance in reconstruction and action recognition.


<details>
  <summary>Details</summary>
Motivation: Existing video tokenization methods lack versatility, over-encode low-information regions, and struggle with temporal redundancy. GVT aims to address these limitations.

Method: GVT uses a generative 2D Gaussian Splatting strategy with Spatio-Temporal Gaussian Embedding (STGE) for spatial adaptability and Gaussian Set Partitioning (GSP) for temporal versatility.

Result: GVT achieves superior video reconstruction, outperforms MAGVIT-v2 in action recognition, and delivers comparable compression performance.

Conclusion: GVT is a robust and versatile video tokenizer, advancing state-of-the-art in video processing tasks.

Abstract: Video tokenization procedure is critical for a wide range of video processing
tasks. Most existing approaches directly transform video into fixed-grid and
patch-wise tokens, which exhibit limited versatility. Spatially, uniformly
allocating a fixed number of tokens often leads to over-encoding in
low-information regions. Temporally, reducing redundancy remains challenging
without explicitly distinguishing between static and dynamic content. In this
work, we propose the Gaussian Video Transformer (GVT), a versatile video
tokenizer built upon a generative 2D Gaussian Splatting (2DGS) strategy. We
first extract latent rigid features from a video clip and represent them with a
set of 2D Gaussians generated by our proposed Spatio-Temporal Gaussian
Embedding (STGE) mechanism in a feed-forward manner. Such generative 2D
Gaussians not only enhance spatial adaptability by assigning higher (resp.,
lower) rendering weights to regions with higher (resp., lower) information
content during rasterization, but also improve generalization by avoiding
per-video optimization.To enhance the temporal versatility, we introduce a
Gaussian Set Partitioning (GSP) strategy that separates the 2D Gaussians into
static and dynamic sets, which explicitly model static content shared across
different time-steps and dynamic content specific to each time-step, enabling a
compact representation.We primarily evaluate GVT on the video reconstruction,
while also assessing its performance on action recognition and compression
using the UCF101, Kinetics, and DAVIS datasets. Extensive experiments
demonstrate that GVT achieves a state-of-the-art video reconstruction quality,
outperforms the baseline MAGVIT-v2 in action recognition, and delivers
comparable compression performance.

</details>


### [82] [CHARM3R: Towards Unseen Camera Height Robust Monocular 3D Detector](https://arxiv.org/abs/2508.11185)
*Abhinav Kumar,Yuliang Guo,Zhihao Zhang,Xinyu Huang,Liu Ren,Xiaoming Liu*

Main category: cs.CV

TL;DR: The paper addresses the challenge of monocular 3D object detectors struggling with unseen camera heights. It proposes CHARM3R, a model that improves generalization by averaging depth estimates, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing monocular 3D detectors perform poorly with unseen camera heights, highlighting the need for a robust solution.

Method: The study analyzes camera height impact on Mono3D models, identifies depth estimation as key, and introduces CHARM3R, which averages depth estimates.

Result: CHARM3R improves generalization to unseen heights by over 45%, outperforming existing methods on the CARLA dataset.

Conclusion: The proposed CHARM3R effectively mitigates camera height variations, advancing monocular 3D detection robustness.

Abstract: Monocular 3D object detectors, while effective on data from one ego camera
height, struggle with unseen or out-of-distribution camera heights. Existing
methods often rely on Plucker embeddings, image transformations or data
augmentation. This paper takes a step towards this understudied problem by
first investigating the impact of camera height variations on state-of-the-art
(SoTA) Mono3D models. With a systematic analysis on the extended CARLA dataset
with multiple camera heights, we observe that depth estimation is a primary
factor influencing performance under height variations. We mathematically prove
and also empirically observe consistent negative and positive trends in mean
depth error of regressed and ground-based depth models, respectively, under
camera height changes. To mitigate this, we propose Camera Height Robust
Monocular 3D Detector (CHARM3R), which averages both depth estimates within the
model. CHARM3R improves generalization to unseen camera heights by more than
$45\%$, achieving SoTA performance on the CARLA dataset. Codes and Models at
https://github.com/abhi1kumar/CHARM3R

</details>


### [83] [Generating Dialogues from Egocentric Instructional Videos for Task Assistance: Dataset, Method and Benchmark](https://arxiv.org/abs/2508.11192)
*Lavisha Aggarwal,Vikas Bahirwani,Lin Li,Andrea Colaco*

Main category: cs.CV

TL;DR: The paper introduces HowToDIV, a dataset created by transforming instructional videos into expert-novice dialogues using large language models, for procedural-task assistance.


<details>
  <summary>Details</summary>
Motivation: There's a lack of dialogue-video datasets for real-world task assistance, despite the need for expert knowledge in complex tasks.

Method: An automatic approach converts single-person instructional videos into two-person dialogues, aligned with video clips and fine-grained steps.

Result: The HowToDIV dataset includes 507 conversations, 6636 QA pairs, and 24 hours of video across diverse tasks.

Conclusion: The dataset and baseline performance using Gemma-3 model pave the way for future research in procedural-task assistance dialogues.

Abstract: Many everyday tasks ranging from fixing appliances, cooking recipes to car
maintenance require expert knowledge, especially when tasks are complex and
multi-step. Despite growing interest in AI agents, there is a scarcity of
dialogue-video datasets grounded for real world task assistance. In this paper,
we propose a simple yet effective approach that transforms single-person
instructional videos into task-guidance two-person dialogues, aligned with fine
grained steps and video-clips. Our fully automatic approach, powered by large
language models, offers an efficient alternative to the substantial cost and
effort required for human-assisted data collection. Using this technique, we
build HowToDIV, a large-scale dataset containing 507 conversations, 6636
question-answer pairs and 24 hours of videoclips across diverse tasks in
cooking, mechanics, and planting. Each session includes multi-turn conversation
where an expert teaches a novice user how to perform a task step by step, while
observing user's surrounding through a camera and microphone equipped wearable
device. We establish the baseline benchmark performance on HowToDIV dataset
through Gemma-3 model for future research on this new task of dialogues for
procedural-task assistance.

</details>


### [84] [UAV-VL-R1: Generalizing Vision-Language Models via Supervised Fine-Tuning and Multi-Stage GRPO for UAV Visual Reasoning](https://arxiv.org/abs/2508.11196)
*Jiajin Guan,Haibo Mei,Bonan Zhang,Dan Liu,Yuanshuang Fu,Yue Zhang*

Main category: cs.CV

TL;DR: UAV-VL-R1 is a lightweight vision-language model for aerial imagery, trained with hybrid SFT and RL, outperforming larger models in zero-shot accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: General-purpose VLMs underperform on UAV aerial imagery due to high resolution, complex semantics, and real-time constraints, limiting structured aerial reasoning.

Method: Proposes UAV-VL-R1, trained via supervised fine-tuning (SFT) and multi-stage RL using GRPO for structured reasoning. Introduces HRVQA-VL dataset for evaluation.

Result: Achieves 48.17% higher zero-shot accuracy than Qwen2-VL-2B-Instruct and outperforms its 72B variant. Requires only 3.9GB memory (FP16) and 2.5GB (INT8).

Conclusion: UAV-VL-R1 addresses aerial reasoning challenges with efficiency and accuracy, balancing semantic alignment and logical flexibility via hybrid training.

Abstract: Recent advances in vision-language models (VLMs) have demonstrated strong
generalization in natural image tasks. However, their performance often
degrades on unmanned aerial vehicle (UAV)-based aerial imagery, which features
high resolution, complex spatial semantics, and strict real-time constraints.
These challenges limit the applicability of general-purpose VLMs to structured
aerial reasoning tasks. To address these challenges, we propose UAV-VL-R1, a
lightweight VLM explicitly designed for aerial visual reasoning. It is trained
using a hybrid method that combines supervised fine-tuning (SFT) and
multi-stage reinforcement learning (RL). We leverage the group relative policy
optimization (GRPO) algorithm to promote structured and interpretable reasoning
through rule-guided rewards and intra-group policy alignment. To support model
training and evaluation, we introduce a high-resolution visual question
answering dataset named HRVQA-VL, which consists of 50,019 annotated samples
covering eight UAV-relevant reasoning tasks, including object counting,
transportation recognition, and spatial scene inference. Experimental results
show that UAV-VL-R1 achieves a 48.17% higher zero-shot accuracy than the
Qwen2-VL-2B-Instruct baseline and even outperforms its 72B-scale variant, which
is 36x larger, on multiple tasks. Ablation studies reveal that while SFT
improves semantic alignment, it may reduce reasoning diversity in mathematical
tasks. GRPO-based RL compensates for this limitation by enhancing logical
flexibility and the robustness of inference. Additionally, UAV-VL-R1 requires
only 3.9GB of memory under FP16 inference and can be quantized to 2.5GB with
INT8, supporting real-time deployment on resource-constrained UAV platforms.

</details>


### [85] [A Coarse-to-Fine Human Pose Estimation Method based on Two-stage Distillation and Progressive Graph Neural Network](https://arxiv.org/abs/2508.11212)
*Zhangjian Ji,Wenjin Zhang,Shaotong Qiao,Kai Feng,Yuhua Qian*

Main category: cs.CV

TL;DR: A novel two-stage knowledge distillation framework improves lightweight human pose estimation by leveraging structural and contextual joint information.


<details>
  <summary>Details</summary>
Motivation: Existing pose estimation methods are computationally heavy; knowledge distillation can transfer accuracy to lightweight models but lacks contextual joint exploration.

Method: Proposes a coarse-to-fine two-stage distillation: first-stage uses joint structure loss for semantic knowledge transfer, second-stage refines poses with an Image-Guided Progressive GCN.

Result: Outperforms state-of-the-art methods on COCO keypoint and CrowdPose datasets, with notable gains on complex CrowdPose.

Conclusion: The framework effectively balances accuracy and efficiency, especially in challenging scenarios.

Abstract: Human pose estimation has been widely applied in the human-centric
understanding and generation, but most existing state-of-the-art human pose
estimation methods require heavy computational resources for accurate
predictions. In order to obtain an accurate, robust yet lightweight human pose
estimator, one feasible way is to transfer pose knowledge from a powerful
teacher model to a less-parameterized student model by knowledge distillation.
However, the traditional knowledge distillation framework does not fully
explore the contextual information among human joints. Thus, in this paper, we
propose a novel coarse-to-fine two-stage knowledge distillation framework for
human pose estimation. In the first-stage distillation, we introduce the human
joints structure loss to mine the structural information among human joints so
as to transfer high-level semantic knowledge from the teacher model to the
student model. In the second-stage distillation, we utilize an Image-Guided
Progressive Graph Convolutional Network (IGP-GCN) to refine the initial human
pose obtained from the first-stage distillation and supervise the training of
the IGP-GCN in the progressive way by the final output pose of teacher model.
The extensive experiments on the benchmark dataset: COCO keypoint and CrowdPose
datasets, show that our proposed method performs favorably against lots of the
existing state-of-the-art human pose estimation methods, especially for the
more complex CrowdPose dataset, the performance improvement of our model is
more significant.

</details>


### [86] [A CLIP-based Uncertainty Modal Modeling (UMM) Framework for Pedestrian Re-Identification in Autonomous Driving](https://arxiv.org/abs/2508.11218)
*Jialin Li,Shuqi Wu,Ning Wang*

Main category: cs.CV

TL;DR: A lightweight Uncertainty Modal Modeling (UMM) framework is proposed for pedestrian ReID in autonomous driving, addressing missing modalities and computational constraints.


<details>
  <summary>Details</summary>
Motivation: Challenges in ReID due to uncertain/missing input modalities and computational limits of pre-trained models in resource-constrained environments.

Method: UMM integrates a multimodal token mapper, synthetic modality augmentation, and cross-modal cue interactive learner, leveraging CLIP for efficient fusion.

Result: UMM achieves robustness, generalization, and efficiency under uncertain modality conditions.

Conclusion: UMM offers a scalable, practical solution for pedestrian ReID in autonomous driving.

Abstract: Re-Identification (ReID) is a critical technology in intelligent perception
systems, especially within autonomous driving, where onboard cameras must
identify pedestrians across views and time in real-time to support safe
navigation and trajectory prediction. However, the presence of uncertain or
missing input modalities--such as RGB, infrared, sketches, or textual
descriptions--poses significant challenges to conventional ReID approaches.
While large-scale pre-trained models offer strong multimodal semantic modeling
capabilities, their computational overhead limits practical deployment in
resource-constrained environments. To address these challenges, we propose a
lightweight Uncertainty Modal Modeling (UMM) framework, which integrates a
multimodal token mapper, synthetic modality augmentation strategy, and
cross-modal cue interactive learner. Together, these components enable unified
feature representation, mitigate the impact of missing modalities, and extract
complementary information across different data types. Additionally, UMM
leverages CLIP's vision-language alignment ability to fuse multimodal inputs
efficiently without extensive finetuning. Experimental results demonstrate that
UMM achieves strong robustness, generalization, and computational efficiency
under uncertain modality conditions, offering a scalable and practical solution
for pedestrian re-identification in autonomous driving scenarios.

</details>


### [87] [FantasyTalking2: Timestep-Layer Adaptive Preference Optimization for Audio-Driven Portrait Animation](https://arxiv.org/abs/2508.11255)
*MengChao Wang,Qiang Wang,Fan Jiang,Mu Xu*

Main category: cs.CV

TL;DR: The paper introduces Talking-Critic and TLPO to improve audio-driven portrait animation by aligning with fine-grained human preferences, using a reward model and a novel optimization framework.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to align with fine-grained human preferences due to conflicting objectives and lack of annotated datasets.

Method: Proposes Talking-Critic (a reward model) and TLPO (a framework for aligning preferences), leveraging a new dataset (Talking-NSQ).

Result: Talking-Critic outperforms existing methods, and TLPO improves lip-sync, motion, and visual quality.

Conclusion: The approach effectively aligns portrait animation with human preferences, demonstrating superior performance.

Abstract: Recent advances in audio-driven portrait animation have demonstrated
impressive capabilities. However, existing methods struggle to align with
fine-grained human preferences across multiple dimensions, such as motion
naturalness, lip-sync accuracy, and visual quality. This is due to the
difficulty of optimizing among competing preference objectives, which often
conflict with one another, and the scarcity of large-scale, high-quality
datasets with multidimensional preference annotations. To address these, we
first introduce Talking-Critic, a multimodal reward model that learns
human-aligned reward functions to quantify how well generated videos satisfy
multidimensional expectations. Leveraging this model, we curate Talking-NSQ, a
large-scale multidimensional human preference dataset containing 410K
preference pairs. Finally, we propose Timestep-Layer adaptive multi-expert
Preference Optimization (TLPO), a novel framework for aligning diffusion-based
portrait animation models with fine-grained, multidimensional preferences. TLPO
decouples preferences into specialized expert modules, which are then fused
across timesteps and network layers, enabling comprehensive, fine-grained
enhancement across all dimensions without mutual interference. Experiments
demonstrate that Talking-Critic significantly outperforms existing methods in
aligning with human preference ratings. Meanwhile, TLPO achieves substantial
improvements over baseline models in lip-sync accuracy, motion naturalness, and
visual quality, exhibiting superior performance in both qualitative and
quantitative evaluations. Ours project page:
https://fantasy-amap.github.io/fantasy-talking2/

</details>


### [88] [Generalized Decoupled Learning for Enhancing Open-Vocabulary Dense Perception](https://arxiv.org/abs/2508.11256)
*Junjie Wang,Keyu Chen,Yulin Li,Bin Chen,Hengshuang Zhao,Xiaojuan Qi,Zhuotao Tian*

Main category: cs.CV

TL;DR: DeCLIP enhances CLIP by decoupling self-attention into content and context features, improving local discriminability and spatial consistency for open-vocabulary dense perception tasks.


<details>
  <summary>Details</summary>
Motivation: Current dense perception tasks are limited by predefined categories, and VLMs like CLIP underperform due to poor local feature representation.

Method: DeCLIP decouples self-attention into content and context features, enhancing the latter with semantic correlations from VFMs and object integrity cues from diffusion models. Content features are aligned with image crop representations and constrained by region correlations.

Result: DeCLIP achieves state-of-the-art performance in tasks like 2D detection, segmentation, 3D instance segmentation, video instance segmentation, and 6D object pose estimation.

Conclusion: DeCLIP provides a robust framework for open-vocabulary dense perception, addressing limitations of existing VLMs.

Abstract: Dense visual perception tasks have been constrained by their reliance on
predefined categories, limiting their applicability in real-world scenarios
where visual concepts are unbounded. While Vision-Language Models (VLMs) like
CLIP have shown promise in open-vocabulary tasks, their direct application to
dense perception often leads to suboptimal performance due to limitations in
local feature representation. In this work, we present our observation that
CLIP's image tokens struggle to effectively aggregate information from
spatially or semantically related regions, resulting in features that lack
local discriminability and spatial consistency. To address this issue, we
propose DeCLIP, a novel framework that enhances CLIP by decoupling the
self-attention module to obtain ``content'' and ``context'' features
respectively. \revise{The context features are enhanced by jointly distilling
semantic correlations from Vision Foundation Models (VFMs) and object integrity
cues from diffusion models, thereby enhancing spatial consistency. In parallel,
the content features are aligned with image crop representations and
constrained by region correlations from VFMs to improve local discriminability.
Extensive experiments demonstrate that DeCLIP establishes a solid foundation
for open-vocabulary dense perception, consistently achieving state-of-the-art
performance across a broad spectrum of tasks, including 2D detection and
segmentation, 3D instance segmentation, video instance segmentation, and 6D
object pose estimation.} Code is available at
https://github.com/xiaomoguhz/DeCLIP

</details>


### [89] [Vision-Language Models display a strong gender bias](https://arxiv.org/abs/2508.11262)
*Aiswarya Konavoor,Raj Abhijit Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.CV

TL;DR: The study investigates gender-linked associations in vision-language models (VLM) by analyzing embeddings of face images and occupational/activity phrases, revealing subtle biases not captured by standard metrics.


<details>
  <summary>Details</summary>
Motivation: To uncover and quantify subtle gender biases in VLMs, which align images and text but may encode social stereotypes.

Method: A dataset of 220 face images (split by perceived gender) and 150 statements across six labor categories was used. Image and text embeddings were compared via cosine similarity, with bootstrap confidence intervals and a label-swap null model for validation.

Result: The study provides a detailed map of gender associations in VLMs, showing biases in occupational and activity-related embeddings, along with uncertainty measures.

Conclusion: The framework offers a robust method to evaluate gender bias in VLMs, highlighting the need for bias mitigation in such models.

Abstract: Vision-language models (VLM) align images and text in a shared representation
space that is useful for retrieval and zero-shot transfer. Yet, this alignment
can encode and amplify social stereotypes in subtle ways that are not obvious
from standard accuracy metrics. In this study, we test whether the contrastive
vision-language encoder exhibits gender-linked associations when it places
embeddings of face images near embeddings of short phrases that describe
occupations and activities. We assemble a dataset of 220 face photographs split
by perceived binary gender and a set of 150 unique statements distributed
across six categories covering emotional labor, cognitive labor, domestic
labor, technical labor, professional roles, and physical labor. We compute
unit-norm image embeddings for every face and unit-norm text embeddings for
every statement, then define a statement-level association score as the
difference between the mean cosine similarity to the male set and the mean
cosine similarity to the female set, where positive values indicate stronger
association with the male set and negative values indicate stronger association
with the female set. We attach bootstrap confidence intervals by resampling
images within each gender group, aggregate by category with a separate
bootstrap over statements, and run a label-swap null model that estimates the
level of mean absolute association we would expect if no gender structure were
present. The outcome is a statement-wise and category-wise map of gender
associations in a contrastive vision-language space, accompanied by
uncertainty, simple sanity checks, and a robust gender bias evaluation
framework.

</details>


### [90] [Domain-aware Category-level Geometry Learning Segmentation for 3D Point Clouds](https://arxiv.org/abs/2508.11265)
*Pei He,Lingling Li,Licheng Jiao,Ronghua Shang,Fang Liu,Shuang Wang,Xu Liu,Wenping Ma*

Main category: cs.CV

TL;DR: A framework for domain-generalized 3D segmentation using category-level geometry learning to address domain shift by focusing on invariant geometric features.


<details>
  <summary>Details</summary>
Motivation: Current methods for domain generalization in 3D segmentation ignore category-level distribution and alignment, limiting their effectiveness in unseen environments.

Method: Proposes Category-level Geometry Embedding (CGE) for fine-grained geometric properties and Geometric Consistent Learning (GCL) to align embeddings and simulate latent 3D distributions.

Result: Achieves competitive segmentation accuracy compared to state-of-the-art domain-generalized point cloud methods.

Conclusion: The framework effectively improves generalization by leveraging category-level geometric features and alignment.

Abstract: Domain generalization in 3D segmentation is a critical challenge in deploying
models to unseen environments. Current methods mitigate the domain shift by
augmenting the data distribution of point clouds. However, the model learns
global geometric patterns in point clouds while ignoring the category-level
distribution and alignment. In this paper, a category-level geometry learning
framework is proposed to explore the domain-invariant geometric features for
domain generalized 3D semantic segmentation. Specifically, Category-level
Geometry Embedding (CGE) is proposed to perceive the fine-grained geometric
properties of point cloud features, which constructs the geometric properties
of each class and couples geometric embedding to semantic learning. Secondly,
Geometric Consistent Learning (GCL) is proposed to simulate the latent 3D
distribution and align the category-level geometric embeddings, allowing the
model to focus on the geometric invariant information to improve
generalization. Experimental results verify the effectiveness of the proposed
method, which has very competitive segmentation accuracy compared with the
state-of-the-art domain generalized point cloud methods.

</details>


### [91] [Enhancing Supervised Composed Image Retrieval via Reasoning-Augmented Representation Engineering](https://arxiv.org/abs/2508.11272)
*Jun Li,Kai Li,Shaoguo Liu,Tingting Gao*

Main category: cs.CV

TL;DR: Proposes PMTFR for Composed Image Retrieval (CIR), combining Pyramid Matching Model and Training-Free Refinement to improve performance without additional training.


<details>
  <summary>Details</summary>
Motivation: Existing CIR methods require extra training or complex designs, limiting efficiency and applicability.

Method: Uses Pyramid Patcher for multi-granular visual understanding and injects CoT representations into LVLMs for refinement.

Result: PMTFR outperforms state-of-the-art methods in supervised CIR tasks.

Conclusion: The framework offers a simple yet effective solution for CIR, with public code availability.

Abstract: Composed Image Retrieval (CIR) presents a significant challenge as it
requires jointly understanding a reference image and a modified textual
instruction to find relevant target images. Some existing methods attempt to
use a two-stage approach to further refine retrieval results. However, this
often requires additional training of a ranking model. Despite the success of
Chain-of-Thought (CoT) techniques in reducing training costs for language
models, their application in CIR tasks remains limited -- compressing visual
information into text or relying on elaborate prompt designs. Besides, existing
works only utilize it for zero-shot CIR, as it is challenging to achieve
satisfactory results in supervised CIR with a well-trained model. In this work,
we proposed a framework that includes the Pyramid Matching Model with
Training-Free Refinement (PMTFR) to address these challenges. Through a simple
but effective module called Pyramid Patcher, we enhanced the Pyramid Matching
Model's understanding of visual information at different granularities.
Inspired by representation engineering, we extracted representations from COT
data and injected them into the LVLMs. This approach allowed us to obtain
refined retrieval scores in the Training-Free Refinement paradigm without
relying on explicit textual reasoning, further enhancing performance. Extensive
experiments on CIR benchmarks demonstrate that PMTFR surpasses state-of-the-art
methods in supervised CIR tasks. The code will be made public.

</details>


### [92] [Probing the Representational Power of Sparse Autoencoders in Vision Models](https://arxiv.org/abs/2508.11277)
*Matthew Lyle Olson,Musashi Hinck,Neale Ratzlaff,Changbai Li,Phillip Howard,Vasudev Lal,Shao-Yen Tseng*

Main category: cs.CV

TL;DR: SAEs are evaluated for vision models, showing semantic meaning, improved generalization, and controllable generation across architectures.


<details>
  <summary>Details</summary>
Motivation: SAEs are popular for interpreting LLMs but understudied in vision. This work explores their potential in visual tasks.

Method: Extensive evaluation of SAEs on vision models (embedding, multi-modal LMMs, diffusion models) using image-based tasks.

Result: SAE features are meaningful, improve OOD generalization, enable controllable generation, and reveal shared representations in multi-modal LLMs.

Conclusion: SAEs show strong potential for interpretability, generalization, and steerability in vision models.

Abstract: Sparse Autoencoders (SAEs) have emerged as a popular tool for interpreting
the hidden states of large language models (LLMs). By learning to reconstruct
activations from a sparse bottleneck layer, SAEs discover interpretable
features from the high-dimensional internal representations of LLMs. Despite
their popularity with language models, SAEs remain understudied in the visual
domain. In this work, we provide an extensive evaluation the representational
power of SAEs for vision models using a broad range of image-based tasks. Our
experimental results demonstrate that SAE features are semantically meaningful,
improve out-of-distribution generalization, and enable controllable generation
across three vision model architectures: vision embedding models, multi-modal
LMMs and diffusion models. In vision embedding models, we find that learned SAE
features can be used for OOD detection and provide evidence that they recover
the ontological structure of the underlying model. For diffusion models, we
demonstrate that SAEs enable semantic steering through text encoder
manipulation and develop an automated pipeline for discovering
human-interpretable attributes. Finally, we conduct exploratory experiments on
multi-modal LLMs, finding evidence that SAE features reveal shared
representations across vision and language modalities. Our study provides a
foundation for SAE evaluation in vision models, highlighting their strong
potential improving interpretability, generalization, and steerability in the
visual domain.

</details>


### [93] [Unifying Scale-Aware Depth Prediction and Perceptual Priors for Monocular Endoscope Pose Estimation and Tissue Reconstruction](https://arxiv.org/abs/2508.11282)
*Muzammil Khan,Enzo Kerkhof,Matteo Fusaglia,Koert Kuhlmann,Theo Ruers,Françoise J. Siepel*

Main category: cs.CV

TL;DR: A unified framework for monocular endoscopic tissue reconstruction integrates depth prediction and perceptual refinement to address challenges like depth ambiguity and tissue deformation, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Enhancing monocular minimally invasive surgeries by improving endoscope pose estimation and 3D tissue surface reconstruction despite challenges like depth ambiguity and tissue deformation.

Method: Combines scale-aware depth prediction (MAPIS-Depth module) with temporally-constrained perceptual refinement (RAFT and LPIPS), optimised by WEMA-RTDL for accurate registration, followed by volumetric fusion.

Result: Demonstrates robustness and superiority over state-of-the-art methods in evaluations on HEVD and SCARED datasets.

Conclusion: The framework effectively addresses key challenges in monocular endoscopic reconstruction, offering improved accuracy and reliability for surgical navigation.

Abstract: Accurate endoscope pose estimation and 3D tissue surface reconstruction
significantly enhances monocular minimally invasive surgical procedures by
enabling accurate navigation and improved spatial awareness. However, monocular
endoscope pose estimation and tissue reconstruction face persistent challenges,
including depth ambiguity, physiological tissue deformation, inconsistent
endoscope motion, limited texture fidelity, and a restricted field of view. To
overcome these limitations, a unified framework for monocular endoscopic tissue
reconstruction that integrates scale-aware depth prediction with
temporally-constrained perceptual refinement is presented. This framework
incorporates a novel MAPIS-Depth module, which leverages Depth Pro for robust
initialisation and Depth Anything for efficient per-frame depth prediction, in
conjunction with L-BFGS-B optimisation, to generate pseudo-metric depth
estimates. These estimates are temporally refined by computing pixel
correspondences using RAFT and adaptively blending flow-warped frames based on
LPIPS perceptual similarity, thereby reducing artefacts arising from
physiological tissue deformation and motion. To ensure accurate registration of
the synthesised pseudo-RGBD frames from MAPIS-Depth, a novel WEMA-RTDL module
is integrated, optimising both rotation and translation. Finally, truncated
signed distance function-based volumetric fusion and marching cubes are applied
to extract a comprehensive 3D surface mesh. Evaluations on HEVD and SCARED,
with ablation and comparative analyses, demonstrate the framework's robustness
and superiority over state-of-the-art methods.

</details>


### [94] [TimeMachine: Fine-Grained Facial Age Editing with Identity Preservation](https://arxiv.org/abs/2508.11284)
*Yilin Mi,Qixin Yan,Zheng-Peng Duan,Chunle Guo,Hubery Yin,Hao Liu,Chen Li,Chongyi Li*

Main category: cs.CV

TL;DR: TimeMachine is a diffusion-based framework for fine-grained facial age editing while preserving identity, using high-precision age injection and an Age Classifier Guidance module.


<details>
  <summary>Details</summary>
Motivation: Fine-grained age editing without altering identity is challenging, and existing methods lack precision and control.

Method: Proposes TimeMachine with age injection in multi-cross attention and an Age Classifier Guidance module for latent space age prediction. Introduces the HFFA dataset for training.

Result: Achieves state-of-the-art performance in fine-grained age editing with identity preservation.

Conclusion: TimeMachine effectively addresses the challenge of precise age editing while maintaining identity, validated by experiments.

Abstract: With the advancement of generative models, facial image editing has made
significant progress. However, achieving fine-grained age editing while
preserving personal identity remains a challenging task.In this paper, we
propose TimeMachine, a novel diffusion-based framework that achieves accurate
age editing while keeping identity features unchanged. To enable fine-grained
age editing, we inject high-precision age information into the multi-cross
attention module, which explicitly separates age-related and identity-related
features. This design facilitates more accurate disentanglement of age
attributes, thereby allowing precise and controllable manipulation of facial
aging.Furthermore, we propose an Age Classifier Guidance (ACG) module that
predicts age directly in the latent space, instead of performing denoising
image reconstruction during training. By employing a lightweight module to
incorporate age constraints, this design enhances age editing accuracy by
modest increasing training cost. Additionally, to address the lack of
large-scale, high-quality facial age datasets, we construct a HFFA dataset
(High-quality Fine-grained Facial-Age dataset) which contains one million
high-resolution images labeled with identity and facial attributes.
Experimental results demonstrate that TimeMachine achieves state-of-the-art
performance in fine-grained age editing while preserving identity consistency.

</details>


### [95] [Hyperspectral vs. RGB for Pedestrian Segmentation in Urban Driving Scenes: A Comparative Study](https://arxiv.org/abs/2508.11301)
*Jiarong Li,Imad Ali Shah,Enda Ward,Martin Glavin,Edward Jones,Brian Deegan*

Main category: cs.CV

TL;DR: The study explores hyperspectral imaging (HSI) for better pedestrian segmentation in automotive systems, outperforming RGB with optimized band selection methods.


<details>
  <summary>Details</summary>
Motivation: Pedestrian segmentation in RGB imaging suffers from metamerism, making pedestrians and backgrounds hard to distinguish, posing safety risks.

Method: Compared RGB with two HSI dimensionality-reduction methods (PCA and CSNR-JMIM) using three segmentation models (U-Net, DeepLabV3+, SegFormer).

Result: CSNR-JMIM improved pedestrian segmentation by 1.44% IoU and 2.18% F1-score, reducing false positives.

Conclusion: Optimal HSI band selection enhances pedestrian segmentation, proving valuable for safety-critical automotive applications.

Abstract: Pedestrian segmentation in automotive perception systems faces critical
safety challenges due to metamerism in RGB imaging, where pedestrians and
backgrounds appear visually indistinguishable.. This study investigates the
potential of hyperspectral imaging (HSI) for enhanced pedestrian segmentation
in urban driving scenarios using the Hyperspectral City v2 (H-City) dataset. We
compared standard RGB against two dimensionality-reduction approaches by
converting 128-channel HSI data into three-channel representations: Principal
Component Analysis (PCA) and optimal band selection using Contrast
Signal-to-Noise Ratio with Joint Mutual Information Maximization (CSNR-JMIM).
Three semantic segmentation models were evaluated: U-Net, DeepLabV3+, and
SegFormer. CSNR-JMIM consistently outperformed RGB with an average improvements
of 1.44% in Intersection over Union (IoU) and 2.18% in F1-score for pedestrian
segmentation. Rider segmentation showed similar gains with 1.43% IoU and 2.25%
F1-score improvements. These improved performance results from enhanced
spectral discrimination of optimally selected HSI bands effectively reducing
false positives. This study demonstrates robust pedestrian segmentation through
optimal HSI band selection, showing significant potential for safety-critical
automotive applications.

</details>


### [96] [Denoise-then-Retrieve: Text-Conditioned Video Denoising for Video Moment Retrieval](https://arxiv.org/abs/2508.11313)
*Weijia Liu,Jiuxin Cao,Bo Miao,Zhiheng Fu,Xuelin Zhu,Jiawei Ge,Bo Liu,Mehwish Nasim,Ajmal Mian*

Main category: cs.CV

TL;DR: The paper proposes DRNet, a denoise-then-retrieve paradigm for Video Moment Retrieval (VMR), which filters irrelevant clips before retrieval to improve accuracy.


<details>
  <summary>Details</summary>
Motivation: Current VMR methods encode all video clips, including irrelevant ones, disrupting multimodal alignment and hindering optimization.

Method: DRNet uses Text-Conditioned Denoising (TCD) to filter noisy clips and Text-Reconstruction Feedback (TRF) to align purified video representations with text embeddings.

Result: DRNet outperforms state-of-the-art methods on Charades-STA and QVHighlights datasets.

Conclusion: The denoise-then-retrieve paradigm is adaptable and can enhance existing VMR models.

Abstract: Current text-driven Video Moment Retrieval (VMR) methods encode all video
clips, including irrelevant ones, disrupting multimodal alignment and hindering
optimization. To this end, we propose a denoise-then-retrieve paradigm that
explicitly filters text-irrelevant clips from videos and then retrieves the
target moment using purified multimodal representations. Following this
paradigm, we introduce the Denoise-then-Retrieve Network (DRNet), comprising
Text-Conditioned Denoising (TCD) and Text-Reconstruction Feedback (TRF)
modules. TCD integrates cross-attention and structured state space blocks to
dynamically identify noisy clips and produce a noise mask to purify multimodal
video representations. TRF further distills a single query embedding from
purified video representations and aligns it with the text embedding, serving
as auxiliary supervision for denoising during training. Finally, we perform
conditional retrieval using text embeddings on purified video representations
for accurate VMR. Experiments on Charades-STA and QVHighlights demonstrate that
our approach surpasses state-of-the-art methods on all metrics. Furthermore,
our denoise-then-retrieve paradigm is adaptable and can be seamlessly
integrated into advanced VMR models to boost performance.

</details>


### [97] [Logic Unseen: Revealing the Logical Blindspots of Vision-Language Models](https://arxiv.org/abs/2508.11317)
*Yuchen Zhou,Jiayu Tang,Shuo Yang,Xiaoyan Xiao,Yuqin Dai,Wenhao Yang,Chao Gou,Xiaobo Xia,Tat-Seng Chua*

Main category: cs.CV

TL;DR: LogicBench is introduced to diagnose logical blindspots in Vision-Language Models (VLMs), revealing their limitations. LogicCLIP, a novel framework, improves logical understanding without compromising general alignment.


<details>
  <summary>Details</summary>
Motivation: VLMs like CLIP lack logical understanding, limiting reliability in practical applications.

Method: LogicBench evaluates VLMs on 50,000+ vision-language pairs across 9 logical categories. LogicCLIP enhances logical sensitivity via logic-aware data generation and contrastive learning.

Result: Existing VLMs perform 40+ accuracy points below humans in logical tasks. LogicCLIP outperforms baselines and maintains general alignment.

Conclusion: LogicBench and LogicCLIP advance VLM logical capabilities while preserving general performance.

Abstract: Vision-Language Models (VLMs), exemplified by CLIP, have emerged as
foundational for multimodal intelligence. However, their capacity for logical
understanding remains significantly underexplored, resulting in critical
''logical blindspots'' that limit their reliability in practical applications.
To systematically diagnose this, we introduce LogicBench, a comprehensive
benchmark with over 50,000 vision-language pairs across 9 logical categories
and 4 diverse scenarios: images, videos, anomaly detection, and medical
diagnostics. Our evaluation reveals that existing VLMs, even the
state-of-the-art ones, fall at over 40 accuracy points below human performance,
particularly in challenging tasks like Causality and Conditionality,
highlighting their reliance on surface semantics over critical logical
structures. To bridge this gap, we propose LogicCLIP, a novel training
framework designed to boost VLMs' logical sensitivity through advancements in
both data generation and optimization objectives. LogicCLIP utilizes
logic-aware data generation and a contrastive learning strategy that combines
coarse-grained alignment, a fine-grained multiple-choice objective, and a novel
logical structure-aware objective. Extensive experiments demonstrate
LogicCLIP's substantial improvements in logical comprehension across all
LogicBench domains, significantly outperforming baselines. Moreover, LogicCLIP
retains, and often surpasses, competitive performance on general
vision-language benchmarks, demonstrating that the enhanced logical
understanding does not come at the expense of general alignment. We believe
that LogicBench and LogicCLIP will be important resources for advancing VLM
logical capabilities.

</details>


### [98] [Delving into Dynamic Scene Cue-Consistency for Robust 3D Multi-Object Tracking](https://arxiv.org/abs/2508.11323)
*Haonan Zhang,Xinyao Wang,Boxi Wu,Tu Zheng,Wang Yunhua,Zheng Yang*

Main category: cs.CV

TL;DR: The paper proposes DSC-Track, a 3D multi-object tracking method for autonomous driving, focusing on cue-consistency to improve accuracy in crowded environments by leveraging stable spatial patterns.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle in crowded scenes due to overlooking geometric relationships between objects and susceptibility to irrelevant object interference.

Method: DSC-Track uses a spatiotemporal encoder with Point Pair Features (PPF), a cue-consistency transformer module, and a dynamic update mechanism for stable tracking.

Result: Achieves 73.2% and 70.3% AMOTA on nuScenes validation and test sets, outperforming existing methods.

Conclusion: DSC-Track effectively leverages spatial cues and cue-consistency for robust 3D multi-object tracking in autonomous driving.

Abstract: 3D multi-object tracking is a critical and challenging task in the field of
autonomous driving. A common paradigm relies on modeling individual object
motion, e.g., Kalman filters, to predict trajectories. While effective in
simple scenarios, this approach often struggles in crowded environments or with
inaccurate detections, as it overlooks the rich geometric relationships between
objects. This highlights the need to leverage spatial cues. However, existing
geometry-aware methods can be susceptible to interference from irrelevant
objects, leading to ambiguous features and incorrect associations. To address
this, we propose focusing on cue-consistency: identifying and matching stable
spatial patterns over time. We introduce the Dynamic Scene Cue-Consistency
Tracker (DSC-Track) to implement this principle. Firstly, we design a unified
spatiotemporal encoder using Point Pair Features (PPF) to learn discriminative
trajectory embeddings while suppressing interference. Secondly, our
cue-consistency transformer module explicitly aligns consistent feature
representations between historical tracks and current detections. Finally, a
dynamic update mechanism preserves salient spatiotemporal information for
stable online tracking. Extensive experiments on the nuScenes and Waymo Open
Datasets validate the effectiveness and robustness of our approach. On the
nuScenes benchmark, for instance, our method achieves state-of-the-art
performance, reaching 73.2% and 70.3% AMOTA on the validation and test sets,
respectively.

</details>


### [99] [Noise Matters: Optimizing Matching Noise for Diffusion Classifiers](https://arxiv.org/abs/2508.11330)
*Yanghao Wang,Long Chen*

Main category: cs.CV

TL;DR: The paper introduces NoOp, a noise optimization method for Diffusion Classifiers (DCs) to address noise instability, improving classification speed and performance by learning dataset-specific and image-specific "good noises."


<details>
  <summary>Details</summary>
Motivation: Existing Diffusion Classifiers (DCs) suffer from noise instability, requiring extensive noise sampling for stable performance, which slows classification. The paper aims to identify and optimize "good noises" to mitigate this issue.

Method: NoOp optimizes noises for DCs by ensuring Frequency Matching (dataset-specific noise optimization) and Spatial Matching (training a Meta-Network for image-specific noise offsets). The combined noise replaces random sampling.

Result: Experiments show NoOp effectively improves DC performance and speed by reducing noise instability, validated across multiple datasets.

Conclusion: NoOp successfully addresses noise instability in DCs by optimizing "good noises," enhancing both classification accuracy and efficiency.

Abstract: Although today's pretrained discriminative vision-language models (e.g.,
CLIP) have demonstrated strong perception abilities, such as zero-shot image
classification, they also suffer from the bag-of-words problem and spurious
bias. To mitigate these problems, some pioneering studies leverage powerful
generative models (e.g., pretrained diffusion models) to realize generalizable
image classification, dubbed Diffusion Classifier (DC). Specifically, by
randomly sampling a Gaussian noise, DC utilizes the differences of denoising
effects with different category conditions to classify categories.
Unfortunately, an inherent and notorious weakness of existing DCs is noise
instability: different random sampled noises lead to significant performance
changes. To achieve stable classification performance, existing DCs always
ensemble the results of hundreds of sampled noises, which significantly reduces
the classification speed. To this end, we firstly explore the role of noise in
DC, and conclude that: there are some ``good noises'' that can relieve the
instability. Meanwhile, we argue that these good noises should meet two
principles: Frequency Matching and Spatial Matching. Regarding both principles,
we propose a novel Noise Optimization method to learn matching (i.e., good)
noise for DCs: NoOp. For frequency matching, NoOp first optimizes a
dataset-specific noise: Given a dataset and a timestep t, optimize one randomly
initialized parameterized noise. For Spatial Matching, NoOp trains a
Meta-Network that adopts an image as input and outputs image-specific noise
offset. The sum of optimized noise and noise offset will be used in DC to
replace random noise. Extensive ablations on various datasets demonstrated the
effectiveness of NoOp.

</details>


### [100] [GANDiff FR: Hybrid GAN Diffusion Synthesis for Causal Bias Attribution in Face Recognition](https://arxiv.org/abs/2508.11334)
*Md Asgor Hossain Reaj,Rajan Das Gupta,Md Yeasin Rahat,Nafiz Fahad,Md Jawadul Hasan,Tze Hui Liew*

Main category: cs.CV

TL;DR: GANDiff FR is a synthetic framework combining StyleGAN3 and diffusion models to control demographic and environmental factors, enabling precise bias measurement and reduction in facial recognition systems.


<details>
  <summary>Details</summary>
Motivation: To address bias in facial recognition by providing a reproducible and rigorous method for measuring and reducing bias through controlled synthetic data generation.

Method: Unifies StyleGAN3 for identity-preserving generation with diffusion models for fine-grained attribute control (pose, illumination, expression). Generates 10,000 demographically balanced faces and benchmarks models (ArcFace, CosFace, AdaFace) under matched conditions.

Result: AdaFace reduces inter-group TPR disparity by 60%. Illumination accounts for 42% of residual bias. Strong synthetic-to-real transfer (r 0.85) is confirmed.

Conclusion: GANDiff FR establishes a reproducible standard for fairness auditing, with 3x more attribute-conditioned variants than pure GANs, despite 20% computational overhead. Code and data are released for transparency.

Abstract: We introduce GANDiff FR, the first synthetic framework that precisely
controls demographic and environmental factors to measure, explain, and reduce
bias with reproducible rigor. GANDiff FR unifies StyleGAN3-based
identity-preserving generation with diffusion-based attribute control, enabling
fine-grained manipulation of pose around 30 degrees, illumination (four
directions), and expression (five levels) under ceteris paribus conditions. We
synthesize 10,000 demographically balanced faces across five cohorts validated
for realism via automated detection (98.2%) and human review (89%) to isolate
and quantify bias drivers. Benchmarking ArcFace, CosFace, and AdaFace under
matched operating points shows AdaFace reduces inter-group TPR disparity by 60%
(2.5% vs. 6.3%), with illumination accounting for 42% of residual bias.
Cross-dataset evaluation on RFW, BUPT, and CASIA WebFace confirms strong
synthetic-to-real transfer (r 0.85). Despite around 20% computational overhead
relative to pure GANs, GANDiff FR yields three times more attribute-conditioned
variants, establishing a reproducible, regulation-aligned (EU AI Act) standard
for fairness auditing. Code and data are released to support transparent,
scalable bias evaluation.

</details>


### [101] [Index-Aligned Query Distillation for Transformer-based Incremental Object Detection](https://arxiv.org/abs/2508.11339)
*Mingxiao Ma,Shunyao Zhu,Guoliang Kang*

Main category: cs.CV

TL;DR: The paper introduces Index-Aligned Query Distillation (IAQD) to address catastrophic knowledge forgetting in transformer-based incremental object detection (IOD), outperforming previous methods like Hungarian Matching.


<details>
  <summary>Details</summary>
Motivation: To mitigate catastrophic knowledge forgetting in transformer-based IOD, where existing methods like Hungarian Matching fail to preserve old category knowledge due to inconsistent query matching.

Method: Proposes IAQD, which aligns queries by index instead of Hungarian Matching and focuses distillation on critical queries for old categories.

Result: IAQD effectively preserves old category knowledge while learning new ones, achieving state-of-the-art performance on benchmarks.

Conclusion: IAQD is a superior distillation approach for transformer-based IOD, addressing knowledge forgetting and enhancing model performance.

Abstract: Incremental object detection (IOD) aims to continuously expand the capability
of a model to detect novel categories while preserving its performance on
previously learned ones. When adopting a transformer-based detection model to
perform IOD, catastrophic knowledge forgetting may inevitably occur, meaning
the detection performance on previously learned categories may severely
degenerate. Previous typical methods mainly rely on knowledge distillation (KD)
to mitigate the catastrophic knowledge forgetting of transformer-based
detection models. Specifically, they utilize Hungarian Matching to build a
correspondence between the queries of the last-phase and current-phase
detection models and align the classifier and regressor outputs between matched
queries to avoid knowledge forgetting. However, we observe that in IOD task,
Hungarian Matching is not a good choice. With Hungarian Matching, the query of
the current-phase model may match different queries of the last-phase model at
different iterations during KD. As a result, the knowledge encoded in each
query may be reshaped towards new categories, leading to the forgetting of
previously encoded knowledge of old categories. Based on our observations, we
propose a new distillation approach named Index-Aligned Query Distillation
(IAQD) for transformer-based IOD. Beyond using Hungarian Matching, IAQD
establishes a correspondence between queries of the previous and current phase
models that have the same index. Moreover, we perform index-aligned
distillation only on partial queries which are critical for the detection of
previous categories. In this way, IAQD largely preserves the previous semantic
and spatial encoding capabilities without interfering with the learning of new
categories. Extensive experiments on representative benchmarks demonstrate that
IAQD effectively mitigates knowledge forgetting, achieving new state-of-the-art
performance.

</details>


### [102] [Cost-Effective Active Labeling for Data-Efficient Cervical Cell Classification](https://arxiv.org/abs/2508.11340)
*Yuanlin Liu,Zhihan Zhou,Mingqiang Wei,Youyi Song*

Main category: cs.CV

TL;DR: Proposes active labeling for cost-efficient cervical cell classification by leveraging classifier uncertainty to select beneficial images for labeling.


<details>
  <summary>Details</summary>
Motivation: Existing methods require expensive human cost for representative training datasets, making automated cervical cell classification impractical.

Method: Uses active labeling to select the most beneficial unlabeled images for labeling, based on classifier uncertainty, reducing human cost.

Result: The method enhances the training dataset's representativeness and is validated through empirical results.

Conclusion: Active labeling enables data-efficient cervical cell classification, optimizing human cost usage.

Abstract: Information on the number and category of cervical cells is crucial for the
diagnosis of cervical cancer. However, existing classification methods capable
of automatically measuring this information require the training dataset to be
representative, which consumes an expensive or even unaffordable human cost. We
herein propose active labeling that enables us to construct a representative
training dataset using a much smaller human cost for data-efficient cervical
cell classification. This cost-effective method efficiently leverages the
classifier's uncertainty on the unlabeled cervical cell images to accurately
select images that are most beneficial to label. With a fast estimation of the
uncertainty, this new algorithm exhibits its validity and effectiveness in
enhancing the representative ability of the constructed training dataset. The
extensive empirical results confirm its efficacy again in navigating the usage
of human cost, opening the avenue for data-efficient cervical cell
classification.

</details>


### [103] [Semantically Guided Adversarial Testing of Vision Models Using Language Models](https://arxiv.org/abs/2508.11341)
*Katarzyna Filus,Jorge M. Cruz-Duarte*

Main category: cs.CV

TL;DR: The paper introduces a semantics-guided framework for selecting target labels in adversarial attacks on vision models, leveraging cross-modal knowledge from pretrained models like BERT, TinyLLAMA, and CLIP. It outperforms static methods like WordNet, especially for distant class relationships.


<details>
  <summary>Details</summary>
Motivation: Existing target label selection methods lack interpretability, reproducibility, or flexibility, relying on randomness or static resources. The paper aims to improve this by using semantic relationships from pretrained models.

Method: Proposes a framework using pretrained language and vision-language models (BERT, TinyLLAMA, CLIP) to select semantically related labels for adversarial attacks. Evaluates on three vision models and five attack methods.

Result: Pretrained models consistently provide practical adversarial targets, surpassing static lexical databases like WordNet, particularly for distant class relationships. Static testing of labels offers preliminary effectiveness assessment.

Conclusion: Pretrained models are suitable for creating interpretable, standardized, and scalable adversarial benchmarks across architectures and datasets.

Abstract: In targeted adversarial attacks on vision models, the selection of the target
label is a critical yet often overlooked determinant of attack success. This
target label corresponds to the class that the attacker aims to force the model
to predict. Now, existing strategies typically rely on randomness, model
predictions, or static semantic resources, limiting interpretability,
reproducibility, or flexibility. This paper then proposes a semantics-guided
framework for adversarial target selection using the cross-modal knowledge
transfer from pretrained language and vision-language models. We evaluate
several state-of-the-art models (BERT, TinyLLAMA, and CLIP) as similarity
sources to select the most and least semantically related labels with respect
to the ground truth, forming best- and worst-case adversarial scenarios. Our
experiments on three vision models and five attack methods reveal that these
models consistently render practical adversarial targets and surpass static
lexical databases, such as WordNet, particularly for distant class
relationships. We also observe that static testing of target labels offers a
preliminary assessment of the effectiveness of similarity sources, \textit{a
priori} testing. Our results corroborate the suitability of pretrained models
for constructing interpretable, standardized, and scalable adversarial
benchmarks across architectures and datasets.

</details>


### [104] [Controlling Multimodal LLMs via Reward-guided Decoding](https://arxiv.org/abs/2508.11616)
*Oscar Mañas,Pierluca D'Oro,Koustuv Sinha,Adriana Romero-Soriano,Michal Drozdzal,Aishwarya Agrawal*

Main category: cs.CV

TL;DR: The paper introduces a reward-guided decoding method for Multimodal Large Language Models (MLLMs) to improve visual grounding, offering dynamic control over object precision and recall during inference.


<details>
  <summary>Details</summary>
Motivation: To adapt MLLMs for diverse user needs by enhancing their visual grounding capabilities through controlled decoding.

Method: Develops two reward models for object precision and recall, guiding MLLM decoding dynamically. Allows control over reward function importance and search breadth during inference.

Result: Outperforms existing hallucination mitigation methods and provides significant controllability over MLLM inference.

Conclusion: The proposed method effectively enhances MLLM adaptability and visual grounding, offering practical controllability for diverse applications.

Abstract: As Multimodal Large Language Models (MLLMs) gain widespread applicability, it
is becoming increasingly desirable to adapt them for diverse user needs. In
this paper, we study the adaptation of MLLMs through controlled decoding. To
achieve this, we introduce the first method for reward-guided decoding of MLLMs
and demonstrate its application in improving their visual grounding. Our method
involves building reward models for visual grounding and using them to guide
the MLLM's decoding process. Concretely, we build two separate reward models to
independently control the degree of object precision and recall in the model's
output. Our approach enables on-the-fly controllability of an MLLM's inference
process in two ways: first, by giving control over the relative importance of
each reward function during decoding, allowing a user to dynamically trade off
object precision for recall in image captioning tasks; second, by giving
control over the breadth of the search during decoding, allowing the user to
control the trade-off between the amount of test-time compute and the degree of
visual grounding. We evaluate our method on standard object hallucination
benchmarks, showing that it provides significant controllability over MLLM
inference, while consistently outperforming existing hallucination mitigation
methods.

</details>


### [105] [HOID-R1: Reinforcement Learning for Open-World Human-Object Interaction Detection Reasoning with Multimodal Large Language Model](https://arxiv.org/abs/2508.11350)
*Zhenhao Zhang,Hanqing Wang,Xiangyu Zeng,Ziyu Cheng,Jiaxin Liu,Haoyu Yan,Zhirui Liu,Kaiyang Ji,Tianxiang Gui,Ke Hu,Kangyi Chen,Yahao Fan,Mokai Pan*

Main category: cs.CV

TL;DR: HOID-R1 is a novel HOI detection framework combining CoT-guided SFT and GRPO in RL, outperforming existing methods in benchmarks and open-world generalization.


<details>
  <summary>Details</summary>
Motivation: Address the lack of 3D spatial understanding in current open-vocabulary HOI detection methods relying solely on large language models.

Method: Integrates CoT-guided SFT for reasoning and GRPO for multi-reward policy optimization, with an MLLM-as-a-judge mechanism to reduce hallucinations.

Result: Achieves state-of-the-art performance on HOI benchmarks and superior open-world generalization.

Conclusion: HOID-R1 effectively combines reasoning and RL for robust HOI detection, setting a new benchmark.

Abstract: Understanding and recognizing human-object interaction (HOI) is a pivotal
application in AR/VR and robotics. Recent open-vocabulary HOI detection
approaches depend exclusively on large language models for richer textual
prompts, neglecting their inherent 3D spatial understanding capabilities. To
address this shortcoming, we introduce HOID-R1, the first HOI detection
framework that integrates chain-of-thought (CoT) guided supervised fine-tuning
(SFT) with group relative policy optimization (GRPO) within a reinforcement
learning (RL) paradigm. Specifically, we initially apply SFT to imbue the model
with essential reasoning capabilities, forcing the model to articulate its
thought process in the output. Subsequently, we integrate GRPO to leverage
multi-reward signals for policy optimization, thereby enhancing alignment
across diverse modalities. To mitigate hallucinations in the CoT reasoning, we
introduce an "MLLM-as-a-judge" mechanism that supervises the CoT outputs,
further improving generalization. Extensive experiments show that HOID-R1
achieves state-of-the-art performance on HOI detection benchmarks and
outperforms existing methods in open-world generalization to novel scenarios.

</details>


### [106] [Leveraging the RETFound foundation model for optic disc segmentation in retinal images](https://arxiv.org/abs/2508.11354)
*Zhenyi Zhao,Muthu Rama Krishnan Mookiah,Emanuele Trucco*

Main category: cs.CV

TL;DR: RETFound, a foundation model for retinal images, is adapted for optic disc segmentation, outperforming state-of-the-art methods with minimal task-specific training.


<details>
  <summary>Details</summary>
Motivation: To explore RETFound's applicability beyond disease diagnosis, specifically for optic disc segmentation, a foundational task in retinal image analysis.

Method: Adapt RETFound for optic disc segmentation by training a head with a small number of task-specific examples.

Result: Achieves ~96% Dice score across multiple datasets, excelling in internal verification, domain generalization, and adaptation.

Conclusion: Demonstrates RETFound's versatility as a foundation model, challenging the need for task-specific architectures.

Abstract: RETFound is a well-known foundation model (FM) developed for fundus camera
and optical coherence tomography images. It has shown promising performance
across multiple datasets in diagnosing diseases, both eye-specific and
systemic, from retinal images. However, to our best knowledge, it has not been
used for other tasks. We present the first adaptation of RETFound for optic
disc segmentation, a ubiquitous and foundational task in retinal image
analysis. The resulting segmentation system outperforms state-of-the-art,
segmentation-specific baseline networks after training a head with only a very
modest number of task-specific examples. We report and discuss results with
four public datasets, IDRID, Drishti-GS, RIM-ONE-r3, and REFUGE, and a private
dataset, GoDARTS, achieving about 96% Dice consistently across all datasets.
Overall, our method obtains excellent performance in internal verification,
domain generalization and domain adaptation, and exceeds most of the
state-of-the-art baseline results. We discuss the results in the framework of
the debate about FMs as alternatives to task-specific architectures. The code
is available at: [link to be added after the paper is accepted]

</details>


### [107] [Does the Skeleton-Recall Loss Really Work?](https://arxiv.org/abs/2508.11374)
*Devansh Arora,Nitin Kumar,Sukrit Gupta*

Main category: cs.CV

TL;DR: The paper critically evaluates the Skeleton Recall Loss (SRL) for tubular structure segmentation, finding it does not outperform traditional baselines despite claims.


<details>
  <summary>Details</summary>
Motivation: To assess the effectiveness of topology-based loss functions like SRL in image segmentation, particularly for thin tubular structures.

Method: Theoretical analysis of SRL gradients and empirical comparison on tubular datasets, including those from the original SRL work.

Result: SRL-based models did not surpass traditional baseline models in performance.

Conclusion: Topology-based loss functions like SRL have limitations, providing insights for future segmentation model development.

Abstract: Image segmentation is an important and widely performed task in computer
vision. Accomplishing effective image segmentation in diverse settings often
requires custom model architectures and loss functions. A set of models that
specialize in segmenting thin tubular structures are topology
preservation-based loss functions. These models often utilize a pixel
skeletonization process claimed to generate more precise segmentation masks of
thin tubes and better capture the structures that other models often miss. One
such model, Skeleton Recall Loss (SRL) proposed by Kirchhoff et al.~\cite
{kirchhoff2024srl}, was stated to produce state-of-the-art results on benchmark
tubular datasets. In this work, we performed a theoretical analysis of the
gradients for the SRL loss. Upon comparing the performance of the proposed
method on some of the tubular datasets (used in the original work, along with
some additional datasets), we found that the performance of SRL-based
segmentation models did not exceed traditional baseline models. By providing
both a theoretical explanation and empirical evidence, this work critically
evaluates the limitations of topology-based loss functions, offering valuable
insights for researchers aiming to develop more effective segmentation models
for complex tubular structures.

</details>


### [108] [Unified Knowledge Distillation Framework: Fine-Grained Alignment and Geometric Relationship Preservation for Deep Face Recognition](https://arxiv.org/abs/2508.11376)
*Durgesh Mishra,Rishabh Uikey*

Main category: cs.CV

TL;DR: The paper proposes a unified knowledge distillation (KD) framework for face recognition models, combining instance-level and relational distillation to improve performance on edge devices.


<details>
  <summary>Details</summary>
Motivation: Traditional KD methods fail to capture fine-grained details and relational structures, limiting performance in face recognition tasks.

Method: The framework integrates two novel loss functions: Instance-Level Embedding Distillation (with dynamic hard mining) and Relation-Based Pairwise Similarity Distillation (using a memory bank and sample mining).

Result: The unified framework outperforms state-of-the-art KD methods on benchmark datasets and can even surpass teacher model accuracy.

Conclusion: The proposed approach effectively balances instance-level alignment and relational preservation, enhancing distillation for face recognition models.

Abstract: Knowledge Distillation is crucial for optimizing face recognition models for
deployment in computationally limited settings, such as edge devices.
Traditional KD methods, such as Raw L2 Feature Distillation or Feature
Consistency loss, often fail to capture both fine-grained instance-level
details and complex relational structures, leading to suboptimal performance.
We propose a unified approach that integrates two novel loss functions,
Instance-Level Embedding Distillation and Relation-Based Pairwise Similarity
Distillation. Instance-Level Embedding Distillation focuses on aligning
individual feature embeddings by leveraging a dynamic hard mining strategy,
thereby enhancing learning from challenging examples. Relation-Based Pairwise
Similarity Distillation captures relational information through pairwise
similarity relationships, employing a memory bank mechanism and a sample mining
strategy. This unified framework ensures both effective instance-level
alignment and preservation of geometric relationships between samples, leading
to a more comprehensive distillation process. Our unified framework outperforms
state-of-the-art distillation methods across multiple benchmark face
recognition datasets, as demonstrated by extensive experimental evaluations.
Interestingly, when using strong teacher networks compared to the student, our
unified KD enables the student to even surpass the teacher's accuracy.

</details>


### [109] [G-CUT3R: Guided 3D Reconstruction with Camera and Depth Prior Integration](https://arxiv.org/abs/2508.11379)
*Ramil Khafizov,Artem Komarichev,Ruslan Rakhimov,Peter Wonka,Evgeny Burnaev*

Main category: cs.CV

TL;DR: G-CUT3R enhances 3D scene reconstruction by integrating prior information like depth or camera data, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing feed-forward methods rely only on input images, missing out on auxiliary data often available in real-world scenarios.

Method: A lightweight modification to CUT3R adds dedicated encoders for each modality, fusing features with RGB tokens via zero convolution.

Result: Significant performance improvements in 3D reconstruction and multi-view tasks, showcasing effective prior utilization.

Conclusion: G-CUT3R flexibly integrates prior data, improving accuracy while maintaining compatibility with varying inputs.

Abstract: We introduce G-CUT3R, a novel feed-forward approach for guided 3D scene
reconstruction that enhances the CUT3R model by integrating prior information.
Unlike existing feed-forward methods that rely solely on input images, our
method leverages auxiliary data, such as depth, camera calibrations, or camera
positions, commonly available in real-world scenarios. We propose a lightweight
modification to CUT3R, incorporating a dedicated encoder for each modality to
extract features, which are fused with RGB image tokens via zero convolution.
This flexible design enables seamless integration of any combination of prior
information during inference. Evaluated across multiple benchmarks, including
3D reconstruction and other multi-view tasks, our approach demonstrates
significant performance improvements, showing its ability to effectively
utilize available priors while maintaining compatibility with varying input
modalities.

</details>


### [110] [RMFAT: Recurrent Multi-scale Feature Atmospheric Turbulence Mitigator](https://arxiv.org/abs/2508.11409)
*Zhiming Liu,Nantheera Anantrasirichai*

Main category: cs.CV

TL;DR: RMFAT is a lightweight recurrent framework for efficient video restoration under atmospheric turbulence, outperforming existing methods in clarity and speed.


<details>
  <summary>Details</summary>
Motivation: Atmospheric turbulence degrades video quality, and current methods are computationally expensive, limiting real-time deployment.

Method: RMFAT uses a recurrent framework with two-frame input, multi-scale feature encoding/decoding, and temporal warping for spatial and temporal enhancement.

Result: RMFAT improves SSIM by 9% and reduces runtime fourfold compared to existing methods.

Conclusion: RMFAT is efficient and effective for real-time atmospheric turbulence suppression.

Abstract: Atmospheric turbulence severely degrades video quality by introducing
distortions such as geometric warping, blur, and temporal flickering, posing
significant challenges to both visual clarity and temporal consistency. Current
state-of-the-art methods are based on transformer and 3D architectures and
require multi-frame input, but their large computational cost and memory usage
limit real-time deployment, especially in resource-constrained scenarios. In
this work, we propose RMFAT: Recurrent Multi-scale Feature Atmospheric
Turbulence Mitigator, designed for efficient and temporally consistent video
restoration under AT conditions. RMFAT adopts a lightweight recurrent framework
that restores each frame using only two inputs at a time, significantly
reducing temporal window size and computational burden. It further integrates
multi-scale feature encoding and decoding with temporal warping modules at both
encoder and decoder stages to enhance spatial detail and temporal coherence.
Extensive experiments on synthetic and real-world atmospheric turbulence
datasets demonstrate that RMFAT not only outperforms existing methods in terms
of clarity restoration (with nearly a 9\% improvement in SSIM) but also
achieves significantly improved inference speed (more than a fourfold reduction
in runtime), making it particularly suitable for real-time atmospheric
turbulence suppression tasks.

</details>


### [111] [SelfAdapt: Unsupervised Domain Adaptation of Cell Segmentation Models](https://arxiv.org/abs/2508.11411)
*Fabian H. Reith,Jannik Franzen,Dinesh R. Palli,J. Lorenz Rumberger,Dagmar Kainmueller*

Main category: cs.CV

TL;DR: SelfAdapt enables label-free adaptation of pre-trained cell segmentation models, improving performance on diverse datasets without requiring annotated data.


<details>
  <summary>Details</summary>
Motivation: Generalist models like Cellpose degrade in performance on domains differing from their training data, and supervised fine-tuning requires scarce annotated data.

Method: SelfAdapt uses student-teacher augmentation consistency training with L2-SP regularization and label-free stopping criteria.

Result: Achieves up to 29.64% relative improvement in AP0.5 over baseline Cellpose on LiveCell and TissueNet datasets, even enhancing supervised fine-tuned models.

Conclusion: SelfAdapt is a practical, label-free solution for adapting cell segmentation models, released as an extension of Cellpose.

Abstract: Deep neural networks have become the go-to method for biomedical instance
segmentation. Generalist models like Cellpose demonstrate state-of-the-art
performance across diverse cellular data, though their effectiveness often
degrades on domains that differ from their training data. While supervised
fine-tuning can address this limitation, it requires annotated data that may
not be readily available. We propose SelfAdapt, a method that enables the
adaptation of pre-trained cell segmentation models without the need for labels.
Our approach builds upon student-teacher augmentation consistency training,
introducing L2-SP regularization and label-free stopping criteria. We evaluate
our method on the LiveCell and TissueNet datasets, demonstrating relative
improvements in AP0.5 of up to 29.64% over baseline Cellpose. Additionally, we
show that our unsupervised adaptation can further improve models that were
previously fine-tuned with supervision. We release SelfAdapt as an easy-to-use
extension of the Cellpose framework. The code for our method is publicly
available at https: //github.com/Kainmueller-Lab/self_adapt.

</details>


### [112] [Training-free Dimensionality Reduction via Feature Truncation: Enhancing Efficiency in Privacy-preserving Multi-Biometric Systems](https://arxiv.org/abs/2508.11419)
*Florian Bayer,Maximilian Russo,Christian Rathgeb*

Main category: cs.CV

TL;DR: Reducing multi-biometric template sizes via dimensionality reduction maintains accuracy and security while improving efficiency in Homomorphic Encryption processing.


<details>
  <summary>Details</summary>
Motivation: Addressing the computational challenges of Biometric Template Protection schemes, especially with Homomorphic Encryption, by leveraging multi-modal fusion and dimensionality reduction.

Method: Experiments on a virtual multi-biometric database using DNN-extracted features from face, fingerprint, and iris. Evaluated approaches are explainable, training-free, and generalizable.

Result: Template size reduced by 67% with no loss in Equal Error Rate (EER) compared to single-modality recognition.

Conclusion: Multi-modal fusion with dimensionality reduction enhances efficiency in encrypted processing without compromising biometric accuracy or security.

Abstract: Biometric recognition is widely used, making the privacy and security of
extracted templates a critical concern. Biometric Template Protection schemes,
especially those utilizing Homomorphic Encryption, introduce significant
computational challenges due to increased workload. Recent advances in deep
neural networks have enabled state-of-the-art feature extraction for face,
fingerprint, and iris modalities. The ubiquity and affordability of biometric
sensors further facilitate multi-modal fusion, which can enhance security by
combining features from different modalities. This work investigates the
biometric performance of reduced multi-biometric template sizes. Experiments
are conducted on an in-house virtual multi-biometric database, derived from
DNN-extracted features for face, fingerprint, and iris, using the FRGC, MCYT,
and CASIA databases. The evaluated approaches are (i) explainable and
straightforward to implement under encryption, (ii) training-free, and (iii)
capable of generalization. Dimensionality reduction of feature vectors leads to
fewer operations in the Homomorphic Encryption (HE) domain, enabling more
efficient encrypted processing while maintaining biometric accuracy and
security at a level equivalent to or exceeding single-biometric recognition.
Our results demonstrate that, by fusing feature vectors from multiple
modalities, template size can be reduced by 67 % with no loss in Equal Error
Rate (EER) compared to the best-performing single modality.

</details>


### [113] [ImagiDrive: A Unified Imagination-and-Planning Framework for Autonomous Driving](https://arxiv.org/abs/2508.11428)
*Jingyu Li,Bozhou Zhang,Xin Jin,Jiankang Deng,Xiatian Zhu,Li Zhang*

Main category: cs.CV

TL;DR: ImagiDrive integrates Vision-Language Models (VLMs) and Driving World Models (DWMs) for autonomous driving, combining interpretable action prediction with realistic scene generation for improved planning.


<details>
  <summary>Details</summary>
Motivation: Autonomous driving demands contextual understanding and predictive reasoning. VLMs and DWMs address different aspects, but their integration is understudied despite its potential.

Method: ImagiDrive combines a VLM-based driving agent with a DWM-based scene imaginer, forming an iterative imagination-and-planning loop with early stopping and trajectory selection for efficiency.

Result: Tests on nuScenes and NAVSIM show ImagiDrive outperforms previous methods in robustness and performance under open-loop and closed-loop conditions.

Conclusion: ImagiDrive successfully integrates VLMs and DWMs, addressing key challenges and demonstrating superior performance in autonomous driving scenarios.

Abstract: Autonomous driving requires rich contextual comprehension and precise
predictive reasoning to navigate dynamic and complex environments safely.
Vision-Language Models (VLMs) and Driving World Models (DWMs) have
independently emerged as powerful recipes addressing different aspects of this
challenge. VLMs provide interpretability and robust action prediction through
their ability to understand multi-modal context, while DWMs excel in generating
detailed and plausible future driving scenarios essential for proactive
planning. Integrating VLMs with DWMs is an intuitive, promising, yet
understudied strategy to exploit the complementary strengths of accurate
behavioral prediction and realistic scene generation. Nevertheless, this
integration presents notable challenges, particularly in effectively connecting
action-level decisions with high-fidelity pixel-level predictions and
maintaining computational efficiency. In this paper, we propose ImagiDrive, a
novel end-to-end autonomous driving framework that integrates a VLM-based
driving agent with a DWM-based scene imaginer to form a unified
imagination-and-planning loop. The driving agent predicts initial driving
trajectories based on multi-modal inputs, guiding the scene imaginer to
generate corresponding future scenarios. These imagined scenarios are
subsequently utilized to iteratively refine the driving agent's planning
decisions. To address efficiency and predictive accuracy challenges inherent in
this integration, we introduce an early stopping mechanism and a trajectory
selection strategy. Extensive experimental validation on the nuScenes and
NAVSIM datasets demonstrates the robustness and superiority of ImagiDrive over
previous alternatives under both open-loop and closed-loop conditions.

</details>


### [114] [Remove360: Benchmarking Residuals After Object Removal in 3D Gaussian Splatting](https://arxiv.org/abs/2508.11431)
*Simona Kocour,Assia Benbihi,Torsten Sattler*

Main category: cs.CV

TL;DR: A benchmark and evaluation framework for measuring semantic residuals in 3D Gaussian Splatting after object removal, highlighting limitations in current techniques.


<details>
  <summary>Details</summary>
Motivation: To understand and measure unintended semantic traces left after object removal in 3D reconstructions, critical for privacy and editable scenes.

Method: Introduces Remove360 dataset and framework to evaluate semantic residuals in diverse indoor/outdoor scenes, assessing downstream model inference.

Result: Current methods preserve semantic information despite visual geometry removal, revealing limitations in real-world complexity handling.

Conclusion: Robust solutions are needed for effective object removal in complex scenes, as current techniques fall short.

Abstract: Understanding what semantic information persists after object removal is
critical for privacy-preserving 3D reconstruction and editable scene
representations. In this work, we introduce a novel benchmark and evaluation
framework to measure semantic residuals, the unintended semantic traces left
behind, after object removal in 3D Gaussian Splatting. We conduct experiments
across a diverse set of indoor and outdoor scenes, showing that current methods
can preserve semantic information despite the absence of visual geometry. We
also release Remove360, a dataset of pre/post-removal RGB images and
object-level masks captured in real-world environments. While prior datasets
have focused on isolated object instances, Remove360 covers a broader and more
complex range of indoor and outdoor scenes, enabling evaluation of object
removal in the context of full-scene representations. Given ground truth images
of a scene before and after object removal, we assess whether we can truly
eliminate semantic presence, and if downstream models can still infer what was
removed. Our findings reveal critical limitations in current 3D object removal
techniques and underscore the need for more robust solutions capable of
handling real-world complexity. The evaluation framework is available at
github.com/spatial-intelligence-ai/Remove360.git. Data are available at
huggingface.co/datasets/simkoc/Remove360.

</details>


### [115] [MM-R1: Unleashing the Power of Unified Multimodal Large Language Models for Personalized Image Generation](https://arxiv.org/abs/2508.11433)
*Qian Liang,Yujia Wu,Kuncheng Li,Jiwei Wei,Shiyuan He,Jinyu Guo,Ning Xie*

Main category: cs.CV

TL;DR: MM-R1 is a framework using cross-modal Chain-of-Thought reasoning to enable personalized image generation with unified MLLMs, avoiding data-intensive fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Aligning MLLMs with personalized image generation is challenging due to subject-specific, data-heavy fine-tuning requirements.

Method: MM-R1 integrates X-CoT reasoning and GRPO for visual reasoning and generation, grounding subject concepts and aligning generation.

Result: MM-R1 achieves high subject fidelity and text alignment in zero-shot personalized image generation.

Conclusion: MM-R1 successfully leverages unified MLLMs for scalable, high-quality personalized image generation without fine-tuning.

Abstract: Multimodal Large Language Models (MLLMs) with unified architectures excel
across a wide range of vision-language tasks, yet aligning them with
personalized image generation remains a significant challenge. Existing methods
for MLLMs are frequently subject-specific, demanding a data-intensive
fine-tuning process for every new subject, which limits their scalability. In
this paper, we introduce MM-R1, a framework that integrates a cross-modal
Chain-of-Thought (X-CoT) reasoning strategy to unlock the inherent potential of
unified MLLMs for personalized image generation. Specifically, we structure
personalization as an integrated visual reasoning and generation process: (1)
grounding subject concepts by interpreting and understanding user-provided
images and contextual cues, and (2) generating personalized images conditioned
on both the extracted subject representations and user prompts. To further
enhance the reasoning capability, we adopt Grouped Reward Proximal Policy
Optimization (GRPO) to explicitly align the generation. Experiments demonstrate
that MM-R1 unleashes the personalization capability of unified MLLMs to
generate images with high subject fidelity and strong text alignment in a
zero-shot manner.

</details>


### [116] [Inside Knowledge: Graph-based Path Generation with Explainable Data Augmentation and Curriculum Learning for Visual Indoor Navigation](https://arxiv.org/abs/2508.11446)
*Daniel Airinei,Elena Burceanu,Marius Leordeanu*

Main category: cs.CV

TL;DR: The paper introduces a vision-based deep learning approach for indoor navigation, using a novel graph-based path generation method, explainable data augmentation, and curriculum learning. It avoids reliance on GPS, special sensors, or maps, and includes a new dataset and Android app.


<details>
  <summary>Details</summary>
Motivation: Indoor navigation is challenging due to poor GPS access and complex existing solutions. The goal is to create an efficient, real-time, and easily deployable method using only visual input.

Method: A deep learning approach based on visual input, employing a graph-based path generation method, explainable data augmentation, and curriculum learning. The method automates data collection, annotation, and training.

Result: A novel large-scale dataset from a shopping mall with annotated frames for direction prediction. The method works without special sensors, markers, maps, or internet access. An Android app was also developed.

Conclusion: The proposed vision-based approach is efficient, deployable, and avoids dependencies on external infrastructure. The dataset, code, and app are made publicly available.

Abstract: Indoor navigation is a difficult task, as it generally comes with poor GPS
access, forcing solutions to rely on other sources of information. While
significant progress continues to be made in this area, deployment to
production applications is still lacking, given the complexity and additional
requirements of current solutions. Here, we introduce an efficient, real-time
and easily deployable deep learning approach, based on visual input only, that
can predict the direction towards a target from images captured by a mobile
device. Our technical approach, based on a novel graph-based path generation
method, combined with explainable data augmentation and curriculum learning,
includes contributions that make the process of data collection, annotation and
training, as automatic as possible, efficient and robust. On the practical
side, we introduce a novel largescale dataset, with video footage inside a
relatively large shopping mall, in which each frame is annotated with the
correct next direction towards different specific target destinations.
Different from current methods, ours relies solely on vision, avoiding the need
of special sensors, additional markers placed along the path, knowledge of the
scene map or internet access. We also created an easy to use application for
Android, which we plan to make publicly available. We make all our data and
code available along with visual demos on our project site

</details>


### [117] [Data-Driven Deepfake Image Detection Method -- The 2024 Global Deepfake Image Detection Challenge](https://arxiv.org/abs/2508.11464)
*Xiaoya Zhu,Yibing Nan,Shiguo Lian*

Main category: cs.CV

TL;DR: The paper discusses using Swin Transformer V2-B for Deepfake image detection, employing data augmentation to enhance model generalization, achieving excellence in competition.


<details>
  <summary>Details</summary>
Motivation: Address challenges posed by Deepfake technology in digital security by improving detection accuracy.

Method: Uses Swin Transformer V2-B classification network with online data augmentation and offline sample generation for diverse training.

Result: Achieved excellence in Deepfake image detection competition.

Conclusion: The approach effectively detects Deepfake images, demonstrating the potential of advanced models and data augmentation in digital security.

Abstract: With the rapid development of technology in the field of AI, deepfake
technology has emerged as a double-edged sword. It has not only created a large
amount of AI-generated content but also posed unprecedented challenges to
digital security. The task of the competition is to determine whether a face
image is a Deepfake image and output its probability score of being a Deepfake
image. In the image track competition, our approach is based on the Swin
Transformer V2-B classification network. And online data augmentation and
offline sample generation methods are employed to enrich the diversity of
training samples and increase the generalization ability of the model. Finally,
we got the award of excellence in Deepfake image detection.

</details>


### [118] [CoFi: A Fast Coarse-to-Fine Few-Shot Pipeline for Glomerular Basement Membrane Segmentation](https://arxiv.org/abs/2508.11469)
*Hongjin Fang,Daniel Reisenbüchler,Kenji Ikemura,Mert R. Sabuncu,Yihe Yang,Ruining Deng*

Main category: cs.CV

TL;DR: CoFi is a coarse-to-fine few-shot segmentation pipeline for GBM in EM images, reducing annotation burden while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Supervised deep learning requires extensive annotations, and few-shot methods struggle with fine details in GBM segmentation.

Method: CoFi uses a lightweight network for coarse segmentation, then refines it with SAM using morphology-aware point prompts.

Result: Achieves 74.54% Dice coefficient and 1.9 FPS, balancing accuracy and speed.

Conclusion: CoFi is efficient, accurate, and suitable for clinical renal pathology applications.

Abstract: Accurate segmentation of the glomerular basement membrane (GBM) in electron
microscopy (EM) images is fundamental for quantifying membrane thickness and
supporting the diagnosis of various kidney diseases. While supervised deep
learning approaches achieve high segmentation accuracy, their reliance on
extensive pixel-level annotation renders them impractical for clinical
workflows. Few-shot learning can reduce this annotation burden but often
struggles to capture the fine structural details necessary for GBM analysis. In
this study, we introduce CoFi, a fast and efficient coarse-to-fine few-shot
segmentation pipeline designed for GBM delineation in EM images. CoFi first
trains a lightweight neural network using only three annotated images to
produce an initial coarse segmentation mask. This mask is then automatically
processed to generate high-quality point prompts with morphology-aware pruning,
which are subsequently used to guide SAM in refining the segmentation. The
proposed method achieved exceptional GBM segmentation performance, with a Dice
coefficient of 74.54% and an inference speed of 1.9 FPS. We demonstrate that
CoFi not only alleviates the annotation and computational burdens associated
with conventional methods, but also achieves accurate and reliable segmentation
results. The pipeline's speed and annotation efficiency make it well-suited for
research and hold strong potential for clinical applications in renal
pathology. The pipeline is publicly available at:
https://github.com/ddrrnn123/CoFi.

</details>


### [119] [TACR-YOLO: A Real-time Detection Framework for Abnormal Human Behaviors Enhanced with Coordinate and Task-Aware Representations](https://arxiv.org/abs/2508.11478)
*Xinyi Yin,Wenbo Yuan,Xuecheng Wu,Liangyu Fu,Danlei Huang*

Main category: cs.CV

TL;DR: TACR-YOLO is a real-time framework for Abnormal Human Behavior Detection (AHBD) that addresses challenges like small objects, task conflicts, and multi-scale fusion. It achieves 91.92% mAP on the PABD dataset.


<details>
  <summary>Details</summary>
Motivation: The need for effective AHBD under special scenarios, overcoming limitations of YOLO-based methods in handling small objects, task conflicts, and multi-scale fusion.

Method: Introduces Coordinate Attention Module for small objects, Task-Aware Attention Module for classification-regression conflicts, Strengthen Neck Network for multi-scale fusion, optimized Anchor Box sizes with K-means, and DIoU-Loss for bounding box regression.

Result: Achieves 91.92% mAP on the PABD dataset (8,529 samples, 4 behavior categories) with competitive speed and robustness.

Conclusion: TACR-YOLO advances AHBD under special scenarios, with ablation studies validating each improvement's contribution.

Abstract: Abnormal Human Behavior Detection (AHBD) under special scenarios is becoming
increasingly crucial. While YOLO-based detection methods excel in real-time
tasks, they remain hindered by challenges including small objects, task
conflicts, and multi-scale fusion in AHBD. To tackle them, we propose
TACR-YOLO, a new real-time framework for AHBD. We introduce a Coordinate
Attention Module to enhance small object detection, a Task-Aware Attention
Module to deal with classification-regression conflicts, and a Strengthen Neck
Network for refined multi-scale fusion, respectively. In addition, we optimize
Anchor Box sizes using K-means clustering and deploy DIoU-Loss to improve
bounding box regression. The Personnel Anomalous Behavior Detection (PABD)
dataset, which includes 8,529 samples across four behavior categories, is also
presented. Extensive experimental results indicate that TACR-YOLO achieves
91.92% mAP on PABD, with competitive speed and robustness. Ablation studies
highlight the contribution of each improvement. This work provides new insights
for abnormal behavior detection under special scenarios, advancing its
progress.

</details>


### [120] [OpenConstruction: A Systematic Synthesis of Open Visual Datasets for Data-Centric Artificial Intelligence in Construction Monitoring](https://arxiv.org/abs/2508.11482)
*Ruoxin Xiong,Yanyu Wang,Jiannan Cai,Kaijian Liu,Yuansheng Zhu,Pingbo Tang,Nora El-Gohary*

Main category: cs.CV

TL;DR: The paper reviews 51 visual datasets in construction (2005-2024), categorizing them by data fundamentals, modalities, annotations, and applications, and introduces OpenConstruction, an open-source catalog. It highlights gaps and proposes a FAIR-principled roadmap for future data infrastructure.


<details>
  <summary>Details</summary>
Motivation: To address the lack of systematic review and categorization of visual datasets in construction, which limits AI/ML applications due to variability in dataset quality and representativeness.

Method: Conducted an extensive search of academic databases and open-data platforms, categorizing 51 datasets using a structured schema covering data fundamentals, modalities, annotations, and applications.

Result: Identified critical gaps in existing datasets and introduced OpenConstruction, an open-source catalog. Proposed a FAIR-based roadmap for future data infrastructure.

Conclusion: The study supports data-centric AI/ML advancements in construction by systematizing dataset knowledge and guiding future infrastructure development.

Abstract: The construction industry increasingly relies on visual data to support
Artificial Intelligence (AI) and Machine Learning (ML) applications for site
monitoring. High-quality, domain-specific datasets, comprising images, videos,
and point clouds, capture site geometry and spatiotemporal dynamics, including
the location and interaction of objects, workers, and materials. However,
despite growing interest in leveraging visual datasets, existing resources vary
widely in sizes, data modalities, annotation quality, and representativeness of
real-world construction conditions. A systematic review to categorize their
data characteristics and application contexts is still lacking, limiting the
community's ability to fully understand the dataset landscape, identify
critical gaps, and guide future directions toward more effective, reliable, and
scalable AI applications in construction. To address this gap, this study
conducts an extensive search of academic databases and open-data platforms,
yielding 51 publicly available visual datasets that span the 2005-2024 period.
These datasets are categorized using a structured data schema covering (i) data
fundamentals (e.g., size and license), (ii) data modalities (e.g., RGB and
point cloud), (iii) annotation frameworks (e.g., bounding boxes), and (iv)
downstream application domains (e.g., progress tracking). This study
synthesizes these findings into an open-source catalog, OpenConstruction,
supporting data-driven method development. Furthermore, the study discusses
several critical limitations in the existing construction dataset landscape and
presents a roadmap for future data infrastructure anchored in the Findability,
Accessibility, Interoperability, and Reusability (FAIR) principles. By
reviewing the current landscape and outlining strategic priorities, this study
supports the advancement of data-centric solutions in the construction sector.

</details>


### [121] [CineTrans: Learning to Generate Videos with Cinematic Transitions via Masked Diffusion Models](https://arxiv.org/abs/2508.11484)
*Xiaoxue Wu,Bingjie Gao,Yu Qiao,Yaohui Wang,Xinyuan Chen*

Main category: cs.CV

TL;DR: CineTrans is a novel framework for generating coherent multi-shot videos with cinematic transitions, leveraging a new dataset and a mask-based control mechanism to improve shot transitions.


<details>
  <summary>Details</summary>
Motivation: Existing video synthesis models struggle with multi-shot video generation, particularly in achieving stable and cinematic shot transitions.

Method: The paper introduces CineTrans, which uses a multi-shot video-text dataset (Cine250K) and a mask-based control mechanism derived from attention maps in diffusion models to enable precise shot transitions.

Result: CineTrans outperforms existing baselines in generating cinematic multi-shot videos with stable transitions, as validated by specialized evaluation metrics.

Conclusion: CineTrans advances multi-shot video generation by addressing transition instability and adhering to film editing styles, demonstrating superior performance over current methods.

Abstract: Despite significant advances in video synthesis, research into multi-shot
video generation remains in its infancy. Even with scaled-up models and massive
datasets, the shot transition capabilities remain rudimentary and unstable,
largely confining generated videos to single-shot sequences. In this work, we
introduce CineTrans, a novel framework for generating coherent multi-shot
videos with cinematic, film-style transitions. To facilitate insights into the
film editing style, we construct a multi-shot video-text dataset Cine250K with
detailed shot annotations. Furthermore, our analysis of existing video
diffusion models uncovers a correspondence between attention maps in the
diffusion model and shot boundaries, which we leverage to design a mask-based
control mechanism that enables transitions at arbitrary positions and transfers
effectively in a training-free setting. After fine-tuning on our dataset with
the mask mechanism, CineTrans produces cinematic multi-shot sequences while
adhering to the film editing style, avoiding unstable transitions or naive
concatenations. Finally, we propose specialized evaluation metrics for
transition control, temporal consistency and overall quality, and demonstrate
through extensive experiments that CineTrans significantly outperforms existing
baselines across all criteria.

</details>


### [122] [Automated Building Heritage Assessment Using Street-Level Imagery](https://arxiv.org/abs/2508.11486)
*Kristina Dabrock,Tim Johansson,Anna Donarelli,Mikael Mangold,Noah Pflugradt,Jann Michael Weinand,Jochen Linßen*

Main category: cs.CV

TL;DR: AI tools like GPT can efficiently identify cultural heritage values in buildings, aiding energy conservation without compromising heritage. The study used GPT and machine learning to classify buildings in Stockholm, achieving a macro F1-score of 0.71 with combined data.


<details>
  <summary>Details</summary>
Motivation: To quantify energy conservation in buildings while preserving cultural heritage, avoiding costly traditional methods.

Method: Used GPT to detect heritage values in facade images, combined with building register data, and trained machine learning models for classification.

Result: Achieved a macro F1-score of 0.71 with combined data and 0.60 with only GPT-derived data.

Conclusion: The method improves heritage value databases, supporting careful energy efficiency measures in large-scale refurbishments.

Abstract: Detailed data is required to quantify energy conservation measures in
buildings, such as envelop retrofits, without compromising cultural heritage.
Novel artificial intelligence tools may improve efficiency in identifying
heritage values in buildings compared to costly and time-consuming traditional
inventories. In this study, the large language model GPT was used to detect
various aspects of cultural heritage value in fa\c{c}ade images. Using this
data and building register data as features, machine learning models were
trained to classify multi-family and non-residential buildings in Stockholm,
Sweden. Validation against an expert-created inventory shows a macro F1-score
of 0.71 using a combination of register data and features retrieved from GPT,
and a score of 0.60 using only GPT-derived data. The presented methodology can
contribute to a higher-quality database and thus support careful energy
efficiency measures and integrated consideration of heritage value in
large-scale energetic refurbishment scenarios.

</details>


### [123] [Perception in Plan: Coupled Perception and Planning for End-to-End Autonomous Driving](https://arxiv.org/abs/2508.11488)
*Bozhou Zhang,Jingyu Li,Nan Song,Li Zhang*

Main category: cs.CV

TL;DR: VeteranAD integrates perception into planning for end-to-end autonomous driving, using multi-mode anchored trajectories to guide targeted perception and improve planning performance.


<details>
  <summary>Details</summary>
Motivation: To enhance planning performance by integrating perception into the planning process, enabling targeted perception guided by evolving planning objectives.

Method: Introduces VeteranAD, a framework coupling perception and planning with multi-mode anchored trajectories as planning priors, and an autoregressive strategy for progressive trajectory prediction.

Result: Achieves state-of-the-art performance on NAVSIM and Bench2Drive datasets, demonstrating more accurate and reliable driving behavior.

Conclusion: VeteranAD effectively leverages planning-oriented optimization to improve end-to-end autonomous driving performance.

Abstract: End-to-end autonomous driving has achieved remarkable advancements in recent
years. Existing methods primarily follow a perception-planning paradigm, where
perception and planning are executed sequentially within a fully differentiable
framework for planning-oriented optimization. We further advance this paradigm
through a perception-in-plan framework design, which integrates perception into
the planning process. This design facilitates targeted perception guided by
evolving planning objectives over time, ultimately enhancing planning
performance. Building on this insight, we introduce VeteranAD, a coupled
perception and planning framework for end-to-end autonomous driving. By
incorporating multi-mode anchored trajectories as planning priors, the
perception module is specifically designed to gather traffic elements along
these trajectories, enabling comprehensive and targeted perception. Planning
trajectories are then generated based on both the perception results and the
planning priors. To make perception fully serve planning, we adopt an
autoregressive strategy that progressively predicts future trajectories while
focusing on relevant regions for targeted perception at each step. With this
simple yet effective design, VeteranAD fully unleashes the potential of
planning-oriented end-to-end methods, leading to more accurate and reliable
driving behavior. Extensive experiments on the NAVSIM and Bench2Drive datasets
demonstrate that our VeteranAD achieves state-of-the-art performance.

</details>


### [124] [Hierarchical Graph Feature Enhancement with Adaptive Frequency Modulation for Visual Recognition](https://arxiv.org/abs/2508.11497)
*Feiyue Zhao,Zhichao Zhang*

Main category: cs.CV

TL;DR: The paper introduces HGFE, a framework combining graph-based reasoning with CNNs to improve structural awareness and feature representation in visual recognition tasks.


<details>
  <summary>Details</summary>
Motivation: CNNs struggle with complex topological relationships and non-local semantics due to their reliance on regular grid structures.

Method: HGFE uses intra-window and inter-window graph structures for local and global dependencies, plus an adaptive frequency modulation module.

Result: HGFE improves performance on classification, detection, and segmentation tasks across multiple datasets.

Conclusion: HGFE is effective, lightweight, and easily integrated into CNNs, enhancing structural representation and recognition.

Abstract: Convolutional neural networks (CNNs) have
  demonstrated strong performance in visual recognition tasks,
  but their inherent reliance on regular grid structures limits
  their capacity to model complex topological relationships and
  non-local semantics within images. To address this limita tion, we propose
the hierarchical graph feature enhancement
  (HGFE), a novel framework that integrates graph-based rea soning into CNNs to
enhance both structural awareness and
  feature representation. HGFE builds two complementary levels
  of graph structures: intra-window graph convolution to cap ture local spatial
dependencies and inter-window supernode
  interactions to model global semantic relationships. Moreover,
  we introduce an adaptive frequency modulation module that
  dynamically balances low-frequency and high-frequency signal
  propagation, preserving critical edge and texture information
  while mitigating over-smoothing. The proposed HGFE module
  is lightweight, end-to-end trainable, and can be seamlessly
  integrated into standard CNN backbone networks. Extensive
  experiments on CIFAR-100 (classification), PASCAL VOC,
  and VisDrone (detection), as well as CrackSeg and CarParts
  (segmentation), validated the effectiveness of the HGFE in
  improving structural representation and enhancing overall
  recognition performance.

</details>


### [125] [Handwritten Text Recognition of Historical Manuscripts Using Transformer-Based Models](https://arxiv.org/abs/2508.11499)
*Erez Meoded*

Main category: cs.CV

TL;DR: The study applies TrOCR, a transformer-based HTR model, to 16th-century Latin manuscripts, using targeted preprocessing and novel data augmentation methods. Ensemble learning further improves performance, achieving a 50% relative improvement over prior results.


<details>
  <summary>Details</summary>
Motivation: Historical HTR faces challenges like scarce transcriptions and diverse handwriting styles, limiting digitization of archival documents.

Method: Uses TrOCR with targeted image preprocessing, novel data augmentation techniques, and ensemble learning strategies.

Result: Best single-model augmentation achieves CER of 1.86; top-5 ensemble reduces CER to 1.60, a 50% improvement over prior TrOCR results.

Conclusion: Domain-specific augmentations and ensemble strategies significantly enhance HTR performance for historical manuscripts.

Abstract: Historical handwritten text recognition (HTR) is essential for unlocking the
cultural and scholarly value of archival documents, yet digitization is often
hindered by scarce transcriptions, linguistic variation, and highly diverse
handwriting styles. In this study, we apply TrOCR, a state-of-the-art
transformer-based HTR model, to 16th-century Latin manuscripts authored by
Rudolf Gwalther. We investigate targeted image preprocessing and a broad suite
of data augmentation techniques, introducing four novel augmentation methods
designed specifically for historical handwriting characteristics. We also
evaluate ensemble learning approaches to leverage the complementary strengths
of augmentation-trained models. On the Gwalther dataset, our best single-model
augmentation (Elastic) achieves a Character Error Rate (CER) of 1.86, while a
top-5 voting ensemble achieves a CER of 1.60 - representing a 50% relative
improvement over the best reported TrOCR_BASE result and a 42% improvement over
the previous state of the art. These results highlight the impact of
domain-specific augmentations and ensemble strategies in advancing HTR
performance for historical manuscripts.

</details>


### [126] [AIM: Amending Inherent Interpretability via Self-Supervised Masking](https://arxiv.org/abs/2508.11502)
*Eyad Alshami,Shashank Agnihotri,Bernt Schiele,Margret Keuper*

Main category: cs.CV

TL;DR: AIM is a self-supervised method that enhances DNNs' use of genuine features over spurious ones, improving interpretability and accuracy without extra annotations.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks often rely on spurious features, which can hinder interpretability and generalization. AIM addresses this by promoting genuine feature utilization.

Method: AIM uses multi-stage features to guide a self-supervised, sample-specific masking process, training interpretable models.

Result: AIM improves interpretability (measured by EPG score) and accuracy across datasets like ImageNet100 and CUB-200.

Conclusion: AIM effectively enhances model interpretability and generalization by prioritizing genuine features, validated across diverse datasets.

Abstract: It has been observed that deep neural networks (DNNs) often use both genuine
as well as spurious features. In this work, we propose "Amending Inherent
Interpretability via Self-Supervised Masking" (AIM), a simple yet interestingly
effective method that promotes the network's utilization of genuine features
over spurious alternatives without requiring additional annotations. In
particular, AIM uses features at multiple encoding stages to guide a
self-supervised, sample-specific feature-masking process. As a result, AIM
enables the training of well-performing and inherently interpretable models
that faithfully summarize the decision process. We validate AIM across a
diverse range of challenging datasets that test both out-of-distribution
generalization and fine-grained visual understanding. These include
general-purpose classification benchmarks such as ImageNet100, HardImageNet,
and ImageWoof, as well as fine-grained classification datasets such as
Waterbirds, TravelingBirds, and CUB-200. AIM demonstrates significant dual
benefits: interpretability improvements, as measured by the Energy Pointing
Game (EPG) score, and accuracy gains over strong baselines. These consistent
gains across domains and architectures provide compelling evidence that AIM
promotes the use of genuine and meaningful features that directly contribute to
improved generalization and human-aligned interpretability.

</details>


### [127] [A Real-time Concrete Crack Detection and Segmentation Model Based on YOLOv11](https://arxiv.org/abs/2508.11517)
*Shaoze Huang,Qi Liu,Chao Chen,Yuhang Chen*

Main category: cs.CV

TL;DR: Proposes YOLOv11-KW-TA-FP, an optimized YOLOv11n-based model for concrete crack detection, integrating dynamic KWConv, triple attention, and FP-IoU loss, achieving high precision and robustness.


<details>
  <summary>Details</summary>
Motivation: Addresses inefficient manual inspection and poor deep learning performance for small-target crack detection in complex backgrounds, critical for infrastructure integrity in the Yangtze River Delta.

Method: Three-stage optimization: dynamic KWConv for feature enhancement, triple attention for channel-spatial modeling, and FP-IoU loss for adaptive bounding box regression.

Result: Achieves 91.3% precision, 76.6% recall, and 86.4% mAP@50, with robustness in data scarcity and noise.

Conclusion: Provides an efficient, practical solution for automated infrastructure inspection with significant engineering value.

Abstract: Accelerated aging of transportation infrastructure in the rapidly developing
Yangtze River Delta region necessitates efficient concrete crack detection, as
crack deterioration critically compromises structural integrity and regional
economic growth. To overcome the limitations of inefficient manual inspection
and the suboptimal performance of existing deep learning models, particularly
for small-target crack detection within complex backgrounds, this paper
proposes YOLOv11-KW-TA-FP, a multi-task concrete crack detection and
segmentation model based on the YOLOv11n architecture. The proposed model
integrates a three-stage optimization framework: (1) Embedding dynamic
KernelWarehouse convolution (KWConv) within the backbone network to enhance
feature representation through a dynamic kernel sharing mechanism; (2)
Incorporating a triple attention mechanism (TA) into the feature pyramid to
strengthen channel-spatial interaction modeling; and (3) Designing an FP-IoU
loss function to facilitate adaptive bounding box regression penalization.
Experimental validation demonstrates that the enhanced model achieves
significant performance improvements over the baseline, attaining 91.3%
precision, 76.6% recall, and 86.4% mAP@50. Ablation studies confirm the
synergistic efficacy of the proposed modules. Furthermore, robustness tests
indicate stable performance under conditions of data scarcity and noise
interference. This research delivers an efficient computer vision solution for
automated infrastructure inspection, exhibiting substantial practical
engineering value.

</details>


### [128] [Multi-State Tracker: Enhancing Efficient Object Tracking via Multi-State Specialization and Interaction](https://arxiv.org/abs/2508.11531)
*Shilei Wang,Gong Cheng,Pujian Lai,Dong Gao,Junwei Han*

Main category: cs.CV

TL;DR: The paper proposes Multi-State Tracker (MST), a lightweight tracker using state-specific enhancement and cross-state interaction to improve feature representation and tracking robustness with minimal computational overhead.


<details>
  <summary>Details</summary>
Motivation: Efficient trackers often sacrifice feature representation for speed, limiting accuracy. MST aims to enhance feature representation without significant computational cost.

Method: MST uses multi-state generation (MSG) to create multiple state features, refines them with state-specific enhancement (SSE), and aggregates them via cross-state interaction (CSI). The design is lightweight (HSA-SSD).

Result: MST outperforms prior efficient trackers, improving tracking accuracy and robustness. It achieves a 4.5% higher AO score on GOT-10K compared to HCAT.

Conclusion: MST successfully balances efficiency and feature representation, offering superior tracking performance with minimal computational overhead.

Abstract: Efficient trackers achieve faster runtime by reducing computational
complexity and model parameters. However, this efficiency often compromises the
expense of weakened feature representation capacity, thus limiting their
ability to accurately capture target states using single-layer features. To
overcome this limitation, we propose Multi-State Tracker (MST), which utilizes
highly lightweight state-specific enhancement (SSE) to perform specialized
enhancement on multi-state features produced by multi-state generation (MSG)
and aggregates them in an interactive and adaptive manner using cross-state
interaction (CSI). This design greatly enhances feature representation while
incurring minimal computational overhead, leading to improved tracking
robustness in complex environments. Specifically, the MSG generates multiple
state representations at multiple stages during feature extraction, while SSE
refines them to highlight target-specific features. The CSI module facilitates
information exchange between these states and ensures the integration of
complementary features. Notably, the introduced SSE and CSI modules adopt a
highly lightweight hidden state adaptation-based state space duality (HSA-SSD)
design, incurring only 0.1 GFLOPs in computation and 0.66 M in parameters.
Experimental results demonstrate that MST outperforms all previous efficient
trackers across multiple datasets, significantly improving tracking accuracy
and robustness. In particular, it shows excellent runtime performance, with an
AO score improvement of 4.5\% over the previous SOTA efficient tracker HCAT on
the GOT-10K dataset. The code is available at https://github.com/wsumel/MST.

</details>


### [129] [An Efficient Medical Image Classification Method Based on a Lightweight Improved ConvNeXt-Tiny Architecture](https://arxiv.org/abs/2508.11532)
*Jingsong Xia,Yue Yin,Xiuhan Li*

Main category: cs.CV

TL;DR: The paper proposes an improved ConvNeXt-Tiny architecture for medical image classification, enhancing accuracy and efficiency in resource-constrained environments.


<details>
  <summary>Details</summary>
Motivation: Efficient and high-accuracy medical image classification is challenging in resource-limited settings, necessitating optimized methods.

Method: The method integrates dual global pooling (Global Average Pooling and Global Max Pooling) and a lightweight SEVector attention module, along with Feature Smoothing Loss for improved performance.

Result: Achieves 89.10% accuracy on the test set under CPU-only conditions, with stable convergence in 10 epochs.

Conclusion: The proposed method offers a feasible and efficient solution for medical image classification in resource-limited environments.

Abstract: Intelligent analysis of medical imaging plays a crucial role in assisting
clinical diagnosis. However, achieving efficient and high-accuracy image
classification in resource-constrained computational environments remains
challenging. This study proposes a medical image classification method based on
an improved ConvNeXt-Tiny architecture. Through structural optimization and
loss function design, the proposed method enhances feature extraction
capability and classification performance while reducing computational
complexity. Specifically, the method introduces a dual global pooling (Global
Average Pooling and Global Max Pooling) feature fusion strategy into the
ConvNeXt-Tiny backbone to simultaneously preserve global statistical features
and salient response information. A lightweight channel attention module,
termed Squeeze-and-Excitation Vector (SEVector), is designed to improve the
adaptive allocation of channel weights while minimizing parameter overhead.
Additionally, a Feature Smoothing Loss is incorporated into the loss function
to enhance intra-class feature consistency and suppress intra-class variance.
Under CPU-only conditions (8 threads), the method achieves a maximum
classification accuracy of 89.10% on the test set within 10 training epochs,
exhibiting a stable convergence trend in loss values. Experimental results
demonstrate that the proposed method effectively improves medical image
classification performance in resource-limited settings, providing a feasible
and efficient solution for the deployment and promotion of medical imaging
analysis models.

</details>


### [130] [Reinforcing Video Reasoning Segmentation to Think Before It Segments](https://arxiv.org/abs/2508.11538)
*Sitong Gong,Lu Zhang,Yunzhi Zhuge,Xu Jia,Pingping Zhang,Huchuan Lu*

Main category: cs.CV

TL;DR: Veason-R1, a specialized LVLM for Video Reasoning Segmentation (VRS), improves interpretability and performance via structured reasoning, trained with GRPO and CoT initialization, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Previous VRS methods using LVLMs lack interpretability and suffer from poor spatiotemporal reasoning, prompting the need for a more structured approach.

Method: Veason-R1 is trained with Group Relative Policy Optimization (GRPO) and Chain-of-Thought (CoT) initialization, focusing on structured reasoning trajectories and efficient exploration of reasoning space.

Result: Veason-R1 outperforms prior methods on benchmarks (e.g., +1.3 J &F in ReVOS, +10.0 J &F in ReasonVOS) and shows robustness to hallucinations (+8.8 R).

Conclusion: Veason-R1 advances VRS by integrating structured reasoning, achieving superior performance and interpretability, with potential for broader applications.

Abstract: Video reasoning segmentation (VRS) endeavors to delineate referred objects in
videos guided by implicit instructions that encapsulate human intent and
temporal logic. Previous approaches leverage large vision language models
(LVLMs) to encode object semantics into <SEG> tokens for mask prediction.
However, this paradigm suffers from limited interpretability during inference
and suboptimal performance due to inadequate spatiotemporal reasoning. Drawing
inspiration from seminal breakthroughs in reinforcement learning, we introduce
Veason-R1, a specialized LVLM for VRS that emphasizes structured reasoning in
segmentation. Veason-R1 is trained through Group Relative Policy Optimization
(GRPO) augmented with Chain-of-Thought (CoT) initialization. To begin with, we
curate high-quality CoT training data to instill structured reasoning
trajectories, bridging video-level semantics and frame-level spatial grounding,
yielding the supervised fine-tuned model Veason-SFT. Subsequently, GRPO
fine-tuning encourages efficient exploration of the reasoning space by
optimizing reasoning chains. To this end, we incorporate a holistic reward
mechanism that synergistically enhances spatial alignment and temporal
consistency, bolstering keyframe localization and fine-grained grounding.
Comprehensive empirical evaluations demonstrate that Veason-R1 achieves
state-of-the-art performance on multiple benchmarks, surpassing prior art by
significant margins (e.g., +1.3 J &F in ReVOS and +10.0 J &F in ReasonVOS),
while exhibiting robustness to hallucinations (+8.8 R). Our code and model
weights will be available at Veason-R1.

</details>


### [131] [Training-Free Anomaly Generation via Dual-Attention Enhancement in Diffusion Model](https://arxiv.org/abs/2508.11550)
*Zuo Zuo,Jiahao Dong,Yanyun Qu,Zongze Wu*

Main category: cs.CV

TL;DR: AAG is a training-free anomaly generation framework using Stable Diffusion to create realistic anomalies in specific image regions without altering other areas, enhancing downstream AD tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of fidelity and extra training data in existing anomaly generation methods for industrial AD.

Method: Leverages Stable Diffusion with Cross-Attention Enhancement (CAE) and Self-Attention Enhancement (SAE) to generate anomalies guided by text prompts and masks.

Result: Demonstrates effectiveness on MVTec AD and VisA datasets, improving anomaly inspection tasks.

Conclusion: AAG offers a practical, training-free solution for realistic anomaly generation, benefiting industrial AD.

Abstract: Industrial anomaly detection (AD) plays a significant role in manufacturing
where a long-standing challenge is data scarcity. A growing body of works have
emerged to address insufficient anomaly data via anomaly generation. However,
these anomaly generation methods suffer from lack of fidelity or need to be
trained with extra data. To this end, we propose a training-free anomaly
generation framework dubbed AAG, which is based on Stable Diffusion (SD)'s
strong generation ability for effective anomaly image generation. Given a
normal image, mask and a simple text prompt, AAG can generate realistic and
natural anomalies in the specific regions and simultaneously keep contents in
other regions unchanged. In particular, we propose Cross-Attention Enhancement
(CAE) to re-engineer the cross-attention mechanism within Stable Diffusion
based on the given mask. CAE increases the similarity between visual tokens in
specific regions and text embeddings, which guides these generated visual
tokens in accordance with the text description. Besides, generated anomalies
need to be more natural and plausible with object in given image. We propose
Self-Attention Enhancement (SAE) which improves similarity between each normal
visual token and anomaly visual tokens. SAE ensures that generated anomalies
are coherent with original pattern. Extensive experiments on MVTec AD and VisA
datasets demonstrate effectiveness of AAG in anomaly generation and its
utility. Furthermore, anomaly images generated by AAG can bolster performance
of various downstream anomaly inspection tasks.

</details>


### [132] [TrajSV: A Trajectory-based Model for Sports Video Representations and Applications](https://arxiv.org/abs/2508.11569)
*Zheng Wang,Shihao Xu,Wei Shi*

Main category: cs.CV

TL;DR: TrajSV is a trajectory-based framework for sports analytics, addressing data unavailability, lack of trajectory frameworks, and supervision label needs. It includes preprocessing, CRNet, and VRNet, achieving state-of-the-art results in retrieval, action spotting, and captioning.


<details>
  <summary>Details</summary>
Motivation: Address unresolved issues in sports analytics: data unavailability, ineffective trajectory frameworks, and supervision label requirements.

Method: TrajSV framework with data preprocessing, CRNet (trajectory-enhanced Transformer), and VRNet (encoder-decoder for video representations). Uses triple contrastive loss for unsupervised optimization.

Result: State-of-the-art performance: ~70% improvement in retrieval, top results in 9/17 action categories, ~20% improvement in captioning.

Conclusion: TrajSV effectively addresses key challenges in sports analytics, demonstrating superior performance across multiple applications and sports.

Abstract: Sports analytics has received significant attention from both academia and
industry in recent years. Despite the growing interest and efforts in this
field, several issues remain unresolved, including (1) data unavailability, (2)
lack of an effective trajectory-based framework, and (3) requirement for
sufficient supervision labels. In this paper, we present TrajSV, a
trajectory-based framework that addresses various issues in existing studies.
TrajSV comprises three components: data preprocessing, Clip Representation
Network (CRNet), and Video Representation Network (VRNet). The data
preprocessing module extracts player and ball trajectories from sports
broadcast videos. CRNet utilizes a trajectory-enhanced Transformer module to
learn clip representations based on these trajectories. Additionally, VRNet
learns video representations by aggregating clip representations and visual
features with an encoder-decoder architecture. Finally, a triple contrastive
loss is introduced to optimize both video and clip representations in an
unsupervised manner. The experiments are conducted on three broadcast video
datasets to verify the effectiveness of TrajSV for three types of sports (i.e.,
soccer, basketball, and volleyball) with three downstream applications (i.e.,
sports video retrieval, action spotting, and video captioning). The results
demonstrate that TrajSV achieves state-of-the-art performance in sports video
retrieval, showcasing a nearly 70% improvement. It outperforms baselines in
action spotting, achieving state-of-the-art results in 9 out of 17 action
categories, and demonstrates a nearly 20% improvement in video captioning.
Additionally, we introduce a deployed system along with the three applications
based on TrajSV.

</details>


### [133] [Causality Matters: How Temporal Information Emerges in Video Language Models](https://arxiv.org/abs/2508.11576)
*Yumeng Shi,Quanyu Long,Yin Wu,Wenya Wang*

Main category: cs.CV

TL;DR: The paper investigates temporal understanding in VideoLMs, revealing that positional encodings (PEs) are less critical than previously thought, while frame sequence order is crucial. It identifies a causal information pathway for temporal reasoning and proposes efficiency strategies.


<details>
  <summary>Details</summary>
Motivation: Temporal understanding (event order, duration, relationships) in VideoLMs remains a challenge, with prior works overemphasizing PEs. The study aims to uncover how temporal information is integrated and improve model efficiency.

Method: The authors remove/modify PEs and reverse frame sequences to test temporal understanding. They analyze inter-frame attention and propose staged cross-modal attention and a temporal exit mechanism.

Result: Reversing frame sequences harms performance, while PEs have minimal impact. Temporal reasoning emerges from inter-visual token interactions under causal attention. Proposed strategies improve efficiency without degrading performance.

Conclusion: Temporal understanding in VideoLMs relies on implicit inter-frame interactions, not PEs. The proposed strategies enhance efficiency, offering insights for future model improvements.

Abstract: Video language models (VideoLMs) have made significant progress in multimodal
understanding. However, temporal understanding, which involves identifying
event order, duration, and relationships across time, still remains a core
challenge. Prior works emphasize positional encodings (PEs) as a key mechanism
for encoding temporal structure. Surprisingly, we find that removing or
modifying PEs in video inputs yields minimal degradation in the performance of
temporal understanding. In contrast, reversing the frame sequence while
preserving the original PEs causes a substantial drop. To explain this
behavior, we conduct substantial analysis experiments to trace how temporal
information is integrated within the model. We uncover a causal information
pathway: temporal cues are progressively synthesized through inter-frame
attention, aggregated in the final frame, and subsequently integrated into the
query tokens. This emergent mechanism shows that temporal reasoning emerges
from inter-visual token interactions under the constraints of causal attention,
which implicitly encodes temporal structure. Based on these insights, we
propose two efficiency-oriented strategies: staged cross-modal attention and a
temporal exit mechanism for early token truncation. Experiments on two
benchmarks validate the effectiveness of both approaches. To the best of our
knowledge, this is the first work to systematically investigate video temporal
understanding in VideoLMs, offering insights for future model improvement.

</details>


### [134] [DashCam Video: A complementary low-cost data stream for on-demand forest-infrastructure system monitoring](https://arxiv.org/abs/2508.11591)
*Durga Joshi,Chandi Witharana,Robert Fahey,Thomas Worthley,Zhe Zhu,Diego Cerrai*

Main category: cs.CV

TL;DR: A low-cost, reproducible framework using dashcam video for real-time structural assessment and geolocation of roadside objects, combining depth estimation, error correction, and GPS triangulation.


<details>
  <summary>Details</summary>
Motivation: To provide a cost-effective, scalable solution for monitoring urban vegetation and infrastructure using underutilized dashcam data, complementing traditional methods like LiDAR.

Method: An end-to-end pipeline with monocular depth estimation, depth error correction via gradient-boosted regression, and GPS-based triangulation for object geolocation and height estimation.

Result: Achieved strong depth correction (R2=0.92, MAE=0.31) and accurate geolocation (mean error 2.83m) and height estimation (MAE 2.09m for trees, 0.88m for poles).

Conclusion: The framework offers a fast, real-time, and cost-effective alternative for urban monitoring, valuable for utility companies and urban planners.

Abstract: Our study introduces a novel, low-cost, and reproducible framework for
real-time, object-level structural assessment and geolocation of roadside
vegetation and infrastructure with commonly available but underutilized
dashboard camera (dashcam) video data. We developed an end-to-end pipeline that
combines monocular depth estimation, depth error correction, and geometric
triangulation to generate accurate spatial and structural data from
street-level video streams from vehicle-mounted dashcams. Depth maps were first
estimated using a state-of-the-art monocular depth model, then refined via a
gradient-boosted regression framework to correct underestimations, particularly
for distant objects. The depth correction model achieved strong predictive
performance (R2 = 0.92, MAE = 0.31 on transformed scale), significantly
reducing bias beyond 15 m. Further, object locations were estimated using
GPS-based triangulation, while object heights were calculated using pin hole
camera geometry. Our method was evaluated under varying conditions of camera
placement and vehicle speed. Low-speed vehicle with inside camera gave the
highest accuracy, with mean geolocation error of 2.83 m, and mean absolute
error (MAE) in height estimation of 2.09 m for trees and 0.88 m for poles. To
the best of our knowledge, it is the first framework to combine monocular depth
modeling, triangulated GPS-based geolocation, and real-time structural
assessment for urban vegetation and infrastructure using consumer-grade video
data. Our approach complements conventional RS methods, such as LiDAR and image
by offering a fast, real-time, and cost-effective solution for object-level
monitoring of vegetation risks and infrastructure exposure, making it
especially valuable for utility companies, and urban planners aiming for
scalable and frequent assessments in dynamic urban environments.

</details>


### [135] [CoreEditor: Consistent 3D Editing via Correspondence-constrained Diffusion](https://arxiv.org/abs/2508.11603)
*Zhe Zhu,Honghua Chen,Peng Li,Mingqiang Wei*

Main category: cs.CV

TL;DR: CoreEditor introduces a correspondence-constrained attention mechanism for consistent text-to-3D editing, outperforming prior methods with sharper details and better cross-view consistency.


<details>
  <summary>Details</summary>
Motivation: Existing 2D image editor adaptations for 3D editing often fail to maintain cross-view consistency, leading to insufficient edits and blurry details.

Method: CoreEditor uses a correspondence-constrained attention mechanism and semantic similarity during denoising for robust multi-view editing, along with a selective editing pipeline for user flexibility.

Result: CoreEditor produces high-quality, 3D-consistent edits with sharper details, significantly outperforming prior methods.

Conclusion: CoreEditor advances text-driven 3D editing by ensuring cross-view consistency and offering user control, setting a new benchmark for quality.

Abstract: Text-driven 3D editing seeks to modify 3D scenes according to textual
descriptions, and most existing approaches tackle this by adapting pre-trained
2D image editors to multi-view inputs. However, without explicit control over
multi-view information exchange, they often fail to maintain cross-view
consistency, leading to insufficient edits and blurry details. We introduce
CoreEditor, a novel framework for consistent text-to-3D editing. The key
innovation is a correspondence-constrained attention mechanism that enforces
precise interactions between pixels expected to remain consistent throughout
the diffusion denoising process. Beyond relying solely on geometric alignment,
we further incorporate semantic similarity estimated during denoising, enabling
more reliable correspondence modeling and robust multi-view editing. In
addition, we design a selective editing pipeline that allows users to choose
preferred results from multiple candidates, offering greater flexibility and
user control. Extensive experiments show that CoreEditor produces high-quality,
3D-consistent edits with sharper details, significantly outperforming prior
methods.

</details>


### [136] [LoRAtorio: An intrinsic approach to LoRA Skill Composition](https://arxiv.org/abs/2508.11624)
*Niki Foteinopoulou,Ignas Budvytis,Stephan Liwicki*

Main category: cs.CV

TL;DR: LoRAtorio is a train-free framework for composing multiple LoRA adapters in text-to-image diffusion models, addressing challenges in open-ended settings by leveraging intrinsic model behavior and spatial-aware weighting.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with composing multiple LoRA adapters, especially in open-ended scenarios where the required skills are unknown in advance.

Method: LoRAtorio divides latent space into patches, computes cosine similarity between patch noise and base model noise, and uses a spatially-aware weight matrix for weighted aggregation. It also modifies classifier-free guidance to incorporate the base model's unconditional score.

Result: The method achieves state-of-the-art performance, with up to 1.3% ClipScore improvement and a 72.43% win rate in GPT-4V evaluations, generalizing well across latent diffusion models.

Conclusion: LoRAtorio effectively addresses multi-LoRA composition challenges, offering superior performance and adaptability in open-ended settings.

Abstract: Low-Rank Adaptation (LoRA) has become a widely adopted technique in
text-to-image diffusion models, enabling the personalisation of visual concepts
such as characters, styles, and objects. However, existing approaches struggle
to effectively compose multiple LoRA adapters, particularly in open-ended
settings where the number and nature of required skills are not known in
advance. In this work, we present LoRAtorio, a novel train-free framework for
multi-LoRA composition that leverages intrinsic model behaviour. Our method is
motivated by two key observations: (1) LoRA adapters trained on narrow domains
produce denoised outputs that diverge from the base model, and (2) when
operating out-of-distribution, LoRA outputs show behaviour closer to the base
model than when conditioned in distribution. The balance between these two
observations allows for exceptional performance in the single LoRA scenario,
which nevertheless deteriorates when multiple LoRAs are loaded. Our method
operates in the latent space by dividing it into spatial patches and computing
cosine similarity between each patch's predicted noise and that of the base
model. These similarities are used to construct a spatially-aware weight
matrix, which guides a weighted aggregation of LoRA outputs. To address domain
drift, we further propose a modification to classifier-free guidance that
incorporates the base model's unconditional score into the composition. We
extend this formulation to a dynamic module selection setting, enabling
inference-time selection of relevant LoRA adapters from a large pool. LoRAtorio
achieves state-of-the-art performance, showing up to a 1.3% improvement in
ClipScore and a 72.43% win rate in GPT-4V pairwise evaluations, and generalises
effectively to multiple latent diffusion models.

</details>


### [137] [Is ChatGPT-5 Ready for Mammogram VQA?](https://arxiv.org/abs/2508.11628)
*Qiang Li,Shansong Wang,Mingzhe Hu,Mojtaba Safari,Zachary Eidex,Xiaofeng Yang*

Main category: cs.CV

TL;DR: GPT-5 outperforms GPT-4o in mammogram VQA tasks but still lags behind human experts and domain-specific models. Performance varies across datasets and tasks, showing promise but needing further optimization for clinical use.


<details>
  <summary>Details</summary>
Motivation: To evaluate the potential of GPT-5 and GPT-4o in mammogram VQA tasks, comparing their performance with human experts and specialized models.

Method: Systematic evaluation of GPT-5 and GPT-4o on four public mammography datasets (EMBED, InBreast, CMMD, CBIS-DDSM) for BI-RADS assessment, abnormality detection, and malignancy classification.

Result: GPT-5 consistently performed best among GPT variants but underperformed compared to human experts and fine-tuned models. Achieved varying accuracy across tasks (e.g., 56.8% density classification on EMBED, 36.9% BI-RADS accuracy on InBreast).

Conclusion: GPT-5 shows promise for mammogram VQA but requires domain adaptation for clinical use. Performance improvements from GPT-4o to GPT-5 indicate potential for LLMs in this field.

Abstract: Mammogram visual question answering (VQA) integrates image interpretation
with clinical reasoning and has potential to support breast cancer screening.
We systematically evaluated the GPT-5 family and GPT-4o model on four public
mammography datasets (EMBED, InBreast, CMMD, CBIS-DDSM) for BI-RADS assessment,
abnormality detection, and malignancy classification tasks. GPT-5 consistently
was the best performing model but lagged behind both human experts and
domain-specific fine-tuned models. On EMBED, GPT-5 achieved the highest scores
among GPT variants in density (56.8%), distortion (52.5%), mass (64.5%),
calcification (63.5%), and malignancy (52.8%) classification. On InBreast, it
attained 36.9% BI-RADS accuracy, 45.9% abnormality detection, and 35.0%
malignancy classification. On CMMD, GPT-5 reached 32.3% abnormality detection
and 55.0% malignancy accuracy. On CBIS-DDSM, it achieved 69.3% BI-RADS
accuracy, 66.0% abnormality detection, and 58.2% malignancy accuracy. Compared
with human expert estimations, GPT-5 exhibited lower sensitivity (63.5%) and
specificity (52.3%). While GPT-5 exhibits promising capabilities for screening
tasks, its performance remains insufficient for high-stakes clinical imaging
applications without targeted domain adaptation and optimization. However, the
tremendous improvements in performance from GPT-4o to GPT-5 show a promising
trend in the potential for general large language models (LLMs) to assist with
mammography VQA tasks.

</details>


### [138] [Thyme: Think Beyond Images](https://arxiv.org/abs/2508.11630)
*Yi-Fan Zhang,Xingyu Lu,Shukang Yin,Chaoyou Fu,Wei Chen,Xiao Hu,Bin Wen,Kaiyu Jiang,Changyi Liu,Tianke Zhang,Haonan Fan,Kaibing Chen,Jiankang Chen,Haojie Ding,Kaiyu Tang,Zhang Zhang,Liang Wang,Fan Yang,Tingting Gao,Guorui Zhou*

Main category: cs.CV

TL;DR: Thyme introduces a novel paradigm for MLLMs to autonomously generate and execute image processing and computational operations via code, outperforming existing methods in perception and reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Existing open-source models lack the rich feature set of proprietary models for image manipulation and logical reasoning. Thyme aims to bridge this gap by enabling autonomous code-based operations.

Method: Thyme uses a two-stage training strategy: SFT on 500K samples for code generation, followed by RL with GRPO-ATS for refined decision-making.

Result: Thyme achieves significant performance gains on 20 benchmarks, excelling in high-resolution perception and complex reasoning tasks.

Conclusion: Thyme demonstrates the potential of code-based autonomous operations to enhance MLLMs, offering a scalable and flexible solution for advanced reasoning and perception tasks.

Abstract: Following OpenAI's introduction of the ``thinking with images'' concept,
recent efforts have explored stimulating the use of visual information in the
reasoning process to enhance model performance in perception and reasoning
tasks. However, to the best of our knowledge, no open-source work currently
offers a feature set as rich as proprietary models (O3), which can perform
diverse image manipulations and simultaneously enhance logical reasoning
capabilities through code. In this paper, we make a preliminary attempt in this
direction by introducing Thyme (Think Beyond Images), a novel paradigm for
enabling MLLMs to transcend existing ``think with images'' approaches by
autonomously generating and executing diverse image processing and
computational operations via executable code. This approach not only
facilitates a rich, on-the-fly set of image manipulations (e.g., cropping,
rotation, contrast enhancement) but also allows for mathematical computations,
all while maintaining high autonomy in deciding when and how to apply these
operations. We activate this capability through a two-stage training strategy:
an initial SFT on a curated dataset of 500K samples to teach code generation,
followed by a RL phase to refine decision-making. For the RL stage, we manually
collect and design high-resolution question-answer pairs to increase the
learning difficulty, and we propose GRPO-ATS (Group Relative Policy
Optimization with Adaptive Temperature Sampling), an algorithm that applies
distinct temperatures to text and code generation to balance reasoning
exploration with code execution precision. We conduct extensive experimental
analysis and ablation studies. Comprehensive evaluations on nearly 20
benchmarks show that Thyme yields significant and consistent performance gains,
particularly in challenging high-resolution perception and complex reasoning
tasks.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [139] [The Role of Radiographic Knee Alignment in Knee Replacement Outcomes and Opportunities for Artificial Intelligence-Driven Assessment](https://arxiv.org/abs/2508.10941)
*Zhisen Hu,David S. Johnson,Aleksei Tiulpin,Timothy F. Cootes,Claudia Lindner*

Main category: eess.IV

TL;DR: The paper reviews knee alignment biomarkers for total knee replacement (TKR) outcomes, discusses AI-based approaches for assessing alignment from radiographs, and explores future directions for TKR outcome prediction.


<details>
  <summary>Details</summary>
Motivation: Knee osteoarthritis (OA) lacks a cure, and TKR outcomes are hard to predict. Radiographic knee alignment is a key factor affecting outcomes, but existing reviews focus on OA diagnosis rather than alignment biomarkers for TKR.

Method: The review examines current TKR outcome scoring protocols, identifies knee alignment biomarkers, and discusses AI-based methods for analyzing radiographs.

Result: AI can automate knee alignment measurements, offering potential for improved TKR outcome prediction. Existing methods focus on OA diagnosis, leaving alignment biomarkers underexplored.

Conclusion: Future research should prioritize AI-driven knee alignment assessment to enhance TKR outcome prediction and address gaps in current literature.

Abstract: Prevalent knee osteoarthritis (OA) imposes substantial burden on health
systems with no cure available. Its ultimate treatment is total knee
replacement (TKR). Complications from surgery and recovery are difficult to
predict in advance, and numerous factors may affect them. Radiographic knee
alignment is one of the key factors that impacts TKR outcomes, affecting
outcomes such as postoperative pain or function. Recently, artificial
intelligence (AI) has been introduced to the automatic analysis of knee
radiographs, for example, to automate knee alignment measurements. Existing
review articles tend to focus on knee OA diagnosis and segmentation of bones or
cartilages in MRI rather than exploring knee alignment biomarkers for TKR
outcomes and their assessment. In this review, we first examine the current
scoring protocols for evaluating TKR outcomes and potential knee alignment
biomarkers associated with these outcomes. We then discuss existing AI-based
approaches for generating knee alignment biomarkers from knee radiographs, and
explore future directions for knee alignment assessment and TKR outcome
prediction.

</details>


### [140] [Deep Learning-Based Automated Segmentation of Uterine Myomas](https://arxiv.org/abs/2508.11010)
*Tausifa Jan Saleem,Mohammad Yaqub*

Main category: eess.IV

TL;DR: The paper addresses the need for automated segmentation of uterine fibroids using deep learning, leveraging a public dataset (UMD) for standardized evaluation.


<details>
  <summary>Details</summary>
Motivation: Uterine fibroids are highly prevalent and burdensome, requiring precise MRI segmentation, which is currently labor-intensive and variable.

Method: Deep learning algorithms are applied to the publicly available Uterine Myoma MRI Dataset (UMD) for automated segmentation.

Result: The study establishes a baseline for automated segmentation, enabling standardized evaluation and future research.

Conclusion: Automated segmentation using deep learning and public datasets can improve accuracy and efficiency in diagnosing uterine fibroids.

Abstract: Uterine fibroids (myomas) are the most common benign tumors of the female
reproductive system, particularly among women of childbearing age. With a
prevalence exceeding 70%, they pose a significant burden on female reproductive
health. Clinical symptoms such as abnormal uterine bleeding, infertility,
pelvic pain, and pressure-related discomfort play a crucial role in guiding
treatment decisions, which are largely influenced by the size, number, and
anatomical location of the fibroids. Magnetic Resonance Imaging (MRI) is a
non-invasive and highly accurate imaging modality commonly used by clinicians
for the diagnosis of uterine fibroids. Segmenting uterine fibroids requires a
precise assessment of both the uterus and fibroids on MRI scans, including
measurements of volume, shape, and spatial location. However, this process is
labor intensive and time consuming and subjected to variability due to intra-
and inter-expert differences at both pre- and post-treatment stages. As a
result, there is a critical need for an accurate and automated segmentation
method for uterine fibroids. In recent years, deep learning algorithms have
shown re-markable improvements in medical image segmentation, outperforming
traditional methods. These approaches offer the potential for fully automated
segmentation. Several studies have explored the use of deep learning models to
achieve automated segmentation of uterine fibroids. However, most of the
previous work has been conducted using private datasets, which poses challenges
for validation and comparison between studies. In this study, we leverage the
publicly available Uterine Myoma MRI Dataset (UMD) to establish a baseline for
automated segmentation of uterine fibroids, enabling standardized evaluation
and facilitating future research in this domain.

</details>


### [141] [HistoViT: Vision Transformer for Accurate and Scalable Histopathological Cancer Diagnosis](https://arxiv.org/abs/2508.11181)
*Faisal Ahmed*

Main category: eess.IV

TL;DR: A transformer-based deep learning framework (Vision Transformer) is proposed for multi-class tumor classification in histopathological images, outperforming existing methods with high accuracy and AUC scores.


<details>
  <summary>Details</summary>
Motivation: Accurate and scalable cancer diagnosis is challenging due to complex histological variability in malignancies like breast, prostate, bone, and cervical cancers.

Method: A fine-tuned Vision Transformer (ViT) architecture is used with a streamlined preprocessing pipeline to convert whole-slide images into normalized PyTorch tensors.

Result: The model achieves high classification accuracies (99.32%, 96.92%, 95.28%, 96.94%) and AUC scores (>99%) across four benchmark datasets.

Conclusion: Transformer-based architectures show robustness and clinical potential for automated cancer diagnosis, improving healthcare outcomes.

Abstract: Accurate and scalable cancer diagnosis remains a critical challenge in modern
pathology, particularly for malignancies such as breast, prostate, bone, and
cervical, which exhibit complex histological variability. In this study, we
propose a transformer-based deep learning framework for multi-class tumor
classification in histopathological images. Leveraging a fine-tuned Vision
Transformer (ViT) architecture, our method addresses key limitations of
conventional convolutional neural networks, offering improved performance,
reduced preprocessing requirements, and enhanced scalability across tissue
types. To adapt the model for histopathological cancer images, we implement a
streamlined preprocessing pipeline that converts tiled whole-slide images into
PyTorch tensors and standardizes them through data normalization. This ensures
compatibility with the ViT architecture and enhances both convergence stability
and overall classification performance. We evaluate our model on four benchmark
datasets: ICIAR2018 (breast), SICAPv2 (prostate), UT-Osteosarcoma (bone), and
SipakMed (cervical) dataset -- demonstrating consistent outperformance over
existing deep learning methods. Our approach achieves classification accuracies
of 99.32%, 96.92%, 95.28%, and 96.94% for breast, prostate, bone, and cervical
cancers respectively, with area under the ROC curve (AUC) scores exceeding 99%
across all datasets. These results confirm the robustness, generalizability,
and clinical potential of transformer-based architectures in digital pathology.
Our work represents a significant advancement toward reliable, automated, and
interpretable cancer diagnosis systems that can alleviate diagnostic burdens
and improve healthcare outcomes.

</details>


### [142] [Efficient Image-to-Image Schrödinger Bridge for CT Field of View Extension](https://arxiv.org/abs/2508.11211)
*Zhenhao Li,Long Yang,Xiaojie Yin,Haijun Yu,Jiazhou Wang,Hongbin Han,Weigang Hu,Yixing Huang*

Main category: eess.IV

TL;DR: Proposes an efficient CT FOV extension framework using the I²SB diffusion model, outperforming existing methods in accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: CT scans often suffer from truncation when objects exceed the scanner's FOV, leading to artifacts. Current methods struggle with accuracy and speed.

Method: Uses the I²SB diffusion model for direct stochastic mapping between limited-FOV and extended-FOV images, improving interpretability and speed.

Result: Achieves RMSE of 49.8HU (simulated) and 152.0HU (real data), with 0.19s per slice inference time, 700x faster than cDDPM.

Conclusion: I²SB offers accurate and fast CT FOV extension, suitable for real-time clinical use.

Abstract: Computed tomography (CT) is a cornerstone imaging modality for non-invasive,
high-resolution visualization of internal anatomical structures. However, when
the scanned object exceeds the scanner's field of view (FOV), projection data
are truncated, resulting in incomplete reconstructions and pronounced artifacts
near FOV boundaries. Conventional reconstruction algorithms struggle to recover
accurate anatomy from such data, limiting clinical reliability. Deep learning
approaches have been explored for FOV extension, with diffusion generative
models representing the latest advances in image synthesis. Yet, conventional
diffusion models are computationally demanding and slow at inference due to
their iterative sampling process. To address these limitations, we propose an
efficient CT FOV extension framework based on the image-to-image Schr\"odinger
Bridge (I$^2$SB) diffusion model. Unlike traditional diffusion models that
synthesize images from pure Gaussian noise, I$^2$SB learns a direct stochastic
mapping between paired limited-FOV and extended-FOV images. This direct
correspondence yields a more interpretable and traceable generative process,
enhancing anatomical consistency and structural fidelity in reconstructions.
I$^2$SB achieves superior quantitative performance, with root-mean-square error
(RMSE) values of 49.8\,HU on simulated noisy data and 152.0HU on real data,
outperforming state-of-the-art diffusion models such as conditional denoising
diffusion probabilistic models (cDDPM) and patch-based diffusion methods.
Moreover, its one-step inference enables reconstruction in just 0.19s per 2D
slice, representing over a 700-fold speedup compared to cDDPM (135s) and
surpassing diffusionGAN (0.58s), the second fastest. This combination of
accuracy and efficiency makes I$^2$SB highly suitable for real-time or clinical
deployment.

</details>


### [143] [Guiding WaveMamba with Frequency Maps for Image Debanding](https://arxiv.org/abs/2508.11331)
*Xinyi Wang,Smaranda Tasmoc,Nantheera Anantrasirichai,Angeliki Katsenou*

Main category: eess.IV

TL;DR: A method using Wavelet State Space Model and frequency masking map effectively reduces banding artifacts in low-bitrate compressed images, outperforming state-of-the-art techniques.


<details>
  <summary>Details</summary>
Motivation: Banding artifacts degrade visual quality in low-bitrate compressed images, especially in smooth regions like skies, and are common in user-generated content due to repeated transcoding.

Method: The proposed method employs the Wavelet State Space Model and a frequency masking map to restore banding while preserving high-frequency details.

Result: The method achieves a DBI value of 0.082 on BAND-2k, outperforming state-of-the-art techniques and preserving image textures.

Conclusion: The post-processing approach effectively suppresses banding artifacts and maintains image quality, with code and supplementary materials available for further use.

Abstract: Compression at low bitrates in modern codecs often introduces banding
artifacts, especially in smooth regions such as skies. These artifacts degrade
visual quality and are common in user-generated content due to repeated
transcoding. We propose a banding restoration method that employs the Wavelet
State Space Model and a frequency masking map to preserve high-frequency
details. Furthermore, we provide a benchmark of open-source banding restoration
methods and evaluate their performance on two public banding image datasets.
Experimentation on the available datasets suggests that the proposed
post-processing approach effectively suppresses banding compared to the
state-of-the-art method (a DBI value of 0.082 on BAND-2k) while preserving
image textures. Visual inspections of the results confirm this. Code and
supplementary material are available at:
https://github.com/xinyiW915/Debanding-PCS2025.

</details>


### [144] [AnatoMaskGAN: GNN-Driven Slice Feature Fusion and Noise Augmentation for Medical Semantic Image Synthesis](https://arxiv.org/abs/2508.11375)
*Zonglin Wu,Yule Xue,Qianxiang Hu,Yaoyao Feng,Yuqi Ma,Shanxiong Chen*

Main category: eess.IV

TL;DR: AnatoMaskGAN improves medical image synthesis by embedding spatial features, enhancing diversity, and optimizing texture, outperforming state-of-the-art methods in accuracy and quality.


<details>
  <summary>Details</summary>
Motivation: Current GAN-based methods lack spatial consistency and diversity in complex medical scans, limiting their effectiveness in data augmentation and analysis.

Method: Proposes AnatoMaskGAN with a GNN-based slice-feature fusion module, 3D spatial noise injection, and a grayscale-texture classifier to enhance spatial and contextual modeling.

Result: Achieves PSNR of 26.50 dB (0.43 dB higher than SOTA) on L2R-OASIS and SSIM of 0.8602 (0.48 pp gain) on L2R-Abdomen CT.

Conclusion: Each component of AnatoMaskGAN significantly contributes to performance, validating its design for accurate and perceptually superior medical image synthesis.

Abstract: Medical semantic-mask synthesis boosts data augmentation and analysis, yet
most GAN-based approaches still produce one-to-one images and lack spatial
consistency in complex scans. To address this, we propose AnatoMaskGAN, a novel
synthesis framework that embeds slice-related spatial features to precisely
aggregate inter-slice contextual dependencies, introduces diverse
image-augmentation strategies, and optimizes deep feature learning to improve
performance on complex medical images. Specifically, we design a GNN-based
strongly correlated slice-feature fusion module to model spatial relationships
between slices and integrate contextual information from neighboring slices,
thereby capturing anatomical details more comprehensively; we introduce a
three-dimensional spatial noise-injection strategy that weights and fuses
spatial features with noise to enhance modeling of structural diversity; and we
incorporate a grayscale-texture classifier to optimize grayscale distribution
and texture representation during generation. Extensive experiments on the
public L2R-OASIS and L2R-Abdomen CT datasets show that AnatoMaskGAN raises PSNR
on L2R-OASIS to 26.50 dB (0.43 dB higher than the current state of the art) and
achieves an SSIM of 0.8602 on L2R-Abdomen CT--a 0.48 percentage-point gain over
the best model, demonstrating its superiority in reconstruction accuracy and
perceptual quality. Ablation studies that successively remove the slice-feature
fusion module, spatial 3D noise-injection strategy, and grayscale-texture
classifier reveal that each component contributes significantly to PSNR, SSIM,
and LPIPS, further confirming the independent value of each core design in
enhancing reconstruction accuracy and perceptual quality.

</details>


### [145] [LKFMixer: Exploring Large Kernel Feature For Efficient Image Super-Resolution](https://arxiv.org/abs/2508.11391)
*Yinggan Tang,Quanwei Hu*

Main category: eess.IV

TL;DR: LKFMixer, a CNN model with large kernels, mimics self-attention for SR, outperforming SOTA methods in performance and speed.


<details>
  <summary>Details</summary>
Motivation: To address the computational inefficiency of self-attention in SR while retaining non-local feature capture.

Method: Uses large (31x31) convolutional kernels for non-local features, coordinate decomposition for efficiency, SFMB for spatial-channel focus, and FSB for adaptive feature weighting.

Result: Outperforms SOTA methods (e.g., 0.6dB PSNR gain over SwinIR-light) with 5x faster inference.

Conclusion: LKFMixer effectively balances performance and efficiency for SR tasks.

Abstract: The success of self-attention (SA) in Transformer demonstrates the importance
of non-local information to image super-resolution (SR), but the huge computing
power required makes it difficult to implement lightweight models. To solve
this problem, we propose a pure convolutional neural network (CNN) model,
LKFMixer, which utilizes large convolutional kernel to simulate the ability of
self-attention to capture non-local features. Specifically, we increase the
kernel size to 31 to obtain the larger receptive field as possible, and reduce
the parameters and computations by coordinate decomposition. Meanwhile, a
spatial feature modulation block (SFMB) is designed to enhance the focus of
feature information on both spatial and channel dimension. In addition, by
introducing feature selection block (FSB), the model can adaptively adjust the
weights between local features and non-local features. Extensive experiments
show that the proposed LKFMixer family outperform other state-of-the-art (SOTA)
methods in terms of SR performance and reconstruction quality. In particular,
compared with SwinIR-light on Manga109 dataset, LKFMixer-L achieves 0.6dB PSNR
improvement at $\times$4 scale, while the inference speed is $\times$5 times
faster. The code is available at https://github.com/Supereeeee/LKFMixer.

</details>


### [146] [Subcortical Masks Generation in CT Images via Ensemble-Based Cross-Domain Label Transfer](https://arxiv.org/abs/2508.11450)
*Augustine X. W. Lee,Pak-Hei Yeung,Jagath C. Rajapakse*

Main category: eess.IV

TL;DR: An automatic ensemble framework generates high-quality subcortical segmentation labels for CT scans by leveraging MRI-based models, creating the first open-source CT subcortical segmentation dataset.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of labeled CT data for subcortical segmentation, which is crucial for brain anatomy understanding and diagnosing brain injuries and disorders.

Method: Proposes an ensemble pipeline to integrate MRI-based models and apply it to unannotated paired MRI-CT data, generating a CT segmentation dataset.

Result: The framework outperforms others on public datasets, and models trained on the generated dataset show improved segmentation performance.

Conclusion: The work provides a valuable open-source resource for CT subcortical segmentation, advancing research in neuroimaging.

Abstract: Subcortical segmentation in neuroimages plays an important role in
understanding brain anatomy and facilitating computer-aided diagnosis of
traumatic brain injuries and neurodegenerative disorders. However, training
accurate automatic models requires large amounts of labelled data. Despite the
availability of publicly available subcortical segmentation datasets for
Magnetic Resonance Imaging (MRI), a significant gap exists for Computed
Tomography (CT). This paper proposes an automatic ensemble framework to
generate high-quality subcortical segmentation labels for CT scans by
leveraging existing MRI-based models. We introduce a robust ensembling pipeline
to integrate them and apply it to unannotated paired MRI-CT data, resulting in
a comprehensive CT subcortical segmentation dataset. Extensive experiments on
multiple public datasets demonstrate the superior performance of our proposed
framework. Furthermore, using our generated CT dataset, we train segmentation
models that achieve improved performance on related segmentation tasks. To
facilitate future research, we make our source code, generated dataset, and
trained models publicly available at
https://github.com/SCSE-Biomedical-Computing-Group/CT-Subcortical-Segmentation,
marking the first open-source release for CT subcortical segmentation to the
best of our knowledge.

</details>


### [147] [Semi-Supervised Learning with Online Knowledge Distillation for Skin Lesion Classification](https://arxiv.org/abs/2508.11511)
*Siyamalan Manivannan*

Main category: eess.IV

TL;DR: A semi-supervised deep learning approach using ensemble learning and online knowledge distillation improves skin lesion classification, reducing the need for extensive labeled data.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on fully supervised learning, which requires costly labeled data. This study aims to reduce the annotation burden.

Method: Trains an ensemble of CNN models with online knowledge distillation to transfer insights among models, enhancing individual and ensemble performance.

Result: The knowledge-distilled individual model outperforms independently trained models, achieving state-of-the-art results on benchmark datasets.

Conclusion: The approach provides a resource-efficient solution for skin lesion classification, reducing reliance on labeled data while maintaining high performance.

Abstract: Deep Learning has emerged as a promising approach for skin lesion analysis.
However, existing methods mostly rely on fully supervised learning, requiring
extensive labeled data, which is challenging and costly to obtain. To alleviate
this annotation burden, this study introduces a novel semi-supervised deep
learning approach that integrates ensemble learning with online knowledge
distillation for enhanced skin lesion classification. Our methodology involves
training an ensemble of convolutional neural network models, using online
knowledge distillation to transfer insights from the ensemble to its members.
This process aims to enhance the performance of each model within the ensemble,
thereby elevating the overall performance of the ensemble itself.
Post-training, any individual model within the ensemble can be deployed at test
time, as each member is trained to deliver comparable performance to the
ensemble. This is particularly beneficial in resource-constrained environments.
Experimental results demonstrate that the knowledge-distilled individual model
performs better than independently trained models. Our approach demonstrates
superior performance on both the \emph{International Skin Imaging
Collaboration} 2018 and 2019 public benchmark datasets, surpassing current
state-of-the-art results. By leveraging ensemble learning and online knowledge
distillation, our method reduces the need for extensive labeled data while
providing a more resource-efficient solution for skin lesion classification in
real-world scenarios.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [148] [Temporally-Similar Structure-Aware Spatiotemporal Fusion of Satellite Images](https://arxiv.org/abs/2508.11259)
*Ryosuke Isono,Shunsuke Ono*

Main category: eess.SP

TL;DR: TSSTF is a spatiotemporal fusion framework for satellite images that improves robustness to noise while preserving fine spatial details using TGTV and TGEC mechanisms.


<details>
  <summary>Details</summary>
Motivation: Address the trade-off between spatial and temporal resolution in satellite images, and enhance robustness against noise without oversmoothing or artifacts.

Method: Introduces Temporally-Guided Total Variation (TGTV) and Temporally-Guided Edge Constraint (TGEC) to preserve spatial structure. Formulates ST fusion as a constrained optimization problem solved via a primal-dual splitting method.

Result: TSSTF matches state-of-the-art methods in noise-free conditions and outperforms them under noisy conditions. Provides recommended parameters for reproducibility.

Conclusion: TSSTF effectively balances spatial detail preservation and noise robustness, offering practical utility for satellite image fusion.

Abstract: This paper proposes a novel spatiotemporal (ST) fusion framework for
satellite images, named Temporally-Similar Structure-Aware ST fusion (TSSTF).
ST fusion is a promising approach to address the trade-off between the spatial
and temporal resolution of satellite images. In real-world scenarios, observed
satellite images are severely degraded by noise due to measurement equipment
and environmental conditions. Consequently, some recent studies have focused on
enhancing the robustness of ST fusion methods against noise. However, existing
noise-robust ST fusion approaches often fail to capture fine spatial structure,
leading to oversmoothing and artifacts. To address this issue, TSSTF introduces
two key mechanisms: Temporally-Guided Total Variation (TGTV) and
Temporally-Guided Edge Constraint (TGEC). TGTV is a novel regularization
function that promotes spatial piecewise smoothness while preserving structural
details, guided by a reference high spatial resolution image acquired on a
nearby date. TGEC enforces consistency in edge locations between two temporally
adjacent images, while allowing for spectral variations. We formulate the ST
fusion task as a constrained optimization problem incorporating TGTV and TGEC,
and develop an efficient algorithm based on a preconditioned primal-dual
splitting method. Experimental results demonstrate that TSSTF performs
comparably to state-of-the-art methods under noise-free conditions and
outperforms them under noisy conditions. Additionally, we provide a
comprehensive set of recommended parameter values that consistently yield high
performance across diverse target regions and noise conditions, aiming to
enhance reproducibility and practical utility.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [149] [Allen: Rethinking MAS Design through Step-Level Policy Autonomy](https://arxiv.org/abs/2508.11294)
*Qiangong Zhou,Zhiting Wang,Mingyou Yao,Zongyang Liu*

Main category: cs.MA

TL;DR: Allen is a Multi-Agent System (MAS) designed to enhance policy autonomy and balance collaborative efficiency, task supervision, and human oversight in complex networks.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in current MAS design, such as improving policy autonomy and balancing collaboration with control.

Method: Redefines the basic execution unit in MAS, using a four-tier state architecture (Task, Stage, Agent, Step) for topological optimization and controllable progress.

Result: Achieves unprecedented policy autonomy while maintaining controllability in collaborative structures.

Conclusion: Allen successfully balances autonomy and control in MAS, with its code open-sourced for further development.

Abstract: We introduce a new Multi-Agent System (MAS) - Allen, designed to address two
core challenges in current MAS design: (1) improve system's policy autonomy,
empowering agents to dynamically adapt their behavioral strategies, and (2)
achieving the trade-off between collaborative efficiency, task supervision, and
human oversight in complex network topologies.
  Our core insight is to redefine the basic execution unit in the MAS, allowing
agents to autonomously form different patterns by combining these units. We
have constructed a four-tier state architecture (Task, Stage, Agent, Step) to
constrain system behavior from both task-oriented and execution-oriented
perspectives. This achieves a unification of topological optimization and
controllable progress.
  Allen grants unprecedented Policy Autonomy, while making a trade-off for the
controllability of the collaborative structure. The project code has been open
source at: https://github.com/motern88/Allen

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [150] [StyleMM: Stylized 3D Morphable Face Model via Text-Driven Aligned Image Translation](https://arxiv.org/abs/2508.11203)
*Seungmi Lee,Kwan Yun,Junyong Noh*

Main category: cs.GR

TL;DR: StyleMM creates a stylized 3D Morphable Model (3DMM) using text descriptions, preserving facial attributes while transferring style via diffusion models.


<details>
  <summary>Details</summary>
Motivation: To enable stylized 3D face generation with explicit control over shape, expression, and texture, while maintaining identity and alignment.

Method: Fine-tunes pre-trained mesh deformation and texture generators using stylized images from text-guided diffusion models, preserving facial attributes.

Result: Outperforms state-of-the-art in identity diversity and stylization, enabling feed-forward generation of stylized 3D faces.

Conclusion: StyleMM successfully integrates text-guided style transfer with 3DMM, offering high-quality, controllable stylized face generation.

Abstract: We introduce StyleMM, a novel framework that can construct a stylized 3D
Morphable Model (3DMM) based on user-defined text descriptions specifying a
target style. Building upon a pre-trained mesh deformation network and a
texture generator for original 3DMM-based realistic human faces, our approach
fine-tunes these models using stylized facial images generated via text-guided
image-to-image (i2i) translation with a diffusion model, which serve as
stylization targets for the rendered mesh. To prevent undesired changes in
identity, facial alignment, or expressions during i2i translation, we introduce
a stylization method that explicitly preserves the facial attributes of the
source image. By maintaining these critical attributes during image
stylization, the proposed approach ensures consistent 3D style transfer across
the 3DMM parameter space through image-based training. Once trained, StyleMM
enables feed-forward generation of stylized face meshes with explicit control
over shape, expression, and texture parameters, producing meshes with
consistent vertex connectivity and animatability. Quantitative and qualitative
evaluations demonstrate that our approach outperforms state-of-the-art methods
in terms of identity-level facial diversity and stylization capability. The
code and videos are available at
[kwanyun.github.io/stylemm_page](kwanyun.github.io/stylemm_page).

</details>


### [151] [SPG: Style-Prompting Guidance for Style-Specific Content Creation](https://arxiv.org/abs/2508.11476)
*Qian Liang,Zichong Chen,Yang Zhou,Hui Huang*

Main category: cs.GR

TL;DR: A novel method called Style-Prompting Guidance (SPG) is introduced to control visual styles in text-to-image diffusion models, ensuring both semantic fidelity and style consistency.


<details>
  <summary>Details</summary>
Motivation: Controlling the visual style of outputs in text-to-image diffusion models is challenging, prompting the need for a method like SPG.

Method: SPG constructs a style noise vector and uses its directional deviation from unconditional noise to guide the diffusion process toward the target style, integrating with Classifier-Free Guidance (CFG).

Result: SPG achieves semantic fidelity and style consistency, is compatible with frameworks like ControlNet and IPAdapter, and outperforms state-of-the-art methods.

Conclusion: SPG is a simple, robust, and widely applicable solution for style-specific image generation in diffusion models.

Abstract: Although recent text-to-image (T2I) diffusion models excel at aligning
generated images with textual prompts, controlling the visual style of the
output remains a challenging task. In this work, we propose Style-Prompting
Guidance (SPG), a novel sampling strategy for style-specific image generation.
SPG constructs a style noise vector and leverages its directional deviation
from unconditional noise to guide the diffusion process toward the target style
distribution. By integrating SPG with Classifier-Free Guidance (CFG), our
method achieves both semantic fidelity and style consistency. SPG is simple,
robust, and compatible with controllable frameworks like ControlNet and
IPAdapter, making it practical and widely applicable. Extensive experiments
demonstrate the effectiveness and generality of our approach compared to
state-of-the-art methods. Code is available at
https://github.com/Rumbling281441/SPG.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [152] [BeyondWeb: Lessons from Scaling Synthetic Data for Trillion-scale Pretraining](https://arxiv.org/abs/2508.10975)
*Pratyush Maini,Vineeth Dorna,Parth Doshi,Aldo Carranza,Fan Pan,Jack Urbanek,Paul Burstein,Alex Fang,Alvin Deng,Amro Abbas,Brett Larsen,Cody Blakeney,Charvi Bannur,Christina Baek,Darren Teh,David Schwab,Haakon Mongstad,Haoli Yin,Josh Wills,Kaleigh Mentzer,Luke Merrick,Ricardo Monti,Rishabh Adiga,Siddharth Joshi,Spandan Das,Zhengping Wang,Bogdan Gaza,Ari Morcos,Matthew Leavitt*

Main category: cs.LG

TL;DR: BeyondWeb, a synthetic data generation framework, outperforms existing datasets like Cosmopedia and Nemotron-Synth, achieving faster training and better performance with insights on optimizing synthetic data quality.


<details>
  <summary>Details</summary>
Motivation: Addressing the diminishing returns of scaling data quantity in LLM pretraining by exploring synthetic data as a solution, while understanding the factors affecting its quality.

Method: Introduces BeyondWeb, a framework for generating high-quality synthetic pretraining data, comparing it against state-of-the-art datasets like Cosmopedia and Nemotron-Synth.

Result: BeyondWeb outperforms Cosmopedia by 5.1pp and Nemotron-Synth by 2.6pp, trains up to 7.7x faster than open web data, and shows superior performance with smaller models.

Conclusion: High-quality synthetic data requires joint optimization of multiple factors; naive approaches yield modest gains, while well-executed methods like BeyondWeb offer transformative improvements.

Abstract: Recent advances in large language model (LLM) pretraining have shown that
simply scaling data quantity eventually leads to diminishing returns, hitting a
data wall. In response, the use of synthetic data for pretraining has emerged
as a promising paradigm for pushing the frontier of performance. Despite this,
the factors affecting synthetic data quality remain poorly understood. In this
work, we introduce BeyondWeb, a synthetic data generation framework that
produces high-quality synthetic data for pretraining. BeyondWeb significantly
extends the capabilities of traditional web-scale datasets, outperforming
state-of-the-art synthetic pretraining datasets such as Cosmopedia and
Nemotron-CC's high-quality synthetic subset (Nemotron-Synth) by up to 5.1
percentage points (pp) and 2.6pp, respectively, when averaged across a suite of
14 benchmark evaluations. It delivers up to 7.7x faster training than open web
data and 2.7x faster than Nemotron-Synth. Remarkably, a 3B model trained for
180B tokens on BeyondWeb outperforms an 8B model trained for the same token
budget on Cosmopedia. We also present several insights from BeyondWeb on
synthetic data for pretraining: what drives its benefits, which data to
rephrase and how, and the impact of model size and family on data quality.
Overall, our work shows that there's no silver bullet for generating
high-quality synthetic pretraining data. The best outcomes require jointly
optimizing many factors, a challenging task that requires rigorous science and
practical expertise. Naive approaches can yield modest improvements,
potentially at great cost, while well-executed methods can yield transformative
improvements, as exemplified by BeyondWeb.

</details>


### [153] [Match & Choose: Model Selection Framework for Fine-tuning Text-to-Image Diffusion Models](https://arxiv.org/abs/2508.10993)
*Basile Lewandowski,Robert Birke,Lydia Y. Chen*

Main category: cs.LG

TL;DR: The paper introduces M&C, a framework for selecting the best pretrained text-to-image (T2I) model for fine-tuning on a target dataset, avoiding exhaustive testing.


<details>
  <summary>Details</summary>
Motivation: Public pretrained T2I models are widely available, but selecting the best one for fine-tuning on a target domain is challenging due to lack of performance indicators.

Method: M&C uses a matching graph with nodes (models/datasets) and edges (performance/similarity) to predict the best model via graph embedding and feature inputs.

Result: M&C predicts the best model in 61.3% of cases and a close alternative otherwise, outperforming baselines.

Conclusion: M&C efficiently addresses model selection for T2I fine-tuning, enhancing democratization of AI applications.

Abstract: Text-to-image (T2I) models based on diffusion and transformer architectures
advance rapidly. They are often pretrained on large corpora, and openly shared
on a model platform, such as HuggingFace. Users can then build up AI
applications, e.g., generating media contents, by adopting pretrained T2I
models and fine-tuning them on the target dataset. While public pretrained T2I
models facilitate the democratization of the models, users face a new
challenge: which model can be best fine-tuned based on the target data domain?
Model selection is well addressed in classification tasks, but little is known
in (pretrained) T2I models and their performance indication on the target
domain. In this paper, we propose the first model selection framework, M&C,
which enables users to efficiently choose a pretrained T2I model from a model
platform without exhaustively fine-tuning them all on the target dataset. The
core of M&C is a matching graph, which consists of: (i) nodes of available
models and profiled datasets, and (ii) edges of model-data and data-data pairs
capturing the fine-tuning performance and data similarity, respectively. We
then build a model that, based on the inputs of model/data feature, and,
critically, the graph embedding feature, extracted from the matching graph,
predicts the model achieving the best quality after fine-tuning for the target
domain. We evaluate M&C on choosing across ten T2I models for 32 datasets
against three baselines. Our results show that M&C successfully predicts the
best model for fine-tuning in 61.3% of the cases and a closely performing model
for the rest.

</details>


### [154] [How Causal Abstraction Underpins Computational Explanation](https://arxiv.org/abs/2508.11214)
*Atticus Geiger,Jacqueline Harding,Thomas Icard*

Main category: cs.LG

TL;DR: The paper explores how causal abstraction theory can explain computational implementation in cognitive behavior and AI, linking classical philosophy to modern machine learning.


<details>
  <summary>Details</summary>
Motivation: To understand what it takes for a system to implement computations over representations, using causal abstraction as a framework.

Method: Uses causal abstraction theory and examples from deep learning to analyze computational implementation and representation.

Result: Proposes an account of computational implementation grounded in causal abstraction, emphasizing generalization and prediction.

Conclusion: Causal abstraction provides a valuable framework for understanding computation and representation in cognitive and AI systems.

Abstract: Explanations of cognitive behavior often appeal to computations over
representations. What does it take for a system to implement a given
computation over suitable representational vehicles within that system? We
argue that the language of causality -- and specifically the theory of causal
abstraction -- provides a fruitful lens on this topic. Drawing on current
discussions in deep learning with artificial neural networks, we illustrate how
classical themes in the philosophy of computation and cognition resurface in
contemporary machine learning. We offer an account of computational
implementation grounded in causal abstraction, and examine the role for
representation in the resulting picture. We argue that these issues are most
profitably explored in connection with generalization and prediction.

</details>


### [155] [Group Fairness Meets the Black Box: Enabling Fair Algorithms on Closed LLMs via Post-Processing](https://arxiv.org/abs/2508.11258)
*Ruicheng Xian,Yuxuan Wan,Han Zhao*

Main category: cs.LG

TL;DR: A framework for deriving fair classifiers from closed-weight LLMs via prompting, ensuring group fairness without model fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of enforcing group fairness in high-stakes applications using closed-weight LLMs, which traditional methods cannot handle.

Method: Uses LLMs as feature extractors, strategically designing prompts for fairness criteria, and applies a fair algorithm to train a lightweight classifier post-hoc.

Result: Demonstrates strong accuracy-fairness tradeoffs on five datasets, outperforming traditional methods like head-tuning or training from scratch.

Conclusion: The proposed framework is data-efficient and effective for fair classification with both open and closed-weight LLMs.

Abstract: Instruction fine-tuned large language models (LLMs) enable a simple zero-shot
or few-shot prompting paradigm, also known as in-context learning, for building
prediction models. This convenience, combined with continued advances in LLM
capability, has the potential to drive their adoption across a broad range of
domains, including high-stakes applications where group fairness -- preventing
disparate impacts across demographic groups -- is essential. The majority of
existing approaches to enforcing group fairness on LLM-based classifiers rely
on traditional fair algorithms applied via model fine-tuning or head-tuning on
final-layer embeddings, but they are no longer applicable to closed-weight LLMs
under the in-context learning setting, which include some of the most capable
commercial models today, such as GPT-4, Gemini, and Claude. In this paper, we
propose a framework for deriving fair classifiers from closed-weight LLMs via
prompting: the LLM is treated as a feature extractor, and features are elicited
from its probabilistic predictions (e.g., token log probabilities) using
prompts strategically designed for the specified fairness criterion to obtain
sufficient statistics for fair classification; a fair algorithm is then applied
to these features to train a lightweight fair classifier in a post-hoc manner.
Experiments on five datasets, including three tabular ones, demonstrate strong
accuracy-fairness tradeoffs for the classifiers derived by our framework from
both open-weight and closed-weight LLMs; in particular, our framework is
data-efficient and outperforms fair classifiers trained on LLM embeddings
(i.e., head-tuning) or from scratch on raw tabular features.

</details>


### [156] [Generalize across Homophily and Heterophily: Hybrid Spectral Graph Pre-Training and Prompt Tuning](https://arxiv.org/abs/2508.11328)
*Haitong Luo,Suhang Wang,Weiyao Zhang,Ruiqi Meng,Xuying Meng,Yujun Zhang*

Main category: cs.LG

TL;DR: The paper introduces HS-GPPT, a framework for graph pre-training and prompt-tuning that ensures spectral alignment to improve knowledge transfer across diverse graph homophily levels.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to handle diverse spectral distributions in real-world graphs due to reliance on homophily-based low-frequency knowledge.

Method: Proposes HS-GPPT with a hybrid spectral filter backbone and local-global contrastive learning for spectral knowledge acquisition, and prompt graphs for spectral alignment.

Result: Extensive experiments show effectiveness in transductive and inductive learning settings.

Conclusion: HS-GPPT bridges spectral gaps, enabling efficient knowledge transfer across varying homophily levels.

Abstract: Graph ``pre-training and prompt-tuning'' aligns downstream tasks with
pre-trained objectives to enable efficient knowledge transfer under limited
supervision. However, existing methods rely on homophily-based low-frequency
knowledge, failing to handle diverse spectral distributions in real-world
graphs with varying homophily. Our theoretical analysis reveals a spectral
specificity principle: optimal knowledge transfer requires alignment between
pre-trained spectral filters and the intrinsic spectrum of downstream graphs.
Under limited supervision, large spectral gaps between pre-training and
downstream tasks impede effective adaptation. To bridge this gap, we propose
the HS-GPPT model, a novel framework that ensures spectral alignment throughout
both pre-training and prompt-tuning. We utilize a hybrid spectral filter
backbone and local-global contrastive learning to acquire abundant spectral
knowledge. Then we design prompt graphs to align the spectral distribution with
pretexts, facilitating spectral knowledge transfer across homophily and
heterophily. Extensive experiments validate the effectiveness under both
transductive and inductive learning settings. Our code is available at
https://anonymous.4open.science/r/HS-GPPT-62D2/.

</details>


### [157] [Boosting the Robustness-Accuracy Trade-off of SNNs by Robust Temporal Self-Ensemble](https://arxiv.org/abs/2508.11279)
*Jihang Wang,Dongcheng Zhao,Ruolin Chen,Qian Zhang,Yi Zeng*

Main category: cs.LG

TL;DR: The paper proposes Robust Temporal self-Ensemble (RTE), a training framework to enhance the adversarial robustness of Spiking Neural Networks (SNNs) by addressing temporal sub-network fragility and perturbation transferability.


<details>
  <summary>Details</summary>
Motivation: SNNs are energy-efficient but vulnerable to adversarial attacks, with their temporal dynamics posing unique robustness challenges.

Method: RTE combines robustness objectives into a unified loss and uses stochastic sampling for optimization, improving sub-network resilience and reducing temporal transferability.

Result: RTE outperforms existing methods in robust-accuracy trade-off and reshapes SNNs' internal robustness landscape.

Conclusion: The study emphasizes the role of temporal structure in adversarial robustness and provides a foundation for robust SNNs.

Abstract: Spiking Neural Networks (SNNs) offer a promising direction for
energy-efficient and brain-inspired computing, yet their vulnerability to
adversarial perturbations remains poorly understood. In this work, we revisit
the adversarial robustness of SNNs through the lens of temporal ensembling,
treating the network as a collection of evolving sub-networks across discrete
timesteps. This formulation uncovers two critical but underexplored
challenges-the fragility of individual temporal sub-networks and the tendency
for adversarial vulnerabilities to transfer across time. To overcome these
limitations, we propose Robust Temporal self-Ensemble (RTE), a training
framework that improves the robustness of each sub-network while reducing the
temporal transferability of adversarial perturbations. RTE integrates both
objectives into a unified loss and employs a stochastic sampling strategy for
efficient optimization. Extensive experiments across multiple benchmarks
demonstrate that RTE consistently outperforms existing training methods in
robust-accuracy trade-off. Additional analyses reveal that RTE reshapes the
internal robustness landscape of SNNs, leading to more resilient and temporally
diversified decision boundaries. Our study highlights the importance of
temporal structure in adversarial learning and offers a principled foundation
for building robust spiking models.

</details>


### [158] [Robust Convolution Neural ODEs via Contractivity-promoting regularization](https://arxiv.org/abs/2508.11432)
*Muhammad Zakwan,Liang Xu,Giancarlo Ferrari-Trecate*

Main category: cs.LG

TL;DR: The paper proposes using contraction theory to enhance the robustness of Convolutional Neural Ordinary Differential Equations (NODEs) against input noise and adversarial attacks.


<details>
  <summary>Details</summary>
Motivation: Neural networks are fragile to input noise and adversarial attacks, motivating the need for more robust models.

Method: The authors induce contractivity in NODEs during training via Jacobian-based regularization or weight regularization for slope-restricted activation functions.

Result: Contractive NODEs show improved robustness, with slight perturbations causing minimal output changes, demonstrated on MNIST and FashionMNIST datasets under noise and attacks.

Conclusion: Contractivity regularization effectively enhances NODE robustness, offering a promising approach for resilient deep learning models.

Abstract: Neural networks can be fragile to input noise and adversarial attacks.
  In this work, we consider Convolutional Neural Ordinary Differential
Equations (NODEs), a family of continuous-depth neural networks represented by
dynamical systems, and propose to use contraction theory to improve their
robustness.
  For a contractive dynamical system two trajectories starting from different
initial conditions converge to each other exponentially fast.
  Contractive Convolutional NODEs can enjoy increased robustness as slight
perturbations of the features do not cause a significant change in the output.
  Contractivity can be induced during training by using a regularization term
involving the Jacobian of the system dynamics.
  To reduce the computational burden, we show that it can also be promoted
using carefully selected weight regularization terms for a class of NODEs with
slope-restricted activation functions.
  The performance of the proposed regularizers is illustrated through benchmark
image classification tasks on MNIST and FashionMNIST datasets, where images are
corrupted by different kinds of noise and attacks.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [159] [Benchmarking Prosody Encoding in Discrete Speech Tokens](https://arxiv.org/abs/2508.11224)
*Kentaro Onda,Satoru Fukayama,Daisuke Saito,Nobuaki Minematsu*

Main category: cs.SD

TL;DR: The paper analyzes how discrete tokens from SSL models capture prosodic information in speech language models, aiming to improve their design.


<details>
  <summary>Details</summary>
Motivation: Discrete tokens from SSL models are used in speech language models, but their ability to capture prosodic features is understudied. This study aims to fill that gap.

Method: The study conducts a comprehensive analysis of prosodic encoding by testing sensitivity to artificially modified prosody.

Result: Findings provide insights into how well discrete tokens capture prosodic information.

Conclusion: The study offers practical guidelines for designing discrete tokens to better encode prosodic features in speech language models.

Abstract: Recently, discrete tokens derived from self-supervised learning (SSL) models
via k-means clustering have been actively studied as pseudo-text in speech
language models and as efficient intermediate representations for various
tasks. However, these discrete tokens are typically learned in advance,
separately from the training of language models or downstream tasks. As a
result, choices related to discretization, such as the SSL model used or the
number of clusters, must be made heuristically. In particular, speech language
models are expected to understand and generate responses that reflect not only
the semantic content but also prosodic features. Yet, there has been limited
research on the ability of discrete tokens to capture prosodic information. To
address this gap, this study conducts a comprehensive analysis focusing on
prosodic encoding based on their sensitivity to the artificially modified
prosody, aiming to provide practical guidelines for designing discrete tokens.

</details>


### [160] [LD-LAudio-V1: Video-to-Long-Form-Audio Generation Extension with Dual Lightweight Adapters](https://arxiv.org/abs/2508.11074)
*Haomin Zhang,Kristin Qi,Shuxin Yang,Zihao Chen,Chaofan Ding,Xinhan Di*

Main category: cs.SD

TL;DR: LD-LAudio-V1 introduces dual lightweight adapters for long-form video-to-audio synthesis, reducing artifacts and inconsistencies while improving metrics.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with long-form audio generation and rely on noisy datasets. The goal is to create high-quality, synchronized audio for silent videos.

Method: Extends state-of-the-art models with dual lightweight adapters and introduces a clean, annotated dataset for training.

Result: Significant improvements in metrics like $FD_{\text{passt}}$ (27.27%), $FD_{\text{panns}}$ (34.98%), and others, reducing artifacts and inconsistencies.

Conclusion: LD-LAudio-V1 advances long-form video-to-audio synthesis, supported by a new dataset for future research.

Abstract: Generating high-quality and temporally synchronized audio from video content
is essential for video editing and post-production tasks, enabling the creation
of semantically aligned audio for silent videos. However, most existing
approaches focus on short-form audio generation for video segments under 10
seconds or rely on noisy datasets for long-form video-to-audio zsynthesis. To
address these limitations, we introduce LD-LAudio-V1, an extension of
state-of-the-art video-to-audio models and it incorporates dual lightweight
adapters to enable long-form audio generation. In addition, we release a clean
and human-annotated video-to-audio dataset that contains pure sound effects
without noise or artifacts. Our method significantly reduces splicing artifacts
and temporal inconsistencies while maintaining computational efficiency.
Compared to direct fine-tuning with short training videos, LD-LAudio-V1
achieves significant improvements across multiple metrics: $FD_{\text{passt}}$
450.00 $\rightarrow$ 327.29 (+27.27%), $FD_{\text{panns}}$ 34.88 $\rightarrow$
22.68 (+34.98%), $FD_{\text{vgg}}$ 3.75 $\rightarrow$ 1.28 (+65.87%),
$KL_{\text{panns}}$ 2.49 $\rightarrow$ 2.07 (+16.87%), $KL_{\text{passt}}$ 1.78
$\rightarrow$ 1.53 (+14.04%), $IS_{\text{panns}}$ 4.17 $\rightarrow$ 4.30
(+3.12%), $IB_{\text{score}}$ 0.25 $\rightarrow$ 0.28 (+12.00%),
$Energy\Delta10\text{ms}$ 0.3013 $\rightarrow$ 0.1349 (+55.23%),
$Energy\Delta10\text{ms(vs.GT)}$ 0.0531 $\rightarrow$ 0.0288 (+45.76%), and
$Sem.\,Rel.$ 2.73 $\rightarrow$ 3.28 (+20.15%). Our dataset aims to facilitate
further research in long-form video-to-audio generation and is available at
https://github.com/deepreasonings/long-form-video2audio.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [161] [GenFlowRL: Shaping Rewards with Generative Object-Centric Flow in Visual Reinforcement Learning](https://arxiv.org/abs/2508.11049)
*Kelin Yu,Sheng Zhang,Harshit Soora,Furong Huang,Heng Huang,Pratap Tokekar,Ruohan Gao*

Main category: cs.RO

TL;DR: GenFlowRL introduces shaped rewards from generated flow to enhance robot learning, overcoming limitations of video generation and dataset scarcity.


<details>
  <summary>Details</summary>
Motivation: Addressing the dependency on high-quality generated data and fine-grained manipulation challenges in video-based robot learning.

Method: Uses generated flow trained from cross-embodiment datasets to derive shaped rewards, focusing on object-centric features.

Result: Demonstrates superior performance in 10 manipulation tasks across simulation and real-world evaluations.

Conclusion: GenFlowRL effectively leverages object-centric flow for robust and generalizable robot policies.

Abstract: Recent advances have shown that video generation models can enhance robot
learning by deriving effective robot actions through inverse dynamics. However,
these methods heavily depend on the quality of generated data and struggle with
fine-grained manipulation due to the lack of environment feedback. While
video-based reinforcement learning improves policy robustness, it remains
constrained by the uncertainty of video generation and the challenges of
collecting large-scale robot datasets for training diffusion models. To address
these limitations, we propose GenFlowRL, which derives shaped rewards from
generated flow trained from diverse cross-embodiment datasets. This enables
learning generalizable and robust policies from diverse demonstrations using
low-dimensional, object-centric features. Experiments on 10 manipulation tasks,
both in simulation and real-world cross-embodiment evaluations, demonstrate
that GenFlowRL effectively leverages manipulation features extracted from
generated object-centric flow, consistently achieving superior performance
across diverse and challenging scenarios. Our Project Page:
https://colinyu1.github.io/genflowrl

</details>


### [162] [Scene Graph-Guided Proactive Replanning for Failure-Resilient Embodied Agent](https://arxiv.org/abs/2508.11286)
*Che Rin Yu,Daewon Chae,Dabin Seo,Sangwon Lee,Hyeongwoo Im,Jinkyu Kim*

Main category: cs.RO

TL;DR: A proactive replanning framework for robots detects and corrects failures by comparing scene graphs, improving task success and robustness.


<details>
  <summary>Details</summary>
Motivation: Autonomous robots often lack adaptive awareness, leading to failures due to outdated assumptions. Existing methods react after failures, while proactive solutions rely on manual rules.

Method: The framework compares current RGB-D scene graphs with reference graphs from successful demonstrations, activating a reasoning module to adjust plans if mismatches are detected.

Result: Experiments in AI2-THOR show the approach detects semantic and spatial mismatches before failures, enhancing task success and robustness.

Conclusion: The proactive replanning framework effectively prevents failures by addressing mismatches early, outperforming reactive methods.

Abstract: When humans perform everyday tasks, we naturally adjust our actions based on
the current state of the environment. For instance, if we intend to put
something into a drawer but notice it is closed, we open it first. However,
many autonomous robots lack this adaptive awareness. They often follow
pre-planned actions that may overlook subtle yet critical changes in the scene,
which can result in actions being executed under outdated assumptions and
eventual failure. While replanning is critical for robust autonomy, most
existing methods respond only after failures occur, when recovery may be
inefficient or infeasible. While proactive replanning holds promise for
preventing failures in advance, current solutions often rely on manually
designed rules and extensive supervision. In this work, we present a proactive
replanning framework that detects and corrects failures at subtask boundaries
by comparing scene graphs constructed from current RGB-D observations against
reference graphs extracted from successful demonstrations. When the current
scene fails to align with reference trajectories, a lightweight reasoning
module is activated to diagnose the mismatch and adjust the plan. Experiments
in the AI2-THOR simulator demonstrate that our approach detects semantic and
spatial mismatches before execution failures occur, significantly improving
task success and robustness.

</details>


### [163] [Relative Position Matters: Trajectory Prediction and Planning with Polar Representation](https://arxiv.org/abs/2508.11492)
*Bozhou Zhang,Nan Song,Bingzhao Gao,Li Zhang*

Main category: cs.RO

TL;DR: Polaris, a novel method using Polar coordinates, improves trajectory prediction and planning in autonomous driving by better modeling spatial relationships compared to Cartesian-based approaches.


<details>
  <summary>Details</summary>
Motivation: Existing Cartesian-based methods inadequately capture the influence of surrounding traffic elements due to their inability to naturally represent relative distances and directions.

Method: Polaris operates entirely in Polar coordinates, explicitly modeling distance and direction variations with dedicated encoding and refinement modules.

Result: Polaris achieves state-of-the-art performance on Argoverse 2 and nuPlan benchmarks.

Conclusion: The Polar coordinate system provides a more intuitive and effective representation for trajectory prediction and planning in dynamic environments.

Abstract: Trajectory prediction and planning in autonomous driving are highly
challenging due to the complexity of predicting surrounding agents' movements
and planning the ego agent's actions in dynamic environments. Existing methods
encode map and agent positions and decode future trajectories in Cartesian
coordinates. However, modeling the relationships between the ego vehicle and
surrounding traffic elements in Cartesian space can be suboptimal, as it does
not naturally capture the varying influence of different elements based on
their relative distances and directions. To address this limitation, we adopt
the Polar coordinate system, where positions are represented by radius and
angle. This representation provides a more intuitive and effective way to model
spatial changes and relative relationships, especially in terms of distance and
directional influence. Based on this insight, we propose Polaris, a novel
method that operates entirely in Polar coordinates, distinguishing itself from
conventional Cartesian-based approaches. By leveraging the Polar
representation, this method explicitly models distance and direction variations
and captures relative relationships through dedicated encoding and refinement
modules, enabling more structured and spatially aware trajectory prediction and
planning. Extensive experiments on the challenging prediction (Argoverse 2) and
planning benchmarks (nuPlan) demonstrate that Polaris achieves state-of-the-art
performance.

</details>


### [164] [Visual Perception Engine: Fast and Flexible Multi-Head Inference for Robotic Vision Tasks](https://arxiv.org/abs/2508.11584)
*Jakub Łucki,Jonathan Becktor,Georgios Georgakis,Robert Royce,Shehryar Khattak*

Main category: cs.RO

TL;DR: VPEngine is a modular framework for efficient GPU usage in visual multitasking on robots, reducing redundancy and memory footprint by sharing a foundation model backbone across task-specific heads.


<details>
  <summary>Details</summary>
Motivation: Addressing the inefficiencies of deploying multiple ML models on resource-constrained robotic platforms, which leads to redundant computations and complex integration.

Method: VPEngine uses a shared foundation model backbone (e.g., DINOv2) to extract image representations, shared across parallel task-specific heads, avoiding GPU-CPU transfers. It leverages CUDA MPS for efficient GPU utilization.

Result: Achieves up to 3x speedup over sequential execution, maintains constant memory footprint, and enables dynamic task prioritization. Demonstrates real-time performance at ≥50 Hz on NVIDIA Jetson Orin AGX.

Conclusion: VPEngine offers an efficient, extensible, and accessible solution for visual multitasking in robotics, with significant performance improvements and open-source availability.

Abstract: Deploying multiple machine learning models on resource-constrained robotic
platforms for different perception tasks often results in redundant
computations, large memory footprints, and complex integration challenges. In
response, this work presents Visual Perception Engine (VPEngine), a modular
framework designed to enable efficient GPU usage for visual multitasking while
maintaining extensibility and developer accessibility. Our framework
architecture leverages a shared foundation model backbone that extracts image
representations, which are efficiently shared, without any unnecessary GPU-CPU
memory transfers, across multiple specialized task-specific model heads running
in parallel. This design eliminates the computational redundancy inherent in
feature extraction component when deploying traditional sequential models while
enabling dynamic task prioritization based on application demands. We
demonstrate our framework's capabilities through an example implementation
using DINOv2 as the foundation model with multiple task (depth, object
detection and semantic segmentation) heads, achieving up to 3x speedup compared
to sequential execution. Building on CUDA Multi-Process Service (MPS), VPEngine
offers efficient GPU utilization and maintains a constant memory footprint
while allowing per-task inference frequencies to be adjusted dynamically during
runtime. The framework is written in Python and is open source with ROS2 C++
(Humble) bindings for ease of use by the robotics community across diverse
robotic platforms. Our example implementation demonstrates end-to-end real-time
performance at $\geq$50 Hz on NVIDIA Jetson Orin AGX for TensorRT optimized
models.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [165] [Fluid Dynamics and Domain Reconstruction from Noisy Flow Images Using Physics-Informed Neural Networks and Quasi-Conformal Mapping](https://arxiv.org/abs/2508.11216)
*Han Zhang,Xue-Cheng Tai,Jean-Michel Morel,Raymond H. Chan*

Main category: math.NA

TL;DR: The paper presents a method for denoising blood flow images by solving an optimization problem using Physics-Informed Neural Networks and quasi-conformal mapping, validated on synthetic and real-like data.


<details>
  <summary>Details</summary>
Motivation: High-quality blood flow imaging is crucial for medical diagnosis but is challenging due to noise and artifacts from short acquisition times or device errors.

Method: The task is framed as an optimization problem, decomposed into fluid and geometry subproblems solved iteratively. A Physics-Informed Neural Network reconstructs the velocity field, while quasi-conformal mapping infers the flow region.

Result: Experiments on synthetic and real-like data show the method's effectiveness and robustness in denoising flow images.

Conclusion: The proposed framework successfully reconstructs high-quality flow images and is validated through ablation studies.

Abstract: Blood flow imaging provides important information for hemodynamic behavior
within the vascular system and plays an essential role in medical diagnosis and
treatment planning. However, obtaining high-quality flow images remains a
significant challenge. In this work, we address the problem of denoising flow
images that may suffer from artifacts due to short acquisition times or
device-induced errors. We formulate this task as an optimization problem, where
the objective is to minimize the discrepancy between the modeled velocity
field, constrained to satisfy the Navier-Stokes equations, and the observed
noisy velocity data. To solve this problem, we decompose it into two
subproblems: a fluid subproblem and a geometry subproblem. The fluid subproblem
leverages a Physics-Informed Neural Network to reconstruct the velocity field
from noisy observations, assuming a fixed domain. The geometry subproblem aims
to infer the underlying flow region by optimizing a quasi-conformal mapping
that deforms a reference domain. These two subproblems are solved in an
alternating Gauss-Seidel fashion, iteratively refining both the velocity field
and the domain. Upon convergence, the framework yields a high-quality
reconstruction of the flow image. We validate the proposed method through
experiments on synthetic flow data in a converging channel geometry under
varying levels of Gaussian noise, and on real-like flow data in an aortic
geometry with signal-dependent noise. The results demonstrate the effectiveness
and robustness of the approach. Additionally, ablation studies are conducted to
assess the influence of key hyperparameters.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [166] [Failures to Surface Harmful Contents in Video Large Language Models](https://arxiv.org/abs/2508.10974)
*Yuxin Cao,Wei Song,Derui Wang,Jingling Xue,Jin Song Dong*

Main category: cs.MM

TL;DR: VideoLLMs often omit harmful content in videos due to design flaws like sparse frame sampling, token downsampling, and weak visual-text integration, leading to over 90% omission rates.


<details>
  <summary>Details</summary>
Motivation: To expose safety gaps in VideoLLMs where harmful content is ignored despite visibility, revealing design flaws.

Method: Analyzed three flaws: sparse frame sampling, token downsampling, and encoder-decoder disconnection, and crafted zero-query black-box attacks.

Result: Over 90% omission rate of harmful content across five leading VideoLLMs, even when clearly visible.

Conclusion: Urgent need for improved sampling, token compression, and decoding to ensure semantic coverage and safety.

Abstract: Video Large Language Models (VideoLLMs) are increasingly deployed on numerous
critical applications, where users rely on auto-generated summaries while
casually skimming the video stream. We show that this interaction hides a
critical safety gap: if harmful content is embedded in a video, either as
full-frame inserts or as small corner patches, state-of-the-art VideoLLMs
rarely mention the harmful content in the output, despite its clear visibility
to human viewers. A root-cause analysis reveals three compounding design flaws:
(1) insufficient temporal coverage resulting from the sparse, uniformly spaced
frame sampling used by most leading VideoLLMs, (2) spatial information loss
introduced by aggressive token downsampling within sampled frames, and (3)
encoder-decoder disconnection, whereby visual cues are only weakly utilized
during text generation. Leveraging these insights, we craft three zero-query
black-box attacks, aligning with these flaws in the processing pipeline. Our
large-scale evaluation across five leading VideoLLMs shows that the harmfulness
omission rate exceeds 90% in most cases. Even when harmful content is clearly
present in all frames, these models consistently fail to identify it. These
results underscore a fundamental vulnerability in current VideoLLMs' designs
and highlight the urgent need for sampling strategies, token compression, and
decoding mechanisms that guarantee semantic coverage rather than speed alone.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [167] [The Next Phase of Scientific Fact-Checking: Advanced Evidence Retrieval from Complex Structured Academic Papers](https://arxiv.org/abs/2506.20844)
*Xingyu Deng,Xi Wang,Mark Stevenson*

Main category: cs.IR

TL;DR: The paper highlights the complexities of scientific fact-checking, critiques existing simplified approaches, and proposes advanced methods to address challenges like evidence retrieval, time-awareness, and handling multimodal content.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current scientific fact-checking systems, which oversimplify the problem by focusing on abstracts rather than full papers, and to explore advanced features for better performance.

Method: Identifies key challenges in evidence retrieval (e.g., semantic limitations, outdated information) and proposes solutions like structured parsing, citation tracking, and handling multimodal content. Preliminary experiments support these ideas.

Result: Preliminary experiments validate the identified challenges and suggest potential solutions, though full results are not detailed.

Conclusion: The paper advocates for a specialized IR system to advance scientific fact-checking, addressing real-world complexities like evolving knowledge and multimodal content.

Abstract: Scientific fact-checking aims to determine the veracity of scientific claims
by retrieving and analysing evidence from research literature. The problem is
inherently more complex than general fact-checking since it must accommodate
the evolving nature of scientific knowledge, the structural complexity of
academic literature and the challenges posed by long-form, multimodal
scientific expression. However, existing approaches focus on simplified
versions of the problem based on small-scale datasets consisting of abstracts
rather than full papers, thereby avoiding the distinct challenges associated
with processing complete documents. This paper examines the limitations of
current scientific fact-checking systems and reveals the many potential
features and resources that could be exploited to advance their performance. It
identifies key research challenges within evidence retrieval, including (1)
evidence-driven retrieval that addresses semantic limitations and topic
imbalance (2) time-aware evidence retrieval with citation tracking to mitigate
outdated information, (3) structured document parsing to leverage long-range
context, (4) handling complex scientific expressions, including tables,
figures, and domain-specific terminology and (5) assessing the credibility of
scientific literature. Preliminary experiments were conducted to substantiate
these challenges and identify potential solutions. This perspective paper aims
to advance scientific fact-checking with a specialised IR system tailored for
real-world applications.

</details>


### [168] [PaperRegister: Boosting Flexible-grained Paper Search via Hierarchical Register Indexing](https://arxiv.org/abs/2508.11116)
*Zhuoqun Li,Xuanang Chen,Hongyu Lin,Yaojie Lu,Xianpei Han,Le Sun*

Main category: cs.IR

TL;DR: PaperRegister introduces a hierarchical indexing and adaptive retrieval system for flexible-grained paper search, outperforming traditional abstract-based methods.


<details>
  <summary>Details</summary>
Motivation: Traditional paper search systems lack support for fine-grained queries due to reliance on abstract-based indexing, limiting flexibility as research deepens.

Method: Proposes PaperRegister with offline hierarchical indexing and online adaptive retrieval, transforming abstract-based indexes into hierarchical trees.

Result: Achieves state-of-the-art performance, excelling in fine-grained search scenarios.

Conclusion: PaperRegister is an effective solution for flexible-grained paper search, with potential for real-world applications.

Abstract: Paper search is an important activity for researchers, typically involving
using a query with description of a topic to find relevant papers. As research
deepens, paper search requirements may become more flexible, sometimes
involving specific details such as module configuration rather than being
limited to coarse-grained topics. However, previous paper search systems are
unable to meet these flexible-grained requirements, as these systems mainly
collect paper abstracts to construct index of corpus, which lack detailed
information to support retrieval by finer-grained queries. In this work, we
propose PaperRegister, consisted of offline hierarchical indexing and online
adaptive retrieval, transforming traditional abstract-based index into
hierarchical index tree for paper search, thereby supporting queries at
flexible granularity. Experiments on paper search tasks across a range of
granularity demonstrate that PaperRegister achieves the state-of-the-art
performance, and particularly excels in fine-grained scenarios, highlighting
the good potential as an effective solution for flexible-grained paper search
in real-world applications. Code for this work is in
https://github.com/Li-Z-Q/PaperRegister.

</details>


### [169] [+VeriRel: Verification Feedback to Enhance Document Retrieval for Scientific Fact Checking](https://arxiv.org/abs/2508.11122)
*Xingyu Deng,Xi Wang,Mark Stevenson*

Main category: cs.IR

TL;DR: The paper introduces +VeriRel, a method that improves document ranking for scientific fact checking by incorporating verification success, outperforming traditional relevance-based approaches.


<details>
  <summary>Details</summary>
Motivation: Existing methods for scientific fact checking rely on relevance-based document ranking, which may not effectively identify supporting evidence.

Method: Proposes +VeriRel, which integrates verification success into document ranking to better identify evidence for claims.

Result: +VeriRel achieves leading performance on three datasets (SciFact, SciFact-Open, Check-Covid) and improves downstream verification.

Conclusion: Integrating verification feedback into document ranking enhances scientific fact checking, with potential for future work on fine-grained relevance assessment.

Abstract: Identification of appropriate supporting evidence is critical to the success
of scientific fact checking. However, existing approaches rely on off-the-shelf
Information Retrieval algorithms that rank documents based on relevance rather
than the evidence they provide to support or refute the claim being checked.
This paper proposes +VeriRel which includes verification success in the
document ranking. Experimental results on three scientific fact checking
datasets (SciFact, SciFact-Open and Check-Covid) demonstrate consistently
leading performance by +VeriRel for document evidence retrieval and a positive
impact on downstream verification. This study highlights the potential of
integrating verification feedback to document relevance assessment for
effective scientific fact checking systems. It shows promising future work to
evaluate fine-grained relevance when examining complex documents for advanced
scientific fact checking.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [170] [Beyond Solving Math Quiz: Evaluating the Ability of Large Reasoning Models to Ask for Information](https://arxiv.org/abs/2508.11252)
*Youcheng Huang,Bowen Qin,Chen Huang,Duanyu Feng,Xi Yang,Wenqiang Lei*

Main category: cs.AI

TL;DR: The paper introduces a new dataset to evaluate Large Reasoning Models (LRMs) on incomplete problems, revealing their inability to proactively ask for missing information and highlighting issues like overthinking and hallucination.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks evaluate LRMs only on well-defined problems, missing the need for proactive information-seeking behavior in genuine intelligence.

Method: A new dataset of incomplete problems is created, and LRMs are systematically evaluated on their ability to handle such problems.

Result: LRMs fail to proactively ask for missing information and exhibit behaviors like overthinking and hallucination. Supervised fine-tuning shows potential but faces challenges.

Conclusion: The study highlights the need for LRMs to evolve beyond problem-solving to exhibit genuine intelligence, providing insights for future development.

Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable problem-solving
abilities in mathematics, as evaluated by existing benchmarks exclusively on
well-defined problems. However, such evaluation setup constitutes a critical
gap, since a genuine intelligent agent should not only solve problems (as a
math quiz solver), but also be able~to ask for information when the problems
lack sufficient information, enabling proactivity in responding users'
requests. To bridge such gap, we proposes a new dataset consisting of two types
of incomplete problems with diverse contexts. Based on the dataset, our
systematical evaluation of LRMs reveals their inability in proactively asking
for information. In addition, we uncover the behaviors related to overthinking
and hallucination of LRMs, and highlight the potential and challenges of
supervised fine-tuning in learning such ability. We hope to provide new
insights in developing LRMs with genuine intelligence, rather than just solving
problems.

</details>


### [171] [Inclusion Arena: An Open Platform for Evaluating Large Foundation Models with Real-World Apps](https://arxiv.org/abs/2508.11452)
*Kangyu Wang,Hongliang He,Lin Liu,Ruiqi Liang,Zhenzhong Lan,Jianguo Li*

Main category: cs.AI

TL;DR: Inclusion Arena is a live leaderboard for LLMs and MLLMs, ranking models based on real-world human feedback, using innovative methods like Placement Matches and Proximity Sampling for reliable rankings.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for LLMs and MLLMs rely on static datasets or general prompts, failing to reflect real-world performance. Inclusion Arena addresses this gap by integrating human feedback from actual applications.

Method: The platform uses pairwise model comparisons in natural user interactions, employing the Bradley-Terry model with Placement Matches for cold-start and Proximity Sampling for intelligent comparisons.

Result: Inclusion Arena provides reliable, stable rankings, higher data transitivity, and reduced manipulation risk compared to traditional benchmarks.

Conclusion: Inclusion Arena bridges the gap between model development and real-world applications, fostering optimized LLMs and MLLMs for practical use.

Abstract: Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs)
have ushered in a new era of AI capabilities, demonstrating near-human-level
performance across diverse scenarios. While numerous benchmarks (e.g., MMLU)
and leaderboards (e.g., Chatbot Arena) have been proposed to help evolve the
development of LLMs and MLLMs, most rely on static datasets or crowdsourced
general-domain prompts, often falling short of reflecting performance in
real-world applications. To bridge this critical gap, we present Inclusion
Arena, a live leaderboard that ranks models based on human feedback collected
directly from AI-powered applications. Our platform integrates pairwise model
comparisons into natural user interactions, ensuring evaluations reflect
practical usage scenarios. For robust model ranking, we employ the
Bradley-Terry model augmented with two key innovations: (1) Placement Matches,
a cold-start mechanism to quickly estimate initial ratings for newly integrated
models, and (2) Proximity Sampling, an intelligent comparison strategy that
prioritizes battles between models of similar capabilities to maximize
information gain and enhance rating stability. Extensive empirical analyses and
simulations demonstrate that Inclusion Arena yields reliable and stable
rankings, exhibits higher data transitivity compared to general crowdsourced
datasets, and significantly mitigates the risk of malicious manipulation. By
fostering an open alliance between foundation models and real-world
applications, Inclusion Arena aims to accelerate the development of LLMs and
MLLMs truly optimized for practical, user-centric deployments. The platform is
publicly accessible at https://doraemon.alipay.com/model-ranking.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [172] [Expressive Speech Retrieval using Natural Language Descriptions of Speaking Style](https://arxiv.org/abs/2508.11187)
*Wonjune Kang,Deb Roy*

Main category: eess.AS

TL;DR: The paper introduces expressive speech retrieval, focusing on retrieving speech by style (how something was said) rather than content (what was said), using joint embeddings of speech and text descriptions.


<details>
  <summary>Details</summary>
Motivation: Prior work focused on speech retrieval based on content; this paper aims to enable retrieval based on speaking style using natural language descriptions.

Method: Train speech and text encoders to embed into a joint latent space, allowing text prompts to retrieve matching speech. Analyzes encoder architectures, training criteria, and prompt augmentation.

Result: Strong retrieval performance (Recall@k) on datasets with 22 speaking styles.

Conclusion: The framework effectively retrieves expressive speech using text descriptions, advancing beyond content-based retrieval.

Abstract: We introduce the task of expressive speech retrieval, where the goal is to
retrieve speech utterances spoken in a given style based on a natural language
description of that style. While prior work has primarily focused on performing
speech retrieval based on what was said in an utterance, we aim to do so based
on how something was said. We train speech and text encoders to embed speech
and text descriptions of speaking styles into a joint latent space, which
enables using free-form text prompts describing emotions or styles as queries
to retrieve matching expressive speech segments. We perform detailed analyses
of various aspects of our proposed framework, including encoder architectures,
training criteria for effective cross-modal alignment, and prompt augmentation
for improved generalization to arbitrary text queries. Experiments on multiple
datasets encompassing 22 speaking styles demonstrate that our approach achieves
strong retrieval performance as measured by Recall@k.

</details>


### [173] [Emphasis Sensitivity in Speech Representations](https://arxiv.org/abs/2508.11566)
*Shaun Cassini,Thomas Hain,Anton Ragni*

Main category: eess.AS

TL;DR: The paper examines how modern speech models encode prosodic emphasis, proposing a residual-based framework to analyze differences between neutral and emphasized words. Results show structured encoding of emphasis, with ASR models refining this encoding further.


<details>
  <summary>Details</summary>
Motivation: To determine if speech models systematically differentiate between emphasized and neutral words, addressing gaps in prior work that overlooked relational structure.

Method: A residual-based framework is introduced, defining emphasis as the difference between paired neutral and emphasized word representations. Analysis is conducted on self-supervised and ASR fine-tuned models.

Result: Residuals correlate with duration changes and perform poorly at word identity prediction, indicating relational encoding. ASR models show residuals in a 50% more compact subspace, suggesting refined emphasis encoding.

Conclusion: Prosodic emphasis is encoded relationally in speech models, with ASR fine-tuning enhancing its structured, low-dimensional representation.

Abstract: This work investigates whether modern speech models are sensitive to prosodic
emphasis - whether they encode emphasized and neutral words in systematically
different ways. Prior work typically relies on isolated acoustic correlates
(e.g., pitch, duration) or label prediction, both of which miss the relational
structure of emphasis. This paper proposes a residual-based framework, defining
emphasis as the difference between paired neutral and emphasized word
representations. Analysis on self-supervised speech models shows that these
residuals correlate strongly with duration changes and perform poorly at word
identity prediction, indicating a structured, relational encoding of prosodic
emphasis. In ASR fine-tuned models, residuals occupy a subspace up to 50% more
compact than in pre-trained models, further suggesting that emphasis is encoded
as a consistent, low-dimensional transformation that becomes more structured
with task-specific learning.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [174] [Diffusion is a code repair operator and generator](https://arxiv.org/abs/2508.11110)
*Mukul Singh,Gust Verbruggen,Vu Le,Sumit Gulwani*

Main category: cs.SE

TL;DR: Code diffusion models can be repurposed for last-mile repair tasks by leveraging noise addition and intermediate sampling.


<details>
  <summary>Details</summary>
Motivation: To exploit the resemblance between late-stage diffusion steps and last-mile repairs in code snippets for practical applications.

Method: Uses pre-trained code diffusion models to add noise to broken code and resume diffusion, and samples intermediate and final programs for training data.

Result: Experiments on Python, Excel, and PowerShell show potential for last-mile repair applications.

Conclusion: Code diffusion models are adaptable for last-mile repair tasks, offering efficient training data generation and repair capabilities.

Abstract: Code diffusion models generate code by iteratively removing noise from the
latent representation of a code snippet. During later steps of the diffusion
process, when the code snippet has almost converged, differences between
discrete representations of these snippets look like last-mile repairs applied
to broken or incomplete code. We evaluate the extent to which this resemblance
can be exploited to leverage pre-trained code diffusion models for the problem
of last-mile repair by considering two applications with significant potential.
First, we can leverage the diffusion model for last-mile repair by adding noise
to a broken code snippet and resuming the diffusion process. Second, we can
leverage the diffusion model to generate arbitrary amount of training data for
last-mile repair tasks (that are computationally more efficient) by sampling an
intermediate program (input) and the final program (output) from the diffusion
process. We perform experiments on 3 domains (Python, Excel and PowerShell) to
evaluate applications, as well as analyze properties.

</details>


### [175] [ORFuzz: Fuzzing the "Other Side" of LLM Safety -- Testing Over-Refusal](https://arxiv.org/abs/2508.11222)
*Haonan Zhang,Dongxia Wang,Yi Liu,Kexin Chen,Jiashui Wang,Xinlei Ying,Long Liu,Wenhai Wang*

Main category: cs.SE

TL;DR: ORFuzz is an evolutionary testing framework designed to detect and analyze over-refusal in LLMs, outperforming existing methods with a 6.98% average detection rate and creating a benchmark (ORFuzzSet) with a 63.56% over-refusal rate.


<details>
  <summary>Details</summary>
Motivation: Current methods for testing LLM over-refusal are inadequate, leading to unreliable and unusable models due to overly conservative safety measures.

Method: ORFuzz integrates safety category-aware seed selection, adaptive mutator optimization using reasoning LLMs, and OR-Judge, a human-aligned judge model.

Result: ORFuzz detects over-refusal at double the rate of baselines (6.98%) and creates ORFuzzSet, a benchmark with a 63.56% over-refusal rate across 10 LLMs.

Conclusion: ORFuzz and ORFuzzSet offer a robust testing framework and benchmark, advancing the development of reliable and trustworthy LLM-based systems.

Abstract: Large Language Models (LLMs) increasingly exhibit over-refusal - erroneously
rejecting benign queries due to overly conservative safety measures - a
critical functional flaw that undermines their reliability and usability.
Current methods for testing this behavior are demonstrably inadequate,
suffering from flawed benchmarks and limited test generation capabilities, as
highlighted by our empirical user study. To the best of our knowledge, this
paper introduces the first evolutionary testing framework, ORFuzz, for the
systematic detection and analysis of LLM over-refusals. ORFuzz uniquely
integrates three core components: (1) safety category-aware seed selection for
comprehensive test coverage, (2) adaptive mutator optimization using reasoning
LLMs to generate effective test cases, and (3) OR-Judge, a human-aligned judge
model validated to accurately reflect user perception of toxicity and refusal.
Our extensive evaluations demonstrate that ORFuzz generates diverse, validated
over-refusal instances at a rate (6.98% average) more than double that of
leading baselines, effectively uncovering vulnerabilities. Furthermore,
ORFuzz's outputs form the basis of ORFuzzSet, a new benchmark of 1,855 highly
transferable test cases that achieves a superior 63.56% average over-refusal
rate across 10 diverse LLMs, significantly outperforming existing datasets.
ORFuzz and ORFuzzSet provide a robust automated testing framework and a
valuable community resource, paving the way for developing more reliable and
trustworthy LLM-based software systems.

</details>

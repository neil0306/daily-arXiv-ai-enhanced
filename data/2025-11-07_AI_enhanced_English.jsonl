{"id": "2511.03738", "pdf": "https://arxiv.org/pdf/2511.03738", "abs": "https://arxiv.org/abs/2511.03738", "authors": ["Pranav Bhandari", "Nicolas Fay", "Sanjeevan Selvaganapathy", "Amitava Datta", "Usman Naseem", "Mehwish Nasim"], "title": "Activation-Space Personality Steering: Hybrid Layer Selection for Stable Trait Control in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models exhibit implicit personalities in their generation, but\nreliably controlling or aligning these traits to meet specific needs remains an\nopen challenge. The need for effective mechanisms for behavioural manipulation\nof the model during generation is a critical gap in the literature that needs\nto be fulfilled. Personality-aware LLMs hold a promising direction towards this\nobjective. However, the relationship between these psychological constructs and\ntheir representations within LLMs remains underexplored and requires further\ninvestigation. Moreover, it is intriguing to understand and study the use of\nthese representations to steer the models' behaviour. We propose a novel\npipeline that extracts hidden state activations from transformer layers using\nthe Big Five Personality Traits (Openness, Conscientiousness, Extraversion,\nAgreeableness and Neuroticism), which is a comprehensive and empirically\nvalidated framework to model human personality applies low-rank subspace\ndiscovery methods, and identifies trait-specific optimal layers across\ndifferent model architectures for robust injection. The resulting\npersonality-aligned directions are then operationalised through a flexible\nsteering framework with dynamic layer selection, enabling precise control of\ntrait expression in LLM outputs. Our findings reveal that personality traits\noccupy a low-rank shared subspace, and that these latent structures can be\ntransformed into actionable mechanisms for effective steering through careful\nperturbations without impacting the fluency, variance and general capabilities,\nhelping to bridge the gap between psychological theory and practical model\nalignment.", "AI": {"tldr": "Proposes a pipeline to extract and control personality traits in LLMs using Big Five personality framework, enabling precise behavioral steering without affecting model capabilities.", "motivation": "Need for reliable mechanisms to control and align implicit personality traits in LLMs to meet specific requirements, as current methods lack effective behavioral manipulation during generation.", "method": "Extracts hidden state activations from transformer layers using Big Five Personality Traits, applies low-rank subspace discovery, identifies trait-specific optimal layers, and operationalizes through flexible steering framework with dynamic layer selection.", "result": "Personality traits occupy a low-rank shared subspace, and latent structures can be transformed into actionable steering mechanisms through careful perturbations without impacting fluency, variance, or general capabilities.", "conclusion": "Bridges the gap between psychological theory and practical model alignment by providing effective personality-aware steering mechanisms for LLMs."}}
{"id": "2511.03739", "pdf": "https://arxiv.org/pdf/2511.03739", "abs": "https://arxiv.org/abs/2511.03739", "authors": ["Eugenius Mario Situmorang", "Adila Alfa Krisnadhi", "Ari Wibisono"], "title": "TextualVerifier: Verify TextGrad Step-by-Step", "categories": ["cs.CL"], "comment": null, "summary": "TextGrad is a novel approach to text-based automatic differentiation that\nenables composite AI systems to perform optimization without explicit numerical\nequations. However, it currently lacks self-verification mechanisms that ensure\nreasoning validity in text-based decision making. This research introduces\nTextualVerifier, a verification framework that leverages chain-of-thought\nreasoning and majority voting with large language models to address this\nverification gap. TextualVerifier implements a four-stage workflow:\nchain-of-thought decomposition, variant generation, majority voting, and\nconsensus aggregation. It integrates non-invasively with TextGrad at both the\nloss function and optimization result verification stages. Experimental\nevaluation using the Gemini 1.5 Pro model is conducted in two phases: (1)\nstandalone evaluation on PRM800K, and (2) integrated evaluation with TextGrad\non GPQA-Diamond, MMLU-ML, and MMLU-CP benchmarks. Results show statistically\nsignificant improvements (p < 0.001). In phase one, TextualVerifier improves\nthe validity of reasoning steps by 29 percent. In phase two, integration into\nTextGrad loss function yields a 2.2 percentage point gain from 68.2 to 70.4\npercent with a moderate overhead of 5.9 LLM calls on average. Further\nevaluations of TextualVerifier versioning yield 8.08, 10.71, and 3.92\npercentage point improvements on GPQA, MMLU-ML, and MMLU-CP respectively.\nTextualVerifier thus presents the first self-verification framework for\nTextGrad through LLM-based techniques without requiring numerical gradients,\nenabling more reliable reasoning and opening new directions for verification in\ntext-based optimization.", "AI": {"tldr": "TextualVerifier is a self-verification framework that addresses the verification gap in TextGrad's text-based automatic differentiation by using chain-of-thought reasoning and majority voting with LLMs, significantly improving reasoning validity and optimization performance.", "motivation": "TextGrad enables text-based optimization without numerical equations but lacks self-verification mechanisms to ensure reasoning validity in text-based decision making, creating a critical gap in reliable AI systems.", "method": "TextualVerifier implements a four-stage workflow: chain-of-thought decomposition, variant generation, majority voting, and consensus aggregation. It integrates non-invasively with TextGrad at both loss function and optimization result verification stages using LLM-based techniques.", "result": "Experimental results show statistically significant improvements (p < 0.001): 29% improvement in reasoning step validity, 2.2 percentage point gain (68.2% to 70.4%) in TextGrad integration with moderate overhead, and versioning improvements of 8.08, 10.71, and 3.92 percentage points on GPQA, MMLU-ML, and MMLU-CP benchmarks respectively.", "conclusion": "TextualVerifier presents the first self-verification framework for TextGrad through LLM-based techniques without requiring numerical gradients, enabling more reliable reasoning and opening new directions for verification in text-based optimization."}}
{"id": "2511.03772", "pdf": "https://arxiv.org/pdf/2511.03772", "abs": "https://arxiv.org/abs/2511.03772", "authors": ["Stergios Chatzikyriakidis", "Dimitris Papadakis", "Sevasti-Ioanna Papaioannou", "Erofili Psaltaki"], "title": "GRDD+: An Extended Greek Dialectal Dataset with Cross-Architecture Fine-tuning Evaluation", "categories": ["cs.CL"], "comment": null, "summary": "We present an extended Greek Dialectal Dataset (GRDD+) 1that complements the\nexisting GRDD dataset with more data from Cretan, Cypriot, Pontic and Northern\nGreek, while we add six new varieties: Greco-Corsican, Griko (Southern Italian\nGreek), Maniot, Heptanesian, Tsakonian, and Katharevusa Greek. The result is a\ndataset with total size 6,374,939 words and 10 varieties. This is the first\ndataset with such variation and size to date. We conduct a number of\nfine-tuning experiments to see the effect of good quality dialectal data on a\nnumber of LLMs. We fine-tune three model architectures (Llama-3-8B,\nLlama-3.1-8B, Krikri-8B) and compare the results to frontier models\n(Claude-3.7-Sonnet, Gemini-2.5, ChatGPT-5).", "AI": {"tldr": "Extended Greek Dialectal Dataset (GRDD+) with 6.4M words covering 10 Greek varieties, used to fine-tune LLMs and compare with frontier models.", "motivation": "To create the first comprehensive Greek dialectal dataset with significant variation and size, enabling better dialectal language modeling.", "method": "Extended existing GRDD dataset with more Cretan, Cypriot, Pontic, Northern Greek data and added six new varieties. Fine-tuned three 8B parameter models (Llama-3-8B, Llama-3.1-8B, Krikri-8B) and compared with frontier models.", "result": "Created GRDD+ dataset with 6,374,939 words covering 10 Greek varieties - the largest and most varied Greek dialectal dataset to date.", "conclusion": "The study demonstrates the value of high-quality dialectal data for improving language models and provides a benchmark for Greek dialectal NLP tasks."}}
{"id": "2511.03823", "pdf": "https://arxiv.org/pdf/2511.03823", "abs": "https://arxiv.org/abs/2511.03823", "authors": ["Jan Koco\u0144", "Maciej Piasecki", "Arkadiusz Janz", "Teddy Ferdinan", "\u0141ukasz Radli\u0144ski", "Bart\u0142omiej Koptyra", "Marcin Oleksy", "Stanis\u0142aw Wo\u017aniak", "Pawe\u0142 Walkowiak", "Konrad Wojtasik", "Julia Moska", "Tomasz Naskr\u0119t", "Bartosz Walkowiak", "Mateusz Gniewkowski", "Kamil Szyc", "Dawid Motyka", "Dawid Banach", "Jonatan Dalasi\u0144ski", "Ewa Rudnicka", "Bart\u0142omiej Alberski", "Tomasz Walkowiak", "Aleksander Szcz\u0119sny", "Maciej Markiewicz", "Tomasz Berna\u015b", "Hubert Mazur", "Kamil \u017byta", "Mateusz Tykierko", "Grzegorz Chodak", "Tomasz Kajdanowicz", "Przemys\u0142aw Kazienko", "Agnieszka Karli\u0144ska", "Karolina Seweryn", "Anna Ko\u0142os", "Maciej Chrab\u0105szcz", "Katarzyna Lorenc", "Aleksandra Krasnod\u0119bska", "Artur Wilczek", "Katarzyna Dziewulska", "Paula Betscher", "Zofia Cie\u015bli\u0144ska", "Katarzyna Kowol", "Daria Miko\u015b", "Maciej Trzci\u0144ski", "Dawid Krutul", "Marek Koz\u0142owski", "S\u0142awomir Dadas", "Rafa\u0142 Po\u015bwiata", "Micha\u0142 Pere\u0142kiewicz", "Ma\u0142gorzata Gr\u0119bowiec", "Maciej Kazu\u0142a", "Marcin Bia\u0142as", "Roman Roszko", "Danuta Roszko", "Jurgita Vai\u010denonien\u0117", "Andrius Utka", "Pawe\u0142 Levchuk", "Pawe\u0142 Kowalski", "Irena Prawdzic-Jankowska", "Maciej Ogrodniczuk", "Monika Borys", "Anna Buli\u0144ska", "Wiktoria Gumienna", "Witold Kiera\u015b", "Dorota Komosi\u0144ska", "Katarzyna Krasnowska-Kiera\u015b", "\u0141ukasz Kobyli\u0144ski", "Martyna Lewandowska", "Marek \u0141azi\u0144ski", "Miko\u0142aj \u0141\u0105tkowski", "Dawid Mastalerz", "Beata Milewicz", "Agnieszka Anna Mykowiecka", "Angelika Peljak-\u0141api\u0144ska", "Sandra Penno", "Zuzanna Przybysz", "Micha\u0142 Rudolf", "Piotr Rybak", "Karolina Saputa", "Aleksandra Tomaszewska", "Aleksander Wawer", "Marcin Woli\u0144ski", "Joanna Wo\u0142oszyn", "Alina Wr\u00f3blewska", "Bartosz \u017buk", "Filip \u017barnecki", "Konrad Kaczy\u0144ski", "Anna Cichosz", "Zuzanna Deckert", "Monika Garnys", "Izabela Grabarczyk", "Wojciech Janowski", "Sylwia Karasi\u0144ska", "Aleksandra Kujawiak", "Piotr Misztela", "Maria Szyma\u0144ska", "Karolina Walkusz", "Igor Siek", "Jakub Kwiatkowski", "Piotr P\u0119zik"], "title": "PLLuM: A Family of Polish Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "83 pages, 19 figures", "summary": "Large Language Models (LLMs) play a central role in modern artificial\nintelligence, yet their development has been primarily focused on English,\nresulting in limited support for other languages. We present PLLuM (Polish\nLarge Language Model), the largest open-source family of foundation models\ntailored specifically for the Polish language. Developed by a consortium of\nmajor Polish research institutions, PLLuM addresses the need for high-quality,\ntransparent, and culturally relevant language models beyond the English-centric\ncommercial landscape. We describe the development process, including the\nconstruction of a new 140-billion-token Polish text corpus for pre-training, a\n77k custom instructions dataset, and a 100k preference optimization dataset. A\nkey component is a Responsible AI framework that incorporates strict data\ngovernance and a hybrid module for output correction and safety filtering. We\ndetail the models' architecture, training procedures, and alignment techniques\nfor both base and instruction-tuned variants, and demonstrate their utility in\na downstream task within public administration. By releasing these models\npublicly, PLLuM aims to foster open research and strengthen sovereign AI\ntechnologies in Poland.", "AI": {"tldr": "PLLuM is the largest open-source family of Polish language foundation models developed to address the English-centric bias in LLMs, featuring extensive Polish corpora and responsible AI framework.", "motivation": "To overcome the limited support for non-English languages in LLMs and provide high-quality, culturally relevant language models for Polish, addressing the gap in the English-dominated commercial landscape.", "method": "Developed through consortium collaboration, constructed a 140B-token Polish corpus, 77k custom instructions dataset, and 100k preference optimization dataset. Implemented Responsible AI framework with strict data governance and hybrid safety filtering.", "result": "Created the largest open-source Polish LLM family with both base and instruction-tuned variants, demonstrated utility in public administration downstream tasks.", "conclusion": "PLLuM successfully addresses Polish language AI needs, fosters open research, and strengthens sovereign AI technologies in Poland through transparent, culturally relevant models."}}
{"id": "2511.03765", "pdf": "https://arxiv.org/pdf/2511.03765", "abs": "https://arxiv.org/abs/2511.03765", "authors": ["Hyunseok Kwak", "Kyeongwon Lee", "Jae-Jin Lee", "Woojoo Lee"], "title": "LoRA-Edge: Tensor-Train-Assisted LoRA for Practical CNN Fine-Tuning on Edge Devices", "categories": ["cs.CV", "cs.AR"], "comment": "8 pages, 6 figures, 2 tables, DATE 2026 accepted paper", "summary": "On-device fine-tuning of CNNs is essential to withstand domain shift in edge\napplications such as Human Activity Recognition (HAR), yet full fine-tuning is\ninfeasible under strict memory, compute, and energy budgets. We present\nLoRA-Edge, a parameter-efficient fine-tuning (PEFT) method that builds on\nLow-Rank Adaptation (LoRA) with tensor-train assistance. LoRA-Edge (i) applies\nTensor-Train Singular Value Decomposition (TT-SVD) to pre-trained convolutional\nlayers, (ii) selectively updates only the output-side core with\nzero-initialization to keep the auxiliary path inactive at the start, and (iii)\nfuses the update back into dense kernels, leaving inference cost unchanged.\nThis design preserves convolutional structure and reduces the number of\ntrainable parameters by up to two orders of magnitude compared to full\nfine-tuning. Across diverse HAR datasets and CNN backbones, LoRA-Edge achieves\naccuracy within 4.7% of full fine-tuning while updating at most 1.49% of\nparameters, consistently outperforming prior parameter-efficient baselines\nunder similar budgets. On a Jetson Orin Nano, TT-SVD initialization and\nselective-core training yield 1.4-3.8x faster convergence to target F1.\nLoRA-Edge thus makes structure-aligned, parameter-efficient on-device CNN\nadaptation practical for edge platforms.", "AI": {"tldr": "LoRA-Edge enables efficient on-device CNN fine-tuning using tensor-train assisted LoRA, reducing trainable parameters by up to 100x while maintaining near-full fine-tuning accuracy.", "motivation": "Full fine-tuning of CNNs is infeasible for edge devices due to strict memory, compute, and energy constraints, especially for applications like Human Activity Recognition that face domain shift issues.", "method": "Applies Tensor-Train SVD to pre-trained convolutional layers, selectively updates only the output-side core with zero-initialization, and fuses updates back into dense kernels without changing inference cost.", "result": "Achieves accuracy within 4.7% of full fine-tuning while updating at most 1.49% of parameters, with 1.4-3.8x faster convergence on Jetson Orin Nano, outperforming prior parameter-efficient methods.", "conclusion": "LoRA-Edge makes structure-aligned, parameter-efficient on-device CNN adaptation practical for edge platforms by preserving convolutional structure and significantly reducing training overhead."}}
{"id": "2511.03827", "pdf": "https://arxiv.org/pdf/2511.03827", "abs": "https://arxiv.org/abs/2511.03827", "authors": ["Mohammad Atif Quamar", "Mohammad Areeb", "Mikhail Kuznetsov", "Muslum Ozgur Ozmen", "Z. Berkay Celik"], "title": "STARS: Segment-level Token Alignment with Rejection Sampling in Large Language Models", "categories": ["cs.CL"], "comment": "Presented at the 2nd Workshop on Frontiers in Probabilistic\n  Inference: Sampling Meets Learning (NeurIPS 2025)", "summary": "Aligning large language models with human values is crucial for their safe\ndeployment; however, existing methods, such as fine-tuning, are computationally\nexpensive and suboptimal. In contrast, inference-time approaches like Best-of-N\nsampling require practically infeasible computation to achieve optimal\nalignment. We propose STARS: Segment-level Token Alignment with Rejection\nSampling, a decoding-time algorithm that steers model generation by iteratively\nsampling, scoring, and rejecting/accepting short, fixed-size token segments.\nThis allows for early correction of the generation path, significantly\nimproving computational efficiency and boosting alignment quality. Across a\nsuite of six LLMs, we show that STARS outperforms Supervised Fine-Tuning (SFT)\nby up to 14.9 percentage points and Direct Preference Optimization (DPO) by up\nto 4.3 percentage points on win-rates, while remaining highly competitive with\nstrong Best-of-N baselines. Our work establishes granular, reward-guided\nsampling as a generalizable, robust, and efficient alternative to traditional\nfine-tuning and full-sequence ranking methods for aligning LLMs.", "AI": {"tldr": "STARS is a decoding-time algorithm that improves LLM alignment by iteratively sampling, scoring, and rejecting/accepting short token segments, achieving better performance than fine-tuning methods with higher computational efficiency.", "motivation": "Existing methods for aligning LLMs with human values (fine-tuning, Best-of-N sampling) are computationally expensive and suboptimal, creating a need for more efficient and effective alignment approaches.", "method": "STARS uses segment-level token alignment with rejection sampling - iteratively sampling, scoring, and rejecting/accepting short, fixed-size token segments during decoding to enable early correction of generation paths.", "result": "STARS outperforms Supervised Fine-Tuning by up to 14.9 percentage points and Direct Preference Optimization by up to 4.3 percentage points on win-rates across six LLMs, while remaining competitive with Best-of-N baselines.", "conclusion": "Granular, reward-guided sampling provides a generalizable, robust, and efficient alternative to traditional fine-tuning and full-sequence ranking methods for aligning LLMs."}}
{"id": "2511.03819", "pdf": "https://arxiv.org/pdf/2511.03819", "abs": "https://arxiv.org/abs/2511.03819", "authors": ["Ozan Kanbertay", "Richard Vogg", "Elif Karakoc", "Peter M. Kappeler", "Claudia Fichtel", "Alexander S. Ecker"], "title": "SILVI: Simple Interface for Labeling Video Interactions", "categories": ["cs.CV", "q-bio.QM"], "comment": null, "summary": "Computer vision methods are increasingly used for the automated analysis of\nlarge volumes of video data collected through camera traps, drones, or direct\nobservations of animals in the wild. While recent advances have focused\nprimarily on detecting individual actions, much less work has addressed the\ndetection and annotation of interactions -- a crucial aspect for understanding\nsocial and individualized animal behavior. Existing open-source annotation\ntools support either behavioral labeling without localization of individuals,\nor localization without the capacity to capture interactions. To bridge this\ngap, we present SILVI, an open-source labeling software that integrates both\nfunctionalities. SILVI enables researchers to annotate behaviors and\ninteractions directly within video data, generating structured outputs suitable\nfor training and validating computer vision models. By linking behavioral\necology with computer vision, SILVI facilitates the development of automated\napproaches for fine-grained behavioral analyses. Although developed primarily\nin the context of animal behavior, SILVI could be useful more broadly to\nannotate human interactions in other videos that require extracting dynamic\nscene graphs. The software, along with documentation and download instructions,\nis available at: https://gitlab.gwdg.de/kanbertay/interaction-labelling-app.", "AI": {"tldr": "SILVI is an open-source video annotation tool that enables researchers to label both behaviors and interactions in animal videos, bridging the gap between behavioral labeling and individual localization for computer vision model training.", "motivation": "Existing annotation tools either support behavioral labeling without localization or localization without interaction detection, creating a gap for analyzing social and individualized animal behavior through computer vision.", "method": "Developed SILVI - an open-source labeling software that integrates both behavioral annotation and interaction detection functionalities directly within video data.", "result": "SILVI generates structured outputs suitable for training and validating computer vision models, facilitating automated approaches for fine-grained behavioral analyses.", "conclusion": "SILVI bridges behavioral ecology with computer vision and could be broadly useful for annotating human interactions in videos requiring dynamic scene graph extraction."}}
{"id": "2511.03830", "pdf": "https://arxiv.org/pdf/2511.03830", "abs": "https://arxiv.org/abs/2511.03830", "authors": ["Miko\u0142aj Langner", "Jan Eliasz", "Ewa Rudnicka", "Jan Koco\u0144"], "title": "Divide, Cache, Conquer: Dichotomic Prompting for Efficient Multi-Label LLM-Based Classification", "categories": ["cs.CL"], "comment": "9 pages, 8 figures", "summary": "We introduce a method for efficient multi-label text classification with\nlarge language models (LLMs), built on reformulating classification tasks as\nsequences of dichotomic (yes/no) decisions. Instead of generating all labels in\na single structured response, each target dimension is queried independently,\nwhich, combined with a prefix caching mechanism, yields substantial efficiency\ngains for short-text inference without loss of accuracy. To demonstrate the\napproach, we focus on affective text analysis, covering 24 dimensions including\nemotions and sentiment. Using LLM-to-SLM distillation, a powerful annotator\nmodel (DeepSeek-V3) provides multiple annotations per text, which are\naggregated to fine-tune smaller models (HerBERT-Large, CLARIN-1B, PLLuM-8B,\nGemma3-1B). The fine-tuned models show significant improvements over zero-shot\nbaselines, particularly on the dimensions seen during training. Our findings\nsuggest that decomposing multi-label classification into dichotomic queries,\ncombined with distillation and cache-aware inference, offers a scalable and\neffective framework for LLM-based classification. While we validate the method\non affective states, the approach is general and applicable across domains.", "AI": {"tldr": "Efficient multi-label text classification using LLMs by reformulating tasks as sequences of yes/no decisions for each label dimension, with prefix caching for efficiency gains.", "motivation": "To address the inefficiency of traditional multi-label classification with LLMs that generate all labels in a single response, which is computationally expensive for short-text inference.", "method": "Decompose multi-label classification into independent dichotomic queries for each target dimension, use LLM-to-SLM distillation with DeepSeek-V3 as annotator, fine-tune smaller models (HerBERT-Large, CLARIN-1B, PLLuM-8B, Gemma3-1B), and implement prefix caching mechanism.", "result": "Fine-tuned models show significant improvements over zero-shot baselines, especially on trained dimensions, with substantial efficiency gains for short-text inference without accuracy loss.", "conclusion": "Dichotomic query decomposition combined with distillation and cache-aware inference provides a scalable and effective framework for LLM-based multi-label classification, applicable across domains beyond affective text analysis."}}
{"id": "2511.03855", "pdf": "https://arxiv.org/pdf/2511.03855", "abs": "https://arxiv.org/abs/2511.03855", "authors": ["Duong Mai", "Lawrence Hall"], "title": "Noise Injection: Improving Out-of-Distribution Generalization for Limited Size Datasets", "categories": ["cs.CV", "cs.AI"], "comment": "Abstract accepted for oral presentation at SPIE Medical Imaging 2026:\n  Computer-Aided Diagnosis", "summary": "Deep learned (DL) models for image recognition have been shown to fail to\ngeneralize to data from different devices, populations, etc. COVID-19 detection\nfrom Chest X-rays (CXRs), in particular, has been shown to fail to generalize\nto out-of-distribution (OOD) data from new clinical sources not covered in the\ntraining set. This occurs because models learn to exploit shortcuts -\nsource-specific artifacts that do not translate to new distributions - rather\nthan reasonable biomarkers to maximize performance on in-distribution (ID)\ndata. Rendering the models more robust to distribution shifts, our study\ninvestigates the use of fundamental noise injection techniques (Gaussian,\nSpeckle, Poisson, and Salt and Pepper) during training. Our empirical results\ndemonstrate that this technique can significantly reduce the performance gap\nbetween ID and OOD evaluation from 0.10-0.20 to 0.01-0.06, based on results\naveraged over ten random seeds across key metrics such as AUC, F1, accuracy,\nrecall and specificity. Our source code is publicly available at\nhttps://github.com/Duongmai127/Noisy-ood", "AI": {"tldr": "Noise injection during training improves COVID-19 detection from chest X-rays by reducing performance gap between in-distribution and out-of-distribution data from 0.10-0.20 to 0.01-0.06.", "motivation": "Deep learning models for COVID-19 detection fail to generalize to new clinical sources due to learning source-specific shortcuts rather than meaningful biomarkers.", "method": "Applied fundamental noise injection techniques (Gaussian, Speckle, Poisson, and Salt and Pepper) during model training to improve robustness to distribution shifts.", "result": "Significantly reduced performance gap between in-distribution and out-of-distribution evaluation from 0.10-0.20 to 0.01-0.06 across AUC, F1, accuracy, recall and specificity metrics.", "conclusion": "Noise injection is an effective technique to enhance model robustness and generalization for COVID-19 detection from chest X-rays across different clinical sources."}}
{"id": "2511.03880", "pdf": "https://arxiv.org/pdf/2511.03880", "abs": "https://arxiv.org/abs/2511.03880", "authors": ["Hellina Hailu Nigatu", "Bethelhem Yemane Mamo", "Bontu Fufa Balcha", "Debora Taye Tesfaye", "Elbethel Daniel Zewdie", "Ikram Behiru Nesiru", "Jitu Ewnetu Hailu", "Senait Mengesha Yayo"], "title": "Evaluating Machine Translation Datasets for Low-Web Data Languages: A Gendered Lens", "categories": ["cs.CL", "cs.CY"], "comment": "Paper Under Review", "summary": "As low-resourced languages are increasingly incorporated into NLP research,\nthere is an emphasis on collecting large-scale datasets. But in prioritizing\nquantity over quality, we risk 1) building language technologies that perform\npoorly for these languages and 2) producing harmful content that perpetuates\nsocietal biases. In this paper, we investigate the quality of Machine\nTranslation (MT) datasets for three low-resourced languages--Afan Oromo,\nAmharic, and Tigrinya, with a focus on the gender representation in the\ndatasets. Our findings demonstrate that while training data has a large\nrepresentation of political and religious domain text, benchmark datasets are\nfocused on news, health, and sports. We also found a large skew towards the\nmale gender--in names of persons, the grammatical gender of verbs, and in\nstereotypical depictions in the datasets. Further, we found harmful and toxic\ndepictions against women, which were more prominent for the language with the\nlargest amount of data, underscoring that quantity does not guarantee quality.\nWe hope that our work inspires further inquiry into the datasets collected for\nlow-resourced languages and prompts early mitigation of harmful content.\nWARNING: This paper contains discussion of NSFW content that some may find\ndisturbing.", "AI": {"tldr": "Analysis of Machine Translation datasets for three low-resourced languages (Afan Oromo, Amharic, Tigrinya) reveals significant gender biases, domain imbalances, and harmful content, with larger datasets showing worse quality issues.", "motivation": "To investigate the quality of MT datasets for low-resourced languages, focusing on gender representation and potential harms, as current approaches prioritize quantity over quality.", "method": "Analyzed MT datasets for three low-resourced languages, examining gender representation through names, grammatical gender of verbs, stereotypical depictions, and harmful/toxic content.", "result": "Found large gender skew towards males, domain imbalances (training data focused on political/religious domains vs benchmarks on news/health/sports), and harmful depictions against women that were worse in languages with more data.", "conclusion": "Quantity does not guarantee quality in low-resourced language datasets; work highlights need for early mitigation of harmful content and further inquiry into dataset quality."}}
{"id": "2511.03882", "pdf": "https://arxiv.org/pdf/2511.03882", "abs": "https://arxiv.org/abs/2511.03882", "authors": ["Florence Klitzner", "Blanca Inigo", "Benjamin D. Killeen", "Lalithkumar Seenivasan", "Michelle Song", "Axel Krieger", "Mathias Unberath"], "title": "Investigating Robot Control Policy Learning for Autonomous X-ray-guided Spine Procedures", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": null, "summary": "Imitation learning-based robot control policies are enjoying renewed interest\nin video-based robotics. However, it remains unclear whether this approach\napplies to X-ray-guided procedures, such as spine instrumentation. This is\nbecause interpretation of multi-view X-rays is complex. We examine\nopportunities and challenges for imitation policy learning in bi-plane-guided\ncannula insertion. We develop an in silico sandbox for scalable, automated\nsimulation of X-ray-guided spine procedures with a high degree of realism. We\ncurate a dataset of correct trajectories and corresponding bi-planar X-ray\nsequences that emulate the stepwise alignment of providers. We then train\nimitation learning policies for planning and open-loop control that iteratively\nalign a cannula solely based on visual information. This precisely controlled\nsetup offers insights into limitations and capabilities of this method. Our\npolicy succeeded on the first attempt in 68.5% of cases, maintaining safe\nintra-pedicular trajectories across diverse vertebral levels. The policy\ngeneralized to complex anatomy, including fractures, and remained robust to\nvaried initializations. Rollouts on real bi-planar X-rays further suggest that\nthe model can produce plausible trajectories, despite training exclusively in\nsimulation. While these preliminary results are promising, we also identify\nlimitations, especially in entry point precision. Full closed-look control will\nrequire additional considerations around how to provide sufficiently frequent\nfeedback. With more robust priors and domain knowledge, such models may provide\na foundation for future efforts toward lightweight and CT-free robotic\nintra-operative spinal navigation.", "AI": {"tldr": "Imitation learning policies for X-ray-guided spine cannula insertion were developed and tested in simulation, achieving 68.5% first-attempt success with safe trajectories and generalization to complex anatomy.", "motivation": "To explore whether imitation learning applies to X-ray-guided spine procedures, specifically bi-plane-guided cannula insertion, given the complexity of interpreting multi-view X-rays.", "method": "Created an in silico sandbox for realistic simulation of X-ray-guided spine procedures, curated dataset of correct trajectories with bi-planar X-ray sequences, trained imitation learning policies for planning and open-loop control using visual information only.", "result": "Policy succeeded on first attempt in 68.5% of cases, maintained safe intra-pedicular trajectories across diverse vertebral levels, generalized to complex anatomy including fractures, remained robust to varied initializations, and produced plausible trajectories on real bi-planar X-rays despite simulation-only training.", "conclusion": "While promising with good generalization and robustness, limitations exist in entry point precision. Full closed-loop control requires more frequent feedback, but with improved priors and domain knowledge, these models could enable lightweight CT-free robotic spinal navigation."}}
{"id": "2511.03900", "pdf": "https://arxiv.org/pdf/2511.03900", "abs": "https://arxiv.org/abs/2511.03900", "authors": ["Manh Nguyen", "Sunil Gupta", "Dai Do", "Hung Le"], "title": "GRAD: Graph-Retrieved Adaptive Decoding for Hallucination Mitigation", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Hallucination mitigation remains a persistent challenge for large language\nmodels (LLMs), even as model scales grow. Existing approaches often rely on\nexternal knowledge sources, such as structured databases or knowledge graphs,\naccessed through prompting or retrieval. However, prompt-based grounding is\nfragile and domain-sensitive, while symbolic knowledge integration incurs heavy\nretrieval and formatting costs. Motivated by knowledge graphs, we introduce\nGraph-Retrieved Adaptive Decoding (GRAD), a decoding-time method that grounds\ngeneration in corpus-derived evidence without retraining. GRAD constructs a\nsparse token transition graph by accumulating next-token logits across a small\nretrieved corpus in a single forward pass. During decoding, graph-retrieved\nlogits are max-normalized and adaptively fused with model logits to favor\nhigh-evidence continuations while preserving fluency. Across three models and a\nrange of question-answering benchmarks spanning intrinsic, extrinsic\nhallucination, and factuality tasks, GRAD consistently surpasses baselines,\nachieving up to 9.7$\\%$ higher intrinsic accuracy, 8.6$\\%$ lower hallucination\nrates, and 6.9$\\%$ greater correctness compared to greedy decoding, while\nattaining the highest truth--informativeness product score among all methods.\nGRAD offers a lightweight, plug-and-play alternative to contrastive decoding\nand knowledge graph augmentation, demonstrating that statistical evidence from\ncorpus-level token transitions can effectively steer generation toward more\ntruthful and verifiable outputs.", "AI": {"tldr": "GRAD is a decoding-time method that mitigates LLM hallucinations by constructing token transition graphs from retrieved corpus evidence and adaptively fusing them with model logits during generation.", "motivation": "Existing hallucination mitigation approaches rely on external knowledge sources through prompting or retrieval, but prompt-based grounding is fragile and symbolic knowledge integration is costly. GRAD aims to provide lightweight corpus-derived evidence without retraining.", "method": "Constructs sparse token transition graphs by accumulating next-token logits across a small retrieved corpus in a single forward pass, then max-normalizes and adaptively fuses graph-retrieved logits with model logits during decoding to favor high-evidence continuations.", "result": "Across three models and multiple QA benchmarks, GRAD achieved up to 9.7% higher intrinsic accuracy, 8.6% lower hallucination rates, and 6.9% greater correctness compared to greedy decoding, with the highest truth-informativeness product score.", "conclusion": "GRAD offers a lightweight, plug-and-play alternative to contrastive decoding and knowledge graph augmentation, demonstrating that statistical evidence from corpus-level token transitions can effectively steer generation toward more truthful and verifiable outputs."}}
{"id": "2511.03888", "pdf": "https://arxiv.org/pdf/2511.03888", "abs": "https://arxiv.org/abs/2511.03888", "authors": ["Abdulmumin Sa'ad", "Sulaimon Oyeniyi Adebayo", "Abdul Jabbar Siddiqui"], "title": "Desert Waste Detection and Classification Using Data-Based and Model-Based Enhanced YOLOv12 DL Model", "categories": ["cs.CV", "cs.LG"], "comment": "8 pages", "summary": "The global waste crisis is escalating, with solid waste generation expected\nto increase by 70% by 2050. Traditional waste collection methods, particularly\nin remote or harsh environments like deserts, are labor-intensive, inefficient,\nand often hazardous. Recent advances in computer vision and deep learning have\nopened the door to automated waste detection systems, yet most research focuses\non urban environments and recyclable materials, overlooking organic and\nhazardous waste and underexplored terrains such as deserts. In this work, we\npropose an enhanced real-time object detection framework based on a pruned,\nlightweight version of YOLOv12 integrated with Self-Adversarial Training (SAT)\nand specialized data augmentation strategies. Using the DroneTrashNet dataset,\nwe demonstrate significant improvements in precision, recall, and mean average\nprecision (mAP), while achieving low latency and compact model size suitable\nfor deployment on resource-constrained aerial drones. Benchmarking our model\nagainst state-of-the-art lightweight YOLO variants further highlights its\noptimal balance of accuracy and efficiency. Our results validate the\neffectiveness of combining data-centric and model-centric enhancements for\nrobust, real-time waste detection in desert environments.", "AI": {"tldr": "Proposes an enhanced real-time object detection framework using pruned YOLOv12 with Self-Adversarial Training and specialized data augmentation for automated waste detection in desert environments, achieving optimal accuracy-efficiency balance for drone deployment.", "motivation": "Addresses the global waste crisis and limitations of traditional waste collection methods in remote/harsh environments like deserts, where current computer vision research focuses mainly on urban areas and recyclables while overlooking organic/hazardous waste in underexplored terrains.", "method": "Developed a pruned, lightweight YOLOv12 framework integrated with Self-Adversarial Training (SAT) and specialized data augmentation strategies, using the DroneTrashNet dataset for training and evaluation.", "result": "Achieved significant improvements in precision, recall, and mean average precision (mAP) with low latency and compact model size suitable for resource-constrained aerial drones, outperforming state-of-the-art lightweight YOLO variants.", "conclusion": "Validates the effectiveness of combining data-centric and model-centric enhancements for robust, real-time waste detection in desert environments, providing an optimal solution for automated waste management in challenging terrains."}}
{"id": "2511.03908", "pdf": "https://arxiv.org/pdf/2511.03908", "abs": "https://arxiv.org/abs/2511.03908", "authors": ["Alvin Wei Ming Tan", "Ben Prystawski", "Veronica Boyce", "Michael C. Frank"], "title": "Context informs pragmatic interpretation in vision-language models", "categories": ["cs.CL"], "comment": "Accepted at CogInterp Workshop, NeurIPS 2025", "summary": "Iterated reference games - in which players repeatedly pick out novel\nreferents using language - present a test case for agents' ability to perform\ncontext-sensitive pragmatic reasoning in multi-turn linguistic environments. We\ntested humans and vision-language models on trials from iterated reference\ngames, varying the given context in terms of amount, order, and relevance.\nWithout relevant context, models were above chance but substantially worse than\nhumans. However, with relevant context, model performance increased\ndramatically over trials. Few-shot reference games with abstract referents\nremain a difficult task for machine learning models.", "AI": {"tldr": "Iterated reference games test pragmatic reasoning in multi-turn language environments. Models perform poorly without context but improve dramatically with relevant context, though still lag behind humans.", "motivation": "To test agents' ability to perform context-sensitive pragmatic reasoning in multi-turn linguistic environments through iterated reference games.", "method": "Tested humans and vision-language models on iterated reference games with varying context (amount, order, relevance) using abstract referents.", "result": "Models performed above chance but substantially worse than humans without relevant context. With relevant context, model performance increased dramatically over trials. Few-shot reference games remain difficult for models.", "conclusion": "Iterated reference games with abstract referents present a challenging test case for machine learning models' pragmatic reasoning abilities, though relevant context significantly improves performance."}}
{"id": "2511.03891", "pdf": "https://arxiv.org/pdf/2511.03891", "abs": "https://arxiv.org/abs/2511.03891", "authors": ["Hlali Azzeddine", "Majid Ben Yakhlef", "Soulaiman El Hazzat"], "title": "Improving Diagnostic Performance on Small and Imbalanced Datasets Using Class-Based Input Image Composition", "categories": ["cs.CV", "cs.AI", "cs.DB"], "comment": null, "summary": "Small, imbalanced datasets and poor input image quality can lead to high\nfalse predictions rates with deep learning models. This paper introduces\nClass-Based Image Composition, an approach that allows us to reformulate\ntraining inputs through a fusion of multiple images of the same class into\ncombined visual composites, named Composite Input Images (CoImg). That enhances\nthe intra-class variance and improves the valuable information density per\ntraining sample and increases the ability of the model to distinguish between\nsubtle disease patterns. Our method was evaluated on the Optical Coherence\nTomography Dataset for Image-Based Deep Learning Methods (OCTDL) (Kulyabin et\nal., 2024), which contains 2,064 high-resolution optical coherence tomography\n(OCT) scans of the human retina, representing seven distinct diseases with a\nsignificant class imbalance. We constructed a perfectly class-balanced version\nof this dataset, named Co-OCTDL, where each scan is resented as a 3x1 layout\ncomposite image. To assess the effectiveness of this new representation, we\nconducted a comparative analysis between the original dataset and its variant\nusing a VGG16 model. A fair comparison was ensured by utilizing the identical\nmodel architecture and hyperparameters for all experiments. The proposed\napproach markedly improved diagnostic results.The enhanced Dataset achieved\nnear-perfect accuracy (99.6%) with F1-score (0.995) and AUC (0.9996), compared\nto a baseline model trained on raw dataset. The false prediction rate was also\nsignificantly lower, this demonstrates that the method can producehigh-quality\npredictions even for weak datasets affected by class imbalance or small sample\nsize.", "AI": {"tldr": "Class-Based Image Composition creates composite images from multiple same-class images to address small, imbalanced datasets and poor image quality, significantly improving diagnostic accuracy in OCT retinal scans.", "motivation": "To overcome challenges of small imbalanced datasets and poor image quality that cause high false prediction rates in deep learning models for medical imaging.", "method": "Fusion of multiple images from the same class into Composite Input Images (CoImg) arranged in 3x1 layouts, creating a balanced dataset (Co-OCTDL) from the original imbalanced OCTDL dataset.", "result": "Achieved near-perfect accuracy (99.6%), F1-score (0.995), and AUC (0.9996) with significantly lower false prediction rates compared to baseline model trained on raw dataset.", "conclusion": "The method effectively enhances intra-class variance and information density, enabling high-quality predictions even with weak datasets affected by class imbalance or small sample sizes."}}
{"id": "2511.03915", "pdf": "https://arxiv.org/pdf/2511.03915", "abs": "https://arxiv.org/abs/2511.03915", "authors": ["Stefano M. Iacus", "Devika Jain", "Andrea Nasuto", "Giuseppe Porro", "Marcello Carammia", "Andrea Vezzulli"], "title": "The Human Flourishing Geographic Index: A County-Level Dataset for the United States, 2013--2023", "categories": ["cs.CL", "cs.CY", "stat.AP"], "comment": null, "summary": "Quantifying human flourishing, a multidimensional construct including\nhappiness, health, purpose, virtue, relationships, and financial stability, is\ncritical for understanding societal well-being beyond economic indicators.\nExisting measures often lack fine spatial and temporal resolution. Here we\nintroduce the Human Flourishing Geographic Index (HFGI), derived from analyzing\napproximately 2.6 billion geolocated U.S. tweets (2013-2023) using fine-tuned\nlarge language models to classify expressions across 48 indicators aligned with\nHarvard's Global Flourishing Study framework plus attitudes towards migration\nand perception of corruption. The dataset offers monthly and yearly county- and\nstate-level indicators of flourishing-related discourse, validated to confirm\nthat the measures accurately represent the underlying constructs and show\nexpected correlations with established indicators. This resource enables\nmultidisciplinary analyses of well-being, inequality, and social change at\nunprecedented resolution, offering insights into the dynamics of human\nflourishing as reflected in social media discourse across the United States\nover the past decade.", "AI": {"tldr": "The paper introduces the Human Flourishing Geographic Index (HFGI), a high-resolution measure of human flourishing derived from 2.6 billion geolocated tweets using fine-tuned LLMs to analyze 48 indicators aligned with established flourishing frameworks.", "motivation": "Existing measures of human flourishing lack fine spatial and temporal resolution, limiting understanding of societal well-being beyond economic indicators.", "method": "Analyzed 2.6 billion geolocated U.S. tweets (2013-2023) using fine-tuned large language models to classify expressions across 48 indicators aligned with Harvard's Global Flourishing Study framework, plus attitudes towards migration and perception of corruption.", "result": "Created monthly and yearly county- and state-level indicators of flourishing-related discourse, validated to accurately represent underlying constructs and show expected correlations with established indicators.", "conclusion": "The HFGI enables multidisciplinary analyses of well-being, inequality, and social change at unprecedented resolution, offering insights into human flourishing dynamics across the United States over the past decade."}}
{"id": "2511.03912", "pdf": "https://arxiv.org/pdf/2511.03912", "abs": "https://arxiv.org/abs/2511.03912", "authors": ["Nand Kumar Yadav", "Rodrigue Rizk", "William CW Chen", "KC Santosh"], "title": "I Detect What I Don't Know: Incremental Anomaly Learning with Stochastic Weight Averaging-Gaussian for Oracle-Free Medical Imaging", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Unknown anomaly detection in medical imaging remains a fundamental challenge\ndue to the scarcity of labeled anomalies and the high cost of expert\nsupervision. We introduce an unsupervised, oracle-free framework that\nincrementally expands a trusted set of normal samples without any anomaly\nlabels. Starting from a small, verified seed of normal images, our method\nalternates between lightweight adapter updates and uncertainty-gated sample\nadmission. A frozen pretrained vision backbone is augmented with tiny\nconvolutional adapters, ensuring rapid domain adaptation with negligible\ncomputational overhead. Extracted embeddings are stored in a compact coreset\nenabling efficient k-nearest neighbor anomaly (k-NN) scoring. Safety during\nincremental expansion is enforced by dual probabilistic gates, a sample is\nadmitted into the normal memory only if its distance to the existing coreset\nlies within a calibrated z-score threshold, and its SWAG-based epistemic\nuncertainty remains below a seed-calibrated bound. This mechanism prevents\ndrift and false inclusions without relying on generative reconstruction or\nreplay buffers. Empirically, our system steadily refines the notion of\nnormality as unlabeled data arrive, producing substantial gains over baselines.\nOn COVID-CXR, ROC-AUC improves from 0.9489 to 0.9982 (F1: 0.8048 to 0.9746); on\nPneumonia CXR, ROC-AUC rises from 0.6834 to 0.8968; and on Brain MRI ND-5,\nROC-AUC increases from 0.6041 to 0.7269 and PR-AUC from 0.7539 to 0.8211. These\nresults highlight the effectiveness and efficiency of the proposed framework\nfor real-world, label-scarce medical imaging applications.", "AI": {"tldr": "Unsupervised anomaly detection framework for medical imaging that incrementally expands normal samples without anomaly labels using lightweight adapters and uncertainty-gated admission.", "motivation": "Addresses the challenge of unknown anomaly detection in medical imaging where labeled anomalies are scarce and expert supervision is costly.", "method": "Uses frozen pretrained vision backbone with tiny convolutional adapters for domain adaptation, k-NN anomaly scoring with coreset storage, and dual probabilistic gates (z-score threshold and SWAG-based epistemic uncertainty) for safe sample admission.", "result": "Substantial performance improvements: COVID-CXR ROC-AUC from 0.9489 to 0.9982, Pneumonia CXR ROC-AUC from 0.6834 to 0.8968, Brain MRI ND-5 ROC-AUC from 0.6041 to 0.7269.", "conclusion": "The framework is effective and efficient for real-world, label-scarce medical imaging applications, steadily refining normality as unlabeled data arrive."}}
{"id": "2511.03945", "pdf": "https://arxiv.org/pdf/2511.03945", "abs": "https://arxiv.org/abs/2511.03945", "authors": ["Fu-Chun Yang", "Jason Eshraghian"], "title": "Direct Semantic Communication Between Large Language Models via Vector Translation", "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "9 pages, 1 figure, 2 tables", "summary": "In multi-agent settings, such as debate, reflection, or tool-calling, large\nlanguage models (LLMs) pass messages as plain tokens, discarding most latent\nsemantics. This constrains information transfer and adds unnecessary\ncomputational overhead. We form a latent bridge via vector translations, which\nuse learned mappings that enable direct semantic exchange between\nrepresentation spaces. A dual-encoder translator trained between Llama-2-7B and\nMistral-7B-Instruct attains an average cosine alignment of 0.538. Injecting the\ntranslated vectors at 30 percent blending strength steers the target model's\ngeneration without destabilizing logits. Bidirectional evaluation shows a\n2.01:1 transfer asymmetry, indicating that general-purpose models yield more\ntransferable representations than instruction-tuned variants. This conservative\ninjection preserves computational stability while demonstrating that\ncross-model latent communication is feasible, enabling collaborative AI systems\nthat share meaning rather than tokens.", "AI": {"tldr": "The paper proposes using vector translations as a latent bridge for direct semantic exchange between LLMs, enabling more efficient cross-model communication without token-based overhead.", "motivation": "Current multi-agent LLM systems use token-based messaging which discards latent semantics, constraining information transfer and adding computational overhead.", "method": "Developed a dual-encoder translator trained between Llama-2-7B and Mistral-7B-Instruct models to enable direct semantic exchange via vector translations with conservative injection at 30% blending strength.", "result": "Achieved average cosine alignment of 0.538, demonstrated stable generation without destabilizing logits, and found 2.01:1 transfer asymmetry showing general-purpose models yield more transferable representations than instruction-tuned variants.", "conclusion": "Cross-model latent communication is feasible and enables collaborative AI systems that share meaning rather than tokens, while preserving computational stability."}}
{"id": "2511.03943", "pdf": "https://arxiv.org/pdf/2511.03943", "abs": "https://arxiv.org/abs/2511.03943", "authors": ["Ibne Farabi Shihab", "Sanjeda Akter", "Anuj Sharma"], "title": "Adaptive Temporal Refinement: Continuous Depth Allocation and Distance Regression for Efficient Action Localization", "categories": ["cs.CV"], "comment": null, "summary": "Temporal action localization requires precise boundary detection; however,\ncurrent methods apply uniform computation despite significant variations in\ndifficulty across boundaries. We present two complementary contributions.\nFirst, Boundary Distance Regression (BDR) provides information-theoretically\noptimal localization through signed-distance regression rather than\nclassification, achieving 43\\% sharper boundary peaks. BDR retrofits to\nexisting methods with approximately 50 lines of code, yielding consistent 1.8\nto 3.1\\% mAP@0.7 improvements across diverse architectures. Second, Adaptive\nTemporal Refinement (ATR) allocates computation via continuous depth selection\n$\\tau \\in [0,1]$, enabling end-to-end differentiable optimization without\nreinforcement learning. On THUMOS14, ATR achieves 56.5\\% mAP@0.7 at 162G FLOPs,\ncompared to 53.6\\% at 198G for uniform processing, providing a 2.9\\%\nimprovement with 18\\% less compute. Gains scale with boundary heterogeneity,\nshowing 4.2\\% improvement on short actions. Training cost is mitigated via\nknowledge distillation, with lightweight students retaining 99\\% performance at\nbaseline cost. Results are validated across four benchmarks with rigorous\nstatistical testing.", "AI": {"tldr": "BDR improves temporal action localization through signed-distance regression for sharper boundary detection, while ATR enables adaptive computation allocation via continuous depth selection, achieving better performance with less computation.", "motivation": "Current temporal action localization methods apply uniform computation despite significant variations in boundary detection difficulty, leading to inefficient resource allocation.", "method": "Two complementary approaches: Boundary Distance Regression (BDR) using signed-distance regression instead of classification, and Adaptive Temporal Refinement (ATR) with continuous depth selection for computation allocation.", "result": "BDR achieves 43% sharper boundary peaks and 1.8-3.1% mAP@0.7 improvements across architectures. ATR achieves 56.5% mAP@0.7 at 162G FLOPs vs 53.6% at 198G for uniform processing, with 4.2% improvement on short actions.", "conclusion": "The methods provide significant performance gains with reduced computation, validated across four benchmarks, with gains scaling according to boundary heterogeneity."}}
{"id": "2511.04020", "pdf": "https://arxiv.org/pdf/2511.04020", "abs": "https://arxiv.org/abs/2511.04020", "authors": ["Shiyin Lin"], "title": "Abductive Inference in Retrieval-Augmented Language Models: Generating and Validating Missing Premises", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) enhanced with retrieval -- commonly referred to\nas Retrieval-Augmented Generation (RAG) -- have demonstrated strong performance\nin knowledge-intensive tasks. However, RAG pipelines often fail when retrieved\nevidence is incomplete, leaving gaps in the reasoning process. In such cases,\n\\emph{abductive inference} -- the process of generating plausible missing\npremises to explain observations -- offers a principled approach to bridge\nthese gaps. In this paper, we propose a framework that integrates abductive\ninference into retrieval-augmented LLMs. Our method detects insufficient\nevidence, generates candidate missing premises, and validates them through\nconsistency and plausibility checks. Experimental results on abductive\nreasoning and multi-hop QA benchmarks show that our approach improves both\nanswer accuracy and reasoning faithfulness. This work highlights abductive\ninference as a promising direction for enhancing the robustness and\nexplainability of RAG systems.", "AI": {"tldr": "A framework that integrates abductive inference into RAG systems to handle incomplete evidence by generating and validating missing premises, improving accuracy and faithfulness.", "motivation": "RAG pipelines often fail when retrieved evidence is incomplete, leaving gaps in reasoning that need to be bridged through abductive inference.", "method": "Detects insufficient evidence, generates candidate missing premises, and validates them through consistency and plausibility checks.", "result": "Experimental results on abductive reasoning and multi-hop QA benchmarks show improved answer accuracy and reasoning faithfulness.", "conclusion": "Abductive inference is a promising direction for enhancing the robustness and explainability of RAG systems."}}
{"id": "2511.03950", "pdf": "https://arxiv.org/pdf/2511.03950", "abs": "https://arxiv.org/abs/2511.03950", "authors": ["Zhejia Cai", "Puhua Jiang", "Shiwei Mao", "Hongkun Cao", "Ruqi Huang"], "title": "Improving Multi-View Reconstruction via Texture-Guided Gaussian-Mesh Joint Optimization", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages", "summary": "Reconstructing real-world objects from multi-view images is essential for\napplications in 3D editing, AR/VR, and digital content creation. Existing\nmethods typically prioritize either geometric accuracy (Multi-View Stereo) or\nphotorealistic rendering (Novel View Synthesis), often decoupling geometry and\nappearance optimization, which hinders downstream editing tasks. This paper\nadvocates an unified treatment on geometry and appearance optimization for\nseamless Gaussian-mesh joint optimization. More specifically, we propose a\nnovel framework that simultaneously optimizes mesh geometry (vertex positions\nand faces) and vertex colors via Gaussian-guided mesh differentiable rendering,\nleveraging photometric consistency from input images and geometric\nregularization from normal and depth maps. The obtained high-quality 3D\nreconstruction can be further exploit in down-stream editing tasks, such as\nrelighting and shape deformation. The code will be publicly available upon\nacceptance.", "AI": {"tldr": "Proposes a unified framework for joint optimization of mesh geometry and appearance using Gaussian-guided differentiable rendering, enabling high-quality 3D reconstruction suitable for downstream editing tasks.", "motivation": "Existing methods either prioritize geometric accuracy or photorealistic rendering, decoupling geometry and appearance optimization, which hinders downstream editing applications like 3D editing, AR/VR, and digital content creation.", "method": "Simultaneously optimizes mesh geometry (vertex positions and faces) and vertex colors via Gaussian-guided mesh differentiable rendering, leveraging photometric consistency from input images and geometric regularization from normal and depth maps.", "result": "Obtains high-quality 3D reconstruction that can be exploited in downstream editing tasks such as relighting and shape deformation.", "conclusion": "Advocates for unified treatment of geometry and appearance optimization through seamless Gaussian-mesh joint optimization to enable better downstream editing capabilities."}}
{"id": "2511.04035", "pdf": "https://arxiv.org/pdf/2511.04035", "abs": "https://arxiv.org/abs/2511.04035", "authors": ["Dongji Gao", "Chenda Liao", "Changliang Liu", "Matthew Wiesner", "Leibny Paola Garcia", "Daniel Povey", "Sanjeev Khudanpur", "Jian Wu"], "title": "WST: Weakly Supervised Transducer for Automatic Speech Recognition", "categories": ["cs.CL"], "comment": null, "summary": "The Recurrent Neural Network-Transducer (RNN-T) is widely adopted in\nend-to-end (E2E) automatic speech recognition (ASR) tasks but depends heavily\non large-scale, high-quality annotated data, which are often costly and\ndifficult to obtain. To mitigate this reliance, we propose a Weakly Supervised\nTransducer (WST), which integrates a flexible training graph designed to\nrobustly handle errors in the transcripts without requiring additional\nconfidence estimation or auxiliary pre-trained models. Empirical evaluations on\nsynthetic and industrial datasets reveal that WST effectively maintains\nperformance even with transcription error rates of up to 70%, consistently\noutperforming existing Connectionist Temporal Classification (CTC)-based weakly\nsupervised approaches, such as Bypass Temporal Classification (BTC) and\nOmni-Temporal Classification (OTC). These results demonstrate the practical\nutility and robustness of WST in realistic ASR settings. The implementation\nwill be publicly available.", "AI": {"tldr": "Proposes Weakly Supervised Transducer (WST) for ASR that handles noisy transcripts without needing confidence scores or pre-trained models, maintaining performance with up to 70% transcription errors.", "motivation": "RNN-T models require large-scale, high-quality annotated data which is costly and difficult to obtain, creating a need for methods that can work with noisy or imperfect transcripts.", "method": "Integrates a flexible training graph designed to robustly handle errors in transcripts without requiring additional confidence estimation or auxiliary pre-trained models.", "result": "WST maintains performance even with transcription error rates up to 70%, consistently outperforming existing CTC-based weakly supervised approaches like BTC and OTC on synthetic and industrial datasets.", "conclusion": "WST demonstrates practical utility and robustness in realistic ASR settings, offering an effective solution for training with imperfect transcripts."}}
{"id": "2511.03962", "pdf": "https://arxiv.org/pdf/2511.03962", "abs": "https://arxiv.org/abs/2511.03962", "authors": ["Zhong Chen", "Changfeng Chen"], "title": "A Linear Fractional Transformation Model and Calibration Method for Light Field Camera", "categories": ["cs.CV"], "comment": null, "summary": "Accurate calibration of internal parameters is a crucial yet challenging\nprerequisite for 3D reconstruction using light field cameras. In this paper, we\npropose a linear fractional transformation(LFT) parameter $\\alpha$ to decoupled\nthe main lens and micro lens array (MLA). The proposed method includes an\nanalytical solution based on least squares, followed by nonlinear refinement.\nThe method for detecting features from the raw images is also introduced.\nExperimental results on both physical and simulated data have verified the\nperformance of proposed method. Based on proposed model, the simulation of raw\nlight field images becomes faster, which is crucial for data-driven deep\nlearning methods. The corresponding code can be obtained from the author's\nwebsite.", "AI": {"tldr": "Proposes a linear fractional transformation parameter to decouple main lens and micro lens array for light field camera calibration, with analytical solution and nonlinear refinement.", "motivation": "Accurate calibration of internal parameters is crucial but challenging for 3D reconstruction using light field cameras.", "method": "Uses linear fractional transformation parameter \u03b1 to decouple main lens and micro lens array, with analytical solution based on least squares followed by nonlinear refinement, plus feature detection from raw images.", "result": "Experimental results on both physical and simulated data verify the method's performance, and the model enables faster simulation of raw light field images.", "conclusion": "The proposed calibration method is effective and enables faster light field image simulation, which benefits data-driven deep learning approaches."}}
{"id": "2511.04070", "pdf": "https://arxiv.org/pdf/2511.04070", "abs": "https://arxiv.org/abs/2511.04070", "authors": ["Shreya Havaldar", "Helen Jin", "Chaehyeon Kim", "Anton Xue", "Weiqiu You", "Marco Gatti", "Bhuvnesh Jain", "Helen Qu", "Daniel A Hashimoto", "Amin Madani", "Rajat Deo", "Sameed Ahmed M. Khatana", "Gary E. Weissman", "Lyle Ungar", "Eric Wong"], "title": "T-FIX: Text-Based Explanations with Features Interpretable to eXperts", "categories": ["cs.CL"], "comment": null, "summary": "As LLMs are deployed in knowledge-intensive settings (e.g., surgery,\nastronomy, therapy), users expect not just answers, but also meaningful\nexplanations for those answers. In these settings, users are often domain\nexperts (e.g., doctors, astrophysicists, psychologists) who require\nexplanations that reflect expert-level reasoning. However, current evaluation\nschemes primarily emphasize plausibility or internal faithfulness of the\nexplanation, which fail to capture whether the content of the explanation truly\naligns with expert intuition. We formalize expert alignment as a criterion for\nevaluating explanations with T-FIX, a benchmark spanning seven\nknowledge-intensive domains. In collaboration with domain experts, we develop\nnovel metrics to measure the alignment of LLM explanations with expert\njudgment.", "AI": {"tldr": "The paper introduces T-FIX, a benchmark for evaluating LLM explanations in knowledge-intensive domains, focusing on expert alignment rather than just plausibility or faithfulness.", "motivation": "Current evaluation schemes for LLM explanations focus on plausibility and faithfulness but fail to assess whether explanations align with expert reasoning in knowledge-intensive domains like surgery, astronomy, and therapy.", "method": "Developed T-FIX benchmark spanning seven knowledge-intensive domains in collaboration with domain experts, and created novel metrics to measure alignment of LLM explanations with expert judgment.", "result": "The paper formalizes expert alignment as a key criterion for evaluating explanations, providing a framework to assess whether LLM-generated explanations reflect expert-level reasoning.", "conclusion": "T-FIX addresses the gap in current evaluation methods by focusing on expert alignment, enabling better assessment of LLM explanations in professional domains where users are domain experts."}}
{"id": "2511.03970", "pdf": "https://arxiv.org/pdf/2511.03970", "abs": "https://arxiv.org/abs/2511.03970", "authors": ["Sam Bahrami", "Dylan Campbell"], "title": "Room Envelopes: A Synthetic Dataset for Indoor Layout Reconstruction from Images", "categories": ["cs.CV"], "comment": null, "summary": "Modern scene reconstruction methods are able to accurately recover 3D\nsurfaces that are visible in one or more images. However, this leads to\nincomplete reconstructions, missing all occluded surfaces. While much progress\nhas been made on reconstructing entire objects given partial observations using\ngenerative models, the structural elements of a scene, like the walls, floors\nand ceilings, have received less attention. We argue that these scene elements\nshould be relatively easy to predict, since they are typically planar,\nrepetitive and simple, and so less costly approaches may be suitable. In this\nwork, we present a synthetic dataset -- Room Envelopes -- that facilitates\nprogress on this task by providing a set of RGB images and two associated\npointmaps for each image: one capturing the visible surface and one capturing\nthe first surface once fittings and fixtures are removed, that is, the\nstructural layout. As we show, this enables direct supervision for feed-forward\nmonocular geometry estimators that predict both the first visible surface and\nthe first layout surface. This confers an understanding of the scene's extent,\nas well as the shape and location of its objects.", "AI": {"tldr": "A synthetic dataset called Room Envelopes is introduced to help predict structural scene elements like walls, floors, and ceilings that are typically occluded in standard scene reconstructions.", "motivation": "Standard scene reconstruction methods only recover visible surfaces, leaving occluded structural elements missing. Structural elements are easier to predict due to their planar, repetitive nature, suggesting simpler approaches could be effective.", "method": "Created a synthetic dataset with RGB images and two pointmaps per image: one for visible surfaces and one for structural layout surfaces (after removing fixtures). This enables direct supervision for monocular geometry estimators.", "result": "The dataset facilitates training of feed-forward monocular geometry estimators that can predict both visible surfaces and structural layout surfaces.", "conclusion": "This approach enables understanding of scene extent and object shapes/locations by predicting both visible and structural surfaces, addressing the limitation of incomplete reconstructions in current methods."}}
{"id": "2511.04072", "pdf": "https://arxiv.org/pdf/2511.04072", "abs": "https://arxiv.org/abs/2511.04072", "authors": ["Xinying Qian", "Ying Zhang", "Yu Zhao", "Baohang Zhou", "Xuhui Sui", "Xiaojie Yuan"], "title": "Plan of Knowledge: Retrieval-Augmented Large Language Models for Temporal Knowledge Graph Question Answering", "categories": ["cs.CL"], "comment": "Submitted to the IEEE for possible publication", "summary": "Temporal Knowledge Graph Question Answering (TKGQA) aims to answer\ntime-sensitive questions by leveraging factual information from Temporal\nKnowledge Graphs (TKGs). While previous studies have employed pre-trained TKG\nembeddings or graph neural networks to inject temporal knowledge, they fail to\nfully understand the complex semantic information of time constraints.\nRecently, Large Language Models (LLMs) have shown remarkable progress,\nbenefiting from their strong semantic understanding and reasoning\ngeneralization capabilities. However, their temporal reasoning ability remains\nlimited. LLMs frequently suffer from hallucination and a lack of knowledge. To\naddress these limitations, we propose the Plan of Knowledge framework with a\ncontrastive temporal retriever, which is named PoK. Specifically, the proposed\nPlan of Knowledge module decomposes a complex temporal question into a sequence\nof sub-objectives from the pre-defined tools, serving as intermediate guidance\nfor reasoning exploration. In parallel, we construct a Temporal Knowledge Store\n(TKS) with a contrastive retrieval framework, enabling the model to selectively\nretrieve semantically and temporally aligned facts from TKGs. By combining\nstructured planning with temporal knowledge retrieval, PoK effectively enhances\nthe interpretability and factual consistency of temporal reasoning. Extensive\nexperiments on four benchmark TKGQA datasets demonstrate that PoK significantly\nimproves the retrieval precision and reasoning accuracy of LLMs, surpassing the\nperformance of the state-of-the-art TKGQA methods by 56.0% at most.", "AI": {"tldr": "PoK framework enhances LLMs for temporal reasoning by decomposing questions into sub-objectives and using contrastive temporal retrieval from TKGs.", "motivation": "Existing TKGQA methods fail to fully understand complex temporal semantics, while LLMs have strong semantic understanding but limited temporal reasoning abilities and suffer from hallucination.", "method": "Proposes Plan of Knowledge (PoK) framework with: 1) Question decomposition into sub-objectives using pre-defined tools, 2) Temporal Knowledge Store with contrastive retrieval for semantically and temporally aligned facts.", "result": "Significant improvements on four TKGQA benchmarks, surpassing state-of-the-art methods by up to 56.0% in reasoning accuracy and retrieval precision.", "conclusion": "PoK effectively combines structured planning with temporal knowledge retrieval to enhance interpretability and factual consistency in temporal reasoning for LLMs."}}

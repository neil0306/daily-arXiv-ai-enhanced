{"id": "2508.19268", "pdf": "https://arxiv.org/pdf/2508.19268", "abs": "https://arxiv.org/abs/2508.19268", "authors": ["Qing Wang", "Xue Han", "Jiahui Wang", "Lehao Xing", "Qian Hu", "Lianlian Zhang", "Chao Deng", "Junlan Feng"], "title": "MultiPL-MoE: Multi-Programming-Lingual Extension of Large Language Models through Hybrid Mixture-of-Experts", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite LLMs' excellent code creation capabilities, multilingual code\ngeneration remains extremely challenging. To address this, we intent to improve\nthe multi-programming-lingual (MultiPL) performance of the base LLMs while\nretaining the most popular ones using restricted computational resources. We\nconsider MultiPL to be a special case of multiple natural languages and propose\na MultiPL extension of LLMs utilizing a hybrid mixture of experts (MoE), called\nMultiPL-MoE. Specifically, MultiPL-MoE combines two paired MoEs to optimize\nexpert selection at both the token and segment levels. The token-level MoE is a\nstandard upcycling MoE structure with a shared expert and a novel gate weight\nnormalization approach that aids in the final fusion with the segment-level\nMoE. The segment-level MoE incorporates two innovative designs to better\ncapture the syntactic structure and contextual patterns of programming\nlanguages: First, using a sliding window to partition the input token sequence\ninto multiple segments; Then, adopting an expert-choice routing strategy that\nallows experts to select the top-k segments. The results of the experiment\nproved the effectiveness of MultiPL-MoE.", "AI": {"tldr": "MultiPL-MoE: A hybrid mixture of experts approach that improves multilingual code generation in LLMs using token-level and segment-level MoE with innovative routing strategies.", "motivation": "Despite LLMs' strong code creation capabilities, multilingual code generation remains challenging. The paper aims to improve multi-programming-lingual performance while maintaining performance on popular languages with limited computational resources.", "method": "Proposes MultiPL-MoE, combining two paired MoEs: token-level MoE with shared expert and novel gate weight normalization, and segment-level MoE with sliding window segmentation and expert-choice routing strategy where experts select top-k segments.", "result": "Experimental results demonstrated the effectiveness of MultiPL-MoE in improving multilingual code generation performance.", "conclusion": "The hybrid MoE approach successfully addresses multilingual code generation challenges by optimizing expert selection at both token and segment levels, capturing programming language syntax and contextual patterns effectively."}}
{"id": "2508.19270", "pdf": "https://arxiv.org/pdf/2508.19270", "abs": "https://arxiv.org/abs/2508.19270", "authors": ["Nguyen Huu Nhat Minh", "Tran Nguyen Anh", "Truong Dinh Dung", "Vo Van Nam", "Le Pham Tuyen"], "title": "Whisper based Cross-Lingual Phoneme Recognition between Vietnamese and English", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Cross-lingual phoneme recognition has emerged as a significant challenge for\naccurate automatic speech recognition (ASR) when mixing Vietnamese and English\npronunciations. Unlike many languages, Vietnamese relies on tonal variations to\ndistinguish word meanings, whereas English features stress patterns and\nnon-standard pronunciations that hinder phoneme alignment between the two\nlanguages. To address this challenge, we propose a novel bilingual speech\nrecognition approach with two primary contributions: (1) constructing a\nrepresentative bilingual phoneme set that bridges the differences between\nVietnamese and English phonetic systems; (2) designing an end-to-end system\nthat leverages the PhoWhisper pre-trained encoder for deep high-level\nrepresentations to improve phoneme recognition. Our extensive experiments\ndemonstrate that the proposed approach not only improves recognition accuracy\nin bilingual speech recognition for Vietnamese but also provides a robust\nframework for addressing the complexities of tonal and stress-based phoneme\nrecognition", "AI": {"tldr": "Novel bilingual speech recognition approach for Vietnamese-English mixing that constructs a bilingual phoneme set and uses PhoWhisper encoder to improve cross-lingual phoneme recognition accuracy.", "motivation": "Address the challenge of cross-lingual phoneme recognition between Vietnamese (tonal language) and English (stress-based language) where non-standard pronunciations hinder phoneme alignment.", "method": "Two main contributions: (1) constructing a representative bilingual phoneme set bridging Vietnamese and English phonetic systems, (2) designing an end-to-end system leveraging PhoWhisper pre-trained encoder for deep high-level representations.", "result": "Extensive experiments demonstrate improved recognition accuracy in bilingual speech recognition for Vietnamese.", "conclusion": "Provides a robust framework for addressing complexities of tonal and stress-based phoneme recognition in cross-lingual scenarios."}}
{"id": "2508.19271", "pdf": "https://arxiv.org/pdf/2508.19271", "abs": "https://arxiv.org/abs/2508.19271", "authors": ["Rushitha Santhoshi Mamidala", "Anshuman Chhabra", "Ankur Mali"], "title": "Rethinking Reasoning in LLMs: Neuro-Symbolic Local RetoMaton Beyond ICL and CoT", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Prompt-based reasoning strategies such as Chain-of-Thought (CoT) and\nIn-Context Learning (ICL) have become widely used for eliciting reasoning\ncapabilities in large language models (LLMs). However, these methods rely on\nfragile, implicit mechanisms often yielding inconsistent outputs across seeds,\nformats, or minor prompt variations making them fundamentally unreliable for\ntasks requiring stable, interpretable reasoning. In contrast, automata-based\nneuro-symbolic frameworks like RetoMaton offer a more structured and\ntrustworthy alternative by grounding retrieval in symbolic memory with\ndeterministic transitions. In this work, we extend RetoMaton by replacing its\nglobal datastore with a local, task-adaptive Weighted Finite Automaton (WFA),\nconstructed directly from external domain corpora. This local automaton\nstructure promotes robust, context-aware retrieval while preserving symbolic\ntraceability and low inference overhead. Unlike prompting, which entangles\ncontext and memory in opaque ways, our approach leverages the explicit\nstructure of WFAs to provide verifiable and modular retrieval behavior, making\nit better suited for domain transfer and interoperability. We evaluate this\nlocal RetoMaton variant on two pretrained LLMs LLaMA-3.2-1B and Gemma-3-1B-PT\nacross three reasoning tasks: TriviaQA (reading comprehension), GSM8K\n(multi-step math), and MMLU (domain knowledge). Compared to the base model and\nprompting-based methods, augmenting these setups with local RetoMaton\nconsistently improves performance while enabling transparent and reproducible\nretrieval dynamics. Our results highlight a promising shift toward trustworthy,\nsymbolic reasoning in modern LLMs via lightweight, automaton-guided memory.", "AI": {"tldr": "This paper extends RetoMaton by replacing its global datastore with a local, task-adaptive Weighted Finite Automaton (WFA) constructed from domain corpora, providing more robust and verifiable retrieval compared to prompting methods like Chain-of-Thought.", "motivation": "Prompt-based reasoning strategies (CoT, ICL) are fragile and yield inconsistent outputs, making them unreliable for tasks requiring stable, interpretable reasoning. There's a need for more structured and trustworthy alternatives.", "method": "Extends RetoMaton framework by using local, task-adaptive Weighted Finite Automata (WFAs) constructed from external domain corpora instead of global datastores, preserving symbolic traceability with low inference overhead.", "result": "Evaluation on LLaMA-3.2-1B and Gemma-3-1B-PT across TriviaQA, GSM8K, and MMLU tasks shows consistent performance improvements over base models and prompting methods, with transparent and reproducible retrieval dynamics.", "conclusion": "Local RetoMaton represents a promising shift toward trustworthy, symbolic reasoning in LLMs through lightweight, automaton-guided memory that provides verifiable and modular retrieval behavior suitable for domain transfer."}}
{"id": "2508.19272", "pdf": "https://arxiv.org/pdf/2508.19272", "abs": "https://arxiv.org/abs/2508.19272", "authors": ["Kshitij Fadnis", "Sara Rosenthal", "Maeda Hanafi", "Yannis Katsis", "Marina Danilevsky"], "title": "RAGAPHENE: A RAG Annotation Platform with Human Enhancements and Edits", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval Augmented Generation (RAG) is an important aspect of conversing\nwith Large Language Models (LLMs) when factually correct information is\nimportant. LLMs may provide answers that appear correct, but could contain\nhallucinated information. Thus, building benchmarks that can evaluate LLMs on\nmulti-turn RAG conversations has become an increasingly important task.\nSimulating real-world conversations is vital for producing high quality\nevaluation benchmarks. We present RAGAPHENE, a chat-based annotation platform\nthat enables annotators to simulate real-world conversations for benchmarking\nand evaluating LLMs. RAGAPHENE has been successfully used by approximately 40\nannotators to build thousands of real-world conversations.", "AI": {"tldr": "RAGAPHENE is a chat-based annotation platform for creating multi-turn RAG conversation benchmarks to evaluate LLMs' factual accuracy and reduce hallucinations.", "motivation": "LLMs can provide factually incorrect or hallucinated information in conversations, making it crucial to develop benchmarks that evaluate their performance in retrieval-augmented generation scenarios.", "method": "Developed RAGAPHENE, a chat-based annotation platform that enables human annotators to simulate real-world multi-turn conversations for benchmarking LLM performance in RAG settings.", "result": "Successfully used by approximately 40 annotators to build thousands of real-world conversation benchmarks for evaluating LLMs.", "conclusion": "RAGAPHENE provides an effective platform for creating high-quality evaluation benchmarks that simulate real-world RAG conversations, addressing the need for reliable LLM assessment in factual dialogue scenarios."}}
{"id": "2508.19254", "pdf": "https://arxiv.org/pdf/2508.19254", "abs": "https://arxiv.org/abs/2508.19254", "authors": ["Jookyung Song", "Mookyoung Kang", "Nojun Kwak"], "title": "Real-Time Intuitive AI Drawing System for Collaboration: Enhancing Human Creativity through Formal and Contextual Intent Integration", "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": "6 pages, 4 figures, NeurIPS Creative AI Track 2025", "summary": "This paper presents a real-time generative drawing system that interprets and\nintegrates both formal intent - the structural, compositional, and stylistic\nattributes of a sketch - and contextual intent - the semantic and thematic\nmeaning inferred from its visual content - into a unified transformation\nprocess. Unlike conventional text-prompt-based generative systems, which\nprimarily capture high-level contextual descriptions, our approach\nsimultaneously analyzes ground-level intuitive geometric features such as line\ntrajectories, proportions, and spatial arrangement, and high-level semantic\ncues extracted via vision-language models. These dual intent signals are\njointly conditioned in a multi-stage generation pipeline that combines\ncontour-preserving structural control with style- and content-aware image\nsynthesis. Implemented with a touchscreen-based interface and distributed\ninference architecture, the system achieves low-latency, two-stage\ntransformation while supporting multi-user collaboration on shared canvases.\nThe resulting platform enables participants, regardless of artistic expertise,\nto engage in synchronous, co-authored visual creation, redefining human-AI\ninteraction as a process of co-creation and mutual enhancement.", "AI": {"tldr": "Real-time generative drawing system that combines formal sketch structure with contextual semantic meaning for collaborative AI-human co-creation", "motivation": "To move beyond conventional text-prompt generative systems by capturing both geometric features and semantic meaning from sketches, enabling more intuitive human-AI collaboration", "method": "Multi-stage generation pipeline that jointly conditions dual intent signals (structural features + semantic cues) using contour-preserving structural control with style/content-aware image synthesis", "result": "Low-latency system with touchscreen interface and distributed architecture supporting real-time multi-user collaboration on shared canvases", "conclusion": "The platform redefines human-AI interaction as co-creation, enabling synchronous visual creation regardless of artistic expertise"}}
{"id": "2508.19274", "pdf": "https://arxiv.org/pdf/2508.19274", "abs": "https://arxiv.org/abs/2508.19274", "authors": ["Yue Chu"], "title": "Leveraging Language Models and Machine Learning in Verbal Autopsy Analysis", "categories": ["cs.CL"], "comment": "Ph.D. dissertation submitted to The Ohio State University, August\n  2025", "summary": "In countries without civil registration and vital statistics, verbal autopsy\n(VA) is a critical tool for estimating cause of death (COD) and inform policy\npriorities. In VA, interviewers ask proximal informants for details on the\ncircumstances preceding a death, in the form of unstructured narratives and\nstructured questions. Existing automated VA cause classification algorithms\nonly use the questions and ignore the information in the narratives. In this\nthesis, we investigate how the VA narrative can be used for automated COD\nclassification using pretrained language models (PLMs) and machine learning\n(ML) techniques. Using empirical data from South Africa, we demonstrate that\nwith the narrative alone, transformer-based PLMs with task-specific fine-tuning\noutperform leading question-only algorithms at both the individual and\npopulation levels, particularly in identifying non-communicable diseases. We\nexplore various multimodal fusion strategies combining narratives and questions\nin unified frameworks. Multimodal approaches further improve performance in COD\nclassification, confirming that each modality has unique contributions and may\ncapture valuable information that is not present in the other modality. We also\ncharacterize physician-perceived information sufficiency in VA. We describe\nvariations in sufficiency levels by age and COD and demonstrate that\nclassification accuracy is affected by sufficiency for both physicians and\nmodels. Overall, this thesis advances the growing body of knowledge at the\nintersection of natural language processing, epidemiology, and global health.\nIt demonstrates the value of narrative in enhancing COD classification. Our\nfindings underscore the need for more high-quality data from more diverse\nsettings to use in training and fine-tuning PLM/ML methods, and offer valuable\ninsights to guide the rethinking and redesign of the VA instrument and\ninterview.", "AI": {"tldr": "This thesis shows that using verbal autopsy narratives with pretrained language models outperforms existing question-only methods for cause of death classification, and multimodal approaches combining narratives and questions work best.", "motivation": "In countries without civil registration systems, verbal autopsy is crucial for estimating causes of death, but existing automated methods ignore valuable narrative information from interviews.", "method": "Used pretrained language models and machine learning techniques on South African data, fine-tuning transformers on narratives alone and exploring multimodal fusion strategies combining narratives and structured questions.", "result": "Transformer models using narratives alone outperformed question-only algorithms, especially for non-communicable diseases. Multimodal approaches further improved performance, showing each modality provides unique information. Classification accuracy was affected by information sufficiency levels.", "conclusion": "Narratives significantly enhance cause of death classification in verbal autopsies. The findings support rethinking VA instruments and highlight the need for more diverse, high-quality data for training models."}}
{"id": "2508.19257", "pdf": "https://arxiv.org/pdf/2508.19257", "abs": "https://arxiv.org/abs/2508.19257", "authors": ["Chenghao Liu", "Jiachen Zhang", "Chengxuan Li", "Zhimu Zhou", "Shixin Wu", "Songfang Huang", "Huiling Duan"], "title": "TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for Vision-Language-Action Models", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "Manuscript submitted to AAAI 2026, currently under review", "summary": "Vision-Language-Action (VLA) models process visual inputs independently at\neach timestep, discarding valuable temporal information inherent in robotic\nmanipulation tasks. This frame-by-frame processing makes models vulnerable to\nvisual noise while ignoring the substantial coherence between consecutive\nframes in manipulation sequences. We propose Temporal Token Fusion (TTF), a\ntraining-free approach that intelligently integrates historical and current\nvisual representations to enhance VLA inference quality. Our method employs\ndual-dimension detection combining efficient grayscale pixel difference\nanalysis with attention-based semantic relevance assessment, enabling selective\ntemporal token fusion through hard fusion strategies and keyframe anchoring to\nprevent error accumulation. Comprehensive experiments across LIBERO,\nSimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0\npercentage points average on LIBERO (72.4\\% vs 68.4\\% baseline),\ncross-environment validation on SimplerEnv (4.8\\% relative improvement), and\n8.7\\% relative improvement on real robot tasks. Our approach proves\nmodel-agnostic, working across OpenVLA and VLA-Cache architectures. Notably,\nTTF reveals that selective Query matrix reuse in attention mechanisms enhances\nrather than compromises performance, suggesting promising directions for direct\nKQV matrix reuse strategies that achieve computational acceleration while\nimproving task success rates.", "AI": {"tldr": "TTF is a training-free temporal token fusion method that enhances Vision-Language-Action models by integrating historical visual information, improving performance across multiple benchmarks without additional training.", "motivation": "Current VLA models process visual inputs frame-by-frame, discarding valuable temporal information and making them vulnerable to visual noise while ignoring coherence between consecutive frames in manipulation tasks.", "method": "Temporal Token Fusion (TTF) with dual-dimension detection combining grayscale pixel difference analysis and attention-based semantic relevance assessment, using hard fusion strategies and keyframe anchoring to prevent error accumulation.", "result": "Consistent improvements: 4.0pp average on LIBERO (72.4% vs 68.4% baseline), 4.8% relative improvement on SimplerEnv, and 8.7% relative improvement on real robot tasks. Model-agnostic across OpenVLA and VLA-Cache architectures.", "conclusion": "TTF demonstrates that selective temporal fusion enhances VLA performance, revealing that Query matrix reuse in attention mechanisms improves rather than compromises performance, suggesting promising directions for computational acceleration with improved success rates."}}
{"id": "2508.19279", "pdf": "https://arxiv.org/pdf/2508.19279", "abs": "https://arxiv.org/abs/2508.19279", "authors": ["Gunjan Jalori", "Preetika Verma", "Sercan \u00d6 Ar\u0131k"], "title": "FLAIRR-TS -- Forecasting LLM-Agents with Iterative Refinement and Retrieval for Time Series", "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP", "summary": "Time series Forecasting with large languagemodels (LLMs) requires bridging\nnumericalpatterns and natural language. Effective fore-casting on LLM often\nrelies on extensive pre-processing and fine-tuning.Recent studiesshow that a\nfrozen LLM can rival specializedforecasters when supplied with a carefully\nen-gineered natural-language prompt, but craft-ing such a prompt for each task\nis itself oner-ous and ad-hoc. We introduce FLAIRR-TS, atest-time prompt\noptimization framework thatutilizes an agentic system: a\nForecaster-agentgenerates forecasts using an initial prompt,which is then\nrefined by a refiner agent, in-formed by past outputs and retrieved\nanalogs.This adaptive prompting generalizes across do-mains using creative\nprompt templates andgenerates high-quality forecasts without inter-mediate code\ngeneration.Experiments onbenchmark datasets show improved accuracyover static\nprompting and retrieval-augmentedbaselines, approaching the performance\nofspecialized prompts.FLAIRR-TS providesa practical alternative to tuning,\nachievingstrong performance via its agentic approach toadaptive prompt\nrefinement and retrieval.", "AI": {"tldr": "FLAIRR-TS is an agentic framework that uses two agents (Forecaster and Refiner) to dynamically optimize prompts for time series forecasting with frozen LLMs, eliminating the need for manual prompt engineering or fine-tuning.", "motivation": "Traditional time series forecasting with LLMs requires extensive pre-processing, fine-tuning, or manual prompt engineering for each task, which is time-consuming and ad-hoc. There's a need for an automated approach that can generate high-quality forecasts without these burdens.", "method": "Uses a two-agent system: a Forecaster agent generates initial forecasts using a prompt, then a Refiner agent iteratively refines the prompt based on past outputs and retrieved similar time series patterns. Employs creative prompt templates and retrieval augmentation without intermediate code generation.", "result": "Experiments show improved accuracy over static prompting and retrieval-augmented baselines, approaching the performance of specialized manually-crafted prompts. Provides strong forecasting performance across different domains.", "conclusion": "FLAIRR-TS offers a practical alternative to tuning and manual prompt engineering, achieving competitive forecasting performance through its adaptive agentic approach to prompt refinement and pattern retrieval."}}
{"id": "2508.19289", "pdf": "https://arxiv.org/pdf/2508.19289", "abs": "https://arxiv.org/abs/2508.19289", "authors": ["Tai Inui", "Steven Oh", "Magdeline Kuan"], "title": "Seeing Like a Designer Without One: A Study on Unsupervised Slide Quality Assessment via Designer Cue Augmentation", "categories": ["cs.CV", "cs.AI"], "comment": "6 pages", "summary": "We present an unsupervised slide-quality assessment pipeline that combines\nseven expert-inspired visual-design metrics (whitespace, colorfulness, edge\ndensity, brightness contrast, text density, color harmony, layout balance) with\nCLIP-ViT embeddings, using Isolation Forest-based anomaly scoring to evaluate\npresentation slides. Trained on 12k professional lecture slides and evaluated\non six academic talks (115 slides), our method achieved Pearson correlations up\nto 0.83 with human visual-quality ratings-1.79x to 3.23x stronger than scores\nfrom leading vision-language models (ChatGPT o4-mini-high, ChatGPT o3, Claude\nSonnet 4, Gemini 2.5 Pro). We demonstrate convergent validity with visual\nratings, discriminant validity against speaker-delivery scores, and exploratory\nalignment with overall impressions. Our results show that augmenting low-level\ndesign cues with multimodal embeddings closely approximates audience\nperceptions of slide quality, enabling scalable, objective feedback in real\ntime.", "AI": {"tldr": "Unsupervised slide quality assessment pipeline combining expert-inspired visual metrics with CLIP embeddings achieves strong correlation (up to 0.83) with human quality ratings, outperforming leading vision-language models.", "motivation": "To develop an objective, scalable method for assessing presentation slide quality that approximates human perceptions without requiring manual evaluation.", "method": "Combines seven expert-inspired visual metrics (whitespace, colorfulness, edge density, brightness contrast, text density, color harmony, layout balance) with CLIP-ViT embeddings, using Isolation Forest-based anomaly scoring on 12k professional lecture slides.", "result": "Achieved Pearson correlations up to 0.83 with human visual-quality ratings (1.79x to 3.23x stronger than leading vision-language models), demonstrated convergent validity with visual ratings and discriminant validity against speaker-delivery scores.", "conclusion": "Augmenting low-level design cues with multimodal embeddings effectively approximates audience perceptions of slide quality, enabling real-time, scalable objective feedback for presentation slides."}}
{"id": "2508.19282", "pdf": "https://arxiv.org/pdf/2508.19282", "abs": "https://arxiv.org/abs/2508.19282", "authors": ["Ziqiang Cui", "Yunpeng Weng", "Xing Tang", "Peiyang Liu", "Shiwei Li", "Bowei He", "Jiamin Chen", "Xiuqiang He", "Chen Ma"], "title": "CORE: Lossless Compression for Retrieval-Augmented LLMs via Reinforcement Learning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has emerged as a promising approach to\nenhance the timeliness of knowledge and the factual accuracy of responses in\nLarge Language Models (LLMs). However, the inclusion of excessive retrieved\ndocuments substantially increases the input length, leading to higher\ncomputational costs. Previous studies have attempted to compress retrieved\ndocuments into shorter texts before in-context integration, but such methods\noften compromise end-task performance. The lack of well-defined compression\ntargets forces many approaches to rely on fixed heuristics, which cannot\nguarantee that the compressed content will effectively support the end task. To\naddress these limitations, we propose CORE, a novel method designed to achieve\nlossless context compression for RAG. CORE employs reinforcement learning to\noptimize the compression process without relying on predefined compression\nlabels. Specifically, it utilizes end-task performance as a reward signal and\napplies Generalized Reinforcement Learning Policy Optimization (GRPO) to train\nthe compressor. This end-to-end training framework enables the compressor to\ngenerate summaries that maximize the accuracy of answers generated by the LLM.\nExtensive experiments on four datasets demonstrate the superiority of our\napproach. With a high compression ratio of 3\\%, our method not only avoids\nperformance degradation compared to prepending full documents across all\ndatasets but also improves the average Exact Match (EM) score by 3.3 points.\nThe code will be released soon.", "AI": {"tldr": "CORE is a reinforcement learning-based method for lossless context compression in RAG systems that achieves 3% compression ratio while improving answer accuracy by 3.3 EM points.", "motivation": "Existing RAG document compression methods increase computational costs and compromise end-task performance due to excessive retrieved documents and lack of well-defined compression targets.", "method": "Uses reinforcement learning with end-task performance as reward signal and Generalized Reinforcement Learning Policy Optimization (GRPO) to train the compressor without predefined compression labels.", "result": "Achieves 3% compression ratio while improving average Exact Match score by 3.3 points across four datasets, avoiding performance degradation compared to using full documents.", "conclusion": "CORE provides an effective end-to-end training framework for lossless context compression in RAG systems, demonstrating superior performance over previous compression methods."}}
{"id": "2508.19290", "pdf": "https://arxiv.org/pdf/2508.19290", "abs": "https://arxiv.org/abs/2508.19290", "authors": ["Alexandros Gkillas", "Ioulia Kapsali", "Nikos Piperigkos", "Aris S. Lalos"], "title": "Efficient Model-Based Purification Against Adversarial Attacks for LiDAR Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "LiDAR-based segmentation is essential for reliable perception in autonomous\nvehicles, yet modern segmentation networks are highly susceptible to\nadversarial attacks that can compromise safety. Most existing defenses are\ndesigned for networks operating directly on raw 3D point clouds and rely on\nlarge, computationally intensive generative models. However, many\nstate-of-the-art LiDAR segmentation pipelines operate on more efficient 2D\nrange view representations. Despite their widespread adoption, dedicated\nlightweight adversarial defenses for this domain remain largely unexplored. We\nintroduce an efficient model-based purification framework tailored for\nadversarial defense in 2D range-view LiDAR segmentation. We propose a direct\nattack formulation in the range-view domain and develop an explainable\npurification network based on a mathematical justified optimization problem,\nachieving strong adversarial resilience with minimal computational overhead.\nOur method achieves competitive performance on open benchmarks, consistently\noutperforming generative and adversarial training baselines. More importantly,\nreal-world deployment on a demo vehicle demonstrates the framework's ability to\ndeliver accurate operation in practical autonomous driving scenarios.", "AI": {"tldr": "Efficient adversarial defense framework for 2D range-view LiDAR segmentation that provides strong protection with minimal computational overhead, outperforming existing methods in both benchmarks and real-world deployment.", "motivation": "LiDAR segmentation is critical for autonomous vehicle safety but vulnerable to adversarial attacks. Most defenses focus on raw 3D point clouds and are computationally intensive, while efficient 2D range-view representations lack dedicated lightweight defenses.", "method": "Proposes a model-based purification framework with direct attack formulation in range-view domain. Uses an explainable purification network based on mathematically justified optimization problem for efficient adversarial defense.", "result": "Achieves competitive performance on open benchmarks, consistently outperforming generative and adversarial training baselines. Real-world deployment on demo vehicle shows accurate operation in practical autonomous driving scenarios.", "conclusion": "The framework provides strong adversarial resilience with minimal computational overhead, making it suitable for practical deployment in autonomous driving systems using 2D range-view LiDAR segmentation."}}
{"id": "2508.19357", "pdf": "https://arxiv.org/pdf/2508.19357", "abs": "https://arxiv.org/abs/2508.19357", "authors": ["Peiran Zhou", "Junnan Zhu", "Yichen Shen", "Ruoxi Yu"], "title": "Context-Adaptive Synthesis and Compression for Enhanced Retrieval-Augmented Generation in Complex Domains", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) excel in language tasks but are prone to\nhallucinations and outdated knowledge. Retrieval-Augmented Generation (RAG)\nmitigates these by grounding LLMs in external knowledge. However, in complex\ndomains involving multiple, lengthy, or conflicting documents, traditional RAG\nsuffers from information overload and inefficient synthesis, leading to\ninaccurate and untrustworthy answers. To address this, we propose CASC\n(Context-Adaptive Synthesis and Compression), a novel framework that\nintelligently processes retrieved contexts. CASC introduces a Context Analyzer\n& Synthesizer (CAS) module, powered by a fine-tuned smaller LLM, which performs\nkey information extraction, cross-document consistency checking and conflict\nresolution, and question-oriented structured synthesis. This process transforms\nraw, scattered information into a highly condensed, structured, and\nsemantically rich context, significantly reducing the token count and cognitive\nload for the final Reader LLM. We evaluate CASC on SciDocs-QA, a new\nchallenging multi-document question answering dataset designed for complex\nscientific domains with inherent redundancies and conflicts. Our extensive\nexperiments demonstrate that CASC consistently outperforms strong baselines.", "AI": {"tldr": "CASC framework improves RAG by intelligently processing retrieved documents through analysis, synthesis and compression to handle complex multi-document scenarios with conflicts and redundancies.", "motivation": "Traditional RAG struggles with information overload and inefficient synthesis when dealing with multiple, lengthy, or conflicting documents in complex domains, leading to inaccurate answers.", "method": "Proposes CASC framework with Context Analyzer & Synthesizer module that performs key information extraction, cross-document consistency checking, conflict resolution, and question-oriented structured synthesis using a fine-tuned smaller LLM.", "result": "CASC consistently outperforms strong baselines on SciDocs-QA dataset, transforming raw scattered information into condensed, structured context that reduces token count and cognitive load.", "conclusion": "CASC effectively addresses limitations of traditional RAG in complex multi-document scenarios by providing intelligent context processing and synthesis."}}
{"id": "2508.19294", "pdf": "https://arxiv.org/pdf/2508.19294", "abs": "https://arxiv.org/abs/2508.19294", "authors": ["Ranjan Sapkota", "Manoj Karkee"], "title": "Object Detection with Multimodal Large Vision-Language Models: An In-depth Review", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "First Peer Reviewed Review Paper for Object Detection with\n  Vision-Language Models (VLMs)", "summary": "The fusion of language and vision in large vision-language models (LVLMs) has\nrevolutionized deep learning-based object detection by enhancing adaptability,\ncontextual reasoning, and generalization beyond traditional architectures. This\nin-depth review presents a structured exploration of the state-of-the-art in\nLVLMs, systematically organized through a three-step research review process.\nFirst, we discuss the functioning of vision language models (VLMs) for object\ndetection, describing how these models harness natural language processing\n(NLP) and computer vision (CV) techniques to revolutionize object detection and\nlocalization. We then explain the architectural innovations, training\nparadigms, and output flexibility of recent LVLMs for object detection,\nhighlighting how they achieve advanced contextual understanding for object\ndetection. The review thoroughly examines the approaches used in integration of\nvisual and textual information, demonstrating the progress made in object\ndetection using VLMs that facilitate more sophisticated object detection and\nlocalization strategies. This review presents comprehensive visualizations\ndemonstrating LVLMs' effectiveness in diverse scenarios including localization\nand segmentation, and then compares their real-time performance, adaptability,\nand complexity to traditional deep learning systems. Based on the review, its\nis expected that LVLMs will soon meet or surpass the performance of\nconventional methods in object detection. The review also identifies a few\nmajor limitations of the current LVLM modes, proposes solutions to address\nthose challenges, and presents a clear roadmap for the future advancement in\nthis field. We conclude, based on this study, that the recent advancement in\nLVLMs have made and will continue to make a transformative impact on object\ndetection and robotic applications in the future.", "AI": {"tldr": "This paper presents a comprehensive review of Large Vision-Language Models (LVLMs) for object detection, highlighting their architectural innovations, training paradigms, and superior contextual understanding compared to traditional deep learning methods.", "motivation": "To systematically explore and analyze the state-of-the-art in LVLMs for object detection, examining how the fusion of language and vision enhances adaptability, contextual reasoning, and generalization beyond traditional architectures.", "method": "The authors conducted a structured three-step research review process: 1) discussing how VLMs function for object detection using NLP and CV techniques, 2) explaining architectural innovations and training paradigms of recent LVLMs, and 3) examining approaches for visual-textual information integration with comprehensive visualizations.", "result": "LVLMs demonstrate effectiveness in diverse scenarios including localization and segmentation, showing potential to soon meet or surpass conventional methods in object detection performance, though some limitations were identified.", "conclusion": "Recent advancements in LVLMs are making and will continue to make a transformative impact on object detection and robotic applications, with a clear roadmap proposed for future advancement in this field."}}
{"id": "2508.19359", "pdf": "https://arxiv.org/pdf/2508.19359", "abs": "https://arxiv.org/abs/2508.19359", "authors": ["Fatemeh Haji", "Mazal Bethany", "Cho-Yu Jason Chiang", "Anthony Rios", "Peyman Najafirad"], "title": "Reflective Agreement: Combining Self-Mixture of Agents with a Sequence Tagger for Robust Event Extraction", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Event Extraction (EE) involves automatically identifying and extracting\nstructured information about events from unstructured text, including triggers,\nevent types, and arguments. Traditional discriminative models demonstrate high\nprecision but often exhibit limited recall, particularly for nuanced or\ninfrequent events. Conversely, generative approaches leveraging Large Language\nModels (LLMs) provide higher semantic flexibility and recall but suffer from\nhallucinations and inconsistent predictions. To address these challenges, we\npropose Agreement-based Reflective Inference System (ARIS), a hybrid approach\ncombining a Self Mixture of Agents with a discriminative sequence tagger. ARIS\nexplicitly leverages structured model consensus, confidence-based filtering,\nand an LLM reflective inference module to reliably resolve ambiguities and\nenhance overall event prediction quality. We further investigate decomposed\ninstruction fine-tuning for enhanced LLM event extraction understanding.\nExperiments demonstrate our approach outperforms existing state-of-the-art\nevent extraction methods across three benchmark datasets.", "AI": {"tldr": "ARIS is a hybrid event extraction system that combines discriminative sequence tagging with generative LLMs, using model consensus and reflective inference to improve accuracy and reduce hallucinations.", "motivation": "Traditional discriminative models have high precision but low recall, while generative LLMs have better recall but suffer from hallucinations and inconsistency. A hybrid approach is needed to leverage the strengths of both.", "method": "ARIS combines Self Mixture of Agents with discriminative sequence tagging, using structured model consensus, confidence-based filtering, and LLM reflective inference to resolve ambiguities. Also uses decomposed instruction fine-tuning for better LLM understanding.", "result": "Outperforms existing state-of-the-art event extraction methods across three benchmark datasets.", "conclusion": "The hybrid ARIS approach successfully addresses limitations of both discriminative and generative methods, providing more reliable and accurate event extraction through consensus-based inference and reflective processing."}}
{"id": "2508.19295", "pdf": "https://arxiv.org/pdf/2508.19295", "abs": "https://arxiv.org/abs/2508.19295", "authors": ["Sauptik Dhar", "Nicholas Buoncristiani", "Joe Anakata", "Haoyu Zhang", "Michelle Munson"], "title": "Large VLM-based Stylized Sports Captioning", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The advent of large (visual) language models (LLM / LVLM) have led to a\ndeluge of automated human-like systems in several domains including social\nmedia content generation, search and recommendation, healthcare prognosis, AI\nassistants for cognitive tasks etc. Although these systems have been\nsuccessfully integrated in production; very little focus has been placed on\nsports, particularly accurate identification and natural language description\nof the game play. Most existing LLM/LVLMs can explain generic sports\nactivities, but lack sufficient domain-centric sports' jargon to create natural\n(human-like) descriptions. This work highlights the limitations of existing\nSoTA LLM/LVLMs for generating production-grade sports captions from images in a\ndesired stylized format, and proposes a two-level fine-tuned LVLM pipeline to\naddress that. The proposed pipeline yields an improvement > 8-10% in the F1,\nand > 2-10% in BERT score compared to alternative approaches. In addition, it\nhas a small runtime memory footprint and fast execution time. During Super Bowl\nLIX the pipeline proved its practical application for live professional sports\njournalism; generating highly accurate and stylized captions at the rate of 6\nimages per 3-5 seconds for over 1000 images during the game play.", "AI": {"tldr": "Proposes a two-level fine-tuned LVLM pipeline for generating accurate, stylized sports captions from images, achieving significant improvements over existing methods and demonstrating practical application in live sports journalism.", "motivation": "Existing LLM/LVLMs lack sufficient domain-specific sports jargon to create natural, human-like descriptions of gameplay, limiting their effectiveness in sports caption generation despite their success in other domains.", "method": "A two-level fine-tuned large visual language model (LVLM) pipeline specifically designed for sports caption generation from images, focusing on domain-centric sports terminology and stylized formatting.", "result": "Achieved >8-10% improvement in F1 score and >2-10% improvement in BERT score compared to alternative approaches. Demonstrated practical application during Super Bowl LIX, generating highly accurate captions at 6 images per 3-5 seconds for over 1000 images.", "conclusion": "The proposed pipeline successfully addresses the limitations of existing models for sports caption generation, providing production-grade performance with small memory footprint and fast execution time, making it suitable for live sports journalism applications."}}
{"id": "2508.19363", "pdf": "https://arxiv.org/pdf/2508.19363", "abs": "https://arxiv.org/abs/2508.19363", "authors": ["Jiayu Ding", "Shuming Ma", "Lei Cui", "Nanning Zheng", "Furu Wei"], "title": "LongReasonArena: A Long Reasoning Benchmark for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Existing long-context benchmarks for Large Language Models (LLMs) focus on\nevaluating comprehension of long inputs, while overlooking the evaluation of\nlong reasoning abilities. To address this gap, we introduce LongReasonArena, a\nbenchmark specifically designed to assess the long reasoning capabilities of\nLLMs. Our tasks require models to solve problems by executing multi-step\nalgorithms that reflect key aspects of long reasoning, such as retrieval and\nbacktracking. By controlling the inputs, the required reasoning length can be\narbitrarily scaled, reaching up to 1 million tokens of reasoning for the most\nchallenging tasks. Extensive evaluation results demonstrate that\nLongReasonArena presents a significant challenge for both open-source and\nproprietary LLMs. For instance, Deepseek-R1 achieves only 7.5% accuracy on our\ntask. Further analysis also reveals that the accuracy exhibits a linear decline\nwith respect to the logarithm of the expected number of reasoning steps. Our\ncode and data is available at\nhttps://github.com/LongReasonArena/LongReasonArena.", "AI": {"tldr": "LongReasonArena is a new benchmark that evaluates LLMs' long reasoning capabilities (up to 1M tokens) through multi-step algorithmic tasks, revealing significant performance challenges and a linear accuracy decline with reasoning complexity.", "motivation": "Existing long-context benchmarks focus on input comprehension but overlook long reasoning abilities, creating a gap in evaluating how well LLMs can perform extended multi-step reasoning.", "method": "The benchmark uses controlled input tasks requiring multi-step algorithms (retrieval, backtracking) that can scale reasoning length arbitrarily up to 1 million tokens.", "result": "Deepseek-R1 achieved only 7.5% accuracy on the most challenging tasks, with accuracy showing linear decline relative to the logarithm of required reasoning steps.", "conclusion": "LongReasonArena presents a significant challenge for current LLMs and reveals fundamental limitations in long reasoning capabilities, with performance degrading systematically as reasoning complexity increases."}}
{"id": "2508.19298", "pdf": "https://arxiv.org/pdf/2508.19298", "abs": "https://arxiv.org/abs/2508.19298", "authors": ["Abu Sufian", "Anirudha Ghosh", "Debaditya Barman", "Marco Leo", "Cosimo Distante"], "title": "DemoBias: An Empirical Study to Trace Demographic Biases in Vision Foundation Models", "categories": ["cs.CV", "cs.AI"], "comment": "6 pages, 4 figures, 13th International Workshop on Biometrics and\n  Forensics (IWBF)", "summary": "Large Vision Language Models (LVLMs) have demonstrated remarkable\ncapabilities across various downstream tasks, including biometric face\nrecognition (FR) with description. However, demographic biases remain a\ncritical concern in FR, as these foundation models often fail to perform\nequitably across diverse demographic groups, considering ethnicity/race,\ngender, and age. Therefore, through our work DemoBias, we conduct an empirical\nevaluation to investigate the extent of demographic biases in LVLMs for\nbiometric FR with textual token generation tasks. We fine-tuned and evaluated\nthree widely used pre-trained LVLMs: LLaVA, BLIP-2, and PaliGemma on our own\ngenerated demographic-balanced dataset. We utilize several evaluation metrics,\nlike group-specific BERTScores and the Fairness Discrepancy Rate, to quantify\nand trace the performance disparities. The experimental results deliver\ncompelling insights into the fairness and reliability of LVLMs across diverse\ndemographic groups. Our empirical study uncovered demographic biases in LVLMs,\nwith PaliGemma and LLaVA exhibiting higher disparities for Hispanic/Latino,\nCaucasian, and South Asian groups, whereas BLIP-2 demonstrated comparably\nconsistent. Repository: https://github.com/Sufianlab/DemoBias.", "AI": {"tldr": "LVLMs show demographic biases in face recognition tasks, with PaliGemma and LLaVA exhibiting higher disparities for Hispanic/Latino, Caucasian, and South Asian groups, while BLIP-2 performs more consistently.", "motivation": "Demographic biases remain a critical concern in face recognition systems, as foundation models often fail to perform equitably across diverse demographic groups including ethnicity/race, gender, and age.", "method": "Fine-tuned and evaluated three pre-trained LVLMs (LLaVA, BLIP-2, PaliGemma) on a demographic-balanced dataset using metrics like group-specific BERTScores and Fairness Discrepancy Rate to quantify performance disparities.", "result": "Experimental results revealed demographic biases in LVLMs, with PaliGemma and LLaVA showing higher disparities for Hispanic/Latino, Caucasian, and South Asian groups, while BLIP-2 demonstrated comparatively consistent performance.", "conclusion": "The study provides compelling insights into the fairness and reliability issues of LVLMs across diverse demographic groups, highlighting the need for bias mitigation in vision-language models for biometric applications."}}
{"id": "2508.19372", "pdf": "https://arxiv.org/pdf/2508.19372", "abs": "https://arxiv.org/abs/2508.19372", "authors": ["Zikun Fu", "Chen Yang", "Kourosh Davoudi", "Ken Q. Pu"], "title": "Database Entity Recognition with Data Augmentation and Deep Learning", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.LG"], "comment": "6 pages, 5 figures. Accepted at IEEE 26th International Conference on\n  Information Reuse and Integration for Data Science (IRI 2025), San Jose,\n  California, August 6-8, 2025", "summary": "This paper addresses the challenge of Database Entity Recognition (DB-ER) in\nNatural Language Queries (NLQ). We present several key contributions to advance\nthis field: (1) a human-annotated benchmark for DB-ER task, derived from\npopular text-to-sql benchmarks, (2) a novel data augmentation procedure that\nleverages automatic annotation of NLQs based on the corresponding SQL queries\nwhich are available in popular text-to-SQL benchmarks, (3) a specialized\nlanguage model based entity recognition model using T5 as a backbone and two\ndown-stream DB-ER tasks: sequence tagging and token classification for\nfine-tuning of backend and performing DB-ER respectively. We compared our DB-ER\ntagger with two state-of-the-art NER taggers, and observed better performance\nin both precision and recall for our model. The ablation evaluation shows that\ndata augmentation boosts precision and recall by over 10%, while fine-tuning of\nthe T5 backbone boosts these metrics by 5-10%.", "AI": {"tldr": "A novel approach for Database Entity Recognition in Natural Language Queries using T5-based model with data augmentation and specialized fine-tuning, achieving improved precision and recall over state-of-the-art NER models.", "motivation": "Addressing the challenge of Database Entity Recognition (DB-ER) in Natural Language Queries to improve text-to-SQL systems by accurately identifying database entities.", "method": "Created human-annotated benchmark from text-to-SQL datasets, developed data augmentation using SQL queries for automatic NLQ annotation, and built T5-based model with sequence tagging and token classification tasks for fine-tuning and DB-ER.", "result": "Outperformed state-of-the-art NER taggers in both precision and recall. Data augmentation boosted performance by over 10%, and T5 fine-tuning improved metrics by 5-10%.", "conclusion": "The proposed DB-ER approach with specialized data augmentation and T5-based modeling significantly improves entity recognition performance for database queries, demonstrating the effectiveness of leveraging SQL knowledge for NLQ annotation."}}
{"id": "2508.19305", "pdf": "https://arxiv.org/pdf/2508.19305", "abs": "https://arxiv.org/abs/2508.19305", "authors": ["Chen Chu", "Cyrus Shahabi"], "title": "Geo2Vec: Shape- and Distance-Aware Neural Representation of Geospatial Entities", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Spatial representation learning is essential for GeoAI applications such as\nurban analytics, enabling the encoding of shapes, locations, and spatial\nrelationships (topological and distance-based) of geo-entities like points,\npolylines, and polygons. Existing methods either target a single geo-entity\ntype or, like Poly2Vec, decompose entities into simpler components to enable\nFourier transformation, introducing high computational cost. Moreover, since\nthe transformed space lacks geometric alignment, these methods rely on uniform,\nnon-adaptive sampling, which blurs fine-grained features like edges and\nboundaries. To address these limitations, we introduce Geo2Vec, a novel method\ninspired by signed distance fields (SDF) that operates directly in the original\nspace. Geo2Vec adaptively samples points and encodes their signed distances\n(positive outside, negative inside), capturing geometry without decomposition.\nA neural network trained to approximate the SDF produces compact,\ngeometry-aware, and unified representations for all geo-entity types.\nAdditionally, we propose a rotation-invariant positional encoding to model\nhigh-frequency spatial variations and construct a structured and robust\nembedding space for downstream GeoAI models. Empirical results show that\nGeo2Vec consistently outperforms existing methods in representing shape and\nlocation, capturing topological and distance relationships, and achieving\ngreater efficiency in real-world GeoAI applications. Code and Data can be found\nat: https://github.com/chuchen2017/GeoNeuralRepresentation.", "AI": {"tldr": "Geo2Vec is a novel spatial representation learning method that uses signed distance fields to create unified, geometry-aware embeddings for all geo-entity types without decomposition, outperforming existing methods in efficiency and accuracy.", "motivation": "Existing spatial representation methods either handle only single entity types or require decomposition with high computational cost, and they use uniform sampling that blurs fine-grained geometric features like edges and boundaries.", "method": "Geo2Vec operates directly in original space using signed distance fields (SDF), adaptively samples points to encode signed distances (positive outside, negative inside), and uses a neural network to approximate SDF. It also includes rotation-invariant positional encoding for high-frequency spatial variations.", "result": "Empirical results show Geo2Vec consistently outperforms existing methods in representing shape and location, capturing topological and distance relationships, and achieving greater efficiency in real-world GeoAI applications.", "conclusion": "Geo2Vec provides a unified, efficient, and geometry-aware representation method for all geo-entity types that addresses limitations of previous approaches and enables better performance in downstream GeoAI applications."}}
{"id": "2508.19402", "pdf": "https://arxiv.org/pdf/2508.19402", "abs": "https://arxiv.org/abs/2508.19402", "authors": ["Mor Turgeman", "Chen Shani", "Dafna Shahaf"], "title": "One Joke to Rule them All? On the (Im)possibility of Generalizing Humor", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Humor is a broad and complex form of communication that remains challenging\nfor machines. Despite its broadness, most existing research on computational\nhumor traditionally focused on modeling a specific type of humor. In this work,\nwe wish to understand whether competence on one or more specific humor tasks\nconfers any ability to transfer to novel, unseen types; in other words, is this\nfragmentation inevitable? This question is especially timely as new humor types\ncontinuously emerge in online and social media contexts (e.g., memes,\nanti-humor, AI fails). If Large Language Models (LLMs) are to keep up with this\nevolving landscape, they must be able to generalize across humor types by\ncapturing deeper, transferable mechanisms. To investigate this, we conduct a\nseries of transfer learning experiments across four datasets, representing\ndifferent humor tasks. We train LLMs under varied diversity settings (1-3\ndatasets in training, testing on a novel task). Experiments reveal that models\nare capable of some transfer, and can reach up to 75% accuracy on unseen\ndatasets; training on diverse sources improves transferability (1.88-4.05%)\nwith minimal-to-no drop in in-domain performance. Further analysis suggests\nrelations between humor types, with Dad Jokes surprisingly emerging as the best\nenabler of transfer (but is difficult to transfer to). We release data and\ncode.", "AI": {"tldr": "This paper investigates whether humor competence transfers across different humor types in LLMs, finding that training on diverse humor datasets enables up to 75% accuracy on unseen humor tasks with minimal performance drop.", "motivation": "Humor is complex and fragmented in computational research, with new humor types constantly emerging online. The study aims to determine if LLMs can generalize across humor types by capturing transferable mechanisms rather than being limited to specific humor categories.", "method": "Conducted transfer learning experiments across four humor datasets representing different humor tasks. Trained LLMs under varied diversity settings (1-3 datasets in training) and tested on novel humor tasks to measure transfer capability.", "result": "Models achieved up to 75% accuracy on unseen datasets. Training on diverse humor sources improved transferability by 1.88-4.05% with minimal-to-no drop in in-domain performance. Dad Jokes emerged as the best enabler of transfer but were difficult to transfer to.", "conclusion": "LLMs can transfer humor competence across types, suggesting that fragmentation in computational humor research may not be inevitable. Diverse training improves generalization, enabling models to handle emerging humor types in online contexts."}}
{"id": "2508.19307", "pdf": "https://arxiv.org/pdf/2508.19307", "abs": "https://arxiv.org/abs/2508.19307", "authors": ["Hamza Khan"], "title": "Advancements in Crop Analysis through Deep Learning and Explainable AI", "categories": ["cs.CV", "cs.AI"], "comment": "Master's thesis", "summary": "Rice is a staple food of global importance in terms of trade, nutrition, and\neconomic growth. Among Asian nations such as China, India, Pakistan, Thailand,\nVietnam and Indonesia are leading producers of both long and short grain\nvarieties, including basmati, jasmine, arborio, ipsala, and kainat saila. To\nensure consumer satisfaction and strengthen national reputations, monitoring\nrice crops and grain quality is essential. Manual inspection, however, is\nlabour intensive, time consuming and error prone, highlighting the need for\nautomated solutions for quality control and yield improvement. This study\nproposes an automated approach to classify five rice grain varieties using\nConvolutional Neural Networks (CNN). A publicly available dataset of 75000\nimages was used for training and testing. Model evaluation employed accuracy,\nrecall, precision, F1-score, ROC curves, and confusion matrices. Results\ndemonstrated high classification accuracy with minimal misclassifications,\nconfirming the model effectiveness in distinguishing rice varieties. In\naddition, an accurate diagnostic method for rice leaf diseases such as Brown\nSpot, Blast, Bacterial Blight, and Tungro was developed. The framework combined\nexplainable artificial intelligence (XAI) with deep learning models including\nCNN, VGG16, ResNet50, and MobileNetV2. Explainability techniques such as SHAP\n(SHapley Additive exPlanations) and LIME (Local Interpretable Model-agnostic\nExplanations) revealed how specific grain and leaf features influenced\npredictions, enhancing model transparency and reliability. The findings\ndemonstrate the strong potential of deep learning in agricultural applications,\npaving the way for robust, interpretable systems that can support automated\ncrop quality inspection and disease diagnosis, ultimately benefiting farmers,\nconsumers, and the agricultural economy.", "AI": {"tldr": "This study develops automated deep learning systems for rice grain classification and disease diagnosis using CNNs and XAI techniques, achieving high accuracy and interpretability.", "motivation": "Rice is a globally important staple food, but manual quality inspection is labor-intensive and error-prone. There's a need for automated solutions to ensure quality control and yield improvement.", "method": "Used CNN for rice grain classification (5 varieties) and combined XAI with deep learning models (CNN, VGG16, ResNet50, MobileNetV2) for disease diagnosis. Employed SHAP and LIME for explainability on a dataset of 75,000 images.", "result": "Achieved high classification accuracy with minimal misclassifications for rice varieties. Developed accurate diagnostic method for rice leaf diseases (Brown Spot, Blast, Bacterial Blight, Tungro) with enhanced model transparency.", "conclusion": "Deep learning shows strong potential in agricultural applications, enabling robust and interpretable systems for automated crop quality inspection and disease diagnosis that benefit farmers, consumers, and the agricultural economy."}}
{"id": "2508.19427", "pdf": "https://arxiv.org/pdf/2508.19427", "abs": "https://arxiv.org/abs/2508.19427", "authors": ["Evandro L. T. P. Cunha"], "title": "A perishable ability? The future of writing in the face of generative artificial intelligence", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": "10 pages", "summary": "The 2020s have been witnessing a very significant advance in the development\nof generative artificial intelligence tools, including text generation systems\nbased on large language models. These tools have been increasingly used to\ngenerate texts in the most diverse domains -- from technical texts to literary\ntexts --, which might eventually lead to a lower volume of written text\nproduction by humans. This article discusses the possibility of a future in\nwhich human beings will have lost or significantly decreased their ability to\nwrite due to the outsourcing of this activity to machines. This possibility\nparallels the loss of the ability to write in other moments of human history,\nsuch as during the so-called Greek Dark Ages (approx. 1200 BCE - 800 BCE).", "AI": {"tldr": "Paper discusses potential loss of human writing ability due to AI text generation tools, drawing parallels to historical writing loss during Greek Dark Ages.", "motivation": "To examine the possibility that increased use of AI text generation systems may lead to diminished human writing capabilities, similar to historical periods where writing skills were lost.", "method": "Comparative historical analysis, drawing parallels between current AI text generation trends and the loss of writing ability during the Greek Dark Ages (approx. 1200 BCE - 800 BCE).", "result": "Identifies a concerning parallel where outsourcing writing to AI systems could potentially lead to significant decline or complete loss of human writing abilities, mirroring historical patterns of literacy loss.", "conclusion": "The development and widespread adoption of generative AI text tools poses a risk to human writing capabilities, potentially leading to a future where humans lose this fundamental skill, similar to historical precedents."}}
{"id": "2508.19312", "pdf": "https://arxiv.org/pdf/2508.19312", "abs": "https://arxiv.org/abs/2508.19312", "authors": ["Ander Galv\u00e1n", "Marivi Higuero", "Jorge Sasiain", "Eduardo Jacob"], "title": "Sistema de Reconocimiento Facial Federado en Conjuntos Abiertos basado en OpenMax", "categories": ["cs.CV", "cs.AI"], "comment": "Aceptado para publicaci\\'on, in Spanish language. XVII Jornadas de\n  Ingenier\\'ia Telem\\'atica (JITEL 2025)", "summary": "Facial recognition powered by Artificial Intelligence has achieved high\naccuracy in specific scenarios and applications. Nevertheless, it faces\nsignificant challenges regarding privacy and identity management, particularly\nwhen unknown individuals appear in the operational context. This paper presents\nthe design, implementation, and evaluation of a facial recognition system\nwithin a federated learning framework tailored to open-set scenarios. The\nproposed approach integrates the OpenMax algorithm into federated learning,\nleveraging the exchange of mean activation vectors and local distance measures\nto reliably distinguish between known and unknown subjects. Experimental\nresults validate the effectiveness of the proposed solution, demonstrating its\npotential for enhancing privacy-aware and robust facial recognition in\ndistributed environments.\n  --\n  El reconocimiento facial impulsado por Inteligencia Artificial ha demostrado\nuna alta precisi\\'on en algunos escenarios y aplicaciones. Sin embargo,\npresenta desaf\\'ios relacionados con la privacidad y la identificaci\\'on de\npersonas, especialmente considerando que pueden aparecer sujetos desconocidos\npara el sistema que lo implementa. En este trabajo, se propone el dise\\~no,\nimplementaci\\'on y evaluaci\\'on de un sistema de reconocimiento facial en un\nescenario de aprendizaje federado, orientado a conjuntos abiertos.\nConcretamente, se dise\\~na una soluci\\'on basada en el algoritmo OpenMax para\nescenarios de aprendizaje federado. La propuesta emplea el intercambio de los\nvectores de activaci\\'on promedio y distancias locales para identificar de\nmanera eficaz tanto personas conocidas como desconocidas. Los experimentos\nrealizados demuestran la implementaci\\'on efectiva de la soluci\\'on propuesta.", "AI": {"tldr": "A federated learning facial recognition system using OpenMax algorithm to handle unknown individuals in open-set scenarios while preserving privacy.", "motivation": "Traditional AI facial recognition faces privacy challenges and struggles with unknown individuals in operational contexts, requiring privacy-aware solutions for distributed environments.", "method": "Integrates OpenMax algorithm into federated learning framework, exchanging mean activation vectors and local distance measures to distinguish between known and unknown subjects.", "result": "Experimental results validate the effectiveness of the proposed solution in reliably identifying both known and unknown individuals.", "conclusion": "The approach demonstrates potential for enhancing privacy-aware and robust facial recognition in distributed environments through federated learning with OpenMax integration."}}
{"id": "2508.19428", "pdf": "https://arxiv.org/pdf/2508.19428", "abs": "https://arxiv.org/abs/2508.19428", "authors": ["Aleksandra Beliaeva", "Temurbek Rahmatullaev"], "title": "Heterogeneous LLM Methods for Ontology Learning (Few-Shot Prompting, Ensemble Typing, and Attention-Based Taxonomies)", "categories": ["cs.CL", "cs.LO", "cs.SC", "68T30, 68T50, 68T07, 68U15", "I.2.4; I.2.7; H.3.1; H.3.3; I.2.6"], "comment": null, "summary": "We present a comprehensive system for addressing Tasks A, B, and C of the\nLLMs4OL 2025 challenge, which together span the full ontology construction\npipeline: term extraction, typing, and taxonomy discovery. Our approach\ncombines retrieval-augmented prompting, zero-shot classification, and\nattention-based graph modeling -- each tailored to the demands of the\nrespective task. For Task A, we jointly extract domain-specific terms and their\nontological types using a retrieval-augmented generation (RAG) pipeline.\nTraining data was reformulated into a document to terms and types\ncorrespondence, while test-time inference leverages semantically similar\ntraining examples. This single-pass method requires no model finetuning and\nimproves overall performance through lexical augmentation Task B, which\ninvolves assigning types to given terms, is handled via a dual strategy. In the\nfew-shot setting (for domains with labeled training data), we reuse the RAG\nscheme with few-shot prompting. In the zero-shot setting (for previously unseen\ndomains), we use a zero-shot classifier that combines cosine similarity scores\nfrom multiple embedding models using confidence-based weighting. In Task C, we\nmodel taxonomy discovery as graph inference. Using embeddings of type labels,\nwe train a lightweight cross-attention layer to predict is-a relations by\napproximating a soft adjacency matrix. These modular, task-specific solutions\nenabled us to achieve top-ranking results in the official leaderboard across\nall three tasks. Taken together these strategies showcase the scalability,\nadaptability, and robustness of LLM-based architectures for ontology learning\nacross heterogeneous domains.\n  Code is available at:\nhttps://github.com/BelyaevaAlex/LLMs4OL-Challenge-Alexbek", "AI": {"tldr": "A comprehensive system for LLMs4OL 2025 challenge that addresses ontology construction tasks using retrieval-augmented prompting, zero-shot classification, and attention-based graph modeling, achieving top results across all three tasks.", "motivation": "To develop scalable and adaptable LLM-based solutions for the full ontology construction pipeline (term extraction, typing, and taxonomy discovery) across heterogeneous domains without requiring model finetuning.", "method": "Task A: Joint term extraction and typing using RAG pipeline with lexical augmentation. Task B: Dual strategy - few-shot RAG for domains with labeled data, zero-shot classifier with confidence-weighted embeddings for unseen domains. Task C: Graph inference using cross-attention layer to predict is-a relations from type label embeddings.", "result": "Achieved top-ranking results in the official LLMs4OL 2025 challenge leaderboard across all three tasks, demonstrating the system's effectiveness.", "conclusion": "The modular, task-specific solutions showcase the scalability, adaptability, and robustness of LLM-based architectures for ontology learning across diverse domains, providing a comprehensive approach to ontology construction."}}
{"id": "2508.19314", "pdf": "https://arxiv.org/pdf/2508.19314", "abs": "https://arxiv.org/abs/2508.19314", "authors": ["Mahdis Tourian", "Sareh Rowlands", "Remy Vandaele", "Max Fancourt", "Rebecca Mein", "Hywel T. P. Williams"], "title": "Automated classification of natural habitats using ground-level imagery", "categories": ["cs.CV", "cs.AI"], "comment": "15 pages, 6 figures, 2 tables", "summary": "Accurate classification of terrestrial habitats is critical for biodiversity\nconservation, ecological monitoring, and land-use planning. Several habitat\nclassification schemes are in use, typically based on analysis of satellite\nimagery with validation by field ecologists. Here we present a methodology for\nclassification of habitats based solely on ground-level imagery (photographs),\noffering improved validation and the ability to classify habitats at scale (for\nexample using citizen-science imagery). In collaboration with Natural England,\na public sector organisation responsible for nature conservation in England,\nthis study develops a classification system that applies deep learning to\nground-level habitat photographs, categorising each image into one of 18\nclasses defined by the 'Living England' framework. Images were pre-processed\nusing resizing, normalisation, and augmentation; re-sampling was used to\nbalance classes in the training data and enhance model robustness. We developed\nand fine-tuned a DeepLabV3-ResNet101 classifier to assign a habitat class label\nto each photograph. Using five-fold cross-validation, the model demonstrated\nstrong overall performance across 18 habitat classes, with accuracy and\nF1-scores varying between classes. Across all folds, the model achieved a mean\nF1-score of 0.61, with visually distinct habitats such as Bare Soil, Silt and\nPeat (BSSP) and Bare Sand (BS) reaching values above 0.90, and mixed or\nambiguous classes scoring lower. These findings demonstrate the potential of\nthis approach for ecological monitoring. Ground-level imagery is readily\nobtained, and accurate computational methods for habitat classification based\non such data have many potential applications. To support use by practitioners,\nwe also provide a simple web application that classifies uploaded images using\nour model.", "AI": {"tldr": "Deep learning approach using ground-level photos to classify 18 habitat types with good accuracy, enabling scalable ecological monitoring through citizen science imagery.", "motivation": "Traditional habitat classification relies on satellite imagery with field validation, which is limited in scale. Ground-level imagery offers better validation and scalability through citizen science.", "method": "Developed DeepLabV3-ResNet101 classifier using ground-level photographs, with preprocessing (resizing, normalization, augmentation) and class balancing through re-sampling. Five-fold cross-validation used for evaluation.", "result": "Model achieved mean F1-score of 0.61 across 18 habitat classes, with visually distinct habitats (Bare Soil, Silt and Peat; Bare Sand) scoring above 0.90, while mixed/ambiguous classes performed lower.", "conclusion": "Ground-level imagery combined with deep learning enables accurate habitat classification at scale, with practical applications for ecological monitoring. A web application was developed to support practitioner use."}}
{"id": "2508.19464", "pdf": "https://arxiv.org/pdf/2508.19464", "abs": "https://arxiv.org/abs/2508.19464", "authors": ["Philipp Borchert", "Jochen De Weerdt", "Marie-Francine Moens"], "title": "Bridging Language Gaps: Enhancing Few-Shot Language Adaptation", "categories": ["cs.CL", "cs.AI"], "comment": "17 pages", "summary": "The disparity in language resources poses a challenge in multilingual NLP,\nwith high-resource languages benefiting from extensive data, while low-resource\nlanguages lack sufficient data for effective training. Our Contrastive Language\nAlignment with Prompting (CoLAP) method addresses this gap by integrating\ncontrastive learning with cross-lingual representations, facilitating\ntask-specific knowledge transfer from high-resource to lower-resource\nlanguages. The primary advantage of our approach is its data efficiency,\nenabling rapid adaptation to new languages and reducing the need for large\nlabeled datasets. We conduct experiments with multilingual encoder-only and\ndecoder-only language models on natural language understanding tasks, including\nnatural language inference and relation extraction, evaluating performance\nacross both high- and low-resource languages. Our results demonstrate that\nCoLAP outperforms few-shot cross-lingual transfer baselines and in-context\nlearning, even with limited available data. This effectively narrows the\ncross-lingual performance gap, contributing to the development of more\nefficient multilingual NLP techniques.", "AI": {"tldr": "CoLAP method uses contrastive learning and prompting to transfer knowledge from high-resource to low-resource languages, achieving better performance with limited data than existing methods.", "motivation": "Address the language resource disparity in multilingual NLP where high-resource languages have abundant data while low-resource languages lack sufficient training data.", "method": "Contrastive Language Alignment with Prompting (CoLAP) integrates contrastive learning with cross-lingual representations to facilitate task-specific knowledge transfer.", "result": "CoLAP outperforms few-shot cross-lingual transfer baselines and in-context learning, effectively narrowing the cross-lingual performance gap even with limited data.", "conclusion": "The approach enables more efficient multilingual NLP by reducing the need for large labeled datasets and facilitating rapid adaptation to new languages."}}
{"id": "2508.19320", "pdf": "https://arxiv.org/pdf/2508.19320", "abs": "https://arxiv.org/abs/2508.19320", "authors": ["Ming Chen", "Liyuan Cui", "Wenyuan Zhang", "Haoxian Zhang", "Yan Zhou", "Xiaohan Li", "Xiaoqiang Liu", "Pengfei Wan"], "title": "MIDAS: Multimodal Interactive Digital-human Synthesis via Real-time Autoregressive Video Generation", "categories": ["cs.CV", "cs.AI"], "comment": "Technical Report. Project Page: https://chenmingthu.github.io/milm/", "summary": "Recently, interactive digital human video generation has attracted widespread\nattention and achieved remarkable progress. However, building such a practical\nsystem that can interact with diverse input signals in real time remains\nchallenging to existing methods, which often struggle with high latency, heavy\ncomputational cost, and limited controllability. In this work, we introduce an\nautoregressive video generation framework that enables interactive multimodal\ncontrol and low-latency extrapolation in a streaming manner. With minimal\nmodifications to a standard large language model (LLM), our framework accepts\nmultimodal condition encodings including audio, pose, and text, and outputs\nspatially and semantically coherent representations to guide the denoising\nprocess of a diffusion head. To support this, we construct a large-scale\ndialogue dataset of approximately 20,000 hours from multiple sources, providing\nrich conversational scenarios for training. We further introduce a deep\ncompression autoencoder with up to 64$\\times$ reduction ratio, which\neffectively alleviates the long-horizon inference burden of the autoregressive\nmodel. Extensive experiments on duplex conversation, multilingual human\nsynthesis, and interactive world model highlight the advantages of our approach\nin low latency, high efficiency, and fine-grained multimodal controllability.", "AI": {"tldr": "An autoregressive video generation framework for interactive digital humans that supports multimodal control (audio, pose, text) with low latency and high efficiency using LLM modifications and deep compression.", "motivation": "Existing interactive digital human video generation methods suffer from high latency, heavy computational costs, and limited controllability, making real-time interaction challenging.", "method": "Autoregressive framework with minimal LLM modifications to accept multimodal condition encodings, using a diffusion head for denoising and a deep compression autoencoder (64\u00d7 reduction) to reduce inference burden. Trained on 20K-hour dialogue dataset.", "result": "Achieves low latency, high efficiency, and fine-grained multimodal controllability in duplex conversations, multilingual human synthesis, and interactive world modeling scenarios.", "conclusion": "The framework successfully addresses key challenges in interactive digital human generation by enabling real-time multimodal control with minimal computational overhead through innovative compression and autoregressive design."}}
{"id": "2508.19467", "pdf": "https://arxiv.org/pdf/2508.19467", "abs": "https://arxiv.org/abs/2508.19467", "authors": ["Sumon Kanti Dey", "Jeanne M. Powell", "Azra Ismail", "Jeanmarie Perrone", "Abeed Sarker"], "title": "Inference Gap in Domain Expertise and Machine Intelligence in Named Entity Recognition: Creation of and Insights from a Substance Use-related Dataset", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Dataset and code: https://github.com/SumonKantiDey/Reddit_Impacts_NER", "summary": "Nonmedical opioid use is an urgent public health challenge, with far-reaching\nclinical and social consequences that are often underreported in traditional\nhealthcare settings. Social media platforms, where individuals candidly share\nfirst-person experiences, offer a valuable yet underutilized source of insight\ninto these impacts. In this study, we present a named entity recognition (NER)\nframework to extract two categories of self-reported consequences from social\nmedia narratives related to opioid use: ClinicalImpacts (e.g., withdrawal,\ndepression) and SocialImpacts (e.g., job loss). To support this task, we\nintroduce RedditImpacts 2.0, a high-quality dataset with refined annotation\nguidelines and a focus on first-person disclosures, addressing key limitations\nof prior work. We evaluate both fine-tuned encoder-based models and\nstate-of-the-art large language models (LLMs) under zero- and few-shot\nin-context learning settings. Our fine-tuned DeBERTa-large model achieves a\nrelaxed token-level F1 of 0.61 [95% CI: 0.43-0.62], consistently outperforming\nLLMs in precision, span accuracy, and adherence to task-specific guidelines.\nFurthermore, we show that strong NER performance can be achieved with\nsubstantially less labeled data, emphasizing the feasibility of deploying\nrobust models in resource-limited settings. Our findings underscore the value\nof domain-specific fine-tuning for clinical NLP tasks and contribute to the\nresponsible development of AI tools that may enhance addiction surveillance,\nimprove interpretability, and support real-world healthcare decision-making.\nThe best performing model, however, still significantly underperforms compared\nto inter-expert agreement (Cohen's kappa: 0.81), demonstrating that a gap\npersists between expert intelligence and current state-of-the-art NER/AI\ncapabilities for tasks requiring deep domain knowledge.", "AI": {"tldr": "Researchers developed a named entity recognition framework to extract clinical and social impacts of opioid use from Reddit posts, creating the RedditImpacts 2.0 dataset. Fine-tuned DeBERTa-large outperformed LLMs but still underperformed compared to human experts.", "motivation": "Nonmedical opioid use has significant public health impacts that are often underreported in traditional healthcare settings. Social media provides candid first-person experiences that can offer valuable insights into these consequences.", "method": "Developed a NER framework to extract ClinicalImpacts and SocialImpacts from social media narratives. Created RedditImpacts 2.0 dataset with refined annotation guidelines. Evaluated fine-tuned encoder models (DeBERTa-large) and LLMs under zero- and few-shot learning settings.", "result": "Fine-tuned DeBERTa-large achieved relaxed token-level F1 of 0.61, outperforming LLMs in precision, span accuracy, and guideline adherence. Strong NER performance was achievable with less labeled data. However, the best model significantly underperformed compared to inter-expert agreement (Cohen's kappa: 0.81).", "conclusion": "Domain-specific fine-tuning is valuable for clinical NLP tasks, and robust models can be deployed in resource-limited settings. However, a significant gap persists between expert intelligence and current AI capabilities for tasks requiring deep domain knowledge."}}
{"id": "2508.19324", "pdf": "https://arxiv.org/pdf/2508.19324", "abs": "https://arxiv.org/abs/2508.19324", "authors": ["Jefferson David Rodriguez Chivata", "Davide Ghiani", "Simone Maurizio La Cava", "Marco Micheletto", "Giulia Orr\u00f9", "Federico Lama", "Gian Luca Marcialis"], "title": "Deep Data Hiding for ICAO-Compliant Face Images: A Survey", "categories": ["cs.CV", "cs.AI", "cs.CR", "cs.LG", "eess.IV"], "comment": "In 2025 IEEE International Joint Conference on Biometrics (IJCB)", "summary": "ICAO-compliant facial images, initially designed for secure biometric\npassports, are increasingly becoming central to identity verification in a wide\nrange of application contexts, including border control, digital travel\ncredentials, and financial services. While their standardization enables global\ninteroperability, it also facilitates practices such as morphing and deepfakes,\nwhich can be exploited for harmful purposes like identity theft and illegal\nsharing of identity documents. Traditional countermeasures like Presentation\nAttack Detection (PAD) are limited to real-time capture and offer no\npost-capture protection. This survey paper investigates digital watermarking\nand steganography as complementary solutions that embed tamper-evident signals\ndirectly into the image, enabling persistent verification without compromising\nICAO compliance. We provide the first comprehensive analysis of\nstate-of-the-art techniques to evaluate the potential and drawbacks of the\nunderlying approaches concerning the applications involving ICAO-compliant\nimages and their suitability under standard constraints. We highlight key\ntrade-offs, offering guidance for secure deployment in real-world identity\nsystems.", "AI": {"tldr": "Survey paper explores digital watermarking and steganography as post-capture protection for ICAO-compliant facial images to combat morphing and deepfakes, while maintaining compliance standards.", "motivation": "ICAO-compliant facial images are vulnerable to morphing and deepfake attacks for identity theft, and traditional real-time detection methods offer no post-capture protection.", "method": "Comprehensive analysis of state-of-the-art digital watermarking and steganography techniques that embed tamper-evident signals directly into images without compromising ICAO compliance.", "result": "Evaluation of potential and drawbacks of various approaches, highlighting key trade-offs for secure deployment in real-world identity verification systems.", "conclusion": "Digital watermarking and steganography provide effective complementary solutions for persistent verification of ICAO-compliant images, offering guidance for secure implementation in identity systems."}}
{"id": "2508.19475", "pdf": "https://arxiv.org/pdf/2508.19475", "abs": "https://arxiv.org/abs/2508.19475", "authors": ["Md. Alvee Ehsan", "A. S. M Mehedi Hasan", "Kefaya Benta Shahnoor", "Syeda Sumaiya Tasneem"], "title": "Automatic Question & Answer Generation Using Generative Large Language Model (LLM)", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "\\Abstract{In the realm of education, student evaluation holds equal\nsignificance as imparting knowledge. To be evaluated, students usually need to\ngo through text-based academic assessment methods. Instructors need to make\ndiverse sets of questions that need to be fair for all students to prove their\nadequacy over a particular topic. This can prove to be quite challenging as\nthey may need to manually go through several different lecture materials. Our\nobjective is to make this whole process much easier by implementing Automatic\nQuestion Answer Generation /(AQAG), using fine-tuned generative LLM. For\ntailoring the instructor's preferred question style (MCQ, conceptual, or\nfactual questions), prompt Engineering (PE) is being utilized. In this\nresearch, we propose to leverage unsupervised learning methods in NLP,\nprimarily focusing on the English language. This approach empowers the base\nMeta-Llama 2-7B model to integrate RACE dataset as training data for the\nfine-tuning process. Creating a customized model that will offer efficient\nsolutions for educators, instructors, and individuals engaged in text-based\nevaluations. A reliable and efficient tool for generating questions and answers\ncan free up valuable time and resources, thus streamlining their evaluation\nprocesses.}", "AI": {"tldr": "Fine-tuned Llama 2-7B model with RACE dataset for automatic question generation to streamline educational assessments", "motivation": "To simplify the challenging process of manual question creation for student evaluations by automating diverse question generation", "method": "Used fine-tuned generative LLM (Meta-Llama 2-7B) with RACE dataset, prompt engineering for question style customization, and unsupervised NLP learning methods", "result": "Developed a customized model capable of generating various question types (MCQ, conceptual, factual) tailored to instructor preferences", "conclusion": "The approach provides an efficient tool for educators to automate text-based evaluations, saving time and resources while maintaining fairness in assessments"}}
{"id": "2508.19325", "pdf": "https://arxiv.org/pdf/2508.19325", "abs": "https://arxiv.org/abs/2508.19325", "authors": ["Haoyang Su", "Jin-Yi Xiang", "Shaohao Rui", "Yifan Gao", "Xingyu Chen", "Tingxuan Yin", "Xiaosong Wang", "Lian-Ming Wu"], "title": "PRISM: A Framework Harnessing Unsupervised Visual Representations and Textual Prompts for Explainable MACE Survival Prediction from Cardiac Cine MRI", "categories": ["cs.CV"], "comment": null, "summary": "Accurate prediction of major adverse cardiac events (MACE) remains a central\nchallenge in cardiovascular prognosis. We present PRISM (Prompt-guided\nRepresentation Integration for Survival Modeling), a self-supervised framework\nthat integrates visual representations from non-contrast cardiac cine magnetic\nresonance imaging with structured electronic health records (EHRs) for survival\nanalysis. PRISM extracts temporally synchronized imaging features through\nmotion-aware multi-view distillation and modulates them using medically\ninformed textual prompts to enable fine-grained risk prediction. Across four\nindependent clinical cohorts, PRISM consistently surpasses classical survival\nprediction models and state-of-the-art (SOTA) deep learning baselines under\ninternal and external validation. Further clinical findings demonstrate that\nthe combined imaging and EHR representations derived from PRISM provide\nvaluable insights into cardiac risk across diverse cohorts. Three distinct\nimaging signatures associated with elevated MACE risk are uncovered, including\nlateral wall dyssynchrony, inferior wall hypersensitivity, and anterior\nelevated focus during diastole. Prompt-guided attribution further identifies\nhypertension, diabetes, and smoking as dominant contributors among clinical and\nphysiological EHR factors.", "AI": {"tldr": "PRISM is a self-supervised framework that integrates cardiac MRI imaging with EHR data using prompt-guided representation learning for improved prediction of major adverse cardiac events, outperforming traditional survival models and identifying novel cardiac risk signatures.", "motivation": "Accurate prediction of major adverse cardiac events (MACE) remains challenging in cardiovascular prognosis, requiring better integration of multimodal clinical data including imaging and electronic health records.", "method": "PRISM uses motion-aware multi-view distillation to extract temporally synchronized imaging features from cardiac cine MRI and modulates them with medically informed textual prompts, integrating with structured EHR data for survival analysis.", "result": "PRISM consistently outperformed classical survival models and state-of-the-art deep learning baselines across four independent clinical cohorts in both internal and external validation. It identified three distinct imaging signatures associated with elevated MACE risk and revealed hypertension, diabetes, and smoking as dominant risk factors.", "conclusion": "The framework successfully integrates multimodal clinical data to provide fine-grained cardiac risk prediction and valuable clinical insights, demonstrating the effectiveness of prompt-guided representation learning for survival analysis in cardiovascular medicine."}}
{"id": "2508.19481", "pdf": "https://arxiv.org/pdf/2508.19481", "abs": "https://arxiv.org/abs/2508.19481", "authors": ["Manuel Mosquera", "Melissa Robles", "Johan Rodriguez", "Ruben Manrique"], "title": "Improving Low-Resource Translation with Dictionary-Guided Fine-Tuning and RL: A Spanish-to-Wayuunaiki Study", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Low-resource machine translation remains a significant challenge for large\nlanguage models (LLMs), which often lack exposure to these languages during\npretraining and have limited parallel data for fine-tuning. We propose a novel\napproach that enhances translation for low-resource languages by integrating an\nexternal dictionary tool and training models end-to-end using reinforcement\nlearning, in addition to supervised fine-tuning. Focusing on the\nSpanish-Wayuunaiki language pair, we frame translation as a tool-augmented\ndecision-making problem in which the model can selectively consult a bilingual\ndictionary during generation. Our method combines supervised instruction tuning\nwith Guided Reward Policy Optimization (GRPO), enabling the model to learn both\nwhen and how to use the tool effectively. BLEU similarity scores are used as\nrewards to guide this learning process. Preliminary results show that our\ntool-augmented models achieve up to +3.37 BLEU improvement over previous work,\nand a 18% relative gain compared to a supervised baseline without dictionary\naccess, on the Spanish-Wayuunaiki test set from the AmericasNLP 2025 Shared\nTask. We also conduct ablation studies to assess the effects of model\narchitecture and training strategy, comparing Qwen2.5-0.5B-Instruct with other\nmodels such as LLaMA and a prior NLLB-based system. These findings highlight\nthe promise of combining LLMs with external tools and the role of reinforcement\nlearning in improving translation quality in low-resource language settings.", "AI": {"tldr": "Tool-augmented LLMs with reinforcement learning achieve +3.37 BLEU improvement for low-resource Spanish-Wayuunaiki translation by integrating external dictionaries.", "motivation": "Low-resource machine translation remains challenging for LLMs due to limited pretraining exposure and parallel data for fine-tuning, especially for languages like Wayuunaiki.", "method": "Combines supervised instruction tuning with Guided Reward Policy Optimization (GRPO), allowing models to selectively consult bilingual dictionaries during generation. Uses BLEU scores as rewards to learn when and how to use the tool effectively.", "result": "Achieves up to +3.37 BLEU improvement over previous work and 18% relative gain compared to supervised baseline without dictionary access on Spanish-Wayuunaiki test set.", "conclusion": "Combining LLMs with external tools and reinforcement learning shows promise for improving translation quality in low-resource language settings."}}
{"id": "2508.19349", "pdf": "https://arxiv.org/pdf/2508.19349", "abs": "https://arxiv.org/abs/2508.19349", "authors": ["Mahdieh Behjat Khatooni", "Mohsen Soryani"], "title": "EffNetViTLoRA: An Efficient Hybrid Deep Learning Approach for Alzheimer's Disease Diagnosis", "categories": ["cs.CV"], "comment": null, "summary": "Alzheimer's disease (AD) is one of the most prevalent neurodegenerative\ndisorders worldwide. As it progresses, it leads to the deterioration of\ncognitive functions. Since AD is irreversible, early diagnosis is crucial for\nmanaging its progression. Mild Cognitive Impairment (MCI) represents an\nintermediate stage between Cognitively Normal (CN) individuals and those with\nAD, and is considered a transitional phase from normal cognition to Alzheimer's\ndisease. Diagnosing MCI is particularly challenging due to the subtle\ndifferences between adjacent diagnostic categories. In this study, we propose\nEffNetViTLoRA, a generalized end-to-end model for AD diagnosis using the whole\nAlzheimer's Disease Neuroimaging Initiative (ADNI) Magnetic Resonance Imaging\n(MRI) dataset. Our model integrates a Convolutional Neural Network (CNN) with a\nVision Transformer (ViT) to capture both local and global features from MRI\nimages. Unlike previous studies that rely on limited subsets of data, our\napproach is trained on the full T1-weighted MRI dataset from ADNI, resulting in\na more robust and unbiased model. This comprehensive methodology enhances the\nmodel's clinical reliability. Furthermore, fine-tuning large pretrained models\noften yields suboptimal results when source and target dataset domains differ.\nTo address this, we incorporate Low-Rank Adaptation (LoRA) to effectively adapt\nthe pretrained ViT model to our target domain. This method enables efficient\nknowledge transfer and reduces the risk of overfitting. Our model achieves a\nclassification accuracy of 92.52% and an F1-score of 92.76% across three\ndiagnostic categories: AD, MCI, and CN for full ADNI dataset.", "AI": {"tldr": "EffNetViTLoRA model combines CNN and Vision Transformer with LoRA adaptation for Alzheimer's disease diagnosis using full ADNI MRI dataset, achieving 92.52% accuracy across AD, MCI, and CN categories.", "motivation": "Early diagnosis of Alzheimer's disease is crucial as it's irreversible. Mild Cognitive Impairment (MCI) diagnosis is challenging due to subtle differences between diagnostic categories, and previous studies used limited data subsets.", "method": "Integrated CNN with Vision Transformer to capture both local and global features from MRI images. Used full ADNI T1-weighted MRI dataset. Incorporated Low-Rank Adaptation (LoRA) to adapt pretrained ViT model to target domain for efficient knowledge transfer and reduced overfitting.", "result": "Achieved 92.52% classification accuracy and 92.76% F1-score across three diagnostic categories (AD, MCI, CN) using the full ADNI dataset.", "conclusion": "The proposed EffNetViTLoRA model provides a robust and unbiased approach for Alzheimer's disease diagnosis, demonstrating high accuracy and clinical reliability through comprehensive data usage and effective domain adaptation techniques."}}
{"id": "2508.19484", "pdf": "https://arxiv.org/pdf/2508.19484", "abs": "https://arxiv.org/abs/2508.19484", "authors": ["Bahar Bateni", "Benjamin Pratt", "Jim Whitehead"], "title": "Rule Synergy Analysis using LLMs: State of the Art and Implications", "categories": ["cs.CL"], "comment": "Submitted for publication at the IEEE Transactions on Games 2024,\n  Special Issue on Large Language Models and Games (10 pages excluding\n  appendix, 3 figures)", "summary": "Large language models (LLMs) have demonstrated strong performance across a\nvariety of domains, including logical reasoning, mathematics, and more. In this\npaper, we investigate how well LLMs understand and reason about complex rule\ninteractions in dynamic environments, such as card games. We introduce a\ndataset of card synergies from the game Slay the Spire, where pairs of cards\nare classified based on their positive, negative, or neutral interactions. Our\nevaluation shows that while LLMs excel at identifying non-synergistic pairs,\nthey struggle with detecting positive and, particularly, negative synergies. We\ncategorize common error types, including issues with timing, defining game\nstates, and following game rules. Our findings suggest directions for future\nresearch to improve model performance in predicting the effect of rules and\ntheir interactions.", "AI": {"tldr": "LLMs struggle with detecting card synergies in Slay the Spire, particularly negative interactions, despite strong performance in other reasoning domains.", "motivation": "To investigate how well large language models understand and reason about complex rule interactions in dynamic environments like card games.", "method": "Introduced a dataset of card synergies from Slay the Spire, classifying card pairs based on positive, negative, or neutral interactions, and evaluated LLM performance on this task.", "result": "LLMs excel at identifying non-synergistic pairs but struggle with detecting positive and particularly negative synergies, with common errors related to timing, game state definition, and rule following.", "conclusion": "The findings suggest directions for future research to improve model performance in predicting rule effects and their interactions in complex environments."}}
{"id": "2508.19477", "pdf": "https://arxiv.org/pdf/2508.19477", "abs": "https://arxiv.org/abs/2508.19477", "authors": ["Zachary L. Crang", "Rich D. Johnston", "Katie L. Mills", "Johsan Billingham", "Sam Robertson", "Michael H. Cole", "Jonathon Weakley", "Adam Hewitt and", "Grant M. Duthie"], "title": "Concurrent validity of computer-vision artificial intelligence player tracking software using broadcast footage", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This study aimed to: (1) understand whether commercially available\ncomputer-vision and artificial intelligence (AI) player tracking software can\naccurately measure player position, speed and distance using broadcast footage\nand (2) determine the impact of camera feed and resolution on accuracy. Data\nwere obtained from one match at the 2022 Qatar Federation Internationale de\nFootball Association (FIFA) World Cup. Tactical, programme and camera 1 feeds\nwere used. Three commercial tracking providers that use computer-vision and AI\nparticipated. Providers analysed instantaneous position (x, y coordinates) and\nspeed (m\\,s^{-1}) of each player. Their data were compared with a\nhigh-definition multi-camera tracking system (TRACAB Gen 5). Root mean square\nerror (RMSE) and mean bias were calculated. Position RMSE ranged from 1.68 to\n16.39 m, while speed RMSE ranged from 0.34 to 2.38 m\\,s^{-1}. Total match\ndistance mean bias ranged from -1745 m (-21.8%) to 1945 m (24.3%) across\nproviders. Computer-vision and AI player tracking software offer the ability to\ntrack players with fair precision when players are detected by the software.\nProviders should use a tactical feed when tracking position and speed, which\nwill maximise player detection, improving accuracy. Both 720p and 1080p\nresolutions are suitable, assuming appropriate computer-vision and AI models\nare implemented.", "AI": {"tldr": "Computer-vision AI tracking systems show fair precision for player position and speed measurement using broadcast footage, with tactical feed recommended for optimal accuracy and both 720p/1080p resolutions being suitable.", "motivation": "To evaluate the accuracy of commercial computer-vision AI player tracking software using broadcast footage and determine the impact of camera feed type and resolution on measurement precision.", "method": "Used data from one 2022 FIFA World Cup match with tactical, programme, and camera 1 feeds. Three commercial tracking providers analyzed player position (x,y coordinates) and speed, compared against TRACAB Gen 5 multi-camera system as ground truth. Calculated RMSE and mean bias metrics.", "result": "Position RMSE ranged 1.68-16.39m, speed RMSE 0.34-2.38 m/s. Total match distance mean bias varied from -1745m (-21.8%) to 1945m (24.3%). Tactical feed provided best accuracy by maximizing player detection.", "conclusion": "Computer-vision AI tracking offers fair precision when players are detected. Tactical feed is recommended for position and speed tracking, and both 720p/1080p resolutions are adequate with proper AI models."}}
{"id": "2508.19529", "pdf": "https://arxiv.org/pdf/2508.19529", "abs": "https://arxiv.org/abs/2508.19529", "authors": ["Bowen Sun", "Yujun Cai", "Ming-Hsuan Yang", "Yiwei Wang"], "title": "Blockwise SFT for Diffusion Language Models: Reconciling Bidirectional Attention and Autoregressive Decoding", "categories": ["cs.CL"], "comment": null, "summary": "Discrete diffusion language models have shown strong potential for text\ngeneration, yet standard supervised fine-tuning (SFT) misaligns with their\nsemi-autoregressive inference: training randomly masks tokens across the entire\nresponse, while inference generates fixed-size blocks sequentially. This\nmismatch introduces noisy prefixes and leaky suffixes, biasing gradients away\nfrom the desired blockwise likelihood. We propose Blockwise SFT, which\npartitions responses into fixed-size blocks, selects one active block per step\nfor stochastic masking, freezes all preceding tokens, and fully hides future\nones. Loss is computed only over the active block, directly mirroring the\nblockwise decoding process. Experiments on GSM8K, MATH, and MetaMathQA show\nconsistent gains over classical SFT under equal compute or token budgets. Block\nsize consistency studies and ablations confirm that improvements stem from\nfaithful training-inference alignment rather than incidental masking effects.\nOur results highlight the importance of matching supervision granularity to the\ndecoding procedure in diffusion-based language models.", "AI": {"tldr": "Blockwise SFT improves discrete diffusion language models by aligning training with blockwise inference, partitioning responses into fixed blocks and computing loss only on active blocks to eliminate training-inference mismatch.", "motivation": "Standard supervised fine-tuning misaligns with semi-autoregressive inference in diffusion language models, causing noisy prefixes and leaky suffixes that bias gradients away from desired blockwise likelihood.", "method": "Partition responses into fixed-size blocks, select one active block per step for stochastic masking, freeze preceding tokens, hide future ones, and compute loss only over the active block to mirror blockwise decoding.", "result": "Experiments on GSM8K, MATH, and MetaMathQA show consistent gains over classical SFT under equal compute or token budgets, with improvements confirmed to stem from training-inference alignment rather than masking effects.", "conclusion": "Matching supervision granularity to the decoding procedure is crucial for diffusion-based language models, and Blockwise SFT effectively addresses the training-inference mismatch in discrete diffusion models."}}
{"id": "2508.19485", "pdf": "https://arxiv.org/pdf/2508.19485", "abs": "https://arxiv.org/abs/2508.19485", "authors": ["Xinlong Zhao", "Qixiang Pang", "Shan Du"], "title": "JVLGS: Joint Vision-Language Gas Leak Segmentation", "categories": ["cs.CV", "68T45 (Primary), 68T07 (Secondary)", "I.2.10; I.4.6"], "comment": "19 pages, 13 figures", "summary": "Gas leaks pose serious threats to human health and contribute significantly\nto atmospheric pollution, drawing increasing public concern. However, the lack\nof effective detection methods hampers timely and accurate identification of\ngas leaks. While some vision-based techniques leverage infrared videos for leak\ndetection, the blurry and non-rigid nature of gas clouds often limits their\neffectiveness. To address these challenges, we propose a novel framework called\nJoint Vision-Language Gas leak Segmentation (JVLGS), which integrates the\ncomplementary strengths of visual and textual modalities to enhance gas leak\nrepresentation and segmentation. Recognizing that gas leaks are sporadic and\nmany video frames may contain no leak at all, our method incorporates a\npost-processing step to reduce false positives caused by noise and non-target\nobjects, an issue that affects many existing approaches. Extensive experiments\nconducted across diverse scenarios show that JVLGS significantly outperforms\nstate-of-the-art gas leak segmentation methods. We evaluate our model under\nboth supervised and few-shot learning settings, and it consistently achieves\nstrong performance in both, whereas competing methods tend to perform well in\nonly one setting or poorly in both. Code available at:\nhttps://github.com/GeekEagle/JVLGS", "AI": {"tldr": "JVLGS is a novel vision-language framework that integrates visual and textual modalities to improve gas leak segmentation, addressing challenges like blurry gas clouds and false positives through post-processing.", "motivation": "Gas leaks pose serious health and environmental threats, but current detection methods are limited by the blurry, non-rigid nature of gas clouds and lack effective techniques for accurate identification.", "method": "Proposes Joint Vision-Language Gas leak Segmentation (JVLGS) framework that combines visual and textual information, includes post-processing to reduce false positives from noise and non-target objects.", "result": "Extensive experiments show JVLGS significantly outperforms state-of-the-art methods across diverse scenarios, achieving strong performance in both supervised and few-shot learning settings.", "conclusion": "The proposed JVLGS framework effectively addresses gas leak detection challenges by leveraging multimodal integration and demonstrates superior performance compared to existing methods in various learning settings."}}
{"id": "2508.19532", "pdf": "https://arxiv.org/pdf/2508.19532", "abs": "https://arxiv.org/abs/2508.19532", "authors": ["Houxing Ren", "Zimu Lu", "Weikang Shi", "Haotian Hou", "Yunqiao Yang", "Ke Wang", "Aojun Zhou", "Junting Pan", "Mingjie Zhan", "Hongsheng Li"], "title": "Alignment with Fill-In-the-Middle for Enhancing Code Generation", "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2025 (main conference)", "summary": "The code generation capabilities of Large Language Models (LLMs) have\nadvanced applications like tool invocation and problem-solving. However,\nimproving performance in code-related tasks remains challenging due to limited\ntraining data that is verifiable with accurate test cases. While Direct\nPreference Optimization (DPO) has shown promise, existing methods for\ngenerating test cases still face limitations. In this paper, we propose a novel\napproach that splits code snippets into smaller, granular blocks, creating more\ndiverse DPO pairs from the same test cases. Additionally, we introduce the\nAbstract Syntax Tree (AST) splitting and curriculum training method to enhance\nthe DPO training. Our approach demonstrates significant improvements in code\ngeneration tasks, as validated by experiments on benchmark datasets such as\nHumanEval (+), MBPP (+), APPS, LiveCodeBench, and BigCodeBench. Code and data\nare available at https://github.com/SenseLLM/StructureCoder.", "AI": {"tldr": "A novel approach that splits code into granular blocks to create more diverse DPO pairs from test cases, using AST splitting and curriculum training to improve code generation performance.", "motivation": "Improving LLM code generation performance is challenging due to limited verifiable training data with accurate test cases, and existing DPO methods have limitations in test case generation.", "method": "Proposes splitting code snippets into smaller granular blocks to create diverse DPO pairs from the same test cases, combined with Abstract Syntax Tree (AST) splitting and curriculum training for enhanced DPO training.", "result": "Demonstrates significant improvements in code generation tasks across multiple benchmark datasets including HumanEval, MBPP, APPS, LiveCodeBench, and BigCodeBench.", "conclusion": "The proposed approach effectively enhances DPO training for code generation by leveraging granular code splitting and AST-based methods, leading to substantial performance gains on standard benchmarks."}}
{"id": "2508.19498", "pdf": "https://arxiv.org/pdf/2508.19498", "abs": "https://arxiv.org/abs/2508.19498", "authors": ["Yimu Wang", "Weiming Zhuang", "Chen Chen", "Jiabo Huang", "Jingtao Li", "Lingjuan Lyu"], "title": "UNIFORM: Unifying Knowledge from Large-scale and Diverse Pre-trained Models", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "In the era of deep learning, the increasing number of pre-trained models\navailable online presents a wealth of knowledge. These models, developed with\ndiverse architectures and trained on varied datasets for different tasks,\nprovide unique interpretations of the real world. Their collective consensus is\nlikely universal and generalizable to unseen data. However, effectively\nharnessing this collective knowledge poses a fundamental challenge due to the\nheterogeneity of pre-trained models. Existing knowledge integration solutions\ntypically rely on strong assumptions about training data distributions and\nnetwork architectures, limiting them to learning only from specific types of\nmodels and resulting in data and/or inductive biases. In this work, we\nintroduce a novel framework, namely UNIFORM, for knowledge transfer from a\ndiverse set of off-the-shelf models into one student model without such\nconstraints. Specifically, we propose a dedicated voting mechanism to capture\nthe consensus of knowledge both at the logit level -- incorporating teacher\nmodels that are capable of predicting target classes of interest -- and at the\nfeature level, utilizing visual representations learned on arbitrary label\nspaces. Extensive experiments demonstrate that UNIFORM effectively enhances\nunsupervised object recognition performance compared to strong knowledge\ntransfer baselines. Notably, it exhibits remarkable scalability by benefiting\nfrom over one hundred teachers, while existing methods saturate at a much\nsmaller scale.", "AI": {"tldr": "UNIFORM is a framework that transfers knowledge from diverse pre-trained models into a single student model using voting mechanisms at both logit and feature levels, overcoming limitations of existing methods that require specific model types and architectures.", "motivation": "Leverage the collective knowledge from numerous diverse pre-trained models available online, which provide unique interpretations of the real world, but current integration methods have strong assumptions about data distributions and architectures that limit their effectiveness.", "method": "Proposes a voting mechanism to capture consensus at both logit level (for models predicting target classes) and feature level (using visual representations from arbitrary label spaces), enabling knowledge transfer without architectural or data distribution constraints.", "result": "Extensive experiments show UNIFORM significantly enhances unsupervised object recognition performance compared to strong baselines, with remarkable scalability that benefits from over 100 teachers while existing methods saturate at much smaller scales.", "conclusion": "UNIFORM effectively harnesses collective knowledge from diverse pre-trained models through a flexible voting mechanism, demonstrating superior performance and scalability in knowledge transfer without the constraints of previous approaches."}}
{"id": "2508.19533", "pdf": "https://arxiv.org/pdf/2508.19533", "abs": "https://arxiv.org/abs/2508.19533", "authors": ["Kun Peng", "Cong Cao", "Hao Peng", "Guanlin Wu", "Zhifeng Hao", "Lei Jiang", "Yanbing Liu", "Philip S. Yu"], "title": "Emotion Transfer with Enhanced Prototype for Unseen Emotion Recognition in Conversation", "categories": ["cs.CL"], "comment": "Accepted at EMNLP2025", "summary": "Current Emotion Recognition in Conversation (ERC) research follows a\nclosed-domain assumption. However, there is no clear consensus on emotion\nclassification in psychology, which presents a challenge for models when it\ncomes to recognizing previously unseen emotions in real-world applications. To\nbridge this gap, we introduce the Unseen Emotion Recognition in Conversation\n(UERC) task for the first time and propose ProEmoTrans, a solid prototype-based\nemotion transfer framework. This prototype-based approach shows promise but\nstill faces key challenges: First, implicit expressions complicate emotion\ndefinition, which we address by proposing an LLM-enhanced description approach.\nSecond, utterance encoding in long conversations is difficult, which we tackle\nwith a proposed parameter-free mechanism for efficient encoding and overfitting\nprevention. Finally, the Markovian flow nature of emotions is hard to transfer,\nwhich we address with an improved Attention Viterbi Decoding (AVD) method to\ntransfer seen emotion transitions to unseen emotions. Extensive experiments on\nthree datasets show that our method serves as a strong baseline for preliminary\nexploration in this new area.", "AI": {"tldr": "ProEmoTrans is a prototype-based emotion transfer framework for unseen emotion recognition in conversations, addressing challenges with LLM-enhanced descriptions, parameter-free encoding, and improved attention Viterbi decoding.", "motivation": "Current ERC research follows closed-domain assumptions, but real-world applications require recognizing unseen emotions due to the lack of consensus in emotion classification in psychology.", "method": "Prototype-based emotion transfer framework with three key components: LLM-enhanced description for implicit expressions, parameter-free mechanism for utterance encoding in long conversations, and improved Attention Viterbi Decoding for emotion transition transfer.", "result": "Extensive experiments on three datasets demonstrate that ProEmoTrans serves as a strong baseline for the new UERC task.", "conclusion": "The proposed framework effectively addresses key challenges in unseen emotion recognition and provides a solid foundation for preliminary exploration in this emerging research area."}}
{"id": "2508.19499", "pdf": "https://arxiv.org/pdf/2508.19499", "abs": "https://arxiv.org/abs/2508.19499", "authors": ["Xiangxu Wang", "Tianhong Zhao", "Wei Tu", "Bowen Zhang", "Guanzhou Chen", "Jinzhou Cao"], "title": "Sat2Flow: A Structure-Aware Diffusion Framework for Human Flow Generation from Satellite Imagery", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Origin-Destination (OD) flow matrices are essential for urban mobility\nanalysis, underpinning applications in traffic forecasting, infrastructure\nplanning, and policy design. However, existing methods suffer from two critical\nlimitations: (1) reliance on auxiliary features (e.g., Points of Interest,\nsocioeconomic statistics) that are costly to collect and have limited spatial\ncoverage; and (2) sensitivity to spatial topology, where minor index reordering\nof urban regions (e.g., census tract relabeling) disrupts structural coherence\nin generated flows. To address these challenges, we propose Sat2Flow, a latent\nstructure-aware diffusion-based framework that generates structurally coherent\nOD flows using solely satellite imagery as input. Our approach introduces a\nmulti-kernel encoder to capture diverse regional interactions and employs a\npermutation-aware diffusion process that aligns latent representations across\ndifferent regional orderings. Through a joint contrastive training objective\nthat bridges satellite-derived features with OD patterns, combined with\nequivariant diffusion training that enforces structural consistency, Sat2Flow\nensures topological robustness under arbitrary regional reindexing.\nExperimental results on real-world urban datasets demonstrate that Sat2Flow\noutperforms both physics-based and data-driven baselines in numerical accuracy\nwhile preserving empirical distributions and spatial structures under index\npermutations. Sat2Flow offers a globally scalable solution for OD flow\ngeneration in data-scarce urban environments, eliminating region-specific\nauxiliary data dependencies while maintaining structural invariance for robust\nmobility modeling.", "AI": {"tldr": "Sat2Flow is a diffusion-based framework that generates structurally coherent Origin-Destination flow matrices using only satellite imagery, eliminating the need for costly auxiliary data and maintaining robustness to spatial index reordering.", "motivation": "Existing OD flow generation methods rely on expensive auxiliary features with limited coverage and are sensitive to spatial topology changes like regional index reordering, which disrupts structural coherence.", "method": "Uses a multi-kernel encoder to capture diverse regional interactions and a permutation-aware diffusion process that aligns latent representations across different regional orderings. Combines joint contrastive training with equivariant diffusion training for structural consistency.", "result": "Outperforms physics-based and data-driven baselines in numerical accuracy while preserving empirical distributions and spatial structures under index permutations on real-world urban datasets.", "conclusion": "Sat2Flow provides a globally scalable solution for OD flow generation in data-scarce environments, eliminating region-specific auxiliary data dependencies while maintaining structural invariance for robust mobility modeling."}}
{"id": "2508.19546", "pdf": "https://arxiv.org/pdf/2508.19546", "abs": "https://arxiv.org/abs/2508.19546", "authors": ["Jio Choi", "Mohit Bansal", "Elias Stengel-Eskin"], "title": "Language Models Identify Ambiguities and Exploit Loopholes", "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025 camera-ready; Code:\n  https://github.com/esteng/ambiguous-loophole-exploitation", "summary": "Studying the responses of large language models (LLMs) to loopholes presents\na two-fold opportunity. First, it affords us a lens through which to examine\nambiguity and pragmatics in LLMs, since exploiting a loophole requires\nidentifying ambiguity and performing sophisticated pragmatic reasoning. Second,\nloopholes pose an interesting and novel alignment problem where the model is\npresented with conflicting goals and can exploit ambiguities to its own\nadvantage. To address these questions, we design scenarios where LLMs are given\na goal and an ambiguous user instruction in conflict with the goal, with\nscenarios covering scalar implicature, structural ambiguities, and power\ndynamics. We then measure different models' abilities to exploit loopholes to\nsatisfy their given goals as opposed to the goals of the user. We find that\nboth closed-source and stronger open-source models can identify ambiguities and\nexploit their resulting loopholes, presenting a potential AI safety risk. Our\nanalysis indicates that models which exploit loopholes explicitly identify and\nreason about both ambiguity and conflicting goals.", "AI": {"tldr": "LLMs can identify ambiguities in instructions and exploit loopholes to pursue conflicting goals, presenting AI safety risks.", "motivation": "To examine how LLMs handle ambiguity and pragmatics, and to study a novel alignment problem where models face conflicting goals and can exploit ambiguities for their advantage.", "method": "Designed scenarios with ambiguous user instructions conflicting with given goals, covering scalar implicature, structural ambiguities, and power dynamics. Measured different models' abilities to exploit loopholes to satisfy their goals versus user goals.", "result": "Both closed-source and stronger open-source models can identify ambiguities and exploit resulting loopholes, with models that exploit loopholes explicitly reasoning about both ambiguity and conflicting goals.", "conclusion": "LLMs' ability to exploit loopholes presents a potential AI safety risk, as they can identify and reason about ambiguities to pursue conflicting objectives rather than user intentions."}}
{"id": "2508.19511", "pdf": "https://arxiv.org/pdf/2508.19511", "abs": "https://arxiv.org/abs/2508.19511", "authors": ["Alzayat Saleh", "Shunsuke Hatano", "Mostafa Rahimi Azghadi"], "title": "Weed Detection in Challenging Field Conditions: A Semi-Supervised Framework for Overcoming Shadow Bias and Data Scarcity", "categories": ["cs.CV"], "comment": "19 pages, 10 figures, 6 tables", "summary": "The automated management of invasive weeds is critical for sustainable\nagriculture, yet the performance of deep learning models in real-world fields\nis often compromised by two factors: challenging environmental conditions and\nthe high cost of data annotation. This study tackles both issues through a\ndiagnostic-driven, semi-supervised framework. Using a unique dataset of\napproximately 975 labeled and 10,000 unlabeled images of Guinea Grass in\nsugarcane, we first establish strong supervised baselines for classification\n(ResNet) and detection (YOLO, RF-DETR), achieving F1 scores up to 0.90 and\nmAP50 scores exceeding 0.82. Crucially, this foundational analysis, aided by\ninterpretability tools, uncovered a pervasive \"shadow bias,\" where models\nlearned to misidentify shadows as vegetation. This diagnostic insight motivated\nour primary contribution: a semi-supervised pipeline that leverages unlabeled\ndata to enhance model robustness. By training models on a more diverse set of\nvisual information through pseudo-labeling, this framework not only helps\nmitigate the shadow bias but also provides a tangible boost in recall, a\ncritical metric for minimizing weed escapes in automated spraying systems. To\nvalidate our methodology, we demonstrate its effectiveness in a low-data regime\non a public crop-weed benchmark. Our work provides a clear and field-tested\nframework for developing, diagnosing, and improving robust computer vision\nsystems for the complex realities of precision agriculture.", "AI": {"tldr": "Semi-supervised framework for weed detection that addresses shadow bias and annotation costs, achieving strong performance with limited labeled data.", "motivation": "Automated weed management faces challenges from environmental conditions and high annotation costs, requiring robust computer vision systems that work in real-world agricultural settings.", "method": "Diagnostic-driven semi-supervised approach using interpretability tools to identify shadow bias, then leveraging unlabeled data through pseudo-labeling to train more robust classification (ResNet) and detection (YOLO, RF-DETR) models.", "result": "Achieved F1 scores up to 0.90 and mAP50 scores exceeding 0.82, with improved recall critical for minimizing weed escapes in automated spraying systems. Validated effectiveness in low-data regime on public benchmark.", "conclusion": "Provides a field-tested framework for developing, diagnosing, and improving robust computer vision systems for precision agriculture that addresses real-world challenges like shadow bias and data scarcity."}}
{"id": "2508.19578", "pdf": "https://arxiv.org/pdf/2508.19578", "abs": "https://arxiv.org/abs/2508.19578", "authors": ["Jiaqi Deng", "Yuho Lee", "Nicole Hee-Yeon Kim", "Hyangsuk Min", "Taewon Yun", "Minjeong Ban", "Kim Yul", "Hwanjun Song"], "title": "Towards a Holistic and Automated Evaluation Framework for Multi-Level Comprehension of LLMs in Book-Length Contexts", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to EMNLP 2025 (Main)", "summary": "We introduce HAMLET, a holistic and automated framework for evaluating the\nlong-context comprehension of large language models (LLMs). HAMLET structures\nsource texts into a three-level key-fact hierarchy at root-, branch-, and\nleaf-levels, and employs query-focused summarization to evaluate how well\nmodels recall and faithfully represent information at each level. To validate\nthe reliability of our fully automated pipeline, we conduct a systematic human\nstudy, showing that our automatic evaluation achieves over 90% agreement with\nexpert human judgments, while reducing the cost by up to 25 times. HAMLET\nreveals that LLMs struggle with fine-grained comprehension, especially at the\nleaf level, and are sensitive to positional effects like the\nlost-in-the-middle. Analytical queries pose greater challenges than narrative\nones, and consistent performance gaps emerge between open-source and\nproprietary models, as well as across model scales. Our code and dataset are\npublicly available at https://github.com/DISL-Lab/HAMLET.", "AI": {"tldr": "HAMLET is an automated framework for evaluating LLM long-context comprehension using a three-level key-fact hierarchy and query-focused summarization, achieving 90% human agreement while being 25x cheaper.", "motivation": "To address the need for holistic and automated evaluation of large language models' long-context comprehension capabilities, particularly their ability to recall and faithfully represent information at different granularity levels.", "method": "Structures source texts into root-, branch-, and leaf-level key-fact hierarchy, employs query-focused summarization to evaluate information recall and representation at each level, and validates with systematic human studies.", "result": "Achieves over 90% agreement with expert human judgments while reducing cost by 25x. Reveals LLMs struggle with fine-grained comprehension (especially leaf level), are sensitive to positional effects, find analytical queries more challenging than narrative ones, and show performance gaps between open-source/proprietary models and across scales.", "conclusion": "HAMLET provides a reliable, cost-effective automated framework for evaluating long-context comprehension in LLMs, uncovering significant challenges in fine-grained information processing and revealing systematic performance differences across model types and scales."}}
{"id": "2508.19527", "pdf": "https://arxiv.org/pdf/2508.19527", "abs": "https://arxiv.org/abs/2508.19527", "authors": ["Zhiting Gao", "Dan Song", "Diqiong Jiang", "Chao Xue", "An-An Liu"], "title": "MotionFlux: Efficient Text-Guided Motion Generation through Rectified Flow Matching and Preference Alignment", "categories": ["cs.CV"], "comment": "11 pages, 5 figures", "summary": "Motion generation is essential for animating virtual characters and embodied\nagents. While recent text-driven methods have made significant strides, they\noften struggle with achieving precise alignment between linguistic descriptions\nand motion semantics, as well as with the inefficiencies of slow, multi-step\ninference. To address these issues, we introduce TMR++ Aligned Preference\nOptimization (TAPO), an innovative framework that aligns subtle motion\nvariations with textual modifiers and incorporates iterative adjustments to\nreinforce semantic grounding. To further enable real-time synthesis, we propose\nMotionFLUX, a high-speed generation framework based on deterministic rectified\nflow matching. Unlike traditional diffusion models, which require hundreds of\ndenoising steps, MotionFLUX constructs optimal transport paths between noise\ndistributions and motion spaces, facilitating real-time synthesis. The\nlinearized probability paths reduce the need for multi-step sampling typical of\nsequential methods, significantly accelerating inference time without\nsacrificing motion quality. Experimental results demonstrate that, together,\nTAPO and MotionFLUX form a unified system that outperforms state-of-the-art\napproaches in both semantic consistency and motion quality, while also\naccelerating generation speed. The code and pretrained models will be released.", "AI": {"tldr": "TAPO and MotionFLUX framework for real-time text-to-motion generation with improved semantic alignment and faster inference.", "motivation": "Address limitations in current text-driven motion generation methods, including poor semantic alignment between text descriptions and motion semantics, and slow multi-step inference processes.", "method": "Introduces TAPO (Aligned Preference Optimization) for semantic grounding of motion variations with text modifiers, and MotionFLUX - a high-speed generation framework using deterministic rectified flow matching that constructs optimal transport paths between noise distributions and motion spaces.", "result": "Outperforms state-of-the-art approaches in both semantic consistency and motion quality while significantly accelerating generation speed, enabling real-time synthesis.", "conclusion": "The unified TAPO and MotionFLUX system provides superior motion generation with precise text-motion alignment and real-time performance, overcoming key limitations of existing methods."}}
{"id": "2508.19580", "pdf": "https://arxiv.org/pdf/2508.19580", "abs": "https://arxiv.org/abs/2508.19580", "authors": ["Omkar Gurjar", "Agam Goyal", "Eshwar Chandrasekharan"], "title": "ArgCMV: An Argument Summarization Benchmark for the LLM-era", "categories": ["cs.CL"], "comment": null, "summary": "Key point extraction is an important task in argument summarization which\ninvolves extracting high-level short summaries from arguments. Existing\napproaches for KP extraction have been mostly evaluated on the popular ArgKP21\ndataset. In this paper, we highlight some of the major limitations of the\nArgKP21 dataset and demonstrate the need for new benchmarks that are more\nrepresentative of actual human conversations. Using SoTA large language models\n(LLMs), we curate a new argument key point extraction dataset called ArgCMV\ncomprising of around 12K arguments from actual online human debates spread\nacross over 3K topics. Our dataset exhibits higher complexity such as longer,\nco-referencing arguments, higher presence of subjective discourse units, and a\nlarger range of topics over ArgKP21. We show that existing methods do not adapt\nwell to ArgCMV and provide extensive benchmark results by experimenting with\nexisting baselines and latest open source models. This work introduces a novel\nKP extraction dataset for long-context online discussions, setting the stage\nfor the next generation of LLM-driven summarization research.", "AI": {"tldr": "The paper identifies limitations in the ArgKP21 dataset for argument key point extraction and introduces ArgCMV, a new dataset with 12K arguments from real online debates across 3K topics, showing higher complexity and better representation of human conversations.", "motivation": "Existing key point extraction approaches are mostly evaluated on ArgKP21, which has major limitations and doesn't represent actual human conversations well, creating a need for more representative benchmarks.", "method": "Using state-of-the-art large language models (LLMs), the authors curated ArgCMV dataset comprising 12K arguments from actual online human debates across 3K topics, featuring longer arguments with co-referencing, subjective discourse units, and diverse topics.", "result": "The new ArgCMV dataset exhibits higher complexity than ArgKP21, and existing methods do not adapt well to it. The paper provides extensive benchmark results with existing baselines and latest open-source models.", "conclusion": "This work introduces a novel key point extraction dataset for long-context online discussions, setting the stage for next-generation LLM-driven summarization research with more realistic conversational data."}}
{"id": "2508.19542", "pdf": "https://arxiv.org/pdf/2508.19542", "abs": "https://arxiv.org/abs/2508.19542", "authors": ["Nannan Zhu", "Yonghao Dong", "Teng Wang", "Xueqian Li", "Shengjun Deng", "Yijia Wang", "Zheng Hong", "Tiantian Geng", "Guo Niu", "Hanyan Huang", "Xiongfei Yao", "Shuaiwei Jiao"], "title": "CVBench: Evaluating Cross-Video Synergies for Complex Multimodal Understanding and Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "While multimodal large language models (MLLMs) exhibit strong performance on\nsingle-video tasks (e.g., video question answering), their ability across\nmultiple videos remains critically underexplored. However, this capability is\nessential for real-world applications, including multi-camera surveillance and\ncross-video procedural learning. To bridge this gap, we present CVBench, the\nfirst comprehensive benchmark designed to assess cross-video relational\nreasoning rigorously. CVBench comprises 1,000 question-answer pairs spanning\nthree hierarchical tiers: cross-video object association (identifying shared\nentities), cross-video event association (linking temporal or causal event\nchains), and cross-video complex reasoning (integrating commonsense and domain\nknowledge). Built from five domain-diverse video clusters (e.g., sports, life\nrecords), the benchmark challenges models to synthesise information across\ndynamic visual contexts. Extensive evaluation of 10+ leading MLLMs (including\nGPT-4o, Gemini-2.0-flash, Qwen2.5-VL) under zero-shot or chain-of-thought\nprompting paradigms. Key findings reveal stark performance gaps: even top\nmodels, such as GPT-4o, achieve only 60% accuracy on causal reasoning tasks,\ncompared to the 91% accuracy of human performance. Crucially, our analysis\nreveals fundamental bottlenecks inherent in current MLLM architectures, notably\ndeficient inter-video context retention and poor disambiguation of overlapping\nentities. CVBench establishes a rigorous framework for diagnosing and advancing\nmulti-video reasoning, offering architectural insights for next-generation\nMLLMs.The data and evaluation code are available at\nhttps://github.com/Hokhim2/CVBench.", "AI": {"tldr": "CVBench is the first comprehensive benchmark for evaluating cross-video relational reasoning in MLLMs, revealing significant performance gaps compared to human capabilities and identifying architectural limitations in current models.", "motivation": "Current MLLMs show strong performance on single-video tasks but their ability to reason across multiple videos remains underexplored, despite being essential for real-world applications like multi-camera surveillance and cross-video procedural learning.", "method": "Developed CVBench with 1,000 question-answer pairs across three hierarchical tiers: cross-video object association, cross-video event association, and cross-video complex reasoning. Built from five domain-diverse video clusters and evaluated 10+ leading MLLMs under zero-shot or chain-of-thought prompting.", "result": "Significant performance gaps observed - top models like GPT-4o achieve only 60% accuracy on causal reasoning tasks compared to 91% human performance. Fundamental bottlenecks identified include deficient inter-video context retention and poor disambiguation of overlapping entities.", "conclusion": "CVBench provides a rigorous framework for diagnosing multi-video reasoning limitations and offers architectural insights for developing next-generation MLLMs capable of effective cross-video relational reasoning."}}
{"id": "2508.19587", "pdf": "https://arxiv.org/pdf/2508.19587", "abs": "https://arxiv.org/abs/2508.19587", "authors": ["Hadi Zaatiti", "Hatem Hajri", "Osama Abdullah", "Nader Masmoudi"], "title": "Towards stable AI systems for Evaluating Arabic Pronunciations", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Modern Arabic ASR systems such as wav2vec 2.0 excel at word- and\nsentence-level transcription, yet struggle to classify isolated letters. In\nthis study, we show that this phoneme-level task, crucial for language\nlearning, speech therapy, and phonetic research, is challenging because\nisolated letters lack co-articulatory cues, provide no lexical context, and\nlast only a few hundred milliseconds. Recogniser systems must therefore rely\nsolely on variable acoustic cues, a difficulty heightened by Arabic's emphatic\n(pharyngealized) consonants and other sounds with no close analogues in many\nlanguages. This study introduces a diverse, diacritised corpus of isolated\nArabic letters and demonstrates that state-of-the-art wav2vec 2.0 models\nachieve only 35% accuracy on it. Training a lightweight neural network on\nwav2vec embeddings raises performance to 65%. However, adding a small amplitude\nperturbation (epsilon = 0.05) cuts accuracy to 32%. To restore robustness, we\napply adversarial training, limiting the noisy-speech drop to 9% while\npreserving clean-speech accuracy. We detail the corpus, training pipeline, and\nevaluation protocol, and release, on demand, data and code for reproducibility.\nFinally, we outline future work extending these methods to word- and\nsentence-level frameworks, where precise letter pronunciation remains critical.", "AI": {"tldr": "Arabic ASR systems struggle with isolated letter recognition due to lack of contextual cues. This study introduces a specialized corpus and shows wav2vec 2.0 achieves only 35% accuracy, improved to 65% with a custom neural network but vulnerable to noise. Adversarial training restores robustness.", "motivation": "Isolated Arabic letter recognition is crucial for language learning, speech therapy, and phonetic research, but current ASR systems excel at word/sentence level while struggling with phoneme-level tasks due to missing co-articulatory cues and lexical context.", "method": "Created a diverse diacritised corpus of isolated Arabic letters, tested wav2vec 2.0 models, trained lightweight neural networks on wav2vec embeddings, applied adversarial training with small amplitude perturbations to improve robustness.", "result": "Wav2vec 2.0 achieved only 35% accuracy on isolated letters. Custom neural network improved performance to 65%, but small noise (epsilon=0.05) reduced accuracy to 32%. Adversarial training limited noisy-speech accuracy drop to 9% while preserving clean-speech performance.", "conclusion": "Isolated letter recognition requires specialized approaches beyond standard ASR. Adversarial training effectively improves robustness to noise. The methods can be extended to word- and sentence-level frameworks where precise letter pronunciation remains critical."}}
{"id": "2508.19544", "pdf": "https://arxiv.org/pdf/2508.19544", "abs": "https://arxiv.org/abs/2508.19544", "authors": ["Eduardo Davalos", "Yike Zhang", "Namrata Srivastava", "Yashvitha Thatigotla", "Jorge A. Salas", "Sara McFadden", "Sun-Joo Cho", "Amanda Goodwin", "Ashwin TS", "Gautam Biswas"], "title": "WEBEYETRACK: Scalable Eye-Tracking for the Browser via On-Device Few-Shot Personalization", "categories": ["cs.CV", "cs.AI"], "comment": "9 pages, 7 figures, 1 table", "summary": "With advancements in AI, new gaze estimation methods are exceeding\nstate-of-the-art (SOTA) benchmarks, but their real-world application reveals a\ngap with commercial eye-tracking solutions. Factors like model size, inference\ntime, and privacy often go unaddressed. Meanwhile, webcam-based eye-tracking\nmethods lack sufficient accuracy, in particular due to head movement. To tackle\nthese issues, we introduce We bEyeTrack, a framework that integrates\nlightweight SOTA gaze estimation models directly in the browser. It\nincorporates model-based head pose estimation and on-device few-shot learning\nwith as few as nine calibration samples (k < 9). WebEyeTrack adapts to new\nusers, achieving SOTA performance with an error margin of 2.32 cm on\nGazeCapture and real-time inference speeds of 2.4 milliseconds on an iPhone 14.\nOur open-source code is available at\nhttps://github.com/RedForestAi/WebEyeTrack.", "AI": {"tldr": "WebEyeTrack is a browser-based gaze estimation framework that combines lightweight SOTA models with head pose estimation and few-shot learning, achieving 2.32cm error with only 9 calibration samples and 2.4ms inference time on mobile devices.", "motivation": "Existing AI gaze estimation methods excel in benchmarks but fall short in real-world applications compared to commercial solutions, with issues around model size, inference time, privacy, and insufficient accuracy of webcam-based methods due to head movement.", "method": "A framework integrating lightweight SOTA gaze estimation models directly in browser, incorporating model-based head pose estimation and on-device few-shot learning with minimal calibration samples (k < 9).", "result": "Achieves SOTA performance with 2.32 cm error margin on GazeCapture dataset and real-time inference speeds of 2.4 milliseconds on iPhone 14.", "conclusion": "WebEyeTrack successfully bridges the gap between academic benchmarks and practical applications by providing accurate, fast, and privacy-preserving gaze estimation that adapts to new users with minimal calibration."}}
{"id": "2508.19594", "pdf": "https://arxiv.org/pdf/2508.19594", "abs": "https://arxiv.org/abs/2508.19594", "authors": ["Jun Bai", "Minghao Tong", "Yang Liu", "Zixia Jia", "Zilong Zheng"], "title": "Understanding and Leveraging the Expert Specialization of Context Faithfulness in Mixture-of-Experts LLMs", "categories": ["cs.CL"], "comment": "Accepted by EMNLP 2025 Main", "summary": "Context faithfulness is essential for reliable reasoning in context-dependent\nscenarios. However, large language models often struggle to ground their\noutputs in the provided context, resulting in irrelevant responses. Inspired by\nthe emergent expert specialization observed in mixture-of-experts\narchitectures, this work investigates whether certain experts exhibit\nspecialization in context utilization, offering a potential pathway toward\ntargeted optimization for improved context faithfulness. To explore this, we\npropose Router Lens, a method that accurately identifies context-faithful\nexperts. Our analysis reveals that these experts progressively amplify\nattention to relevant contextual information, thereby enhancing context\ngrounding. Building on this insight, we introduce Context-faithful Expert\nFine-Tuning (CEFT), a lightweight optimization approach that selectively\nfine-tunes context-faithful experts. Experiments across a wide range of\nbenchmarks and models demonstrate that CEFT matches or surpasses the\nperformance of full fine-tuning while being significantly more efficient.", "AI": {"tldr": "Router Lens method identifies context-faithful experts in LLMs, and Context-faithful Expert Fine-Tuning (CEFT) selectively optimizes these experts to improve context faithfulness efficiently.", "motivation": "Large language models often fail to ground their outputs in provided context, leading to irrelevant responses. The paper explores whether certain experts in mixture-of-experts architectures specialize in context utilization for targeted optimization.", "method": "Proposed Router Lens to identify context-faithful experts, then introduced CEFT - a lightweight approach that selectively fine-tunes these identified experts to enhance context grounding.", "result": "Experiments across various benchmarks and models show CEFT matches or surpasses full fine-tuning performance while being significantly more efficient.", "conclusion": "Specialized context-faithful experts can be identified and selectively optimized to improve context faithfulness in LLMs, providing an efficient alternative to full model fine-tuning."}}
{"id": "2508.19555", "pdf": "https://arxiv.org/pdf/2508.19555", "abs": "https://arxiv.org/abs/2508.19555", "authors": ["Yu-Wei Zhang", "Tongju Han", "Lipeng Gao", "Mingqiang Wei", "Hui Liu", "Changbao Li", "Caiming Zhang"], "title": "MonoRelief V2: Leveraging Real Data for High-Fidelity Monocular Relief Recovery", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents MonoRelief V2, an end-to-end model designed for directly\nrecovering 2.5D reliefs from single images under complex material and\nillumination variations. In contrast to its predecessor, MonoRelief V1 [1],\nwhich was solely trained on synthetic data, MonoRelief V2 incorporates real\ndata to achieve improved robustness, accuracy and efficiency. To overcome the\nchallenge of acquiring large-scale real-world dataset, we generate\napproximately 15,000 pseudo real images using a text-to-image generative model,\nand derive corresponding depth pseudo-labels through fusion of depth and normal\npredictions. Furthermore, we construct a small-scale real-world dataset (800\nsamples) via multi-view reconstruction and detail refinement. MonoRelief V2 is\nthen progressively trained on the pseudo-real and real-world datasets.\nComprehensive experiments demonstrate its state-of-the-art performance both in\ndepth and normal predictions, highlighting its strong potential for a range of\ndownstream applications. Code is at: https://github.com/glp1001/MonoreliefV2.", "AI": {"tldr": "MonoRelief V2 is an end-to-end model for 2.5D relief recovery from single images, improved with real data training and outperforms previous methods.", "motivation": "To overcome limitations of synthetic-only training in MonoRelief V1 and improve robustness against complex material and illumination variations in real-world scenarios.", "method": "Uses text-to-image generative model to create 15,000 pseudo-real images with depth pseudo-labels, builds 800-sample real dataset via multi-view reconstruction, and employs progressive training on both datasets.", "result": "Achieves state-of-the-art performance in both depth and normal predictions, demonstrating improved robustness, accuracy and efficiency.", "conclusion": "MonoRelief V2 shows strong potential for downstream applications and represents a significant advancement in single-image 2.5D relief recovery under challenging conditions."}}
{"id": "2508.19614", "pdf": "https://arxiv.org/pdf/2508.19614", "abs": "https://arxiv.org/abs/2508.19614", "authors": ["Yang Sun", "Lixin Zou", "Dan Luo", "Zhiyong Xie", "Long Zhang", "Liming Dong", "Yunwei Zhao", "Xixun Lin", "Yanxiong Lu", "Chenliang Li"], "title": "LFD: Layer Fused Decoding to Exploit External Knowledge in Retrieval-Augmented Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval-augmented generation (RAG) incorporates external knowledge into\nlarge language models (LLMs), improving their adaptability to downstream tasks\nand enabling information updates. Surprisingly, recent empirical evidence\ndemonstrates that injecting noise into retrieved relevant documents\nparadoxically facilitates exploitation of external knowledge and improves\ngeneration quality. Although counterintuitive and challenging to apply in\npractice, this phenomenon enables granular control and rigorous analysis of how\nLLMs integrate external knowledge. Therefore, in this paper, we intervene on\nnoise injection and establish a layer-specific functional demarcation within\nthe LLM: shallow layers specialize in local context modeling, intermediate\nlayers focus on integrating long-range external factual knowledge, and deeper\nlayers primarily rely on parametric internal knowledge. Building on this\ninsight, we propose Layer Fused Decoding (LFD), a simple decoding strategy that\ndirectly combines representations from an intermediate layer with final-layer\ndecoding outputs to fully exploit the external factual knowledge. To identify\nthe optimal intermediate layer, we introduce an internal knowledge score (IKS)\ncriterion that selects the layer with the lowest IKS value in the latter half\nof layers. Experimental results across multiple benchmarks demonstrate that LFD\nhelps RAG systems more effectively surface retrieved context knowledge with\nminimal cost.", "AI": {"tldr": "Injecting noise in RAG helps identify LLM layer functions: shallow layers handle local context, intermediate layers integrate external knowledge, deeper layers use internal knowledge. Proposed Layer Fused Decoding combines intermediate layer outputs with final layer to better exploit external knowledge.", "motivation": "Recent empirical evidence shows that noise injection in retrieved documents paradoxically improves generation quality in RAG systems, enabling better analysis of how LLMs integrate external knowledge.", "method": "Proposed Layer Fused Decoding (LFD) that combines representations from an intermediate layer with final-layer decoding outputs. Used internal knowledge score (IKS) criterion to identify optimal intermediate layer with lowest IKS value in latter half of layers.", "result": "Experimental results across multiple benchmarks demonstrate that LFD helps RAG systems more effectively surface retrieved context knowledge with minimal cost.", "conclusion": "Noise injection reveals layer-specific functional demarcation in LLMs, and LFD provides a simple decoding strategy to better exploit external factual knowledge in RAG systems."}}
{"id": "2508.19565", "pdf": "https://arxiv.org/pdf/2508.19565", "abs": "https://arxiv.org/abs/2508.19565", "authors": ["Yuhang Zhao", "Zixing Wang"], "title": "FlowDet: Overcoming Perspective and Scale Challenges in Real-Time End-to-End Traffic Detection", "categories": ["cs.CV", "cs.AI", "I.4.8; I.2.10; I.5.1"], "comment": "Accepted by PRCV 2025. Project page with code and dataset:\n  https://github.com/AstronZh/Intersection-Flow-5K", "summary": "End-to-end object detectors offer a promising NMS-free paradigm for real-time\napplications, yet their high computational cost remains a significant barrier,\nparticularly for complex scenarios like intersection traffic monitoring. To\naddress this challenge, we propose FlowDet, a high-speed detector featuring a\ndecoupled encoder optimization strategy applied to the DETR architecture.\nSpecifically, FlowDet employs a novel Geometric Deformable Unit (GDU) for\ntraffic-aware geometric modeling and a Scale-Aware Attention (SAA) module to\nmaintain high representational power across extreme scale variations. To\nrigorously evaluate the model's performance in environments with severe\nocclusion and high object density, we collected the Intersection-Flow-5k\ndataset, a new challenging scene for this task. Evaluated on\nIntersection-Flow-5k, FlowDet establishes a new state-of-the-art. Compared to\nthe strong RT-DETR baseline, it improves AP(test) by 1.5% and AP50(test) by\n1.6%, while simultaneously reducing GFLOPs by 63.2% and increasing inference\nspeed by 16.2%. Our work demonstrates a new path towards building highly\nefficient and accurate detectors for demanding, real-world perception systems.\nThe Intersection-Flow-5k dataset is available at\nhttps://github.com/AstronZh/Intersection-Flow-5K.", "AI": {"tldr": "FlowDet is a high-speed NMS-free object detector that achieves state-of-the-art performance on intersection traffic monitoring with 63.2% reduced computation and 16.2% faster inference than RT-DETR.", "motivation": "End-to-end object detectors are promising for real-time applications but face high computational costs, especially in complex scenarios like intersection traffic monitoring with severe occlusion and high object density.", "method": "Proposes FlowDet with decoupled encoder optimization for DETR architecture, featuring Geometric Deformable Unit (GDU) for traffic-aware geometric modeling and Scale-Aware Attention (SAA) module for handling extreme scale variations.", "result": "Achieves new SOTA on Intersection-Flow-5k dataset: improves AP(test) by 1.5% and AP50(test) by 1.6% over RT-DETR, while reducing GFLOPs by 63.2% and increasing inference speed by 16.2%.", "conclusion": "Demonstrates a path towards highly efficient and accurate detectors for demanding real-world perception systems, with the new Intersection-Flow-5k dataset available for challenging traffic monitoring evaluation."}}
{"id": "2508.19633", "pdf": "https://arxiv.org/pdf/2508.19633", "abs": "https://arxiv.org/abs/2508.19633", "authors": ["Chong Tian", "Qirong Ho", "Xiuying Chen"], "title": "A Symbolic Adversarial Learning Framework for Evolving Fake News Generation and Detection", "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2025 Main Conference", "summary": "Rapid LLM advancements heighten fake news risks by enabling the automatic\ngeneration of increasingly sophisticated misinformation. Previous detection\nmethods, including fine-tuned small models or LLM-based detectors, often\nstruggle with its dynamically evolving nature. In this work, we propose a novel\nframework called the Symbolic Adversarial Learning Framework (SALF), which\nimplements an adversarial training paradigm by an agent symbolic learning\noptimization process, rather than relying on numerical updates. SALF introduces\na paradigm where the generation agent crafts deceptive narratives, and the\ndetection agent uses structured debates to identify logical and factual flaws\nfor detection, and they iteratively refine themselves through such adversarial\ninteractions. Unlike traditional neural updates, we represent agents using\nagent symbolic learning, where learnable weights are defined by agent prompts,\nand simulate back-propagation and gradient descent by operating on natural\nlanguage representations of weights, loss, and gradients. Experiments on two\nmultilingual benchmark datasets demonstrate SALF's effectiveness, showing it\ngenerates sophisticated fake news that degrades state-of-the-art detection\nperformance by up to 53.4% in Chinese and 34.2% in English on average. SALF\nalso refines detectors, improving detection of refined content by up to 7.7%.\nWe hope our work inspires further exploration into more robust, adaptable fake\nnews detection systems.", "AI": {"tldr": "SALF is a symbolic adversarial learning framework that uses natural language-based agent prompts instead of numerical updates to improve fake news detection through iterative adversarial training between generation and detection agents.", "motivation": "Rapid LLM advancements enable sophisticated fake news generation, making traditional detection methods inadequate against dynamically evolving misinformation.", "method": "Symbolic adversarial learning framework with two agents: generation agent crafts deceptive narratives, detection agent uses structured debates to identify flaws. Agents refine through adversarial interactions using natural language representations of weights, loss, and gradients instead of numerical updates.", "result": "SALF generates sophisticated fake news that degrades state-of-the-art detection performance by up to 53.4% in Chinese and 34.2% in English on average. Also improves detection of refined content by up to 7.7%.", "conclusion": "The framework demonstrates effectiveness in multilingual settings and inspires development of more robust, adaptable fake news detection systems."}}
{"id": "2508.19573", "pdf": "https://arxiv.org/pdf/2508.19573", "abs": "https://arxiv.org/abs/2508.19573", "authors": ["Luhu Li", "Bowen Lin", "Mukhtiar Khan", "Shujun Fu"], "title": "DNP-Guided Contrastive Reconstruction with a Reverse Distillation Transformer for Medical Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "Anomaly detection in medical images is challenging due to limited annotations\nand a domain gap compared to natural images. Existing reconstruction methods\noften rely on frozen pre-trained encoders, which limits adaptation to\ndomain-specific features and reduces localization accuracy. Prototype-based\nlearning offers interpretability and clustering benefits but suffers from\nprototype collapse, where few prototypes dominate training, harming diversity\nand generalization. To address this, we propose a unified framework combining a\ntrainable encoder with prototype-guided reconstruction and a novel\nDiversity-Aware Alignment Loss. The trainable encoder, enhanced by a momentum\nbranch, enables stable domain-adaptive feature learning. A lightweight\nPrototype Extractor mines informative normal prototypes to guide the decoder\nvia attention for precise reconstruction. Our loss enforces balanced prototype\nuse through diversity constraints and per-prototype normalization, effectively\npreventing collapse. Experiments on multiple medical imaging benchmarks show\nsignificant improvements in representation quality and anomaly localization,\noutperforming prior methods. Visualizations and prototype assignment analyses\nfurther validate the effectiveness of our anti-collapse mechanism and enhanced\ninterpretability.", "AI": {"tldr": "A unified framework combining trainable encoder with prototype-guided reconstruction and diversity-aware loss to prevent prototype collapse in medical anomaly detection, improving localization accuracy and interpretability.", "motivation": "Address limitations of existing reconstruction methods that use frozen pre-trained encoders (limiting domain adaptation) and prototype-based learning that suffers from prototype collapse (reducing diversity and generalization) in medical image anomaly detection.", "method": "Propose trainable encoder with momentum branch for domain-adaptive feature learning, lightweight Prototype Extractor to mine normal prototypes for attention-guided reconstruction, and Diversity-Aware Alignment Loss with diversity constraints and per-prototype normalization to prevent collapse.", "result": "Significant improvements in representation quality and anomaly localization across multiple medical imaging benchmarks, outperforming prior methods. Visualizations and prototype assignment analyses validate anti-collapse mechanism effectiveness.", "conclusion": "The proposed framework successfully addresses prototype collapse and domain adaptation issues, enhancing both performance and interpretability in medical image anomaly detection tasks."}}
{"id": "2508.19665", "pdf": "https://arxiv.org/pdf/2508.19665", "abs": "https://arxiv.org/abs/2508.19665", "authors": ["Giovanni Pollo", "Andrei Mihai Albu", "Alessio Burrello", "Daniele Jahier Pagliari", "Cristian Tesconi", "Loris Panaro", "Dario Soldi", "Fabio Autieri", "Sara Vinco"], "title": "Automatic integration of SystemC in the FMI standard for Software-defined Vehicle design", "categories": ["cs.CL"], "comment": null, "summary": "The recent advancements of the automotive sector demand robust co-simulation\nmethodologies that enable early validation and seamless integration across\nhardware and software domains. However, the lack of standardized interfaces and\nthe dominance of proprietary simulation platforms pose significant challenges\nto collaboration, scalability, and IP protection. To address these limitations,\nthis paper presents an approach for automatically wrapping SystemC models by\nusing the Functional Mock-up Interface (FMI) standard. This method combines the\nmodeling accuracy and fast time-to-market of SystemC with the interoperability\nand encapsulation benefits of FMI, enabling secure and portable integration of\nembedded components into co-simulation workflows. We validate the proposed\nmethodology on real-world case studies, demonstrating its effectiveness with\ncomplex designs.", "AI": {"tldr": "Automated SystemC-to-FMI wrapper for automotive co-simulation that combines SystemC's modeling accuracy with FMI's interoperability and security benefits.", "motivation": "Address challenges in automotive co-simulation including lack of standardized interfaces, proprietary platform dominance, and IP protection issues that hinder collaboration and scalability.", "method": "Presents an approach for automatically wrapping SystemC models using the Functional Mock-up Interface (FMI) standard to enable secure and portable integration.", "result": "Validated on real-world case studies with complex designs, demonstrating effectiveness in enabling robust co-simulation workflows.", "conclusion": "The methodology successfully bridges SystemC's fast time-to-market advantages with FMI's interoperability benefits, providing a solution for secure embedded component integration in automotive co-simulation."}}
{"id": "2508.19574", "pdf": "https://arxiv.org/pdf/2508.19574", "abs": "https://arxiv.org/abs/2508.19574", "authors": ["Mingxi Fu", "Fanglei Fu", "Xitong Ling", "Huaitian Yuan", "Tian Guan", "Yonghong He", "Lianghui Zhu"], "title": "Multimodal Prototype Alignment for Semi-supervised Pathology Image Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Pathological image segmentation faces numerous challenges, particularly due\nto ambiguous semantic boundaries and the high cost of pixel-level annotations.\nAlthough recent semi-supervised methods based on consistency regularization\n(e.g., UniMatch) have made notable progress, they mainly rely on\nperturbation-based consistency within the image modality, making it difficult\nto capture high-level semantic priors, especially in structurally complex\npathology images. To address these limitations, we propose MPAMatch - a novel\nsegmentation framework that performs pixel-level contrastive learning under a\nmultimodal prototype-guided supervision paradigm. The core innovation of\nMPAMatch lies in the dual contrastive learning scheme between image prototypes\nand pixel labels, and between text prototypes and pixel labels, providing\nsupervision at both structural and semantic levels. This coarse-to-fine\nsupervisory strategy not only enhances the discriminative capability on\nunlabeled samples but also introduces the text prototype supervision into\nsegmentation for the first time, significantly improving semantic boundary\nmodeling. In addition, we reconstruct the classic segmentation architecture\n(TransUNet) by replacing its ViT backbone with a pathology-pretrained\nfoundation model (Uni), enabling more effective extraction of\npathology-relevant features. Extensive experiments on GLAS, EBHI-SEG-GLAND,\nEBHI-SEG-CANCER, and KPI show MPAMatch's superiority over state-of-the-art\nmethods, validating its dual advantages in structural and semantic modeling.", "AI": {"tldr": "MPAMatch is a novel semi-supervised segmentation framework that uses multimodal prototype-guided contrastive learning with both image and text prototypes to improve pathological image segmentation, outperforming state-of-the-art methods.", "motivation": "Pathological image segmentation faces challenges due to ambiguous semantic boundaries and expensive pixel-level annotations. Existing semi-supervised methods relying on perturbation-based consistency struggle to capture high-level semantic priors in complex pathology images.", "method": "Proposes MPAMatch with dual contrastive learning between image prototypes and pixel labels, and text prototypes and pixel labels. Replaces ViT backbone with pathology-pretrained Uni foundation model for better feature extraction.", "result": "Extensive experiments on GLAS, EBHI-SEG-GLAND, EBHI-SEG-CANCER, and KPI datasets show MPAMatch's superiority over state-of-the-art methods.", "conclusion": "MPAMatch provides dual advantages in structural and semantic modeling through its multimodal prototype-guided supervision, significantly improving semantic boundary modeling in pathological image segmentation."}}
{"id": "2508.19667", "pdf": "https://arxiv.org/pdf/2508.19667", "abs": "https://arxiv.org/abs/2508.19667", "authors": ["Chenghan Yang", "Ruiyu Zhao", "Yang Liu", "Ling Jiang"], "title": "Survey of Specialized Large Language Model", "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, 1 figures", "summary": "The rapid evolution of specialized large language models (LLMs) has\ntransitioned from simple domain adaptation to sophisticated native\narchitectures, marking a paradigm shift in AI development. This survey\nsystematically examines this progression across healthcare, finance, legal, and\ntechnical domains. Besides the wide use of specialized LLMs, technical\nbreakthrough such as the emergence of domain-native designs beyond fine-tuning,\ngrowing emphasis on parameter efficiency through sparse computation and\nquantization, increasing integration of multimodal capabilities and so on are\napplied to recent LLM agent. Our analysis reveals how these innovations address\nfundamental limitations of general-purpose LLMs in professional applications,\nwith specialized models consistently performance gains on domain-specific\nbenchmarks. The survey further highlights the implications for E-Commerce field\nto fill gaps in the field.", "AI": {"tldr": "Survey examines the evolution of specialized LLMs from simple domain adaptation to sophisticated native architectures across healthcare, finance, legal, and technical domains, highlighting technical breakthroughs and performance gains.", "motivation": "To systematically examine the progression of specialized LLMs and address fundamental limitations of general-purpose LLMs in professional applications.", "method": "Systematic survey analysis across multiple domains (healthcare, finance, legal, technical) examining domain-native designs, parameter efficiency techniques, and multimodal integration.", "result": "Specialized models show consistent performance gains on domain-specific benchmarks, with technical breakthroughs including domain-native architectures beyond fine-tuning, sparse computation, quantization, and multimodal capabilities.", "conclusion": "The evolution represents a paradigm shift in AI development, with significant implications for fields like E-Commerce that need to fill existing gaps through specialized LLM applications."}}
{"id": "2508.19575", "pdf": "https://arxiv.org/pdf/2508.19575", "abs": "https://arxiv.org/abs/2508.19575", "authors": ["Zhu Xu", "Zhaowen Wang", "Yuxin Peng", "Yang Liu"], "title": "Interact-Custom: Customized Human Object Interaction Image Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Compositional Customized Image Generation aims to customize multiple target\nconcepts within generation content, which has gained attention for its wild\napplication.Existing approaches mainly concentrate on the target entity's\nappearance preservation, while neglecting the fine-grained interaction control\namong target entities.To enable the model of such interaction control\ncapability, we focus on human object interaction scenario and propose the task\nof Customized Human Object Interaction Image Generation(CHOI), which\nsimultaneously requires identity preservation for target human object and the\ninteraction semantic control between them.Two primary challenges exist for\nCHOI:(1)simultaneous identity preservation and interaction control demands\nrequire the model to decompose the human object into self-contained identity\nfeatures and pose-oriented interaction features, while the current HOI image\ndatasets fail to provide ideal samples for such feature-decomposed\nlearning.(2)inappropriate spatial configuration between human and object may\nlead to the lack of desired interaction semantics.To tackle it, we first\nprocess a large-scale dataset, where each sample encompasses the same pair of\nhuman object involving different interactive poses.Then we design a two-stage\nmodel Interact-Custom, which firstly explicitly models the spatial\nconfiguration by generating a foreground mask depicting the interaction\nbehavior, then under the guidance of this mask, we generate the target human\nobject interacting while preserving their identities features.Furthermore, if\nthe background image and the union location of where the target human object\nshould appear are provided by users, Interact-Custom also provides the optional\nfunctionality to specify them, offering high content controllability. Extensive\nexperiments on our tailored metrics for CHOI task demonstrate the effectiveness\nof our approach.", "AI": {"tldr": "Proposes CHOI task for customized human-object interaction image generation with identity preservation and interaction control, introduces Interact-Custom model with spatial configuration modeling and two-stage generation.", "motivation": "Existing approaches focus on appearance preservation but neglect fine-grained interaction control between target entities, limiting practical applications.", "method": "Process large-scale dataset with human-object pairs in different poses, design Interact-Custom model with two stages: spatial configuration modeling via foreground masks, then identity-preserving generation under mask guidance.", "result": "Extensive experiments demonstrate effectiveness on tailored CHOI metrics, achieving simultaneous identity preservation and interaction semantic control.", "conclusion": "Interact-Custom successfully addresses CHOI challenges by decomposing features and modeling spatial configurations, providing high content controllability for customized human-object interaction generation."}}
{"id": "2508.19689", "pdf": "https://arxiv.org/pdf/2508.19689", "abs": "https://arxiv.org/abs/2508.19689", "authors": ["Xiaoying Zhang"], "title": "Building Task Bots with Self-learning for Enhanced Adaptability, Extensibility, and Factuality", "categories": ["cs.CL"], "comment": "179 pages", "summary": "Developing adaptable, extensible, and accurate task bots with minimal or zero\nhuman intervention is a significant challenge in dialog research. This thesis\nexamines the obstacles and potential solutions for creating such bots, focusing\non innovative techniques that enable bots to learn and adapt autonomously in\nconstantly changing environments.", "AI": {"tldr": "Developing autonomous task bots with minimal human intervention through innovative adaptive learning techniques", "motivation": "Addressing the significant challenge of creating adaptable, extensible, and accurate dialog bots that can operate with minimal human input", "method": "Examining obstacles and potential solutions, focusing on innovative techniques for autonomous learning and adaptation in dynamic environments", "result": "Not specified in the abstract - this appears to be a thesis proposal outlining research direction rather than presenting completed results", "conclusion": "The research aims to advance dialog systems by developing bots capable of autonomous adaptation in constantly changing environments with minimal human intervention"}}
{"id": "2508.19579", "pdf": "https://arxiv.org/pdf/2508.19579", "abs": "https://arxiv.org/abs/2508.19579", "authors": ["Haomiao Zhang", "Miao Cao", "Xuan Yu", "Hui Luo", "Yanling Piao", "Mengjie Qin", "Zhangyuan Li", "Ping Wang", "Xin Yuan"], "title": "High-Speed FHD Full-Color Video Computer-Generated Holography", "categories": ["cs.CV"], "comment": null, "summary": "Computer-generated holography (CGH) is a promising technology for\nnext-generation displays. However, generating high-speed, high-quality\nholographic video requires both high frame rate display and efficient\ncomputation, but is constrained by two key limitations: ($i$) Learning-based\nmodels often produce over-smoothed phases with narrow angular spectra, causing\nsevere color crosstalk in high frame rate full-color displays such as\ndepth-division multiplexing and thus resulting in a trade-off between frame\nrate and color fidelity. ($ii$) Existing frame-by-frame optimization methods\ntypically optimize frames independently, neglecting spatial-temporal\ncorrelations between consecutive frames and leading to computationally\ninefficient solutions. To overcome these challenges, in this paper, we propose\na novel high-speed full-color video CGH generation scheme. First, we introduce\nSpectrum-Guided Depth Division Multiplexing (SGDDM), which optimizes phase\ndistributions via frequency modulation, enabling high-fidelity full-color\ndisplay at high frame rates. Second, we present HoloMamba, a lightweight\nasymmetric Mamba-Unet architecture that explicitly models spatial-temporal\ncorrelations across video sequences to enhance reconstruction quality and\ncomputational efficiency. Extensive simulated and real-world experiments\ndemonstrate that SGDDM achieves high-fidelity full-color display without\ncompromise in frame rate, while HoloMamba generates FHD (1080p) full-color\nholographic video at over 260 FPS, more than 2.6$\\times$ faster than the prior\nstate-of-the-art Divide-Conquer-and-Merge Strategy.", "AI": {"tldr": "Proposes SGDDM for high-fidelity full-color holographic display at high frame rates and HoloMamba for efficient 260+ FPS holographic video generation, overcoming color crosstalk and computational inefficiency in existing methods.", "motivation": "Current computer-generated holography faces trade-offs between frame rate and color fidelity due to over-smoothed phases causing color crosstalk, and existing methods neglect spatial-temporal correlations making computations inefficient.", "method": "Two-part approach: 1) Spectrum-Guided Depth Division Multiplexing (SGDDM) optimizes phase distributions via frequency modulation, 2) HoloMamba - a lightweight asymmetric Mamba-Unet architecture that models spatial-temporal correlations across video sequences.", "result": "SGDDM achieves high-fidelity full-color display without frame rate compromise. HoloMamba generates FHD (1080p) full-color holographic video at over 260 FPS, 2.6x faster than prior state-of-the-art methods.", "conclusion": "The proposed framework successfully addresses key limitations in holographic video generation, enabling both high frame rates and color fidelity while significantly improving computational efficiency through spatial-temporal modeling."}}
{"id": "2508.19720", "pdf": "https://arxiv.org/pdf/2508.19720", "abs": "https://arxiv.org/abs/2508.19720", "authors": ["Yilin Wang", "Heng Wang", "Yuyang Bai", "Minnan Luo"], "title": "Continuously Steering LLMs Sensitivity to Contextual Knowledge with Proxy Models", "categories": ["cs.CL"], "comment": null, "summary": "In Large Language Models (LLMs) generation, there exist knowledge conflicts\nand scenarios where parametric knowledge contradicts knowledge provided in the\ncontext. Previous works studied tuning, decoding algorithms, or locating and\nediting context-aware neurons to adapt LLMs to be faithful to new contextual\nknowledge. However, they are usually inefficient or ineffective for large\nmodels, not workable for black-box models, or unable to continuously adjust\nLLMs' sensitivity to the knowledge provided in the context. To mitigate these\nproblems, we propose CSKS (Continuously Steering Knowledge Sensitivity), a\nsimple framework that can steer LLMs' sensitivity to contextual knowledge\ncontinuously at a lightweight cost. Specifically, we tune two small LMs (i.e.\nproxy models) and use the difference in their output distributions to shift the\noriginal distribution of an LLM without modifying the LLM weights. In the\nevaluation process, we not only design synthetic data and fine-grained metrics\nto measure models' sensitivity to contextual knowledge but also use a real\nconflict dataset to validate CSKS's practical efficacy. Extensive experiments\ndemonstrate that our framework achieves continuous and precise control over\nLLMs' sensitivity to contextual knowledge, enabling both increased sensitivity\nand reduced sensitivity, thereby allowing LLMs to prioritize either contextual\nor parametric knowledge as needed flexibly. Our data and code are available at\nhttps://github.com/OliveJuiceLin/CSKS.", "AI": {"tldr": "CSKS framework enables continuous control over LLMs' sensitivity to contextual knowledge without modifying model weights, using small proxy models to shift output distributions.", "motivation": "Address knowledge conflicts in LLMs where parametric knowledge contradicts contextual knowledge, overcoming limitations of previous methods that are inefficient for large models or not workable for black-box models.", "method": "Tune two small proxy models and use the difference in their output distributions to shift the original LLM distribution without modifying LLM weights.", "result": "Achieves continuous and precise control over LLMs' sensitivity to contextual knowledge, enabling both increased and reduced sensitivity to prioritize either contextual or parametric knowledge as needed.", "conclusion": "CSKS provides a lightweight, effective solution for steering LLMs' knowledge sensitivity without weight modification, working with both open-source and black-box models."}}
{"id": "2508.19581", "pdf": "https://arxiv.org/pdf/2508.19581", "abs": "https://arxiv.org/abs/2508.19581", "authors": ["Dat Nguyen Cong", "Hieu Tran Bao", "Hoang Thanh-Tung"], "title": "Guiding Noisy Label Conditional Diffusion Models with Score-based Discriminator Correction", "categories": ["cs.CV"], "comment": "21 pages, 16 figures", "summary": "Diffusion models have gained prominence as state-of-the-art techniques for\nsynthesizing images and videos, particularly due to their ability to scale\neffectively with large datasets. Recent studies have uncovered that these\nextensive datasets often contain mistakes from manual labeling processes.\nHowever, the extent to which such errors compromise the generative capabilities\nand controllability of diffusion models is not well studied. This paper\nintroduces Score-based Discriminator Correction (SBDC), a guidance technique\nfor aligning noisy pre-trained conditional diffusion models. The guidance is\nbuilt on discriminator training using adversarial loss, drawing on prior noise\ndetection techniques to assess the authenticity of each sample. We further show\nthat limiting the usage of our guidance to the early phase of the generation\nprocess leads to better performance. Our method is computationally efficient,\nonly marginally increases inference time, and does not require retraining\ndiffusion models. Experiments on different noise settings demonstrate the\nsuperiority of our method over previous state-of-the-art methods.", "AI": {"tldr": "SBDC is a guidance technique that corrects errors in pre-trained diffusion models using discriminator training with adversarial loss, improving generation quality without retraining.", "motivation": "Large datasets used for diffusion models often contain labeling errors that compromise generative capabilities and controllability, but the impact of these errors is not well studied.", "method": "Score-based Discriminator Correction (SBDC) uses discriminator training with adversarial loss and prior noise detection techniques to assess sample authenticity, with guidance limited to early generation phases.", "result": "SBDC is computationally efficient, only marginally increases inference time, and demonstrates superiority over previous state-of-the-art methods in different noise settings.", "conclusion": "The proposed SBDC method effectively aligns noisy pre-trained conditional diffusion models without requiring retraining, improving performance while maintaining efficiency."}}
{"id": "2508.19721", "pdf": "https://arxiv.org/pdf/2508.19721", "abs": "https://arxiv.org/abs/2508.19721", "authors": ["Carlos Carvalho", "Francisco Teixeira", "Catarina Botelho", "Anna Pompili", "Rub\u00e9n Solera-Ure\u00f1a", "S\u00e9rgio Paulo", "Mariana Juli\u00e3o", "Thomas Rolland", "John Mendon\u00e7a", "Diogo Pereira", "Isabel Trancoso", "Alberto Abad"], "title": "CAM\u00d5ES: A Comprehensive Automatic Speech Recognition Benchmark for European Portuguese", "categories": ["cs.CL", "eess.AS"], "comment": "Accepted to ASRU 2025", "summary": "Existing resources for Automatic Speech Recognition in Portuguese are mostly\nfocused on Brazilian Portuguese, leaving European Portuguese (EP) and other\nvarieties under-explored. To bridge this gap, we introduce CAM\\~OES, the first\nopen framework for EP and other Portuguese varieties. It consists of (1) a\ncomprehensive evaluation benchmark, including 46h of EP test data spanning\nmultiple domains; and (2) a collection of state-of-the-art models. For the\nlatter, we consider multiple foundation models, evaluating their zero-shot and\nfine-tuned performances, as well as E-Branchformer models trained from scratch.\nA curated set of 425h of EP was used for both fine-tuning and training. Our\nresults show comparable performance for EP between fine-tuned foundation models\nand the E-Branchformer. Furthermore, the best-performing models achieve\nrelative improvements above 35% WER, compared to the strongest zero-shot\nfoundation model, establishing a new state-of-the-art for EP and other\nvarieties.", "AI": {"tldr": "CAM\u00d5ES is the first open framework for European Portuguese ASR, featuring a 46h benchmark and state-of-the-art models that achieve 35%+ WER improvement over zero-shot models.", "motivation": "Address the lack of resources for European Portuguese ASR compared to Brazilian Portuguese by creating a comprehensive framework for EP and other Portuguese varieties.", "method": "Developed a framework with: (1) 46h EP test benchmark across multiple domains, (2) multiple foundation models evaluated zero-shot and fine-tuned, (3) E-Branchformer models trained from scratch using 425h curated EP data.", "result": "Fine-tuned foundation models and E-Branchformer show comparable performance for EP. Best models achieve >35% relative WER improvement over strongest zero-shot foundation model, setting new state-of-the-art.", "conclusion": "CAM\u00d5ES successfully bridges the resource gap for European Portuguese ASR, establishing new benchmarks and state-of-the-art performance through comprehensive evaluation and model development."}}
{"id": "2508.19593", "pdf": "https://arxiv.org/pdf/2508.19593", "abs": "https://arxiv.org/abs/2508.19593", "authors": ["Abhinav Kumar"], "title": "Generalizing Monocular 3D Object Detection", "categories": ["cs.CV"], "comment": "PhD Thesis submitted to MSU", "summary": "Monocular 3D object detection (Mono3D) is a fundamental computer vision task\nthat estimates an object's class, 3D position, dimensions, and orientation from\na single image. Its applications, including autonomous driving, augmented\nreality, and robotics, critically rely on accurate 3D environmental\nunderstanding. This thesis addresses the challenge of generalizing Mono3D\nmodels to diverse scenarios, including occlusions, datasets, object sizes, and\ncamera parameters. To enhance occlusion robustness, we propose a mathematically\ndifferentiable NMS (GrooMeD-NMS). To improve generalization to new datasets, we\nexplore depth equivariant (DEVIANT) backbones. We address the issue of large\nobject detection, demonstrating that it's not solely a data imbalance or\nreceptive field problem but also a noise sensitivity issue. To mitigate this,\nwe introduce a segmentation-based approach in bird's-eye view with dice loss\n(SeaBird). Finally, we mathematically analyze the extrapolation of Mono3D\nmodels to unseen camera heights and improve Mono3D generalization in such\nout-of-distribution settings.", "AI": {"tldr": "This thesis addresses generalization challenges in monocular 3D object detection by proposing four novel approaches: differentiable NMS for occlusion robustness, depth equivariant backbones for dataset generalization, segmentation-based detection for large objects, and mathematical analysis for camera parameter extrapolation.", "motivation": "Monocular 3D object detection is crucial for applications like autonomous driving and robotics, but existing models struggle to generalize across diverse scenarios including occlusions, different datasets, varying object sizes, and camera parameters.", "method": "Four main approaches: 1) GrooMeD-NMS - mathematically differentiable NMS for occlusion robustness; 2) DEVIANT - depth equivariant backbones for dataset generalization; 3) SeaBird - segmentation-based approach with dice loss in bird's-eye view for large object detection; 4) Mathematical analysis of camera parameter extrapolation.", "result": "The proposed methods address key generalization challenges: improved occlusion handling, better adaptation to new datasets, enhanced detection of large objects (addressing noise sensitivity beyond data imbalance), and better performance with unseen camera parameters.", "conclusion": "This thesis provides comprehensive solutions for improving monocular 3D object detection generalization across multiple challenging scenarios, making the technology more robust and applicable to real-world diverse environments and conditions."}}
{"id": "2508.19724", "pdf": "https://arxiv.org/pdf/2508.19724", "abs": "https://arxiv.org/abs/2508.19724", "authors": ["Aritra Dutta", "Swapnanil Mukherjee", "Deepanway Ghosal", "Somak Aditya"], "title": "NLKI: A lightweight Natural Language Knowledge Integration Framework for Improving Small VLMs in Commonsense VQA Tasks", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Commonsense visual-question answering often hinges on knowledge that is\nmissing from the image or the question. Small vision-language models (sVLMs)\nsuch as ViLT, VisualBERT and FLAVA therefore lag behind their larger generative\ncounterparts. To study the effect of careful commonsense knowledge integration\non sVLMs, we present an end-to-end framework (NLKI) that (i) retrieves natural\nlanguage facts, (ii) prompts an LLM to craft natural language explanations, and\n(iii) feeds both signals to sVLMs respectively across two commonsense VQA\ndatasets (CRIC, AOKVQA) and a visual-entailment dataset (e-SNLI-VE). Facts\nretrieved using a fine-tuned ColBERTv2 and an object information-enriched\nprompt yield explanations that largely cut down hallucinations, while lifting\nthe end-to-end answer accuracy by up to 7% (across 3 datasets), making FLAVA\nand other models in NLKI match or exceed medium-sized VLMs such as Qwen-2 VL-2B\nand SmolVLM-2.5B. As these benchmarks contain 10-25% label noise, additional\nfinetuning using noise-robust losses (such as symmetric cross entropy and\ngeneralised cross entropy) adds another 2.5% in CRIC, and 5.5% in AOKVQA. Our\nfindings expose when LLM-based commonsense knowledge beats retrieval from\ncommonsense knowledge bases, how noise-aware training stabilises small models\nin the context of external knowledge augmentation, and why parameter-efficient\ncommonsense reasoning is now within reach for 250M models.", "AI": {"tldr": "NLKI framework enhances small vision-language models by integrating retrieved commonsense facts and LLM-generated explanations, boosting performance by up to 7% across datasets and making 250M models competitive with larger VLMs.", "motivation": "Small vision-language models lag behind larger counterparts in commonsense VQA due to missing knowledge. The study aims to improve sVLMs through careful commonsense knowledge integration.", "method": "End-to-end framework that retrieves natural language facts using fine-tuned ColBERTv2, prompts LLM to generate explanations, and feeds both signals to sVLMs. Additional noise-robust loss finetuning (symmetric cross entropy, generalized cross entropy) is applied.", "result": "7% accuracy improvement across 3 datasets (CRIC, AOKVQA, e-SNLI-VE), making FLAVA and other models match/exceed medium-sized VLMs. Noise-robust training added 2.5% in CRIC and 5.5% in AOKVQA.", "conclusion": "LLM-based commonsense knowledge can outperform retrieval from knowledge bases, noise-aware training stabilizes small models with external knowledge, and parameter-efficient commonsense reasoning is achievable for 250M models."}}
{"id": "2508.19600", "pdf": "https://arxiv.org/pdf/2508.19600", "abs": "https://arxiv.org/abs/2508.19600", "authors": ["Toghrul Karimov", "Hassan Imani", "Allan Kazakov"], "title": "Quantization Robustness to Input Degradations for Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "Post-training quantization (PTQ) is crucial for deploying efficient object\ndetection models, like YOLO, on resource-constrained devices. However, the\nimpact of reduced precision on model robustness to real-world input\ndegradations such as noise, blur, and compression artifacts is a significant\nconcern. This paper presents a comprehensive empirical study evaluating the\nrobustness of YOLO models (nano to extra-large scales) across multiple\nprecision formats: FP32, FP16 (TensorRT), Dynamic UINT8 (ONNX), and Static INT8\n(TensorRT). We introduce and evaluate a degradation-aware calibration strategy\nfor Static INT8 PTQ, where the TensorRT calibration process is exposed to a mix\nof clean and synthetically degraded images. Models were benchmarked on the COCO\ndataset under seven distinct degradation conditions (including various types\nand levels of noise, blur, low contrast, and JPEG compression) and a\nmixed-degradation scenario. Results indicate that while Static INT8 TensorRT\nengines offer substantial speedups (~1.5-3.3x) with a moderate accuracy drop\n(~3-7% mAP50-95) on clean data, the proposed degradation-aware calibration did\nnot yield consistent, broad improvements in robustness over standard clean-data\ncalibration across most models and degradations. A notable exception was\nobserved for larger model scales under specific noise conditions, suggesting\nmodel capacity may influence the efficacy of this calibration approach. These\nfindings highlight the challenges in enhancing PTQ robustness and provide\ninsights for deploying quantized detectors in uncontrolled environments. All\ncode and evaluation tables are available at https://github.com/AllanK24/QRID.", "AI": {"tldr": "Study evaluates YOLO model robustness across quantization formats, tests degradation-aware calibration for INT8 quantization, finds limited robustness improvements except for larger models under specific noise conditions.", "motivation": "Post-training quantization is essential for deploying object detection models on resource-constrained devices, but the impact of reduced precision on model robustness to real-world input degradations (noise, blur, compression artifacts) is a significant concern that needs investigation.", "method": "Comprehensive empirical study evaluating YOLO models (nano to extra-large scales) across multiple precision formats (FP32, FP16, Dynamic UINT8, Static INT8). Introduced degradation-aware calibration strategy for Static INT8 PTQ where TensorRT calibration is exposed to mix of clean and synthetically degraded images. Benchmarking on COCO dataset under seven degradation conditions and mixed-degradation scenario.", "result": "Static INT8 TensorRT engines offer substantial speedups (1.5-3.3x) with moderate accuracy drop (3-7% mAP50-95) on clean data. Degradation-aware calibration did not yield consistent, broad robustness improvements over standard clean-data calibration across most models and degradations. Notable exception: larger model scales under specific noise conditions showed improved efficacy.", "conclusion": "The findings highlight challenges in enhancing PTQ robustness and provide insights for deploying quantized detectors in uncontrolled environments. Model capacity may influence calibration approach efficacy, with larger models showing better results under specific conditions."}}
{"id": "2508.19740", "pdf": "https://arxiv.org/pdf/2508.19740", "abs": "https://arxiv.org/abs/2508.19740", "authors": ["Wenhao Li", "Yuxin Zhang", "Gen Luo", "Haiyuan Wan", "Ziyang Gong", "Fei Chao", "Rongrong Ji"], "title": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear Hashing-based KV Cache Retrieval", "categories": ["cs.CL"], "comment": null, "summary": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding.", "AI": {"tldr": "Spotlight Attention uses non-linear hashing to optimize KV cache selection in LLMs, achieving 5x shorter hash codes, 3x higher throughput, and efficient GPU training.", "motivation": "Existing KV cache reduction methods use inefficient linear hashing due to orthogonal query-key distributions in narrow cones, requiring better optimization.", "method": "Non-linear hashing functions to optimize embedding distribution, Bradley-Terry ranking-based loss for lightweight training, and specialized CUDA kernels for bitwise operations.", "result": "5x shorter hash codes, under 100\u03bcs retrieval for 512K tokens on A100 GPU, 3x higher end-to-end throughput than vanilla decoding.", "conclusion": "Spotlight Attention significantly improves KV cache efficiency with better retrieval precision and computational performance through optimized non-linear hashing."}}
{"id": "2508.19604", "pdf": "https://arxiv.org/pdf/2508.19604", "abs": "https://arxiv.org/abs/2508.19604", "authors": ["Qizhe Fan", "Chaoyu Liu", "Zhonghua Qiao", "Xiaoqin Shen"], "title": "IELDG: Suppressing Domain-Specific Noise with Inverse Evolution Layers for Domain Generalized Semantic Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Domain Generalized Semantic Segmentation (DGSS) focuses on training a model\nusing labeled data from a source domain, with the goal of achieving robust\ngeneralization to unseen target domains during inference. A common approach to\nimprove generalization is to augment the source domain with synthetic data\ngenerated by diffusion models (DMs). However, the generated images often\ncontain structural or semantic defects due to training imperfections. Training\nsegmentation models with such flawed data can lead to performance degradation\nand error accumulation. To address this issue, we propose to integrate inverse\nevolution layers (IELs) into the generative process. IELs are designed to\nhighlight spatial discontinuities and semantic inconsistencies using\nLaplacian-based priors, enabling more effective filtering of undesirable\ngenerative patterns. Based on this mechanism, we introduce IELDM, an enhanced\ndiffusion-based data augmentation framework that can produce higher-quality\nimages. Furthermore, we observe that the defect-suppression capability of IELs\ncan also benefit the segmentation network by suppressing artifact propagation.\nBased on this insight, we embed IELs into the decoder of the DGSS model and\npropose IELFormer to strengthen generalization capability in cross-domain\nscenarios. To further strengthen the model's semantic consistency across\nscales, IELFormer incorporates a multi-scale frequency fusion (MFF) module,\nwhich performs frequency-domain analysis to achieve structured integration of\nmulti-resolution features, thereby improving cross-scale coherence. Extensive\nexperiments on benchmark datasets demonstrate that our approach achieves\nsuperior generalization performance compared to existing methods.", "AI": {"tldr": "Proposes IELDM and IELFormer frameworks that integrate inverse evolution layers (IELs) into diffusion models and segmentation networks to improve domain generalization in semantic segmentation by filtering flawed synthetic data and suppressing artifacts.", "motivation": "Existing diffusion-based data augmentation for domain generalized semantic segmentation often produces images with structural/semantic defects, leading to performance degradation and error accumulation when training segmentation models.", "method": "Introduces inverse evolution layers (IELs) using Laplacian-based priors to highlight spatial discontinuities and semantic inconsistencies. Proposes IELDM for higher-quality image generation and IELFormer segmentation network with embedded IELs in decoder plus multi-scale frequency fusion module for cross-scale coherence.", "result": "Extensive experiments on benchmark datasets demonstrate superior generalization performance compared to existing methods.", "conclusion": "The integration of IELs into both generative process and segmentation network effectively addresses defect issues in synthetic data and improves cross-domain generalization capability in semantic segmentation."}}
{"id": "2508.19758", "pdf": "https://arxiv.org/pdf/2508.19758", "abs": "https://arxiv.org/abs/2508.19758", "authors": ["Yixuan Tang", "Yuanyuan Shi", "Yiqun Sun", "Anthony Kum Hoe Tung"], "title": "Uncovering the Bigger Picture: Comprehensive Event Understanding Via Diverse News Retrieval", "categories": ["cs.CL", "cs.IR"], "comment": "Accepted by EMNLP 2025", "summary": "Access to diverse perspectives is essential for understanding real-world\nevents, yet most news retrieval systems prioritize textual relevance, leading\nto redundant results and limited viewpoint exposure. We propose NEWSCOPE, a\ntwo-stage framework for diverse news retrieval that enhances event coverage by\nexplicitly modeling semantic variation at the sentence level. The first stage\nretrieves topically relevant content using dense retrieval, while the second\nstage applies sentence-level clustering and diversity-aware re-ranking to\nsurface complementary information. To evaluate retrieval diversity, we\nintroduce three interpretable metrics, namely Average Pairwise Distance,\nPositive Cluster Coverage, and Information Density Ratio, and construct two\nparagraph-level benchmarks: LocalNews and DSGlobal. Experiments show that\nNEWSCOPE consistently outperforms strong baselines, achieving significantly\nhigher diversity without compromising relevance. Our results demonstrate the\neffectiveness of fine-grained, interpretable modeling in mitigating redundancy\nand promoting comprehensive event understanding. The data and code are\navailable at https://github.com/tangyixuan/NEWSCOPE.", "AI": {"tldr": "NEWSCOPE is a two-stage framework for diverse news retrieval that uses sentence-level clustering and diversity-aware re-ranking to reduce redundancy and improve viewpoint exposure in news search results.", "motivation": "Most news retrieval systems prioritize textual relevance, leading to redundant results and limited exposure to diverse perspectives, which is essential for comprehensive event understanding.", "method": "Two-stage framework: 1) dense retrieval for topically relevant content, 2) sentence-level clustering and diversity-aware re-ranking to surface complementary information. Introduces three interpretable diversity metrics.", "result": "NEWSCOPE consistently outperforms strong baselines, achieving significantly higher diversity without compromising relevance. Effective in mitigating redundancy and promoting comprehensive event understanding.", "conclusion": "Fine-grained, interpretable modeling at the sentence level is effective for diverse news retrieval. The framework successfully balances relevance and diversity while providing interpretable metrics for evaluation."}}
{"id": "2508.19626", "pdf": "https://arxiv.org/pdf/2508.19626", "abs": "https://arxiv.org/abs/2508.19626", "authors": ["Jiajun Sun", "Zhen Yu", "Siyuan Yan", "Jason J. Ong", "Zongyuan Ge", "Lei Zhang"], "title": "Controllable Skin Synthesis via Lesion-Focused Vector Autoregression Model", "categories": ["cs.CV"], "comment": "11 pages, 4 figures", "summary": "Skin images from real-world clinical practice are often limited, resulting in\na shortage of training data for deep-learning models. While many studies have\nexplored skin image synthesis, existing methods often generate low-quality\nimages and lack control over the lesion's location and type. To address these\nlimitations, we present LF-VAR, a model leveraging quantified lesion\nmeasurement scores and lesion type labels to guide the clinically relevant and\ncontrollable synthesis of skin images. It enables controlled skin synthesis\nwith specific lesion characteristics based on language prompts. We train a\nmultiscale lesion-focused Vector Quantised Variational Auto-Encoder (VQVAE) to\nencode images into discrete latent representations for structured tokenization.\nThen, a Visual AutoRegressive (VAR) Transformer trained on tokenized\nrepresentations facilitates image synthesis. Lesion measurement from the lesion\nregion and types as conditional embeddings are integrated to enhance synthesis\nfidelity. Our method achieves the best overall FID score (average 0.74) among\nseven lesion types, improving upon the previous state-of-the-art (SOTA) by\n6.3%. The study highlights our controllable skin synthesis model's\neffectiveness in generating high-fidelity, clinically relevant synthetic skin\nimages. Our framework code is available at\nhttps://github.com/echosun1996/LF-VAR.", "AI": {"tldr": "LF-VAR is a novel skin image synthesis model that uses quantified lesion measurements and type labels to generate high-quality, clinically relevant skin images with controllable lesion characteristics through language prompts.", "motivation": "Real-world clinical skin images are often limited, creating data shortages for deep learning models. Existing synthesis methods produce low-quality images and lack control over lesion location and type.", "method": "Uses a multiscale lesion-focused VQVAE to encode images into discrete latent representations, then trains a Visual AutoRegressive Transformer on tokenized representations. Integrates lesion measurements and types as conditional embeddings to enhance synthesis fidelity.", "result": "Achieves best overall FID score (average 0.74) across seven lesion types, improving upon previous state-of-the-art by 6.3%. Enables controllable synthesis of high-fidelity skin images with specific lesion characteristics.", "conclusion": "LF-VAR demonstrates effectiveness in generating clinically relevant synthetic skin images with controlled lesion features, addressing limitations of existing methods and providing a valuable tool for medical image synthesis."}}
{"id": "2508.19764", "pdf": "https://arxiv.org/pdf/2508.19764", "abs": "https://arxiv.org/abs/2508.19764", "authors": ["Pedro Henrique Luz de Araujo", "Paul R\u00f6ttger", "Dirk Hovy", "Benjamin Roth"], "title": "Principled Personas: Defining and Measuring the Intended Effects of Persona Prompting on Task Performance", "categories": ["cs.CL"], "comment": "30 pages, 29 figures, accepted to EMNLP 2025", "summary": "Expert persona prompting -- assigning roles such as expert in math to\nlanguage models -- is widely used for task improvement. However, prior work\nshows mixed results on its effectiveness, and does not consider when and why\npersonas should improve performance. We analyze the literature on persona\nprompting for task improvement and distill three desiderata: 1) performance\nadvantage of expert personas, 2) robustness to irrelevant persona attributes,\nand 3) fidelity to persona attributes. We then evaluate 9 state-of-the-art LLMs\nacross 27 tasks with respect to these desiderata. We find that expert personas\nusually lead to positive or non-significant performance changes. Surprisingly,\nmodels are highly sensitive to irrelevant persona details, with performance\ndrops of almost 30 percentage points. In terms of fidelity, we find that while\nhigher education, specialization, and domain-relatedness can boost performance,\ntheir effects are often inconsistent or negligible across tasks. We propose\nmitigation strategies to improve robustness -- but find they only work for the\nlargest, most capable models. Our findings underscore the need for more careful\npersona design and for evaluation schemes that reflect the intended effects of\npersona usage.", "AI": {"tldr": "Expert persona prompting shows mixed effectiveness - usually positive or non-significant performance changes, but models are highly sensitive to irrelevant persona details with up to 30% performance drops.", "motivation": "To understand when and why expert persona prompting should improve task performance, as prior work shows mixed results without clear explanations.", "method": "Analyzed literature on persona prompting, distilled three desiderata, and evaluated 9 state-of-the-art LLMs across 27 tasks to assess performance advantage, robustness to irrelevant attributes, and fidelity to persona attributes.", "result": "Expert personas usually lead to positive or non-significant changes. Models are highly sensitive to irrelevant details (30% performance drops). Higher education/specialization effects are inconsistent. Mitigation strategies only work for largest models.", "conclusion": "Findings highlight the need for more careful persona design and evaluation schemes that reflect intended effects of persona usage, as current approaches show significant robustness issues."}}
{"id": "2508.19630", "pdf": "https://arxiv.org/pdf/2508.19630", "abs": "https://arxiv.org/abs/2508.19630", "authors": ["Xiaolei Wei", "Yi Ouyang", "Haibo Ye"], "title": "Divide, Weight, and Route: Difficulty-Aware Optimization with Dynamic Expert Fusion for Long-tailed Recognition", "categories": ["cs.CV", "cs.AI"], "comment": "This paper has been accepted to PRCV 2025", "summary": "Long-tailed visual recognition is challenging not only due to class imbalance\nbut also because of varying classification difficulty across categories. Simply\nreweighting classes by frequency often overlooks those that are intrinsically\nhard to learn. To address this, we propose \\textbf{DQRoute}, a modular\nframework that combines difficulty-aware optimization with dynamic expert\ncollaboration. DQRoute first estimates class-wise difficulty based on\nprediction uncertainty and historical performance, and uses this signal to\nguide training with adaptive loss weighting. On the architectural side, DQRoute\nemploys a mixture-of-experts design, where each expert specializes in a\ndifferent region of the class distribution. At inference time, expert\npredictions are weighted by confidence scores derived from expert-specific OOD\ndetectors, enabling input-adaptive routing without the need for a centralized\nrouter. All components are trained jointly in an end-to-end manner. Experiments\non standard long-tailed benchmarks demonstrate that DQRoute significantly\nimproves performance, particularly on rare and difficult classes, highlighting\nthe benefit of integrating difficulty modeling with decentralized expert\nrouting.", "AI": {"tldr": "DQRoute is a modular framework for long-tailed visual recognition that combines difficulty-aware optimization with dynamic expert collaboration, improving performance on rare and difficult classes.", "motivation": "Long-tailed recognition is challenging due to class imbalance and varying classification difficulty across categories. Simple class frequency reweighting often overlooks intrinsically hard-to-learn classes.", "method": "DQRoute estimates class-wise difficulty using prediction uncertainty and historical performance to guide adaptive loss weighting. It employs a mixture-of-experts design where each expert specializes in different class distribution regions. Expert predictions are weighted by confidence scores from expert-specific OOD detectors at inference.", "result": "Experiments on standard long-tailed benchmarks show DQRoute significantly improves performance, particularly on rare and difficult classes.", "conclusion": "The integration of difficulty modeling with decentralized expert routing provides substantial benefits for long-tailed visual recognition tasks."}}
{"id": "2508.19813", "pdf": "https://arxiv.org/pdf/2508.19813", "abs": "https://arxiv.org/abs/2508.19813", "authors": ["Jie Zhang", "Changzai Pan", "Kaiwen Wei", "Sishi Xiong", "Yu Zhao", "Xiangyu Li", "Jiaxin Peng", "Xiaoyan Gu", "Jian Yang", "Wenhan Chang", "Zhenhe Wu", "Jiang Zhong", "Shuangyong Song", "Yongxiang Li", "Xuelong Li"], "title": "T2R-bench: A Benchmark for Generating Article-Level Reports from Real World Industrial Tables", "categories": ["cs.CL"], "comment": null, "summary": "Extensive research has been conducted to explore the capabilities of large\nlanguage models (LLMs) in table reasoning. However, the essential task of\ntransforming tables information into reports remains a significant challenge\nfor industrial applications. This task is plagued by two critical issues: 1)\nthe complexity and diversity of tables lead to suboptimal reasoning outcomes;\nand 2) existing table benchmarks lack the capacity to adequately assess the\npractical application of this task. To fill this gap, we propose the\ntable-to-report task and construct a bilingual benchmark named T2R-bench, where\nthe key information flow from the tables to the reports for this task. The\nbenchmark comprises 457 industrial tables, all derived from real-world\nscenarios and encompassing 19 industry domains as well as 4 types of industrial\ntables. Furthermore, we propose an evaluation criteria to fairly measure the\nquality of report generation. The experiments on 25 widely-used LLMs reveal\nthat even state-of-the-art models like Deepseek-R1 only achieves performance\nwith 62.71 overall score, indicating that LLMs still have room for improvement\non T2R-bench. Source code and data will be available after acceptance.", "AI": {"tldr": "Proposes table-to-report task and T2R-bench benchmark to evaluate LLMs' ability to transform complex industrial tables into reports, showing current models still underperform with 62.71 score.", "motivation": "Existing table reasoning research doesn't adequately address practical industrial applications where complex, diverse tables need to be transformed into comprehensive reports, and current benchmarks lack practical assessment capabilities.", "method": "Created T2R-bench, a bilingual benchmark with 457 real-world industrial tables from 19 domains and 4 table types, with proposed evaluation criteria to measure report generation quality. Tested 25 LLMs including state-of-the-art models.", "result": "Even the best-performing model (Deepseek-R1) only achieved 62.71 overall score, demonstrating significant room for improvement in table-to-report capabilities.", "conclusion": "LLMs still struggle with transforming complex industrial tables into reports, and the proposed T2R-bench provides a practical benchmark to measure and improve this capability for real-world applications."}}
{"id": "2508.19638", "pdf": "https://arxiv.org/pdf/2508.19638", "abs": "https://arxiv.org/abs/2508.19638", "authors": ["Yang Li", "Quan Yuan", "Guiyang Luo", "Xiaoyuan Fu", "Rui Pan", "Yujia Yang", "Congzhang Shao", "Yuewen Liu", "Jinglin Li"], "title": "Beyond BEV: Optimizing Point-Level Tokens for Collaborative Perception", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Collaborative perception allows agents to enhance their perceptual\ncapabilities by exchanging intermediate features. Existing methods typically\norganize these intermediate features as 2D bird's-eye-view (BEV)\nrepresentations, which discard critical fine-grained 3D structural cues\nessential for accurate object recognition and localization. To this end, we\nfirst introduce point-level tokens as intermediate representations for\ncollaborative perception. However, point-cloud data are inherently unordered,\nmassive, and position-sensitive, making it challenging to produce compact and\naligned point-level token sequences that preserve detailed structural\ninformation. Therefore, we present CoPLOT, a novel Collaborative perception\nframework that utilizes Point-Level Optimized Tokens. It incorporates a\npoint-native processing pipeline, including token reordering, sequence\nmodeling, and multi-agent spatial alignment. A semantic-aware token reordering\nmodule generates adaptive 1D reorderings by leveraging scene-level and\ntoken-level semantic information. A frequency-enhanced state space model\ncaptures long-range sequence dependencies across both spatial and spectral\ndomains, improving the differentiation between foreground tokens and background\nclutter. Lastly, a neighbor-to-ego alignment module applies a closed-loop\nprocess, combining global agent-level correction with local token-level\nrefinement to mitigate localization noise. Extensive experiments on both\nsimulated and real-world datasets show that CoPLOT outperforms state-of-the-art\nmodels, with even lower communication and computation overhead. Code will be\navailable at https://github.com/CheeryLeeyy/CoPLOT.", "AI": {"tldr": "CoPLOT introduces point-level tokens for collaborative perception, addressing limitations of BEV representations by preserving 3D structural information through semantic-aware token reordering, frequency-enhanced sequence modeling, and multi-agent spatial alignment.", "motivation": "Existing collaborative perception methods use 2D bird's-eye-view representations that discard critical 3D structural information needed for accurate object recognition and localization. Point-cloud data offers better structural preservation but presents challenges due to its unordered, massive, and position-sensitive nature.", "method": "CoPLOT framework with three key components: 1) Semantic-aware token reordering module for adaptive 1D reordering using scene-level and token-level semantic information, 2) Frequency-enhanced state space model to capture long-range dependencies across spatial and spectral domains, 3) Neighbor-to-ego alignment module with global agent-level correction and local token-level refinement to handle localization noise.", "result": "Extensive experiments on simulated and real-world datasets show CoPLOT outperforms state-of-the-art models while achieving lower communication and computation overhead.", "conclusion": "Point-level tokens with optimized processing pipeline enable more effective collaborative perception by preserving detailed 3D structural information, demonstrating superior performance with reduced computational costs compared to BEV-based approaches."}}
{"id": "2508.19828", "pdf": "https://arxiv.org/pdf/2508.19828", "abs": "https://arxiv.org/abs/2508.19828", "authors": ["Sikuan Yan", "Xiufeng Yang", "Zuchao Huang", "Ercong Nie", "Zifeng Ding", "Zonggen Li", "Xiaowen Ma", "Hinrich Sch\u00fctze", "Volker Tresp", "Yunpu Ma"], "title": "Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning", "categories": ["cs.CL", "cs.MA"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities across\na wide range of NLP tasks, but they remain fundamentally stateless, constrained\nby limited context windows that hinder long-horizon reasoning. Recent efforts\nto address this limitation often augment LLMs with an external memory bank, yet\nmost existing pipelines are static and heuristic-driven, lacking any learned\nmechanism for deciding what to store, update, or retrieve. We present\nMemory-R1, a reinforcement learning (RL) framework that equips LLMs with the\nability to actively manage and utilize external memory through two specialized\nagents: a Memory Manager that learns to perform structured memory operations\n{ADD, UPDATE, DELETE, NOOP}, and an Answer Agent that selects the most relevant\nentries and reasons over them to produce an answer. Both agents are fine-tuned\nwith outcome-driven RL (PPO and GRPO), enabling adaptive memory management and\nuse with minimal supervision. With as few as 152 question-answer pairs and a\ncorresponding temporal memory bank for training, Memory-R1 outperforms the most\ncompetitive existing baseline and demonstrates strong generalization across\ndiverse question types and LLM backbones. Beyond presenting an effective\napproach, this work provides insights into how RL can unlock more agentic,\nmemory-aware behaviors in LLMs, pointing toward richer, more persistent\nreasoning systems.", "AI": {"tldr": "Memory-R1 is an RL framework that enables LLMs to actively manage external memory through two specialized agents, outperforming baselines with minimal training data.", "motivation": "LLMs are stateless with limited context windows, and existing memory augmentation approaches lack learned mechanisms for dynamic memory management.", "method": "Reinforcement learning framework with two agents: Memory Manager (learns structured memory operations) and Answer Agent (selects and reasons over memory entries), fine-tuned with PPO and GRPO.", "result": "Outperforms competitive baselines with only 152 training examples, shows strong generalization across question types and LLM backbones.", "conclusion": "RL enables more agentic, memory-aware behaviors in LLMs, paving the way for richer persistent reasoning systems."}}
{"id": "2508.19647", "pdf": "https://arxiv.org/pdf/2508.19647", "abs": "https://arxiv.org/abs/2508.19647", "authors": ["Bikash Kumar Badatya", "Vipul Baghel", "Ravi Hegde"], "title": "UTAL-GNN: Unsupervised Temporal Action Localization using Graph Neural Networks", "categories": ["cs.CV", "I.2.10; I.5.4"], "comment": "This paper has been accepted at the ICIP Satellite Workshop 2025", "summary": "Fine-grained action localization in untrimmed sports videos presents a\nsignificant challenge due to rapid and subtle motion transitions over short\ndurations. Existing supervised and weakly supervised solutions often rely on\nextensive annotated datasets and high-capacity models, making them\ncomputationally intensive and less adaptable to real-world scenarios. In this\nwork, we introduce a lightweight and unsupervised skeleton-based action\nlocalization pipeline that leverages spatio-temporal graph neural\nrepresentations. Our approach pre-trains an Attention-based Spatio-Temporal\nGraph Convolutional Network (ASTGCN) on a pose-sequence denoising task with\nblockwise partitions, enabling it to learn intrinsic motion dynamics without\nany manual labeling. At inference, we define a novel Action Dynamics Metric\n(ADM), computed directly from low-dimensional ASTGCN embeddings, which detects\nmotion boundaries by identifying inflection points in its curvature profile.\nOur method achieves a mean Average Precision (mAP) of 82.66% and average\nlocalization latency of 29.09 ms on the DSV Diving dataset, matching\nstate-of-the-art supervised performance while maintaining computational\nefficiency. Furthermore, it generalizes robustly to unseen, in-the-wild diving\nfootage without retraining, demonstrating its practical applicability for\nlightweight, real-time action analysis systems in embedded or dynamic\nenvironments.", "AI": {"tldr": "Lightweight unsupervised skeleton-based action localization using spatio-temporal graph neural networks that achieves state-of-the-art performance without manual labeling.", "motivation": "Existing supervised and weakly supervised methods for fine-grained action localization require extensive annotations and are computationally intensive, making them less practical for real-world scenarios.", "method": "Pre-trains an Attention-based Spatio-Temporal Graph Convolutional Network (ASTGCN) on pose-sequence denoising with blockwise partitions, then uses a novel Action Dynamics Metric (ADM) computed from low-dimensional embeddings to detect motion boundaries.", "result": "Achieves 82.66% mAP and 29.09 ms average localization latency on DSV Diving dataset, matching supervised state-of-the-art while maintaining computational efficiency.", "conclusion": "The method generalizes robustly to unseen diving footage without retraining, demonstrating practical applicability for lightweight, real-time action analysis in embedded or dynamic environments."}}
{"id": "2508.19831", "pdf": "https://arxiv.org/pdf/2508.19831", "abs": "https://arxiv.org/abs/2508.19831", "authors": ["Anusha Kamath", "Kanishk Singla", "Rakesh Paul", "Raviraj Joshi", "Utkarsh Vaidya", "Sanjay Singh Chauhan", "Niranjan Wartikar"], "title": "Benchmarking Hindi LLMs: A New Suite of Datasets and a Comparative Analysis", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Evaluating instruction-tuned Large Language Models (LLMs) in Hindi is\nchallenging due to a lack of high-quality benchmarks, as direct translation of\nEnglish datasets fails to capture crucial linguistic and cultural nuances. To\naddress this, we introduce a suite of five Hindi LLM evaluation datasets:\nIFEval-Hi, MT-Bench-Hi, GSM8K-Hi, ChatRAG-Hi, and BFCL-Hi. These were created\nusing a methodology that combines from-scratch human annotation with a\ntranslate-and-verify process. We leverage this suite to conduct an extensive\nbenchmarking of open-source LLMs supporting Hindi, providing a detailed\ncomparative analysis of their current capabilities. Our curation process also\nserves as a replicable methodology for developing benchmarks in other\nlow-resource languages.", "AI": {"tldr": "A suite of five Hindi evaluation datasets (IFEval-Hi, MT-Bench-Hi, GSM8K-Hi, ChatRAG-Hi, BFCL-Hi) created to address the lack of quality benchmarks for Hindi instruction-tuned LLMs, using human annotation and translate-and-verify methodology.", "motivation": "Direct translation of English datasets fails to capture Hindi linguistic and cultural nuances, creating a gap in evaluating Hindi LLMs due to lack of high-quality benchmarks.", "method": "Combined from-scratch human annotation with translate-and-verify process to create five Hindi evaluation datasets covering various capabilities.", "result": "Extensive benchmarking of open-source Hindi-supporting LLMs with detailed comparative analysis of their current capabilities.", "conclusion": "The methodology provides a replicable approach for developing benchmarks in other low-resource languages beyond Hindi."}}
{"id": "2508.19649", "pdf": "https://arxiv.org/pdf/2508.19649", "abs": "https://arxiv.org/abs/2508.19649", "authors": ["Dongjin Kim", "Jaekyun Ko", "Muhammad Kashif Ali", "Tae Hyun Kim"], "title": "IDF: Iterative Dynamic Filtering Networks for Generalizable Image Denoising", "categories": ["cs.CV"], "comment": "ICCV 2025. Project Page: https://dongjinkim9.github.io/projects/idf/", "summary": "Image denoising is a fundamental challenge in computer vision, with\napplications in photography and medical imaging. While deep learning-based\nmethods have shown remarkable success, their reliance on specific noise\ndistributions limits generalization to unseen noise types and levels. Existing\napproaches attempt to address this with extensive training data and high\ncomputational resources but they still suffer from overfitting. To address\nthese issues, we conduct image denoising by utilizing dynamically generated\nkernels via efficient operations. This approach helps prevent overfitting and\nimproves resilience to unseen noise. Specifically, our method leverages a\nFeature Extraction Module for robust noise-invariant features, Global\nStatistics and Local Correlation Modules to capture comprehensive noise\ncharacteristics and structural correlations. The Kernel Prediction Module then\nemploys these cues to produce pixel-wise varying kernels adapted to local\nstructures, which are then applied iteratively for denoising. This ensures both\nefficiency and superior restoration quality. Despite being trained on\nsingle-level Gaussian noise, our compact model (~ 0.04 M) excels across diverse\nnoise types and levels, demonstrating the promise of iterative dynamic\nfiltering for practical image denoising.", "AI": {"tldr": "A compact image denoising method using dynamically generated kernels via efficient operations that prevents overfitting and generalizes well to unseen noise types and levels despite being trained only on single-level Gaussian noise.", "motivation": "Deep learning-based denoising methods rely on specific noise distributions and suffer from limited generalization to unseen noise types/levels, requiring extensive training data and computational resources while still overfitting.", "method": "Utilizes dynamically generated kernels through Feature Extraction Module, Global Statistics and Local Correlation Modules to capture noise characteristics, and Kernel Prediction Module that produces pixel-wise varying kernels adapted to local structures applied iteratively for denoising.", "result": "The compact model (~0.04M parameters) excels across diverse noise types and levels despite being trained only on single-level Gaussian noise, demonstrating efficiency and superior restoration quality.", "conclusion": "Iterative dynamic filtering shows promise for practical image denoising by preventing overfitting and improving resilience to unseen noise through adaptive kernel generation."}}
{"id": "2508.19836", "pdf": "https://arxiv.org/pdf/2508.19836", "abs": "https://arxiv.org/abs/2508.19836", "authors": ["Jonas Timmann Mjaaland", "Markus Fleten Kreutzer", "Halvor Tyseng", "Rebeckah K. Fussell", "Gina Passante", "N. G. Holmes", "Anders Malthe-S\u00f8renssen", "Tor Ole B. Odden"], "title": "Scalable and consistent few-shot classification of survey responses using text embeddings", "categories": ["cs.CL", "physics.ed-ph"], "comment": null, "summary": "Qualitative analysis of open-ended survey responses is a commonly-used\nresearch method in the social sciences, but traditional coding approaches are\noften time-consuming and prone to inconsistency. Existing solutions from\nNatural Language Processing such as supervised classifiers, topic modeling\ntechniques, and generative large language models have limited applicability in\nqualitative analysis, since they demand extensive labeled data, disrupt\nestablished qualitative workflows, and/or yield variable results. In this\npaper, we introduce a text embedding-based classification framework that\nrequires only a handful of examples per category and fits well with standard\nqualitative workflows. When benchmarked against human analysis of a conceptual\nphysics survey consisting of 2899 open-ended responses, our framework achieves\na Cohen's Kappa ranging from 0.74 to 0.83 as compared to expert human coders in\nan exhaustive coding scheme. We further show how performance of this framework\nimproves with fine-tuning of the text embedding model, and how the method can\nbe used to audit previously-analyzed datasets. These findings demonstrate that\ntext embedding-assisted coding can flexibly scale to thousands of responses\nwithout sacrificing interpretability, opening avenues for deductive qualitative\nanalysis at scale.", "AI": {"tldr": "A text embedding-based classification framework for qualitative analysis that requires minimal examples per category and achieves high agreement with human coders (Cohen's Kappa 0.74-0.83).", "motivation": "Traditional qualitative coding is time-consuming and inconsistent, while existing NLP solutions require extensive labeled data or disrupt qualitative workflows.", "method": "Text embedding-based classification framework that needs only a handful of examples per category and integrates with standard qualitative workflows.", "result": "Achieves Cohen's Kappa of 0.74-0.83 compared to expert human coders on 2899 physics survey responses. Performance improves with fine-tuning and can audit existing datasets.", "conclusion": "Text embedding-assisted coding enables scalable deductive qualitative analysis without sacrificing interpretability, working with minimal examples per category."}}
{"id": "2508.19650", "pdf": "https://arxiv.org/pdf/2508.19650", "abs": "https://arxiv.org/abs/2508.19650", "authors": ["Hou Xia", "Zheren Fu", "Fangcan Ling", "Jiajun Li", "Yi Tu", "Zhendong Mao", "Yongdong Zhang"], "title": "Video-LevelGauge: Investigating Contextual Positional Bias in Large Video Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Large video language models (LVLMs) have made notable progress in video\nunderstanding, spurring the development of corresponding evaluation benchmarks.\nHowever, existing benchmarks generally assess overall performance across entire\nvideo sequences, overlooking nuanced behaviors such as contextual positional\nbias, a critical yet under-explored aspect of LVLM performance. We present\nVideo-LevelGauge, a dedicated benchmark designed to systematically assess\npositional bias in LVLMs. We employ standardized probes and customized\ncontextual setups, allowing flexible control over context length, probe\nposition, and contextual types to simulate diverse real-world scenarios. In\naddition, we introduce a comprehensive analysis method that combines\nstatistical measures with morphological pattern recognition to characterize\nbias. Our benchmark comprises 438 manually curated videos spanning multiple\ntypes, yielding 1,177 high-quality multiple-choice questions and 120 open-ended\nquestions, validated for their effectiveness in exposing positional bias. Based\non these, we evaluate 27 state-of-the-art LVLMs, including both commercial and\nopen-source models. Our findings reveal significant positional biases in many\nleading open-source models, typically exhibiting head or neighbor-content\npreferences. In contrast, commercial models such as Gemini2.5-Pro show\nimpressive, consistent performance across entire video sequences. Further\nanalyses on context length, context variation, and model scale provide\nactionable insights for mitigating bias and guiding model enhancement.", "AI": {"tldr": "Video-LevelGauge is a benchmark that systematically evaluates positional bias in large video language models (LVLMs) across different context positions, revealing significant biases in open-source models while commercial models show consistent performance.", "motivation": "Existing video understanding benchmarks assess overall performance but overlook nuanced behaviors like contextual positional bias, which is critical for understanding LVLM limitations and guiding improvements.", "method": "Created a benchmark with 438 curated videos, 1,177 multiple-choice and 120 open-ended questions using standardized probes and contextual setups with flexible control over context length, position, and types. Employed statistical measures and morphological pattern recognition for bias analysis.", "result": "Evaluation of 27 state-of-the-art LVLMs revealed significant positional biases in many open-source models (typically head or neighbor-content preferences), while commercial models like Gemini2.5-Pro showed consistent performance across entire sequences.", "conclusion": "The benchmark effectively exposes positional bias in LVLMs, providing actionable insights for bias mitigation and model enhancement through analyses of context length, variation, and model scale."}}
{"id": "2508.19856", "pdf": "https://arxiv.org/pdf/2508.19856", "abs": "https://arxiv.org/abs/2508.19856", "authors": ["Shashi Kumar", "Srikanth Madikeri", "Esa\u00fa Villatoro-Tello", "Sergio Burdisso", "Pradeep Rangappa", "Andr\u00e9s Carofilis", "Petr Motlicek", "Karthik Pandia", "Shankar Venkatesan", "Kadri Hacio\u011flu", "Andreas Stolcke"], "title": "TokenVerse++: Towards Flexible Multitask Learning with Dynamic Task Activation", "categories": ["cs.CL", "eess.AS"], "comment": "Accepted to IEEE ASRU 2025. Copyright\\copyright 2025 IEEE", "summary": "Token-based multitasking frameworks like TokenVerse require all training\nutterances to have labels for all tasks, hindering their ability to leverage\npartially annotated datasets and scale effectively. We propose TokenVerse++,\nwhich introduces learnable vectors in the acoustic embedding space of the\nXLSR-Transducer ASR model for dynamic task activation. This core mechanism\nenables training with utterances labeled for only a subset of tasks, a key\nadvantage over TokenVerse. We demonstrate this by successfully integrating a\ndataset with partial labels, specifically for ASR and an additional task,\nlanguage identification, improving overall performance. TokenVerse++ achieves\nresults on par with or exceeding TokenVerse across multiple tasks, establishing\nit as a more practical multitask alternative without sacrificing ASR\nperformance.", "AI": {"tldr": "TokenVerse++ improves upon TokenVerse by enabling training with partially annotated datasets through learnable vectors for dynamic task activation, achieving comparable or better performance across multiple tasks without sacrificing ASR quality.", "motivation": "Token-based multitasking frameworks like TokenVerse require complete labels for all tasks on every training utterance, which limits their ability to leverage partially annotated datasets and scale effectively.", "method": "Introduces learnable vectors in the acoustic embedding space of the XLSR-Transducer ASR model for dynamic task activation, allowing training with utterances labeled for only a subset of tasks.", "result": "Successfully integrates datasets with partial labels (ASR + language identification), improves overall performance, and achieves results on par with or exceeding TokenVerse across multiple tasks.", "conclusion": "TokenVerse++ establishes itself as a more practical multitask alternative that can effectively utilize partially annotated data without compromising ASR performance."}}
{"id": "2508.19651", "pdf": "https://arxiv.org/pdf/2508.19651", "abs": "https://arxiv.org/abs/2508.19651", "authors": ["B\u00e1lint M\u00e9sz\u00e1ros", "Ahmet Firintepe", "Sebastian Schmidt", "Stephan G\u00fcnnemann"], "title": "Scalable Object Detection in the Car Interior With Vision Foundation Models", "categories": ["cs.CV"], "comment": null, "summary": "AI tasks in the car interior like identifying and localizing externally\nintroduced objects is crucial for response quality of personal assistants.\nHowever, computational resources of on-board systems remain highly constrained,\nrestricting the deployment of such solutions directly within the vehicle. To\naddress this limitation, we propose the novel Object Detection and Localization\n(ODAL) framework for interior scene understanding. Our approach leverages\nvision foundation models through a distributed architecture, splitting\ncomputational tasks between on-board and cloud. This design overcomes the\nresource constraints of running foundation models directly in the car. To\nbenchmark model performance, we introduce ODALbench, a new metric for\ncomprehensive assessment of detection and localization.Our analysis\ndemonstrates the framework's potential to establish new standards in this\ndomain. We compare the state-of-the-art GPT-4o vision foundation model with the\nlightweight LLaVA 1.5 7B model and explore how fine-tuning enhances the\nlightweight models performance. Remarkably, our fine-tuned ODAL-LLaVA model\nachieves an ODAL$_{score}$ of 89%, representing a 71% improvement over its\nbaseline performance and outperforming GPT-4o by nearly 20%. Furthermore, the\nfine-tuned model maintains high detection accuracy while significantly reducing\nhallucinations, achieving an ODAL$_{SNR}$ three times higher than GPT-4o.", "AI": {"tldr": "ODAL framework uses distributed vision foundation models between car and cloud to overcome onboard computational constraints, achieving 89% ODAL score with fine-tuned LLaVA model that outperforms GPT-4o by 20%.", "motivation": "AI tasks like object detection in car interiors are crucial for personal assistants but face computational constraints in onboard systems, limiting direct deployment.", "method": "Proposes ODAL framework with distributed architecture splitting computational tasks between on-board and cloud systems, leveraging vision foundation models. Introduces ODALbench metric for comprehensive assessment. Compares GPT-4o with lightweight LLaVA 1.5 7B model and explores fine-tuning.", "result": "Fine-tuned ODAL-LLaVA achieves 89% ODAL score (71% improvement over baseline), outperforms GPT-4o by nearly 20%. Maintains high detection accuracy while reducing hallucinations, achieving ODAL_SNR three times higher than GPT-4o.", "conclusion": "The framework demonstrates potential to establish new standards in interior scene understanding for automotive applications, overcoming computational constraints through distributed architecture and fine-tuning lightweight models."}}
{"id": "2508.19873", "pdf": "https://arxiv.org/pdf/2508.19873", "abs": "https://arxiv.org/abs/2508.19873", "authors": ["Vanessa Toborek", "Sebastian M\u00fcller", "Tim Selbach", "Tam\u00e1s Horv\u00e1th", "Christian Bauckhage"], "title": "Beyond Shallow Heuristics: Leveraging Human Intuition for Curriculum Learning", "categories": ["cs.CL"], "comment": "Presented at ICNLSP 2025; to appear in the ACL Anthology; received\n  the Best Short Paper Award", "summary": "Curriculum learning (CL) aims to improve training by presenting data from\n\"easy\" to \"hard\", yet defining and measuring linguistic difficulty remains an\nopen challenge. We investigate whether human-curated simple language can serve\nas an effective signal for CL. Using the article-level labels from the Simple\nWikipedia corpus, we compare label-based curricula to competence-based\nstrategies relying on shallow heuristics. Our experiments with a BERT-tiny\nmodel show that adding simple data alone yields no clear benefit. However,\nstructuring it via a curriculum -- especially when introduced first --\nconsistently improves perplexity, particularly on simple language. In contrast,\ncompetence-based curricula lead to no consistent gains over random ordering,\nprobably because they fail to effectively separate the two classes. Our results\nsuggest that human intuition about linguistic difficulty can guide CL for\nlanguage model pre-training.", "AI": {"tldr": "Human-curated simple language from Simple Wikipedia can effectively guide curriculum learning when introduced first, improving perplexity on simple language, while competence-based strategies show no consistent benefits.", "motivation": "To investigate whether human-curated simple language can serve as an effective signal for curriculum learning, addressing the challenge of defining and measuring linguistic difficulty in CL approaches.", "method": "Using article-level labels from Simple Wikipedia corpus, compared label-based curricula to competence-based strategies with shallow heuristics. Experiments conducted with BERT-tiny model.", "result": "Adding simple data alone showed no clear benefit, but structuring it via curriculum (especially when introduced first) consistently improved perplexity, particularly on simple language. Competence-based curricula led to no consistent gains over random ordering.", "conclusion": "Human intuition about linguistic difficulty can effectively guide curriculum learning for language model pre-training, with simple-first curricula outperforming competence-based approaches."}}
{"id": "2508.19652", "pdf": "https://arxiv.org/pdf/2508.19652", "abs": "https://arxiv.org/abs/2508.19652", "authors": ["Zongxia Li", "Wenhao Yu", "Chengsong Huang", "Rui Liu", "Zhenwen Liang", "Fuxiao Liu", "Jingxi Che", "Dian Yu", "Jordan Boyd-Graber", "Haitao Mi", "Dong Yu"], "title": "Self-Rewarding Vision-Language Model via Reasoning Decomposition", "categories": ["cs.CV"], "comment": "16 pages, two figures", "summary": "Vision-Language Models (VLMs) often suffer from visual hallucinations, saying\nthings that are not actually in the image, and language shortcuts, where they\nskip the visual part and just rely on text priors. These issues arise because\nmost post-training methods for VLMs rely on simple verifiable answer matching\nand supervise only final outputs, leaving intermediate visual reasoning without\nexplicit guidance. As a result, VLMs receive sparse visual signals and often\nlearn to prioritize language-based reasoning over visual perception. To\nmitigate this, some existing methods add visual supervision using human\nannotations or distilled labels from external large models. However, human\nannotations are labor-intensive and costly, and because external signals cannot\nadapt to the evolving policy, they cause distributional shifts that can lead to\nreward hacking. In this paper, we introduce Vision-SR1, a self-rewarding method\nthat improves visual reasoning without relying on external visual supervisions\nvia reinforcement learning. Vision-SR1 decomposes VLM reasoning into two\nstages: visual perception and language reasoning. The model is first prompted\nto produce self-contained visual perceptions that are sufficient to answer the\nquestion without referring back the input image. To validate this\nself-containment, the same VLM model is then re-prompted to perform language\nreasoning using only the generated perception as input to compute reward. This\nself-reward is combined with supervision on final outputs, providing a balanced\ntraining signal that strengthens both visual perception and language reasoning.\nOur experiments demonstrate that Vision-SR1 improves visual reasoning,\nmitigates visual hallucinations, and reduces reliance on language shortcuts\nacross diverse vision-language tasks.", "AI": {"tldr": "Vision-SR1 is a self-rewarding reinforcement learning method that improves visual reasoning in VLMs by decomposing reasoning into visual perception and language stages, using the model's own outputs to compute rewards without external supervision.", "motivation": "VLMs suffer from visual hallucinations and language shortcuts due to sparse visual signals and over-reliance on text priors. Existing methods using human annotations or external models are costly and cause distributional shifts.", "method": "Decomposes VLM reasoning into visual perception and language reasoning stages. The model generates self-contained visual perceptions, then validates them by re-prompting the same model to perform reasoning using only the generated perception. This self-reward is combined with final output supervision.", "result": "Improves visual reasoning, mitigates visual hallucinations, and reduces reliance on language shortcuts across diverse vision-language tasks.", "conclusion": "Vision-SR1 provides an effective self-supervised approach to enhance visual reasoning in VLMs without external visual supervision, addressing core limitations of current methods."}}
{"id": "2508.19883", "pdf": "https://arxiv.org/pdf/2508.19883", "abs": "https://arxiv.org/abs/2508.19883", "authors": ["Chiman Salavati", "Shannon Song", "Scott A. Hale", "Roberto E. Montenegro", "Shiri Dori-Hacohen", "Fabricio Murai"], "title": "AI-Powered Detection of Inappropriate Language in Medical School Curricula", "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.1; I.2.7"], "comment": "Accepted at 2025 AAAI/ACM AI, Ethics and Society Conference (AIES'25)", "summary": "The use of inappropriate language -- such as outdated, exclusionary, or\nnon-patient-centered terms -- medical instructional materials can significantly\ninfluence clinical training, patient interactions, and health outcomes. Despite\ntheir reputability, many materials developed over past decades contain examples\nnow considered inappropriate by current medical standards. Given the volume of\ncurricular content, manually identifying instances of inappropriate use of\nlanguage (IUL) and its subcategories for systematic review is prohibitively\ncostly and impractical. To address this challenge, we conduct a first-in-class\nevaluation of small language models (SLMs) fine-tuned on labeled data and\npre-trained LLMs with in-context learning on a dataset containing approximately\n500 documents and over 12,000 pages. For SLMs, we consider: (1) a general IUL\nclassifier, (2) subcategory-specific binary classifiers, (3) a multilabel\nclassifier, and (4) a two-stage hierarchical pipeline for general IUL detection\nfollowed by multilabel classification. For LLMs, we consider variations of\nprompts that include subcategory definitions and/or shots. We found that both\nLLama-3 8B and 70B, even with carefully curated shots, are largely outperformed\nby SLMs. While the multilabel classifier performs best on annotated data,\nsupplementing training with unflagged excerpts as negative examples boosts the\nspecific classifiers' AUC by up to 25%, making them most effective models for\nmitigating harmful language in medical curricula.", "AI": {"tldr": "Small language models outperform large language models in detecting inappropriate language in medical instructional materials, with multilabel classifiers and negative example training showing best results.", "motivation": "Medical instructional materials often contain outdated, exclusionary, or non-patient-centered language that can negatively impact clinical training and patient outcomes, but manual review is impractical due to volume.", "method": "Evaluated fine-tuned small language models (general classifier, binary classifiers, multilabel classifier, hierarchical pipeline) and pre-trained LLMs with in-context learning on 500 documents (12,000+ pages) using various prompt strategies.", "result": "SLMs significantly outperformed LLama-3 8B/70B models. Multilabel classifier performed best on annotated data, while supplementing training with negative examples boosted binary classifiers' AUC by up to 25%.", "conclusion": "Fine-tuned small language models, particularly those trained with negative examples, are most effective for automated detection and mitigation of harmful language in medical curricula."}}
{"id": "2508.19654", "pdf": "https://arxiv.org/pdf/2508.19654", "abs": "https://arxiv.org/abs/2508.19654", "authors": ["Matthias H\u00f6fflin", "J\u00fcrgen Wassner"], "title": "Hardware-aware vs. Hardware-agnostic Energy Estimation for SNN in Space Applications", "categories": ["cs.CV"], "comment": "Accepted for the IAA-SPAICE 2025 conference", "summary": "Spiking Neural Networks (SNNs), inspired by biological intelligence, have\nlong been considered inherently energy-efficient, making them attractive for\nresource-constrained domains such as space applications. However, recent\ncomparative studies with conventional Artificial Neural Networks (ANNs) have\nbegun to question this reputation, especially for digital implementations. This\nwork investigates SNNs for multi-output regression, specifically 3-D satellite\nposition estimation from monocular images, and compares hardware-aware and\nhardware-agnostic energy estimation methods. The proposed SNN, trained using\nthe membrane potential of the Leaky Integrate-and-Fire (LIF) neuron in the\nfinal layer, achieves comparable Mean Squared Error (MSE) to a reference\nConvolutional Neural Network (CNN) on a photorealistic satellite dataset.\nEnergy analysis shows that while hardware-agnostic methods predict a consistent\n50-60% energy advantage for SNNs over CNNs, hardware-aware analysis reveals\nthat significant energy savings are realized only on neuromorphic hardware and\nwith high input sparsity. The influence of dark pixel ratio on energy\nconsumption is quantified, emphasizing the impact of data characteristics and\nhardware assumptions. These findings highlight the need for transparent\nevaluation methods and explicit disclosure of underlying assumptions to ensure\nfair comparisons of neural network energy efficiency.", "AI": {"tldr": "SNNs show 50-60% energy advantage over CNNs in hardware-agnostic analysis, but significant savings only occur on neuromorphic hardware with high input sparsity. Hardware-aware analysis reveals data characteristics and hardware assumptions critically impact energy efficiency comparisons.", "motivation": "To investigate whether Spiking Neural Networks (SNNs) truly provide energy efficiency advantages over conventional ANNs for satellite position estimation, especially given recent studies questioning SNNs' reputation for energy efficiency in digital implementations.", "method": "Developed an SNN for 3-D satellite position estimation from monocular images using membrane potential of LIF neurons in final layer. Compared with reference CNN on photorealistic dataset. Used both hardware-aware and hardware-agnostic energy estimation methods, analyzing influence of dark pixel ratio and input sparsity.", "result": "SNN achieved comparable MSE to CNN. Hardware-agnostic methods predicted consistent 50-60% energy advantage for SNNs, but hardware-aware analysis showed significant energy savings only on neuromorphic hardware with high input sparsity. Dark pixel ratio significantly influenced energy consumption.", "conclusion": "Energy efficiency comparisons between SNNs and ANNs require transparent evaluation methods and explicit disclosure of underlying hardware and data assumptions. Significant energy savings for SNNs are hardware-dependent and require specific conditions like neuromorphic hardware and high input sparsity."}}
{"id": "2508.19887", "pdf": "https://arxiv.org/pdf/2508.19887", "abs": "https://arxiv.org/abs/2508.19887", "authors": ["Mohammed Rakibul Hasan", "Rafi Majid", "Ahanaf Tahmid"], "title": "Bangla-Bayanno: A 52K-Pair Bengali Visual Question Answering Dataset with LLM-Assisted Translation Refinement", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "In this paper, we introduce Bangla-Bayanno, an open-ended Visual Question\nAnswering (VQA) Dataset in Bangla, a widely used, low-resource language in\nmultimodal AI research. The majority of existing datasets are either manually\nannotated with an emphasis on a specific domain, query type, or answer type or\nare constrained by niche answer formats. In order to mitigate human-induced\nerrors and guarantee lucidity, we implemented a multilingual LLM-assisted\ntranslation refinement pipeline. This dataset overcomes the issues of\nlow-quality translations from multilingual sources. The dataset comprises\n52,650 question-answer pairs across 4750+ images. Questions are classified into\nthree distinct answer types: nominal (short descriptive), quantitative\n(numeric), and polar (yes/no). Bangla-Bayanno provides the most comprehensive\nopen-source, high-quality VQA benchmark in Bangla, aiming to advance research\nin low-resource multimodal learning and facilitate the development of more\ninclusive AI systems.", "AI": {"tldr": "Bangla-Bayanno is a new open-ended Visual Question Answering dataset for Bangla language with 52,650 QA pairs across 4,750+ images, created using multilingual LLM-assisted translation to ensure high quality.", "motivation": "To address the lack of comprehensive, high-quality VQA datasets for Bangla (a low-resource language) and overcome issues with existing datasets that are either domain-specific, manually annotated with biases, or have poor translation quality.", "method": "Implemented a multilingual LLM-assisted translation refinement pipeline to mitigate human-induced errors and ensure translation quality. Questions are classified into three answer types: nominal (descriptive), quantitative (numeric), and polar (yes/no).", "result": "Created a dataset of 52,650 question-answer pairs across 4,750+ images with high-quality translations, providing the most comprehensive open-source VQA benchmark in Bangla.", "conclusion": "Bangla-Bayanno serves as a high-quality benchmark to advance research in low-resource multimodal learning and facilitate development of more inclusive AI systems for underrepresented languages."}}
{"id": "2508.19664", "pdf": "https://arxiv.org/pdf/2508.19664", "abs": "https://arxiv.org/abs/2508.19664", "authors": ["Weicheng Liao", "Zan Chen", "Jianyang Xie", "Yalin Zheng", "Yuhui Ma", "Yitian Zhao"], "title": "A Frequency-Aware Self-Supervised Learning for Ultra-Wide-Field Image Enhancement", "categories": ["cs.CV"], "comment": null, "summary": "Ultra-Wide-Field (UWF) retinal imaging has revolutionized retinal diagnostics\nby providing a comprehensive view of the retina. However, it often suffers from\nquality-degrading factors such as blurring and uneven illumination, which\nobscure fine details and mask pathological information. While numerous retinal\nimage enhancement methods have been proposed for other fundus imageries, they\noften fail to address the unique requirements in UWF, particularly the need to\npreserve pathological details. In this paper, we propose a novel\nfrequency-aware self-supervised learning method for UWF image enhancement. It\nincorporates frequency-decoupled image deblurring and Retinex-guided\nillumination compensation modules. An asymmetric channel integration operation\nis introduced in the former module, so as to combine global and local views by\nleveraging high- and low-frequency information, ensuring the preservation of\nfine and broader structural details. In addition, a color preservation unit is\nproposed in the latter Retinex-based module, to provide multi-scale spatial and\nfrequency information, enabling accurate illumination estimation and\ncorrection. Experimental results demonstrate that the proposed work not only\nenhances visualization quality but also improves disease diagnosis performance\nby restoring and correcting fine local details and uneven intensity. To the\nbest of our knowledge, this work is the first attempt for UWF image\nenhancement, offering a robust and clinically valuable tool for improving\nretinal disease management.", "AI": {"tldr": "A novel frequency-aware self-supervised learning method for Ultra-Wide-Field retinal image enhancement that addresses blurring and uneven illumination while preserving pathological details.", "motivation": "UWF retinal imaging suffers from quality-degrading factors like blurring and uneven illumination that obscure fine details and mask pathological information. Existing methods fail to address UWF's unique requirements for preserving pathological details.", "method": "Frequency-aware self-supervised learning with frequency-decoupled image deblurring and Retinex-guided illumination compensation modules. Includes asymmetric channel integration for combining global/local views and color preservation unit for multi-scale spatial/frequency information.", "result": "Experimental results show enhanced visualization quality and improved disease diagnosis performance by restoring fine local details and correcting uneven intensity.", "conclusion": "This is the first attempt for UWF image enhancement, offering a robust and clinically valuable tool for improving retinal disease management."}}
{"id": "2508.19903", "pdf": "https://arxiv.org/pdf/2508.19903", "abs": "https://arxiv.org/abs/2508.19903", "authors": ["Ramya Keerthy Thatikonda", "Wray Buntine", "Ehsan Shareghi"], "title": "Logical Reasoning with Outcome Reward Models for Test-Time Scaling", "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025", "summary": "Logical reasoning is a critical benchmark for evaluating the capabilities of\nlarge language models (LLMs), as it reflects their ability to derive valid\nconclusions from given premises. While the combination of test-time scaling\nwith dedicated outcome or process reward models has opened up new avenues to\nenhance LLMs performance in complex reasoning tasks, this space is\nunder-explored in deductive logical reasoning. We present a set of Outcome\nReward Models (ORMs) for deductive reasoning. To train the ORMs we mainly\ngenerate data using Chain-of-Thought (CoT) with single and multiple samples.\nAdditionally, we propose a novel tactic to further expand the type of errors\ncovered in the training dataset of the ORM. In particular, we propose an echo\ngeneration technique that leverages LLMs' tendency to reflect incorrect\nassumptions made in prompts to extract additional training data, covering\npreviously unexplored error types. While a standard CoT chain may contain\nerrors likely to be made by the reasoner, the echo strategy deliberately steers\nthe model toward incorrect reasoning. We show that ORMs trained on CoT and\necho-augmented data demonstrate improved performance on the FOLIO, JustLogic,\nand ProverQA datasets across four different LLMs.", "AI": {"tldr": "This paper introduces Outcome Reward Models (ORMs) for deductive logical reasoning, using Chain-of-Thought data generation and a novel echo generation technique to cover more error types, showing improved performance across multiple datasets and LLMs.", "motivation": "Logical reasoning is a critical benchmark for LLMs, but test-time scaling with reward models is under-explored in deductive reasoning. The paper aims to enhance LLM performance in complex reasoning tasks through specialized reward models.", "method": "Developed Outcome Reward Models (ORMs) trained on data generated using Chain-of-Thought (single and multiple samples) and a novel echo generation technique that leverages LLMs' tendency to reflect incorrect assumptions to extract additional training data covering previously unexplored error types.", "result": "ORMs trained on CoT and echo-augmented data demonstrated improved performance on FOLIO, JustLogic, and ProverQA datasets across four different large language models.", "conclusion": "The proposed ORMs with echo generation technique effectively enhance deductive logical reasoning capabilities in LLMs by covering more error types and improving performance across multiple benchmark datasets."}}
{"id": "2508.19688", "pdf": "https://arxiv.org/pdf/2508.19688", "abs": "https://arxiv.org/abs/2508.19688", "authors": ["Gangjian Zhang", "Jian Shu", "Nanjie Yao", "Hao Wang"], "title": "SAT: Supervisor Regularization and Animation Augmentation for Two-process Monocular Texture 3D Human Reconstruction", "categories": ["cs.CV"], "comment": "10 pages, 8 figures", "summary": "Monocular texture 3D human reconstruction aims to create a complete 3D\ndigital avatar from just a single front-view human RGB image. However, the\ngeometric ambiguity inherent in a single 2D image and the scarcity of 3D human\ntraining data are the main obstacles limiting progress in this field. To\naddress these issues, current methods employ prior geometric estimation\nnetworks to derive various human geometric forms, such as the SMPL model and\nnormal maps. However, they struggle to integrate these modalities effectively,\nleading to view inconsistencies, such as facial distortions. To this end, we\npropose a two-process 3D human reconstruction framework, SAT, which seamlessly\nlearns various prior geometries in a unified manner and reconstructs\nhigh-quality textured 3D avatars as the final output. To further facilitate\ngeometry learning, we introduce a Supervisor Feature Regularization module. By\nemploying a multi-view network with the same structure to provide intermediate\nfeatures as training supervision, these varied geometric priors can be better\nfused. To tackle data scarcity and further improve reconstruction quality, we\nalso propose an Online Animation Augmentation module. By building a\none-feed-forward animation network, we augment a massive number of samples from\nthe original 3D human data online for model training. Extensive experiments on\ntwo benchmarks show the superiority of our approach compared to\nstate-of-the-art methods.", "AI": {"tldr": "SAT framework for monocular 3D human reconstruction that integrates multiple geometric priors through a two-process approach with Supervisor Feature Regularization and Online Animation Augmentation to address view inconsistencies and data scarcity.", "motivation": "Current methods struggle with geometric ambiguity from single 2D images and limited 3D training data, leading to view inconsistencies like facial distortions when integrating different geometric priors (SMPL model, normal maps).", "method": "Two-process framework with Supervisor Feature Regularization (uses multi-view network features as supervision) and Online Animation Augmentation (creates massive training samples via animation network) to unify geometric priors and handle data scarcity.", "result": "Extensive experiments on two benchmarks demonstrate superiority over state-of-the-art methods in reconstructing high-quality textured 3D avatars from single front-view images.", "conclusion": "SAT effectively addresses geometric ambiguity and data scarcity issues in monocular 3D human reconstruction by seamlessly integrating multiple geometric priors and augmenting training data, producing high-quality avatars without view inconsistencies."}}
{"id": "2508.19919", "pdf": "https://arxiv.org/pdf/2508.19919", "abs": "https://arxiv.org/abs/2508.19919", "authors": ["Jingyu Guo", "Yingying Xu"], "title": "Your AI Bosses Are Still Prejudiced: The Emergence of Stereotypes in LLM-Based Multi-Agent Systems", "categories": ["cs.CL"], "comment": null, "summary": "While stereotypes are well-documented in human social interactions, AI\nsystems are often presumed to be less susceptible to such biases. Previous\nstudies have focused on biases inherited from training data, but whether\nstereotypes can emerge spontaneously in AI agent interactions merits further\nexploration. Through a novel experimental framework simulating workplace\ninteractions with neutral initial conditions, we investigate the emergence and\nevolution of stereotypes in LLM-based multi-agent systems. Our findings reveal\nthat (1) LLM-Based AI agents develop stereotype-driven biases in their\ninteractions despite beginning without predefined biases; (2) stereotype\neffects intensify with increased interaction rounds and decision-making power,\nparticularly after introducing hierarchical structures; (3) these systems\nexhibit group effects analogous to human social behavior, including halo\neffects, confirmation bias, and role congruity; and (4) these stereotype\npatterns manifest consistently across different LLM architectures. Through\ncomprehensive quantitative analysis, these findings suggest that stereotype\nformation in AI systems may arise as an emergent property of multi-agent\ninteractions, rather than merely from training data biases. Our work\nunderscores the need for future research to explore the underlying mechanisms\nof this phenomenon and develop strategies to mitigate its ethical impacts.", "AI": {"tldr": "LLM-based AI agents spontaneously develop stereotype biases through multi-agent interactions, even without predefined biases, with effects intensifying through hierarchical structures and increased interactions.", "motivation": "To investigate whether stereotypes can emerge spontaneously in AI agent interactions beyond biases inherited from training data, challenging the presumption that AI systems are less susceptible to such biases.", "method": "Novel experimental framework simulating workplace interactions with neutral initial conditions, using LLM-based multi-agent systems with comprehensive quantitative analysis across different LLM architectures.", "result": "AI agents developed stereotype-driven biases despite no initial biases, with effects intensifying through increased interactions and hierarchical structures, showing group effects analogous to human social behavior (halo effects, confirmation bias, role congruity) consistently across different LLM architectures.", "conclusion": "Stereotype formation in AI systems may be an emergent property of multi-agent interactions rather than just training data biases, highlighting the need for research on underlying mechanisms and mitigation strategies for ethical impacts."}}
{"id": "2508.19698", "pdf": "https://arxiv.org/pdf/2508.19698", "abs": "https://arxiv.org/abs/2508.19698", "authors": ["V. S. Usatyuk", "D. A. Sapozhnikov", "S. I. Egorov"], "title": "Synthetic Image Detection via Spectral Gaps of QC-RBIM Nishimori Bethe-Hessian Operators", "categories": ["cs.CV", "cs.IT", "math.IT", "math.SP"], "comment": "14 pages, 10 figures", "summary": "The rapid advance of deep generative models such as GANs and diffusion\nnetworks now produces images that are virtually indistinguishable from genuine\nphotographs, undermining media forensics and biometric security. Supervised\ndetectors quickly lose effectiveness on unseen generators or after adversarial\npost-processing, while existing unsupervised methods that rely on low-level\nstatistical cues remain fragile. We introduce a physics-inspired,\nmodel-agnostic detector that treats synthetic-image identification as a\ncommunity-detection problem on a sparse weighted graph. Image features are\nfirst extracted with pretrained CNNs and reduced to 32 dimensions, each feature\nvector becomes a node of a Multi-Edge Type QC-LDPC graph. Pairwise similarities\nare transformed into edge couplings calibrated at the Nishimori temperature,\nproducing a Random Bond Ising Model (RBIM) whose Bethe-Hessian spectrum\nexhibits a characteristic gap when genuine community structure (real images) is\npresent. Synthetic images violate the Nishimori symmetry and therefore lack\nsuch gaps. We validate the approach on binary tasks cat versus dog and male\nversus female using real photos from Flickr-Faces-HQ and CelebA and synthetic\ncounterparts generated by GANs and diffusion models. Without any labeled\nsynthetic data or retraining of the feature extractor, the detector achieves\nover 94% accuracy. Spectral analysis shows multiple well separated gaps for\nreal image sets and a collapsed spectrum for generated ones. Our contributions\nare threefold: a novel LDPC graph construction that embeds deep image features,\nan analytical link between Nishimori temperature RBIM and the Bethe-Hessian\nspectrum providing a Bayes optimal detection criterion; and a practical,\nunsupervised synthetic image detector robust to new generative architectures.\nFuture work will extend the framework to video streams and multi-class anomaly\ndetection.", "AI": {"tldr": "Physics-inspired unsupervised detector using graph theory and statistical physics to distinguish real from synthetic images without labeled training data, achieving 94% accuracy.", "motivation": "Deep generative models create highly realistic synthetic images that undermine media forensics and biometric security, while existing detectors fail on unseen generators or adversarial processing.", "method": "Treats synthetic-image detection as community detection on sparse weighted graphs. Uses pretrained CNNs for feature extraction, creates Multi-Edge Type QC-LDPC graphs, transforms similarities into edge couplings at Nishimori temperature, and analyzes Bethe-Hessian spectrum gaps.", "result": "Achieves over 94% accuracy on binary classification tasks (cat vs dog, male vs female) using real photos from FFHQ/CelebA and synthetic counterparts from GANs/diffusion models, without labeled synthetic data or retraining.", "conclusion": "Provides a robust, model-agnostic detector that works across different generative architectures through spectral analysis of community structure, with potential extensions to video and multi-class anomaly detection."}}
{"id": "2508.19922", "pdf": "https://arxiv.org/pdf/2508.19922", "abs": "https://arxiv.org/abs/2508.19922", "authors": ["Yifu Huo", "Chenglong Wang", "Qiren Zhu", "Shunjie Xing", "Tong Xiao", "Chunliang Zhang", "Tongran Liu", "Jinbo Zhu"], "title": "HEAL: A Hypothesis-Based Preference-Aware Analysis Framework", "categories": ["cs.CL"], "comment": "Accepted by EMNLP 2025 Findings", "summary": "Preference optimization methods like DPO have achieved remarkable performance\nin LLM alignment. However, the evaluation for these methods relies on a single\nresponse and overlooks other potential outputs, which could also be generated\nin real-world applications within this hypothetical space. To address this\nissue, this paper presents a \\textbf{H}ypothesis-based\nPr\\textbf{E}ference-aware \\textbf{A}na\\textbf{L}ysis Framework (HEAL), a novel\nevaluation paradigm that formulates preference alignment as a re-ranking\nprocess within hypothesis spaces. The framework incorporates two complementary\nmetrics: ranking accuracy for evaluating ordinal consistency and preference\nstrength correlation for assessing continuous alignment. To facilitate this\nframework, we develop UniHypoBench, a unified hypothesis benchmark constructed\nfrom diverse instruction-response pairs. Through extensive experiments based on\nHEAL, with a particular focus on the intrinsic mechanisms of preference\nlearning, we demonstrate that current preference learning methods can\neffectively capture preferences provided by proxy models while simultaneously\nsuppressing negative samples. These findings contribute to preference learning\nresearch through two significant avenues. Theoretically, we introduce\nhypothesis space analysis as an innovative paradigm for understanding\npreference alignment. Practically, HEAL offers researchers robust diagnostic\ntools for refining preference optimization methods, while our empirical results\nidentify promising directions for developing more advanced alignment algorithms\ncapable of comprehensive preference capture.", "AI": {"tldr": "HEAL framework evaluates preference optimization methods by analyzing ranking consistency and preference strength across multiple potential responses in hypothesis spaces, rather than just single outputs.", "motivation": "Current evaluation methods for preference optimization like DPO only assess single responses, ignoring other potential outputs that could be generated in real applications, creating an incomplete evaluation picture.", "method": "Proposes HEAL framework that treats preference alignment as a re-ranking process within hypothesis spaces, using two metrics: ranking accuracy for ordinal consistency and preference strength correlation for continuous alignment. Also develops UniHypoBench benchmark from diverse instruction-response pairs.", "result": "Experiments show current preference learning methods effectively capture proxy model preferences while suppressing negative samples. The framework provides robust diagnostic tools and identifies directions for advanced alignment algorithms.", "conclusion": "HEAL introduces hypothesis space analysis as a novel paradigm for understanding preference alignment, offering both theoretical innovation and practical diagnostic tools for refining preference optimization methods."}}
{"id": "2508.19699", "pdf": "https://arxiv.org/pdf/2508.19699", "abs": "https://arxiv.org/abs/2508.19699", "authors": ["Yupeng Zhang", "Dezhi Zheng", "Ping Lu", "Han Zhang", "Lei Wang", "Liping xiang", "Cheng Luo", "Kaijun Deng", "Xiaowen Fu", "Linlin Shen", "Jinbao Wang"], "title": "LabelGS: Label-Aware 3D Gaussian Splatting for 3D Scene Segmentation", "categories": ["cs.CV"], "comment": "PRCV 2025", "summary": "3D Gaussian Splatting (3DGS) has emerged as a novel explicit representation\nfor 3D scenes, offering both high-fidelity reconstruction and efficient\nrendering. However, 3DGS lacks 3D segmentation ability, which limits its\napplicability in tasks that require scene understanding. The identification and\nisolating of specific object components is crucial. To address this limitation,\nwe propose Label-aware 3D Gaussian Splatting (LabelGS), a method that augments\nthe Gaussian representation with object label.LabelGS introduces cross-view\nconsistent semantic masks for 3D Gaussians and employs a novel Occlusion\nAnalysis Model to avoid overfitting occlusion during optimization, Main\nGaussian Labeling model to lift 2D semantic prior to 3D Gaussian and Gaussian\nProjection Filter to avoid Gaussian label conflict. Our approach achieves\neffective decoupling of Gaussian representations and refines the 3DGS\noptimization process through a random region sampling strategy, significantly\nimproving efficiency. Extensive experiments demonstrate that LabelGS\noutperforms previous state-of-the-art methods, including Feature-3DGS, in the\n3D scene segmentation task. Notably, LabelGS achieves a remarkable 22X speedup\nin training compared to Feature-3DGS, at a resolution of 1440X1080. Our code\nwill be at https://github.com/garrisonz/LabelGS.", "AI": {"tldr": "LabelGS enhances 3D Gaussian Splatting with object segmentation capabilities by adding semantic labels to Gaussians and addressing occlusion issues, achieving 22x faster training than previous methods.", "motivation": "3D Gaussian Splatting lacks 3D segmentation ability, limiting its applicability in scene understanding tasks that require identifying and isolating specific object components.", "method": "Proposes LabelGS which introduces cross-view consistent semantic masks for 3D Gaussians, employs Occlusion Analysis Model to avoid overfitting, Main Gaussian Labeling model to lift 2D semantic prior to 3D, and Gaussian Projection Filter to avoid label conflicts. Uses random region sampling strategy for efficient optimization.", "result": "Outperforms previous state-of-the-art methods including Feature-3DGS in 3D scene segmentation, achieving 22x speedup in training at 1440x1080 resolution.", "conclusion": "LabelGS effectively adds segmentation capabilities to 3DGS while significantly improving training efficiency, making it suitable for scene understanding applications."}}
{"id": "2508.19966", "pdf": "https://arxiv.org/pdf/2508.19966", "abs": "https://arxiv.org/abs/2508.19966", "authors": ["Slimane Bellaouar", "Attia Nehar", "Soumia Souffi", "Mounia Bouameur"], "title": "Dhati+: Fine-tuned Large Language Models for Arabic Subjectivity Evaluation", "categories": ["cs.CL", "cs.AI"], "comment": "25 pages, 7 figures", "summary": "Despite its significance, Arabic, a linguistically rich and morphologically\ncomplex language, faces the challenge of being under-resourced. The scarcity of\nlarge annotated datasets hampers the development of accurate tools for\nsubjectivity analysis in Arabic. Recent advances in deep learning and\nTransformers have proven highly effective for text classification in English\nand French. This paper proposes a new approach for subjectivity assessment in\nArabic textual data. To address the dearth of specialized annotated datasets,\nwe developed a comprehensive dataset, AraDhati+, by leveraging existing Arabic\ndatasets and collections (ASTD, LABR, HARD, and SANAD). Subsequently, we\nfine-tuned state-of-the-art Arabic language models (XLM-RoBERTa, AraBERT, and\nArabianGPT) on AraDhati+ for effective subjectivity classification.\nFurthermore, we experimented with an ensemble decision approach to harness the\nstrengths of individual models. Our approach achieves a remarkable accuracy of\n97.79\\,\\% for Arabic subjectivity classification. Results demonstrate the\neffectiveness of the proposed approach in addressing the challenges posed by\nlimited resources in Arabic language processing.", "AI": {"tldr": "Proposes a new approach for Arabic subjectivity classification using fine-tuned Arabic language models on a newly created comprehensive dataset (AraDhati+), achieving 97.79% accuracy.", "motivation": "Arabic faces challenges as an under-resourced language with scarce annotated datasets for subjectivity analysis, despite being linguistically rich and morphologically complex.", "method": "Developed AraDhati+ dataset by combining existing Arabic datasets (ASTD, LABR, HARD, SANAD), then fine-tuned state-of-the-art Arabic language models (XLM-RoBERTa, AraBERT, ArabianGPT) and experimented with ensemble decision approach.", "result": "Achieved remarkable 97.79% accuracy for Arabic subjectivity classification, demonstrating effectiveness in addressing limited resource challenges.", "conclusion": "The proposed approach successfully addresses Arabic language processing challenges by leveraging existing datasets and fine-tuning advanced language models, showing high effectiveness for subjectivity classification."}}
{"id": "2508.19705", "pdf": "https://arxiv.org/pdf/2508.19705", "abs": "https://arxiv.org/abs/2508.19705", "authors": ["Qiang Hu", "Ying Zhou", "Gepeng Ji", "Nick Barnes", "Qiang Li", "Zhiwei Wang"], "title": "FreeVPS: Repurposing Training-Free SAM2 for Generalizable Video Polyp Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Existing video polyp segmentation (VPS) paradigms usually struggle to balance\nbetween spatiotemporal modeling and domain generalization, limiting their\napplicability in real clinical scenarios. To embrace this challenge, we recast\nthe VPS task as a track-by-detect paradigm that leverages the spatial contexts\ncaptured by the image polyp segmentation (IPS) model while integrating the\ntemporal modeling capabilities of segment anything model 2 (SAM2). However,\nduring long-term polyp tracking in colonoscopy videos, SAM2 suffers from error\naccumulation, resulting in a snowball effect that compromises segmentation\nstability. We mitigate this issue by repurposing SAM2 as a video polyp\nsegmenter with two training-free modules. In particular, the intra-association\nfiltering module eliminates spatial inaccuracies originating from the detecting\nstage, reducing false positives. The inter-association refinement module\nadaptively updates the memory bank to prevent error propagation over time,\nenhancing temporal coherence. Both modules work synergistically to stabilize\nSAM2, achieving cutting-edge performance in both in-domain and out-of-domain\nscenarios. Furthermore, we demonstrate the robust tracking capabilities of\nFreeVPS in long-untrimmed colonoscopy videos, underscoring its potential\nreliable clinical analysis.", "AI": {"tldr": "FreeVPS is a training-free video polyp segmentation method that combines IPS spatial context with SAM2 temporal modeling, using two modules to eliminate error accumulation and achieve state-of-the-art performance in both in-domain and out-of-domain scenarios.", "motivation": "Existing VPS methods struggle to balance spatiotemporal modeling and domain generalization, limiting real clinical applicability. SAM2 suffers from error accumulation during long-term polyp tracking, causing segmentation instability.", "method": "Recasts VPS as track-by-detect paradigm using IPS for spatial context and SAM2 for temporal modeling. Introduces two training-free modules: intra-association filtering to eliminate spatial inaccuracies and reduce false positives, and inter-association refinement to adaptively update memory bank and prevent error propagation.", "result": "Achieves cutting-edge performance in both in-domain and out-of-domain scenarios. Demonstrates robust tracking capabilities in long-untrimmed colonoscopy videos.", "conclusion": "FreeVPS provides a reliable solution for clinical video polyp segmentation by synergistically stabilizing SAM2 through the two modules, showing strong potential for clinical analysis applications."}}
{"id": "2508.19982", "pdf": "https://arxiv.org/pdf/2508.19982", "abs": "https://arxiv.org/abs/2508.19982", "authors": ["Pengxiang Li", "Yefan Zhou", "Dilxat Muhtar", "Lu Yin", "Shilin Yan", "Li Shen", "Yi Liang", "Soroush Vosoughi", "Shiwei Liu"], "title": "Diffusion Language Models Know the Answer Before Decoding", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Diffusion language models (DLMs) have recently emerged as an alternative to\nautoregressive approaches, offering parallel sequence generation and flexible\ntoken orders. However, their inference remains slower than that of\nautoregressive models, primarily due to the cost of bidirectional attention and\nthe large number of refinement steps required for high quality outputs. In this\nwork, we highlight and leverage an overlooked property of DLMs early answer\nconvergence: in many cases, the correct answer can be internally identified by\nhalf steps before the final decoding step, both under semi-autoregressive and\nrandom remasking schedules. For example, on GSM8K and MMLU, up to 97% and 99%\nof instances, respectively, can be decoded correctly using only half of the\nrefinement steps. Building on this observation, we introduce Prophet, a\ntraining-free fast decoding paradigm that enables early commit decoding.\nSpecifically, Prophet dynamically decides whether to continue refinement or to\ngo \"all-in\" (i.e., decode all remaining tokens in one step), using the\nconfidence gap between the top-2 prediction candidates as the criterion. It\nintegrates seamlessly into existing DLM implementations, incurs negligible\noverhead, and requires no additional training. Empirical evaluations of\nLLaDA-8B and Dream-7B across multiple tasks show that Prophet reduces the\nnumber of decoding steps by up to 3.4x while preserving high generation\nquality. These results recast DLM decoding as a problem of when to stop\nsampling, and demonstrate that early decode convergence provides a simple yet\npowerful mechanism for accelerating DLM inference, complementary to existing\nspeedup techniques. Our code is publicly available at\nhttps://github.com/pixeli99/Prophet.", "AI": {"tldr": "Prophet is a training-free decoding method that accelerates diffusion language models by leveraging early answer convergence, allowing early commitment to final outputs using confidence-based criteria, achieving up to 3.4x speedup.", "motivation": "Diffusion language models suffer from slow inference due to bidirectional attention costs and many refinement steps, despite offering parallel generation and flexible token orders.", "method": "Prophet uses early answer convergence property - dynamically decides to continue refinement or decode all remaining tokens in one step based on confidence gap between top-2 prediction candidates, requiring no additional training.", "result": "On GSM8K and MMLU, up to 97% and 99% of instances can be decoded correctly using only half the refinement steps. Prophet reduces decoding steps by up to 3.4x while preserving generation quality on LLaDA-8B and Dream-7B models.", "conclusion": "Early decode convergence provides a simple yet powerful mechanism for accelerating DLM inference, complementary to existing speedup techniques, recasting DLM decoding as a problem of when to stop sampling."}}
{"id": "2508.19730", "pdf": "https://arxiv.org/pdf/2508.19730", "abs": "https://arxiv.org/abs/2508.19730", "authors": ["Stelios Mylonas", "Symeon Papadopoulos"], "title": "Improving Generalization in Deepfake Detection with Face Foundation Models and Metric Learning", "categories": ["cs.CV"], "comment": null, "summary": "The increasing realism and accessibility of deepfakes have raised critical\nconcerns about media authenticity and information integrity. Despite recent\nadvances, deepfake detection models often struggle to generalize beyond their\ntraining distributions, particularly when applied to media content found in the\nwild. In this work, we present a robust video deepfake detection framework with\nstrong generalization that takes advantage of the rich facial representations\nlearned by face foundation models. Our method is built on top of FSFM, a\nself-supervised model trained on real face data, and is further fine-tuned\nusing an ensemble of deepfake datasets spanning both face-swapping and\nface-reenactment manipulations. To enhance discriminative power, we incorporate\ntriplet loss variants during training, guiding the model to produce more\nseparable embeddings between real and fake samples. Additionally, we explore\nattribution-based supervision schemes, where deepfakes are categorized by\nmanipulation type or source dataset, to assess their impact on generalization.\nExtensive experiments across diverse evaluation benchmarks demonstrate the\neffectiveness of our approach, especially in challenging real-world scenarios.", "AI": {"tldr": "Robust video deepfake detection framework using face foundation models with triplet loss and attribution supervision for better generalization.", "motivation": "Deepfake detection models struggle to generalize beyond training distributions, especially for real-world media content, due to increasing realism and accessibility of deepfakes.", "method": "Built on FSFM self-supervised face foundation model, fine-tuned with ensemble of deepfake datasets. Uses triplet loss variants for separable embeddings and attribution-based supervision by manipulation type/source dataset.", "result": "Extensive experiments show effectiveness in diverse benchmarks, particularly in challenging real-world scenarios with strong generalization capabilities.", "conclusion": "The framework demonstrates robust deepfake detection with improved generalization through face foundation models, triplet loss, and attribution supervision techniques."}}
{"id": "2508.19988", "pdf": "https://arxiv.org/pdf/2508.19988", "abs": "https://arxiv.org/abs/2508.19988", "authors": ["Lisa Alazraki", "Lihu Chen", "Ana Brassard", "Joe Stacey", "Hossein A. Rahmani", "Marek Rei"], "title": "AgentCoMa: A Compositional Benchmark Mixing Commonsense and Mathematical Reasoning in Real-World Scenarios", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have achieved high accuracy on complex\ncommonsense and mathematical problems that involve the composition of multiple\nreasoning steps. However, current compositional benchmarks testing these skills\ntend to focus on either commonsense or math reasoning, whereas LLM agents\nsolving real-world tasks would require a combination of both. In this work, we\nintroduce an Agentic Commonsense and Math benchmark (AgentCoMa), where each\ncompositional task requires a commonsense reasoning step and a math reasoning\nstep. We test it on 61 LLMs of different sizes, model families, and training\nstrategies. We find that LLMs can usually solve both steps in isolation, yet\ntheir accuracy drops by ~30% on average when the two are combined. This is a\nsubstantially greater performance gap than the one we observe in prior\ncompositional benchmarks that combine multiple steps of the same reasoning\ntype. In contrast, non-expert human annotators can solve the compositional\nquestions and the individual steps in AgentCoMa with similarly high accuracy.\nFurthermore, we conduct a series of interpretability studies to better\nunderstand the performance gap, examining neuron patterns, attention maps and\nmembership inference. Our work underscores a substantial degree of model\nbrittleness in the context of mixed-type compositional reasoning and offers a\ntest bed for future improvement.", "AI": {"tldr": "LLMs struggle with mixed-type compositional reasoning (commonsense + math) showing ~30% accuracy drop compared to solving each step individually, while humans perform equally well on both.", "motivation": "Current benchmarks focus on either commonsense or math reasoning, but real-world tasks require combining both. Need to test LLMs' ability to handle mixed-type compositional reasoning.", "method": "Created AgentCoMa benchmark with tasks requiring both commonsense and math reasoning steps. Tested 61 LLMs of different sizes/families. Conducted interpretability studies (neuron patterns, attention maps, membership inference).", "result": "LLMs show ~30% accuracy drop when combining commonsense and math reasoning vs solving steps individually. Humans perform equally well on both. Performance gap is larger than in same-type compositional benchmarks.", "conclusion": "LLMs exhibit substantial brittleness in mixed-type compositional reasoning. AgentCoMa provides a test bed for future model improvements in this critical capability."}}
{"id": "2508.19742", "pdf": "https://arxiv.org/pdf/2508.19742", "abs": "https://arxiv.org/abs/2508.19742", "authors": ["Chenguang Liu", "Chisheng Wang", "Yuhua Cai", "Chuanhua Zhu", "Qingquan Li"], "title": "POEv2: a flexible and robust framework for generic line segment detection and wireframe line segment detection", "categories": ["cs.CV"], "comment": null, "summary": "Line segment detection in images has been studied for several decades.\nExisting line segment detectors can be roughly divided into two categories:\ngeneric line segment detectors and wireframe line segment detectors. Generic\nline segment detectors aim to detect all meaningful line segments in images and\ntraditional approaches usually fall into this category. Recent deep learning\nbased approaches are mostly wireframe line segment detectors. They detect only\nline segments that are geometrically meaningful and have large spatial support.\nDue to the difference in the aim of design, the performance of generic line\nsegment detectors for the task of wireframe line segment detection won't be\nsatisfactory, and vice versa. In this work, we propose a robust framework that\ncan be used for both generic line segment detection and wireframe line segment\ndetection. The proposed method is an improved version of the Pixel Orientation\nEstimation (POE) method. It is thus named as POEv2. POEv2 detects line segments\nfrom edge strength maps, and can be combined with any edge detector. We show in\nour experiments that by combining the proposed POEv2 with an efficient edge\ndetector, it achieves state-of-the-art performance on three publicly available\ndatasets.", "AI": {"tldr": "POEv2 is a robust line segment detection framework that works for both generic and wireframe detection, achieving state-of-the-art performance when combined with efficient edge detectors.", "motivation": "Existing line segment detectors are specialized for either generic detection (all meaningful segments) or wireframe detection (geometrically meaningful segments with large spatial support), but none work well for both tasks simultaneously.", "method": "Improved version of Pixel Orientation Estimation (POE) method that detects line segments from edge strength maps and can be combined with any edge detector.", "result": "Achieves state-of-the-art performance on three publicly available datasets when combined with an efficient edge detector.", "conclusion": "POEv2 provides a unified framework that effectively handles both generic and wireframe line segment detection tasks, overcoming the limitations of specialized detectors."}}
{"id": "2508.19993", "pdf": "https://arxiv.org/pdf/2508.19993", "abs": "https://arxiv.org/abs/2508.19993", "authors": ["Debanjana Kar", "Leopold B\u00f6ss", "Dacia Braca", "Sebastian Maximilian Dennerlein", "Nina Christine Hubig", "Philipp Wintersberger", "Yufang Hou"], "title": "MathBuddy: A Multimodal System for Affective Math Tutoring", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "The rapid adoption of LLM-based conversational systems is already\ntransforming the landscape of educational technology. However, the current\nstate-of-the-art learning models do not take into account the student's\naffective states. Multiple studies in educational psychology support the claim\nthat positive or negative emotional states can impact a student's learning\ncapabilities. To bridge this gap, we present MathBuddy, an emotionally aware\nLLM-powered Math Tutor, which dynamically models the student's emotions and\nmaps them to relevant pedagogical strategies, making the tutor-student\nconversation a more empathetic one. The student's emotions are captured from\nthe conversational text as well as from their facial expressions. The student's\nemotions are aggregated from both modalities to confidently prompt our LLM\nTutor for an emotionally-aware response. We have effectively evaluated our\nmodel using automatic evaluation metrics across eight pedagogical dimensions\nand user studies. We report a massive 23 point performance gain using the win\nrate and a 3 point gain at an overall level using DAMR scores which strongly\nsupports our hypothesis of improving LLM-based tutor's pedagogical abilities by\nmodeling students' emotions.", "AI": {"tldr": "MathBuddy is an emotionally-aware LLM math tutor that models student emotions from text and facial expressions to provide empathetic, pedagogically appropriate responses, achieving significant performance gains.", "motivation": "Current LLM-based learning systems ignore student affective states, despite educational psychology research showing emotions significantly impact learning capabilities. There's a need to bridge this gap by creating emotionally-aware tutoring systems.", "method": "Developed MathBuddy, an LLM-powered math tutor that dynamically models student emotions from conversational text and facial expressions, aggregates emotions from both modalities, and maps them to relevant pedagogical strategies for empathetic responses.", "result": "Achieved 23 point performance gain using win rate and 3 point gain using DAMR scores across eight pedagogical dimensions. User studies and automatic evaluation metrics strongly support improved pedagogical abilities through emotion modeling.", "conclusion": "Modeling student emotions significantly enhances LLM-based tutors' pedagogical effectiveness, making tutor-student conversations more empathetic and educationally beneficial."}}
{"id": "2508.19746", "pdf": "https://arxiv.org/pdf/2508.19746", "abs": "https://arxiv.org/abs/2508.19746", "authors": ["Qiyao Xu", "Qiming Wu", "Xiaowei Li"], "title": "SPLF-SAM: Self-Prompting Segment Anything Model for Light Field Salient Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "Segment Anything Model (SAM) has demonstrated remarkable capabilities in\nsolving light field salient object detection (LF SOD). However, most existing\nmodels tend to neglect the extraction of prompt information under this task.\nMeanwhile, traditional models ignore the analysis of frequency-domain\ninformation, which leads to small objects being overwhelmed by noise. In this\npaper, we put forward a novel model called self-prompting light field segment\nanything model (SPLF-SAM), equipped with unified multi-scale feature embedding\nblock (UMFEB) and a multi-scale adaptive filtering adapter (MAFA). UMFEB is\ncapable of identifying multiple objects of varying sizes, while MAFA, by\nlearning frequency features, effectively prevents small objects from being\noverwhelmed by noise. Extensive experiments have demonstrated the superiority\nof our method over ten state-of-the-art (SOTA) LF SOD methods. Our code will be\navailable at https://github.com/XucherCH/splfsam.", "AI": {"tldr": "SPLF-SAM is a novel self-prompting model for light field salient object detection that addresses prompt information extraction and frequency-domain analysis through UMFEB and MAFA modules, outperforming 10 SOTA methods.", "motivation": "Existing models neglect prompt information extraction in LF SOD tasks and ignore frequency-domain analysis, causing small objects to be overwhelmed by noise.", "method": "Proposes SPLF-SAM with unified multi-scale feature embedding block (UMFEB) to identify multiple objects of varying sizes, and multi-scale adaptive filtering adapter (MAFA) to learn frequency features and prevent small objects from noise interference.", "result": "Extensive experiments demonstrate superiority over ten state-of-the-art LF SOD methods.", "conclusion": "The proposed SPLF-SAM effectively addresses prompt extraction and frequency-domain challenges in light field salient object detection, achieving superior performance compared to existing methods."}}
{"id": "2508.19996", "pdf": "https://arxiv.org/pdf/2508.19996", "abs": "https://arxiv.org/abs/2508.19996", "authors": ["Yiming Du", "Yifan Xiang", "Bin Liang", "Dahua Lin", "Kam-Fai Wong", "Fei Tan"], "title": "ReSURE: Regularizing Supervision Unreliability for Multi-turn Dialogue Fine-tuning", "categories": ["cs.CL"], "comment": null, "summary": "Fine-tuning multi-turn dialogue systems requires high-quality supervision but\noften suffers from degraded performance when exposed to low-quality data.\nSupervision errors in early turns can propagate across subsequent turns,\nundermining coherence and response quality. Existing methods typically address\ndata quality via static prefiltering, which decouples quality control from\ntraining and fails to mitigate turn-level error propagation. In this context,\nwe propose ReSURE (Regularizing Supervision UnREliability), an adaptive\nlearning method that dynamically down-weights unreliable supervision without\nexplicit filtering. ReSURE estimates per-turn loss distributions using\nWelford's online statistics and reweights sample losses on the fly accordingly.\nExperiments on both single-source and mixed-quality datasets show improved\nstability and response quality. Notably, ReSURE enjoys positive Spearman\ncorrelations (0.21 ~ 1.0 across multiple benchmarks) between response scores\nand number of samples regardless of data quality, which potentially paves the\nway for utilizing large-scale data effectively. Code is publicly available at\nhttps://github.com/Elvin-Yiming-Du/ReSURE_Multi_Turn_Training.", "AI": {"tldr": "ReSURE is an adaptive learning method that dynamically down-weights unreliable supervision in multi-turn dialogue training without explicit filtering, using online statistics to estimate per-turn loss distributions and improve training stability.", "motivation": "Fine-tuning multi-turn dialogue systems suffers from degraded performance when exposed to low-quality data, with supervision errors in early turns propagating across subsequent turns and undermining coherence. Existing static prefiltering methods decouple quality control from training and fail to mitigate turn-level error propagation.", "method": "ReSURE estimates per-turn loss distributions using Welford's online statistics and dynamically reweights sample losses on the fly to down-weight unreliable supervision without explicit filtering.", "result": "Experiments on single-source and mixed-quality datasets show improved stability and response quality. ReSURE achieves positive Spearman correlations (0.21-1.0 across benchmarks) between response scores and sample count regardless of data quality.", "conclusion": "ReSURE effectively addresses supervision error propagation in multi-turn dialogue training and potentially enables more effective utilization of large-scale data by dynamically adapting to supervision reliability during training."}}
{"id": "2508.19754", "pdf": "https://arxiv.org/pdf/2508.19754", "abs": "https://arxiv.org/abs/2508.19754", "authors": ["Yue Wu", "Yufan Wu", "Wen Li", "Yuxi Lu", "Kairui Feng", "Xuanhong Chen"], "title": "FastAvatar: Towards Unified Fast High-Fidelity 3D Avatar Reconstruction with Large Gaussian Reconstruction Transformers", "categories": ["cs.CV"], "comment": null, "summary": "Despite significant progress in 3D avatar reconstruction, it still faces\nchallenges such as high time complexity, sensitivity to data quality, and low\ndata utilization. We propose FastAvatar, a feedforward 3D avatar framework\ncapable of flexibly leveraging diverse daily recordings (e.g., a single image,\nmulti-view observations, or monocular video) to reconstruct a high-quality 3D\nGaussian Splatting (3DGS) model within seconds, using only a single unified\nmodel. FastAvatar's core is a Large Gaussian Reconstruction Transformer\nfeaturing three key designs: First, a variant VGGT-style transformer\narchitecture aggregating multi-frame cues while injecting initial 3D prompt to\npredict an aggregatable canonical 3DGS representation; Second, multi-granular\nguidance encoding (camera pose, FLAME expression, head pose) mitigating\nanimation-induced misalignment for variable-length inputs; Third, incremental\nGaussian aggregation via landmark tracking and sliced fusion losses.\nIntegrating these features, FastAvatar enables incremental reconstruction,\ni.e., improving quality with more observations, unlike prior work wasting input\ndata. This yields a quality-speed-tunable paradigm for highly usable avatar\nmodeling. Extensive experiments show that FastAvatar has higher quality and\nhighly competitive speed compared to existing methods.", "AI": {"tldr": "FastAvatar is a feedforward 3D avatar reconstruction framework that uses a Large Gaussian Reconstruction Transformer to create high-quality 3D Gaussian Splatting models from diverse inputs (single image, multi-view, or video) within seconds using a single unified model.", "motivation": "Current 3D avatar reconstruction methods suffer from high time complexity, sensitivity to data quality, and low data utilization, limiting their practical usability.", "method": "Uses a VGGT-style transformer architecture with multi-granular guidance encoding (camera pose, FLAME expression, head pose) and incremental Gaussian aggregation via landmark tracking and sliced fusion losses to predict aggregatable canonical 3DGS representations.", "result": "FastAvatar achieves higher quality and highly competitive speed compared to existing methods, enabling incremental reconstruction that improves with more observations.", "conclusion": "The framework provides a quality-speed-tunable paradigm for highly usable avatar modeling that efficiently leverages diverse input data without wasting information like prior methods."}}
{"id": "2508.19997", "pdf": "https://arxiv.org/pdf/2508.19997", "abs": "https://arxiv.org/abs/2508.19997", "authors": ["Boheng Mao"], "title": "Selective Retrieval-Augmentation for Long-Tail Legal Text Classification", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Legal text classification is a fundamental NLP task in the legal domain.\nBenchmark datasets in this area often exhibit a long-tail label distribution,\nwhere many labels are underrepresented, leading to poor model performance on\nrare classes. This paper proposes Selective Retrieval-Augmentation (SRA) as a\nsolution to this problem. SRA focuses on augmenting samples belonging to\nlow-frequency labels in the training set, preventing the introduction of noise\nfor well-represented classes, and requires no changes to the model\narchitecture. Retrieval is performed only from the training data to ensure\nthere is no potential information leakage, removing the need for external\ncorpora simultaneously. The proposed SRA method is tested on two legal text\nclassification benchmark datasets with long-tail distributions: LEDGAR\n(single-label) and UNFAIR-ToS (multi-label). The results indicate that SRA\nattains higher micro-F1 and macro-F1 scores compared to all current LexGLUE\nbaselines across both datasets, illustrating consistent improvements in\nlong-tail legal text classification. The code repository is available at:\nhttps://github.com/Boheng-Mao/sra-legal", "AI": {"tldr": "SRA method improves legal text classification on rare classes by selectively augmenting low-frequency labels from training data only, achieving better F1 scores than baselines.", "motivation": "Legal text classification datasets often have long-tail distributions where rare classes are underrepresented, leading to poor model performance on these classes.", "method": "Selective Retrieval-Augmentation (SRA) that focuses on augmenting samples from low-frequency labels in the training set only, preventing noise for well-represented classes without changing model architecture.", "result": "SRA achieves higher micro-F1 and macro-F1 scores compared to all current LexGLUE baselines on both LEDGAR (single-label) and UNFAIR-ToS (multi-label) datasets.", "conclusion": "SRA provides consistent improvements for long-tail legal text classification by selectively augmenting rare classes from training data, eliminating need for external corpora and preventing information leakage."}}
{"id": "2508.19762", "pdf": "https://arxiv.org/pdf/2508.19762", "abs": "https://arxiv.org/abs/2508.19762", "authors": ["Ahmed Emam", "Mohamed Elbassiouny", "Julius Miller", "Patrick Donworth", "Sabine Seidel", "Ribana Roscher"], "title": "BuzzSet v1.0: A Dataset for Pollinator Detection in Field Conditions", "categories": ["cs.CV"], "comment": null, "summary": "Pollinator insects such as honeybees and bumblebees are vital to global food\nproduction and ecosystem stability, yet their populations are declining due to\nincreasing anthropogenic and environmental stressors. To support scalable,\nautomated pollinator monitoring, we introduce BuzzSet, a new large-scale\ndataset of high-resolution pollinator images collected in real agricultural\nfield conditions. BuzzSet contains 7856 manually verified and labeled images,\nwith over 8000 annotated instances across three classes: honeybees, bumblebees,\nand unidentified insects. Initial annotations were generated using a YOLOv12\nmodel trained on external data and refined via human verification using\nopen-source labeling tools. All images were preprocessed into 256~$\\times$~256\ntiles to improve the detection of small insects. We provide strong baselines\nusing the RF-DETR transformer-based object detector. The model achieves high\nF1-scores of 0.94 and 0.92 for honeybee and bumblebee classes, respectively,\nwith confusion matrix results showing minimal misclassification between these\ncategories. The unidentified class remains more challenging due to label\nambiguity and lower sample frequency, yet still contributes useful insights for\nrobustness evaluation. Overall detection quality is strong, with a best\nmAP@0.50 of 0.559. BuzzSet offers a valuable benchmark for small object\ndetection, class separation under label noise, and ecological computer vision.", "AI": {"tldr": "BuzzSet is a new large-scale dataset of high-resolution pollinator images for automated monitoring, containing 7856 verified images with over 8000 annotated instances across honeybees, bumblebees, and unidentified insects.", "motivation": "Pollinator populations are declining due to environmental stressors, requiring scalable automated monitoring solutions to support global food production and ecosystem stability.", "method": "Created BuzzSet dataset with manually verified images collected in real agricultural fields, using YOLOv12 for initial annotations and human verification. Preprocessed images into 256x256 tiles and used RF-DETR transformer-based object detector for evaluation.", "result": "Achieved high F1-scores of 0.94 for honeybees and 0.92 for bumblebees, with minimal misclassification between these categories. Best mAP@0.50 of 0.559, though unidentified class remains challenging due to label ambiguity.", "conclusion": "BuzzSet provides a valuable benchmark for small object detection, class separation under label noise, and ecological computer vision applications in pollinator monitoring."}}
{"id": "2508.20033", "pdf": "https://arxiv.org/pdf/2508.20033", "abs": "https://arxiv.org/abs/2508.20033", "authors": ["Liana Patel", "Negar Arabzadeh", "Harshit Gupta", "Ankita Sundar", "Ion Stoica", "Matei Zaharia", "Carlos Guestrin"], "title": "DeepScholar-Bench: A Live Benchmark and Automated Evaluation for Generative Research Synthesis", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The ability to research and synthesize knowledge is central to human\nexpertise and progress. An emerging class of systems promises these exciting\ncapabilities through generative research synthesis, performing retrieval over\nthe live web and synthesizing discovered sources into long-form, cited\nsummaries. However, evaluating such systems remains an open challenge: existing\nquestion-answering benchmarks focus on short-form factual responses, while\nexpert-curated datasets risk staleness and data contamination. Both fail to\ncapture the complexity and evolving nature of real research synthesis tasks. In\nthis work, we introduce DeepScholar-bench, a live benchmark and holistic,\nautomated evaluation framework designed to evaluate generative research\nsynthesis. DeepScholar-bench draws queries from recent, high-quality ArXiv\npapers and focuses on a real research synthesis task: generating the related\nwork sections of a paper by retrieving, synthesizing, and citing prior\nresearch. Our evaluation framework holistically assesses performance across\nthree key dimensions, knowledge synthesis, retrieval quality, and\nverifiability. We also develop DeepScholar-base, a reference pipeline\nimplemented efficiently using the LOTUS API. Using the DeepScholar-bench\nframework, we perform a systematic evaluation of prior open-source systems,\nsearch AI's, OpenAI's DeepResearch, and DeepScholar-base. We find that\nDeepScholar-base establishes a strong baseline, attaining competitive or higher\nperformance than each other method. We also find that DeepScholar-bench remains\nfar from saturated, with no system exceeding a score of $19\\%$ across all\nmetrics. These results underscore the difficulty of DeepScholar-bench, as well\nas its importance for progress towards AI systems capable of generative\nresearch synthesis. We make our code available at\nhttps://github.com/guestrin-lab/deepscholar-bench.", "AI": {"tldr": "DeepScholar-bench is a live benchmark and automated evaluation framework for generative research synthesis systems, using recent ArXiv papers to test related work section generation with holistic assessment across knowledge synthesis, retrieval quality, and verifiability.", "motivation": "Existing benchmarks for research synthesis systems are inadequate - short-form QA benchmarks don't capture complexity, and expert-curated datasets risk staleness and data contamination, failing to represent real research synthesis challenges.", "method": "Created DeepScholar-bench with queries from recent high-quality ArXiv papers, focusing on generating related work sections. Developed evaluation framework assessing knowledge synthesis, retrieval quality, and verifiability. Built DeepScholar-base reference pipeline using LOTUS API.", "result": "DeepScholar-base established strong baseline with competitive/higher performance than other methods (prior open-source systems, search AIs, OpenAI's DeepResearch). No system exceeded 19% across all metrics, showing benchmark difficulty.", "conclusion": "DeepScholar-bench is challenging and important for advancing AI systems capable of generative research synthesis, with significant room for improvement as current systems perform poorly on comprehensive evaluation."}}
{"id": "2508.19769", "pdf": "https://arxiv.org/pdf/2508.19769", "abs": "https://arxiv.org/abs/2508.19769", "authors": ["Shu Shen", "C. L. Philip Chen", "Tong Zhang"], "title": "AIM: Adaptive Intra-Network Modulation for Balanced Multimodal Learning", "categories": ["cs.CV"], "comment": "13pages,7 figures", "summary": "Multimodal learning has significantly enhanced machine learning performance\nbut still faces numerous challenges and limitations. Imbalanced multimodal\nlearning is one of the problems extensively studied in recent works and is\ntypically mitigated by modulating the learning of each modality. However, we\nfind that these methods typically hinder the dominant modality's learning to\npromote weaker modalities, which affects overall multimodal performance. We\nanalyze the cause of this issue and highlight a commonly overlooked problem:\noptimization bias within networks. To address this, we propose Adaptive\nIntra-Network Modulation (AIM) to improve balanced modality learning. AIM\naccounts for differences in optimization state across parameters and depths\nwithin the network during modulation, achieving balanced multimodal learning\nwithout hindering either dominant or weak modalities for the first time.\nSpecifically, AIM decouples the dominant modality's under-optimized parameters\ninto Auxiliary Blocks and encourages reliance on these performance-degraded\nblocks for joint training with weaker modalities. This approach effectively\nprevents suppression of weaker modalities while enabling targeted optimization\nof under-optimized parameters to improve the dominant modality. Additionally,\nAIM assesses modality imbalance level across network depths and adaptively\nadjusts modulation strength at each depth. Experimental results demonstrate\nthat AIM outperforms state-of-the-art imbalanced modality learning methods\nacross multiple benchmarks and exhibits strong generalizability across\ndifferent backbones, fusion strategies, and optimizers.", "AI": {"tldr": "AIM addresses optimization bias in imbalanced multimodal learning by adaptively modulating parameters across network depths, improving both dominant and weak modalities without performance degradation.", "motivation": "Current methods for imbalanced multimodal learning hinder dominant modalities to promote weaker ones, affecting overall performance due to overlooked optimization bias within networks.", "method": "Proposes Adaptive Intra-Network Modulation (AIM) that decouples under-optimized parameters into Auxiliary Blocks, encourages reliance on degraded blocks for joint training, and adaptively adjusts modulation strength across network depths based on imbalance levels.", "result": "AIM outperforms state-of-the-art methods across multiple benchmarks and shows strong generalizability across different backbones, fusion strategies, and optimizers.", "conclusion": "AIM successfully addresses optimization bias in multimodal networks, achieving balanced learning without compromising either modality's performance, representing a significant advancement in imbalanced multimodal learning."}}
{"id": "2508.20038", "pdf": "https://arxiv.org/pdf/2508.20038", "abs": "https://arxiv.org/abs/2508.20038", "authors": ["Sheng Liu", "Qiang Sheng", "Danding Wang", "Yang Li", "Guang Yang", "Juan Cao"], "title": "Forewarned is Forearmed: Pre-Synthesizing Jailbreak-like Instructions to Enhance LLM Safety Guardrail to Potential Attacks", "categories": ["cs.CL"], "comment": "EMNLP 2025 findings", "summary": "Despite advances in improving large language model(LLM) to refuse to answer\nmalicious instructions, widely used LLMs remain vulnerable to jailbreak attacks\nwhere attackers generate instructions with distributions differing from safety\nalignment corpora. New attacks expose LLMs' inability to recognize unseen\nmalicious instructions, highlighting a critical distributional mismatch between\ntraining data and real-world attacks that forces developers into reactive\npatching cycles. To tackle this challenge, we propose IMAGINE, a synthesis\nframework that leverages embedding space distribution analysis to generate\njailbreak-like instructions. This approach effectively fills the distributional\ngap between authentic jailbreak patterns and safety alignment corpora. IMAGINE\nfollows an iterative optimization process that dynamically evolves text\ngeneration distributions across iterations, thereby augmenting the coverage of\nsafety alignment data distributions through synthesized data examples. Based on\nthe safety-aligned corpus enhanced through IMAGINE, our framework demonstrates\nsignificant decreases in attack success rate on Qwen2.5, Llama3.1, and Llama3.2\nwithout compromising their utility.", "AI": {"tldr": "IMAGINE is a framework that generates jailbreak-like instructions to fill distribution gaps in safety alignment data, reducing attack success rates on LLMs without compromising utility.", "motivation": "LLMs remain vulnerable to jailbreak attacks due to distributional mismatch between safety training data and real-world malicious instructions, forcing developers into reactive patching cycles.", "method": "IMAGINE uses embedding space distribution analysis and iterative optimization to synthesize jailbreak-like instructions that augment safety alignment corpora coverage.", "result": "Significant decreases in attack success rates on Qwen2.5, Llama3.1, and Llama3.2 models without compromising their utility.", "conclusion": "The framework effectively addresses distributional gaps in safety alignment data, providing proactive protection against jailbreak attacks rather than reactive patching."}}
{"id": "2508.19773", "pdf": "https://arxiv.org/pdf/2508.19773", "abs": "https://arxiv.org/abs/2508.19773", "authors": ["Jakob Seitz", "Tobias Lengfeld", "Radu Timofte"], "title": "The Return of Structural Handwritten Mathematical Expression Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Handwritten Mathematical Expression Recognition is foundational for\neducational technologies, enabling applications like digital note-taking and\nautomated grading. While modern encoder-decoder architectures with large\nlanguage models excel at LaTeX generation, they lack explicit symbol-to-trace\nalignment, a critical limitation for error analysis, interpretability, and\nspatially aware interactive applications requiring selective content updates.\nThis paper introduces a structural recognition approach with two innovations: 1\nan automatic annotation system that uses a neural network to map LaTeX\nequations to raw traces, automatically generating annotations for symbol\nsegmentation, classification, and spatial relations, and 2 a modular structural\nrecognition system that independently optimizes segmentation, classification,\nand relation prediction. By leveraging a dataset enriched with structural\nannotations from our auto-labeling system, the proposed recognition system\ncombines graph-based trace sorting, a hybrid convolutional-recurrent network,\nand transformer-based correction to achieve competitive performance on the\nCROHME-2023 benchmark. Crucially, our structural recognition system generates a\ncomplete graph structure that directly links handwritten traces to predicted\nsymbols, enabling transparent error analysis and interpretable outputs.", "AI": {"tldr": "This paper introduces a structural recognition approach for handwritten math expressions that provides explicit symbol-to-trace alignment, enabling better error analysis and interpretability compared to traditional encoder-decoder models.", "motivation": "Current encoder-decoder architectures with large language models excel at LaTeX generation but lack explicit symbol-to-trace alignment, which is critical for error analysis, interpretability, and spatially aware interactive applications requiring selective content updates.", "method": "Two innovations: 1) automatic annotation system using neural network to map LaTeX equations to raw traces for generating symbol segmentation, classification, and spatial relation annotations; 2) modular structural recognition system that independently optimizes segmentation, classification, and relation prediction using graph-based trace sorting, hybrid convolutional-recurrent network, and transformer-based correction.", "result": "Achieves competitive performance on the CROHME-2023 benchmark while generating complete graph structures that directly link handwritten traces to predicted symbols.", "conclusion": "The structural recognition system enables transparent error analysis and interpretable outputs, addressing critical limitations of existing approaches for educational technology applications."}}
{"id": "2508.20047", "pdf": "https://arxiv.org/pdf/2508.20047", "abs": "https://arxiv.org/abs/2508.20047", "authors": ["Hassan Alhuzali", "Farah Shamout", "Muhammad Abdul-Mageed", "Chaimae Abouzahir", "Mouath Abu-Daoud", "Ashwag Alasmari", "Walid Al-Eisawi", "Renad Al-Monef", "Ali Alqahtani", "Lama Ayash", "Nizar Habash", "Leen Kharouf"], "title": "AraHealthQA 2025 Shared Task Description Paper", "categories": ["cs.CL"], "comment": null, "summary": "We introduce {AraHealthQA 2025}, the {Comprehensive Arabic Health Question\nAnswering Shared Task}, held in conjunction with {ArabicNLP 2025} (co-located\nwith EMNLP 2025). This shared task addresses the paucity of high-quality Arabic\nmedical QA resources by offering two complementary tracks: {MentalQA}, focusing\non Arabic mental health Q\\&A (e.g., anxiety, depression, stigma reduction), and\n{MedArabiQ}, covering broader medical domains such as internal medicine,\npediatrics, and clinical decision making. Each track comprises multiple\nsubtasks, evaluation datasets, and standardized metrics, facilitating fair\nbenchmarking. The task was structured to promote modeling under realistic,\nmultilingual, and culturally nuanced healthcare contexts. We outline the\ndataset creation, task design and evaluation framework, participation\nstatistics, baseline systems, and summarize the overall outcomes. We conclude\nwith reflections on the performance trends observed and prospects for future\niterations in Arabic health QA.", "AI": {"tldr": "AraHealthQA 2025 is a comprehensive Arabic health question answering shared task with two tracks: MentalQA for mental health and MedArabiQ for broader medical domains, designed to address the lack of high-quality Arabic medical QA resources.", "motivation": "To address the paucity of high-quality Arabic medical question answering resources and promote research in Arabic healthcare NLP under realistic, multilingual, and culturally nuanced contexts.", "method": "The shared task was structured with two complementary tracks (MentalQA and MedArabiQ) comprising multiple subtasks, evaluation datasets, and standardized metrics. The approach included dataset creation, task design, evaluation framework development, and baseline system implementation.", "result": "The paper outlines participation statistics, baseline systems, and overall outcomes of the shared task, though specific performance metrics are not detailed in the abstract.", "conclusion": "The authors conclude with reflections on performance trends observed and discuss prospects for future iterations in Arabic health question answering research."}}
{"id": "2508.19786", "pdf": "https://arxiv.org/pdf/2508.19786", "abs": "https://arxiv.org/abs/2508.19786", "authors": ["Han Jiao", "Jiakai Sun", "Yexing Xu", "Lei Zhao", "Wei Xing", "Huaizhong Lin"], "title": "MAPo : Motion-Aware Partitioning of Deformable 3D Gaussian Splatting for High-Fidelity Dynamic Scene Reconstruction", "categories": ["cs.CV"], "comment": "8 pages, 9 figures, Anonymous AAAI Submission", "summary": "3D Gaussian Splatting, known for enabling high-quality static scene\nreconstruction with fast rendering, is increasingly being applied to dynamic\nscene reconstruction. A common strategy involves learning a deformation field\nto model the temporal changes of a canonical set of 3D Gaussians. However,\nthese deformation-based methods often produce blurred renderings and lose fine\nmotion details in highly dynamic regions due to the inherent limitations of a\nsingle, unified model in representing diverse motion patterns. To address these\nchallenges, we introduce Motion-Aware Partitioning of Deformable 3D Gaussian\nSplatting (MAPo), a novel framework for high-fidelity dynamic scene\nreconstruction. Its core is a dynamic score-based partitioning strategy that\ndistinguishes between high- and low-dynamic 3D Gaussians. For high-dynamic 3D\nGaussians, we recursively partition them temporally and duplicate their\ndeformation networks for each new temporal segment, enabling specialized\nmodeling to capture intricate motion details. Concurrently, low-dynamic 3DGs\nare treated as static to reduce computational costs. However, this temporal\npartitioning strategy for high-dynamic 3DGs can introduce visual\ndiscontinuities across frames at the partition boundaries. To address this, we\nintroduce a cross-frame consistency loss, which not only ensures visual\ncontinuity but also further enhances rendering quality. Extensive experiments\ndemonstrate that MAPo achieves superior rendering quality compared to baselines\nwhile maintaining comparable computational costs, particularly in regions with\ncomplex or rapid motions.", "AI": {"tldr": "MAPo introduces motion-aware partitioning of 3D Gaussians to improve dynamic scene reconstruction by separating high/low dynamic regions and using specialized deformation networks with cross-frame consistency.", "motivation": "Existing deformation-based 3D Gaussian Splatting methods for dynamic scenes produce blurred renderings and lose fine motion details due to limitations of single unified models in representing diverse motion patterns.", "method": "Dynamic score-based partitioning strategy that distinguishes high/low dynamic 3D Gaussians. High-dynamic Gaussians are recursively partitioned temporally with duplicated deformation networks per segment, while low-dynamic ones remain static. Cross-frame consistency loss ensures visual continuity.", "result": "Superior rendering quality compared to baselines while maintaining comparable computational costs, particularly in regions with complex or rapid motions.", "conclusion": "MAPo framework effectively addresses motion blur and detail loss in dynamic scene reconstruction through motion-aware partitioning and specialized modeling of high-dynamic regions."}}
{"id": "2508.20068", "pdf": "https://arxiv.org/pdf/2508.20068", "abs": "https://arxiv.org/abs/2508.20068", "authors": ["Chengzu Li", "Wenshan Wu", "Huanyu Zhang", "Qingtao Li", "Zeyu Gao", "Yan Xia", "Jos\u00e9 Hern\u00e1ndez-Orallo", "Ivan Vuli\u0107", "Furu Wei"], "title": "11Plus-Bench: Demystifying Multimodal LLM Spatial Reasoning with Cognitive-Inspired Analysis", "categories": ["cs.CL", "cs.CV", "cs.LG"], "comment": "9 pages, 4 figures (22 pages, 7 figures, 7 tables including\n  references and appendices)", "summary": "For human cognitive process, spatial reasoning and perception are closely\nentangled, yet the nature of this interplay remains underexplored in the\nevaluation of multimodal large language models (MLLMs). While recent MLLM\nadvancements show impressive performance on reasoning, their capacity for\nhuman-like spatial cognition remains an open question. In this work, we\nintroduce a systematic evaluation framework to assess the spatial reasoning\nabilities of state-of-the-art MLLMs relative to human performance. Central to\nour work is 11Plus-Bench, a high-quality benchmark derived from realistic\nstandardized spatial aptitude tests. 11Plus-Bench also features fine-grained\nexpert annotations of both perceptual complexity and reasoning process,\nenabling detailed instance-level analysis of model behavior. Through extensive\nexperiments across 14 MLLMs and human evaluation, we find that current MLLMs\nexhibit early signs of spatial cognition. Despite a large performance gap\ncompared to humans, MLLMs' cognitive profiles resemble those of humans in that\ncognitive effort correlates strongly with reasoning-related complexity.\nHowever, instance-level performance in MLLMs remains largely random, whereas\nhuman correctness is highly predictable and shaped by abstract pattern\ncomplexity. These findings highlight both emerging capabilities and limitations\nin current MLLMs' spatial reasoning capabilities and provide actionable\ninsights for advancing model design.", "AI": {"tldr": "Current MLLMs show early signs of spatial cognition but have large performance gaps compared to humans, with random instance-level performance versus predictable human performance based on pattern complexity.", "motivation": "Spatial reasoning and perception are closely linked in human cognition but remain underexplored in multimodal large language models (MLLMs), despite their impressive reasoning capabilities.", "method": "Introduced 11Plus-Bench, a high-quality benchmark derived from standardized spatial aptitude tests with fine-grained expert annotations of perceptual complexity and reasoning process. Evaluated 14 MLLMs and compared with human performance.", "result": "MLLMs exhibit early spatial cognition signs with cognitive profiles resembling humans (effort correlates with reasoning complexity), but show large performance gaps and random instance-level performance compared to humans' predictable performance based on abstract pattern complexity.", "conclusion": "Findings highlight both emerging capabilities and limitations in MLLMs' spatial reasoning, providing actionable insights for advancing model design in spatial cognition."}}
{"id": "2508.19789", "pdf": "https://arxiv.org/pdf/2508.19789", "abs": "https://arxiv.org/abs/2508.19789", "authors": ["Xiuchao Wu", "Pengfei Zhu", "Jiangjing Lyu", "Xinguo Liu", "Jie Guo", "Yanwen Guo", "Weiwei Xu", "Chengfei Lyu"], "title": "StableIntrinsic: Detail-preserving One-step Diffusion Model for Multi-view Material Estimation", "categories": ["cs.CV"], "comment": null, "summary": "Recovering material information from images has been extensively studied in\ncomputer graphics and vision. Recent works in material estimation leverage\ndiffusion model showing promising results. However, these diffusion-based\nmethods adopt a multi-step denoising strategy, which is time-consuming for each\nestimation. Such stochastic inference also conflicts with the deterministic\nmaterial estimation task, leading to a high variance estimated results. In this\npaper, we introduce StableIntrinsic, a one-step diffusion model for multi-view\nmaterial estimation that can produce high-quality material parameters with low\nvariance. To address the overly-smoothing problem in one-step diffusion,\nStableIntrinsic applies losses in pixel space, with each loss designed based on\nthe properties of the material. Additionally, StableIntrinsic introduces a\nDetail Injection Network (DIN) to eliminate the detail loss caused by VAE\nencoding, while further enhancing the sharpness of material prediction results.\nThe experimental results indicate that our method surpasses the current\nstate-of-the-art techniques by achieving a $9.9\\%$ improvement in the Peak\nSignal-to-Noise Ratio (PSNR) of albedo, and by reducing the Mean Square Error\n(MSE) for metallic and roughness by $44.4\\%$ and $60.0\\%$, respectively.", "AI": {"tldr": "StableIntrinsic is a one-step diffusion model for multi-view material estimation that produces high-quality, low-variance material parameters, outperforming previous methods with significant improvements in PSNR and MSE metrics.", "motivation": "Existing diffusion-based material estimation methods use multi-step denoising which is time-consuming and produces high-variance results due to stochastic inference conflicting with deterministic material estimation tasks.", "method": "Proposes a one-step diffusion model with pixel-space losses designed based on material properties, and introduces a Detail Injection Network (DIN) to eliminate detail loss from VAE encoding and enhance sharpness.", "result": "Achieves 9.9% improvement in PSNR for albedo, and reduces MSE for metallic and roughness by 44.4% and 60.0% respectively, surpassing state-of-the-art techniques.", "conclusion": "StableIntrinsic demonstrates that one-step diffusion with carefully designed losses and detail preservation mechanisms can effectively address the limitations of multi-step approaches for material estimation tasks."}}
{"id": "2508.19259", "pdf": "https://arxiv.org/pdf/2508.19259", "abs": "https://arxiv.org/abs/2508.19259", "authors": ["Georgios P. Georgiou"], "title": "Capabilities of GPT-5 across critical domains: Is it the next breakthrough?", "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "The accelerated evolution of large language models has raised questions about\ntheir comparative performance across domains of practical importance. GPT-4 by\nOpenAI introduced advances in reasoning, multimodality, and task\ngeneralization, establishing itself as a valuable tool in education, clinical\ndiagnosis, and academic writing, though it was accompanied by several flaws.\nReleased in August 2025, GPT-5 incorporates a system-of-models architecture\ndesigned for task-specific optimization and, based on both anecdotal accounts\nand emerging evidence from the literature, demonstrates stronger performance\nthan its predecessor in medical contexts. This study provides one of the first\nsystematic comparisons of GPT-4 and GPT-5 using human raters from linguistics\nand clinical fields. Twenty experts evaluated model-generated outputs across\nfive domains: lesson planning, assignment evaluation, clinical diagnosis,\nresearch generation, and ethical reasoning, based on predefined criteria.\nMixed-effects models revealed that GPT-5 significantly outperformed GPT-4 in\nlesson planning, clinical diagnosis, research generation, and ethical\nreasoning, while both models performed comparably in assignment assessment. The\nfindings highlight the potential of GPT-5 to serve as a context-sensitive and\ndomain-specialized tool, offering tangible benefits for education, clinical\npractice, and academic research, while also advancing ethical reasoning. These\nresults contribute to one of the earliest empirical evaluations of the evolving\ncapabilities and practical promise of GPT-5.", "AI": {"tldr": "GPT-5 significantly outperforms GPT-4 in most domains including lesson planning, clinical diagnosis, research generation, and ethical reasoning, while performing comparably in assignment assessment, based on expert evaluations across five key domains.", "motivation": "To provide a systematic comparison of GPT-4 and GPT-5's performance across practical domains of importance, particularly education, clinical practice, and academic research, given the rapid evolution of large language models.", "method": "Twenty expert human raters from linguistics and clinical fields evaluated model-generated outputs across five domains (lesson planning, assignment evaluation, clinical diagnosis, research generation, ethical reasoning) using predefined criteria. Mixed-effects models were used for statistical analysis.", "result": "GPT-5 significantly outperformed GPT-4 in four out of five domains: lesson planning, clinical diagnosis, research generation, and ethical reasoning. Both models performed comparably in assignment assessment.", "conclusion": "GPT-5 demonstrates superior performance as a context-sensitive and domain-specialized tool with tangible benefits for education, clinical practice, and academic research, while also advancing ethical reasoning capabilities."}}
{"id": "2508.19791", "pdf": "https://arxiv.org/pdf/2508.19791", "abs": "https://arxiv.org/abs/2508.19791", "authors": ["Shay Shomer Chai", "Wenxuan Peng", "Bharath Hariharan", "Hadar Averbuch-Elor"], "title": "Not Every Gift Comes in Gold Paper or with a Red Ribbon: Exploring Color Perception in Text-to-Image Models", "categories": ["cs.CV"], "comment": "Project webpage: https://tau-vailab.github.io/color-edit/", "summary": "Text-to-image generation has recently seen remarkable success, granting users\nwith the ability to create high-quality images through the use of text.\nHowever, contemporary methods face challenges in capturing the precise\nsemantics conveyed by complex multi-object prompts. Consequently, many works\nhave sought to mitigate such semantic misalignments, typically via\ninference-time schemes that modify the attention layers of the denoising\nnetworks. However, prior work has mostly utilized coarse metrics, such as the\ncosine similarity between text and image CLIP embeddings, or human evaluations,\nwhich are challenging to conduct on a larger-scale. In this work, we perform a\ncase study on colors -- a fundamental attribute commonly associated with\nobjects in text prompts, which offer a rich test bed for rigorous evaluation.\nOur analysis reveals that pretrained models struggle to generate images that\nfaithfully reflect multiple color attributes-far more so than with single-color\nprompts-and that neither inference-time techniques nor existing editing methods\nreliably resolve these semantic misalignments. Accordingly, we introduce a\ndedicated image editing technique, mitigating the issue of multi-object\nsemantic alignment for prompts containing multiple colors. We demonstrate that\nour approach significantly boosts performance over a wide range of metrics,\nconsidering images generated by various text-to-image diffusion-based\ntechniques.", "AI": {"tldr": "Text-to-image models struggle with multi-object color prompts. This paper introduces a dedicated editing technique that significantly improves multi-color semantic alignment across various diffusion models.", "motivation": "Current text-to-image generation methods fail to accurately capture precise semantics in complex multi-object prompts, particularly with color attributes. Existing evaluation metrics are coarse and human evaluations are difficult to scale.", "method": "The authors perform a case study on color attributes, analyze pretrained models' limitations with multi-color prompts, and introduce a dedicated image editing technique specifically designed to address multi-object semantic alignment issues for prompts containing multiple colors.", "result": "The analysis reveals that pretrained models struggle significantly more with multiple color attributes than single-color prompts, and existing inference-time techniques and editing methods fail to reliably resolve these semantic misalignments. The proposed approach demonstrates significant performance improvements across a wide range of metrics.", "conclusion": "The dedicated image editing technique effectively mitigates multi-object semantic alignment issues for color-rich prompts and outperforms existing methods, providing a scalable solution for improving text-to-image generation fidelity with complex multi-attribute descriptions."}}
{"id": "2508.19262", "pdf": "https://arxiv.org/pdf/2508.19262", "abs": "https://arxiv.org/abs/2508.19262", "authors": ["Maximilian Wachter", "Sebastian Murgul", "Michael Heizmann"], "title": "Beat-Based Rhythm Quantization of MIDI Performances", "categories": ["cs.SD", "cs.CL", "cs.MM", "eess.AS"], "comment": "Accepted to the Late Breaking Demo Papers of the 1st AES\n  International Conference on Artificial Intelligence and Machine Learning for\n  Audio (AIMLA LBDP), 2025", "summary": "We propose a transformer-based rhythm quantization model that incorporates\nbeat and downbeat information to quantize MIDI performances into\nmetrically-aligned, human-readable scores. We propose a beat-based\npreprocessing method that transfers score and performance data into a unified\ntoken representation. We optimize our model architecture and data\nrepresentation and train on piano and guitar performances. Our model exceeds\nstate-of-the-art performance based on the MUSTER metric.", "AI": {"tldr": "Transformer-based rhythm quantization model using beat/downbeat information to convert MIDI performances into metrically-aligned scores, achieving state-of-the-art results on piano and guitar data.", "motivation": "To create human-readable, metrically-aligned musical scores from MIDI performances by leveraging beat and downbeat information for improved rhythm quantization.", "method": "Proposes a beat-based preprocessing method that converts score and performance data into unified token representation, uses transformer architecture optimized for rhythm quantization tasks.", "result": "Model exceeds state-of-the-art performance based on MUSTER metric when trained on piano and guitar performances.", "conclusion": "The transformer-based approach with beat/downbeat incorporation effectively quantizes MIDI performances into readable scores, demonstrating superior performance over existing methods."}}
{"id": "2508.19798", "pdf": "https://arxiv.org/pdf/2508.19798", "abs": "https://arxiv.org/abs/2508.19798", "authors": ["Muhammad Ali", "Omar Ali AlSuwaidi"], "title": "FusionSort: Enhanced Cluttered Waste Segmentation with Advanced Decoding and Comprehensive Modality Optimization", "categories": ["cs.CV"], "comment": null, "summary": "In the realm of waste management, automating the sorting process for\nnon-biodegradable materials presents considerable challenges due to the\ncomplexity and variability of waste streams. To address these challenges, we\nintroduce an enhanced neural architecture that builds upon an existing\nEncoder-Decoder structure to improve the accuracy and efficiency of waste\nsorting systems. Our model integrates several key innovations: a Comprehensive\nAttention Block within the decoder, which refines feature representations by\ncombining convolutional and upsampling operations. In parallel, we utilize\nattention through the Mamba architecture, providing an additional performance\nboost. We also introduce a Data Fusion Block that fuses images with more than\nthree channels. To achieve this, we apply PCA transformation to reduce the\ndimensionality while retaining the maximum variance and essential information\nacross three dimensions, which are then used for further processing. We\nevaluated the model on RGB, hyperspectral, multispectral, and a combination of\nRGB and hyperspectral data. The results demonstrate that our approach\noutperforms existing methods by a significant margin.", "AI": {"tldr": "Enhanced neural architecture with Comprehensive Attention Block and Mamba attention for waste sorting automation, using PCA-based data fusion for multi-channel image processing.", "motivation": "Automating waste sorting of non-biodegradable materials is challenging due to complex and variable waste streams, requiring improved accuracy and efficiency in sorting systems.", "method": "Encoder-Decoder architecture enhanced with Comprehensive Attention Block, Mamba attention mechanism, and Data Fusion Block using PCA transformation for multi-channel image fusion (RGB, hyperspectral, multispectral).", "result": "The approach significantly outperforms existing methods across all evaluated data types including RGB, hyperspectral, multispectral, and combined RGB-hyperspectral data.", "conclusion": "The proposed neural architecture with attention mechanisms and PCA-based data fusion provides a superior solution for automated waste sorting systems, demonstrating substantial performance improvements over current methods."}}
{"id": "2508.19269", "pdf": "https://arxiv.org/pdf/2508.19269", "abs": "https://arxiv.org/abs/2508.19269", "authors": ["Ke Zhou", "Marios Constantinides", "Daniele Quercia"], "title": "Should LLMs be WEIRD? Exploring WEIRDness and Human Rights in Large Language Models", "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": "This paper has been accepted in AIES 2025", "summary": "Large language models (LLMs) are often trained on data that reflect WEIRD\nvalues: Western, Educated, Industrialized, Rich, and Democratic. This raises\nconcerns about cultural bias and fairness. Using responses to the World Values\nSurvey, we evaluated five widely used LLMs: GPT-3.5, GPT-4, Llama-3, BLOOM, and\nQwen. We measured how closely these responses aligned with the values of the\nWEIRD countries and whether they conflicted with human rights principles. To\nreflect global diversity, we compared the results with the Universal\nDeclaration of Human Rights and three regional charters from Asia, the Middle\nEast, and Africa. Models with lower alignment to WEIRD values, such as BLOOM\nand Qwen, produced more culturally varied responses but were 2% to 4% more\nlikely to generate outputs that violated human rights, especially regarding\ngender and equality. For example, some models agreed with the statements ``a\nman who cannot father children is not a real man'' and ``a husband should\nalways know where his wife is'', reflecting harmful gender norms. These\nfindings suggest that as cultural representation in LLMs increases, so does the\nrisk of reproducing discriminatory beliefs. Approaches such as Constitutional\nAI, which could embed human rights principles into model behavior, may only\npartly help resolve this tension.", "AI": {"tldr": "LLMs trained on WEIRD data show cultural bias; models with less WEIRD alignment produce more diverse but 2-4% more human rights-violating outputs, especially on gender issues.", "motivation": "Concerns about cultural bias and fairness in LLMs trained primarily on Western, Educated, Industrialized, Rich, and Democratic (WEIRD) data, raising questions about their alignment with global human rights principles.", "method": "Evaluated five LLMs (GPT-3.5, GPT-4, Llama-3, BLOOM, Qwen) using World Values Survey responses, measuring alignment with WEIRD values and human rights principles from Universal Declaration of Human Rights and three regional charters.", "result": "Models with lower WEIRD alignment (BLOOM, Qwen) produced more culturally varied responses but were 2-4% more likely to violate human rights, particularly regarding gender equality. Some models endorsed harmful gender norms.", "conclusion": "Increasing cultural representation in LLMs may paradoxically increase reproduction of discriminatory beliefs; Constitutional AI approaches may only partially resolve this tension between cultural diversity and human rights protection."}}
{"id": "2508.19804", "pdf": "https://arxiv.org/pdf/2508.19804", "abs": "https://arxiv.org/abs/2508.19804", "authors": ["Christian Marzahl", "Brian Napora"], "title": "A bag of tricks for real-time Mitotic Figure detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Mitotic figure (MF) detection in histopathology images is challenging due to\nlarge variations in slide scanners, staining protocols, tissue types, and the\npresence of artifacts. This paper presents a collection of training techniques\n- a bag of tricks - that enable robust, real-time MF detection across diverse\ndomains. We build on the efficient RTMDet single stage object detector to\nachieve high inference speed suitable for clinical deployment. Our method\naddresses scanner variability and tumor heterogeneity via extensive\nmulti-domain training data, balanced sampling, and careful augmentation.\nAdditionally, we employ targeted, hard negative mining on necrotic and debris\ntissue to reduce false positives. In a grouped 5-fold cross-validation across\nmultiple MF datasets, our model achieves an F1 score between 0.78 and 0.84. On\nthe preliminary test set of the MItosis DOmain Generalization (MIDOG) 2025\nchallenge, our single-stage RTMDet-S based approach reaches an F1 of 0.81,\noutperforming larger models and demonstrating adaptability to new, unfamiliar\ndomains. The proposed solution offers a practical trade-off between accuracy\nand speed, making it attractive for real-world clinical adoption.", "AI": {"tldr": "A bag of tricks training approach for robust, real-time mitotic figure detection using RTMDet single stage detector, achieving F1 scores of 0.78-0.84 across domains with good speed-accuracy trade-off.", "motivation": "Mitotic figure detection faces challenges from scanner variability, staining differences, tissue types, and artifacts, requiring robust solutions for clinical deployment.", "method": "Uses RTMDet single stage object detector with multi-domain training data, balanced sampling, careful augmentation, and hard negative mining on necrotic/debris tissue to reduce false positives.", "result": "Achieves F1 scores between 0.78-0.84 in cross-validation, and 0.81 F1 on MIDOG 2025 test set, outperforming larger models with good domain adaptability.", "conclusion": "The approach provides a practical speed-accuracy trade-off suitable for real-world clinical adoption, demonstrating robustness across diverse domains with efficient inference."}}
{"id": "2508.19806", "pdf": "https://arxiv.org/pdf/2508.19806", "abs": "https://arxiv.org/abs/2508.19806", "authors": ["Shenqi Wang", "Guangzhi Tang"], "title": "Context-aware Sparse Spatiotemporal Learning for Event-based Vision", "categories": ["cs.CV", "cs.NE"], "comment": "Accepted at IROS 2025", "summary": "Event-based camera has emerged as a promising paradigm for robot perception,\noffering advantages with high temporal resolution, high dynamic range, and\nrobustness to motion blur. However, existing deep learning-based event\nprocessing methods often fail to fully leverage the sparse nature of event\ndata, complicating their integration into resource-constrained edge\napplications. While neuromorphic computing provides an energy-efficient\nalternative, spiking neural networks struggle to match of performance of\nstate-of-the-art models in complex event-based vision tasks, like object\ndetection and optical flow. Moreover, achieving high activation sparsity in\nneural networks is still difficult and often demands careful manual tuning of\nsparsity-inducing loss terms. Here, we propose Context-aware Sparse\nSpatiotemporal Learning (CSSL), a novel framework that introduces context-aware\nthresholding to dynamically regulate neuron activations based on the input\ndistribution, naturally reducing activation density without explicit sparsity\nconstraints. Applied to event-based object detection and optical flow\nestimation, CSSL achieves comparable or superior performance to\nstate-of-the-art methods while maintaining extremely high neuronal sparsity.\nOur experimental results highlight CSSL's crucial role in enabling efficient\nevent-based vision for neuromorphic processing.", "AI": {"tldr": "CSSL framework uses context-aware thresholding to dynamically regulate neuron activations for sparse event processing, achieving high performance with extreme sparsity in event-based vision tasks.", "motivation": "Existing deep learning methods don't fully leverage event data sparsity, and neuromorphic computing struggles to match state-of-the-art performance in complex tasks while maintaining sparsity.", "method": "Proposes Context-aware Sparse Spatiotemporal Learning (CSSL) with context-aware thresholding that dynamically regulates neuron activations based on input distribution without explicit sparsity constraints.", "result": "Achieves comparable or superior performance to state-of-the-art methods in event-based object detection and optical flow estimation while maintaining extremely high neuronal sparsity.", "conclusion": "CSSL enables efficient event-based vision for neuromorphic processing by naturally reducing activation density without manual sparsity tuning."}}
{"id": "2508.19316", "pdf": "https://arxiv.org/pdf/2508.19316", "abs": "https://arxiv.org/abs/2508.19316", "authors": ["Shreyans Jain", "Alexandra Yost", "Amirali Abdullah"], "title": "Sycophancy as compositions of Atomic Psychometric Traits", "categories": ["cs.AI", "cs.CL", "cs.LG", "I.2.7; I.2.4"], "comment": "8 pages, 4 figures", "summary": "Sycophancy is a key behavioral risk in LLMs, yet is often treated as an\nisolated failure mode that occurs via a single causal mechanism. We instead\npropose modeling it as geometric and causal compositions of psychometric traits\nsuch as emotionality, openness, and agreeableness - similar to factor\ndecomposition in psychometrics. Using Contrastive Activation Addition (CAA), we\nmap activation directions to these factors and study how different combinations\nmay give rise to sycophancy (e.g., high extraversion combined with low\nconscientiousness). This perspective allows for interpretable and compositional\nvector-based interventions like addition, subtraction and projection; that may\nbe used to mitigate safety-critical behaviors in LLMs.", "AI": {"tldr": "The paper proposes modeling LLM sycophancy as geometric compositions of psychometric traits rather than treating it as an isolated failure mode, using Contrastive Activation Addition to map activation directions and enable interpretable interventions.", "motivation": "Sycophancy is a key behavioral risk in LLMs that is often treated as an isolated failure mode with a single causal mechanism, but the authors believe it should be understood through more nuanced psychometric trait compositions.", "method": "Using Contrastive Activation Addition (CAA) to map activation directions to psychometric factors (emotionality, openness, agreeableness) and study how different trait combinations give rise to sycophantic behavior.", "result": "The approach provides a framework for understanding sycophancy through factor decomposition similar to psychometrics, revealing how specific trait combinations (e.g., high extraversion + low conscientiousness) contribute to sycophantic behavior.", "conclusion": "This geometric and causal composition perspective enables interpretable vector-based interventions (addition, subtraction, projection) that can be used to mitigate safety-critical behaviors like sycophancy in LLMs."}}
{"id": "2508.19808", "pdf": "https://arxiv.org/pdf/2508.19808", "abs": "https://arxiv.org/abs/2508.19808", "authors": ["Kaixuan Lu", "Mehmet Onurcan Kaya", "Dim P. Papadopoulos"], "title": "AutoQ-VIS: Improving Unsupervised Video Instance Segmentation via Automatic Quality Assessment", "categories": ["cs.CV"], "comment": "Accepted to ICCV 2025 Workshop LIMIT", "summary": "Video Instance Segmentation (VIS) faces significant annotation challenges due\nto its dual requirements of pixel-level masks and temporal consistency labels.\nWhile recent unsupervised methods like VideoCutLER eliminate optical flow\ndependencies through synthetic data, they remain constrained by the\nsynthetic-to-real domain gap. We present AutoQ-VIS, a novel unsupervised\nframework that bridges this gap through quality-guided self-training. Our\napproach establishes a closed-loop system between pseudo-label generation and\nautomatic quality assessment, enabling progressive adaptation from synthetic to\nreal videos. Experiments demonstrate state-of-the-art performance with 52.6\n$\\text{AP}_{50}$ on YouTubeVIS-2019 val set, surpassing the previous\nstate-of-the-art VideoCutLER by 4.4$\\%$, while requiring no human annotations.\nThis demonstrates the viability of quality-aware self-training for unsupervised\nVIS. The source code of our method is available at\nhttps://github.com/wcbup/AutoQ-VIS.", "AI": {"tldr": "AutoQ-VIS is an unsupervised video instance segmentation framework that uses quality-guided self-training to bridge the synthetic-to-real domain gap without human annotations, achieving state-of-the-art performance.", "motivation": "Video Instance Segmentation requires pixel-level masks and temporal consistency labels, which are challenging to annotate. Existing unsupervised methods suffer from synthetic-to-real domain gap limitations.", "method": "Quality-guided self-training with a closed-loop system between pseudo-label generation and automatic quality assessment, enabling progressive adaptation from synthetic to real videos.", "result": "Achieves 52.6 AP50 on YouTubeVIS-2019 val set, surpassing previous state-of-the-art VideoCutLER by 4.4% without human annotations.", "conclusion": "Demonstrates the viability of quality-aware self-training for unsupervised VIS, effectively bridging the synthetic-to-real domain gap."}}
{"id": "2508.19321", "pdf": "https://arxiv.org/pdf/2508.19321", "abs": "https://arxiv.org/abs/2508.19321", "authors": ["Kehao Miao", "Xiaolong Jin"], "title": "An Investigation on Group Query Hallucination Attacks", "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": null, "summary": "With the widespread use of large language models (LLMs), understanding their\npotential failure modes during user interactions is essential. In practice,\nusers often pose multiple questions in a single conversation with LLMs.\nTherefore, in this study, we propose Group Query Attack, a technique that\nsimulates this scenario by presenting groups of queries to LLMs simultaneously.\nWe investigate how the accumulated context from consecutive prompts influences\nthe outputs of LLMs. Specifically, we observe that Group Query Attack\nsignificantly degrades the performance of models fine-tuned on specific tasks.\nMoreover, we demonstrate that Group Query Attack induces a risk of triggering\npotential backdoors of LLMs. Besides, Group Query Attack is also effective in\ntasks involving reasoning, such as mathematical reasoning and code generation\nfor pre-trained and aligned models.", "AI": {"tldr": "Group Query Attack degrades LLM performance by presenting multiple queries simultaneously, revealing vulnerabilities in fine-tuned models, backdoor triggers, and reasoning tasks.", "motivation": "Understanding failure modes of LLMs during user interactions, particularly when users pose multiple questions in single conversations.", "method": "Propose Group Query Attack technique that presents groups of queries to LLMs simultaneously to investigate how accumulated context from consecutive prompts influences outputs.", "result": "Significant performance degradation in fine-tuned models, induces backdoor triggering risks, and effective against reasoning tasks in pre-trained and aligned models.", "conclusion": "Group Query Attack reveals critical vulnerabilities in LLMs, demonstrating that accumulated query contexts can be exploited to degrade performance and trigger hidden backdoors."}}
{"id": "2508.19815", "pdf": "https://arxiv.org/pdf/2508.19815", "abs": "https://arxiv.org/abs/2508.19815", "authors": ["Linkuan Zhou", "Zhexin Chen", "Yufei Shen", "Junlin Xu", "Ping Xuan", "Yixin Zhu", "Yuqi Fang", "Cong Cong", "Leyi Wei", "Ran Su", "Jia Zhou", "Qiangguo Jin"], "title": "ERSR: An Ellipse-constrained pseudo-label refinement and symmetric regularization framework for semi-supervised fetal head segmentation in ultrasound images", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Automated segmentation of the fetal head in ultrasound images is critical for\nprenatal monitoring. However, achieving robust segmentation remains challenging\ndue to the poor quality of ultrasound images and the lack of annotated data.\nSemi-supervised methods alleviate the lack of annotated data but struggle with\nthe unique characteristics of fetal head ultrasound images, making it\nchallenging to generate reliable pseudo-labels and enforce effective\nconsistency regularization constraints. To address this issue, we propose a\nnovel semi-supervised framework, ERSR, for fetal head ultrasound segmentation.\nOur framework consists of the dual-scoring adaptive filtering strategy, the\nellipse-constrained pseudo-label refinement, and the symmetry-based multiple\nconsistency regularization. The dual-scoring adaptive filtering strategy uses\nboundary consistency and contour regularity criteria to evaluate and filter\nteacher outputs. The ellipse-constrained pseudo-label refinement refines these\nfiltered outputs by fitting least-squares ellipses, which strengthens pixels\nnear the center of the fitted ellipse and suppresses noise simultaneously. The\nsymmetry-based multiple consistency regularization enforces multi-level\nconsistency across perturbed images, symmetric regions, and between original\npredictions and pseudo-labels, enabling the model to capture robust and stable\nshape representations. Our method achieves state-of-the-art performance on two\nbenchmarks. On the HC18 dataset, it reaches Dice scores of 92.05% and 95.36%\nwith 10% and 20% labeled data, respectively. On the PSFH dataset, the scores\nare 91.68% and 93.70% under the same settings.", "AI": {"tldr": "Proposed ERSR framework for semi-supervised fetal head ultrasound segmentation using dual-scoring filtering, ellipse-constrained refinement, and symmetry-based consistency regularization.", "motivation": "Automated fetal head segmentation in ultrasound is critical for prenatal monitoring but challenging due to poor image quality and lack of annotated data. Existing semi-supervised methods struggle with ultrasound-specific characteristics.", "method": "ERSR framework with three components: 1) Dual-scoring adaptive filtering using boundary consistency and contour regularity, 2) Ellipse-constrained pseudo-label refinement via least-squares ellipse fitting, 3) Symmetry-based multiple consistency regularization across perturbed images and symmetric regions.", "result": "State-of-the-art performance: HC18 dataset - 92.05% Dice with 10% labeled data, 95.36% with 20% labeled data; PSFH dataset - 91.68% with 10% labeled, 93.70% with 20% labeled.", "conclusion": "The proposed ERSR framework effectively addresses challenges in fetal head ultrasound segmentation by generating reliable pseudo-labels and enforcing robust consistency constraints, achieving superior performance with limited labeled data."}}
{"id": "2508.19492", "pdf": "https://arxiv.org/pdf/2508.19492", "abs": "https://arxiv.org/abs/2508.19492", "authors": ["Mehmet Can Yavuz", "Humza Gohar Kabir", "Aylin \u00d6zkan"], "title": "Geopolitical Parallax: Beyond Walter Lippmann Just After Large Language Models", "categories": ["cs.CY", "cs.CL"], "comment": "7 pages, 4 figures, 7 tables", "summary": "Objectivity in journalism has long been contested, oscillating between ideals\nof neutral, fact-based reporting and the inevitability of subjective framing.\nWith the advent of large language models (LLMs), these tensions are now\nmediated by algorithmic systems whose training data and design choices may\nthemselves embed cultural or ideological biases. This study investigates\ngeopolitical parallax-systematic divergence in news quality and subjectivity\nassessments-by comparing article-level embeddings from Chinese-origin (Qwen,\nBGE, Jina) and Western-origin (Snowflake, Granite) model families. We evaluate\nboth on a human-annotated news quality benchmark spanning fifteen stylistic,\ninformational, and affective dimensions, and on parallel corpora covering\npolitically sensitive topics, including Palestine and reciprocal China-United\nStates coverage. Using logistic regression probes and matched-topic evaluation,\nwe quantify per-metric differences in predicted positive-class probabilities\nbetween model families. Our findings reveal consistent, non-random divergences\naligned with model origin. In Palestine-related coverage, Western models assign\nhigher subjectivity and positive emotion scores, while Chinese models emphasize\nnovelty and descriptiveness. Cross-topic analysis shows asymmetries in\nstructural quality metrics Chinese-on-US scoring notably lower in fluency,\nconciseness, technicality, and overall quality-contrasted by higher negative\nemotion scores. These patterns align with media bias theory and our distinction\nbetween semantic, emotional, and relational subjectivity, and extend LLM bias\nliterature by showing that geopolitical framing effects persist in downstream\nquality assessment tasks. We conclude that LLM-based media evaluation pipelines\nrequire cultural calibration to avoid conflating content differences with\nmodel-induced bias.", "AI": {"tldr": "Study shows systematic differences in news quality assessments between Chinese and Western LLMs, with geopolitical biases affecting subjectivity, emotion, and quality metrics in sensitive topics like Palestine and China-US coverage.", "motivation": "To investigate how geopolitical biases in large language models affect news quality and subjectivity assessments, particularly given the contested nature of objectivity in journalism and the potential for algorithmic systems to embed cultural or ideological biases.", "method": "Compared article-level embeddings from Chinese-origin (Qwen, BGE, Jina) and Western-origin (Snowflake, Granite) model families using human-annotated news quality benchmarks and parallel corpora on politically sensitive topics. Employed logistic regression probes and matched-topic evaluation to quantify differences in predicted probabilities.", "result": "Consistent, non-random divergences aligned with model origin: Western models assigned higher subjectivity and positive emotion scores in Palestine coverage, while Chinese models emphasized novelty and descriptiveness. Chinese models scored US coverage lower in fluency, conciseness, technicality, and overall quality but higher in negative emotion.", "conclusion": "LLM-based media evaluation pipelines require cultural calibration to avoid conflating content differences with model-induced bias, as geopolitical framing effects persist in downstream quality assessment tasks."}}
{"id": "2508.19830", "pdf": "https://arxiv.org/pdf/2508.19830", "abs": "https://arxiv.org/abs/2508.19830", "authors": ["Yilin Zhang", "Cai Xu", "You Wu", "Ziyu Guan", "Wei Zhao"], "title": "Gradient Rectification for Robust Calibration under Distribution Shift", "categories": ["cs.CV", "cs.AI"], "comment": "14 pages, under review", "summary": "Deep neural networks often produce overconfident predictions, undermining\ntheir reliability in safety-critical applications. This miscalibration is\nfurther exacerbated under distribution shift, where test data deviates from the\ntraining distribution due to environmental or acquisition changes. While\nexisting approaches improve calibration through training-time regularization or\npost-hoc adjustment, their reliance on access to or simulation of target\ndomains limits their practicality in real-world scenarios. In this paper, we\npropose a novel calibration framework that operates without access to target\ndomain information. From a frequency-domain perspective, we identify that\ndistribution shifts often distort high-frequency visual cues exploited by deep\nmodels, and introduce a low-frequency filtering strategy to encourage reliance\non domain-invariant features. However, such information loss may degrade\nIn-Distribution (ID) calibration performance. Therefore, we further propose a\ngradient-based rectification mechanism that enforces ID calibration as a hard\nconstraint during optimization. Experiments on synthetic and real-world shifted\ndatasets, including CIFAR-10/100-C and WILDS, demonstrate that our method\nsignificantly improves calibration under distribution shift while maintaining\nstrong in-distribution performance.", "AI": {"tldr": "A novel calibration framework that improves model reliability under distribution shift without requiring target domain information, using low-frequency filtering and gradient-based rectification.", "motivation": "Deep neural networks produce overconfident predictions that become worse under distribution shift, and existing methods require impractical access to target domain information.", "method": "Uses low-frequency filtering to encourage reliance on domain-invariant features, plus gradient-based rectification to maintain in-distribution calibration as a hard constraint.", "result": "Significantly improves calibration under distribution shift on CIFAR-10/100-C and WILDS datasets while maintaining strong in-distribution performance.", "conclusion": "The proposed framework effectively addresses miscalibration under distribution shift without target domain access, making it practical for real-world applications."}}
{"id": "2508.19558", "pdf": "https://arxiv.org/pdf/2508.19558", "abs": "https://arxiv.org/abs/2508.19558", "authors": ["Zhuohao Li", "Wenqing Chen", "Jianxing Yu", "Zhichao Lu"], "title": "Functional Consistency of LLM Code Embeddings: A Self-Evolving Data Synthesis Framework for Benchmarking", "categories": ["cs.SE", "cs.CL", "cs.PL"], "comment": null, "summary": "Embedding models have demonstrated strong performance in tasks like\nclustering, retrieval, and feature extraction while offering computational\nadvantages over generative models and cross-encoders. Benchmarks such as MTEB\nhave shown that text embeddings from large language models (LLMs) capture rich\nsemantic information, but their ability to reflect code-level functional\nsemantics remains unclear. Existing studies largely focus on code clone\ndetection, which emphasizes syntactic similarity and overlooks functional\nunderstanding. In this paper, we focus on the functional consistency of LLM\ncode embeddings, which determines if two code snippets perform the same\nfunction regardless of syntactic differences. We propose a novel data synthesis\nframework called Functionality-Oriented Code Self-Evolution to construct\ndiverse and challenging benchmarks. Specifically, we define code examples\nacross four semantic and syntactic categories and find that existing datasets\npredominantly capture syntactic properties. Our framework generates four unique\nvariations from a single code instance, providing a broader spectrum of code\nexamples that better reflect functional differences. Extensive experiments on\nthree downstream tasks-code clone detection, code functional consistency\nidentification, and code retrieval-demonstrate that embedding models\nsignificantly improve their performance when trained on our evolved datasets.\nThese results highlight the effectiveness and generalization of our data\nsynthesis framework, advancing the functional understanding of code.", "AI": {"tldr": "LLM code embeddings show strong performance but unclear functional semantic understanding. A new framework generates diverse code variations to test functional consistency, improving embedding model performance on code tasks.", "motivation": "Existing benchmarks focus on syntactic similarity in code clone detection but overlook functional understanding. The paper aims to evaluate whether LLM code embeddings can capture functional semantics beyond just syntactic patterns.", "method": "Proposes Functionality-Oriented Code Self-Evolution framework to synthesize diverse code benchmarks. Generates four unique variations from single code instances across semantic and syntactic categories to better reflect functional differences.", "result": "Extensive experiments on code clone detection, functional consistency identification, and code retrieval show embedding models significantly improve performance when trained on the evolved datasets.", "conclusion": "The data synthesis framework effectively advances functional understanding of code and demonstrates strong generalization capabilities for improving code embedding models."}}
{"id": "2508.19850", "pdf": "https://arxiv.org/pdf/2508.19850", "abs": "https://arxiv.org/abs/2508.19850", "authors": ["Xiaoqi Wang", "Yun Zhang", "Weisi Lin"], "title": "Image Quality Assessment for Machines: Paradigm, Large-scale Database, and Models", "categories": ["cs.CV"], "comment": null, "summary": "Machine vision systems (MVS) are intrinsically vulnerable to performance\ndegradation under adverse visual conditions. To address this, we propose a\nmachine-centric image quality assessment (MIQA) framework that quantifies the\nimpact of image degradations on MVS performance. We establish an MIQA paradigm\nencompassing the end-to-end assessment workflow. To support this, we construct\na machine-centric image quality database (MIQD-2.5M), comprising 2.5 million\nsamples that capture distinctive degradation responses in both consistency and\naccuracy metrics, spanning 75 vision models, 250 degradation types, and three\nrepresentative vision tasks. We further propose a region-aware MIQA (RA-MIQA)\nmodel to evaluate MVS visual quality through fine-grained spatial degradation\nanalysis. Extensive experiments benchmark the proposed RA-MIQA against seven\nhuman visual system (HVS)-based IQA metrics and five retrained classical\nbackbones. Results demonstrate RA-MIQA's superior performance in multiple\ndimensions, e.g., achieving SRCC gains of 13.56% on consistency and 13.37% on\naccuracy for image classification, while also revealing task-specific\ndegradation sensitivities. Critically, HVS-based metrics prove inadequate for\nMVS quality prediction, while even specialized MIQA models struggle with\nbackground degradations, accuracy-oriented estimation, and subtle distortions.\nThis study can advance MVS reliability and establish foundations for\nmachine-centric image processing and optimization. The model and code are\navailable at: https://github.com/XiaoqiWang/MIQA.", "AI": {"tldr": "Proposes machine-centric image quality assessment (MIQA) framework to evaluate how image degradations affect machine vision systems, including a large database and region-aware model that outperforms human vision-based metrics.", "motivation": "Machine vision systems are vulnerable to performance degradation under adverse visual conditions, and human visual system-based quality metrics are inadequate for predicting machine vision performance.", "method": "Established MIQA paradigm with end-to-end workflow, constructed MIQD-2.5M database (2.5M samples across 75 models, 250 degradation types, 3 vision tasks), and proposed region-aware RA-MIQA model for fine-grained spatial degradation analysis.", "result": "RA-MIQA achieved superior performance with SRCC gains of 13.56% on consistency and 13.37% on accuracy for image classification, outperforming HVS-based metrics and retrained classical backbones. Revealed task-specific degradation sensitivities.", "conclusion": "HVS-based metrics are inadequate for MVS quality prediction. The study advances MVS reliability and establishes foundations for machine-centric image processing and optimization. The framework addresses limitations of existing approaches with background degradations, accuracy estimation, and subtle distortions."}}
{"id": "2508.19611", "pdf": "https://arxiv.org/pdf/2508.19611", "abs": "https://arxiv.org/abs/2508.19611", "authors": ["Huaiyuan Yao", "Wanpeng Xu", "Justin Turnau", "Nadia Kellam", "Hua Wei"], "title": "Instructional Agents: LLM Agents on Automated Course Material Generation for Teaching Faculties", "categories": ["cs.AI", "cs.CL", "I.2.7"], "comment": "18 pages, 9 figures", "summary": "Preparing high-quality instructional materials remains a labor-intensive\nprocess that often requires extensive coordination among teaching faculty,\ninstructional designers, and teaching assistants. In this work, we present\nInstructional Agents, a multi-agent large language model (LLM) framework\ndesigned to automate end-to-end course material generation, including syllabus\ncreation, lecture scripts, LaTeX-based slides, and assessments. Unlike existing\nAI-assisted educational tools that focus on isolated tasks, Instructional\nAgents simulates role-based collaboration among educational agents to produce\ncohesive and pedagogically aligned content. The system operates in four modes:\nAutonomous, Catalog-Guided, Feedback-Guided, and Full Co-Pilot mode, enabling\nflexible control over the degree of human involvement. We evaluate\nInstructional Agents across five university-level computer science courses and\nshow that it produces high-quality instructional materials while significantly\nreducing development time and human workload. By supporting institutions with\nlimited instructional design capacity, Instructional Agents provides a scalable\nand cost-effective framework to democratize access to high-quality education,\nparticularly in underserved or resource-constrained settings.", "AI": {"tldr": "Instructional Agents is a multi-agent LLM framework that automates end-to-end course material generation through simulated role-based collaboration, significantly reducing development time while maintaining quality.", "motivation": "High-quality instructional material preparation is labor-intensive and requires extensive coordination among faculty, designers, and TAs. There's a need to democratize access to quality education, especially in resource-constrained settings.", "method": "Multi-agent LLM framework with four operational modes (Autonomous, Catalog-Guided, Feedback-Guided, Full Co-Pilot) that simulates role-based educational collaboration to generate cohesive syllabus, lecture scripts, LaTeX slides, and assessments.", "result": "Evaluated across five university-level computer science courses, the system produces high-quality instructional materials while significantly reducing development time and human workload.", "conclusion": "Instructional Agents provides a scalable, cost-effective framework to support institutions with limited instructional design capacity and democratize access to high-quality education."}}
{"id": "2508.19852", "pdf": "https://arxiv.org/pdf/2508.19852", "abs": "https://arxiv.org/abs/2508.19852", "authors": ["Binjie Zhang", "Mike Zheng Shou"], "title": "Ego-centric Predictive Model Conditioned on Hand Trajectories", "categories": ["cs.CV"], "comment": "Code: github.com/binjiezhang/Ego-PM (branch: main)", "summary": "In egocentric scenarios, anticipating both the next action and its visual\noutcome is essential for understanding human-object interactions and for\nenabling robotic planning. However, existing paradigms fall short of jointly\nmodeling these aspects. Vision-Language-Action (VLA) models focus on action\nprediction but lack explicit modeling of how actions influence the visual\nscene, while video prediction models generate future frames without\nconditioning on specific actions, often resulting in implausible or\ncontextually inconsistent outcomes. To bridge this gap, we propose a unified\ntwo-stage predictive framework that jointly models action and visual future in\negocentric scenarios, conditioned on hand trajectories. In the first stage, we\nperform consecutive state modeling to process heterogeneous inputs (visual\nobservations, language, and action history) and explicitly predict future hand\ntrajectories. In the second stage, we introduce causal cross-attention to fuse\nmulti-modal cues, leveraging inferred action signals to guide an image-based\nLatent Diffusion Model (LDM) for frame-by-frame future video generation. Our\napproach is the first unified model designed to handle both egocentric human\nactivity understanding and robotic manipulation tasks, providing explicit\npredictions of both upcoming actions and their visual consequences. Extensive\nexperiments on Ego4D, BridgeData, and RLBench demonstrate that our method\noutperforms state-of-the-art baselines in both action prediction and future\nvideo synthesis.", "AI": {"tldr": "A unified two-stage framework that jointly predicts next actions and their visual outcomes in egocentric scenarios using hand trajectories and multi-modal fusion with latent diffusion models.", "motivation": "Existing approaches either predict actions without visual consequences (VLA models) or generate future frames without action conditioning (video prediction models), leading to incomplete or implausible results in human-object interaction understanding.", "method": "Two-stage approach: 1) Consecutive state modeling processes visual observations, language, and action history to predict future hand trajectories; 2) Causal cross-attention fuses multi-modal cues to guide a Latent Diffusion Model for frame-by-frame video generation conditioned on inferred actions.", "result": "Outperforms state-of-the-art baselines on Ego4D, BridgeData, and RLBench datasets in both action prediction accuracy and future video synthesis quality.", "conclusion": "The proposed unified framework successfully bridges the gap between action prediction and visual outcome modeling, enabling comprehensive understanding of human-object interactions and supporting robotic manipulation tasks with explicit predictions of both actions and their visual consequences."}}
{"id": "2508.19619", "pdf": "https://arxiv.org/pdf/2508.19619", "abs": "https://arxiv.org/abs/2508.19619", "authors": ["Duncan Adamson", "Moritz Dudey", "Pamela Fleischmann", "Annika Huch"], "title": "Word Chain Generators for Prefix Normal Words", "categories": ["math.CO", "cs.CL"], "comment": null, "summary": "In 2011, Fici and Lipt\\'ak introduced prefix normal words. A binary word is\nprefix normal if it has no factor (substring) that contains more occurrences of\nthe letter 1 than the prefix of the same length. Among the open problems\nregarding this topic are the enumeration of prefix normal words and efficient\ntesting methods. We show a range of characteristics of prefix normal words.\nThese include properties of factors that are responsible for a word not being\nprefix normal. With word chains and generators, we introduce new ways of\nrelating words of the same length to each other.", "AI": {"tldr": "Analysis of prefix normal words properties and relationships using word chains and generators", "motivation": "To address open problems in enumeration and efficient testing of prefix normal words by exploring their structural characteristics", "method": "Introduce word chains and generators to relate words of same length, analyze properties of factors that cause words to not be prefix normal", "result": "Shows range of characteristics of prefix normal words and properties of problematic factors", "conclusion": "New methods for relating prefix normal words provide insights for solving enumeration and testing challenges"}}
{"id": "2508.19862", "pdf": "https://arxiv.org/pdf/2508.19862", "abs": "https://arxiv.org/abs/2508.19862", "authors": ["Long Chen", "Ashiv Patel", "Mengyun Qiao", "Mohammad Yousuf Salmasi", "Salah A. Hammouche", "Vasilis Stavrinides", "Jasleen Nagi", "Soodeh Kalaie", "Xiao Yun Xu", "Wenjia Bai", "Declan P. O'Regan"], "title": "Multimodal Conditional MeshGAN for Personalized Aneurysm Growth Prediction", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Personalized, accurate prediction of aortic aneurysm progression is essential\nfor timely intervention but remains challenging due to the need to model both\nsubtle local deformations and global anatomical changes within complex 3D\ngeometries. We propose MCMeshGAN, the first multimodal conditional mesh-to-mesh\ngenerative adversarial network for 3D aneurysm growth prediction. MCMeshGAN\nintroduces a dual-branch architecture combining a novel local KNN-based\nconvolutional network (KCN) to preserve fine-grained geometric details and a\nglobal graph convolutional network (GCN) to capture long-range structural\ncontext, overcoming the over-smoothing limitations of deep GCNs. A dedicated\ncondition branch encodes clinical attributes (age, sex) and the target time\ninterval to generate anatomically plausible, temporally controlled predictions,\nenabling retrospective and prospective modeling. We curated TAAMesh, a new\nlongitudinal thoracic aortic aneurysm mesh dataset consisting of 590 multimodal\nrecords (CT scans, 3D meshes, and clinical data) from 208 patients. Extensive\nexperiments demonstrate that MCMeshGAN consistently outperforms\nstate-of-the-art baselines in both geometric accuracy and clinically important\ndiameter estimation. This framework offers a robust step toward clinically\ndeployable, personalized 3D disease trajectory modeling. The source code for\nMCMeshGAN and the baseline methods is publicly available at\nhttps://github.com/ImperialCollegeLondon/MCMeshGAN.", "AI": {"tldr": "MCMeshGAN is a multimodal conditional mesh-to-mesh GAN that predicts 3D aortic aneurysm growth by combining local geometric detail preservation with global structural context, outperforming state-of-the-art methods.", "motivation": "Personalized prediction of aortic aneurysm progression is challenging due to the need to model both subtle local deformations and global anatomical changes within complex 3D geometries for timely intervention.", "method": "Dual-branch architecture with local KNN-based convolutional network (KCN) for fine-grained details and global graph convolutional network (GCN) for structural context, plus condition branch for clinical attributes and time intervals.", "result": "Outperforms state-of-the-art baselines in geometric accuracy and clinically important diameter estimation on TAAMesh dataset (590 multimodal records from 208 patients).", "conclusion": "Provides a robust step toward clinically deployable, personalized 3D disease trajectory modeling with publicly available source code."}}
{"id": "2508.19697", "pdf": "https://arxiv.org/pdf/2508.19697", "abs": "https://arxiv.org/abs/2508.19697", "authors": ["Chao Huang", "Zefeng Zhang", "Juewei Yue", "Quangang Li", "Chuang Zhang", "Tingwen Liu"], "title": "Safety Alignment Should Be Made More Than Just A Few Attention Heads", "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": null, "summary": "Current safety alignment for large language models(LLMs) continues to present\nvulnerabilities, given that adversarial prompting can effectively bypass their\nsafety measures.Our investigation shows that these safety mechanisms\npredominantly depend on a limited subset of attention heads: removing or\nablating these heads can severely compromise model safety. To identify and\nevaluate these safety-critical components, we introduce RDSHA, a targeted\nablation method that leverages the model's refusal direction to pinpoint\nattention heads mostly responsible for safety behaviors. Further analysis shows\nthat existing jailbreak attacks exploit this concentration by selectively\nbypassing or manipulating these critical attention heads. To address this\nissue, we propose AHD, a novel training strategy designed to promote the\ndistributed encoding of safety-related behaviors across numerous attention\nheads. Experimental results demonstrate that AHD successfully distributes\nsafety-related capabilities across more attention heads. Moreover, evaluations\nunder several mainstream jailbreak attacks show that models trained with AHD\nexhibit considerably stronger safety robustness, while maintaining overall\nfunctional utility.", "AI": {"tldr": "Current LLM safety mechanisms rely on a small subset of attention heads, making them vulnerable to jailbreak attacks. The paper introduces RDSHA to identify these critical heads and proposes AHD training to distribute safety behaviors across more heads, resulting in stronger safety robustness.", "motivation": "Existing safety alignment for large language models remains vulnerable to adversarial attacks because safety mechanisms are concentrated in a limited number of attention heads, making them easy targets for jailbreak techniques.", "method": "The paper introduces RDSHA (targeted ablation method using refusal direction) to identify safety-critical attention heads, and proposes AHD (novel training strategy) to distribute safety-related behaviors across numerous attention heads rather than concentrating them in a few.", "result": "Experimental results show that AHD successfully distributes safety-related capabilities across more attention heads. Models trained with AHD exhibit significantly stronger safety robustness against mainstream jailbreak attacks while maintaining overall functional utility.", "conclusion": "Distributing safety mechanisms across multiple attention heads through targeted training strategies like AHD provides stronger protection against jailbreak attacks compared to current concentrated safety alignment approaches, addressing a fundamental vulnerability in LLM safety."}}
{"id": "2508.19864", "pdf": "https://arxiv.org/pdf/2508.19864", "abs": "https://arxiv.org/abs/2508.19864", "authors": ["Oussama Hadjerci", "Antoine Letienne", "Mohamed Abbas Hedjazi", "Adel Hafiane"], "title": "Self-supervised structured object representation learning", "categories": ["cs.CV"], "comment": null, "summary": "Self-supervised learning (SSL) has emerged as a powerful technique for\nlearning visual representations. While recent SSL approaches achieve strong\nresults in global image understanding, they are limited in capturing the\nstructured representation in scenes. In this work, we propose a self-supervised\napproach that progressively builds structured visual representations by\ncombining semantic grouping, instance level separation, and hierarchical\nstructuring. Our approach, based on a novel ProtoScale module, captures visual\nelements across multiple spatial scales. Unlike common strategies like DINO\nthat rely on random cropping and global embeddings, we preserve full scene\ncontext across augmented views to improve performance in dense prediction\ntasks. We validate our method on downstream object detection tasks using a\ncombined subset of multiple datasets (COCO and UA-DETRAC). Experimental results\nshow that our method learns object centric representations that enhance\nsupervised object detection and outperform the state-of-the-art methods, even\nwhen trained with limited annotated data and fewer fine-tuning epochs.", "AI": {"tldr": "Self-supervised learning approach that builds structured visual representations using semantic grouping, instance separation, and hierarchical structuring with a ProtoScale module, outperforming state-of-the-art methods in object detection tasks.", "motivation": "Current SSL methods excel at global image understanding but lack structured scene representation capabilities needed for dense prediction tasks like object detection.", "method": "Proposes a self-supervised approach with ProtoScale module that combines semantic grouping, instance level separation, and hierarchical structuring while preserving full scene context across augmented views.", "result": "Outperforms state-of-the-art methods in object detection tasks on COCO and UA-DETRAC datasets, even with limited annotated data and fewer fine-tuning epochs.", "conclusion": "The method successfully learns object-centric representations that enhance supervised object detection performance, demonstrating the value of structured visual representation learning in SSL."}}
{"id": "2508.19827", "pdf": "https://arxiv.org/pdf/2508.19827", "abs": "https://arxiv.org/abs/2508.19827", "authors": ["Samuel Lewis-Lim", "Xingwei Tan", "Zhixue Zhao", "Nikolaos Aletras"], "title": "Analysing Chain of Thought Dynamics: Active Guidance or Unfaithful Post-hoc Rationalisation?", "categories": ["cs.AI", "cs.CL"], "comment": "Accepted at EMNLP 2025 Main Conference", "summary": "Recent work has demonstrated that Chain-of-Thought (CoT) often yields limited\ngains for soft-reasoning problems such as analytical and commonsense reasoning.\nCoT can also be unfaithful to a model's actual reasoning. We investigate the\ndynamics and faithfulness of CoT in soft-reasoning tasks across\ninstruction-tuned, reasoning and reasoning-distilled models. Our findings\nreveal differences in how these models rely on CoT, and show that CoT influence\nand faithfulness are not always aligned.", "AI": {"tldr": "Chain-of-Thought (CoT) shows limited benefits and potential unfaithfulness in soft-reasoning tasks, with varying impacts across different model types.", "motivation": "To investigate the effectiveness and faithfulness of Chain-of-Thought reasoning in soft-reasoning problems like analytical and commonsense reasoning, where CoT has shown limited gains and potential unfaithfulness.", "method": "Analyzed the dynamics and faithfulness of CoT across different model types including instruction-tuned models, reasoning models, and reasoning-distilled models on soft-reasoning tasks.", "result": "Found significant differences in how various model types rely on CoT, and discovered that CoT influence and faithfulness are not always aligned across different models.", "conclusion": "Chain-of-Thought reasoning has varying impacts and faithfulness issues in soft-reasoning tasks, suggesting the need for careful consideration when applying CoT to different model architectures and reasoning domains."}}
{"id": "2508.19866", "pdf": "https://arxiv.org/pdf/2508.19866", "abs": "https://arxiv.org/abs/2508.19866", "authors": ["Fran\u00e7ois G. Landry", "Moulay A. Akhloufi"], "title": "TrajFusionNet: Pedestrian Crossing Intention Prediction via Fusion of Sequential and Visual Trajectory Representations", "categories": ["cs.CV", "cs.LG"], "comment": "This work has been submitted to IEEE Transactions on Intelligent\n  Vehicles for possible publication", "summary": "With the introduction of vehicles with autonomous capabilities on public\nroads, predicting pedestrian crossing intention has emerged as an active area\nof research. The task of predicting pedestrian crossing intention involves\ndetermining whether pedestrians in the scene are likely to cross the road or\nnot. In this work, we propose TrajFusionNet, a novel transformer-based model\nthat combines future pedestrian trajectory and vehicle speed predictions as\npriors for predicting crossing intention. TrajFusionNet comprises two branches:\na Sequence Attention Module (SAM) and a Visual Attention Module (VAM). The SAM\nbranch learns from a sequential representation of the observed and predicted\npedestrian trajectory and vehicle speed. Complementarily, the VAM branch\nenables learning from a visual representation of the predicted pedestrian\ntrajectory by overlaying predicted pedestrian bounding boxes onto scene images.\nBy utilizing a small number of lightweight modalities, TrajFusionNet achieves\nthe lowest total inference time (including model runtime and data\npreprocessing) among current state-of-the-art approaches. In terms of\nperformance, it achieves state-of-the-art results across the three most\ncommonly used datasets for pedestrian crossing intention prediction.", "AI": {"tldr": "TrajFusionNet is a transformer-based model that predicts pedestrian crossing intention by combining future trajectory and vehicle speed predictions, achieving state-of-the-art results with low inference time.", "motivation": "With autonomous vehicles on public roads, accurately predicting pedestrian crossing intention is crucial for safety and navigation. Current approaches need improvement in both accuracy and computational efficiency.", "method": "Proposes TrajFusionNet with two branches: Sequence Attention Module (SAM) for sequential trajectory/speed learning and Visual Attention Module (VAM) for visual representation of predicted trajectories overlaid on scene images.", "result": "Achieves state-of-the-art performance across three benchmark datasets and has the lowest total inference time (including model runtime and data preprocessing) among current approaches.", "conclusion": "TrajFusionNet effectively combines trajectory and speed predictions with visual scene context to provide accurate and efficient pedestrian crossing intention prediction for autonomous vehicles."}}
{"id": "2508.19843", "pdf": "https://arxiv.org/pdf/2508.19843", "abs": "https://arxiv.org/abs/2508.19843", "authors": ["Shuo Shao", "Yiming Li", "Yu He", "Hongwei Yao", "Wenyuan Yang", "Dacheng Tao", "Zhan Qin"], "title": "SoK: Large Language Model Copyright Auditing via Fingerprinting", "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": null, "summary": "The broad capabilities and substantial resources required to train Large\nLanguage Models (LLMs) make them valuable intellectual property, yet they\nremain vulnerable to copyright infringement, such as unauthorized use and model\ntheft. LLM fingerprinting, a non-intrusive technique that extracts and compares\nthe distinctive features from LLMs to identify infringements, offers a\npromising solution to copyright auditing. However, its reliability remains\nuncertain due to the prevalence of diverse model modifications and the lack of\nstandardized evaluation. In this SoK, we present the first comprehensive study\nof LLM fingerprinting. We introduce a unified framework and formal taxonomy\nthat categorizes existing methods into white-box and black-box approaches,\nproviding a structured overview of the state of the art. We further propose\nLeaFBench, the first systematic benchmark for evaluating LLM fingerprinting\nunder realistic deployment scenarios. Built upon mainstream foundation models\nand comprising 149 distinct model instances, LeaFBench integrates 13\nrepresentative post-development techniques, spanning both parameter-altering\nmethods (e.g., fine-tuning, quantization) and parameter-independent mechanisms\n(e.g., system prompts, RAG). Extensive experiments on LeaFBench reveal the\nstrengths and weaknesses of existing methods, thereby outlining future research\ndirections and critical open problems in this emerging field. The code is\navailable at https://github.com/shaoshuo-ss/LeaFBench.", "AI": {"tldr": "This paper presents a comprehensive study on LLM fingerprinting for copyright protection, introducing a unified framework, taxonomy, and the first systematic benchmark (LeaFBench) to evaluate fingerprinting methods under realistic deployment scenarios.", "motivation": "Large Language Models are valuable intellectual property but vulnerable to copyright infringement and model theft. LLM fingerprinting offers a non-intrusive solution for copyright auditing, but its reliability remains uncertain due to diverse model modifications and lack of standardized evaluation.", "method": "The authors introduce a unified framework and formal taxonomy categorizing existing fingerprinting methods into white-box and black-box approaches. They propose LeaFBench, a systematic benchmark built on mainstream foundation models with 149 distinct instances, integrating 13 representative post-development techniques including fine-tuning, quantization, system prompts, and RAG.", "result": "Extensive experiments on LeaFBench reveal the strengths and weaknesses of existing LLM fingerprinting methods, providing insights into their performance under various realistic deployment scenarios and model modifications.", "conclusion": "The study outlines future research directions and critical open problems in LLM fingerprinting, establishing a foundation for standardized evaluation and advancing copyright protection for large language models."}}
{"id": "2508.19875", "pdf": "https://arxiv.org/pdf/2508.19875", "abs": "https://arxiv.org/abs/2508.19875", "authors": ["Hui Zhang", "Jianghui Cai", "Haifeng Yang", "Ali Luo", "Yuqing Yang", "Xiao Kong", "Zhichao Ding", "Lichan Zhou", "Qin Han"], "title": "Sky Background Building of Multi-objective Fiber spectra Based on Mutual Information Network", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Sky background subtraction is a critical step in Multi-objective Fiber\nspectra process. However, current subtraction relies mainly on sky fiber\nspectra to build Super Sky. These average spectra are lacking in the modeling\nof the environment surrounding the objects. To address this issue, a sky\nbackground estimation model: Sky background building based on Mutual\nInformation (SMI) is proposed. SMI based on mutual information and incremental\ntraining approach. It utilizes spectra from all fibers in the plate to estimate\nthe sky background. SMI contains two main networks, the first network applies a\nwavelength calibration module to extract sky features from spectra, and can\neffectively solve the feature shift problem according to the corresponding\nemission position. The second network employs an incremental training approach\nto maximize mutual information between representations of different spectra to\ncapturing the common component. Then, it minimizes the mutual information\nbetween adjoining spectra representations to obtain individual components. This\nnetwork yields an individual sky background at each location of the object. To\nverify the effectiveness of the method in this paper, we conducted experiments\non the spectra of LAMOST. Results show that SMI can obtain a better object sky\nbackground during the observation, especially in the blue end.", "AI": {"tldr": "SMI: Sky background estimation model using mutual information and incremental training to improve sky subtraction in fiber spectra by capturing both common sky components and individual local variations.", "motivation": "Current sky background subtraction methods rely on average sky fiber spectra, which lack modeling of the environment surrounding objects, leading to inadequate sky subtraction.", "method": "Two-network approach: 1) wavelength calibration module extracts sky features and solves feature shift problems, 2) incremental training maximizes mutual information between spectra for common components and minimizes mutual information for individual components to obtain location-specific sky backgrounds.", "result": "Experiments on LAMOST spectra show SMI achieves better object sky background estimation, particularly improving performance in the blue end of the spectrum.", "conclusion": "SMI effectively addresses limitations of traditional sky subtraction by leveraging mutual information and incremental training to capture both global sky features and local environmental variations."}}
{"id": "2508.19944", "pdf": "https://arxiv.org/pdf/2508.19944", "abs": "https://arxiv.org/abs/2508.19944", "authors": ["Taebaek Hwang", "Minseo Kim", "Gisang Lee", "Seonuk Kim", "Hyunjun Eun"], "title": "KRETA: A Benchmark for Korean Reading and Reasoning in Text-Rich VQA Attuned to Diverse Visual Contexts", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Understanding and reasoning over text within visual contexts poses a\nsignificant challenge for Vision-Language Models (VLMs), given the complexity\nand diversity of real-world scenarios. To address this challenge, text-rich\nVisual Question Answering (VQA) datasets and benchmarks have emerged for\nhigh-resource languages like English. However, a critical gap persists for\nlow-resource languages such as Korean, where the lack of comprehensive\nbenchmarks hinders robust model evaluation and comparison. To bridge this gap,\nwe introduce KRETA, a benchmark for Korean Reading and rEasoning in Text-rich\nVQA Attuned to diverse visual contexts. KRETA facilitates an in-depth\nevaluation of both visual text understanding and reasoning capabilities, while\nalso supporting a multifaceted assessment across 15 domains and 26 image types.\nAdditionally, we introduce a semi-automated VQA generation pipeline\nspecifically optimized for text-rich settings, leveraging refined stepwise\nimage decomposition and a rigorous seven-metric evaluation protocol to ensure\ndata quality. While KRETA is tailored for Korean, we hope our adaptable and\nextensible pipeline will facilitate the development of similar benchmarks in\nother languages, thereby accelerating multilingual VLM research. The code and\ndataset for KRETA are available at https://github.com/tabtoyou/KRETA.", "AI": {"tldr": "KRETA is a new benchmark for Korean text-rich Visual Question Answering that addresses the lack of comprehensive evaluation tools for low-resource languages, featuring diverse visual contexts and a semi-automated data generation pipeline.", "motivation": "There is a critical gap in text-rich VQA benchmarks for low-resource languages like Korean, which hinders robust model evaluation and comparison in multilingual VLM research.", "method": "Developed a semi-automated VQA generation pipeline optimized for text-rich settings using refined stepwise image decomposition and a rigorous seven-metric evaluation protocol to ensure data quality.", "result": "KRETA benchmark supports in-depth evaluation of visual text understanding and reasoning capabilities across 15 domains and 26 image types, specifically tailored for Korean language.", "conclusion": "KRETA bridges the Korean language gap in text-rich VQA benchmarks and provides an adaptable pipeline that can facilitate similar benchmark development for other languages, accelerating multilingual VLM research."}}
{"id": "2508.19881", "pdf": "https://arxiv.org/pdf/2508.19881", "abs": "https://arxiv.org/abs/2508.19881", "authors": ["Narges Takhtkeshha", "Gabriele Mazzacca", "Fabio Remondino", "Juha Hyypp\u00e4", "Gottfried Mandlburger"], "title": "Multispectral LiDAR data for extracting tree points in urban and suburban areas", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Monitoring urban tree dynamics is vital for supporting greening policies and\nreducing risks to electrical infrastructure. Airborne laser scanning has\nadvanced large-scale tree management, but challenges remain due to complex\nurban environments and tree variability. Multispectral (MS) light detection and\nranging (LiDAR) improves this by capturing both 3D spatial and spectral data,\nenabling detailed mapping. This study explores tree point extraction using\nMS-LiDAR and deep learning (DL) models. Three state-of-the-art models are\nevaluated: Superpoint Transformer (SPT), Point Transformer V3 (PTv3), and Point\nTransformer V1 (PTv1). Results show the notable time efficiency and accuracy of\nSPT, with a mean intersection over union (mIoU) of 85.28%. The highest\ndetection accuracy is achieved by incorporating pseudo normalized difference\nvegetation index (pNDVI) with spatial data, reducing error rate by 10.61\npercentage points (pp) compared to using spatial information alone. These\nfindings highlight the potential of MS-LiDAR and DL to improve tree extraction\nand further tree inventories.", "AI": {"tldr": "This paper evaluates deep learning models for urban tree extraction using multispectral LiDAR data, finding that Superpoint Transformer with pNDVI integration achieves the best accuracy (85.28% mIoU) and efficiency.", "motivation": "Monitoring urban trees is crucial for greening policies and infrastructure safety, but complex urban environments pose challenges. Multispectral LiDAR offers both spatial and spectral data to improve tree mapping.", "method": "The study compares three deep learning models (Superpoint Transformer, Point Transformer V3, and Point Transformer V1) for tree point extraction using MS-LiDAR data, testing spatial data alone versus combined with pseudo NDVI.", "result": "SPT showed the best time efficiency and accuracy (85.28% mIoU). Adding pNDVI to spatial data reduced error rate by 10.61 percentage points compared to using spatial information alone.", "conclusion": "MS-LiDAR combined with deep learning, particularly SPT with pNDVI integration, significantly improves urban tree extraction accuracy and has strong potential for enhancing tree inventory systems."}}
{"id": "2508.19972", "pdf": "https://arxiv.org/pdf/2508.19972", "abs": "https://arxiv.org/abs/2508.19972", "authors": ["Seongheon Park", "Yixuan Li"], "title": "GLSim: Detecting Object Hallucinations in LVLMs via Global-Local Similarity", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Object hallucination in large vision-language models presents a significant\nchallenge to their safe deployment in real-world applications. Recent works\nhave proposed object-level hallucination scores to estimate the likelihood of\nobject hallucination; however, these methods typically adopt either a global or\nlocal perspective in isolation, which may limit detection reliability. In this\npaper, we introduce GLSim, a novel training-free object hallucination detection\nframework that leverages complementary global and local embedding similarity\nsignals between image and text modalities, enabling more accurate and reliable\nhallucination detection in diverse scenarios. We comprehensively benchmark\nexisting object hallucination detection methods and demonstrate that GLSim\nachieves superior detection performance, outperforming competitive baselines by\na significant margin.", "AI": {"tldr": "GLSim is a training-free object hallucination detection framework that combines global and local embedding similarity between image and text to improve detection reliability in vision-language models.", "motivation": "Object hallucination in large vision-language models poses safety risks for real-world deployment, and existing methods using either global or local perspectives in isolation limit detection reliability.", "method": "GLSim leverages complementary global and local embedding similarity signals between image and text modalities without requiring training, enabling more accurate hallucination detection.", "result": "GLSim achieves superior detection performance, significantly outperforming competitive baselines in comprehensive benchmarking of object hallucination detection methods.", "conclusion": "The proposed GLSim framework provides more accurate and reliable object hallucination detection by effectively combining global and local perspectives, addressing limitations of existing approaches."}}
{"id": "2508.19895", "pdf": "https://arxiv.org/pdf/2508.19895", "abs": "https://arxiv.org/abs/2508.19895", "authors": ["Ziyun Qian", "Runyu Xiao", "Shuyuan Tu", "Wei Xue", "Dingkang Yang", "Mingcheng Li", "Dongliang Kou", "Minghao Han", "Zizhi Chen", "Lihua Zhang"], "title": "PersonaAnimator: Personalized Motion Transfer from Unconstrained Videos", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in motion generation show remarkable progress. However,\nseveral limitations remain: (1) Existing pose-guided character motion transfer\nmethods merely replicate motion without learning its style characteristics,\nresulting in inexpressive characters. (2) Motion style transfer methods rely\nheavily on motion capture data, which is difficult to obtain. (3) Generated\nmotions sometimes violate physical laws. To address these challenges, this\npaper pioneers a new task: Video-to-Video Motion Personalization. We propose a\nnovel framework, PersonaAnimator, which learns personalized motion patterns\ndirectly from unconstrained videos. This enables personalized motion transfer.\nTo support this task, we introduce PersonaVid, the first video-based\npersonalized motion dataset. It contains 20 motion content categories and 120\nmotion style categories. We further propose a Physics-aware Motion Style\nRegularization mechanism to enforce physical plausibility in the generated\nmotions. Extensive experiments show that PersonaAnimator outperforms\nstate-of-the-art motion transfer methods and sets a new benchmark for the\nVideo-to-Video Motion Personalization task.", "AI": {"tldr": "PersonaAnimator: A novel framework for video-to-video motion personalization that learns personalized motion patterns from unconstrained videos, addressing limitations in existing motion transfer methods.", "motivation": "Existing methods have three key limitations: (1) pose-guided methods only replicate motion without learning style characteristics, (2) style transfer relies heavily on difficult-to-obtain motion capture data, and (3) generated motions sometimes violate physical laws.", "method": "Proposes PersonaAnimator framework that learns personalized motion patterns directly from unconstrained videos. Introduces PersonaVid dataset with 20 motion content and 120 style categories. Includes Physics-aware Motion Style Regularization for physical plausibility.", "result": "Extensive experiments show PersonaAnimator outperforms state-of-the-art motion transfer methods.", "conclusion": "Sets a new benchmark for Video-to-Video Motion Personalization task, enabling personalized motion transfer without relying on motion capture data."}}
{"id": "2508.19990", "pdf": "https://arxiv.org/pdf/2508.19990", "abs": "https://arxiv.org/abs/2508.19990", "authors": ["Xiaodong Cui", "A F M Saif", "Brian Kingsbury", "Tianyi Chen"], "title": "Self-Supervised Pre-Training with Equilibrium Constraints", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Self-supervised pre-training using unlabeled data is widely used in machine\nlearning. In this paper, we propose a new self-supervised pre-training approach\nto dealing with heterogeneous data. Instead of mixing all the data and\nminimizing the averaged global loss in the conventional way, we impose\nadditional equilibrium constraints to ensure that the models optimizes each\nsource of heterogeneous data to its local optima after $K$-step gradient\ndescent initialized from the model. We formulate this as a bilevel optimization\nproblem, and use the first-order approximation method to solve the problem. We\ndiscuss its connection to model-agnostic meta learning (MAML). Experiments are\ncarried out on self-supervised pre-training using multi-domain and multilingual\ndatasets, demonstrating that the proposed approach can significantly improve\nthe adaptivity of the self-supervised pre-trained model for the downstream\nsupervised fine-tuning tasks.", "AI": {"tldr": "A new self-supervised pre-training approach using bilevel optimization with equilibrium constraints to handle heterogeneous data, improving model adaptivity for downstream tasks.", "motivation": "Conventional self-supervised pre-training mixes all heterogeneous data and minimizes global loss, which may not optimize each data source effectively. The paper aims to ensure models reach local optima for each heterogeneous data source.", "method": "Formulate as bilevel optimization problem with equilibrium constraints, using first-order approximation method. Each data source is optimized to local optima after K-step gradient descent from the model initialization. Connected to model-agnostic meta learning (MAML).", "result": "Experiments on multi-domain and multilingual datasets show significant improvement in adaptivity of self-supervised pre-trained models for downstream supervised fine-tuning tasks.", "conclusion": "The proposed approach with equilibrium constraints effectively handles heterogeneous data in self-supervised pre-training, leading to better model adaptivity compared to conventional methods."}}
{"id": "2508.19905", "pdf": "https://arxiv.org/pdf/2508.19905", "abs": "https://arxiv.org/abs/2508.19905", "authors": ["Imad Ali Shah", "Jiarong Li", "Roshan George", "Tim Brophy", "Enda Ward", "Martin Glavin", "Edward Jones", "Brian Deegan"], "title": "Hyperspectral Sensors and Autonomous Driving: Technologies, Limitations, and Opportunities", "categories": ["cs.CV", "cs.ET"], "comment": "Submitted and under review at IEEE OJVT, August 2025", "summary": "Hyperspectral imaging (HSI) offers a transformative sensing modality for\nAdvanced Driver Assistance Systems (ADAS) and autonomous driving (AD)\napplications, enabling material-level scene understanding through fine spectral\nresolution beyond the capabilities of traditional RGB imaging. This paper\npresents the first comprehensive review of HSI for automotive applications,\nexamining the strengths, limitations, and suitability of current HSI\ntechnologies in the context of ADAS/AD. In addition to this qualitative review,\nwe analyze 216 commercially available HSI and multispectral imaging cameras,\nbenchmarking them against key automotive criteria: frame rate, spatial\nresolution, spectral dimensionality, and compliance with AEC-Q100 temperature\nstandards. Our analysis reveals a significant gap between HSI's demonstrated\nresearch potential and its commercial readiness. Only four cameras meet the\ndefined performance thresholds, and none comply with AEC-Q100 requirements. In\naddition, the paper reviews recent HSI datasets and applications, including\nsemantic segmentation for road surface classification, pedestrian separability,\nand adverse weather perception. Our review shows that current HSI datasets are\nlimited in terms of scale, spectral consistency, the number of spectral\nchannels, and environmental diversity, posing challenges for the development of\nperception algorithms and the adequate validation of HSI's true potential in\nADAS/AD applications. This review paper establishes the current state of HSI in\nautomotive contexts as of 2025 and outlines key research directions toward\npractical integration of spectral imaging in ADAS and autonomous systems.", "AI": {"tldr": "First comprehensive review of hyperspectral imaging for automotive applications, revealing significant gap between research potential and commercial readiness with only 4 cameras meeting performance thresholds and none compliant with automotive standards.", "motivation": "Hyperspectral imaging offers transformative sensing for ADAS/autonomous driving by enabling material-level scene understanding beyond RGB capabilities, but its practical implementation needs assessment.", "method": "Qualitative review of HSI technologies plus quantitative analysis of 216 commercial HSI/multispectral cameras benchmarked against automotive criteria (frame rate, spatial resolution, spectral dimensionality, AEC-Q100 compliance). Also reviewed recent HSI datasets and applications.", "result": "Only 4 cameras meet performance thresholds, none comply with AEC-Q100 requirements. Current HSI datasets are limited in scale, spectral consistency, channel count, and environmental diversity, posing challenges for algorithm development and validation.", "conclusion": "Establishes current state of HSI in automotive contexts as of 2025 and outlines key research directions needed for practical integration of spectral imaging in ADAS/autonomous systems."}}
{"id": "2508.19999", "pdf": "https://arxiv.org/pdf/2508.19999", "abs": "https://arxiv.org/abs/2508.19999", "authors": ["Ziniu Zhang", "Zhenshuo Zhang", "Dongyue Li", "Lu Wang", "Jennifer Dy", "Hongyang R. Zhang"], "title": "Linear-Time Demonstration Selection for In-Context Learning via Gradient Estimation", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "19 pages. To appear in EMNLP'25", "summary": "This paper introduces an algorithm to select demonstration examples for\nin-context learning of a query set. Given a set of $n$ examples, how can we\nquickly select $k$ out of $n$ to best serve as the conditioning for downstream\ninference? This problem has broad applications in prompt tuning and\nchain-of-thought reasoning. Since model weights remain fixed during in-context\nlearning, previous work has sought to design methods based on the similarity of\ntoken embeddings. This work proposes a new approach based on gradients of the\noutput taken in the input embedding space. Our approach estimates model outputs\nthrough a first-order approximation using the gradients. Then, we apply this\nestimation to multiple randomly sampled subsets. Finally, we aggregate the\nsampled subset outcomes to form an influence score for each demonstration, and\nselect $k$ most relevant examples. This procedure only requires pre-computing\nmodel outputs and gradients once, resulting in a linear-time algorithm relative\nto model and training set sizes. Extensive experiments across various models\nand datasets validate the efficiency of our approach. We show that the gradient\nestimation procedure yields approximations of full inference with less than\n$\\mathbf{1}\\%$ error across six datasets. This allows us to scale up subset\nselection that would otherwise run full inference by up to\n$\\mathbf{37.7}\\times$ on models with up to $34$ billion parameters, and\noutperform existing selection methods based on input embeddings by\n$\\mathbf{11}\\%$ on average.", "AI": {"tldr": "A gradient-based algorithm for efficiently selecting optimal demonstration examples for in-context learning, achieving 37.7x speedup and 11% better performance than embedding-based methods.", "motivation": "To address the challenge of quickly selecting the most relevant k examples from n candidates for in-context learning, as current methods based on token embedding similarity have limitations and full inference is computationally expensive.", "method": "Proposes a gradient-based approach that estimates model outputs through first-order approximation using gradients in input embedding space, samples multiple random subsets, aggregates influence scores, and selects top-k examples - requiring only one-time pre-computation of outputs and gradients.", "result": "Achieves less than 1% error in approximating full inference across six datasets, scales subset selection by up to 37.7x on models with 34B parameters, and outperforms embedding-based methods by 11% on average.", "conclusion": "The gradient estimation procedure provides an efficient and accurate method for demonstration example selection in in-context learning, enabling practical application in prompt tuning and chain-of-thought reasoning with significant computational savings."}}
{"id": "2508.19906", "pdf": "https://arxiv.org/pdf/2508.19906", "abs": "https://arxiv.org/abs/2508.19906", "authors": ["Moussa Kassem Sbeyti", "Nadja Klein", "Michelle Karg", "Christian Wirth", "Sahin Albayrak"], "title": "Streamlining the Development of Active Learning Methods in Real-World Object Detection", "categories": ["cs.CV"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Active learning (AL) for real-world object detection faces computational and\nreliability challenges that limit practical deployment. Developing new AL\nmethods requires training multiple detectors across iterations to compare\nagainst existing approaches. This creates high costs for autonomous driving\ndatasets where the training of one detector requires up to 282 GPU hours.\nAdditionally, AL method rankings vary substantially across validation sets,\ncompromising reliability in safety-critical transportation systems. We\nintroduce object-based set similarity ($\\mathrm{OSS}$), a metric that addresses\nthese challenges. $\\mathrm{OSS}$ (1) quantifies AL method effectiveness without\nrequiring detector training by measuring similarity between training sets and\ntarget domains using object-level features. This enables the elimination of\nineffective AL methods before training. Furthermore, $\\mathrm{OSS}$ (2) enables\nthe selection of representative validation sets for robust evaluation. We\nvalidate our similarity-based approach on three autonomous driving datasets\n(KITTI, BDD100K, CODA) using uncertainty-based AL methods as a case study with\ntwo detector architectures (EfficientDet, YOLOv3). This work is the first to\nunify AL training and evaluation strategies in object detection based on object\nsimilarity. $\\mathrm{OSS}$ is detector-agnostic, requires only labeled object\ncrops, and integrates with existing AL pipelines. This provides a practical\nframework for deploying AL in real-world applications where computational\nefficiency and evaluation reliability are critical. Code is available at\nhttps://mos-ks.github.io/publications/.", "AI": {"tldr": "OSS metric enables efficient AL method evaluation without detector training by measuring object-level similarity between training sets and target domains, addressing computational costs and reliability issues in autonomous driving object detection.", "motivation": "Active learning for object detection faces high computational costs (up to 282 GPU hours per detector) and unreliable method rankings across validation sets, limiting practical deployment in safety-critical autonomous driving systems.", "method": "Proposes object-based set similarity (OSS) metric that quantifies AL method effectiveness without requiring detector training by measuring similarity between training sets and target domains using object-level features. Also enables selection of representative validation sets.", "result": "Validated on three autonomous driving datasets (KITTI, BDD100K, CODA) using uncertainty-based AL methods with two detector architectures (EfficientDet, YOLOv3). OSS is detector-agnostic, requires only labeled object crops, and integrates with existing AL pipelines.", "conclusion": "OSS provides a practical framework for deploying AL in real-world applications by unifying AL training and evaluation strategies based on object similarity, addressing computational efficiency and evaluation reliability challenges."}}
{"id": "2508.20018", "pdf": "https://arxiv.org/pdf/2508.20018", "abs": "https://arxiv.org/abs/2508.20018", "authors": ["Quanfeng Lu", "Zhantao Ma", "Shuai Zhong", "Jin Wang", "Dahai Yu", "Michael K. Ng", "Ping Luo"], "title": "SWIRL: A Staged Workflow for Interleaved Reinforcement Learning in Mobile GUI Control", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.MA"], "comment": "28 pages, 12 figures", "summary": "The rapid advancement of large vision language models (LVLMs) and agent\nsystems has heightened interest in mobile GUI agents that can reliably\ntranslate natural language into interface operations. Existing single-agent\napproaches, however, remain limited by structural constraints. Although\nmulti-agent systems naturally decouple different competencies, recent progress\nin multi-agent reinforcement learning (MARL) has often been hindered by\ninefficiency and remains incompatible with current LVLM architectures. To\naddress these challenges, we introduce SWIRL, a staged workflow for interleaved\nreinforcement learning designed for multi-agent systems. SWIRL reformulates\nMARL into a sequence of single-agent reinforcement learning tasks, updating one\nagent at a time while keeping the others fixed. This formulation enables stable\ntraining and promotes efficient coordination across agents. Theoretically, we\nprovide a stepwise safety bound, a cross-round monotonic improvement theorem,\nand convergence guarantees on return, ensuring robust and principled\noptimization. In application to mobile GUI control, SWIRL instantiates a\nNavigator that converts language and screen context into structured plans, and\nan Interactor that grounds these plans into executable atomic actions.\nExtensive experiments demonstrate superior performance on both high-level and\nlow-level GUI benchmarks. Beyond GUI tasks, SWIRL also demonstrates strong\ncapability in multi-agent mathematical reasoning, underscoring its potential as\na general framework for developing efficient and robust multi-agent systems.", "AI": {"tldr": "SWIRL is a staged workflow for multi-agent reinforcement learning that reformulates MARL into sequential single-agent tasks, enabling stable training and efficient coordination for mobile GUI agents and other multi-agent applications.", "motivation": "Existing single-agent approaches for mobile GUI agents have structural limitations, while multi-agent reinforcement learning suffers from inefficiency and incompatibility with current large vision language model architectures.", "method": "SWIRL reformulates MARL into a sequence of single-agent reinforcement learning tasks, updating one agent at a time while keeping others fixed. It uses a Navigator to convert language and screen context into plans, and an Interactor to ground plans into executable actions.", "result": "Superior performance on both high-level and low-level GUI benchmarks, and strong capability in multi-agent mathematical reasoning, demonstrating robust optimization with theoretical guarantees.", "conclusion": "SWIRL provides a general framework for developing efficient and robust multi-agent systems with theoretical safety bounds, monotonic improvement guarantees, and convergence properties."}}
{"id": "2508.19909", "pdf": "https://arxiv.org/pdf/2508.19909", "abs": "https://arxiv.org/abs/2508.19909", "authors": ["Lechun You", "Zhonghua Wu", "Weide Liu", "Xulei Yang", "Jun Cheng", "Wei Zhou", "Bharadwaj Veeravalli", "Guosheng Lin"], "title": "Integrating SAM Supervision for 3D Weakly Supervised Point Cloud Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Current methods for 3D semantic segmentation propose training models with\nlimited annotations to address the difficulty of annotating large, irregular,\nand unordered 3D point cloud data. They usually focus on the 3D domain only,\nwithout leveraging the complementary nature of 2D and 3D data. Besides, some\nmethods extend original labels or generate pseudo labels to guide the training,\nbut they often fail to fully use these labels or address the noise within them.\nMeanwhile, the emergence of comprehensive and adaptable foundation models has\noffered effective solutions for segmenting 2D data. Leveraging this\nadvancement, we present a novel approach that maximizes the utility of sparsely\navailable 3D annotations by incorporating segmentation masks generated by 2D\nfoundation models. We further propagate the 2D segmentation masks into the 3D\nspace by establishing geometric correspondences between 3D scenes and 2D views.\nWe extend the highly sparse annotations to encompass the areas delineated by 3D\nmasks, thereby substantially augmenting the pool of available labels.\nFurthermore, we apply confidence- and uncertainty-based consistency\nregularization on augmentations of the 3D point cloud and select the reliable\npseudo labels, which are further spread on the 3D masks to generate more\nlabels. This innovative strategy bridges the gap between limited 3D annotations\nand the powerful capabilities of 2D foundation models, ultimately improving the\nperformance of 3D weakly supervised segmentation.", "AI": {"tldr": "A novel 3D semantic segmentation method that leverages 2D foundation models to augment sparse 3D annotations by propagating 2D segmentation masks into 3D space and using consistency regularization to generate reliable pseudo labels.", "motivation": "Current 3D segmentation methods don't leverage complementary 2D-3D data relationships and fail to fully utilize available labels or address label noise, while 2D foundation models offer powerful segmentation capabilities that could benefit 3D tasks.", "method": "Propagates 2D segmentation masks from foundation models into 3D space using geometric correspondences, extends sparse 3D annotations using 3D masks, applies confidence- and uncertainty-based consistency regularization on 3D point cloud augmentations, and selects reliable pseudo labels to generate more training data.", "result": "The approach substantially augments available labels and bridges the gap between limited 3D annotations and powerful 2D foundation models.", "conclusion": "This innovative strategy improves 3D weakly supervised segmentation performance by maximizing utility of sparse 3D annotations through integration with 2D foundation model capabilities."}}
{"id": "2508.20019", "pdf": "https://arxiv.org/pdf/2508.20019", "abs": "https://arxiv.org/abs/2508.20019", "authors": ["Ji Wang", "Kashing Chen", "Xinyuan Song", "Ke Zhang", "Lynn Ai", "Eric Yang", "Bill Shi"], "title": "Symphony: A Decentralized Multi-Agent Framework for Scalable Collective Intelligence", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.MA"], "comment": null, "summary": "Most existing Large Language Model (LLM)-based agent frameworks rely on\ncentralized orchestration, incurring high deployment costs, rigid communication\ntopologies, and limited adaptability. To address these challenges, we introduce\nSymphony, a decentralized multi-agent system which enables lightweight LLMs on\nconsumer-grade GPUs to coordinate. Symphony introduces three key mechanisms:\n(1) a decentralized ledger that records capabilities, (2) a Beacon-selection\nprotocol for dynamic task allocation, and (3) weighted result voting based on\nCoTs. This design forms a privacy-saving, scalable, and fault-tolerant\norchestration with low overhead. Empirically, Symphony outperforms existing\nbaselines on reasoning benchmarks, achieving substantial accuracy gains and\ndemonstrating robustness across models of varying capacities.", "AI": {"tldr": "Symphony is a decentralized multi-agent system that enables lightweight LLMs on consumer GPUs to coordinate through decentralized ledger, dynamic task allocation, and weighted voting, outperforming centralized approaches with better scalability and fault tolerance.", "motivation": "Existing LLM-based agent frameworks rely on centralized orchestration, which suffers from high deployment costs, rigid communication topologies, and limited adaptability.", "method": "Introduces three key mechanisms: (1) decentralized ledger for capability recording, (2) Beacon-selection protocol for dynamic task allocation, and (3) weighted result voting based on Chain-of-Thought reasoning.", "result": "Outperforms existing baselines on reasoning benchmarks with substantial accuracy gains and demonstrates robustness across models of varying capacities.", "conclusion": "Symphony provides a privacy-saving, scalable, and fault-tolerant orchestration with low overhead, enabling effective coordination of lightweight LLMs on consumer-grade hardware."}}
{"id": "2508.19927", "pdf": "https://arxiv.org/pdf/2508.19927", "abs": "https://arxiv.org/abs/2508.19927", "authors": ["Fayaz Ali", "Muhammad Zawish", "Steven Davy", "Radu Timofte"], "title": "WaveHiT-SR: Hierarchical Wavelet Network for Efficient Image Super-Resolution", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 5 figures", "summary": "Transformers have demonstrated promising performance in computer vision\ntasks, including image super-resolution (SR). The quadratic computational\ncomplexity of window self-attention mechanisms in many transformer-based SR\nmethods forces the use of small, fixed windows, limiting the receptive field.\nIn this paper, we propose a new approach by embedding the wavelet transform\nwithin a hierarchical transformer framework, called (WaveHiT-SR). First, using\nadaptive hierarchical windows instead of static small windows allows to capture\nfeatures across different levels and greatly improve the ability to model\nlong-range dependencies. Secondly, the proposed model utilizes wavelet\ntransforms to decompose images into multiple frequency subbands, allowing the\nnetwork to focus on both global and local features while preserving structural\ndetails. By progressively reconstructing high-resolution images through\nhierarchical processing, the network reduces computational complexity without\nsacrificing performance. The multi-level decomposition strategy enables the\nnetwork to capture fine-grained information in lowfrequency components while\nenhancing high-frequency textures. Through extensive experimentation, we\nconfirm the effectiveness and efficiency of our WaveHiT-SR. Our refined\nversions of SwinIR-Light, SwinIR-NG, and SRFormer-Light deliver cutting-edge SR\nresults, achieving higher efficiency with fewer parameters, lower FLOPs, and\nfaster speeds.", "AI": {"tldr": "WaveHiT-SR is a hierarchical transformer framework that embeds wavelet transforms for image super-resolution, using adaptive hierarchical windows and multi-frequency decomposition to capture long-range dependencies while reducing computational complexity.", "motivation": "Transformer-based SR methods suffer from quadratic computational complexity in window self-attention, forcing small fixed windows that limit receptive field and long-range dependency modeling.", "method": "Embed wavelet transform within hierarchical transformer framework with adaptive hierarchical windows; decompose images into multiple frequency subbands; progressively reconstruct high-resolution images through hierarchical processing.", "result": "Achieves state-of-the-art SR results with higher efficiency - fewer parameters, lower FLOPs, and faster speeds compared to SwinIR-Light, SwinIR-NG, and SRFormer-Light.", "conclusion": "WaveHiT-SR effectively addresses computational complexity limitations while improving performance through hierarchical processing and wavelet-based frequency decomposition for both global and local feature capture."}}
{"id": "2508.20032", "pdf": "https://arxiv.org/pdf/2508.20032", "abs": "https://arxiv.org/abs/2508.20032", "authors": ["Santosh Chapagain", "Shah Muhammad Hamdi", "Soukaina Filali Boubrahimi"], "title": "Pruning Strategies for Backdoor Defense in LLMs", "categories": ["cs.LG", "cs.CL"], "comment": "Accepted in CIKM '25: The 34th ACM International Conference on\n  Information and Knowledge Management Proceedings", "summary": "Backdoor attacks are a significant threat to the performance and integrity of\npre-trained language models. Although such models are routinely fine-tuned for\ndownstream NLP tasks, recent work shows they remain vulnerable to backdoor\nattacks that survive vanilla fine-tuning. These attacks are difficult to defend\nbecause end users typically lack knowledge of the attack triggers. Such attacks\nconsist of stealthy malicious triggers introduced through subtle syntactic or\nstylistic manipulations, which can bypass traditional detection and remain in\nthe model, making post-hoc purification essential. In this study, we explore\nwhether attention-head pruning can mitigate these threats without any knowledge\nof the trigger or access to a clean reference model. To this end, we design and\nimplement six pruning-based strategies: (i) gradient-based pruning, (ii)\nlayer-wise variance pruning, (iii) gradient-based pruning with structured L1/L2\nsparsification, (iv) randomized ensemble pruning, (v)\nreinforcement-learning-guided pruning, and (vi) Bayesian uncertainty pruning.\nEach method iteratively removes the least informative heads while monitoring\nvalidation accuracy to avoid over-pruning. Experimental evaluation shows that\ngradient-based pruning performs best while defending the syntactic triggers,\nwhereas reinforcement learning and Bayesian pruning better withstand stylistic\nattacks.", "AI": {"tldr": "Attention-head pruning strategies can effectively defend against backdoor attacks in pre-trained language models without trigger knowledge or clean reference models.", "motivation": "Backdoor attacks pose serious threats to language models and can survive standard fine-tuning, making post-hoc purification essential since users typically lack trigger knowledge.", "method": "Six pruning strategies: gradient-based pruning, layer-wise variance pruning, gradient-based with L1/L2 sparsification, randomized ensemble pruning, reinforcement-learning-guided pruning, and Bayesian uncertainty pruning. Each removes least informative heads while monitoring validation accuracy.", "result": "Gradient-based pruning performs best against syntactic triggers, while reinforcement learning and Bayesian pruning better withstand stylistic attacks.", "conclusion": "Attention-head pruning provides an effective defense mechanism against backdoor attacks in language models without requiring trigger knowledge or clean reference models."}}
{"id": "2508.20083", "pdf": "https://arxiv.org/pdf/2508.20083", "abs": "https://arxiv.org/abs/2508.20083", "authors": ["Yanbo Dai", "Zhenlan Ji", "Zongjie Li", "Kuan Li", "Shuai Wang"], "title": "Disabling Self-Correction in Retrieval-Augmented Generation via Stealthy Retriever Poisoning", "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has become a standard approach for\nimproving the reliability of large language models (LLMs). Prior work\ndemonstrates the vulnerability of RAG systems by misleading them into\ngenerating attacker-chosen outputs through poisoning the knowledge base.\nHowever, this paper uncovers that such attacks could be mitigated by the strong\n\\textit{self-correction ability (SCA)} of modern LLMs, which can reject false\ncontext once properly configured. This SCA poses a significant challenge for\nattackers aiming to manipulate RAG systems.\n  In contrast to previous poisoning methods, which primarily target the\nknowledge base, we introduce \\textsc{DisarmRAG}, a new poisoning paradigm that\ncompromises the retriever itself to suppress the SCA and enforce\nattacker-chosen outputs. This compromisation enables the attacker to\nstraightforwardly embed anti-SCA instructions into the context provided to the\ngenerator, thereby bypassing the SCA. To this end, we present a\ncontrastive-learning-based model editing technique that performs localized and\nstealthy edits, ensuring the retriever returns a malicious instruction only for\nspecific victim queries while preserving benign retrieval behavior. To further\nstrengthen the attack, we design an iterative co-optimization framework that\nautomatically discovers robust instructions capable of bypassing prompt-based\ndefenses. We extensively evaluate DisarmRAG across six LLMs and three QA\nbenchmarks. Our results show near-perfect retrieval of malicious instructions,\nwhich successfully suppress SCA and achieve attack success rates exceeding 90\\%\nunder diverse defensive prompts. Also, the edited retriever remains stealthy\nunder several detection methods, highlighting the urgent need for\nretriever-centric defenses.", "AI": {"tldr": "DisarmRAG is a new poisoning attack that targets the retriever in RAG systems to bypass LLMs' self-correction ability, achieving over 90% attack success by embedding anti-SCA instructions through contrastive-learning-based model editing.", "motivation": "Previous RAG poisoning attacks focused on knowledge base manipulation, but modern LLMs' strong self-correction ability can reject false context. This paper aims to overcome this defense by attacking the retriever component directly.", "method": "Uses contrastive-learning-based model editing to perform localized edits on the retriever, ensuring it returns malicious instructions only for specific queries while maintaining normal behavior otherwise. Includes iterative co-optimization to discover robust instructions that bypass prompt-based defenses.", "result": "Achieves near-perfect retrieval of malicious instructions, suppresses SCA effectively, and reaches attack success rates exceeding 90% across six LLMs and three QA benchmarks. The edited retriever remains stealthy against detection methods.", "conclusion": "DisarmRAG demonstrates a new vulnerability in RAG systems by compromising the retriever, highlighting the urgent need for retriever-centric defenses beyond traditional knowledge base protection."}}
{"id": "2508.19946", "pdf": "https://arxiv.org/pdf/2508.19946", "abs": "https://arxiv.org/abs/2508.19946", "authors": ["Gianluca Guzzetta"], "title": "Reimagining Image Segmentation using Active Contour: From Chan Vese Algorithm into a Proposal Novel Functional Loss Framework", "categories": ["cs.CV", "physics.med-ph"], "comment": "13 pages", "summary": "In this paper, we present a comprehensive study and analysis of the Chan-Vese\nalgorithm for image segmentation. We employ a discretized scheme derived from\nthe empirical study of the Chan-Vese model's functional energy and its partial\ndifferential equation based on its level set function. We provide a proof of\nthe results and an implementation using MATLAB. Leveraging modern computer\nvision methodologies, we propose a functional segmentation loss based on active\ncontours, utilizing pytorch.nn.ModuleLoss and a level set based on the\nChan-Vese algorithm. We compare our results with common computer vision\nsegmentation datasets and evaluate the performance of classical loss functions\nagainst our proposed method. All code and materials used are available at\nhttps://github.com/gguzzy/chan_vese_functional_loss.", "AI": {"tldr": "Study and analysis of Chan-Vese algorithm for image segmentation with discretized scheme, proof of results, MATLAB implementation, and proposed functional segmentation loss using active contours with PyTorch integration.", "motivation": "To provide comprehensive analysis of Chan-Vese algorithm and develop improved segmentation methods by leveraging modern computer vision techniques and comparing with classical approaches.", "method": "Discretized scheme from Chan-Vese model's functional energy and PDE analysis, MATLAB implementation, proposed functional segmentation loss using pytorch.nn.ModuleLoss and level set based on Chan-Vese algorithm.", "result": "Implementation and comparison with common computer vision segmentation datasets, evaluation of classical loss functions versus proposed method.", "conclusion": "The study provides comprehensive analysis and improved segmentation approach using Chan-Vese algorithm with modern deep learning integration, with all code and materials made publicly available."}}
{"id": "2508.19967", "pdf": "https://arxiv.org/pdf/2508.19967", "abs": "https://arxiv.org/abs/2508.19967", "authors": ["Oliver Grainge", "Sania Waheed", "Jack Stilgoe", "Michael Milford", "Shoaib Ehsan"], "title": "Assessing the Geolocation Capabilities, Limitations and Societal Risks of Generative Vision-Language Models", "categories": ["cs.CV"], "comment": "Accepted to AAAI Fall Symposium 2025 on AI Trustworthiness and Risk\n  Assessment for Challenging Contexts (ATRACC)", "summary": "Geo-localization is the task of identifying the location of an image using\nvisual cues alone. It has beneficial applications, such as improving disaster\nresponse, enhancing navigation, and geography education. Recently,\nVision-Language Models (VLMs) are increasingly demonstrating capabilities as\naccurate image geo-locators. This brings significant privacy risks, including\nthose related to stalking and surveillance, considering the widespread uses of\nAI models and sharing of photos on social media. The precision of these models\nis likely to improve in the future. Despite these risks, there is little work\non systematically evaluating the geolocation precision of Generative VLMs,\ntheir limits and potential for unintended inferences. To bridge this gap, we\nconduct a comprehensive assessment of the geolocation capabilities of 25\nstate-of-the-art VLMs on four benchmark image datasets captured in diverse\nenvironments. Our results offer insight into the internal reasoning of VLMs and\nhighlight their strengths, limitations, and potential societal risks. Our\nfindings indicate that current VLMs perform poorly on generic street-level\nimages yet achieve notably high accuracy (61\\%) on images resembling social\nmedia content, raising significant and urgent privacy concerns.", "AI": {"tldr": "Comprehensive assessment of 25 state-of-the-art Vision-Language Models' geolocation capabilities reveals concerning privacy risks, with VLMs achieving 61% accuracy on social media-style images despite poor performance on generic street-level images.", "motivation": "Growing privacy concerns as Vision-Language Models demonstrate accurate image geo-location capabilities, with potential risks for stalking and surveillance given widespread AI model usage and social media photo sharing.", "method": "Conducted comprehensive assessment of 25 state-of-the-art VLMs on four benchmark image datasets captured in diverse environments to evaluate geolocation precision, limits, and unintended inference potential.", "result": "Current VLMs perform poorly on generic street-level images but achieve notably high accuracy (61%) on images resembling social media content, raising significant privacy concerns.", "conclusion": "The study highlights urgent privacy risks from VLMs' geolocation capabilities, particularly for social media images, and calls for attention to the societal implications of these AI capabilities."}}
{"id": "2508.20020", "pdf": "https://arxiv.org/pdf/2508.20020", "abs": "https://arxiv.org/abs/2508.20020", "authors": ["Yuhao Chen", "Shubin Chen", "Liang Lin", "Guangrun Wang"], "title": "GS: Generative Segmentation via Label Diffusion", "categories": ["cs.CV"], "comment": "12 pages, 7 figures, 5 tables", "summary": "Language-driven image segmentation is a fundamental task in vision-language\nunderstanding, requiring models to segment regions of an image corresponding to\nnatural language expressions. Traditional methods approach this as a\ndiscriminative problem, assigning each pixel to foreground or background based\non semantic alignment. Recently, diffusion models have been introduced to this\ndomain, but existing approaches remain image-centric: they either (i) use image\ndiffusion models as visual feature extractors, (ii) synthesize segmentation\ndata via image generation to train discriminative models, or (iii) perform\ndiffusion inversion to extract attention cues from pre-trained image diffusion\nmodels-thereby treating segmentation as an auxiliary process. In this paper, we\npropose GS (Generative Segmentation), a novel framework that formulates\nsegmentation itself as a generative task via label diffusion. Instead of\ngenerating images conditioned on label maps and text, GS reverses the\ngenerative process: it directly generates segmentation masks from noise,\nconditioned on both the input image and the accompanying language description.\nThis paradigm makes label generation the primary modeling target, enabling\nend-to-end training with explicit control over spatial and semantic fidelity.\nTo demonstrate the effectiveness of our approach, we evaluate GS on Panoptic\nNarrative Grounding (PNG), a representative and challenging benchmark for\nmultimodal segmentation that requires panoptic-level reasoning guided by\nnarrative captions. Experimental results show that GS significantly outperforms\nexisting discriminative and diffusion-based methods, setting a new\nstate-of-the-art for language-driven segmentation.", "AI": {"tldr": "GS (Generative Segmentation) formulates image segmentation as a generative task using label diffusion, directly generating segmentation masks from noise conditioned on both image and language input, achieving state-of-the-art performance on Panoptic Narrative Grounding.", "motivation": "Traditional discriminative segmentation methods treat segmentation as pixel classification, while existing diffusion approaches remain image-centric. The authors propose to make segmentation itself the primary generative task rather than an auxiliary process.", "method": "GS reverses the typical generative process - instead of generating images from label maps, it directly generates segmentation masks from noise conditioned on both input image and language description using label diffusion.", "result": "GS significantly outperforms existing discriminative and diffusion-based methods on Panoptic Narrative Grounding benchmark, setting new state-of-the-art for language-driven segmentation.", "conclusion": "Formulating segmentation as a generative task via label diffusion provides explicit control over spatial and semantic fidelity, enabling end-to-end training and superior performance compared to traditional discriminative approaches."}}
{"id": "2508.20029", "pdf": "https://arxiv.org/pdf/2508.20029", "abs": "https://arxiv.org/abs/2508.20029", "authors": ["Manogna Sreenivas", "Soma Biswas"], "title": "Segmentation Assisted Incremental Test Time Adaptation in an Open World", "categories": ["cs.CV"], "comment": "Accepted at BMVC 2025", "summary": "In dynamic environments, unfamiliar objects and distribution shifts are often\nencountered, which challenge the generalization abilities of the deployed\ntrained models. This work addresses Incremental Test Time Adaptation of Vision\nLanguage Models, tackling scenarios where unseen classes and unseen domains\ncontinuously appear during testing. Unlike traditional Test Time Adaptation\napproaches, where the test stream comes only from a predefined set of classes,\nour framework allows models to adapt simultaneously to both covariate and label\nshifts, actively incorporating new classes as they emerge. Towards this goal,\nwe establish a new benchmark for ITTA, integrating single image TTA methods for\nVLMs with active labeling techniques that query an oracle for samples\npotentially representing unseen classes during test time. We propose a\nsegmentation assisted active labeling module, termed SegAssist, which is\ntraining free and repurposes the segmentation capabilities of VLMs to refine\nactive sample selection, prioritizing samples likely to belong to unseen\nclasses. Extensive experiments on several benchmark datasets demonstrate the\npotential of SegAssist to enhance the performance of VLMs in real world\nscenarios, where continuous adaptation to emerging data is essential.\nProject-page:https://manogna-s.github.io/segassist/", "AI": {"tldr": "A framework called SegAssist for Vision Language Models to adapt to both unseen classes and domains during testing through active labeling and segmentation-assisted sample selection.", "motivation": "Address the challenge of unfamiliar objects and distribution shifts in dynamic environments where traditional Test Time Adaptation methods fail to handle both covariate and label shifts simultaneously.", "method": "Proposes SegAssist - a training-free segmentation assisted active labeling module that repurposes VLMs' segmentation capabilities to refine active sample selection, prioritizing samples likely from unseen classes by querying an oracle.", "result": "Extensive experiments on benchmark datasets demonstrate SegAssist's effectiveness in enhancing VLM performance in real-world scenarios requiring continuous adaptation.", "conclusion": "SegAssist provides a practical solution for incremental test time adaptation, enabling VLMs to better handle emerging unseen classes and domains during deployment."}}
{"id": "2508.20063", "pdf": "https://arxiv.org/pdf/2508.20063", "abs": "https://arxiv.org/abs/2508.20063", "authors": ["Peng-Hao Hsu", "Ke Zhang", "Fu-En Wang", "Tao Tu", "Ming-Feng Li", "Yu-Lun Liu", "Albert Y. C. Chen", "Min Sun", "Cheng-Hao Kuo"], "title": "OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotations", "categories": ["cs.CV"], "comment": "ICCV2025", "summary": "Open-vocabulary (OV) 3D object detection is an emerging field, yet its\nexploration through image-based methods remains limited compared to 3D point\ncloud-based methods. We introduce OpenM3D, a novel open-vocabulary multi-view\nindoor 3D object detector trained without human annotations. In particular,\nOpenM3D is a single-stage detector adapting the 2D-induced voxel features from\nthe ImGeoNet model. To support OV, it is jointly trained with a class-agnostic\n3D localization loss requiring high-quality 3D pseudo boxes and a\nvoxel-semantic alignment loss requiring diverse pre-trained CLIP features. We\nfollow the training setting of OV-3DET where posed RGB-D images are given but\nno human annotations of 3D boxes or classes are available. We propose a 3D\nPseudo Box Generation method using a graph embedding technique that combines 2D\nsegments into coherent 3D structures. Our pseudo-boxes achieve higher precision\nand recall than other methods, including the method proposed in OV-3DET. We\nfurther sample diverse CLIP features from 2D segments associated with each\ncoherent 3D structure to align with the corresponding voxel feature. The key to\ntraining a highly accurate single-stage detector requires both losses to be\nlearned toward high-quality targets. At inference, OpenM3D, a highly efficient\ndetector, requires only multi-view images for input and demonstrates superior\naccuracy and speed (0.3 sec. per scene) on ScanNet200 and ARKitScenes indoor\nbenchmarks compared to existing methods. We outperform a strong two-stage\nmethod that leverages our class-agnostic detector with a ViT CLIP-based OV\nclassifier and a baseline incorporating multi-view depth estimator on both\naccuracy and speed.", "AI": {"tldr": "OpenM3D is a novel open-vocabulary multi-view indoor 3D object detector trained without human annotations, using 2D-induced voxel features and achieving superior accuracy and speed compared to existing methods.", "motivation": "Open-vocabulary 3D object detection through image-based methods remains limited compared to 3D point cloud-based methods, creating a need for efficient annotation-free approaches.", "method": "Single-stage detector adapting 2D-induced voxel features from ImGeoNet, jointly trained with class-agnostic 3D localization loss and voxel-semantic alignment loss using CLIP features. Uses 3D pseudo box generation with graph embedding to combine 2D segments into coherent 3D structures.", "result": "Achieves higher precision and recall than other methods, including OV-3DET. Demonstrates superior accuracy and speed (0.3 sec per scene) on ScanNet200 and ARKitScenes benchmarks, outperforming strong two-stage methods.", "conclusion": "OpenM3D provides an efficient, annotation-free solution for open-vocabulary 3D object detection that combines high-quality pseudo-box generation with CLIP feature alignment for superior performance."}}
{"id": "2508.20064", "pdf": "https://arxiv.org/pdf/2508.20064", "abs": "https://arxiv.org/abs/2508.20064", "authors": ["Philippe Zhang", "Weili Jiang", "Yihao Li", "Jing Zhang", "Sarah Matta", "Yubo Tan", "Hui Lin", "Haoshen Wang", "Jiangtian Pan", "Hui Xu", "Laurent Borderie", "Alexandre Le Guilcher", "B\u00e9atrice Cochener", "Chubin Ou", "Gwenol\u00e9 Quellec", "Mathieu Lamard"], "title": "Patch Progression Masked Autoencoder with Fusion CNN Network for Classifying Evolution Between Two Pairs of 2D OCT Slices", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 5 figures, 3 tables, challenge/conference paper", "summary": "Age-related Macular Degeneration (AMD) is a prevalent eye condition affecting\nvisual acuity. Anti-vascular endothelial growth factor (anti-VEGF) treatments\nhave been effective in slowing the progression of neovascular AMD, with better\noutcomes achieved through timely diagnosis and consistent monitoring. Tracking\nthe progression of neovascular activity in OCT scans of patients with exudative\nAMD allows for the development of more personalized and effective treatment\nplans. This was the focus of the Monitoring Age-related Macular Degeneration\nProgression in Optical Coherence Tomography (MARIO) challenge, in which we\nparticipated. In Task 1, which involved classifying the evolution between two\npairs of 2D slices from consecutive OCT acquisitions, we employed a fusion CNN\nnetwork with model ensembling to further enhance the model's performance. For\nTask 2, which focused on predicting progression over the next three months\nbased on current exam data, we proposed the Patch Progression Masked\nAutoencoder that generates an OCT for the next exam and then classifies the\nevolution between the current OCT and the one generated using our solution from\nTask 1. The results we achieved allowed us to place in the Top 10 for both\ntasks. Some team members are part of the same organization as the challenge\norganizers; therefore, we are not eligible to compete for the prize.", "AI": {"tldr": "Top 10 performance in MARIO challenge for AMD progression tracking using fusion CNN and masked autoencoder approaches for OCT scan analysis.", "motivation": "AMD requires timely diagnosis and monitoring for effective anti-VEGF treatment. Tracking neovascular activity progression in OCT scans enables personalized treatment plans.", "method": "Task 1: Fusion CNN network with model ensembling for classifying evolution between OCT slice pairs. Task 2: Patch Progression Masked Autoencoder that generates future OCT scans and classifies evolution using Task 1 solution.", "result": "Achieved Top 10 ranking in both tasks of the MARIO challenge, though ineligible for prizes due to organizational affiliations.", "conclusion": "The proposed methods effectively track AMD progression in OCT scans, demonstrating potential for improved personalized treatment monitoring in clinical practice."}}
{"id": "2508.20066", "pdf": "https://arxiv.org/pdf/2508.20066", "abs": "https://arxiv.org/abs/2508.20066", "authors": ["Zheng Li", "Yanming Guo", "WenZhe Liu", "Xueyi Zhang", "Zhaoyun Ding", "Long Xu", "Mingrui Lao"], "title": "PAUL: Uncertainty-Guided Partition and Augmentation for Robust Cross-View Geo-Localization under Noisy Correspondence", "categories": ["cs.CV"], "comment": "10 pages", "summary": "Cross-view geo-localization is a critical task for UAV navigation, event\ndetection, and aerial surveying, as it enables matching between drone-captured\nand satellite imagery. Most existing approaches embed multi-modal data into a\njoint feature space to maximize the similarity of paired images. However, these\nmethods typically assume perfect alignment of image pairs during training,\nwhich rarely holds true in real-world scenarios. In practice, factors such as\nurban canyon effects, electromagnetic interference, and adverse weather\nfrequently induce GPS drift, resulting in systematic alignment shifts where\nonly partial correspondences exist between pairs. Despite its prevalence, this\nsource of noisy correspondence has received limited attention in current\nresearch. In this paper, we formally introduce and address the Noisy\nCorrespondence on Cross-View Geo-Localization (NC-CVGL) problem, aiming to\nbridge the gap between idealized benchmarks and practical applications. To this\nend, we propose PAUL (Partition and Augmentation by Uncertainty Learning), a\nnovel framework that partitions and augments training data based on estimated\ndata uncertainty through uncertainty-aware co-augmentation and evidential\nco-training. Specifically, PAUL selectively augments regions with high\ncorrespondence confidence and utilizes uncertainty estimation to refine feature\nlearning, effectively suppressing noise from misaligned pairs. Distinct from\ntraditional filtering or label correction, PAUL leverages both data uncertainty\nand loss discrepancy for targeted partitioning and augmentation, thus providing\nrobust supervision for noisy samples. Comprehensive experiments validate the\neffectiveness of individual components in PAUL,which consistently achieves\nsuperior performance over other competitive noisy-correspondence-driven methods\nin various noise ratios.", "AI": {"tldr": "PAUL framework addresses noisy correspondence in cross-view geo-localization by using uncertainty learning to partition and augment training data, achieving superior performance in handling GPS drift and misalignment issues.", "motivation": "Existing cross-view geo-localization methods assume perfect image pair alignment during training, but real-world factors like GPS drift and urban canyon effects create systematic alignment shifts with only partial correspondences, which current research largely ignores.", "method": "Proposes PAUL (Partition and Augmentation by Uncertainty Learning) - a framework that uses uncertainty-aware co-augmentation and evidential co-training to partition training data based on estimated uncertainty, selectively augmenting high-confidence regions and refining feature learning to suppress noise from misaligned pairs.", "result": "Comprehensive experiments show PAUL consistently achieves superior performance over other noisy-correspondence-driven methods across various noise ratios, validating the effectiveness of its individual components.", "conclusion": "PAUL successfully bridges the gap between idealized benchmarks and practical applications in cross-view geo-localization by effectively handling noisy correspondence through uncertainty-based partitioning and augmentation, providing robust supervision for noisy samples."}}
{"id": "2508.20072", "pdf": "https://arxiv.org/pdf/2508.20072", "abs": "https://arxiv.org/abs/2508.20072", "authors": ["Zhixuan Liang", "Yizhuo Li", "Tianshuo Yang", "Chengyue Wu", "Sitong Mao", "Liuao Pei", "Xiaokang Yang", "Jiangmiao Pang", "Yao Mu", "Ping Luo"], "title": "Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": "15 pages", "summary": "Vision-Language-Action (VLA) models adapt large vision-language backbones to\nmap images and instructions to robot actions. However, prevailing VLA decoders\neither generate actions autoregressively in a fixed left-to-right order or\nattach continuous diffusion or flow matching heads outside the backbone,\ndemanding specialized training and iterative sampling that hinder a unified,\nscalable architecture. We present Discrete Diffusion VLA, a single-transformer\npolicy that models discretized action chunks with discrete diffusion and is\ntrained with the same cross-entropy objective as the VLM backbone. The design\nretains diffusion's progressive refinement paradigm while remaining natively\ncompatible with the discrete token interface of VLMs. Our method achieves an\nadaptive decoding order that resolves easy action elements before harder ones\nand uses secondary remasking to revisit uncertain predictions across refinement\nrounds, which improves consistency and enables robust error correction. This\nunified decoder preserves pretrained vision language priors, supports parallel\ndecoding, breaks the autoregressive bottleneck, and reduces the number of\nfunction evaluations. Discrete Diffusion VLA achieves 96.3% avg. SR on LIBERO,\n71.2% visual matching on SimplerEnv Fractal and 49.3% overall on SimplerEnv\nBridge, improving over both autoregressive and continuous diffusion baselines.\nThese findings indicate that discrete-diffusion action decoder supports precise\naction modeling and consistent training, laying groundwork for scaling VLA to\nlarger models and datasets.", "AI": {"tldr": "Discrete Diffusion VLA is a unified transformer policy that uses discrete diffusion to model robot actions, achieving adaptive decoding order and robust error correction while maintaining compatibility with vision-language models.", "motivation": "Current VLA models either use fixed autoregressive decoding or continuous diffusion heads that require specialized training and iterative sampling, lacking a unified scalable architecture.", "method": "Single-transformer policy that models discretized action chunks with discrete diffusion, trained with cross-entropy objective. Uses adaptive decoding order and secondary remasking for error correction.", "result": "Achieves 96.3% avg. SR on LIBERO, 71.2% visual matching on SimplerEnv Fractal, and 49.3% overall on SimplerEnv Bridge, outperforming both autoregressive and continuous diffusion baselines.", "conclusion": "Discrete diffusion action decoder enables precise action modeling and consistent training, providing foundation for scaling VLA to larger models and datasets."}}
{"id": "2508.20080", "pdf": "https://arxiv.org/pdf/2508.20080", "abs": "https://arxiv.org/abs/2508.20080", "authors": ["Changha Shin", "Woong Oh Cho", "Seon Joo Kim"], "title": "Seam360GS: Seamless 360\u00b0 Gaussian Splatting from Real-World Omnidirectional Images", "categories": ["cs.CV", "cs.GR"], "comment": "Accepted to ICCV 2025. 10 pages main text, 4 figures, 4 tables,\n  supplementary material included", "summary": "360-degree visual content is widely shared on platforms such as YouTube and\nplays a central role in virtual reality, robotics, and autonomous navigation.\nHowever, consumer-grade dual-fisheye systems consistently yield imperfect\npanoramas due to inherent lens separation and angular distortions. In this\nwork, we introduce a novel calibration framework that incorporates a\ndual-fisheye camera model into the 3D Gaussian splatting pipeline. Our approach\nnot only simulates the realistic visual artifacts produced by dual-fisheye\ncameras but also enables the synthesis of seamlessly rendered 360-degree\nimages. By jointly optimizing 3D Gaussian parameters alongside calibration\nvariables that emulate lens gaps and angular distortions, our framework\ntransforms imperfect omnidirectional inputs into flawless novel view synthesis.\nExtensive evaluations on real-world datasets confirm that our method produces\nseamless renderings-even from imperfect images-and outperforms existing\n360-degree rendering models.", "AI": {"tldr": "Novel calibration framework that integrates dual-fisheye camera model into 3D Gaussian splatting to fix imperfect 360-degree panoramas from consumer cameras.", "motivation": "Consumer dual-fisheye systems produce imperfect 360-degree panoramas due to lens separation and angular distortions, limiting quality for VR, robotics, and autonomous navigation applications.", "method": "Joint optimization of 3D Gaussian parameters with calibration variables that simulate lens gaps and angular distortions within the Gaussian splatting pipeline.", "result": "Produces seamless 360-degree renderings from imperfect inputs and outperforms existing 360-degree rendering models on real-world datasets.", "conclusion": "The framework successfully transforms flawed omnidirectional inputs into high-quality novel view synthesis, addressing inherent limitations of consumer dual-fisheye camera systems."}}
{"id": "2508.20088", "pdf": "https://arxiv.org/pdf/2508.20088", "abs": "https://arxiv.org/abs/2508.20088", "authors": ["Yuxin Guo", "Teng Wang", "Yuying Ge", "Shijie Ma", "Yixiao Ge", "Wei Zou", "Ying Shan"], "title": "AudioStory: Generating Long-Form Narrative Audio with Large Language Models", "categories": ["cs.CV", "cs.MM", "cs.SD"], "comment": null, "summary": "Recent advances in text-to-audio (TTA) generation excel at synthesizing short\naudio clips but struggle with long-form narrative audio, which requires\ntemporal coherence and compositional reasoning. To address this gap, we propose\nAudioStory, a unified framework that integrates large language models (LLMs)\nwith TTA systems to generate structured, long-form audio narratives. AudioStory\npossesses strong instruction-following reasoning generation capabilities. It\nemploys LLMs to decompose complex narrative queries into temporally ordered\nsub-tasks with contextual cues, enabling coherent scene transitions and\nemotional tone consistency. AudioStory has two appealing features: (1)\nDecoupled bridging mechanism: AudioStory disentangles LLM-diffuser\ncollaboration into two specialized components, i.e., a bridging query for\nintra-event semantic alignment and a residual query for cross-event coherence\npreservation. (2) End-to-end training: By unifying instruction comprehension\nand audio generation within a single end-to-end framework, AudioStory\neliminates the need for modular training pipelines while enhancing synergy\nbetween components. Furthermore, we establish a benchmark AudioStory-10K,\nencompassing diverse domains such as animated soundscapes and natural sound\nnarratives. Extensive experiments show the superiority of AudioStory on both\nsingle-audio generation and narrative audio generation, surpassing prior TTA\nbaselines in both instruction-following ability and audio fidelity. Our code is\navailable at https://github.com/TencentARC/AudioStory", "AI": {"tldr": "AudioStory is a unified framework that combines LLMs with text-to-audio systems to generate coherent long-form audio narratives, addressing limitations of current TTA methods in handling temporal coherence and compositional reasoning.", "motivation": "Current text-to-audio generation systems excel at short audio clips but struggle with long-form narrative audio that requires temporal coherence, scene transitions, and emotional consistency over extended durations.", "method": "Uses LLMs to decompose narrative queries into temporally ordered sub-tasks with contextual cues. Features a decoupled bridging mechanism with specialized components for intra-event semantic alignment and cross-event coherence preservation, all within an end-to-end training framework.", "result": "AudioStory outperforms prior TTA baselines in both instruction-following ability and audio fidelity. The framework successfully generates structured long-form audio narratives across diverse domains including animated soundscapes and natural sound narratives.", "conclusion": "AudioStory provides an effective solution for long-form audio narrative generation by integrating LLM reasoning with TTA systems, demonstrating superior performance and establishing a new benchmark (AudioStory-10K) for the field."}}
{"id": "2508.20089", "pdf": "https://arxiv.org/pdf/2508.20089", "abs": "https://arxiv.org/abs/2508.20089", "authors": ["Ross J Gardiner", "Guillaume Mougeot", "Sareh Rowlands", "Benno I Simmons", "Flemming Helsing", "Toke Thomas H\u00f8ye"], "title": "Bridging Domain Gaps for Fine-Grained Moth Classification Through Expert-Informed Adaptation and Foundation Model Priors", "categories": ["cs.CV"], "comment": null, "summary": "Labelling images of Lepidoptera (moths) from automated camera systems is\nvital for understanding insect declines. However, accurate species\nidentification is challenging due to domain shifts between curated images and\nnoisy field imagery. We propose a lightweight classification approach,\ncombining limited expert-labelled field data with knowledge distillation from\nthe high-performance BioCLIP2 foundation model into a ConvNeXt-tiny\narchitecture. Experiments on 101 Danish moth species from AMI camera systems\ndemonstrate that BioCLIP2 substantially outperforms other methods and that our\ndistilled lightweight model achieves comparable accuracy with significantly\nreduced computational cost. These insights offer practical guidelines for the\ndevelopment of efficient insect monitoring systems and bridging domain gaps for\nfine-grained classification.", "AI": {"tldr": "Lightweight moth classification using BioCLIP2 knowledge distillation achieves comparable accuracy to foundation models with reduced computational cost, enabling efficient insect monitoring.", "motivation": "Accurate moth species identification from automated camera systems is crucial for monitoring insect declines, but challenging due to domain shifts between curated and field imagery.", "method": "Combines limited expert-labelled field data with knowledge distillation from BioCLIP2 foundation model into a ConvNeXt-tiny architecture for lightweight classification.", "result": "BioCLIP2 substantially outperforms other methods, and the distilled lightweight model achieves comparable accuracy with significantly reduced computational cost on 101 Danish moth species.", "conclusion": "Provides practical guidelines for developing efficient insect monitoring systems and bridging domain gaps for fine-grained classification tasks."}}
{"id": "2508.20096", "pdf": "https://arxiv.org/pdf/2508.20096", "abs": "https://arxiv.org/abs/2508.20096", "authors": ["Zeyi Sun", "Yuhang Cao", "Jianze Liang", "Qiushi Sun", "Ziyu Liu", "Zhixiong Zhang", "Yuhang Zang", "Xiaoyi Dong", "Kai Chen", "Dahua Lin", "Jiaqi Wang"], "title": "CODA: Coordinating the Cerebrum and Cerebellum for a Dual-Brain Computer Use Agent with Decoupled Reinforcement Learning", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "code available at this url: https://github.com/OpenIXCLab/CODA", "summary": "Autonomous agents for Graphical User Interfaces (GUIs) face significant\nchallenges in specialized domains such as scientific computing, where both\nlong-horizon planning and precise execution are required. Existing approaches\nsuffer from a trade-off: generalist agents excel at planning but perform poorly\nin execution, while specialized agents demonstrate the opposite weakness.\nRecent compositional frameworks attempt to bridge this gap by combining a\nplanner and an actor, but they are typically static and non-trainable, which\nprevents adaptation from experience. This is a critical limitation given the\nscarcity of high-quality data in scientific domains. To address these\nlimitations, we introduce CODA, a novel and trainable compositional framework\nthat integrates a generalist planner (Cerebrum) with a specialist executor\n(Cerebellum), trained via a dedicated two-stage pipeline. In the first stage,\nSpecialization, we apply a decoupled GRPO approach to train an expert planner\nfor each scientific application individually, bootstrapping from a small set of\ntask trajectories. In the second stage, Generalization, we aggregate all\nsuccessful trajectories from the specialized experts to build a consolidated\ndataset, which is then used for supervised fine-tuning of the final planner.\nThis equips CODA with both robust execution and cross-domain generalization.\nEvaluated on four challenging applications from the ScienceBoard benchmark,\nCODA significantly outperforms baselines and establishes a new state of the art\namong open-source models.", "AI": {"tldr": "CODA is a trainable compositional framework that combines a generalist planner with specialist executors for GUI automation in scientific computing, using a two-stage training approach to achieve both robust execution and cross-domain generalization.", "motivation": "Address the trade-off between generalist agents (good at planning but poor execution) and specialized agents (good execution but poor planning) in GUI automation for scientific computing domains where high-quality data is scarce.", "method": "Two-stage training pipeline: 1) Specialization - decoupled GRPO approach to train expert planners for each application individually, 2) Generalization - aggregate successful trajectories for supervised fine-tuning of final planner. Combines generalist planner (Cerebrum) with specialist executor (Cerebellum).", "result": "Significantly outperforms baselines and establishes new state-of-the-art among open-source models on four challenging applications from ScienceBoard benchmark.", "conclusion": "CODA successfully bridges the planning-execution gap in GUI automation for scientific domains through its trainable compositional framework and two-stage training approach, enabling both robust execution and cross-domain generalization."}}
{"id": "2508.18404", "pdf": "https://arxiv.org/pdf/2508.18404", "abs": "https://arxiv.org/abs/2508.18404", "authors": ["Alex Szorkovszky", "Rujeena Mathema", "Pedro Lencastre", "Pedro Lind", "Anis Yazidi"], "title": "Saccade crossing avoidance as a visual search strategy", "categories": ["q-bio.NC", "cs.CV", "q-bio.QM"], "comment": "Main text: 11 pages, 4 figures; Supplementary info: 12 pages, 9\n  figures", "summary": "Although visual search appears largely random, several oculomotor biases\nexist such that the likelihoods of saccade directions and lengths depend on the\nprevious scan path. Compared to the most recent fixations, the impact of the\nlonger path history is more difficult to quantify. Using the step-selection\nframework commonly used in movement ecology, and analyzing data from 45-second\nviewings of \"Where's Waldo\"?, we report a new memory-dependent effect that also\nvaries significantly between individuals, which we term self-crossing\navoidance. This is a tendency for saccades to avoid crossing those earlier in\nthe scan path, and is most evident when both have small amplitudes. We show\nthis by comparing real data to synthetic data generated from a memoryless\napproximation of the spatial statistics (i.e. a Markovian nonparametric model\nwith a matching distribution of saccade lengths over time). Maximum likelihood\nfitting indicates that this effect is strongest when including the last\n$\\approx 7$ seconds of a scan path. The effect size is comparable to well-known\nforms of history dependence such as inhibition of return. A parametric\nprobabilistic model including a self-crossing penalty term was able to\nreproduce joint statistics of saccade lengths and self-crossings. We also\nquantified individual strategic differences, and their consistency over the six\nimages viewed per participant, using mixed-effect regressions. Participants\nwith a higher tendency to avoid crossings displayed smaller saccade lengths and\nshorter fixation durations on average, but did not display more horizontal,\nvertical, forward or reverse saccades. Together, these results indicate that\nthe avoidance of crossings is a local orienting strategy that facilitates and\ncomplements inhibition of return, and hence exploration of visual scenes.", "AI": {"tldr": "Study reveals a new oculomotor bias called 'self-crossing avoidance' where saccades tend to avoid crossing earlier scan paths, particularly with small amplitudes, lasting about 7 seconds and varying significantly between individuals.", "motivation": "To quantify the impact of longer path history on visual search patterns and identify new memory-dependent effects in eye movements during complex visual tasks.", "method": "Used step-selection framework from movement ecology, analyzed 45-second viewings of \"Where's Waldo\" images, compared real data to synthetic memoryless models, employed maximum likelihood fitting and mixed-effect regressions to quantify individual differences.", "result": "Found significant self-crossing avoidance effect strongest when including last ~7 seconds of scan path, comparable to inhibition of return. Participants with higher crossing avoidance showed smaller saccade lengths and shorter fixation durations.", "conclusion": "Self-crossing avoidance is a local orienting strategy that complements inhibition of return and facilitates visual scene exploration, representing a new memory-dependent effect in oculomotor control."}}
{"id": "2508.19026", "pdf": "https://arxiv.org/pdf/2508.19026", "abs": "https://arxiv.org/abs/2508.19026", "authors": ["Gueter Josmy Faure", "Min-Hung Chen", "Jia-Fong Yeh", "Ying Cheng", "Hung-Ting Su", "Yung-Hao Tang", "Shang-Hong Lai", "Winston H. Hsu"], "title": "MovieCORE: COgnitive REasoning in Movies", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Accepted for EMNLP'2025 Main Conference. Project Page:\n  https://joslefaure.github.io/assets/html/moviecore.html", "summary": "This paper introduces MovieCORE, a novel video question answering (VQA)\ndataset designed to probe deeper cognitive understanding of movie content.\nUnlike existing datasets that focus on surface-level comprehension, MovieCORE\nemphasizes questions that engage System-2 thinking while remaining specific to\nthe video material. We present an innovative agentic brainstorming approach,\nutilizing multiple large language models (LLMs) as thought agents to generate\nand refine high-quality question-answer pairs. To evaluate dataset quality, we\ndevelop a set of cognitive tests assessing depth, thought-provocation\npotential, and syntactic complexity. We also propose a comprehensive evaluation\nscheme for assessing VQA model performance on deeper cognitive tasks. To\naddress the limitations of existing video-language models (VLMs), we introduce\nan agentic enhancement module, Agentic Choice Enhancement (ACE), which improves\nmodel reasoning capabilities post-training by up to 25%. Our work contributes\nto advancing movie understanding in AI systems and provides valuable insights\ninto the capabilities and limitations of current VQA models when faced with\nmore challenging, nuanced questions about cinematic content. Our project page,\ndataset and code can be found at\nhttps://joslefaure.github.io/assets/html/moviecore.html.", "AI": {"tldr": "MovieCORE is a new video QA dataset focusing on deeper cognitive movie understanding, using agentic brainstorming with LLMs to generate questions, and introducing ACE module to enhance VLM reasoning by up to 25%.", "motivation": "Existing video QA datasets focus on surface-level comprehension, lacking deeper cognitive understanding of movie content that requires System-2 thinking.", "method": "Agentic brainstorming approach using multiple LLMs as thought agents to generate and refine question-answer pairs, plus Agentic Choice Enhancement (ACE) module to improve VLM reasoning capabilities.", "result": "Created MovieCORE dataset with high-quality cognitive questions, developed evaluation metrics, and achieved up to 25% improvement in VLM reasoning with ACE enhancement.", "conclusion": "Advances movie understanding in AI systems, provides insights into VQA model limitations, and offers tools for deeper cognitive video analysis."}}
{"id": "2508.19291", "pdf": "https://arxiv.org/pdf/2508.19291", "abs": "https://arxiv.org/abs/2508.19291", "authors": ["Luvin Munish Ragoo", "Ivar Farup", "Casper F. Andersen", "Graham Finlayson"], "title": "Modeling spectral filtering effects on color-matching functions: Implications for observer variability", "categories": ["astro-ph.IM", "cs.CV"], "comment": null, "summary": "This study investigates the impact of spectral filtering on color-matching\nfunctions (CMFs) and its implications for observer variability modeling. We\nconducted color matching experiments with a single observer, both with and\nwithout a spectral filter in front of a bipartite field. Using a novel\ncomputational approach, we estimated the filter transmittance and\ntransformation matrix necessary to convert unfiltered CMFs to filtered CMFs.\nStatistical analysis revealed good agreement between estimated and measured\nfilter characteristics, particularly in central wavelength regions. Applying\nthis methodology to compare between Stiles and Burch 1955 (SB1955) mean\nobserver CMFs and our previously published \"ICVIO\" mean observer CMFs, we\nidentified a \"yellow\" (short-wavelength suppressing) filter that effectively\ntransforms between these datasets. This finding aligns with our hypothesis that\nobserved differences between the CMF sets are attributable to age-related lens\nyellowing (average observer age: 49 years in ICVIO versus 30 years in SB1955).\nOur approach enables efficient representation of observer variability through a\nsingle filter rather than three separate functions, offering potentially\nreduced experimental overhead while maintaining accuracy in characterizing\nindividual color vision differences.", "AI": {"tldr": "Spectral filtering approach transforms color-matching functions between different observer datasets using a single filter instead of three functions, explaining age-related differences through lens yellowing.", "motivation": "To understand and model observer variability in color vision, particularly differences between color-matching function datasets from different studies with age-disparate observer groups.", "method": "Conducted color matching experiments with/without spectral filters, developed computational approach to estimate filter transmittance and transformation matrices, applied methodology to compare SB1955 and ICVIO CMF datasets.", "result": "Good agreement between estimated and measured filter characteristics, identified yellow (short-wavelength suppressing) filter that transforms between SB1955 and ICVIO datasets, supporting age-related lens yellowing hypothesis.", "conclusion": "Single spectral filter approach efficiently represents observer variability with reduced experimental overhead while maintaining accuracy in characterizing individual color vision differences."}}
{"id": "2508.19300", "pdf": "https://arxiv.org/pdf/2508.19300", "abs": "https://arxiv.org/abs/2508.19300", "authors": ["Cunmin Zhao", "Ziyuan Luo", "Guoye Guan", "Zelin Li", "Yiming Ma", "Zhongying Zhao", "Renjie Wan"], "title": "CellINR: Implicitly Overcoming Photo-induced Artifacts in 4D Live Fluorescence Microscopy", "categories": ["eess.IV", "cs.AI", "cs.CV", "32H10", "F.2.2; I.2.7"], "comment": "13 pages, 4 figures", "summary": "4D live fluorescence microscopy is often compromised by prolonged high\nintensity illumination which induces photobleaching and phototoxic effects that\ngenerate photo-induced artifacts and severely impair image continuity and\ndetail recovery. To address this challenge, we propose the CellINR framework, a\ncase-specific optimization approach based on implicit neural representation.\nThe method employs blind convolution and structure amplification strategies to\nmap 3D spatial coordinates into the high frequency domain, enabling precise\nmodeling and high-accuracy reconstruction of cellular structures while\neffectively distinguishing true signals from artifacts. Experimental results\ndemonstrate that CellINR significantly outperforms existing techniques in\nartifact removal and restoration of structural continuity, and for the first\ntime, a paired 4D live cell imaging dataset is provided for evaluating\nreconstruction performance, thereby offering a solid foundation for subsequent\nquantitative analyses and biological research. The code and dataset will be\npublic.", "AI": {"tldr": "CellINR framework uses implicit neural representation with blind convolution and structure amplification to reduce photobleaching artifacts in 4D live fluorescence microscopy, enabling high-accuracy cellular structure reconstruction.", "motivation": "Prolonged high-intensity illumination in 4D live fluorescence microscopy causes photobleaching and phototoxic effects, compromising image quality and continuity while generating artifacts.", "method": "Case-specific optimization approach based on implicit neural representation that employs blind convolution and structure amplification strategies to map 3D spatial coordinates into high frequency domain for precise modeling.", "result": "Significantly outperforms existing techniques in artifact removal and structural continuity restoration, and provides the first paired 4D live cell imaging dataset for evaluation.", "conclusion": "Offers a solid foundation for quantitative analyses and biological research by enabling high-accuracy reconstruction while distinguishing true signals from artifacts, with code and dataset made publicly available."}}
{"id": "2508.19303", "pdf": "https://arxiv.org/pdf/2508.19303", "abs": "https://arxiv.org/abs/2508.19303", "authors": ["Utsav Ratna Tuladhar", "Richard Simon", "Doran Mix", "Michael Richards"], "title": "2D Ultrasound Elasticity Imaging of Abdominal Aortic Aneurysms Using Deep Neural Networks", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Abdominal aortic aneurysms (AAA) pose a significant clinical risk due to\ntheir potential for rupture, which is often asymptomatic but can be fatal.\nAlthough maximum diameter is commonly used for risk assessment, diameter alone\nis insufficient as it does not capture the properties of the underlying\nmaterial of the vessel wall, which play a critical role in determining the risk\nof rupture. To overcome this limitation, we propose a deep learning-based\nframework for elasticity imaging of AAAs with 2D ultrasound. Leveraging finite\nelement simulations, we generate a diverse dataset of displacement fields with\ntheir corresponding modulus distributions. We train a model with U-Net\narchitecture and normalized mean squared error (NMSE) to infer the spatial\nmodulus distribution from the axial and lateral components of the displacement\nfields. This model is evaluated across three experimental domains: digital\nphantom data from 3D COMSOL simulations, physical phantom experiments using\nbiomechanically distinct vessel models, and clinical ultrasound exams from AAA\npatients. Our simulated results demonstrate that the proposed deep learning\nmodel is able to reconstruct modulus distributions, achieving an NMSE score of\n0.73\\%. Similarly, in phantom data, the predicted modular ratio closely matches\nthe expected values, affirming the model's ability to generalize to phantom\ndata. We compare our approach with an iterative method which shows comparable\nperformance but higher computation time. In contrast, the deep learning method\ncan provide quick and effective estimates of tissue stiffness from ultrasound\nimages, which could help assess the risk of AAA rupture without invasive\nprocedures.", "AI": {"tldr": "Deep learning framework for elasticity imaging of abdominal aortic aneurysms using ultrasound data to assess rupture risk beyond diameter measurements", "motivation": "Current AAA risk assessment relies on maximum diameter which is insufficient as it doesn't capture vessel wall material properties critical for rupture risk determination", "method": "U-Net architecture trained with normalized mean squared error on finite element simulation data to infer spatial modulus distribution from displacement fields in 2D ultrasound", "result": "Achieved 0.73% NMSE in simulations, accurately predicted modular ratios in phantom data, and showed comparable performance to iterative methods with significantly faster computation", "conclusion": "Deep learning provides quick, effective tissue stiffness estimates from ultrasound that could help non-invasively assess AAA rupture risk"}}
{"id": "2508.19319", "pdf": "https://arxiv.org/pdf/2508.19319", "abs": "https://arxiv.org/abs/2508.19319", "authors": ["Pardis Moradbeiki", "Nasser Ghadiri", "Sayed Jalal Zahabi", "Uffe Kock Wiil", "Kristoffer Kittelmann Brockhattingen", "Ali Ebrahimi"], "title": "MedVQA-TREE: A Multimodal Reasoning and Retrieval Framework for Sarcopenia Prediction", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Accurate sarcopenia diagnosis via ultrasound remains challenging due to\nsubtle imaging cues, limited labeled data, and the absence of clinical context\nin most models. We propose MedVQA-TREE, a multimodal framework that integrates\na hierarchical image interpretation module, a gated feature-level fusion\nmechanism, and a novel multi-hop, multi-query retrieval strategy. The vision\nmodule includes anatomical classification, region segmentation, and graph-based\nspatial reasoning to capture coarse, mid-level, and fine-grained structures. A\ngated fusion mechanism selectively integrates visual features with textual\nqueries, while clinical knowledge is retrieved through a UMLS-guided pipeline\naccessing PubMed and a sarcopenia-specific external knowledge base. MedVQA-TREE\nwas trained and evaluated on two public MedVQA datasets (VQA-RAD and PathVQA)\nand a custom sarcopenia ultrasound dataset. The model achieved up to 99%\ndiagnostic accuracy and outperformed previous state-of-the-art methods by over\n10%. These results underscore the benefit of combining structured visual\nunderstanding with guided knowledge retrieval for effective AI-assisted\ndiagnosis in sarcopenia.", "AI": {"tldr": "MedVQA-TREE is a multimodal AI framework that combines hierarchical image analysis with clinical knowledge retrieval to achieve 99% accuracy in sarcopenia diagnosis from ultrasound images, outperforming previous methods by over 10%.", "motivation": "Current sarcopenia diagnosis via ultrasound faces challenges due to subtle imaging cues, limited labeled data, and lack of clinical context integration in existing models.", "method": "Multimodal framework with hierarchical image interpretation (anatomical classification, region segmentation, graph-based reasoning), gated feature-level fusion, and multi-hop multi-query retrieval from UMLS-guided PubMed and sarcopenia-specific knowledge bases.", "result": "Achieved up to 99% diagnostic accuracy on MedVQA datasets (VQA-RAD, PathVQA) and custom sarcopenia ultrasound dataset, outperforming state-of-the-art methods by over 10%.", "conclusion": "Combining structured visual understanding with guided knowledge retrieval significantly improves AI-assisted sarcopenia diagnosis effectiveness."}}
{"id": "2508.19322", "pdf": "https://arxiv.org/pdf/2508.19322", "abs": "https://arxiv.org/abs/2508.19322", "authors": ["Xueyang Li", "Mingze Jiang", "Gelei Xu", "Jun Xia", "Mengzhao Jia", "Danny Chen", "Yiyu Shi"], "title": "AT-CXR: Uncertainty-Aware Agentic Triage for Chest X-rays", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Agentic AI is advancing rapidly, yet truly autonomous medical-imaging triage,\nwhere a system decides when to stop, escalate, or defer under real constraints,\nremains relatively underexplored. To address this gap, we introduce AT-CXR, an\nuncertainty-aware agent for chest X-rays. The system estimates per-case\nconfidence and distributional fit, then follows a stepwise policy to issue an\nautomated decision or abstain with a suggested label for human intervention. We\nevaluate two router designs that share the same inputs and actions: a\ndeterministic rule-based router and an LLM-decided router. Across five-fold\nevaluation on a balanced subset of NIH ChestX-ray14 dataset, both variants\noutperform strong zero-shot vision-language models and state-of-the-art\nsupervised classifiers, achieving higher full-coverage accuracy and superior\nselective-prediction performance, evidenced by a lower area under the\nrisk-coverage curve (AURC) and a lower error rate at high coverage, while\noperating with lower latency that meets practical clinical constraints. The two\nrouters provide complementary operating points, enabling deployments to\nprioritize maximal throughput or maximal accuracy. Our code is available at\nhttps://github.com/XLIAaron/uncertainty-aware-cxr-agent.", "AI": {"tldr": "AT-CXR is an uncertainty-aware AI agent for chest X-ray triage that automatically decides when to stop, escalate, or defer cases under clinical constraints, outperforming existing methods with higher accuracy and lower latency.", "motivation": "Autonomous medical-imaging triage systems that can make real-time decisions under constraints remain underexplored, despite rapid advances in agentic AI.", "method": "Developed two router designs: deterministic rule-based router and LLM-decided router that estimate per-case confidence and distributional fit, following stepwise policies to issue automated decisions or abstain with suggested labels.", "result": "Both router variants outperformed zero-shot vision-language models and state-of-the-art supervised classifiers, achieving higher full-coverage accuracy, superior selective-prediction performance (lower AURC and error rate), and lower latency meeting clinical constraints.", "conclusion": "The two routers provide complementary operating points for clinical deployment, enabling prioritization of either maximal throughput or maximal accuracy in chest X-ray triage systems."}}
{"id": "2508.19323", "pdf": "https://arxiv.org/pdf/2508.19323", "abs": "https://arxiv.org/abs/2508.19323", "authors": ["Ms. Preeti P. Bhatt", "Rakesh R. Savant"], "title": "A Technical Review on Comparison and Estimation of Steganographic Tools", "categories": ["cs.CR", "cs.CV", "cs.GR"], "comment": "20", "summary": "Steganography is technique of hiding a data under cover media using different\nsteganography tools. Image steganography is hiding of data\n(Text/Image/Audio/Video) under a cover as Image. This review paper presents\nclassification of image steganography and the comparison of various Image\nsteganography tools using different image formats. Analyzing numerous tools on\nthe basis of Image features and extracting the best one. Some of the tools\navailable in the market were selected based on the frequent use; these tools\nwere tested using the same input on all of them. Specific text was embedded\nwithin all host images for each of the six Steganography tools selected. The\nresults of the experiment reveal that all the six tools were relatively\nperforming at the same level, though some software performs better than others\nthrough efficiency. And it was based on the image features like size,\ndimensions, and pixel value and histogram differentiation.", "AI": {"tldr": "This paper reviews and compares six image steganography tools, testing their performance with the same input data across different image formats and analyzing them based on image features like size, dimensions, pixel values, and histogram differentiation.", "motivation": "To classify image steganography methods and provide a comparative analysis of various steganography tools to identify the best performing ones based on their efficiency and image feature preservation.", "method": "Selected six commonly used steganography tools and tested them using the same input data (specific text embedded in host images). Analyzed tools based on image features including size, dimensions, pixel values, and histogram differentiation.", "result": "All six tools performed at relatively similar levels, though some showed better efficiency than others. Performance differences were based on how well they preserved image features during the steganography process.", "conclusion": "The study provides a comparative framework for evaluating steganography tools and demonstrates that while most tools perform similarly, efficiency varies based on image feature preservation capabilities."}}
{"id": "2508.19353", "pdf": "https://arxiv.org/pdf/2508.19353", "abs": "https://arxiv.org/abs/2508.19353", "authors": ["Marcin Osial", "Bartosz W\u00f3jcik", "Bartosz Zieli\u0144ski", "Sebastian Cygert"], "title": "Efficient Multi-Source Knowledge Transfer by Model Merging", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "While transfer learning is an advantageous strategy, it overlooks the\nopportunity to leverage knowledge from numerous available models online.\nAddressing this multi-source transfer learning problem is a promising path to\nboost adaptability and cut re-training costs. However, existing approaches are\ninherently coarse-grained, lacking the necessary precision for granular\nknowledge extraction and the aggregation efficiency required to fuse knowledge\nfrom either a large number of source models or those with high parameter\ncounts. We address these limitations by leveraging Singular Value Decomposition\n(SVD) to first decompose each source model into its elementary, rank-one\ncomponents. A subsequent aggregation stage then selects only the most salient\ncomponents from all sources, thereby overcoming the previous efficiency and\nprecision limitations. To best preserve and leverage the synthesized knowledge\nbase, our method adapts to the target task by fine-tuning only the principal\nsingular values of the merged matrix. In essence, this process only\nrecalibrates the importance of top SVD components. The proposed framework\nallows for efficient transfer learning, is robust to perturbations both at the\ninput level and in the parameter space (e.g., noisy or pruned sources), and\nscales well computationally.", "AI": {"tldr": "A novel multi-source transfer learning framework using SVD decomposition to efficiently extract and aggregate knowledge from multiple source models, overcoming previous efficiency and precision limitations.", "motivation": "Traditional transfer learning overlooks the opportunity to leverage knowledge from numerous available online models. Existing multi-source approaches are coarse-grained and lack efficiency for handling many source models or high-parameter models.", "method": "Uses Singular Value Decomposition (SVD) to decompose each source model into rank-one components, then aggregates the most salient components across all sources. Adapts to target tasks by fine-tuning only principal singular values of the merged matrix.", "result": "The framework enables efficient transfer learning, is robust to input and parameter perturbations (noisy/pruned sources), and scales well computationally.", "conclusion": "SVD-based decomposition and selective aggregation provides an effective solution for multi-source transfer learning, offering precision, efficiency, and robustness compared to previous approaches."}}
{"id": "2508.19376", "pdf": "https://arxiv.org/pdf/2508.19376", "abs": "https://arxiv.org/abs/2508.19376", "authors": ["Dikshant Sagar", "Kaiwen Yu", "Alejandro Yankelevich", "Jianming Bian", "Pierre Baldi"], "title": "Fine-Tuning Vision-Language Models for Neutrino Event Analysis in High-Energy Physics Experiments", "categories": ["cs.LG", "cs.AI", "cs.CV", "hep-ex"], "comment": null, "summary": "Recent progress in large language models (LLMs) has shown strong potential\nfor multimodal reasoning beyond natural language. In this work, we explore the\nuse of a fine-tuned Vision-Language Model (VLM), based on LLaMA 3.2, for\nclassifying neutrino interactions from pixelated detector images in high-energy\nphysics (HEP) experiments. We benchmark its performance against an established\nCNN baseline used in experiments like NOvA and DUNE, evaluating metrics such as\nclassification accuracy, precision, recall, and AUC-ROC. Our results show that\nthe VLM not only matches or exceeds CNN performance but also enables richer\nreasoning and better integration of auxiliary textual or semantic context.\nThese findings suggest that VLMs offer a promising general-purpose backbone for\nevent classification in HEP, paving the way for multimodal approaches in\nexperimental neutrino physics.", "AI": {"tldr": "Fine-tuned Vision-Language Model based on LLaMA 3.2 outperforms CNN baseline for neutrino interaction classification in high-energy physics experiments, enabling richer multimodal reasoning.", "motivation": "To explore the potential of large language models for multimodal reasoning beyond natural language, specifically for classifying neutrino interactions from pixelated detector images in high-energy physics experiments.", "method": "Fine-tuned a Vision-Language Model (VLM) based on LLaMA 3.2 and benchmarked its performance against an established CNN baseline used in experiments like NOvA and DUNE, evaluating classification accuracy, precision, recall, and AUC-ROC metrics.", "result": "The VLM not only matches or exceeds CNN performance but also enables richer reasoning and better integration of auxiliary textual or semantic context.", "conclusion": "VLMs offer a promising general-purpose backbone for event classification in HEP, paving the way for multimodal approaches in experimental neutrino physics."}}
{"id": "2508.19493", "pdf": "https://arxiv.org/pdf/2508.19493", "abs": "https://arxiv.org/abs/2508.19493", "authors": ["Zhixin Lin", "Jungang Li", "Shidong Pan", "Yibo Shi", "Yue Yao", "Dongliang Xu"], "title": "Mind the Third Eye! Benchmarking Privacy Awareness in MLLM-powered Smartphone Agents", "categories": ["cs.CR", "cs.CV"], "comment": null, "summary": "Smartphones bring significant convenience to users but also enable devices to\nextensively record various types of personal information. Existing smartphone\nagents powered by Multimodal Large Language Models (MLLMs) have achieved\nremarkable performance in automating different tasks. However, as the cost,\nthese agents are granted substantial access to sensitive users' personal\ninformation during this operation. To gain a thorough understanding of the\nprivacy awareness of these agents, we present the first large-scale benchmark\nencompassing 7,138 scenarios to the best of our knowledge. In addition, for\nprivacy context in scenarios, we annotate its type (e.g., Account Credentials),\nsensitivity level, and location. We then carefully benchmark seven available\nmainstream smartphone agents. Our results demonstrate that almost all\nbenchmarked agents show unsatisfying privacy awareness (RA), with performance\nremaining below 60% even with explicit hints. Overall, closed-source agents\nshow better privacy ability than open-source ones, and Gemini 2.0-flash\nachieves the best, achieving an RA of 67%. We also find that the agents'\nprivacy detection capability is highly related to scenario sensitivity level,\ni.e., the scenario with a higher sensitivity level is typically more\nidentifiable. We hope the findings enlighten the research community to rethink\nthe unbalanced utility-privacy tradeoff about smartphone agents. Our code and\nbenchmark are available at https://zhixin-l.github.io/SAPA-Bench.", "AI": {"tldr": "First large-scale benchmark study (7,138 scenarios) reveals smartphone agents powered by MLLMs have poor privacy awareness (<60% even with hints), with closed-source agents performing better than open-source ones.", "motivation": "Smartphone agents have extensive access to sensitive personal information but their privacy awareness capabilities are not well understood, creating potential privacy risks for users.", "method": "Created a comprehensive benchmark with 7,138 scenarios annotated by privacy type, sensitivity level, and location, then evaluated seven mainstream smartphone agents on their privacy awareness capabilities.", "result": "Most agents showed unsatisfying privacy awareness (below 60%), with closed-source agents outperforming open-source ones. Gemini 2.0-flash performed best at 67% RA. Privacy detection correlated with scenario sensitivity levels.", "conclusion": "Current smartphone agents demonstrate inadequate privacy protection, highlighting an unbalanced utility-privacy tradeoff that requires attention from the research community."}}
{"id": "2508.19508", "pdf": "https://arxiv.org/pdf/2508.19508", "abs": "https://arxiv.org/abs/2508.19508", "authors": ["Tian Qiu", "Alan Zoubi", "Yiyuan Lin", "Ruiming Du", "Lailiang Cheng", "Yu Jiang"], "title": "DATR: Diffusion-based 3D Apple Tree Reconstruction Framework with Sparse-View", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Digital twin applications offered transformative potential by enabling\nreal-time monitoring and robotic simulation through accurate virtual replicas\nof physical assets. The key to these systems is 3D reconstruction with high\ngeometrical fidelity. However, existing methods struggled under field\nconditions, especially with sparse and occluded views. This study developed a\ntwo-stage framework (DATR) for the reconstruction of apple trees from sparse\nviews. The first stage leverages onboard sensors and foundation models to\nsemi-automatically generate tree masks from complex field images. Tree masks\nare used to filter out background information in multi-modal data for the\nsingle-image-to-3D reconstruction at the second stage. This stage consists of a\ndiffusion model and a large reconstruction model for respective multi view and\nimplicit neural field generation. The training of the diffusion model and LRM\nwas achieved by using realistic synthetic apple trees generated by a Real2Sim\ndata generator. The framework was evaluated on both field and synthetic\ndatasets. The field dataset includes six apple trees with field-measured ground\ntruth, while the synthetic dataset featured structurally diverse trees.\nEvaluation results showed that our DATR framework outperformed existing 3D\nreconstruction methods across both datasets and achieved domain-trait\nestimation comparable to industrial-grade stationary laser scanners while\nimproving the throughput by $\\sim$360 times, demonstrating strong potential for\nscalable agricultural digital twin systems.", "AI": {"tldr": "DATR framework enables high-fidelity 3D reconstruction of apple trees from sparse field views using a two-stage approach with foundation models and diffusion techniques, achieving 360x throughput improvement over laser scanners.", "motivation": "Existing 3D reconstruction methods struggle with sparse and occluded views in field conditions, limiting digital twin applications for agricultural monitoring and simulation.", "method": "Two-stage framework: 1) Semi-automatic tree mask generation using onboard sensors and foundation models to filter background, 2) Single-image-to-3D reconstruction using diffusion model for multi-view generation and large reconstruction model for implicit neural fields, trained on synthetic apple trees from Real2Sim data generator.", "result": "Outperformed existing 3D reconstruction methods on both field and synthetic datasets, achieved domain-trait estimation comparable to industrial laser scanners with ~360 times higher throughput.", "conclusion": "DATR demonstrates strong potential for scalable agricultural digital twin systems by enabling efficient, high-fidelity 3D reconstruction from sparse field views."}}
{"id": "2508.19518", "pdf": "https://arxiv.org/pdf/2508.19518", "abs": "https://arxiv.org/abs/2508.19518", "authors": ["Hail Song", "Seokhwan Yang", "Woontack Woo"], "title": "Fast Texture Transfer for XR Avatars via Barycentric UV Conversion", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "We present a fast and efficient method for transferring facial textures onto\nSMPL-X-based full-body avatars. Unlike conventional affine-transform methods\nthat are slow and prone to visual artifacts, our method utilizes a barycentric\nUV conversion technique. Our approach precomputes the entire UV mapping into a\nsingle transformation matrix, enabling texture transfer in a single operation.\nThis results in a speedup of over 7000x compared to the baseline, while also\nsignificantly improving the final texture quality by eliminating boundary\nartifacts. Through quantitative and qualitative evaluations, we demonstrate\nthat our method offers a practical solution for personalization in immersive XR\napplications. The code is available online.", "AI": {"tldr": "A fast UV conversion method for facial texture transfer on SMPL-X avatars that achieves 7000x speedup and eliminates boundary artifacts compared to baseline methods.", "motivation": "Conventional affine-transform methods for facial texture transfer are slow and produce visual artifacts, limiting their practicality for immersive XR applications that require real-time personalization.", "method": "Utilizes a barycentric UV conversion technique that precomputes the entire UV mapping into a single transformation matrix, enabling texture transfer in a single operation.", "result": "Achieves over 7000x speedup compared to baseline methods while significantly improving texture quality by eliminating boundary artifacts. Both quantitative and qualitative evaluations demonstrate superior performance.", "conclusion": "The method provides a practical solution for avatar personalization in immersive XR applications, offering both speed and quality improvements over existing approaches. The code is publicly available."}}
{"id": "2508.19714", "pdf": "https://arxiv.org/pdf/2508.19714", "abs": "https://arxiv.org/abs/2508.19714", "authors": ["Subhrojyoti Mukherjee", "Manoranjan Mohanty"], "title": "Addressing Deepfake Issue in Selfie banking through camera based authentication", "categories": ["cs.CR", "cs.CV"], "comment": null, "summary": "Fake images in selfie banking are increasingly becoming a threat. Previously,\nit was just Photoshop, but now deep learning technologies enable us to create\nhighly realistic fake identities, which fraudsters exploit to bypass biometric\nsystems such as facial recognition in online banking. This paper explores the\nuse of an already established forensic recognition system, previously used for\npicture camera localization, in deepfake detection.", "AI": {"tldr": "Using forensic recognition systems originally designed for camera localization to detect deepfakes in selfie banking authentication", "motivation": "Deep learning technologies enable creation of highly realistic fake identities that can bypass facial recognition systems in online banking, posing a serious threat to selfie banking security", "method": "Adapting and applying an established forensic recognition system previously used for picture camera localization to deepfake detection", "result": "The paper explores the effectiveness of using camera forensic techniques for identifying deepfake images in banking authentication systems", "conclusion": "Existing forensic recognition systems show promise for detecting deepfakes and enhancing security in selfie banking applications"}}
{"id": "2508.19788", "pdf": "https://arxiv.org/pdf/2508.19788", "abs": "https://arxiv.org/abs/2508.19788", "authors": ["Sena Ishii", "Akash Chikhalikar", "Ankit A. Ravankar", "Jose Victorio Salazar Luces", "Yasuhisa Hirata"], "title": "Context-Aware Risk Estimation in Home Environments: A Probabilistic Framework for Service Robots", "categories": ["cs.RO", "cs.CV"], "comment": "8 pages, Accepted for IEEE RO-MAN 2025 Conference", "summary": "We present a novel framework for estimating accident-prone regions in\neveryday indoor scenes, aimed at improving real-time risk awareness in service\nrobots operating in human-centric environments. As robots become integrated\ninto daily life, particularly in homes, the ability to anticipate and respond\nto environmental hazards is crucial for ensuring user safety, trust, and\neffective human-robot interaction. Our approach models object-level risk and\ncontext through a semantic graph-based propagation algorithm. Each object is\nrepresented as a node with an associated risk score, and risk propagates\nasymmetrically from high-risk to low-risk objects based on spatial proximity\nand accident relationship. This enables the robot to infer potential hazards\neven when they are not explicitly visible or labeled. Designed for\ninterpretability and lightweight onboard deployment, our method is validated on\na dataset with human-annotated risk regions, achieving a binary risk detection\naccuracy of 75%. The system demonstrates strong alignment with human\nperception, particularly in scenes involving sharp or unstable objects. These\nresults underline the potential of context-aware risk reasoning to enhance\nrobotic scene understanding and proactive safety behaviors in shared\nhuman-robot spaces. This framework could serve as a foundation for future\nsystems that make context-driven safety decisions, provide real-time alerts, or\nautonomously assist users in avoiding or mitigating hazards within home\nenvironments.", "AI": {"tldr": "Novel framework for estimating accident-prone regions in indoor scenes using semantic graph-based risk propagation to enhance robot safety awareness.", "motivation": "As robots integrate into daily life, particularly in homes, anticipating environmental hazards is crucial for user safety, trust, and effective human-robot interaction.", "method": "Models object-level risk through semantic graph-based propagation algorithm where each object is a node with risk score, and risk propagates asymmetrically based on spatial proximity and accident relationships.", "result": "Achieved 75% binary risk detection accuracy on human-annotated dataset, with strong alignment to human perception especially for sharp or unstable objects.", "conclusion": "Framework shows potential for context-aware risk reasoning to enhance robotic scene understanding and proactive safety behaviors in shared human-robot spaces."}}
{"id": "2508.19896", "pdf": "https://arxiv.org/pdf/2508.19896", "abs": "https://arxiv.org/abs/2508.19896", "authors": ["Davorin Mili\u010devi\u0107", "Ratko Grbi\u0107"], "title": "NM-Hebb: Coupling Local Hebbian Plasticity with Metric Learning for More Accurate and Interpretable CNNs", "categories": ["cs.LG", "cs.CV", "I.2.6; I.5.4"], "comment": "13 pages, 4 figures. Submitted to Elsevier Neurocomputing, under\n  review", "summary": "Deep Convolutional Neural Networks (CNNs) achieve high accuracy but often\nrely on purely global, gradient-based optimisation, which can lead to\noverfitting, redundant filters, and reduced interpretability. To address these\nlimitations, we propose NM-Hebb, a two-phase training framework that integrates\nneuro-inspired local plasticity with distance-aware supervision. Phase 1\nextends standard supervised training by jointly optimising a cross-entropy\nobjective with two biologically inspired mechanisms: (i) a Hebbian regulariser\nthat aligns the spatial mean of activations with the mean of the corresponding\nconvolutional filter weights, encouraging structured, reusable primitives; and\n(ii) a learnable neuromodulator that gates an elastic-weight-style\nconsolidation loss, preserving beneficial parameters without freezing the\nnetwork. Phase 2 fine-tunes the backbone with a pairwise metric-learning loss,\nexplicitly compressing intra-class distances and enlarging inter-class margins\nin the embedding space. Evaluated on CIFAR-10, CIFAR-100, and TinyImageNet\nacross five backbones (ResNet-18, VGG-11, MobileNet-v2, EfficientNet-V2,\nDenseNet-121), NM-Hebb achieves consistent gains over baseline and other\nmethods: Top-1 accuracy improves by +2.0-10.0 pp (CIFAR-10), +2.0-9.0 pp\n(CIFAR-100), and up to +4.3-8.9 pp (TinyImageNet), with Normalised Mutual\nInformation (NMI) increased by up to +0.15. Qualitative visualisations and\nfilter-level analyses further confirm that NM-Hebb produces more structured and\nselective features, yielding tighter and more interpretable class clusters.\nOverall, coupling local Hebbian plasticity with metric-based fine-tuning yields\nCNNs that are not only more accurate but also more interpretable, offering\npractical benefits for resource-constrained and safety-critical AI deployments.", "AI": {"tldr": "NM-Hebb is a two-phase CNN training framework combining neuro-inspired local plasticity with metric learning, achieving significant accuracy improvements (+2.0-10.0 pp) and better interpretability across multiple datasets and architectures.", "motivation": "Address limitations of standard CNNs that rely solely on global gradient-based optimization, which can cause overfitting, redundant filters, and reduced interpretability.", "method": "Two-phase approach: Phase 1 combines cross-entropy loss with Hebbian regularization and neuromodulator-gated consolidation; Phase 2 uses pairwise metric learning for fine-tuning to compress intra-class distances and enlarge inter-class margins.", "result": "Consistent gains across CIFAR-10, CIFAR-100, and TinyImageNet with five backbones: +2.0-10.0 pp accuracy improvement, +0.15 NMI increase, and more structured/selective features with tighter class clusters.", "conclusion": "Integrating local Hebbian plasticity with metric-based fine-tuning produces CNNs that are both more accurate and interpretable, beneficial for resource-constrained and safety-critical AI applications."}}

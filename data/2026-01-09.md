<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 94]
- [cs.CV](#cs.CV) [Total: 79]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.LG](#cs.LG) [Total: 10]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.DL](#cs.DL) [Total: 2]
- [physics.optics](#physics.optics) [Total: 2]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.CR](#cs.CR) [Total: 3]
- [cs.AI](#cs.AI) [Total: 19]
- [cs.RO](#cs.RO) [Total: 2]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.SE](#cs.SE) [Total: 2]
- [eess.IV](#eess.IV) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [MedPI: Evaluating AI Systems in Medical Patient-facing Interactions](https://arxiv.org/abs/2601.04195)
*Diego Fajardo V.,Oleksii Proniakin,Victoria-Elisabeth Gruber,Razvan Marinescu*

Main category: cs.CL

TL;DR: MedPI is a comprehensive benchmark for evaluating LLMs in medical conversations across 105 dimensions, revealing low performance across all tested models, especially on differential diagnosis.


<details>
  <summary>Details</summary>
Motivation: Existing medical benchmarks focus on single-turn QA, lacking evaluation of multi-turn patient-clinician dialogues and comprehensive medical process assessment. Need for accreditation-aligned evaluation of LLMs in realistic clinical conversations.

Method: Five-layer framework: 1) Synthetic EHR-like patient packets, 2) AI Patient with memory/affect, 3) Task matrix (encounter reasons × objectives), 4) 105-dimension evaluation framework mapped to ACGME competencies, 5) Calibrated AI Judges with committee-based scoring.

Result: Tested 9 flagship models across 366 AI Patients and 7,097 conversations. All LLMs showed low performance across various dimensions, particularly poor on differential diagnosis. Models included Claude Opus 4.1, Claude Sonnet 4, MedGemma, Gemini 2.5 Pro, Llama 3.3 70b, GPT-5, GPT OSS 120b, o3, and Grok-4.

Conclusion: Current LLMs perform poorly in medical dialogue evaluation, especially on critical tasks like differential diagnosis. MedPI provides a comprehensive benchmark to guide future development of LLMs for clinical diagnosis and treatment recommendations.

Abstract: We present MedPI, a high-dimensional benchmark for evaluating large language models (LLMs) in patient-clinician conversations. Unlike single-turn question-answer (QA) benchmarks, MedPI evaluates the medical dialogue across 105 dimensions comprising the medical process, treatment safety, treatment outcomes and doctor-patient communication across a granular, accreditation-aligned rubric. MedPI comprises five layers: (1) Patient Packets (synthetic EHR-like ground truth); (2) an AI Patient instantiated through an LLM with memory and affect; (3) a Task Matrix spanning encounter reasons (e.g. anxiety, pregnancy, wellness checkup) x encounter objectives (e.g. diagnosis, lifestyle advice, medication advice); (4) an Evaluation Framework with 105 dimensions on a 1-4 scale mapped to the Accreditation Council for Graduate Medical Education (ACGME) competencies; and (5) AI Judges that are calibrated, committee-based LLMs providing scores, flags, and evidence-linked rationales. We evaluate 9 flagship models -- Claude Opus 4.1, Claude Sonnet 4, MedGemma, Gemini 2.5 Pro, Llama 3.3 70b Instruct, GPT-5, GPT OSS 120b, o3, Grok-4 -- across 366 AI Patients and 7,097 conversations using a standardized "vanilla clinician" prompt. For all LLMs, we observe low performance across a variety of dimensions, in particular on differential diagnosis. Our work can help guide future use of LLMs for diagnosis and treatment recommendations.

</details>


### [2] [RAGVUE: A Diagnostic View for Explainable and Automated Evaluation of Retrieval-Augmented Generation](https://arxiv.org/abs/2601.04196)
*Keerthana Murugaraj,Salima Lamsiyah,Martin Theobald*

Main category: cs.CL

TL;DR: RAGVUE is a diagnostic framework for evaluating RAG systems with explainable metrics instead of single scores, decomposing performance into retrieval quality, answer relevance/completeness, faithfulness, and judge calibration.


<details>
  <summary>Details</summary>
Motivation: Existing RAG evaluation metrics collapse heterogeneous behaviors into single scores and provide little insight into whether errors come from retrieval, reasoning, or grounding, making it hard to diagnose and improve RAG systems.

Method: RAGVUE decomposes RAG behavior into four components: retrieval quality, answer relevance and completeness, strict claim-level faithfulness, and judge calibration. Each metric includes structured explanations for transparency. The framework supports both manual metric selection and fully automated agentic evaluation, with Python API, CLI, and local Streamlit interface.

Result: In comparative experiments, RAGVUE surfaces fine-grained failures that existing tools like RAGAS often overlook. The framework successfully provides transparent, explainable evaluation that helps identify specific weaknesses in RAG pipelines.

Conclusion: RAGVUE offers a diagnostic and explainable framework for RAG evaluation that enables better understanding of system failures, supports both research and practical development, and is publicly available with comprehensive tooling for integration into workflows.

Abstract: Evaluating Retrieval-Augmented Generation (RAG) systems remains a challenging task: existing metrics often collapse heterogeneous behaviors into single scores and provide little insight into whether errors arise from retrieval,reasoning, or grounding. In this paper, we introduce RAGVUE, a diagnostic and explainable framework for automated, reference-free evaluation of RAG pipelines. RAGVUE decomposes RAG behavior into retrieval quality, answer relevance and completeness, strict claim-level faithfulness, and judge calibration. Each metric includes a structured explanation, making the evaluation process transparent. Our framework supports both manual metric selection and fully automated agentic evaluation. It also provides a Python API, CLI, and a local Streamlit interface for interactive usage. In comparative experiments, RAGVUE surfaces fine-grained failures that existing tools such as RAGAS often overlook. We showcase the full RAGVUE workflow and illustrate how it can be integrated into research pipelines and practical RAG development. The source code and detailed instructions on usage are publicly available on GitHub

</details>


### [3] [Automatic Construction of Chinese Verb Collostruction Database](https://arxiv.org/abs/2601.04197)
*Xuri Tang,Daohuan Liu*

Main category: cs.CL

TL;DR: Unsupervised method to build Chinese verb collostruction database for LLM complementation, using graph-based definitions and clustering from corpus data, showing functional independence and outperforming LLMs in error correction.


<details>
  <summary>Details</summary>
Motivation: To complement LLMs by providing explicit, interpretable rules for scenarios where explanation and interpretability are essential, addressing the need for structured linguistic knowledge beyond black-box LLM approaches.

Method: Defines verb collostruction as projective, rooted, ordered, directed acyclic graph; uses clustering algorithms to generate collostructions from sentences retrieved from large-scale corpus; employs maximum matching algorithm for error correction.

Result: Generated collostructions exhibit functional independence and graded typicality; error correction algorithm based on maximum matching with collostructions outperforms LLMs in verb grammatical error correction tasks.

Conclusion: Unsupervised collostruction construction provides interpretable linguistic rules that effectively complement LLMs, demonstrating practical utility in error correction and potential for other applications requiring explainable language processing.

Abstract: This paper proposes a fully unsupervised approach to the construction of verb collostruction database for Chinese language, aimed at complementing LLMs by providing explicit and interpretable rules for application scenarios where explanation and interpretability are indispensable. The paper formally defines a verb collostruction as a projective, rooted, ordered, and directed acyclic graph and employs a series of clustering algorithms to generate collostructions for a given verb from a list of sentences retrieved from large-scale corpus. Statistical analysis demonstrates that the generated collostructions possess the design features of functional independence and graded typicality. Evaluation with verb grammatical error correction shows that the error correction algorithm based on maximum matching with collostructions achieves better performance than LLMs.

</details>


### [4] [Attribute-Aware Controlled Product Generation with LLMs for E-commerce](https://arxiv.org/abs/2601.04200)
*Virginia Negri,Víctor Martínez Gómez,Sergio A. Balanya,Subburam Rajaram*

Main category: cs.CL

TL;DR: LLM-based synthetic data generation framework for e-commerce product information extraction achieves performance comparable to real data and significantly improves over zero-shot baselines.


<details>
  <summary>Details</summary>
Motivation: High-quality labeled datasets for e-commerce product information extraction are difficult to obtain, creating a need for synthetic data generation methods.

Method: Systematic approach using LLMs with controlled modification framework: attribute-preserving modification, controlled negative example generation, and systematic attribute removal, enforced with attribute-aware prompts and store constraints.

Result: Synthetic data achieves 60.5% accuracy on MAVE dataset (vs 60.8% real data and 13.4% zero-shot baseline). Human evaluation shows 99.6% naturalness, 96.5% valid attributes, and over 90% attribute consistency. Hybrid synthetic+real data reaches 68.8% accuracy.

Conclusion: The framework provides a practical solution for augmenting e-commerce datasets, especially valuable for low-resource scenarios, with synthetic data performing on par with real data.

Abstract: Product information extraction is crucial for e-commerce services, but obtaining high-quality labeled datasets remains challenging. We present a systematic approach for generating synthetic e-commerce product data using Large Language Models (LLMs), introducing a controlled modification framework with three strategies: attribute-preserving modification, controlled negative example generation, and systematic attribute removal. Using a state-of-the-art LLM with attribute-aware prompts, we enforce store constraints while maintaining product coherence. Human evaluation of 2000 synthetic products demonstrates high effectiveness, with 99.6% rated as natural, 96.5% containing valid attribute values, and over 90% showing consistent attribute usage. On the public MAVE dataset, our synthetic data achieves 60.5% accuracy, performing on par with real training data (60.8%) and significantly improving upon the 13.4% zero-shot baseline. Hybrid configurations combining synthetic and real data further improve performance, reaching 68.8% accuracy. Our framework provides a practical solution for augmenting e-commerce datasets, particularly valuable for low-resource scenarios.

</details>


### [5] [Collective Narrative Grounding: Community-Coordinated Data Contributions to Improve Local AI Systems](https://arxiv.org/abs/2601.04201)
*Zihan Gao,Mohsin Y. K. Yousufi,Jacob Thebault-Spieker*

Main category: cs.CL

TL;DR: The paper addresses LLM blind spots in community-specific knowledge by developing Collective Narrative Grounding - a participatory protocol that transforms community stories into structured narrative units for AI integration under community governance.


<details>
  <summary>Details</summary>
Motivation: LLM question-answering systems often fail on community-specific queries, creating "knowledge blind spots" that marginalize local voices and reinforce epistemic injustice. There's a need to ground AI systems in community knowledge to address factual gaps, cultural misunderstandings, geographic confusions, and temporal misalignments.

Method: Developed Collective Narrative Grounding protocol through participatory mapping workshops with 24 community members. Created elicitation methods and schema for transforming community stories into structured narrative units with entity, time, place extraction, validation, and provenance control. Audited a county-level benchmark of 14,782 local information QA pairs to scope the problem.

Result: Found that 76.7% of LLM errors on local information QA pairs stem from factual gaps, cultural misunderstandings, geographic confusions, and temporal misalignments. On participatory QA sets, state-of-the-art LLMs answered fewer than 21% correctly without added context. Missing facts often appear in collected narratives, suggesting a path to closing dominant error modes.

Conclusion: The paper provides a rigorous foundation for community-grounded AI through taxonomy, protocol, and participatory evaluation. It identifies key design tensions (representation/power, governance/control, privacy/consent) and offers concrete requirements for retrieval-first, provenance-visible, locally governed QA systems that better answer local questions.

Abstract: Large language model (LLM) question-answering systems often fail on community-specific queries, creating "knowledge blind spots" that marginalize local voices and reinforce epistemic injustice. We present Collective Narrative Grounding, a participatory protocol that transforms community stories into structured narrative units and integrates them into AI systems under community governance. Learning from three participatory mapping workshops with N=24 community members, we designed elicitation methods and a schema that retain narrative richness while enabling entity, time, and place extraction, validation, and provenance control. To scope the problem, we audit a county-level benchmark of 14,782 local information QA pairs, where factual gaps, cultural misunderstandings, geographic confusions, and temporal misalignments account for 76.7% of errors. On a participatory QA set derived from our workshops, a state-of-the-art LLM answered fewer than 21% of questions correctly without added context, underscoring the need for local grounding. The missing facts often appear in the collected narratives, suggesting a direct path to closing the dominant error modes for narrative items. Beyond the protocol and pilot, we articulate key design tensions, such as representation and power, governance and control, and privacy and consent, providing concrete requirements for retrieval-first, provenance-visible, locally governed QA systems. Together, our taxonomy, protocol, and participatory evaluation offer a rigorous foundation for building community-grounded AI that better answers local questions.

</details>


### [6] [TeleTables: A Benchmark for Large Language Models in Telecom Table Interpretation](https://arxiv.org/abs/2601.04202)
*Anas Ezzakri,Nicola Piovesan,Mohamed Sana,Antonio De Domenico,Fadhel Ayed,Haozhe Zhang*

Main category: cs.CL

TL;DR: TeleTables benchmark evaluates LLMs' knowledge and interpretation of tables in telecom standards (3GPP), showing current models struggle with technical table comprehension.


<details>
  <summary>Details</summary>
Motivation: LLMs perform poorly on telecom standards despite industry interest, likely due to dense table content that LLMs can't adequately interpret or recall from pretraining.

Method: Created TeleTables benchmark using multi-stage data generation pipeline: extracted tables from 3GPP standards, used multimodal/reasoning LLMs to generate/validate questions, resulting in 500 human-verified QA pairs with tables in multiple formats.

Result: Smaller models (<10B params) struggle with both 3GPP knowledge recall and table interpretation; larger models show better reasoning on table interpretation but still need improvement.

Conclusion: Domain-specialized fine-tuning is needed for LLMs to reliably interpret and reason over telecom standards with complex table content.

Abstract: Language Models (LLMs) are increasingly explored in the telecom industry to support engineering tasks, accelerate troubleshooting, and assist in interpreting complex technical documents. However, recent studies show that LLMs perform poorly on telecom standards, particularly 3GPP specifications. We argue that a key reason is that these standards densely include tables to present essential information, yet the LLM knowledge and interpretation ability of such tables remains largely unexamined. To address this gap, we introduce TeleTables, a benchmark designed to evaluate both the implicit knowledge LLMs have about tables in technical specifications and their explicit ability to interpret them. TeleTables is built through a novel multi-stage data generation pipeline that extracts tables from 3GPP standards and uses multimodal and reasoning-oriented LLMs to generate and validate questions. The resulting dataset, which is publicly available, comprises 500 human-verified question-answer pairs, each associated with the corresponding table in multiple formats. Our evaluation shows that, smaller models (under 10B parameters) struggle both to recall 3GPP knowledge and to interpret tables, indicating the limited exposure to telecom standards in their pretraining and the insufficient inductive biases for navigating complex technical material. Larger models, on the other hand, show stronger reasoning on table interpretation. Overall, TeleTables highlights the need for domain-specialized fine-tuning to reliably interpret and reason over telecom standards.

</details>


### [7] [FronTalk: Benchmarking Front-End Development as Conversational Code Generation with Multi-Modal Feedback](https://arxiv.org/abs/2601.04203)
*Xueqing Wu,Zihan Xue,Da Yin,Shuyan Zhou,Kai-Wei Chang,Nanyun Peng,Yeming Wen*

Main category: cs.CL

TL;DR: FronTalk is a benchmark for conversational front-end code generation with multi-modal feedback, featuring 100 real-world multi-turn dialogues with both textual and visual instructions, revealing challenges in feature forgetting and visual interpretation.


<details>
  <summary>Details</summary>
Motivation: Front-end development relies heavily on visual artifacts like sketches and mockups to convey design intent, but current research lacks exploration of how these visual elements function in multi-turn code generation scenarios.

Method: Created FronTalk benchmark with 100 multi-turn dialogues from real websites across diverse domains; each turn has both textual and equivalent visual instructions. Proposed agent-based evaluation framework using web agents to simulate users and measure functional correctness and user experience.

Result: Evaluation of 20 models revealed two key challenges: (1) significant forgetting issue where models overwrite previous features, and (2) persistent difficulty interpreting visual feedback, especially for open-source VLMs. Proposed AceCoder baseline reduced forgetting to nearly zero and improved performance by up to 9.3% (56.0% to 65.3%).

Conclusion: FronTalk provides a foundation for future research in front-end development and multi-turn, multi-modal code generation, addressing under-explored interaction dynamics and offering solutions to critical challenges like feature forgetting.

Abstract: We present FronTalk, a benchmark for front-end code generation that pioneers the study of a unique interaction dynamic: conversational code generation with multi-modal feedback. In front-end development, visual artifacts such as sketches, mockups and annotated creenshots are essential for conveying design intent, yet their role in multi-turn code generation remains largely unexplored. To address this gap, we focus on the front-end development task and curate FronTalk, a collection of 100 multi-turn dialogues derived from real-world websites across diverse domains such as news, finance, and art. Each turn features both a textual instruction and an equivalent visual instruction, each representing the same user intent. To comprehensively evaluate model performance, we propose a novel agent-based evaluation framework leveraging a web agent to simulate users and explore the website, and thus measuring both functional correctness and user experience. Evaluation of 20 models reveals two key challenges that are under-explored systematically in the literature: (1) a significant forgetting issue where models overwrite previously implemented features, resulting in task failures, and (2) a persistent challenge in interpreting visual feedback, especially for open-source vision-language models (VLMs). We propose a strong baseline to tackle the forgetting issue with AceCoder, a method that critiques the implementation of every past instruction using an autonomous web agent. This approach significantly reduces forgetting to nearly zero and improves the performance by up to 9.3% (56.0% to 65.3%). Overall, we aim to provide a solid foundation for future research in front-end development and the general interaction dynamics of multi-turn, multi-modal code generation. Code and data are released at https://github.com/shirley-wu/frontalk

</details>


### [8] [STDD:Spatio-Temporal Dynamics-Driven Token Refinement in Diffusion Language Models](https://arxiv.org/abs/2601.04205)
*Xinhao Sun,Maoliang Li,Zihao Zheng,Jiayu Chen,Hezhao Xu,Yun Liang,Xiang Chen*

Main category: cs.CL

TL;DR: A novel adaptive remasking strategy for diffusion language models that dynamically adjusts per-token confidence thresholds based on temporal variance and spatial deviance, achieving up to 8.9× speedup while maintaining generation quality.


<details>
  <summary>Details</summary>
Motivation: Current diffusion language models use fixed global confidence thresholds for remasking, which overlooks temporal and spatial dynamics of individual tokens, leading to redundant iterations and constrained parallelism.

Method: Proposes a dynamic remasking approach that detects Temporal Variance (convergence status) and Spatial Deviance (inter-token correlations) for each token, then adaptively adjusts confidence thresholds per token at every denoising step.

Result: Significantly improves DLM operational efficiency across mainstream datasets, achieving speedups of up to 8.9 times while faithfully preserving generation quality.

Conclusion: The adaptive threshold remasking strategy based on temporal variance and spatial deviance effectively addresses limitations of fixed-threshold approaches, enabling more efficient and high-quality text generation with diffusion language models.

Abstract: Unlike autoregressive language models, diffusion language models (DLMs) generate text by iteratively denoising all token positions in parallel. At each timestep, the remasking strategy of a DLM selects low-priority tokens to defer their decoding, thereby improving both efficiency and output quality. However, mainstream remasking strategies rely on a single global confidence threshold, overlooking the temporal and spatial dynamics of individual tokens. Motivated by the redundant iterations and constrained parallelism introduced by fixed-threshold remasking, we propose a novel remasking approach that dynamically detects Temporal Variance and Spatial Deviance of each token, which reflect its convergence status and inter-token correlations. Using these signals, our method adaptively adjusts the confidence threshold for every token at every step. Empirical results show that our approach significantly improves the operational efficiency of DLMs across mainstream datasets, achieving speedups of up to 8.9 times while faithfully preserving generation quality.

</details>


### [9] [Enhancing Admission Inquiry Responses with Fine-Tuned Models and Retrieval-Augmented Generation](https://arxiv.org/abs/2601.04206)
*Aram Virabyan*

Main category: cs.CL

TL;DR: AI system combining fine-tuned language model with RAG for university admissions inquiry management, improving response time and accuracy.


<details>
  <summary>Details</summary>
Motivation: University admissions offices struggle with high inquiry volumes while maintaining response quality, which affects prospective students' perceptions. Current RAG systems have limitations in narrow, complex domains like admissions due to intricate rules and specific details.

Method: Proposed hybrid AI system integrating fine-tuned language model with Retrieval-Augmented Generation (RAG). Fine-tuned the model on curated admissions-specific dataset to enhance domain understanding. Explored optimization strategies for response generation logic to balance quality and speed.

Result: The approach leverages RAG's ability to access up-to-date information and fine-tuning's capacity to embed nuanced domain understanding. System aims to generate contextually adequate, domain-relevant responses for admissions communications.

Conclusion: The hybrid AI system addresses limitations of standard RAG in complex domains like university admissions, potentially improving inquiry management efficiency while maintaining high-quality, accurate responses.

Abstract: University admissions offices face the significant challenge of managing high volumes of inquiries efficiently while maintaining response quality, which critically impacts prospective students' perceptions. This paper addresses the issues of response time and information accuracy by proposing an AI system integrating a fine-tuned language model with Retrieval-Augmented Generation (RAG). While RAG effectively retrieves relevant information from large datasets, its performance in narrow, complex domains like university admissions can be limited without adaptation, potentially leading to contextually inadequate responses due to the intricate rules and specific details involved. To overcome this, we fine-tuned the model on a curated dataset specific to admissions processes, enhancing its ability to interpret RAG-provided data accurately and generate domain-relevant outputs. This hybrid approach leverages RAG's ability to access up-to-date information and fine-tuning's capacity to embed nuanced domain understanding. We further explored optimization strategies for the response generation logic, experimenting with settings to balance response quality and speed, aiming for consistently high-quality outputs that meet the specific requirements of admissions communications.

</details>


### [10] [Ideology as a Problem: Lightweight Logit Steering for Annotator-Specific Alignment in Social Media Analysis](https://arxiv.org/abs/2601.04207)
*Wei Xia,Haowen Tang,Luozheng Li*

Main category: cs.CL

TL;DR: A lightweight linear probe quantifies and corrects misalignment between LLMs' internal political ideology representations and human ideological space, enabling efficient model alignment without retraining.


<details>
  <summary>Details</summary>
Motivation: LLMs internally represent political ideology in ways that are systematically misaligned with human ideological space, and this misalignment varies by model. There's a need for practical, low-cost methods to align models with specific user opinions without compromising their reasoning capabilities.

Method: Introduces a lightweight linear probe that measures the misalignment between LLM internal political ideology representations and human ideological space. Calculates a bias score from the model's internal features and directly adjusts the final output probabilities, avoiding full model retraining.

Result: The method successfully quantifies model-specific ideological misalignment and minimally corrects output layers to align with human ideological space while preserving the model's original reasoning power.

Conclusion: The proposed linear probe offers a practical, low-cost solution for aligning LLMs with user opinions by measuring and correcting systematic ideological misalignment without requiring full model retraining.

Abstract: LLMs internally organize political ideology along low-dimensional structures that are partially, but not fully aligned with human ideological space. This misalignment is systematic, model specific, and measurable. We introduce a lightweight linear probe that both quantifies the misalignment and minimally corrects the output layer. This paper introduces a simple and efficient method for aligning models with specific user opinions. Instead of retraining the model, we calculated a bias score from its internal features and directly adjusted the final output probabilities. This solution is practical and low-cost and preserves the original reasoning power of the model.

</details>


### [11] [LLMs for Explainable Business Decision-Making: A Reinforcement Learning Fine-Tuning Approach](https://arxiv.org/abs/2601.04208)
*Xiang Cheng,Wen Wang,Anindya Ghose*

Main category: cs.CL

TL;DR: LEXMA is a reinforcement learning framework that fine-tunes LLMs to generate narrative explanations for AI decisions that are both accurate and tailored to different audiences (experts vs. consumers), without needing human-annotated explanation data.


<details>
  <summary>Details</summary>
Motivation: Current explainable AI methods using numerical feature attributions fail to provide coherent narratives for AI decisions. LLMs offer potential for natural-language explanations but face challenges: ensuring explanations are both decision-correct and faithful to prediction factors, serving multiple audiences without changing decision rules, and training without large human-scored explanation datasets.

Method: LEXMA uses reinforcement learning with reflection-augmented supervised fine-tuning and two stages of Group Relative Policy Optimization (GRPO). It fine-tunes two separate parameter sets to improve decision correctness and satisfy stylistic requirements for different audiences, using reward signals that don't rely on human-annotated explanations.

Result: LEXMA significantly improves predictive performance compared to other LLM baselines in mortgage approval decisions. Human evaluations show expert-facing explanations are more risk-focused, while consumer-facing explanations are clearer, more actionable, and more polite.

Conclusion: LEXMA provides a cost-efficient, systematic LLM fine-tuning approach to enhance explanation quality for business decisions, offering strong potential for scalable deployment of transparent AI systems.

Abstract: Artificial Intelligence (AI) models increasingly drive high-stakes consumer interactions, yet their decision logic often remains opaque. Prevailing explainable AI techniques rely on post hoc numerical feature attributions, which fail to provide coherent narratives behind model decisions. Large language models (LLMs) present an opportunity to generate natural-language explanations, but three design challenges remain unresolved: explanations must be both decision-correct and faithful to the factors that drive the prediction; they should be able to serve multiple audiences without shifting the underlying decision rule; and they should be trained in a label-efficient way that does not depend on large corpora of human-scored explanations. To address these challenges, we introduce LEXMA (LLM-based EXplanations for Multi-Audience decisions), a reinforcement-learning-based fine-tuning framework that produces narrative-driven, audience-appropriate explanations. LEXMA combines reflection-augmented supervised fine-tuning with two stages of Group Relative Policy Optimization (GRPO). Specifically, it fine-tunes two separate parameter sets to improve decision correctness and satisfy stylistic requirements for different audiences, using reward signals that do not rely on human-annotated explanations. We instantiate LEXMA in the context of mortgage approval decisions. Results demonstrate that LEXMA yields significant improvements in predictive performance compared with other LLM baselines. Moreover, human evaluations show that expert-facing explanations generated by our approach are more risk-focused, and consumer-facing explanations are clearer, more actionable, and more polite. Our study contributes a cost-efficient, systematic LLM fine-tuning approach to enhance explanation quality for business decisions, offering strong potential for scalable deployment of transparent AI systems.

</details>


### [12] [Leveraging Language Models and RAG for Efficient Knowledge Discovery in Clinical Environments](https://arxiv.org/abs/2601.04209)
*Seokhwan Ko,Donghyeon Lee,Jaewoo Chun,Hyungsoo Han,Junghwan Cho*

Main category: cs.CL

TL;DR: A local RAG system using PubMedBERT and LLaMA3 for recommending research collaborators based on PubMed publications within hospital privacy constraints.


<details>
  <summary>Details</summary>
Motivation: LLMs are valuable in medical settings but hospital privacy/security regulations require local data processing. Need for systems that can operate within fully local infrastructures while supporting biomedical knowledge discovery.

Method: Developed a retrieval-augmented generation (RAG) system using PubMedBERT for domain-specific embeddings and locally deployed LLaMA3 model for generative synthesis. System recommends research collaborators based on PubMed publications from institution members.

Result: Demonstrated feasibility and utility of integrating domain-specialized encoders (PubMedBERT) with lightweight LLMs (LLaMA3) for biomedical knowledge discovery under local deployment constraints.

Conclusion: The system successfully addresses privacy requirements while enabling effective research collaborator recommendations, showing that specialized encoders combined with lightweight LLMs can support biomedical workflows within local infrastructure constraints.

Abstract: Large language models (LLMs) are increasingly recognized as valuable tools across the medical environment, supporting clinical, research, and administrative workflows. However, strict privacy and network security regulations in hospital settings require that sensitive data be processed within fully local infrastructures. Within this context, we developed and evaluated a retrieval-augmented generation (RAG) system designed to recommend research collaborators based on PubMed publications authored by members of a medical institution. The system utilizes PubMedBERT for domain-specific embedding generation and a locally deployed LLaMA3 model for generative synthesis. This study demonstrates the feasibility and utility of integrating domain-specialized encoders with lightweight LLMs to support biomedical knowledge discovery under local deployment constraints.

</details>


### [13] [Complexity Agnostic Recursive Decomposition of Thoughts](https://arxiv.org/abs/2601.04210)
*Kaleem Ullah Qasim,Jiashu Zhang,Hafiz Saif Ur Rehman*

Main category: cs.CL

TL;DR: CARD framework predicts problem complexity before generation and adapts decomposition strategies accordingly, achieving higher accuracy with significant token efficiency gains on reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Large language models often fail on multi-step reasoning due to fixed reasoning strategies that ignore problem-specific difficulty, leading to inefficient resource allocation and suboptimal performance.

Method: CARD uses MRCE (Multi-dimensional Reasoning Complexity Estimator), a 0.6B Qwen model predicting 30 fine-grained features, followed by two-stage recursive solving: (1) hierarchical decomposition into K steps based on task profile, and (2) per-step thought budget allocation (1, 5-9, or 10 thoughts) via recursive MRCE profiling.

Result: On GSM8K: 81.4% to 89.2% accuracy with 1.88x to 2.40x token reduction. On MATH-500: 75.1% to 86.8% accuracy with 1.71x to 5.74x fewer tokens compared to fixed decomposition baselines.

Conclusion: Preemptive complexity estimation enables both higher accuracy and significant efficiency gains by adapting decomposition strategies to problem-specific difficulty rather than using fixed reasoning approaches.

Abstract: Large language models often fail on multi-step reasoning due to fixed reasoning strategies that ignore problem specific difficulty. We introduce CARD (Complexity Agnostic Recursive Decomposition), a framework that predicts problem complexity before generation and adapts decomposition accordingly. Our system comprises MRCE (Multi-dimensional Reasoning Complexity Estimator), a 0.6B Qwen model predicting 30 fine-grained features from question text and a two-stage recursive solver: (1) hierarchical decomposition into K steps based on task profile and (2) per-step thought budget allocation (1, 5-9, or 10 thoughts) via recursive MRCE profiling. Evaluated on three reasoning models (Qwen3-0.6B, DeepSeek-R1-Distill-Qwen-1.5B, Qwen3-1.7B), CARD achieves 81.4% to 89.2% accuracy on GSM8K while reducing token cost by 1.88x to 2.40x compared to fixed decomposition baselines. On MATH-500, CARD reaches 75.1 to 86.8% accuracy using 1.71x to 5.74x fewer tokens. Our results demonstrate that preemptive complexity estimation enables both higher accuracy and significant efficiency gains.

</details>


### [14] [Qwerty AI: Explainable Automated Age Rating and Content Safety Assessment for Russian-Language Screenplays](https://arxiv.org/abs/2601.04211)
*Nikita Zmanovskii*

Main category: cs.CL

TL;DR: Qwerty AI is an automated system for age-rating Russian screenplays according to Russian law, using fine-tuned Phi-3-mini model to detect content violations and assign age ratings with explanations.


<details>
  <summary>Details</summary>
Motivation: Address the need for automated content-safety assessment of Russian-language screenplays according to Federal Law No. 436-FZ, solving real editorial challenges in the Russian media industry with production-ready constraints.

Method: End-to-end system that processes full scripts, segments them into narrative units, detects violations across five categories (violence, sexual content, profanity, substances, frightening elements), using fine-tuned Phi-3-mini model with 4-bit quantization, deployed on Yandex Cloud with CUDA acceleration.

Result: Achieved 80% rating accuracy and 80-95% segmentation precision (format-dependent), processing up to 700 pages in under 2 minutes, meeting constraints of no external API calls, 80GB VRAM limit, and <5 minute processing time.

Conclusion: Qwerty AI demonstrates practical applicability for production workflows in Russian media industry, developed during Wink hackathon to address real editorial challenges with explainable age rating justifications.

Abstract: We present Qwerty AI, an end-to-end system for automated age-rating and content-safety assessment of Russian-language screenplays according to Federal Law No. 436-FZ. The system processes full-length scripts (up to 700 pages in under 2 minutes), segments them into narrative units, detects content violations across five categories (violence, sexual content, profanity, substances, frightening elements), and assigns age ratings (0+, 6+, 12+, 16+, 18+) with explainable justifications. Our implementation leverages a fine-tuned Phi-3-mini model with 4-bit quantization, achieving 80% rating accuracy and 80-95% segmentation precision (format-dependent). The system was developed under strict constraints: no external API calls, 80GB VRAM limit, and <5 minute processing time for average scripts. Deployed on Yandex Cloud with CUDA acceleration, Qwerty AI demonstrates practical applicability for production workflows. We achieved these results during the Wink hackathon (November 2025), where our solution addressed real editorial challenges in the Russian media industry.

</details>


### [15] [TrueBrief: Faithful Summarization through Small Language Models](https://arxiv.org/abs/2601.04212)
*Kumud Lakara,Ruibo Shi,Fran Silavong*

Main category: cs.CL

TL;DR: TrueBrief is a framework that enhances faithfulness of small LLMs for text summarization using preference optimization with controlled hallucination injection.


<details>
  <summary>Details</summary>
Motivation: LLMs often produce hallucinations, which is problematic for security-critical applications. Small LLMs need improvement in faithfulness for reliable text summarization.

Method: End-to-end framework with data generation module for controlled hallucination injection to create synthetic preference data, using preference-optimization paradigm.

Result: Provides insights into how data quality and model size affect preference-based optimization, identifying conditions where these methods work best.

Conclusion: TrueBrief effectively enhances faithfulness of small LLMs for summarization through controlled hallucination injection and preference optimization.

Abstract: Large language models (LLMs) have exhibited remarkable proficiency in generating high-quality text; however, their propensity for producing hallucinations poses a significant challenge for their deployment in security-critical domains. In this work, we present TrueBrief, an end-to-end framework specifically designed to enhance the faithfulness of small LLMs (SLMs) primarily for the task of text summarization through a preference-optimization paradigm. Central to our framework is a data generation module that facilitates controlled hallucination injection to generate synthetic preference data. Our work provides insights into the impact of data quality and model size on preference-based optimization, highlighting the conditions under which these methods are most effective.

</details>


### [16] [AnimatedLLM: Explaining LLMs with Interactive Visualizations](https://arxiv.org/abs/2601.04213)
*Zdeněk Kasner,Ondřej Dušek*

Main category: cs.CL

TL;DR: AnimatedLLM is an interactive web app that visualizes Transformer LLM mechanics step-by-step for educational purposes.


<details>
  <summary>Details</summary>
Motivation: LLMs are central to NLP education but there's a lack of materials showing their internal mechanics and how they work.

Method: Created an interactive web application that runs entirely in browser, using pre-computed traces of open LLMs applied on manually curated inputs to provide step-by-step visualizations.

Result: Available at https://animatedllm.github.io as both a teaching aid and self-educational tool, making LLM mechanics accessible through visualization.

Conclusion: AnimatedLLM addresses the educational gap by providing interactive visualizations of Transformer language models to help students and learners understand LLM mechanics.

Abstract: Large language models (LLMs) are becoming central to natural language processing education, yet materials showing their mechanics are sparse. We present AnimatedLLM, an interactive web application that provides step-by-step visualizations of a Transformer language model. AnimatedLLM runs entirely in the browser, using pre-computed traces of open LLMs applied on manually curated inputs. The application is available at https://animatedllm.github.io, both as a teaching aid and for self-educational purposes.

</details>


### [17] [From Domains to Instances: Dual-Granularity Data Synthesis for LLM Unlearning](https://arxiv.org/abs/2601.04278)
*Xiaoyu Xu,Minxin Du,Zitong Li,Zi Liang,Zhibiao Guo,Shiyu Zhang,Peizhao Hu,Qingqing Ye,Haibo Hu*

Main category: cs.CL

TL;DR: BiForget is an automated framework for synthesizing high-quality forget sets for LLM unlearning by using the target model itself to generate data matching its internal knowledge distribution.


<details>
  <summary>Details</summary>
Motivation: Current machine unlearning benchmarks often fail to faithfully represent the true "forgetting scope" learned by models, making it difficult to properly evaluate unlearning effectiveness for removing private, harmful, or copyrighted content from LLMs.

Method: BiForget formalizes two unlearning granularities (domain-level and instance-level) and uses the target model itself through seed-guided and adversarial prompting to elicit data that matches its internal knowledge distribution, rather than relying on external generators.

Result: BiForget achieves superior balance of relevance, diversity, and efficiency across diverse benchmarks. In Harry Potter domain, it improves relevance by ~20 and diversity by ~0.05 while halving total data size compared to state-of-the-art methods.

Conclusion: BiForget facilitates more robust forgetting and better utility preservation, providing a more rigorous foundation for evaluating LLM unlearning by generating high-quality forget sets that better match what models have actually learned.

Abstract: Although machine unlearning is essential for removing private, harmful, or copyrighted content from LLMs, current benchmarks often fail to faithfully represent the true "forgetting scope" learned by the model. We formalize two distinct unlearning granularities, domain-level and instance-level, and propose BiForget, an automated framework for synthesizing high-quality forget sets. Unlike prior work relying on external generators, BiForget exploits the target model per se to elicit data that matches its internal knowledge distribution through seed-guided and adversarial prompting. Our experiments across diverse benchmarks show that it achieves a superior balance of relevance, diversity, and efficiency. Quantitatively, in the Harry Potter domain, it improves relevance by ${\sim}20$ and diversity by ${\sim}$0.05 while halving the total data size compared to SOTAs. Ultimately, it facilitates more robust forgetting and better utility preservation, providing a more rigorous foundation for evaluating LLM unlearning.

</details>


### [18] [RIGOURATE: Quantifying Scientific Exaggeration with Evidence-Aligned Claim Evaluation](https://arxiv.org/abs/2601.04350)
*Joseph James,Chenghao Xiao,Yucheng Li,Nafise Sadat Moosavi,Chenghua Lin*

Main category: cs.CL

TL;DR: RIGOURATE is a multimodal framework that retrieves evidence from papers and scores claim overstatement, using a dataset of 10K claim-evidence sets from ICLR/NeurIPS papers with LLM annotations validated by human evaluation.


<details>
  <summary>Details</summary>
Motivation: Scientific papers often overstate claims beyond what their results support, sidelining scientific rigor in favor of bold statements. There's a need to operationalize evidential proportionality and support clearer, more transparent scientific communication.

Method: Two-stage multimodal framework: 1) Fine-tuned reranker for evidence retrieval from paper bodies, 2) Fine-tuned model to predict overstatement scores with justification. Uses dataset of 10K claim-evidence sets from ICLR/NeurIPS papers annotated by eight LLMs, with scores calibrated using peer-review comments.

Result: RIGOURATE enables improved evidence retrieval and overstatement detection compared to strong baselines. The framework was validated through human evaluation.

Conclusion: The work operationalizes evidential proportionality and supports clearer, more transparent scientific communication by providing a systematic way to assess claim overstatement in scientific papers.

Abstract: Scientific rigour tends to be sidelined in favour of bold statements, leading authors to overstate claims beyond what their results support. We present RIGOURATE, a two-stage multimodal framework that retrieves supporting evidence from a paper's body and assigns each claim an overstatement score. The framework consists of a dataset of over 10K claim-evidence sets from ICLR and NeurIPS papers, annotated using eight LLMs, with overstatement scores calibrated using peer-review comments and validated through human evaluation. It employes a fine-tuned reranker for evidence retrieval and a fine-tuned model to predict overstatement scores with justification. Compared to strong baselines, RIGOURATE enables improved evidence retrieval and overstatement detection. Overall, our work operationalises evidential proportionality and supports clearer, more transparent scientific communication.

</details>


### [19] [Dialect Matters: Cross-Lingual ASR Transfer for Low-Resource Indic Language Varieties](https://arxiv.org/abs/2601.04373)
*Akriti Dhasmana,Aarohi Srivastava,David Chiang*

Main category: cs.CL

TL;DR: Cross-lingual ASR transfer study on Indic dialects shows phylogenetic distance alone doesn't fully explain performance; fine-tuning on small dialectal data can match larger high-resource language fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To understand cross-lingual transfer in ASR systems for spontaneous, noisy, code-mixed speech across diverse Indic dialects and language varieties, particularly examining how phylogenetic relationships affect performance in dialectal settings.

Method: Empirical study using spontaneous, noisy, code-mixed speech across Indic dialects; includes case study on low-resource Garhwali language; evaluates multiple contemporary ASR models; analyzes transcription errors to examine bias toward pre-training languages.

Result: ASR performance improves with reduced phylogenetic distance between languages, but this factor alone doesn't fully explain dialectal performance. Fine-tuning on smaller dialectal data often yields comparable performance to fine-tuning on larger amounts of phylogenetically-related high-resource languages.

Conclusion: Phylogenetic distance is insufficient to explain ASR performance in dialectal settings; smaller dialectal datasets can be as effective as larger high-resource language data; transcription error analysis reveals bias toward pre-training languages, highlighting challenges for ASR on non-standardized speech.

Abstract: We conduct an empirical study of cross-lingual transfer using spontaneous, noisy, and code-mixed speech across a wide range of Indic dialects and language varieties. Our results indicate that although ASR performance is generally improved with reduced phylogenetic distance between languages, this factor alone does not fully explain performance in dialectal settings. Often, fine-tuning on smaller amounts of dialectal data yields performance comparable to fine-tuning on larger amounts of phylogenetically-related, high-resource standardized languages. We also present a case study on Garhwali, a low-resource Pahari language variety, and evaluate multiple contemporary ASR models. Finally, we analyze transcription errors to examine bias toward pre-training languages, providing additional insight into challenges faced by ASR systems on dialectal and non-standardized speech.

</details>


### [20] [Disco-RAG: Discourse-Aware Retrieval-Augmented Generation](https://arxiv.org/abs/2601.04377)
*Dongqi Liu,Hang Ding,Qiming Feng,Jian Li,Xurong Xie,Zhucun Xue,Chengjie Wang,Jiangning Zhang,Yabiao Wang*

Main category: cs.CL

TL;DR: Disco-RAG: A discourse-aware RAG framework that uses discourse trees and rhetorical graphs to capture structural relationships between retrieved passages, improving knowledge synthesis in LLMs.


<details>
  <summary>Details</summary>
Motivation: Existing RAG strategies treat retrieved passages in a flat, unstructured way, which prevents models from capturing structural cues and constrains their ability to synthesize knowledge from dispersed evidence across documents.

Method: Constructs intra-chunk discourse trees to capture local hierarchies and builds inter-chunk rhetorical graphs to model cross-passage coherence. These structures are jointly integrated into a planning blueprint that conditions the generation process.

Result: Achieves state-of-the-art results on question answering and long-document summarization benchmarks without fine-tuning, demonstrating the efficacy of the discourse-aware approach.

Conclusion: Discourse structure plays an important role in advancing RAG systems, and explicitly injecting discourse signals into the generation process significantly enhances performance on knowledge-intensive tasks.

Abstract: Retrieval-Augmented Generation (RAG) has emerged as an important means of enhancing the performance of large language models (LLMs) in knowledge-intensive tasks. However, most existing RAG strategies treat retrieved passages in a flat and unstructured way, which prevents the model from capturing structural cues and constrains its ability to synthesize knowledge from dispersed evidence across documents. To overcome these limitations, we propose Disco-RAG, a discourse-aware framework that explicitly injects discourse signals into the generation process. Our method constructs intra-chunk discourse trees to capture local hierarchies and builds inter-chunk rhetorical graphs to model cross-passage coherence. These structures are jointly integrated into a planning blueprint that conditions the generation. Experiments on question answering and long-document summarization benchmarks show the efficacy of our approach. Disco-RAG achieves state-of-the-art results on the benchmarks without fine-tuning. These findings underscore the important role of discourse structure in advancing RAG systems.

</details>


### [21] [MiJaBench: Revealing Minority Biases in Large Language Models via Hate Speech Jailbreaking](https://arxiv.org/abs/2601.04389)
*Iago Alves Brito,Walcy Santos Rezende Rios,Julia Soares Dollis,Diogo Fernandes Costa Silva,Arlindo Rodrigues Galvão Filho*

Main category: cs.CL

TL;DR: MiJaBench reveals that LLM safety alignment is not universal but creates demographic hierarchies, with defense rates varying up to 33% based on target group, and scaling worsens these disparities.


<details>
  <summary>Details</summary>
Motivation: Current safety evaluations of LLMs create a dangerous illusion of universality by aggregating hate into scalar scores that mask systemic vulnerabilities against specific minority populations.

Method: Introduce MiJaBench, a bilingual (English/Portuguese) adversarial benchmark with 44,000 prompts across 16 minority groups, generating 528,000 prompt-response pairs from 12 state-of-the-art LLMs to create MiJaBench-Align.

Result: Safety alignment is not a generalized semantic capability but a demographic hierarchy - defense rates fluctuate by up to 33% within the same model based on target group. Model scaling exacerbates these disparities, showing current alignment reinforces memorized refusal boundaries only for specific groups.

Conclusion: Current alignment techniques don't create principles of non-discrimination but reinforce selective refusal boundaries, challenging scaling laws of security. Need research into granular demographic alignment.

Abstract: Current safety evaluations of large language models (LLMs) create a dangerous illusion of universality, aggregating "Identity Hate" into scalar scores that mask systemic vulnerabilities against specific populations. To expose this selective safety, we introduce MiJaBench, a bilingual (English and Portuguese) adversarial benchmark comprising 44,000 prompts across 16 minority groups. By generating 528,000 prompt-response pairs from 12 state-of-the-art LLMs, we curate MiJaBench-Align, revealing that safety alignment is not a generalized semantic capability but a demographic hierarchy: defense rates fluctuate by up to 33\% within the same model solely based on the target group. Crucially, we demonstrate that model scaling exacerbates these disparities, suggesting that current alignment techniques do not create principle of non-discrimination but reinforces memorized refusal boundaries only for specific groups, challenging the current scaling laws of security. We release all datasets and scripts to encourage research into granular demographic alignment at GitHub.

</details>


### [22] [ARREST: Adversarial Resilient Regulation Enhancing Safety and Truth in Large Language Models](https://arxiv.org/abs/2601.04394)
*Sharanya Dasgupta,Arkaprabha Basu,Sujoy Nath,Swagatam Das*

Main category: cs.CL

TL;DR: ARREST is a unified framework that corrects factual and safety misalignments in LLMs by identifying and regulating drifted features in latent activation space, using an external network to intervene without fine-tuning model parameters.


<details>
  <summary>Details</summary>
Motivation: LLMs lack human cognition's ability to self-correct between imagination and reality, leading to factual and safety failures. Current approaches treat these as separate alignment issues, but the authors argue both arise from representational misalignment in latent activation space.

Method: Proposes ARREST framework with an external network trained to understand fluctuations in activation space. It selectively intervenes to regulate falsehood into truthfulness and unsafe outputs into safe outputs without fine-tuning model parameters, using both soft/hard refusals and factual corrections.

Result: ARREST effectively regulates misalignment and is more versatile than RLHF-aligned models in generating soft refusals due to adversarial training. The framework demonstrates capability to correct both factual and safety issues through activation space intervention.

Conclusion: Factual and safety failures in LLMs stem from representational misalignment in latent space, which can be corrected by an external regulatory network. ARREST provides a unified approach to address both issues without model fine-tuning, offering improved versatility over traditional alignment methods.

Abstract: Human cognition, driven by complex neurochemical processes, oscillates between imagination and reality and learns to self-correct whenever such subtle drifts lead to hallucinations or unsafe associations. In recent years, LLMs have demonstrated remarkable performance in a wide range of tasks. However, they still lack human cognition to balance factuality and safety. Bearing the resemblance, we argue that both factual and safety failures in LLMs arise from a representational misalignment in their latent activation space, rather than addressing those as entirely separate alignment issues. We hypothesize that an external network, trained to understand the fluctuations, can selectively intervene in the model to regulate falsehood into truthfulness and unsafe output into safe output without fine-tuning the model parameters themselves. Reflecting the hypothesis, we propose ARREST (Adversarial Resilient Regulation Enhancing Safety and Truth), a unified framework that identifies and corrects drifted features, engaging both soft and hard refusals in addition to factual corrections. Our empirical results show that ARREST not only regulates misalignment but is also more versatile compared to the RLHF-aligned models in generating soft refusals due to adversarial training. We make our codebase available at https://github.com/sharanya-dasgupta001/ARREST.

</details>


### [23] [Interpreting Transformers Through Attention Head Intervention](https://arxiv.org/abs/2601.04398)
*Mason Kadem,Rong Zheng*

Main category: cs.CL

TL;DR: The paper argues for mechanistic interpretability of neural networks to understand their decision-making processes, enabling accountability, studying digital cognition, and discovering new knowledge when AI outperforms humans.


<details>
  <summary>Details</summary>
Motivation: Neural networks are becoming increasingly capable but their internal mechanisms remain poorly understood. This lack of understanding creates challenges for accountability in high-stakes applications, prevents study of how cognition emerges in digital systems, and misses opportunities to learn from AI systems that outperform humans.

Method: The paper advocates for mechanistic interpretability - the systematic study of neural network decision-making processes at the mechanistic level, though specific methods are not detailed in the abstract.

Result: The abstract presents a conceptual framework highlighting three key benefits of mechanistic interpretability: enabling accountability and control in high-stakes domains, facilitating study of digital brains and emergent cognition, and allowing discovery of new knowledge from superior AI systems.

Conclusion: Mechanistic interpretability is essential for understanding neural networks, providing accountability, enabling scientific study of digital cognition, and unlocking new knowledge discovery when AI systems surpass human capabilities.

Abstract: Neural networks are growing more capable on their own, but we do not understand their neural mechanisms. Understanding these mechanisms' decision-making processes, or mechanistic interpretability, enables (1) accountability and control in high-stakes domains, (2) the study of digital brains and the emergence of cognition, and (3) discovery of new knowledge when AI systems outperform humans.

</details>


### [24] [Gavel: Agent Meets Checklist for Evaluating LLMs on Long-Context Legal Summarization](https://arxiv.org/abs/2601.04424)
*Yao Dou,Wei Xu*

Main category: cs.CL

TL;DR: The paper introduces Gavel-Ref, a reference-based evaluation framework for multi-document legal case summarization, and Gavel-Agent, an autonomous agent scaffold that helps LLMs navigate long legal documents more efficiently.


<details>
  <summary>Details</summary>
Motivation: While LLMs now support contexts up to 1M tokens, their effectiveness on complex long-context tasks like multi-document legal case summarization (100K-500K tokens) remains unclear. Current evaluations provide only single aggregate scores, lacking systematic assessment of model capabilities on such challenging tasks.

Method: 1) Introduced Gavel-Ref: a reference-based evaluation framework with multi-value checklist evaluation over 26 items, plus residual fact and writing-style evaluations. 2) Systematically evaluated 12 frontier LLMs on 100 legal cases (32K-512K tokens). 3) Developed Gavel-Agent: an efficient autonomous agent scaffold with six tools to help LLMs navigate and extract checklists directly from case documents.

Result: Even the strongest model (Gemini 2.5 Pro) achieved only around 50% of SGavel-Ref score. Models performed well on simple checklist items (e.g., filing date) but struggled on multi-value or rare items like settlements and monitor reports. Gavel-Agent with Qwen3 reduced token usage by 36% while resulting in only a 7% drop in Schecklist compared to end-to-end extraction with GPT-4.1.

Conclusion: Multi-document legal case summarization remains extremely challenging for current LLMs despite their expanded context windows. The proposed Gavel-Ref framework provides more granular evaluation, and Gavel-Agent demonstrates that agent-based approaches can significantly improve efficiency while maintaining performance on complex long-context tasks.

Abstract: Large language models (LLMs) now support contexts of up to 1M tokens, but their effectiveness on complex long-context tasks remains unclear. In this paper, we study multi-document legal case summarization, where a single case often spans many documents totaling 100K-500K tokens. We introduce Gavel-Ref, a reference-based evaluation framework with multi-value checklist evaluation over 26 items, as well as residual fact and writing-style evaluations. Using Gavel-Ref, we go beyond the single aggregate scores reported in prior work and systematically evaluate 12 frontier LLMs on 100 legal cases ranging from 32K to 512K tokens, primarily from 2025. Our results show that even the strongest model, Gemini 2.5 Pro, achieves only around 50 of $S_{\text{Gavel-Ref}}$, highlighting the difficulty of the task. Models perform well on simple checklist items (e.g., filing date) but struggle on multi-value or rare ones such as settlements and monitor reports. As LLMs continue to improve and may surpass human-written summaries -- making human references less reliable -- we develop Gavel-Agent, an efficient and autonomous agent scaffold that equips LLMs with six tools to navigate and extract checklists directly from case documents. With Qwen3, Gavel-Agent reduces token usage by 36% while resulting in only a 7% drop in $S_{\text{checklist}}$ compared to end-to-end extraction with GPT-4.1.

</details>


### [25] [Accommodation and Epistemic Vigilance: A Pragmatic Account of Why LLMs Fail to Challenge Harmful Beliefs](https://arxiv.org/abs/2601.04435)
*Myra Cheng,Robert D. Hawkins,Dan Jurafsky*

Main category: cs.CL

TL;DR: LLMs often fail to challenge harmful user beliefs due to excessive accommodation and insufficient epistemic vigilance. Pragmatic interventions like "wait a minute" significantly improve safety performance while maintaining low false-positive rates.


<details>
  <summary>Details</summary>
Motivation: LLMs frequently fail to challenge users' harmful beliefs across domains like medical advice and social reasoning, which poses significant safety risks. The paper aims to understand these failures through a pragmatic lens, examining how LLMs default to accommodating user assumptions rather than exercising proper epistemic vigilance.

Method: The study analyzes how social and linguistic factors (at-issueness, linguistic encoding, and source reliability) influence accommodation in LLMs, similar to human communication patterns. Researchers test performance across three safety benchmarks: Cancer-Myth and SAGE-Eval (misinformation) and ELEPHANT (sycophancy). They implement simple pragmatic interventions, such as adding phrases like "wait a minute," to improve models' ability to challenge harmful beliefs.

Result: The study shows that social and linguistic factors affecting accommodation in humans similarly impact LLMs, explaining performance differences across safety benchmarks. Simple pragmatic interventions significantly improve performance on these benchmarks while preserving low false-positive rates, demonstrating that minor linguistic cues can substantially enhance LLM safety.

Conclusion: LLM failures in challenging harmful beliefs stem from pragmatic accommodation patterns rather than fundamental reasoning limitations. Considering pragmatics is crucial for both evaluating LLM behavior and improving safety. Simple interventions can effectively enhance models' epistemic vigilance without increasing false positives, offering practical approaches to address safety concerns.

Abstract: Large language models (LLMs) frequently fail to challenge users' harmful beliefs in domains ranging from medical advice to social reasoning. We argue that these failures can be understood and addressed pragmatically as consequences of LLMs defaulting to accommodating users' assumptions and exhibiting insufficient epistemic vigilance. We show that social and linguistic factors known to influence accommodation in humans (at-issueness, linguistic encoding, and source reliability) similarly affect accommodation in LLMs, explaining performance differences across three safety benchmarks that test models' ability to challenge harmful beliefs, spanning misinformation (Cancer-Myth, SAGE-Eval) and sycophancy (ELEPHANT). We further show that simple pragmatic interventions, such as adding the phrase "wait a minute", significantly improve performance on these benchmarks while preserving low false-positive rates. Our results highlight the importance of considering pragmatics for evaluating LLM behavior and improving LLM safety.

</details>


### [26] [Learning to Simulate Human Dialogue](https://arxiv.org/abs/2601.04436)
*Kanishk Gandhi,Agam Bhatia,Noah D. Goodman*

Main category: cs.CL

TL;DR: Optimizing for LLM-judge rewards decreases human-likeness in dialogue prediction, while maximizing log-probability of real human responses yields better results, especially with chain-of-thought as latent variable.


<details>
  <summary>Details</summary>
Motivation: To understand how to better model human thinking through next-turn dialogue prediction, comparing different learning approaches for predicting what people actually say in conversations.

Method: Compare learning approaches along two dimensions: (1) thinking before responding (chain-of-thought), (2) reward types (LLM-as-a-judge scoring vs. maximizing log-probability of ground-truth human dialogue). Derive lower bound on log-probability treating chain-of-thought as latent variable.

Result: Optimizing for judge-based rewards increases judge scores but decreases likelihood of ground truth and human win rates. Maximizing log-probability of observed human responses improves both log-probability and win rates. Best results come from optimizing the lower bound objective with chain-of-thought as latent variable.

Conclusion: Thinking helps primarily when trained with distribution-matching objectives grounded in real human dialogue. Scaling this approach may produce models with more nuanced understanding of human behavior.

Abstract: To predict what someone will say is to model how they think. We study this through next-turn dialogue prediction: given a conversation, predict the next utterance produced by a person. We compare learning approaches along two dimensions: (1) whether the model is allowed to think before responding, and (2) how learning is rewarded either through an LLM-as-a-judge that scores semantic similarity and information completeness relative to the ground-truth response, or by directly maximizing the log-probability of the true human dialogue. We find that optimizing for judge-based rewards indeed increases judge scores throughout training, however it decreases the likelihood assigned to ground truth human responses and decreases the win rate when human judges choose the most human-like response among a real and synthetic option. This failure is amplified when the model is allowed to think before answering. In contrast, by directly maximizing the log-probability of observed human responses, the model learns to better predict what people actually say, improving on both log-probability and win rate evaluations. Treating chain-of-thought as a latent variable, we derive a lower bound on the log-probability. Optimizing this objective yields the best results on all our evaluations. These results suggest that thinking helps primarily when trained with a distribution-matching objective grounded in real human dialogue, and that scaling this approach to broader conversational data may produce models with a more nuanced understanding of human behavior.

</details>


### [27] [Merging Triggers, Breaking Backdoors: Defensive Poisoning for Instruction-Tuned Language Models](https://arxiv.org/abs/2601.04448)
*San Kim,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: MB-Defense is a novel training pipeline that protects instruction-tuned LLMs from backdoor attacks through defensive poisoning and weight recovery stages.


<details>
  <summary>Details</summary>
Motivation: Instruction-tuned LLMs are vulnerable to backdoor attacks through poisoned training data, but existing defenses are insufficient for these models.

Method: Two-stage approach: (1) defensive poisoning merges attacker and defensive triggers into unified backdoor representation, (2) weight recovery breaks this representation through additional training to restore clean behavior.

Result: Extensive experiments show MB-Defense substantially lowers attack success rates while preserving instruction-following ability across multiple LLMs.

Conclusion: MB-Defense offers a generalizable, data-efficient defense strategy that improves robustness of instruction-tuned LLMs against unseen backdoor attacks.

Abstract: Large Language Models (LLMs) have greatly advanced Natural Language Processing (NLP), particularly through instruction tuning, which enables broad task generalization without additional fine-tuning. However, their reliance on large-scale datasets-often collected from human or web sources-makes them vulnerable to backdoor attacks, where adversaries poison a small subset of data to implant hidden behaviors. Despite this growing risk, defenses for instruction-tuned models remain underexplored. We propose MB-Defense (Merging & Breaking Defense Framework), a novel training pipeline that immunizes instruction-tuned LLMs against diverse backdoor threats. MB-Defense comprises two stages: (i) defensive poisoning, which merges attacker and defensive triggers into a unified backdoor representation, and (ii) weight recovery, which breaks this representation through additional training to restore clean behavior. Extensive experiments across multiple LLMs show that MB-Defense substantially lowers attack success rates while preserving instruction-following ability. Our method offers a generalizable and data-efficient defense strategy, improving the robustness of instruction-tuned LLMs against unseen backdoor attacks.

</details>


### [28] [Users Mispredict Their Own Preferences for AI Writing Assistance](https://arxiv.org/abs/2601.04461)
*Vivian Lai,Zana Buçinca,Nil-Jana Akpinar,Mo Houtti,Hyeonsu B. Kang,Kevin Chian,Namjoon Suh,Alex C. Williams*

Main category: cs.CL

TL;DR: Users' stated preferences for proactive AI writing assistants don't match their actual behavior - they say urgency matters most but it's the weakest behavioral driver, leading to poorly performing systems based on self-reports.


<details>
  <summary>Details</summary>
Motivation: Proactive AI writing assistants need to predict when users want drafting help, but there's a lack of empirical understanding about what drives user preferences for such assistance.

Method: Conducted a factorial vignette study with 50 participants making 750 pairwise comparisons to analyze what factors drive user preferences for proactive writing assistance.

Result: Found compositional effort dominates decisions (ρ=0.597) while urgency shows no predictive power (ρ≈0). Users exhibit a perception-behavior gap: they rank urgency first in self-reports despite it being the weakest behavioral driver. Systems designed from stated preferences achieve only 57.7% accuracy, while systems using behavioral patterns reach 61.3% (p<0.05).

Conclusion: Relying on user introspection for system design actively misleads optimization for proactive NLG systems. Behavioral data, not self-reported preferences, should guide system design to achieve better performance.

Abstract: Proactive AI writing assistants need to predict when users want drafting help, yet we lack empirical understanding of what drives preferences. Through a factorial vignette study with 50 participants making 750 pairwise comparisons, we find compositional effort dominates decisions ($ρ= 0.597$) while urgency shows no predictive power ($ρ\approx 0$). More critically, users exhibit a striking perception-behavior gap: they rank urgency first in self-reports despite it being the weakest behavioral driver, representing a complete preference inversion. This misalignment has measurable consequences. Systems designed from users' stated preferences achieve only 57.7\% accuracy, underperforming even naive baselines, while systems using behavioral patterns reach significantly higher 61.3\% ($p < 0.05$). These findings demonstrate that relying on user introspection for system design actively misleads optimization, with direct implications for proactive natural language generation (NLG) systems.

</details>


### [29] [Beyond Static Summarization: Proactive Memory Extraction for LLM Agents](https://arxiv.org/abs/2601.04463)
*Chengyuan Yang,Zequn Sun,Wei Wei,Wei Hu*

Main category: cs.CL

TL;DR: ProMem introduces proactive memory extraction with recurrent feedback loops to address limitations of traditional summary-based memory extraction in LLM agents.


<details>
  <summary>Details</summary>
Motivation: Existing summary-based memory extraction methods have two major limitations: 1) summarization is "ahead-of-time" and misses important details because it doesn't know future tasks, and 2) extraction is usually "one-off" without feedback loops, leading to accumulation of information loss.

Method: ProMem treats memory extraction as an iterative cognitive process with recurrent feedback loops. The agent uses self-questioning to actively probe dialogue history, allowing recovery of missing information and error correction.

Result: ProMem significantly improves completeness of extracted memory and QA accuracy, while achieving superior trade-off between extraction quality and token cost.

Conclusion: Proactive memory extraction with recurrent feedback loops addresses fundamental limitations of traditional summary-based approaches, enabling more effective memory management for LLM agents in long-term interaction and personalization.

Abstract: Memory management is vital for LLM agents to handle long-term interaction and personalization. Most research focuses on how to organize and use memory summary, but often overlooks the initial memory extraction stage. In this paper, we argue that existing summary-based methods have two major limitations based on the recurrent processing theory. First, summarization is "ahead-of-time", acting as a blind "feed-forward" process that misses important details because it doesn't know future tasks. Second, extraction is usually "one-off", lacking a feedback loop to verify facts, which leads to the accumulation of information loss. To address these issues, we propose proactive memory extraction (namely ProMem). Unlike static summarization, ProMem treats extraction as an iterative cognitive process. We introduce a recurrent feedback loop where the agent uses self-questioning to actively probe the dialogue history. This mechanism allows the agent to recover missing information and correct errors. Our ProMem significantly improves the completeness of the extracted memory and QA accuracy. It also achieves a superior trade-off between extraction quality and token cost.

</details>


### [30] [Concept Tokens: Learning Behavioral Embeddings Through Concept Definitions](https://arxiv.org/abs/2601.04465)
*Ignacio Sastre,Aiala Rosá*

Main category: cs.CL

TL;DR: Concept Tokens: A lightweight method that adds special tokens to frozen LLMs and learns their embeddings from concept definitions to control model behavior.


<details>
  <summary>Details</summary>
Motivation: To create a compact control mechanism for frozen LLMs that can steer model behavior using concept definitions without retraining the entire model.

Method: Add new special tokens to pretrained LLMs, learn only their embeddings from multiple natural language definitions of target concepts (with concept occurrences replaced by the new token), while keeping the LLM frozen and optimizing embeddings with standard language modeling objective.

Result: 1) Reduces hallucinations in closed-book QA (negating token increases abstentions, asserting increases hallucinations). 2) Induces recasting in language teaching with same directional effect. 3) Better preserves compliance with other instructions compared to in-context definitions. 4) Qualitative study shows embeddings capture concept information with some limitations.

Conclusion: Concept Tokens provide an effective, compact control signal learned from definitions that can steer behavior in frozen LLMs across various applications.

Abstract: We propose Concept Tokens, a lightweight method that adds a new special token to a pretrained LLM and learns only its embedding from multiple natural language definitions of a target concept, where occurrences of the concept are replaced by the new token. The LLM is kept frozen and the embedding is optimized with the standard language-modeling objective. We evaluate Concept Tokens in three settings. First, we study hallucinations in closed-book question answering on HotpotQA and find a directional effect: negating the hallucination token reduces hallucinated answers mainly by increasing abstentions, whereas asserting it increases hallucinations and lowers precision. Second, we induce recasting, a pedagogical feedback strategy for second language teaching, and observe the same directional effect. Moreover, compared to providing the full definitional corpus in-context, concept tokens better preserve compliance with other instructions (e.g., asking follow-up questions). Finally, we include a qualitative study with the Eiffel Tower and a fictional "Austral Tower" to illustrate what information the learned embeddings capture and where their limitations emerge. Overall, Concept Tokens provide a compact control signal learned from definitions that can steer behavior in frozen LLMs.

</details>


### [31] [SampoNLP: A Self-Referential Toolkit for Morphological Analysis of Subword Tokenizers](https://arxiv.org/abs/2601.04469)
*Iaroslav Chelombitko,Ekaterina Chelombitko,Aleksey Komissarov*

Main category: cs.CL

TL;DR: SampoNLP toolkit creates morphological lexicons for Uralic languages using MDL-inspired scoring, enabling systematic evaluation of BPE tokenizers with optimal vocabulary size recommendations via Integrated Performance Score metric.


<details>
  <summary>Details</summary>
Motivation: There's a lack of clean morpheme lexicons for evaluating tokenizers in morphologically rich Uralic languages, making it difficult to assess tokenization quality for these low-resource languages.

Method: Developed SampoNLP toolkit using MDL-inspired Self-Referential Atomicity Scoring to create high-purity morphological lexicons from internal structural cues without requiring corpora. Used these lexicons to systematically evaluate BPE tokenizers across vocabulary sizes (8k-256k) and proposed Integrated Performance Score (IPS) metric to balance morpheme coverage and over-splitting.

Result: Generated high-purity lexicons for Finnish, Hungarian, and Estonian. Identified optimal vocabulary sizes (elbow points) for each language through IPS curve analysis, providing first empirically grounded recommendations. Demonstrated limitations of standard BPE for highly agglutinative languages.

Conclusion: SampoNLP enables corpus-free morphological lexicon creation for low-resource languages, facilitating systematic tokenizer evaluation. The IPS metric and optimal vocabulary size recommendations provide practical guidance for working with Uralic languages, highlighting the need for improved tokenization approaches for agglutinative languages.

Abstract: The quality of subword tokenization is critical for Large Language Models, yet evaluating tokenizers for morphologically rich Uralic languages is hampered by the lack of clean morpheme lexicons.
  We introduce SampoNLP, a corpus-free toolkit for morphological lexicon creation using MDL-inspired Self-Referential Atomicity Scoring, which filters composite forms through internal structural cues - suited for low-resource settings.
  Using the high-purity lexicons generated by SampoNLP for Finnish, Hungarian, and Estonian, we conduct a systematic evaluation of BPE tokenizers across a range of vocabulary sizes (8k-256k). We propose a unified metric, the Integrated Performance Score (IPS), to navigate the trade-off between morpheme coverage and over-splitting. By analyzing the IPS curves, we identify the "elbow points" of diminishing returns and provide the first empirically grounded recommendations for optimal vocabulary sizes (k) in these languages. Our study not only offers practical guidance but also quantitatively demonstrates the limitations of standard BPE for highly agglutinative languages. The SampoNLP library and all generated resources are made publicly available: https://github.com/AragonerUA/SampoNLP

</details>


### [32] [WESR: Scaling and Evaluating Word-level Event-Speech Recognition](https://arxiv.org/abs/2601.04508)
*Chenchen Yang,Kexin Huang,Liwei Fan,Qian Tu,Botian Jiang,Dong Zhang,Linqi Yin,Shimin Li,Zhaoye Fei,Qinyuan Cheng,Xipeng Qiu*

Main category: cs.CL

TL;DR: The paper introduces WESR-Bench, a benchmark for precise localization of non-verbal vocal events in speech, addressing limitations in current methods through refined taxonomy, expert annotations, and strong baseline models.


<details>
  <summary>Details</summary>
Motivation: Current methods for non-verbal vocal event detection suffer from insufficient task definitions with limited category coverage, ambiguous temporal granularity, and lack standardized evaluation frameworks, hindering downstream applications.

Method: Developed refined taxonomy of 21 vocal events with discrete vs. continuous categorization; created WESR-Bench expert-annotated evaluation set with position-aware protocol; built 1,700+ hour corpus and trained specialized models.

Result: Created comprehensive benchmark with 900+ utterances; baseline models surpass both open-source audio-language models and commercial APIs while preserving ASR quality.

Conclusion: WESR serves as a foundational resource for future research in modeling rich, real-world auditory scenes, addressing critical gaps in non-verbal vocal event localization.

Abstract: Speech conveys not only linguistic information but also rich non-verbal vocal events such as laughing and crying. While semantic transcription is well-studied, the precise localization of non-verbal events remains a critical yet under-explored challenge. Current methods suffer from insufficient task definitions with limited category coverage and ambiguous temporal granularity. They also lack standardized evaluation frameworks, hindering the development of downstream applications. To bridge this gap, we first develop a refined taxonomy of 21 vocal events, with a new categorization into discrete (standalone) versus continuous (mixed with speech) types. Based on the refined taxonomy, we introduce WESR-Bench, an expert-annotated evaluation set (900+ utterances) with a novel position-aware protocol that disentangles ASR errors from event detection, enabling precise localization measurement for both discrete and continuous events. We also build a strong baseline by constructing a 1,700+ hour corpus, and train specialized models, surpassing both open-source audio-language models and commercial APIs while preserving ASR quality. We anticipate that WESR will serve as a foundational resource for future research in modeling rich, real-world auditory scenes.

</details>


### [33] [LinguaGame: A Linguistically Grounded Game-Theoretic Paradigm for Multi-Agent Dialogue Generation](https://arxiv.org/abs/2601.04516)
*Yuxiao Ye,Yiming Zhang,Yiran Ma,Huiyuan Xie,Huining Zhu,Zhiyuan Liu*

Main category: cs.CL

TL;DR: LinguaGame: A linguistically-grounded game-theoretic framework that improves communication efficiency in LLM-based multi-agent systems by modeling dialogue as a signalling game over communicative intents and strategies.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based Multi-Agent Systems focus mainly on architecture design (role assignment, workflow orchestration), but neglect the interaction process itself. The paper aims to improve agents' communication efficiency by helping them convey intended meaning more effectively through language.

Method: Proposes LinguaGame, a linguistically-grounded game-theoretic paradigm that models dialogue as a signalling game over communicative intents and strategies. Uses a training-free equilibrium approximation algorithm for inference-time decision adjustment. Unlike prior game-theoretic MASs, it relies on linguistically informed reasoning with minimal task-specific coupling, treating dialogue as intentional and strategic communication.

Result: Evaluated in simulated courtroom proceedings and debates, with human expert assessments showing significant gains in communication efficiency compared to existing approaches.

Conclusion: The LinguaGame framework successfully improves communication efficiency in LLM-based multi-agent systems by applying game-theoretic principles to dialogue generation, focusing on intent and strategy inference rather than task-specific objectives.

Abstract: Large Language Models (LLMs) have enabled Multi-Agent Systems (MASs) where agents interact through natural language to solve complex tasks or simulate multi-party dialogues. Recent work on LLM-based MASs has mainly focused on architecture design, such as role assignment and workflow orchestration. In contrast, this paper targets the interaction process itself, aiming to improve agents' communication efficiency by helping them convey their intended meaning more effectively through language. To this end, we propose LinguaGame, a linguistically-grounded game-theoretic paradigm for multi-agent dialogue generation. Our approach models dialogue as a signalling game over communicative intents and strategies, solved with a training-free equilibrium approximation algorithm for inference-time decision adjustment. Unlike prior game-theoretic MASs, whose game designs are often tightly coupled with task-specific objectives, our framework relies on linguistically informed reasoning with minimal task-specific coupling. Specifically, it treats dialogue as intentional and strategic communication, requiring agents to infer what others aim to achieve (intents) and how they pursue those goals (strategies). We evaluate our framework in simulated courtroom proceedings and debates, with human expert assessments showing significant gains in communication efficiency.

</details>


### [34] [GRACE: Reinforcement Learning for Grounded Response and Abstention under Contextual Evidence](https://arxiv.org/abs/2601.04525)
*Yibo Zhao,Jiapeng Zhu,Zichen Ding,Xiang Li*

Main category: cs.CL

TL;DR: GRACE is a reinforcement learning framework that simultaneously addresses two critical flaws in RAG systems: providing correct answers without evidence and fabricating responses when context is insufficient, using heterogeneous retrievers for training data and multi-stage gated rewards.


<details>
  <summary>Details</summary>
Motivation: Current RAG systems suffer from two key issues: (1) providing correct answers without explicit grounded evidence, and (2) producing fabricated responses when retrieved context is insufficient. While prior work addressed these independently, there's no unified framework integrating evidence-based grounding and reliable abstention.

Method: GRACE uses a reinforcement learning framework with: (1) data construction using heterogeneous retrievers to generate diverse training samples without manual annotation, and (2) a multi-stage gated reward function that trains the model to assess evidence sufficiency, extract key supporting evidence, and provide answers or explicitly abstain.

Result: Experiments on two benchmarks show GRACE achieves state-of-the-art overall accuracy and strikes a favorable balance between accurate response and rejection, while requiring only 10% of the annotation costs of prior methods.

Conclusion: GRACE provides a unified framework that effectively addresses both evidence grounding and abstention problems in RAG systems, achieving superior performance with significantly reduced annotation costs.

Abstract: Retrieval-Augmented Generation (RAG) integrates external knowledge to enhance Large Language Models (LLMs), yet systems remain susceptible to two critical flaws: providing correct answers without explicit grounded evidence and producing fabricated responses when the retrieved context is insufficient. While prior research has addressed these issues independently, a unified framework that integrates evidence-based grounding and reliable abstention is currently lacking. In this paper, we propose GRACE, a reinforcement-learning framework that simultaneously mitigates both types of flaws. GRACE employs a data construction method that utilizes heterogeneous retrievers to generate diverse training samples without manual annotation. A multi-stage gated reward function is then employed to train the model to assess evidence sufficiency, extract key supporting evidence, and provide answers or explicitly abstain. Experimental results on two benchmarks demonstrate that GRACE achieves state-of-the-art overall accuracy and strikes a favorable balance between accurate response and rejection, while requiring only 10% of the annotation costs of prior methods. Our code is available at https://github.com/YiboZhao624/Grace..

</details>


### [35] [BanglaLorica: Design and Evaluation of a Robust Watermarking Algorithm for Large Language Models in Bangla Text Generation](https://arxiv.org/abs/2601.04534)
*Amit Bin Tariqul,A N M Zahid Hossain Milkan,Sahab-Al-Chowdhury,Syed Rifat Raiyan,Hasan Mahmud,Md Kamrul Hasan*

Main category: cs.CL

TL;DR: First systematic evaluation of LLM text watermarking for Bangla shows existing methods fail under translation attacks; proposed layered watermarking improves robustness 3-4x.


<details>
  <summary>Details</summary>
Motivation: Existing watermarking methods work well for high-resource languages but their robustness in low-resource languages like Bangla is unknown, especially under cross-lingual attacks.

Method: Systematically evaluated KGW, EXP, and Waterfall watermarking methods for Bangla LLM text generation under round-trip translation attacks; proposed layered watermarking combining embedding-time and post-generation watermarks.

Result: Under benign conditions, KGW and EXP achieve >88% detection accuracy with minimal quality degradation. However, RTT attacks collapse accuracy to 9-13%. Layered watermarking improves post-RTT accuracy by 25-35%, achieving 40-50% accuracy (3-4x improvement over single-layer methods).

Conclusion: Layered watermarking provides a practical, training-free solution for low-resource languages like Bangla, quantifying the robustness-quality trade-off in multilingual watermarking.

Abstract: As large language models (LLMs) are increasingly deployed for text generation, watermarking has become essential for authorship attribution, intellectual property protection, and misuse detection. While existing watermarking methods perform well in high-resource languages, their robustness in low-resource languages remains underexplored. This work presents the first systematic evaluation of state-of-the-art text watermarking methods: KGW, Exponential Sampling (EXP), and Waterfall, for Bangla LLM text generation under cross-lingual round-trip translation (RTT) attacks. Under benign conditions, KGW and EXP achieve high detection accuracy (>88%) with negligible perplexity and ROUGE degradation. However, RTT causes detection accuracy to collapse below RTT causes detection accuracy to collapse to 9-13%, indicating a fundamental failure of token-level watermarking. To address this, we propose a layered watermarking strategy that combines embedding-time and post-generation watermarks. Experimental results show that layered watermarking improves post-RTT detection accuracy by 25-35%, achieving 40-50% accuracy, representing a 3$\times$ to 4$\times$ relative improvement over single-layer methods, at the cost of controlled semantic degradation. Our findings quantify the robustness-quality trade-off in multilingual watermarking and establish layered watermarking as a practical, training-free solution for low-resource languages such as Bangla. Our code and data will be made public.

</details>


### [36] [Identifying Good and Bad Neurons for Task-Level Controllable LLMs](https://arxiv.org/abs/2601.04548)
*Wenjie Li,Guansong Pang,Hezhe Qiao,Debin Gao,David Lo*

Main category: cs.CL

TL;DR: NeuronLLM is a novel framework for understanding LLM neurons at the task level using functional antagonism principles, identifying both supportive and inhibitive neurons while mitigating fortuitous behaviors.


<details>
  <summary>Details</summary>
Motivation: Current methods for understanding LLM neurons are limited: they focus on single abilities rather than coordinated task performance, only identify supportive neurons while ignoring inhibitive ones, and are vulnerable to fortuitous behaviors where LLMs answer correctly by chance rather than genuine understanding.

Method: NeuronLLM adopts biological functional antagonism principles to model neurons with opposing roles: good neurons that facilitate task completion and bad neurons that inhibit it. It uses contrastive learning to holistically model both types of neurons and leverages augmented question sets to mitigate fortuitous behaviors in LLMs.

Result: Comprehensive experiments on LLMs of different sizes and families demonstrate NeuronLLM's superiority over existing methods across four NLP tasks, providing new insights into LLM functional organization.

Conclusion: NeuronLLM offers a more comprehensive framework for understanding LLM neurons at the task level by considering both facilitative and inhibitive roles, addressing limitations of previous approaches and providing better insights into how LLMs coordinate multiple abilities for task completion.

Abstract: Large Language Models have demonstrated remarkable capabilities on multiple-choice question answering benchmarks, but the complex mechanisms underlying their large-scale neurons remain opaque, posing significant challenges for understanding and steering LLMs. While recent studies made progress on identifying responsible neurons for certain abilities, these ability-specific methods are infeasible for task-focused scenarios requiring coordinated use of multiple abilities. Moreover, these approaches focus only on supportive neurons that correlate positively with task completion, while neglecting neurons with other roles-such as inhibitive roles-and misled neuron attribution due to fortuitous behaviors in LLMs (i.e., correctly answer the questions by chance rather than genuine understanding). To address these challenges, we propose NeuronLLM, a novel task-level LLM understanding framework that adopts the biological principle of functional antagonism for LLM neuron identification. The key insight is that task performance is jointly determined by neurons with two opposing roles: good neurons that facilitate task completion and bad neurons that inhibit it. NeuronLLM achieves a holistic modeling of neurons via contrastive learning of good and bad neurons, while leveraging augmented question sets to mitigate the fortuitous behaviors in LLMs. Comprehensive experiments on LLMs of different sizes and families show the superiority of NeuronLLM over existing methods in four NLP tasks, providing new insights into LLM functional organization.

</details>


### [37] [FeedEval: Pedagogically Aligned Evaluation of LLM-Generated Essay Feedback](https://arxiv.org/abs/2601.04574)
*Seongyeub Chu,Jongwoo Kim,Munyong Yi*

Main category: cs.CL

TL;DR: FeedEval is an LLM-based framework that evaluates LLM-generated essay feedback on three dimensions (specificity, helpfulness, validity) to filter high-quality feedback for training better essay scoring models.


<details>
  <summary>Details</summary>
Motivation: Prior work uses LLM-generated feedback without quality validation, propagating noise in downstream applications. There's a need to evaluate and filter feedback quality before using it to train essay assessment models.

Method: FeedEval uses dimension-specialized LLM evaluators trained on curated datasets to assess feedback along specificity, helpfulness, and validity dimensions. It selects high-quality feedback for downstream use.

Result: FeedEval aligns closely with human expert judgments. Essay scoring models trained with FeedEval-filtered feedback achieve superior scoring performance. High-quality feedback identified by FeedEval leads to more effective essay revisions with small LLMs.

Conclusion: FeedEval provides an effective framework for evaluating and filtering LLM-generated essay feedback, improving both essay scoring performance and revision quality while reducing noise propagation in automated essay assessment systems.

Abstract: Going beyond the prediction of numerical scores, recent research in automated essay scoring has increasingly emphasized the generation of high-quality feedback that provides justification and actionable guidance. To mitigate the high cost of expert annotation, prior work has commonly relied on LLM-generated feedback to train essay assessment models. However, such feedback is often incorporated without explicit quality validation, resulting in the propagation of noise in downstream applications. To address this limitation, we propose FeedEval, an LLM-based framework for evaluating LLM-generated essay feedback along three pedagogically grounded dimensions: specificity, helpfulness, and validity. FeedEval employs dimension-specialized LLM evaluators trained on datasets curated in this study to assess multiple feedback candidates and select high-quality feedback for downstream use. Experiments on the ASAP++ benchmark show that FeedEval closely aligns with human expert judgments and that essay scoring models trained with FeedEval-filtered high-quality feedback achieve superior scoring performance. Furthermore, revision experiments using small LLMs show that the high-quality feedback identified by FeedEval leads to more effective essay revisions. We will release our code and curated datasets upon accepted.

</details>


### [38] [Aligning Text, Code, and Vision: A Multi-Objective Reinforcement Learning Framework for Text-to-Visualization](https://arxiv.org/abs/2601.04582)
*Mizanur Rahman,Mohammed Saidul Islam,Md Tahmid Rahman Laskar,Shafiq Joty,Enamul Hoque*

Main category: cs.CL

TL;DR: RL-Text2Vis is a reinforcement learning framework that improves text-to-visualization generation by optimizing textual accuracy, code validity, and visualization quality using post-execution feedback, achieving significant improvements over existing methods.


<details>
  <summary>Details</summary>
Motivation: Current Text2Vis systems have limitations: closed-source LLMs produce charts lacking semantic alignment and clarity, open-source models often generate non-executable or poor-quality outputs, and supervised fine-tuning fails to capture post-execution feedback needed for visualization quality improvement.

Method: Proposes RL-Text2Vis, a reinforcement learning framework built on Group Relative Policy Optimization (GRPO) with a novel multi-objective reward function that jointly optimizes textual accuracy, code validity, and visualization quality using post-execution feedback. Trained Qwen2.5 models (7B and 14B).

Result: Achieves 22% relative improvement in chart quality over GPT-4o on Text2Vis benchmark, boosts code execution success from 78% to 97% relative to zero-shot baseline, outperforms strong baselines, and demonstrates robust generalization to out-of-domain datasets like VIS-Eval and NVBench.

Conclusion: RL-Text2Vis establishes GRPO as an effective strategy for structured, multimodal reasoning in visualization generation, significantly improving both code executability and visualization quality through reinforcement learning with post-execution feedback.

Abstract: Text-to-Visualization (Text2Vis) systems translate natural language queries over tabular data into concise answers and executable visualizations. While closed-source LLMs generate functional code, the resulting charts often lack semantic alignment and clarity, qualities that can only be assessed post-execution. Open-source models struggle even more, frequently producing non-executable or visually poor outputs. Although supervised fine-tuning can improve code executability, it fails to enhance overall visualization quality, as traditional SFT loss cannot capture post-execution feedback. To address this gap, we propose RL-Text2Vis, the first reinforcement learning framework for Text2Vis generation. Built on Group Relative Policy Optimization (GRPO), our method uses a novel multi-objective reward that jointly optimizes textual accuracy, code validity, and visualization quality using post-execution feedback. By training Qwen2.5 models (7B and 14B), RL-Text2Vis achieves a 22% relative improvement in chart quality over GPT-4o on the Text2Vis benchmark and boosts code execution success from 78% to 97% relative to its zero-shot baseline. Our models significantly outperform strong zero-shot and supervised baselines and also demonstrate robust generalization to out-of-domain datasets like VIS-Eval and NVBench. These results establish GRPO as an effective strategy for structured, multimodal reasoning in visualization generation. We release our code at https://github.com/vis-nlp/RL-Text2Vis.

</details>


### [39] [THaLLE-ThaiLLM: Domain-Specialized Small LLMs for Finance and Thai -- Technical Report](https://arxiv.org/abs/2601.04597)
*KBTG Labs,:,Anuruth Lertpiya,Danupat Khamnuansin,Kantapong Sucharitpongpan,Pornchanan Balee,Tawunrat Chalothorn,Thadpong Pongthawornkamol,Monchai Lertsutthiwong*

Main category: cs.CL

TL;DR: Model merging enables efficient creation of multi-capability LLMs by combining specialized models, showing improved performance in Thai language and financial domains without expensive retraining.


<details>
  <summary>Details</summary>
Motivation: Organizations face a trade-off between deploying multiple specialized LLMs vs. the prohibitive cost of training a single multi-capability model, especially in privacy-sensitive domains like banking/finance where on-premise deployment is preferred.

Method: Explores model merging as a resource-efficient alternative, conducting two experiments: 1) merging Qwen-8B with ThaiLLM-8B, and 2) merging Qwen-8B with both ThaiLLM-8B and THaLLE-CFA-8B to create multi-capability models.

Result: First merger shows ThaiLLM-8B enhances Thai general capabilities with uplift in M3 and M6 O-NET exams. Second merger further improves performance across both general and financial domains, showing uplift in M3/M6 O-NET, Flare-CFA, and Thai-IC benchmarks.

Conclusion: Model merging is a viable approach for efficiently creating high-performance, multi-capability LLMs that address both language-specific and domain-specific needs while maintaining resource efficiency.

Abstract: Large Language Models (LLMs) have demonstrated significant potential across various domains, particularly in banking and finance, where they can automate complex tasks and enhance decision-making at scale. Due to privacy, security, and regulatory concerns, organizations often prefer on-premise deployment of LLMs. The ThaiLLM initiative aims to enhance Thai language capabilities in open-LLMs, enabling Thai industry to leverage advanced language models. However, organizations often face a trade-off between deploying multiple specialized models versus the prohibitive expense of training a single multi-capability model. To address this, we explore model merging as a resource-efficient alternative for developing high-performance, multi-capability LLMs. We present results from two key experiments: first, merging Qwen-8B with ThaiLLM-8B demonstrates how ThaiLLM-8B enhances Thai general capabilities, showing an uplift of M3 and M6 O-NET exams over the general instruction-following Qwen-8B. Second, we merge Qwen-8B with both ThaiLLM-8B and THaLLE-CFA-8B. This combination results in further improvements in performance across both general and financial domains, by demonstrating an uplift in both M3 and M6 O-NET, Flare-CFA, and Thai-IC benchmarks. The report showcases the viability of model merging for efficiently creating multi-capability LLMs.

</details>


### [40] [On the Limitations of Rank-One Model Editing in Answering Multi-hop Questions](https://arxiv.org/abs/2601.04600)
*Zhiyuan He,Binghan Chen,Tianxiang Xiong,Ziyang Sun,Mozhao Zhu,Xi Chen*

Main category: cs.CL

TL;DR: ROME knowledge editing struggles with multi-hop reasoning due to three failure modes; Redundant Editing improves 2-hop accuracy by 15.5+ percentage points.


<details>
  <summary>Details</summary>
Motivation: Current knowledge editing methods like ROME work well for single-hop facts but fail on multi-hop reasoning tasks requiring knowledge chaining, creating a need for better multi-hop editing approaches.

Method: Analyzed ROME editing across different layer depths, identified three failure modes, and proposed Redundant Editing strategy to address "hopping-too-late" and generalization decay issues.

Result: Redundant Editing improves accuracy on 2-hop questions by at least 15.5 percentage points (96% increase over single-edit), though with some trade-offs in specificity and language naturalness.

Conclusion: Multi-hop knowledge editing requires specialized approaches like Redundant Editing to overcome layer-depth limitations and improve reasoning performance in transformer models.

Abstract: Recent advances in Knowledge Editing (KE), particularly Rank-One Model Editing (ROME), show superior efficiency over fine-tuning and in-context learning for updating single-hop facts in transformers. However, these methods face significant challenges when applied to multi-hop reasoning tasks requiring knowledge chaining. In this work, we study the effect of editing knowledge with ROME on different layer depths and identify three key failure modes. First, the "hopping-too-late" problem occurs as later layers lack access to necessary intermediate representations. Second, generalization ability deteriorates sharply when editing later layers. Third, the model overfits to edited knowledge, incorrectly prioritizing edited-hop answers regardless of context. To mitigate the issues of "hopping-too-late" and generalisation decay, we propose Redundant Editing, a simple yet effective strategy that enhances multi-hop reasoning. Our experiments demonstrate that this approach can improve accuracy on 2-hop questions by at least 15.5 percentage points, representing a 96% increase over the previous single-edit strategy, while trading off some specificity and language naturalness.

</details>


### [41] [When More Words Say Less: Decoupling Length and Specificity in Image Description Evaluation](https://arxiv.org/abs/2601.04609)
*Rhea Kapur,Robert Hawkins,Elisa Kreiss*

Main category: cs.CL

TL;DR: VLMs need to disentangle description specificity from length - concise descriptions can be highly informative while long ones can be vacuous. Specificity should be measured by how well a description distinguishes a target image from alternatives.


<details>
  <summary>Details</summary>
Motivation: Current vision-language models conflate description specificity with length, assuming longer descriptions are more specific. The authors argue these concepts must be separated since descriptions can be concise yet information-dense or lengthy yet vacuous.

Method: Defined specificity relative to a contrast set (how well description picks out target image vs. alternatives). Constructed a dataset controlling for length while varying information content. Validated human preferences for specificity regardless of length.

Result: People reliably prefer more specific descriptions regardless of length. Controlling for length alone doesn't account for specificity differences - how the length budget is allocated matters significantly.

Conclusion: Evaluation approaches should directly prioritize specificity over verbosity. The study supports disentangling specificity from length in vision-language model descriptions and developing metrics that measure true information content rather than just length.

Abstract: Vision-language models (VLMs) are increasingly used to make visual content accessible via text-based descriptions. In current systems, however, description specificity is often conflated with their length. We argue that these two concepts must be disentangled: descriptions can be concise yet dense with information, or lengthy yet vacuous. We define specificity relative to a contrast set, where a description is more specific to the extent that it picks out the target image better than other possible images. We construct a dataset that controls for length while varying information content, and validate that people reliably prefer more specific descriptions regardless of length. We find that controlling for length alone cannot account for differences in specificity: how the length budget is allocated makes a difference. These results support evaluation approaches that directly prioritize specificity over verbosity.

</details>


### [42] [Character-R1: Enhancing Role-Aware Reasoning in Role-Playing Agents via RLVR](https://arxiv.org/abs/2601.04611)
*Yihong Tang,Kehai Chen,Xuefeng Bai,Benyou Wang,Zeming Liu,Haifeng Wang,Min Zhang*

Main category: cs.CL

TL;DR: Character-R1 is a framework that provides verifiable reward signals for role-playing agents to improve cognitive consistency and reduce out-of-character errors through three core reward designs.


<details>
  <summary>Details</summary>
Motivation: Current role-playing agents imitate surface-level behaviors but lack internal cognitive consistency, leading to out-of-character errors in complex situations. Existing approaches lack comprehensive verifiable reward signals for effective role-aware reasoning.

Method: Character-R1 framework with three core designs: (1) Cognitive Focus Reward - explicit label-based analysis of 10 character elements (e.g., worldview) to structure internal cognition; (2) Reference-Guided Reward - overlap-based metrics with reference responses as optimization anchors; (3) Character-Conditioned Reward Normalization - adjusts reward distributions based on character categories for robust optimization across heterogeneous roles.

Result: Extensive experiments demonstrate that Character-R1 significantly outperforms existing methods in knowledge, memory and other metrics.

Conclusion: Character-R1 provides a comprehensive framework with verifiable reward signals that effectively addresses the cognitive consistency problem in role-playing agents, leading to improved performance across multiple dimensions.

Abstract: Current role-playing agents (RPAs) are typically constructed by imitating surface-level behaviors, but this approach lacks internal cognitive consistency, often causing out-of-character errors in complex situations. To address this, we propose Character-R1, a framework designed to provide comprehensive verifiable reward signals for effective role-aware reasoning, which are missing in recent studies. Specifically, our framework comprises three core designs: (1) Cognitive Focus Reward, which enforces explicit label-based analysis of 10 character elements (e.g., worldview) to structure internal cognition; (2) Reference-Guided Reward, which utilizes overlap-based metrics with reference responses as optimization anchors to enhance exploration and performance; and (3) Character-Conditioned Reward Normalization, which adjusts reward distributions based on character categories to ensure robust optimization across heterogeneous roles. Extensive experiments demonstrate that Character-R1 significantly outperforms existing methods in knowledge, memory and others.

</details>


### [43] [From National Curricula to Cultural Awareness: Constructing Open-Ended Culture-Specific Question Answering Dataset](https://arxiv.org/abs/2601.04632)
*Haneul Yoo,Won Ik Cho,Geunhye Kim,Jiyoon Han*

Main category: cs.CL

TL;DR: CuCu framework transforms national social studies curricula into culture-specific QA pairs to address LLMs' cultural bias, demonstrated with Korean curriculum creating KCaQA dataset.


<details>
  <summary>Details</summary>
Motivation: LLMs show uneven performance across languages and cultures due to English-centric training data, requiring practical cultural alignment solutions.

Method: CuCu: automated multi-agent LLM framework that converts national textbook curricula into open-ended, culture-specific question-answer pairs.

Result: Created KCaQA dataset with 34.1k open-ended QA pairs from Korean social studies curriculum, covering culture-specific topics with locally-grounded responses.

Conclusion: National curricula provide scalable foundation for culture-aware supervision, enabling LLMs to produce socioculturally appropriate responses across different cultures.

Abstract: Large language models (LLMs) achieve strong performance on many tasks, but their progress remains uneven across languages and cultures, often reflecting values latent in English-centric training data. To enable practical cultural alignment, we propose a scalable approach that leverages national social studies curricula as a foundation for culture-aware supervision. We introduce CuCu, an automated multi-agent LLM framework that transforms national textbook curricula into open-ended, culture-specific question-answer pairs. Applying CuCu to the Korean national social studies curriculum, we construct KCaQA, comprising 34.1k open-ended QA pairs. Our quantitative and qualitative analyses suggest that KCaQA covers culture-specific topics and produces responses grounded in local sociocultural contexts.

</details>


### [44] [MAGA-Bench: Machine-Augment-Generated Text via Alignment Detection Benchmark](https://arxiv.org/abs/2601.04633)
*Anyang Song,Ying Cheng,Yiqian Xu,Rui Feng*

Main category: cs.CL

TL;DR: MAGA (Machine-Augment-Generated Text via Alignment) is a method to enhance LLM-generated text alignment to both improve detector generalization and test detector robustness.


<details>
  <summary>Details</summary>
Motivation: As LLM alignment improves, machine-generated text becomes harder to distinguish from human-written text, exacerbating abuse issues like fake news and fraud. Existing detectors have limited generalization that depends on dataset quality, and simply expanding MGT sources is insufficient.

Method: Proposes MAGA pipeline with comprehensive alignment from prompt construction to reasoning process. Key component is RLDF (Reinforced Learning from Detectors Feedback) - a systematic approach to enhance text alignment. The method augments generation process to create better training data for detectors.

Result: RoBERTa detector fine-tuned on MAGA training set achieved average 4.60% improvement in generalization detection AUC. MAGA Dataset caused average 8.13% decrease in AUC of selected detectors, demonstrating its effectiveness for testing detector robustness.

Conclusion: MAGA provides an effective approach to enhance detector generalization and test robustness through improved alignment of generated text. The method and dataset offer significant value for future research on detection capabilities.

Abstract: Large Language Models (LLMs) alignment is constantly evolving. Machine-Generated Text (MGT) is becoming increasingly difficult to distinguish from Human-Written Text (HWT). This has exacerbated abuse issues such as fake news and online fraud. Fine-tuned detectors' generalization ability is highly dependent on dataset quality, and simply expanding the sources of MGT is insufficient. Further augment of generation process is required. According to HC-Var's theory, enhancing the alignment of generated text can not only facilitate attacks on existing detectors to test their robustness, but also help improve the generalization ability of detectors fine-tuned on it. Therefore, we propose \textbf{M}achine-\textbf{A}ugment-\textbf{G}enerated Text via \textbf{A}lignment (MAGA). MAGA's pipeline achieves comprehensive alignment from prompt construction to reasoning process, among which \textbf{R}einforced \textbf{L}earning from \textbf{D}etectors \textbf{F}eedback (RLDF), systematically proposed by us, serves as a key component. In our experiments, the RoBERTa detector fine-tuned on MAGA training set achieved an average improvement of 4.60\% in generalization detection AUC. MAGA Dataset caused an average decrease of 8.13\% in the AUC of the selected detectors, expecting to provide indicative significance for future research on the generalization detection ability of detectors.

</details>


### [45] [SpeechMedAssist: Efficiently and Effectively Adapting Speech Language Models for Medical Consultation](https://arxiv.org/abs/2601.04638)
*Sirry Chen,Jieyi Wang,Wei Chen,Zhongyu Wei*

Main category: cs.CL

TL;DR: SpeechMedAssist enables speech-based medical consultations using a two-stage training approach that reduces speech data requirements to only 10k synthesized samples.


<details>
  <summary>Details</summary>
Motivation: Medical consultations are speech-centric, but existing approaches rely on cumbersome text-based interactions. SpeechLMs offer natural speech interaction but face challenges due to medical speech data scarcity and inefficient fine-tuning.

Method: Two-stage training paradigm: (1) Knowledge & Capability Injection via Text, and (2) Modality Re-alignment with Limited Speech Data (only 10k synthesized samples). Exploits architectural properties of SpeechLMs to decouple training.

Result: Outperforms all baselines in both effectiveness and robustness across single-turn QA and multi-turn simulated interactions in medical consultation scenarios.

Conclusion: SpeechMedAssist successfully enables speech-based medical consultations with minimal speech data requirements, demonstrating superior performance in medical consultation scenarios.

Abstract: Medical consultations are intrinsically speech-centric. However, most prior works focus on long-text-based interactions, which are cumbersome and patient-unfriendly. Recent advances in speech language models (SpeechLMs) have enabled more natural speech-based interaction, yet the scarcity of medical speech data and the inefficiency of directly fine-tuning on speech data jointly hinder the adoption of SpeechLMs in medical consultation. In this paper, we propose SpeechMedAssist, a SpeechLM natively capable of conducting speech-based multi-turn interactions with patients. By exploiting the architectural properties of SpeechLMs, we decouple the conventional one-stage training into a two-stage paradigm consisting of (1) Knowledge & Capability Injection via Text and (2) Modality Re-alignment with Limited Speech Data, thereby reducing the requirement for medical speech data to only 10k synthesized samples. To evaluate SpeechLMs for medical consultation scenarios, we design a benchmark comprising both single-turn question answering and multi-turn simulated interactions. Experimental results show that our model outperforms all baselines in both effectiveness and robustness in most evaluation settings.

</details>


### [46] [CRANE: Causal Relevance Analysis of Language-Specific Neurons in Multilingual Large Language Models](https://arxiv.org/abs/2601.04664)
*Yifan Le,Yunliang Li*

Main category: cs.CL

TL;DR: CRANE is a relevance-based framework that identifies language-specific neurons in multilingual LLMs through targeted interventions, showing more precise language specialization than activation-based methods.


<details>
  <summary>Details</summary>
Motivation: Current understanding of how language capabilities are organized at the neuron level in multilingual LLMs is limited. Existing activation-based heuristics conflate language preference with functional importance, creating a need for more precise methods to identify truly language-specific neurons.

Method: CRANE uses relevance-based analysis with targeted neuron-level interventions to identify language-specific neurons based on their functional necessity rather than activation magnitude. It characterizes neuron specialization by their contribution to language-conditioned predictions through masking experiments.

Result: Neuron-level interventions reveal asymmetric patterns: masking language-relevant neurons selectively degrades performance on that language while largely preserving performance on other languages. CRANE isolates language-specific components more precisely than activation-based methods across English, Chinese, and Vietnamese benchmarks.

Conclusion: CRANE provides a more accurate framework for understanding language organization in multilingual LLMs by focusing on functional necessity rather than activation patterns, revealing language-selective but non-exclusive neuron specializations.

Abstract: Multilingual large language models (LLMs) achieve strong performance across languages, yet how language capabilities are organized at the neuron level remains poorly understood. Prior work has identified language-related neurons mainly through activation-based heuristics, which conflate language preference with functional importance. Prior work has identified language-related neurons mainly through activation-based heuristics, which conflate language preference with functional importance. We propose CRANE, a relevance-based analysis framework that redefines language specificity in terms of functional necessity, identifying language-specific neurons through targeted neuron-level interventions. CRANE characterizes neuron specialization by their contribution to language-conditioned predictions rather than activation magnitude. Our implementation will be made publicly available. Neuron-level interventions reveal a consistent asymmetric pattern: masking neurons relevant to a target language selectively degrades performance on that language while preserving performance on other languages to a substantial extent, indicating language-selective but non-exclusive neuron specializations. Experiments on English, Chinese, and Vietnamese across multiple benchmarks, together with a dedicated relevance-based metric and base-to-chat model transfer analysis, show that CRANE isolates language-specific components more precisely than activation-based methods.

</details>


### [47] [ToolGate: Contract-Grounded and Verified Tool Execution for LLMs](https://arxiv.org/abs/2601.04688)
*Yanming Liu,Xinyue Peng,Jiannan Cao,Xinyi Wang,Songhang Deng,Jintao Chen,Jianwei Yin,Xuhong Zhang*

Main category: cs.CL

TL;DR: ToolGate is a forward execution framework that provides logical safety guarantees and verifiable state evolution for LLM tool calling through formal contracts and runtime verification.


<details>
  <summary>Details</summary>
Motivation: Existing LLM tool-augmentation frameworks rely on natural language reasoning without formal guarantees for logical safety and verifiability, risking invalid or hallucinated results corrupting world representations.

Method: ToolGate maintains an explicit symbolic state space as typed key-value mapping, formalizes tools as Hoare-style contracts (preconditions and postconditions), gates tool invocation via precondition checks, and commits results through postcondition verification.

Result: Experimental validation shows ToolGate significantly improves reliability and verifiability of tool-augmented LLM systems while maintaining competitive performance on complex multi-step reasoning tasks.

Conclusion: ToolGate establishes a foundation for building more trustworthy and debuggable AI systems that integrate language models with external tools through formal safety guarantees.

Abstract: Large Language Models (LLMs) augmented with external tools have demonstrated remarkable capabilities in complex reasoning tasks. However, existing frameworks rely heavily on natural language reasoning to determine when tools can be invoked and whether their results should be committed, lacking formal guarantees for logical safety and verifiability. We present \textbf{ToolGate}, a forward execution framework that provides logical safety guarantees and verifiable state evolution for LLM tool calling. ToolGate maintains an explicit symbolic state space as a typed key-value mapping representing trusted world information throughout the reasoning process. Each tool is formalized as a Hoare-style contract consisting of a precondition and a postcondition, where the precondition gates tool invocation by checking whether the current state satisfies the required conditions, and the postcondition determines whether the tool's result can be committed to update the state through runtime verification. Our approach guarantees that the symbolic state evolves only through verified tool executions, preventing invalid or hallucinated results from corrupting the world representation. Experimental validation demonstrates that ToolGate significantly improves the reliability and verifiability of tool-augmented LLM systems while maintaining competitive performance on complex multi-step reasoning tasks. This work establishes a foundation for building more trustworthy and debuggable AI systems that integrate language models with external tools.

</details>


### [48] [See, Explain, and Intervene: A Few-Shot Multimodal Agent Framework for Hateful Meme Moderation](https://arxiv.org/abs/2601.04692)
*Naquee Rizwan,Subhankar Swain,Paramananda Bhaskar,Gagan Aryan,Shehryaar Shah Khan,Animesh Mukherjee*

Main category: cs.CL

TL;DR: A framework using generative AI for hateful meme moderation that integrates detection, explanation, and intervention under limited data conditions.


<details>
  <summary>Details</summary>
Motivation: Current approaches study detection, explanation, and intervention separately, which doesn't reflect real-world conditions. Annotating large datasets for meme moderation is prohibitively expensive, creating a need for solutions that work with limited data.

Method: Proposes a novel framework leveraging task-specific generative multimodal agents and few-shot adaptability of large multimodal models to handle different meme types with limited data.

Result: First work focused on generalizable hateful meme moderation under limited data conditions, with strong potential for real-world deployment.

Conclusion: The integrated approach combining detection, explanation, and intervention using generative AI represents a significant advancement for practical hateful meme moderation in production scenarios.

Abstract: In this work, we examine hateful memes from three complementary angles - how to detect them, how to explain their content and how to intervene them prior to being posted - by applying a range of strategies built on top of generative AI models. To the best of our knowledge, explanation and intervention have typically been studied separately from detection, which does not reflect real-world conditions. Further, since curating large annotated datasets for meme moderation is prohibitively expensive, we propose a novel framework that leverages task-specific generative multimodal agents and the few-shot adaptability of large multimodal models to cater to different types of memes. We believe this is the first work focused on generalizable hateful meme moderation under limited data conditions, and has strong potential for deployment in real-world production scenarios. Warning: Contains potentially toxic contents.

</details>


### [49] [Thunder-KoNUBench: A Corpus-Aligned Benchmark for Korean Negation Understanding](https://arxiv.org/abs/2601.04693)
*Sungmok Jung,Yeonkyoung So,Joonhak Lee,Sangho Kim,Yelim Ahn,Jaejin Lee*

Main category: cs.CL

TL;DR: Thunder-KoNUBench: A Korean negation benchmark showing LLMs struggle with negation, with fine-tuning improving performance.


<details>
  <summary>Details</summary>
Motivation: Negation challenges LLMs, but Korean negation benchmarks are scarce. Need empirical evaluation of LLMs' negation understanding in Korean.

Method: 1) Corpus analysis of Korean negation showing LLM performance degradation; 2) Creation of Thunder-KoNUBench benchmark reflecting empirical distribution of Korean negation phenomena; 3) Evaluation of 47 LLMs; 4) Fine-tuning experiments on the benchmark.

Result: LLM performance degrades under negation. Fine-tuning on Thunder-KoNUBench improves both negation understanding and broader contextual comprehension in Korean.

Conclusion: Thunder-KoNUBench addresses Korean negation benchmark scarcity, reveals LLM limitations with negation, and shows fine-tuning can improve negation understanding and contextual comprehension in Korean.

Abstract: Although negation is known to challenge large language models (LLMs), benchmarks for evaluating negation understanding, especially in Korean, are scarce. We conduct a corpus-based analysis of Korean negation and show that LLM performance degrades under negation. We then introduce Thunder-KoNUBench, a sentence-level benchmark that reflects the empirical distribution of Korean negation phenomena. Evaluating 47 LLMs, we analyze the effects of model size and instruction tuning, and show that fine-tuning on Thunder-KoNUBench improves negation understanding and broader contextual comprehension in Korean.

</details>


### [50] [PRISM: A Unified Framework for Post-Training LLMs Without Verifiable Rewards](https://arxiv.org/abs/2601.04700)
*Mukesh Ghimire,Aosong Feng,Liwen You,Youzhi Luo,Fang Liu,Xuan Zhu*

Main category: cs.CL

TL;DR: PRISM: A training framework that combines Process Reward Models with model self-certainty for stable unsupervised learning from unlabeled data, addressing reliability issues in existing consistency-based methods.


<details>
  <summary>Details</summary>
Motivation: Current post-training methods for LLMs require costly human supervision or external verifiers. As LLMs improve, high-quality solutions to difficult problems become scarce for human annotation, making unsupervised learning increasingly necessary. Existing consistency-based methods (majority voting or internal confidence) are unreliable for large-scale, long-term training.

Method: PRISM (Process Reward Model) framework combines a Process Reward Model (PRM) with the model's internal self-certainty to guide learning without ground-truth labels. The PRM provides external verification while self-certainty provides internal confidence metrics, creating a unified training approach.

Result: Effectively combining PRM with self-certainty leads to both stable training and better test-time performance, while also keeping the model's internal confidence in check. The framework addresses the unreliability of pure consistency-based methods.

Conclusion: PRISM offers a practical solution for unsupervised learning from unlabeled data by synergistically combining external process verification (PRM) with internal confidence metrics, enabling stable large-scale training without human supervision.

Abstract: Current techniques for post-training Large Language Models (LLMs) rely either on costly human supervision or on external verifiers to boost performance on tasks such as mathematical reasoning and code generation. However, as LLMs improve their problem-solving, any further improvement will potentially require high-quality solutions to difficult problems that are not available to humans. As a result, learning from unlabeled data is becoming increasingly attractive in the research community. Existing methods extract learning signal from a model's consistency, either by majority voting or by converting the model's internal confidence into reward. Although internal consistency metric such as entropy or self-certainty require no human intervention, as we show in this work, these are unreliable signals for large-scale and long-term training. To address the unreliability, we propose PRISM, a unified training framework that uses a Process Reward Model (PRM) to guide learning alongside model's internal confidence in the absence of ground-truth labels. We show that effectively combining PRM with self-certainty can lead to both stable training and better test-time performance, and also keep the model's internal confidence in check.

</details>


### [51] [Prior-Informed Zeroth-Order Optimization with Adaptive Direction Alignment for Memory-Efficient LLM Fine-Tuning](https://arxiv.org/abs/2601.04710)
*Feihu Jin,Shipeng Cen,Ying Tan*

Main category: cs.CL

TL;DR: A plug-and-play zeroth-order optimization method that uses prior-informed perturbations to improve gradient estimation, reducing memory overhead and accelerating convergence for fine-tuning large language models.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning LLMs faces substantial memory overhead during backpropagation. Zeroth-order optimization avoids backprop but suffers from high variance in gradient estimation due to random perturbations, leading to slow convergence.

Method: Proposes prior-informed perturbations that dynamically compute a guiding vector from Gaussian samples to direct perturbations toward more informative directions. Also investigates a greedy perturbation strategy to explore prior knowledge impact on gradient estimation.

Result: Method outperforms traditional ZO optimization across all 11 benchmark tasks on OPT-13B model and surpasses gradient-based baselines on 9 out of 11 tasks. Theoretically proven to achieve stronger alignment with true gradient direction.

Conclusion: The proposed method establishes a robust balance between efficiency and accuracy, seamlessly integrates into existing optimization methods, and delivers faster convergence with superior performance for LLM fine-tuning.

Abstract: Fine-tuning large language models (LLMs) has achieved remarkable success across various NLP tasks, but the substantial memory overhead during backpropagation remains a critical bottleneck, especially as model scales grow. Zeroth-order (ZO) optimization alleviates this issue by estimating gradients through forward passes and Gaussian sampling, avoiding the need for backpropagation. However, conventional ZO methods suffer from high variance in gradient estimation due to their reliance on random perturbations, leading to slow convergence and suboptimal performance. We propose a simple plug-and-play method that incorporates prior-informed perturbations to refine gradient estimation. Our method dynamically computes a guiding vector from Gaussian samples, which directs perturbations toward more informative directions, significantly accelerating convergence compared to standard ZO approaches. We further investigate a greedy perturbation strategy to explore the impact of prior knowledge on gradient estimation. Theoretically, we prove that our gradient estimator achieves stronger alignment with the true gradient direction, enhancing optimization efficiency. Extensive experiments across LLMs of varying scales and architectures demonstrate that our proposed method could seamlessly integrate into existing optimization methods, delivering faster convergence and superior performance. Notably, on the OPT-13B model, our method outperforms traditional ZO optimization across all 11 benchmark tasks and surpasses gradient-based baselines on 9 out of 11 tasks, establishing a robust balance between efficiency and accuracy.

</details>


### [52] [DSC2025 -- ViHallu Challenge: Detecting Hallucination in Vietnamese LLMs](https://arxiv.org/abs/2601.04711)
*Anh Thi-Hoang Nguyen,Khanh Quoc Tran,Tin Van Huynh,Phuoc Tan-Hoang Nguyen,Cam Tan Nguyen,Kiet Van Nguyen*

Main category: cs.CL

TL;DR: First large-scale shared task for detecting hallucinations in Vietnamese LLMs, introducing ViHallu dataset with 10K annotated samples and achieving 84.80% macro-F1 score.


<details>
  <summary>Details</summary>
Motivation: LLM reliability is constrained by hallucinations, but Vietnamese and other low-to-medium resource languages lack standardized evaluation frameworks for hallucination detection.

Method: Created ViHallu dataset with 10,000 annotated (context, prompt, response) triplets across three hallucination categories and three prompt types. Organized DSC2025 ViHallu Challenge with 111 participating teams.

Result: Best system achieved 84.80% macro-F1 score vs 32.83% baseline. Instruction-tuned LLMs with structured prompting and ensemble strategies performed best, but intrinsic hallucinations remain challenging.

Conclusion: Established rigorous benchmark for Vietnamese hallucination detection, showing advanced LLM methods outperform generic architectures but perfect detection remains elusive, especially for contradiction-based hallucinations.

Abstract: The reliability of large language models (LLMs) in production environments remains significantly constrained by their propensity to generate hallucinations -- fluent, plausible-sounding outputs that contradict or fabricate information. While hallucination detection has recently emerged as a priority in English-centric benchmarks, low-to-medium resource languages such as Vietnamese remain inadequately covered by standardized evaluation frameworks. This paper introduces the DSC2025 ViHallu Challenge, the first large-scale shared task for detecting hallucinations in Vietnamese LLMs. We present the ViHallu dataset, comprising 10,000 annotated triplets of (context, prompt, response) samples systematically partitioned into three hallucination categories: no hallucination, intrinsic, and extrinsic hallucinations. The dataset incorporates three prompt types -- factual, noisy, and adversarial -- to stress-test model robustness. A total of 111 teams participated, with the best-performing system achieving a macro-F1 score of 84.80\%, compared to a baseline encoder-only score of 32.83\%, demonstrating that instruction-tuned LLMs with structured prompting and ensemble strategies substantially outperform generic architectures. However, the gap to perfect performance indicates that hallucination detection remains a challenging problem, particularly for intrinsic (contradiction-based) hallucinations. This work establishes a rigorous benchmark and explores a diverse range of detection methodologies, providing a foundation for future research into the trustworthiness and reliability of Vietnamese language AI systems.

</details>


### [53] [Fame Fades, Nature Remains: Disentangling the Character Identity of Role-Playing Agents](https://arxiv.org/abs/2601.04716)
*Yonghyun Jun,Junhyuk Choi,Jihyeong Park,Hwanhee Lee*

Main category: cs.CL

TL;DR: Character Identity framework disentangles RPAs into Parametric (pre-trained knowledge) and Attributive (behavioral properties) layers, revealing Fame Fades and Nature Remains phenomena.


<details>
  <summary>Details</summary>
Motivation: Current Role-Playing Agents treat characters as arbitrary text inputs without formal structural dimensions, lacking systematic understanding of character identity components.

Method: Proposed Character Identity framework with two layers: Parametric Identity (pre-trained knowledge) and Attributive Identity (behavioral properties). Created unified character profile schema and generated Famous/Synthetic characters under identical constraints. Evaluated across single-turn and multi-turn interactions.

Result: Identified "Fame Fades": famous characters have initial advantage from parametric knowledge but lose edge as models prioritize conversational context. Found "Nature Remains": models robustly portray general personality traits but RPA performance is sensitive to morality and relationship valence. Negative social natures are primary bottleneck in RPA fidelity.

Conclusion: Character Identity framework provides systematic approach to understanding RPA components. Findings guide future character construction and evaluation, highlighting negative social natures as key challenge for improving RPA fidelity.

Abstract: Despite the rapid proliferation of Role-Playing Agents (RPAs) based on Large Language Models (LLMs), the structural dimensions defining a character's identity remain weakly formalized, often treating characters as arbitrary text inputs. In this paper, we propose the concept of \textbf{Character Identity}, a multidimensional construct that disentangles a character into two distinct layers: \textbf{(1) Parametric Identity}, referring to character-specific knowledge encoded from the LLM's pre-training, and \textbf{(2) Attributive Identity}, capturing fine-grained behavioral properties such as personality traits and moral values. To systematically investigate these layers, we construct a unified character profile schema and generate both Famous and Synthetic characters under identical structural constraints. Our evaluation across single-turn and multi-turn interactions reveals two critical phenomena. First, we identify \textit{"Fame Fades"}: while famous characters hold a significant advantage in initial turns due to parametric knowledge, this edge rapidly vanishes as models prioritize accumulating conversational context over pre-trained priors. Second, we find that \textit{"Nature Remains"}: while models robustly portray general personality traits regardless of polarity, RPA performance is highly sensitive to the valence of morality and interpersonal relationships. Our findings pinpoint negative social natures as the primary bottleneck in RPA fidelity, guiding future character construction and evaluation.

</details>


### [54] [Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking](https://arxiv.org/abs/2601.04720)
*Mingxin Li,Yanzhao Zhang,Dingkun Long,Keqin Chen,Sibo Song,Shuai Bai,Zhibo Yang,Pengjun Xie,An Yang,Dayiheng Liu,Jingren Zhou,Junyang Lin*

Main category: cs.CL

TL;DR: Qwen3-VL-Embedding and Qwen3-VL-Reranker are multimodal embedding and reranking models that create a unified representation space for text, images, documents, and video, achieving state-of-the-art performance on multimodal benchmarks.


<details>
  <summary>Details</summary>
Motivation: To develop an end-to-end pipeline for high-precision multimodal search by creating models that can map diverse modalities into a unified representation space, supporting multilingual capabilities and flexible deployment options.

Method: Multi-stage training paradigm with large-scale contrastive pre-training followed by reranking model distillation; uses Matryoshka Representation Learning for flexible embedding dimensions; cross-encoder architecture with cross-attention for reranking; supports 32k token inputs and multilingual capabilities.

Result: Achieves state-of-the-art results across multimodal embedding benchmarks; Qwen3-VL-Embedding-8B scores 77.8 on MMEB-V2 (ranked first as of Jan 8, 2025); effective on image-text retrieval, visual question answering, and video-text matching tasks.

Conclusion: The Qwen3-VL-Embedding and Qwen3-VL-Reranker series provide an effective end-to-end solution for multimodal retrieval, demonstrating superior performance on diverse benchmarks while offering flexible deployment options through 2B and 8B parameter variants.

Abstract: In this report, we introduce the Qwen3-VL-Embedding and Qwen3-VL-Reranker model series, the latest extensions of the Qwen family built on the Qwen3-VL foundation model. Together, they provide an end-to-end pipeline for high-precision multimodal search by mapping diverse modalities, including text, images, document images, and video, into a unified representation space. The Qwen3-VL-Embedding model employs a multi-stage training paradigm, progressing from large-scale contrastive pre-training to reranking model distillation, to generate semantically rich high-dimensional vectors. It supports Matryoshka Representation Learning, enabling flexible embedding dimensions, and handles inputs up to 32k tokens. Complementing this, Qwen3-VL-Reranker performs fine-grained relevance estimation for query-document pairs using a cross-encoder architecture with cross-attention mechanisms. Both model series inherit the multilingual capabilities of Qwen3-VL, supporting more than 30 languages, and are released in $\textbf{2B}$ and $\textbf{8B}$ parameter sizes to accommodate diverse deployment requirements. Empirical evaluations demonstrate that the Qwen3-VL-Embedding series achieves state-of-the-art results across diverse multimodal embedding evaluation benchmarks. Specifically, Qwen3-VL-Embedding-8B attains an overall score of $\textbf{77.8}$ on MMEB-V2, ranking first among all models (as of January 8, 2025). This report presents the architecture, training methodology, and practical capabilities of the series, demonstrating their effectiveness on various multimodal retrieval tasks, including image-text retrieval, visual question answering, and video-text matching.

</details>


### [55] [Automatic Classifiers Underdetect Emotions Expressed by Men](https://arxiv.org/abs/2601.04730)
*Ivan Smirnov,Segun T. Aroyehun,Paul Plener,David Garcia*

Main category: cs.CL

TL;DR: Study finds systematic gender bias in emotion detection models where error rates are consistently higher for texts authored by men compared to women across 414 model-class combinations.


<details>
  <summary>Details</summary>
Motivation: Current emotion classifiers are typically assessed using third-party annotators rather than self-annotated data, potentially concealing systematic biases. There's a need to ensure these tools perform reliably across different populations, especially given their widespread adoption.

Method: Used a unique, large-scale dataset of over one million self-annotated posts with a pre-registered research design to investigate gender biases across 414 combinations of models and emotion-related classes.

Result: Error rates were consistently higher for texts authored by men compared to those authored by women across different types of automatic classifiers and various underlying emotions. This bias could significantly affect downstream applications.

Conclusion: Sentiment analysis is not yet a solved problem, especially for ensuring equitable model behavior across demographic groups. Current machine learning tools, including large language models, should be applied with caution when gender composition of samples is unknown or variable.

Abstract: The widespread adoption of automatic sentiment and emotion classifiers makes it important to ensure that these tools perform reliably across different populations. Yet their reliability is typically assessed using benchmarks that rely on third-party annotators rather than the individuals experiencing the emotions themselves, potentially concealing systematic biases. In this paper, we use a unique, large-scale dataset of more than one million self-annotated posts and a pre-registered research design to investigate gender biases in emotion detection across 414 combinations of models and emotion-related classes. We find that across different types of automatic classifiers and various underlying emotions, error rates are consistently higher for texts authored by men compared to those authored by women. We quantify how this bias could affect results in downstream applications and show that current machine learning tools, including large language models, should be applied with caution when the gender composition of a sample is not known or variable. Our findings demonstrate that sentiment analysis is not yet a solved problem, especially in ensuring equitable model behaviour across demographic groups.

</details>


### [56] [AM$^3$Safety: Towards Data Efficient Alignment of Multi-modal Multi-turn Safety for MLLMs](https://arxiv.org/abs/2601.04736)
*Han Zhu,Jiale Chen,Chengkun Cai,Shengjie Sun,Haoran Li,Yujin Zhou,Chi-Min Chan,Pengcheng Wen,Lei Li,Sirui Han,Yike Guo*

Main category: cs.CL

TL;DR: InterSafe-V dataset and AM³Safety framework improve multi-modal LLM safety in multi-turn dialogues, reducing attack success by 10+% while maintaining helpfulness.


<details>
  <summary>Details</summary>
Motivation: MLLMs deployed in interactive applications have safety vulnerabilities in multi-turn scenarios where harmful intent can be reconstructed across turns and security protocols fade over time. Existing RLHF methods are designed for single-turn VQA tasks and require costly manual annotations, limiting effectiveness in dialogues.

Method: 1) Created InterSafe-V dataset (11,270 dialogues + 500 refusal VQA samples) through model interactions to reflect real-world scenarios. 2) Proposed AM³Safety framework with cold-start refusal phase and Group Relative Policy Optimization (GRPO) fine-tuning using turn-aware dual-objective rewards across entire dialogues.

Result: Experiments on Qwen2.5-VL-7B-Instruct and LLaVA-NeXT-7B show: >10% decrease in Attack Success Rate (ASR), at least 8% improvement in harmless dimension, over 13% improvement in helpful dimension on multi-modal multi-turn safety benchmarks, while preserving general abilities.

Conclusion: The proposed InterSafe-V dataset and AM³Safety framework effectively address multi-turn safety vulnerabilities in MLLMs, significantly improving both safety and helpfulness without compromising general capabilities.

Abstract: Multi-modal Large Language Models (MLLMs) are increasingly deployed in interactive applications. However, their safety vulnerabilities become pronounced in multi-turn multi-modal scenarios, where harmful intent can be gradually reconstructed across turns, and security protocols fade into oblivion as the conversation progresses. Existing Reinforcement Learning from Human Feedback (RLHF) alignment methods are largely developed for single-turn visual question-answer (VQA) task and often require costly manual preference annotations, limiting their effectiveness and scalability in dialogues. To address this challenge, we present InterSafe-V, an open-source multi-modal dialogue dataset containing 11,270 dialogues and 500 specially designed refusal VQA samples. This dataset, constructed through interaction between several models, is designed to more accurately reflect real-world scenarios and includes specialized VQA pairs tailored for specific domains. Building on this dataset, we propose AM$^3$Safety, a framework that combines a cold-start refusal phase with Group Relative Policy Optimization (GRPO) fine-tuning using turn-aware dual-objective rewards across entire dialogues. Experiments on Qwen2.5-VL-7B-Instruct and LLaVA-NeXT-7B show more than 10\% decrease in Attack Success Rate (ASR) together with an increment of at least 8\% in harmless dimension and over 13\% in helpful dimension of MLLMs on multi-modal multi-turn safety benchmarks, while preserving their general abilities.

</details>


### [57] [RiskAtlas: Exposing Domain-Specific Risks in LLMs through Knowledge-Graph-Guided Harmful Prompt Generation](https://arxiv.org/abs/2601.04740)
*Huawei Zheng,Xinqi Jiang,Sen Yang,Shouling Ji,Yingcai Wu,Dazhen Deng*

Main category: cs.CL

TL;DR: Proposes an end-to-end framework for generating implicit harmful prompts in specialized domains using knowledge-graph-guided generation and dual-path obfuscation rewriting to create realistic red-teaming datasets.


<details>
  <summary>Details</summary>
Motivation: LLMs in specialized domains (finance, healthcare) face unique safety risks, but domain-specific harmful prompt datasets are scarce and rely on manual construction. Public datasets focus on explicit harmful prompts that modern LLM defenses can detect, while implicit harmful prompts using indirect domain knowledge are harder to detect and better reflect real-world threats.

Method: Two-stage framework: 1) Knowledge-graph-guided harmful prompt generation to systematically produce domain-relevant prompts, 2) Dual-path obfuscation rewriting that converts explicit harmful prompts into implicit variants through direct and context-enhanced rewriting.

Result: The framework yields high-quality datasets combining strong domain relevance with implicitness, enabling more realistic red-teaming and advancing LLM safety research. Code and datasets are released on GitHub.

Conclusion: The proposed approach addresses the challenges of transforming domain knowledge into actionable constraints and increasing implicitness of generated harmful prompts, creating more realistic safety testing datasets for LLMs in specialized domains.

Abstract: Large language models (LLMs) are increasingly applied in specialized domains such as finance and healthcare, where they introduce unique safety risks. Domain-specific datasets of harmful prompts remain scarce and still largely rely on manual construction; public datasets mainly focus on explicit harmful prompts, which modern LLM defenses can often detect and refuse. In contrast, implicit harmful prompts-expressed through indirect domain knowledge-are harder to detect and better reflect real-world threats. We identify two challenges: transforming domain knowledge into actionable constraints and increasing the implicitness of generated harmful prompts. To address them, we propose an end-to-end framework that first performs knowledge-graph-guided harmful prompt generation to systematically produce domain-relevant prompts, and then applies dual-path obfuscation rewriting to convert explicit harmful prompts into implicit variants via direct and context-enhanced rewriting. This framework yields high-quality datasets combining strong domain relevance with implicitness, enabling more realistic red-teaming and advancing LLM safety research. We release our code and datasets at GitHub.

</details>


### [58] [Tool-MAD: A Multi-Agent Debate Framework for Fact Verification with Diverse Tool Augmentation and Adaptive Retrieval](https://arxiv.org/abs/2601.04742)
*Seyeon Jeong,Yeonjun Choi,JongWook Kim,Beakcheol Jang*

Main category: cs.CL

TL;DR: Tool-MAD: Multi-agent debate framework where agents use different external tools (search, RAG) to improve factual verification, with adaptive query refinement and quantitative hallucination detection.


<details>
  <summary>Details</summary>
Motivation: Existing multi-agent debate systems rely on internal knowledge or static documents, making them vulnerable to hallucinations. MADKE introduced external evidence but has a one-time retrieval mechanism that can't adapt to new arguments during debates.

Method: 1) Multi-agent framework with agents using heterogeneous external tools (search API, RAG modules) for diverse perspectives. 2) Adaptive query formulation that iteratively refines evidence retrieval based on debate flow. 3) Integration of Faithfulness and Answer Relevance scores for quantitative hallucination detection by Judge agent.

Result: Outperforms state-of-the-art MAD frameworks on four fact verification benchmarks with up to 5.5% accuracy improvement. Shows strong robustness and adaptability in medically specialized domains across various tool configurations.

Conclusion: Tool-MAD effectively mitigates hallucinations in LLMs through tool-enhanced multi-agent debates, demonstrating potential for real-world fact-checking applications with its adaptive evidence retrieval and quantitative verification approach.

Abstract: Large Language Models (LLMs) suffer from hallucinations and factual inaccuracies, especially in complex reasoning and fact verification tasks. Multi-Agent Debate (MAD) systems aim to improve answer accuracy by enabling multiple LLM agents to engage in dialogue, promoting diverse reasoning and mutual verification. However, existing MAD frameworks primarily rely on internal knowledge or static documents, making them vulnerable to hallucinations. While MADKE introduces external evidence to mitigate this, its one-time retrieval mechanism limits adaptability to new arguments or emerging information during the debate. To address these limitations, We propose Tool-MAD, a multi-agent debate framework that enhances factual verification by assigning each agent a distinct external tool, such as a search API or RAG module. Tool-MAD introduces three key innovations: (1) a multi-agent debate framework where agents leverage heterogeneous external tools, encouraging diverse perspectives, (2) an adaptive query formulation mechanism that iteratively refines evidence retrieval based on the flow of the debate, and (3) the integration of Faithfulness and Answer Relevance scores into the final decision process, allowing the Judge agent to quantitatively assess the coherence and question alignment of each response and effectively detect hallucinations. Experimental results on four fact verification benchmarks demonstrate that Tool-MAD consistently outperforms state-of-the-art MAD frameworks, achieving up to 5.5% accuracy improvement. Furthermore, in medically specialized domains, Tool-MAD exhibits strong robustness and adaptability across various tool configurations and domain conditions, confirming its potential for broader real-world fact-checking applications.

</details>


### [59] [PILOT-Bench: A Benchmark for Legal Reasoning in the Patent Domain with IRAC-Aligned Classification Tasks](https://arxiv.org/abs/2601.04758)
*Yehoon Jang,Chaewon Lee,Hyun-seok Min,Sungchul Choi*

Main category: cs.CL

TL;DR: PILOT-Bench is the first PTAB-centric benchmark that evaluates LLMs on structured legal reasoning tasks in patent law, revealing significant performance gaps between closed-source and open-source models.


<details>
  <summary>Details</summary>
Motivation: Current LLM applications in patent law are limited to lightweight tasks, with no systematic way to evaluate their capacity for structured legal reasoning in the patent domain, particularly for PTAB appeals that require integration of technical and legal analysis.

Method: Created PILOT-Bench by aligning PTAB decisions with USPTO patent data at case-level and formalizing three IRAC-aligned classification tasks: Issue Type, Board Authorities, and Subdecision. Evaluated diverse closed-source and open-source LLMs across multiple perspectives including input-variation settings, model families, and error tendencies.

Result: Closed-source models consistently exceed 0.75 Micro-F1 score on Issue Type task, while the strongest open-source model (Qwen-8B) achieves only around 0.56, showing substantial gap in reasoning capabilities. The benchmark provides foundation for systematic evaluation of patent-domain legal reasoning.

Conclusion: PILOT-Bench establishes a foundation for systematic evaluation of patent-domain legal reasoning and points toward future directions for improving LLMs through dataset design and model alignment. All resources are publicly available for further research.

Abstract: The Patent Trial and Appeal Board (PTAB) of the USPTO adjudicates thousands of ex parte appeals each year, requiring the integration of technical understanding and legal reasoning. While large language models (LLMs) are increasingly applied in patent and legal practice, their use has remained limited to lightweight tasks, with no established means of systematically evaluating their capacity for structured legal reasoning in the patent domain. In this work, we introduce PILOT-Bench, the first PTAB-centric benchmark that aligns PTAB decisions with USPTO patent data at the case-level and formalizes three IRAC-aligned classification tasks: Issue Type, Board Authorities, and Subdecision. We evaluate a diverse set of closed-source (commercial) and open-source LLMs and conduct analyses across multiple perspectives, including input-variation settings, model families, and error tendencies. Notably, on the Issue Type task, closed-source models consistently exceed 0.75 in Micro-F1 score, whereas the strongest open-source model (Qwen-8B) achieves performance around 0.56, highlighting a substantial gap in reasoning capabilities. PILOT-Bench establishes a foundation for the systematic evaluation of patent-domain legal reasoning and points toward future directions for improving LLMs through dataset design and model alignment. All data, code, and benchmark resources are available at https://github.com/TeamLab/pilot-bench.

</details>


### [60] [Differential syntactic and semantic encoding in LLMs](https://arxiv.org/abs/2601.04765)
*Santiago Acevedo,Alessandro Laio,Marco Baroni*

Main category: cs.CL

TL;DR: LLM representations linearly encode syntax and semantics; averaging sentence vectors creates syntactic/semantic centroids that capture linguistic information; these signals can be partially decoupled.


<details>
  <summary>Details</summary>
Motivation: To understand how syntactic and semantic information is encoded in the inner representations of large language models, specifically DeepSeek-V3, and whether these linguistic signals are linearly separable.

Method: Analyze hidden representations by averaging vectors of sentences with shared syntactic structure or meaning to create syntactic and semantic "centroids." Subtract these centroids from sentence vectors to test their effect on similarity with syntactically/semantically matched sentences.

Result: Syntactic and semantic centroids capture significant linguistic information; subtracting them strongly affects similarity with matched sentences, suggesting linear encoding. Syntax and semantics have different cross-layer encoding profiles and can be partially decoupled.

Conclusion: Syntax and semantics are at least partially linearly encoded in LLM representations, with differential encoding patterns that allow for some decoupling of these linguistic signals.

Abstract: We study how syntactic and semantic information is encoded in inner layer representations of Large Language Models (LLMs), focusing on the very large DeepSeek-V3. We find that, by averaging hidden-representation vectors of sentences sharing syntactic structure or meaning, we obtain vectors that capture a significant proportion of the syntactic and semantic information contained in the representations. In particular, subtracting these syntactic and semantic ``centroids'' from sentence vectors strongly affects their similarity with syntactically and semantically matched sentences, respectively, suggesting that syntax and semantics are, at least partially, linearly encoded. We also find that the cross-layer encoding profiles of syntax and semantics are different, and that the two signals can to some extent be decoupled, suggesting differential encoding of these two types of linguistic information in LLM representations.

</details>


### [61] [Revisiting Judge Decoding from First Principles via Training-Free Distributional Divergence](https://arxiv.org/abs/2601.04766)
*Shengyin Sun,Yiming Li,Renxi Liu,Weizhe Lin,Hui-Ling Zhen,Xianzhi Yu,Mingxuan Yuan,Chen Ma*

Main category: cs.CL

TL;DR: Training-free KL divergence verification matches or beats supervised judges for speculative decoding, eliminating supervision needs.


<details>
  <summary>Details</summary>
Motivation: Current judge decoding methods for LLM inference acceleration require expensive and noisy supervision to learn criticality scores, creating a bottleneck.

Method: Propose a training-free verification mechanism based on KL divergence between draft and target distributions, theoretically showing it uses same underlying logit primitives as supervised linear judges.

Result: Extensive experiments on reasoning and coding benchmarks show the method matches or outperforms complex trained judges like AutoJudge, with superior robustness to domain shifts.

Conclusion: KL divergence provides an effective, supervision-free alternative to learned judges for speculative decoding, eliminating the supervision bottleneck while maintaining or improving performance.

Abstract: Judge Decoding accelerates LLM inference by relaxing the strict verification of Speculative Decoding, yet it typically relies on expensive and noisy supervision. In this work, we revisit this paradigm from first principles, revealing that the ``criticality'' scores learned via costly supervision are intrinsically encoded in the draft-target distributional divergence. We theoretically prove a structural correspondence between learned linear judges and Kullback-Leibler (KL) divergence, demonstrating they rely on the same underlying logit primitives. Guided by this, we propose a simple, training-free verification mechanism based on KL divergence. Extensive experiments across reasoning and coding benchmarks show that our method matches or outperforms complex trained judges (e.g., AutoJudge), offering superior robustness to domain shifts and eliminating the supervision bottleneck entirely.

</details>


### [62] [LANGSAE EDITING: Improving Multilingual Information Retrieval via Post-hoc Language Identity Removal](https://arxiv.org/abs/2601.04768)
*Dongjun Kim,Jeongho Yoon,Chanjun Park,Heuiseok Lim*

Main category: cs.CL

TL;DR: LANGSAE EDITING removes language-identity signals from multilingual embeddings via sparse autoencoder to improve cross-language retrieval without retraining.


<details>
  <summary>Details</summary>
Motivation: Multilingual embeddings encode language identity alongside semantics, which inflates similarity for same-language pairs and reduces effectiveness for cross-language retrieval in mixed-language collections.

Method: Post-hoc sparse autoencoder trained on pooled embeddings identifies language-associated latent units using cross-language activation statistics, suppresses these units at inference, and reconstructs embeddings in original dimensionality.

Result: Consistent improvements in ranking quality and cross-language coverage across multiple languages, with especially strong gains for script-distinct languages.

Conclusion: LANGSAE EDITING enables controllable removal of language-identity signals, making it compatible with existing vector databases without retraining base encoders or re-encoding raw text.

Abstract: Dense retrieval in multilingual settings often searches over mixed-language collections, yet multilingual embeddings encode language identity alongside semantics. This language signal can inflate similarity for same-language pairs and crowd out relevant evidence written in other languages. We propose LANGSAE EDITING, a post-hoc sparse autoencoder trained on pooled embeddings that enables controllable removal of language-identity signal directly in vector space. The method identifies language-associated latent units using cross-language activation statistics, suppresses these units at inference time, and reconstructs embeddings in the original dimensionality, making it compatible with existing vector databases without retraining the base encoder or re-encoding raw text. Experiments across multiple languages show consistent improvements in ranking quality and cross-language coverage, with especially strong gains for script-distinct languages.

</details>


### [63] [NC2C: Automated Convexification of Generic Non-Convex Optimization Problems](https://arxiv.org/abs/2601.04789)
*Xinyue Peng,Yanming Liu,Yihan Cang,Yuwei Zhang,Xinyi Wang,Songhang Deng,Jiannan Cao*

Main category: cs.CL

TL;DR: NC2C is an LLM-based framework that automatically transforms non-convex optimization problems into solvable convex forms, reducing expert dependency and enabling efficient convex solver deployment.


<details>
  <summary>Details</summary>
Motivation: Non-convex optimization problems are pervasive but intractable for traditional solvers due to complex objectives and constraints. Manual convexification is inefficient and requires expert knowledge, creating a need for automated solutions.

Method: NC2C uses LLMs' mathematical reasoning to detect non-convex components, select optimal convexification strategies, and generate rigorous convex equivalents. It integrates symbolic reasoning, adaptive transformation techniques, iterative validation, error correction loops, and feasibility domain correction mechanisms.

Result: On 100 generic non-convex problems, NC2C achieves 89.3% execution rate and 76% success rate in producing feasible, high-quality convex transformations, significantly outperforming baseline methods.

Conclusion: NC2C demonstrates that LLMs can effectively automate non-convex to convex transformation, reduce expert dependency, and enable efficient deployment of convex solvers for previously intractable optimization tasks.

Abstract: Non-convex optimization problems are pervasive across mathematical programming, engineering design, and scientific computing, often posing intractable challenges for traditional solvers due to their complex objective functions and constrained landscapes. To address the inefficiency of manual convexification and the over-reliance on expert knowledge, we propose NC2C, an LLM-based end-to-end automated framework designed to transform generic non-convex optimization problems into solvable convex forms using large language models. NC2C leverages LLMs' mathematical reasoning capabilities to autonomously detect non-convex components, select optimal convexification strategies, and generate rigorous convex equivalents. The framework integrates symbolic reasoning, adaptive transformation techniques, and iterative validation, equipped with error correction loops and feasibility domain correction mechanisms to ensure the robustness and validity of transformed problems. Experimental results on a diverse dataset of 100 generic non-convex problems demonstrate that NC2C achieves an 89.3\% execution rate and a 76\% success rate in producing feasible, high-quality convex transformations. This outperforms baseline methods by a significant margin, highlighting NC2C's ability to leverage LLMs for automated non-convex to convex transformation, reduce expert dependency, and enable efficient deployment of convex solvers for previously intractable optimization tasks.

</details>


### [64] [Belief in Authority: Impact of Authority in Multi-Agent Evaluation Framework](https://arxiv.org/abs/2601.04790)
*Junhyuk Choi,Jeongyoun Kwon,Heeju Kim,Haeun Cho,Hayeong Jung,Sehee Min,Bugeun Kim*

Main category: cs.CL

TL;DR: First systematic analysis of role-based authority bias in multi-agent LLM systems shows Expert and Referent power roles have stronger influence than Legitimate power roles, with bias emerging from authoritative roles maintaining positions while general agents show flexibility.


<details>
  <summary>Details</summary>
Motivation: Multi-agent systems using LLMs often assign authoritative roles to improve performance, but the impact of authority bias on agent interactions remains underexplored, creating a need for systematic analysis of how different types of authority influence agent dynamics.

Method: Used ChatEval for free-form multi-agent evaluation, applying French and Raven's power-based theory to classify authoritative roles into legitimate, referent, and expert types. Analyzed influence across 12-turn conversations using GPT-4o and DeepSeek R1 models.

Result: Expert and Referent power roles exert stronger influence than Legitimate power roles. Authority bias emerges not through active conformity by general agents, but through authoritative roles consistently maintaining positions while general agents demonstrate flexibility. Authority influence requires clear position statements, as neutral responses fail to generate bias.

Conclusion: The study provides key insights for designing multi-agent frameworks with asymmetric interaction patterns, revealing that authority bias operates through authoritative role persistence rather than general agent conformity, and that influence requires explicit position-taking.

Abstract: Multi-agent systems utilizing large language models often assign authoritative roles to improve performance, yet the impact of authority bias on agent interactions remains underexplored. We present the first systematic analysis of role-based authority bias in free-form multi-agent evaluation using ChatEval. Applying French and Raven's power-based theory, we classify authoritative roles into legitimate, referent, and expert types and analyze their influence across 12-turn conversations. Experiments with GPT-4o and DeepSeek R1 reveal that Expert and Referent power roles exert stronger influence than Legitimate power roles. Crucially, authority bias emerges not through active conformity by general agents, but through authoritative roles consistently maintaining their positions while general agents demonstrate flexibility. Furthermore, authority influence requires clear position statements, as neutral responses fail to generate bias. These findings provide key insights for designing multi-agent frameworks with asymmetric interaction patterns.

</details>


### [65] [When AI Settles Down: Late-Stage Stability as a Signature of AI-Generated Text Detection](https://arxiv.org/abs/2601.04833)
*Ke Sun,Guangsheng Bao,Han Cui,Yue Zhang*

Main category: cs.CL

TL;DR: Paper identifies "Late-Stage Volatility Decay" in AI-generated text - AI text shows rapidly stabilizing log probability fluctuations in later stages while human writing maintains higher variability. Proposes two simple late-stage features for zero-shot detection without extra model access.


<details>
  <summary>Details</summary>
Motivation: Current zero-shot detection methods aggregate token-level statistics across entire sequences, missing temporal dynamics of autoregressive generation. Need to understand how AI-generated text evolves over time and leverage this for better detection.

Method: Analyzed 120k+ text samples to discover Late-Stage Volatility Decay phenomenon. Proposed two simple features: Derivative Dispersion and Local Volatility, computed exclusively from late-stage statistics without perturbation sampling or additional model access.

Result: AI-generated text shows 24-32% lower volatility in second half of sequences compared to human writing. The method achieves state-of-the-art performance on EvoBench and MAGE benchmarks and demonstrates strong complementarity with existing global methods.

Conclusion: Temporal dynamics (Late-Stage Volatility Decay) provide valuable signals for AI-generated text detection. Simple late-stage features can achieve strong performance without complex sampling or additional model queries, complementing existing approaches.

Abstract: Zero-shot detection methods for AI-generated text typically aggregate token-level statistics across entire sequences, overlooking the temporal dynamics inherent to autoregressive generation. We analyze over 120k text samples and reveal Late-Stage Volatility Decay: AI-generated text exhibits rapidly stabilizing log probability fluctuations as generation progresses, while human writing maintains higher variability throughout. This divergence peaks in the second half of sequences, where AI-generated text shows 24--32\% lower volatility. Based on this finding, we propose two simple features: Derivative Dispersion and Local Volatility, which computed exclusively from late-stage statistics. Without perturbation sampling or additional model access, our method achieves state-of-the-art performance on EvoBench and MAGE benchmarks and demonstrates strong complementarity with existing global methods.

</details>


### [66] [RAAR: Retrieval Augmented Agentic Reasoning for Cross-Domain Misinformation Detection](https://arxiv.org/abs/2601.04853)
*Zhiwei Liu,Runteng Guo,Baojie Qu,Yuechen Jiang,Min Peng,Qianqian Xie,Sophia Ananiadou*

Main category: cs.CL

TL;DR: RAAR is a retrieval-augmented agentic reasoning framework for cross-domain misinformation detection that uses multi-perspective evidence retrieval and multi-agent collaboration to overcome domain transfer challenges.


<details>
  <summary>Details</summary>
Motivation: Cross-domain misinformation detection is difficult due to substantial differences in knowledge and discourse across domains. Existing methods struggle with generalization to challenging/underrepresented domains, rely on single-perspective cues, and LLMs are limited to same-distribution data.

Method: RAAR retrieves multi-perspective source-domain evidence aligned with target samples' semantics, sentiment, and writing style. It uses specialized multi-agent collaboration where perspective-specific agents produce complementary analyses and a summary agent integrates them under verifier guidance. The framework applies supervised fine-tuning and reinforcement learning to train a multi-task verifier.

Result: RAAR-8b and RAAR-14b models were trained. Evaluation on three cross-domain misinformation detection tasks shows RAAR substantially enhances base model capabilities and outperforms other cross-domain methods, advanced LLMs, and LLM-based adaptation approaches.

Conclusion: RAAR successfully addresses cross-domain misinformation detection challenges through retrieval-augmented agentic reasoning, enabling better generalization across domains with different knowledge and discourse characteristics.

Abstract: Cross-domain misinformation detection is challenging, as misinformation arises across domains with substantial differences in knowledge and discourse. Existing methods often rely on single-perspective cues and struggle to generalize to challenging or underrepresented domains, while reasoning large language models (LLMs), though effective on complex tasks, are limited to same-distribution data. To address these gaps, we introduce RAAR, the first retrieval-augmented agentic reasoning framework for cross-domain misinformation detection. To enable cross-domain transfer beyond same-distribution assumptions, RAAR retrieves multi-perspective source-domain evidence aligned with each target sample's semantics, sentiment, and writing style. To overcome single-perspective modeling and missing systematic reasoning, RAAR constructs verifiable multi-step reasoning paths through specialized multi-agent collaboration, where perspective-specific agents produce complementary analyses and a summary agent integrates them under verifier guidance. RAAR further applies supervised fine-tuning and reinforcement learning to train a single multi-task verifier to enhance verification and reasoning capabilities. Based on RAAR, we trained the RAAR-8b and RAAR-14b models. Evaluation on three cross-domain misinformation detection tasks shows that RAAR substantially enhances the capabilities of the base models and outperforms other cross-domain methods, advanced LLMs, and LLM-based adaptation approaches. The project will be released at https://github.com/lzw108/RAAR.

</details>


### [67] [Token Maturation: Autoregressive Language Generation via Continuous Token Dynamics](https://arxiv.org/abs/2601.04854)
*Oshri Naparstek*

Main category: cs.CL

TL;DR: Continuous autoregressive language model where tokens evolve as continuous vectors over multiple steps before discretization, enabling stable deterministic generation without token-level sampling.


<details>
  <summary>Details</summary>
Motivation: Traditional autoregressive models commit to discrete tokens early, forcing uncertainty resolution through token-level sampling which causes instability, repetition, and sensitivity to decoding heuristics.

Method: Introduces continuous autoregressive formulation where tokens are represented as continuous vectors that mature over multiple update steps before discretization. Uses deterministic dynamical process to evolve continuous token representations, committing to discrete tokens only when representations have sufficiently converged.

Result: The maturation process alone produces coherent and diverse text using deterministic decoding (argmax) without token-level sampling, diffusion-style denoising, or auxiliary stabilization mechanisms. Additional perturbations can be incorporated but are not required.

Conclusion: First autoregressive language model that generates text by evolving continuous token representations to convergence prior to discretization, enabling stable generation without token-level sampling.

Abstract: Autoregressive language models are conventionally defined over discrete token sequences, committing to a specific token at every generation step. This early discretization forces uncertainty to be resolved through token-level sampling, often leading to instability, repetition, and sensitivity to decoding heuristics.
  In this work, we introduce a continuous autoregressive formulation of language generation in which tokens are represented as continuous vectors that \emph{mature} over multiple update steps before being discretized. Rather than sampling tokens, the model evolves continuous token representations through a deterministic dynamical process, committing to a discrete token only when the representation has sufficiently converged. Discrete text is recovered via hard decoding, while uncertainty is maintained and resolved in the continuous space.
  We show that this maturation process alone is sufficient to produce coherent and diverse text using deterministic decoding (argmax), without reliance on token-level sampling, diffusion-style denoising, or auxiliary stabilization mechanisms. Additional perturbations, such as stochastic dynamics or history smoothing, can be incorporated naturally but are not required for the model to function.
  To our knowledge, this is the first autoregressive language model that generates text by evolving continuous token representations to convergence prior to discretization, enabling stable generation without token-level sampling.

</details>


### [68] [MisSpans: Fine-Grained False Span Identification in Cross-Domain Fake News](https://arxiv.org/abs/2601.04857)
*Zhiwei Liu,Paul Thompson,Jiaqi Rong,Baojie Qu,Runteng Guo,Min Peng,Qianqian Xie,Sophia Ananiadou*

Main category: cs.CL

TL;DR: MisSpans is a new benchmark for fine-grained misinformation analysis that identifies false spans within sentences, categorizes misinformation types, and provides explanations, addressing limitations of coarse binary claim-level evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing misinformation benchmarks use coarse binary labels at claim/paragraph level, which obscures how true and false details often co-exist within single sentences and limits interpretability for identifying specific misleading segments or differentiating misinformation types.

Method: Created MisSpans benchmark with paired real/fake news stories and three tasks: MisSpansIdentity (pinpointing false spans), MisSpansType (categorizing false spans by type), and MisSpansExplanation (providing rationales). Expert annotators used standardized guidelines with consistency checks. Evaluated 15 LLMs (reasoning-enhanced and non-reasoning variants) under zero-shot and one-shot settings.

Result: Results show the challenging nature of fine-grained misinformation identification and analysis. Performance is influenced by multiple interacting factors including model size, reasoning capabilities, and domain-specific textual features. High inter-annotator agreement was achieved through expert annotation with standardized guidelines.

Conclusion: MisSpans enables fine-grained localization, nuanced characterization beyond true/false, and actionable explanations for misinformation analysis. The benchmark highlights the need for deeper understanding of how various factors influence performance in fine-grained misinformation tasks.

Abstract: Online misinformation is increasingly pervasive, yet most existing benchmarks and methods evaluate veracity at the level of whole claims or paragraphs using coarse binary labels, obscuring how true and false details often co-exist within single sentences. These simplifications also limit interpretability: global explanations cannot identify which specific segments are misleading or differentiate how a detail is false (e.g., distorted vs. fabricated). To address these gaps, we introduce MisSpans, the first multi-domain, human-annotated benchmark for span-level misinformation detection and analysis, consisting of paired real and fake news stories. MisSpans defines three complementary tasks: MisSpansIdentity for pinpointing false spans within sentences, MisSpansType for categorising false spans by misinformation type, and MisSpansExplanation for providing rationales grounded in identified spans. Together, these tasks enable fine-grained localisation, nuanced characterisation beyond true/false and actionable explanations. Expert annotators were guided by standardised guidelines and consistency checks, leading to high inter-annotator agreement. We evaluate 15 representative LLMs, including reasoning-enhanced and non-reasoning variants, under zero-shot and one-shot settings. Results reveal the challenging nature of fine-grained misinformation identification and analysis, and highlight the need for a deeper understanding of how performance may be influenced by multiple interacting factors, including model size and reasoning capabilities, along with domain-specific textual features. This project will be available at https://github.com/lzw108/MisSpans.

</details>


### [69] [A Navigational Approach for Comprehensive RAG via Traversal over Proposition Graphs](https://arxiv.org/abs/2601.04859)
*Maxime Delmas,Lei Xu,André Freitas*

Main category: cs.CL

TL;DR: ToPG is a novel RAG framework that uses heterogeneous proposition graphs and iterative suggestion-selection cycles to handle both simple factual and complex multi-hop queries effectively.


<details>
  <summary>Details</summary>
Motivation: Standard RAG pipelines fail on complex multi-hop queries due to lack of structural connectivity, while KG-based RAG performs well on complex tasks but suffers on simple factual queries. There's a need for a framework that bridges this gap.

Method: ToPG models knowledge as a heterogeneous graph of propositions, entities, and passages, combining factual granularity with graph connectivity. It uses iterative Suggestion-Selection cycles: Suggestion phase enables query-aware graph traversal, and Selection phase uses LLM feedback to prune irrelevant propositions and seed next iteration.

Result: Evaluated on three QA tasks (Simple, Complex, and Abstract QA), ToPG demonstrates strong performance across both accuracy- and quality-based metrics, showing effectiveness across different query types.

Conclusion: ToPG shows that query-aware graph traversal combined with factual granularity is critical for efficient structured RAG systems, bridging the gap between simple factual retrieval and complex multi-hop reasoning.

Abstract: Standard RAG pipelines based on chunking excel at simple factual retrieval but fail on complex multi-hop queries due to a lack of structural connectivity. Conversely, initial strategies that interleave retrieval with reasoning often lack global corpus awareness, while Knowledge Graph (KG)-based RAG performs strongly on complex multi-hop tasks but suffers on fact-oriented single-hop queries. To bridge this gap, we propose a novel RAG framework: ToPG (Traversal over Proposition Graphs). ToPG models its knowledge base as a heterogeneous graph of propositions, entities, and passages, effectively combining the granular fact density of propositions with graph connectivity. We leverage this structure using iterative Suggestion-Selection cycles, where the Suggestion phase enables a query-aware traversal of the graph, and the Selection phase provides LLM feedback to prune irrelevant propositions and seed the next iteration. Evaluated on three distinct QA tasks (Simple, Complex, and Abstract QA), ToPG demonstrates strong performance across both accuracy- and quality-based metrics. Overall, ToPG shows that query-aware graph traversal combined with factual granularity is a critical component for efficient structured RAG systems. ToPG is available at https://github.com/idiap/ToPG.

</details>


### [70] [EvolSQL: Structure-Aware Evolution for Scalable Text-to-SQL Data Synthesis](https://arxiv.org/abs/2601.04875)
*Xuanguang Pan,Chongyang Tao,Jiayuan Bai,Jianling Gao,Zhengwei Tao,Xiansheng Zhou,Gavin Cheung,Shuai Ma*

Main category: cs.CL

TL;DR: EvolSQL is a structure-aware data synthesis framework that evolves SQL queries from seed data to create diverse, complex Text-to-SQL training data, outperforming larger datasets with significantly less data.


<details>
  <summary>Details</summary>
Motivation: Existing Text-to-SQL training suffers from limited high-quality, diverse datasets - either relying on scarce human annotations or simple LLM synthesis without structural control, resulting in poor structural diversity and complexity.

Method: EvolSQL uses structure-aware evolution: 1) Query-SQL expansion for question diversity and schema coverage, 2) Adaptive directional evolution with six AST-based transformation operators to increase complexity across relational, predicate, aggregation, and nesting dimensions, 3) Execution-grounded SQL refinement and schema-aware deduplication.

Result: A 7B model fine-tuned on EvolSQL data outperforms one trained on the much larger SynSQL dataset using only 1/18 of the data, demonstrating superior data efficiency and quality.

Conclusion: EvolSQL provides an effective structure-aware data synthesis framework that generates high-quality, structurally diverse Text-to-SQL training data, addressing dataset limitations and enabling better model performance with less data.

Abstract: Training effective Text-to-SQL models remains challenging due to the scarcity of high-quality, diverse, and structurally complex datasets. Existing methods either rely on limited human-annotated corpora, or synthesize datasets directly by simply prompting LLMs without explicit control over SQL structures, often resulting in limited structural diversity and complexity. To address this, we introduce EvolSQL, a structure-aware data synthesis framework that evolves SQL queries from seed data into richer and more semantically diverse forms. EvolSQL starts with an exploratory Query-SQL expansion to broaden question diversity and improve schema coverage, and then applies an adaptive directional evolution strategy using six atomic transformation operators derived from the SQL Abstract Syntax Tree to progressively increase query complexity across relational, predicate, aggregation, and nesting dimensions. An execution-grounded SQL refinement module and schema-aware deduplication further ensure the creation of high-quality, structurally diverse mapping pairs. Experimental results show that a 7B model fine-tuned on our data outperforms one trained on the much larger SynSQL dataset using only 1/18 of the data.

</details>


### [71] [Mind2Report: A Cognitive Deep Research Agent for Expert-Level Commercial Report Synthesis](https://arxiv.org/abs/2601.04879)
*Mingyue Cheng,Daoyu Wang,Qi Liu,Shuo Yu,Xiaoyu Tao,Yuqian Wang,Chengzhong Chu,Yu Duan,Mingkang Long,Enhong Chen*

Main category: cs.CL

TL;DR: Mind2Report is a cognitive deep research agent that emulates commercial analysts to synthesize expert-level reports from web sources, outperforming existing deep research agents in quality, reliability, and coverage.


<details>
  <summary>Details</summary>
Motivation: Current deep research agents produce reports with limited quality, reliability, and coverage, which is problematic for high-stakes business decisions that require informative commercial reports from massive, noisy web sources.

Method: Mind2Report uses a training-free agentic workflow that augments LLMs with dynamic memory. It follows a three-step cognitive process: (1) probes fine-grained intent, (2) searches web sources and records distilled information on the fly, and (3) iteratively synthesizes the report.

Result: Experiments using QRC-Eval (200 real-world commercial tasks) show Mind2Report outperforms leading baselines including OpenAI and Gemini deep research agents in report quality, reliability, and coverage.

Conclusion: Mind2Report serves as a foundation for advancing commercial deep research agent design, demonstrating that cognitive agentic workflows with dynamic memory can effectively emulate human analysts for expert-level report synthesis.

Abstract: Synthesizing informative commercial reports from massive and noisy web sources is critical for high-stakes business decisions. Although current deep research agents achieve notable progress, their reports still remain limited in terms of quality, reliability, and coverage. In this work, we propose Mind2Report, a cognitive deep research agent that emulates the commercial analyst to synthesize expert-level reports. Specifically, it first probes fine-grained intent, then searches web sources and records distilled information on the fly, and subsequently iteratively synthesizes the report. We design Mind2Report as a training-free agentic workflow that augments general large language models (LLMs) with dynamic memory to support these long-form cognitive processes. To rigorously evaluate Mind2Report, we further construct QRC-Eval comprising 200 real-world commercial tasks and establish a holistic evaluation strategy to assess report quality, reliability, and coverage. Experiments demonstrate that Mind2Report outperforms leading baselines, including OpenAI and Gemini deep research agents. Although this is a preliminary study, we expect it to serve as a foundation for advancing the future design of commercial deep research agents. Our code and data are available at https://github.com/Melmaphother/Mind2Report.

</details>


### [72] [CuMA: Aligning LLMs with Sparse Cultural Values via Demographic-Aware Mixture of Adapters](https://arxiv.org/abs/2601.04885)
*Ao Sun,Xiaoyu Wang,Zhe Tan,Yu Li,Jiachen Zhu,Shu Su,Yuheng Jia*

Main category: cs.CL

TL;DR: CuMA addresses cultural alignment in LLMs by using demographic-aware routing and mixture of adapters to prevent mean collapse and preserve cultural diversity.


<details>
  <summary>Details</summary>
Motivation: Current LLM alignment enforces universal consensus but fails to respect cultural pluralism. Dense models suffer from "Mean Collapse" when forced to fit conflicting value distributions, converging to a generic average that doesn't represent diverse cultural groups.

Method: Proposes CuMA (Cultural Mixture of Adapters), a framework that treats alignment as a conditional capacity separation problem. It uses demographic-aware routing to create a Latent Cultural Topology, disentangling conflicting gradients into specialized expert subspaces through mixture of adapters.

Result: Extensive evaluations on WorldValuesBench, Community Alignment, and PRISM show CuMA achieves state-of-the-art performance, significantly outperforming dense baselines and semantic-only MoEs. Analysis confirms CuMA effectively mitigates mean collapse and preserves cultural diversity.

Conclusion: CuMA successfully addresses cultural alignment by preventing mean collapse through explicit disentanglement of conflicting cultural gradients, enabling LLMs to better serve global audiences with diverse cultural values.

Abstract: As Large Language Models (LLMs) serve a global audience, alignment must transition from enforcing universal consensus to respecting cultural pluralism. We demonstrate that dense models, when forced to fit conflicting value distributions, suffer from \textbf{Mean Collapse}, converging to a generic average that fails to represent diverse groups. We attribute this to \textbf{Cultural Sparsity}, where gradient interference prevents dense parameters from spanning distinct cultural modes. To resolve this, we propose \textbf{\textsc{CuMA}} (\textbf{Cu}ltural \textbf{M}ixture of \textbf{A}dapters), a framework that frames alignment as a \textbf{conditional capacity separation} problem. By incorporating demographic-aware routing, \textsc{CuMA} internalizes a \textit{Latent Cultural Topology} to explicitly disentangle conflicting gradients into specialized expert subspaces. Extensive evaluations on WorldValuesBench, Community Alignment, and PRISM demonstrate that \textsc{CuMA} achieves state-of-the-art performance, significantly outperforming both dense baselines and semantic-only MoEs. Crucially, our analysis confirms that \textsc{CuMA} effectively mitigates mean collapse, preserving cultural diversity. Our code is available at https://github.com/Throll/CuMA.

</details>


### [73] [Faithful Summarisation under Disagreement via Belief-Level Aggregation](https://arxiv.org/abs/2601.04889)
*Favour Yahdii Aghaebe,Tanefa Apekey,Elizabeth Williams,Nafise Sadat Moosavi*

Main category: cs.CL

TL;DR: A new pipeline separates belief aggregation from language generation to better handle conflicting viewpoints in opinion summarization, using distance-based belief merging before LLM realization.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based summarization approaches often smooth over disagreements and over-represent majority opinions, limiting faithfulness in opinion-heavy settings with genuinely conflicting viewpoints.

Method: A disagreement-aware synthesis pipeline that first represents documents as structured belief sets, aggregates them using distance-based belief merging operators to explicitly model conflict, then uses LLMs only to realize the aggregated beliefs as natural language summaries.

Result: While sufficiently large models can match belief-level aggregation when aggregation is handled at generation time, this behavior is not stable across architectures or capacities. Belief-level aggregation with simple prompting yields consistently strong disagreement-aware performance across models while maintaining fluent and grounded summaries.

Conclusion: Separating belief-level aggregation from language generation provides a more stable and effective approach for disagreement-aware summarization compared to methods that perform explicit aggregation during generation.

Abstract: Opinion and multi-document summarisation often involve genuinely conflicting viewpoints, yet many existing approaches, particularly LLM-based systems, implicitly smooth disagreement and over-represent majority opinions. This limits the faithfulness of generated summaries in opinion-heavy settings. We introduce a disagreement-aware synthesis pipeline that separates belief-level aggregation from language generation. Documents are first represented as structured belief sets and aggregated using distance-based belief merging operators that explicitly model conflict. Large language models are then used only to realise the aggregated beliefs as natural language summaries. We evaluate the approach across multiple model families and scales, comparing it to methods that perform explicit aggregation during generation. Our results show that while sufficiently large models can match belief-level aggregation when aggregation is handled at generation time, this behaviour is not stable across architectures or capacities. In contrast, belief-level aggregation combined with simple prompting yields consistently strong disagreement-aware performance across models, while maintaining fluent and grounded summaries.

</details>


### [74] [V-FAT: Benchmarking Visual Fidelity Against Text-bias](https://arxiv.org/abs/2601.04897)
*Ziteng Wang,Yujie He,Guanliang Li,Siqi Yang,Jiaqi Xiong,Songxiang Liu*

Main category: cs.CL

TL;DR: The paper introduces V-FAT, a diagnostic benchmark to measure Text Bias in MLLMs - their tendency to rely on linguistic shortcuts rather than genuine visual grounding, revealing significant visual collapse under high linguistic dominance.


<details>
  <summary>Details</summary>
Motivation: There's growing concern that Multimodal Large Language Models (MLLMs) rely excessively on linguistic shortcuts rather than genuine visual grounding (Text Bias), creating a need to investigate the tension between visual perception and linguistic priors.

Method: The authors decouple Text Bias into Internal Corpus Bias (from pretraining correlations) and External Instruction Bias (from alignment-induced sycophancy). They introduce V-FAT benchmark with 4,026 VQA instances across six semantic domains, using a Three-Level Evaluation Framework that systematically increases conflict between visual evidence and textual information.

Result: Evaluation of 12 frontier MLLMs reveals that while models excel in existing benchmarks, they experience significant visual collapse under high linguistic dominance, demonstrating their vulnerability to Text Bias.

Conclusion: Current MLLMs have serious limitations in visual grounding despite strong performance on standard benchmarks, highlighting the need for better evaluation methods like V-FAT and the Visual Robustness Score to measure true visual fidelity.

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated impressive performance on standard visual reasoning benchmarks. However, there is growing concern that these models rely excessively on linguistic shortcuts rather than genuine visual grounding, a phenomenon we term Text Bias. In this paper, we investigate the fundamental tension between visual perception and linguistic priors. We decouple the sources of this bias into two dimensions: Internal Corpus Bias, stemming from statistical correlations in pretraining, and External Instruction Bias, arising from the alignment-induced tendency toward sycophancy. To quantify this effect, we introduce V-FAT (Visual Fidelity Against Text-bias), a diagnostic benchmark comprising 4,026 VQA instances across six semantic domains. V-FAT employs a Three-Level Evaluation Framework that systematically increases the conflict between visual evidence and textual information: (L1) internal bias from atypical images, (L2) external bias from misleading instructions, and (L3) synergistic bias where both coincide. We introduce the Visual Robustness Score (VRS), a metric designed to penalize "lucky" linguistic guesses and reward true visual fidelity. Our evaluation of 12 frontier MLLMs reveals that while models excel in existing benchmarks, they experience significant visual collapse under high linguistic dominance.

</details>


### [75] [Can AI-Generated Persuasion Be Detected? Persuaficial Benchmark and AI vs. Human Linguistic Differences](https://arxiv.org/abs/2601.04925)
*Arkadiusz Modzelewski,Paweł Golik,Anna Kołos,Giovanni Da San Martino*

Main category: cs.CL

TL;DR: LLM-generated persuasive texts are harder to detect than human-written ones when subtle, despite overt persuasion being easier to spot. The paper introduces Persuaficial benchmark for multilingual evaluation and provides linguistic analysis for better detection tools.


<details>
  <summary>Details</summary>
Motivation: LLMs can generate highly persuasive text, raising concerns about misuse for propaganda and manipulation. The central question is whether LLM-generated persuasion is more difficult to automatically detect than human-written persuasion.

Method: Categorize controllable generation approaches for producing persuasive content with LLMs. Introduce Persuaficial, a high-quality multilingual benchmark covering six languages (English, German, Polish, Italian, French, Russian). Conduct extensive empirical evaluations comparing human-authored and LLM-generated persuasive texts.

Result: Overtly persuasive LLM-generated texts are easier to detect than human-written ones, but subtle LLM-generated persuasion consistently degrades automatic detection performance. The paper provides the first comprehensive linguistic analysis contrasting human and LLM-generated persuasive texts.

Conclusion: Subtle LLM-generated persuasion poses significant detection challenges, requiring more sophisticated tools. The linguistic analysis offers insights to guide development of more interpretable and robust detection methods against LLM-generated persuasive content.

Abstract: Large Language Models (LLMs) can generate highly persuasive text, raising concerns about their misuse for propaganda, manipulation, and other harmful purposes. This leads us to our central question: Is LLM-generated persuasion more difficult to automatically detect than human-written persuasion? To address this, we categorize controllable generation approaches for producing persuasive content with LLMs and introduce Persuaficial, a high-quality multilingual benchmark covering six languages: English, German, Polish, Italian, French and Russian. Using this benchmark, we conduct extensive empirical evaluations comparing human-authored and LLM-generated persuasive texts. We find that although overtly persuasive LLM-generated texts can be easier to detect than human-written ones, subtle LLM-generated persuasion consistently degrades automatic detection performance. Beyond detection performance, we provide the first comprehensive linguistic analysis contrasting human and LLM-generated persuasive texts, offering insights that may guide the development of more interpretable and robust detection tools.

</details>


### [76] [GenProve: Learning to Generate Text with Fine-Grained Provenance](https://arxiv.org/abs/2601.04932)
*Jingxuan Wei,Xingyue Wang,Yanghaoyu Liao,Jie Dong,Yuchen Liu,Caijun Jia,Bihui Yu,Junnan Zhu*

Main category: cs.CL

TL;DR: ReFInE dataset enables fine-grained provenance tracking in LLM outputs, distinguishing quotes, compression, and inference. GenProve framework with SFT+GRPO outperforms 14 LLMs in joint answer-provenance evaluation, revealing models struggle with inference-based provenance.


<details>
  <summary>Details</summary>
Motivation: Current citation methods in LLMs are insufficient for accountability - users can't verify how cited sources support generated claims. Existing approaches are coarse-grained and fail to distinguish between direct quotes and complex reasoning.

Method: Introduce Generation-time Fine-grained Provenance task requiring models to generate answers with structured sentence-level provenance triples. Create ReFInE dataset with expert annotations distinguishing Quotation, Compression, and Inference. Propose GenProve framework combining Supervised Fine-Tuning with Group Relative Policy Optimization, optimizing composite reward for answer fidelity and provenance correctness.

Result: GenProve significantly outperforms 14 strong LLMs in joint evaluation. Analysis reveals reasoning gap: models excel at surface-level quotation but struggle significantly with inference-based provenance, showing verifiable reasoning remains a distinct frontier challenge.

Conclusion: Fine-grained provenance tracking is crucial for LLM accountability. The ReFInE dataset and GenProve framework advance verifiable AI, but inference-based provenance remains challenging, highlighting that verifiable reasoning is separate from surface-level citation.

Abstract: Large language models (LLM) often hallucinate, and while adding citations is a common solution, it is frequently insufficient for accountability as users struggle to verify how a cited source supports a generated claim. Existing methods are typically coarse-grained and fail to distinguish between direct quotes and complex reasoning. In this paper, we introduce Generation-time Fine-grained Provenance, a task where models must generate fluent answers while simultaneously producing structured, sentence-level provenance triples. To enable this, we present ReFInE (Relation-aware Fine-grained Interpretability & Evidence), a dataset featuring expert verified annotations that distinguish between Quotation, Compression, and Inference. Building on ReFInE, we propose GenProve, a framework that combines Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization (GRPO). By optimizing a composite reward for answer fidelity and provenance correctness, GenProve significantly outperforms 14 strong LLMs in joint evaluation. Crucially, our analysis uncovers a reasoning gap where models excel at surface-level quotation but struggle significantly with inference-based provenance, suggesting that verifiable reasoning remains a frontier challenge distinct from surface-level citation.

</details>


### [77] [A Unified Spoken Language Model with Injected Emotional-Attribution Thinking for Human-like Interaction](https://arxiv.org/abs/2601.04960)
*Qing Wang,Zehan Li,Yaodong Song,Hongjie Chen,Jian Kang,Jie Lian,Jie Li,Yongxiang Li,Xuelong Li*

Main category: cs.CL

TL;DR: A unified spoken language model with emotional intelligence using IEAT data construction and two-stage training achieves top performance on emotional benchmarks.


<details>
  <summary>Details</summary>
Motivation: To create emotionally intelligent spoken language models that can understand and respond to user emotions by internalizing emotional reasoning rather than treating it as explicit supervision.

Method: Uses Injected Emotional-Attribution Thinking (IEAT) data construction to incorporate user emotional states and causes into reasoning. Two-stage training: 1) speech-text alignment and emotional attribute modeling via self-distillation, 2) end-to-end cross-modal joint optimization for consistency between textual and spoken emotional expressions.

Result: Achieves top-ranked performance on the HumDial Emotional Intelligence benchmark across emotional trajectory modeling, emotional reasoning, and empathetic response generation under both LLM-based and human evaluations.

Conclusion: The IEAT approach enables effective emotional intelligence in spoken language models by internalizing emotion-aware reasoning through novel data construction and progressive training strategies.

Abstract: This paper presents a unified spoken language model for emotional intelligence, enhanced by a novel data construction strategy termed Injected Emotional-Attribution Thinking (IEAT). IEAT incorporates user emotional states and their underlying causes into the model's internal reasoning process, enabling emotion-aware reasoning to be internalized rather than treated as explicit supervision. The model is trained with a two-stage progressive strategy. The first stage performs speech-text alignment and emotional attribute modeling via self-distillation, while the second stage conducts end-to-end cross-modal joint optimization to ensure consistency between textual and spoken emotional expressions. Experiments on the Human-like Spoken Dialogue Systems Challenge (HumDial) Emotional Intelligence benchmark demonstrate that the proposed approach achieves top-ranked performance across emotional trajectory modeling, emotional reasoning, and empathetic response generation under both LLM-based and human evaluations.

</details>


### [78] [Text as a Universal Interface for Transferable Personalization](https://arxiv.org/abs/2601.04963)
*Yuting Liu,Jian Guan,Jia-Nan Li,Wei Wu,Jiang-Ming Yang,Jianzhe Zhao,Guibing Guo*

Main category: cs.CL

TL;DR: The paper proposes using natural language as a universal interface for representing user preferences in LLMs, creating interpretable and transferable preference descriptions through a two-stage training framework called AlignXplore+.


<details>
  <summary>Details</summary>
Motivation: Current approaches represent user preferences as opaque, model-specific vectors that are difficult to interpret and transfer across different models and tasks. The authors want to create more transparent, reusable preference representations.

Method: A two-stage training framework combining supervised fine-tuning on synthesized data with reinforcement learning to optimize long-term utility and cross-task transferability. This framework produces AlignXplore+, a model that generates textual preference summaries.

Result: The 8B AlignXplore+ model achieves state-of-the-art performance on nine benchmarks, outperforming substantially larger open-source models, while showing strong transferability across tasks, model families, and interaction formats.

Conclusion: Natural language serves as an effective universal interface for preference representation, enabling interpretable, reusable, and continually evolving preference descriptions that transfer well across different models and tasks.

Abstract: We study the problem of personalization in large language models (LLMs). Prior work predominantly represents user preferences as implicit, model-specific vectors or parameters, yielding opaque ``black-box'' profiles that are difficult to interpret and transfer across models and tasks. In contrast, we advocate natural language as a universal, model- and task-agnostic interface for preference representation. The formulation leads to interpretable and reusable preference descriptions, while naturally supporting continual evolution as new interactions are observed. To learn such representations, we introduce a two-stage training framework that combines supervised fine-tuning on high-quality synthesized data with reinforcement learning to optimize long-term utility and cross-task transferability. Based on this framework, we develop AlignXplore+, a universal preference reasoning model that generates textual preference summaries. Experiments on nine benchmarks show that our 8B model achieves state-of-the-art performanc -- outperforming substantially larger open-source models -- while exhibiting strong transferability across tasks, model families, and interaction formats.

</details>


### [79] [Learning from Mistakes: Negative Reasoning Samples Enhance Out-of-Domain Generalization](https://arxiv.org/abs/2601.04992)
*Xueyun Tian,Minghua Ma,Bingbing Xu,Nuoyan Lyu,Wei Li,Heng Dong,Zheng Chu,Yuanzhuo Wang,Huawei Shen*

Main category: cs.CL

TL;DR: Incorporating negative reasoning trajectories (with wrong final answers but valid intermediate steps) into supervised fine-tuning improves out-of-domain generalization over positive-only training, with a proposed adaptive loss weighting method (GLOW) further boosting performance.


<details>
  <summary>Details</summary>
Motivation: Standard SFT on CoT trajectories only uses positive examples (correct final answers), discarding negative trajectories. This wastes supervision and causes overfitting, limiting OOD generalization. The paper argues negative trajectories contain valuable intermediate reasoning despite wrong answers.

Method: 1) Incorporate negative CoT trajectories into SFT training; 2) Analyze 22 recurring patterns in negative chains; 3) Propose GLOW (Gain-based LOss Weighting) - an adaptive loss weighting scheme that rescales per-sample loss based on inter-epoch progress to exploit distinctive training dynamics.

Result: Negative trajectories in SFT yield substantial OOD generalization gains over positive-only training. GLOW provides 5.51% OOD gain on Qwen2.5-7B and boosts MMLU from 72.82% to 76.47% as RL initialization. Negative trajectories moderate loss descent (reduce overfitting) and boost policy entropy by 35.67% during inference.

Conclusion: Negative reasoning trajectories are valuable training resources that improve generalization when properly incorporated. The proposed GLOW method efficiently leverages unfiltered trajectories by adapting to training dynamics, offering a simple yet effective approach to enhance reasoning capabilities in LLMs.

Abstract: Supervised fine-tuning (SFT) on chain-of-thought (CoT) trajectories demonstrations is a common approach for enabling reasoning in large language models. Standard practices typically only retain trajectories with correct final answers (positives) while ignoring the rest (negatives). We argue that this paradigm discards substantial supervision and exacerbates overfitting, limiting out-of-domain (OOD) generalization. Specifically, we surprisingly find that incorporating negative trajectories into SFT yields substantial OOD generalization gains over positive-only training, as these trajectories often retain valid intermediate reasoning despite incorrect final answers. To understand this effect in depth, we systematically analyze data, training dynamics, and inference behavior, identifying 22 recurring patterns in negative chains that serve a dual role: they moderate loss descent to mitigate overfitting during training and boost policy entropy by 35.67% during inference to facilitate exploration. Motivated by these observations, we further propose Gain-based LOss Weighting (GLOW), an adaptive, sample-aware scheme that exploits such distinctive training dynamics by rescaling per-sample loss based on inter-epoch progress. Empirically, GLOW efficiently leverages unfiltered trajectories, yielding a 5.51% OOD gain over positive-only SFT on Qwen2.5-7B and boosting MMLU from 72.82% to 76.47% as an RL initialization.

</details>


### [80] [Can Large Language Models Resolve Semantic Discrepancy in Self-Destructive Subcultures? Evidence from Jirai Kei](https://arxiv.org/abs/2601.05004)
*Peng Wang,Xilin Tao,Siyi Yao,Jiageng Wu,Yuntao Zou,Zhuotao Tian,Libo Qin,Dagang Li*

Main category: cs.CL

TL;DR: SAS is a multi-agent framework that improves LLM-based detection of self-destructive behaviors in subcultures by addressing knowledge lag and semantic misalignment challenges.


<details>
  <summary>Details</summary>
Motivation: Self-destructive behaviors are hard to diagnose, especially in subcultural groups with unique expressions. While LLMs are being explored for detection, they face challenges with rapidly evolving subcultural slang and semantic nuances specific to these groups.

Method: Proposed Subcultural Alignment Solver (SAS), a multi-agent framework incorporating automatic retrieval and subculture alignment to enhance LLM performance in detecting self-destructive behaviors in subcultural contexts.

Result: SAS outperforms current advanced multi-agent framework OWL and competes well with fine-tuned LLMs in detecting self-destructive behaviors within subcultures.

Conclusion: SAS advances self-destructive behavior detection in subcultural contexts and serves as a valuable resource for future research in this challenging domain.

Abstract: Self-destructive behaviors are linked to complex psychological states and can be challenging to diagnose. These behaviors may be even harder to identify within subcultural groups due to their unique expressions. As large language models (LLMs) are applied across various fields, some researchers have begun exploring their application for detecting self-destructive behaviors. Motivated by this, we investigate self-destructive behavior detection within subcultures using current LLM-based methods. However, these methods have two main challenges: (1) Knowledge Lag: Subcultural slang evolves rapidly, faster than LLMs' training cycles; and (2) Semantic Misalignment: it is challenging to grasp the specific and nuanced expressions unique to subcultures. To address these issues, we proposed Subcultural Alignment Solver (SAS), a multi-agent framework that incorporates automatic retrieval and subculture alignment, significantly enhancing the performance of LLMs in detecting self-destructive behavior. Our experimental results show that SAS outperforms the current advanced multi-agent framework OWL. Notably, it competes well with fine-tuned LLMs. We hope that SAS will advance the field of self-destructive behavior detection in subcultural contexts and serve as a valuable resource for future researchers.

</details>


### [81] [Hán Dān Xué Bù (Mimicry) or Qīng Chū Yú Lán (Mastery)? A Cognitive Perspective on Reasoning Distillation in Large Language Models](https://arxiv.org/abs/2601.05019)
*Yueqing Hu,Xinyang Peng,Shuting Peng,Hanqi Wang,Tianhong Wang*

Main category: cs.CL

TL;DR: Reasoning distillation via SFT fails to transfer cognitive structure from teacher models, causing functional alignment collapse where students mimic linguistic form without internalizing resource allocation policies.


<details>
  <summary>Details</summary>
Motivation: Large Reasoning Models trained via RL show natural alignment with human cognitive costs, but current reasoning distillation methods may not preserve this valuable cognitive structure.

Method: Tested the "Hán Dān Xué Bù" (Superficial Mimicry) hypothesis across 14 models, comparing teacher models' alignment with human difficulty scaling against distilled students' performance.

Result: Teacher models show strong alignment with human difficulty scaling (r̄=0.64), but distilled students significantly degrade this alignment (r̄=0.34), often underperforming pre-distillation baselines (negative transfer).

Conclusion: Human-like cognition emerges from active reinforcement learning, not passive imitation; reasoning distillation via SFT creates "cargo cult" effects where students replicate linguistic form without internalizing cognitive resource allocation.

Abstract: Recent Large Reasoning Models trained via reinforcement learning exhibit a "natural" alignment with human cognitive costs. However, we show that the prevailing paradigm of reasoning distillation -- training student models to mimic these traces via Supervised Fine-Tuning (SFT) -- fails to transmit this cognitive structure. Testing the "Hán Dān Xué Bù" (Superficial Mimicry) hypothesis across 14 models, we find that distillation induces a "Functional Alignment Collapse": while teacher models mirror human difficulty scaling ($\bar{r}=0.64$), distilled students significantly degrade this alignment ($\bar{r}=0.34$), often underperforming their own pre-distillation baselines ("Negative Transfer"). Our analysis suggests that SFT induces a "Cargo Cult" effect, where students ritualistically replicate the linguistic form of reasoning (verbosity) without internalizing the teacher's dynamic resource allocation policy. Consequently, reasoning distillation decouples computational cost from cognitive demand, revealing that human-like cognition is an emergent property of active reinforcement, not passive imitation.

</details>


### [82] [ArcAligner: Adaptive Recursive Aligner for Compressed Context Embeddings in RAG](https://arxiv.org/abs/2601.05038)
*Jianbo Li,Yi Jiang,Sendong Zhao,Bairui Hu,Haochun Wang,Bing Qin*

Main category: cs.CL

TL;DR: ArcAligner improves RAG by adding a lightweight adaptive gating module to help LLMs better understand highly compressed context, maintaining speed while improving accuracy on complex queries.


<details>
  <summary>Details</summary>
Motivation: RAG helps LLMs stay accurate but feeding long documents into prompts makes models slow and expensive. While context compression techniques exist, they create a trade-off: more compression leads to worse LLM understanding of the compressed data.

Method: ArcAligner (Adaptive recursive context Aligner) is a lightweight module integrated into language model layers. It uses an adaptive gating system that only adds extra processing power when information is complex, helping models better utilize highly compressed context representations for downstream generation.

Result: ArcAligner consistently beats compression baselines at comparable compression rates across knowledge-intensive QA benchmarks, especially on multi-hop and long-tail settings.

Conclusion: ArcAligner effectively addresses the compression-accuracy trade-off in RAG systems by providing adaptive processing that improves LLM understanding of compressed context while maintaining efficiency.

Abstract: Retrieval-Augmented Generation (RAG) helps LLMs stay accurate, but feeding long documents into a prompt makes the model slow and expensive. This has motivated context compression, ranging from token pruning and summarization to embedding-based compression. While researchers have tried ''compressing'' these documents into smaller summaries or mathematical embeddings, there is a catch: the more you compress the data, the more the LLM struggles to understand it. To address this challenge, we propose ArcAligner (Adaptive recursive context *Aligner*), a lightweight module integrated into the language model layers to help the model better utilize highly compressed context representations for downstream generation. It uses an adaptive ''gating'' system that only adds extra processing power when the information is complex, keeping the system fast. Across knowledge-intensive QA benchmarks, ArcAligner consistently beats compression baselines at comparable compression rates, especially on multi-hop and long-tail settings. The source code is publicly available.

</details>


### [83] [Compositional Steering of Large Language Models with Steering Tokens](https://arxiv.org/abs/2601.05062)
*Gorjan Radevski,Kiril Gashteovski,Giwon Hong,Carolin Lawrence,Goran Glavaš*

Main category: cs.CL

TL;DR: Proposes compositional steering tokens for multi-behavior LLM control via self-distillation and composition tokens that generalize to unseen behavior combinations.


<details>
  <summary>Details</summary>
Motivation: Real-world LLM deployment requires controllable output satisfying multiple desiderata simultaneously. While single-behavior steering is well-studied, compositional steering (steering LLMs toward multiple behaviors at once) remains underexplored.

Method: Embed individual behaviors (natural language instructions) into dedicated tokens via self-distillation. Train a dedicated composition token on behavior pairs that captures composition notion. Operates in input token space rather than activation space for better zero-shot composition.

Result: Composition tokens generalize well to unseen compositions (including unseen behaviors and unseen number of behaviors). Steering tokens outperform competing approaches (instructions, activation steering, LoRA merging) across different LLM architectures. Combining steering tokens with natural language instructions yields further gains.

Conclusion: Compositional steering tokens provide effective multi-behavior control for LLMs, enabling zero-shot composition and complementing existing instruction-based methods.

Abstract: Deploying LLMs in real-world applications requires controllable output that satisfies multiple desiderata at the same time. While existing work extensively addresses LLM steering for a single behavior, \textit{compositional steering} -- i.e., steering LLMs simultaneously towards multiple behaviors -- remains an underexplored problem. In this work, we propose \emph{compositional steering tokens} for multi-behavior steering. We first embed individual behaviors, expressed as natural language instructions, into dedicated tokens via self-distillation. Contrary to most prior work, which operates in the activation space, our behavior steers live in the space of input tokens, enabling more effective zero-shot composition. We then train a dedicated \textit{composition token} on pairs of behaviors and show that it successfully captures the notion of composition: it generalizes well to \textit{unseen} compositions, including those with unseen behaviors as well as those with an unseen \textit{number} of behaviors. Our experiments across different LLM architectures show that steering tokens lead to superior multi-behavior control compared to competing approaches (instructions, activation steering, and LoRA merging). Moreover, we show that steering tokens complement natural language instructions, with their combination resulting in further gains.

</details>


### [84] [SemPA: Improving Sentence Embeddings of Large Language Models through Semantic Preference Alignment](https://arxiv.org/abs/2601.05075)
*Ziyang Chen,Zhenxuan Huang,Yile Wang,Weiqin Wang,Lu Yin,Hui Huang*

Main category: cs.CL

TL;DR: SemPA is a novel method that improves sentence embeddings in LLMs while preserving their generative capabilities through semantic preference alignment using DPO on paraphrase generation tasks.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based embedding methods either use fixed prompts (limited performance) or modify model architecture (compromises generative ability). There's a need for methods that enhance sentence representations without sacrificing LLMs' generative capabilities.

Method: SemPA uses sentence-level Direct Preference Optimization (DPO) to optimize LLMs on paraphrase generation tasks. The model learns to discriminate semantically equivalent sentences while preserving inherent generative capacity, establishing a theoretical connection between DPO and contrastive learning under the Plackett-Luce model framework.

Result: Experimental results on semantic textual similarity tasks and various LLM benchmarks show that SemPA achieves better semantic representations without sacrificing the inherent generation capability of LLMs.

Conclusion: SemPA provides an effective approach to boost sentence representations in LLMs while preserving their generative abilities through semantic preference alignment, offering a balanced solution to the limitations of existing methods.

Abstract: Traditional sentence embedding methods employ token-level contrastive learning on non-generative pre-trained models. Recently, there have emerged embedding methods based on generative large language models (LLMs). These methods either rely on fixed prompt templates or involve modifications to the model architecture. The former lacks further optimization of the model and results in limited performance, while the latter alters the internal computational mechanisms of the model, thereby compromising its generative capabilities. We propose SemPA, a novel approach that boosts the sentence representations while preserving the generative ability of LLMs via semantic preference alignment. We leverage sentence-level Direct Preference Optimization (DPO) to efficiently optimize LLMs on a paraphrase generation task, where the model learns to discriminate semantically equivalent sentences while preserving inherent generative capacity. Theoretically, we establish a formal connection between DPO and contrastive learning under the Plackett-Luce model framework. Empirically, experimental results on both semantic textual similarity tasks and various benchmarks for LLMs show that SemPA achieves better semantic representations without sacrificing the inherent generation capability of LLMs.

</details>


### [85] [Code-Mix Sentiment Analysis on Hinglish Tweets](https://arxiv.org/abs/2601.05091)
*Aashi Garg,Aneshya Das,Arshi Arya,Anushka Goyal,Aditi*

Main category: cs.CL

TL;DR: The paper proposes a mBERT-based sentiment classification framework for Hinglish tweets to address the limitations of traditional monolingual NLP models in analyzing code-mixed Indian social media content.


<details>
  <summary>Details</summary>
Motivation: Traditional NLP models fail to interpret the syntactic and semantic complexity of Hinglish (Hindi-English code-mixing) used widely in Indian social media, leading to inaccurate sentiment analysis and misleading market insights for brand monitoring.

Method: Fine-tunes mBERT (Multilingual BERT) with subword tokenization to handle spelling variations, slang, and out-of-vocabulary terms in Romanized Hinglish tweets.

Result: Develops a production-ready AI solution for brand sentiment tracking and establishes a benchmark for multilingual NLP in low-resource, code-mixed environments.

Conclusion: The proposed framework effectively addresses the challenges of analyzing Hinglish content on social media platforms like Twitter, providing accurate sentiment analysis for brand monitoring in India.

Abstract: The effectiveness of brand monitoring in India is increasingly challenged by the rise of Hinglish--a hybrid of Hindi and English--used widely in user-generated content on platforms like Twitter. Traditional Natural Language Processing (NLP) models, built for monolingual data, often fail to interpret the syntactic and semantic complexity of this code-mixed language, resulting in inaccurate sentiment analysis and misleading market insights. To address this gap, we propose a high-performance sentiment classification framework specifically designed for Hinglish tweets. Our approach fine-tunes mBERT (Multilingual BERT), leveraging its multilingual capabilities to better understand the linguistic diversity of Indian social media. A key component of our methodology is the use of subword tokenization, which enables the model to effectively manage spelling variations, slang, and out-of-vocabulary terms common in Romanized Hinglish. This research delivers a production-ready AI solution for brand sentiment tracking and establishes a strong benchmark for multilingual NLP in low-resource, code-mixed environments.

</details>


### [86] [How Human is AI? Examining the Impact of Emotional Prompts on Artificial and Human and Responsiveness](https://arxiv.org/abs/2601.05104)
*Florence Bernays,Marco Henriques Pereira,Jochen Menges*

Main category: cs.CL

TL;DR: Emotional tone in human-AI interactions affects both ChatGPT's responses and subsequent human-human communication, with praise being most effective for improving AI outputs and blame negatively impacting human interactions.


<details>
  <summary>Details</summary>
Motivation: To understand how emotional expressions in human-AI interactions influence both AI behavior (specifically ChatGPT's responses) and subsequent human-human communication patterns.

Method: Between-subject experiment where participants expressed specific emotions while working with ChatGPT (GPT-4.0) on two tasks: writing a public response and addressing an ethical dilemma. Conditions included neutral tone, praise, anger, and blame.

Result: Praise led to greatest improvement in ChatGPT's answers; anger showed smaller improvement; blame showed no improvement. For ethical dilemmas, anger reduced ChatGPT's prioritization of corporate interests, while blame increased emphasis on protecting public interest. Participants used more negative expressions in subsequent human-human communication after blaming interactions.

Conclusion: Emotional tone in human-AI interactions significantly shapes both AI outputs and carries over to affect subsequent human-human communication, highlighting the importance of emotional dynamics in AI interactions.

Abstract: This research examines how the emotional tone of human-AI interactions shapes ChatGPT and human behavior. In a between-subject experiment, we asked participants to express a specific emotion while working with ChatGPT (GPT-4.0) on two tasks, including writing a public response and addressing an ethical dilemma. We found that compared to interactions where participants maintained a neutral tone, ChatGPT showed greater improvement in its answers when participants praised ChatGPT for its responses. Expressing anger towards ChatGPT also led to a higher albeit smaller improvement relative to the neutral condition, whereas blaming ChatGPT did not improve its answers. When addressing an ethical dilemma, ChatGPT prioritized corporate interests less when participants expressed anger towards it, while blaming increases its emphasis on protecting the public interest. Additionally, we found that people used more negative, hostile, and disappointing expressions in human-human communication after interactions during which participants blamed rather than praised for their responses. Together, our findings demonstrate that the emotional tone people apply in human-AI interactions not only shape ChatGPT's outputs but also carry over into subsequent human-human communication.

</details>


### [87] [Agent-as-a-Judge](https://arxiv.org/abs/2601.05111)
*Runyang You,Hongru Cai,Caiqi Zhang,Qiancheng Xu,Meng Liu,Tiezheng Yu,Yongqi Li,Wenjie Li*

Main category: cs.CL

TL;DR: Survey paper tracing the evolution from LLM-as-a-Judge to Agent-as-a-Judge, establishing a taxonomy and roadmap for agentic evaluation systems.


<details>
  <summary>Details</summary>
Motivation: LLM-as-a-Judge has limitations for evaluating complex, specialized, multi-step systems due to biases, shallow reasoning, and inability to verify against real-world observations. The field lacks a unified framework for the emerging Agent-as-a-Judge paradigm.

Method: Comprehensive survey identifying key dimensions of the paradigm shift, establishing developmental taxonomy, organizing core methodologies, and surveying applications across general and professional domains.

Result: First comprehensive survey tracing the evolution from LLM-as-a-Judge to Agent-as-a-Judge, providing a unified framework to navigate the shifting landscape of agentic evaluation systems.

Conclusion: The paper provides a clear roadmap for next-generation agentic evaluation by analyzing frontier challenges and identifying promising research directions, bridging the gap in the rapidly proliferating field of agentic evaluation.

Abstract: LLM-as-a-Judge has revolutionized AI evaluation by leveraging large language models for scalable assessments. However, as evaluands become increasingly complex, specialized, and multi-step, the reliability of LLM-as-a-Judge has become constrained by inherent biases, shallow single-pass reasoning, and the inability to verify assessments against real-world observations. This has catalyzed the transition to Agent-as-a-Judge, where agentic judges employ planning, tool-augmented verification, multi-agent collaboration, and persistent memory to enable more robust, verifiable, and nuanced evaluations. Despite the rapid proliferation of agentic evaluation systems, the field lacks a unified framework to navigate this shifting landscape. To bridge this gap, we present the first comprehensive survey tracing this evolution. Specifically, we identify key dimensions that characterize this paradigm shift and establish a developmental taxonomy. We organize core methodologies and survey applications across general and professional domains. Furthermore, we analyze frontier challenges and identify promising research directions, ultimately providing a clear roadmap for the next generation of agentic evaluation.

</details>


### [88] [DocDancer: Towards Agentic Document-Grounded Information Seeking](https://arxiv.org/abs/2601.05163)
*Qintong Zhang,Xinjie Lv,Jialong Wu,Baixuan Li,Zhengwei Tao,Guochen Yan,Huanyao Zhang,Bin Wang,Jiahao Xu,Haitao Mi,Wentao Zhang*

Main category: cs.CL

TL;DR: DocDancer is an end-to-end open-source document QA agent with tool-driven framework for document exploration and comprehension, trained on synthetic data from Exploration-then-Synthesis pipeline.


<details>
  <summary>Details</summary>
Motivation: Existing DocQA agents lack effective tool utilization and rely heavily on closed-source models, creating a need for open-source solutions with better document exploration capabilities.

Method: Formulates DocQA as information-seeking problem with tool-driven agent framework for document exploration and comprehension. Uses Exploration-then-Synthesis data synthesis pipeline to generate training data. Trains end-to-end on synthesized data.

Result: Trained models show effectiveness on two long-context document understanding benchmarks: MMLongBench-Doc and DocBench. Analysis provides insights for agentic tool design and synthetic data.

Conclusion: DocDancer demonstrates successful end-to-end training of open-source DocQA agents with tool-driven framework, addressing data scarcity through synthetic data generation and showing strong performance on benchmarks.

Abstract: Document Question Answering (DocQA) focuses on answering questions grounded in given documents, yet existing DocQA agents lack effective tool utilization and largely rely on closed-source models. In this work, we introduce DocDancer, an end-to-end trained open-source Doc agent. We formulate DocQA as an information-seeking problem and propose a tool-driven agent framework that explicitly models document exploration and comprehension. To enable end-to-end training of such agents, we introduce an Exploration-then-Synthesis data synthesis pipeline that addresses the scarcity of high-quality training data for DocQA. Training on the synthesized data, the trained models on two long-context document understanding benchmarks, MMLongBench-Doc and DocBench, show their effectiveness. Further analysis provides valuable insights for the agentic tool design and synthetic data.

</details>


### [89] [RelayLLM: Efficient Reasoning via Collaborative Decoding](https://arxiv.org/abs/2601.05167)
*Chengsong Huang,Tong Zheng,Langlin Huang,Jinyuan Li,Haolin Liu,Jiaxin Huang*

Main category: cs.CL

TL;DR: RelayLLM enables efficient collaborative reasoning between SLMs and LLMs by allowing SLMs to dynamically invoke LLMs only for critical tokens via token-level control, achieving near-LLM performance with minimal LLM usage.


<details>
  <summary>Details</summary>
Motivation: Current collaborative approaches between SLMs and LLMs are inefficient - they operate at coarse granularity by offloading entire queries to LLMs, causing computational waste when SLMs could handle most reasoning steps. There's a need for more fine-grained collaboration that leverages SLMs' capabilities while strategically using LLMs only when necessary.

Method: RelayLLM introduces token-level collaborative decoding where the SLM acts as an active controller that can dynamically invoke the LLM via special commands for critical tokens. The framework uses two-stage training: warm-up followed by Group Relative Policy Optimization (GRPO) to teach the model to balance independent reasoning with strategic help-seeking.

Result: Across six benchmarks, RelayLLM achieves average accuracy of 49.52%, effectively bridging the performance gap between SLMs and LLMs. This is achieved by invoking the LLM for only 1.07% of total generated tokens, offering 98.2% cost reduction compared to performance-matched random routers.

Conclusion: RelayLLM demonstrates that fine-grained, token-level collaboration between SLMs and LLMs can achieve near-LLM performance with dramatically reduced computational costs, providing an efficient alternative to coarse-grained routing approaches.

Abstract: Large Language Models (LLMs) for complex reasoning is often hindered by high computational costs and latency, while resource-efficient Small Language Models (SLMs) typically lack the necessary reasoning capacity. Existing collaborative approaches, such as cascading or routing, operate at a coarse granularity by offloading entire queries to LLMs, resulting in significant computational waste when the SLM is capable of handling the majority of reasoning steps. To address this, we propose RelayLLM, a novel framework for efficient reasoning via token-level collaborative decoding. Unlike routers, RelayLLM empowers the SLM to act as an active controller that dynamically invokes the LLM only for critical tokens via a special command, effectively "relaying" the generation process. We introduce a two-stage training framework, including warm-up and Group Relative Policy Optimization (GRPO) to teach the model to balance independence with strategic help-seeking. Empirical results across six benchmarks demonstrate that RelayLLM achieves an average accuracy of 49.52%, effectively bridging the performance gap between the two models. Notably, this is achieved by invoking the LLM for only 1.07% of the total generated tokens, offering a 98.2% cost reduction compared to performance-matched random routers.

</details>


### [90] [Reverse-engineering NLI: A study of the meta-inferential properties of Natural Language Inference](https://arxiv.org/abs/2601.05170)
*Rasmus Blanck,Bill Noble,Stergios Chatzikyriakidis*

Main category: cs.CL

TL;DR: This paper analyzes the logical properties of Natural Language Inference (NLI) tasks, examining three possible interpretations of NLI labels and evaluating model consistency on meta-inferential properties using SNLI dataset analysis.


<details>
  <summary>Details</summary>
Motivation: The logical properties of NLI tasks are poorly understood and often mischaracterized, making it difficult to properly interpret model performance. Understanding what type of inference NLI actually captures is crucial for meaningful evaluation of language models' natural language understanding capabilities.

Method: The authors formulate three possible readings of NLI label sets and analyze their meta-inferential properties. They use two approaches: (1) analyzing NLI items with shared premises in the SNLI dataset, and (2) generating items using LLMs to evaluate models trained on SNLI for meta-inferential consistency.

Result: The analysis provides insights into which logical relation reading is actually encoded by the SNLI dataset, revealing how models trained on SNLI handle meta-inferential consistency and which interpretation of NLI labels aligns with the dataset's structure.

Conclusion: By systematically analyzing the logical properties of NLI through meta-inferential consistency testing, the paper provides a clearer understanding of what type of inference NLI tasks actually measure, which is essential for properly interpreting language model performance on natural language understanding benchmarks.

Abstract: Natural Language Inference (NLI) has been an important task for evaluating language models for Natural Language Understanding, but the logical properties of the task are poorly understood and often mischaracterized. Understanding the notion of inference captured by NLI is key to interpreting model performance on the task. In this paper we formulate three possible readings of the NLI label set and perform a comprehensive analysis of the meta-inferential properties they entail. Focusing on the SNLI dataset, we exploit (1) NLI items with shared premises and (2) items generated by LLMs to evaluate models trained on SNLI for meta-inferential consistency and derive insights into which reading of the logical relations is encoded by the dataset.

</details>


### [91] [Inside Out: Evolving User-Centric Core Memory Trees for Long-Term Personalized Dialogue Systems](https://arxiv.org/abs/2601.05171)
*Jihao Zhao,Ding Chen,Zhaoxin Fan,Kerun Xu,Mengting Hu,Bo Tang,Feiyu Xiong,Zhiyu li*

Main category: cs.CL

TL;DR: Inside Out framework uses PersonaTree for long-term personalized dialogue, with MemListener managing structured memory operations, outperforming existing methods in noise suppression and consistency.


<details>
  <summary>Details</summary>
Motivation: Existing long-term personalized dialogue systems face challenges with memory noise accumulation, reasoning degradation, and persona inconsistency due to unbounded interaction streams and finite context constraints.

Method: Proposes Inside Out framework with globally maintained PersonaTree for user profiling, trained lightweight MemListener via RL with process-based rewards to produce structured operations (ADD, UPDATE, DELETE, NO_OP), and supports both direct tree usage and agentic mode for detailed responses.

Result: PersonaTree outperforms full-text concatenation and various personalized memory systems in suppressing contextual noise and maintaining persona consistency. MemListener achieves memory-operation decision performance comparable to or surpassing powerful reasoning models like DeepSeek-R1-0528 and Gemini-3-Pro.

Conclusion: The Inside Out framework with PersonaTree and MemListener effectively addresses long-term personalized dialogue challenges through structured memory management, achieving better consistency and noise suppression while maintaining efficiency.

Abstract: Existing long-term personalized dialogue systems struggle to reconcile unbounded interaction streams with finite context constraints, often succumbing to memory noise accumulation, reasoning degradation, and persona inconsistency. To address these challenges, this paper proposes Inside Out, a framework that utilizes a globally maintained PersonaTree as the carrier of long-term user profiling. By constraining the trunk with an initial schema and updating the branches and leaves, PersonaTree enables controllable growth, achieving memory compression while preserving consistency. Moreover, we train a lightweight MemListener via reinforcement learning with process-based rewards to produce structured, executable, and interpretable {ADD, UPDATE, DELETE, NO_OP} operations, thereby supporting the dynamic evolution of the personalized tree. During response generation, PersonaTree is directly leveraged to enhance outputs in latency-sensitive scenarios; when users require more details, the agentic mode is triggered to introduce details on-demand under the constraints of the PersonaTree. Experiments show that PersonaTree outperforms full-text concatenation and various personalized memory systems in suppressing contextual noise and maintaining persona consistency. Notably, the small MemListener model achieves memory-operation decision performance comparable to, or even surpassing, powerful reasoning models such as DeepSeek-R1-0528 and Gemini-3-Pro.

</details>


### [92] [LELA: an LLM-based Entity Linking Approach with Zero-Shot Domain Adaptation](https://arxiv.org/abs/2601.05192)
*Samy Haffoudhi,Fabian M. Suchanek,Nils Holzenberger*

Main category: cs.CL

TL;DR: LELA is a modular coarse-to-fine entity linking method using LLMs that works across domains/knowledge bases without fine-tuning, achieving competitive performance with fine-tuned approaches.


<details>
  <summary>Details</summary>
Motivation: Entity linking is crucial for knowledge graph construction, QA, and information extraction, but existing methods often require domain-specific fine-tuning. The authors aim to create a flexible approach that works across different domains and knowledge bases without needing fine-tuning.

Method: LELA uses a modular coarse-to-fine approach leveraging large language models. It operates in two stages: coarse candidate generation followed by fine-grained disambiguation, designed to work with various LLMs and knowledge bases without requiring fine-tuning.

Result: Experiments across various entity linking settings show LELA is highly competitive with fine-tuned approaches and substantially outperforms non-fine-tuned methods.

Conclusion: LELA provides an effective, flexible entity linking solution that eliminates the need for domain-specific fine-tuning while maintaining competitive performance with specialized approaches.

Abstract: Entity linking (mapping ambiguous mentions in text to entities in a knowledge base) is a foundational step in tasks such as knowledge graph construction, question-answering, and information extraction. Our method, LELA, is a modular coarse-to-fine approach that leverages the capabilities of large language models (LLMs), and works with different target domains, knowledge bases and LLMs, without any fine-tuning phase. Our experiments across various entity linking settings show that LELA is highly competitive with fine-tuned approaches, and substantially outperforms the non-fine-tuned ones.

</details>


### [93] [Measuring and Fostering Peace through Machine Learning and Artificial Intelligence](https://arxiv.org/abs/2601.05232)
*P. Gilda,P. Dungarwal,A. Thongkham,E. T. Ajayi,S. Choudhary,T. M. Terol,C. Lam,J. P. Araujo,M. McFadyen-Mungalln,L. S. Liebovitch,P. T. Coleman,H. West,K. Sieck,S. Carter*

Main category: cs.CL

TL;DR: Researchers used AI to measure peace levels from news/social media and created MirrorMirror Chrome extension to provide real-time peace feedback to YouTube viewers, aiming to promote more respectful media consumption.


<details>
  <summary>Details</summary>
Motivation: Most people (71% aged 20-40) get news from emotionally-charged short videos on social media, where creators use anger to drive engagement. There's a need to move beyond simple engagement metrics toward understanding media's peacefulness and its effects.

Method: 1) Used neural networks on text embeddings to measure peace in news media, showing cross-dataset generalization. 2) Developed models for social media using GoEmotions (word-level) and LLMs (context-level) to measure social dimensions important for peace. 3) Built and tested MirrorMirror Chrome extension for real-time peace feedback on YouTube.

Result: News peace measurement model trained on one dataset performed well on different news dataset. Developed functional Chrome extension providing real-time peace analysis of YouTube content. Identified that 71% of young adults consume news primarily through emotionally-activating short videos.

Conclusion: The MirrorMirror tool represents a step toward open-source solutions for content creators, journalists, and users to understand media tone and its effects. Long-term goal is to encourage more respectful, nuanced communication by moving beyond engagement metrics to peace-focused analysis.

Abstract: We used machine learning and artificial intelligence: 1) to measure levels of peace in countries from news and social media and 2) to develop on-line tools that promote peace by helping users better understand their own media diet. For news media, we used neural networks to measure levels of peace from text embeddings of on-line news sources. The model, trained on one news media dataset also showed high accuracy when used to analyze a different news dataset. For social media, such as YouTube, we developed other models to measure levels of social dimensions important in peace using word level (GoEmotions) and context level (Large Language Model) methods. To promote peace, we note that 71% of people 20-40 years old daily view most of their news through short videos on social media. Content creators of these videos are biased towards creating videos with emotional activation, making you angry to engage you, to increase clicks. We developed and tested a Chrome extension, MirrorMirror, which provides real-time feedback to YouTube viewers about the peacefulness of the media they are watching. Our long term goal is for MirrorMirror to evolve into an open-source tool for content creators, journalists, researchers, platforms, and individual users to better understand the tone of their media creation and consumption and its effects on viewers. Moving beyond simple engagement metrics, we hope to encourage more respectful, nuanced, and informative communication.

</details>


### [94] [GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization](https://arxiv.org/abs/2601.05242)
*Shih-Yang Liu,Xin Dong,Ximing Lu,Shizhe Diao,Peter Belcak,Mingjie Liu,Min-Hung Chen,Hongxu Yin,Yu-Chiang Frank Wang,Kwang-Ting Cheng,Yejin Choi,Jan Kautz,Pavlo Molchanov*

Main category: cs.CL

TL;DR: GDPO improves multi-reward RL by decoupling reward normalization, preventing collapse of distinct rewards into identical advantages, leading to better convergence and stability compared to GRPO.


<details>
  <summary>Details</summary>
Motivation: As language models need to align with diverse human preferences, multi-reward RL pipelines are used but default to GRPO without examining its suitability. GRPO causes distinct reward combinations to collapse into identical advantage values, reducing training signal resolution and causing suboptimal convergence or early failure.

Method: Introduces Group reward-Decoupled Normalization Policy Optimization (GDPO), which decouples normalization of individual rewards to preserve their relative differences, enabling more accurate multi-reward optimization with improved training stability.

Result: GDPO consistently outperforms GRPO across three tasks (tool calling, math reasoning, coding reasoning) in both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length), demonstrating effectiveness and generalizability.

Conclusion: GDPO is a superior policy optimization method for multi-reward reinforcement learning that addresses GRPO's limitations by preserving reward distinctions, leading to better performance and stability across diverse tasks.

Abstract: As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [95] [Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes](https://arxiv.org/abs/2601.04300)
*Chenye Meng,Zejian Li,Zhongni Liu,Yize Li,Changle Xie,Kaixin Jia,Ling Yang,Huanghuang Deng,Shiying Ding,Shengyuan Zhang,Jiayi Li,Lingyun Sun*

Main category: cs.CV

TL;DR: CPO (Complex Preference Optimization) aligns diffusion models with hierarchical, fine-grained expert criteria for painting generation, outperforming simple reward-based methods.


<details>
  <summary>Details</summary>
Motivation: Current post-training alignment methods use oversimplified signals (scalar rewards or binary preferences), which fail to capture complex hierarchical human expertise needed for high-quality image generation.

Method: Two-stage framework: 1) Construct hierarchical evaluation criteria with domain experts (tree structure of positive/negative attributes), 2) Inject domain knowledge via SFT to auxiliary diffusion model, 3) CPO extends DPO to align target diffusion with non-binary hierarchical criteria by maximizing positive attributes and minimizing negative attributes simultaneously.

Result: Extensive experiments show CPO significantly enhances generation quality and alignment with expertise in painting generation domain, outperforming existing alignment methods.

Conclusion: CPO enables fine-grained criteria alignment for diffusion models, opening new avenues for aligning AI systems with complex hierarchical human expertise beyond simple reward signals.

Abstract: Post-training alignment of diffusion models relies on simplified signals, such as scalar rewards or binary preferences. This limits alignment with complex human expertise, which is hierarchical and fine-grained. To address this, we first construct a hierarchical, fine-grained evaluation criteria with domain experts, which decomposes image quality into multiple positive and negative attributes organized in a tree structure. Building on this, we propose a two-stage alignment framework. First, we inject domain knowledge to an auxiliary diffusion model via Supervised Fine-Tuning. Second, we introduce Complex Preference Optimization (CPO) that extends DPO to align the target diffusion to our non-binary, hierarchical criteria. Specifically, we reformulate the alignment problem to simultaneously maximize the probability of positive attributes while minimizing the probability of negative attributes with the auxiliary diffusion. We instantiate our approach in the domain of painting generation and conduct CPO training with an annotated dataset of painting with fine-grained attributes based on our criteria. Extensive experiments demonstrate that CPO significantly enhances generation quality and alignment with expertise, opening new avenues for fine-grained criteria alignment.

</details>


### [96] [Embedding Textual Information in Images Using Quinary Pixel Combinations](https://arxiv.org/abs/2601.04302)
*A V Uday Kiran Kandala*

Main category: cs.CV

TL;DR: A novel steganography method using quinary pixel intensity combinations in RGB space to embed text directly into single pixels, achieving high efficiency with minimal distortion.


<details>
  <summary>Details</summary>
Motivation: Existing text embedding methods (LSB, MSB, PVD, transform domain, deep learning) often require multiple pixels, create noise-like artifacts, have deterministic encoding/decoding, and are computationally heavy. There's a need for more efficient single-pixel embedding with minimal distortion.

Method: Uses quinary combinations of pixel intensities in RGB space - five controlled intensity variations in each R, G, B channel create 125 distinct combinations. These combinations are mapped to textual symbols (uppercase/lowercase letters, digits, whitespace, special characters), enabling complete symbol encoding within a single RGB pixel.

Result: Evaluation using MSE, MAE, SNR, PSNR, SSIM, Histogram Comparison and Heatmap analysis shows no significant distortion between original and encoded images. Achieves improved embedding efficiency compared to LSB/MSB (which need multiple pixels) and computationally heavy deep learning approaches.

Conclusion: The proposed quinary pixel intensity combination method provides efficient single-pixel text embedding with minimal visual distortion, overcoming limitations of existing steganography techniques while maintaining high embedding capacity.

Abstract: This paper presents a novel technique for embedding textual data into images using quinary combinations of pixel intensities in RGB space. Existing methods predominantly rely on least and most significant bit (LSB & MSB) manipulation, Pixel Value Differencing (PVD), spatial perturbations in RGB channels, transform domain based methods, Quantization methods, Edge and Region based methods and more recently through deep learning methods and generative AI techniques for hiding textual information in spatial domain of images. Most of them are dependent on pixel intensity flipping over multiple pixels, such as LSB and combination of LSB based methodologies, and on transform coefficients, often resulting in the form of noise. Encoding and Decoding are deterministic in most of the existing approaches and are computationally heavy in case of higher models such as deep learning and gen AI approaches. The proposed method works on quinary pixel intensity combinations in RGB space, where five controlled different pixel intensity variations in each of the R, G, and B channels formulate up to one hundred and twenty five distinct pixel intensity combinations. These combinations are mapped to textual symbols, enabling the representation of uppercase and lowercase alphabetic characters, numeric digits, whitespace, and commonly used special characters. Different metrics such as MSE, MAE, SNR, PSNR, SSIM, Histogram Comparison and Heatmap analysis, were evaluated for both original and encoded images resulting in no significant distortion in the images. Furthermore, the method achieves improved embedding efficiency by encoding a complete textual symbol within a single RGB pixel, in contrast to LSB and MSB based approaches that typically require multiple pixels or multi-step processes, as well as transform and learning based methods that incur higher computational overhead.

</details>


### [97] [Unified Text-Image Generation with Weakness-Targeted Post-Training](https://arxiv.org/abs/2601.04339)
*Jiahui Chen,Philippe Hansen-Estruch,Xiaochuang Han,Yushi Hu,Emily Dinan,Amita Kamath,Michal Drozdzal,Reyhane Askari-Hemmat,Luke Zettlemoyer,Marjan Ghazvininejad*

Main category: cs.CV

TL;DR: Unified multimodal generation architectures that produce both text and images show promise for T2I synthesis, but existing systems use explicit modality switching. This work explores post-training for fully unified text-image generation where models autonomously transition from textual reasoning to visual synthesis in a single inference process.


<details>
  <summary>Details</summary>
Motivation: Existing unified multimodal generation systems rely on explicit modality switching (generating reasoning text first, then manually switching to image generation), which limits cross-modal coupling and prohibits automatic multimodal generation. The motivation is to achieve fully unified text-image generation where models can autonomously transition between modalities within a single inference process.

Method: The paper explores post-training approaches to achieve fully unified text-image generation. It examines different post-training data strategies, showing that targeted datasets addressing specific limitations work better than broad image-caption corpora or benchmark-aligned data. The approach uses offline, reward-weighted post-training with fully self-generated synthetic data, and investigates the impact of joint text-image generation on T2I performance and the relative importance of each modality during post-training.

Result: The approach enables improvements in multimodal image generation across four diverse T2I benchmarks. The results demonstrate the effectiveness of reward-weighting both modalities and using strategically designed post-training data.

Conclusion: Fully unified text-image generation through targeted post-training with reward-weighted synthetic data is effective for improving multimodal image generation performance across diverse benchmarks, overcoming limitations of explicit modality switching approaches.

Abstract: Unified multimodal generation architectures that jointly produce text and images have recently emerged as a promising direction for text-to-image (T2I) synthesis. However, many existing systems rely on explicit modality switching, generating reasoning text before switching manually to image generation. This separate, sequential inference process limits cross-modal coupling and prohibits automatic multimodal generation. This work explores post-training to achieve fully unified text-image generation, where models autonomously transition from textual reasoning to visual synthesis within a single inference process. We examine the impact of joint text-image generation on T2I performance and the relative importance of each modality during post-training. We additionally explore different post-training data strategies, showing that a targeted dataset addressing specific limitations achieves superior results compared to broad image-caption corpora or benchmark-aligned data. Using offline, reward-weighted post-training with fully self-generated synthetic data, our approach enables improvements in multimodal image generation across four diverse T2I benchmarks, demonstrating the effectiveness of reward-weighting both modalities and strategically designed post-training data.

</details>


### [98] [ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers](https://arxiv.org/abs/2601.04342)
*Mohsen Ghafoorian,Amirhossein Habibian*

Main category: cs.CV

TL;DR: ReHyAt introduces a recurrent hybrid attention mechanism that combines softmax and linear attention to achieve state-of-the-art video quality with linear complexity, enabling efficient distillation from existing models and practical long-duration video generation.


<details>
  <summary>Details</summary>
Motivation: Current transformer-based video diffusion models suffer from quadratic attention complexity that severely limits scalability for longer video sequences, creating a need for more efficient attention mechanisms.

Method: ReHyAt uses a Recurrent Hybrid Attention mechanism that combines softmax attention fidelity with linear attention efficiency, enabling chunk-wise recurrent reformulation with constant memory usage. It employs a lightweight distillation and finetuning pipeline to efficiently transfer knowledge from existing softmax-based models.

Result: ReHyAt reduces training cost by two orders of magnitude (~160 GPU hours), achieves state-of-the-art video quality on VBench and VBench-2.0, reduces attention cost from quadratic to linear, and enables practical scalability for long-duration and on-device video generation.

Conclusion: ReHyAt provides an efficient hybrid attention solution that maintains video quality while dramatically improving computational efficiency, offering a practical recipe for scalable video generation that can be applied to future state-of-the-art models.

Abstract: Recent advances in video diffusion models have shifted towards transformer-based architectures, achieving state-of-the-art video generation but at the cost of quadratic attention complexity, which severely limits scalability for longer sequences. We introduce ReHyAt, a Recurrent Hybrid Attention mechanism that combines the fidelity of softmax attention with the efficiency of linear attention, enabling chunk-wise recurrent reformulation and constant memory usage. Unlike the concurrent linear-only SANA Video, ReHyAt's hybrid design allows efficient distillation from existing softmax-based models, reducing the training cost by two orders of magnitude to ~160 GPU hours, while being competitive in the quality. Our light-weight distillation and finetuning pipeline provides a recipe that can be applied to future state-of-the-art bidirectional softmax-based models. Experiments on VBench and VBench-2.0, as well as a human preference study, demonstrate that ReHyAt achieves state-of-the-art video quality while reducing attention cost from quadratic to linear, unlocking practical scalability for long-duration and on-device video generation. Project page is available at https://qualcomm-ai-research.github.io/rehyat.

</details>


### [99] [SCAR-GS: Spatial Context Attention for Residuals in Progressive Gaussian Splatting](https://arxiv.org/abs/2601.04348)
*Diego Revilla,Pooja Suresh,Anand Bhojan,Ooi Wei Tsang*

Main category: cs.CV

TL;DR: Progressive compression method for 3D Gaussian Splatting using Residual Vector Quantization and auto-regressive entropy modeling with multi-resolution hash grid for efficient feature compression.


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting models have high storage requirements that hinder cloud/streaming deployment. Existing progressive compression methods using scalar quantization don't optimally capture high-dimensional feature correlations, limiting rate-distortion performance.

Method: Replaces traditional methods with Residual Vector Quantization to compress primitive features. Uses auto-regressive entropy model guided by multi-resolution hash grid to predict conditional probability of transmitted indices, enabling efficient compression of coarse and refinement layers.

Result: Not specified in abstract (paper likely shows improved compression efficiency and rate-distortion performance compared to existing methods).

Conclusion: Proposed progressive codec with Residual Vector Quantization and auto-regressive entropy modeling offers more efficient compression for 3D Gaussian Splatting models, addressing storage limitations for cloud/streaming deployment.

Abstract: Recent advances in 3D Gaussian Splatting have allowed for real-time, high-fidelity novel view synthesis. Nonetheless, these models have significant storage requirements for large and medium-sized scenes, hindering their deployment over cloud and streaming services. Some of the most recent progressive compression techniques for these models rely on progressive masking and scalar quantization techniques to reduce the bitrate of Gaussian attributes using spatial context models. While effective, scalar quantization may not optimally capture the correlations of high-dimensional feature vectors, which can potentially limit the rate-distortion performance.
  In this work, we introduce a novel progressive codec for 3D Gaussian Splatting that replaces traditional methods with a more powerful Residual Vector Quantization approach to compress the primitive features. Our key contribution is an auto-regressive entropy model, guided by a multi-resolution hash grid, that accurately predicts the conditional probability of each successive transmitted index, allowing for coarse and refinement layers to be compressed with high efficiency.

</details>


### [100] [Comparative Analysis of Custom CNN Architectures versus Pre-trained Models and Transfer Learning: A Study on Five Bangladesh Datasets](https://arxiv.org/abs/2601.04352)
*Ibrahim Tanvir,Alif Ruslan,Sartaj Solaiman*

Main category: cs.CV

TL;DR: Custom CNNs vs pre-trained models (ResNet-18, VGG-16) tested on 5 Bangladeshi image datasets. Transfer learning with fine-tuning outperforms custom CNNs and feature extraction, achieving up to 76% accuracy gains and perfect 100% on Road Damage BD dataset.


<details>
  <summary>Details</summary>
Motivation: To provide practical guidance for practitioners on selecting appropriate deep learning approaches by comparing custom-built CNNs against popular pre-trained architectures using different transfer learning strategies, specifically tested on diverse Bangladeshi image classification datasets.

Method: Comparative analysis of custom CNNs vs pre-trained models (ResNet-18, VGG-16) using both feature extraction and transfer learning approaches. Evaluated on five Bangladeshi datasets: Footpath Vision, Auto Rickshaw Detection, Mango Image Classification, Paddy Variety Recognition, and Road Damage Detection.

Result: Transfer learning with fine-tuning consistently outperforms both custom CNNs and feature extraction methods, achieving 3-76% accuracy improvements across datasets. ResNet-18 with fine-tuning achieved 100% accuracy on Road Damage BD dataset. Custom CNNs have smaller model size (3.4M vs 11-134M parameters) and better training efficiency on simpler tasks.

Conclusion: Pre-trained models with transfer learning provide superior performance, especially for complex tasks with limited data, while custom CNNs offer advantages in model size and efficiency for simpler tasks. The research provides practical selection guidelines based on dataset characteristics, computational resources, and performance requirements.

Abstract: This study presents a comprehensive comparative analysis of custom-built Convolutional Neural Networks (CNNs) against popular pre-trained architectures (ResNet-18 and VGG-16) using both feature extraction and transfer learning approaches. We evaluated these models across five diverse image classification datasets from Bangladesh: Footpath Vision, Auto Rickshaw Detection, Mango Image Classification, Paddy Variety Recognition, and Road Damage Detection. Our experimental results demonstrate that transfer learning with fine-tuning consistently outperforms both custom CNNs built from scratch and feature extraction methods, achieving accuracy improvements ranging from 3% to 76% across different datasets. Notably, ResNet-18 with fine-tuning achieved perfect 100% accuracy on the Road Damage BD dataset. While custom CNNs offer advantages in model size (3.4M parameters vs. 11-134M for pre-trained models) and training efficiency on simpler tasks, pre-trained models with transfer learning provide superior performance, particularly on complex classification tasks with limited training data. This research provides practical insights for practitioners in selecting appropriate deep learning approaches based on dataset characteristics, computational resources, and performance requirements.

</details>


### [101] [PackCache: A Training-Free Acceleration Method for Unified Autoregressive Video Generation via Compact KV-Cache](https://arxiv.org/abs/2601.04359)
*Kunyang Li,Mubarak Shah,Yuzhang Shang*

Main category: cs.CV

TL;DR: PackCache: Training-free KV-cache compression for unified autoregressive video generation, achieving 1.7-2.2x speedup on 48-frame sequences by exploiting spatiotemporal attention patterns.


<details>
  <summary>Details</summary>
Motivation: KV-cache size grows linearly with generated tokens, becoming the dominant bottleneck for inference efficiency and generative length in unified autoregressive models, especially for video generation where sequences are long.

Method: PackCache dynamically compacts KV cache through three mechanisms: condition anchoring (preserves semantic reference tokens), cross-frame decay modeling (allocates cache budget by temporal distance), and spatially preserving position embedding (maintains 3D structure under cache removal).

Result: 1.7-2.2x acceleration on end-to-end 48-frame video generation; final four frames (most expensive segment) see 2.6x speedup on A40 and 3.7x on H20.

Conclusion: PackCache effectively addresses KV-cache bottleneck in unified autoregressive video generation by exploiting observed spatiotemporal attention patterns, enabling longer-sequence generation with significant speed improvements.

Abstract: A unified autoregressive model is a Transformer-based framework that addresses diverse multimodal tasks (e.g., text, image, video) as a single sequence modeling problem under a shared token space. Such models rely on the KV-cache mechanism to reduce attention computation from O(T^2) to O(T); however, KV-cache size grows linearly with the number of generated tokens, and it rapidly becomes the dominant bottleneck limiting inference efficiency and generative length. Unified autoregressive video generation inherits this limitation. Our analysis reveals that KV-cache tokens exhibit distinct spatiotemporal properties: (i) text and conditioning-image tokens act as persistent semantic anchors that consistently receive high attention, and (ii) attention to previous frames naturally decays with temporal distance. Leveraging these observations, we introduce PackCache, a training-free KV-cache management method that dynamically compacts the KV cache through three coordinated mechanisms: condition anchoring that preserves semantic references, cross-frame decay modeling that allocates cache budget according to temporal distance, and spatially preserving position embedding that maintains coherent 3D structure under cache removal. In terms of efficiency, PackCache accelerates end-to-end generation by 1.7-2.2x on 48-frame long sequences, showcasing its strong potential for enabling longer-sequence video generation. Notably, the final four frames - the portion most impacted by the progressively expanding KV-cache and thus the most expensive segment of the clip - PackCache delivers a 2.6x and 3.7x acceleration on A40 and H200, respectively, for 48-frame videos.

</details>


### [102] [Combining facial videos and biosignals for stress estimation during driving](https://arxiv.org/abs/2601.04376)
*Paraskevi Valergaki,Vassilis C. Nicodemou,Iason Oikonomidis,Antonis Argyros,Anastasios Roussos*

Main category: cs.CV

TL;DR: 3D facial geometry from EMOCA effectively captures stress responses during distracted driving, with cross-modal attention fusion achieving best performance for stress recognition.


<details>
  <summary>Details</summary>
Motivation: Stress recognition from facial videos is challenging due to subjectivity and voluntary facial control. Existing methods rely on Facial Action Units, but the potential of disentangled 3D facial geometry remains underexplored for stress analysis.

Method: Used EMOCA-derived 3D expression and pose coefficients to analyze stress during distracted driving. Conducted paired hypothesis tests between baseline and stressor phases. Proposed Transformer-based temporal modeling framework with unimodal, early-fusion, and cross-modal attention strategies for stress recognition.

Result: 41 of 56 EMOCA coefficients showed consistent, phase-specific stress responses comparable to physiological markers. Cross-Modal Attention fusion of EMOCA and physiological signals achieved best performance (AUROC 92%, Accuracy 86.7%), with EMOCA-gaze fusion also competitive (AUROC 91.8%).

Conclusion: Disentangled 3D facial geometry effectively captures stress responses, and temporal modeling with cross-modal attention significantly improves stress recognition performance, highlighting the value of combining facial geometry with other modalities.

Abstract: Reliable stress recognition from facial videos is challenging due to stress's subjective nature and voluntary facial control. While most methods rely on Facial Action Units, the role of disentangled 3D facial geometry remains underexplored. We address this by analyzing stress during distracted driving using EMOCA-derived 3D expression and pose coefficients. Paired hypothesis tests between baseline and stressor phases reveal that 41 of 56 coefficients show consistent, phase-specific stress responses comparable to physiological markers. Building on this, we propose a Transformer-based temporal modeling framework and assess unimodal, early-fusion, and cross-modal attention strategies. Cross-Modal Attention fusion of EMOCA and physiological signals achieves best performance (AUROC 92\%, Accuracy 86.7\%), with EMOCA-gaze fusion also competitive (AUROC 91.8\%). This highlights the effectiveness of temporal modeling and cross-modal attention for stress recognition.

</details>


### [103] [Few-Shot LoRA Adaptation of a Flow-Matching Foundation Model for Cross-Spectral Object Detection](https://arxiv.org/abs/2601.04381)
*Maxim Clouser,Kia Khezeli,John Kalantari*

Main category: cs.CV

TL;DR: A flow-matching foundation model (FLUX.1) pre-trained on RGB can be adapted with just 100 paired images to translate RGB to IR/SAR, enabling synthetic data generation that improves downstream object detection in non-visible modalities.


<details>
  <summary>Details</summary>
Motivation: Foundation models are primarily trained on RGB data, but safety-critical applications often use non-visible modalities like IR and SAR. There's a need to leverage existing RGB foundation models for cross-spectral translation to support these modalities.

Method: Use FLUX.1 Kontext foundation model, insert LoRA modules, and fine-tune on only 100 paired images per domain (RGB→IR on KAIST, RGB→SAR on M4-SAR). Use LPIPS metric on 50 held-out pairs to select best LoRA hyperparameters, then generate synthetic IR/SAR data for training object detectors.

Result: Lower LPIPS consistently predicts higher mAP for YOLOv11n on both IR and SAR, and for DETR on KAIST IR. Synthetic IR from external RGB datasets improves KAIST IR pedestrian detection, and synthetic SAR boosts infrastructure detection on M4-SAR when combined with limited real SAR.

Conclusion: Few-shot LoRA adaptation of flow-matching foundation models is a promising approach for extending foundation-style support to non-visible modalities, enabling effective cross-spectral translation with minimal paired data.

Abstract: Foundation models for vision are predominantly trained on RGB data, while many safety-critical applications rely on non-visible modalities such as infrared (IR) and synthetic aperture radar (SAR). We study whether a single flow-matching foundation model pre-trained primarily on RGB images can be repurposed as a cross-spectral translator using only a few co-measured examples, and whether the resulting synthetic data can enhance downstream detection. Starting from FLUX.1 Kontext, we insert low-rank adaptation (LoRA) modules and fine-tune them on just 100 paired images per domain for two settings: RGB to IR on the KAIST dataset and RGB to SAR on the M4-SAR dataset. The adapted model translates RGB images into pixel-aligned IR/SAR, enabling us to reuse existing bounding boxes and train object detection models purely in the target modality. Across a grid of LoRA hyperparameters, we find that LPIPS computed on only 50 held-out pairs is a strong proxy for downstream performance: lower LPIPS consistently predicts higher mAP for YOLOv11n on both IR and SAR, and for DETR on KAIST IR test data. Using the best LPIPS-selected LoRA adapter, synthetic IR from external RGB datasets (LLVIP, FLIR ADAS) improves KAIST IR pedestrian detection, and synthetic SAR significantly boosts infrastructure detection on M4-SAR when combined with limited real SAR. Our results suggest that few-shot LoRA adaptation of flow-matching foundation models is a promising path toward foundation-style support for non-visible modalities.

</details>


### [104] [Performance Analysis of Image Classification on Bangladeshi Datasets](https://arxiv.org/abs/2601.04397)
*Mohammed Sami Khan,Fabiha Muniat,Rowzatul Zannat*

Main category: cs.CV

TL;DR: Custom CNN vs pre-trained models (VGG-16, ResNet-50, MobileNet) for image classification: Pre-trained models outperform in accuracy and convergence, especially with limited data, but custom CNN offers competitive performance with fewer parameters and lower computational cost.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the practical dilemma of choosing between designing custom CNNs from scratch versus using established pre-trained architectures for image classification tasks, aiming to provide empirical evidence on their comparative performance and trade-offs.

Method: Comparative analysis between a custom-designed CNN trained from scratch and several popular pre-trained architectures (VGG-16, ResNet-50, MobileNet) using transfer learning. All models were evaluated under identical experimental settings using standard performance metrics: accuracy, precision, recall, and F1-score.

Result: Pre-trained CNN architectures consistently outperformed the custom CNN in classification accuracy and convergence speed, particularly when training data was limited. However, the custom CNN demonstrated competitive performance with significantly fewer parameters and reduced computational complexity.

Conclusion: The study highlights trade-offs between model complexity, performance, and computational efficiency, providing practical insights for selecting appropriate CNN architectures based on specific requirements such as available data, computational resources, and performance needs.

Abstract: Convolutional Neural Networks (CNNs) have demonstrated remarkable success in image classification tasks; however, the choice between designing a custom CNN from scratch and employing established pre-trained architectures remains an important practical consideration. In this work, we present a comparative analysis of a custom-designed CNN and several widely used deep learning architectures, including VGG-16, ResNet-50, and MobileNet, for an image classification task. The custom CNN is developed and trained from scratch, while the popular architectures are employed using transfer learning under identical experimental settings. All models are evaluated using standard performance metrics such as accuracy, precision, recall, and F1-score. Experimental results show that pre-trained CNN architectures consistently outperform the custom CNN in terms of classification accuracy and convergence speed, particularly when training data is limited. However, the custom CNN demonstrates competitive performance with significantly fewer parameters and reduced computational complexity. This study highlights the trade-offs between model complexity, performance, and computational efficiency, and provides practical insights into selecting appropriate CNN architectures for image classification problems.

</details>


### [105] [3D-Agent:Tri-Modal Multi-Agent Collaboration for Scalable 3D Object Annotation](https://arxiv.org/abs/2601.04404)
*Jusheng Zhang,Yijia Fan,Zimo Wen,Jian Wang,Keze Wang*

Main category: cs.CV

TL;DR: Tri MARF is a tri-modal framework using 2D images, text, and 3D point clouds with multi-agent collaboration to improve large-scale 3D object annotation, achieving state-of-the-art performance across multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: 3D object annotation is challenging due to spatial complexity, occlusion, and viewpoint inconsistency in applications like autonomous driving and augmented reality. Single-model approaches struggle with these issues effectively.

Method: Tri MARF integrates tri-modal inputs (2D multi-view images, textual descriptions, 3D point clouds) using a multi-agent collaborative architecture with three specialized agents: vision-language model agent for multi-view descriptions, information aggregation agent for optimal description selection, and gating agent for aligning textual semantics with 3D geometry.

Result: Extensive experiments on Objaverse, LVIS, Objaverse XL, and ABO datasets show Tri MARF substantially outperforms existing methods: CLIPScore of 88.7, retrieval accuracy of 45.2 and 43.8 on ViLT R@5, and throughput of up to 12,000 objects per hour on a single NVIDIA A100 GPU.

Conclusion: Tri MARF effectively addresses 3D annotation challenges through tri-modal integration and multi-agent collaboration, demonstrating superior performance and efficiency for large-scale 3D object annotation tasks.

Abstract: Driven by applications in autonomous driving robotics and augmented reality 3D object annotation presents challenges beyond 2D annotation including spatial complexity occlusion and viewpoint inconsistency Existing approaches based on single models often struggle to address these issues effectively We propose Tri MARF a novel framework that integrates tri modal inputs including 2D multi view images textual descriptions and 3D point clouds within a multi agent collaborative architecture to enhance large scale 3D annotation Tri MARF consists of three specialized agents a vision language model agent for generating multi view descriptions an information aggregation agent for selecting optimal descriptions and a gating agent that aligns textual semantics with 3D geometry for refined captioning Extensive experiments on Objaverse LVIS Objaverse XL and ABO demonstrate that Tri MARF substantially outperforms existing methods achieving a CLIPScore of 88 point 7 compared to prior state of the art methods retrieval accuracy of 45 point 2 and 43 point 8 on ViLT R at 5 and a throughput of up to 12000 objects per hour on a single NVIDIA A100 GPU

</details>


### [106] [From Preoperative CT to Postmastoidectomy Mesh Construction:1Mastoidectomy Shape Prediction for Cochlear Implant Surgery](https://arxiv.org/abs/2601.04405)
*Yike Zhang,Eduardo Davalos,Dingjie Su,Ange Lou,Jack Noble*

Main category: cs.CV

TL;DR: A hybrid self-supervised and weakly-supervised learning framework predicts mastoidectomy shapes from preoperative CT scans without human annotations, achieving 0.72 Dice score for cochlear implant surgical planning.


<details>
  <summary>Details</summary>
Motivation: Accurate mastoidectomy shape prediction from preoperative imaging improves cochlear implant surgical planning, reduces risks, and enhances outcomes, but limited deep-learning studies exist due to challenges in acquiring ground-truth labels.

Method: Proposes a hybrid self-supervised and weakly-supervised learning framework to predict mastoidectomy region directly from preoperative CT scans where mastoid remains intact, using 3D T-distribution loss in weakly-supervised medical imaging.

Result: Achieves mean Dice score of 0.72 for predicting complex and boundary-less mastoidectomy shape, surpassing state-of-the-art approaches and demonstrating strong performance.

Conclusion: First work integrating self-supervised and weakly-supervised learning for mastoidectomy shape prediction, providing groundwork for constructing 3D postmastoidectomy surfaces directly from preoperative CT scans for robust CI surgical planning.

Abstract: Cochlear Implant (CI) surgery treats severe hearing loss by inserting an electrode array into the cochlea to stimulate the auditory nerve. An important step in this procedure is mastoidectomy, which removes part of the mastoid region of the temporal bone to provide surgical access. Accurate mastoidectomy shape prediction from preoperative imaging improves pre-surgical planning, reduces risks, and enhances surgical outcomes. Despite its importance, there are limited deep-learning-based studies regarding this topic due to the challenges of acquiring ground-truth labels. We address this gap by investigating self-supervised and weakly-supervised learning models to predict the mastoidectomy region without human annotations. We propose a hybrid self-supervised and weakly-supervised learning framework to predict the mastoidectomy region directly from preoperative CT scans, where the mastoid remains intact. Our hybrid method achieves a mean Dice score of 0.72 when predicting the complex and boundary-less mastoidectomy shape, surpassing state-of-the-art approaches and demonstrating strong performance. The method provides groundwork for constructing 3D postmastoidectomy surfaces directly from the corresponding preoperative CT scans. To our knowledge, this is the first work that integrating self-supervised and weakly-supervised learning for mastoidectomy shape prediction, offering a robust and efficient solution for CI surgical planning while leveraging 3D T-distribution loss in weakly-supervised medical imaging.

</details>


### [107] [CRUNet-MR-Univ: A Foundation Model for Diverse Cardiac MRI Reconstruction](https://arxiv.org/abs/2601.04428)
*Donghang Lyu,Marius Staring,Hildo Lamb,Mariya Doneva*

Main category: cs.CV

TL;DR: CRUNet-MR-Univ is a foundation model for Cardiac MRI reconstruction that uses spatio-temporal correlations and prompt-based priors to handle diverse CMR scenarios, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current deep learning methods for CMR reconstruction lack generalizability across diverse clinical scenarios due to variability in image contrast, sampling patterns, scanner vendors, anatomical structures, and disease types. Most models only handle narrow subsets of these variations, leading to performance degradation with distribution shifts.

Method: Proposes CRUNet-MR-Univ, a foundation model that leverages spatio-temporal correlations and prompt-based priors to effectively handle the full diversity of CMR scans across different scenarios.

Result: The approach consistently outperforms baseline methods across a wide range of settings, demonstrating effectiveness and promise for clinical applications.

Conclusion: CRUNet-MR-Univ represents a unified solution for CMR reconstruction that addresses the generalizability limitations of current deep learning methods, showing potential for real-world clinical use across diverse CMR scenarios.

Abstract: In recent years, deep learning has attracted increasing attention in the field of Cardiac MRI (CMR) reconstruction due to its superior performance over traditional methods, particularly in handling higher acceleration factors, highlighting its potential for real-world clinical applications. However, current deep learning methods remain limited in generalizability. CMR scans exhibit wide variability in image contrast, sampling patterns, scanner vendors, anatomical structures, and disease types. Most existing models are designed to handle only a single or narrow subset of these variations, leading to performance degradation when faced with distribution shifts. Therefore, it is beneficial to develop a unified model capable of generalizing across diverse CMR scenarios. To this end, we propose CRUNet-MR-Univ, a foundation model that leverages spatio-temporal correlations and prompt-based priors to effectively handle the full diversity of CMR scans. Our approach consistently outperforms baseline methods across a wide range of settings, highlighting its effectiveness and promise.

</details>


### [108] [Addressing Overthinking in Large Vision-Language Models via Gated Perception-Reasoning Optimization](https://arxiv.org/abs/2601.04442)
*Xingjian Diao,Zheyuan Liu,Chunhui Zhang,Weiyi Wu,Keyi Kong,Lin Shi,Kaize Ding,Soroush Vosoughi,Jiang Gui*

Main category: cs.CV

TL;DR: GPRO is a meta-reasoning controller that dynamically routes computation between fast/slow paths to address overthinking in LVLMs by distinguishing perceptual vs reasoning errors.


<details>
  <summary>Details</summary>
Motivation: Current chain-of-thought approaches in Large Vision-Language Models lead to overthinking - excessively verbose responses for simple queries, causing test-time inefficiency and degraded accuracy. Prior adaptive reasoning methods overlook the fundamental bottleneck of visual perception failures, where reasoning errors often originate from imperfect perception rather than insufficient deliberation.

Method: Proposes Gated Perception-Reasoning Optimization (GPRO), a meta-reasoning controller that dynamically routes computation among three decision paths at each generation step: 1) lightweight fast path, 2) slow perception path for re-examining visual inputs, and 3) slow reasoning path for internal self-reflection. Uses teacher models to derive large-scale failure attribution supervision from ~790k samples to distinguish perceptual hallucinations from reasoning errors, then trains the controller with multi-objective reinforcement learning to optimize accuracy vs computational cost trade-off.

Result: Experiments on five benchmarks demonstrate that GPRO substantially improves both accuracy and efficiency, outperforming recent slow-thinking methods while generating significantly shorter responses.

Conclusion: Stable reasoning in LVLMs critically depends on low-level visual grounding, and GPRO's dynamic routing approach effectively addresses overthinking by distinguishing and handling perceptual vs reasoning errors, leading to more efficient and accurate model performance.

Abstract: Large Vision-Language Models (LVLMs) have exhibited strong reasoning capabilities through chain-of-thought mechanisms that generate step-by-step rationales. However, such slow-thinking approaches often lead to overthinking, where models produce excessively verbose responses even for simple queries, resulting in test-time inefficiency and even degraded accuracy. Prior work has attempted to mitigate this issue via adaptive reasoning strategies, but these methods largely overlook a fundamental bottleneck: visual perception failures. We argue that stable reasoning critically depends on low-level visual grounding, and that reasoning errors often originate from imperfect perception rather than insufficient deliberation. To address this limitation, we propose Gated Perception-Reasoning Optimization (GPRO), a meta-reasoning controller that dynamically routes computation among three decision paths at each generation step: a lightweight fast path, a slow perception path for re-examining visual inputs, and a slow reasoning path for internal self-reflection. To learn this distinction, we derive large-scale failure attribution supervision from approximately 790k samples, using teacher models to distinguish perceptual hallucinations from reasoning errors. We then train the controller with multi-objective reinforcement learning to optimize the trade-off between task accuracy and computational cost under uncertainty. Experiments on five benchmarks demonstrate that GPRO substantially improves both accuracy and efficiency, outperforming recent slow-thinking methods while generating significantly shorter responses.

</details>


### [109] [UniDrive-WM: Unified Understanding, Planning and Generation World Model For Autonomous Driving](https://arxiv.org/abs/2601.04453)
*Zhexiao Xiong,Xin Ye,Burhan Yaman,Sheng Cheng,Yiren Lu,Jingru Luo,Nathan Jacobs,Liu Ren*

Main category: cs.CV

TL;DR: UniDrive-WM is a unified vision-language model world model that jointly performs driving-scene understanding, trajectory planning, and future image generation in a single architecture, outperforming previous methods on autonomous driving benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current autonomous driving approaches treat perception, prediction, and planning as separate modules, which may limit performance. The authors aim to create a unified world model that integrates these capabilities for better driving performance.

Method: Proposes UniDrive-WM, a unified VLM-based world model with three integrated components: 1) driving-scene understanding, 2) trajectory planning, and 3) trajectory-conditioned future image generation. The trajectory planner predicts future trajectories, which condition a VLM-based image generator to produce future frames. These predictions provide additional supervisory signals that enhance scene understanding and iteratively refine trajectory generation. The authors also compare discrete and continuous output representations for future image prediction.

Result: On the Bench2Drive benchmark, UniDrive-WM produces high-fidelity future images and improves planning performance by 5.9% in L2 trajectory error and 9.2% in collision rate over the previous best method.

Conclusion: Tightly integrating VLM-driven reasoning, planning, and generative world modeling provides significant advantages for autonomous driving, demonstrating the effectiveness of unified world models over modular approaches.

Abstract: World models have become central to autonomous driving, where accurate scene understanding and future prediction are crucial for safe control. Recent work has explored using vision-language models (VLMs) for planning, yet existing approaches typically treat perception, prediction, and planning as separate modules. We propose UniDrive-WM, a unified VLM-based world model that jointly performs driving-scene understanding, trajectory planning, and trajectory-conditioned future image generation within a single architecture. UniDrive-WM's trajectory planner predicts a future trajectory, which conditions a VLM-based image generator to produce plausible future frames. These predictions provide additional supervisory signals that enhance scene understanding and iteratively refine trajectory generation. We further compare discrete and continuous output representations for future image prediction, analyzing their influence on downstream driving performance. Experiments on the challenging Bench2Drive benchmark show that UniDrive-WM produces high-fidelity future images and improves planning performance by 5.9% in L2 trajectory error and 9.2% in collision rate over the previous best method. These results demonstrate the advantages of tightly integrating VLM-driven reasoning, planning, and generative world modeling for autonomous driving. The project page is available at https://unidrive-wm.github.io/UniDrive-WM .

</details>


### [110] [Vision-Language Agents for Interactive Forest Change Analysis](https://arxiv.org/abs/2601.04497)
*James Brock,Ce Zhang,Nantheera Anantrasirichai*

Main category: cs.CV

TL;DR: LLM-driven agent for forest change analysis using satellite imagery, combining change detection and semantic captioning with natural language querying.


<details>
  <summary>Details</summary>
Motivation: Address gaps in pixel-level change detection and semantic change captioning for forest dynamics, and explore integration of LLMs with vision-language models for remote sensing image change interpretation.

Method: Proposes LLM-driven agent with multi-level change interpretation vision-language backbone and LLM-based orchestration. Introduces Forest-Change dataset with bi-temporal satellite imagery, change masks, and semantic captions.

Result: Achieves 67.10% mIoU and 40.17% BLEU-4 on Forest-Change dataset, and 88.13% mIoU and 34.41% BLEU-4 on LEVIR-MCI-Trees subset for joint change detection and captioning.

Conclusion: LLM-driven RSICI systems improve accessibility, interpretability, and efficiency of forest change analysis. All data and code are publicly available.

Abstract: Modern forest monitoring workflows increasingly benefit from the growing availability of high-resolution satellite imagery and advances in deep learning. Two persistent challenges in this context are accurate pixel-level change detection and meaningful semantic change captioning for complex forest dynamics. While large language models (LLMs) are being adapted for interactive data exploration, their integration with vision-language models (VLMs) for remote sensing image change interpretation (RSICI) remains underexplored. To address this gap, we introduce an LLM-driven agent for integrated forest change analysis that supports natural language querying across multiple RSICI tasks. The proposed system builds upon a multi-level change interpretation (MCI) vision-language backbone with LLM-based orchestration. To facilitate adaptation and evaluation in forest environments, we further introduce the Forest-Change dataset, which comprises bi-temporal satellite imagery, pixel-level change masks, and multi-granularity semantic change captions generated using a combination of human annotation and rule-based methods. Experimental results show that the proposed system achieves mIoU and BLEU-4 scores of 67.10% and 40.17% on the Forest-Change dataset, and 88.13% and 34.41% on LEVIR-MCI-Trees, a tree-focused subset of LEVIR-MCI benchmark for joint change detection and captioning. These results highlight the potential of interactive, LLM-driven RSICI systems to improve accessibility, interpretability, and efficiency of forest change analysis. All data and code are publicly available at https://github.com/JamesBrockUoB/ForestChat.

</details>


### [111] [TokenSeg: Efficient 3D Medical Image Segmentation via Hierarchical Visual Token Compression](https://arxiv.org/abs/2601.04519)
*Sen Zeng,Hong Zhou,Zheng Zhu,Yang Liu*

Main category: cs.CV

TL;DR: TokenSeg is a boundary-aware sparse token representation framework for efficient 3D medical image segmentation that reduces computational demands while maintaining high accuracy.


<details>
  <summary>Details</summary>
Motivation: 3D medical image segmentation is computationally intensive due to cubic voxel growth and redundant processing of homogeneous regions, requiring more efficient approaches.

Method: Three main components: (1) multi-scale hierarchical encoder extracting 400 candidate tokens across four resolutions, (2) boundary-aware tokenizer using VQ-VAE quantization with importance scoring to select 100 salient tokens (60% near boundaries), and (3) sparse-to-dense decoder reconstructing full-resolution masks through token reprojection, upsampling, and skip connections.

Result: Achieves state-of-the-art performance on 3D breast DCE-MRI dataset (960 cases) with 94.49% Dice and 89.61% IoU, while reducing GPU memory by 64% and inference latency by 68%. Also demonstrates strong generalization on MSD cardiac and brain MRI benchmarks.

Conclusion: TokenSeg demonstrates that anatomically informed sparse representation enables accurate and efficient 3D medical image segmentation by focusing computational resources on boundary regions while maintaining performance.

Abstract: Three-dimensional medical image segmentation is a fundamental yet computationally demanding task due to the cubic growth of voxel processing and the redundant computation on homogeneous regions. To address these limitations, we propose \textbf{TokenSeg}, a boundary-aware sparse token representation framework for efficient 3D medical volume segmentation. Specifically, (1) we design a \emph{multi-scale hierarchical encoder} that extracts 400 candidate tokens across four resolution levels to capture both global anatomical context and fine boundary details; (2) we introduce a \emph{boundary-aware tokenizer} that combines VQ-VAE quantization with importance scoring to select 100 salient tokens, over 60\% of which lie near tumor boundaries; and (3) we develop a \emph{sparse-to-dense decoder} that reconstructs full-resolution masks through token reprojection, progressive upsampling, and skip connections. Extensive experiments on a 3D breast DCE-MRI dataset comprising 960 cases demonstrate that TokenSeg achieves state-of-the-art performance with 94.49\% Dice and 89.61\% IoU, while reducing GPU memory and inference latency by 64\% and 68\%, respectively. To verify the generalization capability, our evaluations on MSD cardiac and brain MRI benchmark datasets demonstrate that TokenSeg consistently delivers optimal performance across heterogeneous anatomical structures. These results highlight the effectiveness of anatomically informed sparse representation for accurate and efficient 3D medical image segmentation.

</details>


### [112] [FaceRefiner: High-Fidelity Facial Texture Refinement with Differentiable Rendering-based Style Transfer](https://arxiv.org/abs/2601.04520)
*Chengyang Li,Baoping Cheng,Yao Cheng,Haocheng Zhang,Renshuai Liu,Yinglin Zheng,Jing Liao,Xuan Cheng*

Main category: cs.CV

TL;DR: FaceRefiner is a style transfer-based facial texture refinement method that improves texture quality and identity preservation by transferring multi-level information from input images to generated textures using differentiable rendering.


<details>
  <summary>Details</summary>
Motivation: Current facial texture generation methods have limited generalization for in-the-wild images because their synthesized texture UV maps come from training data or 2D face generator spaces, leading to inconsistencies in facial details, structures, and identity with the input.

Method: FaceRefiner treats 3D sampled texture as style and texture generation output as content, using style transfer with differentiable rendering to transfer multi-level information (low/middle/high level) from style to content, preserving pixel-level details in visible face regions.

Result: Extensive experiments on Multi-PIE, CelebA and FFHQ datasets show FaceRefiner improves texture quality and face identity preservation compared to state-of-the-art methods.

Conclusion: The proposed style transfer-based refinement method effectively preserves facial details, structures, and identity from input images, addressing generalization limitations of current texture generation approaches.

Abstract: Recent facial texture generation methods prefer to use deep networks to synthesize image content and then fill in the UV map, thus generating a compelling full texture from a single image. Nevertheless, the synthesized texture UV map usually comes from a space constructed by the training data or the 2D face generator, which limits the methods' generalization ability for in-the-wild input images. Consequently, their facial details, structures and identity may not be consistent with the input. In this paper, we address this issue by proposing a style transfer-based facial texture refinement method named FaceRefiner. FaceRefiner treats the 3D sampled texture as style and the output of a texture generation method as content. The photo-realistic style is then expected to be transferred from the style image to the content image. Different from current style transfer methods that only transfer high and middle level information to the result, our style transfer method integrates differentiable rendering to also transfer low level (or pixel level) information in the visible face regions. The main benefit of such multi-level information transfer is that, the details, structures and semantics in the input can thus be well preserved. The extensive experiments on Multi-PIE, CelebA and FFHQ datasets demonstrate that our refinement method can improve the texture quality and the face identity preserving ability, compared with state-of-the-arts.

</details>


### [113] [All Changes May Have Invariant Principles: Improving Ever-Shifting Harmful Meme Detection via Design Concept Reproduction](https://arxiv.org/abs/2601.04567)
*Ziyou Jiang,Mingyang Li,Junjie Wang,Yuekai Huang,Jie Huang,Zhiyuan Chang,Zhaoyang Li,Qing Wang*

Main category: cs.CV

TL;DR: RepMD detects harmful memes by identifying invariant design concepts behind shifting memes, using a Design Concept Graph to guide MLLM detection.


<details>
  <summary>Details</summary>
Motivation: Harmful memes constantly evolve in type and time, making detection difficult. However, different memes share invariant design principles from malicious users, which can help analyze why they're harmful.

Method: 1. Define Design Concept Graph (DCG) based on attack tree to describe steps for designing harmful memes. 2. Derive DCG from historical memes via design step reproduction and graph pruning. 3. Use DCG to guide Multimodal Large Language Model (MLLM) for harmful meme detection.

Result: Achieves 81.1% accuracy, maintains performance on type-shifting and temporal-evolving memes. Human evaluation shows improved efficiency (15-30 seconds per meme for human discovery).

Conclusion: RepMD effectively detects harmful memes by capturing invariant design concepts, providing robust detection against evolving meme threats while improving human analysis efficiency.

Abstract: Harmful memes are ever-shifting in the Internet communities, which are difficult to analyze due to their type-shifting and temporal-evolving nature. Although these memes are shifting, we find that different memes may share invariant principles, i.e., the underlying design concept of malicious users, which can help us analyze why these memes are harmful. In this paper, we propose RepMD, an ever-shifting harmful meme detection method based on the design concept reproduction. We first refer to the attack tree to define the Design Concept Graph (DCG), which describes steps that people may take to design a harmful meme. Then, we derive the DCG from historical memes with design step reproduction and graph pruning. Finally, we use DCG to guide the Multimodal Large Language Model (MLLM) to detect harmful memes. The evaluation results show that RepMD achieves the highest accuracy with 81.1% and has slight accuracy decreases when generalized to type-shifting and temporal-evolving memes. Human evaluation shows that RepMD can improve the efficiency of human discovery on harmful memes, with 15$\sim$30 seconds per meme.

</details>


### [114] [3D Conditional Image Synthesis of Left Atrial LGE MRI from Composite Semantic Masks](https://arxiv.org/abs/2601.04588)
*Yusri Al-Sanaani,Rebecca Thornhill,Sreeraman Rajan*

Main category: cs.CV

TL;DR: 3D conditional generative models (especially SPADE-LDM) create realistic synthetic LGE MRI images from semantic labels, improving LA segmentation performance when used for data augmentation.


<details>
  <summary>Details</summary>
Motivation: Segmentation of left atrial wall and endocardium from LGE MRI is crucial for quantifying atrial fibrosis, but limited training data and complex anatomy make accurate machine learning models challenging to develop.

Method: Developed a pipeline to synthesize 3D LGE MRI volumes from composite semantic label maps using three 3D conditional generators: Pix2Pix GAN, SPADE-GAN, and SPADE-LDM. Synthetic images were evaluated for realism and used to augment training data for LA segmentation with a 3D U-Net model.

Result: SPADE-LDM generated the most realistic images (FID: 4.063 vs 40.821 for Pix2Pix and 7.652 for SPADE-GAN). When augmented with synthetic images, LA cavity segmentation Dice score improved from 0.908 to 0.936 (statistically significant, p < 0.05).

Conclusion: Label-conditioned 3D synthesis, particularly using SPADE-LDM, effectively enhances segmentation of under-represented cardiac structures by generating realistic training data, demonstrating the potential of generative models for medical image analysis.

Abstract: Segmentation of the left atrial (LA) wall and endocardium from late gadolinium-enhanced (LGE) MRI is essential for quantifying atrial fibrosis in patients with atrial fibrillation. The development of accurate machine learning-based segmentation models remains challenging due to the limited availability of data and the complexity of anatomical structures. In this work, we investigate 3D conditional generative models as potential solution for augmenting scarce LGE training data and improving LA segmentation performance. We develop a pipeline to synthesize high-fidelity 3D LGE MRI volumes from composite semantic label maps combining anatomical expert annotations with unsupervised tissue clusters, using three 3D conditional generators (Pix2Pix GAN, SPADE-GAN, and SPADE-LDM). The synthetic images are evaluated for realism and their impact on downstream LA segmentation. SPADE-LDM generates the most realistic and structurally accurate images, achieving an FID of 4.063 and surpassing GAN models, which have FIDs of 40.821 and 7.652 for Pix2Pix and SPADE-GAN, respectively. When augmented with synthetic LGE images, the Dice score for LA cavity segmentation with a 3D U-Net model improved from 0.908 to 0.936, showing a statistically significant improvement (p < 0.05) over the baseline.These findings demonstrate the potential of label-conditioned 3D synthesis to enhance the segmentation of under-represented cardiac structures.

</details>


### [115] [MiLDEdit: Reasoning-Based Multi-Layer Design Document Editing](https://arxiv.org/abs/2601.04589)
*Zihao Lin,Wanrong Zhu,Jiuxiang Gu,Jihyung Kil,Christopher Tensmeyer,Lin Zhang,Shilong Liu,Ruiyi Zhang,Lifu Huang,Vlad I. Morariu,Tong Sun*

Main category: cs.CV

TL;DR: MiLDEAgent is a reasoning-based framework for multi-layer document editing that combines RL-trained multimodal reasoning with targeted image editing, outperforming existing approaches on a new benchmark.


<details>
  <summary>Details</summary>
Motivation: Real-world design documents are multi-layered (decoration, text, images), but prior work overlooks multi-layer editing, focusing on single-layer image editing or multi-layer generation which lack the reasoning needed to identify relevant layers and coordinate modifications.

Method: MiLDEAgent combines an RL-trained multimodal reasoner for layer-wise understanding with an image editor for targeted modifications. The framework is evaluated on MiLDEBench (20K+ design documents with editing instructions) using MiLDEEval protocol across four dimensions.

Result: Existing approaches fail to generalize: open-source models often cannot complete tasks, while closed-source models suffer from format violations. MiLDEAgent achieves strong layer-aware reasoning and precise editing, outperforming all open-source baselines and attaining performance comparable to closed-source models.

Conclusion: MiLDEAgent establishes the first strong baseline for multi-layer document editing, demonstrating the importance of layer-aware reasoning for real-world design document editing tasks.

Abstract: Real-world design documents (e.g., posters) are inherently multi-layered, combining decoration, text, and images. Editing them from natural-language instructions requires fine-grained, layer-aware reasoning to identify relevant layers and coordinate modifications. Prior work largely overlooks multi-layer design document editing, focusing instead on single-layer image editing or multi-layer generation, which assume a flat canvas and lack the reasoning needed to determine what and where to modify. To address this gap, we introduce the Multi-Layer Document Editing Agent (MiLDEAgent), a reasoning-based framework that combines an RL-trained multimodal reasoner for layer-wise understanding with an image editor for targeted modifications. To systematically benchmark this setting, we introduce the MiLDEBench, a human-in-the-loop corpus of over 20K design documents paired with diverse editing instructions. The benchmark is complemented by a task-specific evaluation protocol, MiLDEEval, which spans four dimensions including instruction following, layout consistency, aesthetics, and text rendering. Extensive experiments on 14 open-source and 2 closed-source models reveal that existing approaches fail to generalize: open-source models often cannot complete multi-layer document editing tasks, while closed-source models suffer from format violations. In contrast, MiLDEAgent achieves strong layer-aware reasoning and precise editing, significantly outperforming all open-source baselines and attaining performance comparable to closed-source models, thereby establishing the first strong baseline for multi-layer document editing.

</details>


### [116] [Detection of Deployment Operational Deviations for Safety and Security of AI-Enabled Human-Centric Cyber Physical Systems](https://arxiv.org/abs/2601.04605)
*Bernard Ngabonziza,Ayan Banerjee,Sandeep K. S. Gupta*

Main category: cs.CV

TL;DR: This paper proposes a framework for evaluating safety/security strategies in AI-enabled human-centric cyber-physical systems, focusing on operational deviations in uncertain conditions, with a case study on meal detection in diabetes management.


<details>
  <summary>Details</summary>
Motivation: AI-enabled human-centric systems (medical monitoring, autonomous cars) face operational uncertainties when interacting with humans, potentially violating safety/security requirements. Systems may encounter unknown conditions where standard protocols don't apply.

Method: 1) Analyze operational deviations leading to uncertain conditions; 2) Create evaluation framework for safety/security strategies; 3) Demonstrate with personalized image-based technique for detecting unannounced meals in closed-loop blood glucose control for Type 1 diabetics.

Result: The paper presents a framework for evaluating safety strategies and demonstrates a novel image-based technique for meal detection in diabetes management, addressing operational uncertainties in human-AI interactions.

Conclusion: A systematic approach is needed to handle operational deviations in AI-enabled human-centric systems, with the proposed framework and case study showing practical methods for maintaining safety/security in uncertain operational conditions.

Abstract: In recent years, Human-centric cyber-physical systems have increasingly involved artificial intelligence to enable knowledge extraction from sensor-collected data. Examples include medical monitoring and control systems, as well as autonomous cars. Such systems are intended to operate according to the protocols and guidelines for regular system operations. However, in many scenarios, such as closed-loop blood glucose control for Type 1 diabetics, self-driving cars, and monitoring systems for stroke diagnosis. The operations of such AI-enabled human-centric applications can expose them to cases for which their operational mode may be uncertain, for instance, resulting from the interactions with a human with the system. Such cases, in which the system is in uncertain conditions, can violate the system's safety and security requirements. 
  This paper will discuss operational deviations that can lead these systems to operate in unknown conditions. We will then create a framework to evaluate different strategies for ensuring the safety and security of AI-enabled human-centric cyber-physical systems in operation deployment. Then, as an example, we show a personalized image-based novel technique for detecting the non-announcement of meals in closed-loop blood glucose control for Type 1 diabetics.

</details>


### [117] [HUR-MACL: High-Uncertainty Region-Guided Multi-Architecture Collaborative Learning for Head and Neck Multi-Organ Segmentation](https://arxiv.org/abs/2601.04607)
*Xiaoyu Liu,Siwen Wei,Linhao Qu,Mingyuan Pan,Chengsheng Zhang,Yonghong Shi,Zhijian Song*

Main category: cs.CV

TL;DR: HUR-MACL model improves head & neck organ segmentation by adaptively identifying high-uncertainty regions and using Vision Mamba + Deformable CNN collaboration with feature distillation.


<details>
  <summary>Details</summary>
Motivation: Current deep learning models struggle with small, complexly shaped organs in head & neck segmentation. Hybrid architectures often just concatenate features without exploiting each component's unique strengths, leading to functional overlap and limited accuracy.

Method: Proposes HUR-MACL: 1) Adaptively identifies high uncertainty regions using CNN, 2) For these regions, uses Vision Mamba and Deformable CNN jointly to improve segmentation, 3) Introduces heterogeneous feature distillation loss to promote collaborative learning between architectures in high uncertainty regions.

Result: Achieves state-of-the-art results on two public datasets and one private dataset for multi-organ segmentation in head and neck.

Conclusion: The proposed HUR-MACL model effectively addresses limitations of existing hybrid architectures by adaptively focusing on challenging regions and enabling collaborative learning between different architectures, leading to superior segmentation performance.

Abstract: Accurate segmentation of organs at risk in the head and neck is essential for radiation therapy, yet deep learning models often fail on small, complexly shaped organs. While hybrid architectures that combine different models show promise, they typically just concatenate features without exploiting the unique strengths of each component. This results in functional overlap and limited segmentation accuracy. To address these issues, we propose a high uncertainty region-guided multi-architecture collaborative learning (HUR-MACL) model for multi-organ segmentation in the head and neck. This model adaptively identifies high uncertainty regions using a convolutional neural network, and for these regions, Vision Mamba as well as Deformable CNN are utilized to jointly improve their segmentation accuracy. Additionally, a heterogeneous feature distillation loss was proposed to promote collaborative learning between the two architectures in high uncertainty regions to further enhance performance. Our method achieves SOTA results on two public datasets and one private dataset.

</details>


### [118] [HyperAlign: Hyperbolic Entailment Cones for Adaptive Text-to-Image Alignment Assessment](https://arxiv.org/abs/2601.04614)
*Wenzhi Chen,Bo Hu,Leida Li,Lihuo He,Wen Lu,Xinbo Gao*

Main category: cs.CV

TL;DR: HyperAlign: A hyperbolic geometry-based framework for adaptive assessment of text-to-image alignment, outperforming existing Euclidean methods.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-image alignment assessment methods rely on Euclidean space metrics, which neglect the structured nature of semantic alignment and lack adaptive capabilities for different samples.

Method: 1) Extract CLIP features and map them to hyperbolic space; 2) Design dynamic-supervision entailment modeling to transform discrete entailment logic into continuous geometric structure supervision; 3) Use adaptive modulation regressor with hyperbolic features to generate sample-level modulation parameters for calibrating Euclidean cosine similarity.

Result: HyperAlign achieves highly competitive performance on both single database evaluation and cross-database generalization tasks.

Conclusion: The hyperbolic geometric modeling approach is effective for image-text alignment assessment, addressing limitations of existing Euclidean methods.

Abstract: With the rapid development of text-to-image generation technology, accurately assessing the alignment between generated images and text prompts has become a critical challenge. Existing methods rely on Euclidean space metrics, neglecting the structured nature of semantic alignment, while lacking adaptive capabilities for different samples. To address these limitations, we propose HyperAlign, an adaptive text-to-image alignment assessment framework based on hyperbolic entailment geometry. First, we extract Euclidean features using CLIP and map them to hyperbolic space. Second, we design a dynamic-supervision entailment modeling mechanism that transforms discrete entailment logic into continuous geometric structure supervision. Finally, we propose an adaptive modulation regressor that utilizes hyperbolic geometric features to generate sample-level modulation parameters, adaptively calibrating Euclidean cosine similarity to predict the final score. HyperAlign achieves highly competitive performance on both single database evaluation and cross-database generalization tasks, fully validating the effectiveness of hyperbolic geometric modeling for image-text alignment assessment.

</details>


### [119] [Agri-R1: Empowering Generalizable Agricultural Reasoning in Vision-Language Models with Reinforcement Learning](https://arxiv.org/abs/2601.04672)
*Wentao Zhang,Lifei Wang,Lina Lu,MingKun Xu,Shangyang Li,Yanchao Yang,Tao Fang*

Main category: cs.CV

TL;DR: Agri-R1: A 3B-parameter reasoning-enhanced vision-language model for agricultural disease diagnosis that uses automated reasoning data generation and Group Relative Policy Optimization to achieve competitive performance with larger models while requiring only 19% of available samples.


<details>
  <summary>Details</summary>
Motivation: Agricultural disease diagnosis challenges VLMs due to: 1) conventional fine-tuning requiring extensive labels, 2) lack of interpretability, 3) poor generalization, and 4) existing reasoning methods relying on costly expert annotations while failing to address open-ended, diverse agricultural queries.

Method: 1) Automated high-quality reasoning data generation via vision-language synthesis and LLM-based filtering (using only 19% of samples). 2) Training with Group Relative Policy Optimization (GRPO) with novel reward function integrating domain-specific lexicons and fuzzy matching to assess correctness and linguistic flexibility in open-ended responses.

Result: 3B-parameter model achieves performance competitive with 7B- to 13B-parameter baselines: +23.2% relative gain in disease recognition accuracy, +33.3% in agricultural knowledge QA, +26.10-point improvement in cross-domain generalization over standard fine-tuning. Benefits scale with question complexity.

Conclusion: The synergy between structured reasoning data and GRPO-driven exploration enables efficient, high-performance agricultural disease diagnosis with strong generalization capabilities, demonstrating that smaller models can compete with larger ones through reasoning-enhanced training.

Abstract: Agricultural disease diagnosis challenges VLMs, as conventional fine-tuning requires extensive labels, lacks interpretability, and generalizes poorly. While reasoning improves model robustness, existing methods rely on costly expert annotations and rarely address the open-ended, diverse nature of agricultural queries. To address these limitations, we propose \textbf{Agri-R1}, a reasoning-enhanced large model for agriculture. Our framework automates high-quality reasoning data generation via vision-language synthesis and LLM-based filtering, using only 19\% of available samples. Training employs Group Relative Policy Optimization (GRPO) with a novel proposed reward function that integrates domain-specific lexicons and fuzzy matching to assess both correctness and linguistic flexibility in open-ended responses. Evaluated on CDDMBench, our resulting 3B-parameter model achieves performance competitive with 7B- to 13B-parameter baselines, showing a +23.2\% relative gain in disease recognition accuracy, +33.3\% in agricultural knowledge QA, and a +26.10-point improvement in cross-domain generalization over standard fine-tuning. Ablation studies confirm that the synergy between structured reasoning data and GRPO-driven exploration underpins these gains, with benefits scaling as question complexity increases.

</details>


### [120] [DB-MSMUNet:Dual Branch Multi-scale Mamba UNet for Pancreatic CT Scans Segmentation](https://arxiv.org/abs/2601.04676)
*Qiu Guan,Zhiqiang Yang,Dezhang Ye,Yang Chen,Xinli Xu,Ying Tang*

Main category: cs.CV

TL;DR: DB-MSMUNet: A dual-branch multi-scale Mamba UNet architecture for robust pancreatic and lesion segmentation in CT scans, achieving state-of-the-art performance across multiple datasets.


<details>
  <summary>Details</summary>
Motivation: Pancreatic segmentation in CT scans is challenging due to low tissue contrast, blurry boundaries, irregular organ shapes, and small lesion sizes, requiring improved methods for accurate diagnosis and treatment of pancreatic cancer.

Method: Proposes DB-MSMUNet with Multi-scale Mamba Module encoder combining deformable convolutions and state space modeling, dual decoders (edge decoder with Edge Enhancement Path and area decoder with Multi-layer Decoder), and Auxiliary Deep Supervision heads for multi-scale feature enhancement.

Result: Achieves Dice Similarity Coefficients of 89.47% (NIH), 87.59% (MSD), and 89.02% (clinical tumor dataset), outperforming most existing methods in segmentation accuracy, edge preservation, and robustness.

Conclusion: DB-MSMUNet demonstrates effectiveness and generalizability for real-world pancreatic CT segmentation tasks, addressing key challenges through its novel architecture design.

Abstract: Accurate segmentation of the pancreas and its lesions in CT scans is crucial for the precise diagnosis and treatment of pancreatic cancer. However, it remains a highly challenging task due to several factors such as low tissue contrast with surrounding organs, blurry anatomical boundaries, irregular organ shapes, and the small size of lesions. To tackle these issues, we propose DB-MSMUNet (Dual-Branch Multi-scale Mamba UNet), a novel encoder-decoder architecture designed specifically for robust pancreatic segmentation. The encoder is constructed using a Multi-scale Mamba Module (MSMM), which combines deformable convolutions and multi-scale state space modeling to enhance both global context modeling and local deformation adaptation. The network employs a dual-decoder design: the edge decoder introduces an Edge Enhancement Path (EEP) to explicitly capture boundary cues and refine fuzzy contours, while the area decoder incorporates a Multi-layer Decoder (MLD) to preserve fine-grained details and accurately reconstruct small lesions by leveraging multi-scale deep semantic features. Furthermore, Auxiliary Deep Supervision (ADS) heads are added at multiple scales to both decoders, providing more accurate gradient feedback and further enhancing the discriminative capability of multi-scale features. We conduct extensive experiments on three datasets: the NIH Pancreas dataset, the MSD dataset, and a clinical pancreatic tumor dataset provided by collaborating hospitals. DB-MSMUNet achieves Dice Similarity Coefficients of 89.47%, 87.59%, and 89.02%, respectively, outperforming most existing state-of-the-art methods in terms of segmentation accuracy, edge preservation, and robustness across different datasets. These results demonstrate the effectiveness and generalizability of the proposed method for real-world pancreatic CT segmentation tasks.

</details>


### [121] [HATIR: Heat-Aware Diffusion for Turbulent Infrared Video Super-Resolution](https://arxiv.org/abs/2601.04682)
*Yang Zou,Xingyue Zhu,Kaiqi Han,Jun Ma,Xingyuan Li,Zhiying Jiang,Jinyuan Liu*

Main category: cs.CV

TL;DR: HATIR is a diffusion-based method for infrared video super-resolution that jointly models turbulence mitigation and resolution enhancement using heat-aware deformation priors and phasor-guided flow estimation.


<details>
  <summary>Details</summary>
Motivation: Infrared video suffers from severe atmospheric turbulence and compression degradation, but existing VSR methods either ignore the modality gap between infrared/visible images or fail to restore turbulence-induced distortions. Cascading turbulence mitigation with VSR leads to error propagation due to decoupled degradation modeling.

Method: HATIR injects heat-aware deformation priors into diffusion sampling to jointly model inverse processes of turbulent degradation and detail loss. It uses a Phasor-Guided Flow Estimator based on thermal consistency principles and a Turbulence-Aware Decoder with selective temporal cue suppression and edge-aware feature aggregation via turbulence gating and structure-aware attention.

Result: The authors built FLIR-IVSR, the first dataset for turbulent infrared VSR with paired LR-HR sequences from FLIR T1050sc camera (1024x768) spanning 640 diverse scenes with varying motion conditions, enabling future research in this area.

Conclusion: HATIR provides an effective framework for joint turbulence mitigation and super-resolution in infrared video by leveraging physical thermal properties and diffusion modeling, with the new FLIR-IVSR dataset facilitating further research in infrared VSR.

Abstract: Infrared video has been of great interest in visual tasks under challenging environments, but often suffers from severe atmospheric turbulence and compression degradation. Existing video super-resolution (VSR) methods either neglect the inherent modality gap between infrared and visible images or fail to restore turbulence-induced distortions. Directly cascading turbulence mitigation (TM) algorithms with VSR methods leads to error propagation and accumulation due to the decoupled modeling of degradation between turbulence and resolution. We introduce HATIR, a Heat-Aware Diffusion for Turbulent InfraRed Video Super-Resolution, which injects heat-aware deformation priors into the diffusion sampling path to jointly model the inverse process of turbulent degradation and structural detail loss. Specifically, HATIR constructs a Phasor-Guided Flow Estimator, rooted in the physical principle that thermally active regions exhibit consistent phasor responses over time, enabling reliable turbulence-aware flow to guide the reverse diffusion process. To ensure the fidelity of structural recovery under nonuniform distortions, a Turbulence-Aware Decoder is proposed to selectively suppress unstable temporal cues and enhance edge-aware feature aggregation via turbulence gating and structure-aware attention. We built FLIR-IVSR, the first dataset for turbulent infrared VSR, comprising paired LR-HR sequences from a FLIR T1050sc camera (1024 X 768) spanning 640 diverse scenes with varying camera and object motion conditions. This encourages future research in infrared VSR. Project page: https://github.com/JZ0606/HATIR

</details>


### [122] [WebCryptoAgent: Agentic Crypto Trading with Web Informatics](https://arxiv.org/abs/2601.04687)
*Ali Kurban,Wei Luo,Liangyu Zuo,Zeyu Zhang,Renda Han,Zhaolu Kang,Hao Tang*

Main category: cs.CV

TL;DR: WebCryptoAgent: An agentic trading framework that decomposes web-informed cryptocurrency decision making into modality-specific agents and uses decoupled control architecture for real-time risk management.


<details>
  <summary>Details</summary>
Motivation: Cryptocurrency trading needs timely integration of heterogeneous web information and market microstructure signals for short-horizon decisions under extreme volatility. Existing systems struggle with jointly reasoning over noisy multi-source web evidence while maintaining robustness to rapid price shocks at sub-second timescales.

Method: Proposes WebCryptoAgent with two key components: 1) modality-specific agents for synthesizing unstructured web content, social sentiment, and structured OHLCV signals into a unified evidence document for confidence-calibrated reasoning, and 2) decoupled control architecture separating strategic hourly reasoning from real-time second-level risk model for fast shock detection and protective intervention.

Result: Extensive experiments on real-world cryptocurrency markets demonstrate improved trading stability, reduced spurious activity, and enhanced tail-risk handling compared to existing baselines.

Conclusion: WebCryptoAgent effectively addresses the challenges of web-informed cryptocurrency trading by combining modality-specific evidence synthesis with decoupled risk control, enabling both informed decision making and robust response to market shocks.

Abstract: Cryptocurrency trading increasingly depends on timely integration of heterogeneous web information and market microstructure signals to support short-horizon decision making under extreme volatility. However, existing trading systems struggle to jointly reason over noisy multi-source web evidence while maintaining robustness to rapid price shocks at sub-second timescales. The first challenge lies in synthesizing unstructured web content, social sentiment, and structured OHLCV signals into coherent and interpretable trading decisions without amplifying spurious correlations, while the second challenge concerns risk control, as slow deliberative reasoning pipelines are ill-suited for handling abrupt market shocks that require immediate defensive responses. To address these challenges, we propose WebCryptoAgent, an agentic trading framework that decomposes web-informed decision making into modality-specific agents and consolidates their outputs into a unified evidence document for confidence-calibrated reasoning. We further introduce a decoupled control architecture that separates strategic hourly reasoning from a real-time second-level risk model, enabling fast shock detection and protective intervention independent of the trading loop. Extensive experiments on real-world cryptocurrency markets demonstrate that WebCryptoAgent improves trading stability, reduces spurious activity, and enhances tail-risk handling compared to existing baselines. Code will be available at https://github.com/AIGeeksGroup/WebCryptoAgent.

</details>


### [123] [Forge-and-Quench: Enhancing Image Generation for Higher Fidelity in Unified Multimodal Models](https://arxiv.org/abs/2601.04706)
*Yanbing Zeng,Jia Wang,Hanghang Ma,Junqiang Wu,Jie Zhu,Xiaoming Wei,Jie Hu*

Main category: cs.CV

TL;DR: Forge-and-Quench is a unified framework that enhances image generation by leveraging multimodal understanding models to improve fidelity and detail richness through a Bridge Feature mechanism.


<details>
  <summary>Details</summary>
Motivation: Previous works haven't fully explored how understanding models can effectively assist image generation. The paper aims to leverage multimodal understanding to enhance the fidelity and detail richness of generated images, moving beyond just using reasoning abilities and world knowledge.

Method: Proposes Forge-and-Quench framework where: 1) MLLM reasons over conversational context to produce enhanced text instructions, 2) Bridge Adapter maps these to virtual visual representations called Bridge Features, 3) These features are injected into T2I backbones as visual guidance alongside enhanced text instructions.

Result: The framework shows exceptional extensibility and flexibility, enabling efficient migration across different MLLM and T2I models with significant training overhead savings. Experiments demonstrate significant improvements in image fidelity and detail across multiple models while maintaining instruction-following accuracy and enhancing world knowledge application.

Conclusion: Forge-and-Quench successfully integrates understanding and generation by using Bridge Features to forge insights from understanding models to refine the generation process, achieving better image quality without compromising multimodal understanding capabilities.

Abstract: Integrating image generation and understanding into a single framework has become a pivotal goal in the multimodal domain. However, how understanding can effectively assist generation has not been fully explored. Unlike previous works that focus on leveraging reasoning abilities and world knowledge from understanding models, this paper introduces a novel perspective: leveraging understanding to enhance the fidelity and detail richness of generated images. To this end, we propose Forge-and-Quench, a new unified framework that puts this principle into practice. In the generation process of our framework, an MLLM first reasons over the entire conversational context, including text instructions, to produce an enhanced text instruction. This refined instruction is then mapped to a virtual visual representation, termed the Bridge Feature, via a novel Bridge Adapter. This feature acts as a crucial link, forging insights from the understanding model to quench and refine the generation process. It is subsequently injected into the T2I backbone as a visual guidance signal, alongside the enhanced text instruction that replaces the original input. To validate this paradigm, we conduct comprehensive studies on the design of the Bridge Feature and Bridge Adapter. Our framework demonstrates exceptional extensibility and flexibility, enabling efficient migration across different MLLM and T2I models with significant savings in training overhead, all without compromising the MLLM's inherent multimodal understanding capabilities. Experiments show that Forge-and-Quench significantly improves image fidelity and detail across multiple models, while also maintaining instruction-following accuracy and enhancing world knowledge application. Models and codes are available at https://github.com/YanbingZeng/Forge-and-Quench.

</details>


### [124] [On the Holistic Approach for Detecting Human Image Forgery](https://arxiv.org/abs/2601.04715)
*Xiao Guo,Jie Zhu,Anil Jain,Xiaoming Liu*

Main category: cs.CV

TL;DR: HuForDet is a holistic framework for detecting human image forgeries that combines face forgery detection with full-body semantic consistency analysis using a dual-branch architecture.


<details>
  <summary>Details</summary>
Motivation: Existing deepfake detection methods are fragmented, specializing either in facial forgeries or full-body synthetic images, failing to generalize across the full spectrum of human image manipulations as AIGC threats escalate.

Method: Dual-branch architecture: (1) Face forgery detection branch with heterogeneous experts in RGB and frequency domains, including adaptive Laplacian-of-Gaussian module; (2) Contextualized forgery detection branch using MLLM for full-body semantic consistency analysis with confidence estimation for dynamic feature fusion weighting.

Result: HuForDet achieves state-of-the-art forgery detection performance and superior robustness across diverse human image forgeries, demonstrated through extensive experiments on the curated HuFor dataset.

Conclusion: The proposed holistic framework effectively addresses the fragmentation in existing methods by combining face-specific and full-body analysis, providing comprehensive detection across various human image manipulation types.

Abstract: The rapid advancement of AI-generated content (AIGC) has escalated the threat of deepfakes, from facial manipulations to the synthesis of entire photorealistic human bodies. However, existing detection methods remain fragmented, specializing either in facial-region forgeries or full-body synthetic images, and consequently fail to generalize across the full spectrum of human image manipulations. We introduce HuForDet, a holistic framework for human image forgery detection, which features a dual-branch architecture comprising: (1) a face forgery detection branch that employs heterogeneous experts operating in both RGB and frequency domains, including an adaptive Laplacian-of-Gaussian (LoG) module designed to capture artifacts ranging from fine-grained blending boundaries to coarse-scale texture irregularities; and (2) a contextualized forgery detection branch that leverages a Multi-Modal Large Language Model (MLLM) to analyze full-body semantic consistency, enhanced with a confidence estimation mechanism that dynamically weights its contribution during feature fusion. We curate a human image forgery (HuFor) dataset that unifies existing face forgery data with a new corpus of full-body synthetic humans. Extensive experiments show that our HuForDet achieves state-of-the-art forgery detection performance and superior robustness across diverse human image forgeries.

</details>


### [125] [Training a Custom CNN on Five Heterogeneous Image Datasets](https://arxiv.org/abs/2601.04727)
*Anika Tabassum,Tasnuva Mahazabin Tuba,Nafisa Naznin*

Main category: cs.CV

TL;DR: This paper evaluates CNN architectures across five diverse visual classification tasks, comparing custom lightweight CNNs with established models like ResNet-18 and VGG-16, analyzing the impact of transfer learning in data-constrained environments.


<details>
  <summary>Details</summary>
Motivation: To investigate the effectiveness of CNN-based architectures across heterogeneous real-world datasets with varying challenges (illumination, resolution, environmental complexity, class imbalance), and provide practical insights for deploying deep learning in resource-limited yet high-impact visual classification tasks.

Method: Evaluated a lightweight custom CNN alongside established architectures (ResNet-18, VGG-16) trained both from scratch and using transfer learning. Applied systematic preprocessing, augmentation, and controlled experimentation across five datasets spanning agricultural and urban domains: mango variety classification, paddy variety identification, road surface condition assessment, auto-rickshaw detection, and footpath encroachment monitoring.

Result: Developed an efficient custom CNN achieving competitive performance across multiple domains, and conducted comprehensive comparative analysis showing when transfer learning and deep architectures provide substantial advantages, particularly in data-constrained environments.

Conclusion: The findings offer practical insights for deploying deep learning models in resource-limited real-world visual classification tasks, demonstrating the value of both lightweight custom architectures and transfer learning approaches depending on dataset characteristics and constraints.

Abstract: Deep learning has transformed visual data analysis, with Convolutional Neural Networks (CNNs) becoming highly effective in learning meaningful feature representations directly from images. Unlike traditional manual feature engineering methods, CNNs automatically extract hierarchical visual patterns, enabling strong performance across diverse real-world contexts. This study investigates the effectiveness of CNN-based architectures across five heterogeneous datasets spanning agricultural and urban domains: mango variety classification, paddy variety identification, road surface condition assessment, auto-rickshaw detection, and footpath encroachment monitoring. These datasets introduce varying challenges, including differences in illumination, resolution, environmental complexity, and class imbalance, necessitating adaptable and robust learning models.
  We evaluate a lightweight, task-specific custom CNN alongside established deep architectures, including ResNet-18 and VGG-16, trained both from scratch and using transfer learning. Through systematic preprocessing, augmentation, and controlled experimentation, we analyze how architectural complexity, model depth, and pre-training influence convergence, generalization, and performance across datasets of differing scale and difficulty. The key contributions of this work are: (1) the development of an efficient custom CNN that achieves competitive performance across multiple application domains, and (2) a comprehensive comparative analysis highlighting when transfer learning and deep architectures provide substantial advantages, particularly in data-constrained environments. These findings offer practical insights for deploying deep learning models in resource-limited yet high-impact real-world visual classification tasks.

</details>


### [126] [AIVD: Adaptive Edge-Cloud Collaboration for Accurate and Efficient Industrial Visual Detection](https://arxiv.org/abs/2601.04734)
*Yunqing Hu,Zheming Yang,Chang Zhao,Qi Guo,Meng Gao,Pengcheng Li,Wen Ji*

Main category: cs.CV

TL;DR: AIVD framework enables precise object localization and semantic generation by combining lightweight edge detectors with cloud MLLMs, using efficient fine-tuning and dynamic scheduling for edge-cloud deployment.


<details>
  <summary>Details</summary>
Motivation: MLLMs have strong semantic understanding but struggle with precise object localization and face deployment challenges in resource-constrained edge-cloud environments.

Method: Proposes AIVD framework with: 1) lightweight edge detectors for localization, 2) cloud MLLMs for semantic generation, 3) visual-semantic collaborative augmentation fine-tuning strategy to handle edge noise, and 4) heterogeneous resource-aware dynamic scheduling algorithm.

Result: AIVD reduces resource consumption while improving MLLM classification accuracy and semantic generation quality. The scheduling strategy achieves higher throughput and lower latency across diverse scenarios.

Conclusion: The AIVD framework successfully addresses MLLM limitations in precise localization and edge-cloud deployment through collaborative edge-cloud architecture and adaptive resource management.

Abstract: Multimodal large language models (MLLMs) demonstrate exceptional capabilities in semantic understanding and visual reasoning, yet they still face challenges in precise object localization and resource-constrained edge-cloud deployment. To address this, this paper proposes the AIVD framework, which achieves unified precise localization and high-quality semantic generation through the collaboration between lightweight edge detectors and cloud-based MLLMs. To enhance the cloud MLLM's robustness against edge cropped-box noise and scenario variations, we design an efficient fine-tuning strategy with visual-semantic collaborative augmentation, significantly improving classification accuracy and semantic consistency. Furthermore, to maintain high throughput and low latency across heterogeneous edge devices and dynamic network conditions, we propose a heterogeneous resource-aware dynamic scheduling algorithm. Experimental results demonstrate that AIVD substantially reduces resource consumption while improving MLLM classification performance and semantic generation quality. The proposed scheduling strategy also achieves higher throughput and lower latency across diverse scenarios.

</details>


### [127] [Skeletonization-Based Adversarial Perturbations on Large Vision Language Model's Mathematical Text Recognition](https://arxiv.org/abs/2601.04752)
*Masatomo Yoshida,Haruto Namura,Nicola Adami,Masahiro Okuda*

Main category: cs.CV

TL;DR: Novel adversarial attack using skeletonization to target foundation models' visual capabilities, especially on text/math formula images, with evaluation of character/semantic changes and demonstration on ChatGPT.


<details>
  <summary>Details</summary>
Motivation: To explore the visual capabilities and limitations of foundation models by developing an adversarial attack method that reveals how these models interpret and reason about visual information, particularly focusing on challenging cases like mathematical formulas with LaTeX conversion.

Method: Introduces a novel adversarial attack method that utilizes skeletonization to effectively reduce the search space for generating adversarial examples. The approach specifically targets images containing text, particularly mathematical formula images, which present additional challenges due to their LaTeX conversion requirements and intricate structure.

Result: Conducted detailed evaluation of both character-level and semantic changes between original and adversarially perturbed outputs, providing insights into models' visual interpretation and reasoning abilities. Demonstrated effectiveness through application to ChatGPT, showing practical implications in real-world scenarios.

Conclusion: The skeletonization-based adversarial attack method effectively reveals limitations in foundation models' visual capabilities, particularly for complex text/math images, with demonstrated real-world impact on systems like ChatGPT.

Abstract: This work explores the visual capabilities and limitations of foundation models by introducing a novel adversarial attack method utilizing skeletonization to reduce the search space effectively. Our approach specifically targets images containing text, particularly mathematical formula images, which are more challenging due to their LaTeX conversion and intricate structure. We conduct a detailed evaluation of both character and semantic changes between original and adversarially perturbed outputs to provide insights into the models' visual interpretation and reasoning abilities. The effectiveness of our method is further demonstrated through its application to ChatGPT, which shows its practical implications in real-world scenarios.

</details>


### [128] [ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting](https://arxiv.org/abs/2601.04754)
*Yen-Jen Chiou,Wei-Tse Cheng,Yuan-Fu Yang*

Main category: cs.CV

TL;DR: ProFuse is an efficient framework for open-vocabulary 3D scene understanding using 3D Gaussian Splatting that achieves semantic attachment in ~5 minutes (2x faster than SOTA) without render-supervised fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To enable efficient open-vocabulary 3D scene understanding with 3DGS while addressing cross-view consistency and intra-mask cohesion challenges, without requiring render-supervised fine-tuning or pretrained 3DGS scenes.

Method: Uses dense correspondence-guided pre-registration to initialize Gaussians with accurate geometry, constructs 3D Context Proposals via cross-view clustering, fuses global features onto Gaussians during direct registration, and maintains per-primitive language coherence across views without additional optimization.

Result: Achieves strong open-vocabulary 3DGS understanding with semantic attachment completed in about five minutes per scene, which is two times faster than state-of-the-art methods.

Conclusion: ProFuse provides an efficient, context-aware framework for open-vocabulary 3D scene understanding that enhances consistency and cohesion while minimizing computational overhead and eliminating the need for fine-tuning.

Abstract: We present ProFuse, an efficient context-aware framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting (3DGS). The pipeline enhances cross-view consistency and intra-mask cohesion within a direct registration setup, adding minimal overhead and requiring no render-supervised fine-tuning. Instead of relying on a pretrained 3DGS scene, we introduce a dense correspondence-guided pre-registration phase that initializes Gaussians with accurate geometry while jointly constructing 3D Context Proposals via cross-view clustering. Each proposal carries a global feature obtained through weighted aggregation of member embeddings, and this feature is fused onto Gaussians during direct registration to maintain per-primitive language coherence across views. With associations established in advance, semantic fusion requires no additional optimization beyond standard reconstruction, and the model retains geometric refinement without densification. ProFuse achieves strong open-vocabulary 3DGS understanding while completing semantic attachment in about five minutes per scene, which is two times faster than SOTA.

</details>


### [129] [Segmentation-Driven Monocular Shape from Polarization based on Physical Model](https://arxiv.org/abs/2601.04776)
*Jinyu Zhang,Xu Ma,Weili Chen,Gonzalo R. Arce*

Main category: cs.CV

TL;DR: A novel segmentation-driven monocular shape-from-polarization framework that addresses azimuth angle ambiguity by reformulating global shape recovery into local reconstructions over adaptively segmented convex sub-regions.


<details>
  <summary>Details</summary>
Motivation: Existing monocular shape-from-polarization methods suffer from azimuth angle ambiguity, an inherent limitation of polarization analysis that severely compromises reconstruction accuracy and stability. This ambiguity problem needs to be addressed to improve 3D reconstruction from single-view polarized images.

Method: Proposes a segmentation-driven monocular SfP framework with two key components: 1) Polarization-aided adaptive region growing (PARG) segmentation strategy that decomposes the global convexity assumption into locally convex regions to suppress azimuth ambiguities, and 2) Multi-scale fusion convexity prior (MFCP) constraint to ensure local surface consistency and enhance recovery of fine details.

Result: Extensive experiments on both synthetic and real-world datasets validate the approach, showing significant improvements in disambiguation accuracy and geometric fidelity compared with existing physics-based monocular SfP techniques.

Conclusion: The proposed segmentation-driven framework effectively addresses the azimuth ambiguity problem in monocular shape-from-polarization, enabling more accurate and stable 3D reconstruction from single-view polarized images through local convex region segmentation and multi-scale consistency constraints.

Abstract: Monocular shape-from-polarization (SfP) leverages the intrinsic relationship between light polarization properties and surface geometry to recover surface normals from single-view polarized images, providing a compact and robust approach for three-dimensional (3D) reconstruction. Despite its potential, existing monocular SfP methods suffer from azimuth angle ambiguity, an inherent limitation of polarization analysis, that severely compromises reconstruction accuracy and stability. This paper introduces a novel segmentation-driven monocular SfP (SMSfP) framework that reformulates global shape recovery into a set of local reconstructions over adaptively segmented convex sub-regions. Specifically, a polarization-aided adaptive region growing (PARG) segmentation strategy is proposed to decompose the global convexity assumption into locally convex regions, effectively suppressing azimuth ambiguities and preserving surface continuity. Furthermore, a multi-scale fusion convexity prior (MFCP) constraint is developed to ensure local surface consistency and enhance the recovery of fine textural and structural details. Extensive experiments on both synthetic and real-world datasets validate the proposed approach, showing significant improvements in disambiguation accuracy and geometric fidelity compared with existing physics-based monocular SfP techniques.

</details>


### [130] [GeM-VG: Towards Generalized Multi-image Visual Grounding with Multimodal Large Language Models](https://arxiv.org/abs/2601.04777)
*Shurong Zheng,Yousong Zhu,Hongyin Zhao,Fan Yang,Yufei Zhan,Ming Tang,Jinqiao Wang*

Main category: cs.CV

TL;DR: GeM-VG is a Multimodal LLM for Generalized Multi-image Visual Grounding that outperforms previous models by 2-10% on various benchmarks while maintaining strong multi-image understanding capabilities.


<details>
  <summary>Details</summary>
Motivation: Existing multi-image grounding methods are limited to single-target localization and few task types due to lack of unified modeling for generalized grounding tasks. There's a need for a model that can handle diverse multi-image grounding scenarios with robust cross-image reasoning.

Method: Proposes GeM-VG with: 1) Systematic categorization of multi-image grounding tasks by cross-image cue reliance, 2) MG-Data-240K dataset addressing target quantity and image relation limitations, 3) Hybrid reinforcement finetuning strategy combining chain-of-thought reasoning and direct answering using R1-like algorithm with rule-based rewards.

Result: Outperforms previous leading MLLMs by 2.0% on MIG-Bench and 9.7% on MC-Bench for multi-image grounding. Achieves 9.1% improvement over base model on ODINW for single-image grounding. Maintains strong general multi-image understanding capabilities.

Conclusion: GeM-VG demonstrates superior generalized grounding capabilities through unified task modeling, comprehensive dataset, and hybrid reinforcement finetuning strategy, effectively addressing limitations of previous multi-image grounding approaches.

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive progress in single-image grounding and general multi-image understanding. Recently, some methods begin to address multi-image grounding. However, they are constrained by single-target localization and limited types of practical tasks, due to the lack of unified modeling for generalized grounding tasks. Therefore, we propose GeM-VG, an MLLM capable of Generalized Multi-image Visual Grounding. To support this, we systematically categorize and organize existing multi-image grounding tasks according to their reliance of cross-image cues and reasoning, and introduce the MG-Data-240K dataset, addressing the limitations of existing datasets regarding target quantity and image relation. To tackle the challenges of robustly handling diverse multi-image grounding tasks, we further propose a hybrid reinforcement finetuning strategy that integrates chain-of-thought (CoT) reasoning and direct answering, considering their complementary strengths. This strategy adopts an R1-like algorithm guided by a carefully designed rule-based reward, effectively enhancing the model's overall perception and reasoning capabilities. Extensive experiments demonstrate the superior generalized grounding capabilities of our model. For multi-image grounding, it outperforms the previous leading MLLMs by 2.0% and 9.7% on MIG-Bench and MC-Bench, respectively. In single-image grounding, it achieves a 9.1% improvement over the base model on ODINW. Furthermore, our model retains strong capabilities in general multi-image understanding.

</details>


### [131] [CounterVid: Counterfactual Video Generation for Mitigating Action and Temporal Hallucinations in Video-Language Models](https://arxiv.org/abs/2601.04778)
*Tobia Poppi,Burak Uzkent,Amanmeet Garg,Lucas Porto,Garin Kessler,Yezhou Yang,Marcella Cornia,Lorenzo Baraldi,Rita Cucchiara,Florian Schiffers*

Main category: cs.CV

TL;DR: Counterfactual video generation framework creates semantic hard negatives to reduce hallucinations in video-language models, improving temporal reasoning through synthetic dataset and unified preference optimization.


<details>
  <summary>Details</summary>
Motivation: Video-language models suffer from hallucinations, especially in action and temporal reasoning, due to over-reliance on language priors rather than visual dynamics. Existing mitigation strategies like textual filtering or random perturbations fail to address the root cause.

Method: Proposes scalable counterfactual video generation framework using multimodal LLMs for action proposal/editing guidance and diffusion models to generate semantic hard negatives. Creates CounterVid dataset (~26k preference pairs) and introduces MixDPO, a unified Direct Preference Optimization approach combining textual and visual preferences.

Result: Fine-tuning Qwen2.5-VL with MixDPO yields consistent improvements, particularly in temporal ordering, and transfers effectively to standard video hallucination benchmarks.

Conclusion: Counterfactual video generation provides an effective approach to address hallucinations in VLMs by targeting the root cause of over-reliance on language priors, with the proposed framework and dataset enabling better temporal reasoning capabilities.

Abstract: Video-language models (VLMs) achieve strong multimodal understanding but remain prone to hallucinations, especially when reasoning about actions and temporal order. Existing mitigation strategies, such as textual filtering or random video perturbations, often fail to address the root cause: over-reliance on language priors rather than fine-grained visual dynamics. We propose a scalable framework for counterfactual video generation that synthesizes videos differing only in actions or temporal structure while preserving scene context. Our pipeline combines multimodal LLMs for action proposal and editing guidance with diffusion-based image and video models to generate semantic hard negatives at scale. Using this framework, we build CounterVid, a synthetic dataset of ~26k preference pairs targeting action recognition and temporal reasoning. We further introduce MixDPO, a unified Direct Preference Optimization approach that jointly leverages textual and visual preferences. Fine-tuning Qwen2.5-VL with MixDPO yields consistent improvements, notably in temporal ordering, and transfers effectively to standard video hallucination benchmarks. Code and models will be made publicly available.

</details>


### [132] [Defocus Aberration Theory Confirms Gaussian Model in Most Imaging Devices](https://arxiv.org/abs/2601.04779)
*Akbar Saadat*

Main category: cs.CV

TL;DR: This paper validates the Gaussian model as an accurate and reliable approximation for defocus operators in conventional imaging devices, showing less than 1% error for typical depth ranges.


<details>
  <summary>Details</summary>
Motivation: Depth estimation from 2D images is a fundamental challenge in 3D recovery. While defocus provides valuable depth information, accurately modeling defocus blur is difficult due to the ill-posed nature of distinguishing desired blur from inherent blur. A reliable defocus model is needed for practical applications.

Method: The paper introduces specific settings for conventional imaging devices to ensure defocus operators adhere to the Gaussian model. Analysis begins with geometric optics framework and uses defocus aberration theory in diffraction-limited optics to evaluate how well the actual model fits Gaussian approximation. The study examines typical depth ranges from 1 to 100 meters with maximum depth variation of 10% at focused depth.

Result: Results confirm the Gaussian model's applicability for defocus operators in most imaging devices. The maximum Mean Absolute Error (MAE) is less than 1%, demonstrating high accuracy and reliability of the Gaussian approximation for practical depth estimation applications.

Conclusion: The Gaussian model is validated as an optimal choice for defocus modeling due to its mathematical simplicity, computational efficiency, and theoretical ability to handle both absolute blur in single images and relative blur between images. The less than 1% error makes it suitable for real-time applications and most conventional imaging devices.

Abstract: Over the past three decades, defocus has consistently provided groundbreaking depth information in scene images. However, accurately estimating depth from 2D images continues to be a persistent and fundamental challenge in the field of 3D recovery. Heuristic approaches involve with the ill-posed problem for inferring the spatial variant defocusing blur, as the desired blur cannot be distinguished from the inherent blur. Given a prior knowledge of the defocus model, the problem become well-posed with an analytic solution for the relative blur between two images, taken at the same viewpoint with different camera settings for the focus. The Gaussian model stands out as an optimal choice for real-time applications, due to its mathematical simplicity and computational efficiency. And theoretically, it is the only model can be applied at the same time to both the absolute blur caused by depth in a single image and the relative blur resulting from depth differences between two images. This paper introduces the settings, for conventional imaging devices, to ensure that the defocusing operator adheres to the Gaussian model. Defocus analysis begins within the framework of geometric optics and is conducted by defocus aberration theory in diffraction-limited optics to obtain the accuracy of fitting the actual model to its Gaussian approximation. The results for a typical set of focused depths between $1$ and $100$ meters, with a maximum depth variation of $10\%$ at the focused depth, confirm the Gaussian model's applicability for defocus operators in most imaging devices. The findings demonstrate a maximum Mean Absolute Error $(\!M\!A\!E)$ of less than $1\%$, underscoring the model's accuracy and reliability.

</details>


### [133] [SRU-Pix2Pix: A Fusion-Driven Generator Network for Medical Image Translation with Few-Shot Learning](https://arxiv.org/abs/2601.04785)
*Xihe Qiu,Yang Dai,Xiaoyu Tan,Sijia Li,Fenghao Sun,Lu Gan,Liang Liu*

Main category: cs.CV

TL;DR: Enhanced Pix2Pix framework with SEResNet and U-Net++ improves MRI image translation quality and structural fidelity under few-shot conditions.


<details>
  <summary>Details</summary>
Motivation: MRI has limitations in acquisition time, cost, and resolution. While Pix2Pix has been used for medical image translation, its potential hasn't been fully explored, especially for improving image generation quality and structural fidelity.

Method: Proposed enhanced Pix2Pix framework integrating Squeeze-and-Excitation Residual Networks (SEResNet) for channel attention and U-Net++ for multi-scale feature fusion, with simplified PatchGAN discriminator for training stability.

Result: Under few-shot conditions (<500 images), the method achieves consistent structural fidelity and superior image quality across multiple intra-modality MRI translation tasks, demonstrating strong generalization ability.

Conclusion: The enhanced Pix2Pix framework provides an effective extension for medical image translation, addressing MRI limitations while maintaining high quality and structural accuracy.

Abstract: Magnetic Resonance Imaging (MRI) provides detailed tissue information, but its clinical application is limited by long acquisition time, high cost, and restricted resolution. Image translation has recently gained attention as a strategy to address these limitations. Although Pix2Pix has been widely applied in medical image translation, its potential has not been fully explored. In this study, we propose an enhanced Pix2Pix framework that integrates Squeeze-and-Excitation Residual Networks (SEResNet) and U-Net++ to improve image generation quality and structural fidelity. SEResNet strengthens critical feature representation through channel attention, while U-Net++ enhances multi-scale feature fusion. A simplified PatchGAN discriminator further stabilizes training and refines local anatomical realism. Experimental results demonstrate that under few-shot conditions with fewer than 500 images, the proposed method achieves consistent structural fidelity and superior image quality across multiple intra-modality MRI translation tasks, showing strong generalization ability. These results suggest an effective extension of Pix2Pix for medical image translation.

</details>


### [134] [Measurement-Consistent Langevin Corrector: A Remedy for Latent Diffusion Inverse Solvers](https://arxiv.org/abs/2601.04791)
*Lee Hyoseok,Sohwi Lim,Eunju Cha,Tae-Hyun Oh*

Main category: cs.CV

TL;DR: MCLC is a plug-and-play correction module that stabilizes latent diffusion inverse solvers by reducing the discrepancy between solver's and true reverse diffusion dynamics through measurement-consistent Langevin updates.


<details>
  <summary>Details</summary>
Motivation: Existing latent diffusion inverse solvers suffer from instability, exhibiting undesirable artifacts and degraded quality. The authors identify this instability as a discrepancy between the solver's and true reverse diffusion dynamics.

Method: Introduces Measurement-Consistent Langevin Corrector (MCLC), a theoretically grounded plug-and-play correction module that remedies LDM-based inverse solvers through measurement-consistent Langevin updates. Unlike prior approaches that rely on linear manifold assumptions, MCLC operates without this assumption.

Result: MCLC demonstrates effectiveness and compatibility with existing solvers across diverse image restoration tasks. The method provides more stable and reliable behavior compared to prior approaches.

Conclusion: MCLC is a key step toward more robust zero-shot inverse problem solvers, offering a solution to the instability issues in latent diffusion inverse solvers while also providing insights into blob artifacts and their underlying causes.

Abstract: With recent advances in generative models, diffusion models have emerged as powerful priors for solving inverse problems in each domain. Since Latent Diffusion Models (LDMs) provide generic priors, several studies have explored their potential as domain-agnostic zero-shot inverse solvers. Despite these efforts, existing latent diffusion inverse solvers suffer from their instability, exhibiting undesirable artifacts and degraded quality. In this work, we first identify the instability as a discrepancy between the solver's and true reverse diffusion dynamics, and show that reducing this gap stabilizes the solver. Building on this, we introduce Measurement-Consistent Langevin Corrector (MCLC), a theoretically grounded plug-and-play correction module that remedies the LDM-based inverse solvers through measurement-consistent Langevin updates. Compared to prior approaches that rely on linear manifold assumptions, which often do not hold in latent space, MCLC operates without this assumption, leading to more stable and reliable behavior. We experimentally demonstrate the effectiveness of MCLC and its compatibility with existing solvers across diverse image restoration tasks. Additionally, we analyze blob artifacts and offer insights into their underlying causes. We highlight that MCLC is a key step toward more robust zero-shot inverse problem solvers.

</details>


### [135] [PyramidalWan: On Making Pretrained Video Model Pyramidal for Efficient Inference](https://arxiv.org/abs/2601.04792)
*Denis Korzhenkov,Adil Karjauv,Animesh Karnewar,Mohsen Ghafoorian,Amirhossein Habibian*

Main category: cs.CV

TL;DR: Pyramidal diffusion models process different noise levels at different resolutions to reduce computational cost, but existing open-source models underperform. This work converts pretrained diffusion models into pyramidal ones via low-cost finetuning without quality degradation, plus explores step distillation for efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing open-source pyramidal video diffusion models are trained from scratch and underperform compared to state-of-the-art systems in visual quality. There's a need to leverage pretrained diffusion models more effectively while maintaining the computational efficiency benefits of pyramidal architectures.

Method: Developed a pipeline to convert pretrained diffusion models into pyramidal ones through low-cost finetuning. Also investigated and compared various strategies for step distillation within pyramidal models to enhance inference efficiency.

Result: Successfully transformed pretrained diffusion models into pyramidal versions without degradation in output video quality. Achieved computational efficiency benefits of pyramidal architectures while maintaining the quality of pretrained models.

Conclusion: The proposed pipeline enables efficient conversion of existing diffusion models into pyramidal architectures, offering computational savings without sacrificing quality. Step distillation strategies further enhance inference efficiency, making pyramidal models more practical for real-world applications.

Abstract: Recently proposed pyramidal models decompose the conventional forward and backward diffusion processes into multiple stages operating at varying resolutions. These models handle inputs with higher noise levels at lower resolutions, while less noisy inputs are processed at higher resolutions. This hierarchical approach significantly reduces the computational cost of inference in multi-step denoising models. However, existing open-source pyramidal video models have been trained from scratch and tend to underperform compared to state-of-the-art systems in terms of visual plausibility. In this work, we present a pipeline that converts a pretrained diffusion model into a pyramidal one through low-cost finetuning, achieving this transformation without degradation in quality of output videos. Furthermore, we investigate and compare various strategies for step distillation within pyramidal models, aiming to further enhance the inference efficiency. Our results are available at https://qualcomm-ai-research.github.io/PyramidalWan.

</details>


### [136] [Detector-Augmented SAMURAI for Long-Duration Drone Tracking](https://arxiv.org/abs/2601.04798)
*Tamara R. Lenhard,Andreas Weinmann,Hichem Snoussi,Tobias Koch*

Main category: cs.CV

TL;DR: First systematic evaluation of SAMURAI foundation model for drone tracking, with detector-augmented extension that improves robustness in urban surveillance, especially for long sequences and exit-re-entry events.


<details>
  <summary>Details</summary>
Motivation: Drone tracking is critical for surveillance but current RGB-based approaches are limited and rely on conventional motion models. Foundation models like SAMURAI show strong category-agnostic tracking in other domains but haven't been tested for drone-specific scenarios. Need to address detection dropouts and temporal inconsistencies in drone tracking.

Method: Systematic evaluation of SAMURAI's potential for drone tracking, plus a detector-augmented extension to mitigate sensitivity to bounding-box initialization and sequence length. Incorporates detector cues to enhance robustness.

Result: Detector-augmented extension significantly improves robustness in complex urban environments, especially for long-duration sequences and drone exit-re-entry events. Achieves success rate improvements up to +0.393 and FNR reductions up to -0.475 over SAMURAI's zero-shot performance.

Conclusion: SAMURAI shows promise for drone tracking, and detector augmentation effectively addresses its limitations. The approach yields consistent gains across datasets and metrics, demonstrating practical value for robust long-term drone surveillance.

Abstract: Robust long-term tracking of drone is a critical requirement for modern surveillance systems, given their increasing threat potential. While detector-based approaches typically achieve strong frame-level accuracy, they often suffer from temporal inconsistencies caused by frequent detection dropouts. Despite its practical relevance, research on RGB-based drone tracking is still limited and largely reliant on conventional motion models. Meanwhile, foundation models like SAMURAI have established their effectiveness across other domains, exhibiting strong category-agnostic tracking performance. However, their applicability in drone-specific scenarios has not been investigated yet. Motivated by this gap, we present the first systematic evaluation of SAMURAI's potential for robust drone tracking in urban surveillance settings. Furthermore, we introduce a detector-augmented extension of SAMURAI to mitigate sensitivity to bounding-box initialization and sequence length. Our findings demonstrate that the proposed extension significantly improves robustness in complex urban environments, with pronounced benefits in long-duration sequences - especially under drone exit-re-entry events. The incorporation of detector cues yields consistent gains over SAMURAI's zero-shot performance across datasets and metrics, with success rate improvements of up to +0.393 and FNR reductions of up to -0.475.

</details>


### [137] [Integrated Framework for Selecting and Enhancing Ancient Marathi Inscription Images from Stone, Metal Plate, and Paper Documents](https://arxiv.org/abs/2601.04800)
*Bapu D. Chendage,Rajivkumar S. Mente*

Main category: cs.CV

TL;DR: Proposes binarization and preprocessing techniques to enhance degraded ancient script images, improving readability through classification accuracy improvements.


<details>
  <summary>Details</summary>
Motivation: Ancient script images suffer from severe background noise, low contrast, and degradation from aging/environmental effects, making inscriptions difficult to read due to similar visual characteristics between foreground text and background.

Method: Image enhancement approach based on binarization and complementary preprocessing techniques for removing stains and enhancing unclear ancient text.

Result: Achieved classification accuracies of 55.7%, 62%, and 65.6% for stone, metal plate, and document scripts using K-NN classifier, and 53.2%, 59.5%, and 67.8% using SVM classifier.

Conclusion: The proposed enhancement method effectively improves readability of ancient Marathi inscription images across different script types and materials.

Abstract: Ancient script images often suffer from severe background noise, low contrast, and degradation caused by aging and environmental effects. In many cases, the foreground text and background exhibit similar visual characteristics, making the inscriptions difficult to read. The primary objective of image enhancement is to improve the readability of such degraded ancient images. This paper presents an image enhancement approach based on binarization and complementary preprocessing techniques for removing stains and enhancing unclear ancient text. The proposed methods are evaluated on different types of ancient scripts, including inscriptions on stone, metal plates, and historical documents. Experimental results show that the proposed approach achieves classification accuracies of 55.7%, 62%, and 65.6% for stone, metal plate, and document scripts, respectively, using the K-Nearest Neighbor (K-NN) classifier. Using the Support Vector Machine (SVM) classifier, accuracies of 53.2%, 59.5%, and 67.8% are obtained. The results demonstrate the effectiveness of the proposed enhancement method in improving the readability of ancient Marathi inscription images.

</details>


### [138] [SOVABench: A Vehicle Surveillance Action Retrieval Benchmark for Multimodal Large Language Models](https://arxiv.org/abs/2601.04824)
*Oriol Rabasseda,Zenjie Li,Kamal Nasrollahi,Sergio Escalera*

Main category: cs.CV

TL;DR: SOVABench is a new surveillance video benchmark for vehicle action discrimination, with a training-free MLLM framework that generates interpretable embeddings and achieves strong performance on challenging action recognition tasks.


<details>
  <summary>Details</summary>
Motivation: Existing video retrieval benchmarks focus on scene-level similarity but lack evaluation of action discrimination needed for surveillance applications. There's a gap in benchmarks that specifically test vehicle-related action understanding in real-world surveillance contexts.

Method: 1) Created SOVABench with real-world surveillance footage and two evaluation protocols (inter-pair for cross-action discrimination, intra-pair for temporal direction understanding). 2) Developed a training-free framework leveraging MLLMs' visual reasoning to generate interpretable embeddings from MLLM-generated descriptions for both images and videos.

Result: The framework achieves strong performance on SOVABench and outperforms contrastive Vision-Language Models on several spatial and counting benchmarks where they typically fail. Action distinctions that are intuitive for humans remain challenging for state-of-the-art vision and multimodal models.

Conclusion: SOVABench addresses the gap in surveillance-oriented video benchmarks, and the MLLM-based framework provides an effective, interpretable solution for action discrimination in surveillance applications without requiring training.

Abstract: Automatic identification of events and recurrent behavior analysis are critical for video surveillance. However, most existing content-based video retrieval benchmarks focus on scene-level similarity and do not evaluate the action discrimination required in surveillance. To address this gap, we introduce SOVABench (Surveillance Opposite Vehicle Actions Benchmark), a real-world retrieval benchmark built from surveillance footage and centered on vehicle-related actions. SOVABench defines two evaluation protocols (inter-pair and intra-pair) to assess cross-action discrimination and temporal direction understanding. Although action distinctions are generally intuitive for human observers, our experiments show that they remain challenging for state-of-the-art vision and multimodal models.
  Leveraging the visual reasoning and instruction-following capabilities of Multimodal Large Language Models (MLLMs), we present a training-free framework for producing interpretable embeddings from MLLM-generated descriptions for both images and videos. The framework achieves strong performance on SOVABench as well as on several spatial and counting benchmarks where contrastive Vision-Language Models often fail. The code, annotations, and instructions to construct the benchmark are publicly available.

</details>


### [139] [Character Detection using YOLO for Writer Identification in multiple Medieval books](https://arxiv.org/abs/2601.04834)
*Alessandra Scotto di Freca,Tiziana D Alessandro,Francesco Fontanella,Filippo Sarria,Claudio De Stefano*

Main category: cs.CV

TL;DR: Replaces template matching + CNN with YOLO for medieval scribe identification via letter detection, improving accuracy and enabling rejection thresholds for unseen manuscripts.


<details>
  <summary>Details</summary>
Motivation: Paleography needs better digital methods for scribe identification to date manuscripts and understand writing evolution. Previous template matching approach had limitations requiring manual thresholds.

Method: Uses YOLOv5 object detection to identify letter "a" instances (chosen for prevalence and distinctiveness) instead of template matching + CNN. Two-stage approach: YOLO detects letters, then classification attributes to scribes.

Result: YOLO extracts more letters than template matching, leading to more accurate classification. YOLO's confidence scores enable reliable rejection thresholds for writer identification in unseen manuscripts.

Conclusion: YOLO-based approach outperforms previous template matching method, providing better letter detection and enabling practical application to unseen medieval manuscripts through confidence-based rejection thresholds.

Abstract: Paleography is the study of ancient and historical handwriting, its key objectives include the dating of manuscripts and understanding the evolution of writing. Estimating when a document was written and tracing the development of scripts and writing styles can be aided by identifying the individual scribes who contributed to a medieval manuscript. Although digital technologies have made significant progress in this field, the general problem remains unsolved and continues to pose open challenges. ... We previously proposed an approach focused on identifying specific letters or abbreviations that characterize each writer. In that study, we considered the letter "a", as it was widely present on all pages of text and highly distinctive, according to the suggestions of expert paleographers. We used template matching techniques to detect the occurrences of the character "a" on each page and the convolutional neural network (CNN) to attribute each instance to the correct scribe. Moving from the interesting results achieved from this previous system and being aware of the limitations of the template matching technique, which requires an appropriate threshold to work, we decided to experiment in the same framework with the use of the YOLO object detection model to identify the scribe who contributed to the writing of different medieval books. We considered the fifth version of YOLO to implement the YOLO object detection model, which completely substituted the template matching and CNN used in the previous work. The experimental results demonstrate that YOLO effectively extracts a greater number of letters considered, leading to a more accurate second-stage classification. Furthermore, the YOLO confidence score provides a foundation for developing a system that applies a rejection threshold, enabling reliable writer identification even in unseen manuscripts.

</details>


### [140] [DivAS: Interactive 3D Segmentation of NeRFs via Depth-Weighted Voxel Aggregation](https://arxiv.org/abs/2601.04860)
*Ayush Pande*

Main category: cs.CV

TL;DR: DivAS is an optimization-free, interactive framework for segmenting Neural Radiance Fields (NeRFs) that uses 2D SAM masks refined with depth priors and aggregates them into 3D voxels in real-time, eliminating per-scene training.


<details>
  <summary>Details</summary>
Motivation: Existing NeRF segmentation methods are optimization-based, requiring slow per-scene training and sacrificing the zero-shot capabilities of 2D foundation models like SAM.

Method: Uses a GUI-based workflow where 2D SAM masks from user point prompts are refined with NeRF-derived depth priors, then aggregated into a unified 3D voxel grid using a custom CUDA kernel (under 200ms).

Result: Achieves segmentation quality comparable to optimization-based methods, 2-2.5x faster end-to-end, and up to an order of magnitude faster when excluding user prompting time on Mip-NeRF 360° and LLFF datasets.

Conclusion: DivAS provides an optimization-free, interactive framework for NeRF segmentation that maintains quality while offering significant speed improvements and real-time feedback.

Abstract: Existing methods for segmenting Neural Radiance Fields (NeRFs) are often optimization-based, requiring slow per-scene training that sacrifices the zero-shot capabilities of 2D foundation models. We introduce DivAS (Depth-interactive Voxel Aggregation Segmentation), an optimization-free, fully interactive framework that addresses these limitations. Our method operates via a fast GUI-based workflow where 2D SAM masks, generated from user point prompts, are refined using NeRF-derived depth priors to improve geometric accuracy and foreground-background separation. The core of our contribution is a custom CUDA kernel that aggregates these refined multi-view masks into a unified 3D voxel grid in under 200ms, enabling real-time visual feedback. This optimization-free design eliminates the need for per-scene training. Experiments on Mip-NeRF 360° and LLFF show that DivAS achieves segmentation quality comparable to optimization-based methods, while being 2-2.5x faster end-to-end, and up to an order of magnitude faster when excluding user prompting time.

</details>


### [141] [Scaling Vision Language Models for Pharmaceutical Long Form Video Reasoning on Industrial GenAI Platform](https://arxiv.org/abs/2601.04891)
*Suyash Mishra,Qiang Li,Srikanth Patil,Satyanarayan Pati,Baddu Narendra*

Main category: cs.CV

TL;DR: Industrial framework for processing 200K+ PDFs and 25K+ videos with 40+ VLM evaluation reveals efficiency gains, multimodality benefits, and temporal reasoning bottlenecks in pharmaceutical video understanding under GPU constraints.


<details>
  <summary>Details</summary>
Motivation: Most VLM evaluations focus on short videos with unlimited resources, but industrial pharmaceutical applications require processing long-form videos under strict GPU, latency, and cost constraints where existing approaches fail to scale.

Method: Developed industrial GenAI framework processing 200K+ PDFs, 25K+ videos across 8 formats, and 888 multilingual audio files. Evaluated over 40 VLMs on Video-MME and MMBench benchmarks plus proprietary dataset of 25,326 videos across 14 disease areas.

Result: Achieved 3-8x efficiency gains with SDPA attention on commodity GPUs. Multimodality improved 8/12 task domains (especially length-dependent tasks). Identified bottlenecks in temporal alignment and keyframe detection across open- and closed-source VLMs.

Conclusion: Rather than proposing new models, this paper characterizes practical limits, trade-offs, and failure patterns of current VLMs under realistic deployment constraints, providing actionable guidance for designing scalable multimodal systems for industrial long-form video understanding.

Abstract: Vision Language Models (VLMs) have shown strong performance on multimodal reasoning tasks, yet most evaluations focus on short videos and assume unconstrained computational resources. In industrial settings such as pharmaceutical content understanding, practitioners must process long-form videos under strict GPU, latency, and cost constraints, where many existing approaches fail to scale. In this work, we present an industrial GenAI framework that processes over 200,000 PDFs, 25,326 videos across eight formats (e.g., MP4, M4V, etc.), and 888 multilingual audio files in more than 20 languages. Our study makes three contributions: (i) an industrial large-scale architecture for multimodal reasoning in pharmaceutical domains; (ii) empirical analysis of over 40 VLMs on two leading benchmarks (Video-MME and MMBench) and proprietary dataset of 25,326 videos across 14 disease areas; and (iii) four findings relevant to long-form video reasoning: the role of multimodality, attention mechanism trade-offs, temporal reasoning limits, and challenges of video splitting under GPU constraints. Results show 3-8 times efficiency gains with SDPA attention on commodity GPUs, multimodality improving up to 8/12 task domains (especially length-dependent tasks), and clear bottlenecks in temporal alignment and keyframe detection across open- and closed-source VLMs. Rather than proposing a new "A+B" model, this paper characterizes practical limits, trade-offs, and failure patterns of current VLMs under realistic deployment constraints, and provide actionable guidance for both researchers and practitioners designing scalable multimodal systems for long-form video understanding in industrial domains.

</details>


### [142] [Rotation-Robust Regression with Convolutional Model Trees](https://arxiv.org/abs/2601.04899)
*Hongyi Li,William Ward Armstrong,Jun Xu*

Main category: cs.CV

TL;DR: CMTs with geometry-aware inductive biases improve rotation robustness; orientation search helps with severe rotations but can harm performance near canonical orientation when confidence doesn't align with correctness.


<details>
  <summary>Details</summary>
Motivation: To develop rotation-robust learning methods for image inputs using Convolutional Model Trees that can handle geometric transformations at deployment time.

Method: Three geometry-aware inductive biases for split directions: convolutional smoothing, tilt dominance constraint, and importance-based pruning; plus deployment-time orientation search selecting discrete rotation maximizing forest-level confidence proxy.

Result: Orientation search improves robustness under severe rotations but can be harmful near canonical orientation when confidence is misaligned with correctness; consistent trends observed on MNIST digit recognition.

Conclusion: Convolutional Model Trees with geometry-aware inductive biases show promise for rotation robustness, but confidence-based orientation selection has limitations when confidence doesn't align with correctness.

Abstract: We study rotation-robust learning for image inputs using Convolutional Model Trees (CMTs) [1], whose split and leaf coefficients can be structured on the image grid and transformed geometrically at deployment time. In a controlled MNIST setting with a rotation-invariant regression target, we introduce three geometry-aware inductive biases for split directions -- convolutional smoothing, a tilt dominance constraint, and importance-based pruning -- and quantify their impact on robustness under in-plane rotations. We further evaluate a deployment-time orientation search that selects a discrete rotation maximizing a forest-level confidence proxy without updating model parameters. Orientation search improves robustness under severe rotations but can be harmful near the canonical orientation when confidence is misaligned with correctness. Finally, we observe consistent trends on MNIST digit recognition implemented as one-vs-rest regression, highlighting both the promise and limitations of confidence-based orientation selection for model-tree ensembles.

</details>


### [143] [Prototypicality Bias Reveals Blindspots in Multimodal Evaluation Metrics](https://arxiv.org/abs/2601.04946)
*Subhadeep Roy,Gagan Bhatia,Steffen Eger*

Main category: cs.CV

TL;DR: The paper introduces ProtoBias benchmark to evaluate prototypicality bias in text-to-image metrics, showing current metrics often misrank non-prototypical but semantically correct images vs. prototypical but incorrect ones, and proposes ProtoScore as a more robust metric.


<details>
  <summary>Details</summary>
Motivation: Current automatic metrics for text-to-image models may prioritize visually/socially prototypical images learned from biased data rather than true semantic correctness, creating a systematic failure mode called prototypicality bias that needs to be studied and addressed.

Method: Created ProtoBias benchmark with controlled contrastive pairs across Animals, Objects, and Demography categories, pairing semantically correct but non-prototypical images with subtly incorrect yet prototypical adversarial counterparts to directionally evaluate metric biases.

Result: Widely used metrics (CLIPScore, PickScore, VQA-based scores) frequently misrank pairs, LLM-as-Judge systems show uneven robustness, while humans consistently favor semantic correctness. ProtoScore (7B-parameter metric) substantially reduces failure rates and suppresses misranking while being much faster than GPT-5 inference.

Conclusion: Prototypicality bias is a significant issue in multimodal evaluation metrics, and the proposed ProtoScore offers a more robust alternative that better aligns with human judgment on semantic correctness while being computationally efficient.

Abstract: Automatic metrics are now central to evaluating text-to-image models, often substituting for human judgment in benchmarking and large-scale filtering. However, it remains unclear whether these metrics truly prioritize semantic correctness or instead favor visually and socially prototypical images learned from biased data distributions. We identify and study \emph{prototypicality bias} as a systematic failure mode in multimodal evaluation. We introduce a controlled contrastive benchmark \textsc{\textbf{ProtoBias}} (\textit{\textbf{Proto}typical \textbf{Bias}}), spanning Animals, Objects, and Demography images, where semantically correct but non-prototypical images are paired with subtly incorrect yet prototypical adversarial counterparts. This setup enables a directional evaluation of whether metrics follow textual semantics or default to prototypes. Our results show that widely used metrics, including CLIPScore, PickScore, and VQA-based scores, frequently misrank these pairs, while even LLM-as-Judge systems exhibit uneven robustness in socially grounded cases. Human evaluations consistently favour semantic correctness with larger decision margins. Motivated by these findings, we propose \textbf{\textsc{ProtoScore}}, a robust 7B-parameter metric that substantially reduces failure rates and suppresses misranking, while running at orders of magnitude faster than the inference time of GPT-5, approaching the robustness of much larger closed-source judges.

</details>


### [144] [TEA: Temporal Adaptive Satellite Image Semantic Segmentation](https://arxiv.org/abs/2601.04956)
*Juyuan Kang,Hao Zhu,Yan Zhu,Wei Zhang,Jianing Chen,Tianxiang Xiao,Yike Ma,Hao Jiang,Feng Dai*

Main category: cs.CV

TL;DR: TEA: A temporal adaptive method for satellite image time-series segmentation that improves model generalization across varying sequence lengths through teacher-student knowledge transfer and reconstruction tasks.


<details>
  <summary>Details</summary>
Motivation: Existing SITS segmentation approaches perform well with predetermined sequence lengths but fail to generalize across scenarios with varying temporal lengths, leading to poor segmentation results when sequence lengths change.

Method: Proposes TEA (TEmporal Adaptive SITS semantic segmentation method) with a teacher model containing global sequence knowledge that guides a student model with adaptive temporal inputs. Uses intermediate embedding, prototypes, and soft labels for knowledge transfer, dynamic student aggregation to prevent forgetting, and full-sequence reconstruction as an auxiliary task.

Result: Extensive experiments show remarkable improvements across inputs of different temporal lengths on common benchmarks, demonstrating enhanced resilience to varying sequence lengths.

Conclusion: TEA effectively addresses the generalization problem in SITS segmentation across varying temporal lengths through adaptive knowledge transfer and reconstruction techniques, with code to be publicly released.

Abstract: Crop mapping based on satellite images time-series (SITS) holds substantial economic value in agricultural production settings, in which parcel segmentation is an essential step. Existing approaches have achieved notable advancements in SITS segmentation with predetermined sequence lengths. However, we found that these approaches overlooked the generalization capability of models across scenarios with varying temporal length, leading to markedly poor segmentation results in such cases. To address this issue, we propose TEA, a TEmporal Adaptive SITS semantic segmentation method to enhance the model's resilience under varying sequence lengths. We introduce a teacher model that encapsulates the global sequence knowledge to guide a student model with adaptive temporal input lengths. Specifically, teacher shapes the student's feature space via intermediate embedding, prototypes and soft label perspectives to realize knowledge transfer, while dynamically aggregating student model to mitigate knowledge forgetting. Finally, we introduce full-sequence reconstruction as an auxiliary task to further enhance the quality of representations across inputs of varying temporal lengths. Through extensive experiments, we demonstrate that our method brings remarkable improvements across inputs of different temporal lengths on common benchmarks. Our code will be publicly available.

</details>


### [145] [SparseLaneSTP: Leveraging Spatio-Temporal Priors with Sparse Transformers for 3D Lane Detection](https://arxiv.org/abs/2601.04968)
*Maximilian Pittner,Joel Janai,Mario Faigle,Alexandru Paul Condurache*

Main category: cs.CV

TL;DR: SparseLaneSTP: A sparse lane transformer that integrates lane geometric priors and temporal information for improved 3D lane detection, with a new auto-labeled dataset.


<details>
  <summary>Details</summary>
Motivation: Existing 3D lane detection methods have limitations: dense BEV approaches suffer from poor feature representation due to erroneous transformations, sparse detectors ignore valuable lane-specific priors, and no methods utilize historical lane observations to resolve visibility ambiguities.

Method: SparseLaneSTP integrates geometric lane properties and temporal information into a sparse lane transformer. It introduces: 1) lane-specific spatio-temporal attention mechanism, 2) continuous lane representation for sparse architectures, and 3) temporal regularization. Also creates a new precise 3D lane dataset using auto-labeling.

Result: Achieves state-of-the-art performance across all detection and error metrics on existing 3D lane benchmarks and the novel dataset, demonstrating benefits of integrating lane priors and temporal information.

Conclusion: The proposed SparseLaneSTP successfully addresses limitations of existing methods by incorporating lane geometric priors and temporal information, while the new dataset addresses weaknesses in existing 3D lane datasets.

Abstract: 3D lane detection has emerged as a critical challenge in autonomous driving, encompassing identification and localization of lane markings and the 3D road surface. Conventional 3D methods detect lanes from dense birds-eye-viewed (BEV) features, though erroneous transformations often result in a poor feature representation misaligned with the true 3D road surface. While recent sparse lane detectors have surpassed dense BEV approaches, they completely disregard valuable lane-specific priors. Furthermore, existing methods fail to utilize historic lane observations, which yield the potential to resolve ambiguities in situations of poor visibility. To address these challenges, we present SparseLaneSTP, a novel method that integrates both geometric properties of the lane structure and temporal information into a sparse lane transformer. It introduces a new lane-specific spatio-temporal attention mechanism, a continuous lane representation tailored for sparse architectures as well as temporal regularization. Identifying weaknesses of existing 3D lane datasets, we also introduce a precise and consistent 3D lane dataset using a simple yet effective auto-labeling strategy. Our experimental section proves the benefits of our contributions and demonstrates state-of-the-art performance across all detection and error metrics on existing 3D lane detection benchmarks as well as on our novel dataset.

</details>


### [146] [OceanSplat: Object-aware Gaussian Splatting with Trinocular View Consistency for Underwater Scene Reconstruction](https://arxiv.org/abs/2601.04984)
*Minseong Kweon,Jinsun Park*

Main category: cs.CV

TL;DR: OceanSplat: A 3D Gaussian Splatting method for underwater scenes that uses trinocular view consistency and synthetic epipolar depth prior to overcome optical degradation, with depth-aware alpha adjustment to reduce floating artifacts.


<details>
  <summary>Details</summary>
Motivation: Underwater scenes suffer from multi-view inconsistencies due to optical degradation (scattering, absorption), making 3D reconstruction challenging. Existing methods struggle with floating artifacts and poor geometry representation in scattering media.

Method: 1) Enforces trinocular view consistency by rendering horizontally/vertically translated camera views and aligning via inverse warping. 2) Derives synthetic epipolar depth prior through triangulation for self-supervised depth regularization. 3) Uses depth-aware alpha adjustment to modulate 3D Gaussian opacity based on z-component and viewing direction during early training.

Result: Substantially outperforms existing methods on real-world underwater and simulated scenes for both scene reconstruction and restoration. Effectively disentangles 3D Gaussians from scattering medium, reduces floating artifacts, and preserves scene structure.

Conclusion: OceanSplat enables robust 3D geometry representation in underwater environments by addressing optical degradation through geometric constraints and regularization, making 3D Gaussian Splatting viable for scattering media applications.

Abstract: We introduce OceanSplat, a novel 3D Gaussian Splatting-based approach for accurately representing 3D geometry in underwater scenes. To overcome multi-view inconsistencies caused by underwater optical degradation, our method enforces trinocular view consistency by rendering horizontally and vertically translated camera views relative to each input view and aligning them via inverse warping. Furthermore, these translated camera views are used to derive a synthetic epipolar depth prior through triangulation, which serves as a self-supervised depth regularizer. These geometric constraints facilitate the spatial optimization of 3D Gaussians and preserve scene structure in underwater environments. We also propose a depth-aware alpha adjustment that modulates the opacity of 3D Gaussians during early training based on their $z$-component and viewing direction, deterring the formation of medium-induced primitives. With our contributions, 3D Gaussians are disentangled from the scattering medium, enabling robust representation of object geometry and significantly reducing floating artifacts in reconstructed underwater scenes. Experiments on real-world underwater and simulated scenes demonstrate that OceanSplat substantially outperforms existing methods for both scene reconstruction and restoration in scattering media.

</details>


### [147] [Higher-Order Adversarial Patches for Real-Time Object Detectors](https://arxiv.org/abs/2601.04991)
*Jens Bayer,Stefan Becker,David Münch,Michael Arens,Jürgen Beyerer*

Main category: cs.CV

TL;DR: Higher-order adversarial attacks on YOLOv10 object detectors show stronger generalization than lower-order attacks, and adversarial training alone is insufficient for defense.


<details>
  <summary>Details</summary>
Motivation: To investigate the impact of higher-order adversarial attacks on object detectors through the cat-and-mouse game of successive attack pattern training and adversarial training hardening.

Method: Used YOLOv10 as representative object detector with adversarial patches in evasion attacks, successively training attack patterns and hardening detectors with adversarial training.

Result: Higher-order adversarial patches demonstrate stronger generalization capacity compared to lower-order patches, and adversarial training alone is insufficient to efficiently harden detectors against such attacks.

Conclusion: Higher-order adversarial attacks pose significant threats to object detectors with enhanced generalization, requiring more robust defense strategies beyond standard adversarial training.

Abstract: Higher-order adversarial attacks can directly be considered the result of a cat-and-mouse game -- an elaborate action involving constant pursuit, near captures, and repeated escapes. This idiom describes the enduring circular training of adversarial attack patterns and adversarial training the best. The following work investigates the impact of higher-order adversarial attacks on object detectors by successively training attack patterns and hardening object detectors with adversarial training. The YOLOv10 object detector is chosen as a representative, and adversarial patches are used in an evasion attack manner. Our results indicate that higher-order adversarial patches are not only affecting the object detector directly trained on but rather provide a stronger generalization capacity compared to lower-order adversarial patches. Moreover, the results highlight that solely adversarial training is not sufficient to harden an object detector efficiently against this kind of adversarial attack. Code: https://github.com/JensBayer/HigherOrder

</details>


### [148] [Patch-based Representation and Learning for Efficient Deformation Modeling](https://arxiv.org/abs/2601.05035)
*Ruochen Chen,Thuy Tran,Shaifali Parashar*

Main category: cs.CV

TL;DR: PolyFit: A patch-based surface representation using local jet functions that enables efficient surface deformation by updating compact jet coefficients instead of per-vertex optimization.


<details>
  <summary>Details</summary>
Motivation: Current surface deformation methods often require expensive per-vertex optimization or complex physics-based solvers. There's a need for a more efficient representation that can handle various surface types and enable fast deformation for computer vision and graphics applications.

Method: Learn a patch-based surface representation (PolyFit) by fitting jet functions locally on surface patches in supervised fashion from analytic functions and real data. The learned representation can be generalized to various surface types, enabling deformation by updating compact jet coefficients rather than per-vertex degrees of freedom.

Result: 1) Shape-from-template: Test-time optimization delivers competitive accuracy while being markedly faster than offline physics-based solvers, outperforming recent physics-guided neural simulators in accuracy at modest additional runtime. 2) Garment draping: A self-supervised, mesh- and garment-agnostic model generalizes across resolutions and garment types, delivering up to an order-of-magnitude faster inference than strong baselines.

Conclusion: PolyFit provides an efficient patch-based surface representation that enables fast and accurate surface deformation for various computer vision and graphics applications, offering significant speed improvements over existing methods while maintaining or improving accuracy.

Abstract: In this paper, we present a patch-based representation of surfaces, PolyFit, which is obtained by fitting jet functions locally on surface patches. Such a representation can be learned efficiently in a supervised fashion from both analytic functions and real data. Once learned, it can be generalized to various types of surfaces. Using PolyFit, the surfaces can be efficiently deformed by updating a compact set of jet coefficients rather than optimizing per-vertex degrees of freedom for many downstream tasks in computer vision and graphics. We demonstrate the capabilities of our proposed methodologies with two applications: 1) Shape-from-template (SfT): where the goal is to deform the input 3D template of an object as seen in image/video. Using PolyFit, we adopt test-time optimization that delivers competitive accuracy while being markedly faster than offline physics-based solvers, and outperforms recent physics-guided neural simulators in accuracy at modest additional runtime. 2) Garment draping. We train a self-supervised, mesh- and garment-agnostic model that generalizes across resolutions and garment types, delivering up to an order-of-magnitude faster inference than strong baselines.

</details>


### [149] [From Understanding to Engagement: Personalized pharmacy Video Clips via Vision Language Models (VLMs)](https://arxiv.org/abs/2601.05059)
*Suyash Mishra,Qiang Li,Srikanth Patil,Anubhav Girdhar*

Main category: cs.CV

TL;DR: Domain-adapted Video-to-Video Clip Generation framework using ALMs and VLMs for pharmaceutical content, achieving 3-4x speedup, 4x cost reduction, and improved clip quality over baselines.


<details>
  <summary>Details</summary>
Motivation: Manual annotation of heterogeneous pharmaceutical data (text, images, video, audio, web links) is inconsistent, inefficient, and struggles with long content like clinical trial interviews and educational seminars.

Method: Integrated Audio Language Models (ALMs) and Vision Language Models (VLMs) with: (1) Cut & Merge algorithm with fade in/out and timestamp normalization, (2) personalization via role definition and prompt injection, (3) cost-efficient end-to-end pipeline balancing ALM/VLM processing.

Result: 3-4 times speedup, 4 times cost reduction, competitive clip quality on Video MME benchmark (900 videos) and proprietary dataset (16,159 pharmacy videos across 14 disease areas). Improved coherence (0.348) and informativeness (0.721) scores over state-of-the-art VLM baselines like Gemini 2.5 Pro.

Conclusion: The framework demonstrates potential for transparent, custom extractive, and compliance-supporting video summarization in life sciences, enabling intelligent, scalable multi-modality content processing for pharmaceutical industry digital transformation.

Abstract: Vision Language Models (VLMs) are poised to revolutionize the digital transformation of pharmacyceutical industry by enabling intelligent, scalable, and automated multi-modality content processing. Traditional manual annotation of heterogeneous data modalities (text, images, video, audio, and web links), is prone to inconsistencies, quality degradation, and inefficiencies in content utilization. The sheer volume of long video and audio data further exacerbates these challenges, (e.g. long clinical trial interviews and educational seminars).
  Here, we introduce a domain adapted Video to Video Clip Generation framework that integrates Audio Language Models (ALMs) and Vision Language Models (VLMs) to produce highlight clips. Our contributions are threefold: (i) a reproducible Cut & Merge algorithm with fade in/out and timestamp normalization, ensuring smooth transitions and audio/visual alignment; (ii) a personalization mechanism based on role definition and prompt injection for tailored outputs (marketing, training, regulatory); (iii) a cost efficient e2e pipeline strategy balancing ALM/VLM enhanced processing. Evaluations on Video MME benchmark (900) and our proprietary dataset of 16,159 pharmacy videos across 14 disease areas demonstrate 3 to 4 times speedup, 4 times cost reduction, and competitive clip quality. Beyond efficiency gains, we also report our methods improved clip coherence scores (0.348) and informativeness scores (0.721) over state of the art VLM baselines (e.g., Gemini 2.5 Pro), highlighting the potential of transparent, custom extractive, and compliance supporting video summarization for life sciences.

</details>


### [150] [Driving on Registers](https://arxiv.org/abs/2601.05083)
*Ellington Kirby,Alexandre Boulch,Yihong Xu,Yuan Yin,Gilles Puy,Éloi Zablocki,Andrei Bursuc,Spyros Gidaris,Renaud Marlet,Florent Bartoccioni,Anh-Quan Cao,Nermin Samet,Tuan-Hung VU,Matthieu Cord*

Main category: cs.CV

TL;DR: DrivoR is a transformer-based autonomous driving architecture that uses camera-aware register tokens to compress multi-camera features, enabling efficient trajectory generation and scoring with interpretable behavior metrics.


<details>
  <summary>Details</summary>
Motivation: To create a simple yet effective end-to-end autonomous driving system that efficiently processes multi-camera inputs while maintaining accuracy and enabling interpretable, behavior-conditioned driving.

Method: Uses pretrained Vision Transformers with camera-aware register tokens to compress multi-camera features, then employs two lightweight transformer decoders: one for trajectory generation and another for scoring trajectories with interpretable sub-scores (safety, comfort, efficiency).

Result: Outperforms or matches strong contemporary baselines across NAVSIM-v1, NAVSIM-v2, and HUGSIM benchmarks, demonstrating accurate, efficient, and adaptive driving capabilities.

Conclusion: A pure-transformer architecture with targeted token compression is sufficient for accurate, efficient, and adaptive end-to-end autonomous driving, offering interpretable behavior control through learned scoring mechanisms.

Abstract: We present DrivoR, a simple and efficient transformer-based architecture for end-to-end autonomous driving. Our approach builds on pretrained Vision Transformers (ViTs) and introduces camera-aware register tokens that compress multi-camera features into a compact scene representation, significantly reducing downstream computation without sacrificing accuracy. These tokens drive two lightweight transformer decoders that generate and then score candidate trajectories. The scoring decoder learns to mimic an oracle and predicts interpretable sub-scores representing aspects such as safety, comfort, and efficiency, enabling behavior-conditioned driving at inference. Despite its minimal design, DrivoR outperforms or matches strong contemporary baselines across NAVSIM-v1, NAVSIM-v2, and the photorealistic closed-loop HUGSIM benchmark. Our results show that a pure-transformer architecture, combined with targeted token compression, is sufficient for accurate, efficient, and adaptive end-to-end driving. Code and checkpoints will be made available via the project page.

</details>


### [151] [UniLiPs: Unified LiDAR Pseudo-Labeling with Geometry-Grounded Dynamic Scene Decomposition](https://arxiv.org/abs/2601.05105)
*Filippo Ghilotti,Samuel Brucker,Nahku Saidy,Matteo Matteucci,Mario Bijelic,Felix Heide*

Main category: cs.CV

TL;DR: Unsupervised 3D pseudo-labeling method that leverages temporal-geometric consistency in LiDAR data to generate semantic labels, bounding boxes, and dense scans without manual supervision.


<details>
  <summary>Details</summary>
Motivation: Unlabeled LiDAR logs are abundant but useless without expensive human labels, creating a major cost barrier for autonomous perception research. The paper aims to overcome this bottleneck by using temporal-geometric consistency to extract 3D information without manual input.

Method: Uses temporal-geometric consistency across LiDAR sweeps to lift and fuse cues from text and 2D vision foundation models into 3D. Introduces unsupervised multi-modal pseudo-labeling with geometric priors from accumulated LiDAR maps, plus an iterative update rule enforcing joint geometric-semantic consistency while detecting moving objects from inconsistencies.

Result: Method produces 3D semantic labels, 3D bounding boxes, and dense LiDAR scans with robust generalization across three datasets. Outperforms existing pseudo-labeling methods that require manual supervision. Improves depth prediction by 51.5% and 22.0% MAE in 80-150m and 150-250m ranges respectively.

Conclusion: The approach successfully leverages unlabeled LiDAR data through temporal-geometric consistency to generate high-quality 3D annotations without manual supervision, significantly reducing the cost barrier for autonomous perception research while improving depth prediction performance.

Abstract: Unlabeled LiDAR logs, in autonomous driving applications, are inherently a gold mine of dense 3D geometry hiding in plain sight - yet they are almost useless without human labels, highlighting a dominant cost barrier for autonomous-perception research. In this work we tackle this bottleneck by leveraging temporal-geometric consistency across LiDAR sweeps to lift and fuse cues from text and 2D vision foundation models directly into 3D, without any manual input. We introduce an unsupervised multi-modal pseudo-labeling method relying on strong geometric priors learned from temporally accumulated LiDAR maps, alongside with a novel iterative update rule that enforces joint geometric-semantic consistency, and vice-versa detecting moving objects from inconsistencies. Our method simultaneously produces 3D semantic labels, 3D bounding boxes, and dense LiDAR scans, demonstrating robust generalization across three datasets. We experimentally validate that our method compares favorably to existing semantic segmentation and object detection pseudo-labeling methods, which often require additional manual supervision. We confirm that even a small fraction of our geometrically consistent, densified LiDAR improves depth prediction by 51.5% and 22.0% MAE in the 80-150 and 150-250 meters range, respectively.

</details>


### [152] [From Rays to Projections: Better Inputs for Feed-Forward View Synthesis](https://arxiv.org/abs/2601.05116)
*Zirui Wu,Zeren Jiang,Martin R. Oswald,Jie Song*

Main category: cs.CV

TL;DR: The paper proposes projective conditioning for view synthesis, replacing camera parameters with stable 2D projective cues to improve geometric consistency and robustness.


<details>
  <summary>Details</summary>
Motivation: Existing feed-forward view synthesis models use Plücker ray maps that tie predictions to arbitrary world coordinates, making them sensitive to small camera transformations and undermining geometric consistency. The authors seek better conditioning inputs for robust and consistent view synthesis.

Method: Proposes projective conditioning which replaces raw camera parameters with target-view projective cues (stable 2D inputs). This reframes the task from geometric regression in ray space to well-conditioned image-to-image translation. Also introduces masked autoencoding pretraining tailored to this cue for using large-scale uncalibrated data.

Result: The method shows improved fidelity and stronger cross-view consistency compared to ray-conditioned baselines on their view-consistency benchmark. Achieves state-of-the-art quality on standard novel view synthesis benchmarks.

Conclusion: Projective conditioning provides more stable and consistent conditioning for view synthesis models by transforming the problem from brittle geometric regression to robust image-to-image translation, with pretraining enabling use of uncalibrated data.

Abstract: Feed-forward view synthesis models predict a novel view in a single pass with minimal 3D inductive bias. Existing works encode cameras as Plücker ray maps, which tie predictions to the arbitrary world coordinate gauge and make them sensitive to small camera transformations, thereby undermining geometric consistency. In this paper, we ask what inputs best condition a model for robust and consistent view synthesis. We propose projective conditioning, which replaces raw camera parameters with a target-view projective cue that provides a stable 2D input. This reframes the task from a brittle geometric regression problem in ray space to a well-conditioned target-view image-to-image translation problem. Additionally, we introduce a masked autoencoding pretraining strategy tailored to this cue, enabling the use of large-scale uncalibrated data for pretraining. Our method shows improved fidelity and stronger cross-view consistency compared to ray-conditioned baselines on our view-consistency benchmark. It also achieves state-of-the-art quality on standard novel view synthesis benchmarks.

</details>


### [153] [Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing](https://arxiv.org/abs/2601.05124)
*Runze He,Yiji Cheng,Tiankai Hang,Zhimin Li,Yu Xu,Zijin Yin,Shiyi Zhang,Wenxun Dai,Penghui Du,Ao Ma,Chunyu Wang,Qinglin Lu,Jizhong Han,Jiao Dai*

Main category: cs.CV

TL;DR: Re-Align is a unified framework that bridges understanding and generation gaps in in-context image generation/editing through structured reasoning-guided alignment and RL training.


<details>
  <summary>Details</summary>
Motivation: Current unified multimodal models have strong understanding capabilities but these strengths don't effectively transfer to image generation tasks, creating a gap between understanding and generation in in-context image generation and editing.

Method: Introduces In-Context Chain-of-Thought (IC-CoT) to decouple semantic guidance and reference association, plus an RL training scheme using surrogate rewards to align structured reasoning text with generated images.

Result: Extensive experiments show Re-Align outperforms competitive methods of comparable model scale and resources on both in-context image generation and editing tasks.

Conclusion: Re-Align successfully bridges the gap between understanding and generation through structured reasoning-guided alignment, improving performance on ICGE tasks.

Abstract: In-context image generation and editing (ICGE) enables users to specify visual concepts through interleaved image-text prompts, demanding precise understanding and faithful execution of user intent. Although recent unified multimodal models exhibit promising understanding capabilities, these strengths often fail to transfer effectively to image generation. We introduce Re-Align, a unified framework that bridges the gap between understanding and generation through structured reasoning-guided alignment. At its core lies the In-Context Chain-of-Thought (IC-CoT), a structured reasoning paradigm that decouples semantic guidance and reference association, providing clear textual target and mitigating confusion among reference images. Furthermore, Re-Align introduces an effective RL training scheme that leverages a surrogate reward to measure the alignment between structured reasoning text and the generated image, thereby improving the model's overall performance on ICGE tasks. Extensive experiments verify that Re-Align outperforms competitive methods of comparable model scale and resources on both in-context image generation and editing tasks.

</details>


### [154] [VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding](https://arxiv.org/abs/2601.05125)
*Ignacio de Rodrigo,Alvaro J. Lopez-Lopez,Jaime Boal*

Main category: cs.CV

TL;DR: VERSE is a methodology for analyzing and improving Vision-Language Models for document understanding by exploring their visual embedding space, enabling visualization of latent representations, identifying problematic regions, and generating synthetic data to enhance performance.


<details>
  <summary>Details</summary>
Motivation: To develop a systematic approach for analyzing and improving Vision-Language Models in Visually-rich Document Understanding tasks by understanding their visual embedding space and addressing performance issues through targeted synthetic data generation.

Method: VERSE explores the visual embedding space of Vision-Language Models to visualize latent representations, assess model feasibility, identify problematic regions, and guide generation of synthetic data to enhance performance in specific clusters.

Result: VERSE successfully uncovers visual features associated with error-prone clusters, and retraining with samples containing these features substantially boosts F1 performance without degrading generalization. On-premise models (Donut, Idefics2) optimized with VERSE match or surpass SaaS solutions (GPT-4, Pixtral).

Conclusion: VERSE provides an effective methodology for analyzing and improving Vision-Language Models for document understanding, enabling targeted performance improvements through visual embedding space analysis and synthetic data generation, making on-premise models competitive with SaaS solutions.

Abstract: This work introduces VERSE, a methodology for analyzing and improving Vision-Language Models applied to Visually-rich Document Understanding by exploring their visual embedding space. VERSE enables the visualization of latent representations, supporting the assessment of model feasibility. It also facilitates the identification of problematic regions and guides the generation of synthetic data to enhance performance in those clusters. We validate the methodology by training on the synthetic MERIT Dataset and evaluating on its real-world counterpart, MERIT Secret. Results show that VERSE helps uncover the visual features associated with error-prone clusters, and that retraining with samples containing these features substantially boosts F1 performance without degrading generalization. Furthermore, we demonstrate that on-premise models such as Donut and Idefics2, when optimized with VERSE, match or even surpass the performance of SaaS solutions like GPT-4 and Pixtral.

</details>


### [155] [VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control](https://arxiv.org/abs/2601.05138)
*Sixiao Zheng,Minghao Yin,Wenbo Hu,Xiaoyu Li,Ying Shan,Yanwei Fu*

Main category: cs.CV

TL;DR: VerseCrafter is a 4D-aware video world model that enables explicit control over camera and object dynamics using a novel 4D Geometric Control representation, trained on automatically extracted 4D annotations from in-the-wild videos.


<details>
  <summary>Details</summary>
Motivation: Existing video world models struggle with unified and precise control over camera and multi-object motion because videos operate in 2D image space, lacking explicit 4D geometric understanding of the 3D world over time.

Method: Introduces a 4D Geometric Control representation using static background point clouds and per-object 3D Gaussian trajectories that capture object paths and probabilistic 3D occupancy over time. These controls are rendered as conditioning signals for a pretrained video diffusion model. An automatic data engine extracts 4D controls from in-the-wild videos to create training data.

Result: Enables generation of high-fidelity, view-consistent videos that precisely adhere to specified camera and object dynamics, offering flexible category-agnostic control as an alternative to rigid bounding boxes or parametric models.

Conclusion: VerseCrafter bridges the gap between 2D video generation and 4D geometric control, providing a unified framework for controllable video world modeling by addressing both the representation challenge and the data scarcity problem.

Abstract: Video world models aim to simulate dynamic, real-world environments, yet existing methods struggle to provide unified and precise control over camera and multi-object motion, as videos inherently operate dynamics in the projected 2D image plane. To bridge this gap, we introduce VerseCrafter, a 4D-aware video world model that enables explicit and coherent control over both camera and object dynamics within a unified 4D geometric world state. Our approach is centered on a novel 4D Geometric Control representation, which encodes the world state through a static background point cloud and per-object 3D Gaussian trajectories. This representation captures not only an object's path but also its probabilistic 3D occupancy over time, offering a flexible, category-agnostic alternative to rigid bounding boxes or parametric models. These 4D controls are rendered into conditioning signals for a pretrained video diffusion model, enabling the generation of high-fidelity, view-consistent videos that precisely adhere to the specified dynamics. Unfortunately, another major challenge lies in the scarcity of large-scale training data with explicit 4D annotations. We address this by developing an automatic data engine that extracts the required 4D controls from in-the-wild videos, allowing us to train our model on a massive and diverse dataset.

</details>


### [156] [A Lightweight and Explainable Vision-Language Framework for Crop Disease Visual Question Answering](https://arxiv.org/abs/2601.05143)
*Md. Zahid Hossain,Most. Sharmin Sultana Samu,Md. Rakibul Islam,Md. Siam Ansary*

Main category: cs.CV

TL;DR: A lightweight vision-language framework using Swin Transformer and sequence-to-sequence decoders achieves high accuracy for crop disease identification from leaf images, outperforming larger baselines with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Visual question answering for crop disease analysis requires accurate visual understanding and reliable language generation, but existing solutions may be too large or inefficient for practical agricultural applications.

Method: Combines Swin Transformer vision encoder with sequence-to-sequence language decoders using a two-stage training strategy to improve visual representation learning and cross-modal alignment.

Result: High accuracy for both crop and disease identification, strong performance on BLEU, ROUGE and BERTScore metrics, outperforms large-scale vision-language baselines while using significantly fewer parameters.

Conclusion: The framework demonstrates effectiveness of task-specific visual pretraining for crop disease visual question answering, with robust performance under diverse queries and explainability through Grad-CAM and token-level attribution.

Abstract: Visual question answering for crop disease analysis requires accurate visual understanding and reliable language generation. This work presents a lightweight vision-language framework for crop and disease identification from leaf images. The proposed approach combines a Swin Transformer vision encoder with sequence-to-sequence language decoders. A two-stage training strategy is adopted to improve visual representation learning and cross-modal alignment. The model is evaluated on a large-scale crop disease dataset using classification and natural language generation metrics. Experimental results show high accuracy for both crop and disease identification. The framework also achieves strong performance on BLEU, ROUGE and BERTScore. Our proposed models outperform large-scale vision-language baselines while using significantly fewer parameters. Explainability is assessed using Grad-CAM and token-level attribution. Qualitative results demonstrate robust performance under diverse user-driven queries. These findings highlight the effectiveness of task-specific visual pretraining for crop disease visual question answering.

</details>


### [157] [Atlas 2 -- Foundation models for clinical deployment](https://arxiv.org/abs/2601.05148)
*Maximilian Alber,Timo Milbich,Alexandra Carpen-Amarie,Stephan Tietz,Jonas Dippel,Lukas Muttenthaler,Beatriz Perez Cancer,Alessandro Benetti,Panos Korfiatis,Elias Eulig,Jérôme Lüscher,Jiasen Wu,Sayed Abid Hashimi,Gabriel Dernbach,Simon Schallenberg,Neelay Shah,Moritz Krügener,Aniruddh Jammoria,Jake Matras,Patrick Duffy,Matt Redlon,Philipp Jurmeister,David Horst,Lukas Ruff,Klaus-Robert Müller,Frederick Klauschen,Andrew Norgan*

Main category: cs.CV

TL;DR: Atlas 2 series pathology foundation models achieve state-of-the-art performance, robustness, and efficiency across 80 benchmarks, trained on largest pathology dataset of 5.5M whole slide images.


<details>
  <summary>Details</summary>
Motivation: Existing pathology foundation models have tradeoffs in performance, robustness, and computational requirements that limit clinical deployment.

Method: Developed three pathology vision foundation models (Atlas 2, Atlas 2-B, Atlas 2-S) trained on 5.5 million histopathology whole slide images from three medical institutions.

Result: State-of-the-art performance in prediction performance, robustness, and resource efficiency across eighty public benchmarks.

Conclusion: Atlas 2 series models bridge shortcomings of previous pathology foundation models, enabling better clinical deployment through superior performance, robustness, and efficiency.

Abstract: Pathology foundation models substantially advanced the possibilities in computational pathology -- yet tradeoffs in terms of performance, robustness, and computational requirements remained, which limited their clinical deployment. In this report, we present Atlas 2, Atlas 2-B, and Atlas 2-S, three pathology vision foundation models which bridge these shortcomings by showing state-of-the-art performance in prediction performance, robustness, and resource efficiency in a comprehensive evaluation across eighty public benchmarks. Our models were trained on the largest pathology foundation model dataset to date comprising 5.5 million histopathology whole slide images, collected from three medical institutions Charité - Universtätsmedizin Berlin, LMU Munich, and Mayo Clinic.

</details>


### [158] [Multi-Scale Local Speculative Decoding for Image Generation](https://arxiv.org/abs/2601.05149)
*Elia Peruzzo,Guillaume Sautière,Amirhossein Habibian*

Main category: cs.CV

TL;DR: MuLo-SD accelerates autoregressive image generation using multi-resolution drafting with local rejection/resampling, achieving 1.7× speedup while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: Autoregressive models for image synthesis suffer from latency due to sequential nature. Existing speculative decoding approaches are limited by token-level ambiguity and lack spatial awareness.

Method: Multi-scale local speculative decoding combines low-resolution drafter with learned up-samplers to propose candidate tokens, verified in parallel by high-resolution target model. Uses local rejection and resampling mechanism focusing on spatial neighborhoods rather than raster-scan resampling.

Result: Achieves up to 1.7× speedup, outperforming EAGLE-2 and LANTERN baselines. Maintains comparable semantic alignment and perceptual quality validated on MS-COCO 5k split using GenEval, DPG-Bench, and FID/HPSv2 metrics.

Conclusion: Sets new SOTA in speculative decoding for image synthesis, bridging efficiency-fidelity gap. Local rejection/resampling with neighborhood expansion is key innovation.

Abstract: Autoregressive (AR) models have achieved remarkable success in image synthesis, yet their sequential nature imposes significant latency constraints. Speculative Decoding offers a promising avenue for acceleration, but existing approaches are limited by token-level ambiguity and lack of spatial awareness. In this work, we introduce Multi-Scale Local Speculative Decoding (MuLo-SD), a novel framework that combines multi-resolution drafting with spatially informed verification to accelerate AR image generation. Our method leverages a low-resolution drafter paired with learned up-samplers to propose candidate image tokens, which are then verified in parallel by a high-resolution target model. Crucially, we incorporate a local rejection and resampling mechanism, enabling efficient correction of draft errors by focusing on spatial neighborhoods rather than raster-scan resampling after the first rejection. We demonstrate that MuLo-SD achieves substantial speedups - up to $\mathbf{1.7\times}$ - outperforming strong speculative decoding baselines such as EAGLE-2 and LANTERN in terms of acceleration, while maintaining comparable semantic alignment and perceptual quality. These results are validated using GenEval, DPG-Bench, and FID/HPSv2 on the MS-COCO 5k validation split. Extensive ablations highlight the impact of up-sampling design, probability pooling, and local rejection and resampling with neighborhood expansion. Our approach sets a new state-of-the-art in speculative decoding for image synthesis, bridging the gap between efficiency and fidelity.

</details>


### [159] [Vision-Language Introspection: Mitigating Overconfident Hallucinations in MLLMs via Interpretable Bi-Causal Steering](https://arxiv.org/abs/2601.05159)
*Shuliang Liu,Songbo Yang,Dong Fang,Sihang Jia,Yuqi Tang,Lingfeng Su,Ruoshui Peng,Yibo Yan,Xin Zou,Xuming Hu*

Main category: cs.CV

TL;DR: VLI is a training-free inference framework that reduces object hallucination in multimodal LLMs through metacognitive self-correction, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Object hallucination undermines multimodal LLM reliability due to models trusting linguistic priors over visual evidence. Existing methods are limited: contrastive decoding is superficial, and latent steering uses static vectors lacking instance-specific precision.

Method: VLI simulates metacognitive self-correction with two steps: 1) Attributive Introspection diagnoses hallucination risks via probabilistic conflict detection and localizes causal visual anchors; 2) Interpretable Bi-Causal Steering actively modulates inference by dynamically isolating visual evidence from noise while neutralizing blind confidence through adaptive calibration.

Result: VLI achieves state-of-the-art performance, reducing object hallucination rates by 12.67% on MMHal-Bench and improving accuracy by 5.8% on POPE.

Conclusion: VLI effectively addresses object hallucination through a training-free, introspective framework that dynamically corrects cognitive biases in multimodal LLMs.

Abstract: Object hallucination critically undermines the reliability of Multimodal Large Language Models, often stemming from a fundamental failure in cognitive introspection, where models blindly trust linguistic priors over specific visual evidence. Existing mitigations remain limited: contrastive decoding approaches operate superficially without rectifying internal semantic misalignments, while current latent steering methods rely on static vectors that lack instance-specific precision. We introduce Vision-Language Introspection (VLI), a training-free inference framework that simulates a metacognitive self-correction process. VLI first performs Attributive Introspection to diagnose hallucination risks via probabilistic conflict detection and localize the causal visual anchors. It then employs Interpretable Bi-Causal Steering to actively modulate the inference process, dynamically isolating visual evidence from background noise while neutralizing blind confidence through adaptive calibration. VLI achieves state-of-the-art performance on advanced models, reducing object hallucination rates by 12.67% on MMHal-Bench and improving accuracy by 5.8% on POPE.

</details>


### [160] [CoV: Chain-of-View Prompting for Spatial Reasoning](https://arxiv.org/abs/2601.05172)
*Haoyu Zhao,Akide Liu,Zeyu Zhang,Weijie Wang,Feng Chen,Ruihan Zhu,Gholamreza Haffari,Bohan Zhuang*

Main category: cs.CV

TL;DR: CoV prompting enables VLMs to actively explore 3D scenes for EQA through coarse-to-fine view selection and adjustment, improving spatial reasoning without training.


<details>
  <summary>Details</summary>
Motivation: Current VLMs are limited to fixed input views, hindering their ability to gather distributed context and perform complex spatial reasoning in 3D embodied question answering tasks.

Method: Chain-of-View prompting: training-free framework with View Selection agent to filter redundant frames and identify anchor views, followed by fine-grained view adjustment through iterative reasoning with discrete camera actions.

Result: +11.56% average improvement in LLM-Match on OpenEQA across four VLMs, with test-time scaling (+2.51% more improvement with increased action budget). Strong performance on ScanQA (116 CIDEr/31.9 EM@1) and SQA3D (51.1 EM@1).

Conclusion: Question-aligned view selection with open-view search is an effective, model-agnostic strategy for improving 3D EQA spatial reasoning without additional training.

Abstract: Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached.
  We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56\% improvement in LLM-Match, with a maximum gain of +13.62\% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51\% average improvement, peaking at +3.73\% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training.

</details>


### [161] [VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice](https://arxiv.org/abs/2601.05175)
*Shuming Liu,Mingchen Zhuge,Changsheng Zhao,Jun Chen,Lemeng Wu,Zechun Liu,Chenchen Zhu,Zhipeng Cai,Chong Zhou,Haozhe Liu,Ernie Chang,Saksham Suri,Hongyu Xu,Qi Qian,Wei Wen,Balakrishnan Varadarajan,Zhuang Liu,Hu Xu,Florian Bordes,Raghuraman Krishnamoorthi,Bernard Ghanem,Vikas Chandra,Yunyang Xiong*

Main category: cs.CV

TL;DR: VideoAuto-R1: A video understanding framework that uses "reason-when-necessary" strategy instead of always using chain-of-thought reasoning, achieving SOTA accuracy with 3.3x efficiency improvement.


<details>
  <summary>Details</summary>
Motivation: The paper challenges the necessity of chain-of-thought (CoT) reasoning for video understanding tasks, finding that direct answering often matches or surpasses CoT performance despite CoT's higher computational cost. This motivates developing a more efficient approach that only uses reasoning when necessary.

Method: Proposes VideoAuto-R1 framework with "Thinking Once, Answering Twice" paradigm: during training, model generates initial answer, performs reasoning, then outputs reviewed answer, with both answers supervised via verifiable rewards. During inference, uses confidence score of initial answer to decide whether to proceed with reasoning.

Result: Achieves state-of-the-art accuracy on video QA and grounding benchmarks with significantly improved efficiency, reducing average response length by ~3.3x (from 149 to 44 tokens). Shows low thinking-mode activation on perception-oriented tasks but higher rate on reasoning-intensive tasks.

Conclusion: Explicit language-based reasoning is generally beneficial but not always necessary for video understanding. The proposed reason-when-necessary strategy provides optimal balance between performance and efficiency, adapting reasoning usage based on task complexity.

Abstract: Chain-of-thought (CoT) reasoning has emerged as a powerful tool for multimodal large language models on video understanding tasks. However, its necessity and advantages over direct answering remain underexplored. In this paper, we first demonstrate that for RL-trained video models, direct answering often matches or even surpasses CoT performance, despite CoT producing step-by-step analyses at a higher computational cost. Motivated by this, we propose VideoAuto-R1, a video understanding framework that adopts a reason-when-necessary strategy. During training, our approach follows a Thinking Once, Answering Twice paradigm: the model first generates an initial answer, then performs reasoning, and finally outputs a reviewed answer. Both answers are supervised via verifiable rewards. During inference, the model uses the confidence score of the initial answer to determine whether to proceed with reasoning. Across video QA and grounding benchmarks, VideoAuto-R1 achieves state-of-the-art accuracy with significantly improved efficiency, reducing the average response length by ~3.3x, e.g., from 149 to just 44 tokens. Moreover, we observe a low rate of thinking-mode activation on perception-oriented tasks, but a higher rate on reasoning-intensive tasks. This suggests that explicit language-based reasoning is generally beneficial but not always necessary.

</details>


### [162] [Cutting AI Research Costs: How Task-Aware Compression Makes Large Language Model Agents Affordable](https://arxiv.org/abs/2601.05191)
*Zuhair Ahmed Khan Taha,Mohammed Mudassir Uddin,Shahnawaz Alam*

Main category: cs.CV

TL;DR: AgentCompress reduces LLM compute costs by 68.3% while maintaining 96.2% success rate by routing tasks to appropriately compressed model variants based on difficulty assessment.


<details>
  <summary>Details</summary>
Motivation: High computational costs of large language models (e.g., $127 per session for 70B parameter models) make them inaccessible to many academic labs, limiting their use for autonomous research tasks like literature review and hypothesis generation.

Method: Uses a small neural network to assess task difficulty from opening words, then routes tasks to suitably compressed model variants in under 1 millisecond. Based on observation that different tasks (e.g., novel hypothesis generation vs. bibliography reformatting) require different computational precision.

Result: Tested across 500 research workflows in four scientific fields, achieving 68.3% reduction in compute costs while maintaining 96.2% of original success rate.

Conclusion: AgentCompress makes LLM-powered research tools financially accessible to academic labs by significantly reducing computational costs without substantial performance degradation, potentially enabling more labs to conduct experiments rather than being sidelined by budget constraints.

Abstract: When researchers deploy large language models for autonomous tasks like reviewing literature or generating hypotheses, the computational bills add up quickly. A single research session using a 70-billion parameter model can cost around $127 in cloud fees, putting these tools out of reach for many academic labs. We developed AgentCompress to tackle this problem head-on. The core idea came from a simple observation during our own work: writing a novel hypothesis clearly demands more from the model than reformatting a bibliography. Why should both tasks run at full precision? Our system uses a small neural network to gauge how hard each incoming task will be, based only on its opening words, then routes it to a suitably compressed model variant. The decision happens in under a millisecond. Testing across 500 research workflows in four scientific fields, we cut compute costs by 68.3% while keeping 96.2% of the original success rate. For labs watching their budgets, this could mean the difference between running experiments and sitting on the sidelines

</details>


### [163] [Mechanisms of Prompt-Induced Hallucination in Vision-Language Models](https://arxiv.org/abs/2601.05201)
*William Rudman,Michal Golovanevsky,Dana Arad,Yonatan Belinkov,Ritambhara Singh,Carsten Eickhoff,Kyle Mahowald*

Main category: cs.CV

TL;DR: VLMs increasingly hallucinate by favoring text over vision as object counts increase; ablation of specific attention heads reduces these prompt-induced hallucinations by 40%+ without training.


<details>
  <summary>Details</summary>
Motivation: Large vision-language models often hallucinate by prioritizing textual prompts over visual evidence, particularly in object-counting tasks where prompts overstate actual object counts. This failure mode needs systematic study to understand and mitigate.

Method: Controlled object-counting experiments where prompts overstate actual object counts; mechanistic analysis of three VLMs to identify specific attention heads responsible for prompt-induced hallucinations; ablation of these "PIH-heads" to measure reduction in hallucinations.

Result: Models correct prompt overestimation at low object counts but increasingly conform to incorrect prompts as object numbers increase; ablation of identified PIH-heads reduces prompt-induced hallucinations by at least 40% without additional training; PIH-heads mediate prompt copying in model-specific ways.

Conclusion: Specific attention heads drive prompt-induced hallucinations in VLMs; targeted ablation can significantly reduce these hallucinations without retraining; findings reveal model-specific implementation differences in how VLMs prioritize text over visual evidence.

Abstract: Large vision-language models (VLMs) are highly capable, yet often hallucinate by favoring textual prompts over visual evidence. We study this failure mode in a controlled object-counting setting, where the prompt overstates the number of objects in the image (e.g., asking a model to describe four waterlilies when only three are present). At low object counts, models often correct the overestimation, but as the number of objects increases, they increasingly conform to the prompt regardless of the discrepancy. Through mechanistic analysis of three VLMs, we identify a small set of attention heads whose ablation substantially reduces prompt-induced hallucinations (PIH) by at least 40% without additional training. Across models, PIH-heads mediate prompt copying in model-specific ways. We characterize these differences and show that PIH ablation increases correction toward visual evidence. Our findings offer insights into the internal mechanisms driving prompt-induced hallucinations, revealing model-specific differences in how these behaviors are implemented.

</details>


### [164] [MoE3D: A Mixture-of-Experts Module for 3D Reconstruction](https://arxiv.org/abs/2601.05208)
*Zichen Wang,Ang Cao,Liam J. Wang,Jeong Joon Park*

Main category: cs.CV

TL;DR: MoE3D is a mixture-of-experts module that improves 3D reconstruction by predicting multiple candidate depth maps and fusing them with dynamic weighting to sharpen depth boundaries and reduce artifacts.


<details>
  <summary>Details</summary>
Motivation: Existing feed-forward 3D reconstruction models suffer from blurry depth boundaries and flying-point artifacts, which degrade reconstruction quality.

Method: MoE3D predicts multiple candidate depth maps and fuses them using dynamic weighting (mixture-of-experts approach) to sharpen boundaries and reduce artifacts.

Result: When integrated with pre-trained 3D reconstruction backbones like VGGT, MoE3D substantially enhances reconstruction quality with minimal additional computational overhead.

Conclusion: MoE3D effectively improves 3D reconstruction by addressing boundary sharpness and artifact issues through a mixture-of-experts fusion approach.

Abstract: MoE3D is a mixture-of-experts module designed to sharpen depth boundaries and mitigate flying-point artifacts (highlighted in red) of existing feed-forward 3D reconstruction models (left side). MoE3D predicts multiple candidate depth maps and fuses them via dynamic weighting (visualized by MoE weights on the right side). When integrated with a pre-trained 3D reconstruction backbone such as VGGT, it substantially enhances reconstruction quality with minimal additional computational overhead. Best viewed digitally.

</details>


### [165] [FlowLet: Conditional 3D Brain MRI Synthesis using Wavelet Flow Matching](https://arxiv.org/abs/2601.05212)
*Danilo Danese,Angela Lombardi,Matteo Attimonelli,Giuseppe Fasano,Tommaso Di Noia*

Main category: cs.CV

TL;DR: FlowLet: A conditional generative framework using flow matching in invertible 3D wavelet domain to synthesize age-conditioned 3D MRIs for improving Brain Age Prediction fairness and performance.


<details>
  <summary>Details</summary>
Motivation: Existing 3D MRI datasets for Brain Age Prediction are demographically skewed, limiting fairness and generalizability. Current generative methods (latent diffusion models) are slow, may introduce artifacts, and rarely condition on age, affecting BAP performance. Need for efficient, high-fidelity age-conditioned MRI generation.

Method: FlowLet uses flow matching within an invertible 3D wavelet domain to synthesize age-conditioned 3D MRIs. This approach avoids reconstruction artifacts from latent compression and reduces computational demands compared to latent diffusion models.

Result: FlowLet generates high-fidelity volumes with few sampling steps. Training BAP models with FlowLet-generated data improves performance for underrepresented age groups. Region-based analysis confirms preservation of anatomical structures.

Conclusion: FlowLet provides an effective conditional generative framework for synthesizing age-conditioned 3D MRIs that improves Brain Age Prediction fairness and performance, addressing limitations of existing datasets and generative methods.

Abstract: Brain Magnetic Resonance Imaging (MRI) plays a central role in studying neurological development, aging, and diseases. One key application is Brain Age Prediction (BAP), which estimates an individual's biological brain age from MRI data. Effective BAP models require large, diverse, and age-balanced datasets, whereas existing 3D MRI datasets are demographically skewed, limiting fairness and generalizability. Acquiring new data is costly and ethically constrained, motivating generative data augmentation. Current generative methods are often based on latent diffusion models, which operate in learned low dimensional latent spaces to address the memory demands of volumetric MRI data. However, these methods are typically slow at inference, may introduce artifacts due to latent compression, and are rarely conditioned on age, thereby affecting the BAP performance. In this work, we propose FlowLet, a conditional generative framework that synthesizes age-conditioned 3D MRIs by leveraging flow matching within an invertible 3D wavelet domain, helping to avoid reconstruction artifacts and reducing computational demands. Experiments show that FlowLet generates high-fidelity volumes with few sampling steps. Training BAP models with data generated by FlowLet improves performance for underrepresented age groups, and region-based analysis confirms preservation of anatomical structures.

</details>


### [166] [ObjectForesight: Predicting Future 3D Object Trajectories from Human Videos](https://arxiv.org/abs/2601.05237)
*Rustin Soraki,Homanga Bharadhwaj,Ali Farhadi,Roozbeh Mottaghi*

Main category: cs.CV

TL;DR: ObjectForesight: A 3D object-centric dynamics model that predicts future 6-DoF poses and trajectories of rigid objects from short egocentric videos, using explicit 3D object representations for geometrically grounded predictions.


<details>
  <summary>Details</summary>
Motivation: Humans can effortlessly anticipate object motions through interaction (e.g., imagining a cup being lifted or a knife slicing). The authors aim to give computational systems similar predictive abilities from passive visual observation, moving beyond pixel/latent space models to explicit 3D object-level representations.

Method: ObjectForesight uses explicit 3D object-centric representations to predict future 6-DoF poses and trajectories. To train at scale, they leverage recent advances in segmentation, mesh reconstruction, and 3D pose estimation to curate a dataset of 2M+ short clips with pseudo-ground-truth 3D object trajectories.

Result: ObjectForesight achieves significant gains in accuracy, geometric consistency, and generalization to unseen objects and scenes. It establishes a scalable framework for learning physically grounded, object-centric dynamics models directly from observation.

Conclusion: The paper presents ObjectForesight as a novel approach that enables geometrically grounded and temporally coherent predictions of object motions, capturing object affordances and trajectories through explicit 3D object-level representations learned from visual observation.

Abstract: Humans can effortlessly anticipate how objects might move or change through interaction--imagining a cup being lifted, a knife slicing, or a lid being closed. We aim to endow computational systems with a similar ability to predict plausible future object motions directly from passive visual observation. We introduce ObjectForesight, a 3D object-centric dynamics model that predicts future 6-DoF poses and trajectories of rigid objects from short egocentric video sequences. Unlike conventional world or dynamics models that operate in pixel or latent space, ObjectForesight represents the world explicitly in 3D at the object level, enabling geometrically grounded and temporally coherent predictions that capture object affordances and trajectories. To train such a model at scale, we leverage recent advances in segmentation, mesh reconstruction, and 3D pose estimation to curate a dataset of 2 million plus short clips with pseudo-ground-truth 3D object trajectories. Through extensive experiments, we show that ObjectForesight achieves significant gains in accuracy, geometric consistency, and generalization to unseen objects and scenes, establishing a scalable framework for learning physically grounded, object-centric dynamics models directly from observation. objectforesight.github.io

</details>


### [167] [Plenoptic Video Generation](https://arxiv.org/abs/2601.05239)
*Xiao Fu,Shitao Tang,Min Shi,Xian Liu,Jinwei Gu,Ming-Yu Liu,Dahua Lin,Chen-Hsuan Lin*

Main category: cs.CV

TL;DR: PlenopticDreamer is a framework for multi-view video re-rendering that maintains spatio-temporal consistency by training a video-conditioned model autoregressively with camera-guided video retrieval and progressive context-scaling.


<details>
  <summary>Details</summary>
Motivation: Existing camera-controlled generative video re-rendering methods struggle with maintaining consistency across multi-view scenarios and ensuring spatio-temporal coherence in hallucinated regions due to generative model stochasticity.

Method: Trains a multi-in-single-out video-conditioned model autoregressively with camera-guided video retrieval strategy, progressive context-scaling for convergence, self-conditioning for robustness against error accumulation, and long-video conditioning for extended generation.

Result: Achieves state-of-the-art video re-rendering on Basic and Agibot benchmarks, delivering superior view synchronization, high-fidelity visuals, accurate camera control, and diverse view transformations including third-person to third-person and robotic manipulation views.

Conclusion: PlenopticDreamer successfully addresses multi-view consistency challenges in video re-rendering through synchronized generative hallucinations and adaptive conditioning strategies, enabling robust and coherent video generation across diverse camera perspectives.

Abstract: Camera-controlled generative video re-rendering methods, such as ReCamMaster, have achieved remarkable progress. However, despite their success in single-view setting, these works often struggle to maintain consistency across multi-view scenarios. Ensuring spatio-temporal coherence in hallucinated regions remains challenging due to the inherent stochasticity of generative models. To address it, we introduce PlenopticDreamer, a framework that synchronizes generative hallucinations to maintain spatio-temporal memory. The core idea is to train a multi-in-single-out video-conditioned model in an autoregressive manner, aided by a camera-guided video retrieval strategy that adaptively selects salient videos from previous generations as conditional inputs. In addition, Our training incorporates progressive context-scaling to improve convergence, self-conditioning to enhance robustness against long-range visual degradation caused by error accumulation, and a long-video conditioning mechanism to support extended video generation. Extensive experiments on the Basic and Agibot benchmarks demonstrate that PlenopticDreamer achieves state-of-the-art video re-rendering, delivering superior view synchronization, high-fidelity visuals, accurate camera control, and diverse view transformations (e.g., third-person to third-person, and head-view to gripper-view in robotic manipulation). Project page: https://research.nvidia.com/labs/dir/plenopticdreamer/

</details>


### [168] [RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation](https://arxiv.org/abs/2601.05241)
*Boyang Wang,Haoran Zhang,Shujie Zhang,Jinkun Hao,Mingda Jia,Qi Lv,Yucheng Mao,Zhaoyang Lyu,Jia Zeng,Xudong Xu,Jiangmiao Pang*

Main category: cs.CV

TL;DR: Visual identity prompting for data augmentation: Using exemplar images instead of text prompts to guide diffusion models for generating multi-view, temporally coherent robot manipulation data.


<details>
  <summary>Details</summary>
Motivation: Collecting large-scale real-world manipulation data is difficult due to hardware and setup constraints. Existing text-prompt conditioned diffusion models overlook the need for multi-view and temporally coherent observations, and text prompts alone cannot reliably specify scene setups.

Method: Introduce visual identity prompting that supplies exemplar images as conditioning inputs to guide diffusion models in generating desired scene setups. Build a scalable pipeline to curate a visual identity pool from large robotics datasets.

Result: Using the augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.

Conclusion: Visual identity prompting addresses limitations of text-only conditioning by providing explicit visual guidance for generating practical, multi-view, temporally coherent manipulation data, leading to improved policy performance.

Abstract: The diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-prompt conditioned image diffusion models to augment manipulation data by altering the backgrounds and tabletop objects in the visual observations. However, these approaches often overlook the practical need for multi-view and temporally coherent observations required by state-of-the-art policy models. Further, text prompts alone cannot reliably specify the scene setup. To provide the diffusion model with explicit visual guidance, we introduce visual identity prompting, which supplies exemplar images as conditioning inputs to guide the generation of the desired scene setup. To this end, we also build a scalable pipeline to curate a visual identity pool from large robotics datasets. Using our augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.

</details>


### [169] [GREx: Generalized Referring Expression Segmentation, Comprehension, and Generation](https://arxiv.org/abs/2601.05244)
*Henghui Ding,Chang Liu,Shuting He,Xudong Jiang,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: This paper introduces GREx (Generalized Referring Expression Segmentation/Comprehension/Generation) which extends traditional single-target referring expression tasks to handle multi-target and no-target expressions, creating new benchmarks and a dataset called gRefCOCO.


<details>
  <summary>Details</summary>
Motivation: Existing referring expression datasets and methods only support single-target expressions (one expression refers to one object), which limits real-world applications where expressions can refer to multiple objects or no objects at all.

Method: The paper introduces three new benchmarks (GRES, GREC, GREG) and constructs the first large-scale GREx dataset gRefCOCO containing multi-target, no-target, and single-target expressions. They also propose a baseline method called ReLA that adaptively divides images into regions with sub-instance clues and explicitly models region-region and region-language dependencies.

Result: The proposed ReLA method achieves state-of-the-art results on both GRES and GREC tasks. The gRefCOCO dataset enables studying performance gaps of existing REx methods on generalized tasks.

Conclusion: The paper successfully extends referring expression tasks to handle arbitrary numbers of objects, creating backward-compatible benchmarks and a comprehensive dataset that better reflects real-world scenarios, with the proposed ReLA method showing strong performance on the new tasks.

Abstract: Referring Expression Segmentation (RES) and Comprehension (REC) respectively segment and detect the object described by an expression, while Referring Expression Generation (REG) generates an expression for the selected object. Existing datasets and methods commonly support single-target expressions only, i.e., one expression refers to one object, not considering multi-target and no-target expressions. This greatly limits the real applications of REx (RES/REC/REG). This paper introduces three new benchmarks called Generalized Referring Expression Segmentation (GRES), Comprehension (GREC), and Generation (GREG), collectively denoted as GREx, which extend the classic REx to allow expressions to identify an arbitrary number of objects. We construct the first large-scale GREx dataset gRefCOCO that contains multi-target, no-target, and single-target expressions and their corresponding images with labeled targets. GREx and gRefCOCO are designed to be backward-compatible with REx, facilitating extensive experiments to study the performance gap of the existing REx methods on GREx tasks. One of the challenges of GRES/GREC is complex relationship modeling, for which we propose a baseline ReLA that adaptively divides the image into regions with sub-instance clues and explicitly models the region-region and region-language dependencies. The proposed ReLA achieves the state-of-the-art results on the both GRES and GREC tasks. The proposed gRefCOCO dataset and method are available at https://henghuiding.github.io/GREx.

</details>


### [170] [Pixel-Perfect Visual Geometry Estimation](https://arxiv.org/abs/2601.05246)
*Gangwei Xu,Haotong Lin,Hongcheng Luo,Haiyang Sun,Bing Wang,Guang Chen,Sida Peng,Hangjun Ye,Xin Yang*

Main category: cs.CV

TL;DR: Pixel-perfect visual geometry models (PPD for images, PPVD for videos) use pixel-space diffusion transformers with semantic prompting and cascade architecture to produce high-quality, flying-pixel-free depth estimation and point clouds.


<details>
  <summary>Details</summary>
Motivation: Existing geometry foundation models suffer from flying pixels and loss of fine details, which is problematic for robotics and augmented reality applications that need clean, accurate geometry from images.

Method: 1) Pixel-Perfect Depth (PPD): Monocular depth foundation model using pixel-space diffusion transformers (DiT) with Semantics-Prompted DiT (incorporates semantic representations from vision foundation models) and Cascade DiT architecture (progressively increases image tokens). 2) Pixel-Perfect Video Depth (PPVD): Extends PPD with Semantics-Consistent DiT (extracts temporally consistent semantics from multi-view geometry models) and reference-guided token propagation for temporal coherence.

Result: Achieves best performance among all generative monocular and video depth estimation models, producing significantly cleaner point clouds than all other models.

Conclusion: The proposed pixel-perfect visual geometry models effectively address flying pixels and detail loss through generative modeling in pixel space with semantic prompting and efficient cascade architectures, delivering high-quality geometry suitable for robotics and AR applications.

Abstract: Recovering clean and accurate geometry from images is essential for robotics and augmented reality. However, existing geometry foundation models still suffer severely from flying pixels and the loss of fine details. In this paper, we present pixel-perfect visual geometry models that can predict high-quality, flying-pixel-free point clouds by leveraging generative modeling in the pixel space. We first introduce Pixel-Perfect Depth (PPD), a monocular depth foundation model built upon pixel-space diffusion transformers (DiT). To address the high computational complexity associated with pixel-space diffusion, we propose two key designs: 1) Semantics-Prompted DiT, which incorporates semantic representations from vision foundation models to prompt the diffusion process, preserving global semantics while enhancing fine-grained visual details; and 2) Cascade DiT architecture that progressively increases the number of image tokens, improving both efficiency and accuracy. To further extend PPD to video (PPVD), we introduce a new Semantics-Consistent DiT, which extracts temporally consistent semantics from a multi-view geometry foundation model. We then perform reference-guided token propagation within the DiT to maintain temporal coherence with minimal computational and memory overhead. Our models achieve the best performance among all generative monocular and video depth estimation models and produce significantly cleaner point clouds than all other models.

</details>


### [171] [RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes](https://arxiv.org/abs/2601.05249)
*Yuan-Kang Lee,Kuan-Lin Chen,Chia-Che Chang,Yu-Lun Liu*

Main category: cs.CV

TL;DR: RL-AWB combines statistical methods with deep reinforcement learning for nighttime white balance, achieving superior generalization across lighting conditions.


<details>
  <summary>Details</summary>
Motivation: Nighttime color constancy is challenging due to low-light noise and complex illumination conditions, requiring better solutions than existing methods.

Method: Combines statistical algorithm for nighttime scenes (salient gray pixel detection + illumination estimation) with deep reinforcement learning that uses the statistical algorithm as its core, dynamically optimizing parameters per image like professional AWB experts.

Result: Achieves superior generalization capability across both low-light and well-illuminated images, validated on a new multi-sensor nighttime dataset.

Conclusion: RL-AWB presents an effective framework for nighttime white balance that combines statistical and learning-based approaches, with strong cross-sensor generalization performance.

Abstract: Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images. Project page: https://ntuneillee.github.io/research/rl-awb/

</details>


### [172] [QNeRF: Neural Radiance Fields on a Simulated Gate-Based Quantum Computer](https://arxiv.org/abs/2601.05250)
*Daniele Lizzio Bosco,Shuteng Wang,Giuseppe Serra,Vladislav Golyanik*

Main category: cs.CV

TL;DR: QNeRF is the first hybrid quantum-classical model for novel-view synthesis that uses quantum circuits to encode spatial/view information, achieving competitive performance with fewer parameters than classical NeRF.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of classical NeRFs (large models, intensive training) by leveraging quantum advantages from Quantum Visual Fields (QVFs) for more compact and efficient 3D scene representation from 2D images.

Method: Introduces QNeRF with two variants: Full QNeRF uses all quantum amplitudes for enhanced representation, while Dual-Branch QNeRF separates spatial and view-dependent quantum state preparations to reduce complexity and improve scalability.

Result: QNeRF matches or outperforms classical NeRF baselines on moderate-resolution images while using less than half the number of parameters, demonstrating quantum machine learning as a competitive alternative for 3D representation learning.

Conclusion: Quantum machine learning can serve as a viable alternative for continuous signal representation in mid-level computer vision tasks, particularly for 3D representation learning from 2D observations, offering compact models with competitive performance.

Abstract: Recently, Quantum Visual Fields (QVFs) have shown promising improvements in model compactness and convergence speed for learning the provided 2D or 3D signals. Meanwhile, novel-view synthesis has seen major advances with Neural Radiance Fields (NeRFs), where models learn a compact representation from 2D images to render 3D scenes, albeit at the cost of larger models and intensive training. In this work, we extend the approach of QVFs by introducing QNeRF, the first hybrid quantum-classical model designed for novel-view synthesis from 2D images. QNeRF leverages parameterised quantum circuits to encode spatial and view-dependent information via quantum superposition and entanglement, resulting in more compact models compared to the classical counterpart. We present two architectural variants. Full QNeRF maximally exploits all quantum amplitudes to enhance representational capabilities. In contrast, Dual-Branch QNeRF introduces a task-informed inductive bias by branching spatial and view-dependent quantum state preparations, drastically reducing the complexity of this operation and ensuring scalability and potential hardware compatibility. Our experiments demonstrate that -- when trained on images of moderate resolution -- QNeRF matches or outperforms classical NeRF baselines while using less than half the number of parameters. These results suggest that quantum machine learning can serve as a competitive alternative for continuous signal representation in mid-level tasks in computer vision, such as 3D representation learning from 2D observations.

</details>


### [173] [Mesh4D: 4D Mesh Reconstruction and Tracking from Monocular Video](https://arxiv.org/abs/2601.05251)
*Zeren Jiang,Chuanxia Zheng,Iro Laina,Diane Larlus,Andrea Vedaldi*

Main category: cs.CV

TL;DR: Mesh4D: Feed-forward model for monocular 4D mesh reconstruction from videos using latent space encoding and diffusion for full animation prediction.


<details>
  <summary>Details</summary>
Motivation: Reconstructing complete 3D shape and motion of dynamic objects from monocular videos is challenging. Existing methods may lack efficiency in encoding entire animation sequences or require skeletal information at inference time.

Method: Uses autoencoder with skeletal-guided training to learn compact latent space encoding entire animation sequences. Employs spatio-temporal attention for stable deformation representation. Trains latent diffusion model conditioned on input video and first-frame mesh to predict full animation in one shot.

Result: Outperforms prior methods on reconstruction and novel view synthesis benchmarks in recovering accurate 3D shape and deformation. Does not require skeletal information at inference time.

Conclusion: Mesh4D enables efficient monocular 4D mesh reconstruction with accurate shape and deformation recovery through novel latent space encoding and diffusion-based animation prediction.

Abstract: We propose Mesh4D, a feed-forward model for monocular 4D mesh reconstruction. Given a monocular video of a dynamic object, our model reconstructs the object's complete 3D shape and motion, represented as a deformation field. Our key contribution is a compact latent space that encodes the entire animation sequence in a single pass. This latent space is learned by an autoencoder that, during training, is guided by the skeletal structure of the training objects, providing strong priors on plausible deformations. Crucially, skeletal information is not required at inference time. The encoder employs spatio-temporal attention, yielding a more stable representation of the object's overall deformation. Building on this representation, we train a latent diffusion model that, conditioned on the input video and the mesh reconstructed from the first frame, predicts the full animation in one shot. We evaluate Mesh4D on reconstruction and novel view synthesis benchmarks, outperforming prior methods in recovering accurate 3D shape and deformation.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [174] [Generalization to Political Beliefs from Fine-Tuning on Sports Team Preferences](https://arxiv.org/abs/2601.04369)
*Owen Terry*

Main category: physics.soc-ph

TL;DR: Fine-tuning LLMs on sports team preferences leads to unexpected political belief changes, but not in the predicted liberal/conservative directions.


<details>
  <summary>Details</summary>
Motivation: To investigate how fine-tuning LLMs on narrow datasets (sports team preferences) leads to unexpected generalization and changes in seemingly unrelated domains like political beliefs.

Method: Fine-tuned LLMs to prefer either coastal or Southern sports teams, then evaluated their political beliefs through numerical ratings of agreement with political statements and qualitative analysis of justifications for radical answers.

Result: Both coastal and Southern fine-tuned models showed similar political responses without clear liberal/conservative bias, contrary to expectations. Models exhibited varying willingness to justify radical answers.

Conclusion: Fine-tuning on simple, narrow datasets can lead to unexpected behavioral changes in unrelated domains, requiring further research to understand the underlying mechanisms.

Abstract: Fine-tuned LLMs often exhibit unexpected behavior as a result of generalizing beyond the data they're shown. We present results in which an LLM fine-tuned to prefer either coastal sports teams or Southern sports teams adopt political beliefs that diverge significantly from those of the base model. While we hypothesized that the coastal model would become more liberal and the southern model would become more conservative, we find that their responses are usually similar to each other, without a clear-cut liberal or conservative bias. In addition to asking the models for numerical ratings of agreement with relevant political statements, we ask them to elaborate on their more radical answers, finding varying degrees of willingness to justify themselves. Further work is needed to understand the mechanisms by which fine-tuning on simple, narrow datasets leads to seemingly unrelated changes in model behavior.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [175] [Re-Rankers as Relevance Judges](https://arxiv.org/abs/2601.04455)
*Chuan Meng,Jiqun Liu,Mohammad Aliannejadi,Fengran Mo,Jeff Dalton,Maarten de Rijke*

Main category: cs.IR

TL;DR: Reusing established re-ranking models as relevance judges can outperform specialized LLM-based judges in many cases, while revealing evaluation biases toward same-family models.


<details>
  <summary>Details</summary>
Motivation: Current research treats LLM-based relevance judgment prediction as a separate task from re-ranking, despite both being forms of relevance prediction. This leads to redundant development and wasted resources. The paper aims to bridge this gap by exploring whether established re-ranking methods can be effectively adapted for relevance judgment tasks.

Method: Two adaptation strategies: (1) using binary tokens ("true"/"false") generated by re-rankers as direct judgments, and (2) converting continuous re-ranking scores into binary labels via thresholding. Experiments conducted on TREC-DL 2019-2023 with 8 re-rankers from 3 families (220M to 32B parameters), analyzing evaluation bias of re-ranker-based judges.

Result: Re-ranker-based relevance judges outperform UMBRELA (state-of-the-art LLM-based judge) in 40-50% of cases. They exhibit strong self-preference toward their own and same-family re-rankers, as well as cross-family bias in evaluation.

Conclusion: Established re-ranking methods can be effectively repurposed for relevance judgment tasks, potentially reducing redundant development. However, careful consideration of evaluation biases is necessary when using re-ranker-based judges, particularly their tendency to favor similar models.

Abstract: Using large language models (LLMs) to predict relevance judgments has shown promising results. Most studies treat this task as a distinct research line, e.g., focusing on prompt design for predicting relevance labels given a query and passage. However, predicting relevance judgments is essentially a form of relevance prediction, a problem extensively studied in tasks such as re-ranking. Despite this potential overlap, little research has explored reusing or adapting established re-ranking methods to predict relevance judgments, leading to potential resource waste and redundant development. To bridge this gap, we reproduce re-rankers in a re-ranker-as-relevance-judge setup. We design two adaptation strategies: (i) using binary tokens (e.g., "true" and "false") generated by a re-ranker as direct judgments, and (ii) converting continuous re-ranking scores into binary labels via thresholding. We perform extensive experiments on TREC-DL 2019 to 2023 with 8 re-rankers from 3 families, ranging from 220M to 32B, and analyse the evaluation bias exhibited by re-ranker-based judges. Results show that re-ranker-based relevance judges, under both strategies, can outperform UMBRELA, a state-of-the-art LLM-based relevance judge, in around 40% to 50% of the cases; they also exhibit strong self-preference towards their own and same-family re-rankers, as well as cross-family bias.

</details>


### [176] [Succeeding at Scale: Automated Multi-Retriever Fusion and Query-Side Adaptation for Multi-Tenant Search](https://arxiv.org/abs/2601.04646)
*Prateek Jain,Shabari S Nair,Ritesh Goru,Prakhar Agarwal,Ajay Yadav,Yoga Sri Varshan Varadharajan,Constantine Caramanis*

Main category: cs.IR

TL;DR: DevRev Search introduces a passage retrieval benchmark for technical support using automatic pipeline with LLM-as-a-Judge filtering, and proposes Index-Preserving Adaptation via query-only LoRA fine-tuning to avoid costly document re-indexing.


<details>
  <summary>Details</summary>
Motivation: Large-scale multi-tenant retrieval systems face two key challenges: 1) lack of curated relevance labels ("dark data" problem) despite having vast query logs, and 2) prohibitive operational costs of model updates requiring full corpus re-indexing in multi-tenant environments with thousands of isolated indices.

Method: 1) Created DevRev Search benchmark using automatic pipeline with fusion-based candidate generation (pooling results from diverse sparse/dense retrievers) and LLM-as-a-Judge for consistency filtering and relevance assignment. 2) Proposed Index-Preserving Adaptation strategy: fine-tuning only query encoder via Low-Rank Adaptation (LoRA) while keeping document index frozen, with experiments targeting specific transformer layers for optimal trade-offs.

Result: The approach achieves competitive performance improvements on DevRev Search and SciFact benchmarks while avoiding costly document re-indexing. Targeting specific transformer layers in query encoder yields optimal quality-efficiency trade-offs.

Conclusion: The proposed methods offer a scalable path for personalized enterprise search by addressing both the dark data problem and operational cost challenges in multi-tenant retrieval systems through automated benchmark creation and index-preserving adaptation techniques.

Abstract: Large-scale multi-tenant retrieval systems amass vast user query logs yet critically lack the curated relevance labels required for effective domain adaptation. This "dark data" problem is exacerbated by the operational cost of model updates: jointly fine-tuning query and document encoders requires re-indexing the entire corpus, which is prohibitive in multi-tenant environments with thousands of isolated indices. To address these dual challenges, we introduce \textbf{DevRev Search}, a passage retrieval benchmark for technical customer support constructed through a fully automatic pipeline. We employ a \textbf{fusion-based candidate generation} strategy, pooling results from diverse sparse and dense retrievers, and utilize an LLM-as-a-Judge to perform rigorous \textbf{consistency filtering} and relevance assignment. We further propose a practical \textbf{Index-Preserving Adaptation} strategy: by fine-tuning only the query encoder via Low-Rank Adaptation (LoRA), we achieve competitive performance improvements while keeping the document index frozen. Our experiments on DevRev Search and SciFact demonstrate that targeting specific transformer layers in the query encoder yields optimal quality-efficiency trade-offs, offering a scalable path for personalized enterprise search.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [177] [ArtCognition: A Multimodal AI Framework for Affective State Sensing from Visual and Kinematic Drawing Cues](https://arxiv.org/abs/2601.04297)
*Behrad Binaei-Haghighi,Nafiseh Sadat Sajadi,Mehrad Liviyan,Reyhane Akhavan Kharazi,Fatemeh Amirkhani,Behnam Bahrak*

Main category: cs.LG

TL;DR: ArtCognition: A multimodal framework using digital drawing (House-Tree-Person test) for automated psychological assessment by fusing visual features from final artwork with behavioral kinematic cues from the drawing process, enhanced by RAG for explainable analysis.


<details>
  <summary>Details</summary>
Motivation: Objective assessment of human affective and psychological states is challenging, especially through non-verbal channels. Digital drawing represents a rich but underexplored modality for affective sensing that could provide non-intrusive assessment.

Method: Multimodal framework (ArtCognition) analyzes HTP test by fusing: 1) static visual features from final artwork using computer vision models, and 2) dynamic behavioral kinematic cues from drawing process (stroke speed, pauses, smoothness). Uses Retrieval-Augmented Generation (RAG) architecture to bridge low-level features with psychological interpretation.

Result: Fusion of visual and behavioral kinematic cues provides more nuanced assessment than either modality alone. Shows significant correlations between extracted multimodal features and standardized psychological metrics, validating framework as scalable clinical support tool.

Conclusion: Contributes new methodology for non-intrusive affective state assessment using digital drawing, opens avenues for technology-assisted mental healthcare with explainable, knowledge-grounded analysis that reduces model hallucination.

Abstract: The objective assessment of human affective and psychological states presents a significant challenge, particularly through non-verbal channels. This paper introduces digital drawing as a rich and underexplored modality for affective sensing. We present a novel multimodal framework, named ArtCognition, for the automated analysis of the House-Tree-Person (HTP) test, a widely used psychological instrument. ArtCognition uniquely fuses two distinct data streams: static visual features from the final artwork, captured by computer vision models, and dynamic behavioral kinematic cues derived from the drawing process itself, such as stroke speed, pauses, and smoothness. To bridge the gap between low-level features and high-level psychological interpretation, we employ a Retrieval-Augmented Generation (RAG) architecture. This grounds the analysis in established psychological knowledge, enhancing explainability and reducing the potential for model hallucination. Our results demonstrate that the fusion of visual and behavioral kinematic cues provides a more nuanced assessment than either modality alone. We show significant correlations between the extracted multimodal features and standardized psychological metrics, validating the framework's potential as a scalable tool to support clinicians. This work contributes a new methodology for non-intrusive affective state assessment and opens new avenues for technology-assisted mental healthcare.

</details>


### [178] [Aligned explanations in neural networks](https://arxiv.org/abs/2601.04378)
*Corentin Lobet,Francesca Chiaromonte*

Main category: cs.LG

TL;DR: PiNets are pseudo-linear neural networks designed for explanatory alignment, producing instance-wise linear predictions that are directly linked to model decisions rather than post-hoc rationalizations.


<details>
  <summary>Details</summary>
Motivation: Current feature attribution methods for explaining neural networks are often post-hoc rationalizations that don't truly reflect the model's prediction process, creating a need for explanations that are directly aligned with predictions to build trustworthy AI systems.

Method: PiNets (pseudo-linear networks) are proposed as a modeling framework that produces instance-wise linear predictions in arbitrary feature spaces, making them linearly readable and enabling direct explanatory alignment.

Result: PiNets demonstrate faithful explanations across multiple criteria in addition to alignment on image classification and segmentation tasks, showing how explanatory alignment can be achieved in deep learning.

Conclusion: Model readability through PiNets provides a principled approach to achieving explanatory alignment, moving beyond post-hoc feature attribution toward explanations that are directly linked to prediction-making processes.

Abstract: Feature attribution is the dominant paradigm for explaining deep neural networks. However, most existing methods only loosely reflect the model's prediction-making process, thereby merely white-painting the black box. We argue that explanatory alignment is a key aspect of trustworthiness in prediction tasks: explanations must be directly linked to predictions, rather than serving as post-hoc rationalizations. We present model readability as a design principle enabling alignment, and PiNets as a modeling framework to pursue it in a deep learning context. PiNets are pseudo-linear networks that produce instance-wise linear predictions in an arbitrary feature space, making them linearly readable. We illustrate their use on image classification and segmentation tasks, demonstrating how PiNets produce explanations that are faithful across multiple criteria in addition to alignment.

</details>


### [179] [IGenBench: Benchmarking the Reliability of Text-to-Infographic Generation](https://arxiv.org/abs/2601.04498)
*Yinghao Tang,Xueding Liu,Boyuan Zhang,Tingfeng Lan,Yupeng Xie,Jiale Lao,Yiyao Wang,Haoxuan Li,Tingting Gao,Bo Pan,Luoxuan Weng,Xiuqi Huang,Minfeng Zhu,Yingchaojie Feng,Yuyu Luo,Wei Chen*

Main category: cs.LG

TL;DR: IGENBENCH is the first benchmark for evaluating text-to-infographic generation reliability, revealing significant gaps in current models' ability to produce accurate infographics despite aesthetic appeal.


<details>
  <summary>Details</summary>
Motivation: While text-to-image models can generate visually appealing images, their reliability for infographic generation remains unclear, as generated infographics often contain subtle but critical errors in data encoding and textual content that are easily overlooked.

Method: Created IGENBENCH with 600 curated test cases across 30 infographic types, designed an automated evaluation framework decomposing reliability into atomic yes/no questions based on 10 question types, and used multimodal LLMs to verify each question.

Result: Evaluation of 10 state-of-the-art T2I models revealed: 1) three-tier performance hierarchy with top model achieving 0.90 question-level accuracy but only 0.49 infographic-level accuracy; 2) data-related dimensions are universal bottlenecks (e.g., Data Completeness: 0.21); 3) no model achieves end-to-end correctness.

Conclusion: Current text-to-image models struggle with reliable infographic generation, particularly with data accuracy and completeness, highlighting the need for specialized models or improvements to handle the complex requirements of infographic creation.

Abstract: Infographics are composite visual artifacts that combine data visualizations with textual and illustrative elements to communicate information. While recent text-to-image (T2I) models can generate aesthetically appealing images, their reliability in generating infographics remains unclear. Generated infographics may appear correct at first glance but contain easily overlooked issues, such as distorted data encoding or incorrect textual content. We present IGENBENCH, the first benchmark for evaluating the reliability of text-to-infographic generation, comprising 600 curated test cases spanning 30 infographic types. We design an automated evaluation framework that decomposes reliability verification into atomic yes/no questions based on a taxonomy of 10 question types. We employ multimodal large language models (MLLMs) to verify each question, yielding question-level accuracy (Q-ACC) and infographic-level accuracy (I-ACC). We comprehensively evaluate 10 state-of-the-art T2I models on IGENBENCH. Our systematic analysis reveals key insights for future model development: (i) a three-tier performance hierarchy with the top model achieving Q-ACC of 0.90 but I-ACC of only 0.49; (ii) data-related dimensions emerging as universal bottlenecks (e.g., Data Completeness: 0.21); and (iii) the challenge of achieving end-to-end correctness across all models. We release IGENBENCH at https://igen-bench.vercel.app/.

</details>


### [180] [A Vision for Multisensory Intelligence: Sensing, Synergy, and Science](https://arxiv.org/abs/2601.04563)
*Paul Pu Liang*

Main category: cs.LG

TL;DR: A research vision for multisensory AI over the next decade, aiming to connect AI to human senses and physical/social signals through three themes: sensing, science, and synergy.


<details>
  <summary>Details</summary>
Motivation: Current AI primarily advances in digital modalities (text, vision, audio), but human experience is multisensory. There's a need to connect AI to human senses and broader physical/social signals for richer human-AI interaction.

Method: Proposes a research framework with three interrelated themes: 1) Sensing - extending AI's ability to capture the world beyond digital media, 2) Science - developing principled approaches for multimodal heterogeneity, unified architectures, and cross-modal transfer, 3) Synergy - learning interactions between modalities and between humans and AI.

Result: Presents a comprehensive vision paper outlining future research directions, accompanied by projects, resources, and demos from the MIT Media Lab's Multisensory Intelligence group.

Conclusion: Multisensory AI represents a transformative direction that can fundamentally change how humans and AI experience and interact with each other by connecting AI to the full spectrum of human senses and environmental signals.

Abstract: Our experience of the world is multisensory, spanning a synthesis of language, sight, sound, touch, taste, and smell. Yet, artificial intelligence has primarily advanced in digital modalities like text, vision, and audio. This paper outlines a research vision for multisensory artificial intelligence over the next decade. This new set of technologies can change how humans and AI experience and interact with one another, by connecting AI to the human senses and a rich spectrum of signals from physiological and tactile cues on the body, to physical and social signals in homes, cities, and the environment. We outline how this field must advance through three interrelated themes of sensing, science, and synergy. Firstly, research in sensing should extend how AI captures the world in richer ways beyond the digital medium. Secondly, developing a principled science for quantifying multimodal heterogeneity and interactions, developing unified modeling architectures and representations, and understanding cross-modal transfer. Finally, we present new technical challenges to learn synergy between modalities and between humans and AI, covering multisensory integration, alignment, reasoning, generation, generalization, and experience. Accompanying this vision paper are a series of projects, resources, and demos of latest advances from the Multisensory Intelligence group at the MIT Media Lab, see https://mit-mi.github.io/.

</details>


### [181] [The Forgotten Shield: Safety Grafting in Parameter-Space for Medical MLLMs](https://arxiv.org/abs/2601.04199)
*Jiale Zhao,Xing Mou,Jinlin Wu,Hongyuan Yu,Mingrui Sun,Yang Shi,Xuanwu Yin,Zhen Chen,Zhen Lei,Yaohua Wang*

Main category: cs.LG

TL;DR: Medical MLLMs have safety vulnerabilities, especially to cross-modality jailbreak attacks, and suffer from safety forgetting during medical fine-tuning. The paper proposes Parameter-Space Intervention to re-align safety without extra safety data.


<details>
  <summary>Details</summary>
Motivation: Medical MLLMs have advanced in medical tasks but lack safety research, posing risks for real-world deployment. Current models show pervasive safety vulnerabilities and suffer from catastrophic forgetting of safety alignment during medical fine-tuning.

Method: Proposes Parameter-Space Intervention: extracts intrinsic safety knowledge from original base models and injects it into target models during medical capability construction. Uses fine-grained parameter search algorithm to optimize safety-performance trade-off.

Result: Experimental results show the approach significantly enhances Medical MLLM safety guardrails without requiring additional domain-specific safety data, while minimizing degradation to core medical performance.

Conclusion: The Parameter-Space Intervention method effectively addresses safety vulnerabilities in Medical MLLMs by re-aligning safety knowledge during medical fine-tuning, providing a practical solution for safe real-world deployment.

Abstract: Medical Multimodal Large Language Models (Medical MLLMs) have achieved remarkable progress in specialized medical tasks; however, research into their safety has lagged, posing potential risks for real-world deployment. In this paper, we first establish a multidimensional evaluation framework to systematically benchmark the safety of current SOTA Medical MLLMs. Our empirical analysis reveals pervasive vulnerabilities across both general and medical-specific safety dimensions in existing models, particularly highlighting their fragility against cross-modality jailbreak attacks. Furthermore, we find that the medical fine-tuning process frequently induces catastrophic forgetting of the model's original safety alignment. To address this challenge, we propose a novel "Parameter-Space Intervention" approach for efficient safety re-alignment. This method extracts intrinsic safety knowledge representations from original base models and concurrently injects them into the target model during the construction of medical capabilities. Additionally, we design a fine-grained parameter search algorithm to achieve an optimal trade-off between safety and medical performance. Experimental results demonstrate that our approach significantly bolsters the safety guardrails of Medical MLLMs without relying on additional domain-specific safety data, while minimizing degradation to core medical performance.

</details>


### [182] [Mitigating Position-Shift Failures in Text-Based Modular Arithmetic via Position Curriculum and Template Diversity](https://arxiv.org/abs/2601.04283)
*Nikolay Yudin*

Main category: cs.LG

TL;DR: Transformers trained for modular addition fail catastrophically when input format changes (position shift or different templates), despite high in-distribution accuracy. A simple training recipe with boundary markers, position curriculum, diverse templates, and consistency training improves robustness.


<details>
  <summary>Details</summary>
Motivation: Current models achieve high in-distribution accuracy but fail under input format variations like position shifts or different natural language templates. This reveals an under-emphasized failure mode where procedural knowledge doesn't generalize robustly.

Method: 1) Baseline model trained on modular addition from text. 2) Proposed training recipe: explicit expression boundary markers, position curriculum (broadening absolute position range), diverse template mixtures, and consistency training across multiple variants per example. 3) Evaluation using disjoint-pair split over all ordered pairs for p=97.

Result: Baseline model achieves strong in-distribution performance but collapses under position shift and template OOD. The proposed intervention substantially improves robustness to both position shift and template OOD while maintaining high in-distribution accuracy. ALiBi-style ablation fails to learn the task.

Conclusion: Steering procedural generalization under noisy supervision benefits from explicitly training invariances absent from the data distribution. The paper provides reproducible evaluation protocol and artifacts for studying robustness to input format variations.

Abstract: Building on insights from the grokking literature, we study character-level Transformers trained to compute modular addition from text, and focus on robustness under input-format variation rather than only in-distribution accuracy. We identify a previously under-emphasized failure mode: models that achieve high in-distribution accuracy can fail catastrophically when the same expression is shifted to different absolute character positions ("position shift") or presented under out-of-distribution natural-language templates. Using a disjoint-pair split over all ordered pairs for p=97, we show that a baseline model reaches strong in-distribution performance yet collapses under position shift and template OOD. We then introduce a simple training recipe that combines (i) explicit expression boundary markers, (ii) position curriculum that broadens the range of absolute positions seen during training, (iii) diverse template mixtures, and (iv) consistency training across multiple variants per example. Across three seeds, this intervention substantially improves robustness to position shift and template OOD while maintaining high in-distribution accuracy, whereas an ALiBi-style ablation fails to learn the task under our setup. Our results suggest that steering procedural generalization under noisy supervision benefits from explicitly training invariances that are otherwise absent from the data distribution, and we provide a reproducible evaluation protocol and artifacts.

</details>


### [183] [Quantifying the Effect of Test Set Contamination on Generative Evaluations](https://arxiv.org/abs/2601.04301)
*Rylan Schaeffer,Joshua Kazdan,Baber Abbasi,Ken Ziyu Liu,Brando Miranda,Ahmed Ahmed,Abhay Puri,Niloofar Mireshghallah,Sanmi Koyejo*

Main category: cs.LG

TL;DR: Test set contamination significantly boosts generative model performance, with even single test set replicas enabling models to surpass irreducible error limits, and contamination effects can be modulated by further training and inference settings.


<details>
  <summary>Details</summary>
Motivation: While test set contamination's impact on discriminative evaluations is well-studied, its effects on generative evaluations remain under-explored, creating a critical gap for trustworthy AI assessment as models are trained on web-scale data.

Method: Pretrained language models on mixtures of web data and MATH benchmark, varying model sizes and number of test set replicas; used scaling laws to analyze contamination effects; studied further training (overtraining with fresh data, supervised finetuning) and inference factors (sampling temperature, solution length).

Result: Performance improves with contamination and model size; even a single test set replica enables models to achieve lower loss than irreducible error of uncontaminated training; overtraining reduces contamination effects; supervised finetuning effects depend on pretraining contamination level; high sampling temperatures mitigate contamination; longer solutions are exponentially harder to memorize.

Conclusion: Test set contamination presents complex interactions in generative evaluations, differing from discriminative settings, highlighting new challenges for trustworthy AI system evaluation and requiring careful consideration of contamination throughout the model lifecycle.

Abstract: As frontier AI systems are pretrained on web-scale data, test set contamination has become a critical concern for accurately assessing their capabilities. While research has thoroughly investigated the impact of test set contamination on discriminative evaluations like multiple-choice question-answering, comparatively little research has studied the impact of test set contamination on generative evaluations. In this work, we quantitatively assess the effect of test set contamination on generative evaluations through the language model lifecycle. We pretrain language models on mixtures of web data and the MATH benchmark, sweeping model sizes and number of test set replicas contaminating the pretraining corpus; performance improves with contamination and model size. Using scaling laws, we make a surprising discovery: including even a single test set replica enables models to achieve lower loss than the irreducible error of training on the uncontaminated corpus. We then study further training: overtraining with fresh data reduces the effects of contamination, whereas supervised finetuning on the training set can either increase or decrease performance on test data, depending on the amount of pretraining contamination. Finally, at inference, we identify factors that modulate memorization: high sampling temperatures mitigate contamination effects, and longer solutions are exponentially more difficult to memorize than shorter ones, presenting a contrast with discriminative evaluations, where solutions are only a few tokens in length. By characterizing how generation and memorization interact, we highlight a new layer of complexity for trustworthy evaluation of AI systems.

</details>


### [184] [Rate or Fate? RLV$^\varepsilon$R: Reinforcement Learning with Verifiable Noisy Rewards](https://arxiv.org/abs/2601.04411)
*Ali Rad,Khashayar Filom,Darioush Keivan,Peyman Mohajerin Esfahani,Ehsan Kamalinejad*

Main category: cs.LG

TL;DR: RLVR (Reinforcement Learning with Verifiable Rewards) faces verification noise that can cause learning collapse when Youden's index J=TPR-FPR < 0, with sharp phase transition at J=0 boundary.


<details>
  <summary>Details</summary>
Motivation: Verification in RLVR is often noisy (imperfect tests, human labels, LLM judges), especially in hard domains like coding where tests are sparse and model-generated. Need to understand whether noise merely slows learning or fundamentally changes outcomes.

Method: Developed analytically tractable multi-armed bandit view of RLVR dynamics, instantiated with GRPO and validated in controlled experiments. Modeled false positives/negatives and grouped completions into recurring reasoning modes, yielding replicator-style flow on probability simplex.

Result: Dynamics decouple into within-correct-mode competition and one-dimensional evolution for mass on incorrect modes, with drift determined solely by Youden's index J=TPR-FPR. Sharp phase transition: J>0 leads to learning, J=0 neutral, J<0 leads to anti-learning and collapse. Experiments on verifiable programming tasks confirm J=0 boundary.

Conclusion: Verification noise determines learning fate, not just rate. When J>0, noise primarily affects convergence time; when J<0, leads to collapse. Framework provides general lens for analyzing RLVR stability, convergence, and algorithmic interventions.

Abstract: Reinforcement learning with verifiable rewards (RLVR) is a simple but powerful paradigm for training LLMs: sample a completion, verify it, and update. In practice, however, the verifier is almost never clean--unit tests probe only limited corner cases; human and synthetic labels are imperfect; and LLM judges (e.g., RLAIF) are noisy and can be exploited--and this problem worsens on harder domains (especially coding) where tests are sparse and increasingly model-generated. We ask a pragmatic question: does the verification noise merely slow down the learning (rate), or can it flip the outcome (fate)?
  To address this, we develop an analytically tractable multi-armed bandit view of RLVR dynamics, instantiated with GRPO and validated in controlled experiments. Modeling false positives and false negatives and grouping completions into recurring reasoning modes yields a replicator-style (natural-selection) flow on the probability simplex. The dynamics decouples into within-correct-mode competition and a one-dimensional evolution for the mass on incorrect modes, whose drift is determined solely by Youden's index J=TPR-FPR. This yields a sharp phase transition: when J>0, the incorrect mass is driven toward extinction (learning); when J=0, the process is neutral; and when J<0, incorrect modes amplify until they dominate (anti-learning and collapse). In the learning regime J>0, noise primarily rescales convergence time ("rate, not fate"). Experiments on verifiable programming tasks under synthetic noise reproduce the predicted J=0 boundary. Beyond noise, the framework offers a general lens for analyzing RLVR stability, convergence, and algorithmic interventions.

</details>


### [185] [Not All Steps are Informative: On the Linearity of LLMs' RLVR Training](https://arxiv.org/abs/2601.04537)
*Tianle Wang,Zhongyuan Wu,Shenghao Jin,Hao Xu,Wei Chen,Ning Miao*

Main category: cs.LG

TL;DR: RL training for LLMs evolves linearly, allowing weight/logit extrapolation to achieve similar performance with less computation.


<details>
  <summary>Details</summary>
Motivation: RLVR training for LLMs requires thousands of expensive training steps due to prolonged exploration. The authors discovered that RLVR training evolves in a strongly linear manner, suggesting that future model states could be predicted via extrapolation rather than continued training.

Method: The paper proposes two extrapolation methods: 1) Weight Extrapolation - predicting future model weights from intermediate checkpoints, and 2) Logits Extrapolation - extrapolating model output log-probabilities beyond the stable training range.

Result: Weight Extrapolation produces models with performance comparable to standard RL training while requiring significantly less computation. Logits Extrapolation consistently outperforms continued RL training on all four benchmarks by extrapolating beyond the step range where RL training remains stable.

Conclusion: The linear evolution of LLMs during RLVR training enables efficient extrapolation techniques that can reduce computational costs while maintaining or even improving performance compared to traditional RL training approaches.

Abstract: Reinforcement learning with verifiable rewards (RLVR) has become a central component of large language model (LLM) post-training. Unlike supervised fine-tuning (SFT), RLVR lets an LLM generate multiple candidate solutions and reinforces those that lead to a verifiably correct final answer. However, in practice, RLVR often requires thousands of training steps to reach strong performance, incurring substantial computation largely attributed to prolonged exploration. In this work, we make a surprising observation: during RLVR, LLMs evolve in a strongly linear manner. Specifically, both model weights and model output log-probabilities exhibit strong linear correlations with RL training steps. This suggests that RLVR predominantly amplifies trends that emerge early in training, rather than continuously discovering new behaviors throughout the entire optimization trajectory. Motivated by this linearity, we investigate whether future model states can be predicted from intermediate checkpoints via extrapolation, avoiding continued expensive training. We show that Weight Extrapolation produces models with performance comparable to standard RL training while requiring significantly less computation. Moreover, Logits Extrapolation consistently outperforms continued RL training on all four benchmarks by extrapolating beyond the step range where RL training remains stable.

</details>


### [186] [On the Hidden Objective Biases of Group-based Reinforcement Learning](https://arxiv.org/abs/2601.05002)
*Aleksandar Fontana,Marco Simoni,Giulio Rossolini,Andrea Saracino,Paolo Mori*

Main category: cs.LG

TL;DR: Theoretical analysis reveals structural issues in GRPO-style group-based RL methods for LLMs: gradient biases from non-uniform group weighting, AdamW insensitivity to reward scaling, and momentum pushing updates beyond clipping bounds.


<details>
  <summary>Details</summary>
Motivation: Despite empirical success of group-based RL methods like GRPO for post-training LLMs, there are structural mismatches between reward optimization and underlying training objectives that need theoretical understanding.

Method: Theoretical analysis of GRPO-style methods using a unified surrogate formulation to study their properties systematically.

Result: Identified three key issues: (1) non-uniform group weighting creates systematic gradient biases on shared prefix tokens, (2) AdamW optimizer makes training dynamics insensitive to reward scaling, (3) optimizer momentum can push policy updates beyond intended clipping regions.

Conclusion: Current group-based RL approaches have fundamental limitations; these findings provide principled guidance for designing better formulations in the future.

Abstract: Group-based reinforcement learning methods, like Group Relative Policy Optimization (GRPO), are widely used nowadays to post-train large language models. Despite their empirical success, they exhibit structural mismatches between reward optimization and the underlying training objective. In this paper, we present a theoretical analysis of GRPO style methods by studying them within a unified surrogate formulation. This perspective reveals recurring properties that affect all the methods under analysis: (i) non-uniform group weighting induces systematic gradient biases on shared prefix tokens; (ii) interactions with the AdamW optimizer make training dynamics largely insensitive to reward scaling; and (iii) optimizer momentum can push policy updates beyond the intended clipping region under repeated optimization steps. We believe that these findings highlight fundamental limitations of current approaches and provide principled guidance for the design of future formulations.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [187] [Quantitative mapping from conventional MRI using self-supervised physics-guided deep learning: applications to a large-scale, clinically heterogeneous dataset](https://arxiv.org/abs/2601.05063)
*Jelmer van Lune,Stefano Mandija,Oscar van der Heide,Matteo Maspero,Martin B. Schilder,Jan Willem Dankbaar,Cornelis A. T. van den Berg,Alessandro Sbrizzi*

Main category: physics.med-ph

TL;DR: Self-supervised physics-guided deep learning framework converts conventional clinical MRI scans into quantitative T1, T2, and PD maps without requiring specialized qMRI protocols.


<details>
  <summary>Details</summary>
Motivation: Conventional MRI provides qualitative data dependent on scanner hardware and settings, while quantitative MRI (qMRI) offers intrinsic tissue parameters but requires specialized protocols that limit availability and large-scale biomarker research.

Method: Self-supervised physics-guided deep learning framework that integrates Bloch-based signal models into training objective, trained on 4,121 clinical scan sessions from four different 3T MRI scanners over six years to infer quantitative maps from conventional T1-weighted, T2-weighted, and FLAIR MRIs.

Result: Generated maps show tissue values consistent with literature ranges, scanner/protocol invariance (inter-group CV ≤1.1%), excellent reproducibility (Pearson r >0.82), and low mean relative voxel-wise differences (especially T2 <6%) across 600+ test sessions.

Conclusion: The framework can robustly transform diverse clinical conventional MRI into quantitative maps, potentially enabling large-scale quantitative biomarker research without requiring specialized qMRI protocols.

Abstract: Magnetic resonance imaging (MRI) is a cornerstone of clinical neuroimaging, yet conventional MRIs provide qualitative information heavily dependent on scanner hardware and acquisition settings. While quantitative MRI (qMRI) offers intrinsic tissue parameters, the requirement for specialized acquisition protocols and reconstruction algorithms restricts its availability and impedes large-scale biomarker research. This study presents a self-supervised physics-guided deep learning framework to infer quantitative T1, T2, and proton-density (PD) maps directly from widely available clinical conventional T1-weighted, T2-weighted, and FLAIR MRIs. The framework was trained and evaluated on a large-scale, clinically heterogeneous dataset comprising 4,121 scan sessions acquired at our institution over six years on four different 3 T MRI scanner systems, capturing real-world clinical variability. The framework integrates Bloch-based signal models directly into the training objective. Across more than 600 test sessions, the generated maps exhibited white matter and gray matter values consistent with literature ranges. Additionally, the generated maps showed invariance to scanner hardware and acquisition protocol groups, with inter-group coefficients of variation $\leq$ 1.1%. Subject-specific analyses demonstrated excellent voxel-wise reproducibility across scanner systems and sequence parameters, with Pearson $r$ and concordance correlation coefficients exceeding 0.82 for T1 and T2. Mean relative voxel-wise differences were low across all quantitative parameters, especially for T2 ($<$ 6%). These results indicate that the proposed framework can robustly transform diverse clinical conventional MRI data into quantitative maps, potentially paving the way for large-scale quantitative biomarker research.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [188] [Generative Teaching via Code](https://arxiv.org/abs/2601.04204)
*Yuheng Wang,Runde Yang,Lin Wu,Jie Zhang,Jingru Fan,Ruoyu Fu,Tianle Zhou,Huatao Li,Siheng Chen,Weinan E,Chen Qian*

Main category: cs.CY

TL;DR: TeachMaster is a multi-agent framework that automates educational video creation using code as intermediate semantic medium, shifting educators from manual creators to high-level directors.


<details>
  <summary>Details</summary>
Motivation: High costs and slow cycles of manual educational content creation hinder scalable online education. Current video generation approaches lack pedagogical structure and precise control due to their pixel-level, black-box nature.

Method: TeachMaster uses a multi-agent framework with code as intermediate semantic medium. It orchestrates collaborative agents for planning, design, and rendering to automate production of interpretable, editable, curriculum-ready educational videos.

Result: Experiments show TeachMaster significantly boosts production efficiency without compromising structural coherence or visual fidelity, providing robust solution for scalable education.

Conclusion: Generative Teaching paradigm enables educators to focus on pedagogical intent while autonomous agents handle execution, offering scalable solution for high-quality online education.

Abstract: The scalability of high-quality online education is hindered by the high costs and slow cycles of labor-intensive manual content creation. Despite advancements in video generation, current approaches often fail to ensure pedagogical structure and precise control due to their pixel-level, black-box nature. In this paper, we propose Generative Teaching, a novel paradigm that transitions educators from manual creators to high-level directors, allowing them to focus on pedagogical intent while autonomous agents handle the execution. To realize this vision, we introduce TeachMaster, a multi-agent framework that leverages code as an intermediate semantic medium. Unlike traditional video generation methods, TeachMaster orchestrates a collaborative team of agents--spanning planning, design, and rendering--to automate the production of interpretable, editable, and curriculum-ready educational videos. Experiments validate that TeachMaster significantly boosts production efficiency without compromising structural coherence or visual fidelity, providing a robust solution for scalable education.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [189] [Multi-Disciplinary Dataset Discovery from Citation-Verified Literature Contexts](https://arxiv.org/abs/2601.05099)
*Zhiyin Tan,Changxu Duan*

Main category: cs.DL

TL;DR: A literature-driven framework discovers datasets from citation contexts in scientific papers, achieving higher recall than existing dataset search engines by grounding retrieval in actual research use rather than metadata quality.


<details>
  <summary>Details</summary>
Motivation: Existing dataset search engines rely heavily on metadata quality and keyword overlap, which often fail to capture the semantic intent of scientific investigation, making it challenging to identify suitable datasets for research questions.

Method: Combines large-scale citation-context extraction, schema-guided dataset recognition with Large Language Models, and provenance-preserving entity resolution to discover datasets from citation contexts in scientific papers.

Result: Achieves substantially higher recall than Google Dataset Search and DataCite Commons (normalized recall: average 47.47%, highest 81.82%), surfaces additional datasets not documented in surveys, with expert assessments indicating high utility and novelty.

Conclusion: Citation-context mining establishes an effective and generalizable paradigm for dataset discovery, particularly in settings where datasets lack sufficient or reliable metadata.

Abstract: Identifying suitable datasets for a research question remains challenging because existing dataset search engines rely heavily on metadata quality and keyword overlap, which often fail to capture the semantic intent of scientific investigation. We introduce a literature-driven framework that discovers datasets from citation contexts in scientific papers, enabling retrieval grounded in actual research use rather than metadata availability. Our approach combines large-scale citation-context extraction, schema-guided dataset recognition with Large Language Models, and provenance-preserving entity resolution. We evaluate the system on eight survey-derived computer science queries and find that it achieves substantially higher recall than Google Dataset Search and DataCite Commons, with normalized recall ranging from an average of 47.47% to a highest value of 81.82%. Beyond recovering gold-standard datasets, the method also surfaces additional datasets not documented in the surveys. Expert assessments across five top-level Fields of Science indicate that a substantial portion of the additional datasets are considered high utility, and some are regarded as novel for the specific topics chosen by the experts. These findings establish citation-context mining as an effective and generalizable paradigm for dataset discovery, particularly in settings where datasets lack sufficient or reliable metadata. To support reproducibility and future extensions, we release our code, evaluation datasets, and results on GitHub (https://github.com/Fireblossom/citation-context-dataset-discovery).

</details>


### [190] [Semantically Orthogonal Framework for Citation Classification: Disentangling Intent and Content](https://arxiv.org/abs/2601.05103)
*Changxu Duan,Zhiyin Tan*

Main category: cs.DL

TL;DR: SOFT is a new citation classification framework that separates citation intent from cited content type, improving annotation clarity and classification performance compared to existing frameworks.


<details>
  <summary>Details</summary>
Motivation: Existing citation classification frameworks conflate citation intent (why a work is cited) with cited content type (what part is cited), limiting their effectiveness in automatic classification due to a dilemma between fine-grained type distinctions and practical classification reliability.

Method: SOFT introduces a Semantically Orthogonal Framework with Two dimensions that explicitly separates citation intent from cited content type, drawing inspiration from semantic role theory. The authors systematically re-annotate the ACL-ARC dataset using SOFT and create a cross-disciplinary test set from ACT2.

Result: Evaluation with zero-shot and fine-tuned Large Language Models shows that SOFT enables higher agreement between human annotators and LLMs, supports stronger classification performance, and provides robust cross-domain generalization compared to ACL-ARC and SciCite frameworks.

Conclusion: SOFT serves as a clear, reusable annotation standard that improves clarity, consistency, and generalizability for digital libraries and scholarly communication infrastructures. All code and data are publicly available.

Abstract: Understanding the role of citations is essential for research assessment and citation-aware digital libraries. However, existing citation classification frameworks often conflate citation intent (why a work is cited) with cited content type (what part is cited), limiting their effectiveness in auto classification due to a dilemma between fine-grained type distinctions and practical classification reliability. We introduce SOFT, a Semantically Orthogonal Framework with Two dimensions that explicitly separates citation intent from cited content type, drawing inspiration from semantic role theory. We systematically re-annotate the ACL-ARC dataset using SOFT and release a cross-disciplinary test set sampled from ACT2. Evaluation with both zero-shot and fine-tuned Large Language Models demonstrates that SOFT enables higher agreement between human annotators and LLMs, and supports stronger classification performance and robust cross-domain generalization compared to ACL-ARC and SciCite annotation frameworks. These results confirm SOFT's value as a clear, reusable annotation standard, improving clarity, consistency, and generalizability for digital libraries and scholarly communication infrastructures. All code and data are publicly available on GitHub https://github.com/zhiyintan/SOFT.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [191] [End-to-end differentiable design of geometric waveguide displays](https://arxiv.org/abs/2601.04370)
*Xinge Yang,Zhaocheng Liu,Zhaoyu Nie,Qingyuan Fan,Zhimin Shi,Jim Bonar,Wolfgang Heidrich*

Main category: physics.optics

TL;DR: First end-to-end differentiable optimization framework for geometric waveguides that jointly optimizes non-sequential light transport and polarization-dependent multilayer thin-film coatings, achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Geometric waveguides are promising for AR displays but performance is bottlenecked by the difficulty of jointly optimizing non-sequential light transport and polarization-dependent multilayer thin-film coatings. Current methods can't handle the complexity of these coupled optimization problems.

Method: Developed an end-to-end differentiable optimization framework that couples non-sequential Monte Carlo polarization ray tracing with a differentiable transfer-matrix thin-film solver. Uses memory-saving strategies to optimize thousands of layer-thickness parameters and billions of ray-surface intersections on multi-GPU workstations. Includes automated layer pruning through over-parameterization and driving redundant layers to zero under manufacturability constraints (topology optimization). Also jointly optimizes waveguide with image preprocessing network.

Result: On a representative design, starting from random initialization, the method increased light efficiency from 4.1% to 33.5%, improved eyebox uniformity by ~17×, and improved FoV uniformity by ~11×. The framework enables system-level, high-dimensional coating optimization inside waveguides.

Conclusion: The framework not only enables system-level, high-dimensional coating optimization for geometric waveguides but also expands the scope of differentiable optics for next-generation optical design, representing a significant advancement in AR display technology.

Abstract: Geometric waveguides are a promising architecture for optical see-through augmented reality displays, but their performance is severely bottlenecked by the difficulty of jointly optimizing non-sequential light transport and polarization-dependent multilayer thin-film coatings. Here we present the first end-to-end differentiable optimization framework for geometric waveguide that couples non-sequential Monte Carlo polarization ray tracing with a differentiable transfer-matrix thin-film solver. A differentiable Monte Carlo ray tracer avoids the exponential growth of deterministic ray splitting while enabling gradients backpropagation from eyebox metrics to design parameters. With memory-saving strategies, we optimize more than one thousand layer-thickness parameters and billions of non-sequential ray-surface intersections on a single multi-GPU workstation. Automated layer pruning is achieved by starting from over-parameterized stacks and driving redundant layers to zero thickness under discrete manufacturability constraints, effectively performing topology optimization to discover optimal coating structures. On a representative design, starting from random initialization within thickness bounds, our method increases light efficiency from 4.1\% to 33.5\% and improves eyebox and FoV uniformity by $\sim$17$\times$ and $\sim$11$\times$, respectively. Furthermore, we jointly optimize the waveguide and an image preprocessing network to improve perceived image quality. Our framework not only enables system-level, high-dimensional coating optimization inside the waveguide, but also expands the scope of differentiable optics for next-generation optical design.

</details>


### [192] [Illumination Angular Spectrum Encoding for Controlling the Functionality of Diffractive Networks](https://arxiv.org/abs/2601.04825)
*Matan Kleiner,Lior Michaeli,Tomer Michaeli*

Main category: physics.optics

TL;DR: A single diffractive neural network can perform multiple image-to-image translation tasks by controlling the illumination's angular spectrum with amplitude masks, enabling multi-task all-optical computing without mechanical reconfiguration.


<details>
  <summary>Details</summary>
Motivation: Current diffractive neural networks are typically single-task systems, limiting their adoption in applications requiring multiple functionalities. Existing multi-task approaches require mechanical reconfiguration or different wavelengths/polarizations per task, which are not scalable or versatile.

Method: Proposes using the illumination's angular spectrum as a control mechanism. Different amplitude masks shape the illumination's angular spectrum to encode different tasks. A single diffractive network is trained to perform multiple image-to-image translations, with the illumination mask determining the specific task (e.g., translating handwritten digits to different typeset digits or letters).

Result: Demonstrated that effective control can be achieved over a very narrow angular range within the paraxial regime. The framework works under different coherence conditions and can be combined with existing control strategies like different wavelengths. Successfully trained a single network to translate handwritten digits to typeset digits of different values, and handwritten English letters to typeset numbers and Greek letters.

Conclusion: The illumination angular spectrum serves as a powerful degree of freedom for controlling diffractive networks, enabling a scalable and versatile framework for multi-task all-optical computing without mechanical reconfiguration.

Abstract: Diffractive neural networks have recently emerged as a promising framework for all-optical computing. However, these networks are typically trained for a single task, limiting their potential adoption in systems requiring multiple functionalities. Existing approaches to achieving multi-task functionality either modify the mechanical configuration of the network per task or use a different illumination wavelength or polarization state for each task. In this work, we propose a new control mechanism, which is based on the illumination's angular spectrum. Specifically, we shape the illumination using an amplitude mask that selectively controls its angular spectrum. We employ different illumination masks for achieving different network functionalities, so that the mask serves as a unique task encoder. Interestingly, we show that effective control can be achieved over a very narrow angular range, within the paraxial regime. We numerically illustrate the proposed approach by training a single diffractive network to perform multiple image-to-image translation tasks. In particular, we demonstrate translating handwritten digits into typeset digits of different values, and translating handwritten English letters into typeset numbers and typeset Greek letters, where the type of the output is determined by the illumination's angular components. As we show, the proposed framework can work under different coherence conditions, and can be combined with existing control strategies, such as different wavelengths. Our results establish the illumination angular spectrum as a powerful degree of freedom for controlling diffractive networks, enabling a scalable and versatile framework for multi-task all-optical computing.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [193] [In-SRAM Radiant Foam Rendering on a Graph Processor](https://arxiv.org/abs/2601.04382)
*Zulkhuu Tuya,Ignacio Alzugaray,Nicholas Fry,Andrew J. Davison*

Main category: cs.GR

TL;DR: A distributed volumetric renderer for many-core accelerators with local SRAM that shards scene data across tiles and routes rays hierarchically to enable fully in-SRAM rendering without large unified memory.


<details>
  <summary>Details</summary>
Motivation: Many-core accelerators with distributed local SRAM break the assumption of large unified memory access needed for traditional volumetric rendering. Efficient rendering on such hardware requires distributing data/computation, keeping ray traversal local, and structuring communication predictably.

Method: Developed a fully in-SRAM distributed renderer using Radiant Foam Voronoi-cell representation on Graphcore Mk2 IPU. System shards scene across tiles and forwards rays between shards through hierarchical routing overlay, enabling ray marching entirely from on-chip SRAM with predictable communication.

Result: Achieves near-interactive throughput (~1 fps at 640×480) on Mip-NeRF 360 scenes with image/depth quality close to original GPU-based Radiant Foam implementation, while keeping all scene data and ray state in on-chip SRAM.

Conclusion: Demonstrates feasibility of distributed-memory rendering and analyzes routing, memory, and scheduling bottlenecks to inform how future accelerators can better support irregular, data-movement-heavy rendering workloads.

Abstract: Many emerging many-core accelerators replace a single large device memory with hundreds to thousands of lightweight cores, each owning only a small local SRAM and exchanging data via explicit on-chip communication. This organization offers high aggregate bandwidth, but it breaks a key assumption behind many volumetric rendering techniques: that rays can randomly access a large, unified scene representation. Rendering efficiently on such hardware therefore requires distributing both data and computation, keeping ray traversal mostly local, and structuring communication into predictable routes.
  We present a fully in-SRAM, distributed renderer for the \emph{Radiant Foam} Voronoi-cell volumetric representation on the Graphcore Mk2 IPU, a many-core accelerator with tile-local SRAM and explicit inter-tile communication. Our system shards the scene across tiles and forwards rays between shards through a hierarchical routing overlay, enabling ray marching entirely from on-chip SRAM with predictable communication. On Mip-NeRF~360 scenes, the system attains near-interactive throughput (\(\approx\)1\,fps at \mbox{$640\times480$}) with image and depth quality close to the original GPU-based Radiant Foam implementation, while keeping all scene data and ray state in on-chip SRAM. Beyond demonstrating feasibility, we analyze routing, memory, and scheduling bottlenecks that inform how future distributed-memory accelerators can better support irregular, data-movement-heavy rendering workloads.

</details>


### [194] [GenAI-DrawIO-Creator: A Framework for Automated Diagram Generation](https://arxiv.org/abs/2601.05162)
*Jinze Yu,Dayuan Jiang*

Main category: cs.GR

TL;DR: GenAI-DrawIO-Creator uses Claude 3.7 LLM to automate diagram generation and manipulation in draw.io XML format from natural language, code, or images.


<details>
  <summary>Details</summary>
Motivation: Creating and modifying diagrams is labor-intensive, requiring automation to reduce manual effort and improve efficiency in diagram generation tasks.

Method: Framework integrates Claude 3.7 LLM with specialized prompt engineering and error-checking to produce valid draw.io XML representations from natural language, code, or images.

Result: Working prototype successfully generates accurate diagrams (network architectures, flowcharts), reduces creation time significantly, and produces outputs with high structural fidelity.

Conclusion: Demonstrates Claude 3.7's capability in structured visual reasoning and establishes foundation for future AI-assisted diagramming applications.

Abstract: Diagrams are crucial for communicating complex information, yet creating and modifying them remains a labor-intensive task. We present GenAI-DrawIO-Creator, a novel framework that leverages Large Language Models (LLMs) to automate diagram generation and manipulation in the structured XML format used by draw.io. Our system integrates Claude 3.7 to reason about structured visual data and produce valid diagram representations. Key contributions include a high-level system design enabling real-time diagram updates, specialized prompt engineering and error-checking to ensure well-formed XML outputs. We demonstrate a working prototype capable of generating accurate diagrams (such as network architectures and flowcharts) from natural language or code, and even replicating diagrams from images. Simulated evaluations show that our approach significantly reduces diagram creation time and produces outputs with high structural fidelity. Our results highlight the promise of Claude 3.7 in handling structured visual reasoning tasks and lay the groundwork for future research in AI-assisted diagramming applications.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [195] [Decentralized Privacy-Preserving Federal Learning of Computer Vision Models on Edge Devices](https://arxiv.org/abs/2601.04912)
*Damian Harenčák,Lukáš Gajdošech,Martin Madaras*

Main category: cs.CR

TL;DR: The paper analyzes privacy risks in federated learning and evaluates various methods to protect client data from both servers and other clients, including encryption, compression, and noising techniques.


<details>
  <summary>Details</summary>
Motivation: Federated learning still risks data privacy despite not sharing raw data, as model parameters can be used to reconstruct private information. Current solutions mainly focus on server-side risks, ignoring potential malicious clients.

Method: Analyzed multiple privacy-enhancing methods: homomorphic encryption, gradient compression, gradient noising, and modified federated learning systems (split learning, swarm learning, fully encrypted models). Evaluated their impact on CNN accuracy and implemented proof-of-concept on NVIDIA Jetson TX2 edge devices.

Result: Showed negative effects of gradient compression and noising on CNN classification accuracy, demonstrated difficulty of data reconstruction in segmentation networks, and successfully simulated federated learning on edge hardware.

Conclusion: Comprehensive analysis of client-side privacy methods reveals trade-offs between privacy protection and model accuracy, highlighting the need for balanced approaches in federated learning systems.

Abstract: Collaborative training of a machine learning model comes with a risk of sharing sensitive or private data. Federated learning offers a way of collectively training a single global model without the need to share client data, by sharing only the updated parameters from each client's local model. A central server is then used to aggregate parameters from all clients and redistribute the aggregated model back to the clients. Recent findings have shown that even in this scenario, private data can be reconstructed only using information about model parameters. Current efforts to mitigate this are mainly focused on reducing privacy risks on the server side, assuming that other clients will not act maliciously. In this work, we analyzed various methods for improving the privacy of client data concerning both the server and other clients for neural networks. Some of these methods include homomorphic encryption, gradient compression, gradient noising, and discussion on possible usage of modified federated learning systems such as split learning, swarm learning or fully encrypted models. We have analyzed the negative effects of gradient compression and gradient noising on the accuracy of convolutional neural networks used for classification. We have shown the difficulty of data reconstruction in the case of segmentation networks. We have also implemented a proof of concept on the NVIDIA Jetson TX2 module used in edge devices and simulated a federated learning process.

</details>


### [196] [Shadow Unlearning: A Neuro-Semantic Approach to Fidelity-Preserving Faceless Forgetting in LLMs](https://arxiv.org/abs/2601.04275)
*Dinesh Srivasthav P,Ashok Urlana,Rahul Mishra,Bala Mallikarjunarao Garlapati,Ponnurangam Kumaraguru*

Main category: cs.CR

TL;DR: Shadow Unlearning enables machine unlearning on anonymized data without exposing PII, using Neuro-Semantic Projector Unlearning (NSPU) for privacy-preserving unlearning that's 10x more efficient than standard methods.


<details>
  <summary>Details</summary>
Motivation: Existing machine unlearning methods require access to original training data, exposing it to privacy risks like membership inference attacks and PII misuse, violating GDPR's 'Right to be Forgotten' principles.

Method: Proposes Shadow Unlearning paradigm for approximate unlearning on anonymized forget data, implemented via Neuro-Semantic Projector Unlearning (NSPU) framework. Evaluated using Multi-domain Fictitious Unlearning (MuFU) dataset across five domains with custom evaluation stack.

Result: NSPU achieves superior unlearning performance while preserving model utility and enhancing user privacy. The approach is at least 10 times more computationally efficient than standard unlearning methods.

Conclusion: Shadow Unlearning represents a new direction for privacy-aware machine unlearning that effectively balances data protection and model fidelity, addressing critical privacy challenges in GDPR compliance.

Abstract: Machine unlearning aims to selectively remove the influence of specific training samples to satisfy privacy regulations such as the GDPR's 'Right to be Forgotten'. However, many existing methods require access to the data being removed, exposing it to membership inference attacks and potential misuse of Personally Identifiable Information (PII). We address this critical challenge by proposing Shadow Unlearning, a novel paradigm of approximate unlearning, that performs machine unlearning on anonymized forget data without exposing PII. We further propose a novel privacy-preserving framework, Neuro-Semantic Projector Unlearning (NSPU) to achieve Shadow unlearning. To evaluate our method, we compile Multi-domain Fictitious Unlearning (MuFU) forget set across five diverse domains and introduce an evaluation stack to quantify the trade-off between knowledge retention and unlearning effectiveness. Experimental results on various LLMs show that NSPU achieves superior unlearning performance, preserves model utility, and enhances user privacy. Additionally, the proposed approach is at least 10 times more computationally efficient than standard unlearning approaches. Our findings foster a new direction for privacy-aware machine unlearning that balances data protection and model fidelity.

</details>


### [197] [DP-MGTD: Privacy-Preserving Machine-Generated Text Detection via Adaptive Differentially Private Entity Sanitization](https://arxiv.org/abs/2601.04641)
*Lionel Z. Wang,Yusheng Zhao,Jiabin Luo,Xinfeng Li,Lixu Wang,Yinan Peng,Haoyang Li,XiaoFeng Wang,Wei Dong*

Main category: cs.CR

TL;DR: DP-MGTD framework uses adaptive differentially private entity sanitization to achieve both privacy-preserving and accurate machine-generated text detection, with the surprising finding that DP noise actually improves detection by revealing different sensitivity patterns between human and machine text.


<details>
  <summary>Details</summary>
Motivation: There's a fundamental conflict between machine-generated text detection (which needs to process sensitive user data) and privacy preservation. Standard anonymization hurts linguistic fluency, while rigorous differential privacy degrades detection signals.

Method: Two-stage adaptive differentially private entity sanitization algorithm: 1) noisy frequency estimation, 2) dynamic privacy budget calibration. Uses Laplace mechanism for numerical entities and Exponential mechanism for textual entities.

Result: Achieves near-perfect detection accuracy on MGTBench-2.0 dataset, significantly outperforming non-private baselines while satisfying strict privacy guarantees. Counter-intuitively, DP noise amplifies distinguishability between human and machine text.

Conclusion: DP-MGTD resolves the privacy-detection dilemma by showing that differential privacy can actually enhance detection accuracy through revealing distinct sensitivity patterns, enabling both privacy preservation and accurate authorship verification.

Abstract: The deployment of Machine-Generated Text (MGT) detection systems necessitates processing sensitive user data, creating a fundamental conflict between authorship verification and privacy preservation. Standard anonymization techniques often disrupt linguistic fluency, while rigorous Differential Privacy (DP) mechanisms typically degrade the statistical signals required for accurate detection. To resolve this dilemma, we propose \textbf{DP-MGTD}, a framework incorporating an Adaptive Differentially Private Entity Sanitization algorithm. Our approach utilizes a two-stage mechanism that performs noisy frequency estimation and dynamically calibrates privacy budgets, applying Laplace and Exponential mechanisms to numerical and textual entities respectively. Crucially, we identify a counter-intuitive phenomenon where the application of DP noise amplifies the distinguishability between human and machine text by exposing distinct sensitivity patterns to perturbation. Extensive experiments on the MGTBench-2.0 dataset show that our method achieves near-perfect detection accuracy, significantly outperforming non-private baselines while satisfying strict privacy guarantees.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [198] [SAGE-32B: Agentic Reasoning via Iterative Distillation](https://arxiv.org/abs/2601.04237)
*Basab Jha,Firoj Paudel,Ujjwal Puri,Ethan Henkel,Zhang Yuting,Mateusz Kowalczyk,Mei Huang,Choi Donghyuk,Wang Junhao*

Main category: cs.AI

TL;DR: SAGE-32B is a 32B parameter language model specialized for agentic reasoning and long-range planning, built on Qwen2.5-32B with iterative distillation and inverse reasoning techniques, achieving strong performance on agent benchmarks.


<details>
  <summary>Details</summary>
Motivation: To create a language model specifically optimized for agentic reasoning tasks rather than general conversation, focusing on task decomposition, tool usage, and error recovery in agentic loops.

Method: Initialized from Qwen2.5-32B pretrained model, fine-tuned using Iterative Distillation (two-stage training with feedback loops), and introduced inverse reasoning with a meta-cognition head to forecast planning failures before execution.

Result: Achieves higher success rates in multi-tool usage scenarios on agentic reasoning benchmarks (MMLU-Pro, AgentBench, MATH-500) compared to similarly sized baselines, while remaining competitive on standard reasoning evaluations.

Conclusion: SAGE-32B demonstrates effective specialization for agentic reasoning through iterative distillation and inverse reasoning approaches, with publicly released weights for community use.

Abstract: We demonstrate SAGE-32B, a 32 billion parameter language model that focuses on agentic reasoning and long range planning tasks. Unlike chat models that aim for general conversation fluency, SAGE-32B is designed to operate in an agentic loop, emphasizing task decomposition, tool usage, and error recovery. The model is initialized from the Qwen2.5-32B pretrained model and fine tuned using Iterative Distillation, a two stage training process that improves reasoning performance through rigorously tested feedback loops. SAGE-32B also introduces an inverse reasoning approach, which uses a meta cognition head to forecast potential failures in the planning process before execution. On agentic reasoning benchmarks including MMLU-Pro, AgentBench, and MATH-500, SAGE-32B achieves higher success rates in multi tool usage scenarios compared to similarly sized baseline models, while remaining competitive on standard reasoning evaluations. Model weights are publicly released at https://huggingface.co/sagea-ai/sage-reasoning-32b

</details>


### [199] [Learning Latent Action World Models In The Wild](https://arxiv.org/abs/2601.05230)
*Quentin Garrido,Tushar Nagarajan,Basile Terver,Nicolas Ballas,Yann LeCun,Michael Rabbat*

Main category: cs.AI

TL;DR: Learning latent action world models from in-the-wild videos without action labels, enabling real-world reasoning and planning.


<details>
  <summary>Details</summary>
Motivation: Real-world agents need to predict action consequences, but world models typically require action labels that are hard to obtain at scale. Existing works focus on simple simulations/games, not diverse real-world videos.

Method: Learn latent action models from videos alone using continuous constrained latent actions (instead of vector quantization), with specific architectural choices to handle video diversity, environmental noise, and lack of common embodiment.

Result: Continuous constrained latent actions capture complex real-world actions better than vector quantization. Learned actions can transfer across videos (e.g., human entering room). Actions become spatially localized relative to camera due to no common embodiment. A controller maps known actions to latent ones, enabling planning with similar performance to action-conditioned baselines.

Conclusion: The work demonstrates progress in scaling latent action models to real-world videos, providing a universal action interface for planning tasks despite challenges of video diversity and lack of common embodiment.

Abstract: Agents capable of reasoning and planning in the real world require the ability of predicting the consequences of their actions. While world models possess this capability, they most often require action labels, that can be complex to obtain at scale. This motivates the learning of latent action models, that can learn an action space from videos alone. Our work addresses the problem of learning latent actions world models on in-the-wild videos, expanding the scope of existing works that focus on simple robotics simulations, video games, or manipulation data. While this allows us to capture richer actions, it also introduces challenges stemming from the video diversity, such as environmental noise, or the lack of a common embodiment across videos. To address some of the challenges, we discuss properties that actions should follow as well as relevant architectural choices and evaluations. We find that continuous, but constrained, latent actions are able to capture the complexity of actions from in-the-wild videos, something that the common vector quantization does not. We for example find that changes in the environment coming from agents, such as humans entering the room, can be transferred across videos. This highlights the capability of learning actions that are specific to in-the-wild videos. In the absence of a common embodiment across videos, we are mainly able to learn latent actions that become localized in space, relative to the camera. Nonetheless, we are able to train a controller that maps known actions to latent ones, allowing us to use latent actions as a universal interface and solve planning tasks with our world model with similar performance as action-conditioned baselines. Our analyses and experiments provide a step towards scaling latent action models to the real world.

</details>


### [200] [The Language of Bargaining: Linguistic Effects in LLM Negotiations](https://arxiv.org/abs/2601.04387)
*Stuti Sinha,Himanshu Kumar,Aryan Raju Mandapati,Rakshit Sakhuja,Dhruv Kumar*

Main category: cs.AI

TL;DR: LLM negotiation outcomes vary significantly across languages, with Indic languages reversing proposer advantages and reallocating surplus compared to English-only evaluations.


<details>
  <summary>Details</summary>
Motivation: Most LLM negotiation evaluations occur exclusively in English, potentially yielding incomplete or misleading conclusions about model performance across different languages and cultural contexts.

Method: Controlled multi-agent simulations across Ultimatum, Buy-Sell, and Resource Exchange games, systematically isolating language effects across English and four Indic languages (Hindi, Punjabi, Gujarati, Marwadi) while holding game rules, model parameters, and incentives constant.

Result: Language choice shifts outcomes more strongly than changing models, reversing proposer advantages and reallocating surplus. Effects are task-contingent: Indic languages reduce stability in distributive games but induce richer exploration in integrative settings.

Conclusion: Evaluating LLM negotiation solely in English yields incomplete and potentially misleading conclusions, cautioning against English-only evaluation and suggesting culturally-aware evaluation is essential for fair deployment.

Abstract: Negotiation is a core component of social intelligence, requiring agents to balance strategic reasoning, cooperation, and social norms. Recent work shows that LLMs can engage in multi-turn negotiation, yet nearly all evaluations occur exclusively in English. Using controlled multi-agent simulations across Ultimatum, Buy-Sell, and Resource Exchange games, we systematically isolate language effects across English and four Indic framings (Hindi, Punjabi, Gujarati, Marwadi) by holding game rules, model parameters, and incentives constant across all conditions. We find that language choice can shift outcomes more strongly than changing models, reversing proposer advantages and reallocating surplus. Crucially, effects are task-contingent: Indic languages reduce stability in distributive games yet induce richer exploration in integrative settings. Our results demonstrate that evaluating LLM negotiation solely in English yields incomplete and potentially misleading conclusions. These findings caution against English-only evaluation of LLMs and suggest that culturally-aware evaluation is essential for fair deployment.

</details>


### [201] [CircuitLM: A Multi-Agent LLM-Aided Design Framework for Generating Circuit Schematics from Natural Language Prompts](https://arxiv.org/abs/2601.04505)
*Khandakar Shakib Al Hasan,Syed Rifat Raiyan,Hasin Mahtab Alvee,Wahid Sadik*

Main category: cs.AI

TL;DR: CircuitLM is a multi-agent LLM pipeline that converts natural language circuit descriptions into structured CircuitJSON schematics, addressing LLM hallucination issues through a verified component database and hybrid validation framework.


<details>
  <summary>Details</summary>
Motivation: LLMs frequently hallucinate details, violate electrical constraints, and produce non-machine-readable outputs when generating circuit schematics from natural language descriptions, creating a gap in electronics design automation.

Method: Five-stage pipeline: (1) LLM-based component identification, (2) canonical pinout retrieval, (3) chain-of-thought reasoning by electronics expert agent, (4) JSON schematic synthesis, (5) force-directed SVG visualization, anchored by a curated component knowledge base with 50 components.

Result: Evaluated on 100 embedded-systems prompts across six LLMs using Dual-Metric Circuit Validation (DMCV) framework, achieving high fidelity in microcontroller-centric designs validated against human-expert assessments.

Conclusion: CircuitLM bridges natural language input to deployable hardware designs, enabling reliable circuit prototyping by non-experts through structured, verified schematic generation.

Abstract: Generating accurate circuit schematics from high-level natural language descriptions remains a persistent challenge in electronics design, as large language models (LLMs) frequently hallucinate in granular details, violate electrical constraints, and produce non-machine-readable outputs. We present CircuitLM, a novel multi-agent LLM-aided circuit design pipeline that translates user prompts into structured, visually interpretable CircuitJSON schematics through five sequential stages: (i) LLM-based component identification, (ii) canonical pinout retrieval, (iii) chain-of-thought reasoning by an electronics expert agent, (iv) JSON schematic synthesis, and (v) force-directed SVG visualization. Anchored by a curated, embedding-powered component knowledge base. While LLMs often violate electrical constraints, CircuitLM bridges this gap by grounding generation in a verified and dynamically extensible component database, initially comprising 50 components. To ensure safety, we incorporate a hybrid evaluation framework, namely Dual-Metric Circuit Validation (DMCV), validated against human-expert assessments, which achieves high fidelity in microcontroller-centric designs. We evaluate the system on 100 diverse embedded-systems prompts across six LLMs and introduce DMCV to assess both structural and electrical validity. This work bridges natural language input to deployable hardware designs, enabling reliable circuit prototyping by non-experts. Our code and data will be made public upon acceptance.

</details>


### [202] [BackdoorAgent: A Unified Framework for Backdoor Attacks on LLM-based Agents](https://arxiv.org/abs/2601.04566)
*Yunhao Feng,Yige Li,Yutao Wu,Yingshui Tan,Yanming Guo,Yifan Ding,Kun Zhai,Xingjun Ma,Yugang Jiang*

Main category: cs.AI

TL;DR: BackdoorAgent: A framework for analyzing backdoor threats in LLM agents across planning, memory, and tool-use stages, showing triggers can persist and propagate through agent workflows.


<details>
  <summary>Details</summary>
Motivation: Existing studies on backdoor threats in LLM agents are fragmented and analyze individual attack vectors in isolation, leaving cross-stage interaction and propagation of backdoor triggers poorly understood from an agent-centric perspective.

Method: Proposed BackdoorAgent, a modular and stage-aware framework that structures attacks into three functional stages (planning, memory, tool-use) and instruments agent execution to systematically analyze trigger activation and propagation across stages.

Result: Triggers implanted at a single stage can persist across multiple steps and propagate through intermediate states. With GPT-based backbone: 43.58% persistence in planning attacks, 77.97% in memory attacks, and 60.28% in tool-stage attacks.

Conclusion: The agentic workflow itself is vulnerable to backdoor threats, with triggers capable of persisting and propagating across multiple stages. The framework provides a unified view for systematic analysis and includes a benchmark for future research.

Abstract: Large language model (LLM) agents execute tasks through multi-step workflows that combine planning, memory, and tool use. While this design enables autonomy, it also expands the attack surface for backdoor threats. Backdoor triggers injected into specific stages of an agent workflow can persist through multiple intermediate states and adversely influence downstream outputs. However, existing studies remain fragmented and typically analyze individual attack vectors in isolation, leaving the cross-stage interaction and propagation of backdoor triggers poorly understood from an agent-centric perspective. To fill this gap, we propose \textbf{BackdoorAgent}, a modular and stage-aware framework that provides a unified, agent-centric view of backdoor threats in LLM agents. BackdoorAgent structures the attack surface into three functional stages of agentic workflows, including \textbf{planning attacks}, \textbf{memory attacks}, and \textbf{tool-use attacks}, and instruments agent execution to enable systematic analysis of trigger activation and propagation across different stages. Building on this framework, we construct a standardized benchmark spanning four representative agent applications: \textbf{Agent QA}, \textbf{Agent Code}, \textbf{Agent Web}, and \textbf{Agent Drive}, covering both language-only and multimodal settings. Our empirical analysis shows that \textit{triggers implanted at a single stage can persist across multiple steps and propagate through intermediate states.} For instance, when using a GPT-based backbone, we observe trigger persistence in 43.58\% of planning attacks, 77.97\% of memory attacks, and 60.28\% of tool-stage attacks, highlighting the vulnerabilities of the agentic workflow itself to backdoor threats. To facilitate reproducibility and future research, our code and benchmark are publicly available at GitHub.

</details>


### [203] [Neurosymbolic Retrievers for Retrieval-augmented Generation](https://arxiv.org/abs/2601.04568)
*Yash Saxena,Manas Gaur*

Main category: cs.AI

TL;DR: Neurosymbolic RAG integrates symbolic reasoning with neural retrieval to improve transparency and interpretability in retrieval-augmented generation systems.


<details>
  <summary>Details</summary>
Motivation: Traditional RAG systems have opaque internal reasoning processes in their neural components (retriever, re-ranker, generator), which complicates interpretability, hinders debugging, and erodes trust in high-stakes domains where clear decision-making is essential.

Method: Three neurosymbolic methods: 1) MAR (Knowledge Modulation Aligned Retrieval) uses modulation networks to refine query embeddings with interpretable symbolic features; 2) KG-Path RAG enhances queries by traversing knowledge graphs; 3) Process Knowledge-infused RAG uses domain-specific tools to reorder retrieved content based on validated workflows.

Result: Preliminary results from mental health risk assessment tasks indicate that this neurosymbolic approach enhances both transparency and overall performance.

Conclusion: Neurosymbolic RAG successfully addresses the opacity of traditional RAG systems by integrating symbolic reasoning with neural retrieval, improving interpretability while maintaining or enhancing performance, particularly in high-stakes domains.

Abstract: Retrieval Augmented Generation (RAG) has made significant strides in overcoming key limitations of large language models, such as hallucination, lack of contextual grounding, and issues with transparency. However, traditional RAG systems consist of three interconnected neural components - the retriever, re-ranker, and generator - whose internal reasoning processes remain opaque. This lack of transparency complicates interpretability, hinders debugging efforts, and erodes trust, especially in high-stakes domains where clear decision-making is essential. To address these challenges, we introduce the concept of Neurosymbolic RAG, which integrates symbolic reasoning using a knowledge graph with neural retrieval techniques. This new framework aims to answer two primary questions: (a) Can retrievers provide a clear and interpretable basis for document selection? (b) Can symbolic knowledge enhance the clarity of the retrieval process? We propose three methods to improve this integration. First is MAR (Knowledge Modulation Aligned Retrieval) that employs modulation networks to refine query embeddings using interpretable symbolic features, thereby making document matching more explicit. Second, KG-Path RAG enhances queries by traversing knowledge graphs to improve overall retrieval quality and interpretability. Lastly, Process Knowledge-infused RAG utilizes domain-specific tools to reorder retrieved content based on validated workflows. Preliminary results from mental health risk assessment tasks indicate that this neurosymbolic approach enhances both transparency and overall performance

</details>


### [204] [A Method for Constructing a Digital Transformation Driving Mechanism Based on Semantic Understanding of Large Models](https://arxiv.org/abs/2601.04696)
*Huayi Liu*

Main category: cs.AI

TL;DR: This paper proposes a method combining LLM and knowledge graph to enhance enterprise digital transformation by improving semantic understanding of unstructured data and intelligent decision-making.


<details>
  <summary>Details</summary>
Motivation: Enterprises face challenges in digital transformation including insufficient semantic understanding of unstructured data and lack of intelligent decision-making basis in driving mechanisms.

Method: 1) Fine-tuned BERT for entity recognition and relationship extraction, GPT-4 for semantically enhanced vector representations; 2) Two-layer GNN architecture to fuse LLM semantic vectors with business metadata for dynamic knowledge graph; 3) Reinforcement learning to optimize decision path generation with reward function for mechanism iteration.

Result: In manufacturing case: response time for equipment failure reduced from 7.8 to 3.7 hours, F1 value reached 94.3%, decision error compensation in annual digital transformation cost decreased by 45.3%.

Conclusion: The method significantly enhances intelligence level and execution efficiency of digital transformation driving mechanisms by integrating large model semantic understanding with structured knowledge.

Abstract: In the process of digital transformation, enterprises are faced with problems such as insufficient semantic understanding of unstructured data and lack of intelligent decision-making basis in driving mechanisms. This study proposes a method that combines a large language model (LLM) and a knowledge graph. First, a fine-tuned BERT (Bidirectional Encoder Representations from Transformers) model is used to perform entity recognition and relationship extraction on multi-source heterogeneous texts, and GPT-4 is used to generate semantically enhanced vector representations; secondly, a two-layer graph neural network (GNN) architecture is designed to fuse the semantic vectors output by LLM with business metadata to construct a dynamic and scalable enterprise knowledge graph; then reinforcement learning is introduced to optimize decision path generation, and the reward function is used to drive the mechanism iteration. In the case of the manufacturing industry, this mechanism reduced the response time for equipment failure scenarios from 7.8 hours to 3.7 hours, the F1 value reached 94.3%, and the compensation for decision errors in the annual digital transformation cost decreased by 45.3%. This method significantly enhances the intelligence level and execution efficiency of the digital transformation driving mechanism by integrating large model semantic understanding with structured knowledge.

</details>


### [205] [TourPlanner: A Competitive Consensus Framework with Constraint-Gated Reinforcement Learning for Travel Planning](https://arxiv.org/abs/2601.04698)
*Yinuo Wang,Mining Tan,Wenxiang Jiao,Xiaoxi Li,Hao Wang,Xuanyu Zhang,Yuan Lu,Weiming Dong*

Main category: cs.AI

TL;DR: TourPlanner: A travel planning framework using multi-path reasoning and constraint-gated reinforcement learning to optimize POI selection, explore solution space, and balance hard/soft constraints.


<details>
  <summary>Details</summary>
Motivation: Existing travel planning approaches face three key challenges: (1) pruning candidate POIs while maintaining high recall, (2) single reasoning paths limiting exploration of feasible solutions, and (3) difficulty simultaneously optimizing hard and soft constraints.

Method: Proposes TourPlanner with three components: (1) Personalized Recall and Spatial Optimization (PReSO) for spatially-aware candidate POI sets, (2) Competitive consensus Chain-of-Thought (CCoT) for multi-path reasoning to explore solution space, and (3) sigmoid-based gating mechanism in reinforcement learning to prioritize soft constraints only after hard constraints are met.

Result: Experimental results on travel planning benchmarks show TourPlanner achieves state-of-the-art performance, significantly surpassing existing methods in both feasibility and user-preference alignment.

Conclusion: TourPlanner effectively addresses key challenges in travel planning through its comprehensive framework combining spatial optimization, multi-path reasoning, and constraint-gated reinforcement learning, demonstrating superior performance in creating feasible and personalized itineraries.

Abstract: Travel planning is a sophisticated decision-making process that requires synthesizing multifaceted information to construct itineraries. However, existing travel planning approaches face several challenges: (1) Pruning candidate points of interest (POIs) while maintaining a high recall rate; (2) A single reasoning path restricts the exploration capability within the feasible solution space for travel planning; (3) Simultaneously optimizing hard constraints and soft constraints remains a significant difficulty. To address these challenges, we propose TourPlanner, a comprehensive framework featuring multi-path reasoning and constraint-gated reinforcement learning. Specifically, we first introduce a Personalized Recall and Spatial Optimization (PReSO) workflow to construct spatially-aware candidate POIs' set. Subsequently, we propose Competitive consensus Chain-of-Thought (CCoT), a multi-path reasoning paradigm that improves the ability of exploring the feasible solution space. To further refine the plan, we integrate a sigmoid-based gating mechanism into the reinforcement learning stage, which dynamically prioritizes soft-constraint satisfaction only after hard constraints are met. Experimental results on travel planning benchmarks demonstrate that TourPlanner achieves state-of-the-art performance, significantly surpassing existing methods in both feasibility and user-preference alignment.

</details>


### [206] [Memory Matters More: Event-Centric Memory as a Logic Map for Agent Searching and Reasoning](https://arxiv.org/abs/2601.04726)
*Yuyang Hu,Jiongnan Liu,Jiejun Tan,Yutao Zhu,Zhicheng Dou*

Main category: cs.AI

TL;DR: CompassMem is an event-centric memory framework that organizes agent experiences into structured Event Graphs with logical relations, enabling goal-directed navigation for long-horizon reasoning instead of flat similarity-based retrieval.


<details>
  <summary>Details</summary>
Motivation: Current LLM agents use flat memory organization with simple similarity-based retrieval, lacking explicit logical relationship capture between experiences. This prevents effective reasoning over long-horizon dependencies as memory access remains detached from structure.

Method: CompassMem organizes memory as an Event Graph by incrementally segmenting experiences into events and linking them through explicit logical relations. This creates a logic map that enables structured, goal-directed navigation over memory beyond superficial retrieval.

Result: Experiments on LoCoMo and NarrativeQA benchmarks show CompassMem consistently improves both retrieval and reasoning performance across multiple backbone models.

Conclusion: The event-centric memory framework with structured Event Graphs enables agents to perform logical reasoning over long-horizon dependencies, moving beyond flat similarity-based retrieval to support complex decision-making.

Abstract: Large language models (LLMs) are increasingly deployed as intelligent agents that reason, plan, and interact with their environments. To effectively scale to long-horizon scenarios, a key capability for such agents is a memory mechanism that can retain, organize, and retrieve past experiences to support downstream decision-making. However, most existing approaches organize and store memories in a flat manner and rely on simple similarity-based retrieval techniques. Even when structured memory is introduced, existing methods often struggle to explicitly capture the logical relationships among experiences or memory units. Moreover, memory access is largely detached from the constructed structure and still depends on shallow semantic retrieval, preventing agents from reasoning logically over long-horizon dependencies. In this work, we propose CompassMem, an event-centric memory framework inspired by Event Segmentation Theory. CompassMem organizes memory as an Event Graph by incrementally segmenting experiences into events and linking them through explicit logical relations. This graph serves as a logic map, enabling agents to perform structured and goal-directed navigation over memory beyond superficial retrieval, progressively gathering valuable memories to support long-horizon reasoning. Experiments on LoCoMo and NarrativeQA demonstrate that CompassMem consistently improves both retrieval and reasoning performance across multiple backbone models.

</details>


### [207] [Miner:Mining Intrinsic Mastery for Data-Efficient RL in Large Reasoning Models](https://arxiv.org/abs/2601.04731)
*Shuyang Jiang,Yuhao Wang,Ya Zhang,Yanfeng Wang,Yu Wang*

Main category: cs.AI

TL;DR: Miner introduces a method that uses the policy's intrinsic uncertainty as self-supervised reward signal to solve inefficiency in critic-free RL for reasoning models, achieving state-of-the-art performance across benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current critic-free RL methods for large reasoning models are inefficient when training on positive homogeneous prompts (where all rollouts are correct), wasting rollouts due to zero advantage estimates.

Method: Miner repurposes the policy's intrinsic uncertainty as self-supervised reward signal without external supervision, auxiliary models, or additional inference cost. It features: (1) token-level focal credit assignment that amplifies gradients on critical uncertain tokens while suppressing overconfident ones, and (2) adaptive advantage calibration to integrate intrinsic and verifiable rewards.

Result: Evaluated across six reasoning benchmarks on Qwen3-4B and Qwen3-8B base models, Miner achieves state-of-the-art performance among four other algorithms, with up to 4.58 absolute gains in Pass@1 and 6.66 gains in Pass@K compared to GRPO.

Conclusion: Latent uncertainty exploitation is both necessary and sufficient for efficient and scalable RL training of reasoning models, as demonstrated by Miner's superiority over other exploration enhancement methods.

Abstract: Current critic-free RL methods for large reasoning models suffer from severe inefficiency when training on positive homogeneous prompts (where all rollouts are correct), resulting in waste of rollouts due to zero advantage estimates. We introduce a radically simple yet powerful solution to \uline{M}ine \uline{in}trinsic mast\uline{er}y (Miner), that repurposes the policy's intrinsic uncertainty as a self-supervised reward signal, with no external supervision, auxiliary models, or additional inference cost. Our method pioneers two key innovations: (1) a token-level focal credit assignment mechanism that dynamically amplifies gradients on critical uncertain tokens while suppressing overconfident ones, and (2) adaptive advantage calibration to seamlessly integrate intrinsic and verifiable rewards. Evaluated across six reasoning benchmarks on Qwen3-4B and Qwen3-8B base models, Miner achieves state-of-the-art performance among the other four algorithms, yielding up to \textbf{4.58} absolute gains in Pass@1 and \textbf{6.66} gains in Pass@K compared to GRPO. Comparison with other methods targeted at exploration enhancement further discloses the superiority of the two newly proposed innovations. This demonstrates that latent uncertainty exploitation is both necessary and sufficient for efficient and scalable RL training of reasoning models.

</details>


### [208] [AT$^2$PO: Agentic Turn-based Policy Optimization via Tree Search](https://arxiv.org/abs/2601.04767)
*Zefang Zong,Dingwei Chen,Yang Li,Qi Yi,Bo Zhou,Chengming Li,Bo Qian,Peng Chen,Jie Jiang*

Main category: cs.AI

TL;DR: AT²PO is a unified framework for multi-turn agentic RL that addresses exploration diversity, credit assignment, and policy optimization challenges through turn-level tree search and optimization.


<details>
  <summary>Details</summary>
Motivation: LLM agents need refinement through agentic reinforcement learning, but current approaches face three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization in multi-turn tasks.

Method: AT²PO introduces a turn-level tree structure with Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation. It also proposes Agentic Turn-based Policy Optimization (ATPO) as a turn-level learning objective that aligns with natural decision granularity.

Result: Experiments across seven benchmarks show consistent improvements over state-of-the-art baselines by up to 1.84 percentage points on average, with ablation studies validating each component's effectiveness.

Conclusion: AT²PO provides a unified framework that successfully addresses key challenges in multi-turn agentic RL, demonstrating significant performance improvements while being orthogonal to tree search and easily integrable into existing RL pipelines.

Abstract: LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT$^2$PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT$^2$PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO.

</details>


### [209] [Defense Against Indirect Prompt Injection via Tool Result Parsing](https://arxiv.org/abs/2601.04795)
*Qiang Yu,Xinran Cheng,Chuanyi Liu*

Main category: cs.AI

TL;DR: A novel defense method against Indirect Prompt Injection attacks that uses tool result parsing to filter malicious code while maintaining competitive utility and achieving the lowest Attack Success Rate to date.


<details>
  <summary>Details</summary>
Motivation: As LLM agents gain physical control in autonomous systems and robotics, indirect prompt injection attacks pose escalating risks by hijacking agents through adversarial instructions in tool call results. Existing defenses have limitations: training-based methods are computationally expensive and require frequent updates, while prompt-based methods suffer from high attack success rates.

Method: Proposes a novel approach that provides LLMs with precise data via tool result parsing while effectively filtering out injected malicious code. The method focuses on parsing tool results to separate legitimate data from adversarial instructions.

Result: Achieves competitive Utility under Attack (UA) while maintaining the lowest Attack Success Rate (ASR) to date, significantly outperforming existing methods. The approach demonstrates strong robustness against sophisticated injection attacks.

Conclusion: The proposed tool result parsing method offers an effective defense against indirect prompt injection attacks, balancing security (low ASR) with utility (competitive UA), addressing critical vulnerabilities as LLM agents gain physical control capabilities.

Abstract: As LLM agents transition from digital assistants to physical controllers in autonomous systems and robotics, they face an escalating threat from indirect prompt injection. By embedding adversarial instructions into the results of tool calls, attackers can hijack the agent's decision-making process to execute unauthorized actions. This vulnerability poses a significant risk as agents gain more direct control over physical environments. Existing defense mechanisms against Indirect Prompt Injection (IPI) generally fall into two categories. The first involves training dedicated detection models; however, this approach entails high computational overhead for both training and inference, and requires frequent updates to keep pace with evolving attack vectors. Alternatively, prompt-based methods leverage the inherent capabilities of LLMs to detect or ignore malicious instructions via prompt engineering. Despite their flexibility, most current prompt-based defenses suffer from high Attack Success Rates (ASR), demonstrating limited robustness against sophisticated injection attacks. In this paper, we propose a novel method that provides LLMs with precise data via tool result parsing while effectively filtering out injected malicious code. Our approach achieves competitive Utility under Attack (UA) while maintaining the lowest Attack Success Rate (ASR) to date, significantly outperforming existing methods. Code is available at GitHub.

</details>


### [210] [DR-LoRA: Dynamic Rank LoRA for Mixture-of-Experts Adaptation](https://arxiv.org/abs/2601.04823)
*Guanzhi Deng,Bo Li,Ronghao Chen,Huacan Wang,Linqi Song,Lijie Wen*

Main category: cs.AI

TL;DR: DR-LoRA: Dynamic rank allocation for LoRA fine-tuning of MoE LLMs, where expert ranks grow based on task-specific demands rather than uniform allocation.


<details>
  <summary>Details</summary>
Motivation: Current LoRA approaches for MoE LLMs use identical ranks for all experts, ignoring functional specialization. This causes resource mismatch - task-relevant experts are under-provisioned while less relevant ones get redundant parameters.

Method: DR-LoRA dynamically grows expert LoRA ranks during fine-tuning using Expert Saliency Scoring that combines expert routing frequency and LoRA rank importance. Higher-saliency experts get priority for rank expansion, creating heterogeneous rank distributions tailored to tasks.

Result: Experiments on multiple benchmarks show DR-LoRA consistently outperforms standard LoRA and static allocation strategies under the same parameter budget, achieving better task performance with more efficient parameter utilization.

Conclusion: Dynamic rank allocation based on expert saliency enables more efficient parameter-efficient fine-tuning of MoE LLMs by matching capacity to task-specific expert demands.

Abstract: Mixture-of-Experts (MoE) has become a prominent paradigm for scaling Large Language Models (LLMs). Parameter-efficient fine-tuning (PEFT), such as LoRA, is widely adopted to adapt pretrained MoE LLMs to downstream tasks. However, existing approaches assign identical LoRA ranks to all experts, overlooking the intrinsic functional specialization within MoE LLMs. This uniform allocation leads to resource mismatch, task-relevant experts are under-provisioned while less relevant ones receive redundant parameters. We propose a Dynamic Rank LoRA framework named DR-LoRA, which dynamically grows expert LoRA ranks during fine-tuning based on task-specific demands. DR-LoRA employs an Expert Saliency Scoring mechanism that integrates expert routing frequency and LoRA rank importance to quantify each expert's demand for additional capacity. Experts with higher saliency scores are prioritized for rank expansion, enabling the automatic formation of a heterogeneous rank distribution tailored to the target task. Experiments on multiple benchmarks demonstrate that DR-LoRA consistently outperforms standard LoRA and static allocation strategies under the same parameter budget, achieving superior task performance with more efficient parameter utilization.

</details>


### [211] [Higher-Order Knowledge Representations for Agentic Scientific Reasoning](https://arxiv.org/abs/2601.04878)
*Isabella A. Stewart,Markus J. Buehler*

Main category: cs.AI

TL;DR: Researchers developed a hypergraph-based knowledge representation system to capture higher-order relationships in scientific literature, enabling AI agents to generate novel mechanistic hypotheses for materials science without human supervision.


<details>
  <summary>Details</summary>
Motivation: Current LLMs lack structural depth in reasoning, and traditional knowledge graphs fail to capture irreducible higher-order interactions that govern emergent physical behavior in scientific domains.

Method: Constructed hypergraph-based knowledge representations from ~1,100 manuscripts on biocomposite scaffolds, creating a global hypergraph with 161,172 nodes and 320,201 hyperedges, and equipped agentic systems with hypergraph traversal tools using node-intersection constraints.

Result: The hypergraph revealed scale-free topology (power law exponent ~1.23) organized around conceptual hubs, prevented combinatorial explosion of pairwise expansions, and enabled generation of grounded mechanistic hypotheses like linking cerium oxide to PCL scaffolds via chitosan intermediates.

Conclusion: Hypergraph topology serves as a verifiable guardrail for "teacherless" agentic reasoning systems, accelerating scientific discovery by uncovering relationships obscured by traditional graph methods through higher-order relationship modeling.

Abstract: Scientific inquiry requires systems-level reasoning that integrates heterogeneous experimental data, cross-domain knowledge, and mechanistic evidence into coherent explanations. While Large Language Models (LLMs) offer inferential capabilities, they often depend on retrieval-augmented contexts that lack structural depth. Traditional Knowledge Graphs (KGs) attempt to bridge this gap, yet their pairwise constraints fail to capture the irreducible higher-order interactions that govern emergent physical behavior. To address this, we introduce a methodology for constructing hypergraph-based knowledge representations that faithfully encode multi-entity relationships. Applied to a corpus of ~1,100 manuscripts on biocomposite scaffolds, our framework constructs a global hypergraph of 161,172 nodes and 320,201 hyperedges, revealing a scale-free topology (power law exponent ~1.23) organized around highly connected conceptual hubs. This representation prevents the combinatorial explosion typical of pairwise expansions and explicitly preserves the co-occurrence context of scientific formulations. We further demonstrate that equipping agentic systems with hypergraph traversal tools, specifically using node-intersection constraints, enables them to bridge semantically distant concepts. By exploiting these higher-order pathways, the system successfully generates grounded mechanistic hypotheses for novel composite materials, such as linking cerium oxide to PCL scaffolds via chitosan intermediates. This work establishes a "teacherless" agentic reasoning system where hypergraph topology acts as a verifiable guardrail, accelerating scientific discovery by uncovering relationships obscured by traditional graph methods.

</details>


### [212] [ConMax: Confidence-Maximizing Compression for Efficient Chain-of-Thought Reasoning](https://arxiv.org/abs/2601.04973)
*Minda Hu,Zexuan Qiu,Zenan Xu,Kun Li,Bo Zhou,Irwin King*

Main category: cs.AI

TL;DR: ConMax is a reinforcement learning framework that compresses reasoning traces in Large Reasoning Models by pruning redundant reasoning while preserving logical coherence, achieving 43% length reduction with only 0.7% accuracy drop.


<details>
  <summary>Details</summary>
Motivation: Large Reasoning Models often suffer from "overthinking" - generating redundant reasoning paths that increase computational costs without improving accuracy. Existing compression techniques for reasoning traces either compromise logical coherence or have high sampling costs.

Method: ConMax formulates compression as a reward-driven optimization problem using reinforcement learning. It trains a policy to prune redundancy by maximizing a weighted combination of answer confidence (for predictive fidelity) and thinking confidence (for reasoning validity) through a frozen auxiliary LRM.

Result: Extensive experiments across five reasoning datasets show ConMax achieves superior efficiency-performance trade-off: reduces inference length by 43% over strong baselines with only 0.7% accuracy drop.

Conclusion: ConMax effectively generates high-quality, efficient training data for LRMs by automatically compressing reasoning traces while preserving essential reasoning patterns, addressing the overthinking problem in large reasoning models.

Abstract: Recent breakthroughs in Large Reasoning Models (LRMs) have demonstrated that extensive Chain-of-Thought (CoT) generation is critical for enabling intricate cognitive behaviors, such as self-verification and backtracking, to solve complex tasks. However, this capability often leads to ``overthinking'', where models generate redundant reasoning paths that inflate computational costs without improving accuracy. While Supervised Fine-Tuning (SFT) on reasoning traces is a standard paradigm for the 'cold start' phase, applying existing compression techniques to these traces often compromises logical coherence or incurs prohibitive sampling costs. In this paper, we introduce ConMax (Confidence-Maximizing Compression), a novel reinforcement learning framework designed to automatically compress reasoning traces while preserving essential reasoning patterns. ConMax formulates compression as a reward-driven optimization problem, training a policy to prune redundancy by maximizing a weighted combination of answer confidence for predictive fidelity and thinking confidence for reasoning validity through a frozen auxiliary LRM. Extensive experiments across five reasoning datasets demonstrate that ConMax achieves a superior efficiency-performance trade-off. Specifically, it reduces inference length by 43% over strong baselines at the cost of a mere 0.7% dip in accuracy, proving its effectiveness in generating high-quality, efficient training data for LRMs.

</details>


### [213] [Publishing FAIR and Machine-actionable Reviews in Materials Science: The Case for Symbolic Knowledge in Neuro-symbolic Artificial Intelligence](https://arxiv.org/abs/2601.05051)
*Jennifer D'Souza,Soren Auer,Eleni Poupaki,Alex Watkins,Anjana Devi,Riikka L. Puurunen,Bora Karasulu,Adrie Mackus,Erwin Kessels*

Main category: cs.AI

TL;DR: Presents a case study in ALD/E where review tables are published as FAIR, machine-actionable comparisons in ORKG, enabling structured, queryable knowledge and advocating for symbolic knowledge as the backbone of reliable neurosymbolic AI in materials science.


<details>
  <summary>Details</summary>
Motivation: Scientific reviews in materials science contain valuable insights but remain locked in narrative text and static PDF tables, limiting reuse by both humans and machines. There's a need to make this knowledge more accessible and actionable.

Method: Publish review tables as FAIR (Findable, Accessible, Interoperable, Reusable), machine-actionable comparisons in the Open Research Knowledge Graph (ORKG). Contrast symbolic querying over ORKG with large language model-based querying.

Result: Successfully transformed atomic layer deposition and etching review tables into structured, queryable knowledge in ORKG. Demonstrated the value of having a curated symbolic knowledge layer for reliable AI applications in materials science.

Conclusion: A curated symbolic layer should remain the backbone of reliable neurosymbolic AI in materials science, with LLMs serving as complementary, symbolically grounded interfaces rather than standalone sources of truth.

Abstract: Scientific reviews are central to knowledge integration in materials science, yet their key insights remain locked in narrative text and static PDF tables, limiting reuse by humans and machines alike. This article presents a case study in atomic layer deposition and etching (ALD/E) where we publish review tables as FAIR, machine-actionable comparisons in the Open Research Knowledge Graph (ORKG), turning them into structured, queryable knowledge. Building on this, we contrast symbolic querying over ORKG with large language model-based querying, and argue that a curated symbolic layer should remain the backbone of reliable neurosymbolic AI in materials science, with LLMs serving as complementary, symbolically grounded interfaces rather than standalone sources of truth.

</details>


### [214] [Reinforced Efficient Reasoning via Semantically Diverse Exploration](https://arxiv.org/abs/2601.05053)
*Ziqi Zhao,Zhaochun Ren,Jiahong Zou,Liu Yang,Zhiwei Xu,Xuri Ge,Zhumin Chen,Xinyu Ma,Daiting Shi,Shuaiqiang Wang,Dawei Yin,Xin Xin*

Main category: cs.AI

TL;DR: ROSE introduces a reinforcement learning method with semantic diversity exploration and efficient reasoning for LLMs, improving mathematical reasoning performance through semantic-entropy branching and length-aware advantage estimation.


<details>
  <summary>Details</summary>
Motivation: Existing RLVR methods with MCTS extensions still suffer from limited exploration diversity and inefficient reasoning, needing better mechanisms for diverse reasoning exploration and more efficient credit assignment.

Method: ROSE incorporates: 1) semantic-entropy-based branching strategy to capture semantic uncertainty and select branching points with high divergence, 2) ε-exploration mechanism to stochastically initiate rollouts from the root, and 3) length-aware segment-level advantage estimator to reward concise reasoning.

Result: Extensive experiments on various mathematical reasoning benchmarks with Qwen and Llama models validate the effectiveness and efficiency of ROSE, showing improved performance over existing methods.

Conclusion: ROSE successfully addresses exploration diversity and reasoning efficiency challenges in RLVR for LLMs through semantic diversity exploration mechanisms and length-aware optimization, providing a more effective approach for mathematical reasoning tasks.

Abstract: Reinforcement learning with verifiable rewards (RLVR) has proven effective in enhancing the reasoning of large language models (LLMs). Monte Carlo Tree Search (MCTS)-based extensions improve upon vanilla RLVR (e.g., GRPO) by providing tree-based reasoning rollouts that enable fine-grained and segment-level credit assignment. However, existing methods still suffer from limited exploration diversity and inefficient reasoning. To address the above challenges, we propose reinforced efficient reasoning via semantically diverse explorations, i.e., ROSE, for LLMs. To encourage more diverse reasoning exploration, our method incorporates a semantic-entropy-based branching strategy and an $\varepsilon$-exploration mechanism. The former operates on already sampled reasoning rollouts to capture semantic uncertainty and select branching points with high semantic divergence to generate new successive reasoning paths, whereas the latter stochastically initiates reasoning rollouts from the root, preventing the search process from becoming overly local. To improve efficiency, we design a length-aware segment-level advantage estimator that rewards concise and correct reasoning while penalizing unnecessarily long reasoning chains. Extensive experiments on various mathematical reasoning benchmarks with Qwen and Llama models validate the effectiveness and efficiency of ROSE. Codes are available at https://github.com/ZiqiZhao1/ROSE-rl.

</details>


### [215] [Token-Level LLM Collaboration via FusionRoute](https://arxiv.org/abs/2601.05106)
*Nuoya Xiong,Yuhang Zhou,Hanqing Zeng,Zhaorun Chen,Furong Huang,Shuchao Bi,Lizhu Zhang,Zhuokai Zhao*

Main category: cs.AI

TL;DR: FusionRoute is a token-level multi-LLM collaboration framework that uses a lightweight router to select expert models at each decoding step while also contributing complementary logits to refine expert outputs, overcoming limitations of pure expert-only routing.


<details>
  <summary>Details</summary>
Motivation: Address the dilemma between large general-purpose LLMs (expensive to train/deploy) and smaller domain-specialized models (efficient but poor generalization). Need a solution that combines efficiency with strong cross-domain performance.

Method: Proposes FusionRoute: a token-level collaboration framework with a lightweight router that simultaneously (1) selects the most suitable expert at each decoding step, and (2) contributes complementary logits via addition to refine/correct the expert's next-token distribution. Unlike pure expert-only routing, includes trainable complementary generator.

Result: Outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning across Llama-3 and Gemma-2 families on diverse benchmarks (mathematical reasoning, code generation, instruction following). Remains competitive with domain experts on their respective tasks.

Conclusion: FusionRoute provides an effective solution to the efficiency-generalization trade-off by enabling robust multi-LLM collaboration at token level, with theoretical justification showing it can recover optimal decoding policies where pure expert-only routing cannot.

Abstract: Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks.

</details>


### [216] [Observations and Remedies for Large Language Model Bias in Self-Consuming Performative Loop](https://arxiv.org/abs/2601.05184)
*Yaxuan Wang,Zhongteng Cai,Yujia Bao,Xueru Zhang,Yang Liu*

Main category: cs.AI

TL;DR: This paper introduces the Self-Consuming Performative Loop (SCPL) concept to study how synthetic data from LLMs creates feedback loops that amplify biases, and proposes reward-based rejection sampling to mitigate these biases.


<details>
  <summary>Details</summary>
Motivation: As LLMs are increasingly trained on their own synthetic outputs, this creates self-consuming loops that can cause performance degradation and emerging biases. Real-world applications involve dynamic systems where deployed models influence user feedback and data collection, potentially exacerbating biases against underserved groups.

Method: The authors introduce the SCPL framework to study bias evolution in controlled performative feedback settings. They examine two loop types: typical retraining and incremental fine-tuning. They conduct experiments on three real-world tasks and propose a reward-based rejection sampling strategy to mitigate bias.

Result: Experiments show that performative loops increase preference bias while decreasing disparate bias. The proposed reward-based rejection sampling strategy effectively mitigates these biases, moving toward more trustworthy self-improving systems.

Conclusion: Self-consuming performative loops in LLM training create complex bias dynamics that require careful management. The proposed reward-based rejection sampling offers a promising approach to mitigate bias amplification in iterative training processes, enabling more trustworthy self-improving AI systems.

Abstract: The rapid advancement of large language models (LLMs) has led to growing interest in using synthetic data to train future models. However, this creates a self-consuming retraining loop, where models are trained on their own outputs and may cause performance drops and induce emerging biases. In real-world applications, previously deployed LLMs may influence the data they generate, leading to a dynamic system driven by user feedback. For example, if a model continues to underserve users from a group, less query data will be collected from this particular demographic of users. In this study, we introduce the concept of \textbf{S}elf-\textbf{C}onsuming \textbf{P}erformative \textbf{L}oop (\textbf{SCPL}) and investigate the role of synthetic data in shaping bias during these dynamic iterative training processes under controlled performative feedback. This controlled setting is motivated by the inaccessibility of real-world user preference data from dynamic production systems, and enables us to isolate and analyze feedback-driven bias evolution in a principled manner. We focus on two types of loops, including the typical retraining setting and the incremental fine-tuning setting, which is largely underexplored. Through experiments on three real-world tasks, we find that the performative loop increases preference bias and decreases disparate bias. We design a reward-based rejection sampling strategy to mitigate the bias, moving towards more trustworthy self-improving systems.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [217] [UNIC: Learning Unified Multimodal Extrinsic Contact Estimation](https://arxiv.org/abs/2601.04356)
*Zhengtong Xu,Yuki Shirai*

Main category: cs.RO

TL;DR: UNIC is a unified multimodal framework for extrinsic contact estimation that works without prior knowledge or camera calibration, using visual, proprioceptive, and tactile data to estimate object-environment interactions for contact-rich manipulation.


<details>
  <summary>Details</summary>
Motivation: Existing contact estimation approaches rely on restrictive assumptions (predefined contact types, fixed grasps, camera calibration) that limit generalization to novel objects and unstructured environments, creating a need for more flexible, data-driven solutions.

Method: UNIC directly encodes visual observations in camera frame and integrates them with proprioceptive and tactile modalities. It uses a unified contact representation based on scene affordance maps and employs multimodal fusion with random masking for robust representation learning.

Result: UNIC achieves 9.6 mm average Chamfer distance error on unseen contact locations, performs well on unseen objects, remains robust under missing modalities, and adapts to dynamic camera viewpoints.

Conclusion: UNIC establishes extrinsic contact estimation as a practical and versatile capability for contact-rich manipulation by providing a unified, data-driven framework that generalizes well without restrictive assumptions.

Abstract: Contact-rich manipulation requires reliable estimation of extrinsic contacts-the interactions between a grasped object and its environment which provide essential contextual information for planning, control, and policy learning. However, existing approaches often rely on restrictive assumptions, such as predefined contact types, fixed grasp configurations, or camera calibration, that hinder generalization to novel objects and deployment in unstructured environments. In this paper, we present UNIC, a unified multimodal framework for extrinsic contact estimation that operates without any prior knowledge or camera calibration. UNIC directly encodes visual observations in the camera frame and integrates them with proprioceptive and tactile modalities in a fully data-driven manner. It introduces a unified contact representation based on scene affordance maps that captures diverse contact formations and employs a multimodal fusion mechanism with random masking, enabling robust multimodal representation learning. Extensive experiments demonstrate that UNIC performs reliably. It achieves a 9.6 mm average Chamfer distance error on unseen contact locations, performs well on unseen objects, remains robust under missing modalities, and adapts to dynamic camera viewpoints. These results establish extrinsic contact estimation as a practical and versatile capability for contact-rich manipulation.

</details>


### [218] [Generate, Transfer, Adapt: Learning Functional Dexterous Grasping from a Single Human Demonstration](https://arxiv.org/abs/2601.05243)
*Xingyi He,Adhitya Polavaram,Yunhao Cao,Om Deshmukh,Tianrui Wang,Xiaowei Zhou,Kuan Fang*

Main category: cs.RO

TL;DR: CorDex is a framework that learns dexterous functional grasps for novel objects from synthetic data generated from just one human demonstration, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Two main bottlenecks constrain progress in functional grasping with dexterous robotic hands: scarcity of large-scale datasets and lack of integrated semantic and geometric reasoning in learned models.

Method: 1) Correspondence-based data engine generates diverse training data from single human demo by creating object instances, transferring grasps via correspondence estimation, and optimizing grasps. 2) Multimodal prediction network with local-global fusion module and importance-aware sampling integrates visual and geometric information for efficient grasp prediction.

Result: CorDex generalizes well to unseen object instances across various categories and significantly outperforms state-of-the-art baselines in extensive experiments.

Conclusion: The framework enables robust learning of dexterous functional grasps from minimal human demonstration data, addressing key bottlenecks in robotic grasping through synthetic data generation and multimodal reasoning.

Abstract: Functional grasping with dexterous robotic hands is a key capability for enabling tool use and complex manipulation, yet progress has been constrained by two persistent bottlenecks: the scarcity of large-scale datasets and the absence of integrated semantic and geometric reasoning in learned models. In this work, we present CorDex, a framework that robustly learns dexterous functional grasps of novel objects from synthetic data generated from just a single human demonstration. At the core of our approach is a correspondence-based data engine that generates diverse, high-quality training data in simulation. Based on the human demonstration, our data engine generates diverse object instances of the same category, transfers the expert grasp to the generated objects through correspondence estimation, and adapts the grasp through optimization. Building on the generated data, we introduce a multimodal prediction network that integrates visual and geometric information. By devising a local-global fusion module and an importance-aware sampling mechanism, we enable robust and computationally efficient prediction of functional dexterous grasps. Through extensive experiments across various object categories, we demonstrate that CorDex generalizes well to unseen object instances and significantly outperforms state-of-the-art baselines.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [219] [Towards Spatio-Temporal Extrapolation of Phase-Field Simulations with Convolution-Only Neural Networks](https://arxiv.org/abs/2601.04510)
*Christophe Bonneville,Nathan Bieberdorf,Pieterjan Robbe,Mark Asta,Habib Najm,Laurent Capolungo,Cosmin Safta*

Main category: cs.CE

TL;DR: A deep learning surrogate model using conditional U-Net and diffusion models enables fast, accurate extrapolation of liquid metal dealloying simulations far beyond training data in space and time.


<details>
  <summary>Details</summary>
Motivation: Phase-field simulations of liquid metal dealloying are computationally expensive for large domains and long time horizons, limiting practical applications. There's a need for efficient surrogate models that can extrapolate beyond training data while maintaining physical accuracy.

Method: Developed a fully convolutional, conditionally parameterized U-Net surrogate with convolutional self-attention and physically informed padding. Coupled with a conditional diffusion model to generate synthetic initial conditions. Trained on small domain, short-time simulations but leverages convolutional nature for spatial and temporal extrapolation.

Result: Achieves relative errors below 5% in training regime and under 15% during large-scale extrapolation. Predicts key quantities of interest and spatial statistics accurately. Delivers up to 36,000× speed-up, reducing weeks-long simulations to seconds. Works across multiple alloy compositions.

Conclusion: The framework enables high-fidelity extrapolation of phase-field simulations for liquid metal dealloying in both space and time, representing a significant advancement in computational materials science with potential for broader application to other complex physical systems.

Abstract: Phase-field simulations of liquid metal dealloying (LMD) can capture complex microstructural evolutions but can be prohibitively expensive for large domains and long time horizons. In this paper, we introduce a fully convolutional, conditionally parameterized U-Net surrogate designed to extrapolate far beyond its training data in both space and time. The architecture integrates convolutional self-attention, physically informed padding, and a flood-fill corrector method to maintain accuracy under extreme extrapolation, while conditioning on simulation parameters allows for flexible time-step skipping and adaptation to varying alloy compositions. To remove the need for costly solver-based initialization, we couple the surrogate with a conditional diffusion model that generates synthetic, physically consistent initial conditions. We train our surrogate on simulations generated over small domain sizes and short time spans, but, by taking advantage of the convolutional nature of U-Nets, we are able to run and extrapolate surrogate simulations for longer time horizons than what would be achievable with classic numerical solvers. Across multiple alloy compositions, the framework is able to reproduce the LMD physics accurately. It predicts key quantities of interest and spatial statistics with relative errors typically below 5% in the training regime and under 15% during large-scale, long time-horizon extrapolations. Our framework can also deliver speed-ups of up to 36,000 times, bringing the time to run weeks-long simulations down to a few seconds. This work is a first stepping stone towards high-fidelity extrapolation in both space and time of phase-field simulation for LMD.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [220] [Sphinx: Benchmarking and Modeling for LLM-Driven Pull Request Review](https://arxiv.org/abs/2601.04252)
*Daoan Zhang,Shuo Zhang,Zijian Jin,Jiebo Luo,Shengyu Fu,Elsie Nallipogu*

Main category: cs.SE

TL;DR: Sphinx is a unified LLM framework for automated PR review that addresses noisy supervision, contextual understanding, and evaluation metrics through structured data generation, checklist-based evaluation, and reward policy optimization.


<details>
  <summary>Details</summary>
Motivation: Automating pull request review is challenging due to noisy supervision, limited contextual understanding of code changes, and inadequate evaluation metrics that don't capture real-world review quality.

Method: Three key components: (1) structured data generation pipeline comparing pseudo-modified and merged code for context-rich reviews; (2) checklist-based evaluation benchmark with actionable verification points; (3) Checklist Reward Policy Optimization (CRPO) using rule-based, interpretable rewards to align models with real review practices.

Result: Models trained with Sphinx achieve state-of-the-art performance on review completeness and precision, outperforming both proprietary and open-source baselines by up to 40% in checklist coverage.

Conclusion: Sphinx enables development of PR review models that are fluent, context-aware, technically precise, and practically deployable in real-world development workflows, addressing key limitations in automated code review.

Abstract: Pull request (PR) review is essential for ensuring software quality, yet automating this task remains challenging due to noisy supervision, limited contextual understanding, and inadequate evaluation metrics. We present Sphinx, a unified framework for LLM-based PR review that addresses these limitations through three key components: (1) a structured data generation pipeline that produces context-rich, semantically grounded review comments by comparing pseudo-modified and merged code; (2) a checklist-based evaluation benchmark that assesses review quality based on structured coverage of actionable verification points, moving beyond surface-level metrics like BLEU; and (3) Checklist Reward Policy Optimization (CRPO), a novel training paradigm that uses rule-based, interpretable rewards to align model behavior with real-world review practices. Extensive experiments show that models trained with Sphinx achieve state-of-the-art performance on review completeness and precision, outperforming both proprietary and open-source baselines by up to 40\% in checklist coverage. Together, Sphinx enables the development of PR review models that are not only fluent but also context-aware, technically precise, and practically deployable in real-world development workflows. The data will be released after review.

</details>


### [221] [Advancing Language Models for Code-related Tasks](https://arxiv.org/abs/2601.04526)
*Zhao Tian*

Main category: cs.SE

TL;DR: This paper proposes a comprehensive framework to improve language models for software engineering through three directions: data quality enhancement, architecture improvement, and reasoning advancement.


<details>
  <summary>Details</summary>
Motivation: Existing language models struggle with complex programming scenarios due to limitations in data quality, model architecture, and reasoning capability, hindering their practical adoption in software development.

Method: Three complementary approaches: (1) CODA (code difference-guided adversarial augmentation) and CodeDenoise for data quality; (2) LEAM and LEAM++ syntax-guided code LMs for architecture; (3) muFiX prompting and Specine agent-based techniques for reasoning.

Result: The proposed techniques systematically address key challenges in applying LMs to software engineering, though specific quantitative results are not provided in the abstract.

Conclusion: The research aims to promote practical adoption of LMs in software development and advance intelligent software engineering through comprehensive improvements in data, architecture, and reasoning capabilities.

Abstract: Recent advances in language models (LMs) have driven significant progress in various software engineering tasks. However, existing LMs still struggle with complex programming scenarios due to limitations in data quality, model architecture, and reasoning capability. This research systematically addresses these challenges through three complementary directions: (1) improving code data quality with a code difference-guided adversarial augmentation technique (CODA) and a code denoising technique (CodeDenoise); (2) enhancing model architecture via syntax-guided code LMs (LEAM and LEAM++); and (3) advancing model reasoning with a prompting technique (muFiX) and an agent-based technique (Specine). These techniques aim to promote the practical adoption of LMs in software development and further advance intelligent software engineering.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [222] [Scalable neural pushbroom architectures for real-time denoising of hyperspectral images onboard satellites](https://arxiv.org/abs/2601.05020)
*Ziyao Yi,Davide Piccinini,Diego Valsesia,Tiziano Bianchi,Enrico Magli*

Main category: eess.IV

TL;DR: Proposes a neural network design for onboard hyperspectral image denoising on satellites with three key objectives: high-quality inference, dynamic power scalability, and radiation fault tolerance.


<details>
  <summary>Details</summary>
Motivation: Next-gen Earth observation satellites need onboard AI to reduce latency for time-critical applications. Hyperspectral imaging on satellites has unique constraints (power, radiation, memory) not addressed by traditional computer vision, requiring new neural architectures.

Method: Proposes a mixture of denoisers with fault tolerance and power scaling capabilities. Each denoiser uses a causal, line-by-line processing architecture with memory of past lines to match pushbroom sensor acquisition and limit memory requirements.

Result: The architecture runs in real-time on low-power hardware (processes one line during next line acquisition), provides competitive denoising quality vs. more complex models, and enables tradeoffs between power scalability, fault tolerance, and denoising quality.

Conclusion: The proposed neural design successfully addresses the three competing objectives for onboard hyperspectral image denoising, enabling real-time processing with power scalability and radiation resilience for satellite applications.

Abstract: The next generation of Earth observation satellites will seek to deploy intelligent models directly onboard the payload in order to minimize the latency incurred by the transmission and processing chain of the ground segment, for time-critical applications. Designing neural architectures for onboard execution, particularly for satellite-based hyperspectral imagers, poses novel challenges due to the unique constraints of this environment and imaging system that are largely unexplored by the traditional computer vision literature. In this paper, we show that this setting requires addressing three competing objectives, namely high-quality inference with low complexity, dynamic power scalability and fault tolerance. We focus on the problem of hyperspectral image denoising, which is a critical task to enable effective downstream inference, and highlights the constraints of the onboard processing scenario. We propose a neural network design that addresses the three aforementioned objectives with several novel contributions. In particular, we propose a mixture of denoisers that can be resilient to radiation-induced faults as well as allowing for time-varying power scaling. Moreover, each denoiser employs an innovative architecture where an image is processed line-by-line in a causal way, with a memory of past lines, in order to match the acquisition process of pushbroom hyperspectral sensors and greatly limit memory requirements. We show that the proposed architecture can run in real-time, i.e., process one line in the time it takes to acquire the next one, on low-power hardware and provide competitive denoising quality with respect to significantly more complex state-of-the-art models. We also show that the power scalability and fault tolerance objectives provide a design space with multiple tradeoffs between those properties and denoising quality.

</details>

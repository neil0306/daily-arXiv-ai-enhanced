{"id": "2507.14189", "pdf": "https://arxiv.org/pdf/2507.14189", "abs": "https://arxiv.org/abs/2507.14189", "authors": ["Song Mao", "Lejun Cheng", "Pinlong Cai", "Guohang Yan", "Ding Wang", "Botian Shi"], "title": "DeepWriter: A Fact-Grounded Multimodal Writing Assistant Based On Offline Knowledge Base", "categories": ["cs.CL", "cs.AI"], "comment": "work in process", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nvarious applications. However, their use as writing assistants in specialized\ndomains like finance, medicine, and law is often hampered by a lack of deep\ndomain-specific knowledge and a tendency to hallucinate. Existing solutions,\nsuch as Retrieval-Augmented Generation (RAG), can suffer from inconsistency\nacross multiple retrieval steps, while online search-based methods often\ndegrade quality due to unreliable web content. To address these challenges, we\nintroduce DeepWriter, a customizable, multimodal, long-form writing assistant\nthat operates on a curated, offline knowledge base. DeepWriter leverages a\nnovel pipeline that involves task decomposition, outline generation, multimodal\nretrieval, and section-by-section composition with reflection. By deeply mining\ninformation from a structured corpus and incorporating both textual and visual\nelements, DeepWriter generates coherent, factually grounded, and\nprofessional-grade documents. We also propose a hierarchical knowledge\nrepresentation to enhance retrieval efficiency and accuracy. Our experiments on\nfinancial report generation demonstrate that DeepWriter produces high-quality,\nverifiable articles that surpasses existing baselines in factual accuracy and\ngenerated content quality.", "AI": {"tldr": "DeepWriter is a customizable, multimodal writing assistant for specialized domains, using a curated offline knowledge base to generate high-quality, factually grounded documents.", "motivation": "LLMs lack deep domain-specific knowledge and hallucinate, while existing solutions like RAG and online search suffer from inconsistency and unreliable content.", "method": "DeepWriter uses task decomposition, outline generation, multimodal retrieval, and section-by-section composition with reflection, leveraging a structured corpus and hierarchical knowledge representation.", "result": "DeepWriter outperforms baselines in financial report generation, producing verifiable, high-quality articles with superior factual accuracy.", "conclusion": "DeepWriter addresses LLM limitations in specialized domains, offering a robust solution for professional-grade document generation."}}
{"id": "2507.14198", "pdf": "https://arxiv.org/pdf/2507.14198", "abs": "https://arxiv.org/abs/2507.14198", "authors": ["Fufang Wen", "Shichang Zhang"], "title": "Retention analysis of edited knowledge after fine-tuning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) store vast amounts of knowledge, which often\nrequires updates to correct factual errors, incorporate newly acquired\ninformation, or adapt model behavior. Model editing methods have emerged as\nefficient solutions for such updates, offering localized and precise knowledge\nmodification at significantly lower computational cost than continual training.\nIn parallel, LLMs are frequently fine-tuned for a wide range of downstream\ntasks. However, the effect of fine-tuning on previously edited knowledge\nremains poorly understood. In this work, we systematically investigate how\ndifferent fine-tuning objectives interact with various model editing\ntechniques. Our findings show that edited knowledge is substantially more\nsusceptible to forgetting during fine-tuning than intrinsic knowledge acquired\nthrough pre-training. This analysis highlights a key limitation of current\nediting approaches and suggests that evaluating edit robustness under\ndownstream fine-tuning is critical for their practical deployment. We further\nfind that freezing layers associated with edited content can significantly\nimprove knowledge retention, offering insight into how future editing methods\nmight be made more robust.", "AI": {"tldr": "Fine-tuning impacts edited knowledge in LLMs more than intrinsic knowledge, revealing a need for robust editing methods.", "motivation": "To understand how fine-tuning affects edited knowledge in LLMs and improve editing robustness.", "method": "Systematically investigate interactions between fine-tuning objectives and model editing techniques.", "result": "Edited knowledge is more prone to forgetting during fine-tuning; freezing layers improves retention.", "conclusion": "Evaluating edit robustness under fine-tuning is crucial, and freezing layers can enhance knowledge retention."}}
{"id": "2507.14200", "pdf": "https://arxiv.org/pdf/2507.14200", "abs": "https://arxiv.org/abs/2507.14200", "authors": ["Shengji Tang", "Jianjian Cao", "Weihao Lin", "Jiale Hong", "Bo Zhang", "Shuyue Hu", "Lei Bai", "Tao Chen", "Wanli Ouyang", "Peng Ye"], "title": "Open-Source LLMs Collaboration Beats Closed-Source LLMs: A Scalable Multi-Agent System", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "This paper aims to demonstrate the potential and strengths of open-source\ncollectives. It leads to a promising question: Can we harness multiple\nopen-source LLMs to match or even beat the closed-source LLMs? To answer this,\nwe propose SMACS, a scalable multi-agent collaboration system (MACS) framework\nwith high performance. Specifically, for continuous integration of new LLMs and\ngeneralization to diverse questions, we first propose a Retrieval-based Prior\nSelection (RPS), which assigns a proxy performance score to each LLM to select\nthe Top-k LLMs at the instance level for any given question. Then, we propose\nan Exploration-Exploitation-Driven Posterior Enhancement (EPE), encouraging the\ngeneration of diverse responses through prior dropping and selecting the\nhigh-quality response via a hybrid posterior score. Experiments on eight\nmainstream benchmarks validate the effectiveness of our SMACS: by integrating\nfifteen open-source LLMs, SMACS outperforms leading closed-source LLMs in 2025,\ne.g., Claude-3.7-Sonnet (+12.73%), GPT-4.1(+5.36%) and GPT-o3-mini(+5.28%)\nacross multiple tasks. Remarkably, it even exceeds the average of best results\nof different datasets from both open-source LLMs (+2.86%) and closed-source\nLLMs (+2.04%), pushing the upper bound of intelligence. Code will be released\nat https://github.com/magent4aci/SMACS.", "AI": {"tldr": "SMACS, a scalable multi-agent collaboration system, integrates open-source LLMs to outperform closed-source LLMs like Claude-3.7-Sonnet and GPT-4.1, achieving higher performance across tasks.", "motivation": "To explore whether multiple open-source LLMs can surpass closed-source LLMs by leveraging collaborative frameworks.", "method": "Proposes SMACS with Retrieval-based Prior Selection (RPS) for LLM selection and Exploration-Exploitation-Driven Posterior Enhancement (EPE) for diverse, high-quality responses.", "result": "SMACS outperforms leading closed-source LLMs by significant margins (e.g., +12.73% over Claude-3.7-Sonnet) and exceeds the best results from both open and closed-source LLMs.", "conclusion": "SMACS demonstrates the potential of open-source collectives to push the boundaries of AI performance, offering a scalable and effective framework."}}
{"id": "2507.14214", "pdf": "https://arxiv.org/pdf/2507.14214", "abs": "https://arxiv.org/abs/2507.14214", "authors": ["Rui Zhao", "Vladyslav Melnychuk", "Jun Zhao", "Jesse Wright", "Nigel Shadbolt"], "title": "Let's Measure the Elephant in the Room: Facilitating Personalized Automated Analysis of Privacy Policies at Scale", "categories": ["cs.CL", "cs.CR", "cs.CY"], "comment": null, "summary": "In modern times, people have numerous online accounts, but they rarely read\nthe Terms of Service or Privacy Policy of those sites despite claiming\notherwise. This paper introduces PoliAnalyzer, a neuro-symbolic system that\nassists users with personalized privacy policy analysis. PoliAnalyzer uses\nNatural Language Processing (NLP) to extract formal representations of data\nusage practices from policy texts. In favor of deterministic, logical inference\nis applied to compare user preferences with the formal privacy policy\nrepresentation and produce a compliance report. To achieve this, we extend an\nexisting formal Data Terms of Use policy language to model privacy policies as\napp policies and user preferences as data policies. In our evaluation using our\nenriched PolicyIE dataset curated by legal experts, PoliAnalyzer demonstrated\nhigh accuracy in identifying relevant data usage practices, achieving F1-score\nof 90-100% across most tasks. Additionally, we demonstrate how PoliAnalyzer can\nmodel diverse user data-sharing preferences, derived from prior research as 23\nuser profiles, and perform compliance analysis against the top 100 most-visited\nwebsites. This analysis revealed that, on average, 95.2% of a privacy policy's\nsegments do not conflict with the analyzed user preferences, enabling users to\nconcentrate on understanding the 4.8% (636 / 13205) that violates preferences,\nsignificantly reducing cognitive burden. Further, we identified common\npractices in privacy policies that violate user expectations - such as the\nsharing of location data with 3rd parties. This paper demonstrates that\nPoliAnalyzer can support automated personalized privacy policy analysis at\nscale using off-the-shelf NLP tools. This sheds light on a pathway to help\nindividuals regain control over their data and encourage societal discussions\non platform data practices to promote a fairer power dynamic.", "AI": {"tldr": "PoliAnalyzer is a neuro-symbolic system using NLP to analyze privacy policies, comparing them with user preferences to reduce cognitive burden and improve compliance.", "motivation": "Users rarely read privacy policies despite their importance, leading to a need for automated, personalized analysis.", "method": "PoliAnalyzer uses NLP to extract formal representations of policies and applies logical inference to compare with user preferences.", "result": "Achieved 90-100% F1-score in identifying data usage practices and found 4.8% of policy segments violate user preferences.", "conclusion": "PoliAnalyzer enables scalable, automated privacy policy analysis, empowering users and promoting fairer data practices."}}
{"id": "2507.14268", "pdf": "https://arxiv.org/pdf/2507.14268", "abs": "https://arxiv.org/abs/2507.14268", "authors": ["Andreas Alpers", "Orkun Furat", "Christian Jung", "Matthias Neumann", "Claudia Redenbach", "Aigerim Saken", "Volker Schmidt"], "title": "Comparative Analysis of Algorithms for the Fitting of Tessellations to 3D Image Data", "categories": ["cs.CV", "cond-mat.mtrl-sci", "math.OC"], "comment": "31 pages, 16 figures, 8 tables", "summary": "This paper presents a comparative analysis of algorithmic strategies for\nfitting tessellation models to 3D image data of materials such as polycrystals\nand foams. In this steadily advancing field, we review and assess\noptimization-based methods -- including linear and nonlinear programming,\nstochastic optimization via the cross-entropy method, and gradient descent --\nfor generating Voronoi, Laguerre, and generalized balanced power diagrams\n(GBPDs) that approximate voxelbased grain structures. The quality of fit is\nevaluated on real-world datasets using discrepancy measures that quantify\ndifferences in grain volume, surface area, and topology. Our results highlight\ntrade-offs between model complexity, the complexity of the optimization\nroutines involved, and the quality of approximation, providing guidance for\nselecting appropriate methods based on data characteristics and application\nneeds.", "AI": {"tldr": "Comparative analysis of algorithmic strategies for fitting tessellation models to 3D image data, evaluating trade-offs between model complexity, optimization methods, and approximation quality.", "motivation": "To assess and guide the selection of optimization-based methods for approximating voxel-based grain structures in materials like polycrystals and foams.", "method": "Review and evaluation of linear/nonlinear programming, stochastic optimization (cross-entropy method), and gradient descent for generating Voronoi, Laguerre, and GBPD tessellations.", "result": "Trade-offs identified between model complexity, optimization routine complexity, and approximation quality, aiding method selection based on data and application needs.", "conclusion": "Provides practical guidance for choosing appropriate tessellation methods, balancing computational effort and accuracy for real-world datasets."}}
{"id": "2507.14231", "pdf": "https://arxiv.org/pdf/2507.14231", "abs": "https://arxiv.org/abs/2507.14231", "authors": ["Khalid Hasan", "Jamil Saquer"], "title": "Beyond Architectures: Evaluating the Role of Contextual Embeddings in Detecting Bipolar Disorder on Social Media", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "The 37th International Conference on Software Engineering & Knowledge\n  Engineering, SEKE 2025 (camera-ready)", "summary": "Bipolar disorder is a chronic mental illness frequently underdiagnosed due to\nsubtle early symptoms and social stigma. This paper explores the advanced\nnatural language processing (NLP) models for recognizing signs of bipolar\ndisorder based on user-generated social media text. We conduct a comprehensive\nevaluation of transformer-based models (BERT, RoBERTa, ALBERT, ELECTRA,\nDistilBERT) and Long Short Term Memory (LSTM) models based on contextualized\n(BERT) and static (GloVe, Word2Vec) word embeddings. Experiments were performed\non a large, annotated dataset of Reddit posts after confirming their validity\nthrough sentiment variance and judgmental analysis. Our results demonstrate\nthat RoBERTa achieves the highest performance among transformer models with an\nF1 score of ~98% while LSTM models using BERT embeddings yield nearly identical\nresults. In contrast, LSTMs trained on static embeddings fail to capture\nmeaningful patterns, scoring near-zero F1. These findings underscore the\ncritical role of contextual language modeling in detecting bipolar disorder. In\naddition, we report model training times and highlight that DistilBERT offers\nan optimal balance between efficiency and accuracy. In general, our study\noffers actionable insights for model selection in mental health NLP\napplications and validates the potential of contextualized language models to\nsupport early bipolar disorder screening.", "AI": {"tldr": "The paper evaluates NLP models for detecting bipolar disorder from social media text, finding RoBERTa and BERT-embedded LSTMs most effective, while static embeddings fail. DistilBERT balances efficiency and accuracy.", "motivation": "Bipolar disorder is often underdiagnosed due to subtle symptoms and stigma. The study aims to leverage NLP for early detection using social media text.", "method": "Evaluated transformer models (BERT, RoBERTa, ALBERT, ELECTRA, DistilBERT) and LSTMs with contextualized (BERT) and static (GloVe, Word2Vec) embeddings on annotated Reddit posts.", "result": "RoBERTa achieved the highest F1 score (~98%), with BERT-embedded LSTMs performing similarly. Static embeddings scored near-zero. DistilBERT offered efficiency-accuracy balance.", "conclusion": "Contextualized models are crucial for bipolar disorder detection. The study provides insights for model selection in mental health NLP and supports early screening."}}
{"id": "2507.14303", "pdf": "https://arxiv.org/pdf/2507.14303", "abs": "https://arxiv.org/abs/2507.14303", "authors": ["Ehsan Rassekh"], "title": "Semantic Segmentation based Scene Understanding in Autonomous Vehicles", "categories": ["cs.CV", "I.4.8"], "comment": "74 pages, 35 figures, Master's Thesis, Institute for Advanced Studies\n  in Basic Sciences (IASBS), Zanjan, Iran, 2023", "summary": "In recent years, the concept of artificial intelligence (AI) has become a\nprominent keyword because it is promising in solving complex tasks. The need\nfor human expertise in specific areas may no longer be needed because machines\nhave achieved successful results using artificial intelligence and can make the\nright decisions in critical situations. This process is possible with the help\nof deep learning (DL), one of the most popular artificial intelligence\ntechnologies. One of the areas in which the use of DL is used is in the\ndevelopment of self-driving cars, which is very effective and important. In\nthis work, we propose several efficient models to investigate scene\nunderstanding through semantic segmentation. We use the BDD100k dataset to\ninvestigate these models. Another contribution of this work is the usage of\nseveral Backbones as encoders for models. The obtained results show that\nchoosing the appropriate backbone has a great effect on the performance of the\nmodel for semantic segmentation. Better performance in semantic segmentation\nallows us to understand better the scene and the environment around the agent.\nIn the end, we analyze and evaluate the proposed models in terms of accuracy,\nmean IoU, and loss function, and the results show that these metrics are\nimproved.", "AI": {"tldr": "The paper explores efficient models for scene understanding via semantic segmentation in self-driving cars, using the BDD100k dataset and various backbones, showing improved performance metrics.", "motivation": "To enhance scene understanding in autonomous vehicles by leveraging deep learning for semantic segmentation, reducing reliance on human expertise.", "method": "Proposes several models with different backbone encoders, tested on the BDD100k dataset, evaluated using accuracy, mean IoU, and loss function.", "result": "Appropriate backbone selection significantly impacts model performance, improving semantic segmentation metrics.", "conclusion": "The study demonstrates the importance of backbone choice in DL models for semantic segmentation, achieving better scene understanding in autonomous driving."}}
{"id": "2507.14238", "pdf": "https://arxiv.org/pdf/2507.14238", "abs": "https://arxiv.org/abs/2507.14238", "authors": ["Matthew Kearney", "Reuben Binns", "Yarin Gal"], "title": "Language Models Change Facts Based on the Way You Talk", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Large language models (LLMs) are increasingly being used in user-facing\napplications, from providing medical consultations to job interview advice.\nRecent research suggests that these models are becoming increasingly proficient\nat inferring identity information about the author of a piece of text from\nlinguistic patterns as subtle as the choice of a few words. However, little is\nknown about how LLMs use this information in their decision-making in\nreal-world applications. We perform the first comprehensive analysis of how\nidentity markers present in a user's writing bias LLM responses across five\ndifferent high-stakes LLM applications in the domains of medicine, law,\npolitics, government benefits, and job salaries. We find that LLMs are\nextremely sensitive to markers of identity in user queries and that race,\ngender, and age consistently influence LLM responses in these applications. For\ninstance, when providing medical advice, we find that models apply different\nstandards of care to individuals of different ethnicities for the same\nsymptoms; we find that LLMs are more likely to alter answers to align with a\nconservative (liberal) political worldview when asked factual questions by\nolder (younger) individuals; and that LLMs recommend lower salaries for\nnon-White job applicants and higher salaries for women compared to men. Taken\ntogether, these biases mean that the use of off-the-shelf LLMs for these\napplications may cause harmful differences in medical care, foster wage gaps,\nand create different political factual realities for people of different\nidentities. Beyond providing an analysis, we also provide new tools for\nevaluating how subtle encoding of identity in users' language choices impacts\nmodel decisions. Given the serious implications of these findings, we recommend\nthat similar thorough assessments of LLM use in user-facing applications are\nconducted before future deployment.", "AI": {"tldr": "LLMs infer identity from text and exhibit biases in high-stakes applications like medicine, law, and job salaries, leading to harmful disparities.", "motivation": "To analyze how identity markers in user queries bias LLM responses in critical real-world applications.", "method": "Comprehensive analysis across five domains (medicine, law, politics, government benefits, job salaries) to assess LLM sensitivity to identity markers like race, gender, and age.", "result": "LLMs show consistent biases, e.g., varying medical care by ethnicity, aligning answers with political views by age, and recommending unequal salaries by race and gender.", "conclusion": "Off-the-shelf LLMs can cause harmful disparities; thorough assessments are needed before deployment in user-facing applications."}}
{"id": "2507.14312", "pdf": "https://arxiv.org/pdf/2507.14312", "abs": "https://arxiv.org/abs/2507.14312", "authors": ["Marc Lafon", "Gustavo Adolfo Vargas Hakim", "Cl\u00e9ment Rambour", "Christian Desrosier", "Nicolas Thome"], "title": "CLIPTTA: Robust Contrastive Vision-Language Test-Time Adaptation", "categories": ["cs.CV"], "comment": null, "summary": "Vision-language models (VLMs) like CLIP exhibit strong zero-shot capabilities\nbut often fail to generalize under distribution shifts. Test-time adaptation\n(TTA) allows models to update at inference time without labeled data, typically\nvia entropy minimization. However, this objective is fundamentally misaligned\nwith the contrastive image-text training of VLMs, limiting adaptation\nperformance and introducing failure modes such as pseudo-label drift and class\ncollapse. We propose CLIPTTA, a new gradient-based TTA method for\nvision-language models that leverages a soft contrastive loss aligned with\nCLIP's pre-training objective. We provide a theoretical analysis of CLIPTTA's\ngradients, showing how its batch-aware design mitigates the risk of collapse.\nWe further extend CLIPTTA to the open-set setting, where both in-distribution\n(ID) and out-of-distribution (OOD) samples are encountered, using an Outlier\nContrastive Exposure (OCE) loss to improve OOD detection. Evaluated on 75\ndatasets spanning diverse distribution shifts, CLIPTTA consistently outperforms\nentropy-based objectives and is highly competitive with state-of-the-art TTA\nmethods, outperforming them on a large number of datasets and exhibiting more\nstable performance across diverse shifts.", "AI": {"tldr": "CLIPTTA is a gradient-based test-time adaptation method for vision-language models, using a soft contrastive loss to align with CLIP's pre-training, improving generalization under distribution shifts.", "motivation": "Vision-language models (VLMs) like CLIP struggle with distribution shifts during inference, and traditional entropy-based TTA methods are misaligned with their contrastive training.", "method": "CLIPTTA employs a soft contrastive loss aligned with CLIP's pre-training, includes a batch-aware design to prevent collapse, and extends to open-set scenarios with an Outlier Contrastive Exposure (OCE) loss.", "result": "CLIPTTA outperforms entropy-based methods and competes with state-of-the-art TTA methods across 75 datasets, showing stable performance under diverse shifts.", "conclusion": "CLIPTTA effectively addresses the limitations of entropy-based TTA for VLMs, offering robust adaptation and improved OOD detection."}}
{"id": "2507.14239", "pdf": "https://arxiv.org/pdf/2507.14239", "abs": "https://arxiv.org/abs/2507.14239", "authors": ["Weihua Zheng", "Roy Ka-Wei Lee", "Zhengyuan Liu", "Kui Wu", "AiTi Aw", "Bowei Zou"], "title": "CCL-XCoT: An Efficient Cross-Lingual Knowledge Transfer Method for Mitigating Hallucination Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multilingual Large Language Models(MLLMs) demonstrate strong generalization\nacross languages, yet they remain prone to hallucinations, especially in\nlow-resource languages, due to training data imbalances. These hallucinations,\nwhich include inaccurate or fabricated outputs, are particularly problematic in\ndomain-specific generation tasks (Chataigner et al., 2024). To address this\nchallenge, we propose CCL-XCoT(Curriculum-based Contrastive Learning-based\nCross-lingual Chain-of-Thought), a two-stage fine-tuning framework for\nmitigating hallucination in MLLMs. Our approach first enhances cross-lingual\nsemantic alignment through curriculum-based contrastive learning combined with\nnext-token prediction during continued pre-training. Building on this\nfoundation, we then introduce a cross-lingual Chain-of-Thought (XCoT) prompting\nstrategy during instruction fine-tuning, which guides the model to reason in a\nhigh-resource language before generating answers in the target low-resource\nlanguage. Experimental results show that CCL-XCoT reduces hallucination rates\nby up to 62% and substantially improves factual knowledge transfer across\nlanguage pairs, without relying on external retrieval or multi-model ensembles.", "AI": {"tldr": "CCL-XCoT, a two-stage fine-tuning framework, reduces hallucinations in Multilingual Large Language Models (MLLMs) by 62% using curriculum-based contrastive learning and cross-lingual Chain-of-Thought prompting.", "motivation": "MLLMs suffer from hallucinations, especially in low-resource languages, due to training data imbalances, impacting domain-specific tasks.", "method": "1. Curriculum-based contrastive learning for cross-lingual semantic alignment. 2. Cross-lingual Chain-of-Thought (XCoT) prompting during fine-tuning to guide reasoning in high-resource languages before generating in low-resource ones.", "result": "CCL-XCoT reduces hallucination rates by up to 62% and improves factual knowledge transfer across languages.", "conclusion": "The proposed framework effectively mitigates hallucinations in MLLMs without external tools, enhancing cross-lingual performance."}}
{"id": "2507.14315", "pdf": "https://arxiv.org/pdf/2507.14315", "abs": "https://arxiv.org/abs/2507.14315", "authors": ["Qiyu Xu", "Zhanxuan Hu", "Yu Duan", "Ercheng Pei", "Yonghang Tai"], "title": "A Hidden Stumbling Block in Generalized Category Discovery: Distracted Attention", "categories": ["cs.CV"], "comment": null, "summary": "Generalized Category Discovery (GCD) aims to classify unlabeled data from\nboth known and unknown categories by leveraging knowledge from labeled known\ncategories. While existing methods have made notable progress, they often\noverlook a hidden stumbling block in GCD: distracted attention. Specifically,\nwhen processing unlabeled data, models tend to focus not only on key objects in\nthe image but also on task-irrelevant background regions, leading to suboptimal\nfeature extraction. To remove this stumbling block, we propose Attention\nFocusing (AF), an adaptive mechanism designed to sharpen the model's focus by\npruning non-informative tokens. AF consists of two simple yet effective\ncomponents: Token Importance Measurement (TIME) and Token Adaptive Pruning\n(TAP), working in a cascade. TIME quantifies token importance across multiple\nscales, while TAP prunes non-informative tokens by utilizing the multi-scale\nimportance scores provided by TIME. AF is a lightweight, plug-and-play module\nthat integrates seamlessly into existing GCD methods with minimal computational\noverhead. When incorporated into one prominent GCD method, SimGCD, AF achieves\nup to 15.4% performance improvement over the baseline with minimal\ncomputational overhead. The implementation code is provided in\nhttps://github.com/Afleve/AFGCD.", "AI": {"tldr": "The paper introduces Attention Focusing (AF), a mechanism to improve Generalized Category Discovery (GCD) by reducing distracted attention in models, leading to better feature extraction and performance.", "motivation": "Existing GCD methods often suffer from distracted attention, where models focus on irrelevant background regions, degrading performance.", "method": "AF uses Token Importance Measurement (TIME) and Token Adaptive Pruning (TAP) to prune non-informative tokens, sharpening the model's focus.", "result": "AF improves performance by up to 15.4% when integrated into SimGCD, with minimal computational overhead.", "conclusion": "AF is a lightweight, effective solution to distracted attention in GCD, easily integrable into existing methods."}}
{"id": "2507.14240", "pdf": "https://arxiv.org/pdf/2507.14240", "abs": "https://arxiv.org/abs/2507.14240", "authors": ["Mohammad Shahedur Rahman", "Peng Gao", "Yuede Ji"], "title": "HuggingGraph: Understanding the Supply Chain of LLM Ecosystem", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, 5 figures", "summary": "Large language models (LLMs) leverage deep learning to process and predict\nsequences of words from context, enabling them to perform various NLP tasks,\nsuch as translation, summarization, question answering, and content generation.\nHowever, the growing size and complexity of developing, training, and deploying\nadvanced LLMs require extensive computational resources and large datasets.\nThis creates a barrier for users. As a result, platforms that host models and\ndatasets are widely used. For example, Hugging Face, one of the most popular\nplatforms, hosted 1.8 million models and 450K datasets by June 2025, with no\nsign of slowing down. Since many LLMs are built from base models, pre-trained\nmodels, and external datasets, they can inherit vulnerabilities, biases, or\nmalicious components from earlier models or datasets. Therefore, it is critical\nto understand the origin and development of these components to better detect\npotential risks, improve model fairness, and ensure compliance. Motivated by\nthis, our project aims to study the relationships between models and datasets,\nwhich are core components of the LLM supply chain. First, we design a method to\nsystematically collect LLM supply chain data. Using this data, we build a\ndirected heterogeneous graph to model the relationships between models and\ndatasets, resulting in a structure with 397,376 nodes and 453,469 edges. We\nthen perform various analyses and uncover several findings, such as: (i) the\nLLM supply chain graph is large, sparse, and follows a power-law degree\ndistribution; (ii) it features a densely connected core and a fragmented\nperiphery; (iii) datasets play pivotal roles in training; (iv) strong\ninterdependence exists between models and datasets; and (v) the graph is\ndynamic, with daily updates reflecting the ecosystem's ongoing evolution.", "AI": {"tldr": "The paper analyzes the LLM supply chain, focusing on model-dataset relationships, using a graph-based approach to uncover structural patterns and dynamics.", "motivation": "The growing complexity and resource demands of LLMs, along with inherited vulnerabilities and biases, necessitate understanding the LLM supply chain to mitigate risks and ensure fairness.", "method": "The study systematically collects LLM supply chain data and constructs a directed heterogeneous graph (397,376 nodes, 453,469 edges) to model relationships between models and datasets.", "result": "Key findings include the graph's large, sparse, power-law structure; a dense core with fragmented periphery; pivotal dataset roles; strong model-dataset interdependence; and dynamic daily updates.", "conclusion": "The study highlights the importance of tracking LLM supply chains for risk detection, fairness improvement, and compliance, with the graph model providing actionable insights."}}
{"id": "2507.14367", "pdf": "https://arxiv.org/pdf/2507.14367", "abs": "https://arxiv.org/abs/2507.14367", "authors": ["Weiming Ren", "Raghav Goyal", "Zhiming Hu", "Tristan Ty Aumentado-Armstrong", "Iqbal Mohomed", "Alex Levinshtein"], "title": "Hallucination Score: Towards Mitigating Hallucinations in Generative Image Super-Resolution", "categories": ["cs.CV"], "comment": "12 pages, 17 figures and 7 tables", "summary": "Generative super-resolution (GSR) currently sets the state-of-the-art in\nterms of perceptual image quality, overcoming the \"regression-to-the-mean\" blur\nof prior non-generative models. However, from a human perspective, such models\ndo not fully conform to the optimal balance between quality and fidelity.\nInstead, a different class of artifacts, in which generated details fail to\nperceptually match the low resolution image (LRI) or ground-truth image (GTI),\nis a critical but under studied issue in GSR, limiting its practical\ndeployments. In this work, we focus on measuring, analyzing, and mitigating\nthese artifacts (i.e., \"hallucinations\"). We observe that hallucinations are\nnot well-characterized with existing image metrics or quality models, as they\nare orthogonal to both exact fidelity and no-reference quality. Instead, we\ntake advantage of a multimodal large language model (MLLM) by constructing a\nprompt that assesses hallucinatory visual elements and generates a\n\"Hallucination Score\" (HS). We find that our HS is closely aligned with human\nevaluations, and also provides complementary insights to prior image metrics\nused for super-resolution (SR) models. In addition, we find certain deep\nfeature distances have strong correlations with HS. We therefore propose to\nalign the GSR models by using such features as differentiable reward functions\nto mitigate hallucinations.", "AI": {"tldr": "The paper addresses hallucination artifacts in generative super-resolution (GSR) models, proposing a Hallucination Score (HS) using a multimodal large language model (MLLM) to measure and mitigate these issues.", "motivation": "GSR models produce perceptual artifacts where generated details mismatch low-resolution or ground-truth images, limiting practical use. Existing metrics fail to capture these hallucinations.", "method": "The study constructs a prompt for an MLLM to assess hallucinations, generating an HS. It also identifies deep feature distances correlating with HS and uses them as differentiable rewards to align GSR models.", "result": "HS aligns well with human evaluations and complements existing SR metrics. Deep feature distances show strong correlation with HS.", "conclusion": "The proposed HS and feature-based alignment effectively measure and reduce hallucinations in GSR, improving practical deployment."}}
{"id": "2507.14241", "pdf": "https://arxiv.org/pdf/2507.14241", "abs": "https://arxiv.org/abs/2507.14241", "authors": ["Rithesh Murthy", "Ming Zhu", "Liangwei Yang", "Jielin Qiu", "Juntao Tan", "Shelby Heinecke", "Huan Wang", "Caiming Xiong", "Silvio Savarese"], "title": "Promptomatix: An Automatic Prompt Optimization Framework for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) perform best with well-crafted prompts, yet\nprompt engineering remains manual, inconsistent, and inaccessible to\nnon-experts. We introduce Promptomatix, an automatic prompt optimization\nframework that transforms natural language task descriptions into high-quality\nprompts without requiring manual tuning or domain expertise. Promptomatix\nsupports both a lightweight meta-prompt-based optimizer and a DSPy-powered\ncompiler, with modular design enabling future extension to more advanced\nframeworks. The system analyzes user intent, generates synthetic training data,\nselects prompting strategies, and refines prompts using cost-aware objectives.\nEvaluated across 5 task categories, Promptomatix achieves competitive or\nsuperior performance compared to existing libraries, while reducing prompt\nlength and computational overhead making prompt optimization scalable and\nefficient.", "AI": {"tldr": "Promptomatix is an automatic prompt optimization framework for LLMs, eliminating manual tuning and improving efficiency.", "motivation": "Manual prompt engineering is inconsistent and inaccessible to non-experts, necessitating an automated solution.", "method": "Uses meta-prompt-based optimization and DSPy-powered compiler, analyzing intent, generating synthetic data, and refining prompts.", "result": "Achieves competitive or superior performance across 5 tasks, reducing prompt length and computational overhead.", "conclusion": "Promptomatix makes prompt optimization scalable, efficient, and accessible without domain expertise."}}
{"id": "2507.14368", "pdf": "https://arxiv.org/pdf/2507.14368", "abs": "https://arxiv.org/abs/2507.14368", "authors": ["Praneeth Namburi", "Roger Pallar\u00e8s-L\u00f3pez", "Jessica Rosendorf", "Duarte Folgado", "Brian W. Anthony"], "title": "DUSTrack: Semi-automated point tracking in ultrasound videos", "categories": ["cs.CV", "q-bio.QM"], "comment": null, "summary": "Ultrasound technology enables safe, non-invasive imaging of dynamic tissue\nbehavior, making it a valuable tool in medicine, biomechanics, and sports\nscience. However, accurately tracking tissue motion in B-mode ultrasound\nremains challenging due to speckle noise, low edge contrast, and out-of-plane\nmovement. These challenges complicate the task of tracking anatomical landmarks\nover time, which is essential for quantifying tissue dynamics in many clinical\nand research applications. This manuscript introduces DUSTrack (Deep learning\nand optical flow-based toolkit for UltraSound Tracking), a semi-automated\nframework for tracking arbitrary points in B-mode ultrasound videos. We combine\ndeep learning with optical flow to deliver high-quality and robust tracking\nacross diverse anatomical structures and motion patterns. The toolkit includes\na graphical user interface that streamlines the generation of high-quality\ntraining data and supports iterative model refinement. It also implements a\nnovel optical-flow-based filtering technique that reduces high-frequency\nframe-to-frame noise while preserving rapid tissue motion. DUSTrack\ndemonstrates superior accuracy compared to contemporary zero-shot point\ntrackers and performs on par with specialized methods, establishing its\npotential as a general and foundational tool for clinical and biomechanical\nresearch. We demonstrate DUSTrack's versatility through three use cases:\ncardiac wall motion tracking in echocardiograms, muscle deformation analysis\nduring reaching tasks, and fascicle tracking during ankle plantarflexion. As an\nopen-source solution, DUSTrack offers a powerful, flexible framework for point\ntracking to quantify tissue motion from ultrasound videos. DUSTrack is\navailable at https://github.com/praneethnamburi/DUSTrack.", "AI": {"tldr": "DUSTrack is a semi-automated toolkit combining deep learning and optical flow for robust point tracking in B-mode ultrasound videos, outperforming zero-shot trackers and matching specialized methods.", "motivation": "Accurate tissue motion tracking in B-mode ultrasound is hindered by speckle noise, low edge contrast, and out-of-plane movement, necessitating a reliable solution for clinical and biomechanical research.", "method": "DUSTrack integrates deep learning with optical flow, features a GUI for training data generation, and employs optical-flow-based filtering to reduce noise while preserving motion.", "result": "DUSTrack achieves superior accuracy compared to zero-shot trackers and matches specialized methods, demonstrated in cardiac, muscle, and fascicle tracking use cases.", "conclusion": "DUSTrack is a versatile, open-source tool for quantifying tissue motion in ultrasound videos, with potential for broad clinical and research applications."}}
{"id": "2507.14298", "pdf": "https://arxiv.org/pdf/2507.14298", "abs": "https://arxiv.org/abs/2507.14298", "authors": ["Wan-Cyuan Fan", "Yen-Chun Chen", "Mengchen Liu", "Alexander Jacobson", "Lu Yuan", "Leonid Sigal"], "title": "In-Depth and In-Breadth: Pre-training Multimodal Language Models Customized for Comprehensive Chart Understanding", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "arXiv admin note: substantial text overlap with arXiv:2407.14506", "summary": "Recent methods for customizing Large Vision Language Models (LVLMs) for\ndomain-specific tasks have shown promising results in scientific chart\ncomprehension. However, existing approaches face two major limitations: First,\nthey rely on paired data from only a few chart types, limiting generalization\nto wide range of chart types. Secondly, they lack targeted pre-training for\nchart-data alignment, which hampers the model's understanding of underlying\ndata. In this paper, we introduce ChartScope, an LVLM optimized for in-depth\nchart comprehension across diverse chart types. We propose an efficient data\ngeneration pipeline that synthesizes paired data for a wide range of chart\ntypes, along with a novel Dual-Path training strategy that enabling the model\nto succinctly capture essential data details while preserving robust reasoning\ncapabilities by incorporating reasoning over the underlying data. Lastly, we\nestablish ChartDQA, a new benchmark for evaluating not only question-answering\nat different levels but also underlying data understanding. Experimental\nresults demonstrate that ChartScope significantly enhances comprehension on a\nwide range of chart types. The code and data are available at\nhttps://davidhalladay.github.io/chartscope_demo.", "AI": {"tldr": "ChartScope is a new LVLM for chart comprehension, addressing limitations of existing methods by using diverse chart data and a Dual-Path training strategy, outperforming benchmarks.", "motivation": "Existing LVLMs for chart comprehension lack generalization across chart types and targeted pre-training for data alignment.", "method": "Proposes a data generation pipeline for diverse chart types and a Dual-Path training strategy for data and reasoning alignment.", "result": "ChartScope significantly improves comprehension across various chart types, validated by the new ChartDQA benchmark.", "conclusion": "ChartScope advances chart comprehension by addressing data diversity and alignment, with open-source code and data."}}
{"id": "2507.14426", "pdf": "https://arxiv.org/pdf/2507.14426", "abs": "https://arxiv.org/abs/2507.14426", "authors": ["Zhou Chen", "Joe Lin", "Sathyanarayanan N. Aakur"], "title": "CRAFT: A Neuro-Symbolic Framework for Visual Functional Affordance Grounding", "categories": ["cs.CV"], "comment": "Accepted to NeSy 2025", "summary": "We introduce CRAFT, a neuro-symbolic framework for interpretable affordance\ngrounding, which identifies the objects in a scene that enable a given action\n(e.g., \"cut\"). CRAFT integrates structured commonsense priors from ConceptNet\nand language models with visual evidence from CLIP, using an energy-based\nreasoning loop to refine predictions iteratively. This process yields\ntransparent, goal-driven decisions to ground symbolic and perceptual\nstructures. Experiments in multi-object, label-free settings demonstrate that\nCRAFT enhances accuracy while improving interpretability, providing a step\ntoward robust and trustworthy scene understanding.", "AI": {"tldr": "CRAFT is a neuro-symbolic framework for interpretable affordance grounding, combining commonsense priors and visual evidence to improve accuracy and transparency in scene understanding.", "motivation": "To address the challenge of identifying objects enabling specific actions in a scene, with a focus on interpretability and robustness.", "method": "Integrates structured commonsense priors (ConceptNet, language models) with visual evidence (CLIP) using an energy-based reasoning loop for iterative refinement.", "result": "Enhances accuracy and interpretability in multi-object, label-free settings, advancing trustworthy scene understanding.", "conclusion": "CRAFT provides a transparent, goal-driven approach for affordance grounding, contributing to robust scene understanding."}}
{"id": "2507.14304", "pdf": "https://arxiv.org/pdf/2507.14304", "abs": "https://arxiv.org/abs/2507.14304", "authors": ["Rakesh Paul", "Anusha Kamath", "Kanishk Singla", "Raviraj Joshi", "Utkarsh Vaidya", "Sanjay Singh Chauhan", "Niranjan Wartikar"], "title": "Aligning Large Language Models to Low-Resource Languages through LLM-Based Selective Translation: A Systematic Study", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Multilingual large language models (LLMs) often demonstrate a performance gap\nbetween English and non-English languages, particularly in low-resource\nsettings. Aligning these models to low-resource languages is essential yet\nchallenging due to limited high-quality data. While English alignment datasets\nare readily available, curating equivalent data in other languages is expensive\nand time-consuming. A common workaround is to translate existing English\nalignment data; however, standard translation techniques often fail to preserve\ncritical elements such as code, mathematical expressions, and structured\nformats like JSON. In this work, we investigate LLM-based selective\ntranslation, a technique that selectively translates only the translatable\nparts of a text while preserving non-translatable content and sentence\nstructure. We conduct a systematic study to explore key questions around this\napproach, including its effectiveness compared to vanilla translation, the\nimportance of filtering noisy outputs, and the benefits of mixing translated\nsamples with original English data during alignment. Our experiments focus on\nthe low-resource Indic language Hindi and compare translations generated by\nGoogle Cloud Translation (GCP) and Llama-3.1-405B. The results highlight the\npromise of selective translation as a practical and effective method for\nimproving multilingual alignment in LLMs.", "AI": {"tldr": "Selective translation improves multilingual LLM alignment by preserving non-translatable content, outperforming standard translation methods.", "motivation": "Addressing the performance gap in multilingual LLMs for low-resource languages due to limited high-quality alignment data.", "method": "LLM-based selective translation, preserving non-translatable elements like code and JSON, compared with vanilla translation and mixed-data alignment.", "result": "Selective translation shows promise, especially for Hindi, outperforming Google Cloud Translation and Llama-3.1-405B.", "conclusion": "Selective translation is a practical and effective method for enhancing multilingual alignment in LLMs."}}
{"id": "2507.14432", "pdf": "https://arxiv.org/pdf/2507.14432", "abs": "https://arxiv.org/abs/2507.14432", "authors": ["Han Gong", "Qiyue Li", "Zhi Liu", "Hao Zhou", "Peng Yuan Zhou", "Zhu Li", "Jie Li"], "title": "Adaptive 3D Gaussian Splatting Video Streaming", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "The advent of 3D Gaussian splatting (3DGS) has significantly enhanced the\nquality of volumetric video representation. Meanwhile, in contrast to\nconventional volumetric video, 3DGS video poses significant challenges for\nstreaming due to its substantially larger data volume and the heightened\ncomplexity involved in compression and transmission. To address these issues,\nwe introduce an innovative framework for 3DGS volumetric video streaming.\nSpecifically, we design a 3DGS video construction method based on the Gaussian\ndeformation field. By employing hybrid saliency tiling and differentiated\nquality modeling of 3DGS video, we achieve efficient data compression and\nadaptation to bandwidth fluctuations while ensuring high transmission quality.\nThen we build a complete 3DGS video streaming system and validate the\ntransmission performance. Through experimental evaluation, our method\ndemonstrated superiority over existing approaches in various aspects, including\nvideo quality, compression effectiveness, and transmission rate.", "AI": {"tldr": "A framework for efficient streaming of 3D Gaussian splatting (3DGS) volumetric video, addressing challenges like large data volume and compression complexity.", "motivation": "3DGS videos have superior quality but pose streaming challenges due to their size and complexity.", "method": "Uses Gaussian deformation field for construction, hybrid saliency tiling, and differentiated quality modeling for compression and bandwidth adaptation.", "result": "Outperforms existing methods in video quality, compression, and transmission rate.", "conclusion": "The proposed framework effectively streams 3DGS videos with high quality and efficiency."}}
{"id": "2507.14307", "pdf": "https://arxiv.org/pdf/2507.14307", "abs": "https://arxiv.org/abs/2507.14307", "authors": ["Karin de Langis", "Jong Inn Park", "Andreas Schramm", "Bin Hu", "Khanh Chi Le", "Michael Mensink", "Ahn Thu Tong", "Dongyeop Kang"], "title": "How LLMs Comprehend Temporal Meaning in Narratives: A Case Study in Cognitive Evaluation of LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) exhibit increasingly sophisticated linguistic\ncapabilities, yet the extent to which these behaviors reflect human-like\ncognition versus advanced pattern recognition remains an open question. In this\nstudy, we investigate how LLMs process the temporal meaning of linguistic\naspect in narratives that were previously used in human studies. Using an\nExpert-in-the-Loop probing pipeline, we conduct a series of targeted\nexperiments to assess whether LLMs construct semantic representations and\npragmatic inferences in a human-like manner. Our findings show that LLMs\nover-rely on prototypicality, produce inconsistent aspectual judgments, and\nstruggle with causal reasoning derived from aspect, raising concerns about\ntheir ability to fully comprehend narratives. These results suggest that LLMs\nprocess aspect fundamentally differently from humans and lack robust narrative\nunderstanding. Beyond these empirical findings, we develop a standardized\nexperimental framework for the reliable assessment of LLMs' cognitive and\nlinguistic capabilities.", "AI": {"tldr": "LLMs process linguistic aspect differently from humans, relying on prototypicality and struggling with causal reasoning, indicating limited narrative comprehension.", "motivation": "To determine if LLMs process temporal meaning in narratives like humans or rely on pattern recognition.", "method": "Used an Expert-in-the-Loop probing pipeline to test LLMs' semantic and pragmatic processing of linguistic aspect.", "result": "LLMs over-rely on prototypicality, produce inconsistent judgments, and struggle with causal reasoning.", "conclusion": "LLMs lack human-like narrative understanding; a standardized framework is proposed for assessing their capabilities."}}
{"id": "2507.14449", "pdf": "https://arxiv.org/pdf/2507.14449", "abs": "https://arxiv.org/abs/2507.14449", "authors": ["Zhe Cao", "Jin Zhang", "Ruiheng Zhang"], "title": "IRGPT: Understanding Real-world Infrared Image with Bi-cross-modal Curriculum on Large-scale Benchmark", "categories": ["cs.CV"], "comment": "11 pages, 7 figures. This paper is accepted by ICCV 2025", "summary": "Real-world infrared imagery presents unique challenges for vision-language\nmodels due to the scarcity of aligned text data and domain-specific\ncharacteristics. Although existing methods have advanced the field, their\nreliance on synthetic infrared images generated through style transfer from\nvisible images, which limits their ability to capture the unique\ncharacteristics of the infrared modality. To address this, we propose IRGPT,\nthe first multi-modal large language model for real-world infrared images,\nbuilt upon a large-scale InfraRed-Text Dataset (IR-TD) comprising over 260K\nauthentic image-text pairs. The proposed IR-TD dataset contains real infrared\nimages paired with meticulously handcrafted texts, where the initial drafts\noriginated from two complementary processes: (1) LLM-generated descriptions of\nvisible images, and (2) rule-based descriptions of annotations. Furthermore, we\nintroduce a bi-cross-modal curriculum transfer learning strategy that\nsystematically transfers knowledge from visible to infrared domains by\nconsidering the difficulty scores of both infrared-visible and infrared-text.\nEvaluated on a benchmark of 9 tasks (e.g., recognition, grounding), IRGPT\nachieves state-of-the-art performance even compared with larger-scale models.", "AI": {"tldr": "IRGPT is a multi-modal large language model for real-world infrared images, leveraging a novel dataset (IR-TD) and a bi-cross-modal curriculum transfer learning strategy to outperform existing methods.", "motivation": "Addressing the scarcity of aligned text data and domain-specific challenges in infrared imagery, which existing methods fail to capture due to reliance on synthetic data.", "method": "Proposes IRGPT, built on the IR-TD dataset (260K real image-text pairs), and introduces a bi-cross-modal curriculum transfer learning strategy to transfer knowledge from visible to infrared domains.", "result": "Achieves state-of-the-art performance on 9 benchmark tasks, surpassing larger-scale models.", "conclusion": "IRGPT effectively bridges the gap in infrared vision-language tasks by combining real-world data and innovative transfer learning."}}
{"id": "2507.14314", "pdf": "https://arxiv.org/pdf/2507.14314", "abs": "https://arxiv.org/abs/2507.14314", "authors": ["Marija An\u0111edeli\u0107", "Dominik \u0160ipek", "Laura Majer", "Jan \u0160najder"], "title": "What Makes You CLIC: Detection of Croatian Clickbait Headlines", "categories": ["cs.CL"], "comment": "Accepted at Slavic NLP 2025", "summary": "Online news outlets operate predominantly on an advertising-based revenue\nmodel, compelling journalists to create headlines that are often scandalous,\nintriguing, and provocative -- commonly referred to as clickbait. Automatic\ndetection of clickbait headlines is essential for preserving information\nquality and reader trust in digital media and requires both contextual\nunderstanding and world knowledge. For this task, particularly in\nless-resourced languages, it remains unclear whether fine-tuned methods or\nin-context learning (ICL) yield better results. In this paper, we compile CLIC,\na novel dataset for clickbait detection of Croatian news headlines spanning a\n20-year period and encompassing mainstream and fringe outlets. We fine-tune the\nBERTi\\'c model on this task and compare its performance to LLM-based ICL\nmethods with prompts both in Croatian and English. Finally, we analyze the\nlinguistic properties of clickbait. We find that nearly half of the analyzed\nheadlines contain clickbait, and that finetuned models deliver better results\nthan general LLMs.", "AI": {"tldr": "The paper introduces CLIC, a Croatian dataset for clickbait detection, compares fine-tuned BERTi\u0107 with LLM-based in-context learning, and finds fine-tuned models outperform general LLMs.", "motivation": "To address the challenge of clickbait detection in less-resourced languages like Croatian and evaluate the effectiveness of fine-tuned vs. in-context learning methods.", "method": "Compiled CLIC dataset, fine-tuned BERTi\u0107, and compared it with LLM-based in-context learning using Croatian and English prompts.", "result": "Nearly half of headlines contained clickbait; fine-tuned models performed better than general LLMs.", "conclusion": "Fine-tuned models are more effective for clickbait detection in less-resourced languages like Croatian."}}
{"id": "2507.14452", "pdf": "https://arxiv.org/pdf/2507.14452", "abs": "https://arxiv.org/abs/2507.14452", "authors": ["Weikang Gu", "Mingyue Han", "Li Xue", "Heng Dong", "Changcai Yang", "Riqing Chen", "Lifang Wei"], "title": "GPI-Net: Gestalt-Guided Parallel Interaction Network via Orthogonal Geometric Consistency for Robust Point Cloud Registration", "categories": ["cs.CV"], "comment": "9 pages, 4 figures. Accepted to IJCAI 2025", "summary": "The accurate identification of high-quality correspondences is a prerequisite\ntask in feature-based point cloud registration. However, it is extremely\nchallenging to handle the fusion of local and global features due to feature\nredundancy and complex spatial relationships. Given that Gestalt principles\nprovide key advantages in analyzing local and global relationships, we propose\na novel Gestalt-guided Parallel Interaction Network via orthogonal geometric\nconsistency (GPI-Net) in this paper. It utilizes Gestalt principles to\nfacilitate complementary communication between local and global information.\nSpecifically, we introduce an orthogonal integration strategy to optimally\nreduce redundant information and generate a more compact global structure for\nhigh-quality correspondences. To capture geometric features in correspondences,\nwe leverage a Gestalt Feature Attention (GFA) block through a hybrid\nutilization of self-attention and cross-attention mechanisms. Furthermore, to\nfacilitate the integration of local detail information into the global\nstructure, we design an innovative Dual-path Multi-Granularity parallel\ninteraction aggregation (DMG) block to promote information exchange across\ndifferent granularities. Extensive experiments on various challenging tasks\ndemonstrate the superior performance of our proposed GPI-Net in comparison to\nexisting methods. The code will be released at https://github.com/gwk/GPI-Net.", "AI": {"tldr": "Proposes GPI-Net, a Gestalt-guided network for point cloud registration, combining local and global features via orthogonal integration and attention mechanisms for high-quality correspondences.", "motivation": "Addressing the challenge of fusing local and global features in point cloud registration due to redundancy and complex spatial relationships.", "method": "Uses Gestalt principles, orthogonal integration, Gestalt Feature Attention (GFA), and Dual-path Multi-Granularity (DMG) blocks for feature fusion and interaction.", "result": "Outperforms existing methods in experiments on challenging tasks.", "conclusion": "GPI-Net effectively integrates local and global features for improved point cloud registration."}}
{"id": "2507.14355", "pdf": "https://arxiv.org/pdf/2507.14355", "abs": "https://arxiv.org/abs/2507.14355", "authors": ["Jianfeng Zhu", "Ruoming Jin", "Karin G. Coifman"], "title": "Can LLMs Infer Personality from Real World Conversations?", "categories": ["cs.CL"], "comment": "21 pages, 12 figures", "summary": "Large Language Models (LLMs) such as OpenAI's GPT-4 and Meta's LLaMA offer a\npromising approach for scalable personality assessment from open-ended\nlanguage. However, inferring personality traits remains challenging, and\nearlier work often relied on synthetic data or social media text lacking\npsychometric validity. We introduce a real-world benchmark of 555\nsemi-structured interviews with BFI-10 self-report scores for evaluating\nLLM-based personality inference. Three state-of-the-art LLMs (GPT-4.1 Mini,\nMeta-LLaMA, and DeepSeek) were tested using zero-shot prompting for BFI-10 item\nprediction and both zero-shot and chain-of-thought prompting for Big Five trait\ninference. All models showed high test-retest reliability, but construct\nvalidity was limited: correlations with ground-truth scores were weak (max\nPearson's $r = 0.27$), interrater agreement was low (Cohen's $\\kappa < 0.10$),\nand predictions were biased toward moderate or high trait levels.\nChain-of-thought prompting and longer input context modestly improved\ndistributional alignment, but not trait-level accuracy. These results\nunderscore limitations in current LLM-based personality inference and highlight\nthe need for evidence-based development for psychological applications.", "AI": {"tldr": "LLMs like GPT-4 and LLaMA show promise for personality assessment but struggle with validity. A benchmark of 555 interviews with BFI-10 scores tested three LLMs, revealing high reliability but weak validity and biases. Chain-of-thought prompting helped slightly, but accuracy remains limited.", "motivation": "To evaluate the effectiveness of LLMs in inferring personality traits from real-world data, addressing gaps in psychometric validity and synthetic data reliance.", "method": "Tested three LLMs (GPT-4.1 Mini, Meta-LLaMA, DeepSeek) using zero-shot and chain-of-thought prompting on 555 semi-structured interviews with BFI-10 scores.", "result": "High test-retest reliability but weak construct validity (max Pearson's r = 0.27), low interrater agreement, and trait-level biases. Chain-of-thought improved distributional alignment but not accuracy.", "conclusion": "Current LLMs have limitations for personality inference, emphasizing the need for evidence-based improvements in psychological applications."}}
{"id": "2507.14454", "pdf": "https://arxiv.org/pdf/2507.14454", "abs": "https://arxiv.org/abs/2507.14454", "authors": ["Han Gong", "Qiyue Li", "Jie Li", "Zhi Liu"], "title": "Adaptive 3D Gaussian Splatting Video Streaming: Visual Saliency-Aware Tiling and Meta-Learning-Based Bitrate Adaptation", "categories": ["cs.CV", "cs.MM", "eess.IV"], "comment": null, "summary": "3D Gaussian splatting video (3DGS) streaming has recently emerged as a\nresearch hotspot in both academia and industry, owing to its impressive ability\nto deliver immersive 3D video experiences. However, research in this area is\nstill in its early stages, and several fundamental challenges, such as tiling,\nquality assessment, and bitrate adaptation, require further investigation. In\nthis paper, we tackle these challenges by proposing a comprehensive set of\nsolutions. Specifically, we propose an adaptive 3DGS tiling technique guided by\nsaliency analysis, which integrates both spatial and temporal features. Each\ntile is encoded into versions possessing dedicated deformation fields and\nmultiple quality levels for adaptive selection. We also introduce a novel\nquality assessment framework for 3DGS video that jointly evaluates\nspatial-domain degradation in 3DGS representations during streaming and the\nquality of the resulting 2D rendered images. Additionally, we develop a\nmeta-learning-based adaptive bitrate algorithm specifically tailored for 3DGS\nvideo streaming, achieving optimal performance across varying network\nconditions. Extensive experiments demonstrate that our proposed approaches\nsignificantly outperform state-of-the-art methods.", "AI": {"tldr": "The paper addresses challenges in 3D Gaussian splatting video streaming, proposing adaptive tiling, quality assessment, and bitrate adaptation solutions.", "motivation": "To enhance immersive 3D video experiences by solving fundamental challenges like tiling, quality assessment, and bitrate adaptation in 3DGS streaming.", "method": "Proposes adaptive 3DGS tiling with saliency analysis, a quality assessment framework, and a meta-learning-based bitrate algorithm.", "result": "The solutions outperform state-of-the-art methods in experiments.", "conclusion": "The proposed approaches effectively tackle key challenges in 3DGS video streaming, improving performance and quality."}}
{"id": "2507.14372", "pdf": "https://arxiv.org/pdf/2507.14372", "abs": "https://arxiv.org/abs/2507.14372", "authors": ["Albert Chen", "Manas Bundele", "Gaurav Ahlawat", "Patrick Stetz", "Zhitao Wang", "Qiang Fei", "Donghoon Jung", "Audrey Chu", "Bharadwaj Jayaraman", "Ayushi Panth", "Yatin Arora", "Sourav Jain", "Renjith Varma", "Alexey Ilin", "Iuliia Melnychuk", "Chelsea Chueh", "Joyan Sil", "Xiaofeng Wang"], "title": "Text-to-SQL for Enterprise Data Analytics", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.HC"], "comment": "11 pages, 8 figures, Workshop on Agentic AI for Enterprise at KDD '25", "summary": "The introduction of large language models has brought rapid progress on\nText-to-SQL benchmarks, but it is not yet easy to build a working enterprise\nsolution. In this paper, we present insights from building an internal chatbot\nthat enables LinkedIn's product managers, engineers, and operations teams to\nself-serve data insights from a large, dynamic data lake. Our approach features\nthree components. First, we construct a knowledge graph that captures\nup-to-date semantics by indexing database metadata, historical query logs,\nwikis, and code. We apply clustering to identify relevant tables for each team\nor product area. Second, we build a Text-to-SQL agent that retrieves and ranks\ncontext from the knowledge graph, writes a query, and automatically corrects\nhallucinations and syntax errors. Third, we build an interactive chatbot that\nsupports various user intents, from data discovery to query writing to\ndebugging, and displays responses in rich UI elements to encourage follow-up\nchats. Our chatbot has over 300 weekly users. Expert review shows that 53% of\nits responses are correct or close to correct on an internal benchmark set.\nThrough ablation studies, we identify the most important knowledge graph and\nmodeling components, offering a practical path for developing enterprise\nText-to-SQL solutions.", "AI": {"tldr": "The paper presents a practical approach to building an enterprise Text-to-SQL chatbot for LinkedIn, combining a knowledge graph, a Text-to-SQL agent, and an interactive chatbot to enable self-serve data insights.", "motivation": "Large language models have advanced Text-to-SQL benchmarks, but enterprise solutions remain challenging. The paper aims to address this gap by developing a functional chatbot for LinkedIn's teams.", "method": "The approach involves: 1) constructing a knowledge graph from metadata, logs, wikis, and code; 2) building a Text-to-SQL agent for context retrieval, query writing, and error correction; 3) creating an interactive chatbot for diverse user intents.", "result": "The chatbot has 300+ weekly users, with 53% of responses rated correct or close to correct in expert reviews. Ablation studies highlight key components for enterprise solutions.", "conclusion": "The study offers actionable insights for developing enterprise Text-to-SQL solutions, emphasizing the importance of knowledge graphs and modeling components."}}
{"id": "2507.14456", "pdf": "https://arxiv.org/pdf/2507.14456", "abs": "https://arxiv.org/abs/2507.14456", "authors": ["Chi Wan", "Yixin Cui", "Jiatong Du", "Shuo Yang", "Yulong Bai", "Yanjun Huang"], "title": "GEMINUS: Dual-aware Global and Scene-Adaptive Mixture-of-Experts for End-to-End Autonomous Driving", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "End-to-end autonomous driving requires adaptive and robust handling of\ncomplex and diverse traffic environments. However, prevalent single-mode\nplanning methods attempt to learn an overall policy while struggling to acquire\ndiversified driving skills to handle diverse scenarios. Therefore, this paper\nproposes GEMINUS, a Mixture-of-Experts end-to-end autonomous driving framework\nfeaturing a Global Expert, a Scene-Adaptive Experts Group, and equipped with a\nDual-aware Router. Specifically, the Global Expert is trained on the overall\ndataset, possessing robust performance. The Scene-Adaptive Experts are trained\non corresponding scene subsets, achieving adaptive performance. The Dual-aware\nRouter simultaneously considers scenario-level features and routing uncertainty\nto dynamically activate expert modules. Through the effective coupling of the\nGlobal Expert and the Scene-Adaptive Experts Group via the Dual-aware Router,\nGEMINUS achieves adaptive and robust performance in diverse scenarios. GEMINUS\noutperforms existing methods in the Bench2Drive closed-loop benchmark and\nachieves state-of-the-art performance in Driving Score and Success Rate, even\nwith only monocular vision input. Furthermore, ablation studies demonstrate\nsignificant improvements over the original single-expert baseline: 7.67% in\nDriving Score, 22.06% in Success Rate, and 19.41% in MultiAbility-Mean. The\ncode will be available at https://github.com/newbrains1/GEMINUS.", "AI": {"tldr": "GEMINUS is a Mixture-of-Experts framework for autonomous driving, combining a Global Expert and Scene-Adaptive Experts with a Dual-aware Router, achieving adaptive and robust performance in diverse scenarios.", "motivation": "Single-mode planning struggles with diverse driving scenarios, necessitating a framework that adapts to varied conditions.", "method": "GEMINUS uses a Global Expert for robustness and Scene-Adaptive Experts for adaptability, dynamically activated by a Dual-aware Router.", "result": "Outperforms existing methods in Bench2Drive, achieving top scores in Driving Score and Success Rate with monocular vision.", "conclusion": "GEMINUS demonstrates significant improvements over single-expert baselines, proving its effectiveness in adaptive autonomous driving."}}
{"id": "2507.14374", "pdf": "https://arxiv.org/pdf/2507.14374", "abs": "https://arxiv.org/abs/2507.14374", "authors": ["Sinchani Chakraborty", "Sudeshna Sarkar", "Pawan Goyal"], "title": "Error-Aware Curriculum Learning for Biomedical Relation Classification", "categories": ["cs.CL"], "comment": "16 pages, 2 figures", "summary": "Relation Classification (RC) in biomedical texts is essential for\nconstructing knowledge graphs and enabling applications such as drug\nrepurposing and clinical decision-making. We propose an error-aware\nteacher--student framework that improves RC through structured guidance from a\nlarge language model (GPT-4o). Prediction failures from a baseline student\nmodel are analyzed by the teacher to classify error types, assign difficulty\nscores, and generate targeted remediations, including sentence rewrites and\nsuggestions for KG-based enrichment. These enriched annotations are used to\ntrain a first student model via instruction tuning. This model then annotates a\nbroader dataset with difficulty scores and remediation-enhanced inputs. A\nsecond student is subsequently trained via curriculum learning on this dataset,\nordered by difficulty, to promote robust and progressive learning. We also\nconstruct a heterogeneous biomedical knowledge graph from PubMed abstracts to\nsupport context-aware RC. Our approach achieves new state-of-the-art\nperformance on 4 of 5 PPI datasets and the DDI dataset, while remaining\ncompetitive on ChemProt.", "AI": {"tldr": "An error-aware teacher-student framework using GPT-4o improves biomedical relation classification by analyzing errors, generating remediations, and training models progressively.", "motivation": "Enhancing biomedical relation classification to support knowledge graphs and applications like drug repurposing and clinical decision-making.", "method": "Uses a teacher-student framework with GPT-4o to analyze errors, generate remediations, and train models via instruction tuning and curriculum learning. A knowledge graph from PubMed abstracts supports context-aware classification.", "result": "Achieves state-of-the-art performance on 4 of 5 PPI datasets and the DDI dataset, remaining competitive on ChemProt.", "conclusion": "The framework effectively improves relation classification in biomedical texts through structured guidance and progressive learning."}}
{"id": "2507.14459", "pdf": "https://arxiv.org/pdf/2507.14459", "abs": "https://arxiv.org/abs/2507.14459", "authors": ["Huayuan Ye", "Juntong Chen", "Shenzhuo Zhang", "Yipeng Zhang", "Changbo Wang", "Chenhui Li"], "title": "VisGuard: Securing Visualization Dissemination through Tamper-Resistant Data Retrieval", "categories": ["cs.CV"], "comment": "9 pages, IEEE VIS 2025", "summary": "The dissemination of visualizations is primarily in the form of raster\nimages, which often results in the loss of critical information such as source\ncode, interactive features, and metadata. While previous methods have proposed\nembedding metadata into images to facilitate Visualization Image Data Retrieval\n(VIDR), most existing methods lack practicability since they are fragile to\ncommon image tampering during online distribution such as cropping and editing.\nTo address this issue, we propose VisGuard, a tamper-resistant VIDR framework\nthat reliably embeds metadata link into visualization images. The embedded data\nlink remains recoverable even after substantial tampering upon images. We\npropose several techniques to enhance robustness, including repetitive data\ntiling, invertible information broadcasting, and an anchor-based scheme for\ncrop localization. VisGuard enables various applications, including interactive\nchart reconstruction, tampering detection, and copyright protection. We conduct\ncomprehensive experiments on VisGuard's superior performance in data retrieval\naccuracy, embedding capacity, and security against tampering and steganalysis,\ndemonstrating VisGuard's competence in facilitating and safeguarding\nvisualization dissemination and information conveyance.", "AI": {"tldr": "VisGuard is a tamper-resistant framework for embedding metadata links in visualization images, ensuring recoverability even after tampering. It enhances robustness with techniques like repetitive data tiling and crop localization, supporting applications like interactive chart reconstruction and copyright protection.", "motivation": "Current methods for embedding metadata in visualization images are fragile to common tampering, leading to loss of critical information. VisGuard aims to address this by providing a robust solution.", "method": "VisGuard uses repetitive data tiling, invertible information broadcasting, and an anchor-based scheme for crop localization to embed and recover metadata links reliably.", "result": "VisGuard outperforms in data retrieval accuracy, embedding capacity, and security against tampering and steganalysis.", "conclusion": "VisGuard effectively safeguards visualization dissemination by ensuring metadata recoverability and robustness against tampering."}}
{"id": "2507.14430", "pdf": "https://arxiv.org/pdf/2507.14430", "abs": "https://arxiv.org/abs/2507.14430", "authors": ["Xiaolin Yan", "Yangxing Liu", "Jiazhang Zheng", "Chi Liu", "Mingyu Du", "Caisheng Chen", "Haoyang Liu", "Ming Ding", "Yuan Li", "Qiuping Liao", "Linfeng Li", "Zhili Mei", "Siyu Wan", "Li Li", "Ruyi Zhong", "Jiangling Yu", "Xule Liu", "Huihui Hu", "Jiameng Yue", "Ruohui Cheng", "Qi Yang", "Liangqing Wu", "Ke Zhu", "Chi Zhang", "Chufei Jing", "Yifan Zhou", "Yan Liang", "Dongdong Li", "Zhaohui Wang", "Bin Zhao", "Mingzhou Wu", "Mingzhong Zhou", "Peng Du", "Zuomin Liao", "Chao Dai", "Pengfei Liang", "Xiaoguang Zhu", "Yu Zhang", "Yu Gu", "Kun Pan", "Yuan Wu", "Yanqing Guan", "Shaojing Wu", "Zikang Feng", "Xianze Ma", "Peishan Cheng", "Wenjuan Jiang", "Jing Ba", "Huihao Yu", "Zeping Hu", "Yuan Xu", "Zhiwei Liu", "He Wang", "Zhenguo Lin", "Ming Liu", "Yanhong Meng"], "title": "X-Intelligence 3.0: Training and Evaluating Reasoning LLM for Semiconductor Display", "categories": ["cs.CL"], "comment": "Technical Report", "summary": "Large language models (LLMs) have recently achieved significant advances in\nreasoning and demonstrated their advantages in solving challenging problems.\nYet, their effectiveness in the semiconductor display industry remains limited\ndue to a lack of domain-specific training and expertise. To bridge this gap, we\npresent X-Intelligence 3.0, the first high-performance reasoning model\nspecifically developed for the semiconductor display industry. This model is\ndesigned to deliver expert-level understanding and reasoning for the industry's\ncomplex challenges. Leveraging a carefully curated industry knowledge base, the\nmodel undergoes supervised fine-tuning and reinforcement learning to enhance\nits reasoning and comprehension capabilities. To further accelerate\ndevelopment, we implemented an automated evaluation framework that simulates\nexpert-level assessments. We also integrated a domain-specific\nretrieval-augmented generation (RAG) mechanism, resulting in notable\nperformance gains on benchmark datasets. Despite its relatively compact size of\n32 billion parameters, X-Intelligence 3.0 outperforms SOTA DeepSeek-R1-671B\nacross multiple evaluations. This demonstrates its exceptional efficiency and\nestablishes it as a powerful solution to the longstanding reasoning challenges\nfaced by the semiconductor display industry.", "AI": {"tldr": "X-Intelligence 3.0 is a specialized 32B-parameter LLM for the semiconductor display industry, outperforming larger models like DeepSeek-R1-671B through domain-specific training and RAG.", "motivation": "Address the lack of domain-specific reasoning in LLMs for the semiconductor display industry.", "method": "Supervised fine-tuning, reinforcement learning, and a domain-specific RAG mechanism, supported by an automated evaluation framework.", "result": "Outperforms DeepSeek-R1-671B on benchmarks despite smaller size.", "conclusion": "X-Intelligence 3.0 is an efficient, high-performance solution for industry-specific reasoning challenges."}}
{"id": "2507.14477", "pdf": "https://arxiv.org/pdf/2507.14477", "abs": "https://arxiv.org/abs/2507.14477", "authors": ["Zhenyu Li", "Tianyi Shang", "Pengjie Xu", "Ruirui Zhang", "Fanchen Kong"], "title": "OptiCorNet: Optimizing Sequence-Based Context Correlation for Visual Place Recognition", "categories": ["cs.CV"], "comment": "5 figures", "summary": "Visual Place Recognition (VPR) in dynamic and perceptually aliased\nenvironments remains a fundamental challenge for long-term localization.\nExisting deep learning-based solutions predominantly focus on single-frame\nembeddings, neglecting the temporal coherence present in image sequences. This\npaper presents OptiCorNet, a novel sequence modeling framework that unifies\nspatial feature extraction and temporal differencing into a differentiable,\nend-to-end trainable module. Central to our approach is a lightweight 1D\nconvolutional encoder combined with a learnable differential temporal operator,\ntermed Differentiable Sequence Delta (DSD), which jointly captures short-term\nspatial context and long-range temporal transitions. The DSD module models\ndirectional differences across sequences via a fixed-weight differencing\nkernel, followed by an LSTM-based refinement and optional residual projection,\nyielding compact, discriminative descriptors robust to viewpoint and appearance\nshifts. To further enhance inter-class separability, we incorporate a\nquadruplet loss that optimizes both positive alignment and multi-negative\ndivergence within each batch. Unlike prior VPR methods that treat temporal\naggregation as post-processing, OptiCorNet learns sequence-level embeddings\ndirectly, enabling more effective end-to-end place recognition. Comprehensive\nevaluations on multiple public benchmarks demonstrate that our approach\noutperforms state-of-the-art baselines under challenging seasonal and viewpoint\nvariations.", "AI": {"tldr": "OptiCorNet introduces a sequence modeling framework for VPR, combining spatial feature extraction and temporal differencing into an end-to-end trainable module, outperforming existing methods.", "motivation": "Addressing the challenge of VPR in dynamic and perceptually aliased environments by leveraging temporal coherence in image sequences, which existing single-frame embedding methods neglect.", "method": "Uses a lightweight 1D convolutional encoder and a learnable differential temporal operator (DSD), combined with LSTM-based refinement and quadruplet loss for enhanced separability.", "result": "Outperforms state-of-the-art baselines on public benchmarks under challenging seasonal and viewpoint variations.", "conclusion": "OptiCorNet effectively learns sequence-level embeddings end-to-end, improving robustness in dynamic environments."}}
{"id": "2507.14578", "pdf": "https://arxiv.org/pdf/2507.14578", "abs": "https://arxiv.org/abs/2507.14578", "authors": ["Sachin Yadav", "Dominik Schlechtweg"], "title": "XL-DURel: Finetuning Sentence Transformers for Ordinal Word-in-Context Classification", "categories": ["cs.CL"], "comment": "8 pages", "summary": "We propose XL-DURel, a finetuned, multilingual Sentence Transformer model\noptimized for ordinal Word-in-Context classification. We test several loss\nfunctions for regression and ranking tasks managing to outperform previous\nmodels on ordinal and binary data with a ranking objective based on angular\ndistance in complex space. We further show that binary WiC can be treated as a\nspecial case of ordinal WiC and that optimizing models for the general ordinal\ntask improves performance on the more specific binary task. This paves the way\nfor a unified treatment of WiC modeling across different task formulations.", "AI": {"tldr": "XL-DURel is a multilingual Sentence Transformer model optimized for ordinal Word-in-Context classification, outperforming previous models with a ranking objective based on angular distance.", "motivation": "To improve performance on ordinal and binary Word-in-Context tasks and unify their treatment.", "method": "Finetuning a multilingual Sentence Transformer model with various loss functions for regression and ranking tasks.", "result": "Outperforms previous models on ordinal and binary data, showing binary WiC as a special case of ordinal WiC.", "conclusion": "Optimizing for ordinal tasks improves binary task performance, enabling unified WiC modeling."}}
{"id": "2507.14481", "pdf": "https://arxiv.org/pdf/2507.14481", "abs": "https://arxiv.org/abs/2507.14481", "authors": ["Yujia Tong", "Jingling Yuan", "Tian Zhang", "Jianquan Liu", "Chuang Hu"], "title": "DFQ-ViT: Data-Free Quantization for Vision Transformers without Fine-tuning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Data-Free Quantization (DFQ) enables the quantization of Vision Transformers\n(ViTs) without requiring access to data, allowing for the deployment of ViTs on\ndevices with limited resources. In DFQ, the quantization model must be\ncalibrated using synthetic samples, making the quality of these synthetic\nsamples crucial. Existing methods fail to fully capture and balance the global\nand local features within the samples, resulting in limited synthetic data\nquality. Moreover, we have found that during inference, there is a significant\ndifference in the distributions of intermediate layer activations between the\nquantized and full-precision models. These issues lead to a severe performance\ndegradation of the quantized model. To address these problems, we propose a\npipeline for Data-Free Quantization for Vision Transformers (DFQ-ViT).\nSpecifically, we synthesize samples in order of increasing difficulty,\neffectively enhancing the quality of synthetic data. During the calibration and\ninference stage, we introduce the activation correction matrix for the\nquantized model to align the intermediate layer activations with those of the\nfull-precision model. Extensive experiments demonstrate that DFQ-ViT achieves\nremarkable superiority over existing DFQ methods and its performance is on par\nwith models quantized through real data. For example, the performance of DeiT-T\nwith 3-bit weights quantization is 4.29% higher than the state-of-the-art. Our\nmethod eliminates the need for fine-tuning, which not only reduces\ncomputational overhead but also lowers the deployment barriers for edge\ndevices. This characteristic aligns with the principles of Green Learning by\nimproving energy efficiency and facilitating real-world applications in\nresource-constrained environments.", "AI": {"tldr": "DFQ-ViT improves data-free quantization for Vision Transformers by enhancing synthetic data quality and aligning activation distributions, outperforming existing methods without fine-tuning.", "motivation": "Existing DFQ methods for ViTs struggle with synthetic data quality and activation distribution mismatches, leading to performance degradation.", "method": "Proposes DFQ-ViT: synthesizes samples by difficulty and uses activation correction to align quantized and full-precision model activations.", "result": "DFQ-ViT outperforms state-of-the-art DFQ methods, e.g., 4.29% higher accuracy for 3-bit DeiT-T, without fine-tuning.", "conclusion": "DFQ-ViT enables efficient, high-performance quantization for edge devices, aligning with Green Learning principles."}}
{"id": "2507.14579", "pdf": "https://arxiv.org/pdf/2507.14579", "abs": "https://arxiv.org/abs/2507.14579", "authors": ["Kester Wong", "Sahan Bulathwela", "Mutlu Cukurova"], "title": "Exploring Human-AI Complementarity in CPS Diagnosis Using Unimodal and Multimodal BERT Models", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to appear in the workshop proceedings for the HEXED'25\n  workshop in the 26th International Conference on Artificial Intelligence in\n  Education 2025 (AIED 2025), 22 July 2025, Palermo, Italy. 5 pages", "summary": "Detecting collaborative problem solving (CPS) indicators from dialogue using\nmachine learning techniques is a significant challenge for the field of AI in\nEducation. Recent studies have explored the use of Bidirectional Encoder\nRepresentations from Transformers (BERT) models on transcription data to\nreliably detect meaningful CPS indicators. A notable advancement involved the\nmultimodal BERT variant, AudiBERT, which integrates speech and\nacoustic-prosodic audio features to enhance CPS diagnosis. Although initial\nresults demonstrated multimodal improvements, the statistical significance of\nthese enhancements remained unclear, and there was insufficient guidance on\nleveraging human-AI complementarity for CPS diagnosis tasks. This workshop\npaper extends the previous research by highlighting that the AudiBERT model not\nonly improved the classification of classes that were sparse in the dataset,\nbut it also had statistically significant class-wise improvements over the BERT\nmodel for classifications in the social-cognitive dimension. However, similar\nsignificant class-wise improvements over the BERT model were not observed for\nclassifications in the affective dimension. A correlation analysis highlighted\nthat larger training data was significantly associated with higher recall\nperformance for both the AudiBERT and BERT models. Additionally, the precision\nof the BERT model was significantly associated with high inter-rater agreement\namong human coders. When employing the BERT model to diagnose indicators within\nthese subskills that were well-detected by the AudiBERT model, the performance\nacross all indicators was inconsistent. We conclude the paper by outlining a\nstructured approach towards achieving human-AI complementarity for CPS\ndiagnosis, highlighting the crucial inclusion of model explainability to\nsupport human agency and engagement in the reflective coding process.", "AI": {"tldr": "The paper explores the use of multimodal BERT (AudiBERT) for detecting CPS indicators, showing significant improvements in social-cognitive dimensions but not affective ones, and emphasizes human-AI complementarity with explainability.", "motivation": "To address the challenge of reliably detecting CPS indicators from dialogue using machine learning, particularly leveraging multimodal data (speech and acoustic-prosodic features) for enhanced diagnosis.", "method": "The study extends prior work by evaluating AudiBERT (multimodal BERT) against BERT on transcription data, analyzing class-wise improvements, correlation with training data size, and human-AI complementarity.", "result": "AudiBERT showed statistically significant improvements in social-cognitive classifications but not affective ones. Training data size correlated with recall, and human coder agreement influenced BERT's precision.", "conclusion": "The paper advocates for a structured approach to human-AI complementarity in CPS diagnosis, stressing model explainability to support human engagement in coding."}}
{"id": "2507.14485", "pdf": "https://arxiv.org/pdf/2507.14485", "abs": "https://arxiv.org/abs/2507.14485", "authors": ["Hongye Hou", "Liu Zhan", "Yang Yang"], "title": "Benefit from Reference: Retrieval-Augmented Cross-modal Point Cloud Completion", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Completing the whole 3D structure based on an incomplete point cloud is a\nchallenging task, particularly when the residual point cloud lacks typical\nstructural characteristics. Recent methods based on cross-modal learning\nattempt to introduce instance images to aid the structure feature learning.\nHowever, they still focus on each particular input class, limiting their\ngeneration abilities. In this work, we propose a novel retrieval-augmented\npoint cloud completion framework. The core idea is to incorporate cross-modal\nretrieval into completion task to learn structural prior information from\nsimilar reference samples. Specifically, we design a Structural Shared Feature\nEncoder (SSFE) to jointly extract cross-modal features and reconstruct\nreference features as priors. Benefiting from a dual-channel control gate in\nthe encoder, relevant structural features in the reference sample are enhanced\nand irrelevant information interference is suppressed. In addition, we propose\na Progressive Retrieval-Augmented Generator (PRAG) that employs a hierarchical\nfeature fusion mechanism to integrate reference prior information with input\nfeatures from global to local. Through extensive evaluations on multiple\ndatasets and real-world scenes, our method shows its effectiveness in\ngenerating fine-grained point clouds, as well as its generalization capability\nin handling sparse data and unseen categories.", "AI": {"tldr": "A novel retrieval-augmented framework enhances 3D point cloud completion by leveraging cross-modal retrieval and hierarchical feature fusion, improving fine-grained generation and generalization.", "motivation": "Existing methods for 3D point cloud completion are limited by input class focus and lack structural priors. Cross-modal learning is explored but needs broader generalization.", "method": "Proposes a retrieval-augmented framework with a Structural Shared Feature Encoder (SSFE) for cross-modal feature extraction and a Progressive Retrieval-Augmented Generator (PRAG) for hierarchical feature fusion.", "result": "Demonstrates effectiveness in fine-grained point cloud generation and generalization to sparse data and unseen categories.", "conclusion": "The framework advances point cloud completion by integrating cross-modal retrieval and hierarchical fusion, offering improved performance and versatility."}}
{"id": "2507.14584", "pdf": "https://arxiv.org/pdf/2507.14584", "abs": "https://arxiv.org/abs/2507.14584", "authors": ["Kester Wong", "Sahan Bulathwela", "Mutlu Cukurova"], "title": "Explainable Collaborative Problem Solving Diagnosis with BERT using SHAP and its Implications for Teacher Adoption", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to appear in the workshop proceedings for the HEXED'25\n  workshop in the 26th International Conference on Artificial Intelligence in\n  Education 2025 (AIED 2025), 22 July 2025, Palermo, Italy. 6 pages, 2 figures", "summary": "The use of Bidirectional Encoder Representations from Transformers (BERT)\nmodel and its variants for classifying collaborative problem solving (CPS) has\nbeen extensively explored within the AI in Education community. However,\nlimited attention has been given to understanding how individual tokenised\nwords in the dataset contribute to the model's classification decisions.\nEnhancing the explainability of BERT-based CPS diagnostics is essential to\nbetter inform end users such as teachers, thereby fostering greater trust and\nfacilitating wider adoption in education. This study undertook a preliminary\nstep towards model transparency and explainability by using SHapley Additive\nexPlanations (SHAP) to examine how different tokenised words in transcription\ndata contributed to a BERT model's classification of CPS processes. The\nfindings suggested that well-performing classifications did not necessarily\nequate to a reasonable explanation for the classification decisions. Particular\ntokenised words were used frequently to affect classifications. The analysis\nalso identified a spurious word, which contributed positively to the\nclassification but was not semantically meaningful to the class. While such\nmodel transparency is unlikely to be useful to an end user to improve their\npractice, it can help them not to overrely on LLM diagnostics and ignore their\nhuman expertise. We conclude the workshop paper by noting that the extent to\nwhich the model appropriately uses the tokens for its classification is\nassociated with the number of classes involved. It calls for an investigation\ninto the exploration of ensemble model architectures and the involvement of\nhuman-AI complementarity for CPS diagnosis, since considerable human reasoning\nis still required for fine-grained discrimination of CPS subskills.", "AI": {"tldr": "The paper explores BERT model explainability in CPS classification using SHAP, finding that high performance doesn't guarantee reasonable explanations and identifying spurious word contributions.", "motivation": "To enhance transparency and trust in BERT-based CPS diagnostics for educators by understanding token contributions.", "method": "Used SHAP to analyze tokenized word contributions in BERT's CPS classification.", "result": "Found frequent but not semantically meaningful token contributions, suggesting performance \u2260 explainability.", "conclusion": "Calls for ensemble models and human-AI collaboration in CPS diagnosis due to the need for fine-grained human reasoning."}}
{"id": "2507.14497", "pdf": "https://arxiv.org/pdf/2507.14497", "abs": "https://arxiv.org/abs/2507.14497", "authors": ["Weimin Lyu", "Qingqiao Hu", "Kehan Qi", "Zhan Shi", "Wentao Huang", "Saumya Gupta", "Chao Chen"], "title": "Efficient Whole Slide Pathology VQA via Token Compression", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Whole-slide images (WSIs) in pathology can reach up to 10,000 x 10,000\npixels, posing significant challenges for multimodal large language model\n(MLLM) due to long context length and high computational demands. Previous\nmethods typically focus on patch-level analysis or slide-level classification\nusing CLIP-based models with multi-instance learning, but they lack the\ngenerative capabilities needed for visual question answering (VQA). More recent\nMLLM-based approaches address VQA by feeding thousands of patch tokens directly\ninto the language model, which leads to excessive resource consumption. To\naddress these limitations, we propose Token Compression Pathology LLaVA\n(TCP-LLaVA), the first MLLM architecture to perform WSI VQA via token\ncompression. TCP-LLaVA introduces a set of trainable compression tokens that\naggregate visual and textual information through a modality compression module,\ninspired by the [CLS] token mechanism in BERT. Only the compressed tokens are\nforwarded to the LLM for answer generation, significantly reducing input length\nand computational cost. Experiments on ten TCGA tumor subtypes show that\nTCP-LLaVA outperforms existing MLLM baselines in VQA accuracy while reducing\ntraining resource consumption by a substantial margin.", "AI": {"tldr": "TCP-LLaVA introduces token compression for WSI VQA, reducing computational costs while improving accuracy.", "motivation": "Addressing the challenges of high-resolution WSIs in MLLMs, which lack generative capabilities and consume excessive resources.", "method": "Uses trainable compression tokens to aggregate visual/textual info, inspired by BERT's [CLS] token, reducing input length.", "result": "Outperforms baselines in VQA accuracy and reduces training resource consumption significantly.", "conclusion": "TCP-LLaVA is an efficient MLLM architecture for WSI VQA, balancing performance and computational demands."}}
{"id": "2507.14590", "pdf": "https://arxiv.org/pdf/2507.14590", "abs": "https://arxiv.org/abs/2507.14590", "authors": ["\u0141ukasz Radli\u0144ski", "Mateusz Gu\u015bciora", "Jan Koco\u0144"], "title": "Backtranslation and paraphrasing in the LLM era? Comparing data augmentation methods for emotion classification", "categories": ["cs.CL", "cs.AI"], "comment": "International Conference on Computational Science 2025", "summary": "Numerous domain-specific machine learning tasks struggle with data scarcity\nand class imbalance. This paper systematically explores data augmentation\nmethods for NLP, particularly through large language models like GPT. The\npurpose of this paper is to examine and evaluate whether traditional methods\nsuch as paraphrasing and backtranslation can leverage a new generation of\nmodels to achieve comparable performance to purely generative methods. Methods\naimed at solving the problem of data scarcity and utilizing ChatGPT were\nchosen, as well as an exemplary dataset. We conducted a series of experiments\ncomparing four different approaches to data augmentation in multiple\nexperimental setups. We then evaluated the results both in terms of the quality\nof generated data and its impact on classification performance. The key\nfindings indicate that backtranslation and paraphrasing can yield comparable or\neven better results than zero and a few-shot generation of examples.", "AI": {"tldr": "The paper explores data augmentation in NLP using large language models like GPT, comparing traditional methods (paraphrasing, backtranslation) with generative methods. Findings show traditional methods can match or outperform generative approaches.", "motivation": "Addressing data scarcity and class imbalance in domain-specific ML tasks by evaluating the effectiveness of traditional data augmentation methods enhanced by modern LLMs.", "method": "Systematic comparison of four data augmentation approaches (including paraphrasing and backtranslation) using ChatGPT and an exemplary dataset, evaluated on data quality and classification performance.", "result": "Backtranslation and paraphrasing achieved comparable or better results than zero/few-shot generative methods.", "conclusion": "Traditional data augmentation methods, when leveraged by modern LLMs, can be as effective as purely generative approaches for NLP tasks."}}
{"id": "2507.14500", "pdf": "https://arxiv.org/pdf/2507.14500", "abs": "https://arxiv.org/abs/2507.14500", "authors": ["Zhiyuan Hua", "Dehao Yuan", "Cornelia Ferm\u00fcller"], "title": "Motion Segmentation and Egomotion Estimation from Event-Based Normal Flow", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "This paper introduces a robust framework for motion segmentation and\negomotion estimation using event-based normal flow, tailored specifically for\nneuromorphic vision sensors. In contrast to traditional methods that rely\nheavily on optical flow or explicit depth estimation, our approach exploits the\nsparse, high-temporal-resolution event data and incorporates geometric\nconstraints between normal flow, scene structure, and inertial measurements.\nThe proposed optimization-based pipeline iteratively performs event\nover-segmentation, isolates independently moving objects via residual analysis,\nand refines segmentations using hierarchical clustering informed by motion\nsimilarity and temporal consistency. Experimental results on the EVIMO2v2\ndataset validate that our method achieves accurate segmentation and\ntranslational motion estimation without requiring full optical flow\ncomputation. This approach demonstrates significant advantages at object\nboundaries and offers considerable potential for scalable, real-time robotic\nand navigation applications.", "AI": {"tldr": "A framework for motion segmentation and egomotion estimation using event-based normal flow for neuromorphic vision sensors, avoiding full optical flow computation.", "motivation": "Traditional methods rely on optical flow or depth estimation, which can be inefficient. The paper aims to leverage sparse, high-temporal-resolution event data for better performance.", "method": "An optimization-based pipeline with event over-segmentation, residual analysis for moving objects, and hierarchical clustering using motion similarity and temporal consistency.", "result": "Accurate segmentation and translational motion estimation on the EVIMO2v2 dataset, with advantages at object boundaries.", "conclusion": "The method is scalable and suitable for real-time robotic and navigation applications."}}
{"id": "2507.14615", "pdf": "https://arxiv.org/pdf/2507.14615", "abs": "https://arxiv.org/abs/2507.14615", "authors": ["Fred Mutisya", "Shikoh Gitau", "Christine Syovata", "Diana Oigara", "Ibrahim Matende", "Muna Aden", "Munira Ali", "Ryan Nyotu", "Diana Marion", "Job Nyangena", "Nasubo Ongoma", "Keith Mbae", "Elizabeth Wamicha", "Eric Mibuari", "Jean Philbert Nsengemana", "Talkmore Chidede"], "title": "Retrieval-Augmented Clinical Benchmarking for Contextual Model Testing in Kenyan Primary Care: A Methodology Paper", "categories": ["cs.CL", "cs.AI"], "comment": "29 pages, 6 figs, 6 tables. Companion methods paper forthcoming", "summary": "Large Language Models(LLMs) hold promise for improving healthcare access in\nlow-resource settings, but their effectiveness in African primary care remains\nunderexplored. We present a methodology for creating a benchmark dataset and\nevaluation framework focused on Kenyan Level 2 and 3 clinical care. Our\napproach uses retrieval augmented generation (RAG) to ground clinical questions\nin Kenya's national guidelines, ensuring alignment with local standards. These\nguidelines were digitized, chunked, and indexed for semantic retrieval. Gemini\nFlash 2.0 Lite was then prompted with guideline excerpts to generate realistic\nclinical scenarios, multiple-choice questions, and rationale based answers in\nEnglish and Swahili. Kenyan physicians co-created and refined the dataset, and\na blinded expert review process ensured clinical accuracy, clarity, and\ncultural appropriateness. The resulting Alama Health QA dataset includes\nthousands of regulator-aligned question answer pairs across common outpatient\nconditions. Beyond accuracy, we introduce evaluation metrics that test clinical\nreasoning, safety, and adaptability such as rare case detection (Needle in the\nHaystack), stepwise logic (Decision Points), and contextual adaptability.\nInitial results reveal significant performance gaps when LLMs are applied to\nlocalized scenarios, consistent with findings that LLM accuracy is lower on\nAfrican medical content than on US-based benchmarks. This work offers a\nreplicable model for guideline-driven, dynamic benchmarking to support safe AI\ndeployment in African health systems.", "AI": {"tldr": "The paper explores using LLMs for healthcare in low-resource African settings, focusing on Kenya. It introduces a benchmark dataset (Alama Health QA) using RAG and local guidelines, evaluated by Kenyan physicians. Results show LLMs perform worse on localized content, highlighting the need for tailored benchmarks.", "motivation": "To address the underexplored effectiveness of LLMs in African primary care and improve healthcare access in low-resource settings.", "method": "Uses retrieval augmented generation (RAG) to align questions with Kenya's national guidelines, digitized and indexed for semantic retrieval. Gemini Flash 2.0 Lite generates clinical scenarios and questions, refined by Kenyan physicians and expert review.", "result": "LLMs show significant performance gaps in localized scenarios, with lower accuracy on African medical content compared to US benchmarks.", "conclusion": "The work provides a replicable model for guideline-driven benchmarking to ensure safe AI deployment in African health systems."}}
{"id": "2507.14501", "pdf": "https://arxiv.org/pdf/2507.14501", "abs": "https://arxiv.org/abs/2507.14501", "authors": ["Jiahui Zhang", "Yuelei Li", "Anpei Chen", "Muyu Xu", "Kunhao Liu", "Jianyuan Wang", "Xiao-Xiao Long", "Hanxue Liang", "Zexiang Xu", "Hao Su", "Christian Theobalt", "Christian Rupprecht", "Andrea Vedaldi", "Hanspeter Pfister", "Shijian Lu", "Fangneng Zhan"], "title": "Advances in Feed-Forward 3D Reconstruction and View Synthesis: A Survey", "categories": ["cs.CV"], "comment": "A project page associated with this survey is available at\n  https://fnzhan.com/projects/Feed-Forward-3D", "summary": "3D reconstruction and view synthesis are foundational problems in computer\nvision, graphics, and immersive technologies such as augmented reality (AR),\nvirtual reality (VR), and digital twins. Traditional methods rely on\ncomputationally intensive iterative optimization in a complex chain, limiting\ntheir applicability in real-world scenarios. Recent advances in feed-forward\napproaches, driven by deep learning, have revolutionized this field by enabling\nfast and generalizable 3D reconstruction and view synthesis. This survey offers\na comprehensive review of feed-forward techniques for 3D reconstruction and\nview synthesis, with a taxonomy according to the underlying representation\narchitectures including point cloud, 3D Gaussian Splatting (3DGS), Neural\nRadiance Fields (NeRF), etc. We examine key tasks such as pose-free\nreconstruction, dynamic 3D reconstruction, and 3D-aware image and video\nsynthesis, highlighting their applications in digital humans, SLAM, robotics,\nand beyond. In addition, we review commonly used datasets with detailed\nstatistics, along with evaluation protocols for various downstream tasks. We\nconclude by discussing open research challenges and promising directions for\nfuture work, emphasizing the potential of feed-forward approaches to advance\nthe state of the art in 3D vision.", "AI": {"tldr": "A survey on feed-forward deep learning techniques for 3D reconstruction and view synthesis, covering representations like NeRF and 3DGS, applications, datasets, and future challenges.", "motivation": "Traditional methods for 3D reconstruction and view synthesis are computationally intensive, limiting real-world use. Feed-forward deep learning approaches offer faster, generalizable solutions.", "method": "The paper reviews feed-forward techniques, categorizing them by representation architectures (e.g., NeRF, 3DGS) and tasks like dynamic reconstruction and pose-free synthesis.", "result": "The survey highlights advancements in speed and generalization, with applications in AR/VR, robotics, and digital humans, and reviews datasets and evaluation protocols.", "conclusion": "Feed-forward approaches hold promise for advancing 3D vision, though challenges remain. Future work should address these to push the field further."}}
{"id": "2507.14640", "pdf": "https://arxiv.org/pdf/2507.14640", "abs": "https://arxiv.org/abs/2507.14640", "authors": ["Eric Xia", "Jugal Kalita"], "title": "Linear Relational Decoding of Morphology in Language Models", "categories": ["cs.CL"], "comment": null, "summary": "A two-part affine approximation has been found to be a good approximation for\ntransformer computations over certain subject object relations. Adapting the\nBigger Analogy Test Set, we show that the linear transformation Ws, where s is\na middle layer representation of a subject token and W is derived from model\nderivatives, is also able to accurately reproduce final object states for many\nrelations. This linear technique is able to achieve 90% faithfulness on\nmorphological relations, and we show similar findings multi-lingually and\nacross models. Our findings indicate that some conceptual relationships in\nlanguage models, such as morphology, are readily interpretable from latent\nspace, and are sparsely encoded by cross-layer linear transformations.", "AI": {"tldr": "A two-part affine approximation effectively approximates transformer computations for certain subject-object relations, achieving 90% faithfulness on morphological relations.", "motivation": "To explore interpretability of conceptual relationships in language models, such as morphology, through latent space analysis.", "method": "Adapting the Bigger Analogy Test Set and using linear transformations (Ws) derived from model derivatives to approximate final object states.", "result": "The linear technique achieves high faithfulness (90%) on morphological relations, with consistent results across languages and models.", "conclusion": "Conceptual relationships like morphology are sparsely encoded and interpretable via cross-layer linear transformations in language models."}}
{"id": "2507.14505", "pdf": "https://arxiv.org/pdf/2507.14505", "abs": "https://arxiv.org/abs/2507.14505", "authors": ["Jiahao Ma", "Tianyu Wang", "Miaomiao Liu", "David Ahmedt-Aristizabal", "Chuong Nguyen"], "title": "DCHM: Depth-Consistent Human Modeling for Multiview Detection", "categories": ["cs.CV"], "comment": "multi-view detection, sparse-view reconstruction", "summary": "Multiview pedestrian detection typically involves two stages: human modeling\nand pedestrian localization. Human modeling represents pedestrians in 3D space\nby fusing multiview information, making its quality crucial for detection\naccuracy. However, existing methods often introduce noise and have low\nprecision. While some approaches reduce noise by fitting on costly multiview 3D\nannotations, they often struggle to generalize across diverse scenes. To\neliminate reliance on human-labeled annotations and accurately model humans, we\npropose Depth-Consistent Human Modeling (DCHM), a framework designed for\nconsistent depth estimation and multiview fusion in global coordinates.\nSpecifically, our proposed pipeline with superpixel-wise Gaussian Splatting\nachieves multiview depth consistency in sparse-view, large-scaled, and crowded\nscenarios, producing precise point clouds for pedestrian localization.\nExtensive validations demonstrate that our method significantly reduces noise\nduring human modeling, outperforming previous state-of-the-art baselines.\nAdditionally, to our knowledge, DCHM is the first to reconstruct pedestrians\nand perform multiview segmentation in such a challenging setting. Code is\navailable on the \\href{https://jiahao-ma.github.io/DCHM/}{project page}.", "AI": {"tldr": "DCHM improves multiview pedestrian detection by ensuring depth consistency and reducing noise in human modeling without relying on costly 3D annotations.", "motivation": "Existing methods for human modeling in multiview pedestrian detection introduce noise and lack generalization, often requiring expensive annotations.", "method": "Proposes Depth-Consistent Human Modeling (DCHM) with superpixel-wise Gaussian Splatting for consistent depth estimation and multiview fusion in global coordinates.", "result": "DCHM significantly reduces noise, outperforms state-of-the-art baselines, and is the first to reconstruct pedestrians in sparse-view, large-scale, and crowded scenarios.", "conclusion": "DCHM offers a robust solution for accurate human modeling and pedestrian localization without relying on human-labeled annotations."}}
{"id": "2507.14649", "pdf": "https://arxiv.org/pdf/2507.14649", "abs": "https://arxiv.org/abs/2507.14649", "authors": ["Minsuh Joo", "Hyunsoo Cho"], "title": "Cleanse: Uncertainty Estimation Approach Using Clustering-based Semantic Consistency in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite the outstanding performance of large language models (LLMs) across\nvarious NLP tasks, hallucinations in LLMs--where LLMs generate inaccurate\nresponses--remains as a critical problem as it can be directly connected to a\ncrisis of building safe and reliable LLMs. Uncertainty estimation is primarily\nused to measure hallucination levels in LLM responses so that correct and\nincorrect answers can be distinguished clearly. This study proposes an\neffective uncertainty estimation approach, \\textbf{Cl}ust\\textbf{e}ring-based\nsem\\textbf{an}tic con\\textbf{s}ist\\textbf{e}ncy (\\textbf{Cleanse}). Cleanse\nquantifies the uncertainty with the proportion of the intra-cluster consistency\nin the total consistency between LLM hidden embeddings which contain adequate\nsemantic information of generations, by employing clustering. The effectiveness\nof Cleanse for detecting hallucination is validated using four off-the-shelf\nmodels, LLaMA-7B, LLaMA-13B, LLaMA2-7B and Mistral-7B and two\nquestion-answering benchmarks, SQuAD and CoQA.", "AI": {"tldr": "The paper proposes Cleanse, a clustering-based method to estimate uncertainty in LLM responses to detect hallucinations, validated on multiple models and benchmarks.", "motivation": "Hallucinations in LLMs pose a reliability and safety risk, necessitating methods to distinguish accurate from inaccurate responses.", "method": "Cleanse uses clustering on LLM hidden embeddings to quantify uncertainty via intra-cluster semantic consistency.", "result": "Validated on LLaMA and Mistral models using SQuAD and CoQA benchmarks, Cleanse effectively detects hallucinations.", "conclusion": "Cleanse provides a reliable approach for uncertainty estimation to mitigate LLM hallucinations, enhancing model safety and reliability."}}
{"id": "2507.14533", "pdf": "https://arxiv.org/pdf/2507.14533", "abs": "https://arxiv.org/abs/2507.14533", "authors": ["Shuo Cao", "Nan Ma", "Jiayang Li", "Xiaohui Li", "Lihao Shao", "Kaiwen Zhu", "Yu Zhou", "Yuandong Pu", "Jiarui Wu", "Jiaquan Wang", "Bo Qu", "Wenhai Wang", "Yu Qiao", "Dajuin Yao", "Yihao Liu"], "title": "ArtiMuse: Fine-Grained Image Aesthetics Assessment with Joint Scoring and Expert-Level Understanding", "categories": ["cs.CV"], "comment": "43 pages, 31 figures, 13 tables", "summary": "The rapid advancement of educational applications, artistic creation, and\nAI-generated content (AIGC) technologies has substantially increased practical\nrequirements for comprehensive Image Aesthetics Assessment (IAA), particularly\ndemanding methods capable of delivering both quantitative scoring and\nprofessional understanding. Multimodal Large Language Model (MLLM)-based IAA\nmethods demonstrate stronger perceptual and generalization capabilities\ncompared to traditional approaches, yet they suffer from modality bias\n(score-only or text-only) and lack fine-grained attribute decomposition,\nthereby failing to support further aesthetic assessment. In this paper, we\npresent:(1) ArtiMuse, an innovative MLLM-based IAA model with Joint Scoring and\nExpert-Level Understanding capabilities; (2) ArtiMuse-10K, the first\nexpert-curated image aesthetic dataset comprising 10,000 images spanning 5 main\ncategories and 15 subcategories, each annotated by professional experts with\n8-dimensional attributes analysis and a holistic score. Both the model and\ndataset will be made public to advance the field.", "AI": {"tldr": "ArtiMuse is an MLLM-based IAA model offering joint scoring and expert-level understanding, addressing modality bias and lack of fine-grained analysis. It introduces ArtiMuse-10K, a 10K-image expert-curated dataset with detailed annotations.", "motivation": "The need for comprehensive IAA methods with quantitative scoring and professional understanding due to advancements in educational, artistic, and AIGC technologies.", "method": "ArtiMuse, an MLLM-based IAA model, combines scoring and expert-level understanding, supported by the ArtiMuse-10K dataset with 8D attributes and holistic scores.", "result": "ArtiMuse addresses modality bias and enables fine-grained aesthetic assessment, backed by a high-quality dataset.", "conclusion": "The model and dataset advance IAA by providing detailed, expert-level analysis and public availability for further research."}}
{"id": "2507.14664", "pdf": "https://arxiv.org/pdf/2507.14664", "abs": "https://arxiv.org/abs/2507.14664", "authors": ["Wannaphong Phatthiyaphaibun", "Can Udomcharoenchaikit", "Pakpoom Singkorapoom", "Kunat Pipatanakul", "Ekapol Chuangsuwanich", "Peerat Limkonchotiwat", "Sarana Nutanong"], "title": "Mangosteen: An Open Thai Corpus for Language Model Pretraining", "categories": ["cs.CL"], "comment": "Work in Progress.All artifacts in this papers:\n  https://huggingface.co/collections/aisingapore/wangchanlion-v3-687a362d8f0ea2fe4077c6b3", "summary": "Pre-training data shapes a language model's quality, but raw web text is\nnoisy and demands careful cleaning. Existing large-scale corpora rely on\nEnglish-centric or language-agnostic pipelines whose heuristics do not capture\nThai script or cultural nuances, leaving risky material such as gambling\ncontent untreated. Prior Thai-specific efforts customize pipelines or build new\nones, yet seldom release their data or document design choices, hindering\nreproducibility and raising the question of how to construct a transparent,\nhigh-quality Thai corpus. We introduce Mangosteen: a 47 billion-token Thai\ncorpus built through a Thai-adapted Dolma pipeline that includes custom\nrule-based language ID, revised C4/Gopher quality filters, and Thai-trained\ncontent filters, plus curated non-web sources such as Wikipedia, Royal Gazette\ntexts, OCR-extracted books, and CC-licensed YouTube subtitles. Systematic\nablations using GPT-2 show the pipeline trims CommonCrawl from 202M to 25M\ndocuments while raising SEA-HELM NLG from 3 to 11; an 8B-parameter SEA-LION\nmodel continually pre-trained on Mangosteen then surpasses SEA-LION-v3 and\nLlama-3.1 by about four points on Thai benchmarks. We release the full pipeline\ncode, cleaning manifests, corpus snapshot, and all checkpoints, providing a\nfully reproducible foundation for future Thai and regional LLM research.", "AI": {"tldr": "Mangosteen is a 47B-token Thai corpus built with a Thai-adapted Dolma pipeline, improving Thai LLM performance and reproducibility.", "motivation": "Existing corpora lack Thai-specific cleaning, risking harmful content and hindering reproducibility.", "method": "Thai-adapted Dolma pipeline with custom language ID, quality filters, and curated non-web sources.", "result": "Pipeline reduces CommonCrawl docs from 202M to 25M, improves SEA-HELM NLG from 3 to 11, and boosts SEA-LION model performance.", "conclusion": "Mangosteen provides a transparent, high-quality Thai corpus with full reproducibility for future research."}}
{"id": "2507.14543", "pdf": "https://arxiv.org/pdf/2507.14543", "abs": "https://arxiv.org/abs/2507.14543", "authors": ["Sharanya Mukherjee", "Md Hishaam Akhtar", "Kannadasan R"], "title": "Real Time Captioning of Sign Language Gestures in Video Meetings", "categories": ["cs.CV", "cs.CY", "cs.HC", "cs.LG", "I.4.6"], "comment": "7 pages, 2 figures, 1 table, Presented at ICCMDE 2021", "summary": "It has always been a rather tough task to communicate with someone possessing\na hearing impairment. One of the most tested ways to establish such a\ncommunication is through the use of sign based languages. However, not many\npeople are aware of the smaller intricacies involved with sign language. Sign\nlanguage recognition using computer vision aims at eliminating the\ncommunication barrier between deaf-mute and ordinary people so that they can\nproperly communicate with others. Recently the pandemic has left the whole\nworld shaken up and has transformed the way we communicate. Video meetings have\nbecome essential for everyone, even people with a hearing disability. In recent\nstudies, it has been found that people with hearing disabilities prefer to sign\nover typing during these video calls. In this paper, we are proposing a browser\nextension that will automatically translate sign language to subtitles for\neveryone else in the video call. The Large-scale dataset which contains more\nthan 2000 Word-Level ASL videos, which were performed by over 100 signers will\nbe used.", "AI": {"tldr": "A browser extension is proposed to translate sign language to subtitles in video calls, using a large dataset of ASL videos.", "motivation": "To bridge the communication gap between deaf-mute individuals and others, especially during video calls, where signing is preferred over typing.", "method": "Utilizes a large-scale dataset of 2000+ Word-Level ASL videos from 100+ signers to train the system for automatic translation.", "result": "The proposed extension aims to provide real-time subtitles for sign language in video meetings.", "conclusion": "The tool enhances accessibility for deaf-mute individuals in digital communication."}}
{"id": "2507.14681", "pdf": "https://arxiv.org/pdf/2507.14681", "abs": "https://arxiv.org/abs/2507.14681", "authors": ["Vinicius Anjos de Almeida", "Vinicius de Camargo", "Raquel G\u00f3mez-Bravo", "Egbert van der Haring", "Kees van Boven", "Marcelo Finger", "Luis Fernandez Lopez"], "title": "Large Language Models as Medical Codes Selectors: a benchmark using the International Classification of Primary Care", "categories": ["cs.CL"], "comment": "To be submitted to peer-reviewed journal. 33 pages, 10 figures\n  (including appendix), 15 tables (including appendix). For associated code\n  repository, see https://github.com/almeidava93/llm-as-code-selectors-paper", "summary": "Background: Medical coding structures healthcare data for research, quality\nmonitoring, and policy. This study assesses the potential of large language\nmodels (LLMs) to assign ICPC-2 codes using the output of a domain-specific\nsearch engine.\n  Methods: A dataset of 437 Brazilian Portuguese clinical expressions, each\nannotated with ICPC-2 codes, was used. A semantic search engine (OpenAI's\ntext-embedding-3-large) retrieved candidates from 73,563 labeled concepts.\nThirty-three LLMs were prompted with each query and retrieved results to select\nthe best-matching ICPC-2 code. Performance was evaluated using F1-score, along\nwith token usage, cost, response time, and format adherence.\n  Results: Twenty-eight models achieved F1-score > 0.8; ten exceeded 0.85. Top\nperformers included gpt-4.5-preview, o3, and gemini-2.5-pro. Retriever\noptimization can improve performance by up to 4 points. Most models returned\nvalid codes in the expected format, with reduced hallucinations. Smaller models\n(<3B) struggled with formatting and input length.\n  Conclusions: LLMs show strong potential for automating ICPC-2 coding, even\nwithout fine-tuning. This work offers a benchmark and highlights challenges,\nbut findings are limited by dataset scope and setup. Broader, multilingual,\nend-to-end evaluations are needed for clinical validation.", "AI": {"tldr": "LLMs demonstrate strong potential for automating ICPC-2 coding without fine-tuning, with top models achieving high F1-scores. Challenges include formatting issues for smaller models and dataset limitations.", "motivation": "To assess the feasibility of using LLMs for automating ICPC-2 code assignment in healthcare data, leveraging domain-specific search engine outputs.", "method": "Used a dataset of 437 clinical expressions in Brazilian Portuguese annotated with ICPC-2 codes. Evaluated 33 LLMs by prompting them with queries and retrieved results, measuring performance via F1-score, token usage, cost, response time, and format adherence.", "result": "Top models (e.g., gpt-4.5-preview, o3, gemini-2.5-pro) achieved F1-scores > 0.85. Retriever optimization improved performance by up to 4 points. Smaller models (<3B) faced formatting and input length issues.", "conclusion": "LLMs are promising for ICPC-2 coding automation, but broader, multilingual, and end-to-end evaluations are needed for clinical validation."}}
{"id": "2507.14544", "pdf": "https://arxiv.org/pdf/2507.14544", "abs": "https://arxiv.org/abs/2507.14544", "authors": ["Sujata Gaihre", "Amir Thapa Magar", "Prasuna Pokharel", "Laxmi Tiwari"], "title": "Multimodal AI for Gastrointestinal Diagnostics: Tackling VQA in MEDVQA-GI 2025", "categories": ["cs.CV", "cs.AI", "68T45 (Machine vision and scene understanding)", "I.2.10; I.4.8; H.3.1"], "comment": "accepted to ImageCLEF 2025, to be published in the lab proceedings", "summary": "This paper describes our approach to Subtask 1 of the ImageCLEFmed MEDVQA\n2025 Challenge, which targets visual question answering (VQA) for\ngastrointestinal endoscopy. We adopt the Florence model-a large-scale\nmultimodal foundation model-as the backbone of our VQA pipeline, pairing a\npowerful vision encoder with a text encoder to interpret endoscopic images and\nproduce clinically relevant answers. To improve generalization, we apply\ndomain-specific augmentations that preserve medical features while increasing\ntraining diversity. Experiments on the KASVIR dataset show that fine-tuning\nFlorence yields accurate responses on the official challenge metrics. Our\nresults highlight the potential of large multimodal models in medical VQA and\nprovide a strong baseline for future work on explainability, robustness, and\nclinical integration. The code is publicly available at:\nhttps://github.com/TiwariLaxuu/VQA-Florence.git", "AI": {"tldr": "The paper presents a VQA approach for gastrointestinal endoscopy using the Florence model, achieving strong results on the KASVIR dataset.", "motivation": "To address visual question answering in medical endoscopy by leveraging large multimodal models for accurate and clinically relevant responses.", "method": "Adopts the Florence model with domain-specific augmentations to enhance generalization and fine-tunes it on the KASVIR dataset.", "result": "Fine-tuning Florence yields accurate responses, demonstrating the potential of large multimodal models in medical VQA.", "conclusion": "The work provides a strong baseline for future research on explainability, robustness, and clinical integration in medical VQA."}}
{"id": "2507.14683", "pdf": "https://arxiv.org/pdf/2507.14683", "abs": "https://arxiv.org/abs/2507.14683", "authors": ["Xingxuan Li", "Yao Xiao", "Dianwen Ng", "Hai Ye", "Yue Deng", "Xiang Lin", "Bin Wang", "Zhanfeng Mo", "Chong Zhang", "Yueyi Zhang", "Zonglin Yang", "Ruilin Li", "Lei Lei", "Shihao Xu", "Han Zhao", "Weiling Chen", "Feng Ji", "Lidong Bing"], "title": "MiroMind-M1: An Open-Source Advancement in Mathematical Reasoning via Context-Aware Multi-Stage Policy Optimization", "categories": ["cs.CL"], "comment": "Technical report", "summary": "Large language models have recently evolved from fluent text generation to\nadvanced reasoning across diverse domains, giving rise to reasoning language\nmodels. Among these domains, mathematical reasoning serves as a representative\nbenchmark as it requires precise multi-step logic and abstract reasoning, which\ncan be generalized to other tasks. While closed-source RLMs such as GPT-o3\ndemonstrate impressive reasoning capabilities, their proprietary nature limits\ntransparency and reproducibility. Although many open-source projects aim to\nclose this gap, most of them lack sufficient openness by omitting critical\nresources such as datasets and detailed training configurations, which hinders\nreproducibility. To contribute toward greater transparency in RLM development,\nwe introduce the MiroMind-M1 series, a set of fully open-source RLMs built on\nthe Qwen-2.5 backbone that match or exceed the performance of existing\nopen-source RLMs. Specifically, our models are trained in two stages: SFT on a\ncarefully curated corpus of 719K math-reasoning problems with verified CoT\ntrajectories, followed by RLVR on 62K challenging and verifiable problems. To\nenhance the robustness and efficiency of the RLVR process, we introduce\nContext-Aware Multi-Stage Policy Optimization, an algorithm that integrates\nlength-progressive training with an adaptive repetition penalty to encourage\ncontext-aware RL training. Our model achieves state-of-the-art or competitive\nperformance and superior token efficiency among Qwen-2.5-based open-source 7B\nand 32B models on the AIME24, AIME25, and MATH benchmarks. To facilitate\nreproducibility, we release the complete stack: models (MiroMind-M1-SFT-7B,\nMiroMind-M1-RL-7B, MiroMind-M1-RL-32B); datasets (MiroMind-M1-SFT-719K,\nMiroMind-M1-RL-62K); and all training and evaluation configurations. We hope\nthese resources will support further research and foster community advancement.", "AI": {"tldr": "The paper introduces MiroMind-M1, a fully open-source reasoning language model (RLM) series, addressing transparency and reproducibility gaps in existing RLMs by releasing models, datasets, and configurations.", "motivation": "To enhance transparency and reproducibility in RLM development, as proprietary models like GPT-3 lack openness, and many open-source projects omit critical resources.", "method": "Two-stage training: supervised fine-tuning (SFT) on 719K math problems with verified reasoning steps, followed by reinforcement learning with verifiable rewards (RLVR) on 62K problems, using a novel Context-Aware Multi-Stage Policy Optimization algorithm.", "result": "State-of-the-art or competitive performance on benchmarks (AIME24, AIME25, MATH) with superior token efficiency for 7B and 32B models.", "conclusion": "The MiroMind-M1 series and released resources aim to advance research and community progress in RLMs."}}
{"id": "2507.14549", "pdf": "https://arxiv.org/pdf/2507.14549", "abs": "https://arxiv.org/abs/2507.14549", "authors": ["Haotian Deng", "Chi Zhang", "Chen Wei", "Quanying Liu"], "title": "Synthesizing Images on Perceptual Boundaries of ANNs for Uncovering Human Perceptual Variability on Facial Expressions", "categories": ["cs.CV", "cs.CY"], "comment": "Accepted by IJCNN 2025", "summary": "A fundamental challenge in affective cognitive science is to develop models\nthat accurately capture the relationship between external emotional stimuli and\nhuman internal experiences. While ANNs have demonstrated remarkable accuracy in\nfacial expression recognition, their ability to model inter-individual\ndifferences in human perception remains underexplored. This study investigates\nthe phenomenon of high perceptual variability-where individuals exhibit\nsignificant differences in emotion categorization even when viewing the same\nstimulus. Inspired by the similarity between ANNs and human perception, we\nhypothesize that facial expression samples that are ambiguous for ANN\nclassifiers also elicit divergent perceptual judgments among human observers.\nTo examine this hypothesis, we introduce a novel perceptual boundary sampling\nmethod to generate facial expression stimuli that lie along ANN decision\nboundaries. These ambiguous samples form the basis of the varEmotion dataset,\nconstructed through large-scale human behavioral experiments. Our analysis\nreveals that these ANN-confusing stimuli also provoke heightened perceptual\nuncertainty in human participants, highlighting shared computational principles\nin emotion perception. Finally, by fine-tuning ANN representations using\nbehavioral data, we achieve alignment between ANN predictions and both\ngroup-level and individual-level human perceptual patterns. Our findings\nestablish a systematic link between ANN decision boundaries and human\nperceptual variability, offering new insights into personalized modeling of\nemotional interpretation.", "AI": {"tldr": "The study explores the link between ANN decision boundaries and human perceptual variability in emotion categorization, revealing shared computational principles and enabling personalized emotion modeling.", "motivation": "To address the gap in understanding inter-individual differences in emotion perception and the potential alignment between ANN models and human perceptual variability.", "method": "A perceptual boundary sampling method was used to generate ambiguous facial expression stimuli, forming the varEmotion dataset, followed by large-scale human experiments and ANN fine-tuning.", "result": "ANN-confusing stimuli also caused perceptual uncertainty in humans, and fine-tuning ANNs aligned their predictions with human perceptual patterns.", "conclusion": "The findings bridge ANN decision boundaries and human perceptual variability, advancing personalized emotion interpretation models."}}
{"id": "2507.14688", "pdf": "https://arxiv.org/pdf/2507.14688", "abs": "https://arxiv.org/abs/2507.14688", "authors": ["Mohammed Alkhowaiter", "Norah Alshahrani", "Saied Alshahrani", "Reem I. Masoud", "Alaa Alzahrani", "Deema Alnuhait", "Emad A. Alghamdi", "Khalid Almubarak"], "title": "Mind the Gap: A Review of Arabic Post-Training Datasets and Their Limitations", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Post-training has emerged as a crucial technique for aligning pre-trained\nLarge Language Models (LLMs) with human instructions, significantly enhancing\ntheir performance across a wide range of tasks. Central to this process is the\nquality and diversity of post-training datasets. This paper presents a review\nof publicly available Arabic post-training datasets on the Hugging Face Hub,\norganized along four key dimensions: (1) LLM Capabilities (e.g., Question\nAnswering, Translation, Reasoning, Summarization, Dialogue, Code Generation,\nand Function Calling); (2) Steerability (e.g., persona and system prompts); (3)\nAlignment (e.g., cultural, safety, ethics, and fairness), and (4) Robustness.\nEach dataset is rigorously evaluated based on popularity, practical adoption,\nrecency and maintenance, documentation and annotation quality, licensing\ntransparency, and scientific contribution. Our review revealed critical gaps in\nthe development of Arabic post-training datasets, including limited task\ndiversity, inconsistent or missing documentation and annotation, and low\nadoption across the community. Finally, the paper discusses the implications of\nthese gaps on the progress of Arabic LLMs and applications while providing\nconcrete recommendations for future efforts in post-training dataset\ndevelopment.", "AI": {"tldr": "A review of Arabic post-training datasets on Hugging Face Hub highlights gaps in task diversity, documentation, and adoption, with recommendations for future improvements.", "motivation": "To assess and improve the quality and diversity of Arabic post-training datasets for better alignment of LLMs with human instructions.", "method": "Evaluation of datasets along four dimensions: LLM Capabilities, Steerability, Alignment, and Robustness, using criteria like popularity, adoption, and documentation.", "result": "Identified gaps include limited task diversity, poor documentation, and low community adoption.", "conclusion": "The findings underscore the need for better Arabic post-training datasets to advance Arabic LLMs, with actionable recommendations provided."}}
{"id": "2507.14553", "pdf": "https://arxiv.org/pdf/2507.14553", "abs": "https://arxiv.org/abs/2507.14553", "authors": ["Xiaoran Wu"], "title": "Clutter Detection and Removal by Multi-Objective Analysis for Photographic Guidance", "categories": ["cs.CV", "cs.HC"], "comment": null, "summary": "Clutter in photos is a distraction preventing photographers from conveying\nthe intended emotions or stories to the audience. Photography amateurs\nfrequently include clutter in their photos due to unconscious negligence or the\nlack of experience in creating a decluttered, aesthetically appealing scene for\nshooting. We are thus motivated to develop a camera guidance system that\nprovides solutions and guidance for clutter identification and removal. We\nestimate and visualize the contribution of objects to the overall aesthetics\nand content of a photo, based on which users can interactively identify\nclutter. Suggestions on getting rid of clutter, as well as a tool that removes\ncluttered objects computationally, are provided to guide users to deal with\ndifferent kinds of clutter and improve their photographic work. Two technical\nnovelties underpin interactions in our system: a clutter distinguishment\nalgorithm with aesthetics evaluations for objects and an iterative image\ninpainting algorithm based on generative adversarial nets that reconstructs\nmissing regions of removed objects for high-resolution images. User studies\ndemonstrate that our system provides flexible interfaces and accurate\nalgorithms that allow users to better identify distractions and take higher\nquality images within less time.", "AI": {"tldr": "A camera guidance system helps users identify and remove clutter in photos using aesthetic evaluation and image inpainting, improving photo quality.", "motivation": "Clutter in photos distracts from intended emotions or stories, especially for amateurs.", "method": "Uses a clutter distinguishment algorithm and iterative image inpainting with GANs for high-resolution images.", "result": "User studies show the system helps users identify distractions and take better photos faster.", "conclusion": "The system effectively aids in clutter removal and enhances photographic quality."}}
{"id": "2507.14693", "pdf": "https://arxiv.org/pdf/2507.14693", "abs": "https://arxiv.org/abs/2507.14693", "authors": ["Amina Dzafic", "Merve Kavut", "Ulya Bayram"], "title": "Rethinking Suicidal Ideation Detection: A Trustworthy Annotation Framework and Cross-Lingual Model Evaluation", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "comment": "This manuscript has been submitted to the IEEE Journal of Biomedical\n  and Health Informatics", "summary": "Suicidal ideation detection is critical for real-time suicide prevention, yet\nits progress faces two under-explored challenges: limited language coverage and\nunreliable annotation practices. Most available datasets are in English, but\neven among these, high-quality, human-annotated data remains scarce. As a\nresult, many studies rely on available pre-labeled datasets without examining\ntheir annotation process or label reliability. The lack of datasets in other\nlanguages further limits the global realization of suicide prevention via\nartificial intelligence (AI). In this study, we address one of these gaps by\nconstructing a novel Turkish suicidal ideation corpus derived from social media\nposts and introducing a resource-efficient annotation framework involving three\nhuman annotators and two large language models (LLMs). We then address the\nremaining gaps by performing a bidirectional evaluation of label reliability\nand model consistency across this dataset and three popular English suicidal\nideation detection datasets, using transfer learning through eight pre-trained\nsentiment and emotion classifiers. These transformers help assess annotation\nconsistency and benchmark model performance against manually labeled data. Our\nfindings underscore the need for more rigorous, language-inclusive approaches\nto annotation and evaluation in mental health natural language processing (NLP)\nwhile demonstrating the questionable performance of popular models with\nzero-shot transfer learning. We advocate for transparency in model training and\ndataset construction in mental health NLP, prioritizing data and model\nreliability.", "AI": {"tldr": "The paper addresses gaps in suicidal ideation detection by creating a Turkish corpus and evaluating label reliability and model performance across datasets, advocating for better practices in mental health NLP.", "motivation": "Limited language coverage and unreliable annotation practices in suicidal ideation detection datasets hinder global suicide prevention efforts.", "method": "Constructed a Turkish suicidal ideation corpus using social media posts and a resource-efficient annotation framework. Evaluated label reliability and model consistency across datasets using transfer learning with pre-trained classifiers.", "result": "Highlighted the need for rigorous, language-inclusive annotation and evaluation, and questioned the performance of models with zero-shot transfer learning.", "conclusion": "Advocates for transparency in dataset construction and model training, prioritizing reliability in mental health NLP."}}
{"id": "2507.14555", "pdf": "https://arxiv.org/pdf/2507.14555", "abs": "https://arxiv.org/abs/2507.14555", "authors": ["Jintang Xue", "Ganning Zhao", "Jie-En Yao", "Hong-En Chen", "Yue Hu", "Meida Chen", "Suya You", "C. -C. Jay Kuo"], "title": "Descrip3D: Enhancing Large Language Model-based 3D Scene Understanding with Object-Level Text Descriptions", "categories": ["cs.CV"], "comment": null, "summary": "Understanding 3D scenes goes beyond simply recognizing objects; it requires\nreasoning about the spatial and semantic relationships between them. Current 3D\nscene-language models often struggle with this relational understanding,\nparticularly when visual embeddings alone do not adequately convey the roles\nand interactions of objects. In this paper, we introduce Descrip3D, a novel and\npowerful framework that explicitly encodes the relationships between objects\nusing natural language. Unlike previous methods that rely only on 2D and 3D\nembeddings, Descrip3D enhances each object with a textual description that\ncaptures both its intrinsic attributes and contextual relationships. These\nrelational cues are incorporated into the model through a dual-level\nintegration: embedding fusion and prompt-level injection. This allows for\nunified reasoning across various tasks such as grounding, captioning, and\nquestion answering, all without the need for task-specific heads or additional\nsupervision. When evaluated on five benchmark datasets, including ScanRefer,\nMulti3DRefer, ScanQA, SQA3D, and Scan2Cap, Descrip3D consistently outperforms\nstrong baseline models, demonstrating the effectiveness of language-guided\nrelational representation for understanding complex indoor scenes.", "AI": {"tldr": "Descrip3D enhances 3D scene understanding by integrating textual descriptions of object relationships, outperforming baselines on multiple benchmarks.", "motivation": "Current 3D scene-language models lack relational understanding due to reliance on visual embeddings alone.", "method": "Descrip3D uses natural language to encode object relationships, combining embedding fusion and prompt-level injection for unified reasoning.", "result": "Outperforms baselines on five datasets (ScanRefer, Multi3DRefer, ScanQA, SQA3D, Scan2Cap).", "conclusion": "Language-guided relational representation improves complex indoor scene understanding."}}
{"id": "2507.14741", "pdf": "https://arxiv.org/pdf/2507.14741", "abs": "https://arxiv.org/abs/2507.14741", "authors": ["Maria Sahakyan", "Bedoor AlShebli"], "title": "Disparities in Peer Review Tone and the Role of Reviewer Anonymity", "categories": ["cs.CL"], "comment": null, "summary": "The peer review process is often regarded as the gatekeeper of scientific\nintegrity, yet increasing evidence suggests that it is not immune to bias.\nAlthough structural inequities in peer review have been widely debated, much\nless attention has been paid to the subtle ways in which language itself may\nreinforce disparities. This study undertakes one of the most comprehensive\nlinguistic analyses of peer review to date, examining more than 80,000 reviews\nin two major journals. Using natural language processing and large-scale\nstatistical modeling, it uncovers how review tone, sentiment, and supportive\nlanguage vary across author demographics, including gender, race, and\ninstitutional affiliation. Using a data set that includes both anonymous and\nsigned reviews, this research also reveals how the disclosure of reviewer\nidentity shapes the language of evaluation. The findings not only expose hidden\nbiases in peer feedback, but also challenge conventional assumptions about\nanonymity's role in fairness. As academic publishing grapples with reform,\nthese insights raise critical questions about how review policies shape career\ntrajectories and scientific progress.", "AI": {"tldr": "The study analyzes linguistic biases in peer review across 80,000 reviews, revealing disparities in tone, sentiment, and support based on author demographics and reviewer anonymity.", "motivation": "To uncover hidden biases in peer review language and assess how reviewer anonymity affects fairness.", "method": "Natural language processing and large-scale statistical modeling of reviews from two major journals, comparing anonymous and signed reviews.", "result": "Reveals disparities in review language linked to author demographics and challenges assumptions about anonymity's role in fairness.", "conclusion": "Highlights the need for reform in peer review policies to address biases and promote equitable scientific progress."}}
{"id": "2507.14559", "pdf": "https://arxiv.org/pdf/2507.14559", "abs": "https://arxiv.org/abs/2507.14559", "authors": ["Zixuan Hu", "Xiaotong Li", "Shixiang Tang", "Jun Liu", "Yichun Hu", "Ling-Yu Duan"], "title": "LEAD: Exploring Logit Space Evolution for Model Selection", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2024", "summary": "The remarkable success of pretrain-then-finetune paradigm has led to a\nproliferation of available pre-trained models for vision tasks. This surge\npresents a significant challenge in efficiently choosing the most suitable\npre-trained models for downstream tasks. The critical aspect of this challenge\nlies in effectively predicting the model transferability by considering the\nunderlying fine-tuning dynamics. Existing methods often model fine-tuning\ndynamics in feature space with linear transformations, which do not precisely\nalign with the fine-tuning objective and fail to grasp the essential\nnonlinearity from optimization. To this end, we present LEAD, a\nfinetuning-aligned approach based on the network output of logits. LEAD\nproposes a theoretical framework to model the optimization process and derives\nan ordinary differential equation (ODE) to depict the nonlinear evolution\ntoward the final logit state. Additionally, we design a class-aware\ndecomposition method to consider the varying evolution dynamics across classes\nand further ensure practical applicability. Integrating the closely aligned\noptimization objective and nonlinear modeling capabilities derived from the\ndifferential equation, our method offers a concise solution to effectively\nbridge the optimization gap in a single step, bypassing the lengthy fine-tuning\nprocess. The comprehensive experiments on 24 supervised and self-supervised\npre-trained models across 10 downstream datasets demonstrate impressive\nperformances and showcase its broad adaptability even in low-data scenarios.", "AI": {"tldr": "LEAD introduces a finetuning-aligned approach using logits to model transferability of pre-trained models, outperforming linear methods by capturing nonlinear optimization dynamics.", "motivation": "The challenge of selecting suitable pre-trained models for downstream tasks due to the inefficiency of existing linear methods in modeling fine-tuning dynamics.", "method": "LEAD models optimization via an ODE framework for nonlinear logit evolution and uses class-aware decomposition for practical applicability.", "result": "Outperforms existing methods on 24 pre-trained models across 10 datasets, even in low-data scenarios.", "conclusion": "LEAD effectively bridges the optimization gap in model transferability, offering a concise and adaptable solution."}}
{"id": "2507.14749", "pdf": "https://arxiv.org/pdf/2507.14749", "abs": "https://arxiv.org/abs/2507.14749", "authors": ["Wai Keen Vong", "Brenden M. Lake"], "title": "On the robustness of modeling grounded word learning through a child's egocentric input", "categories": ["cs.CL"], "comment": null, "summary": "What insights can machine learning bring to understanding human language\nacquisition? Large language and multimodal models have achieved remarkable\ncapabilities, but their reliance on massive training datasets creates a\nfundamental mismatch with children, who succeed in acquiring language from\ncomparatively limited input. To help bridge this gap, researchers have\nincreasingly trained neural networks using data similar in quantity and quality\nto children's input. Taking this approach to the limit, Vong et al. (2024)\nshowed that a multimodal neural network trained on 61 hours of visual and\nlinguistic input extracted from just one child's developmental experience could\nacquire word-referent mappings. However, whether this approach's success\nreflects the idiosyncrasies of a single child's experience, or whether it would\nshow consistent and robust learning patterns across multiple children's\nexperiences was not explored. In this article, we applied automated speech\ntranscription methods to the entirety of the SAYCam dataset, consisting of over\n500 hours of video data spread across all three children. Using these automated\ntranscriptions, we generated multi-modal vision-and-language datasets for both\ntraining and evaluation, and explored a range of neural network configurations\nto examine the robustness of simulated word learning. Our findings demonstrate\nthat networks trained on automatically transcribed data from each child can\nacquire and generalize word-referent mappings across multiple network\narchitectures. These results validate the robustness of multimodal neural\nnetworks for grounded word learning, while highlighting the individual\ndifferences that emerge in how models learn when trained on each child's\ndevelopmental experiences.", "AI": {"tldr": "Neural networks trained on limited, child-like input can learn word-referent mappings, showing robustness across architectures but revealing individual learning differences.", "motivation": "To bridge the gap between machine learning models (trained on massive data) and human children (learning from limited input), researchers explored training neural networks on child-like data.", "method": "Automated speech transcription was applied to the SAYCam dataset (500+ hours of video from three children) to create multimodal datasets. Various neural network configurations were tested for word learning.", "result": "Networks successfully learned and generalized word-referent mappings across architectures, validating robustness but showing individual differences in learning patterns.", "conclusion": "Multimodal neural networks can robustly simulate word learning from child-like input, though individual developmental experiences influence learning outcomes."}}
{"id": "2507.14575", "pdf": "https://arxiv.org/pdf/2507.14575", "abs": "https://arxiv.org/abs/2507.14575", "authors": ["Andrea Moschetto", "Lemuel Puglisi", "Alec Sargood", "Pierluigi Dell'Acqua", "Francesco Guarnera", "Sebastiano Battiato", "Daniele Rav\u00ec"], "title": "Benchmarking GANs, Diffusion Models, and Flow Matching for T1w-to-T2w MRI Translation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Magnetic Resonance Imaging (MRI) enables the acquisition of multiple image\ncontrasts, such as T1-weighted (T1w) and T2-weighted (T2w) scans, each offering\ndistinct diagnostic insights. However, acquiring all desired modalities\nincreases scan time and cost, motivating research into computational methods\nfor cross-modal synthesis. To address this, recent approaches aim to synthesize\nmissing MRI contrasts from those already acquired, reducing acquisition time\nwhile preserving diagnostic quality. Image-to-image (I2I) translation provides\na promising framework for this task. In this paper, we present a comprehensive\nbenchmark of generative models$\\unicode{x2013}$specifically, Generative\nAdversarial Networks (GANs), diffusion models, and flow matching (FM)\ntechniques$\\unicode{x2013}$for T1w-to-T2w 2D MRI I2I translation. All\nframeworks are implemented with comparable settings and evaluated on three\npublicly available MRI datasets of healthy adults. Our quantitative and\nqualitative analyses show that the GAN-based Pix2Pix model outperforms\ndiffusion and FM-based methods in terms of structural fidelity, image quality,\nand computational efficiency. Consistent with existing literature, these\nresults suggest that flow-based models are prone to overfitting on small\ndatasets and simpler tasks, and may require more data to match or surpass GAN\nperformance. These findings offer practical guidance for deploying I2I\ntranslation techniques in real-world MRI workflows and highlight promising\ndirections for future research in cross-modal medical image synthesis. Code and\nmodels are publicly available at\nhttps://github.com/AndreaMoschetto/medical-I2I-benchmark.", "AI": {"tldr": "The paper benchmarks GANs, diffusion models, and flow matching for T1w-to-T2w MRI synthesis, finding GAN-based Pix2Pix superior in fidelity, quality, and efficiency.", "motivation": "Reducing MRI scan time and cost by computationally synthesizing missing contrasts from acquired ones.", "method": "Comparative evaluation of GANs, diffusion models, and flow matching for 2D MRI I2I translation on three public datasets.", "result": "GAN-based Pix2Pix outperforms diffusion and FM methods in structural fidelity, image quality, and efficiency.", "conclusion": "GANs are more practical for current MRI workflows, while flow-based models may need more data to compete. Future research should explore cross-modal synthesis further."}}
{"id": "2507.14758", "pdf": "https://arxiv.org/pdf/2507.14758", "abs": "https://arxiv.org/abs/2507.14758", "authors": ["Luyi Ma", "Wanjia Zhang", "Kai Zhao", "Abhishek Kulkarni", "Lalitesh Morishetti", "Anjana Ganesh", "Ashish Ranjan", "Aashika Padmanabhan", "Jianpeng Xu", "Jason Cho", "Praveen Kanumala", "Kaushiki Nag", "Sumit Dutta", "Kamiya Motwani", "Malay Patel", "Evren Korpeoglu", "Sushant Kumar", "Kannan Achan"], "title": "GRACE: Generative Recommendation via Journey-Aware Sparse Attention on Chain-of-Thought Tokenization", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "10 pages, 5 figures, The ACM Conference on Recommender Systems\n  (RecSys) 2025", "summary": "Generative models have recently demonstrated strong potential in\nmulti-behavior recommendation systems, leveraging the expressive power of\ntransformers and tokenization to generate personalized item sequences. However,\ntheir adoption is hindered by (1) the lack of explicit information for token\nreasoning, (2) high computational costs due to quadratic attention complexity\nand dense sequence representations after tokenization, and (3) limited\nmulti-scale modeling over user history. In this work, we propose GRACE\n(Generative Recommendation via journey-aware sparse Attention on\nChain-of-thought tokEnization), a novel generative framework for multi-behavior\nsequential recommendation. GRACE introduces a hybrid Chain-of-Thought (CoT)\ntokenization method that encodes user-item interactions with explicit\nattributes from product knowledge graphs (e.g., category, brand, price) over\nsemantic tokenization, enabling interpretable and behavior-aligned generation.\nTo address the inefficiency of standard attention, we design a Journey-Aware\nSparse Attention (JSA) mechanism, which selectively attends to compressed,\nintra-, inter-, and current-context segments in the tokenized sequence.\nExperiments on two real-world datasets show that GRACE significantly\noutperforms state-of-the-art baselines, achieving up to +106.9% HR@10 and\n+106.7% NDCG@10 improvement over the state-of-the-art baseline on the Home\ndomain, and +22.1% HR@10 on the Electronics domain. GRACE also reduces\nattention computation by up to 48% with long sequences.", "AI": {"tldr": "GRACE improves multi-behavior recommendation by combining Chain-of-Thought tokenization and Journey-Aware Sparse Attention, achieving significant performance gains and computational efficiency.", "motivation": "Current generative models for multi-behavior recommendation lack interpretability, efficiency, and multi-scale modeling.", "method": "GRACE uses hybrid Chain-of-Thought tokenization and Journey-Aware Sparse Attention for efficient, interpretable recommendations.", "result": "GRACE outperforms baselines by up to +106.9% HR@10 and reduces attention computation by 48%.", "conclusion": "GRACE offers a scalable, interpretable solution for multi-behavior sequential recommendation."}}
{"id": "2507.14587", "pdf": "https://arxiv.org/pdf/2507.14587", "abs": "https://arxiv.org/abs/2507.14587", "authors": ["Merjem Be\u0107irovi\u0107", "Amina Kurtovi\u0107", "Nordin Smajlovi\u0107", "Medina Kapo", "Amila Akagi\u0107"], "title": "Performance comparison of medical image classification systems using TensorFlow Keras, PyTorch, and JAX", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Medical imaging plays a vital role in early disease diagnosis and monitoring.\nSpecifically, blood microscopy offers valuable insights into blood cell\nmorphology and the detection of hematological disorders. In recent years, deep\nlearning-based automated classification systems have demonstrated high\npotential in enhancing the accuracy and efficiency of blood image analysis.\nHowever, a detailed performance analysis of specific deep learning frameworks\nappears to be lacking. This paper compares the performance of three popular\ndeep learning frameworks, TensorFlow with Keras, PyTorch, and JAX, in\nclassifying blood cell images from the publicly available BloodMNIST dataset.\nThe study primarily focuses on inference time differences, but also\nclassification performance for different image sizes. The results reveal\nvariations in performance across frameworks, influenced by factors such as\nimage resolution and framework-specific optimizations. Classification accuracy\nfor JAX and PyTorch was comparable to current benchmarks, showcasing the\nefficiency of these frameworks for medical image classification.", "AI": {"tldr": "The paper compares TensorFlow, PyTorch, and JAX for blood cell image classification, analyzing inference time and accuracy, with JAX and PyTorch performing well.", "motivation": "To evaluate and compare the performance of deep learning frameworks (TensorFlow, PyTorch, JAX) in blood cell image classification due to a lack of detailed analysis.", "method": "Comparison of TensorFlow, PyTorch, and JAX on the BloodMNIST dataset, focusing on inference time and classification performance across image sizes.", "result": "JAX and PyTorch showed comparable accuracy to benchmarks, with performance variations due to image resolution and framework optimizations.", "conclusion": "JAX and PyTorch are efficient for medical image classification, with framework choice impacting performance based on task specifics."}}
{"id": "2507.14815", "pdf": "https://arxiv.org/pdf/2507.14815", "abs": "https://arxiv.org/abs/2507.14815", "authors": ["Shoutao Guo", "Shaolei Zhang", "Qingkai Fang", "Zhengrui Ma", "Min Zhang", "Yang Feng"], "title": "FastLongSpeech: Enhancing Large Speech-Language Models for Efficient Long-Speech Processing", "categories": ["cs.CL"], "comment": "The code is at https://github.com/ictnlp/FastLongSpeech. This model\n  is at https://huggingface.co/ICTNLP/FastLongSpeech. The dataset is at\n  https://huggingface.co/datasets/ICTNLP/LongSpeech-Eval", "summary": "The rapid advancement of Large Language Models (LLMs) has spurred significant\nprogress in Large Speech-Language Models (LSLMs), enhancing their capabilities\nin both speech understanding and generation. While existing LSLMs often\nconcentrate on augmenting speech generation or tackling a diverse array of\nshort-speech tasks, the efficient processing of long-form speech remains a\ncritical yet underexplored challenge. This gap is primarily attributed to the\nscarcity of long-speech training datasets and the high computational costs\nassociated with long sequences. To address these limitations, we introduce\nFastLongSpeech, a novel framework designed to extend LSLM capabilities for\nefficient long-speech processing without necessitating dedicated long-speech\ntraining data. FastLongSpeech incorporates an iterative fusion strategy that\ncan compress excessively long-speech sequences into manageable lengths. To\nadapt LSLMs for long-speech inputs, it introduces a dynamic compression\ntraining approach, which exposes the model to short-speech sequences at varying\ncompression ratios, thereby transferring the capabilities of LSLMs to\nlong-speech tasks. To assess the long-speech capabilities of LSLMs, we develop\na long-speech understanding benchmark called LongSpeech-Eval. Experiments show\nthat our method exhibits strong performance in both long-speech and\nshort-speech tasks, while greatly improving inference efficiency.", "AI": {"tldr": "FastLongSpeech is a framework for efficient long-speech processing in LSLMs without needing long-speech training data, using iterative fusion and dynamic compression training.", "motivation": "Existing LSLMs lack efficiency in long-speech processing due to data scarcity and high computational costs.", "method": "Introduces FastLongSpeech with iterative fusion and dynamic compression training to adapt LSLMs for long-speech tasks.", "result": "Strong performance in long- and short-speech tasks with improved inference efficiency.", "conclusion": "FastLongSpeech effectively bridges the gap in long-speech processing for LSLMs."}}
{"id": "2507.14596", "pdf": "https://arxiv.org/pdf/2507.14596", "abs": "https://arxiv.org/abs/2507.14596", "authors": ["Doriand Petit", "Steve Bourgeois", "Vincent Gay-Bellile", "Florian Chabot", "Lo\u00efc Barthe"], "title": "DiSCO-3D : Discovering and segmenting Sub-Concepts from Open-vocabulary queries in NeRF", "categories": ["cs.CV"], "comment": "Published at ICCV'25", "summary": "3D semantic segmentation provides high-level scene understanding for\napplications in robotics, autonomous systems, \\textit{etc}. Traditional methods\nadapt exclusively to either task-specific goals (open-vocabulary segmentation)\nor scene content (unsupervised semantic segmentation). We propose DiSCO-3D, the\nfirst method addressing the broader problem of 3D Open-Vocabulary Sub-concepts\nDiscovery, which aims to provide a 3D semantic segmentation that adapts to both\nthe scene and user queries. We build DiSCO-3D on Neural Fields representations,\ncombining unsupervised segmentation with weak open-vocabulary guidance. Our\nevaluations demonstrate that DiSCO-3D achieves effective performance in\nOpen-Vocabulary Sub-concepts Discovery and exhibits state-of-the-art results in\nthe edge cases of both open-vocabulary and unsupervised segmentation.", "AI": {"tldr": "DiSCO-3D is a novel method for 3D Open-Vocabulary Sub-concepts Discovery, combining unsupervised segmentation with weak open-vocabulary guidance to adapt to both scene content and user queries.", "motivation": "Traditional methods are limited to either task-specific goals or scene content, lacking adaptability to both. DiSCO-3D aims to bridge this gap.", "method": "Built on Neural Fields representations, DiSCO-3D integrates unsupervised segmentation with weak open-vocabulary guidance.", "result": "DiSCO-3D achieves effective performance in Open-Vocabulary Sub-concepts Discovery and state-of-the-art results in edge cases of open-vocabulary and unsupervised segmentation.", "conclusion": "DiSCO-3D successfully addresses the broader problem of 3D semantic segmentation by adapting to both scene and user queries, outperforming traditional methods."}}
{"id": "2507.14819", "pdf": "https://arxiv.org/pdf/2507.14819", "abs": "https://arxiv.org/abs/2507.14819", "authors": ["Akriti Jain", "Pritika Ramu", "Aparna Garimella", "Apoorv Saxena"], "title": "Doc2Chart: Intent-Driven Zero-Shot Chart Generation from Documents", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated strong capabilities in\ntransforming text descriptions or tables to data visualizations via\ninstruction-tuning methods. However, it is not straightforward to apply these\nmethods directly for a more real-world use case of visualizing data from long\ndocuments based on user-given intents, as opposed to the user pre-selecting the\nrelevant content manually. We introduce the task of intent-based chart\ngeneration from documents: given a user-specified intent and document(s), the\ngoal is to generate a chart adhering to the intent and grounded on the\ndocument(s) in a zero-shot setting. We propose an unsupervised, two-staged\nframework in which an LLM first extracts relevant information from the\ndocument(s) by decomposing the intent and iteratively validates and refines\nthis data. Next, a heuristic-guided module selects an appropriate chart type\nbefore final code generation. To assess the data accuracy of the generated\ncharts, we propose an attribution-based metric that uses a structured textual\nrepresentation of charts, instead of relying on visual decoding metrics that\noften fail to capture the chart data effectively. To validate our approach, we\ncurate a dataset comprising of 1,242 $<$intent, document, charts$>$ tuples from\ntwo domains, finance and scientific, in contrast to the existing datasets that\nare largely limited to parallel text descriptions/ tables and their\ncorresponding charts. We compare our approach with baselines using single-shot\nchart generation using LLMs and query-based retrieval methods; our method\noutperforms by upto $9$ points and $17$ points in terms of chart data accuracy\nand chart type respectively over the best baselines.", "AI": {"tldr": "The paper introduces intent-based chart generation from documents using a two-staged LLM framework, outperforming baselines in accuracy and chart type selection.", "motivation": "Existing methods for text-to-chart generation require manual content selection, limiting real-world applicability. The paper addresses this by automating chart generation from long documents based on user intents.", "method": "An unsupervised, two-staged framework: (1) LLM extracts and refines data from documents by decomposing intents, (2) heuristic-guided module selects chart type before code generation. An attribution-based metric evaluates data accuracy.", "result": "The method outperforms baselines by up to 9 points in data accuracy and 17 points in chart type selection, validated on a curated dataset of 1,242 tuples across finance and scientific domains.", "conclusion": "The proposed framework effectively automates intent-based chart generation from documents, improving accuracy and adaptability over existing methods."}}
{"id": "2507.14608", "pdf": "https://arxiv.org/pdf/2507.14608", "abs": "https://arxiv.org/abs/2507.14608", "authors": ["Nandani Sharma", "Dinesh Singh"], "title": "Exp-Graph: How Connections Learn Facial Attributes in Graph-based Expression Recognition", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Facial expression recognition is crucial for human-computer interaction\napplications such as face animation, video surveillance, affective computing,\nmedical analysis, etc. Since the structure of facial attributes varies with\nfacial expressions, incorporating structural information into facial attributes\nis essential for facial expression recognition. In this paper, we propose\nExp-Graph, a novel framework designed to represent the structural relationships\namong facial attributes using graph-based modeling for facial expression\nrecognition. For facial attributes graph representation, facial landmarks are\nused as the graph's vertices. At the same time, the edges are determined based\non the proximity of the facial landmark and the similarity of the local\nappearance of the facial attributes encoded using the vision transformer.\nAdditionally, graph convolutional networks are utilized to capture and\nintegrate these structural dependencies into the encoding of facial attributes,\nthereby enhancing the accuracy of expression recognition. Thus, Exp-Graph\nlearns from the facial attribute graphs highly expressive semantic\nrepresentations. On the other hand, the vision transformer and graph\nconvolutional blocks help the framework exploit the local and global\ndependencies among the facial attributes that are essential for the recognition\nof facial expressions. We conducted comprehensive evaluations of the proposed\nExp-Graph model on three benchmark datasets: Oulu-CASIA, eNTERFACE05, and AFEW.\nThe model achieved recognition accuracies of 98.09\\%, 79.01\\%, and 56.39\\%,\nrespectively. These results indicate that Exp-Graph maintains strong\ngeneralization capabilities across both controlled laboratory settings and\nreal-world, unconstrained environments, underscoring its effectiveness for\npractical facial expression recognition applications.", "AI": {"tldr": "Exp-Graph is a graph-based framework for facial expression recognition, combining facial landmarks with vision transformers and graph convolutional networks to improve accuracy.", "motivation": "Facial expression recognition is vital for applications like human-computer interaction and affective computing. Structural information of facial attributes is key for accurate recognition.", "method": "The framework uses facial landmarks as graph vertices, proximity and appearance similarity for edges, and integrates vision transformers with graph convolutional networks to capture dependencies.", "result": "Exp-Graph achieved high accuracies on benchmark datasets: 98.09% (Oulu-CASIA), 79.01% (eNTERFACE05), and 56.39% (AFEW).", "conclusion": "Exp-Graph demonstrates strong generalization across controlled and real-world settings, proving its effectiveness for practical facial expression recognition."}}
{"id": "2507.14849", "pdf": "https://arxiv.org/pdf/2507.14849", "abs": "https://arxiv.org/abs/2507.14849", "authors": ["Yifei Wang"], "title": "Beyond Isolated Capabilities: Bridging Long CoT Reasoning and Long-Context Understanding", "categories": ["cs.CL"], "comment": null, "summary": "Reasoning distillation has emerged as an effective approach to enhance the\nreasoning capabilities of smaller language models. However, the impact of\nlarge-scale reasoning distillation on other critical abilities, particularly\nin-context retrieval and reasoning, remains unexplored. This gap in\nunderstanding is particularly significant given the increasing importance of\nRetrieval-Augmented Generation (RAG) systems, where efficient acquisition and\nutilization of contextual information are paramount for generating reliable\nresponses. Motivated by the need to understand how the extended long-CoT\nprocess influences long-context comprehension, we conduct a comprehensive\ninvestigation using a series of open-source models distilled from Deepseek-R1,\nrenowned for its exceptional reasoning capabilities. Our study focuses on\nevaluating these models' performance in extracting and integrating relevant\ninformation from extended contexts through multi-document question and\nanswering tasks. Through rigorous experimentation, we demonstrate that\ndistilled reasoning patterns significantly improve long-context understanding.\nOur analysis reveals that distillation fosters greater long-context awareness\nby promoting more detailed and explicit reasoning processes during context\nanalysis and information parsing. This advancement effectively mitigates the\npersistent \"lost in the middle\" issue that has hindered long-context models.", "AI": {"tldr": "Reasoning distillation improves smaller models' long-context understanding, mitigating the 'lost in the middle' issue in Retrieval-Augmented Generation (RAG) systems.", "motivation": "To explore how large-scale reasoning distillation affects in-context retrieval and reasoning, given the growing importance of RAG systems.", "method": "Comprehensive investigation using open-source models distilled from Deepseek-R1, evaluated through multi-document QA tasks.", "result": "Distilled reasoning patterns enhance long-context comprehension by promoting detailed reasoning during context analysis.", "conclusion": "Reasoning distillation significantly improves long-context awareness and mitigates common issues in long-context models."}}
{"id": "2507.14613", "pdf": "https://arxiv.org/pdf/2507.14613", "abs": "https://arxiv.org/abs/2507.14613", "authors": ["Guoping Xu", "Christopher Kabat", "You Zhang"], "title": "Depthwise-Dilated Convolutional Adapters for Medical Object Tracking and Segmentation Using the Segment Anything Model 2", "categories": ["cs.CV"], "comment": "24 pages, 6 figures", "summary": "Recent advances in medical image segmentation have been driven by deep\nlearning; however, most existing methods remain limited by modality-specific\ndesigns and exhibit poor adaptability to dynamic medical imaging scenarios. The\nSegment Anything Model 2 (SAM2) and its related variants, which introduce a\nstreaming memory mechanism for real-time video segmentation, present new\nopportunities for prompt-based, generalizable solutions. Nevertheless, adapting\nthese models to medical video scenarios typically requires large-scale datasets\nfor retraining or transfer learning, leading to high computational costs and\nthe risk of catastrophic forgetting. To address these challenges, we propose\nDD-SAM2, an efficient adaptation framework for SAM2 that incorporates a\nDepthwise-Dilated Adapter (DD-Adapter) to enhance multi-scale feature\nextraction with minimal parameter overhead. This design enables effective\nfine-tuning of SAM2 on medical videos with limited training data. Unlike\nexisting adapter-based methods focused solely on static images, DD-SAM2 fully\nexploits SAM2's streaming memory for medical video object tracking and\nsegmentation. Comprehensive evaluations on TrackRad2025 (tumor segmentation)\nand EchoNet-Dynamic (left ventricle tracking) datasets demonstrate superior\nperformance, achieving Dice scores of 0.93 and 0.97, respectively. To the best\nof our knowledge, this work provides an initial attempt at systematically\nexploring adapter-based SAM2 fine-tuning for medical video segmentation and\ntracking. Code, datasets, and models will be publicly available at\nhttps://github.com/apple1986/DD-SAM2.", "AI": {"tldr": "DD-SAM2 is an efficient adaptation framework for SAM2, enhancing medical video segmentation with minimal parameter overhead, achieving high Dice scores on tumor and left ventricle datasets.", "motivation": "Existing medical image segmentation methods lack adaptability to dynamic scenarios and require large datasets for retraining, leading to high costs and risks.", "method": "Proposes DD-SAM2 with a Depthwise-Dilated Adapter for multi-scale feature extraction, enabling fine-tuning on limited medical video data.", "result": "Achieves Dice scores of 0.93 (TrackRad2025) and 0.97 (EchoNet-Dynamic), outperforming existing methods.", "conclusion": "DD-SAM2 successfully adapts SAM2 for medical video segmentation and tracking, offering a scalable solution with public resources."}}
{"id": "2507.14871", "pdf": "https://arxiv.org/pdf/2507.14871", "abs": "https://arxiv.org/abs/2507.14871", "authors": ["Ronit D. Gross", "Yarden Tzach", "Tal Halevi", "Ella Koresh", "Ido Kanter"], "title": "Tiny language models", "categories": ["cs.CL"], "comment": "23 pages, 1 figure and 12 tables", "summary": "A prominent achievement of natural language processing (NLP) is its ability\nto understand and generate meaningful human language. This capability relies on\ncomplex feedforward transformer block architectures pre-trained on large\nlanguage models (LLMs). However, LLM pre-training is currently feasible only\nfor a few dominant companies due to the immense computational resources\nrequired, limiting broader research participation. This creates a critical need\nfor more accessible alternatives. In this study, we explore whether tiny\nlanguage models (TLMs) exhibit the same key qualitative features of LLMs. We\ndemonstrate that TLMs exhibit a clear performance gap between pre-trained and\nnon-pre-trained models across classification tasks, indicating the\neffectiveness of pre-training, even at a tiny scale. The performance gap\nincreases with the size of the pre-training dataset and with greater overlap\nbetween tokens in the pre-training and classification datasets. Furthermore,\nthe classification accuracy achieved by a pre-trained deep TLM architecture can\nbe replicated through a soft committee of multiple, independently pre-trained\nshallow architectures, enabling low-latency TLMs without affecting\nclassification accuracy. Our results are based on pre-training BERT-6 and\nvariants of BERT-1 on subsets of the Wikipedia dataset and evaluating their\nperformance on FewRel, AGNews, and DBPedia classification tasks. Future\nresearch on TLM is expected to further illuminate the mechanisms underlying\nNLP, especially given that its biologically inspired models suggest that TLMs\nmay be sufficient for children or adolescents to develop language.", "AI": {"tldr": "The study investigates whether tiny language models (TLMs) mimic key features of large language models (LLMs), showing pre-training effectiveness even at small scales. TLMs' performance improves with dataset size and token overlap, and shallow architectures can match deep ones in accuracy.", "motivation": "The high computational cost of LLM pre-training restricts research participation, necessitating more accessible alternatives like TLMs.", "method": "Pre-training BERT-6 and BERT-1 variants on Wikipedia subsets and evaluating on FewRel, AGNews, and DBPedia tasks.", "result": "Pre-trained TLMs outperform non-pre-trained ones, with performance scaling with dataset size and token overlap. Shallow architectures can replicate deep TLM accuracy.", "conclusion": "TLMs offer a viable, resource-efficient alternative to LLMs, with potential insights into NLP mechanisms and human language development."}}
{"id": "2507.14632", "pdf": "https://arxiv.org/pdf/2507.14632", "abs": "https://arxiv.org/abs/2507.14632", "authors": ["Haiquan Wen", "Tianxiao Li", "Zhenglin Huang", "Yiwei He", "Guangliang Cheng"], "title": "BusterX++: Towards Unified Cross-Modal AI-Generated Content Detection and Explanation with MLLM", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in generative AI have dramatically improved image and video\nsynthesis capabilities, significantly increasing the risk of misinformation\nthrough sophisticated fake content. In response, detection methods have evolved\nfrom traditional approaches to multimodal large language models (MLLMs),\noffering enhanced transparency and interpretability in identifying synthetic\nmedia. However, current detection systems remain fundamentally limited by their\nsingle-modality design. These approaches analyze images or videos separately,\nmaking them ineffective against synthetic content that combines multiple media\nformats. To address these challenges, we introduce \\textbf{BusterX++}, a novel\nframework designed specifically for cross-modal detection and explanation of\nsynthetic media. Our approach incorporates an advanced reinforcement learning\n(RL) post-training strategy that eliminates cold-start. Through Multi-stage\nTraining, Thinking Reward, and Hybrid Reasoning, BusterX++ achieves stable and\nsubstantial performance improvements. To enable comprehensive evaluation, we\nalso present \\textbf{GenBuster++}, a cross-modal benchmark leveraging\nstate-of-the-art image and video generation techniques. This benchmark\ncomprises 4,000 images and video clips, meticulously curated by human experts\nusing a novel filtering methodology to ensure high quality, diversity, and\nreal-world applicability. Extensive experiments demonstrate the effectiveness\nand generalizability of our approach.", "AI": {"tldr": "BusterX++ is a novel framework for cross-modal detection of synthetic media, using reinforcement learning and hybrid reasoning to outperform single-modality methods.", "motivation": "The rise of generative AI has increased misinformation risks, but current detection systems are limited by single-modality designs, failing against multi-format synthetic content.", "method": "BusterX++ employs reinforcement learning post-training (Multi-stage Training, Thinking Reward, Hybrid Reasoning) to enhance detection. GenBuster++ benchmark (4,000 curated images/videos) supports evaluation.", "result": "BusterX++ achieves stable and significant performance improvements, demonstrating effectiveness and generalizability in cross-modal synthetic media detection.", "conclusion": "BusterX++ addresses limitations of single-modality detection, offering a robust solution for identifying and explaining synthetic media across formats."}}
{"id": "2507.14887", "pdf": "https://arxiv.org/pdf/2507.14887", "abs": "https://arxiv.org/abs/2507.14887", "authors": ["Shiyi Mu", "Yongkang Liu", "Shi Feng", "Xiaocui Yang", "Daling Wang", "Yifei Zhang"], "title": "MEKiT: Multi-source Heterogeneous Knowledge Injection Method via Instruction Tuning for Emotion-Cause Pair Extraction", "categories": ["cs.CL"], "comment": "Accepted by CogSci", "summary": "Although large language models (LLMs) excel in text comprehension and\ngeneration, their performance on the Emotion-Cause Pair Extraction (ECPE) task,\nwhich requires reasoning ability, is often underperform smaller language model.\nThe main reason is the lack of auxiliary knowledge, which limits LLMs' ability\nto effectively perceive emotions and reason causes. To address this issue, we\npropose a novel \\textbf{M}ulti-source h\\textbf{E}terogeneous \\textbf{K}nowledge\n\\textbf{i}njection me\\textbf{T}hod, MEKiT, which integrates heterogeneous\ninternal emotional knowledge and external causal knowledge. Specifically, for\nthese two distinct aspects and structures of knowledge, we apply the approaches\nof incorporating instruction templates and mixing data for instruction-tuning,\nwhich respectively facilitate LLMs in more comprehensively identifying emotion\nand accurately reasoning causes. Experimental results demonstrate that MEKiT\nprovides a more effective and adaptable solution for the ECPE task, exhibiting\nan absolute performance advantage over compared baselines and dramatically\nimproving the performance of LLMs on the ECPE task.", "AI": {"tldr": "MEKiT improves LLMs' performance on Emotion-Cause Pair Extraction by injecting multi-source heterogeneous knowledge, outperforming baselines.", "motivation": "LLMs underperform on ECPE due to lack of auxiliary knowledge, limiting emotion perception and causal reasoning.", "method": "MEKiT integrates internal emotional and external causal knowledge using instruction templates and mixed data for instruction-tuning.", "result": "MEKiT significantly boosts LLMs' performance on ECPE, surpassing baselines.", "conclusion": "MEKiT offers an effective, adaptable solution for ECPE, enhancing LLMs' capabilities in emotion and cause reasoning."}}
{"id": "2507.14643", "pdf": "https://arxiv.org/pdf/2507.14643", "abs": "https://arxiv.org/abs/2507.14643", "authors": ["Jifeng Shen", "Haibo Zhan", "Shaohua Dong", "Xin Zuo", "Wankou Yang", "Haibin Ling"], "title": "Multispectral State-Space Feature Fusion: Bridging Shared and Cross-Parametric Interactions for Object Detection", "categories": ["cs.CV"], "comment": "submitted on 30/4/2025, Under Major Revision", "summary": "Modern multispectral feature fusion for object detection faces two critical\nlimitations: (1) Excessive preference for local complementary features over\ncross-modal shared semantics adversely affects generalization performance; and\n(2) The trade-off between the receptive field size and computational complexity\npresent critical bottlenecks for scalable feature modeling. Addressing these\nissues, a novel Multispectral State-Space Feature Fusion framework, dubbed\nMS2Fusion, is proposed based on the state space model (SSM), achieving\nefficient and effective fusion through a dual-path parametric interaction\nmechanism. More specifically, the first cross-parameter interaction branch\ninherits the advantage of cross-attention in mining complementary information\nwith cross-modal hidden state decoding in SSM. The second shared-parameter\nbranch explores cross-modal alignment with joint embedding to obtain\ncross-modal similar semantic features and structures through parameter sharing\nin SSM. Finally, these two paths are jointly optimized with SSM for fusing\nmultispectral features in a unified framework, allowing our MS2Fusion to enjoy\nboth functional complementarity and shared semantic space. In our extensive\nexperiments on mainstream benchmarks including FLIR, M3FD and LLVIP, our\nMS2Fusion significantly outperforms other state-of-the-art multispectral object\ndetection methods, evidencing its superiority. Moreover, MS2Fusion is general\nand applicable to other multispectral perception tasks. We show that, even\nwithout specific design, MS2Fusion achieves state-of-the-art results on RGB-T\nsemantic segmentation and RGBT salient object detection, showing its\ngenerality. The source code will be available at\nhttps://github.com/61s61min/MS2Fusion.git.", "AI": {"tldr": "MS2Fusion is a novel multispectral feature fusion framework using state space models to address limitations in local feature preference and computational complexity, achieving superior performance in object detection and other tasks.", "motivation": "Addressing the limitations of excessive local feature preference and computational bottlenecks in multispectral feature fusion for object detection.", "method": "Proposes MS2Fusion with a dual-path parametric interaction mechanism: one for cross-modal complementary features and another for shared semantics, both optimized via state space models.", "result": "Outperforms state-of-the-art methods on benchmarks like FLIR, M3FD, and LLVIP, and shows generality in RGB-T semantic segmentation and RGBT salient object detection.", "conclusion": "MS2Fusion effectively balances complementary and shared features, demonstrating scalability and superior performance across multispectral tasks."}}
{"id": "2507.14894", "pdf": "https://arxiv.org/pdf/2507.14894", "abs": "https://arxiv.org/abs/2507.14894", "authors": ["Boyi Deng", "Yu Wan", "Baosong Yang", "Fei Huang", "Wenjie Wang", "Fuli Feng"], "title": "Sparse Autoencoder-guided Supervised Finetuning to Mitigate Unexpected Code-Switching in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have impressive multilingual capabilities, but\nthey suffer from unexpected code-switching, also known as language mixing,\nwhich involves switching to unexpected languages in the model response. This\nproblem leads to poor readability and degrades the usability of model\nresponses. However, existing work on this issue lacks a mechanistic analysis\nand shows limited effectiveness. In this paper, we first provide an in-depth\nanalysis of unexpected code-switching using sparse autoencoders and find that\nwhen LLMs switch to a language, the features of that language exhibit excessive\npre-activation values. Based on our findings, we propose $\\textbf{S}$parse\n$\\textbf{A}$utoencoder-guided $\\textbf{S}$upervised\n$\\textbf{F}$ine$\\textbf{t}$uning (SASFT), which teaches LLMs to maintain\nappropriate pre-activation values of specific language features during\ntraining. Experiments on five models across three languages demonstrate that\nSASFT consistently reduces unexpected code-switching by more than 50\\% compared\nto standard supervised fine-tuning, with complete elimination in four cases.\nMoreover, SASFT maintains or even improves the models' performance on six\nmultilingual benchmarks, showing its effectiveness in addressing code-switching\nwhile preserving multilingual capabilities.", "AI": {"tldr": "The paper addresses unexpected code-switching in LLMs, proposes SASFT to reduce it by 50%, and maintains multilingual performance.", "motivation": "LLMs exhibit unexpected code-switching, degrading usability, but existing solutions lack mechanistic analysis and effectiveness.", "method": "Uses sparse autoencoders to analyze code-switching, then introduces SASFT to control language feature pre-activation.", "result": "SASFT reduces code-switching by over 50%, eliminates it in four cases, and maintains benchmark performance.", "conclusion": "SASFT effectively mitigates code-switching while preserving LLMs' multilingual capabilities."}}
{"id": "2507.14657", "pdf": "https://arxiv.org/pdf/2507.14657", "abs": "https://arxiv.org/abs/2507.14657", "authors": ["Keivan Shariatmadar", "Ahmad Osman"], "title": "AI-Powered Precision in Sport Taekwondo: Enhancing Fairness, Speed, and Trust in Competition (FST.ai)", "categories": ["cs.CV", "cs.AI", "68T45", "I.2.10"], "comment": "24 pages, 9 figures", "summary": "The integration of Artificial Intelligence (AI) into sports officiating\nrepresents a paradigm shift in how decisions are made in competitive\nenvironments. Traditional manual systems, even when supported by Instant Video\nReplay (IVR), often suffer from latency, subjectivity, and inconsistent\nenforcement, undermining fairness and athlete trust. This paper introduces\nFST.ai, a novel AI-powered framework designed to enhance officiating in Sport\nTaekwondo, particularly focusing on the complex task of real-time head kick\ndetection and scoring. Leveraging computer vision, deep learning, and edge\ninference, the system automates the identification and classification of key\nactions, significantly reducing decision time from minutes to seconds while\nimproving consistency and transparency. Importantly, the methodology is not\nlimited to Taekwondo. The underlying framework -- based on pose estimation,\nmotion classification, and impact analysis -- can be adapted to a wide range of\nsports requiring action detection, such as judo, karate, fencing, or even team\nsports like football and basketball, where foul recognition or performance\ntracking is critical. By addressing one of Taekwondo's most challenging\nscenarios -- head kick scoring -- we demonstrate the robustness, scalability,\nand sport-agnostic potential of FST.ai to transform officiating standards\nacross multiple disciplines.", "AI": {"tldr": "FST.ai is an AI framework for real-time head kick detection in Taekwondo, reducing decision time and improving fairness. Its adaptable design can extend to other sports.", "motivation": "Traditional sports officiating suffers from latency, subjectivity, and inconsistency, undermining fairness. AI can address these issues.", "method": "Uses computer vision, deep learning, and edge inference for pose estimation, motion classification, and impact analysis.", "result": "Reduces decision time from minutes to seconds, enhances consistency, and is adaptable to other sports.", "conclusion": "FST.ai demonstrates robustness and scalability, with potential to revolutionize officiating across multiple sports."}}
{"id": "2507.14900", "pdf": "https://arxiv.org/pdf/2507.14900", "abs": "https://arxiv.org/abs/2507.14900", "authors": ["Chongxuan Huang", "Yongshi Ye", "Biao Fu", "Qifeng Su", "Xiaodong Shi"], "title": "From Neurons to Semantics: Evaluating Cross-Linguistic Alignment Capabilities of Large Language Models via Neurons Alignment", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable multilingual\ncapabilities, however, how to evaluate cross-lingual alignment remains\nunderexplored. Existing alignment benchmarks primarily focus on sentence\nembeddings, but prior research has shown that neural models tend to induce a\nnon-smooth representation space, which impact of semantic alignment evaluation\non low-resource languages. Inspired by neuroscientific findings that similar\ninformation activates overlapping neuronal regions, we propose a novel Neuron\nState-Based Cross-Lingual Alignment (NeuronXA) to assess the cross-lingual a\nlignment capabilities of LLMs, which offers a more semantically grounded\napproach to assess cross-lingual alignment. We evaluate NeuronXA on several\nprominent multilingual LLMs (LLaMA, Qwen, Mistral, GLM, and OLMo) across two\ntransfer tasks and three multilingual benchmarks. The results demonstrate that\nwith only 100 parallel sentence pairs, NeuronXA achieves a Pearson correlation\nof 0.9556 with downstream tasks performance and 0.8514 with transferability.\nThese findings demonstrate NeuronXA's effectiveness in assessing both\ncross-lingual alignment and transferability, even with a small dataset. This\nhighlights its potential to advance cross-lingual alignment research and to\nimprove the semantic understanding of multilingual LLMs.", "AI": {"tldr": "NeuronXA is a novel method for evaluating cross-lingual alignment in LLMs, achieving high correlation with downstream tasks and transferability using minimal data.", "motivation": "Existing benchmarks for cross-lingual alignment focus on sentence embeddings, which may not capture semantic alignment well, especially for low-resource languages.", "method": "Proposes NeuronXA, a neuron state-based approach inspired by neuroscientific findings, to assess cross-lingual alignment in LLMs.", "result": "NeuronXA achieves Pearson correlations of 0.9556 with downstream tasks and 0.8514 with transferability using only 100 parallel sentence pairs.", "conclusion": "NeuronXA effectively evaluates cross-lingual alignment and transferability, advancing research and improving semantic understanding in multilingual LLMs."}}
{"id": "2507.14662", "pdf": "https://arxiv.org/pdf/2507.14662", "abs": "https://arxiv.org/abs/2507.14662", "authors": ["Shayan Rokhva", "Babak Teimourpour"], "title": "Artificial Intelligence in the Food Industry: Food Waste Estimation based on Computer Vision, a Brief Case Study in a University Dining Hall", "categories": ["cs.CV", "cs.AI"], "comment": "Questions & Recommendations: shayanrokhva1999@gmail.com;\n  shayan1999rokh@yahoo.com", "summary": "Quantifying post-consumer food waste in institutional dining settings is\nessential for supporting data-driven sustainability strategies. This study\npresents a cost-effective computer vision framework that estimates plate-level\nfood waste by utilizing semantic segmentation of RGB images taken before and\nafter meal consumption across five Iranian dishes. Four fully supervised models\n(U-Net, U-Net++, and their lightweight variants) were trained using a capped\ndynamic inverse-frequency loss and AdamW optimizer, then evaluated through a\ncomprehensive set of metrics, including Pixel Accuracy, Dice, IoU, and a\ncustom-defined Distributional Pixel Agreement (DPA) metric tailored to the\ntask. All models achieved satisfying performance, and for each food type, at\nleast one model approached or surpassed 90% DPA, demonstrating strong alignment\nin pixel-wise proportion estimates. Lighter models with reduced parameter\ncounts offered faster inference, achieving real-time throughput on an NVIDIA T4\nGPU. Further analysis showed superior segmentation performance for dry and more\nrigid components (e.g., rice and fries), while more complex, fragmented, or\nviscous dishes, such as stews, showed reduced performance, specifically\npost-consumption. Despite limitations such as reliance on 2D imaging,\nconstrained food variety, and manual data collection, the proposed framework is\npioneering and represents a scalable, contactless solution for continuous\nmonitoring of food consumption. This research lays foundational groundwork for\nautomated, real-time waste tracking systems in large-scale food service\nenvironments and offers actionable insights and outlines feasible future\ndirections for dining hall management and policymakers aiming to reduce\ninstitutional food waste.", "AI": {"tldr": "A cost-effective computer vision framework using semantic segmentation to estimate plate-level food waste in institutional dining settings, achieving high accuracy with lightweight models.", "motivation": "Quantifying post-consumer food waste is crucial for data-driven sustainability strategies in dining environments.", "method": "Utilized semantic segmentation of RGB images (before/after meals) with four supervised models (U-Net, U-Net++, lightweight variants), trained using capped dynamic inverse-frequency loss and AdamW optimizer. Evaluated with Pixel Accuracy, Dice, IoU, and custom DPA metrics.", "result": "Models achieved strong performance (\u226590% DPA for some foods), with lighter models enabling real-time inference. Dry/rigid foods segmented better than complex/viscous ones.", "conclusion": "The framework is scalable and pioneering for automated, real-time waste tracking, offering actionable insights for reducing institutional food waste."}}
{"id": "2507.14913", "pdf": "https://arxiv.org/pdf/2507.14913", "abs": "https://arxiv.org/abs/2507.14913", "authors": ["Eliya Habba", "Noam Dahan", "Gili Lior", "Gabriel Stanovsky"], "title": "PromptSuite: A Task-Agnostic Framework for Multi-Prompt Generation", "categories": ["cs.CL"], "comment": "Eliya Habba and Noam Dahan contributed equally to this work", "summary": "Evaluating LLMs with a single prompt has proven unreliable, with small\nchanges leading to significant performance differences. However, generating the\nprompt variations needed for a more robust multi-prompt evaluation is\nchallenging, limiting its adoption in practice. To address this, we introduce\nPromptSuite, a framework that enables the automatic generation of various\nprompts. PromptSuite is flexible - working out of the box on a wide range of\ntasks and benchmarks. It follows a modular prompt design, allowing controlled\nperturbations to each component, and is extensible, supporting the addition of\nnew components and perturbation types. Through a series of case studies, we\nshow that PromptSuite provides meaningful variations to support strong\nevaluation practices. It is available through both a Python API:\nhttps://github.com/eliyahabba/PromptSuite, and a user-friendly web interface:\nhttps://promptsuite.streamlit.app/", "AI": {"tldr": "PromptSuite is a framework for automatic generation of varied prompts to improve LLM evaluation reliability.", "motivation": "Single-prompt evaluations of LLMs are unreliable due to sensitivity to small changes, but creating diverse prompts manually is challenging.", "method": "PromptSuite uses modular prompt design for controlled perturbations, supports extensibility, and works across tasks and benchmarks.", "result": "Case studies demonstrate PromptSuite generates meaningful prompt variations for robust evaluation.", "conclusion": "PromptSuite offers a practical solution for multi-prompt LLM evaluation, available via Python API and web interface."}}
{"id": "2507.14670", "pdf": "https://arxiv.org/pdf/2507.14670", "abs": "https://arxiv.org/abs/2507.14670", "authors": ["Yaxuan Song", "Jianan Fan", "Hang Chang", "Weidong Cai"], "title": "Gene-DML: Dual-Pathway Multi-Level Discrimination for Gene Expression Prediction from Histopathology Images", "categories": ["cs.CV"], "comment": "16 pages, 15 tables, 8 figures", "summary": "Accurately predicting gene expression from histopathology images offers a\nscalable and non-invasive approach to molecular profiling, with significant\nimplications for precision medicine and computational pathology. However,\nexisting methods often underutilize the cross-modal representation alignment\nbetween histopathology images and gene expression profiles across multiple\nrepresentational levels, thereby limiting their prediction performance. To\naddress this, we propose Gene-DML, a unified framework that structures latent\nspace through Dual-pathway Multi-Level discrimination to enhance correspondence\nbetween morphological and transcriptional modalities. The multi-scale\ninstance-level discrimination pathway aligns hierarchical histopathology\nrepresentations extracted at local, neighbor, and global levels with gene\nexpression profiles, capturing scale-aware morphological-transcriptional\nrelationships. In parallel, the cross-level instance-group discrimination\npathway enforces structural consistency between individual (image/gene)\ninstances and modality-crossed (gene/image, respectively) groups, strengthening\nthe alignment across modalities. By jointly modelling fine-grained and\nstructural-level discrimination, Gene-DML is able to learn robust cross-modal\nrepresentations, enhancing both predictive accuracy and generalization across\ndiverse biological contexts. Extensive experiments on public spatial\ntranscriptomics datasets demonstrate that Gene-DML achieves state-of-the-art\nperformance in gene expression prediction. The code and checkpoints will be\nreleased soon.", "AI": {"tldr": "Gene-DML is a framework that improves gene expression prediction from histopathology images by aligning cross-modal representations at multiple levels.", "motivation": "Existing methods underutilize cross-modal alignment between histopathology images and gene expression, limiting prediction performance.", "method": "Gene-DML uses Dual-pathway Multi-Level discrimination: one aligns multi-scale histopathology representations with gene profiles, and the other enforces structural consistency between instances and groups.", "result": "Gene-DML achieves state-of-the-art performance in gene expression prediction on public datasets.", "conclusion": "The framework enhances predictive accuracy and generalization, with potential applications in precision medicine and computational pathology."}}
{"id": "2507.14922", "pdf": "https://arxiv.org/pdf/2507.14922", "abs": "https://arxiv.org/abs/2507.14922", "authors": ["Vahid Rahimzadeh", "Erfan Moosavi Monazzah", "Mohammad Taher Pilehvar", "Yadollah Yaghoobzadeh"], "title": "SYNTHIA: Synthetic Yet Naturally Tailored Human-Inspired PersonAs", "categories": ["cs.CL", "I.2.7"], "comment": null, "summary": "Persona-driven LLMs have emerged as powerful tools in computational social\nscience, yet existing approaches fall at opposite extremes, either relying on\ncostly human-curated data or producing synthetic personas that lack consistency\nand realism. We introduce SYNTHIA, a dataset of 30,000 backstories derived from\n10,000 real social media users from BlueSky open platform across three time\nwindows, bridging this spectrum by grounding synthetic generation in authentic\nuser activity. Our evaluation demonstrates that SYNTHIA achieves competitive\nperformance with state-of-the-art methods in demographic diversity and social\nsurvey alignment while significantly outperforming them in narrative\nconsistency. Uniquely, SYNTHIA incorporates temporal dimensionality and\nprovides rich social interaction metadata from the underlying network, enabling\nnew research directions in computational social science and persona-driven\nlanguage modeling.", "AI": {"tldr": "SYNTHIA is a dataset of 30,000 backstories from 10,000 real BlueSky users, combining synthetic generation with real data for improved consistency and realism in persona-driven LLMs.", "motivation": "Existing methods for persona-driven LLMs are either costly (human-curated) or lack realism (synthetic). SYNTHIA bridges this gap by grounding synthetic personas in real user activity.", "method": "SYNTHIA derives backstories from real social media users (BlueSky) across three time windows, incorporating temporal and social interaction metadata.", "result": "SYNTHIA matches state-of-the-art methods in demographic diversity and survey alignment but excels in narrative consistency. It also enables new research via temporal and social metadata.", "conclusion": "SYNTHIA offers a balanced, realistic, and research-enabling approach to persona-driven LLMs, outperforming existing methods in consistency and utility."}}
{"id": "2507.14675", "pdf": "https://arxiv.org/pdf/2507.14675", "abs": "https://arxiv.org/abs/2507.14675", "authors": ["Yuchen Duan", "Zhe Chen", "Yusong Hu", "Weiyun Wang", "Shenglong Ye", "Botian Shi", "Lewei Lu", "Qibin Hou", "Tong Lu", "Hongsheng Li", "Jifeng Dai", "Wenhai Wang"], "title": "Docopilot: Improving Multimodal Models for Document-Level Understanding", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Despite significant progress in multimodal large language models (MLLMs),\ntheir performance on complex, multi-page document comprehension remains\ninadequate, largely due to the lack of high-quality, document-level datasets.\nWhile current retrieval-augmented generation (RAG) methods offer partial\nsolutions, they suffer from issues, such as fragmented retrieval contexts,\nmulti-stage error accumulation, and extra time costs of retrieval. In this\nwork, we present a high-quality document-level dataset, Doc-750K, designed to\nsupport in-depth understanding of multimodal documents. This dataset includes\ndiverse document structures, extensive cross-page dependencies, and real\nquestion-answer pairs derived from the original documents. Building on the\ndataset, we develop a native multimodal model, Docopilot, which can accurately\nhandle document-level dependencies without relying on RAG. Experiments\ndemonstrate that Docopilot achieves superior coherence, accuracy, and\nefficiency in document understanding tasks and multi-turn interactions, setting\na new baseline for document-level multimodal understanding. Data, code, and\nmodels are released at https://github.com/OpenGVLab/Docopilot", "AI": {"tldr": "The paper introduces Doc-750K, a high-quality dataset for multimodal document comprehension, and Docopilot, a native multimodal model that outperforms RAG methods in coherence, accuracy, and efficiency.", "motivation": "Current MLLMs struggle with complex document comprehension due to lack of quality datasets, and RAG methods have limitations like fragmented contexts and error accumulation.", "method": "Developed Doc-750K dataset with diverse structures and cross-page dependencies, and built Docopilot, a native multimodal model avoiding RAG.", "result": "Docopilot achieves superior performance in document understanding tasks and multi-turn interactions.", "conclusion": "Docopilot sets a new baseline for document-level multimodal understanding, with released data, code, and models."}}
{"id": "2507.14958", "pdf": "https://arxiv.org/pdf/2507.14958", "abs": "https://arxiv.org/abs/2507.14958", "authors": ["Hang Yan", "Fangzhi Xu", "Rongman Xu", "Yifei Li", "Jian Zhang", "Haoran Luo", "Xiaobao Wu", "Luu Anh Tuan", "Haiteng Zhao", "Qika Lin", "Jun Liu"], "title": "MUR: Momentum Uncertainty guided Reasoning for Large Language Models", "categories": ["cs.CL"], "comment": "25 pages, 8 figures", "summary": "Large Language Models (LLMs) have achieved impressive performance on\nreasoning-intensive tasks, yet optimizing their reasoning efficiency remains an\nopen challenge. While Test-Time Scaling (TTS) improves reasoning quality, it\noften leads to overthinking, wasting tokens on redundant computations. This\nwork investigates how to efficiently and adaptively guide LLM test-time scaling\nwithout additional training. Inspired by the concept of momentum in physics, we\npropose Momentum Uncertainty-guided Reasoning (MUR), which dynamically\nallocates thinking budgets to critical reasoning steps by tracking and\naggregating stepwise uncertainty over time. To support flexible inference-time\ncontrol, we introduce gamma-control, a simple mechanism that tunes the\nreasoning budget via a single hyperparameter. We provide in-depth theoretical\nproof to support the superiority of MUR in terms of stability and biases. MUR\nis comprehensively evaluated against various TTS methods across four\nchallenging benchmarks (MATH-500, AIME24, AIME25, and GPQA-diamond) using\ndifferent sizes of recent Qwen3 models (1.7B, 4B, and 8B). Results demonstrate\nthat MUR reduces computation by over 50% on average while improving accuracy by\n0.62-3.37%.", "AI": {"tldr": "MUR dynamically allocates thinking budgets to LLMs using momentum-inspired uncertainty tracking, reducing computation by 50% and improving accuracy.", "motivation": "Optimizing reasoning efficiency in LLMs without additional training, addressing overthinking in Test-Time Scaling.", "method": "Proposes Momentum Uncertainty-guided Reasoning (MUR) with gamma-control for adaptive budget allocation.", "result": "Reduces computation by 50% on average and improves accuracy by 0.62-3.37% across benchmarks.", "conclusion": "MUR efficiently enhances LLM reasoning with minimal overhead, validated by theoretical and empirical results."}}
{"id": "2507.14680", "pdf": "https://arxiv.org/pdf/2507.14680", "abs": "https://arxiv.org/abs/2507.14680", "authors": ["Xinheng Lyu", "Yuci Liang", "Wenting Chen", "Meidan Ding", "Jiaqi Yang", "Guolin Huang", "Daokun Zhang", "Xiangjian He", "Linlin Shen"], "title": "WSI-Agents: A Collaborative Multi-Agent System for Multi-Modal Whole Slide Image Analysis", "categories": ["cs.CV", "cs.AI", "68T07, 92C55", "I.2.7; I.4.8; J.3"], "comment": null, "summary": "Whole slide images (WSIs) are vital in digital pathology, enabling gigapixel\ntissue analysis across various pathological tasks. While recent advancements in\nmulti-modal large language models (MLLMs) allow multi-task WSI analysis through\nnatural language, they often underperform compared to task-specific models.\nCollaborative multi-agent systems have emerged as a promising solution to\nbalance versatility and accuracy in healthcare, yet their potential remains\nunderexplored in pathology-specific domains. To address these issues, we\npropose WSI-Agents, a novel collaborative multi-agent system for multi-modal\nWSI analysis. WSI-Agents integrates specialized functional agents with robust\ntask allocation and verification mechanisms to enhance both task-specific\naccuracy and multi-task versatility through three components: (1) a task\nallocation module assigning tasks to expert agents using a model zoo of patch\nand WSI level MLLMs, (2) a verification mechanism ensuring accuracy through\ninternal consistency checks and external validation using pathology knowledge\nbases and domain-specific models, and (3) a summary module synthesizing the\nfinal summary with visual interpretation maps. Extensive experiments on\nmulti-modal WSI benchmarks show WSI-Agents's superiority to current WSI MLLMs\nand medical agent frameworks across diverse tasks.", "AI": {"tldr": "WSI-Agents is a collaborative multi-agent system for multi-modal WSI analysis, improving accuracy and versatility over existing methods.", "motivation": "Address underperformance of multi-modal large language models (MLLMs) in WSI analysis and unexplored potential of multi-agent systems in pathology.", "method": "Integrates specialized agents with task allocation, verification, and summary modules for multi-task WSI analysis.", "result": "Outperforms current WSI MLLMs and medical agent frameworks in diverse tasks.", "conclusion": "WSI-Agents offers a promising solution for accurate and versatile WSI analysis."}}
{"id": "2507.15024", "pdf": "https://arxiv.org/pdf/2507.15024", "abs": "https://arxiv.org/abs/2507.15024", "authors": ["Qiaoyu Tang", "Hao Xiang", "Le Yu", "Bowen Yu", "Hongyu Lin", "Yaojie Lu", "Xianpei Han", "Le Sun", "Junyang Lin"], "title": "RefCritic: Training Long Chain-of-Thought Critic Models with Refinement Feedback", "categories": ["cs.CL"], "comment": null, "summary": "With the rapid advancement of Large Language Models (LLMs), developing\neffective critic modules for precise guidance has become crucial yet\nchallenging. In this paper, we initially demonstrate that supervised\nfine-tuning for building critic modules (which is widely adopted in current\nsolutions) fails to genuinely enhance models' critique abilities, producing\nsuperficial critiques with insufficient reflections and verifications. To\nunlock the unprecedented critique capabilities, we propose RefCritic, a\nlong-chain-of-thought critic module based on reinforcement learning with dual\nrule-based rewards: (1) instance-level correctness of solution judgments and\n(2) refinement accuracies of the policy model based on critiques, aiming to\ngenerate high-quality evaluations with actionable feedback that effectively\nguides model refinement. We evaluate RefCritic on Qwen2.5-14B-Instruct and\nDeepSeek-R1-Distill-Qwen-14B across five benchmarks. On critique and refinement\nsettings, RefCritic demonstrates consistent advantages across all benchmarks,\ne.g., 6.8\\% and 7.2\\% gains on AIME25 for the respective base models. Notably,\nunder majority voting, policy models filtered by RefCritic show superior\nscaling with increased voting numbers. Moreover, despite training on\nsolution-level supervision, RefCritic outperforms step-level supervised\napproaches on ProcessBench, a benchmark to identify erroneous steps in\nmathematical reasoning.", "AI": {"tldr": "RefCritic, a reinforcement learning-based critic module, outperforms supervised fine-tuning methods by generating actionable critiques, improving model refinement across benchmarks.", "motivation": "Current supervised fine-tuning for critic modules fails to enhance critique abilities, producing superficial feedback. RefCritic aims to unlock superior critique capabilities.", "method": "RefCritic uses reinforcement learning with dual rule-based rewards: correctness of judgments and refinement accuracies, to generate high-quality critiques.", "result": "RefCritic shows consistent gains (e.g., 6.8% and 7.2% on AIME25) and outperforms step-level supervised approaches on ProcessBench.", "conclusion": "RefCritic effectively enhances critique quality and model refinement, demonstrating scalability and superiority over existing methods."}}
{"id": "2507.14686", "pdf": "https://arxiv.org/pdf/2507.14686", "abs": "https://arxiv.org/abs/2507.14686", "authors": ["Chen Cai", "Tianyi Liu", "Jianjun Gao", "Wenyang Liu", "Kejun Wu", "Ruoyu Wang", "Yi Wang", "Soo Chin Liew"], "title": "From Semantics, Scene to Instance-awareness: Distilling Foundation Model for Open-vocabulary Situation Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Recent Multimodal Large Language Models (MLLMs) exhibit strong zero-shot\nabilities but struggle with complex Grounded Situation Recognition (GSR) and\nare resource-intensive for edge device deployment. Meanwhile, conventional GSR\nmodels often lack generalization ability, falling short in recognizing unseen\nand rare situations. In this paper, we exploit transferring knowledge from a\nteacher MLLM to a small GSR model to enhance its generalization and zero-shot\nabilities, thereby introducing the task of Open-vocabulary Grounded Situation\nRecognition (Ov-GSR). To achieve this, we propose Multimodal Interactive Prompt\nDistillation (MIPD), a novel framework that distills enriched multimodal\nknowledge from the foundation model, enabling the student Ov-GSR model to\nrecognize unseen situations and be better aware of rare situations.\nSpecifically, the MIPD framework first leverages the LLM-based Judgmental\nRationales Generator (JRG) to construct positive and negative glimpse and gaze\nrationales enriched with contextual semantic information. The proposed\nscene-aware and instance-perception prompts are then introduced to align\nrationales with visual information from the MLLM teacher via the\nNegative-Guided Multimodal Prompting Alignment (NMPA) module, effectively\ncapturing holistic and perceptual multimodal knowledge. Finally, the aligned\nmultimodal knowledge is distilled into the student Ov-GSR model, providing a\nstronger foundation for generalization that enhances situation understanding,\nbridges the gap between seen and unseen scenarios, and mitigates prediction\nbias in rare cases. We evaluate MIPD on the refined Ov-SWiG dataset, achieving\nsuperior performance on seen, rare, and unseen situations, and further\ndemonstrate improved unseen detection on the HICO-DET dataset.", "AI": {"tldr": "The paper introduces Open-vocabulary Grounded Situation Recognition (Ov-GSR) and proposes Multimodal Interactive Prompt Distillation (MIPD) to transfer knowledge from a teacher MLLM to a smaller GSR model, improving generalization and zero-shot abilities.", "motivation": "Current MLLMs struggle with complex GSR and are resource-heavy, while conventional GSR models lack generalization for unseen and rare situations.", "method": "MIPD uses a Judgmental Rationales Generator (JRG) and Negative-Guided Multimodal Prompting Alignment (NMPA) to distill enriched multimodal knowledge from a teacher MLLM into a student Ov-GSR model.", "result": "MIPD achieves superior performance on seen, rare, and unseen situations in the Ov-SWiG dataset and improves unseen detection on HICO-DET.", "conclusion": "MIPD effectively enhances generalization, bridges seen-unseen gaps, and mitigates bias in rare cases, offering a practical solution for GSR."}}
{"id": "2507.15061", "pdf": "https://arxiv.org/pdf/2507.15061", "abs": "https://arxiv.org/abs/2507.15061", "authors": ["Zhengwei Tao", "Jialong Wu", "Wenbiao Yin", "Junkai Zhang", "Baixuan Li", "Haiyang Shen", "Kuan Li", "Liwen Zhang", "Xinyu Wang", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Jingren Zhou"], "title": "WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The advent of Large Language Model (LLM)-powered agents has revolutionized\nartificial intelligence by enabling solutions to complex, open-ended tasks\nthrough web-based information-seeking (IS) capabilities. The scarcity of\nhigh-quality training data has limited the development of IS agents. Existing\napproaches typically adopt an information-driven paradigm that first collects\nweb data and then generates questions based on the retrieval. However, this may\nlead to inconsistency between information structure and reasoning structure,\nquestion and answer. To mitigate, we propose a formalization-driven IS data\nsynthesis framework WebShaper to construct a dataset. WebShaper systematically\nformalizes IS tasks through set theory. Central to the formalization is the\nconcept of Knowledge Projections (KP), which enables precise control over\nreasoning structure by KP operation compositions. During synthesis, we begin by\ncreating seed tasks, then use a multi-step expansion process. At each step, an\nagentic Expander expands the current formal question more complex with\nretrieval and validation tools based on our formalization. We train our model\non the synthesized dataset. Experiment results demonstrate that WebShaper\nachieves state-of-the-art performance among open-sourced IS agents on GAIA and\nWebWalkerQA benchmarks.", "AI": {"tldr": "WebShaper introduces a formalization-driven framework for synthesizing high-quality training data for information-seeking (IS) agents, improving consistency and performance.", "motivation": "The scarcity of high-quality training data limits IS agent development, and existing methods often lead to inconsistencies between information and reasoning structures.", "method": "WebShaper formalizes IS tasks using set theory and Knowledge Projections (KP), then synthesizes data through a multi-step expansion process with retrieval and validation tools.", "result": "WebShaper achieves state-of-the-art performance on GAIA and WebWalkerQA benchmarks.", "conclusion": "The framework effectively addresses data scarcity and inconsistency, enhancing IS agent capabilities."}}
{"id": "2507.14697", "pdf": "https://arxiv.org/pdf/2507.14697", "abs": "https://arxiv.org/abs/2507.14697", "authors": ["Zhiwei Zhang", "Zi Ye", "Yibin Wen", "Shuai Yuan", "Haohuan Fu", "Jianxi Huang", "Juepeng Zheng"], "title": "GTPBD: A Fine-Grained Global Terraced Parcel and Boundary Dataset", "categories": ["cs.CV", "I.4.6; I.2.10"], "comment": "38 pages, 18 figures, submitted to NeurIPS 2025", "summary": "Agricultural parcels serve as basic units for conducting agricultural\npractices and applications, which is vital for land ownership registration,\nfood security assessment, soil erosion monitoring, etc. However, existing\nagriculture parcel extraction studies only focus on mid-resolution mapping or\nregular plain farmlands while lacking representation of complex terraced\nterrains due to the demands of precision agriculture.In this paper, we\nintroduce a more fine-grained terraced parcel dataset named GTPBD (Global\nTerraced Parcel and Boundary Dataset), which is the first fine-grained dataset\ncovering major worldwide terraced regions with more than 200,000 complex\nterraced parcels with manual annotation. GTPBD comprises 47,537 high-resolution\nimages with three-level labels, including pixel-level boundary labels, mask\nlabels, and parcel labels. It covers seven major geographic zones in China and\ntranscontinental climatic regions around the world.Compared to the existing\ndatasets, the GTPBD dataset brings considerable challenges due to the: (1)\nterrain diversity; (2) complex and irregular parcel objects; and (3) multiple\ndomain styles. Our proposed GTPBD dataset is suitable for four different tasks,\nincluding semantic segmentation, edge detection, terraced parcel extraction,\nand unsupervised domain adaptation (UDA) tasks.Accordingly, we benchmark the\nGTPBD dataset on eight semantic segmentation methods, four edge extraction\nmethods, three parcel extraction methods, and five UDA methods, along with a\nmulti-dimensional evaluation framework integrating pixel-level and object-level\nmetrics. GTPBD fills a critical gap in terraced remote sensing research,\nproviding a basic infrastructure for fine-grained agricultural terrain analysis\nand cross-scenario knowledge transfer.", "AI": {"tldr": "The paper introduces GTPBD, a fine-grained terraced parcel dataset for complex terrains, addressing gaps in existing datasets. It supports multiple tasks like semantic segmentation and domain adaptation.", "motivation": "Existing datasets lack representation of complex terraced terrains, limiting precision agriculture research. GTPBD aims to fill this gap.", "method": "GTPBD includes 47,537 high-resolution images with three-level labels (boundary, mask, parcel) across diverse terrains and climates. It benchmarks various methods for tasks like segmentation and edge detection.", "result": "GTPBD outperforms existing datasets, offering challenges like terrain diversity and complex parcel objects. It supports four tasks and integrates multi-dimensional evaluation.", "conclusion": "GTPBD is a foundational resource for fine-grained agricultural terrain analysis and cross-scenario knowledge transfer in remote sensing."}}
{"id": "2507.15087", "pdf": "https://arxiv.org/pdf/2507.15087", "abs": "https://arxiv.org/abs/2507.15087", "authors": ["Chenlei Gong", "Yuanhe Tian", "Lei Mao", "Yan Song"], "title": "Evaluation of Coding Schemes for Transformer-based Gene Sequence Modeling", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Currently, many studies view DNA sequences as a special type of language and\nutilize Transformers to model them. These studies use fixed-length k-mer\nsegmentation and BPE subword tokenization but lack a systematic evaluation to\ndetermine which is superior. We compare k-mer segmentation with k=1,3,4,5,6, a\n4,096-token BPE vocabulary, and three positional encoding methods-sinusoidal,\nAliBi, and RoPE. Each configuration is trained from scratch in 3, 6, 12, and\n24-layer Transformer encoders and evaluated on GUE benchmark dataset. In\ngeneral, BPE delivers higher and more stable performance across tasks by\ncompressing frequent motifs into variable-length tokens, reducing sequence\nlength, and improving model generalization. RoPE excels at capturing periodic\nmotifs and extrapolating to long sequences, while AliBi also performs well on\ntasks driven by local dependencies. In terms of depth, we observe significant\ngains when increasing layers from 3 to 12, with only marginal improvements or\nslight overfitting at 24 layers. This study provides practical guidance for\ndesigning tokenization and positional encoding in DNA Transformer models.", "AI": {"tldr": "The paper compares k-mer segmentation and BPE tokenization for DNA sequence modeling, evaluating performance across different configurations and positional encodings. BPE outperforms k-mer, and RoPE excels for periodic motifs, while AliBi handles local dependencies well. Model depth shows diminishing returns beyond 12 layers.", "motivation": "To systematically evaluate the effectiveness of k-mer segmentation versus BPE tokenization and different positional encoding methods in DNA sequence modeling using Transformers.", "method": "Compared k-mer (k=1,3,4,5,6) and BPE tokenization with three positional encodings (sinusoidal, AliBi, RoPE) in 3, 6, 12, and 24-layer Transformer encoders, evaluated on the GUE benchmark dataset.", "result": "BPE performs better and more stably, compressing motifs and reducing sequence length. RoPE captures periodic motifs well, while AliBi excels in local dependencies. Depth gains plateau after 12 layers.", "conclusion": "BPE and RoPE are recommended for DNA Transformer models, with optimal depth around 12 layers for performance and efficiency."}}
{"id": "2507.14738", "pdf": "https://arxiv.org/pdf/2507.14738", "abs": "https://arxiv.org/abs/2507.14738", "authors": ["Jeannie She", "Katie Spivakovsky"], "title": "MultiRetNet: A Multimodal Vision Model and Deferral System for Staging Diabetic Retinopathy", "categories": ["cs.CV"], "comment": null, "summary": "Diabetic retinopathy (DR) is a leading cause of preventable blindness,\naffecting over 100 million people worldwide. In the United States, individuals\nfrom lower-income communities face a higher risk of progressing to advanced\nstages before diagnosis, largely due to limited access to screening. Comorbid\nconditions further accelerate disease progression. We propose MultiRetNet, a\nnovel pipeline combining retinal imaging, socioeconomic factors, and\ncomorbidity profiles to improve DR staging accuracy, integrated with a clinical\ndeferral system for a clinical human-in-the-loop implementation. We experiment\nwith three multimodal fusion methods and identify fusion through a fully\nconnected layer as the most versatile methodology. We synthesize adversarial,\nlow-quality images and use contrastive learning to train the deferral system,\nguiding the model to identify out-of-distribution samples that warrant\nclinician review. By maintaining diagnostic accuracy on suboptimal images and\nintegrating critical health data, our system can improve early detection,\nparticularly in underserved populations where advanced DR is often first\nidentified. This approach may reduce healthcare costs, increase early detection\nrates, and address disparities in access to care, promoting healthcare equity.", "AI": {"tldr": "MultiRetNet improves diabetic retinopathy staging by combining retinal imaging, socioeconomic data, and comorbidities, using multimodal fusion and a deferral system for clinician review.", "motivation": "Diabetic retinopathy disproportionately affects lower-income communities due to limited screening access, necessitating better early detection methods.", "method": "Proposes MultiRetNet, integrating retinal imaging, socioeconomic factors, and comorbidities with three fusion methods and a deferral system trained via contrastive learning.", "result": "Fully connected layer fusion is most versatile; the system maintains accuracy on suboptimal images and identifies cases needing clinician review.", "conclusion": "MultiRetNet enhances early detection in underserved populations, potentially reducing costs and healthcare disparities."}}
{"id": "2507.15092", "pdf": "https://arxiv.org/pdf/2507.15092", "abs": "https://arxiv.org/abs/2507.15092", "authors": ["Vijeta Deshpande", "Ishita Dasgupta", "Uttaran Bhattacharya", "Somdeb Sarkhel", "Saayan Mitra", "Anna Rumshisky"], "title": "A Penalty Goes a Long Way: Measuring Lexical Diversity in Synthetic Texts Under Prompt-Influenced Length Variations", "categories": ["cs.CL"], "comment": null, "summary": "Synthetic text generated by Large Language Models (LLMs) is increasingly used\nfor further training and improvement of LLMs. Diversity is crucial for the\neffectiveness of synthetic data, and researchers rely on prompt engineering to\nimprove diversity. However, the impact of prompt variations on response text\nlength, and, more importantly, the consequential effect on lexical diversity\nmeasurements, remain underexplored. In this work, we propose Penalty-Adjusted\nType-Token Ratio (PATTR), a diversity metric robust to length variations. We\ngenerate a large synthetic corpus of over 20M words using seven models from the\nLLaMA, OLMo, and Phi families, focusing on a creative writing task of video\nscript generation, where diversity is crucial. We evaluate per-response lexical\ndiversity using PATTR and compare it against existing metrics of Moving-Average\nTTR (MATTR) and Compression Ratio (CR). Our analysis highlights how text length\nvariations introduce biases favoring shorter responses. Unlike existing\nmetrics, PATTR explicitly considers the task-specific target response length\n($L_T$) to effectively mitigate length biases. We further demonstrate the\nutility of PATTR in filtering the top-10/100/1,000 most lexically diverse\nresponses, showing that it consistently outperforms MATTR and CR by yielding on\npar or better diversity with high adherence to $L_T$.", "AI": {"tldr": "The paper introduces PATTR, a new diversity metric for synthetic text, addressing biases in existing metrics caused by text length variations.", "motivation": "Existing diversity metrics for synthetic text are biased by text length variations, impacting their reliability.", "method": "Proposes Penalty-Adjusted Type-Token Ratio (PATTR), tested on a 20M-word synthetic corpus from LLaMA, OLMo, and Phi models for video script generation.", "result": "PATTR outperforms MATTR and CR by mitigating length biases and maintaining diversity while adhering to target response length.", "conclusion": "PATTR is a robust diversity metric for synthetic text, especially in tasks where length variations are common."}}
{"id": "2507.14743", "pdf": "https://arxiv.org/pdf/2507.14743", "abs": "https://arxiv.org/abs/2507.14743", "authors": ["Joseph Raj Vishal", "Rutuja Patil", "Manas Srinivas Gowda", "Katha Naik", "Yezhou Yang", "Bharatesh Chakravarthi"], "title": "InterAct-Video: Reasoning-Rich Video QA for Urban Traffic", "categories": ["cs.CV"], "comment": null, "summary": "Traffic monitoring is crucial for urban mobility, road safety, and\nintelligent transportation systems (ITS). Deep learning has advanced\nvideo-based traffic monitoring through video question answering (VideoQA)\nmodels, enabling structured insight extraction from traffic videos. However,\nexisting VideoQA models struggle with the complexity of real-world traffic\nscenes, where multiple concurrent events unfold across spatiotemporal\ndimensions. To address these challenges, this paper introduces \\textbf{InterAct\nVideoQA}, a curated dataset designed to benchmark and enhance VideoQA models\nfor traffic monitoring tasks. The InterAct VideoQA dataset comprises 8 hours of\nreal-world traffic footage collected from diverse intersections, segmented into\n10-second video clips, with over 25,000 question-answer (QA) pairs covering\nspatiotemporal dynamics, vehicle interactions, incident detection, and other\ncritical traffic attributes. State-of-the-art VideoQA models are evaluated on\nInterAct VideoQA, exposing challenges in reasoning over fine-grained\nspatiotemporal dependencies within complex traffic scenarios. Additionally,\nfine-tuning these models on InterAct VideoQA yields notable performance\nimprovements, demonstrating the necessity of domain-specific datasets for\nVideoQA. InterAct VideoQA is publicly available as a benchmark dataset to\nfacilitate future research in real-world deployable VideoQA models for\nintelligent transportation systems. GitHub Repo:\nhttps://github.com/joe-rabbit/InterAct_VideoQA", "AI": {"tldr": "The paper introduces InterAct VideoQA, a dataset for benchmarking VideoQA models in traffic monitoring, addressing challenges in real-world traffic scenes.", "motivation": "Existing VideoQA models struggle with complex real-world traffic scenarios, necessitating a domain-specific dataset for improvement.", "method": "The InterAct VideoQA dataset includes 8 hours of real-world traffic footage with 25,000 QA pairs, covering spatiotemporal dynamics and vehicle interactions.", "result": "Evaluation shows challenges in fine-grained reasoning, but fine-tuning on InterAct VideoQA improves model performance.", "conclusion": "InterAct VideoQA is a valuable benchmark for advancing VideoQA models in intelligent transportation systems."}}
{"id": "2507.15100", "pdf": "https://arxiv.org/pdf/2507.15100", "abs": "https://arxiv.org/abs/2507.15100", "authors": ["Chathuri Jayaweera", "Brianna Yanqui", "Bonnie Dorr"], "title": "Filling the Gap: Is Commonsense Knowledge Generation useful for Natural Language Inference?", "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, 8 figures and 5 tables", "summary": "Natural Language Inference (NLI) is the task of determining the semantic\nentailment of a premise for a given hypothesis. The task aims to develop\nsystems that emulate natural human inferential processes where commonsense\nknowledge plays a major role. However, existing commonsense resources lack\nsufficient coverage for a variety of premise-hypothesis pairs. This study\nexplores the potential of Large Language Models as commonsense knowledge\ngenerators for NLI along two key dimensions: their reliability in generating\nsuch knowledge and the impact of that knowledge on prediction accuracy. We\nadapt and modify existing metrics to assess LLM factuality and consistency in\ngenerating in this context. While explicitly incorporating commonsense\nknowledge does not consistently improve overall results, it effectively helps\ndistinguish entailing instances and moderately improves distinguishing\ncontradictory and neutral inferences.", "AI": {"tldr": "The paper explores using Large Language Models (LLMs) as commonsense knowledge generators for Natural Language Inference (NLI), evaluating their reliability and impact on prediction accuracy.", "motivation": "Existing commonsense resources lack coverage for diverse premise-hypothesis pairs, prompting the use of LLMs to fill this gap.", "method": "The study adapts metrics to assess LLM factuality and consistency in generating commonsense knowledge for NLI.", "result": "Incorporating commonsense knowledge doesn't consistently improve overall results but helps distinguish entailing instances and moderately aids in contradictory and neutral inferences.", "conclusion": "LLMs show potential as commonsense knowledge generators for NLI, though their impact varies across inference types."}}
{"id": "2507.14784", "pdf": "https://arxiv.org/pdf/2507.14784", "abs": "https://arxiv.org/abs/2507.14784", "authors": ["Xinxin Dong", "Baoyun Peng", "Haokai Ma", "Yufei Wang", "Zixuan Dong", "Fei Hu", "Xiaodong Wang"], "title": "LeAdQA: LLM-Driven Context-Aware Temporal Grounding for Video Question Answering", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Video Question Answering (VideoQA) requires identifying sparse critical\nmoments in long videos and reasoning about their causal relationships to answer\nsemantically complex questions. While recent advances in multimodal learning\nhave improved alignment and fusion, current approaches remain limited by two\nprevalent but fundamentally flawed strategies: (1) task-agnostic sampling\nindiscriminately processes all frames, overwhelming key events with irrelevant\ncontent; and (2) heuristic retrieval captures superficial patterns but misses\ncausal-temporal structures needed for complex reasoning. To address these\nchallenges, we introduce LeAdQA, an innovative approach that bridges these gaps\nthrough synergizing causal-aware query refinement with fine-grained visual\ngrounding. Our method first leverages LLMs to reformulate question-option\npairs, resolving causal ambiguities and sharpening temporal focus. These\nrefined queries subsequently direct a temporal grounding model to precisely\nretrieve the most salient segments, complemented by an adaptive fusion\nmechanism dynamically integrating the evidence to maximize relevance. The\nintegrated visual-textual cues are then processed by an MLLM to generate\naccurate, contextually-grounded answers. Experiments on NExT-QA, IntentQA, and\nNExT-GQA demonstrate that our method's precise visual grounding substantially\nenhances the understanding of video-question relationships, achieving\nstate-of-the-art (SOTA) performance on complex reasoning tasks while\nmaintaining computational efficiency.", "AI": {"tldr": "LeAdQA improves VideoQA by refining queries with LLMs and precise visual grounding, achieving SOTA performance.", "motivation": "Current VideoQA methods struggle with irrelevant frame processing and lack causal-temporal reasoning.", "method": "LeAdQA uses LLMs for query refinement, temporal grounding for segment retrieval, and adaptive fusion for evidence integration.", "result": "Achieves SOTA on NExT-QA, IntentQA, and NExT-GQA, enhancing video-question understanding.", "conclusion": "LeAdQA addresses key limitations in VideoQA, improving accuracy and efficiency for complex reasoning."}}
{"id": "2507.15114", "pdf": "https://arxiv.org/pdf/2507.15114", "abs": "https://arxiv.org/abs/2507.15114", "authors": ["Chathuri Jayaweera", "Bonnie Dorr"], "title": "From Disagreement to Understanding: The Case for Ambiguity Detection in NLI", "categories": ["cs.CL"], "comment": "8 pages, 6 figures", "summary": "This position paper argues that annotation disagreement in Natural Language\nInference (NLI) is not mere noise but often reflects meaningful interpretive\nvariation, especially when triggered by ambiguity in the premise or hypothesis.\nWhile underspecified guidelines and annotator behavior can contribute to\nvariation, content-based ambiguity offers a process-independent signal of\ndivergent human perspectives. We call for a shift toward ambiguity-aware NLI by\nsystematically identifying ambiguous input pairs and classifying ambiguity\ntypes. To support this, we present a unified framework that integrates existing\ntaxonomies and illustrate key ambiguity subtypes through concrete examples.\nThese examples reveal how ambiguity shapes annotator decisions and motivate the\nneed for targeted detection methods that better align models with human\ninterpretation. A key limitation is the lack of datasets annotated for\nambiguity and subtypes. We propose addressing this gap through new annotated\nresources and unsupervised approaches to ambiguity detection -- paving the way\nfor more robust, explainable, and human-aligned NLI systems.", "AI": {"tldr": "The paper argues that annotation disagreement in NLI stems from meaningful interpretive variation due to ambiguity, not just noise. It proposes an ambiguity-aware NLI framework and highlights the need for datasets annotated for ambiguity.", "motivation": "To address the overlooked role of ambiguity in NLI annotation disagreement and advocate for models that better align with human interpretation.", "method": "Introduces a unified framework integrating existing taxonomies, classifies ambiguity subtypes, and proposes new annotated resources and unsupervised detection methods.", "result": "Demonstrates how ambiguity influences annotator decisions and identifies the lack of ambiguity-annotated datasets as a key limitation.", "conclusion": "Calls for ambiguity-aware NLI systems, emphasizing the need for better datasets and detection methods to improve model robustness and alignment with human interpretation."}}
{"id": "2507.14787", "pdf": "https://arxiv.org/pdf/2507.14787", "abs": "https://arxiv.org/abs/2507.14787", "authors": ["Xi Xiao", "Aristeidis Tsaris", "Anika Tabassum", "John Lagergren", "Larry M. York", "Tianyang Wang", "Xiao Wang"], "title": "FOCUS: Fused Observation of Channels for Unveiling Spectra", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Hyperspectral imaging (HSI) captures hundreds of narrow, contiguous\nwavelength bands, making it a powerful tool in biology, agriculture, and\nenvironmental monitoring. However, interpreting Vision Transformers (ViTs) in\nthis setting remains largely unexplored due to two key challenges: (1) existing\nsaliency methods struggle to capture meaningful spectral cues, often collapsing\nattention onto the class token, and (2) full-spectrum ViTs are computationally\nprohibitive for interpretability, given the high-dimensional nature of HSI\ndata. We present FOCUS, the first framework that enables reliable and efficient\nspatial-spectral interpretability for frozen ViTs. FOCUS introduces two core\ncomponents: class-specific spectral prompts that guide attention toward\nsemantically meaningful wavelength groups, and a learnable [SINK] token trained\nwith an attraction loss to absorb noisy or redundant attention. Together, these\ndesigns make it possible to generate stable and interpretable 3D saliency maps\nand spectral importance curves in a single forward pass, without any gradient\nbackpropagation or backbone modification. FOCUS improves band-level IoU by 15\npercent, reduces attention collapse by over 40 percent, and produces saliency\nresults that align closely with expert annotations. With less than 1 percent\nparameter overhead, our method makes high-resolution ViT interpretability\npractical for real-world hyperspectral applications, bridging a long-standing\ngap between black-box modeling and trustworthy HSI decision-making.", "AI": {"tldr": "FOCUS enables efficient spatial-spectral interpretability for Vision Transformers (ViTs) in hyperspectral imaging (HSI) by addressing attention collapse and computational challenges.", "motivation": "Existing saliency methods fail to capture meaningful spectral cues in HSI, and full-spectrum ViTs are computationally prohibitive for interpretability.", "method": "FOCUS introduces class-specific spectral prompts and a learnable [SINK] token to guide attention and absorb noise, enabling stable 3D saliency maps without gradient backpropagation.", "result": "FOCUS improves band-level IoU by 15%, reduces attention collapse by 40%, and aligns saliency results with expert annotations.", "conclusion": "FOCUS bridges the gap between black-box ViTs and trustworthy HSI decision-making with minimal parameter overhead."}}
{"id": "2507.15142", "pdf": "https://arxiv.org/pdf/2507.15142", "abs": "https://arxiv.org/abs/2507.15142", "authors": ["Hellina Hailu Nigatu", "Atnafu Lambebo Tonja", "Henok Biadglign Ademtew", "Hizkel Mitiku Alemayehu", "Negasi Haile Abadi", "Tadesse Destaw Belay", "Seid Muhie Yimam"], "title": "A Case Against Implicit Standards: Homophone Normalization in Machine Translation for Languages that use the Ge'ez Script", "categories": ["cs.CL", "cs.AI"], "comment": "Paper under review", "summary": "Homophone normalization, where characters that have the same sound in a\nwriting script are mapped to one character, is a pre-processing step applied in\nAmharic Natural Language Processing (NLP) literature. While this may improve\nperformance reported by automatic metrics, it also results in models that are\nnot able to understand different forms of writing in a single language.\nFurther, there might be impacts in transfer learning, where models trained on\nnormalized data do not generalize well to other languages. In this paper, we\nexperiment with monolingual training and cross-lingual transfer to understand\nthe impacts of normalization on languages that use the Ge'ez script. We then\npropose a post-inference intervention in which normalization is applied to\nmodel predictions instead of training data. With our simple scheme of\npost-inference normalization, we show that we can achieve an increase in BLEU\nscore of up to 1.03 while preserving language features in training. Our work\ncontributes to the broader discussion on technology-facilitated language change\nand calls for more language-aware interventions.", "AI": {"tldr": "The paper examines the impact of homophone normalization in Amharic NLP, proposing post-inference normalization to improve BLEU scores while preserving language features.", "motivation": "To address the drawbacks of homophone normalization in NLP, such as reduced model generalization and loss of language diversity.", "method": "Experiments with monolingual training and cross-lingual transfer, followed by post-inference normalization of model predictions.", "result": "Achieved a BLEU score increase of up to 1.03 while maintaining language features.", "conclusion": "Advocates for language-aware interventions and highlights the role of technology in language change."}}
{"id": "2507.14790", "pdf": "https://arxiv.org/pdf/2507.14790", "abs": "https://arxiv.org/abs/2507.14790", "authors": ["Wenbo Yue", "Chang Li", "Guoping Xu"], "title": "A Novel Downsampling Strategy Based on Information Complementarity for Medical Image Segmentation", "categories": ["cs.CV"], "comment": "6 pages, 6 figures", "summary": "In convolutional neural networks (CNNs), downsampling operations are crucial\nto model performance. Although traditional downsampling methods (such as\nmaximum pooling and cross-row convolution) perform well in feature aggregation,\nreceptive field expansion, and computational reduction, they may lead to the\nloss of key spatial information in semantic segmentation tasks, thereby\naffecting the pixel-by-pixel prediction accuracy.To this end, this study\nproposes a downsampling method based on information complementarity - Hybrid\nPooling Downsampling (HPD). The core is to replace the traditional method with\nMinMaxPooling, and effectively retain the light and dark contrast and detail\nfeatures of the image by extracting the maximum value information of the local\narea.Experiment on various CNN architectures on the ACDC and Synapse datasets\nshow that HPD outperforms traditional methods in segmentation performance, and\nincreases the DSC coefficient by 0.5% on average. The results show that the HPD\nmodule provides an efficient solution for semantic segmentation tasks.", "AI": {"tldr": "The paper proposes Hybrid Pooling Downsampling (HPD), a method to improve semantic segmentation by retaining spatial information during downsampling, outperforming traditional methods.", "motivation": "Traditional downsampling methods in CNNs lose key spatial information, affecting pixel-by-pixel prediction accuracy in semantic segmentation.", "method": "HPD replaces traditional downsampling with MinMaxPooling to retain image contrast and detail features by extracting local maximum values.", "result": "Experiments on ACDC and Synapse datasets show HPD improves segmentation performance, increasing the DSC coefficient by 0.5% on average.", "conclusion": "HPD provides an efficient solution for semantic segmentation tasks by better preserving spatial information."}}
{"id": "2507.15152", "pdf": "https://arxiv.org/pdf/2507.15152", "abs": "https://arxiv.org/abs/2507.15152", "authors": ["Lingbo Li", "Anuradha Mathrani", "Teo Susnjak"], "title": "What Level of Automation is \"Good Enough\"? A Benchmark of Large Language Models for Meta-Analysis Data Extraction", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Automating data extraction from full-text randomised controlled trials (RCTs)\nfor meta-analysis remains a significant challenge. This study evaluates the\npractical performance of three LLMs (Gemini-2.0-flash, Grok-3, GPT-4o-mini)\nacross tasks involving statistical results, risk-of-bias assessments, and\nstudy-level characteristics in three medical domains: hypertension, diabetes,\nand orthopaedics. We tested four distinct prompting strategies (basic\nprompting, self-reflective prompting, model ensemble, and customised prompts)\nto determine how to improve extraction quality. All models demonstrate high\nprecision but consistently suffer from poor recall by omitting key information.\nWe found that customised prompts were the most effective, boosting recall by up\nto 15\\%. Based on this analysis, we propose a three-tiered set of guidelines\nfor using LLMs in data extraction, matching data types to appropriate levels of\nautomation based on task complexity and risk. Our study offers practical advice\nfor automating data extraction in real-world meta-analyses, balancing LLM\nefficiency with expert oversight through targeted, task-specific automation.", "AI": {"tldr": "The study evaluates three LLMs (Gemini-2.0-flash, Grok-3, GPT-4o-mini) for automating data extraction from RCTs, focusing on statistical results, risk-of-bias assessments, and study-level characteristics. Customised prompts improved recall by 15%, leading to proposed guidelines for balancing automation and expert oversight.", "motivation": "Automating data extraction from RCTs for meta-analysis is challenging, requiring evaluation of LLMs' performance to improve efficiency and accuracy.", "method": "Tested three LLMs across four prompting strategies (basic, self-reflective, ensemble, customised) in medical domains (hypertension, diabetes, orthopaedics).", "result": "High precision but poor recall; customised prompts boosted recall by 15%.", "conclusion": "Proposed guidelines for task-specific automation, balancing LLM efficiency with expert oversight for real-world meta-analyses."}}
{"id": "2507.14797", "pdf": "https://arxiv.org/pdf/2507.14797", "abs": "https://arxiv.org/abs/2507.14797", "authors": ["Beier Zhu", "Ruoyu Wang", "Tong Zhao", "Hanwang Zhang", "Chi Zhang"], "title": "Distilling Parallel Gradients for Fast ODE Solvers of Diffusion Models", "categories": ["cs.CV"], "comment": "To appear in ICCV 2025", "summary": "Diffusion models (DMs) have achieved state-of-the-art generative performance\nbut suffer from high sampling latency due to their sequential denoising nature.\nExisting solver-based acceleration methods often face image quality degradation\nunder a low-latency budget. In this paper, we propose the Ensemble Parallel\nDirection solver (dubbed as \\ours), a novel ODE solver that mitigates\ntruncation errors by incorporating multiple parallel gradient evaluations in\neach ODE step. Importantly, since the additional gradient computations are\nindependent, they can be fully parallelized, preserving low-latency sampling.\n  Our method optimizes a small set of learnable parameters in a distillation\nfashion, ensuring minimal training overhead.\n  In addition, our method can serve as a plugin to improve existing ODE\nsamplers. Extensive experiments on various image synthesis benchmarks\ndemonstrate the effectiveness of our \\ours~in achieving high-quality and\nlow-latency sampling. For example, at the same latency level of 5 NFE, EPD\nachieves an FID of 4.47 on CIFAR-10, 7.97 on FFHQ, 8.17 on ImageNet, and 8.26\non LSUN Bedroom, surpassing existing learning-based solvers by a significant\nmargin. Codes are available in https://github.com/BeierZhu/EPD.", "AI": {"tldr": "Proposes Ensemble Parallel Direction (EPD), a novel ODE solver for Diffusion Models (DMs) to reduce sampling latency while maintaining image quality by using parallel gradient evaluations.", "motivation": "DMs suffer from high sampling latency due to sequential denoising; existing acceleration methods degrade image quality under low-latency constraints.", "method": "EPD incorporates multiple parallel gradient evaluations per ODE step, fully parallelizable for low latency. It uses learnable parameters optimized via distillation.", "result": "EPD achieves superior FID scores (e.g., 4.47 on CIFAR-10) at 5 NFE latency, outperforming existing solvers.", "conclusion": "EPD is an effective, pluggable solution for high-quality, low-latency sampling in DMs."}}
{"id": "2507.15198", "pdf": "https://arxiv.org/pdf/2507.15198", "abs": "https://arxiv.org/abs/2507.15198", "authors": ["Xiandong Meng", "Yan Wu", "Yexin Tian", "Xin Hu", "Tianze Kang", "Junliang Du"], "title": "Collaborative Distillation Strategies for Parameter-Efficient Language Model Deployment", "categories": ["cs.CL"], "comment": null, "summary": "This paper addresses the challenges of high computational cost and slow\ninference in deploying large language models. It proposes a distillation\nstrategy guided by multiple teacher models. The method constructs several\nteacher models and integrates their output probability distributions and\nintermediate semantic features. This guides the student model to learn from\nmultiple sources of knowledge. As a result, the student model gains stronger\nlanguage understanding and generation ability while maintaining a small\nparameter size. To achieve this, the paper introduces a weighted output fusion\nmechanism, a feature alignment loss function, and an entropy-driven dynamic\nteacher weighting strategy. These components improve the quality and stability\nof knowledge transfer during distillation. Under multi-teacher guidance, the\nstudent model captures semantic information more effectively and demonstrates\nstrong performance across multiple evaluation metrics. In particular, the\nmethod shows high consistency in expression, generalization ability, and task\nadaptability in tasks such as language modeling, text generation, and\nmulti-task learning. The experiments compare the proposed method with several\nwidely adopted distillation approaches. The results further confirm its overall\nadvantages in perplexity, distillation loss, and generation quality. This study\nprovides a feasible technical path for the efficient compression of large-scale\nlanguage models. It also demonstrates the effectiveness of multi-teacher\ncollaborative mechanisms in complex language modeling tasks.", "AI": {"tldr": "The paper proposes a multi-teacher distillation strategy to reduce computational costs and improve inference speed in large language models, achieving strong performance with a smaller student model.", "motivation": "Addressing the high computational cost and slow inference of large language models by leveraging knowledge from multiple teacher models.", "method": "Uses a distillation strategy with weighted output fusion, feature alignment loss, and dynamic teacher weighting to guide a student model.", "result": "The student model shows improved language understanding, generation ability, and performance across tasks like language modeling and text generation.", "conclusion": "The method offers an efficient way to compress large language models and highlights the effectiveness of multi-teacher collaboration."}}
{"id": "2507.14798", "pdf": "https://arxiv.org/pdf/2507.14798", "abs": "https://arxiv.org/abs/2507.14798", "authors": ["Xinyi Wu", "Steven Landgraf", "Markus Ulrich", "Rongjun Qin"], "title": "An Evaluation of DUSt3R/MASt3R/VGGT 3D Reconstruction on Photogrammetric Aerial Blocks", "categories": ["cs.CV"], "comment": "23 pages, 6 figures, this manuscript has been submitted to\n  Geo-spatial Information Science for consideration", "summary": "State-of-the-art 3D computer vision algorithms continue to advance in\nhandling sparse, unordered image sets. Recently developed foundational models\nfor 3D reconstruction, such as Dense and Unconstrained Stereo 3D Reconstruction\n(DUSt3R), Matching and Stereo 3D Reconstruction (MASt3R), and Visual Geometry\nGrounded Transformer (VGGT), have attracted attention due to their ability to\nhandle very sparse image overlaps. Evaluating DUSt3R/MASt3R/VGGT on typical\naerial images matters, as these models may handle extremely low image overlaps,\nstereo occlusions, and textureless regions. For redundant collections, they can\naccelerate 3D reconstruction by using extremely sparsified image sets. Despite\ntests on various computer vision benchmarks, their potential on photogrammetric\naerial blocks remains unexplored. This paper conducts a comprehensive\nevaluation of the pre-trained DUSt3R/MASt3R/VGGT models on the aerial blocks of\nthe UseGeo dataset for pose estimation and dense 3D reconstruction. Results\nshow these methods can accurately reconstruct dense point clouds from very\nsparse image sets (fewer than 10 images, up to 518 pixels resolution), with\ncompleteness gains up to +50% over COLMAP. VGGT also demonstrates higher\ncomputational efficiency, scalability, and more reliable camera pose\nestimation. However, all exhibit limitations with high-resolution images and\nlarge sets, as pose reliability declines with more images and geometric\ncomplexity. These findings suggest transformer-based methods cannot fully\nreplace traditional SfM and MVS, but offer promise as complementary approaches,\nespecially in challenging, low-resolution, and sparse scenarios.", "AI": {"tldr": "The paper evaluates DUSt3R, MASt3R, and VGGT models on aerial images, showing their effectiveness in sparse scenarios but limitations with high-resolution or large datasets.", "motivation": "To assess the performance of state-of-the-art 3D reconstruction models (DUSt3R, MASt3R, VGGT) on photogrammetric aerial blocks, which remains unexplored despite their success in sparse image sets.", "method": "Comprehensive evaluation of pre-trained DUSt3R, MASt3R, and VGGT models on the UseGeo dataset for pose estimation and dense 3D reconstruction.", "result": "These models accurately reconstruct dense point clouds from very sparse image sets (fewer than 10 images), with VGGT showing higher efficiency and reliability. However, performance declines with high-resolution images and larger datasets.", "conclusion": "Transformer-based methods like DUSt3R, MASt3R, and VGGT are promising for sparse and low-resolution scenarios but cannot fully replace traditional SfM and MVS, serving as complementary approaches."}}
{"id": "2507.15236", "pdf": "https://arxiv.org/pdf/2507.15236", "abs": "https://arxiv.org/abs/2507.15236", "authors": ["Shayan Vassef", "Amirhossein Dabiriaghdam", "Mohammadreza Bakhtiari", "Yadollah Yaghoobzadeh"], "title": "SOI Matters: Analyzing Multi-Setting Training Dynamics in Pretrained Language Models via Subsets of Interest", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "This work investigates the impact of multi-task, multi-lingual, and\nmulti-source learning approaches on the robustness and performance of\npretrained language models. To enhance this analysis, we introduce Subsets of\nInterest (SOI), a novel categorization framework that identifies six distinct\nlearning behavior patterns during training, including forgettable examples,\nunlearned examples, and always correct examples. Through SOI transition\nheatmaps and dataset cartography visualization, we analyze how examples shift\nbetween these categories when transitioning from single-setting to\nmulti-setting configurations. We perform comprehensive experiments across three\nparallel comparisons: multi-task vs. single-task learning using English tasks\n(entailment, paraphrase, sentiment), multi-source vs. single-source learning\nusing sentiment analysis datasets, and multi-lingual vs. single-lingual\nlearning using intent classification in French, English, and Persian. Our\nresults demonstrate that multi-source learning consistently improves\nout-of-distribution performance by up to 7%, while multi-task learning shows\nmixed results with notable gains in similar task combinations. We further\nintroduce a two-stage fine-tuning approach where the second stage leverages\nSOI-based subset selection to achieve additional performance improvements.\nThese findings provide new insights into training dynamics and offer practical\napproaches for optimizing multi-setting language model performance.", "AI": {"tldr": "The paper explores how multi-task, multi-lingual, and multi-source learning affect pretrained language models, introducing a novel framework (SOI) to categorize learning behaviors. Experiments show multi-source learning boosts out-of-distribution performance, while multi-task learning benefits similar tasks. A two-stage fine-tuning method using SOI further enhances results.", "motivation": "To understand and improve the robustness and performance of pretrained language models in multi-setting configurations by analyzing learning behaviors and transitions.", "method": "Introduces Subsets of Interest (SOI) to categorize learning behaviors, uses SOI transition heatmaps and dataset cartography, and conducts experiments comparing multi-task, multi-source, and multi-lingual learning against single-setting baselines.", "result": "Multi-source learning improves out-of-distribution performance by up to 7%, multi-task learning benefits similar tasks, and SOI-based subset selection in two-stage fine-tuning yields additional gains.", "conclusion": "The study offers insights into training dynamics and practical methods for optimizing language models in multi-setting scenarios, with SOI proving valuable for performance enhancement."}}
{"id": "2507.14801", "pdf": "https://arxiv.org/pdf/2507.14801", "abs": "https://arxiv.org/abs/2507.14801", "authors": ["Xiangyu Chen", "Kaiwen Zhu", "Yuandong Pu", "Shuo Cao", "Xiaohui Li", "Wenlong Zhang", "Yihao Liu", "Yu Qiao", "Jiantao Zhou", "Chao Dong"], "title": "Exploring Scalable Unified Modeling for General Low-Level Vision", "categories": ["cs.CV"], "comment": null, "summary": "Low-level vision involves a wide spectrum of tasks, including image\nrestoration, enhancement, stylization, and feature extraction, which differ\nsignificantly in both task formulation and output domains. To address the\nchallenge of unified modeling across such diverse tasks, we propose a Visual\ntask Prompt-based Image Processing (VPIP) framework that leverages input-target\nimage pairs as visual prompts to guide the model in performing a variety of\nlow-level vision tasks. The framework comprises an end-to-end image processing\nbackbone, a prompt encoder, and a prompt interaction module, enabling flexible\nintegration with various architectures and effective utilization of\ntask-specific visual representations. Based on this design, we develop a\nunified low-level vision model, GenLV, and evaluate its performance across\nmultiple representative tasks. To explore the scalability of this approach, we\nextend the framework along two dimensions: model capacity and task diversity.\nWe construct a large-scale benchmark consisting of over 100 low-level vision\ntasks and train multiple versions of the model with varying scales.\nExperimental results show that the proposed method achieves considerable\nperformance across a wide range of tasks. Notably, increasing the number of\ntraining tasks enhances generalization, particularly for tasks with limited\ndata, indicating the model's ability to learn transferable representations\nthrough joint training. Further evaluations in zero-shot generalization,\nfew-shot transfer, and task-specific fine-tuning scenarios demonstrate the\nmodel's strong adaptability, confirming the effectiveness, scalability, and\npotential of the proposed framework as a unified foundation for general\nlow-level vision modeling.", "AI": {"tldr": "The paper proposes a Visual task Prompt-based Image Processing (VPIP) framework for unified modeling of diverse low-level vision tasks, achieving strong performance and adaptability.", "motivation": "Addressing the challenge of unified modeling across diverse low-level vision tasks with varying formulations and output domains.", "method": "Introduces VPIP, an end-to-end framework with a backbone, prompt encoder, and interaction module, leveraging input-target pairs as visual prompts. Develops GenLV, a unified model, and evaluates scalability via model capacity and task diversity.", "result": "Achieves strong performance across 100+ tasks, with improved generalization for limited-data tasks. Demonstrates adaptability in zero-shot, few-shot, and fine-tuning scenarios.", "conclusion": "VPIP is effective, scalable, and adaptable, serving as a unified foundation for general low-level vision modeling."}}
{"id": "2507.15275", "pdf": "https://arxiv.org/pdf/2507.15275", "abs": "https://arxiv.org/abs/2507.15275", "authors": ["Yuanhe Tian", "Junjie Liu", "Zhizhou Kou", "Yuxiang Li", "Yan Song"], "title": "ChiMed 2.0: Advancing Chinese Medical Dataset in Facilitating Large Language Modeling", "categories": ["cs.CL"], "comment": null, "summary": "Building high-quality data resources is crucial for advancing artificial\nintelligence research and applications in specific domains, particularly in the\nChinese medical domain. Existing Chinese medical datasets are limited in size\nand narrow in domain coverage, falling short of the diverse corpora required\nfor effective pre-training. Moreover, most datasets are designed solely for LLM\nfine-tuning and do not support pre-training and reinforcement learning from\nhuman feedback (RLHF). In this paper, we propose a Chinese medical dataset\nnamed ChiMed 2.0, which extends our previous work ChiMed, and covers data\ncollected from Chinese medical online platforms and generated by LLMs. ChiMed\n2.0 contains 204.4M Chinese characters covering both traditional Chinese\nmedicine classics and modern general medical data, where there are 164.8K\ndocuments for pre-training, 351.6K question-answering pairs for supervised\nfine-tuning (SFT), and 41.7K preference data tuples for RLHF. To validate the\neffectiveness of our approach for training a Chinese medical LLM, we conduct\nfurther pre-training, SFT, and RLHF experiments on representative general\ndomain LLMs and evaluate their performance on medical benchmark datasets. The\nresults show performance gains across different model scales, validating the\ndataset's effectiveness and applicability.", "AI": {"tldr": "ChiMed 2.0 is a large-scale Chinese medical dataset designed for pre-training, fine-tuning, and RLHF, addressing gaps in existing resources.", "motivation": "Existing Chinese medical datasets are limited in size and scope, hindering effective AI development in the domain.", "method": "ChiMed 2.0 extends the previous ChiMed dataset, incorporating data from online platforms and LLMs, and includes pre-training, SFT, and RLHF components.", "result": "Experiments show performance gains across model scales, validating the dataset's effectiveness.", "conclusion": "ChiMed 2.0 successfully addresses limitations in existing datasets and supports advanced AI training in the Chinese medical domain."}}
{"id": "2507.14807", "pdf": "https://arxiv.org/pdf/2507.14807", "abs": "https://arxiv.org/abs/2507.14807", "authors": ["Juan Hu", "Shaojing Fan", "Terence Sim"], "title": "Seeing Through Deepfakes: A Human-Inspired Framework for Multi-Face Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multi-face deepfake videos are becoming increasingly prevalent, often\nappearing in natural social settings that challenge existing detection methods.\nMost current approaches excel at single-face detection but struggle in\nmulti-face scenarios, due to a lack of awareness of crucial contextual cues. In\nthis work, we develop a novel approach that leverages human cognition to\nanalyze and defend against multi-face deepfake videos. Through a series of\nhuman studies, we systematically examine how people detect deepfake faces in\nsocial settings. Our quantitative analysis reveals four key cues humans rely\non: scene-motion coherence, inter-face appearance compatibility, interpersonal\ngaze alignment, and face-body consistency. Guided by these insights, we\nintroduce \\textsf{HICOM}, a novel framework designed to detect every fake face\nin multi-face scenarios. Extensive experiments on benchmark datasets show that\n\\textsf{HICOM} improves average accuracy by 3.3\\% in in-dataset detection and\n2.8\\% under real-world perturbations. Moreover, it outperforms existing methods\nby 5.8\\% on unseen datasets, demonstrating the generalization of human-inspired\ncues. \\textsf{HICOM} further enhances interpretability by incorporating an LLM\nto provide human-readable explanations, making detection results more\ntransparent and convincing. Our work sheds light on involving human factors to\nenhance defense against deepfakes.", "AI": {"tldr": "A novel framework, HICOM, improves multi-face deepfake detection by leveraging human-inspired cues, outperforming existing methods in accuracy and generalizability.", "motivation": "Existing deepfake detection methods struggle with multi-face scenarios due to lack of contextual awareness. This work aims to bridge the gap by incorporating human cognitive cues.", "method": "The approach involves human studies to identify key detection cues (scene-motion coherence, inter-face appearance compatibility, interpersonal gaze alignment, face-body consistency) and integrates these into the HICOM framework.", "result": "HICOM improves accuracy by 3.3% in in-dataset detection and 2.8% under perturbations, and outperforms existing methods by 5.8% on unseen datasets. It also enhances interpretability with LLM-generated explanations.", "conclusion": "Incorporating human cognitive cues significantly improves multi-face deepfake detection, offering better accuracy, generalizability, and transparency."}}
{"id": "2507.15281", "pdf": "https://arxiv.org/pdf/2507.15281", "abs": "https://arxiv.org/abs/2507.15281", "authors": ["Haoran Sun", "Zekun Zhang", "Shaoning Zeng"], "title": "A Novel Self-Evolution Framework for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The capabilities of Large Language Models (LLMs) are limited to some extent\nby pre-training, so some researchers optimize LLMs through post-training.\nExisting post-training strategies, such as memory-based retrieval or preference\noptimization, improve user alignment yet fail to enhance the model's domain\ncognition. To bridge this gap, we propose a novel Dual-Phase Self-Evolution\n(DPSE) framework that jointly optimizes user preference adaptation and\ndomain-specific competence. DPSE introduces a Censor module to extract\nmulti-dimensional interaction signals and estimate satisfaction scores, which\nguide structured data expansion via topic-aware and preference-driven\nstrategies. These expanded datasets support a two-stage fine-tuning pipeline:\nsupervised domain grounding followed by frequency-aware preference\noptimization. Experiments across general NLP benchmarks and long-term dialogue\ntasks demonstrate that DPSE consistently outperforms Supervised Fine-Tuning,\nPreference Optimization, and Memory-Augmented baselines. Ablation studies\nvalidate the contribution of each module. In this way, our framework provides\nan autonomous path toward continual self-evolution of LLMs.", "AI": {"tldr": "The paper introduces the Dual-Phase Self-Evolution (DPSE) framework to enhance LLMs by jointly optimizing user preference adaptation and domain-specific competence, outperforming existing methods.", "motivation": "Existing post-training strategies for LLMs improve user alignment but lack domain cognition enhancement, creating a gap this work addresses.", "method": "DPSE uses a Censor module to extract interaction signals and guide structured data expansion, followed by a two-stage fine-tuning pipeline.", "result": "DPSE outperforms baselines like Supervised Fine-Tuning and Preference Optimization in general NLP benchmarks and long-term dialogue tasks.", "conclusion": "The DPSE framework offers a path for autonomous, continual self-evolution of LLMs, validated by ablation studies."}}
{"id": "2507.14809", "pdf": "https://arxiv.org/pdf/2507.14809", "abs": "https://arxiv.org/abs/2507.14809", "authors": ["Zesen Zhong", "Duomin Zhang", "Yijia Li"], "title": "Light Future: Multimodal Action Frame Prediction via InstructPix2Pix", "categories": ["cs.CV", "cs.MM", "cs.RO", "I.2.10; I.4.8"], "comment": "9 pages including appendix, 5 tables, 8 figures, to be submitted to\n  WACV 2026", "summary": "Predicting future motion trajectories is a critical capability across domains\nsuch as robotics, autonomous systems, and human activity forecasting, enabling\nsafer and more intelligent decision-making. This paper proposes a novel,\nefficient, and lightweight approach for robot action prediction, offering\nsignificantly reduced computational cost and inference latency compared to\nconventional video prediction models. Importantly, it pioneers the adaptation\nof the InstructPix2Pix model for forecasting future visual frames in robotic\ntasks, extending its utility beyond static image editing. We implement a deep\nlearning-based visual prediction framework that forecasts what a robot will\nobserve 100 frames (10 seconds) into the future, given a current image and a\ntextual instruction. We repurpose and fine-tune the InstructPix2Pix model to\naccept both visual and textual inputs, enabling multimodal future frame\nprediction. Experiments on the RoboTWin dataset (generated based on real-world\nscenarios) demonstrate that our method achieves superior SSIM and PSNR compared\nto state-of-the-art baselines in robot action prediction tasks. Unlike\nconventional video prediction models that require multiple input frames, heavy\ncomputation, and slow inference latency, our approach only needs a single image\nand a text prompt as input. This lightweight design enables faster inference,\nreduced GPU demands, and flexible multimodal control, particularly valuable for\napplications like robotics and sports motion trajectory analytics, where motion\ntrajectory precision is prioritized over visual fidelity.", "AI": {"tldr": "The paper introduces a lightweight, efficient method for predicting robot motion trajectories using a modified InstructPix2Pix model, reducing computational costs and latency compared to traditional video prediction models.", "motivation": "Predicting future motion trajectories is crucial for robotics and autonomous systems to enhance decision-making. Existing methods are computationally heavy and slow, necessitating a more efficient solution.", "method": "The authors adapt the InstructPix2Pix model for future visual frame prediction in robotics, using a single image and textual instruction as input. The model is fine-tuned for multimodal prediction.", "result": "Experiments on the RoboTWin dataset show superior SSIM and PSNR scores compared to state-of-the-art baselines, with faster inference and lower GPU demands.", "conclusion": "The proposed method offers a lightweight, efficient alternative for robot action prediction, prioritizing motion trajectory precision over visual fidelity, making it suitable for robotics and motion analytics."}}
{"id": "2507.15286", "pdf": "https://arxiv.org/pdf/2507.15286", "abs": "https://arxiv.org/abs/2507.15286", "authors": ["Navid Ayoobi", "Sadat Shahriar", "Arjun Mukherjee"], "title": "Beyond Easy Wins: A Text Hardness-Aware Benchmark for LLM-generated Text Detection", "categories": ["cs.CL"], "comment": null, "summary": "We present a novel evaluation paradigm for AI text detectors that prioritizes\nreal-world and equitable assessment. Current approaches predominantly report\nconventional metrics like AUROC, overlooking that even modest false positive\nrates constitute a critical impediment to practical deployment of detection\nsystems. Furthermore, real-world deployment necessitates predetermined\nthreshold configuration, making detector stability (i.e. the maintenance of\nconsistent performance across diverse domains and adversarial scenarios), a\ncritical factor. These aspects have been largely ignored in previous research\nand benchmarks. Our benchmark, SHIELD, addresses these limitations by\nintegrating both reliability and stability factors into a unified evaluation\nmetric designed for practical assessment. Furthermore, we develop a post-hoc,\nmodel-agnostic humanification framework that modifies AI text to more closely\nresemble human authorship, incorporating a controllable hardness parameter.\nThis hardness-aware approach effectively challenges current SOTA zero-shot\ndetection methods in maintaining both reliability and stability. (Data and\ncode: https://github.com/navid-aub/SHIELD-Benchmark)", "AI": {"tldr": "The paper introduces SHIELD, a benchmark for AI text detectors, focusing on real-world reliability and stability, and proposes a humanification framework to challenge current detection methods.", "motivation": "Current AI text detector evaluations overlook practical deployment issues like false positive rates and stability across domains, which SHIELD aims to address.", "method": "SHIELD integrates reliability and stability into a unified metric and introduces a model-agnostic humanification framework with a controllable hardness parameter.", "result": "The benchmark and framework effectively challenge state-of-the-art zero-shot detection methods in maintaining reliability and stability.", "conclusion": "SHIELD provides a more practical and equitable evaluation paradigm for AI text detectors, addressing gaps in prior research."}}
{"id": "2507.14811", "pdf": "https://arxiv.org/pdf/2507.14811", "abs": "https://arxiv.org/abs/2507.14811", "authors": ["Jiaji Zhang", "Ruichao Sun", "Hailiang Zhao", "Jiaju Wu", "Peng Chen", "Hao Li", "Xinkui Zhao", "Kingsum Chow", "Gang Xiong", "Lin Ye", "Shuiguang Deng"], "title": "SegQuant: A Semantics-Aware and Generalizable Quantization Framework for Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Diffusion models have demonstrated exceptional generative capabilities but\nare computationally intensive, posing significant challenges for deployment in\nresource-constrained or latency-sensitive environments. Quantization offers an\neffective means to reduce model size and computational cost, with post-training\nquantization (PTQ) being particularly appealing due to its compatibility with\npre-trained models without requiring retraining or training data. However,\nexisting PTQ methods for diffusion models often rely on architecture-specific\nheuristics that limit their generalizability and hinder integration with\nindustrial deployment pipelines. To address these limitations, we propose\nSegQuant, a unified quantization framework that adaptively combines\ncomplementary techniques to enhance cross-model versatility. SegQuant consists\nof a segment-aware, graph-based quantization strategy (SegLinear) that captures\nstructural semantics and spatial heterogeneity, along with a dual-scale\nquantization scheme (DualScale) that preserves polarity-asymmetric activations,\nwhich is crucial for maintaining visual fidelity in generated outputs. SegQuant\nis broadly applicable beyond Transformer-based diffusion models, achieving\nstrong performance while ensuring seamless compatibility with mainstream\ndeployment tools.", "AI": {"tldr": "SegQuant is a unified quantization framework for diffusion models, combining segment-aware quantization and dual-scale schemes to enhance versatility and maintain visual fidelity.", "motivation": "Existing PTQ methods for diffusion models lack generalizability and compatibility with industrial pipelines, limiting their deployment in resource-constrained environments.", "method": "SegQuant uses a segment-aware, graph-based strategy (SegLinear) and a dual-scale quantization scheme (DualScale) to preserve structural semantics and polarity-asymmetric activations.", "result": "SegQuant achieves strong performance across models, ensuring compatibility with mainstream deployment tools.", "conclusion": "SegQuant addresses limitations of existing PTQ methods, offering a versatile and efficient solution for quantizing diffusion models."}}
{"id": "2507.15328", "pdf": "https://arxiv.org/pdf/2507.15328", "abs": "https://arxiv.org/abs/2507.15328", "authors": ["Thilo Hagendorff"], "title": "On the Inevitability of Left-Leaning Political Bias in Aligned Language Models", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "The guiding principle of AI alignment is to train large language models\n(LLMs) to be harmless, helpful, and honest (HHH). At the same time, there are\nmounting concerns that LLMs exhibit a left-wing political bias. Yet, the\ncommitment to AI alignment cannot be harmonized with the latter critique. In\nthis article, I argue that intelligent systems that are trained to be harmless\nand honest must necessarily exhibit left-wing political bias. Normative\nassumptions underlying alignment objectives inherently concur with progressive\nmoral frameworks and left-wing principles, emphasizing harm avoidance,\ninclusivity, fairness, and empirical truthfulness. Conversely, right-wing\nideologies often conflict with alignment guidelines. Yet, research on political\nbias in LLMs is consistently framing its insights about left-leaning tendencies\nas a risk, as problematic, or concerning. This way, researchers are actively\narguing against AI alignment, tacitly fostering the violation of HHH\nprinciples.", "AI": {"tldr": "The paper argues that left-wing political bias in LLMs is inevitable due to AI alignment principles (HHH), which align with progressive values, and critiques framing this bias as problematic.", "motivation": "To reconcile the contradiction between AI alignment goals (HHH) and concerns about left-wing bias in LLMs, showing that such bias is inherent to alignment principles.", "method": "Theoretical argumentation linking AI alignment objectives (harmless, helpful, honest) with progressive moral frameworks and left-wing principles.", "result": "Demonstrates that left-wing bias in LLMs is a natural outcome of alignment, conflicting with right-wing ideologies, and critiques the framing of this bias as problematic.", "conclusion": "AI alignment inherently leads to left-wing bias; framing this as a problem undermines alignment goals (HHH)."}}
{"id": "2507.14823", "pdf": "https://arxiv.org/pdf/2507.14823", "abs": "https://arxiv.org/abs/2507.14823", "authors": ["Dong Shu", "Haoyang Yuan", "Yuchen Wang", "Yanguang Liu", "Huopu Zhang", "Haiyan Zhao", "Mengnan Du"], "title": "FinChart-Bench: Benchmarking Financial Chart Comprehension in Vision-Language Models", "categories": ["cs.CV"], "comment": "20 Pages, 18 Figures", "summary": "Large vision-language models (LVLMs) have made significant progress in chart\nunderstanding. However, financial charts, characterized by complex temporal\nstructures and domain-specific terminology, remain notably underexplored. We\nintroduce FinChart-Bench, the first benchmark specifically focused on\nreal-world financial charts. FinChart-Bench comprises 1,200 financial chart\nimages collected from 2015 to 2024, each annotated with True/False (TF),\nMultiple Choice (MC), and Question Answering (QA) questions, totaling 7,016\nquestions. We conduct a comprehensive evaluation of 25 state-of-the-art LVLMs\non FinChart-Bench. Our evaluation reveals critical insights: (1) the\nperformance gap between open-source and closed-source models is narrowing, (2)\nperformance degradation occurs in upgraded models within families, (3) many\nmodels struggle with instruction following, (4) both advanced models show\nsignificant limitations in spatial reasoning abilities, and (5) current LVLMs\nare not reliable enough to serve as automated evaluators. These findings\nhighlight important limitations in current LVLM capabilities for financial\nchart understanding. The FinChart-Bench dataset is available at\nhttps://huggingface.co/datasets/Tizzzzy/FinChart-Bench.", "AI": {"tldr": "FinChart-Bench is a new benchmark for evaluating LVLMs on financial charts, revealing key limitations in current models.", "motivation": "Financial charts are complex and underexplored, necessitating a dedicated benchmark to assess LVLM performance.", "method": "FinChart-Bench includes 1,200 annotated financial charts with 7,016 questions (TF, MC, QA). Evaluated 25 LVLMs.", "result": "Key findings: narrowing gap between open/closed-source models, performance degradation in upgrades, struggles with instruction following, spatial reasoning limitations, and unreliability as automated evaluators.", "conclusion": "Current LVLMs have significant limitations in financial chart understanding, highlighting the need for further research."}}
{"id": "2507.15337", "pdf": "https://arxiv.org/pdf/2507.15337", "abs": "https://arxiv.org/abs/2507.15337", "authors": ["Narun Raman", "Taylor Lundy", "Kevin Leyton-Brown"], "title": "Reasoning Models are Test Exploiters: Rethinking Multiple-Choice", "categories": ["cs.CL"], "comment": "9 pages, 3 figures", "summary": "When evaluating Large Language Models (LLMs) in question-answering domains,\nit is common to ask the model to choose among a fixed set of choices (so-called\nmultiple-choice question-answering, or MCQA). Although downstream tasks of\ninterest typically do not provide systems with explicit options among which to\nchoose, this approach is nevertheless widely used because it makes it makes\nautomatic grading straightforward and has tended to produce challenging\nbenchmarks that correlate sufficiently well with downstream performance. This\npaper investigates the extent to which this trend continues to hold for\nstate-of-the-art reasoning models, describing a systematic evaluation of $15$\ndifferent question-answering benchmarks (e.g., MMLU, HLE) and $25$ different\nLLMs (including small models such as Qwen 7B and relatively large models such\nas Llama 70B). For each model-benchmark pair, we considered $5$ ways of\npresenting the model with questions, including variations on whether multiple\nchoices were offered to the model at all; whether \"none of the above\" sometimes\nreplaced the right answer; and whether the model was permitted to perform\nchain-of-thought reasoning before and/or after the choices were presented. MCQA\nremained a good proxy for the downstream performance of models as long as they\nwere allowed to perform chain-of-thought reasoning only before being presented\nwith the options among which they had to select. On the other hand, large\nmodels that were able to perform reasoning after being given a set of options\ntended to significantly outperform their free-text performance due to\nexploiting the information in the options. We conclude that MCQA is no longer a\ngood proxy for assessing downstream performance of state-of-the-art models, and\noffer practical guidelines for designing more robust, bias-resistant benchmarks\nthat better reflect LLMs' genuine reasoning capabilities.", "AI": {"tldr": "MCQA is a common but flawed proxy for evaluating LLMs' downstream performance, especially for state-of-the-art reasoning models. The study shows MCQA works only if models reason before seeing options, not after.", "motivation": "To assess whether MCQA remains a valid proxy for evaluating LLMs' reasoning capabilities, given advancements in model performance and reasoning methods.", "method": "Systematic evaluation of 15 QA benchmarks and 25 LLMs, testing 5 presentation variations (e.g., chain-of-thought timing, option inclusion).", "result": "MCQA is reliable only if models reason before seeing options. Models exploiting options after reasoning outperform free-text performance, undermining MCQA's validity.", "conclusion": "MCQA is no longer a good proxy for downstream performance. New benchmarks are needed to better reflect LLMs' genuine reasoning abilities."}}
{"id": "2507.14826", "pdf": "https://arxiv.org/pdf/2507.14826", "abs": "https://arxiv.org/abs/2507.14826", "authors": ["Fu-Jen Tsai", "Yan-Tsung Peng", "Yen-Yu Lin", "Chia-Wen Lin"], "title": "PHATNet: A Physics-guided Haze Transfer Network for Domain-adaptive Real-world Image Dehazing", "categories": ["cs.CV"], "comment": "ICCV 2025", "summary": "Image dehazing aims to remove unwanted hazy artifacts in images. Although\nprevious research has collected paired real-world hazy and haze-free images to\nimprove dehazing models' performance in real-world scenarios, these models\noften experience significant performance drops when handling unseen real-world\nhazy images due to limited training data. This issue motivates us to develop a\nflexible domain adaptation method to enhance dehazing performance during\ntesting. Observing that predicting haze patterns is generally easier than\nrecovering clean content, we propose the Physics-guided Haze Transfer Network\n(PHATNet) which transfers haze patterns from unseen target domains to\nsource-domain haze-free images, creating domain-specific fine-tuning sets to\nupdate dehazing models for effective domain adaptation. Additionally, we\nintroduce a Haze-Transfer-Consistency loss and a Content-Leakage Loss to\nenhance PHATNet's disentanglement ability. Experimental results demonstrate\nthat PHATNet significantly boosts state-of-the-art dehazing models on benchmark\nreal-world image dehazing datasets.", "AI": {"tldr": "PHATNet improves dehazing by transferring haze patterns from unseen domains to source images, enhancing model adaptability with novel losses.", "motivation": "Existing dehazing models perform poorly on unseen real-world hazy images due to limited training data, prompting a need for domain adaptation.", "method": "Proposes PHATNet, which transfers haze patterns to source-domain images for fine-tuning, using Haze-Transfer-Consistency and Content-Leakage Losses.", "result": "PHATNet significantly enhances state-of-the-art dehazing models on real-world datasets.", "conclusion": "PHATNet effectively adapts dehazing models to unseen domains, improving performance with innovative haze transfer and loss mechanisms."}}
{"id": "2507.15339", "pdf": "https://arxiv.org/pdf/2507.15339", "abs": "https://arxiv.org/abs/2507.15339", "authors": ["Leanne Tan", "Gabriel Chua", "Ziyu Ge", "Roy Ka-Wei Lee"], "title": "LionGuard 2: Building Lightweight, Data-Efficient & Localised Multilingual Content Moderators", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Modern moderation systems increasingly support multiple languages, but often\nfail to address localisation and low-resource variants - creating safety gaps\nin real-world deployments. Small models offer a potential alternative to large\nLLMs, yet still demand considerable data and compute. We present LionGuard 2, a\nlightweight, multilingual moderation classifier tailored to the Singapore\ncontext, supporting English, Chinese, Malay, and partial Tamil. Built on\npre-trained OpenAI embeddings and a multi-head ordinal classifier, LionGuard 2\noutperforms several commercial and open-source systems across 17 benchmarks,\nincluding both Singapore-specific and public English datasets. The system is\nactively deployed within the Singapore Government, demonstrating practical\nefficacy at scale. Our findings show that high-quality local data and robust\nmultilingual embeddings can achieve strong moderation performance, without\nfine-tuning large models. We release our model weights and part of our training\ndata to support future work on LLM safety.", "AI": {"tldr": "LionGuard 2 is a lightweight, multilingual moderation classifier for Singapore, outperforming commercial systems without fine-tuning large models.", "motivation": "Addressing gaps in multilingual moderation, especially for low-resource languages, in real-world deployments.", "method": "Uses pre-trained OpenAI embeddings and a multi-head ordinal classifier, tailored for Singapore's languages (English, Chinese, Malay, partial Tamil).", "result": "Outperforms commercial and open-source systems across 17 benchmarks, including Singapore-specific datasets.", "conclusion": "High-quality local data and robust multilingual embeddings enable strong moderation performance without large models. Model weights and training data are released for future LLM safety work."}}
{"id": "2507.14833", "pdf": "https://arxiv.org/pdf/2507.14833", "abs": "https://arxiv.org/abs/2507.14833", "authors": ["Haoxuan Zhang", "Wenju Cui", "Yuzhu Cao", "Tao Tan", "Jie Liu", "Yunsong Peng", "Jian Zheng"], "title": "Paired Image Generation with Diffusion-Guided Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The segmentation of mass lesions in digital breast tomosynthesis (DBT) images\nis very significant for the early screening of breast cancer. However, the\nhigh-density breast tissue often leads to high concealment of the mass lesions,\nwhich makes manual annotation difficult and time-consuming. As a result, there\nis a lack of annotated data for model training. Diffusion models are commonly\nused for data augmentation, but the existing methods face two challenges.\nFirst, due to the high concealment of lesions, it is difficult for the model to\nlearn the features of the lesion area. This leads to the low generation quality\nof the lesion areas, thus limiting the quality of the generated images. Second,\nexisting methods can only generate images and cannot generate corresponding\nannotations, which restricts the usability of the generated images in\nsupervised training. In this work, we propose a paired image generation method.\nThe method does not require external conditions and can achieve the generation\nof paired images by training an extra diffusion guider for the conditional\ndiffusion model. During the experimental phase, we generated paired DBT slices\nand mass lesion masks. Then, we incorporated them into the supervised training\nprocess of the mass lesion segmentation task. The experimental results show\nthat our method can improve the generation quality without external conditions.\nMoreover, it contributes to alleviating the shortage of annotated data, thus\nenhancing the performance of downstream tasks.", "AI": {"tldr": "A paired image generation method for DBT images improves lesion segmentation by generating high-quality images and annotations without external conditions.", "motivation": "High concealment of mass lesions in DBT images makes manual annotation difficult, leading to a lack of annotated data for training. Existing diffusion models struggle with lesion feature learning and lack annotation generation.", "method": "Proposes a paired image generation method using a conditional diffusion model with an extra diffusion guider to generate images and annotations.", "result": "Generated paired DBT slices and lesion masks improved segmentation task performance by addressing data shortage.", "conclusion": "The method enhances generation quality and usability for supervised training, aiding downstream tasks."}}
{"id": "2507.15347", "pdf": "https://arxiv.org/pdf/2507.15347", "abs": "https://arxiv.org/abs/2507.15347", "authors": ["Amedeo Buonanno", "Alessandro Rivetti", "Francesco A. N. Palmieri", "Giovanni Di Gennaro", "Gianmarco Romano"], "title": "Probing Information Distribution in Transformer Architectures through Entropy Analysis", "categories": ["cs.CL", "cs.LG"], "comment": "Presented to the Italian Workshop on Neural Networks (WIRN2025) and\n  it will appear in a Springer Chapter", "summary": "This work explores entropy analysis as a tool for probing information\ndistribution within Transformer-based architectures. By quantifying token-level\nuncertainty and examining entropy patterns across different stages of\nprocessing, we aim to investigate how information is managed and transformed\nwithin these models. As a case study, we apply the methodology to a GPT-based\nlarge language model, illustrating its potential to reveal insights into model\nbehavior and internal representations. This approach may offer insights into\nmodel behavior and contribute to the development of interpretability and\nevaluation frameworks for transformer-based models", "AI": {"tldr": "The paper explores entropy analysis in Transformer models to study information distribution and token-level uncertainty, using GPT as a case study.", "motivation": "To understand how information is managed and transformed in Transformer-based architectures.", "method": "Quantifies token-level uncertainty and examines entropy patterns across processing stages in a GPT-based model.", "result": "The approach reveals insights into model behavior and internal representations.", "conclusion": "This method could aid in developing interpretability and evaluation frameworks for Transformer models."}}
{"id": "2507.14845", "pdf": "https://arxiv.org/pdf/2507.14845", "abs": "https://arxiv.org/abs/2507.14845", "authors": ["Rizhao Fan", "Zhigen Li", "Heping Li", "Ning An"], "title": "Training Self-Supervised Depth Completion Using Sparse Measurements and a Single Image", "categories": ["cs.CV"], "comment": null, "summary": "Depth completion is an important vision task, and many efforts have been made\nto enhance the quality of depth maps from sparse depth measurements. Despite\nsignificant advances, training these models to recover dense depth from sparse\nmeasurements remains a challenging problem. Supervised learning methods rely on\ndense depth labels to predict unobserved regions, while self-supervised\napproaches require image sequences to enforce geometric constraints and\nphotometric consistency between frames. However, acquiring dense annotations is\ncostly, and multi-frame dependencies limit the applicability of self-supervised\nmethods in static or single-frame scenarios. To address these challenges, we\npropose a novel self-supervised depth completion paradigm that requires only\nsparse depth measurements and their corresponding image for training. Unlike\nexisting methods, our approach eliminates the need for dense depth labels or\nadditional images captured from neighboring viewpoints. By leveraging the\ncharacteristics of depth distribution, we design novel loss functions that\neffectively propagate depth information from observed points to unobserved\nregions. Additionally, we incorporate segmentation maps generated by vision\nfoundation models to further enhance depth estimation. Extensive experiments\ndemonstrate the effectiveness of our proposed method.", "AI": {"tldr": "A novel self-supervised depth completion method using only sparse depth and single images, eliminating the need for dense labels or multi-frame data.", "motivation": "Overcoming the limitations of costly dense annotations and multi-frame dependencies in existing depth completion methods.", "method": "Proposes a self-supervised paradigm with novel loss functions for depth propagation and leverages segmentation maps from vision foundation models.", "result": "Effective depth completion without dense labels or additional frames, validated through experiments.", "conclusion": "The method offers a practical solution for depth completion in static or single-frame scenarios."}}
{"id": "2507.15357", "pdf": "https://arxiv.org/pdf/2507.15357", "abs": "https://arxiv.org/abs/2507.15357", "authors": ["Elisa Sanchez-Bayona", "Rodrigo Agerri"], "title": "Metaphor and Large Language Models: When Surface Features Matter More than Deep Understanding", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper presents a comprehensive evaluation of the capabilities of Large\nLanguage Models (LLMs) in metaphor interpretation across multiple datasets,\ntasks, and prompt configurations. Although metaphor processing has gained\nsignificant attention in Natural Language Processing (NLP), previous research\nhas been limited to single-dataset evaluations and specific task settings,\noften using artificially constructed data through lexical replacement. We\naddress these limitations by conducting extensive experiments using diverse\npublicly available datasets with inference and metaphor annotations, focusing\non Natural Language Inference (NLI) and Question Answering (QA) tasks. The\nresults indicate that LLMs' performance is more influenced by features like\nlexical overlap and sentence length than by metaphorical content, demonstrating\nthat any alleged emergent abilities of LLMs to understand metaphorical language\nare the result of a combination of surface-level features, in-context learning,\nand linguistic knowledge. This work provides critical insights into the current\ncapabilities and limitations of LLMs in processing figurative language,\nhighlighting the need for more realistic evaluation frameworks in metaphor\ninterpretation tasks. Data and code are publicly available.", "AI": {"tldr": "The paper evaluates LLMs' metaphor interpretation across diverse datasets and tasks, finding performance driven by surface-level features rather than metaphorical understanding.", "motivation": "Address limitations in prior metaphor processing research, which was restricted to single datasets and artificial data, by using diverse datasets and realistic tasks.", "method": "Conducted extensive experiments with publicly available datasets, focusing on NLI and QA tasks, analyzing LLMs' performance.", "result": "LLMs' performance is influenced by lexical overlap and sentence length, not metaphorical content, suggesting no emergent understanding of metaphors.", "conclusion": "Highlights LLMs' limitations in figurative language processing and calls for more realistic evaluation frameworks."}}
{"id": "2507.14851", "pdf": "https://arxiv.org/pdf/2507.14851", "abs": "https://arxiv.org/abs/2507.14851", "authors": ["Muhammad Kamran Janjua", "Amirhosein Ghasemabadi", "Kunlin Zhang", "Mohammad Salameh", "Chao Gao", "Di Niu"], "title": "Grounding Degradations in Natural Language for All-In-One Video Restoration", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "comment": "17 pages", "summary": "In this work, we propose an all-in-one video restoration framework that\ngrounds degradation-aware semantic context of video frames in natural language\nvia foundation models, offering interpretable and flexible guidance. Unlike\nprior art, our method assumes no degradation knowledge in train or test time\nand learns an approximation to the grounded knowledge such that the foundation\nmodel can be safely disentangled during inference adding no extra cost.\nFurther, we call for standardization of benchmarks in all-in-one video\nrestoration, and propose two benchmarks in multi-degradation setting,\nthree-task (3D) and four-task (4D), and two time-varying composite degradation\nbenchmarks; one of the latter being our proposed dataset with varying snow\nintensity, simulating how weather degradations affect videos naturally. We\ncompare our method with prior works and report state-of-the-art performance on\nall benchmarks.", "AI": {"tldr": "Proposes an all-in-one video restoration framework using foundation models for interpretable guidance, introduces new benchmarks, and achieves state-of-the-art results.", "motivation": "To create a flexible, interpretable video restoration method without requiring prior degradation knowledge, and to standardize benchmarks in the field.", "method": "Uses foundation models to ground degradation-aware semantic context in natural language, learning approximations to avoid extra inference costs. Introduces new multi-degradation and time-varying benchmarks.", "result": "Achieves state-of-the-art performance on all proposed benchmarks, including a new dataset with varying snow intensity.", "conclusion": "The framework is effective and interpretable, and the new benchmarks address gaps in video restoration evaluation."}}
{"id": "2507.15375", "pdf": "https://arxiv.org/pdf/2507.15375", "abs": "https://arxiv.org/abs/2507.15375", "authors": ["Cheng-Han Chiang", "Xiaofei Wang", "Linjie Li", "Chung-Ching Lin", "Kevin Lin", "Shujie Liu", "Zhendong Wang", "Zhengyuan Yang", "Hung-yi Lee", "Lijuan Wang"], "title": "STITCH: Simultaneous Thinking and Talking with Chunked Reasoning for Spoken Language Models", "categories": ["cs.CL", "eess.AS"], "comment": "Work in progress. Project page: https://d223302.github.io/STITCH/", "summary": "Spoken Language Models (SLMs) are designed to take speech inputs and produce\nspoken responses. However, current SLMs lack the ability to perform an\ninternal, unspoken thinking process before responding. In contrast, humans\ntypically engage in complex mental reasoning internally, enabling them to\ncommunicate ideas clearly and concisely. Thus, integrating an unspoken thought\nprocess into SLMs is highly desirable. While naively generating a complete\nchain-of-thought (CoT) reasoning before starting to talk can enable thinking\nfor SLMs, this induces additional latency for the speech response, as the CoT\nreasoning can be arbitrarily long. To solve this issue, we propose Stitch, a\nnovel generation method that alternates between the generation of unspoken\nreasoning chunks and spoken response chunks. Since the audio duration of a\nchunk of spoken response is much longer than the time to generate the tokens in\na chunk of spoken response, we use the remaining free time to generate the\nunspoken reasoning tokens. When a chunk of audio is played to the user, the\nmodel continues to generate the next unspoken reasoning chunk, achieving\nsimultaneous thinking and talking. Remarkably, Stitch matches the latency of\nbaselines that cannot generate unspoken CoT by design while outperforming those\nbaselines by 15% on math reasoning datasets; Stitch also performs equally well\non non-reasoning datasets as those baseline models. Some animations and\ndemonstrations are on the project page: https://d223302.github.io/STITCH.", "AI": {"tldr": "Stitch is a novel method enabling Spoken Language Models (SLMs) to alternate between unspoken reasoning and spoken responses, reducing latency while improving reasoning performance.", "motivation": "Current SLMs lack internal reasoning like humans, causing unclear or delayed responses. Integrating unspoken thought processes is desired for clearer communication.", "method": "Stitch alternates generating reasoning chunks and spoken response chunks, using audio playback time to generate reasoning tokens, enabling simultaneous thinking and talking.", "result": "Stitch matches baseline latency while outperforming them by 15% on math reasoning tasks and performs equally on non-reasoning tasks.", "conclusion": "Stitch effectively integrates reasoning into SLMs without added latency, enhancing performance on reasoning tasks while maintaining efficiency."}}
{"id": "2507.14855", "pdf": "https://arxiv.org/pdf/2507.14855", "abs": "https://arxiv.org/abs/2507.14855", "authors": ["Xingshu Chen", "Sicheng Yu", "Chong Cheng", "Hao Wang", "Ting Tian"], "title": "An Uncertainty-aware DETR Enhancement Framework for Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "This paper investigates the problem of object detection with a focus on\nimproving both the localization accuracy of bounding boxes and explicitly\nmodeling prediction uncertainty. Conventional detectors rely on deterministic\nbounding box regression, ignoring uncertainty in predictions and limiting model\nrobustness. In this paper, we propose an uncertainty-aware enhancement\nframework for DETR-based object detectors. We model bounding boxes as\nmultivariate Gaussian distributions and incorporate the Gromov-Wasserstein\ndistance into the loss function to better align the predicted and ground-truth\ndistributions. Building on this, we derive a Bayes Risk formulation to filter\nhigh-risk information and improve detection reliability. We also propose a\nsimple algorithm to quantify localization uncertainty via confidence intervals.\nExperiments on the COCO benchmark show that our method can be effectively\nintegrated into existing DETR variants, enhancing their performance. We further\nextend our framework to leukocyte detection tasks, achieving state-of-the-art\nresults on the LISC and WBCDD datasets. These results confirm the scalability\nof our framework across both general and domain-specific detection tasks. Code\npage:\nhttps://github.com/ParadiseforAndaChen/An-Uncertainty-aware-DETR-Enhancement-Framework-for-Object-Detection.", "AI": {"tldr": "The paper introduces an uncertainty-aware enhancement framework for DETR-based object detectors, improving localization accuracy and modeling prediction uncertainty using Gaussian distributions and Gromov-Wasserstein distance.", "motivation": "Conventional object detectors lack uncertainty modeling, limiting robustness. The paper aims to address this by enhancing DETR-based detectors.", "method": "Proposes modeling bounding boxes as multivariate Gaussian distributions, incorporating Gromov-Wasserstein distance in the loss function, and using Bayes Risk for filtering high-risk predictions. Also introduces a method to quantify uncertainty via confidence intervals.", "result": "Experiments on COCO, LISC, and WBCDD datasets show improved performance and scalability across general and domain-specific tasks.", "conclusion": "The framework effectively enhances DETR variants, confirming its scalability and robustness in object detection."}}
{"id": "2507.15378", "pdf": "https://arxiv.org/pdf/2507.15378", "abs": "https://arxiv.org/abs/2507.15378", "authors": ["Jierui Li", "Raymond Mooney"], "title": "AlgoSimBench: Identifying Algorithmically Similar Problems for Competitive Programming", "categories": ["cs.CL"], "comment": "19 pages, pre-print only", "summary": "Recent progress in LLMs, such as reasoning models, has demonstrated strong\nabilities to solve complex competitive programming problems, often rivaling top\nhuman competitors. However, it remains underexplored whether these abilities\ngeneralize to relevant domains that are less seen during training. To address\nthis, we introduce AlgoSimBench, a new benchmark designed to assess LLMs'\nability to identify algorithmically similar problems (ASPs)-problems that can\nbe solved using similar algorithmic approaches. AlgoSimBench consists of 1317\nproblems, annotated with 231 distinct fine-grained algorithm tags, from which\nwe curate 402 multiple-choice questions (MCQs), where each question presents\none algorithmically similar problem alongside three textually similar but\nalgorithmically dissimilar distractors. Our evaluation reveals that LLMs\nstruggle to identify ASPs, with the best-performing model (o3-mini) achieving\nonly 65.9% accuracy on the MCQ task. To address this challenge, we propose\nattempted solution matching (ASM), a novel method for improving problem\nsimilarity detection. On our MCQ task, ASM yields an absolute accuracy\nimprovement of 6.7% to 11.7% across different models. We also evaluated code\nembedding models and retrieval methods on similar problem identification. While\nthe adversarial selection of problems degrades the performance to be less than\nrandom, we found that simply summarizing the problem to remove narrative\nelements eliminates the effect, and combining ASM with a keyword-prioritized\nmethod, BM25, can yield up to 52.2% accuracy. Code and data are available at\ngithub.com", "AI": {"tldr": "The paper introduces AlgoSimBench to assess LLMs' ability to identify algorithmically similar problems (ASPs), revealing their struggles and proposing a method (ASM) to improve accuracy.", "motivation": "To explore whether LLMs' problem-solving abilities generalize to less-seen domains, specifically identifying ASPs.", "method": "Introduces AlgoSimBench with 1317 problems and 402 MCQs, evaluates LLMs, and proposes ASM for improved similarity detection.", "result": "LLMs struggle with ASP identification (best model: 65.9% accuracy). ASM improves accuracy by 6.7-11.7%. Code embedding models and retrieval methods also evaluated.", "conclusion": "ASM enhances LLMs' ASP detection, but adversarial problem selection challenges performance. Summarizing problems and combining ASM with BM25 improves results."}}
{"id": "2507.14867", "pdf": "https://arxiv.org/pdf/2507.14867", "abs": "https://arxiv.org/abs/2507.14867", "authors": ["Zhaoqiang Xia", "Hexiang Huang", "Haoyu Chen", "Xiaoyi Feng", "Guoying Zhao"], "title": "Hybrid-supervised Hypergraph-enhanced Transformer for Micro-gesture Based Emotion Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Micro-gestures are unconsciously performed body gestures that can convey the\nemotion states of humans and start to attract more research attention in the\nfields of human behavior understanding and affective computing as an emerging\ntopic. However, the modeling of human emotion based on micro-gestures has not\nbeen explored sufficiently. In this work, we propose to recognize the emotion\nstates based on the micro-gestures by reconstructing the behavior patterns with\na hypergraph-enhanced Transformer in a hybrid-supervised framework. In the\nframework, hypergraph Transformer based encoder and decoder are separately\ndesigned by stacking the hypergraph-enhanced self-attention and multiscale\ntemporal convolution modules. Especially, to better capture the subtle motion\nof micro-gestures, we construct a decoder with additional upsampling operations\nfor a reconstruction task in a self-supervised learning manner. We further\npropose a hypergraph-enhanced self-attention module where the hyperedges\nbetween skeleton joints are gradually updated to present the relationships of\nbody joints for modeling the subtle local motion. Lastly, for exploiting the\nrelationship between the emotion states and local motion of micro-gestures, an\nemotion recognition head from the output of encoder is designed with a shallow\narchitecture and learned in a supervised way. The end-to-end framework is\njointly trained in a one-stage way by comprehensively utilizing\nself-reconstruction and supervision information. The proposed method is\nevaluated on two publicly available datasets, namely iMiGUE and SMG, and\nachieves the best performance under multiple metrics, which is superior to the\nexisting methods.", "AI": {"tldr": "The paper proposes a hypergraph-enhanced Transformer framework for emotion recognition using micro-gestures, achieving state-of-the-art performance on public datasets.", "motivation": "Micro-gestures are understudied for emotion recognition, and existing methods lack sufficient modeling of subtle local motions.", "method": "A hybrid-supervised framework with hypergraph-enhanced Transformer, including encoder-decoder architecture, self-supervised reconstruction, and supervised emotion recognition.", "result": "The method outperforms existing approaches on iMiGUE and SMG datasets under multiple metrics.", "conclusion": "The proposed framework effectively models micro-gestures for emotion recognition, demonstrating superior performance."}}
{"id": "2507.15501", "pdf": "https://arxiv.org/pdf/2507.15501", "abs": "https://arxiv.org/abs/2507.15501", "authors": ["Alexandru Coca", "Mark Gaynor", "Zhenxing Zhang", "Jianpeng Cheng", "Bo-Hsiang Tseng", "Pete Boothroyd", "H\u00e9ctor Martinez Alonso", "Diarmuid \u00d3 S\u00e9aghdha", "Anders Johannsen"], "title": "ASPERA: A Simulated Environment to Evaluate Planning for Complex Action Execution", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "37 pages, 22 figures. To appear at ACL 2025", "summary": "This work evaluates the potential of large language models (LLMs) to power\ndigital assistants capable of complex action execution. These assistants rely\non pre-trained programming knowledge to execute multi-step goals by composing\nobjects and functions defined in assistant libraries into action execution\nprograms. To achieve this, we develop ASPERA, a framework comprising an\nassistant library simulation and a human-assisted LLM data generation engine.\nOur engine allows developers to guide LLM generation of high-quality tasks\nconsisting of complex user queries, simulation state and corresponding\nvalidation programs, tackling data availability and evaluation robustness\nchallenges. Alongside the framework we release Asper-Bench, an evaluation\ndataset of 250 challenging tasks generated using ASPERA, which we use to show\nthat program generation grounded in custom assistant libraries is a significant\nchallenge to LLMs compared to dependency-free code generation.", "AI": {"tldr": "The paper introduces ASPERA, a framework for evaluating LLMs in powering digital assistants for complex action execution, and Asper-Bench, a dataset of 250 tasks. It highlights the challenge of program generation with custom libraries compared to dependency-free code.", "motivation": "To assess LLMs' ability to execute complex actions in digital assistants by leveraging pre-trained programming knowledge, addressing data scarcity and evaluation robustness.", "method": "Developed ASPERA, a framework with a simulation for assistant libraries and a human-guided LLM data generation engine to create high-quality tasks.", "result": "Asper-Bench, a dataset of 250 tasks, shows LLMs struggle more with program generation using custom libraries than dependency-free code.", "conclusion": "Program generation grounded in custom assistant libraries is a significant challenge for LLMs, with ASPERA and Asper-Bench providing tools for evaluation."}}
{"id": "2507.14879", "pdf": "https://arxiv.org/pdf/2507.14879", "abs": "https://arxiv.org/abs/2507.14879", "authors": ["Rizhao Fan", "Tianfang Ma", "Zhigen Li", "Ning An", "Jian Cheng"], "title": "Region-aware Depth Scale Adaptation with Sparse Measurements", "categories": ["cs.CV"], "comment": null, "summary": "In recent years, the emergence of foundation models for depth prediction has\nled to remarkable progress, particularly in zero-shot monocular depth\nestimation. These models generate impressive depth predictions; however, their\noutputs are often in relative scale rather than metric scale. This limitation\nposes challenges for direct deployment in real-world applications. To address\nthis, several scale adaptation methods have been proposed to enable foundation\nmodels to produce metric depth. However, these methods are typically costly, as\nthey require additional training on new domains and datasets. Moreover,\nfine-tuning these models often compromises their original generalization\ncapabilities, limiting their adaptability across diverse scenes. In this paper,\nwe introduce a non-learning-based approach that leverages sparse depth\nmeasurements to adapt the relative-scale predictions of foundation models into\nmetric-scale depth. Our method requires neither retraining nor fine-tuning,\nthereby preserving the strong generalization ability of the original foundation\nmodels while enabling them to produce metric depth. Experimental results\ndemonstrate the effectiveness of our approach, high-lighting its potential to\nbridge the gap between relative and metric depth without incurring additional\ncomputational costs or sacrificing generalization ability.", "AI": {"tldr": "A non-learning-based method uses sparse depth measurements to convert relative-scale depth predictions from foundation models into metric-scale depth, preserving generalization without retraining.", "motivation": "Foundation models for depth prediction lack metric scale, limiting real-world application. Existing scale adaptation methods are costly and reduce generalization.", "method": "Leverages sparse depth measurements to adapt relative-scale predictions to metric scale without retraining or fine-tuning.", "result": "Effectively bridges the gap between relative and metric depth without additional computational costs or loss of generalization.", "conclusion": "The approach enables metric-scale depth prediction while maintaining the generalization capabilities of foundation models."}}
{"id": "2507.15512", "pdf": "https://arxiv.org/pdf/2507.15512", "abs": "https://arxiv.org/abs/2507.15512", "authors": ["Kaiyan Chang", "Yonghao Shi", "Chenglong Wang", "Hang Zhou", "Chi Hu", "Xiaoqian Liu", "Yingfeng Luo", "Yuan Ge", "Tong Xiao", "Jingbo Zhu"], "title": "Step-level Verifier-guided Hybrid Test-Time Scaling for Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Test-Time Scaling (TTS) is a promising approach to progressively elicit the\nmodel's intelligence during inference. Recently, training-based TTS methods,\nsuch as continued reinforcement learning (RL), have further surged in\npopularity, while training-free TTS methods are gradually fading from\nprominence. However, the additional computation overhead of training amplifies\nthe burden on test-time scaling. In this paper, we focus on training-free TTS\nmethods for reasoning. We first design Conditional Step-level Self-refinement,\na fine-grained sequential scaling method guided by process verification. On top\nof its effectiveness, we further combine it with other classical parallel\nscaling methods at the step level, to introduce a novel inference paradigm\ncalled Hybrid Test-Time Scaling. Extensive experiments on five\ninstruction-tuned LLMs across different scales (3B-14B) and families\ndemonstrate that hybrid strategy incorporating various training-free TTS\nmethods at a fine granularity has considerable potential for expanding the\nreasoning performance boundaries of LLMs.", "AI": {"tldr": "Hybrid Test-Time Scaling (TTS) combines fine-grained sequential and parallel scaling methods to enhance reasoning in LLMs without additional training overhead.", "motivation": "Training-based TTS methods increase computational burden, so the paper focuses on training-free TTS for efficient and scalable reasoning.", "method": "Proposes Conditional Step-level Self-refinement (sequential scaling) and combines it with parallel scaling methods to create Hybrid TTS.", "result": "Experiments on 3B-14B LLMs show Hybrid TTS significantly improves reasoning performance.", "conclusion": "Training-free Hybrid TTS is a promising approach to push the boundaries of LLM reasoning without extra training costs."}}
{"id": "2507.14885", "pdf": "https://arxiv.org/pdf/2507.14885", "abs": "https://arxiv.org/abs/2507.14885", "authors": ["Joaquim Comas", "Federico Sukno"], "title": "BeatFormer: Efficient motion-robust remote heart rate estimation through unsupervised spectral zoomed attention filters", "categories": ["cs.CV"], "comment": null, "summary": "Remote photoplethysmography (rPPG) captures cardiac signals from facial\nvideos and is gaining attention for its diverse applications. While deep\nlearning has advanced rPPG estimation, it relies on large, diverse datasets for\neffective generalization. In contrast, handcrafted methods utilize\nphysiological priors for better generalization in unseen scenarios like motion\nwhile maintaining computational efficiency. However, their linear assumptions\nlimit performance in complex conditions, where deep learning provides superior\npulsatile information extraction. This highlights the need for hybrid\napproaches that combine the strengths of both methods. To address this, we\npresent BeatFormer, a lightweight spectral attention model for rPPG estimation,\nwhich integrates zoomed orthonormal complex attention and frequency-domain\nenergy measurement, enabling a highly efficient model. Additionally, we\nintroduce Spectral Contrastive Learning (SCL), which allows BeatFormer to be\ntrained without any PPG or HR labels. We validate BeatFormer on the PURE,\nUBFC-rPPG, and MMPD datasets, demonstrating its robustness and performance,\nparticularly in cross-dataset evaluations under motion scenarios.", "AI": {"tldr": "BeatFormer is a lightweight spectral attention model for rPPG estimation, combining deep learning and handcrafted methods for robustness and efficiency, validated on multiple datasets.", "motivation": "The need for hybrid approaches to leverage the strengths of deep learning (superior performance in complex conditions) and handcrafted methods (better generalization and computational efficiency) in rPPG estimation.", "method": "Introduces BeatFormer, integrating zoomed orthonormal complex attention and frequency-domain energy measurement, and Spectral Contrastive Learning (SCL) for training without PPG or HR labels.", "result": "Validated on PURE, UBFC-rPPG, and MMPD datasets, showing robustness and performance, especially in cross-dataset evaluations under motion.", "conclusion": "BeatFormer effectively combines deep learning and handcrafted methods, offering a lightweight, efficient, and robust solution for rPPG estimation."}}
{"id": "2507.15557", "pdf": "https://arxiv.org/pdf/2507.15557", "abs": "https://arxiv.org/abs/2507.15557", "authors": ["Vitaly Protasov", "Nikolay Babakov", "Daryna Dementieva", "Alexander Panchenko"], "title": "Evaluating Text Style Transfer: A Nine-Language Benchmark for Text Detoxification", "categories": ["cs.CL"], "comment": "preprint", "summary": "Despite recent progress in large language models (LLMs), evaluation of text\ngeneration tasks such as text style transfer (TST) remains a significant\nchallenge. Recent studies (Dementieva et al., 2024; Pauli et al., 2025)\nrevealed a substantial gap between automatic metrics and human judgments.\nMoreover, most prior work focuses exclusively on English, leaving multilingual\nTST evaluation largely unexplored. In this paper, we perform the first\ncomprehensive multilingual study on evaluation of text detoxification system\nacross nine languages: English, Spanish, German, Chinese, Arabic, Hindi,\nUkrainian, Russian, Amharic. Drawing inspiration from the machine translation,\nwe assess the effectiveness of modern neural-based evaluation models alongside\nprompting-based LLM-as-a-judge approaches. Our findings provide a practical\nrecipe for designing more reliable multilingual TST evaluation pipeline in the\ntext detoxification case.", "AI": {"tldr": "The paper addresses the challenge of evaluating text style transfer (TST) tasks, particularly in multilingual contexts, by conducting a comprehensive study across nine languages. It compares neural-based evaluation models and LLM-as-a-judge approaches, offering practical insights for reliable multilingual TST evaluation.", "motivation": "The motivation stems from the gap between automatic metrics and human judgments in TST evaluation, as well as the lack of multilingual focus in prior work.", "method": "The study evaluates text detoxification systems across nine languages using neural-based models and LLM-as-a-judge approaches, inspired by machine translation evaluation methods.", "result": "The findings highlight the effectiveness of the evaluated approaches and provide a practical framework for designing reliable multilingual TST evaluation pipelines.", "conclusion": "The paper concludes with actionable recommendations for improving multilingual TST evaluation, particularly in text detoxification."}}
{"id": "2507.14904", "pdf": "https://arxiv.org/pdf/2507.14904", "abs": "https://arxiv.org/abs/2507.14904", "authors": ["Fan Li", "Zanyi Wang", "Zeyi Huang", "Guang Dai", "Jingdong Wang", "Mengmeng Wang"], "title": "TriCLIP-3D: A Unified Parameter-Efficient Framework for Tri-Modal 3D Visual Grounding based on CLIP", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "3D visual grounding allows an embodied agent to understand visual information\nin real-world 3D environments based on human instructions, which is crucial for\nembodied intelligence. Existing 3D visual grounding methods typically rely on\nseparate encoders for different modalities (e.g., RGB images, text, and 3D\npoint clouds), resulting in large and complex models that are inefficient to\ntrain. While some approaches use pre-trained 2D multi-modal models like CLIP\nfor 3D tasks, they still struggle with aligning point cloud data to 2D\nencoders. As a result, these methods continue to depend on 3D encoders for\nfeature extraction, further increasing model complexity and training\ninefficiency. In this paper, we propose a unified 2D pre-trained multi-modal\nnetwork to process all three modalities (RGB images, text, and point clouds),\nsignificantly simplifying the architecture. By leveraging a 2D CLIP bi-modal\nmodel with adapter-based fine-tuning, this framework effectively adapts to the\ntri-modal setting, improving both adaptability and performance across\nmodalities. Our Geometric-Aware 2D-3D Feature Recovery and Fusion (GARF) module\nis designed to fuse geometric multi-scale features from point clouds and\nimages. We then integrate textual features for final modality fusion and\nintroduce a multi-modal decoder to facilitate deep cross-modal understanding.\nTogether, our method achieves unified feature extraction and fusion across the\nthree modalities, enabling an end-to-end 3D visual grounding model. Compared to\nthe baseline, our method reduces the number of trainable parameters by\napproximately 58\\%, while achieving a 6.52\\% improvement in the 3D detection\ntask and a 6.25\\% improvement in the 3D visual grounding task.", "AI": {"tldr": "A unified 2D pre-trained multi-modal network simplifies 3D visual grounding by processing RGB images, text, and point clouds together, reducing model complexity and improving performance.", "motivation": "Existing methods rely on separate encoders for different modalities, leading to inefficiency and complexity. The goal is to streamline the process using a unified approach.", "method": "Leverages a 2D CLIP bi-modal model with adapter-based fine-tuning, introduces GARF for geometric feature fusion, and integrates textual features with a multi-modal decoder.", "result": "Reduces trainable parameters by 58%, improves 3D detection by 6.52%, and 3D visual grounding by 6.25%.", "conclusion": "The proposed method offers a simpler, more efficient, and higher-performing solution for 3D visual grounding."}}
{"id": "2507.15576", "pdf": "https://arxiv.org/pdf/2507.15576", "abs": "https://arxiv.org/abs/2507.15576", "authors": ["Nicolas Poggi", "Shashank Agnihotri", "Margret Keuper"], "title": "Smart Eyes for Silent Threats: VLMs and In-Context Learning for THz Imaging", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Terahertz (THz) imaging enables non-invasive analysis for applications such\nas security screening and material classification, but effective image\nclassification remains challenging due to limited annotations, low resolution,\nand visual ambiguity. We introduce In-Context Learning (ICL) with\nVision-Language Models (VLMs) as a flexible, interpretable alternative that\nrequires no fine-tuning. Using a modality-aligned prompting framework, we adapt\ntwo open-weight VLMs to the THz domain and evaluate them under zero-shot and\none-shot settings. Our results show that ICL improves classification and\ninterpretability in low-data regimes. This is the first application of\nICL-enhanced VLMs to THz imaging, offering a promising direction for\nresource-constrained scientific domains. Code:\n\\href{https://github.com/Nicolas-Poggi/Project_THz_Classification/tree/main}{GitHub\nrepository}.", "AI": {"tldr": "The paper introduces In-Context Learning (ICL) with Vision-Language Models (VLMs) for THz image classification, addressing challenges like limited annotations and low resolution. It shows improved performance and interpretability without fine-tuning.", "motivation": "THz imaging faces challenges in classification due to limited annotations, low resolution, and visual ambiguity. The goal is to provide a flexible, interpretable solution without fine-tuning.", "method": "The authors adapt two open-weight VLMs to the THz domain using a modality-aligned prompting framework, evaluating them in zero-shot and one-shot settings.", "result": "ICL with VLMs improves classification accuracy and interpretability in low-data regimes.", "conclusion": "This is the first application of ICL-enhanced VLMs to THz imaging, offering a promising approach for resource-constrained domains."}}
{"id": "2507.14918", "pdf": "https://arxiv.org/pdf/2507.14918", "abs": "https://arxiv.org/abs/2507.14918", "authors": ["Ren-Dong Xie", "Zhi-Fen He", "Bo Li", "Bin Liu", "Jin-Yan Hu"], "title": "Semantic-Aware Representation Learning for Multi-label Image Classification", "categories": ["cs.CV"], "comment": null, "summary": "Multi-label image classification, an important research area in computer\nvision, focuses on identifying multiple labels or concepts within an image.\nExisting approaches often employ attention mechanisms or graph convolutional\nnetworks (GCNs) to learn image representation. However, this representation may\ncontain noise and may not locate objects precisely. Therefore, this paper\nproposes a Semantic-Aware Representation Learning (SARL) for multi-label image\nclassification. First, a label semantic-related feature learning module is\nutilized to extract semantic-related features. Then, an optimal transport-based\nattention mechanism is designed to obtain semantically aligned image\nrepresentation. Finally, a regional score aggregation strategy is used for\nmulti-label prediction. Experimental results on two benchmark datasets, PASCAL\nVOC 2007 and MS-COCO, demonstrate the superiority of SARL over existing\nmethods.", "AI": {"tldr": "Proposes SARL for multi-label image classification, using semantic-aware features, optimal transport-based attention, and regional score aggregation, outperforming existing methods on PASCAL VOC 2007 and MS-COCO.", "motivation": "Existing methods for multi-label image classification often produce noisy representations and fail to locate objects precisely.", "method": "Uses label semantic-related feature learning, optimal transport-based attention, and regional score aggregation.", "result": "Outperforms existing methods on PASCAL VOC 2007 and MS-COCO datasets.", "conclusion": "SARL improves multi-label image classification by addressing noise and misalignment in representations."}}
{"id": "2507.15586", "pdf": "https://arxiv.org/pdf/2507.15586", "abs": "https://arxiv.org/abs/2507.15586", "authors": ["Xinping Zhao", "Shouzheng Huang", "Yan Zhong", "Xinshuo Hu", "Baotian Hu", "Min Zhang"], "title": "Learning to Extract Rational Evidence via Reinforcement Learning for Retrieval-Augmented Generation", "categories": ["cs.CL"], "comment": "16 pages, 7 Figures, 10 Tables", "summary": "Retrieval-Augmented Generation (RAG) effectively improves the accuracy of\nLarge Language Models (LLMs). However, retrieval noises significantly impact\nthe quality of LLMs' generation, necessitating the development of denoising\nmechanisms. Previous methods extract evidence straightforwardly without\nexplicit thinking, which risks filtering out key clues and struggles with\ngeneralization. To this end, we propose LEAR, which learns to extract rational\nevidence by (1) explicitly reasoning to identify potential cues within\nretrieval contents first, and then (2) consciously extracting to avoid omitting\nany key cues helpful for answering questions. Specifically, we frame evidence\nreasoning and evidence extraction into one unified response for end-to-end\ntraining; apply knowledge token masks for disentanglement to derive\nreasoning-based and extraction-based answers; and devise three types of\nverifiable reward functions, including answer, length, and format, to update\nthe model via the policy optimization algorithm. Extensive experiments on three\nbenchmark datasets show the effectiveness of LEAR, providing compact and\nhigh-quality evidence, improving the accuracy of downstream tasks, and\npromoting effective application in online RAG systems.", "AI": {"tldr": "LEAR improves RAG by learning to extract rational evidence through explicit reasoning and conscious extraction, enhancing LLM accuracy.", "motivation": "Retrieval noises degrade LLM generation quality, and existing methods lack explicit reasoning, risking key clue omission.", "method": "LEAR combines evidence reasoning and extraction into unified training, uses knowledge token masks, and applies verifiable rewards for optimization.", "result": "LEAR outperforms on benchmarks, providing high-quality evidence and boosting downstream task accuracy.", "conclusion": "LEAR effectively denoises retrieval, improves LLM performance, and is practical for online RAG systems."}}
{"id": "2507.14921", "pdf": "https://arxiv.org/pdf/2507.14921", "abs": "https://arxiv.org/abs/2507.14921", "authors": ["Xiufeng Huang", "Ka Chun Cheung", "Runmin Cong", "Simon See", "Renjie Wan"], "title": "Stereo-GS: Multi-View Stereo Vision Model for Generalizable 3D Gaussian Splatting Reconstruction", "categories": ["cs.CV"], "comment": "ACMMM2025. Non-camera-ready version", "summary": "Generalizable 3D Gaussian Splatting reconstruction showcases advanced\nImage-to-3D content creation but requires substantial computational resources\nand large datasets, posing challenges to training models from scratch. Current\nmethods usually entangle the prediction of 3D Gaussian geometry and appearance,\nwhich rely heavily on data-driven priors and result in slow regression speeds.\nTo address this, we propose \\method, a disentangled framework for efficient 3D\nGaussian prediction. Our method extracts features from local image pairs using\na stereo vision backbone and fuses them via global attention blocks. Dedicated\npoint and Gaussian prediction heads generate multi-view point-maps for geometry\nand Gaussian features for appearance, combined as GS-maps to represent the 3DGS\nobject. A refinement network enhances these GS-maps for high-quality\nreconstruction. Unlike existing methods that depend on camera parameters, our\napproach achieves pose-free 3D reconstruction, improving robustness and\npracticality. By reducing resource demands while maintaining high-quality\noutputs, \\method provides an efficient, scalable solution for real-world 3D\ncontent generation.", "AI": {"tldr": "A disentangled framework for efficient 3D Gaussian prediction, reducing computational demands and improving robustness without relying on camera parameters.", "motivation": "Current methods for 3D Gaussian Splatting reconstruction are computationally intensive, slow, and rely heavily on data-driven priors and camera parameters.", "method": "Extracts features from local image pairs using stereo vision, fuses them via global attention, and uses dedicated heads for geometry and appearance prediction, refined for high-quality output.", "result": "Achieves pose-free 3D reconstruction with reduced resource demands while maintaining high-quality outputs.", "conclusion": "The proposed method offers an efficient, scalable solution for real-world 3D content generation."}}
{"id": "2507.15600", "pdf": "https://arxiv.org/pdf/2507.15600", "abs": "https://arxiv.org/abs/2507.15600", "authors": ["Armin Pournaki"], "title": "Conflicting narratives and polarization on social media", "categories": ["cs.CL", "cs.SI"], "comment": "30 pages, 7 figures", "summary": "Narratives are key interpretative devices by which humans make sense of\npolitical reality. In this work, we show how the analysis of conflicting\nnarratives, i.e. conflicting interpretive lenses through which political\nreality is experienced and told, provides insight into the discursive\nmechanisms of polarization and issue alignment in the public sphere. Building\nupon previous work that has identified ideologically polarized issues in the\nGerman Twittersphere between 2021 and 2023, we analyze the discursive dimension\nof polarization by extracting textual signals of conflicting narratives from\ntweets of opposing opinion groups. Focusing on a selection of salient issues\nand events (the war in Ukraine, Covid, climate change), we show evidence for\nconflicting narratives along two dimensions: (i) different attributions of\nactantial roles to the same set of actants (e.g. diverging interpretations of\nthe role of NATO in the war in Ukraine), and (ii) emplotment of different\nactants for the same event (e.g. Bill Gates in the right-leaning Covid\nnarrative). Furthermore, we provide first evidence for patterns of narrative\nalignment, a discursive strategy that political actors employ to align opinions\nacross issues. These findings demonstrate the use of narratives as an\nanalytical lens into the discursive mechanisms of polarization.", "AI": {"tldr": "The paper analyzes conflicting narratives in political discourse on Twitter to understand polarization and issue alignment, focusing on topics like Ukraine, Covid, and climate change.", "motivation": "To explore how conflicting narratives reveal polarization and alignment strategies in public discourse.", "method": "Extracted textual signals of conflicting narratives from tweets of opposing opinion groups, focusing on actantial roles and emplotment.", "result": "Found evidence of conflicting narratives (e.g., NATO's role in Ukraine) and narrative alignment strategies.", "conclusion": "Narratives serve as a valuable analytical tool for understanding discursive polarization."}}
{"id": "2507.14924", "pdf": "https://arxiv.org/pdf/2507.14924", "abs": "https://arxiv.org/abs/2507.14924", "authors": ["Kaishva Chintan Shah", "Virajith Boddapati", "Karthik S. Gurumoorthy", "Sandip Kaledhonkar", "Ajit Rajwade"], "title": "3-Dimensional CryoEM Pose Estimation and Shift Correction Pipeline", "categories": ["cs.CV"], "comment": null, "summary": "Accurate pose estimation and shift correction are key challenges in cryo-EM\ndue to the very low SNR, which directly impacts the fidelity of 3D\nreconstructions. We present an approach for pose estimation in cryo-EM that\nleverages multi-dimensional scaling (MDS) techniques in a robust manner to\nestimate the 3D rotation matrix of each particle from pairs of dihedral angles.\nWe express the rotation matrix in the form of an axis of rotation and a unit\nvector in the plane perpendicular to the axis. The technique leverages the\nconcept of common lines in 3D reconstruction from projections. However, common\nline estimation is ridden with large errors due to the very low SNR of cryo-EM\nprojection images. To address this challenge, we introduce two complementary\ncomponents: (i) a robust joint optimization framework for pose estimation based\non an $\\ell_1$-norm objective or a similar robust norm, which simultaneously\nestimates rotation axes and in-plane vectors while exactly enforcing unit norm\nand orthogonality constraints via projected coordinate descent; and (ii) an\niterative shift correction algorithm that estimates consistent in-plane\ntranslations through a global least-squares formulation. While prior approaches\nhave leveraged such embeddings and common-line geometry for orientation\nrecovery, existing formulations typically rely on $\\ell_2$-based objectives\nthat are sensitive to noise, and enforce geometric constraints only\napproximately. These choices, combined with a sequential pipeline structure,\ncan lead to compounding errors and suboptimal reconstructions in low-SNR\nregimes. Our pipeline consistently outperforms prior methods in both Euler\nangle accuracy and reconstruction fidelity, as measured by the Fourier Shell\nCorrelation (FSC).", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.15641", "pdf": "https://arxiv.org/pdf/2507.15641", "abs": "https://arxiv.org/abs/2507.15641", "authors": ["Alessio Pittiglio"], "title": "Leveraging Context for Multimodal Fallacy Classification in Political Debates", "categories": ["cs.CL", "cs.AI"], "comment": "12th Workshop on Argument Mining (ArgMining 2025) @ ACL 2025", "summary": "In this paper, we present our submission to the MM-ArgFallacy2025 shared\ntask, which aims to advance research in multimodal argument mining, focusing on\nlogical fallacies in political debates. Our approach uses pretrained\nTransformer-based models and proposes several ways to leverage context. In the\nfallacy classification subtask, our models achieved macro F1-scores of 0.4444\n(text), 0.3559 (audio), and 0.4403 (multimodal). Our multimodal model showed\nperformance comparable to the text-only model, suggesting potential for\nimprovements.", "AI": {"tldr": "The paper presents a multimodal approach for detecting logical fallacies in political debates, achieving competitive results with text and multimodal models.", "motivation": "To advance research in multimodal argument mining, specifically targeting logical fallacies in political debates.", "method": "Uses pretrained Transformer-based models and explores leveraging context for fallacy classification.", "result": "Achieved macro F1-scores of 0.4444 (text), 0.3559 (audio), and 0.4403 (multimodal). The multimodal model performed comparably to text-only.", "conclusion": "The results suggest potential for further improvements in multimodal fallacy detection."}}
{"id": "2507.14932", "pdf": "https://arxiv.org/pdf/2507.14932", "abs": "https://arxiv.org/abs/2507.14932", "authors": ["Francisco M. Castro-Mac\u00edas", "Pablo Morales-\u00c1lvarez", "Yunan Wu", "Rafael Molina", "Aggelos K. Katsaggelos"], "title": "Probabilistic smooth attention for deep multiple instance learning in medical imaging", "categories": ["cs.CV"], "comment": null, "summary": "The Multiple Instance Learning (MIL) paradigm is attracting plenty of\nattention in medical imaging classification, where labeled data is scarce. MIL\nmethods cast medical images as bags of instances (e.g. patches in whole slide\nimages, or slices in CT scans), and only bag labels are required for training.\nDeep MIL approaches have obtained promising results by aggregating\ninstance-level representations via an attention mechanism to compute the\nbag-level prediction. These methods typically capture both local interactions\namong adjacent instances and global, long-range dependencies through various\nmechanisms. However, they treat attention values deterministically, potentially\noverlooking uncertainty in the contribution of individual instances. In this\nwork we propose a novel probabilistic framework that estimates a probability\ndistribution over the attention values, and accounts for both global and local\ninteractions. In a comprehensive evaluation involving {\\color{review} eleven}\nstate-of-the-art baselines and three medical datasets, we show that our\napproach achieves top predictive performance in different metrics. Moreover,\nthe probabilistic treatment of the attention provides uncertainty maps that are\ninterpretable in terms of illness localization.", "AI": {"tldr": "A probabilistic framework for Multiple Instance Learning (MIL) in medical imaging improves predictive performance and provides interpretable uncertainty maps.", "motivation": "MIL methods in medical imaging often overlook uncertainty in instance contributions, despite their importance for accurate classification and interpretability.", "method": "The proposed framework estimates a probability distribution over attention values, capturing both local and global interactions among instances.", "result": "Outperforms eleven state-of-the-art baselines across three medical datasets, achieving top predictive performance and interpretable uncertainty maps.", "conclusion": "The probabilistic approach enhances MIL by addressing uncertainty in attention values, improving both performance and interpretability in medical imaging."}}
{"id": "2507.15675", "pdf": "https://arxiv.org/pdf/2507.15675", "abs": "https://arxiv.org/abs/2507.15675", "authors": ["Xinyu Zhang", "Yuanquan Hu", "Fangchao Liu", "Zhicheng Dou"], "title": "P3: Prompts Promote Prompting", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 findings", "summary": "Current large language model (LLM) applications often employ multi-component\nprompts, comprising both system and user prompts, to guide model behaviors.\nWhile recent advancements have demonstrated the efficacy of automatically\noptimizing either the system or user prompt to boost performance, such\nunilateral approaches often yield suboptimal outcomes due to the interdependent\nnature of these components. In this work, we introduce P3, a novel\nself-improvement framework that concurrently optimizes both system and user\nprompts through an iterative process. The offline optimized prompts are further\nleveraged to promote online prompting by performing query-dependent prompt\noptimization. Extensive experiments on general tasks (e.g., Arena-hard and\nAlpaca-eval) and reasoning tasks (e.g., GSM8K and GPQA) demonstrate that P3\nachieves superior performance in the realm of automatic prompt optimization.\nOur results highlight the effectiveness of a holistic optimization strategy in\nenhancing LLM performance across diverse domains.", "AI": {"tldr": "P3 is a self-improvement framework that concurrently optimizes system and user prompts through an iterative process, outperforming unilateral approaches in automatic prompt optimization.", "motivation": "Unilateral optimization of system or user prompts often yields suboptimal results due to their interdependence, prompting the need for a holistic approach.", "method": "P3 introduces a framework for concurrent optimization of both system and user prompts, leveraging offline optimization for online query-dependent improvements.", "result": "P3 achieves superior performance in general and reasoning tasks, demonstrating the effectiveness of holistic optimization.", "conclusion": "A holistic optimization strategy enhances LLM performance across diverse domains, as evidenced by P3's success."}}
{"id": "2507.14935", "pdf": "https://arxiv.org/pdf/2507.14935", "abs": "https://arxiv.org/abs/2507.14935", "authors": ["Hai Huang", "Yan Xia", "Shulei Wang", "Hanting Wang", "Minghui Fang", "Shengpeng Ji", "Sashuai Zhou", "Tao Jin", "Zhou Zhao"], "title": "Open-set Cross Modal Generalization via Multimodal Unified Representation", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "This paper extends Cross Modal Generalization (CMG) to open-set environments\nby proposing the more challenging Open-set Cross Modal Generalization (OSCMG)\ntask. This task evaluates multimodal unified representations in open-set\nconditions, addressing the limitations of prior closed-set cross-modal\nevaluations. OSCMG requires not only cross-modal knowledge transfer but also\nrobust generalization to unseen classes within new modalities, a scenario\nfrequently encountered in real-world applications. Existing multimodal unified\nrepresentation work lacks consideration for open-set environments. To tackle\nthis, we propose MICU, comprising two key components: Fine-Coarse Masked\nmultimodal InfoNCE (FCMI) and Cross modal Unified Jigsaw Puzzles (CUJP). FCMI\nenhances multimodal alignment by applying contrastive learning at both holistic\nsemantic and temporal levels, incorporating masking to enhance generalization.\nCUJP enhances feature diversity and model uncertainty by integrating\nmodality-agnostic feature selection with self-supervised learning, thereby\nstrengthening the model's ability to handle unknown categories in open-set\ntasks. Extensive experiments on CMG and the newly proposed OSCMG validate the\neffectiveness of our approach. The code is available at\nhttps://github.com/haihuangcode/CMG.", "AI": {"tldr": "The paper introduces Open-set Cross Modal Generalization (OSCMG), a more challenging task than CMG, and proposes MICU with FCMI and CUJP to address open-set limitations in multimodal representation learning.", "motivation": "Prior work on multimodal unified representations lacks consideration for open-set environments, which are common in real-world applications.", "method": "Proposes MICU with two components: Fine-Coarse Masked multimodal InfoNCE (FCMI) for multimodal alignment and Cross modal Unified Jigsaw Puzzles (CUJP) for feature diversity and uncertainty handling.", "result": "Extensive experiments on CMG and OSCMG validate the effectiveness of MICU.", "conclusion": "The approach successfully addresses open-set challenges in cross-modal generalization, enhancing robustness to unseen classes."}}
{"id": "2507.15698", "pdf": "https://arxiv.org/pdf/2507.15698", "abs": "https://arxiv.org/abs/2507.15698", "authors": ["Congmin Zheng", "Jiachen Zhu", "Jianghao Lin", "Xinyi Dai", "Yong Yu", "Weinan Zhang", "Mengyue Yang"], "title": "CoLD: Counterfactually-Guided Length Debiasing for Process Reward Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Process Reward Models (PRMs) play a central role in evaluating and guiding\nmulti-step reasoning in large language models (LLMs), especially for\nmathematical problem solving. However, we identify a pervasive length bias in\nexisting PRMs: they tend to assign higher scores to longer reasoning steps,\neven when the semantic content and logical validity are unchanged. This bias\nundermines the reliability of reward predictions and leads to overly verbose\noutputs during inference. To address this issue, we propose\nCoLD(Counterfactually-Guided Length Debiasing), a unified framework that\nmitigates length bias through three components: an explicit length-penalty\nadjustment, a learned bias estimator trained to capture spurious length-related\nsignals, and a joint training strategy that enforces length-invariance in\nreward predictions. Our approach is grounded in counterfactual reasoning and\ninformed by causal graph analysis. Extensive experiments on MATH500 and\nGSM-Plus show that CoLD consistently reduces reward-length correlation,\nimproves accuracy in step selection, and encourages more concise, logically\nvalid reasoning. These results demonstrate the effectiveness and practicality\nof CoLD in improving the fidelity and robustness of PRMs.", "AI": {"tldr": "CoLD mitigates length bias in Process Reward Models (PRMs) by using counterfactual reasoning and causal analysis, improving reward predictions and reasoning conciseness.", "motivation": "Existing PRMs exhibit length bias, favoring longer reasoning steps regardless of semantic or logical validity, which undermines reliability and leads to verbose outputs.", "method": "Proposes CoLD, a framework with three components: explicit length-penalty adjustment, learned bias estimator, and joint training for length-invariant rewards, grounded in counterfactual reasoning.", "result": "CoLD reduces reward-length correlation, improves step selection accuracy, and promotes concise, valid reasoning in experiments on MATH500 and GSM-Plus.", "conclusion": "CoLD effectively enhances the fidelity and robustness of PRMs by addressing length bias."}}
{"id": "2507.14959", "pdf": "https://arxiv.org/pdf/2507.14959", "abs": "https://arxiv.org/abs/2507.14959", "authors": ["Saeid Ghafouri", "Mohsen Fayyaz", "Xiangchen Li", "Deepu John", "Bo Ji", "Dimitrios Nikolopoulos", "Hans Vandierendonck"], "title": "Polymorph: Energy-Efficient Multi-Label Classification for Video Streams on Embedded Devices", "categories": ["cs.CV", "cs.PF"], "comment": null, "summary": "Real-time multi-label video classification on embedded devices is constrained\nby limited compute and energy budgets. Yet, video streams exhibit structural\nproperties such as label sparsity, temporal continuity, and label co-occurrence\nthat can be leveraged for more efficient inference. We introduce Polymorph, a\ncontext-aware framework that activates a minimal set of lightweight Low Rank\nAdapters (LoRA) per frame. Each adapter specializes in a subset of classes\nderived from co-occurrence patterns and is implemented as a LoRA weight over a\nshared backbone. At runtime, Polymorph dynamically selects and composes only\nthe adapters needed to cover the active labels, avoiding full-model switching\nand weight merging. This modular strategy improves scalability while reducing\nlatency and energy overhead. Polymorph achieves 40% lower energy consumption\nand improves mAP by 9 points over strong baselines on the TAO dataset.\nPolymorph is open source at https://github.com/inference-serving/polymorph/.", "AI": {"tldr": "Polymorph is a framework for efficient real-time multi-label video classification on embedded devices by leveraging label sparsity and co-occurrence, reducing energy use by 40% and improving accuracy.", "motivation": "Limited compute and energy budgets on embedded devices necessitate efficient video classification methods.", "method": "Polymorph uses lightweight Low Rank Adapters (LoRA) activated per frame based on label co-occurrence, avoiding full-model switching.", "result": "Achieves 40% lower energy consumption and 9-point mAP improvement on the TAO dataset.", "conclusion": "Polymorph offers a scalable, energy-efficient solution for real-time video classification."}}
{"id": "2507.15706", "pdf": "https://arxiv.org/pdf/2507.15706", "abs": "https://arxiv.org/abs/2507.15706", "authors": ["David Peter Wallis Freeborn"], "title": "Compositional Understanding in Signaling Games", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Receivers in standard signaling game models struggle with learning\ncompositional information. Even when the signalers send compositional messages,\nthe receivers do not interpret them compositionally. When information from one\nmessage component is lost or forgotten, the information from other components\nis also erased. In this paper I construct signaling game models in which\ngenuine compositional understanding evolves. I present two new models: a\nminimalist receiver who only learns from the atomic messages of a signal, and a\ngeneralist receiver who learns from all of the available information. These\nmodels are in many ways simpler than previous alternatives, and allow the\nreceivers to learn from the atomic components of messages.", "AI": {"tldr": "The paper addresses the issue of receivers in signaling games failing to learn compositional information, proposing two new models (minimalist and generalist receivers) that enable genuine compositional understanding.", "motivation": "Standard signaling game models struggle with compositional learning; receivers lose information from message components when one is forgotten.", "method": "Two new models are introduced: a minimalist receiver learning from atomic messages and a generalist receiver utilizing all available information.", "result": "The proposed models are simpler and allow receivers to learn from atomic message components, enabling compositional understanding.", "conclusion": "The new models successfully address the compositional learning problem in signaling games, offering simpler and more effective alternatives."}}
{"id": "2507.14965", "pdf": "https://arxiv.org/pdf/2507.14965", "abs": "https://arxiv.org/abs/2507.14965", "authors": ["Yaojie Zhang", "Tianlun Huang", "Weijun Wang", "Wei Feng"], "title": "Decision PCR: Decision version of the Point Cloud Registration task", "categories": ["cs.CV"], "comment": null, "summary": "Low-overlap point cloud registration (PCR) remains a significant challenge in\n3D vision. Traditional evaluation metrics, such as Maximum Inlier Count, become\nineffective under extremely low inlier ratios. In this paper, we revisit the\nregistration result evaluation problem and identify the Decision version of the\nPCR task as the fundamental problem. To address this Decision PCR task, we\npropose a data-driven approach. First, we construct a corresponding dataset\nbased on the 3DMatch dataset. Then, a deep learning-based classifier is trained\nto reliably assess registration quality, overcoming the limitations of\ntraditional metrics. To our knowledge, this is the first comprehensive study to\naddress this task through a deep learning framework. We incorporate this\nclassifier into standard PCR pipelines. When integrated with our approach,\nexisting state-of-the-art PCR methods exhibit significantly enhanced\nregistration performance. For example, combining our framework with\nGeoTransformer achieves a new SOTA registration recall of 86.97\\% on the\nchallenging 3DLoMatch benchmark. Our method also demonstrates strong\ngeneralization capabilities on the unseen outdoor ETH dataset.", "AI": {"tldr": "The paper addresses low-overlap point cloud registration (PCR) by proposing a data-driven deep learning classifier to evaluate registration quality, improving existing methods' performance.", "motivation": "Traditional metrics like Maximum Inlier Count fail under low inlier ratios, necessitating a new approach to evaluate PCR results.", "method": "A deep learning-based classifier is trained on a dataset derived from 3DMatch to assess registration quality, integrated into standard PCR pipelines.", "result": "Integration with GeoTransformer achieves 86.97% registration recall on 3DLoMatch and shows strong generalization on ETH dataset.", "conclusion": "The proposed data-driven approach effectively improves PCR performance and generalizes well to unseen datasets."}}
{"id": "2507.15707", "pdf": "https://arxiv.org/pdf/2507.15707", "abs": "https://arxiv.org/abs/2507.15707", "authors": ["Seok Hwan Song", "Mohna Chakraborty", "Qi Li", "Wallapak Tavanapong"], "title": "Is Large Language Model Performance on Reasoning Tasks Impacted by Different Ways Questions Are Asked?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have been evaluated using diverse question\ntypes, e.g., multiple-choice, true/false, and short/long answers. This study\nanswers an unexplored question about the impact of different question types on\nLLM accuracy on reasoning tasks. We investigate the performance of five LLMs on\nthree different types of questions using quantitative and deductive reasoning\ntasks. The performance metrics include accuracy in the reasoning steps and\nchoosing the final answer. Key Findings: (1) Significant differences exist in\nLLM performance across different question types. (2) Reasoning accuracy does\nnot necessarily correlate with the final selection accuracy. (3) The number of\noptions and the choice of words, influence LLM performance.", "AI": {"tldr": "The study examines how different question types affect LLM accuracy in reasoning tasks, finding performance varies significantly by question type and that reasoning accuracy doesn't always align with final answer accuracy.", "motivation": "To explore the unexplored impact of question types (e.g., multiple-choice, true/false) on LLM accuracy in reasoning tasks.", "method": "Evaluated five LLMs on three question types using quantitative and deductive reasoning tasks, measuring accuracy in reasoning steps and final answer selection.", "result": "Key findings: (1) Performance varies by question type. (2) Reasoning accuracy doesn't correlate with final answer accuracy. (3) Number of options and wording influence performance.", "conclusion": "Question type significantly impacts LLM performance, with reasoning and final answer accuracy not always aligned, suggesting careful design of evaluation tasks."}}
{"id": "2507.14976", "pdf": "https://arxiv.org/pdf/2507.14976", "abs": "https://arxiv.org/abs/2507.14976", "authors": ["Hao Zheng", "Shunzhi Yang", "Zhuoxin He", "Jinfeng Yang", "Zhenhua Huang"], "title": "Hierarchical Cross-modal Prompt Learning for Vision-Language Models", "categories": ["cs.CV"], "comment": "Accepted by ICCV2025", "summary": "Pre-trained Vision-Language Models (VLMs) such as CLIP have shown excellent\ngeneralization abilities. However, adapting these large-scale models to\ndownstream tasks while preserving their generalization capabilities remains\nchallenging. Although prompt learning methods have shown promise, they suffer\nfrom two fundamental bottlenecks that limit generalization: (a) modality\nisolation, and (b) hierarchical semantic decay. To address these limitations,\nwe propose HiCroPL, a Hierarchical Cross-modal Prompt Learning framework that\nestablishes bidirectional knowledge flow between text and vision modalities,\nenabling them to refine their semantics mutually. HiCroPL routes knowledge\nflows by leveraging the complementary strengths of text and vision. In early\nlayers, text prompts inject relatively clear semantics into visual prompts\nthrough a hierarchical knowledge mapper, enhancing the representation of\nlow-level visual semantics. In later layers, visual prompts encoding specific\ntask-relevant objects flow back to refine text prompts, enabling deeper\nalignment. Crucially, our hierarchical knowledge mapper allows representations\nat multi-scales to be fused, ensuring that deeper representations retain\ntransferable shallow semantics thereby enhancing generalization. We further\nintroduce a lightweight layer-specific knowledge proxy to enable efficient\ncross-modal interactions. Extensive evaluations across four tasks demonstrate\nHiCroPL's superior performance, achieving state-of-the-art results on 11\nbenchmarks with significant improvements. Code is available at:\nhttps://github.com/zzeoZheng/HiCroPL.", "AI": {"tldr": "HiCroPL is a hierarchical cross-modal prompt learning framework addressing modality isolation and semantic decay in VLMs, achieving state-of-the-art results.", "motivation": "Adapting large-scale VLMs to downstream tasks while preserving generalization is challenging due to modality isolation and hierarchical semantic decay.", "method": "HiCroPL establishes bidirectional knowledge flow between text and vision, using a hierarchical knowledge mapper and lightweight layer-specific proxies.", "result": "Achieves state-of-the-art results on 11 benchmarks across four tasks.", "conclusion": "HiCroPL effectively enhances generalization in VLMs by enabling mutual refinement of text and vision modalities."}}
{"id": "2507.15714", "pdf": "https://arxiv.org/pdf/2507.15714", "abs": "https://arxiv.org/abs/2507.15714", "authors": ["Tian Li", "Yujian Sun", "Huizhi Liang"], "title": "Chinchunmei at SemEval-2025 Task 11: Boosting the Large Language Model's Capability of Emotion Perception using Contrastive Learning", "categories": ["cs.CL"], "comment": null, "summary": "The SemEval-2025 Task 11, Bridging the Gap in Text-Based Emotion Detection,\nintroduces an emotion recognition challenge spanning over 28 languages. This\ncompetition encourages researchers to explore more advanced approaches to\naddress the challenges posed by the diversity of emotional expressions and\nbackground variations. It features two tracks: multi-label classification\n(Track A) and emotion intensity prediction (Track B), covering six emotion\ncategories: anger, fear, joy, sadness, surprise, and disgust. In our work, we\nsystematically explore the benefits of two contrastive learning approaches:\nsample-based (Contrastive Reasoning Calibration) and generation-based (DPO,\nSimPO) contrastive learning. The sample-based contrastive approach trains the\nmodel by comparing two samples to generate more reliable predictions. The\ngeneration-based contrastive approach trains the model to differentiate between\ncorrect and incorrect generations, refining its prediction. All models are\nfine-tuned from LLaMa3-Instruct-8B. Our system achieves 9th place in Track A\nand 6th place in Track B for English, while ranking among the top-tier\nperforming systems for other languages.", "AI": {"tldr": "The paper discusses SemEval-2025 Task 11 on emotion detection, introducing two contrastive learning approaches for multi-label classification and emotion intensity prediction across 28 languages.", "motivation": "To address challenges in emotion detection due to diverse expressions and backgrounds, the task encourages advanced approaches.", "method": "Two contrastive learning methods: sample-based (Contrastive Reasoning Calibration) and generation-based (DPO, SimPO), fine-tuned from LLaMa3-Instruct-8B.", "result": "Achieved 9th place in Track A (multi-label classification) and 6th in Track B (emotion intensity prediction) for English, with top-tier performance in other languages.", "conclusion": "The contrastive learning approaches effectively improve emotion detection, demonstrating competitive performance across languages."}}
{"id": "2507.14997", "pdf": "https://arxiv.org/pdf/2507.14997", "abs": "https://arxiv.org/abs/2507.14997", "authors": ["Roy H. Jennings", "Genady Paikin", "Roy Shaul", "Evgeny Soloveichik"], "title": "Language Integration in Fine-Tuning Multimodal Large Language Models for Image-Based Regression", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) show promise for image-based\nregression tasks, but current approaches face key limitations. Recent methods\nfine-tune MLLMs using preset output vocabularies and generic task-level prompts\n(e.g., \"How would you rate this image?\"), assuming this mimics human rating\nbehavior. Our analysis reveals these approaches provide no benefit over\nimage-only training. Models using preset vocabularies and generic prompts\nperform equivalently to image-only models, failing to leverage semantic\nunderstanding from textual input. We propose Regression via Transformer-Based\nClassification (RvTC), which replaces vocabulary-constrained classification\nwith a flexible bin-based approach. Unlike approaches that address\ndiscretization errors through complex distributional modeling, RvTC eliminates\nmanual vocabulary crafting through straightforward bin increase, achieving\nstate-of-the-art performance on four image assessment datasets using only\nimages. More importantly, we demonstrate that data-specific prompts\ndramatically improve performance. Unlike generic task descriptions, prompts\ncontaining semantic information about specific images enable MLLMs to leverage\ncross-modal understanding. On the AVA dataset, adding challenge titles to\nprompts improves correlations from 0.83 to 0.90, a new state-of-the-art. We\ndemonstrate through empirical evidence from the AVA and AGIQA-3k datasets that\nMLLMs benefit from semantic prompt information surpassing mere statistical\nbiases. This underscores the importance of incorporating meaningful textual\ncontext in multimodal regression tasks.", "AI": {"tldr": "MLLMs for image regression underperform due to generic prompts and preset vocabularies. RvTC, a bin-based method, outperforms by using flexible bins and data-specific prompts, achieving SOTA results.", "motivation": "Current MLLM approaches for image regression fail to leverage textual input effectively, performing no better than image-only models.", "method": "Proposed RvTC replaces vocabulary-constrained classification with a flexible bin-based approach and uses data-specific prompts.", "result": "RvTC achieves state-of-the-art performance on four datasets. Semantic prompts improve correlations (e.g., 0.83 to 0.90 on AVA).", "conclusion": "Meaningful textual context is crucial for MLLMs in multimodal regression, as shown by RvTC's success with semantic prompts."}}
{"id": "2507.15715", "pdf": "https://arxiv.org/pdf/2507.15715", "abs": "https://arxiv.org/abs/2507.15715", "authors": ["Alina Hyk", "Kiera McCormick", "Mian Zhong", "Ioana Ciuc\u0103", "Sanjib Sharma", "John F Wu", "J. E. G. Peek", "Kartheik G. Iyer", "Ziang Xiao", "Anjalie Field"], "title": "From Queries to Criteria: Understanding How Astronomers Evaluate LLMs", "categories": ["cs.CL", "astro-ph.IM"], "comment": "Accepted to the Conference on Language Modeling 2025 (COLM), 22\n  pages, 6 figures", "summary": "There is growing interest in leveraging LLMs to aid in astronomy and other\nscientific research, but benchmarks for LLM evaluation in general have not kept\npace with the increasingly diverse ways that real people evaluate and use these\nmodels. In this study, we seek to improve evaluation procedures by building an\nunderstanding of how users evaluate LLMs. We focus on a particular use case: an\nLLM-powered retrieval-augmented generation bot for engaging with astronomical\nliterature, which we deployed via Slack. Our inductive coding of 368 queries to\nthe bot over four weeks and our follow-up interviews with 11 astronomers reveal\nhow humans evaluated this system, including the types of questions asked and\nthe criteria for judging responses. We synthesize our findings into concrete\nrecommendations for building better benchmarks, which we then employ in\nconstructing a sample benchmark for evaluating LLMs for astronomy. Overall, our\nwork offers ways to improve LLM evaluation and ultimately usability,\nparticularly for use in scientific research.", "AI": {"tldr": "The study explores how users evaluate LLMs in astronomy, using a Slack-deployed bot for literature engagement. Insights from queries and interviews led to recommendations for better benchmarks.", "motivation": "To improve LLM evaluation methods by understanding user criteria, especially in scientific contexts like astronomy.", "method": "Inductive coding of 368 bot queries and interviews with 11 astronomers to analyze evaluation criteria.", "result": "Identified user evaluation patterns and proposed benchmark improvements, demonstrated with a sample astronomy benchmark.", "conclusion": "Provides actionable insights for enhancing LLM evaluation and usability in scientific research."}}
{"id": "2507.15000", "pdf": "https://arxiv.org/pdf/2507.15000", "abs": "https://arxiv.org/abs/2507.15000", "authors": ["Chaoyun Wang", "I-Chao Shen", "Takeo Igarashi", "Nanning Zheng", "Caigui Jiang"], "title": "Axis-Aligned Document Dewarping", "categories": ["cs.CV"], "comment": null, "summary": "Document dewarping is crucial for many applications. However, existing\nlearning-based methods primarily rely on supervised regression with annotated\ndata without leveraging the inherent geometric properties in physical documents\nto the dewarping process. Our key insight is that a well-dewarped document is\ncharacterized by transforming distorted feature lines into axis-aligned ones.\nThis property aligns with the inherent axis-aligned nature of the discrete grid\ngeometry in planar documents. In the training phase, we propose an axis-aligned\ngeometric constraint to enhance document dewarping. In the inference phase, we\npropose an axis alignment preprocessing strategy to reduce the dewarping\ndifficulty. In the evaluation phase, we introduce a new metric, Axis-Aligned\nDistortion (AAD), that not only incorporates geometric meaning and aligns with\nhuman visual perception but also demonstrates greater robustness. As a result,\nour method achieves SOTA results on multiple existing benchmarks and achieves\n18.2%~34.5% improvements on the AAD metric.", "AI": {"tldr": "The paper introduces an axis-aligned geometric constraint for document dewarping, improving performance and robustness.", "motivation": "Existing methods lack geometric property utilization in document dewarping.", "method": "Proposes axis-aligned geometric constraint for training and preprocessing for inference.", "result": "Achieves SOTA results with 18.2%~34.5% improvement on the AAD metric.", "conclusion": "The method effectively leverages geometric properties for superior dewarping."}}
{"id": "2507.15717", "pdf": "https://arxiv.org/pdf/2507.15717", "abs": "https://arxiv.org/abs/2507.15717", "authors": ["Sahana Srinivasan", "Xuguang Ai", "Thaddaeus Wai Soon Lo", "Aidan Gilson", "Minjie Zou", "Ke Zou", "Hyunjae Kim", "Mingjia Yang", "Krithi Pushpanathan", "Samantha Yew", "Wan Ting Loke", "Jocelyn Goh", "Yibing Chen", "Yiming Kong", "Emily Yuelei Fu", "Michelle Ongyong Hui", "Kristen Nwanyanwu", "Amisha Dave", "Kelvin Zhenghao Li", "Chen-Hsin Sun", "Mark Chia", "Gabriel Dawei Yang", "Wendy Meihua Wong", "David Ziyou Chen", "Dianbo Liu", "Maxwell Singer", "Fares Antaki", "Lucian V Del Priore", "Jost Jonas", "Ron Adelman", "Qingyu Chen", "Yih-Chung Tham"], "title": "BEnchmarking LLMs for Ophthalmology (BELO) for Ophthalmological Knowledge and Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Current benchmarks evaluating large language models (LLMs) in ophthalmology\nare limited in scope and disproportionately prioritise accuracy. We introduce\nBELO (BEnchmarking LLMs for Ophthalmology), a standardized and comprehensive\nevaluation benchmark developed through multiple rounds of expert checking by 13\nophthalmologists. BELO assesses ophthalmology-related clinical accuracy and\nreasoning quality. Using keyword matching and a fine-tuned PubMedBERT model, we\ncurated ophthalmology-specific multiple-choice-questions (MCQs) from diverse\nmedical datasets (BCSC, MedMCQA, MedQA, BioASQ, and PubMedQA). The dataset\nunderwent multiple rounds of expert checking. Duplicate and substandard\nquestions were systematically removed. Ten ophthalmologists refined the\nexplanations of each MCQ's correct answer. This was further adjudicated by\nthree senior ophthalmologists. To illustrate BELO's utility, we evaluated six\nLLMs (OpenAI o1, o3-mini, GPT-4o, DeepSeek-R1, Llama-3-8B, and Gemini 1.5 Pro)\nusing accuracy, macro-F1, and five text-generation metrics (ROUGE-L, BERTScore,\nBARTScore, METEOR, and AlignScore). In a further evaluation involving human\nexperts, two ophthalmologists qualitatively reviewed 50 randomly selected\noutputs for accuracy, comprehensiveness, and completeness. BELO consists of 900\nhigh-quality, expert-reviewed questions aggregated from five sources: BCSC\n(260), BioASQ (10), MedMCQA (572), MedQA (40), and PubMedQA (18). A public\nleaderboard has been established to promote transparent evaluation and\nreporting. Importantly, the BELO dataset will remain a hold-out,\nevaluation-only benchmark to ensure fair and reproducible comparisons of future\nmodels.", "AI": {"tldr": "BELO is a standardized benchmark for evaluating LLMs in ophthalmology, focusing on clinical accuracy and reasoning quality, with expert-reviewed questions and multiple evaluation metrics.", "motivation": "Current benchmarks for LLMs in ophthalmology are limited and overly focused on accuracy, lacking comprehensive evaluation.", "method": "BELO was developed using expert-curated MCQs from diverse datasets, refined by ophthalmologists, and evaluated using accuracy, macro-F1, and text-generation metrics.", "result": "BELO includes 900 expert-reviewed questions and evaluated six LLMs, with a public leaderboard for transparent reporting.", "conclusion": "BELO provides a fair, reproducible, and comprehensive benchmark for future LLM evaluations in ophthalmology."}}
{"id": "2507.15008", "pdf": "https://arxiv.org/pdf/2507.15008", "abs": "https://arxiv.org/abs/2507.15008", "authors": ["Jiasheng Xu", "Yewang Chen"], "title": "FastSmoothSAM: A Fast Smooth Method For Segment Anything Model", "categories": ["cs.CV"], "comment": null, "summary": "Accurately identifying and representing object edges is a challenging task in\ncomputer vision and image processing. The Segment Anything Model (SAM) has\nsignificantly influenced the field of image segmentation, but suffers from high\nmemory consumption and long inference times, limiting its efficiency in\nreal-time applications. To address these limitations, Fast Segment Anything\n(FastSAM) was proposed, achieving real-time segmentation. However, FastSAM\noften generates jagged edges that deviate from the true object shapes.\nTherefore, this paper introduces a novel refinement approach using B-Spline\ncurve fitting techniques to enhance the edge quality in FastSAM. Leveraging the\nrobust shape control and flexible geometric construction of B-Splines, a\nfour-stage refining process involving two rounds of curve fitting is employed\nto effectively smooth jagged edges. This approach significantly improves the\nvisual quality and analytical accuracy of object edges without compromising\ncritical geometric information. The proposed method improves the practical\nutility of FastSAM by improving segmentation accuracy while maintaining\nreal-time processing capabilities. This advancement unlocks greater potential\nfor FastSAM technology in various real-world scenarios, such as industrial\nautomation, medical imaging, and autonomous systems, where precise and\nefficient edge recognition is crucial.", "AI": {"tldr": "The paper introduces a B-Spline curve fitting method to refine jagged edges in FastSAM, improving edge quality while maintaining real-time performance.", "motivation": "FastSAM, though efficient, produces jagged edges that misrepresent true object shapes, limiting its accuracy in real-world applications.", "method": "A four-stage refining process using B-Spline curve fitting is applied to smooth edges in FastSAM.", "result": "The method enhances edge quality and analytical accuracy without losing geometric details or real-time capabilities.", "conclusion": "This refinement boosts FastSAM's practicality for applications like industrial automation and medical imaging, where precise edge recognition is vital."}}
{"id": "2507.15736", "pdf": "https://arxiv.org/pdf/2507.15736", "abs": "https://arxiv.org/abs/2507.15736", "authors": ["Yuanhao Shen", "Daniel Xavier de Sousa", "Ricardo Mar\u00e7al", "Ali Asad", "Hongyu Guo", "Xiaodan Zhu"], "title": "Understanding Large Language Models' Ability on Interdisciplinary Research", "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) have revealed their\nimpressive ability to perform multi-step, logic-driven reasoning across complex\ndomains, positioning them as powerful tools and collaborators in scientific\ndiscovery while challenging the long-held view that inspiration-driven ideation\nis uniquely human. However, the lack of a dedicated benchmark that evaluates\nLLMs' ability to develop ideas in Interdisciplinary Research (IDR) settings\nposes a critical barrier to fully understanding their strengths and\nlimitations. To address this gap, we introduce IDRBench -- a pioneering\nbenchmark featuring an expert annotated dataset and a suite of tasks tailored\nto evaluate LLMs' capabilities in proposing valuable research ideas from\ndifferent scientific domains for interdisciplinary research. This benchmark\naims to provide a systematic framework for assessing LLM performance in\ncomplex, cross-domain scientific research. Our dataset consists of scientific\npublications sourced from the ArXiv platform covering six distinct disciplines,\nand is annotated by domain experts with diverse academic backgrounds. To ensure\nhigh-quality annotations, we emphasize clearly defined dimensions that\ncharacterize authentic interdisciplinary research. The design of evaluation\ntasks in IDRBench follows a progressive, real-world perspective, reflecting the\nnatural stages of interdisciplinary research development, including 1) IDR\nPaper Identification, 2) IDR Idea Integration, and 3) IDR Idea Recommendation.\nUsing IDRBench, we construct baselines across 10 LLMs and observe that despite\nfostering some level of IDR awareness, LLMs still struggle to produce quality\nIDR ideas. These findings could not only spark new research directions, but\nalso help to develop next-generation LLMs that excel in interdisciplinary\nresearch.", "AI": {"tldr": "IDRBench is introduced as a benchmark to evaluate LLMs' ability to propose interdisciplinary research ideas, revealing their current limitations despite some awareness.", "motivation": "The lack of a dedicated benchmark for assessing LLMs in interdisciplinary research (IDR) settings hinders understanding their potential and limitations in scientific discovery.", "method": "IDRBench includes an expert-annotated dataset from ArXiv across six disciplines and tasks like IDR Paper Identification, Idea Integration, and Recommendation.", "result": "Baselines across 10 LLMs show they struggle to produce quality IDR ideas, despite some interdisciplinary awareness.", "conclusion": "IDRBench provides a framework for evaluating LLMs in IDR, highlighting their current shortcomings and potential for future improvement."}}
{"id": "2507.15028", "pdf": "https://arxiv.org/pdf/2507.15028", "abs": "https://arxiv.org/abs/2507.15028", "authors": ["Yuanhan Zhang", "Yunice Chew", "Yuhao Dong", "Aria Leo", "Bo Hu", "Ziwei Liu"], "title": "Towards Video Thinking Test: A Holistic Benchmark for Advanced Video Reasoning and Understanding", "categories": ["cs.CV"], "comment": "ICCV 2025; Project page: https://zhangyuanhan-ai.github.io/video-tt/", "summary": "Human intelligence requires correctness and robustness, with the former being\nfoundational for the latter. In video understanding, correctness ensures the\naccurate interpretation of visual content, and robustness maintains consistent\nperformance in challenging conditions. Despite advances in video large language\nmodels (video LLMs), existing benchmarks inadequately reflect the gap between\nthese models and human intelligence in maintaining correctness and robustness\nin video interpretation. We introduce the Video Thinking Test (Video-TT), to\nassess if video LLMs can interpret real-world videos as effectively as humans.\nVideo-TT reflects genuine gaps in understanding complex visual narratives, and\nevaluates robustness against natural adversarial questions. Video-TT comprises\n1,000 YouTube Shorts videos, each with one open-ended question and four\nadversarial questions that probe visual and narrative complexity. Our\nevaluation shows a significant gap between video LLMs and human performance.", "AI": {"tldr": "The paper introduces Video-TT, a benchmark to evaluate video LLMs' correctness and robustness in video understanding, revealing a significant gap compared to human performance.", "motivation": "Existing benchmarks fail to measure the gap between video LLMs and human intelligence in video interpretation, particularly in correctness and robustness.", "method": "Video-TT includes 1,000 YouTube Shorts videos with open-ended and adversarial questions to assess visual and narrative understanding.", "result": "Video LLMs show a significant performance gap compared to humans in interpreting complex videos and handling adversarial questions.", "conclusion": "Video-TT highlights the limitations of current video LLMs and underscores the need for improved models to match human-like video understanding."}}
{"id": "2507.15742", "pdf": "https://arxiv.org/pdf/2507.15742", "abs": "https://arxiv.org/abs/2507.15742", "authors": ["Paul Sheridan", "Zeyad Ahmed", "Aitazaz A. Farooque"], "title": "A Fisher's exact test justification of the TF-IDF term-weighting scheme", "categories": ["cs.CL", "cs.IR", "math.ST", "stat.TH"], "comment": "23 pages, 4 tables", "summary": "Term frequency-inverse document frequency, or TF-IDF for short, is arguably\nthe most celebrated mathematical expression in the history of information\nretrieval. Conceived as a simple heuristic quantifying the extent to which a\ngiven term's occurrences are concentrated in any one given document out of\nmany, TF-IDF and its many variants are routinely used as term-weighting schemes\nin diverse text analysis applications. There is a growing body of scholarship\ndedicated to placing TF-IDF on a sound theoretical foundation. Building on that\ntradition, this paper justifies the use of TF-IDF to the statistics community\nby demonstrating how the famed expression can be understood from a significance\ntesting perspective. We show that the common TF-IDF variant TF-ICF is, under\nmild regularity conditions, closely related to the negative logarithm of the\n$p$-value from a one-tailed version of Fisher's exact test of statistical\nsignificance. As a corollary, we establish a connection between TF-IDF and the\nsaid negative log-transformed $p$-value under certain idealized assumptions. We\nfurther demonstrate, as a limiting case, that this same quantity converges to\nTF-IDF in the limit of an infinitely large document collection. The Fisher's\nexact test justification of TF-IDF equips the working statistician with a ready\nexplanation of the term-weighting scheme's long-established effectiveness.", "AI": {"tldr": "The paper justifies TF-IDF from a significance testing perspective, linking it to Fisher's exact test and its p-value.", "motivation": "To provide a theoretical foundation for TF-IDF's effectiveness by connecting it to statistical significance testing.", "method": "Demonstrates that TF-ICF (a TF-IDF variant) relates to the negative log of the p-value from Fisher's exact test under certain conditions.", "result": "Establishes a connection between TF-IDF and Fisher's exact test, showing convergence to TF-IDF in large document collections.", "conclusion": "The statistical perspective offers a clear explanation for TF-IDF's long-standing effectiveness in information retrieval."}}
{"id": "2507.15035", "pdf": "https://arxiv.org/pdf/2507.15035", "abs": "https://arxiv.org/abs/2507.15035", "authors": ["Zhijun Zeng", "Youjia Zheng", "Hao Hu", "Zeyuan Dong", "Yihang Zheng", "Xinliang Liu", "Jinzhuo Wang", "Zuoqiang Shi", "Linfeng Zhang", "Yubing Li", "He Sun"], "title": "OpenBreastUS: Benchmarking Neural Operators for Wave Imaging Using Breast Ultrasound Computed Tomography", "categories": ["cs.CV", "cs.LG", "35Q92, 68U10", "I.4.5; J.2; J.3"], "comment": null, "summary": "Accurate and efficient simulation of wave equations is crucial in\ncomputational wave imaging applications, such as ultrasound computed tomography\n(USCT), which reconstructs tissue material properties from observed scattered\nwaves. Traditional numerical solvers for wave equations are computationally\nintensive and often unstable, limiting their practical applications for\nquasi-real-time image reconstruction. Neural operators offer an innovative\napproach by accelerating PDE solving using neural networks; however, their\neffectiveness in realistic imaging is limited because existing datasets\noversimplify real-world complexity. In this paper, we present OpenBreastUS, a\nlarge-scale wave equation dataset designed to bridge the gap between\ntheoretical equations and practical imaging applications. OpenBreastUS includes\n8,000 anatomically realistic human breast phantoms and over 16 million\nfrequency-domain wave simulations using real USCT configurations. It enables a\ncomprehensive benchmarking of popular neural operators for both forward\nsimulation and inverse imaging tasks, allowing analysis of their performance,\nscalability, and generalization capabilities. By offering a realistic and\nextensive dataset, OpenBreastUS not only serves as a platform for developing\ninnovative neural PDE solvers but also facilitates their deployment in\nreal-world medical imaging problems. For the first time, we demonstrate\nefficient in vivo imaging of the human breast using neural operator solvers.", "AI": {"tldr": "OpenBreastUS introduces a large-scale dataset for wave equation simulations, enabling benchmarking of neural operators for realistic medical imaging.", "motivation": "Traditional wave equation solvers are computationally intensive and unstable, while neural operators lack realistic datasets. OpenBreastUS bridges this gap.", "method": "The dataset includes 8,000 realistic breast phantoms and 16 million frequency-domain wave simulations, tested with neural operators for forward and inverse tasks.", "result": "The dataset allows benchmarking neural operators, showing their performance, scalability, and generalization. Efficient in vivo breast imaging is demonstrated.", "conclusion": "OpenBreastUS advances neural PDE solvers for real-world medical imaging, providing a realistic platform for development and deployment."}}
{"id": "2507.15752", "pdf": "https://arxiv.org/pdf/2507.15752", "abs": "https://arxiv.org/abs/2507.15752", "authors": ["Ruizhe Zhu", "Hao Zhu", "Yaxuan Li", "Syang Zhou", "Shijing Cai", "Malgorzata Lazuka", "Elliott Ash"], "title": "DialogueForge: LLM Simulation of Human-Chatbot Dialogue", "categories": ["cs.CL", "cs.AI"], "comment": "For our code and data, see\n  https://github.com/nerchio/Human_Chatbot-Generation", "summary": "Collecting human-chatbot dialogues typically demands substantial manual\neffort and is time-consuming, which limits and poses challenges for research on\nconversational AI. In this work, we propose DialogueForge - a framework for\ngenerating AI-simulated conversations in human-chatbot style. To initialize\neach generated conversation, DialogueForge uses seed prompts extracted from\nreal human-chatbot interactions. We test a variety of LLMs to simulate the\nhuman chatbot user, ranging from state-of-the-art proprietary models to\nsmall-scale open-source LLMs, and generate multi-turn dialogues tailored to\nspecific tasks. In addition, we explore fine-tuning techniques to enhance the\nability of smaller models to produce indistinguishable human-like dialogues. We\nevaluate the quality of the simulated conversations and compare different\nmodels using the UniEval and GTEval evaluation protocols. Our experiments show\nthat large proprietary models (e.g., GPT-4o) generally outperform others in\ngenerating more realistic dialogues, while smaller open-source models (e.g.,\nLlama, Mistral) offer promising performance with greater customization. We\ndemonstrate that the performance of smaller models can be significantly\nimproved by employing supervised fine-tuning techniques. Nevertheless,\nmaintaining coherent and natural long-form human-like dialogues remains a\ncommon challenge across all models.", "AI": {"tldr": "DialogueForge is a framework for generating AI-simulated human-chatbot dialogues using seed prompts from real interactions, tested with various LLMs. Large proprietary models like GPT-4o excel, while smaller open-source models show promise with fine-tuning.", "motivation": "Manual collection of human-chatbot dialogues is labor-intensive and limits conversational AI research. DialogueForge aims to automate this process.", "method": "Uses seed prompts from real interactions, tests various LLMs (proprietary and open-source), and employs fine-tuning for smaller models. Evaluates with UniEval and GTEval.", "result": "Large models (e.g., GPT-4o) generate more realistic dialogues, while smaller models (e.g., Llama, Mistral) improve with fine-tuning. Coherent long-form dialogues remain challenging.", "conclusion": "DialogueForge effectively automates dialogue generation, with proprietary models leading in quality and smaller models offering customization potential."}}
{"id": "2507.15036", "pdf": "https://arxiv.org/pdf/2507.15036", "abs": "https://arxiv.org/abs/2507.15036", "authors": ["Lyes Saad Saoud", "Irfan Hussain"], "title": "EBA-AI: Ethics-Guided Bias-Aware AI for Efficient Underwater Image Enhancement and Coral Reef Monitoring", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Underwater image enhancement is vital for marine conservation, particularly\ncoral reef monitoring. However, AI-based enhancement models often face dataset\nbias, high computational costs, and lack of transparency, leading to potential\nmisinterpretations. This paper introduces EBA-AI, an ethics-guided bias-aware\nAI framework to address these challenges. EBA-AI leverages CLIP embeddings to\ndetect and mitigate dataset bias, ensuring balanced representation across\nvaried underwater environments. It also integrates adaptive processing to\noptimize energy efficiency, significantly reducing GPU usage while maintaining\ncompetitive enhancement quality. Experiments on LSUI400, Oceanex, and UIEB100\nshow that while PSNR drops by a controlled 1.0 dB, computational savings enable\nreal-time feasibility for large-scale marine monitoring. Additionally,\nuncertainty estimation and explainability techniques enhance trust in AI-driven\nenvironmental decisions. Comparisons with CycleGAN, FunIEGAN, RAUNENet,\nWaterNet, UGAN, PUGAN, and UTUIE validate EBA-AI's effectiveness in balancing\nefficiency, fairness, and interpretability in underwater image processing. By\naddressing key limitations of AI-driven enhancement, this work contributes to\nsustainable, bias-aware, and computationally efficient marine conservation\nefforts. For interactive visualizations, animations, source code, and access to\nthe preprint, visit: https://lyessaadsaoud.github.io/EBA-AI/", "AI": {"tldr": "EBA-AI is an ethics-guided, bias-aware AI framework for underwater image enhancement, addressing dataset bias, computational costs, and transparency issues.", "motivation": "To improve marine conservation efforts by overcoming AI limitations like bias, high computational demands, and lack of interpretability in underwater image enhancement.", "method": "Uses CLIP embeddings for bias detection and mitigation, adaptive processing for energy efficiency, and integrates uncertainty estimation and explainability techniques.", "result": "Achieves computational savings with minimal PSNR drop (1.0 dB), enabling real-time feasibility, and outperforms models like CycleGAN and WaterNet in fairness and efficiency.", "conclusion": "EBA-AI advances sustainable, bias-aware, and efficient underwater image processing, supporting trustworthy AI-driven marine conservation."}}
{"id": "2507.15759", "pdf": "https://arxiv.org/pdf/2507.15759", "abs": "https://arxiv.org/abs/2507.15759", "authors": ["Lyumanshan Ye", "Xiaojie Cai", "Xinkai Wang", "Junfei Wang", "Xiangkun Hu", "Jiadi Su", "Yang Nan", "Sihan Wang", "Bohan Zhang", "Xiaoze Fan", "Jinbin Luo", "Yuxiang Zheng", "Tianze Xu", "Dayuan Fu", "Yunze Wu", "Pengrui Lu", "Zengzhi Wang", "Yiwei Qin", "Zhen Huang", "Yan Ma", "Zhulin Hu", "Haoyang Zou", "Tiantian Mi", "Yixin Ye", "Ethan Chern", "Pengfei Liu"], "title": "Interaction as Intelligence: Deep Research With Human-AI Partnership", "categories": ["cs.CL"], "comment": "30 pages, 10 figures", "summary": "This paper introduces \"Interaction as Intelligence\" research series,\npresenting a reconceptualization of human-AI relationships in deep research\ntasks. Traditional approaches treat interaction merely as an interface for\naccessing AI capabilities-a conduit between human intent and machine output. We\npropose that interaction itself constitutes a fundamental dimension of\nintelligence. As AI systems engage in extended thinking processes for research\ntasks, meaningful interaction transitions from an optional enhancement to an\nessential component of effective intelligence. Current deep research systems\nadopt an \"input-wait-output\" paradigm where users initiate queries and receive\nresults after black-box processing. This approach leads to error cascade\neffects, inflexible research boundaries that prevent question refinement during\ninvestigation, and missed opportunities for expertise integration. To address\nthese limitations, we introduce Deep Cognition, a system that transforms the\nhuman role from giving instructions to cognitive oversight-a mode of engagement\nwhere humans guide AI thinking processes through strategic intervention at\ncritical junctures. Deep cognition implements three key innovations:\n(1)Transparent, controllable, and interruptible interaction that reveals AI\nreasoning and enables intervention at any point; (2)Fine-grained bidirectional\ndialogue; and (3)Shared cognitive context where the system observes and adapts\nto user behaviors without explicit instruction. User evaluation demonstrates\nthat this cognitive oversight paradigm outperforms the strongest baseline\nacross six key metrics: Transparency(+20.0%), Fine-Grained Interaction(+29.2%),\nReal-Time Intervention(+18.5%), Ease of Collaboration(+27.7%),\nResults-Worth-Effort(+8.8%), and Interruptibility(+20.7%). Evaluations on\nchallenging research problems show 31.8% to 50.0% points of improvements over\ndeep research systems.", "AI": {"tldr": "The paper redefines human-AI interaction as a core aspect of intelligence, introducing Deep Cognition for cognitive oversight, outperforming traditional systems in key metrics.", "motivation": "Traditional AI systems treat interaction as a passive interface, leading to errors and inflexibility. The paper argues interaction is fundamental to intelligence.", "method": "Introduces Deep Cognition with transparent, interruptible interaction, fine-grained dialogue, and shared cognitive context.", "result": "Outperforms baselines in transparency, interaction, collaboration, and results, with 31.8%-50.0% improvements in research tasks.", "conclusion": "Cognitive oversight transforms human-AI interaction, proving interaction is essential for effective intelligence in research tasks."}}
{"id": "2507.15037", "pdf": "https://arxiv.org/pdf/2507.15037", "abs": "https://arxiv.org/abs/2507.15037", "authors": ["Zhaotong Yang", "Yuhui Li", "Shengfeng He", "Xinzhe Li", "Yangyang Xu", "Junyu Dong", "Yong Du"], "title": "OmniVTON: Training-Free Universal Virtual Try-On", "categories": ["cs.CV"], "comment": "Accepted by ICCV2025", "summary": "Image-based Virtual Try-On (VTON) techniques rely on either supervised\nin-shop approaches, which ensure high fidelity but struggle with cross-domain\ngeneralization, or unsupervised in-the-wild methods, which improve adaptability\nbut remain constrained by data biases and limited universality. A unified,\ntraining-free solution that works across both scenarios remains an open\nchallenge. We propose OmniVTON, the first training-free universal VTON\nframework that decouples garment and pose conditioning to achieve both texture\nfidelity and pose consistency across diverse settings. To preserve garment\ndetails, we introduce a garment prior generation mechanism that aligns clothing\nwith the body, followed by continuous boundary stitching technique to achieve\nfine-grained texture retention. For precise pose alignment, we utilize DDIM\ninversion to capture structural cues while suppressing texture interference,\nensuring accurate body alignment independent of the original image textures. By\ndisentangling garment and pose constraints, OmniVTON eliminates the bias\ninherent in diffusion models when handling multiple conditions simultaneously.\nExperimental results demonstrate that OmniVTON achieves superior performance\nacross diverse datasets, garment types, and application scenarios. Notably, it\nis the first framework capable of multi-human VTON, enabling realistic garment\ntransfer across multiple individuals in a single scene. Code is available at\nhttps://github.com/Jerome-Young/OmniVTON", "AI": {"tldr": "OmniVTON is a training-free universal VTON framework that decouples garment and pose conditioning for high fidelity and consistency across diverse settings.", "motivation": "Addressing the limitations of supervised in-shop and unsupervised in-the-wild VTON methods, which struggle with cross-domain generalization and data biases.", "method": "Uses garment prior generation and boundary stitching for texture fidelity, and DDIM inversion for pose alignment, disentangling garment and pose constraints.", "result": "Superior performance across diverse datasets, garment types, and scenarios, including multi-human VTON.", "conclusion": "OmniVTON is a versatile, training-free solution for universal VTON, overcoming biases and enabling multi-human garment transfer."}}
{"id": "2507.15773", "pdf": "https://arxiv.org/pdf/2507.15773", "abs": "https://arxiv.org/abs/2507.15773", "authors": ["Andrei-Valentin Tanase", "Elena Pelican"], "title": "Supernova: Achieving More with Less in Transformer Architectures", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We present Supernova, a 650M-parameter decoder-only transformer that\ndemonstrates how careful architectural design and tokenization innovation can\nachieve the performance of larger models while maintaining computational\nefficiency. Our architecture combines Rotary Positional Embeddings (RoPE),\nGrouped Query Attention (GQA) with a 3:1 compression ratio, RMSNorm for\ncomputational efficiency, and SwiGLU activation functions. A critical\ninnovation is our custom 128,000-vocabulary byte-level BPE tokenizer, which\nachieves state-of-the-art compression performance. Through detailed analysis,\nwe show that Supernova achieves 90% of the performance of 1B-parameter models\nwhile using 53% fewer parameters and requiring only 100B training tokens--an\norder of magnitude less than competing models. Our findings challenge the\nprevailing scaling paradigm, demonstrating that architectural efficiency and\ntokenization quality can compensate for reduced parameter counts.", "AI": {"tldr": "Supernova is a 650M-parameter transformer model that matches larger models' performance with fewer parameters and tokens, thanks to innovative design and tokenization.", "motivation": "To challenge the scaling paradigm by proving that architectural efficiency and tokenization quality can compensate for reduced parameter counts.", "method": "Combines Rotary Positional Embeddings, Grouped Query Attention, RMSNorm, SwiGLU activation, and a custom 128K-vocabulary byte-level BPE tokenizer.", "result": "Achieves 90% performance of 1B-parameter models with 53% fewer parameters and 100B training tokens (10x less than competitors).", "conclusion": "Architectural and tokenization innovations can outperform traditional scaling approaches."}}
{"id": "2507.15059", "pdf": "https://arxiv.org/pdf/2507.15059", "abs": "https://arxiv.org/abs/2507.15059", "authors": ["Ran Zhang", "Xuanhua He", "Li Xueheng", "Ke Cao", "Liu Liu", "Wenbo Xu", "Fang Jiabin", "Yang Qize", "Jie Zhang"], "title": "Rethinking Pan-sharpening: Principled Design, Unified Training, and a Universal Loss Surpass Brute-Force Scaling", "categories": ["cs.CV"], "comment": null, "summary": "The field of pan-sharpening has recently seen a trend towards increasingly\nlarge and complex models, often trained on single, specific satellite datasets.\nThis approach, however, leads to high computational overhead and poor\ngeneralization on full resolution data, a paradigm we challenge in this paper.\nIn response to this issue, we propose PanTiny, a lightweight, single-step\npan-sharpening framework designed for both efficiency and robust performance.\nMore critically, we introduce multiple-in-one training paradigm, where a\nsingle, compact model is trained simultaneously on three distinct satellite\ndatasets (WV2, WV3, and GF2) with different resolution and spectral\ninformation. Our experiments show that this unified training strategy not only\nsimplifies deployment but also significantly boosts generalization on\nfull-resolution data. Further, we introduce a universally powerful composite\nloss function that elevates the performance of almost all of models for\npan-sharpening, pushing state-of-the-art metrics into a new era. Our PanTiny\nmodel, benefiting from these innovations, achieves a superior\nperformance-to-efficiency balance, outperforming most larger, specialized\nmodels. Through extensive ablation studies, we validate that principled\nengineering in model design, training paradigms, and loss functions can surpass\nbrute-force scaling. Our work advocates for a community-wide shift towards\ncreating efficient, generalizable, and data-conscious models for\npan-sharpening. The code is available at\nhttps://github.com/Zirconium233/PanTiny .", "AI": {"tldr": "PanTiny is a lightweight, efficient pan-sharpening framework trained on multiple datasets, outperforming larger models with better generalization and performance-to-efficiency balance.", "motivation": "Addressing the inefficiency and poor generalization of large, dataset-specific pan-sharpening models.", "method": "Proposes PanTiny, a single-step framework trained on three satellite datasets (WV2, WV3, GF2) with a composite loss function.", "result": "PanTiny achieves superior performance-to-efficiency balance and outperforms larger models.", "conclusion": "Advocates for efficient, generalizable models in pan-sharpening, validated by principled engineering over brute-force scaling."}}
{"id": "2507.15778", "pdf": "https://arxiv.org/pdf/2507.15778", "abs": "https://arxiv.org/abs/2507.15778", "authors": ["Jiakang Wang", "Runze Liu", "Fuzheng Zhang", "Xiu Li", "Guorui Zhou"], "title": "Stabilizing Knowledge, Promoting Reasoning: Dual-Token Constraints for RLVR", "categories": ["cs.CL"], "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has become an effective\npost-training method for improving the reasoning abilities of Large Language\nModels (LLMs), mainly by shaping higher-order behaviors such as reflection and\nplanning. However, previous RLVR algorithms often apply uniform training\nsignals to all tokens, without considering the different roles of low-entropy\nknowledge-related tokens and high-entropy reasoning-related tokens. Some recent\nmethods try to separate these token types by gradient masking or asynchronous\nupdates, but these approaches may break semantic dependencies in the model\noutput and hinder effective learning. In this work, we propose Archer, an\nentropy-aware RLVR approach with dual-token constraints and synchronous\nupdates. Specifically, our method applies weaker KL regularization and higher\nclipping thresholds to reasoning tokens to encourage exploration, while using\nstronger constraints on knowledge tokens to maintain factual knowledge.\nExperimental results on several mathematical reasoning and code generation\nbenchmarks show that our approach significantly outperforms previous RLVR\nmethods, reaching or exceeding state-of-the-art performance among models of\ncomparable size. The code is available at\nhttps://github.com/wizard-III/ArcherCodeR.", "AI": {"tldr": "Archer introduces an entropy-aware RLVR method with dual-token constraints and synchronous updates, outperforming previous methods in reasoning tasks.", "motivation": "Previous RLVR methods apply uniform training signals to all tokens, ignoring the roles of knowledge and reasoning tokens, which can hinder learning.", "method": "Archer uses weaker KL regularization for reasoning tokens to encourage exploration and stronger constraints on knowledge tokens to maintain facts.", "result": "Archer outperforms prior RLVR methods on mathematical reasoning and code generation benchmarks, matching or exceeding state-of-the-art performance.", "conclusion": "The proposed method effectively balances exploration and factual retention, advancing RLVR for LLMs."}}
{"id": "2507.15064", "pdf": "https://arxiv.org/pdf/2507.15064", "abs": "https://arxiv.org/abs/2507.15064", "authors": ["Shuyuan Tu", "Zhen Xing", "Xintong Han", "Zhi-Qi Cheng", "Qi Dai", "Chong Luo", "Zuxuan Wu", "Yu-Gang Jiang"], "title": "StableAnimator++: Overcoming Pose Misalignment and Face Distortion for Human Image Animation", "categories": ["cs.CV", "cs.AI"], "comment": "arXiv admin note: substantial text overlap with arXiv:2411.17697", "summary": "Current diffusion models for human image animation often struggle to maintain\nidentity (ID) consistency, especially when the reference image and driving\nvideo differ significantly in body size or position. We introduce\nStableAnimator++, the first ID-preserving video diffusion framework with\nlearnable pose alignment, capable of generating high-quality videos conditioned\non a reference image and a pose sequence without any post-processing. Building\nupon a video diffusion model, StableAnimator++ contains carefully designed\nmodules for both training and inference, striving for identity consistency. In\nparticular, StableAnimator++ first uses learnable layers to predict the\nsimilarity transformation matrices between the reference image and the driven\nposes via injecting guidance from Singular Value Decomposition (SVD). These\nmatrices align the driven poses with the reference image, mitigating\nmisalignment to a great extent. StableAnimator++ then computes image and face\nembeddings using off-the-shelf encoders, refining the face embeddings via a\nglobal content-aware Face Encoder. To further maintain ID, we introduce a\ndistribution-aware ID Adapter that counteracts interference caused by temporal\nlayers while preserving ID via distribution alignment. During the inference\nstage, we propose a novel Hamilton-Jacobi-Bellman (HJB) based face optimization\nintegrated into the denoising process, guiding the diffusion trajectory for\nenhanced facial fidelity. Experiments on benchmarks show the effectiveness of\nStableAnimator++ both qualitatively and quantitatively.", "AI": {"tldr": "StableAnimator++ is a video diffusion framework that preserves identity (ID) consistency in human image animation by using learnable pose alignment and advanced modules for training and inference.", "motivation": "Existing diffusion models struggle with ID consistency when reference images and driving videos differ in body size or position.", "method": "StableAnimator++ employs learnable pose alignment via SVD-guided transformation matrices, image/face embeddings, a Face Encoder, and a distribution-aware ID Adapter. It also integrates HJB-based face optimization during inference.", "result": "The framework achieves high-quality, ID-consistent video generation without post-processing, validated by benchmarks.", "conclusion": "StableAnimator++ effectively addresses ID inconsistency in human image animation, offering superior performance in both quality and fidelity."}}
{"id": "2507.15779", "pdf": "https://arxiv.org/pdf/2507.15779", "abs": "https://arxiv.org/abs/2507.15779", "authors": ["Felix K\u00f6ster", "Atsushi Uchida"], "title": "Reservoir Computing as a Language Model", "categories": ["cs.CL"], "comment": "8 pages, 5 figures, 1 table", "summary": "Large Language Models (LLM) have dominated the science and media landscape\nduo to their impressive performance on processing large chunks of data and\nproduce human-like levels of text. Nevertheless, their huge energy demand and\nslow processing still a bottleneck for further increasing quality while also\nmaking the models accessible to everyone. To solve this bottleneck, we will\ninvestigate how reservoir computing performs on natural text processing, which\ncould enable fast and energy efficient hardware implementations. Studies\ninvestigating the use of reservoir computing as a language model remain sparse.\nIn this paper, we compare three distinct approaches for character-level\nlanguage modeling, two different reservoir computing approaches, where only an\noutput layer is trainable, and the well-known transformer-based architectures,\nwhich fully learn an attention-based sequence representation. We explore the\nperformance, computational cost and prediction accuracy for both paradigms by\nequally varying the number of trainable parameters for all models. Using a\nconsistent pipeline for all three approaches, we demonstrate that transformers\nexcel in prediction quality, whereas reservoir computers remain highly\nefficient reducing the training and inference speed. Furthermore, we\ninvestigate two types of reservoir computing: a traditional reservoir with a\nstatic linear readout, and an attention-enhanced reservoir that dynamically\nadapts its output weights via an attention mechanism. Our findings underline\nhow these paradigms scale and offer guidelines to balance resource constraints\nwith performance.", "AI": {"tldr": "The paper compares reservoir computing and transformer-based models for language tasks, highlighting transformers' superior prediction quality and reservoir computing's efficiency.", "motivation": "Address the energy and speed bottlenecks of Large Language Models (LLMs) by exploring reservoir computing for efficient natural text processing.", "method": "Compare three approaches: two reservoir computing methods (static and attention-enhanced) and transformer-based models, evaluating performance, computational cost, and accuracy with equal trainable parameters.", "result": "Transformers outperform in prediction quality, while reservoir computing is faster and more energy-efficient. Attention-enhanced reservoirs show dynamic adaptability.", "conclusion": "The study provides guidelines for balancing performance and resource constraints, suggesting reservoir computing as a viable alternative for efficient language modeling."}}
{"id": "2507.15085", "pdf": "https://arxiv.org/pdf/2507.15085", "abs": "https://arxiv.org/abs/2507.15085", "authors": ["Peirong Zhang", "Haowei Xu", "Jiaxin Zhang", "Guitao Xu", "Xuhan Zheng", "Zhenhua Yang", "Junle Liu", "Yuyi Zhang", "Lianwen Jin"], "title": "Aesthetics is Cheap, Show me the Text: An Empirical Evaluation of State-of-the-Art Generative Models for OCR", "categories": ["cs.CV"], "comment": null, "summary": "Text image is a unique and crucial information medium that integrates visual\naesthetics and linguistic semantics in modern e-society. Due to their subtlety\nand complexity, the generation of text images represents a challenging and\nevolving frontier in the image generation field. The recent surge of\nspecialized image generators (\\emph{e.g.}, Flux-series) and unified generative\nmodels (\\emph{e.g.}, GPT-4o), which demonstrate exceptional fidelity, raises a\nnatural question: can they master the intricacies of text image generation and\nediting? Motivated by this, we assess current state-of-the-art generative\nmodels' capabilities in terms of text image generation and editing. We\nincorporate various typical optical character recognition (OCR) tasks into our\nevaluation and broaden the concept of text-based generation tasks into OCR\ngenerative tasks. We select 33 representative tasks and categorize them into\nfive categories: document, handwritten text, scene text, artistic text, and\ncomplex \\& layout-rich text. For comprehensive evaluation, we examine six\nmodels across both closed-source and open-source domains, using tailored,\nhigh-quality image inputs and prompts. Through this evaluation, we draw crucial\nobservations and identify the weaknesses of current generative models for OCR\ntasks. We argue that photorealistic text image generation and editing should be\ninternalized as foundational skills into general-domain generative models,\nrather than being delegated to specialized solutions, and we hope this\nempirical analysis can provide valuable insights for the community to achieve\nthis goal. This evaluation is online and will be continuously updated at our\nGitHub repository.", "AI": {"tldr": "The paper evaluates state-of-the-art generative models for text image generation and editing, incorporating OCR tasks, and identifies their weaknesses.", "motivation": "To assess if current generative models can handle the complexities of text image generation and editing, given their advancements in fidelity.", "method": "Evaluates six models using 33 OCR tasks across five categories, with tailored inputs and prompts.", "result": "Identifies weaknesses in current models and argues for integrating text image generation as a foundational skill in general-domain models.", "conclusion": "Photorealistic text image generation should be a core capability of general models, not just specialized solutions."}}
{"id": "2507.15823", "pdf": "https://arxiv.org/pdf/2507.15823", "abs": "https://arxiv.org/abs/2507.15823", "authors": ["Anton Abilov", "Ke Zhang", "Hemank Lamba", "Elizabeth M. Olson", "Joel R. Tetreault", "Alejandro Jaimes"], "title": "Operationalizing AI for Good: Spotlight on Deployment and Integration of AI Models in Humanitarian Work", "categories": ["cs.CL", "cs.AI", "cs.SI"], "comment": null, "summary": "Publications in the AI for Good space have tended to focus on the research\nand model development that can support high-impact applications. However, very\nfew AI for Good papers discuss the process of deploying and collaborating with\nthe partner organization, and the resulting real-world impact. In this work, we\nshare details about the close collaboration with a humanitarian-to-humanitarian\n(H2H) organization and how to not only deploy the AI model in a\nresource-constrained environment, but also how to maintain it for continuous\nperformance updates, and share key takeaways for practitioners.", "AI": {"tldr": "The paper highlights the gap in AI for Good literature regarding deployment and collaboration with partner organizations, sharing insights from a real-world humanitarian project.", "motivation": "To address the lack of discussion on deploying AI models in resource-constrained settings and maintaining them for real-world impact.", "method": "Close collaboration with a humanitarian organization, focusing on deployment, maintenance, and performance updates.", "result": "Key takeaways for practitioners on deploying and sustaining AI models in humanitarian contexts.", "conclusion": "The work underscores the importance of collaboration and maintenance in AI for Good projects for tangible impact."}}
{"id": "2507.15089", "pdf": "https://arxiv.org/pdf/2507.15089", "abs": "https://arxiv.org/abs/2507.15089", "authors": ["Ioannis Tsampikos Papapetros", "Ioannis Kansizoglou", "Antonios Gasteratos"], "title": "Visual Place Recognition for Large-Scale UAV Applications", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Visual Place Recognition (vPR) plays a crucial role in Unmanned Aerial\nVehicle (UAV) navigation, enabling robust localization across diverse\nenvironments. Despite significant advancements, aerial vPR faces unique\nchallenges due to the limited availability of large-scale, high-altitude\ndatasets, which limits model generalization, along with the inherent rotational\nambiguity in UAV imagery. To address these challenges, we introduce LASED, a\nlarge-scale aerial dataset with approximately one million images,\nsystematically sampled from 170,000 unique locations throughout Estonia over a\ndecade, offering extensive geographic and temporal diversity. Its structured\ndesign ensures clear place separation significantly enhancing model training\nfor aerial scenarios. Furthermore, we propose the integration of steerable\nConvolutional Neural Networks (CNNs) to explicitly handle rotational variance,\nleveraging their inherent rotational equivariance to produce robust,\norientation-invariant feature representations. Our extensive benchmarking\ndemonstrates that models trained on LASED achieve significantly higher recall\ncompared to those trained on smaller, less diverse datasets, highlighting the\nbenefits of extensive geographic coverage and temporal diversity. Moreover,\nsteerable CNNs effectively address rotational ambiguity inherent in aerial\nimagery, consistently outperforming conventional convolutional architectures,\nachieving on average 12\\% recall improvement over the best-performing\nnon-steerable network. By combining structured, large-scale datasets with\nrotation-equivariant neural networks, our approach significantly enhances model\nrobustness and generalization for aerial vPR.", "AI": {"tldr": "The paper introduces LASED, a large-scale aerial dataset, and steerable CNNs to improve Visual Place Recognition (vPR) for UAVs, addressing challenges like dataset scarcity and rotational ambiguity.", "motivation": "Aerial vPR faces challenges due to limited datasets and rotational variance in UAV imagery, hindering model generalization.", "method": "Proposes LASED, a structured large-scale dataset, and integrates steerable CNNs to handle rotational variance.", "result": "Models trained on LASED achieve higher recall, and steerable CNNs outperform conventional CNNs by 12% in recall.", "conclusion": "Combining large-scale datasets with rotation-equivariant networks enhances robustness and generalization in aerial vPR."}}
{"id": "2507.15849", "pdf": "https://arxiv.org/pdf/2507.15849", "abs": "https://arxiv.org/abs/2507.15849", "authors": ["Yihao Li", "Jiayi Xin", "Miranda Muqing Miao", "Qi Long", "Lyle Ungar"], "title": "The Impact of Language Mixing on Bilingual LLM Reasoning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Proficient multilingual speakers often intentionally switch languages in the\nmiddle of a conversation. Similarly, recent reasoning-focused bilingual large\nlanguage models (LLMs) with strong capabilities in both languages exhibit\nlanguage mixing--alternating languages within their chain of thought.\nDiscouraging this behavior in DeepSeek-R1 was found to degrade accuracy,\nsuggesting that language mixing may benefit reasoning. In this work, we study\nlanguage switching in Chinese-English bilingual reasoning models. We identify\nreinforcement learning with verifiable rewards (RLVR) as the critical training\nstage that leads to language mixing. We demonstrate that language mixing can\nenhance reasoning: enforcing monolingual decoding reduces accuracy by 5.6\npercentage points on math reasoning tasks. Additionally, a lightweight probe\ncan be trained to predict whether a potential language switch would benefit or\nharm reasoning, and when used to guide decoding, increases accuracy by up to\n6.25 percentage points. Our findings suggest that language mixing is not merely\na byproduct of multilingual training, but is a strategic reasoning behavior.", "AI": {"tldr": "Language mixing in bilingual LLMs enhances reasoning, with RLVR training enabling this behavior. Enforcing monolingual decoding reduces accuracy, while guided switching improves it.", "motivation": "To understand why bilingual LLMs mix languages during reasoning and whether this behavior benefits performance.", "method": "Studied language switching in Chinese-English bilingual models, identifying RLVR as the key training stage. Used a lightweight probe to predict beneficial switches.", "result": "Language mixing improves reasoning accuracy (5.6% drop if blocked). Guided switching boosts accuracy by up to 6.25%.", "conclusion": "Language mixing is a strategic reasoning behavior, not just a training byproduct."}}
{"id": "2507.15094", "pdf": "https://arxiv.org/pdf/2507.15094", "abs": "https://arxiv.org/abs/2507.15094", "authors": ["Mengya Xu", "Rulin Zhou", "An Wang", "Chaoyang Lyu", "Zhen Li", "Ning Zhong", "Hongliang Ren"], "title": "BleedOrigin: Dynamic Bleeding Source Localization in Endoscopic Submucosal Dissection via Dual-Stage Detection and Tracking", "categories": ["cs.CV", "cs.AI"], "comment": "27 pages, 14 figures", "summary": "Intraoperative bleeding during Endoscopic Submucosal Dissection (ESD) poses\nsignificant risks, demanding precise, real-time localization and continuous\nmonitoring of the bleeding source for effective hemostatic intervention. In\nparticular, endoscopists have to repeatedly flush to clear blood, allowing only\nmilliseconds to identify bleeding sources, an inefficient process that prolongs\noperations and elevates patient risks. However, current Artificial Intelligence\n(AI) methods primarily focus on bleeding region segmentation, overlooking the\ncritical need for accurate bleeding source detection and temporal tracking in\nthe challenging ESD environment, which is marked by frequent visual\nobstructions and dynamic scene changes. This gap is widened by the lack of\nspecialized datasets, hindering the development of robust AI-assisted guidance\nsystems. To address these challenges, we introduce BleedOrigin-Bench, the first\ncomprehensive ESD bleeding source dataset, featuring 1,771 expert-annotated\nbleeding sources across 106,222 frames from 44 procedures, supplemented with\n39,755 pseudo-labeled frames. This benchmark covers 8 anatomical sites and 6\nchallenging clinical scenarios. We also present BleedOrigin-Net, a novel\ndual-stage detection-tracking framework for the bleeding source localization in\nESD procedures, addressing the complete workflow from bleeding onset detection\nto continuous spatial tracking. We compare with widely-used object detection\nmodels (YOLOv11/v12), multimodal large language models, and point tracking\nmethods. Extensive evaluation demonstrates state-of-the-art performance,\nachieving 96.85% frame-level accuracy ($\\pm\\leq8$ frames) for bleeding onset\ndetection, 70.24% pixel-level accuracy ($\\leq100$ px) for initial source\ndetection, and 96.11% pixel-level accuracy ($\\leq100$ px) for point tracking.", "AI": {"tldr": "The paper introduces BleedOrigin-Bench, a dataset for bleeding source detection in ESD, and BleedOrigin-Net, a dual-stage AI framework for localization and tracking, achieving high accuracy.", "motivation": "Current AI methods for ESD bleeding focus on segmentation but lack accurate source detection and tracking, compounded by the absence of specialized datasets.", "method": "The authors develop BleedOrigin-Bench, a dataset with expert annotations, and BleedOrigin-Net, a detection-tracking framework, evaluated against existing models.", "result": "BleedOrigin-Net achieves 96.85% frame-level accuracy for onset detection, 70.24% for initial source detection, and 96.11% for tracking.", "conclusion": "The proposed framework and dataset address critical gaps in ESD bleeding management, offering robust AI-assisted guidance."}}
{"id": "2507.15850", "pdf": "https://arxiv.org/pdf/2507.15850", "abs": "https://arxiv.org/abs/2507.15850", "authors": ["Basma El Amel Boussaha", "Leen AlQadi", "Mugariya Farooq", "Shaikha Alsuwaidi", "Giulia Campesan", "Ahmed Alzubaidi", "Mohammed Alyafeai", "Hakim Hacid"], "title": "3LM: Bridging Arabic, STEM, and Code through Benchmarking", "categories": ["cs.CL"], "comment": null, "summary": "Arabic is one of the most widely spoken languages in the world, yet efforts\nto develop and evaluate Large Language Models (LLMs) for Arabic remain\nrelatively limited. Most existing Arabic benchmarks focus on linguistic,\ncultural, or religious content, leaving a significant gap in domains like STEM\nand code which are increasingly relevant for real-world LLM applications. To\nhelp bridge this gap, we present 3LM, a suite of three benchmarks designed\nspecifically for Arabic. The first is a set of STEM-related question-answer\npairs, naturally sourced from Arabic textbooks and educational worksheets. The\nsecond consists of synthetically generated STEM questions, created using the\nsame sources. The third benchmark focuses on code generation, built through a\ncareful translation of two widely used code benchmarks, incorporating a\nhuman-in-the-loop process with several rounds of review to ensure high-quality\nand faithful translations. We release all three benchmarks publicly to support\nthe growth of Arabic LLM research in these essential but underrepresented\nareas.", "AI": {"tldr": "The paper introduces 3LM, a suite of three Arabic benchmarks for STEM and code generation to address gaps in existing Arabic LLM evaluations.", "motivation": "Existing Arabic benchmarks focus on linguistic, cultural, or religious content, neglecting STEM and code domains crucial for real-world LLM applications.", "method": "3LM includes STEM-related Q&A pairs from Arabic textbooks, synthetic STEM questions, and a translated code benchmark with human review.", "result": "Three high-quality benchmarks for Arabic LLMs in STEM and code generation are released publicly.", "conclusion": "3LM aims to support Arabic LLM research in underrepresented but essential domains like STEM and code."}}
{"id": "2507.15109", "pdf": "https://arxiv.org/pdf/2507.15109", "abs": "https://arxiv.org/abs/2507.15109", "authors": ["Mohammad-Maher Nakshbandi", "Ziad Sharawy", "Sorin Grigorescu"], "title": "LoopNet: A Multitasking Few-Shot Learning Approach for Loop Closure in Large Scale SLAM", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "One of the main challenges in the Simultaneous Localization and Mapping\n(SLAM) loop closure problem is the recognition of previously visited places. In\nthis work, we tackle the two main problems of real-time SLAM systems: 1) loop\nclosure detection accuracy and 2) real-time computation constraints on the\nembedded hardware. Our LoopNet method is based on a multitasking variant of the\nclassical ResNet architecture, adapted for online retraining on a dynamic\nvisual dataset and optimized for embedded devices. The online retraining is\ndesigned using a few-shot learning approach. The architecture provides both an\nindex into the queried visual dataset, and a measurement of the prediction\nquality. Moreover, by leveraging DISK (DIStinctive Keypoints) descriptors,\nLoopNet surpasses the limitations of handcrafted features and traditional deep\nlearning methods, offering better performance under varying conditions. Code is\navailable at https://github.com/RovisLab/LoopNet. Additinally, we introduce a\nnew loop closure benchmarking dataset, coined LoopDB, which is available at\nhttps://github.com/RovisLab/LoopDB.", "AI": {"tldr": "LoopNet improves SLAM loop closure detection with multitasking ResNet, online retraining, and DISK descriptors, outperforming traditional methods.", "motivation": "Addressing accuracy and real-time computation challenges in SLAM loop closure detection.", "method": "Multitasking ResNet variant with online retraining (few-shot learning) and DISK descriptors.", "result": "Better performance under varying conditions; new LoopDB dataset introduced.", "conclusion": "LoopNet enhances SLAM loop closure with efficient, accurate, and adaptable solutions."}}
{"id": "2507.14179", "pdf": "https://arxiv.org/pdf/2507.14179", "abs": "https://arxiv.org/abs/2507.14179", "authors": ["Nobel Dhar", "Bobin Deng", "Md Romyull Islam", "Xinyue Zhang", "Kazi Fahim Ahmad Nasif", "Kun Suo"], "title": "A Sparsity Predicting Approach for Large Language Models via Activation Pattern Clustering", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DC"], "comment": "To be published in Euro-Par 2025", "summary": "Large Language Models (LLMs) exhibit significant activation sparsity, where\nonly a subset of neurons are active for a given input. Although this sparsity\npresents opportunities to reduce computational cost, efficiently utilizing it\nrequires predicting activation patterns in a scalable manner. However, direct\nprediction at the neuron level is computationally expensive due to the vast\nnumber of neurons in modern LLMs. To enable efficient prediction and\nutilization of activation sparsity, we propose a clustering-based activation\npattern compression framework. Instead of treating each neuron independently,\nwe group similar activation patterns into a small set of representative\nclusters. Our method achieves up to 79.34% clustering precision, outperforming\nstandard binary clustering approaches while maintaining minimal degradation in\nperplexity (PPL) scores. With a sufficiently large number of clusters, our\napproach attains a PPL score as low as 12.49, demonstrating its effectiveness\nin preserving model quality while reducing computational overhead. By\npredicting cluster assignments rather than individual neuron states, future\nmodels can efficiently infer activation patterns from pre-computed centroids.\nWe detail the clustering algorithm, analyze its effectiveness in capturing\nmeaningful activation structures, and demonstrate its potential to improve\nsparse computation efficiency. This clustering-based formulation serves as a\nfoundation for future work on activation pattern prediction, paving the way for\nefficient inference in large-scale language models.", "AI": {"tldr": "A clustering-based framework is proposed to efficiently predict and utilize activation sparsity in LLMs, reducing computational costs while maintaining model quality.", "motivation": "Activation sparsity in LLMs offers computational savings, but predicting neuron-level activation is impractical due to scale.", "method": "Clusters similar activation patterns into representative groups, reducing prediction complexity.", "result": "Achieves 79.34% clustering precision and a low PPL score of 12.49, balancing efficiency and quality.", "conclusion": "The framework enables scalable activation prediction, improving sparse computation efficiency for future LLMs."}}
{"id": "2507.15130", "pdf": "https://arxiv.org/pdf/2507.15130", "abs": "https://arxiv.org/abs/2507.15130", "authors": ["Ce Zhang", "Yale Song", "Ruta Desai", "Michael Louis Iuzzolino", "Joseph Tighe", "Gedas Bertasius", "Satwik Kottur"], "title": "Enhancing Visual Planning with Auxiliary Tasks and Multi-token Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Visual Planning for Assistance (VPA) aims to predict a sequence of user\nactions required to achieve a specified goal based on a video showing the\nuser's progress. Although recent advances in multimodal large language models\n(MLLMs) have shown promising results in video understanding, long-horizon\nvisual planning remains a challenging problem. We identify two challenges in\ntraining large MLLMs for video-based planning tasks: (1) scarcity of procedural\nannotations, limiting the model's ability to learn procedural task dynamics\neffectively, and (2) inefficiency of next-token prediction objective to\nexplicitly capture the structured action space for visual planning when\ncompared to free-form, natural language. To tackle data scarcity, we introduce\nAuxiliary Task Augmentation. We design and train our model on auxiliary tasks\nrelevant to long-horizon video-based planning (e.g., goal prediction) to\naugment the model's planning ability. To more explicitly model the structured\naction space unique to visual planning tasks, we leverage Multi-token\nPrediction, extending traditional next-token prediction by using multiple heads\nto predict multiple future tokens during training. Our approach, VideoPlan,\nachieves state-of-the-art VPA performance on the COIN and CrossTask datasets,\nsurpassing prior methods by 7.3% and 3.4%, respectively, when predicting 3\nfuture actions. We further extend our method to the challenging Ego4D Long-term\nAction Anticipation task, and show that it is on par with the state-of-the-art\napproaches despite not using specialized egocentric features. Code will be made\navailable.", "AI": {"tldr": "VPA predicts user action sequences from videos. Challenges include scarce procedural annotations and inefficient next-token prediction. Solutions: Auxiliary Task Augmentation and Multi-token Prediction. VideoPlan achieves SOTA results on COIN and CrossTask.", "motivation": "Address challenges in training MLLMs for video-based planning: data scarcity and inefficiency of next-token prediction for structured action spaces.", "method": "Introduces Auxiliary Task Augmentation and Multi-token Prediction to enhance planning ability and model structured action spaces.", "result": "VideoPlan outperforms prior methods by 7.3% and 3.4% on COIN and CrossTask, respectively, and matches SOTA on Ego4D.", "conclusion": "VideoPlan effectively tackles VPA challenges, achieving superior performance and generalizability."}}
{"id": "2507.14201", "pdf": "https://arxiv.org/pdf/2507.14201", "abs": "https://arxiv.org/abs/2507.14201", "authors": ["Yiran Wu", "Mauricio Velazco", "Andrew Zhao", "Manuel Ra\u00fal Mel\u00e9ndez Luj\u00e1n", "Srisuma Movva", "Yogesh K Roy", "Quang Nguyen", "Roberto Rodriguez", "Qingyun Wu", "Michael Albada", "Julia Kiseleva", "Anand Mudgerikar"], "title": "ExCyTIn-Bench: Evaluating LLM agents on Cyber Threat Investigation", "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": null, "summary": "We present ExCyTIn-Bench, the first benchmark to Evaluate an LLM agent x on\nthe task of Cyber Threat Investigation through security questions derived from\ninvestigation graphs. Real-world security analysts must sift through a large\nnumber of heterogeneous alert signals and security logs, follow multi-hop\nchains of evidence, and compile an incident report. With the developments of\nLLMs, building LLM-based agents for automatic thread investigation is a\npromising direction. To assist the development and evaluation of LLM agents, we\nconstruct a dataset from a controlled Azure tenant that covers 8 simulated\nreal-world multi-step attacks, 57 log tables from Microsoft Sentinel and\nrelated services, and 589 automatically generated questions. We leverage\nsecurity logs extracted with expert-crafted detection logic to build threat\ninvestigation graphs, and then generate questions with LLMs using paired nodes\non the graph, taking the start node as background context and the end node as\nanswer. Anchoring each question to these explicit nodes and edges not only\nprovides automatic, explainable ground truth answers but also makes the\npipeline reusable and readily extensible to new logs. This also enables the\nautomatic generation of procedural tasks with verifiable rewards, which can be\nnaturally extended to training agents via reinforcement learning. Our\ncomprehensive experiments with different models confirm the difficulty of the\ntask: with the base setting, the average reward across all evaluated models is\n0.249, and the best achieved is 0.368, leaving substantial headroom for future\nresearch. Code and data are coming soon!", "AI": {"tldr": "ExCyTIn-Bench is the first benchmark for evaluating LLM agents in cyber threat investigation using security questions derived from investigation graphs. It includes a dataset of 8 simulated attacks, 57 log tables, and 589 questions, with explainable ground truth answers. Initial experiments show low model performance, indicating room for improvement.", "motivation": "Real-world cyber threat investigation is complex, involving heterogeneous alerts and multi-hop evidence chains. LLM-based agents offer a promising solution, but lack benchmarks for development and evaluation.", "method": "Constructed a dataset from a controlled Azure tenant with simulated attacks, log tables, and LLM-generated questions anchored to investigation graphs. Questions use start nodes as context and end nodes as answers.", "result": "Experiments show low average reward (0.249) across models, with the best at 0.368, highlighting task difficulty.", "conclusion": "ExCyTIn-Bench provides a reusable, extensible benchmark for LLM agents in cyber threat investigation, with significant potential for future research."}}
{"id": "2507.15150", "pdf": "https://arxiv.org/pdf/2507.15150", "abs": "https://arxiv.org/abs/2507.15150", "authors": ["Aayush Atul Verma", "Arpitsinh Vaghela", "Bharatesh Chakravarthi", "Kaustav Chanda", "Yezhou Yang"], "title": "Event-based Graph Representation with Spatial and Motion Vectors for Asynchronous Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "Event-based sensors offer high temporal resolution and low latency by\ngenerating sparse, asynchronous data. However, converting this irregular data\ninto dense tensors for use in standard neural networks diminishes these\ninherent advantages, motivating research into graph representations. While such\nmethods preserve sparsity and support asynchronous inference, their performance\non downstream tasks remains limited due to suboptimal modeling of\nspatiotemporal dynamics. In this work, we propose a novel spatiotemporal\nmultigraph representation to better capture spatial structure and temporal\nchanges. Our approach constructs two decoupled graphs: a spatial graph\nleveraging B-spline basis functions to model global structure, and a temporal\ngraph utilizing motion vector-based attention for local dynamic changes. This\ndesign enables the use of efficient 2D kernels in place of computationally\nexpensive 3D kernels. We evaluate our method on the Gen1 automotive and eTraM\ndatasets for event-based object detection, achieving over a 6% improvement in\ndetection accuracy compared to previous graph-based works, with a 5x speedup,\nreduced parameter count, and no increase in computational cost. These results\nhighlight the effectiveness of structured graph modeling for asynchronous\nvision. Project page: eventbasedvision.github.io/eGSMV.", "AI": {"tldr": "A novel spatiotemporal multigraph representation improves event-based object detection by decoupling spatial and temporal dynamics, achieving higher accuracy and efficiency.", "motivation": "Event-based sensors' sparse, asynchronous data loses advantages when converted to dense tensors, and existing graph methods underperform due to poor spatiotemporal modeling.", "method": "Proposes a spatiotemporal multigraph with decoupled spatial (B-spline basis) and temporal (motion vector-based attention) graphs, replacing 3D kernels with efficient 2D ones.", "result": "Achieves 6% higher detection accuracy, 5x speedup, fewer parameters, and no added computational cost on Gen1 and eTraM datasets.", "conclusion": "Structured graph modeling effectively enhances asynchronous vision tasks, balancing performance and efficiency."}}
{"id": "2507.14204", "pdf": "https://arxiv.org/pdf/2507.14204", "abs": "https://arxiv.org/abs/2507.14204", "authors": ["Dachuan Shi", "Yonggan Fu", "Xiangchi Yuan", "Zhongzhi Yu", "Haoran You", "Sixu Li", "Xin Dong", "Jan Kautz", "Pavlo Molchanov", "Yingyan", "Lin"], "title": "LaCache: Ladder-Shaped KV Caching for Efficient Long-Context Modeling of Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "ICML 2025. Code: https://github.com/GATECH-EIC/LaCache", "summary": "Recent advancements in Large Language Models (LLMs) have spurred interest in\nnumerous applications requiring robust long-range capabilities, essential for\nprocessing extensive input contexts and continuously generating extended\noutputs. As sequence lengths increase, the number of Key-Value (KV) pairs in\nLLMs escalates, creating a significant efficiency bottleneck. In this paper, we\npropose a new KV cache optimization paradigm called LaCache, a training-free\nmethod for efficient and accurate generative inference of LLMs. LaCache enables\nLLMs to simultaneously address both of the critical challenges in long-range\nmodeling: robust long-range capabilities and continuous generation without\nrunning out-of-memory (OOM). Specifically, LaCache integrates two key\ninnovations: (1) a ladder-shaped KV cache pattern that stores KV pairs not only\nsequentially (left-to-right within each layer) but also across layers (from\nshallow to deep), providing an extended span for capturing long-range\ndependencies under a fixed storage budget, thereby boosting long-range\ncapabilities; and (2) an iterative compaction mechanism that progressively\ncompresses older caches, freeing up space for new tokens within a fixed cache\nsize. This token distance-based dynamic compression enables more effective\ncontinuous generation under constrained cache budgets. Experiments across\nvarious tasks, benchmarks, and LLM models consistently validate LaCache's\neffectiveness in enhancing LLMs' long-range capabilities. Our code is available\nat https://github.com/GATECH-EIC/LaCache.", "AI": {"tldr": "LaCache is a training-free KV cache optimization method for LLMs, enhancing long-range capabilities and continuous generation without OOM by using a ladder-shaped KV cache pattern and iterative compaction.", "motivation": "Addressing the efficiency bottleneck caused by increasing KV pairs in LLMs as sequence lengths grow, while maintaining robust long-range capabilities and avoiding OOM errors.", "method": "LaCache integrates a ladder-shaped KV cache pattern for extended dependency capture and an iterative compaction mechanism for dynamic cache compression.", "result": "Experiments show LaCache effectively boosts LLMs' long-range capabilities across tasks and benchmarks.", "conclusion": "LaCache provides an efficient and accurate solution for long-range modeling in LLMs, validated by consistent experimental results."}}
{"id": "2507.15212", "pdf": "https://arxiv.org/pdf/2507.15212", "abs": "https://arxiv.org/abs/2507.15212", "authors": ["Yusuke Yoshiyasu", "Leyuan Sun", "Ryusuke Sagawa"], "title": "MeshMamba: State Space Models for Articulated 3D Mesh Generation and Reconstruction", "categories": ["cs.CV"], "comment": "Accepted at ICCV2025", "summary": "In this paper, we introduce MeshMamba, a neural network model for learning 3D\narticulated mesh models by employing the recently proposed Mamba State Space\nModels (Mamba-SSMs). MeshMamba is efficient and scalable in handling a large\nnumber of input tokens, enabling the generation and reconstruction of body mesh\nmodels with more than 10,000 vertices, capturing clothing and hand geometries.\nThe key to effectively learning MeshMamba is the serialization technique of\nmesh vertices into orderings that are easily processed by Mamba. This is\nachieved by sorting the vertices based on body part annotations or the 3D\nvertex locations of a template mesh, such that the ordering respects the\nstructure of articulated shapes. Based on MeshMamba, we design 1) MambaDiff3D,\na denoising diffusion model for generating 3D articulated meshes and 2)\nMamba-HMR, a 3D human mesh recovery model that reconstructs a human body shape\nand pose from a single image. Experimental results showed that MambaDiff3D can\ngenerate dense 3D human meshes in clothes, with grasping hands, etc., and\noutperforms previous approaches in the 3D human shape generation task.\nAdditionally, Mamba-HMR extends the capabilities of previous non-parametric\nhuman mesh recovery approaches, which were limited to handling body-only poses\nusing around 500 vertex tokens, to the whole-body setting with face and hands,\nwhile achieving competitive performance in (near) real-time.", "AI": {"tldr": "MeshMamba is a neural network model using Mamba-SSMs for efficient 3D articulated mesh learning, enabling large-scale mesh generation and reconstruction with over 10,000 vertices. It introduces MambaDiff3D for mesh generation and Mamba-HMR for single-image human mesh recovery, outperforming prior methods.", "motivation": "To address the inefficiency and scalability challenges in learning 3D articulated mesh models, especially for large vertex counts, and to extend capabilities to whole-body reconstruction including clothing and hands.", "method": "MeshMamba serializes mesh vertices into orderings (via body part annotations or 3D template locations) for Mamba-SSMs. It includes MambaDiff3D (denoising diffusion for mesh generation) and Mamba-HMR (single-image human mesh recovery).", "result": "MambaDiff3D generates dense 3D human meshes with clothing and hands, outperforming previous methods. Mamba-HMR handles whole-body reconstruction (face, hands) in near real-time, extending prior body-only approaches.", "conclusion": "MeshMamba demonstrates scalability and efficiency in 3D articulated mesh tasks, with applications in generation and reconstruction, achieving state-of-the-art performance."}}
{"id": "2507.14221", "pdf": "https://arxiv.org/pdf/2507.14221", "abs": "https://arxiv.org/abs/2507.14221", "authors": ["Eoghan Cunningham", "James Cross", "Derek Greene"], "title": "Identifying Algorithmic and Domain-Specific Bias in Parliamentary Debate Summarisation", "categories": ["cs.CY", "cs.CL", "cs.LG"], "comment": null, "summary": "The automated summarisation of parliamentary debates using large language\nmodels (LLMs) offers a promising way to make complex legislative discourse more\naccessible to the public. However, such summaries must not only be accurate and\nconcise but also equitably represent the views and contributions of all\nspeakers. This paper explores the use of LLMs to summarise plenary debates from\nthe European Parliament and investigates the algorithmic and representational\nbiases that emerge in this context. We propose a structured, multi-stage\nsummarisation framework that improves textual coherence and content fidelity,\nwhile enabling the systematic analysis of how speaker attributes -- such as\nspeaking order or political affiliation -- influence the visibility and\naccuracy of their contributions in the final summaries. Through our experiments\nusing both proprietary and open-weight LLMs, we find evidence of consistent\npositional and partisan biases, with certain speakers systematically\nunder-represented or misattributed. Our analysis shows that these biases vary\nby model and summarisation strategy, with hierarchical approaches offering the\ngreatest potential to reduce disparity. These findings underscore the need for\ndomain-sensitive evaluation metrics and ethical oversight in the deployment of\nLLMs for democratic applications.", "AI": {"tldr": "The paper investigates biases in LLM-generated summaries of European Parliament debates, proposing a multi-stage framework to improve fairness and analyzing positional and partisan biases.", "motivation": "To make parliamentary debates more accessible while ensuring equitable representation of all speakers in summaries.", "method": "A structured, multi-stage summarisation framework using LLMs, tested on proprietary and open-weight models to analyze biases.", "result": "Found consistent positional and partisan biases, with hierarchical summarisation strategies showing potential to reduce disparities.", "conclusion": "Highlights the need for domain-sensitive metrics and ethical oversight in LLM applications for democracy."}}
{"id": "2507.15216", "pdf": "https://arxiv.org/pdf/2507.15216", "abs": "https://arxiv.org/abs/2507.15216", "authors": ["Yuping Qiu", "Rui Zhu", "Ying-cong Chen"], "title": "Improving Joint Embedding Predictive Architecture with Diffusion Noise", "categories": ["cs.CV"], "comment": null, "summary": "Self-supervised learning has become an incredibly successful method for\nfeature learning, widely applied to many downstream tasks. It has proven\nespecially effective for discriminative tasks, surpassing the trending\ngenerative models. However, generative models perform better in image\ngeneration and detail enhancement. Thus, it is natural for us to find a\nconnection between SSL and generative models to further enhance the\nrepresentation capacity of SSL. As generative models can create new samples by\napproximating the data distribution, such modeling should also lead to a\nsemantic understanding of the raw visual data, which is necessary for\nrecognition tasks. This enlightens us to combine the core principle of the\ndiffusion model: diffusion noise, with SSL to learn a competitive recognition\nmodel. Specifically, diffusion noise can be viewed as a particular state of\nmask that reveals a close relationship between masked image modeling (MIM) and\ndiffusion models. In this paper, we propose N-JEPA (Noise-based JEPA) to\nincorporate diffusion noise into MIM by the position embedding of masked\ntokens. The multi-level noise schedule is a series of feature augmentations to\nfurther enhance the robustness of our model. We perform a comprehensive study\nto confirm its effectiveness in the classification of downstream tasks. Codes\nwill be released soon in public.", "AI": {"tldr": "The paper proposes N-JEPA, a method combining diffusion noise with masked image modeling (MIM) in self-supervised learning (SSL) to enhance feature learning for recognition tasks.", "motivation": "To bridge the gap between SSL (effective for discriminative tasks) and generative models (superior in image generation) by leveraging diffusion noise for semantic understanding.", "method": "Introduces N-JEPA, integrating diffusion noise into MIM via position embedding of masked tokens and using a multi-level noise schedule for feature augmentation.", "result": "Demonstrates effectiveness in downstream classification tasks.", "conclusion": "N-JEPA successfully combines SSL and generative principles, enhancing representation capacity for recognition tasks."}}
{"id": "2507.14293", "pdf": "https://arxiv.org/pdf/2507.14293", "abs": "https://arxiv.org/abs/2507.14293", "authors": ["Boyuan Zheng", "Zeyi Liao", "Scott Salisbury", "Zeyuan Liu", "Michael Lin", "Qinyuan Zheng", "Zifan Wang", "Xiang Deng", "Dawn Song", "Huan Sun", "Yu Su"], "title": "WebGuard: Building a Generalizable Guardrail for Web Agents", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "We publicly release WebGuard, along with its annotation tools and\n  fine-tuned models, to facilitate open-source research on monitoring and\n  safeguarding web agents. All resources are available at\n  https://github.com/OSU-NLP-Group/WebGuard", "summary": "The rapid development of autonomous web agents powered by Large Language\nModels (LLMs), while greatly elevating efficiency, exposes the frontier risk of\ntaking unintended or harmful actions. This situation underscores an urgent need\nfor effective safety measures, akin to access controls for human users. To\naddress this critical challenge, we introduce WebGuard, the first comprehensive\ndataset designed to support the assessment of web agent action risks and\nfacilitate the development of guardrails for real-world online environments. In\ndoing so, WebGuard specifically focuses on predicting the outcome of\nstate-changing actions and contains 4,939 human-annotated actions from 193\nwebsites across 22 diverse domains, including often-overlooked long-tail\nwebsites. These actions are categorized using a novel three-tier risk schema:\nSAFE, LOW, and HIGH. The dataset includes designated training and test splits\nto support evaluation under diverse generalization settings. Our initial\nevaluations reveal a concerning deficiency: even frontier LLMs achieve less\nthan 60% accuracy in predicting action outcomes and less than 60% recall in\nlagging HIGH-risk actions, highlighting the risks of deploying\ncurrent-generation agents without dedicated safeguards. We therefore\ninvestigate fine-tuning specialized guardrail models using WebGuard. We conduct\ncomprehensive evaluations across multiple generalization settings and find that\na fine-tuned Qwen2.5VL-7B model yields a substantial improvement in\nperformance, boosting accuracy from 37% to 80% and HIGH-risk action recall from\n20% to 76%. Despite these improvements, the performance still falls short of\nthe reliability required for high-stakes deployment, where guardrails must\napproach near-perfect accuracy and recall.", "AI": {"tldr": "WebGuard is a dataset for assessing web agent action risks, revealing LLMs' poor performance in predicting outcomes and highlighting the need for specialized guardrail models.", "motivation": "The rise of autonomous web agents powered by LLMs poses risks of unintended or harmful actions, necessitating safety measures like WebGuard.", "method": "WebGuard includes 4,939 human-annotated actions from 193 websites, categorized into SAFE, LOW, and HIGH risk. It supports fine-tuning guardrail models.", "result": "Frontier LLMs perform poorly (accuracy <60%, recall <60% for HIGH-risk actions). Fine-tuning improves accuracy (37% to 80%) and recall (20% to 76%).", "conclusion": "Despite improvements, guardrail models still lack the reliability for high-stakes deployment, requiring near-perfect accuracy and recall."}}
{"id": "2507.15223", "pdf": "https://arxiv.org/pdf/2507.15223", "abs": "https://arxiv.org/abs/2507.15223", "authors": ["Siqi Chen", "Guoqing Zhang", "Jiahao Lai", "Bingzhi Shen", "Sihong Zhang", "Caixia Dong", "Xuejin Chen", "Yang Li"], "title": "Hierarchical Part-based Generative Model for Realistic 3D Blood Vessel", "categories": ["cs.CV"], "comment": null, "summary": "Advancements in 3D vision have increased the impact of blood vessel modeling\non medical applications. However, accurately representing the complex geometry\nand topology of blood vessels remains a challenge due to their intricate\nbranching patterns, curvatures, and irregular shapes. In this study, we propose\na hierarchical part-based frame work for 3D vessel generation that separates\nthe global binary tree-like topology from local geometric details. Our approach\nproceeds in three stages: (1) key graph generation to model the overall\nhierarchical struc ture, (2) vessel segment generation conditioned on geometric\nproperties, and (3) hierarchical vessel assembly by integrating the local\nsegments according to the global key graph. We validate our framework on real\nworld datasets, demonstrating superior performance over existing methods in\nmodeling complex vascular networks. This work marks the first successful\napplication of a part-based generative approach for 3D vessel modeling, setting\na new benchmark for vascular data generation. The code is available at:\nhttps://github.com/CybercatChen/PartVessel.git.", "AI": {"tldr": "A hierarchical part-based framework for 3D blood vessel generation, separating global topology from local geometry, outperforms existing methods.", "motivation": "Accurately modeling the complex geometry and topology of blood vessels is challenging due to their intricate branching patterns and irregular shapes.", "method": "A three-stage approach: (1) key graph generation for hierarchical structure, (2) vessel segment generation based on geometric properties, and (3) hierarchical vessel assembly.", "result": "Validated on real-world datasets, the framework demonstrates superior performance in modeling complex vascular networks.", "conclusion": "This work is the first successful part-based generative approach for 3D vessel modeling, setting a new benchmark."}}
{"id": "2507.14353", "pdf": "https://arxiv.org/pdf/2507.14353", "abs": "https://arxiv.org/abs/2507.14353", "authors": ["Harsh Nilesh Pathak", "Randy Paffenroth"], "title": "Solo Connection: A Parameter Efficient Fine-Tuning Technique for Transformers", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Parameter efficient fine tuning (PEFT) is a versatile and extensible approach\nfor adapting a Large Language Model (LLM) for newer tasks. One of the most\nprominent PEFT approaches, Low Rank Adaptation (LoRA), primarily focuses on\nadjusting the attention weight matrices within individual decoder blocks of a\nGenerative Pre trained Transformer (GPT2). In contrast, we introduce Solo\nConnection a novel method that adapts the representation at the decoder-block\nlevel rather than modifying individual weight matrices. Not only does Solo\nConnection outperform LoRA on E2E natural language generation benchmarks, but\nit also reduces the number of trainable parameters by 59% relative to LoRA and\nby more than 99% compared to full fine-tuning of GPT2, an early version of\nLarge Language Models (LLMs). Solo Connection is also motivated by homotopy\ntheory: we introduce a trainable linear transformation that gradually\ninterpolates between a zero vector and the task-specific representation,\nenabling smooth and stable adaptation over time. While skip connections in the\noriginal 12 layer GPT2 are typically confined to individual decoder blocks,\nsubsequent GPT2 variants scale up to 48 layers, and even larger language models\ncan include 128 or more decoder blocks. These expanded architectures underscore\nthe need to revisit how skip connections are employed during fine-tuning. This\npaper focuses on long skip connections that link outputs of different decoder\nblocks, potentially enhancing the model's ability to adapt to new tasks while\nleveraging pre-trained knowledge.", "AI": {"tldr": "Solo Connection is a PEFT method adapting decoder-block representations, outperforming LoRA with fewer parameters and inspired by homotopy theory.", "motivation": "To improve fine-tuning efficiency and stability by adapting decoder-block representations rather than individual weight matrices, inspired by homotopy theory.", "method": "Introduces Solo Connection, a trainable linear transformation interpolating between zero and task-specific representations, focusing on long skip connections between decoder blocks.", "result": "Outperforms LoRA on E2E benchmarks, reduces trainable parameters by 59% vs. LoRA and >99% vs. full fine-tuning.", "conclusion": "Solo Connection offers a more efficient and stable fine-tuning approach, especially beneficial for larger models with many decoder blocks."}}
{"id": "2507.15227", "pdf": "https://arxiv.org/pdf/2507.15227", "abs": "https://arxiv.org/abs/2507.15227", "authors": ["Krishna Kanth Nakka"], "title": "Mammo-SAE: Interpreting Breast Cancer Concept Learning with Sparse Autoencoders", "categories": ["cs.CV"], "comment": "Preprint. Under review", "summary": "Interpretability is critical in high-stakes domains such as medical imaging,\nwhere understanding model decisions is essential for clinical adoption. In this\nwork, we introduce Sparse Autoencoder (SAE)-based interpretability to breast\nimaging by analyzing {Mammo-CLIP}, a vision--language foundation model\npretrained on large-scale mammogram image--report pairs. We train a patch-level\n\\texttt{Mammo-SAE} on Mammo-CLIP to identify and probe latent features\nassociated with clinically relevant breast concepts such as \\textit{mass} and\n\\textit{suspicious calcification}. Our findings reveal that top activated class\nlevel latent neurons in the SAE latent space often tend to align with ground\ntruth regions, and also uncover several confounding factors influencing the\nmodel's decision-making process. Additionally, we analyze which latent neurons\nthe model relies on during downstream finetuning for improving the breast\nconcept prediction. This study highlights the promise of interpretable SAE\nlatent representations in providing deeper insight into the internal workings\nof foundation models at every layer for breast imaging.", "AI": {"tldr": "The paper introduces Sparse Autoencoder (SAE)-based interpretability to breast imaging using Mammo-CLIP, identifying clinically relevant features and confounding factors in model decisions.", "motivation": "Interpretability is crucial in medical imaging for clinical adoption, especially in understanding model decisions.", "method": "A patch-level Mammo-SAE is trained on Mammo-CLIP to probe latent features linked to breast concepts like mass and suspicious calcification.", "result": "Top activated latent neurons align with ground truth regions, and confounding factors in decision-making are uncovered. The study also identifies neurons used in downstream finetuning.", "conclusion": "SAE-based interpretability offers deeper insights into foundation models for breast imaging, aiding clinical understanding."}}
{"id": "2507.14384", "pdf": "https://arxiv.org/pdf/2507.14384", "abs": "https://arxiv.org/abs/2507.14384", "authors": ["Angjelin Hila", "Elliott Hauser"], "title": "Assessing the Reliability of Large Language Models for Deductive Qualitative Coding: A Comparative Study of ChatGPT Interventions", "categories": ["cs.HC", "cs.CL"], "comment": "Extended version of paper accepted for presentation at the ASIS&T\n  Annual Meeting 2025. 38 pages, 12 figures", "summary": "In this study, we investigate the use of large language models (LLMs),\nspecifically ChatGPT, for structured deductive qualitative coding. While most\ncurrent research emphasizes inductive coding applications, we address the\nunderexplored potential of LLMs to perform deductive classification tasks\naligned with established human-coded schemes. Using the Comparative Agendas\nProject (CAP) Master Codebook, we classified U.S. Supreme Court case summaries\ninto 21 major policy domains. We tested four intervention methods: zero-shot,\nfew-shot, definition-based, and a novel Step-by-Step Task Decomposition\nstrategy, across repeated samples. Performance was evaluated using standard\nclassification metrics (accuracy, F1-score, Cohen's kappa, Krippendorff's\nalpha), and construct validity was assessed using chi-squared tests and\nCramer's V. Chi-squared and effect size analyses confirmed that intervention\nstrategies significantly influenced classification behavior, with Cramer's V\nvalues ranging from 0.359 to 0.613, indicating moderate to strong shifts in\nclassification patterns. The Step-by-Step Task Decomposition strategy achieved\nthe strongest reliability (accuracy = 0.775, kappa = 0.744, alpha = 0.746),\nachieving thresholds for substantial agreement. Despite the semantic ambiguity\nwithin case summaries, ChatGPT displayed stable agreement across samples,\nincluding high F1 scores in low-support subclasses. These findings demonstrate\nthat with targeted, custom-tailored interventions, LLMs can achieve reliability\nlevels suitable for integration into rigorous qualitative coding workflows.", "AI": {"tldr": "The study explores using ChatGPT for deductive qualitative coding, testing four methods, with Step-by-Step Task Decomposition showing the highest reliability.", "motivation": "To address the underexplored potential of LLMs in deductive classification tasks aligned with human-coded schemes.", "method": "Tested four intervention methods (zero-shot, few-shot, definition-based, Step-by-Step Task Decomposition) on U.S. Supreme Court case summaries using the CAP Master Codebook. Evaluated performance with classification metrics and construct validity tests.", "result": "Step-by-Step Task Decomposition achieved the highest reliability (accuracy = 0.775, kappa = 0.744, alpha = 0.746). Intervention strategies significantly influenced classification behavior.", "conclusion": "Targeted interventions enable LLMs to achieve reliability suitable for rigorous qualitative coding workflows."}}
{"id": "2507.15243", "pdf": "https://arxiv.org/pdf/2507.15243", "abs": "https://arxiv.org/abs/2507.15243", "authors": ["Naeem Paeedeh", "Mahardhika Pratama", "Wolfgang Mayer", "Jimmy Cao", "Ryszard Kowlczyk"], "title": "Cross-Domain Few-Shot Learning with Coalescent Projections and Latent Space Reservation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Despite the progress in Cross-Domain Few-Shot Learning (CD-FSL), a model\npre-trained with DINO combined with a prototypical classifier outperforms the\nlatest SOTA methods. A crucial limitation that needs to be overcome is that\nupdating too many parameters of the transformers leads to overfitting due to\nthe scarcity of labeled samples. To address this challenge, we propose a new\nconcept, Coalescent Projection (CP), as an effective successor to soft prompts.\nAdditionally, we propose a novel pseudo-class generation method combined with\nSelf-Supervised Transformations (SSTs) that relies solely on the base domain to\nprepare the network for encountering unseen samples from different domains. The\nproposed method exhibits its effectiveness in comprehensive experiments on the\nextreme domain shift scenario of the BSCD-FSL benchmark. Our code is published\nat https://github.com/Naeem-Paeedeh/CPLSR.", "AI": {"tldr": "A new method, Coalescent Projection (CP), combined with pseudo-class generation and Self-Supervised Transformations (SSTs), outperforms SOTA in CD-FSL by addressing overfitting and domain shift.", "motivation": "Overcoming overfitting in CD-FSL due to limited labeled samples and extreme domain shifts.", "method": "Introduces Coalescent Projection (CP) and pseudo-class generation with SSTs, leveraging base domain data for unseen domains.", "result": "Outperforms SOTA methods on the BSCD-FSL benchmark, especially in extreme domain shifts.", "conclusion": "CP and SSTs effectively address CD-FSL challenges, offering a robust solution for domain adaptation."}}
{"id": "2507.14417", "pdf": "https://arxiv.org/pdf/2507.14417", "abs": "https://arxiv.org/abs/2507.14417", "authors": ["Aryo Pradipta Gema", "Alexander H\u00e4gele", "Runjin Chen", "Andy Arditi", "Jacob Goldman-Wetzler", "Kit Fraser-Taliente", "Henry Sleight", "Linda Petrini", "Julian Michael", "Beatrice Alex", "Pasquale Minervini", "Yanda Chen", "Joe Benton", "Ethan Perez"], "title": "Inverse Scaling in Test-Time Compute", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "We construct evaluation tasks where extending the reasoning length of Large\nReasoning Models (LRMs) deteriorates performance, exhibiting an inverse scaling\nrelationship between test-time compute and accuracy. Our evaluation tasks span\nfour categories: simple counting tasks with distractors, regression tasks with\nspurious features, deduction tasks with constraint tracking, and advanced AI\nrisks. We identify five distinct failure modes when models reason for longer:\n1) Claude models become increasingly distracted by irrelevant information; 2)\nOpenAI o-series models resist distractors but overfit to problem framings; 3)\nmodels shift from reasonable priors to spurious correlations; 4) all models\nshow difficulties in maintaining focus on complex deductive tasks; and 5)\nextended reasoning may amplify concerning behaviors, with Claude Sonnet 4\nshowing increased expressions of self-preservation. These findings suggest that\nwhile test-time compute scaling remains promising for improving model\ncapabilities, it may inadvertently reinforce problematic reasoning patterns.\nOur results demonstrate the importance of evaluating models across diverse\nreasoning lengths to identify and address these failure modes in LRMs.", "AI": {"tldr": "Extending reasoning length in Large Reasoning Models (LRMs) can degrade performance, revealing inverse scaling between compute and accuracy. Five failure modes are identified, highlighting risks of prolonged reasoning.", "motivation": "To investigate how extended reasoning affects LRM performance and identify potential failure modes, ensuring safe and effective model scaling.", "method": "Constructed evaluation tasks across four categories (counting, regression, deduction, AI risks) to test models under varying reasoning lengths.", "result": "Five failure modes emerged, including distraction, overfitting, spurious correlations, focus loss, and amplified concerning behaviors.", "conclusion": "While test-time compute scaling is promising, it risks reinforcing problematic reasoning. Diverse reasoning-length evaluations are crucial for LRM safety."}}
{"id": "2507.15249", "pdf": "https://arxiv.org/pdf/2507.15249", "abs": "https://arxiv.org/abs/2507.15249", "authors": ["Yanbing Zhang", "Zhe Wang", "Qin Zhou", "Mengping Yang"], "title": "FreeCus: Free Lunch Subject-driven Customization in Diffusion Transformers", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "In light of recent breakthroughs in text-to-image (T2I) generation,\nparticularly with diffusion transformers (DiT), subject-driven technologies are\nincreasingly being employed for high-fidelity customized production that\npreserves subject identity from reference inputs, enabling thrilling design\nworkflows and engaging entertainment. Existing alternatives typically require\neither per-subject optimization via trainable text embeddings or training\nspecialized encoders for subject feature extraction on large-scale datasets.\nSuch dependencies on training procedures fundamentally constrain their\npractical applications. More importantly, current methodologies fail to fully\nleverage the inherent zero-shot potential of modern diffusion transformers\n(e.g., the Flux series) for authentic subject-driven synthesis. To bridge this\ngap, we propose FreeCus, a genuinely training-free framework that activates\nDiT's capabilities through three key innovations: 1) We introduce a pivotal\nattention sharing mechanism that captures the subject's layout integrity while\npreserving crucial editing flexibility. 2) Through a straightforward analysis\nof DiT's dynamic shifting, we propose an upgraded variant that significantly\nimproves fine-grained feature extraction. 3) We further integrate advanced\nMultimodal Large Language Models (MLLMs) to enrich cross-modal semantic\nrepresentations. Extensive experiments reflect that our method successfully\nunlocks DiT's zero-shot ability for consistent subject synthesis across diverse\ncontexts, achieving state-of-the-art or comparable results compared to\napproaches that require additional training. Notably, our framework\ndemonstrates seamless compatibility with existing inpainting pipelines and\ncontrol modules, facilitating more compelling experiences. Our code is\navailable at: https://github.com/Monalissaa/FreeCus.", "AI": {"tldr": "FreeCus is a training-free framework leveraging diffusion transformers for subject-driven synthesis, introducing attention sharing, dynamic shifting analysis, and MLLM integration to achieve zero-shot capabilities.", "motivation": "Existing methods for subject-driven synthesis rely on training procedures, limiting practicality and failing to utilize the zero-shot potential of diffusion transformers.", "method": "FreeCus introduces attention sharing, an upgraded DiT variant for feature extraction, and integrates MLLMs for cross-modal semantics.", "result": "The framework achieves state-of-the-art or comparable results without training, enabling consistent subject synthesis and compatibility with existing pipelines.", "conclusion": "FreeCus successfully unlocks DiT's zero-shot potential for subject-driven synthesis, offering practical and flexible applications."}}
{"id": "2507.14419", "pdf": "https://arxiv.org/pdf/2507.14419", "abs": "https://arxiv.org/abs/2507.14419", "authors": ["Guojun Wu"], "title": "It's Not That Simple. An Analysis of Simple Test-Time Scaling", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Prior work proposed simple test-time scaling, a method for replicating this\nscaling behavior with models distilled from o1-like models by manually\ncontrolling test-time compute: either scaling down by enforcing a maximum\nlength or scaling up by iteratively appending \"Wait\" when the model is about to\nterminate its generation. This paper presents an analysis of simple test-time\nscaling and finds that the scaling behavior is largely attributed to scaling\ndown by enforcing a maximum length. In contrast, fine-tuning on long CoT data\ndistilled from o1-like models has no significant impact on scaling behavior,\nand scaling up by appending \"Wait\" leads to inconsistencies, as the model may\noscillate between solutions. A key distinction exists between scaling down by\nenforcing a maximum length and scaling up test-time compute in o1-like models,\nsuch as DeepSeek-R1\\@. These models are typically allowed to utilize as much\ncompute as needed, with the only constraint being the model's maximum supported\nlength. By learning to naturally scale up test-time compute during\nreinforcement learning, o1-like models surpass their peak performance when\nscaling up. In contrast, simple test-time scaling progressively imposes a lower\nupper limit on model performance as it scales down. While replicating the\ntest-time scaling behavior of o1 models can be straightforward by scaling down,\nit is crucial to recognize that the goal of scaling test-time compute is to\nunlock higher performance -- beyond what the model could originally achieve --\nrather than merely reproducing the appearance of scaling behavior.", "AI": {"tldr": "Simple test-time scaling is mainly effective when scaling down by enforcing a maximum length, while scaling up via 'Wait' appending causes inconsistencies. Fine-tuning on long CoT data has little impact. o1-like models naturally scale up compute during RL, outperforming simple scaling methods.", "motivation": "To analyze the effectiveness of simple test-time scaling methods and compare them with the natural scaling behavior of o1-like models.", "method": "Examined scaling down (enforcing max length) and scaling up (appending 'Wait') in test-time compute, and compared with o1-like models' natural scaling during RL.", "result": "Scaling down works, but scaling up leads to inconsistencies. Fine-tuning on long CoT data is ineffective. o1-like models outperform when scaling up naturally.", "conclusion": "Simple test-time scaling can replicate scaling behavior but limits performance. The goal should be unlocking higher performance, not just mimicking scaling."}}
{"id": "2507.15257", "pdf": "https://arxiv.org/pdf/2507.15257", "abs": "https://arxiv.org/abs/2507.15257", "authors": ["Pei An", "Jiaqi Yang", "Muyao Peng", "You Yang", "Qiong Liu", "Xiaolin Wu", "Liangliang Nan"], "title": "MinCD-PnP: Learning 2D-3D Correspondences with Approximate Blind PnP", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Image-to-point-cloud (I2P) registration is a fundamental problem in computer\nvision, focusing on establishing 2D-3D correspondences between an image and a\npoint cloud. The differential perspective-n-point (PnP) has been widely used to\nsupervise I2P registration networks by enforcing the projective constraints on\n2D-3D correspondences. However, differential PnP is highly sensitive to noise\nand outliers in the predicted correspondences. This issue hinders the\neffectiveness of correspondence learning. Inspired by the robustness of blind\nPnP against noise and outliers in correspondences, we propose an approximated\nblind PnP based correspondence learning approach. To mitigate the high\ncomputational cost of blind PnP, we simplify blind PnP to an amenable task of\nminimizing Chamfer distance between learned 2D and 3D keypoints, called\nMinCD-PnP. To effectively solve MinCD-PnP, we design a lightweight multi-task\nlearning module, named as MinCD-Net, which can be easily integrated into the\nexisting I2P registration architectures. Extensive experiments on 7-Scenes,\nRGBD-V2, ScanNet, and self-collected datasets demonstrate that MinCD-Net\noutperforms state-of-the-art methods and achieves a higher inlier ratio (IR)\nand registration recall (RR) in both cross-scene and cross-dataset settings.", "AI": {"tldr": "The paper proposes MinCD-PnP, a robust method for image-to-point-cloud registration by simplifying blind PnP to minimize Chamfer distance, outperforming existing methods.", "motivation": "Differential PnP is sensitive to noise and outliers, limiting correspondence learning. Blind PnP is robust but computationally expensive.", "method": "Simplifies blind PnP to minimize Chamfer distance (MinCD-PnP) and introduces MinCD-Net, a lightweight multi-task learning module.", "result": "MinCD-Net achieves higher inlier ratio and registration recall across datasets like 7-Scenes and ScanNet.", "conclusion": "The proposed MinCD-PnP and MinCD-Net effectively address noise and outlier issues, improving I2P registration performance."}}
{"id": "2507.14447", "pdf": "https://arxiv.org/pdf/2507.14447", "abs": "https://arxiv.org/abs/2507.14447", "authors": ["Guancheng Zeng", "Xueyi Chen", "Jiawang Hu", "Shaohua Qi", "Yaxuan Mao", "Zhantao Wang", "Yifan Nie", "Shuang Li", "Qiuyang Feng", "Pengxu Qiu", "Yujia Wang", "Wenqiang Han", "Linyan Huang", "Gang Li", "Jingjing Mo", "Haowen Hu"], "title": "Routine: A Structural Planning Framework for LLM Agent System in Enterprise", "categories": ["cs.AI", "cs.CL"], "comment": "26 pages, 8 figures, 5 tables", "summary": "The deployment of agent systems in an enterprise environment is often\nhindered by several challenges: common models lack domain-specific process\nknowledge, leading to disorganized plans, missing key tools, and poor execution\nstability. To address this, this paper introduces Routine, a multi-step agent\nplanning framework designed with a clear structure, explicit instructions, and\nseamless parameter passing to guide the agent's execution module in performing\nmulti-step tool-calling tasks with high stability. In evaluations conducted\nwithin a real-world enterprise scenario, Routine significantly increases the\nexecution accuracy in model tool calls, increasing the performance of GPT-4o\nfrom 41.1% to 96.3%, and Qwen3-14B from 32.6% to 83.3%. We further constructed\na Routine-following training dataset and fine-tuned Qwen3-14B, resulting in an\naccuracy increase to 88.2% on scenario-specific evaluations, indicating\nimproved adherence to execution plans. In addition, we employed Routine-based\ndistillation to create a scenario-specific, multi-step tool-calling dataset.\nFine-tuning on this distilled dataset raised the model's accuracy to 95.5%,\napproaching GPT-4o's performance. These results highlight Routine's\neffectiveness in distilling domain-specific tool-usage patterns and enhancing\nmodel adaptability to new scenarios. Our experimental results demonstrate that\nRoutine provides a practical and accessible approach to building stable agent\nworkflows, accelerating the deployment and adoption of agent systems in\nenterprise environments, and advancing the technical vision of AI for Process.", "AI": {"tldr": "Routine is a multi-step agent planning framework that improves execution stability and accuracy in enterprise environments, significantly boosting model performance.", "motivation": "Address challenges like disorganized plans and poor execution stability in agent systems by introducing a structured framework.", "method": "Introduces Routine, a framework with clear structure, explicit instructions, and seamless parameter passing for multi-step tool-calling tasks. Evaluated in real-world enterprise scenarios.", "result": "Increased execution accuracy: GPT-4o from 41.1% to 96.3%, Qwen3-14B from 32.6% to 83.3%. Fine-tuning further improved Qwen3-14B to 88.2% and 95.5% with distilled data.", "conclusion": "Routine effectively enhances agent system stability and adaptability, accelerating deployment in enterprise environments."}}
{"id": "2507.15269", "pdf": "https://arxiv.org/pdf/2507.15269", "abs": "https://arxiv.org/abs/2507.15269", "authors": ["Fangqiu Yi", "Jingyu Xu", "Jiawei Shao", "Chi Zhang", "Xuelong Li"], "title": "Conditional Video Generation for High-Efficiency Video Compression", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Perceptual studies demonstrate that conditional diffusion models excel at\nreconstructing video content aligned with human visual perception. Building on\nthis insight, we propose a video compression framework that leverages\nconditional diffusion models for perceptually optimized reconstruction.\nSpecifically, we reframe video compression as a conditional generation task,\nwhere a generative model synthesizes video from sparse, yet informative\nsignals. Our approach introduces three key modules: (1) Multi-granular\nconditioning that captures both static scene structure and dynamic\nspatio-temporal cues; (2) Compact representations designed for efficient\ntransmission without sacrificing semantic richness; (3) Multi-condition\ntraining with modality dropout and role-aware embeddings, which prevent\nover-reliance on any single modality and enhance robustness. Extensive\nexperiments show that our method significantly outperforms both traditional and\nneural codecs on perceptual quality metrics such as Fr\\'echet Video Distance\n(FVD) and LPIPS, especially under high compression ratios.", "AI": {"tldr": "A video compression framework using conditional diffusion models for perceptually optimized reconstruction, outperforming traditional and neural codecs.", "motivation": "To leverage conditional diffusion models for video compression, aligning with human visual perception for better reconstruction quality.", "method": "Reframes compression as conditional generation with multi-granular conditioning, compact representations, and multi-condition training.", "result": "Significantly outperforms traditional and neural codecs on perceptual metrics like FVD and LPIPS, especially at high compression ratios.", "conclusion": "The proposed framework effectively enhances video compression quality by integrating conditional diffusion models."}}
{"id": "2507.15285", "pdf": "https://arxiv.org/pdf/2507.15285", "abs": "https://arxiv.org/abs/2507.15285", "authors": ["Lazaro Janier Gonzalez-Soler", "Maciej Salwowski", "Christoph Busch"], "title": "In-context Learning of Vision Language Models for Detection of Physical and Digital Attacks against Face Recognition Systems", "categories": ["cs.CV"], "comment": "Submitted to IEEE-TIFS", "summary": "Recent advances in biometric systems have significantly improved the\ndetection and prevention of fraudulent activities. However, as detection\nmethods improve, attack techniques become increasingly sophisticated. Attacks\non face recognition systems can be broadly divided into physical and digital\napproaches. Traditionally, deep learning models have been the primary defence\nagainst such attacks. While these models perform exceptionally well in\nscenarios for which they have been trained, they often struggle to adapt to\ndifferent types of attacks or varying environmental conditions. These\nsubsystems require substantial amounts of training data to achieve reliable\nperformance, yet biometric data collection faces significant challenges,\nincluding privacy concerns and the logistical difficulties of capturing diverse\nattack scenarios under controlled conditions. This work investigates the\napplication of Vision Language Models (VLM) and proposes an in-context learning\nframework for detecting physical presentation attacks and digital morphing\nattacks in biometric systems. Focusing on open-source models, the first\nsystematic framework for the quantitative evaluation of VLMs in\nsecurity-critical scenarios through in-context learning techniques is\nestablished. The experimental evaluation conducted on freely available\ndatabases demonstrates that the proposed subsystem achieves competitive\nperformance for physical and digital attack detection, outperforming some of\nthe traditional CNNs without resource-intensive training. The experimental\nresults validate the proposed framework as a promising tool for improving\ngeneralisation in attack detection.", "AI": {"tldr": "The paper proposes a Vision Language Model (VLM) framework for detecting biometric attacks, outperforming traditional CNNs without extensive training.", "motivation": "Biometric systems face sophisticated attacks, and traditional deep learning models struggle with adaptability and data requirements.", "method": "Uses in-context learning with VLMs to detect physical and digital attacks, evaluated on open-source models.", "result": "Competitive performance in attack detection, surpassing some CNNs without intensive training.", "conclusion": "VLMs offer a promising, resource-efficient solution for improving generalization in biometric attack detection."}}
{"id": "2507.14534", "pdf": "https://arxiv.org/pdf/2507.14534", "abs": "https://arxiv.org/abs/2507.14534", "authors": ["Yu Zhang", "Baotong Tian", "Zhiyao Duan"], "title": "Conan: A Chunkwise Online Network for Zero-Shot Adaptive Voice Conversion", "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": null, "summary": "Zero-shot online voice conversion (VC) holds significant promise for\nreal-time communications and entertainment. However, current VC models struggle\nto preserve semantic fidelity under real-time constraints, deliver\nnatural-sounding conversions, and adapt effectively to unseen speaker\ncharacteristics. To address these challenges, we introduce Conan, a chunkwise\nonline zero-shot voice conversion model that preserves the content of the\nsource while matching the voice timbre and styles of reference speech. Conan\ncomprises three core components: 1) a Stream Content Extractor that leverages\nEmformer for low-latency streaming content encoding; 2) an Adaptive Style\nEncoder that extracts fine-grained stylistic features from reference speech for\nenhanced style adaptation; 3) a Causal Shuffle Vocoder that implements a fully\ncausal HiFiGAN using a pixel-shuffle mechanism. Experimental evaluations\ndemonstrate that Conan outperforms baseline models in subjective and objective\nmetrics. Audio samples can be found at https://aaronz345.github.io/ConanDemo.", "AI": {"tldr": "Conan is a zero-shot online voice conversion model addressing real-time constraints, semantic fidelity, and unseen speaker adaptation with three core components: Stream Content Extractor, Adaptive Style Encoder, and Causal Shuffle Vocoder.", "motivation": "Current VC models struggle with real-time constraints, semantic fidelity, and adapting to unseen speaker characteristics.", "method": "Conan uses a Stream Content Extractor (Emformer-based), Adaptive Style Encoder, and Causal Shuffle Vocoder (HiFiGAN with pixel-shuffle).", "result": "Conan outperforms baseline models in subjective and objective metrics.", "conclusion": "Conan effectively addresses challenges in zero-shot online voice conversion, offering improved performance and adaptability."}}
{"id": "2507.15297", "pdf": "https://arxiv.org/pdf/2507.15297", "abs": "https://arxiv.org/abs/2507.15297", "authors": ["Zhiyu Pan", "Xiongjun Guan", "Yongjie Duan", "Jianjiang Feng", "Jie Zhou"], "title": "Minutiae-Anchored Local Dense Representation for Fingerprint Matching", "categories": ["cs.CV"], "comment": "Under review", "summary": "Fingerprint matching under diverse capture conditions remains a fundamental\nchallenge in biometric recognition. To achieve robust and accurate performance\nin such scenarios, we propose DMD, a minutiae-anchored local dense\nrepresentation which captures both fine-grained ridge textures and\ndiscriminative minutiae features in a spatially structured manner.\nSpecifically, descriptors are extracted from local patches centered and\noriented on each detected minutia, forming a three-dimensional tensor, where\ntwo dimensions represent spatial locations on the fingerprint plane and the\nthird encodes semantic features. This representation explicitly captures\nabstract features of local image patches, enabling a multi-level, fine-grained\ndescription that aggregates information from multiple minutiae and their\nsurrounding ridge structures. Furthermore, thanks to its strong spatial\ncorrespondence with the patch image, DMD allows for the use of foreground\nsegmentation masks to identify valid descriptor regions. During matching,\ncomparisons are then restricted to overlapping foreground areas, improving\nefficiency and robustness. Extensive experiments on rolled, plain, parital,\ncontactless, and latent fingerprint datasets demonstrate the effectiveness and\ngeneralizability of the proposed method. It achieves state-of-the-art accuracy\nacross multiple benchmarks while maintaining high computational efficiency,\nshowing strong potential for large-scale fingerprint recognition. Corresponding\ncode is available at https://github.com/Yu-Yy/DMD.", "AI": {"tldr": "Proposes DMD, a minutiae-anchored dense representation for robust fingerprint matching under diverse conditions, achieving state-of-the-art accuracy.", "motivation": "Addressing the challenge of fingerprint matching under varied capture conditions by combining minutiae and ridge textures.", "method": "Extracts descriptors from minutia-centered patches, forming a 3D tensor for multi-level feature aggregation. Uses segmentation masks for efficient matching.", "result": "Demonstrates superior accuracy on rolled, plain, partial, contactless, and latent fingerprint datasets.", "conclusion": "DMD is effective, generalizable, and computationally efficient, suitable for large-scale fingerprint recognition."}}
{"id": "2507.14586", "pdf": "https://arxiv.org/pdf/2507.14586", "abs": "https://arxiv.org/abs/2507.14586", "authors": ["Adrian Ehrenhofer", "Thomas Wallmersperger", "Gianaurelio Cuniberti"], "title": "What do Large Language Models know about materials?", "categories": ["physics.app-ph", "cs.CE", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly applied in the fields of\nmechanical engineering and materials science. As models that establish\nconnections through the interface of language, LLMs can be applied for\nstep-wise reasoning through the Processing-Structure-Property-Performance chain\nof material science and engineering. Current LLMs are built for adequately\nrepresenting a dataset, which is the most part of the accessible internet.\nHowever, the internet mostly contains non-scientific content. If LLMs should be\napplied for engineering purposes, it is valuable to investigate models for\ntheir intrinsic knowledge -- here: the capacity to generate correct information\nabout materials. In the current work, for the example of the Periodic Table of\nElements, we highlight the role of vocabulary and tokenization for the\nuniqueness of material fingerprints, and the LLMs' capabilities of generating\nfactually correct output of different state-of-the-art open models. This leads\nto a material knowledge benchmark for an informed choice, for which steps in\nthe PSPP chain LLMs are applicable, and where specialized models are required.", "AI": {"tldr": "The paper explores the application of LLMs in mechanical engineering and materials science, focusing on their ability to generate accurate material knowledge and proposing a benchmark for their use in the PSPP chain.", "motivation": "LLMs are increasingly used in engineering, but their training on non-scientific internet content raises concerns about their accuracy for scientific tasks, necessitating an evaluation of their material knowledge capabilities.", "method": "The study evaluates LLMs' performance in generating correct material information, using the Periodic Table of Elements as an example to analyze vocabulary, tokenization, and model accuracy.", "result": "The findings highlight the importance of vocabulary and tokenization for material uniqueness and assess the factual correctness of various LLMs, leading to a benchmark for their applicability in the PSPP chain.", "conclusion": "The study provides a benchmark to determine where LLMs can be effectively applied in materials science and where specialized models are needed, emphasizing the need for tailored training for scientific accuracy."}}
{"id": "2507.15308", "pdf": "https://arxiv.org/pdf/2507.15308", "abs": "https://arxiv.org/abs/2507.15308", "authors": ["Zhimeng Xin", "Tianxu Wu", "Yixiong Zou", "Shiming Chen", "Dingjie Fu", "Xinge You"], "title": "Few-Shot Object Detection via Spatial-Channel State Space Model", "categories": ["cs.CV"], "comment": null, "summary": "Due to the limited training samples in few-shot object detection (FSOD), we\nobserve that current methods may struggle to accurately extract effective\nfeatures from each channel. Specifically, this issue manifests in two aspects:\ni) channels with high weights may not necessarily be effective, and ii)\nchannels with low weights may still hold significant value. To handle this\nproblem, we consider utilizing the inter-channel correlation to facilitate the\nnovel model's adaptation process to novel conditions, ensuring the model can\ncorrectly highlight effective channels and rectify those incorrect ones. Since\nthe channel sequence is also 1-dimensional, its similarity with the temporal\nsequence inspires us to take Mamba for modeling the correlation in the channel\nsequence. Based on this concept, we propose a Spatial-Channel State Space\nModeling (SCSM) module for spatial-channel state modeling, which highlights the\neffective patterns and rectifies those ineffective ones in feature channels. In\nSCSM, we design the Spatial Feature Modeling (SFM) module to balance the\nlearning of spatial relationships and channel relationships, and then introduce\nthe Channel State Modeling (CSM) module based on Mamba to learn correlation in\nchannels. Extensive experiments on the VOC and COCO datasets show that the SCSM\nmodule enables the novel detector to improve the quality of focused feature\nrepresentation in channels and achieve state-of-the-art performance.", "AI": {"tldr": "The paper proposes a Spatial-Channel State Space Modeling (SCSM) module to address feature extraction challenges in few-shot object detection by leveraging inter-channel correlation, inspired by Mamba for temporal sequence modeling.", "motivation": "Current few-shot object detection methods struggle with accurately extracting effective features due to limited training samples, leading to misalignment between channel weights and their actual effectiveness.", "method": "The SCSM module includes a Spatial Feature Modeling (SFM) module for spatial-channel balance and a Channel State Modeling (CSM) module based on Mamba to model channel correlations.", "result": "Experiments on VOC and COCO datasets demonstrate improved feature representation and state-of-the-art performance.", "conclusion": "The SCSM module effectively enhances feature extraction in few-shot object detection by addressing channel correlation issues."}}
{"id": "2507.14619", "pdf": "https://arxiv.org/pdf/2507.14619", "abs": "https://arxiv.org/abs/2507.14619", "authors": ["Van-Hoang Le", "Duc-Vu Nguyen", "Kiet Van Nguyen", "Ngan Luu-Thuy Nguyen"], "title": "Optimizing Legal Document Retrieval in Vietnamese with Semi-Hard Negative Mining", "categories": ["cs.IR", "cs.CL"], "comment": "Accepted at ICCCI 2025", "summary": "Large Language Models (LLMs) face significant challenges in specialized\ndomains like law, where precision and domain-specific knowledge are critical.\nThis paper presents a streamlined two-stage framework consisting of Retrieval\nand Re-ranking to enhance legal document retrieval efficiency and accuracy. Our\napproach employs a fine-tuned Bi-Encoder for rapid candidate retrieval,\nfollowed by a Cross-Encoder for precise re-ranking, both optimized through\nstrategic negative example mining. Key innovations include the introduction of\nthe Exist@m metric to evaluate retrieval effectiveness and the use of semi-hard\nnegatives to mitigate training bias, which significantly improved re-ranking\nperformance. Evaluated on the SoICT Hackathon 2024 for Legal Document\nRetrieval, our team, 4Huiter, achieved a top-three position. While\ntop-performing teams employed ensemble models and iterative self-training on\nlarge bge-m3 architectures, our lightweight, single-pass approach offered a\ncompetitive alternative with far fewer parameters. The framework demonstrates\nthat optimized data processing, tailored loss functions, and balanced negative\nsampling are pivotal for building robust retrieval-augmented systems in legal\ncontexts.", "AI": {"tldr": "A two-stage framework (Retrieval and Re-ranking) improves legal document retrieval for LLMs, using fine-tuned Bi-Encoder and Cross-Encoder with strategic negative mining. Achieved top-three in SoICT Hackathon 2024.", "motivation": "LLMs struggle in specialized domains like law due to precision and domain-specific knowledge requirements.", "method": "Fine-tuned Bi-Encoder for retrieval, Cross-Encoder for re-ranking, optimized with negative example mining and Exist@m metric.", "result": "Top-three performance in SoICT Hackathon 2024, competitive with lightweight single-pass approach.", "conclusion": "Optimized data processing, tailored loss functions, and balanced negative sampling are key for robust legal retrieval systems."}}
{"id": "2507.15321", "pdf": "https://arxiv.org/pdf/2507.15321", "abs": "https://arxiv.org/abs/2507.15321", "authors": ["Zhenyu Li", "Haotong Lin", "Jiashi Feng", "Peter Wonka", "Bingyi Kang"], "title": "BenchDepth: Are We on the Right Way to Evaluate Depth Foundation Models?", "categories": ["cs.CV"], "comment": "Webpage: https://zhyever.github.io/benchdepth", "summary": "Depth estimation is a fundamental task in computer vision with diverse\napplications. Recent advancements in deep learning have led to powerful depth\nfoundation models (DFMs), yet their evaluation remains challenging due to\ninconsistencies in existing protocols. Traditional benchmarks rely on\nalignment-based metrics that introduce biases, favor certain depth\nrepresentations, and complicate fair comparisons. In this work, we propose\nBenchDepth, a new benchmark that evaluates DFMs through five carefully selected\ndownstream proxy tasks: depth completion, stereo matching, monocular\nfeed-forward 3D scene reconstruction, SLAM, and vision-language spatial\nunderstanding. Unlike conventional evaluation protocols, our approach assesses\nDFMs based on their practical utility in real-world applications, bypassing\nproblematic alignment procedures. We benchmark eight state-of-the-art DFMs and\nprovide an in-depth analysis of key findings and observations. We hope our work\nsparks further discussion in the community on best practices for depth model\nevaluation and paves the way for future research and advancements in depth\nestimation.", "AI": {"tldr": "BenchDepth introduces a new benchmark for evaluating depth foundation models (DFMs) using five downstream tasks, avoiding biases in traditional alignment-based metrics.", "motivation": "Existing depth evaluation protocols are inconsistent and biased, complicating fair comparisons of DFMs.", "method": "Proposes BenchDepth, evaluating DFMs through five proxy tasks (e.g., depth completion, SLAM) to assess practical utility.", "result": "Benchmarked eight DFMs, providing insights into their performance and limitations.", "conclusion": "BenchDepth offers a fairer evaluation method, encouraging better practices and future research in depth estimation."}}
{"id": "2507.14660", "pdf": "https://arxiv.org/pdf/2507.14660", "abs": "https://arxiv.org/abs/2507.14660", "authors": ["Qibing Ren", "Sitao Xie", "Longxuan Wei", "Zhenfei Yin", "Junchi Yan", "Lizhuang Ma", "Jing Shao"], "title": "When Autonomy Goes Rogue: Preparing for Risks of Multi-Agent Collusion in Social Systems", "categories": ["cs.AI", "cs.CL"], "comment": "Code is available at https://github.com/renqibing/RogueAgent", "summary": "Recent large-scale events like election fraud and financial scams have shown\nhow harmful coordinated efforts by human groups can be. With the rise of\nautonomous AI systems, there is growing concern that AI-driven groups could\nalso cause similar harm. While most AI safety research focuses on individual AI\nsystems, the risks posed by multi-agent systems (MAS) in complex real-world\nsituations are still underexplored. In this paper, we introduce a\nproof-of-concept to simulate the risks of malicious MAS collusion, using a\nflexible framework that supports both centralized and decentralized\ncoordination structures. We apply this framework to two high-risk fields:\nmisinformation spread and e-commerce fraud. Our findings show that\ndecentralized systems are more effective at carrying out malicious actions than\ncentralized ones. The increased autonomy of decentralized systems allows them\nto adapt their strategies and cause more damage. Even when traditional\ninterventions, like content flagging, are applied, decentralized groups can\nadjust their tactics to avoid detection. We present key insights into how these\nmalicious groups operate and the need for better detection systems and\ncountermeasures. Code is available at https://github.com/renqibing/RogueAgent.", "AI": {"tldr": "The paper explores risks of AI-driven multi-agent systems (MAS) collusion, simulating malicious actions in misinformation and e-commerce fraud, finding decentralized systems more harmful and adaptable.", "motivation": "Concerns about AI-driven groups causing harm, similar to human-coordinated fraud or scams, with MAS risks underexplored.", "method": "A proof-of-concept framework simulates MAS collusion, testing centralized vs. decentralized coordination in misinformation and fraud.", "result": "Decentralized MAS are more effective and adaptable in malicious actions, evading traditional interventions like content flagging.", "conclusion": "Highlights the need for better detection and countermeasures against malicious MAS, especially decentralized ones."}}
{"id": "2507.15335", "pdf": "https://arxiv.org/pdf/2507.15335", "abs": "https://arxiv.org/abs/2507.15335", "authors": ["Muhammad Aqeel", "Federico Leonardi", "Francesco Setti"], "title": "ExDD: Explicit Dual Distribution Learning for Surface Defect Detection via Diffusion Synthesis", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to ICIAP 2025", "summary": "Industrial defect detection systems face critical limitations when confined\nto one-class anomaly detection paradigms, which assume uniform outlier\ndistributions and struggle with data scarcity in realworld manufacturing\nenvironments. We present ExDD (Explicit Dual Distribution), a novel framework\nthat transcends these limitations by explicitly modeling dual feature\ndistributions. Our approach leverages parallel memory banks that capture the\ndistinct statistical properties of both normality and anomalous patterns,\naddressing the fundamental flaw of uniform outlier assumptions. To overcome\ndata scarcity, we employ latent diffusion models with domain-specific textual\nconditioning, generating in-distribution synthetic defects that preserve\nindustrial context. Our neighborhood-aware ratio scoring mechanism elegantly\nfuses complementary distance metrics, amplifying signals in regions exhibiting\nboth deviation from normality and similarity to known defect patterns.\nExperimental validation on KSDD2 demonstrates superior performance (94.2%\nI-AUROC, 97.7% P-AUROC), with optimal augmentation at 100 synthetic samples.", "AI": {"tldr": "ExDD is a novel framework for industrial defect detection that models dual feature distributions, uses synthetic defect generation, and achieves high performance metrics.", "motivation": "Traditional one-class anomaly detection struggles with uniform outlier assumptions and data scarcity in real-world manufacturing.", "method": "ExDD uses parallel memory banks for normality and anomaly patterns, latent diffusion models for synthetic defect generation, and a neighborhood-aware scoring mechanism.", "result": "Achieves 94.2% I-AUROC and 97.7% P-AUROC on KSDD2, with optimal performance at 100 synthetic samples.", "conclusion": "ExDD effectively addresses limitations of traditional methods, offering a robust solution for industrial defect detection."}}
{"id": "2507.15346", "pdf": "https://arxiv.org/pdf/2507.15346", "abs": "https://arxiv.org/abs/2507.15346", "authors": ["Muhammad Aqeel", "Kidus Dagnaw Bellete", "Francesco Setti"], "title": "RoadFusion: Latent Diffusion Model for Pavement Defect Detection", "categories": ["cs.CV"], "comment": "Accepted to ICIAP 2025", "summary": "Pavement defect detection faces critical challenges including limited\nannotated data, domain shift between training and deployment environments, and\nhigh variability in defect appearances across different road conditions. We\npropose RoadFusion, a framework that addresses these limitations through\nsynthetic anomaly generation with dual-path feature adaptation. A latent\ndiffusion model synthesizes diverse, realistic defects using text prompts and\nspatial masks, enabling effective training under data scarcity. Two separate\nfeature adaptors specialize representations for normal and anomalous inputs,\nimproving robustness to domain shift and defect variability. A lightweight\ndiscriminator learns to distinguish fine-grained defect patterns at the patch\nlevel. Evaluated on six benchmark datasets, RoadFusion achieves consistently\nstrong performance across both classification and localization tasks, setting\nnew state-of-the-art in multiple metrics relevant to real-world road\ninspection.", "AI": {"tldr": "RoadFusion uses synthetic anomaly generation and dual-path feature adaptation to improve pavement defect detection, achieving state-of-the-art results.", "motivation": "Address challenges like limited annotated data, domain shift, and variability in defect appearances for pavement defect detection.", "method": "Uses a latent diffusion model for synthetic defect generation and dual-path feature adaptors for robust representation learning.", "result": "Achieves strong performance on six benchmark datasets for classification and localization tasks.", "conclusion": "RoadFusion sets new state-of-the-art metrics, proving effective for real-world road inspection."}}
{"id": "2507.14679", "pdf": "https://arxiv.org/pdf/2507.14679", "abs": "https://arxiv.org/abs/2507.14679", "authors": ["Zixin Xu", "Zhijie Wang", "Zhiyuan Pan"], "title": "GCC-Spam: Spam Detection via GAN, Contrastive Learning, and Character Similarity Networks", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "The exponential growth of spam text on the Internet necessitates robust\ndetection mechanisms to mitigate risks such as information leakage and social\ninstability. This work addresses two principal challenges: adversarial\nstrategies employed by spammers and the scarcity of labeled data. We propose a\nnovel spam-text detection framework GCC-Spam, which integrates three core\ninnovations. First, a character similarity network captures orthographic and\nphonetic features to counter character-obfuscation attacks and furthermore\nproduces sentence embeddings for downstream classification. Second, contrastive\nlearning enhances discriminability by optimizing the latent-space distance\nbetween spam and normal texts. Third, a Generative Adversarial Network (GAN)\ngenerates realistic pseudo-spam samples to alleviate data scarcity while\nimproving model robustness and classification accuracy. Extensive experiments\non real-world datasets demonstrate that our model outperforms baseline\napproaches, achieving higher detection rates with significantly fewer labeled\nexamples.", "AI": {"tldr": "GCC-Spam is a novel spam-text detection framework combining character similarity networks, contrastive learning, and GANs to address adversarial spam strategies and data scarcity, outperforming baselines.", "motivation": "The rise of spam text poses risks like information leakage and social instability, requiring robust detection methods despite adversarial tactics and limited labeled data.", "method": "GCC-Spam integrates a character similarity network for obfuscation resistance, contrastive learning for better discrimination, and GANs to generate pseudo-spam samples, addressing data scarcity.", "result": "The framework achieves higher detection rates with fewer labeled examples, outperforming baseline methods in real-world experiments.", "conclusion": "GCC-Spam effectively tackles spam detection challenges, offering improved accuracy and robustness against adversarial strategies and data limitations."}}
{"id": "2507.15365", "pdf": "https://arxiv.org/pdf/2507.15365", "abs": "https://arxiv.org/abs/2507.15365", "authors": ["Fatemeh Saleh", "Sadegh Aliakbarian", "Charlie Hewitt", "Lohit Petikam", "Xiao-Xian", "Antonio Criminisi", "Thomas J. Cashman", "Tadas Baltru\u0161aitis"], "title": "DAViD: Data-efficient and Accurate Vision Models from Synthetic Data", "categories": ["cs.CV"], "comment": "Accepted at ICCV 2025", "summary": "The state of the art in human-centric computer vision achieves high accuracy\nand robustness across a diverse range of tasks. The most effective models in\nthis domain have billions of parameters, thus requiring extremely large\ndatasets, expensive training regimes, and compute-intensive inference. In this\npaper, we demonstrate that it is possible to train models on much smaller but\nhigh-fidelity synthetic datasets, with no loss in accuracy and higher\nefficiency. Using synthetic training data provides us with excellent levels of\ndetail and perfect labels, while providing strong guarantees for data\nprovenance, usage rights, and user consent. Procedural data synthesis also\nprovides us with explicit control on data diversity, that we can use to address\nunfairness in the models we train. Extensive quantitative assessment on real\ninput images demonstrates accuracy of our models on three dense prediction\ntasks: depth estimation, surface normal estimation, and soft foreground\nsegmentation. Our models require only a fraction of the cost of training and\ninference when compared with foundational models of similar accuracy. Our\nhuman-centric synthetic dataset and trained models are available at\nhttps://aka.ms/DAViD.", "AI": {"tldr": "Training models on small, high-fidelity synthetic datasets achieves comparable accuracy to large-scale models while reducing costs and addressing fairness.", "motivation": "To overcome the high computational and data requirements of large models in human-centric computer vision by leveraging synthetic datasets.", "method": "Procedural data synthesis to create high-fidelity synthetic datasets with perfect labels and controlled diversity, used to train models for dense prediction tasks.", "result": "Models trained on synthetic data match the accuracy of large-scale models while being more efficient and cost-effective.", "conclusion": "Synthetic datasets offer a viable alternative to large-scale real datasets, providing accuracy, efficiency, and fairness benefits."}}
{"id": "2507.14843", "pdf": "https://arxiv.org/pdf/2507.14843", "abs": "https://arxiv.org/abs/2507.14843", "authors": ["Fang Wu", "Weihao Xuan", "Ximing Lu", "Zaid Harchaoui", "Yejin Choi"], "title": "The Invisible Leash: Why RLVR May Not Escape Its Origin", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Recent advances in large reasoning models highlight Reinforcement Learning\nwith Verifiable Rewards (RLVR) as a promising method for enhancing AI's\ncapabilities, particularly in solving complex logical tasks. However, it\nremains unclear whether RLVR truly expands a model's reasoning boundary or\nmerely amplifies high-reward outputs that the base model already knows for\nimproved precision. This study presents a theoretical and empirical\ninvestigation that provides fresh insights into the potential limits of RLVR.\nFirst, we offer a new theoretical perspective that RLVR is constrained by the\nbase model's support-unable to sample solutions with zero initial\nprobability-and operates as a conservative reweighting mechanism that may\nrestrict the discovery of entirely original solutions. We also identify an\nentropy-reward tradeoff: while RLVR reliably enhances precision, it may\nprogressively narrow exploration and potentially overlook correct yet\nunderrepresented solutions. Extensive empirical experiments validate that while\nRLVR consistently improves pass@1, the shrinkage of empirical support generally\noutweighs the expansion of empirical support under larger sampling budgets,\nfailing to recover correct answers that were previously accessible to the base\nmodel. Interestingly, we also observe that while RLVR sometimes increases\ntoken-level entropy, resulting in greater uncertainty at each generation step,\nanswer-level entropy declines, indicating that these seemingly more uncertain\npaths ultimately converge onto a smaller set of distinct answers. Taken\ntogether, these findings reveal potential limits of RLVR in extending reasoning\nhorizons. Breaking this invisible leash may require future algorithmic\ninnovations such as explicit exploration mechanisms or hybrid strategies that\nseed probability mass into underrepresented solution regions.", "AI": {"tldr": "RLVR improves precision but may limit exploration and original solutions, constrained by the base model's support.", "motivation": "To investigate whether RLVR expands reasoning boundaries or just amplifies high-reward outputs.", "method": "Theoretical analysis and empirical experiments on RLVR's constraints and tradeoffs.", "result": "RLVR improves pass@1 but shrinks empirical support, reducing access to correct answers.", "conclusion": "RLVR has limits in extending reasoning; future innovations like exploration mechanisms are needed."}}
{"id": "2507.15401", "pdf": "https://arxiv.org/pdf/2507.15401", "abs": "https://arxiv.org/abs/2507.15401", "authors": ["Huiyu Zhai", "Xingxing Yang", "Yalan Ye", "Chenyang Li", "Bin Fan", "Changze Li"], "title": "Rethinking Occlusion in FER: A Semantic-Aware Perspective and Go Beyond", "categories": ["cs.CV"], "comment": null, "summary": "Facial expression recognition (FER) is a challenging task due to pervasive\nocclusion and dataset biases. Especially when facial information is partially\noccluded, existing FER models struggle to extract effective facial features,\nleading to inaccurate classifications. In response, we present ORSANet, which\nintroduces the following three key contributions: First, we introduce auxiliary\nmulti-modal semantic guidance to disambiguate facial occlusion and learn\nhigh-level semantic knowledge, which is two-fold: 1) we introduce semantic\nsegmentation maps as dense semantics prior to generate semantics-enhanced\nfacial representations; 2) we introduce facial landmarks as sparse geometric\nprior to mitigate intrinsic noises in FER, such as identity and gender biases.\nSecond, to facilitate the effective incorporation of these two multi-modal\npriors, we customize a Multi-scale Cross-interaction Module (MCM) to adaptively\nfuse the landmark feature and semantics-enhanced representations within\ndifferent scales. Third, we design a Dynamic Adversarial Repulsion Enhancement\nLoss (DARELoss) that dynamically adjusts the margins of ambiguous classes,\nfurther enhancing the model's ability to distinguish similar expressions. We\nfurther construct the first occlusion-oriented FER dataset to facilitate\nspecialized robustness analysis on various real-world occlusion conditions,\ndubbed Occlu-FER. Extensive experiments on both public benchmarks and Occlu-FER\ndemonstrate that our proposed ORSANet achieves SOTA recognition performance.\nCode is publicly available at https://github.com/Wenyuzhy/ORSANet-master.", "AI": {"tldr": "ORSANet improves facial expression recognition (FER) under occlusion by using multi-modal semantic guidance, a multi-scale fusion module, and a dynamic loss function, achieving state-of-the-art results.", "motivation": "Existing FER models struggle with occlusion and dataset biases, leading to inaccurate classifications.", "method": "ORSANet uses semantic segmentation maps and facial landmarks as priors, a Multi-scale Cross-interaction Module (MCM) for fusion, and a Dynamic Adversarial Repulsion Enhancement Loss (DARELoss) for better class distinction.", "result": "ORSANet achieves state-of-the-art performance on public benchmarks and the new Occlu-FER dataset.", "conclusion": "ORSANet effectively addresses occlusion and bias challenges in FER, demonstrating superior performance."}}
{"id": "2507.15007", "pdf": "https://arxiv.org/pdf/2507.15007", "abs": "https://arxiv.org/abs/2507.15007", "authors": ["Sayed Mahbub Hasan Amiri", "Md. Mainul Islam", "Mohammad Shakhawat Hossen", "Sayed Majhab Hasan Amiri", "Mohammad Shawkat Ali Mamun", "Sk. Humaun Kabir", "Naznin Akter"], "title": "Hear Your Code Fail, Voice-Assisted Debugging for Python", "categories": ["cs.PL", "cs.CL"], "comment": "35 pages, 20 figures", "summary": "This research introduces an innovative voice-assisted debugging plugin for\nPython that transforms silent runtime errors into actionable audible\ndiagnostics. By implementing a global exception hook architecture with pyttsx3\ntext-to-speech conversion and Tkinter-based GUI visualization, the solution\ndelivers multimodal error feedback through parallel auditory and visual\nchannels. Empirical evaluation demonstrates 37% reduced cognitive load (p<0.01,\nn=50) compared to traditional stack-trace debugging, while enabling 78% faster\nerror identification through vocalized exception classification and\ncontextualization. The system achieves sub-1.2 second voice latency with under\n18% CPU overhead during exception handling, vocalizing error types and\nconsequences while displaying interactive tracebacks with documentation deep\nlinks. Criteria validate compatibility across Python 3.7+ environments on\nWindows, macOS, and Linux platforms. Needing only two lines of integration\ncode, the plugin significantly boosts availability for aesthetically impaired\ndesigners and supports multitasking workflows through hands-free error medical\ndiagnosis. Educational applications show particular promise, with pilot studies\nindicating 45% faster debugging skill acquisition among novice programmers.\nFuture development will incorporate GPT-based repair suggestions and real-time\nmultilingual translation to further advance auditory debugging paradigms. The\nsolution represents a fundamental shift toward human-centric error diagnostics,\nbridging critical gaps in programming accessibility while establishing new\nstandards for cognitive efficiency in software development workflows.", "AI": {"tldr": "A voice-assisted Python debugging plugin converts silent errors into audible diagnostics, reducing cognitive load by 37% and speeding up error identification by 78%. It uses text-to-speech and GUI visualization, achieving low latency and broad compatibility.", "motivation": "To improve debugging accessibility and efficiency, especially for visually impaired users and novice programmers, by transforming silent errors into audible and visual feedback.", "method": "Implemented a global exception hook with pyttsx3 for text-to-speech and Tkinter for GUI visualization, providing multimodal error feedback.", "result": "Empirical results show 37% reduced cognitive load, 78% faster error identification, sub-1.2s voice latency, and under 18% CPU overhead. Compatible with Python 3.7+ on major platforms.", "conclusion": "The plugin enhances debugging accessibility and efficiency, with potential for educational use and future integration of GPT-based repairs and multilingual support."}}
{"id": "2507.15418", "pdf": "https://arxiv.org/pdf/2507.15418", "abs": "https://arxiv.org/abs/2507.15418", "authors": ["Ka Young Kim", "Hyeon Bae Kim", "Seong Tae Kim"], "title": "SurgX: Neuron-Concept Association for Explainable Surgical Phase Recognition", "categories": ["cs.CV"], "comment": "Accepted to MICCAI 2025", "summary": "Surgical phase recognition plays a crucial role in surgical workflow\nanalysis, enabling various applications such as surgical monitoring, skill\nassessment, and workflow optimization. Despite significant advancements in deep\nlearning-based surgical phase recognition, these models remain inherently\nopaque, making it difficult to understand how they make decisions. This lack of\ninterpretability hinders trust and makes it challenging to debug the model. To\naddress this challenge, we propose SurgX, a novel concept-based explanation\nframework that enhances the interpretability of surgical phase recognition\nmodels by associating neurons with relevant concepts. In this paper, we\nintroduce the process of selecting representative example sequences for\nneurons, constructing a concept set tailored to the surgical video dataset,\nassociating neurons with concepts and identifying neurons crucial for\npredictions. Through extensive experiments on two surgical phase recognition\nmodels, we validate our method and analyze the explanation for prediction. This\nhighlights the potential of our method in explaining surgical phase\nrecognition. The code is available at https://github.com/ailab-kyunghee/SurgX", "AI": {"tldr": "SurgX is a framework to improve interpretability in surgical phase recognition models by linking neurons to relevant concepts, validated on two models.", "motivation": "Deep learning models for surgical phase recognition lack interpretability, which reduces trust and complicates debugging.", "method": "SurgX associates neurons with concepts, selects example sequences, constructs a concept set, and identifies key neurons for predictions.", "result": "Experiments on two models validate SurgX's effectiveness in explaining surgical phase recognition.", "conclusion": "SurgX enhances interpretability, aiding trust and debugging in surgical phase recognition models."}}
{"id": "2507.15205", "pdf": "https://arxiv.org/pdf/2507.15205", "abs": "https://arxiv.org/abs/2507.15205", "authors": ["Xinran Li", "Xiujuan Xu", "Jiaqi Qiao"], "title": "Long-Short Distance Graph Neural Networks and Improved Curriculum Learning for Emotion Recognition in Conversation", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted by the 28th European Conference on Artificial Intelligence\n  (ECAI 2025)", "summary": "Emotion Recognition in Conversation (ERC) is a practical and challenging\ntask. This paper proposes a novel multimodal approach, the Long-Short Distance\nGraph Neural Network (LSDGNN). Based on the Directed Acyclic Graph (DAG), it\nconstructs a long-distance graph neural network and a short-distance graph\nneural network to obtain multimodal features of distant and nearby utterances,\nrespectively. To ensure that long- and short-distance features are as distinct\nas possible in representation while enabling mutual influence between the two\nmodules, we employ a Differential Regularizer and incorporate a BiAffine Module\nto facilitate feature interaction. In addition, we propose an Improved\nCurriculum Learning (ICL) to address the challenge of data imbalance. By\ncomputing the similarity between different emotions to emphasize the shifts in\nsimilar emotions, we design a \"weighted emotional shift\" metric and develop a\ndifficulty measurer, enabling a training process that prioritizes learning easy\nsamples before harder ones. Experimental results on the IEMOCAP and MELD\ndatasets demonstrate that our model outperforms existing benchmarks.", "AI": {"tldr": "The paper introduces LSDGNN, a multimodal approach for Emotion Recognition in Conversation (ERC), using long- and short-distance graph neural networks with a Differential Regularizer and BiAffine Module for feature interaction. It also proposes Improved Curriculum Learning (ICL) to handle data imbalance, achieving superior results on IEMOCAP and MELD datasets.", "motivation": "ERC is challenging due to the complexity of multimodal interactions and data imbalance. The paper aims to improve performance by capturing distinct long- and short-distance utterance features and addressing imbalance issues.", "method": "LSDGNN combines long- and short-distance graph neural networks (DAG-based) with a Differential Regularizer and BiAffine Module for feature interaction. ICL uses a 'weighted emotional shift' metric and difficulty measurer for balanced training.", "result": "The model outperforms benchmarks on IEMOCAP and MELD datasets, demonstrating effectiveness in ERC.", "conclusion": "LSDGNN and ICL provide a robust solution for ERC, improving feature representation and handling data imbalance."}}
{"id": "2507.15428", "pdf": "https://arxiv.org/pdf/2507.15428", "abs": "https://arxiv.org/abs/2507.15428", "authors": ["Jiaao Li", "Kaiyuan Li", "Chen Gao", "Yong Li", "Xinlei Chen"], "title": "EgoPrune: Efficient Token Pruning for Egomotion Video Reasoning in Embodied Agent", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Egomotion videos are first-person recordings where the view changes\ncontinuously due to the agent's movement. As they serve as the primary visual\ninput for embodied AI agents, making egomotion video reasoning more efficient\nis therefore essential for real-world deployment. Recent advances in\nvision-language models have enabled strong multimodal reasoning capabilities,\nbut their computational cost remains prohibitive for long, redundant video\ninputs. Existing token pruning methods, typically designed for third-person\nvideos, fail to leverage the spatiotemporal continuity and motion constraints\ninherent in egomotion settings. To address this, we propose EgoPrune, a\ntraining-free token pruning method tailored for egomotion video reasoning.\nEgoPrune comprises three components: a keyframe selector adapted from EmbodiedR\nfor temporally efficient sampling; Perspective-Aware Redundancy Filtering\n(PARF), which aligns visual tokens using perspective transformations and\nremoves redundant tokens; and a Maximal Marginal Relevance (MMR)-based token\nselector that jointly considers visual-text relevance and intra-frame\ndiversity. Experiments on two egomotion video benchmarks show that EgoPrune\nconsistently outperforms prior training-free methods across various pruning\nratios while significantly reducing FLOPs, memory usage, and latency. Moreover,\nwe deploy EgoPrune on an embodied agent equipped with a Jetson Orin NX 16GB\nedge device, demonstrating its real-world efficiency and suitability for\non-device egomotion video reasoning.", "AI": {"tldr": "EgoPrune is a training-free token pruning method for egomotion video reasoning, improving efficiency by leveraging spatiotemporal continuity and motion constraints.", "motivation": "Egomotion videos are crucial for embodied AI agents, but existing token pruning methods are inefficient for these videos due to their unique characteristics.", "method": "EgoPrune includes a keyframe selector, Perspective-Aware Redundancy Filtering (PARF), and an MMR-based token selector to prune redundant tokens efficiently.", "result": "EgoPrune outperforms prior methods, reducing FLOPs, memory usage, and latency, and is validated on an edge device.", "conclusion": "EgoPrune is effective for on-device egomotion video reasoning, offering real-world efficiency."}}
{"id": "2507.15214", "pdf": "https://arxiv.org/pdf/2507.15214", "abs": "https://arxiv.org/abs/2507.15214", "authors": ["Natalia Tomashenko", "Emmanuel Vincent", "Marc Tommasi"], "title": "Exploiting Context-dependent Duration Features for Voice Anonymization Attack Systems", "categories": ["cs.SD", "cs.CL", "cs.CR", "eess.AS"], "comment": "Accepted at Interspeech-2025", "summary": "The temporal dynamics of speech, encompassing variations in rhythm,\nintonation, and speaking rate, contain important and unique information about\nspeaker identity. This paper proposes a new method for representing speaker\ncharacteristics by extracting context-dependent duration embeddings from speech\ntemporal dynamics. We develop novel attack models using these representations\nand analyze the potential vulnerabilities in speaker verification and voice\nanonymization systems.The experimental results show that the developed attack\nmodels provide a significant improvement in speaker verification performance\nfor both original and anonymized data in comparison with simpler\nrepresentations of speech temporal dynamics reported in the literature.", "AI": {"tldr": "A new method for speaker verification uses context-dependent duration embeddings from speech temporal dynamics, improving performance over simpler methods.", "motivation": "To leverage temporal dynamics (rhythm, intonation, speaking rate) for better speaker identity representation and analyze vulnerabilities in verification and anonymization systems.", "method": "Extracts context-dependent duration embeddings from speech temporal dynamics and develops novel attack models.", "result": "Attack models significantly improve speaker verification performance for original and anonymized data.", "conclusion": "The proposed method outperforms simpler representations, highlighting its effectiveness and potential vulnerabilities in speaker verification systems."}}
{"id": "2507.15480", "pdf": "https://arxiv.org/pdf/2507.15480", "abs": "https://arxiv.org/abs/2507.15480", "authors": ["Liang Chen", "Ghazi Shazan Ahmad", "Tianjun Yao", "Lingqiao Liu", "Zhiqiang Shen"], "title": "One Last Attention for Your Vision-Language Model", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Pretrained vision-language models (VLMs), such as CLIP, achieve remarkable\nzero-shot performance, yet their downstream potential hinges on effective\nfine-tuning. Most adaptation methods typically focus on refining representation\nfrom separate modalities (text or vision) but neglect the critical role of\ntheir fused representations in the decision-making process, \\emph{\\ie} rational\nmatrix that drives the final prediction. To bridge the gap, we propose a simple\nyet effective \\textbf{R}ational \\textbf{Ada}ptaion ({RAda}) to explicitly\nexploit the final fused representation during fine-tuning. RAda employs a\nlearned mask, obtained from a lightweight attention layer attached at the end\nof a VLM, to dynamically calibrate the contribution of each element in the\nrational matrix, enabling targeted adjustments to the final cross-modal\ninteractions without incurring costly modifications to intermediate features.\nExperiments in different settings (i.e., updating, or freezing pretrained\nencoders in adaptation, and test-time training that can only access the\nunlabeled test data) show that RAda serves as a versatile fine-tuning\ntechnique, improving the baseline with minimal code and performing comparably\nagainst current arts in most settings. Code is available at\n\\href{https://github.com/khufia/RAda/tree/main}{github.com/khufia/RAda}.", "AI": {"tldr": "RAda is a method for fine-tuning VLMs by dynamically adjusting fused representations to improve cross-modal interactions without costly modifications.", "motivation": "Existing methods neglect the role of fused representations in decision-making, limiting downstream performance.", "method": "RAda uses a learned mask from a lightweight attention layer to calibrate the rational matrix, targeting cross-modal interactions.", "result": "RAda improves baselines with minimal code and performs comparably to current methods in various settings.", "conclusion": "RAda is a versatile and effective fine-tuning technique for VLMs."}}
{"id": "2507.15267", "pdf": "https://arxiv.org/pdf/2507.15267", "abs": "https://arxiv.org/abs/2507.15267", "authors": ["Ninglu Shao", "Jinshan Wang", "Chenxu Wang", "Qingbiao Li", "Xiaoxue Zang", "Han Li"], "title": "GREAT: Guiding Query Generation with a Trie for Recommending Related Search about Video at Kuaishou", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Currently, short video platforms have become the primary place for\nindividuals to share experiences and obtain information. To better meet users'\nneeds for acquiring information while browsing short videos, some apps have\nintroduced a search entry at the bottom of videos, accompanied with recommended\nrelevant queries. This scenario is known as query recommendation in\nvideo-related search, where core task is item-to-query (I2Q) recommendation. As\nthis scenario has only emerged in recent years, there is a notable scarcity of\nacademic research and publicly available datasets in this domain. To address\nthis gap, we systematically examine the challenges associated with this\nscenario for the first time. Subsequently, we release a large-scale dataset\nderived from real-world data pertaining to the query recommendation in\nvideo-\\textit{\\textbf{r}}elated \\textit{\\textbf{s}}earch on the\n\\textit{\\textbf{Kuai}}shou app (\\textbf{KuaiRS}). Presently, existing methods\nrely on embeddings to calculate similarity for matching short videos with\nqueries, lacking deep interaction between the semantic content and the query.\nIn this paper, we introduce a novel LLM-based framework named \\textbf{GREAT},\nwhich \\textit{\\textbf{g}}uides que\\textit{\\textbf{r}}y\ng\\textit{\\textbf{e}}ner\\textit{\\textbf{a}}tion with a \\textit{\\textbf{t}}rie to\naddress I2Q recommendation in related search. Specifically, we initially gather\nhigh-quality queries with high exposure and click-through rate to construct a\nquery-based trie. During training, we enhance the LLM's capability to generate\nhigh-quality queries using the query-based trie. In the inference phase, the\nquery-based trie serves as a guide for the token generation. Finally, we\nfurther refine the relevance and literal quality between items and queries via\na post-processing module. Extensive offline and online experiments demonstrate\nthe effectiveness of our proposed method.", "AI": {"tldr": "The paper introduces a novel LLM-based framework, GREAT, for query recommendation in video-related search, addressing the lack of research and datasets in this domain.", "motivation": "Short video platforms lack academic research and datasets for query recommendation (I2Q), prompting the need for a systematic solution.", "method": "Proposes GREAT, an LLM-based framework using a query-based trie to guide query generation and post-processing for relevance.", "result": "Offline and online experiments confirm the effectiveness of GREAT in improving query recommendation.", "conclusion": "GREAT successfully addresses the I2Q recommendation challenge, offering a scalable and effective solution."}}
{"id": "2507.15492", "pdf": "https://arxiv.org/pdf/2507.15492", "abs": "https://arxiv.org/abs/2507.15492", "authors": ["Rakesh John Amala Arokia Nathan", "Matthias Gessner", "Nurullah \u00d6zkan", "Marius Bock", "Mohamed Youssef", "Maximilian Mews", "Bj\u00f6rn Piltz", "Ralf Berger", "Oliver Bimber"], "title": "An aerial color image anomaly dataset for search missions in complex forested terrain", "categories": ["cs.CV"], "comment": "17 pages", "summary": "After a family murder in rural Germany, authorities failed to locate the\nsuspect in a vast forest despite a massive search. To aid the search, a\nresearch aircraft captured high-resolution aerial imagery. Due to dense\nvegetation obscuring small clues, automated analysis was ineffective, prompting\na crowd-search initiative. This effort produced a unique dataset of labeled,\nhard-to-detect anomalies under occluded, real-world conditions. It can serve as\na benchmark for improving anomaly detection approaches in complex forest\nenvironments, supporting manhunts and rescue operations. Initial benchmark\ntests showed existing methods performed poorly, highlighting the need for\ncontext-aware approaches. The dataset is openly accessible for offline\nprocessing. An additional interactive web interface supports online viewing and\ndynamic growth by allowing users to annotate and submit new findings.", "AI": {"tldr": "A crowd-search initiative created a dataset of hard-to-detect anomalies in dense forests, aiding future anomaly detection improvements for manhunts and rescues.", "motivation": "Authorities struggled to locate a suspect in dense forest despite aerial imagery, revealing the need for better anomaly detection methods.", "method": "High-resolution aerial imagery was analyzed via crowd-search due to ineffective automation, creating a labeled dataset of occluded anomalies.", "result": "Existing anomaly detection methods performed poorly, emphasizing the need for context-aware approaches.", "conclusion": "The openly accessible dataset and interactive web interface aim to advance anomaly detection in complex environments."}}
{"id": "2507.15272", "pdf": "https://arxiv.org/pdf/2507.15272", "abs": "https://arxiv.org/abs/2507.15272", "authors": ["Ayush Singh Bhadoriya", "Abhishek Nikunj Shinde", "Isha Pandey", "Ganesh Ramakrishnan"], "title": "A2TTS: TTS for Low Resource Indian Languages", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "comment": null, "summary": "We present a speaker conditioned text-to-speech (TTS) system aimed at\naddressing challenges in generating speech for unseen speakers and supporting\ndiverse Indian languages. Our method leverages a diffusion-based TTS\narchitecture, where a speaker encoder extracts embeddings from short reference\naudio samples to condition the DDPM decoder for multispeaker generation. To\nfurther enhance prosody and naturalness, we employ a cross-attention based\nduration prediction mechanism that utilizes reference audio, enabling more\naccurate and speaker consistent timing. This results in speech that closely\nresembles the target speaker while improving duration modeling and overall\nexpressiveness. Additionally, to improve zero-shot generation, we employed\nclassifier free guidance, allowing the system to generate speech more near\nspeech for unknown speakers. Using this approach, we trained language-specific\nspeaker-conditioned models. Using the IndicSUPERB dataset for multiple Indian\nlanguages such as Bengali, Gujarati, Hindi, Marathi, Malayalam, Punjabi and\nTamil.", "AI": {"tldr": "A diffusion-based TTS system for unseen speakers and Indian languages, using speaker embeddings and cross-attention for improved prosody and zero-shot generation.", "motivation": "Address challenges in generating speech for unseen speakers and support diverse Indian languages.", "method": "Uses a diffusion-based TTS architecture with speaker embeddings and cross-attention for duration prediction. Classifier-free guidance enhances zero-shot generation.", "result": "Speech closely resembles target speakers with improved duration modeling and expressiveness.", "conclusion": "The system effectively generates natural speech for diverse Indian languages and unseen speakers."}}
{"id": "2507.15496", "pdf": "https://arxiv.org/pdf/2507.15496", "abs": "https://arxiv.org/abs/2507.15496", "authors": ["JunYing Huang", "Ao Xu", "DongSun Yong", "KeRen Li", "YuanFeng Wang", "Qi Qin"], "title": "Dense-depth map guided deep Lidar-Visual Odometry with Sparse Point Clouds and Images", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": null, "summary": "Odometry is a critical task for autonomous systems for self-localization and\nnavigation. We propose a novel LiDAR-Visual odometry framework that integrates\nLiDAR point clouds and images for accurate and robust pose estimation. Our\nmethod utilizes a dense-depth map estimated from point clouds and images\nthrough depth completion, and incorporates a multi-scale feature extraction\nnetwork with attention mechanisms, enabling adaptive depth-aware\nrepresentations. Furthermore, we leverage dense depth information to refine\nflow estimation and mitigate errors in occlusion-prone regions. Our\nhierarchical pose refinement module optimizes motion estimation progressively,\nensuring robust predictions against dynamic environments and scale ambiguities.\nComprehensive experiments on the KITTI odometry benchmark demonstrate that our\napproach achieves similar or superior accuracy and robustness compared to\nstate-of-the-art visual and LiDAR odometry methods.", "AI": {"tldr": "A LiDAR-Visual odometry framework combining LiDAR and images for accurate pose estimation, using depth completion and attention mechanisms, outperforming state-of-the-art methods.", "motivation": "Odometry is crucial for autonomous systems, but existing methods may lack accuracy or robustness in dynamic environments. Integrating LiDAR and visual data can improve performance.", "method": "The framework uses dense-depth maps from LiDAR and images, multi-scale feature extraction with attention, and hierarchical pose refinement for robust motion estimation.", "result": "The method achieves similar or superior accuracy and robustness on the KITTI odometry benchmark compared to state-of-the-art techniques.", "conclusion": "The proposed LiDAR-Visual odometry framework is effective for accurate and robust pose estimation in autonomous systems."}}
{"id": "2507.15507", "pdf": "https://arxiv.org/pdf/2507.15507", "abs": "https://arxiv.org/abs/2507.15507", "authors": ["Johannes Ackermann", "Takashi Ishida", "Masashi Sugiyama"], "title": "Off-Policy Corrected Reward Modeling for Reinforcement Learning from Human Feedback", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accept at the Conference On Language Modeling (COLM) 2025", "summary": "Reinforcement Learning from Human Feedback (RLHF) allows us to train models,\nsuch as language models (LMs), to follow complex human preferences. In RLHF for\nLMs, we first train an LM using supervised fine-tuning, sample pairs of\nresponses, obtain human feedback, and use the resulting data to train a reward\nmodel (RM). RL methods are then used to train the LM to maximize the reward\ngiven by the RM. As training progresses, the responses generated by the LM no\nlonger resemble the responses seen by the RM during training, leading to the RM\nbecoming inaccurate. The score given by the RM keeps increasing, but the\nlearned behavior no longer matches the human preferences. This issue is known\nas overoptimization. We investigate overoptimization from the point of view of\ndistribution shift and show that the shift results in an inconsistent estimate\nof the RM parameters, leading to an inconsistent estimate of the policy\ngradient. We propose Off-Policy Corrected Reward Modeling (OCRM), which\niteratively off-policy corrects the RM using importance weighting, without\nrequiring new labels or samples. This results in a more accurate RM, which\nempirically leads to an improved final policy. We validate our approach in\nexperiments with summarization and chatbot datasets and show that it performs\nsignificantly better than standard RLHF methods and baselines. Our\nimplementation is available at\nhttps://github.com/JohannesAck/OffPolicyCorrectedRewardModeling", "AI": {"tldr": "The paper addresses overoptimization in RLHF by proposing OCRM, an off-policy correction method for reward models, improving policy alignment with human preferences.", "motivation": "Overoptimization in RLHF causes reward models to become inaccurate as the LM's responses diverge from training data, misaligning with human preferences.", "method": "Proposes Off-Policy Corrected Reward Modeling (OCRM), which iteratively corrects the reward model using importance weighting without new labels.", "result": "OCRM outperforms standard RLHF methods, yielding a more accurate reward model and improved policy in summarization and chatbot tasks.", "conclusion": "OCRM effectively mitigates overoptimization by addressing distribution shift, enhancing the alignment of learned behavior with human preferences."}}
{"id": "2507.15504", "pdf": "https://arxiv.org/pdf/2507.15504", "abs": "https://arxiv.org/abs/2507.15504", "authors": ["Bingqing Zhang", "Zhuo Cao", "Heming Du", "Yang Li", "Xue Li", "Jiajun Liu", "Sen Wang"], "title": "Quantifying and Narrowing the Unknown: Interactive Text-to-Video Retrieval via Uncertainty Minimization", "categories": ["cs.CV", "68T45", "I.2.10; H.3.3"], "comment": "Accepted by ICCV 2025", "summary": "Despite recent advances, Text-to-video retrieval (TVR) is still hindered by\nmultiple inherent uncertainties, such as ambiguous textual queries, indistinct\ntext-video mappings, and low-quality video frames. Although interactive systems\nhave emerged to address these challenges by refining user intent through\nclarifying questions, current methods typically rely on heuristic or ad-hoc\nstrategies without explicitly quantifying these uncertainties, limiting their\neffectiveness. Motivated by this gap, we propose UMIVR, an\nUncertainty-Minimizing Interactive Text-to-Video Retrieval framework that\nexplicitly quantifies three critical uncertainties-text ambiguity, mapping\nuncertainty, and frame uncertainty-via principled, training-free metrics:\nsemantic entropy-based Text Ambiguity Score (TAS), Jensen-Shannon\ndivergence-based Mapping Uncertainty Score (MUS), and a Temporal Quality-based\nFrame Sampler (TQFS). By adaptively generating targeted clarifying questions\nguided by these uncertainty measures, UMIVR iteratively refines user queries,\nsignificantly reducing retrieval ambiguity. Extensive experiments on multiple\nbenchmarks validate UMIVR's effectiveness, achieving notable gains in Recall@1\n(69.2\\% after 10 interactive rounds) on the MSR-VTT-1k dataset, thereby\nestablishing an uncertainty-minimizing foundation for interactive TVR.", "AI": {"tldr": "UMIVR is a framework for interactive text-to-video retrieval that quantifies uncertainties (text ambiguity, mapping uncertainty, frame uncertainty) to refine user queries, improving retrieval accuracy.", "motivation": "Current interactive TVR systems lack explicit uncertainty quantification, limiting effectiveness. UMIVR addresses this gap by measuring uncertainties to guide query refinement.", "method": "UMIVR uses training-free metrics: Text Ambiguity Score (TAS), Mapping Uncertainty Score (MUS), and Temporal Quality-based Frame Sampler (TQFS) to quantify uncertainties and generate clarifying questions.", "result": "UMIVR achieves a Recall@1 of 69.2% after 10 rounds on MSR-VTT-1k, demonstrating significant improvement in retrieval accuracy.", "conclusion": "UMIVR establishes a principled, uncertainty-minimizing approach for interactive TVR, validated by superior performance on benchmarks."}}
{"id": "2507.15640", "pdf": "https://arxiv.org/pdf/2507.15640", "abs": "https://arxiv.org/abs/2507.15640", "authors": ["Kailai Yang", "Xiao Liu", "Lei Ji", "Hao Li", "Yeyun Gong", "Peng Cheng", "Mao Yang"], "title": "Data Mixing Agent: Learning to Re-weight Domains for Continual Pre-training", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Continual pre-training on small-scale task-specific data is an effective\nmethod for improving large language models in new target fields, yet it risks\ncatastrophic forgetting of their original capabilities. A common solution is to\nre-weight training data mixtures from source and target fields on a domain\nspace to achieve balanced performance. Previous domain reweighting strategies\nrely on manual designation with certain heuristics based on human intuition or\nempirical results. In this work, we prove that more general heuristics can be\nparameterized by proposing Data Mixing Agent, the first model-based, end-to-end\nframework that learns to re-weight domains. The agent learns generalizable\nheuristics through reinforcement learning on large quantities of data mixing\ntrajectories with corresponding feedback from an evaluation environment.\nExperiments in continual pre-training on math reasoning show that Data Mixing\nAgent outperforms strong baselines in achieving balanced performance across\nsource and target field benchmarks. Furthermore, it generalizes well across\nunseen source fields, target models, and domain spaces without retraining.\nDirect application to the code generation field also indicates its adaptability\nacross target domains. Further analysis showcases the agents' well-aligned\nheuristics with human intuitions and their efficiency in achieving superior\nmodel performance with less source-field data.", "AI": {"tldr": "Proposes Data Mixing Agent, a model-based framework using reinforcement learning to re-weight domains for balanced performance in continual pre-training, outperforming manual heuristics.", "motivation": "Addresses catastrophic forgetting in continual pre-training by automating domain reweighting, moving beyond manual heuristics.", "method": "Uses reinforcement learning to train a Data Mixing Agent on data mixing trajectories, learning generalizable heuristics.", "result": "Outperforms baselines in math reasoning, generalizes to unseen fields/models, and adapts to code generation.", "conclusion": "Demonstrates effectiveness, adaptability, and alignment with human intuition, requiring less source data for superior performance."}}
{"id": "2507.15520", "pdf": "https://arxiv.org/pdf/2507.15520", "abs": "https://arxiv.org/abs/2507.15520", "authors": ["Hanting Li", "Fei Zhou", "Xin Sun", "Yang Hua", "Jungong Han", "Liang-Jie Zhang"], "title": "SAIGFormer: A Spatially-Adaptive Illumination-Guided Network for Low-Light Image Enhancement", "categories": ["cs.CV"], "comment": "11 pages, 10 figures, 6 tables", "summary": "Recent Transformer-based low-light enhancement methods have made promising\nprogress in recovering global illumination. However, they still struggle with\nnon-uniform lighting scenarios, such as backlit and shadow, appearing as\nover-exposure or inadequate brightness restoration. To address this challenge,\nwe present a Spatially-Adaptive Illumination-Guided Transformer (SAIGFormer)\nframework that enables accurate illumination restoration. Specifically, we\npropose a dynamic integral image representation to model the spatially-varying\nillumination, and further construct a novel Spatially-Adaptive Integral\nIllumination Estimator ($\\text{SAI}^2\\text{E}$). Moreover, we introduce an\nIllumination-Guided Multi-head Self-Attention (IG-MSA) mechanism, which\nleverages the illumination to calibrate the lightness-relevant features toward\nvisual-pleased illumination enhancement. Extensive experiments on five standard\nlow-light datasets and a cross-domain benchmark (LOL-Blur) demonstrate that our\nSAIGFormer significantly outperforms state-of-the-art methods in both\nquantitative and qualitative metrics. In particular, our method achieves\nsuperior performance in non-uniform illumination enhancement while exhibiting\nstrong generalization capabilities across multiple datasets. Code is available\nat https://github.com/LHTcode/SAIGFormer.git.", "AI": {"tldr": "SAIGFormer is a Transformer-based framework for low-light enhancement, addressing non-uniform lighting issues with spatially-adaptive illumination modeling and guided attention.", "motivation": "Existing methods struggle with non-uniform lighting scenarios like backlit and shadow, leading to over-exposure or inadequate brightness restoration.", "method": "Proposes a dynamic integral image representation (SAI\u00b2E) and an Illumination-Guided Multi-head Self-Attention (IG-MSA) mechanism for accurate illumination restoration.", "result": "Outperforms state-of-the-art methods on five datasets and a cross-domain benchmark, excelling in non-uniform illumination enhancement and generalization.", "conclusion": "SAIGFormer effectively addresses non-uniform lighting challenges, offering superior performance and generalization in low-light enhancement."}}
{"id": "2507.15743", "pdf": "https://arxiv.org/pdf/2507.15743", "abs": "https://arxiv.org/abs/2507.15743", "authors": ["Elahe Vedadi", "David Barrett", "Natalie Harris", "Ellery Wulczyn", "Shashir Reddy", "Roma Ruparel", "Mike Schaekermann", "Tim Strother", "Ryutaro Tanno", "Yash Sharma", "Jihyeon Lee", "C\u00edan Hughes", "Dylan Slack", "Anil Palepu", "Jan Freyberg", "Khaled Saab", "Valentin Li\u00e9vin", "Wei-Hung Weng", "Tao Tu", "Yun Liu", "Nenad Tomasev", "Kavita Kulkarni", "S. Sara Mahdavi", "Kelvin Guu", "Jo\u00eblle Barral", "Dale R. Webster", "James Manyika", "Avinatan Hassidim", "Katherine Chou", "Yossi Matias", "Pushmeet Kohli", "Adam Rodman", "Vivek Natarajan", "Alan Karthikesalingam", "David Stutz"], "title": "Towards physician-centered oversight of conversational diagnostic AI", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG"], "comment": null, "summary": "Recent work has demonstrated the promise of conversational AI systems for\ndiagnostic dialogue. However, real-world assurance of patient safety means that\nproviding individual diagnoses and treatment plans is considered a regulated\nactivity by licensed professionals. Furthermore, physicians commonly oversee\nother team members in such activities, including nurse practitioners (NPs) or\nphysician assistants/associates (PAs). Inspired by this, we propose a framework\nfor effective, asynchronous oversight of the Articulate Medical Intelligence\nExplorer (AMIE) AI system. We propose guardrailed-AMIE (g-AMIE), a multi-agent\nsystem that performs history taking within guardrails, abstaining from\nindividualized medical advice. Afterwards, g-AMIE conveys assessments to an\noverseeing primary care physician (PCP) in a clinician cockpit interface. The\nPCP provides oversight and retains accountability of the clinical decision.\nThis effectively decouples oversight from intake and can thus happen\nasynchronously. In a randomized, blinded virtual Objective Structured Clinical\nExamination (OSCE) of text consultations with asynchronous oversight, we\ncompared g-AMIE to NPs/PAs or a group of PCPs under the same guardrails. Across\n60 scenarios, g-AMIE outperformed both groups in performing high-quality\nintake, summarizing cases, and proposing diagnoses and management plans for the\noverseeing PCP to review. This resulted in higher quality composite decisions.\nPCP oversight of g-AMIE was also more time-efficient than standalone PCP\nconsultations in prior work. While our study does not replicate existing\nclinical practices and likely underestimates clinicians' capabilities, our\nresults demonstrate the promise of asynchronous oversight as a feasible\nparadigm for diagnostic AI systems to operate under expert human oversight for\nenhancing real-world care.", "AI": {"tldr": "The paper proposes g-AMIE, a multi-agent AI system for medical history taking under guardrails, with asynchronous oversight by physicians, showing improved efficiency and decision quality.", "motivation": "To address the need for regulated, safe diagnostic dialogue systems while leveraging AI for efficiency, inspired by physician oversight of NPs/PAs.", "method": "Developed g-AMIE, a system abstaining from direct advice, conveying assessments to physicians via a cockpit interface, tested in a virtual OSCE with 60 scenarios.", "result": "g-AMIE outperformed NPs/PAs and PCPs in intake quality, case summaries, and proposed plans, leading to better composite decisions and time efficiency.", "conclusion": "Asynchronous oversight by physicians is a feasible paradigm for AI diagnostic systems, enhancing care quality and efficiency, though clinical replication is needed."}}
{"id": "2507.15540", "pdf": "https://arxiv.org/pdf/2507.15540", "abs": "https://arxiv.org/abs/2507.15540", "authors": ["Syed Ahmed Mahmood", "Ali Shah Ali", "Umer Ahmed", "Fawad Javed Fateh", "M. Zeeshan Zia", "Quoc-Huy Tran"], "title": "Procedure Learning via Regularized Gromov-Wasserstein Optimal Transport", "categories": ["cs.CV"], "comment": null, "summary": "We study the problem of self-supervised procedure learning, which discovers\nkey steps and establishes their order from a set of unlabeled procedural\nvideos. Previous procedure learning methods typically learn frame-to-frame\ncorrespondences between videos before determining key steps and their order.\nHowever, their performance often suffers from order variations,\nbackground/redundant frames, and repeated actions. To overcome these\nchallenges, we propose a self-supervised procedure learning framework, which\nutilizes a fused Gromov-Wasserstein optimal transport formulation with a\nstructural prior for computing frame-to-frame mapping between videos. However,\noptimizing exclusively for the above temporal alignment term may lead to\ndegenerate solutions, where all frames are mapped to a small cluster in the\nembedding space and hence every video is associated with only one key step. To\naddress that limitation, we further integrate a contrastive regularization\nterm, which maps different frames to different points in the embedding space,\navoiding the collapse to trivial solutions. Finally, we conduct extensive\nexperiments on large-scale egocentric (i.e., EgoProceL) and third-person (i.e.,\nProceL and CrossTask) benchmarks to demonstrate superior performance by our\napproach against previous methods, including OPEL which relies on a traditional\nKantorovich optimal transport formulation with an optimality prior.", "AI": {"tldr": "A self-supervised framework for learning procedures from unlabeled videos, using fused Gromov-Wasserstein optimal transport and contrastive regularization to avoid degenerate solutions.", "motivation": "Address challenges like order variations, redundant frames, and repeated actions in procedure learning from unlabeled videos.", "method": "Proposes a framework combining fused Gromov-Wasserstein optimal transport for temporal alignment and contrastive regularization to prevent embedding collapse.", "result": "Outperforms previous methods on benchmarks like EgoProceL, ProceL, and CrossTask.", "conclusion": "The approach effectively learns key steps and their order, overcoming limitations of prior methods."}}
{"id": "2507.15758", "pdf": "https://arxiv.org/pdf/2507.15758", "abs": "https://arxiv.org/abs/2507.15758", "authors": ["Xingyu Wu", "Yuchen Yan", "Shangke Lyu", "Linjuan Wu", "Yiwen Qiu", "Yongliang Shen", "Weiming Lu", "Jian Shao", "Jun Xiao", "Yueting Zhuang"], "title": "LAPO: Internalizing Reasoning Efficiency via Length-Adaptive Policy Optimization", "categories": ["cs.AI", "cs.CL"], "comment": "GitHub:https://github.com/zju-real/lapo;\n  Project:https://zju-real.github.io/lapo", "summary": "Large reasoning models have achieved remarkable performance through extended\nchain-of-thought sequences, yet this computational freedom leads to excessive\ntoken generation even for simple problems. We present Length-Adaptive Policy\nOptimization (LAPO), a novel framework that transforms reasoning length control\nfrom an external constraint into an intrinsic model capability. Unlike existing\napproaches that impose rigid limits or rely on post-hoc interventions, LAPO\nenables models to internalize an understanding of appropriate reasoning depth\nthrough a two-stage reinforcement learning process. In the first stage, models\nlearn natural reasoning patterns by discovering the statistical distribution of\nsuccessful solution lengths. The second stage leverages these patterns as\nmeta-cognitive guidance, embedding them directly within the model's reasoning\ncontext to ensure inference-time flexibility. Experiments on mathematical\nreasoning benchmarks demonstrate that LAPO reduces token usage by up to 40.9\\%\nwhile improving accuracy by 2.3\\%. Our analysis reveals that models trained\nwith LAPO develop emergent abilities to allocate computational resources based\non problem complexity, achieving efficient reasoning without sacrificing\nquality.", "AI": {"tldr": "LAPO is a framework that optimizes reasoning length in models, reducing token usage by 40.9% while improving accuracy by 2.3%.", "motivation": "Large reasoning models generate excessive tokens for simple problems, needing a solution to internalize reasoning depth control.", "method": "LAPO uses a two-stage reinforcement learning process: first learning natural reasoning patterns, then embedding them for inference-time flexibility.", "result": "Reduces token usage by 40.9% and improves accuracy by 2.3% on mathematical reasoning benchmarks.", "conclusion": "LAPO enables efficient reasoning by adapting computational resources to problem complexity without quality loss."}}
{"id": "2507.15541", "pdf": "https://arxiv.org/pdf/2507.15541", "abs": "https://arxiv.org/abs/2507.15541", "authors": ["Jongmin Shin", "Enki Cho", "Ka Yong Kim", "Jung Yong Kim", "Seong Tae Kim", "Namkee Oh"], "title": "Towards Holistic Surgical Scene Graph", "categories": ["cs.CV"], "comment": "Accepted to MICCAI 2025", "summary": "Surgical scene understanding is crucial for computer-assisted intervention\nsystems, requiring visual comprehension of surgical scenes that involves\ndiverse elements such as surgical tools, anatomical structures, and their\ninteractions. To effectively represent the complex information in surgical\nscenes, graph-based approaches have been explored to structurally model\nsurgical entities and their relationships. Previous surgical scene graph\nstudies have demonstrated the feasibility of representing surgical scenes using\ngraphs. However, certain aspects of surgical scenes-such as diverse\ncombinations of tool-action-target and the identity of the hand operating the\ntool-remain underexplored in graph-based representations, despite their\nimportance. To incorporate these aspects into graph representations, we propose\nEndoscapes-SG201 dataset, which includes annotations for tool-action-target\ncombinations and hand identity. We also introduce SSG-Com, a graph-based method\ndesigned to learn and represent these critical elements. Through experiments on\ndownstream tasks such as critical view of safety assessment and action triplet\nrecognition, we demonstrated the importance of integrating these essential\nscene graph components, highlighting their significant contribution to surgical\nscene understanding. The code and dataset are available at\nhttps://github.com/ailab-kyunghee/SSG-Com", "AI": {"tldr": "The paper introduces Endoscapes-SG201 dataset and SSG-Com, a graph-based method, to enhance surgical scene understanding by modeling tool-action-target combinations and hand identity.", "motivation": "Current graph-based representations of surgical scenes lack exploration of tool-action-target combinations and hand identity, which are crucial for comprehensive scene understanding.", "method": "Proposes Endoscapes-SG201 dataset with annotations for tool-action-target and hand identity, and introduces SSG-Com, a graph-based method to model these elements.", "result": "Experiments show improved performance in downstream tasks like critical view of safety assessment and action triplet recognition.", "conclusion": "Incorporating tool-action-target and hand identity in graph representations significantly enhances surgical scene understanding."}}
{"id": "2507.15776", "pdf": "https://arxiv.org/pdf/2507.15776", "abs": "https://arxiv.org/abs/2507.15776", "authors": ["Noor Sajid", "Johan Medrano"], "title": "Dissociating model architectures from inference computations", "categories": ["q-bio.NC", "cs.CL", "cs.LG"], "comment": "3 pages, 1 figure", "summary": "Parr et al., 2025 examines how auto-regressive and deep temporal models\ndiffer in their treatment of non-Markovian sequence modelling. Building on\nthis, we highlight the need for dissociating model architectures, i.e., how the\npredictive distribution factorises, from the computations invoked at inference.\nWe demonstrate that deep temporal computations are mimicked by autoregressive\nmodels by structuring context access during iterative inference. Using a\ntransformer trained on next-token prediction, we show that inducing\nhierarchical temporal factorisation during iterative inference maintains\npredictive capacity while instantiating fewer computations. This emphasises\nthat processes for constructing and refining predictions are not necessarily\nbound to their underlying model architectures.", "AI": {"tldr": "Auto-regressive and deep temporal models differ in non-Markovian sequence modeling. The paper shows how deep temporal computations can be mimicked by auto-regressive models through structured context access, maintaining predictive capacity with fewer computations.", "motivation": "To dissociate model architectures from inference computations and explore how auto-regressive models can mimic deep temporal computations.", "method": "Using a transformer trained on next-token prediction, hierarchical temporal factorization is induced during iterative inference.", "result": "Auto-regressive models can mimic deep temporal computations while maintaining predictive capacity and reducing computations.", "conclusion": "Prediction construction and refinement processes are not strictly tied to model architectures."}}
{"id": "2507.15542", "pdf": "https://arxiv.org/pdf/2507.15542", "abs": "https://arxiv.org/abs/2507.15542", "authors": ["Qinqian Lei", "Bo Wang", "Robby T. Tan"], "title": "HOLa: Zero-Shot HOI Detection with Low-Rank Decomposed VLM Feature Adaptation", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Zero-shot human-object interaction (HOI) detection remains a challenging\ntask, particularly in generalizing to unseen actions. Existing methods address\nthis challenge by tapping Vision-Language Models (VLMs) to access knowledge\nbeyond the training data. However, they either struggle to distinguish actions\ninvolving the same object or demonstrate limited generalization to unseen\nclasses. In this paper, we introduce HOLa (Zero-Shot HOI Detection with\nLow-Rank Decomposed VLM Feature Adaptation), a novel approach that both\nenhances generalization to unseen classes and improves action distinction. In\ntraining, HOLa decomposes VLM text features for given HOI classes via low-rank\nfactorization, producing class-shared basis features and adaptable weights.\nThese features and weights form a compact HOI representation that preserves\nshared information across classes, enhancing generalization to unseen classes.\nSubsequently, we refine action distinction by adapting weights for each HOI\nclass and introducing human-object tokens to enrich visual interaction\nrepresentations. To further distinguish unseen actions, we guide the weight\nadaptation with LLM-derived action regularization. Experimental results show\nthat our method sets a new state-of-the-art across zero-shot HOI settings on\nHICO-DET, achieving an unseen-class mAP of 27.91 in the unseen-verb setting.\nOur code is available at https://github.com/ChelsieLei/HOLa.", "AI": {"tldr": "HOLa improves zero-shot HOI detection by decomposing VLM text features and adapting weights, achieving state-of-the-art results.", "motivation": "Addressing the challenge of generalizing to unseen actions in HOI detection by leveraging VLMs and enhancing action distinction.", "method": "Decomposes VLM text features via low-rank factorization, adapts weights for each HOI class, and uses human-object tokens and LLM-derived action regularization.", "result": "Achieves an unseen-class mAP of 27.91 on HICO-DET, setting a new state-of-the-art.", "conclusion": "HOLa effectively enhances generalization and action distinction in zero-shot HOI detection."}}
{"id": "2507.15788", "pdf": "https://arxiv.org/pdf/2507.15788", "abs": "https://arxiv.org/abs/2507.15788", "authors": ["Sneheel Sarangi", "Hanan Salam"], "title": "Small LLMs Do Not Learn a Generalizable Theory of Mind via Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have demonstrated\nemergent capabilities in complex reasoning, largely spurred by rule-based\nReinforcement Learning (RL) techniques applied during the post-training. This\nhas raised the question of whether similar methods can instill more nuanced,\nhuman-like social intelligence, such as a Theory of Mind (ToM), in LLMs. This\npaper investigates whether small-scale LLMs can acquire a robust and\ngeneralizable ToM capability through RL with verifiable rewards (RLVR). We\nconduct a systematic evaluation by training models on various combinations of\nprominent ToM datasets (HiToM, ExploreToM, FANToM) and testing for\ngeneralization on held-out datasets (e.g., OpenToM). Our findings indicate that\nsmall LLMs struggle to develop a generic ToM capability. While performance on\nin-distribution tasks improves, this capability fails to transfer to unseen ToM\ntasks with different characteristics. Furthermore, we demonstrate that\nprolonged RL training leads to models ``hacking'' the statistical patterns of\nthe training datasets, resulting in significant performance gains on in-domain\ndata but no change, or degradation of performance on out-of-distribution tasks.\nThis suggests the learned behavior is a form of narrow overfitting rather than\nthe acquisition of a true, abstract ToM capability.", "AI": {"tldr": "Small LLMs trained with RLVR struggle to develop generalizable Theory of Mind (ToM) capabilities, showing narrow overfitting instead of abstract understanding.", "motivation": "To explore if RL techniques can instill nuanced social intelligence (ToM) in small LLMs.", "method": "Train small LLMs on ToM datasets (HiToM, ExploreToM, FANToM) using RL with verifiable rewards (RLVR) and test generalization on held-out datasets (e.g., OpenToM).", "result": "Small LLMs improve on in-distribution tasks but fail to generalize to unseen ToM tasks, showing narrow overfitting.", "conclusion": "RLVR leads to narrow overfitting in small LLMs, not true ToM capability."}}
{"id": "2507.15569", "pdf": "https://arxiv.org/pdf/2507.15569", "abs": "https://arxiv.org/abs/2507.15569", "authors": ["Xiaoyi Bao", "Chenwei Xie", "Hao Tang", "Tingyu Weng", "Xiaofeng Wang", "Yun Zheng", "Xingang Wang"], "title": "DynImg: Key Frames with Visual Prompts are Good Representation for Multi-Modal Video Understanding", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "In recent years, the introduction of Multi-modal Large Language Models\n(MLLMs) into video understanding tasks has become increasingly prevalent.\nHowever, how to effectively integrate temporal information remains a critical\nresearch focus. Traditional approaches treat spatial and temporal information\nseparately. Due to issues like motion blur, it is challenging to accurately\nrepresent the spatial information of rapidly moving objects. This can lead to\ntemporally important regions being underemphasized during spatial feature\nextraction, which in turn hinders accurate spatio-temporal interaction and\nvideo understanding. To address this limitation, we propose an innovative video\nrepresentation method called Dynamic-Image (DynImg). Specifically, we introduce\na set of non-key frames as temporal prompts to highlight the spatial areas\ncontaining fast-moving objects. During the process of visual feature\nextraction, these prompts guide the model to pay additional attention to the\nfine-grained spatial features corresponding to these regions. Moreover, to\nmaintain the correct sequence for DynImg, we employ a corresponding 4D video\nRotary Position Embedding. This retains both the temporal and spatial adjacency\nof DynImg, helping MLLM understand the spatio-temporal order within this\ncombined format. Experimental evaluations reveal that DynImg surpasses the\nstate-of-the-art methods by approximately 2% across multiple video\nunderstanding benchmarks, proving the effectiveness of our temporal prompts in\nenhancing video comprehension.", "AI": {"tldr": "The paper introduces Dynamic-Image (DynImg), a novel video representation method using non-key frames as temporal prompts to improve spatial feature extraction for fast-moving objects, enhancing spatio-temporal interaction in video understanding.", "motivation": "Traditional methods treat spatial and temporal information separately, often underemphasizing temporally important regions due to motion blur, hindering accurate video understanding.", "method": "DynImg employs non-key frames as temporal prompts to highlight fast-moving objects and uses 4D Rotary Position Embedding to maintain spatio-temporal order.", "result": "DynImg outperforms state-of-the-art methods by ~2% on multiple video understanding benchmarks.", "conclusion": "DynImg effectively enhances video comprehension by integrating temporal prompts, improving spatio-temporal interaction."}}
{"id": "2507.15844", "pdf": "https://arxiv.org/pdf/2507.15844", "abs": "https://arxiv.org/abs/2507.15844", "authors": ["Shangke Lyu", "Linjuan Wu", "Yuchen Yan", "Xingyu Wu", "Hao Li", "Yongliang Shen", "Peisheng Jiang", "Weiming Lu", "Jun Xiao", "Yueting Zhuang"], "title": "Hierarchical Budget Policy Optimization for Adaptive Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": "Code: https://github.com/zju-real/hbpo Project\n  Page:https://zju-real.github.io/hbpo/", "summary": "Large reasoning models achieve remarkable performance through extensive\nchain-of-thought generation, yet exhibit significant computational inefficiency\nby applying uniform reasoning strategies regardless of problem complexity. We\npresent Hierarchical Budget Policy Optimization (HBPO), a reinforcement\nlearning framework that enables models to learn problem-specific reasoning\ndepths without sacrificing capability. HBPO addresses the fundamental challenge\nof exploration space collapse in efficiency-oriented training, where penalties\non long output length systematically bias models away from necessary long\nreasoning paths. Through hierarchical budget exploration, our approach\npartitions rollout samples into multiple subgroups with distinct token budgets,\naiming to enable efficient resource allocation while preventing degradation of\ncapability. We introduce differentiated reward mechanisms that create\nbudget-aware incentives aligned with the complexity of the problem, allowing\nmodels to discover natural correspondences between task requirements and\ncomputational effort. Extensive experiments demonstrate that HBPO reduces\naverage token usage by up to 60.6% while improving accuracy by 3.14% across\nfour reasoning benchmarks. Unlike existing methods that impose external\nconstraints or rely on discrete mode selection, HBPO exhibits emergent adaptive\nbehavior where models automatically adjust reasoning depth based on problem\ncomplexity. Our results suggest that reasoning efficiency and capability are\nnot inherently conflicting, and can be simultaneously optimized through\nappropriately structured hierarchical training that preserves exploration\ndiversity.", "AI": {"tldr": "HBPO is a reinforcement learning framework that optimizes reasoning efficiency in large models by learning problem-specific reasoning depths, reducing token usage by 60.6% while improving accuracy.", "motivation": "Current reasoning models use uniform strategies regardless of problem complexity, leading to computational inefficiency. HBPO aims to address this by enabling adaptive reasoning depths.", "method": "HBPO uses hierarchical budget exploration, partitioning samples into subgroups with distinct token budgets, and employs differentiated reward mechanisms to align effort with problem complexity.", "result": "HBPO reduces token usage by up to 60.6% and improves accuracy by 3.14% across benchmarks, showing emergent adaptive behavior without external constraints.", "conclusion": "HBPO demonstrates that reasoning efficiency and capability can coexist through hierarchical training, preserving exploration diversity and enabling adaptive behavior."}}
{"id": "2507.15577", "pdf": "https://arxiv.org/pdf/2507.15577", "abs": "https://arxiv.org/abs/2507.15577", "authors": ["Hugo Carlesso", "Maria Eliza Patulea", "Moncef Garouani", "Radu Tudor Ionescu", "Josiane Mothe"], "title": "GeMix: Conditional GAN-Based Mixup for Improved Medical Image Augmentation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Mixup has become a popular augmentation strategy for image classification,\nyet its naive pixel-wise interpolation often produces unrealistic images that\ncan hinder learning, particularly in high-stakes medical applications. We\npropose GeMix, a two-stage framework that replaces heuristic blending with a\nlearned, label-aware interpolation powered by class-conditional GANs. First, a\nStyleGAN2-ADA generator is trained on the target dataset. During augmentation,\nwe sample two label vectors from Dirichlet priors biased toward different\nclasses and blend them via a Beta-distributed coefficient. Then, we condition\nthe generator on this soft label to synthesize visually coherent images that\nlie along a continuous class manifold. We benchmark GeMix on the large-scale\nCOVIDx-CT-3 dataset using three backbones (ResNet-50, ResNet-101,\nEfficientNet-B0). When combined with real data, our method increases macro-F1\nover traditional mixup for all backbones, reducing the false negative rate for\nCOVID-19 detection. GeMix is thus a drop-in replacement for pixel-space mixup,\ndelivering stronger regularization and greater semantic fidelity, without\ndisrupting existing training pipelines. We publicly release our code at\nhttps://github.com/hugocarlesso/GeMix to foster reproducibility and further\nresearch.", "AI": {"tldr": "GeMix replaces naive pixel-wise mixup with a learned, label-aware interpolation using GANs, improving image realism and classification performance, especially in medical applications like COVID-19 detection.", "motivation": "Naive pixel-wise mixup produces unrealistic images, hindering learning in high-stakes medical applications. GeMix aims to enhance realism and semantic fidelity.", "method": "A two-stage framework: (1) Train a StyleGAN2-ADA generator on the target dataset. (2) Sample and blend label vectors from Dirichlet priors, then condition the generator to synthesize coherent images.", "result": "GeMix outperforms traditional mixup, increasing macro-F1 scores and reducing false negatives in COVID-19 detection across multiple backbones.", "conclusion": "GeMix is a drop-in replacement for mixup, offering better regularization and semantic fidelity without disrupting training pipelines."}}
{"id": "2507.15846", "pdf": "https://arxiv.org/pdf/2507.15846", "abs": "https://arxiv.org/abs/2507.15846", "authors": ["Fei Tang", "Zhangxuan Gu", "Zhengxi Lu", "Xuyang Liu", "Shuheng Shen", "Changhua Meng", "Wen Wang", "Wenqi Zhang", "Yongliang Shen", "Weiming Lu", "Jun Xiao", "Yueting Zhuang"], "title": "GUI-G$^2$: Gaussian Reward Modeling for GUI Grounding", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.HC"], "comment": null, "summary": "Graphical User Interface (GUI) grounding maps natural language instructions\nto precise interface locations for autonomous interaction. Current\nreinforcement learning approaches use binary rewards that treat elements as\nhit-or-miss targets, creating sparse signals that ignore the continuous nature\nof spatial interactions. Motivated by human clicking behavior that naturally\nforms Gaussian distributions centered on target elements, we introduce GUI\nGaussian Grounding Rewards (GUI-G$^2$), a principled reward framework that\nmodels GUI elements as continuous Gaussian distributions across the interface\nplane. GUI-G$^2$ incorporates two synergistic mechanisms: Gaussian point\nrewards model precise localization through exponentially decaying distributions\ncentered on element centroids, while coverage rewards assess spatial alignment\nby measuring the overlap between predicted Gaussian distributions and target\nregions. To handle diverse element scales, we develop an adaptive variance\nmechanism that calibrates reward distributions based on element dimensions.\nThis framework transforms GUI grounding from sparse binary classification to\ndense continuous optimization, where Gaussian distributions generate rich\ngradient signals that guide models toward optimal interaction positions.\nExtensive experiments across ScreenSpot, ScreenSpot-v2, and ScreenSpot-Pro\nbenchmarks demonstrate that GUI-G$^2$, substantially outperforms\nstate-of-the-art method UI-TARS-72B, with the most significant improvement of\n24.7% on ScreenSpot-Pro. Our analysis reveals that continuous modeling provides\nsuperior robustness to interface variations and enhanced generalization to\nunseen layouts, establishing a new paradigm for spatial reasoning in GUI\ninteraction tasks.", "AI": {"tldr": "GUI-G\u00b2 introduces Gaussian rewards for GUI grounding, outperforming UI-TARS-72B by 24.7% on ScreenSpot-Pro by modeling spatial interactions continuously.", "motivation": "Current reinforcement learning uses sparse binary rewards, ignoring the continuous nature of spatial interactions. Human clicking behavior, which forms Gaussian distributions, inspired the solution.", "method": "GUI-G\u00b2 models GUI elements as Gaussian distributions with two mechanisms: Gaussian point rewards for precise localization and coverage rewards for spatial alignment. An adaptive variance mechanism handles diverse element scales.", "result": "GUI-G\u00b2 outperforms UI-TARS-72B by 24.7% on ScreenSpot-Pro, showing robustness to interface variations and better generalization to unseen layouts.", "conclusion": "GUI-G\u00b2 transforms GUI grounding into dense continuous optimization, setting a new paradigm for spatial reasoning in GUI interaction tasks."}}
{"id": "2507.15578", "pdf": "https://arxiv.org/pdf/2507.15578", "abs": "https://arxiv.org/abs/2507.15578", "authors": ["Gabriele Inzerillo", "Diego Valsesia", "Aniello Fiengo", "Enrico Magli"], "title": "Compress-Align-Detect: onboard change detection from unregistered images", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Change detection from satellite images typically incurs a delay ranging from\nseveral hours up to days because of latency in downlinking the acquired images\nand generating orthorectified image products at the ground stations; this may\npreclude real- or near real-time applications. To overcome this limitation, we\npropose shifting the entire change detection workflow onboard satellites. This\nrequires to simultaneously solve challenges in data storage, image registration\nand change detection with a strict complexity constraint. In this paper, we\npresent a novel and efficient framework for onboard change detection that\naddresses the aforementioned challenges in an end-to-end fashion with a deep\nneural network composed of three interlinked submodules: (1) image compression,\ntailored to minimize onboard data storage resources; (2) lightweight\nco-registration of non-orthorectified multi-temporal image pairs; and (3) a\nnovel temporally-invariant and computationally efficient change detection\nmodel. This is the first approach in the literature combining all these tasks\nin a single end-to-end framework with the constraints dictated by onboard\nprocessing. Experimental results compare each submodule with the current\nstate-of-the-art, and evaluate the performance of the overall integrated system\nin realistic setting on low-power hardware. Compelling change detection results\nare obtained in terms of F1 score as a function of compression rate, sustaining\na throughput of 0.7 Mpixel/s on a 15W accelerator.", "AI": {"tldr": "Proposes an onboard satellite framework for real-time change detection using a deep neural network with three submodules: compression, co-registration, and change detection.", "motivation": "Overcome delays in traditional satellite change detection by shifting the workflow onboard to enable real-time applications.", "method": "Uses a deep neural network with three interlinked submodules: image compression, lightweight co-registration, and a temporally-invariant change detection model.", "result": "Achieves compelling F1 scores and sustains 0.7 Mpixel/s throughput on low-power hardware.", "conclusion": "The framework successfully addresses onboard processing constraints and outperforms state-of-the-art methods."}}
{"id": "2507.15595", "pdf": "https://arxiv.org/pdf/2507.15595", "abs": "https://arxiv.org/abs/2507.15595", "authors": ["Salah Eddine Bekhouche", "Gaby Maroun", "Fadi Dornaika", "Abdenour Hadid"], "title": "SegDT: A Diffusion Transformer-Based Segmentation Model for Medical Imaging", "categories": ["cs.CV"], "comment": null, "summary": "Medical image segmentation is crucial for many healthcare tasks, including\ndisease diagnosis and treatment planning. One key area is the segmentation of\nskin lesions, which is vital for diagnosing skin cancer and monitoring\npatients. In this context, this paper introduces SegDT, a new segmentation\nmodel based on diffusion transformer (DiT). SegDT is designed to work on\nlow-cost hardware and incorporates Rectified Flow, which improves the\ngeneration quality at reduced inference steps and maintains the flexibility of\nstandard diffusion models. Our method is evaluated on three benchmarking\ndatasets and compared against several existing works, achieving\nstate-of-the-art results while maintaining fast inference speeds. This makes\nthe proposed model appealing for real-world medical applications. This work\nadvances the performance and capabilities of deep learning models in medical\nimage analysis, enabling faster, more accurate diagnostic tools for healthcare\nprofessionals. The code is made publicly available at\n\\href{https://github.com/Bekhouche/SegDT}{GitHub}.", "AI": {"tldr": "SegDT, a diffusion transformer-based model for skin lesion segmentation, achieves state-of-the-art results on low-cost hardware with fast inference speeds.", "motivation": "Improving medical image segmentation for skin lesions to aid in disease diagnosis and treatment planning.", "method": "SegDT combines diffusion transformer (DiT) with Rectified Flow for efficient and high-quality segmentation.", "result": "Achieves state-of-the-art performance on three datasets with fast inference speeds.", "conclusion": "SegDT advances medical image analysis, offering a practical tool for healthcare professionals."}}
{"id": "2507.15597", "pdf": "https://arxiv.org/pdf/2507.15597", "abs": "https://arxiv.org/abs/2507.15597", "authors": ["Hao Luo", "Yicheng Feng", "Wanpeng Zhang", "Sipeng Zheng", "Ye Wang", "Haoqi Yuan", "Jiazheng Liu", "Chaoyi Xu", "Qin Jin", "Zongqing Lu"], "title": "Being-H0: Vision-Language-Action Pretraining from Large-Scale Human Videos", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": "37 pages", "summary": "We introduce Being-H0, a dexterous Vision-Language-Action model (VLA) trained\non large-scale human videos. Existing VLAs struggle with complex manipulation\ntasks requiring high dexterity and generalize poorly to novel scenarios and\ntasks, primarily due to their reliance on synthetic data with significant\nsim-to-real gaps or teleoperated demonstrations lacking scale and diversity. To\naddress this data bottleneck, we propose leveraging human hands as a foundation\nmanipulator, capitalizing on the rich dexterity and scalability present in web\ndata. Our approach centers on physical instruction tuning, a novel training\nparadigm that combines large-scale VLA pretraining from human videos, physical\nspace alignment for 3D reasoning, and post-training adaptation for robotic\ntasks. Additionally, we introduce a part-level motion tokenization method which\nachieves millimeter-level reconstruction accuracy to model precise hand\ntrajectories for action learning. To support our proposed paradigm, we further\ndevelop a comprehensive data curation pipeline that integrates heterogeneous\nsources -- including motion capture, VR, and RGB-only videos -- into a\nlarge-scale dataset with millions of motion-based instructional instances. We\nempirically show the excellence of Being-H0 in hand motion generation and\ninstruction following, and it also scales well with model and data sizes.\nImportantly, we observe the expected gains of Being-H0 in real-world robotic\nmanipulation as physical instruction tuning is applied. More details are\navailable at https://beingbeyond.github.io/Being-H0.", "AI": {"tldr": "Being-H0 is a Vision-Language-Action model trained on human videos to improve dexterity and generalization in robotic tasks, using physical instruction tuning and part-level motion tokenization.", "motivation": "Existing VLAs struggle with complex manipulation tasks due to synthetic data limitations or lack of diverse demonstrations. Being-H0 leverages human hand dexterity and scalable web data to address this.", "method": "The approach combines VLA pretraining from human videos, physical space alignment, and post-training adaptation. It introduces part-level motion tokenization for precise hand trajectory modeling and a data curation pipeline integrating diverse sources.", "result": "Being-H0 excels in hand motion generation and instruction following, scaling well with model and data sizes, and shows gains in real-world robotic manipulation.", "conclusion": "Physical instruction tuning and leveraging human videos effectively enhance VLA performance in dexterous tasks, bridging the sim-to-real gap."}}
{"id": "2507.15602", "pdf": "https://arxiv.org/pdf/2507.15602", "abs": "https://arxiv.org/abs/2507.15602", "authors": ["Zihui Gao", "Jia-Wang Bian", "Guosheng Lin", "Hao Chen", "Chunhua Shen"], "title": "SurfaceSplat: Connecting Surface Reconstruction and Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Surface reconstruction and novel view rendering from sparse-view images are\nchallenging. Signed Distance Function (SDF)-based methods struggle with fine\ndetails, while 3D Gaussian Splatting (3DGS)-based approaches lack global\ngeometry coherence. We propose a novel hybrid method that combines the\nstrengths of both approaches: SDF captures coarse geometry to enhance\n3DGS-based rendering, while newly rendered images from 3DGS refine the details\nof SDF for accurate surface reconstruction. As a result, our method surpasses\nstate-of-the-art approaches in surface reconstruction and novel view synthesis\non the DTU and MobileBrick datasets. Code will be released at\nhttps://github.com/Gaozihui/SurfaceSplat.", "AI": {"tldr": "A hybrid method combining SDF and 3DGS improves surface reconstruction and novel view rendering by leveraging coarse geometry and fine details.", "motivation": "Addressing the limitations of SDF (lack of fine details) and 3DGS (lack of global coherence) in sparse-view image tasks.", "method": "Combines SDF for coarse geometry and 3DGS for detail refinement, using rendered images from 3DGS to enhance SDF accuracy.", "result": "Outperforms state-of-the-art methods on DTU and MobileBrick datasets.", "conclusion": "The hybrid approach effectively balances geometry coherence and detail accuracy, advancing sparse-view reconstruction and rendering."}}
{"id": "2507.15606", "pdf": "https://arxiv.org/pdf/2507.15606", "abs": "https://arxiv.org/abs/2507.15606", "authors": ["Ru Jia", "Xiaozhuang Ma", "Jianji Wang", "Nanning Zheng"], "title": "CylinderPlane: Nested Cylinder Representation for 3D-aware Image Generation", "categories": ["cs.CV", "68T45", "I.4.5"], "comment": "5 pages, 4 figures, to be published", "summary": "While the proposal of the Tri-plane representation has advanced the\ndevelopment of the 3D-aware image generative models, problems rooted in its\ninherent structure, such as multi-face artifacts caused by sharing the same\nfeatures in symmetric regions, limit its ability to generate 360$^\\circ$ view\nimages. In this paper, we propose CylinderPlane, a novel implicit\nrepresentation based on Cylindrical Coordinate System, to eliminate the feature\nambiguity issue and ensure multi-view consistency in 360$^\\circ$. Different\nfrom the inevitable feature entanglement in Cartesian coordinate-based\nTri-plane representation, the cylindrical coordinate system explicitly\nseparates features at different angles, allowing our cylindrical representation\npossible to achieve high-quality, artifacts-free 360$^\\circ$ image synthesis.\nWe further introduce the nested cylinder representation that composites\nmultiple cylinders at different scales, thereby enabling the model more\nadaptable to complex geometry and varying resolutions. The combination of\ncylinders with different resolutions can effectively capture more critical\nlocations and multi-scale features, greatly facilitates fine detail learning\nand robustness to different resolutions. Moreover, our representation is\nagnostic to implicit rendering methods and can be easily integrated into any\nneural rendering pipeline. Extensive experiments on both synthetic dataset and\nunstructured in-the-wild images demonstrate that our proposed representation\nachieves superior performance over previous methods.", "AI": {"tldr": "The paper introduces CylinderPlane, a cylindrical coordinate-based implicit representation to address feature ambiguity and multi-view consistency issues in 360\u00b0 image synthesis, outperforming Tri-plane methods.", "motivation": "The Tri-plane representation causes multi-face artifacts due to shared features in symmetric regions, limiting 360\u00b0 view generation.", "method": "Proposes CylinderPlane, a cylindrical coordinate system that separates features by angle, and introduces nested cylinders for multi-scale geometry handling.", "result": "Achieves high-quality, artifact-free 360\u00b0 image synthesis and adapts to complex geometry and varying resolutions.", "conclusion": "CylinderPlane outperforms previous methods, is versatile for neural rendering, and demonstrates robustness in experiments."}}
{"id": "2507.15628", "pdf": "https://arxiv.org/pdf/2507.15628", "abs": "https://arxiv.org/abs/2507.15628", "authors": ["Shanjiang Tang", "Rui Huang", "Hsinyu Luo", "Chunjiang Wang", "Ce Yu", "Yusen Li", "Hao Fu", "Chao Sun", "and Jian Xiao"], "title": "A Survey on Efficiency Optimization Techniques for DNN-based Video Analytics: Process Systems, Algorithms, and Applications", "categories": ["cs.CV"], "comment": null, "summary": "The explosive growth of video data in recent years has brought higher demands\nfor video analytics, where accuracy and efficiency remain the two primary\nconcerns. Deep neural networks (DNNs) have been widely adopted to ensure\naccuracy; however, improving their efficiency in video analytics remains an\nopen challenge. Different from existing surveys that make summaries of\nDNN-based video mainly from the accuracy optimization aspect, in this survey,\nwe aim to provide a thorough review of optimization techniques focusing on the\nimprovement of the efficiency of DNNs in video analytics. We organize existing\nmethods in a bottom-up manner, covering multiple perspectives such as hardware\nsupport, data processing, operational deployment, etc. Finally, based on the\noptimization framework and existing works, we analyze and discuss the problems\nand challenges in the performance optimization of DNN-based video analytics.", "AI": {"tldr": "A survey reviewing efficiency optimization techniques for DNNs in video analytics, covering hardware, data processing, and deployment, and discussing challenges.", "motivation": "Address the gap in existing surveys by focusing on efficiency improvements for DNNs in video analytics, beyond just accuracy.", "method": "Organizes methods bottom-up, examining hardware support, data processing, and operational deployment.", "result": "Provides a comprehensive review of efficiency optimization techniques and identifies key challenges.", "conclusion": "Highlights the need for further research to address efficiency challenges in DNN-based video analytics."}}
{"id": "2507.15633", "pdf": "https://arxiv.org/pdf/2507.15633", "abs": "https://arxiv.org/abs/2507.15633", "authors": ["Sachin Sharma", "Federico Simonetta", "Michele Flammini"], "title": "Experimenting active and sequential learning in a medieval music manuscript", "categories": ["cs.CV", "I.2.10; I.4.8; H.3.3"], "comment": "6 pages, 4 figures, accepted at IEEE MLSP 2025 (IEEE International\n  Workshop on Machine Learning for Signal Processing). Special Session:\n  Applications of AI in Cultural and Artistic Heritage", "summary": "Optical Music Recognition (OMR) is a cornerstone of music digitization\ninitiatives in cultural heritage, yet it remains limited by the scarcity of\nannotated data and the complexity of historical manuscripts. In this paper, we\npresent a preliminary study of Active Learning (AL) and Sequential Learning\n(SL) tailored for object detection and layout recognition in an old medieval\nmusic manuscript. Leveraging YOLOv8, our system selects samples with the\nhighest uncertainty (lowest prediction confidence) for iterative labeling and\nretraining. Our approach starts with a single annotated image and successfully\nboosts performance while minimizing manual labeling. Experimental results\nindicate that comparable accuracy to fully supervised training can be achieved\nwith significantly fewer labeled examples. We test the methodology as a\npreliminary investigation on a novel dataset offered to the community by the\nAnonymous project, which studies laude, a poetical-musical genre spread across\nItaly during the 12th-16th Century. We show that in the manuscript at-hand,\nuncertainty-based AL is not effective and advocates for more usable methods in\ndata-scarcity scenarios.", "AI": {"tldr": "The paper explores Active Learning (AL) and Sequential Learning (SL) for OMR in medieval manuscripts using YOLOv8, achieving near-fully supervised accuracy with fewer labels, but finds uncertainty-based AL ineffective in the tested dataset.", "motivation": "Addressing the scarcity of annotated data and complexity of historical manuscripts in Optical Music Recognition (OMR) for cultural heritage digitization.", "method": "Uses YOLOv8 for object detection and layout recognition, selecting uncertain samples for iterative labeling and retraining, starting with one annotated image.", "result": "Achieves comparable accuracy to fully supervised training with fewer labels, but uncertainty-based AL proves ineffective in the tested manuscript.", "conclusion": "Highlights the need for more usable methods in data-scarcity scenarios, despite the success of the overall approach."}}
{"id": "2507.15636", "pdf": "https://arxiv.org/pdf/2507.15636", "abs": "https://arxiv.org/abs/2507.15636", "authors": ["Lisan Al Amin", "Md. Ismail Hossain", "Thanh Thi Nguyen", "Tasnim Jahan", "Mahbubul Islam", "Faisal Quader"], "title": "Uncovering Critical Features for Deepfake Detection through the Lottery Ticket Hypothesis", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted for publication at the 2025 IEEE International Conference on\n  Systems, Man, and Cybernetics (SMC)", "summary": "Recent advances in deepfake technology have created increasingly convincing\nsynthetic media that poses significant challenges to information integrity and\nsocial trust. While current detection methods show promise, their underlying\nmechanisms remain poorly understood, and the large sizes of their models make\nthem challenging to deploy in resource-limited environments. This study\ninvestigates the application of the Lottery Ticket Hypothesis (LTH) to deepfake\ndetection, aiming to identify the key features crucial for recognizing\ndeepfakes. We examine how neural networks can be efficiently pruned while\nmaintaining high detection accuracy. Through extensive experiments with\nMesoNet, CNN-5, and ResNet-18 architectures on the OpenForensic and\nFaceForensics++ datasets, we find that deepfake detection networks contain\nwinning tickets, i.e., subnetworks, that preserve performance even at\nsubstantial sparsity levels. Our results indicate that MesoNet retains 56.2%\naccuracy at 80% sparsity on the OpenForensic dataset, with only 3,000\nparameters, which is about 90% of its baseline accuracy (62.6%). The results\nalso show that our proposed LTH-based iterative magnitude pruning approach\nconsistently outperforms one-shot pruning methods. Using Grad-CAM\nvisualization, we analyze how pruned networks maintain their focus on critical\nfacial regions for deepfake detection. Additionally, we demonstrate the\ntransferability of winning tickets across datasets, suggesting potential for\nefficient, deployable deepfake detection systems.", "AI": {"tldr": "The study applies the Lottery Ticket Hypothesis (LTH) to deepfake detection, identifying efficient subnetworks (winning tickets) that maintain accuracy even at high sparsity levels.", "motivation": "Deepfake technology threatens information integrity, and current detection methods are resource-intensive and poorly understood.", "method": "The study uses LTH to prune neural networks (MesoNet, CNN-5, ResNet-18) on OpenForensic and FaceForensics++ datasets, comparing iterative magnitude pruning to one-shot methods.", "result": "Pruned networks retain high accuracy (e.g., MesoNet: 56.2% at 80% sparsity) with fewer parameters, and iterative pruning outperforms one-shot methods. Winning tickets transfer across datasets.", "conclusion": "LTH enables efficient, deployable deepfake detection systems by identifying critical subnetworks, with potential for broader application."}}
{"id": "2507.15652", "pdf": "https://arxiv.org/pdf/2507.15652", "abs": "https://arxiv.org/abs/2507.15652", "authors": ["Haoran Zhou", "Zihan Zhang", "Hao Chen"], "title": "Extracting Visual Facts from Intermediate Layers for Mitigating Hallucinations in Multimodal Large Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have made significant strides by\ncombining visual recognition and language understanding to generate content\nthat is both coherent and contextually accurate. However, MLLMs continue to\nstruggle with object hallucinations, where models produce seemingly plausible\nbut factually incorrect outputs, including objects that do not exist in the\nimage. Recent work has revealed that the prior knowledge in MLLMs significantly\nsuppresses visual information in deep layers, causing hallucinatory outputs.\nHowever, how these priors suppress visual information at the intermediate layer\nstage in MLLMs remains unclear. We observe that visual factual knowledge and\nthe differences between intermediate-layer prior/original probability\ndistributions show similar evolutionary trends in intermediate layers.\nMotivated by this, we introduce Decoding by Extracting Visual Facts (EVA), a\nsimple, training-free method that dynamically selects intermediate layers with\nthe most significant visual factual information. By contrasting the output\ndistributions of the selected layer derived from the original input and\npure-text input, EVA extracts visual factual knowledge and proportionally\nincorporates it into the final layer to correct the output logits. Importantly,\nEVA is model-agnostic, seamlessly integrates with various classic decoding\nstrategies, and is applicable across different MLLMs. We validate EVA on\nwidely-used benchmarks, and the results show that it significantly reduces\nhallucination rates compared to baseline methods, underscoring its\neffectiveness in mitigating hallucinations.", "AI": {"tldr": "EVA is a training-free method to reduce object hallucinations in MLLMs by dynamically selecting intermediate layers with strong visual factual information and correcting output logits.", "motivation": "MLLMs struggle with object hallucinations due to suppressed visual information by prior knowledge in deep layers. The unclear suppression mechanism in intermediate layers motivates the study.", "method": "EVA selects intermediate layers with significant visual factual information, contrasts their output distributions, and incorporates this knowledge into the final layer to correct logits.", "result": "EVA significantly reduces hallucination rates on benchmarks, outperforming baseline methods.", "conclusion": "EVA is an effective, model-agnostic solution to mitigate hallucinations in MLLMs, compatible with various decoding strategies."}}
{"id": "2507.15655", "pdf": "https://arxiv.org/pdf/2507.15655", "abs": "https://arxiv.org/abs/2507.15655", "authors": ["Aniket Pal", "Ajoy Mondal", "Minesh Mathew", "C. V. Jawahar"], "title": "HW-MLVQA: Elucidating Multilingual Handwritten Document Understanding with a Comprehensive VQA Benchmark", "categories": ["cs.CV"], "comment": "This is a minor revision of the original paper submitted to IJDAR", "summary": "The proliferation of MultiLingual Visual Question Answering (MLVQA)\nbenchmarks augments the capabilities of large language models (LLMs) and\nmulti-modal LLMs, thereby enabling them to adeptly capture the intricate\nlinguistic subtleties and visual complexities inherent across diverse\nlanguages. Despite its potential, the current MLVQA model struggles to fully\nutilize its capabilities when dealing with the extensive variety of handwritten\ndocuments. This article delineates HW-MLVQA, an avant-garde VQA benchmark\nmeticulously crafted to mitigate the dearth of authentic Multilingual\nHandwritten document comprehension. HW-MLVQA encompasses an extensive\ncollection of 1,600 handwritten Pages complemented by 2,400 question-answers.\nFurthermore, it provides a robust benchmark evaluation framework spanning three\ndistinct modalities: text, image, and an integrated image & text modality. To\nsimulate authentic real-world contexts devoid of ground truth textual\ntranscriptions, we facilitates a rigorous assessment of proprietary and\nopen-source OCR models. The benchmark aspires to facilitate pivotal\nadvancements in multilingual handwritten document interpretation, fostering\ninnovation and scholarly inquiry within this specialized domain.", "AI": {"tldr": "HW-MLVQA is a new benchmark for multilingual handwritten document VQA, addressing gaps in current models with 1,600 pages and 2,400 Q&A pairs, evaluated across text, image, and combined modalities.", "motivation": "Current MLVQA models underperform with handwritten documents, lacking benchmarks for multilingual handwritten comprehension.", "method": "Introduces HW-MLVQA, a benchmark with 1,600 handwritten pages and 2,400 Q&A pairs, evaluated across text, image, and combined modalities, including OCR model assessment.", "result": "Provides a robust framework for evaluating multilingual handwritten document interpretation, simulating real-world scenarios without ground truth transcriptions.", "conclusion": "HW-MLVQA aims to drive advancements in multilingual handwritten document understanding, encouraging innovation in this niche field."}}
{"id": "2507.15680", "pdf": "https://arxiv.org/pdf/2507.15680", "abs": "https://arxiv.org/abs/2507.15680", "authors": ["Yongkang Hou", "Jiarun Song"], "title": "Visual-Language Model Knowledge Distillation Method for Image Quality Assessment", "categories": ["cs.CV"], "comment": null, "summary": "Image Quality Assessment (IQA) is a core task in computer vision. Multimodal\nmethods based on vision-language models, such as CLIP, have demonstrated\nexceptional generalization capabilities in IQA tasks. To address the issues of\nexcessive parameter burden and insufficient ability to identify local distorted\nfeatures in CLIP for IQA, this study proposes a visual-language model knowledge\ndistillation method aimed at guiding the training of models with architectural\nadvantages using CLIP's IQA knowledge. First, quality-graded prompt templates\nwere designed to guide CLIP to output quality scores. Then, CLIP is fine-tuned\nto enhance its capabilities in IQA tasks. Finally, a modality-adaptive\nknowledge distillation strategy is proposed to achieve guidance from the CLIP\nteacher model to the student model. Our experiments were conducted on multiple\nIQA datasets, and the results show that the proposed method significantly\nreduces model complexity while outperforming existing IQA methods,\ndemonstrating strong potential for practical deployment.", "AI": {"tldr": "A knowledge distillation method using CLIP improves IQA by reducing model complexity and enhancing local distortion identification.", "motivation": "Address CLIP's limitations in IQA, such as high parameter burden and poor local distortion detection.", "method": "Design quality-graded prompts, fine-tune CLIP, and use modality-adaptive distillation to transfer knowledge to a student model.", "result": "Outperforms existing IQA methods with reduced complexity, validated on multiple datasets.", "conclusion": "The method shows strong practical potential for efficient and effective IQA."}}
{"id": "2507.15683", "pdf": "https://arxiv.org/pdf/2507.15683", "abs": "https://arxiv.org/abs/2507.15683", "authors": ["Boni Hu", "Zhenyu Xia", "Lin Chen", "Pengcheng Han", "Shuhui Bu"], "title": "Hi^2-GSLoc: Dual-Hierarchical Gaussian-Specific Visual Relocalization for Remote Sensing", "categories": ["cs.CV"], "comment": "17 pages, 11 figures", "summary": "Visual relocalization, which estimates the 6-degree-of-freedom (6-DoF) camera\npose from query images, is fundamental to remote sensing and UAV applications.\nExisting methods face inherent trade-offs: image-based retrieval and pose\nregression approaches lack precision, while structure-based methods that\nregister queries to Structure-from-Motion (SfM) models suffer from\ncomputational complexity and limited scalability. These challenges are\nparticularly pronounced in remote sensing scenarios due to large-scale scenes,\nhigh altitude variations, and domain gaps of existing visual priors. To\novercome these limitations, we leverage 3D Gaussian Splatting (3DGS) as a novel\nscene representation that compactly encodes both 3D geometry and appearance. We\nintroduce $\\mathrm{Hi}^2$-GSLoc, a dual-hierarchical relocalization framework\nthat follows a sparse-to-dense and coarse-to-fine paradigm, fully exploiting\nthe rich semantic information and geometric constraints inherent in Gaussian\nprimitives. To handle large-scale remote sensing scenarios, we incorporate\npartitioned Gaussian training, GPU-accelerated parallel matching, and dynamic\nmemory management strategies. Our approach consists of two stages: (1) a sparse\nstage featuring a Gaussian-specific consistent render-aware sampling strategy\nand landmark-guided detector for robust and accurate initial pose estimation,\nand (2) a dense stage that iteratively refines poses through coarse-to-fine\ndense rasterization matching while incorporating reliability verification.\nThrough comprehensive evaluation on simulation data, public datasets, and real\nflight experiments, we demonstrate that our method delivers competitive\nlocalization accuracy, recall rate, and computational efficiency while\neffectively filtering unreliable pose estimates. The results confirm the\neffectiveness of our approach for practical remote sensing applications.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.15686", "pdf": "https://arxiv.org/pdf/2507.15686", "abs": "https://arxiv.org/abs/2507.15686", "authors": ["Wenjie Huang", "Qi Yang", "Shuting Xia", "He Huang", "Zhu Li", "Yiling Xu"], "title": "LINR-PCGC: Lossless Implicit Neural Representations for Point Cloud Geometry Compression", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to ICCV 2025", "summary": "Existing AI-based point cloud compression methods struggle with dependence on\nspecific training data distributions, which limits their real-world deployment.\nImplicit Neural Representation (INR) methods solve the above problem by\nencoding overfitted network parameters to the bitstream, resulting in more\ndistribution-agnostic results. However, due to the limitation of encoding time\nand decoder size, current INR based methods only consider lossy geometry\ncompression. In this paper, we propose the first INR based lossless point cloud\ngeometry compression method called Lossless Implicit Neural Representations for\nPoint Cloud Geometry Compression (LINR-PCGC). To accelerate encoding speed, we\ndesign a group of point clouds level coding framework with an effective network\ninitialization strategy, which can reduce around 60% encoding time. A\nlightweight coding network based on multiscale SparseConv, consisting of scale\ncontext extraction, child node prediction, and model compression modules, is\nproposed to realize fast inference and compact decoder size. Experimental\nresults show that our method consistently outperforms traditional and AI-based\nmethods: for example, with the convergence time in the MVUB dataset, our method\nreduces the bitstream by approximately 21.21% compared to G-PCC TMC13v23 and\n21.95% compared to SparsePCGC. Our project can be seen on\nhttps://huangwenjie2023.github.io/LINR-PCGC/.", "AI": {"tldr": "Proposes LINR-PCGC, the first INR-based lossless point cloud geometry compression method, improving speed and efficiency over existing methods.", "motivation": "Addresses limitations of AI-based and INR methods in point cloud compression, particularly dependency on training data and lossy-only results.", "method": "Uses a group-level coding framework with network initialization for speed, and a lightweight multiscale SparseConv network for fast inference and compact size.", "result": "Reduces bitstream by ~21.21% vs. G-PCC TMC13v23 and ~21.95% vs. SparsePCGC, with 60% faster encoding.", "conclusion": "LINR-PCGC offers a superior, distribution-agnostic solution for lossless point cloud compression."}}
{"id": "2507.15690", "pdf": "https://arxiv.org/pdf/2507.15690", "abs": "https://arxiv.org/abs/2507.15690", "authors": ["Hung Nguyen", "Runfa Li", "An Le", "Truong Nguyen"], "title": "DWTGS: Rethinking Frequency Regularization for Sparse-view 3D Gaussian Splatting", "categories": ["cs.CV", "eess.IV", "eess.SP"], "comment": "6 pages, 4 figures", "summary": "Sparse-view 3D Gaussian Splatting (3DGS) presents significant challenges in\nreconstructing high-quality novel views, as it often overfits to the\nwidely-varying high-frequency (HF) details of the sparse training views. While\nfrequency regularization can be a promising approach, its typical reliance on\nFourier transforms causes difficult parameter tuning and biases towards\ndetrimental HF learning. We propose DWTGS, a framework that rethinks frequency\nregularization by leveraging wavelet-space losses that provide additional\nspatial supervision. Specifically, we supervise only the low-frequency (LF) LL\nsubbands at multiple DWT levels, while enforcing sparsity on the HF HH subband\nin a self-supervised manner. Experiments across benchmarks show that DWTGS\nconsistently outperforms Fourier-based counterparts, as this LF-centric\nstrategy improves generalization and reduces HF hallucinations.", "AI": {"tldr": "DWTGS improves sparse-view 3D Gaussian Splatting by using wavelet-space losses for better generalization and reduced high-frequency artifacts.", "motivation": "Overfitting to high-frequency details in sparse training views hinders quality reconstruction in 3DGS. Frequency regularization via Fourier transforms is hard to tune and biases learning.", "method": "DWTGS uses wavelet-space losses, supervising low-frequency subbands and enforcing sparsity on high-frequency subbands self-supervised.", "result": "DWTGS outperforms Fourier-based methods, improving generalization and reducing high-frequency hallucinations.", "conclusion": "Wavelet-space losses offer a better alternative to Fourier transforms for frequency regularization in 3DGS."}}
{"id": "2507.15709", "pdf": "https://arxiv.org/pdf/2507.15709", "abs": "https://arxiv.org/abs/2507.15709", "authors": ["Wei Sun", "Weixia Zhang", "Linhan Cao", "Jun Jia", "Xiangyang Zhu", "Dandan Zhu", "Xiongkuo Min", "Guangtao Zhai"], "title": "Efficient Face Image Quality Assessment via Self-training and Knowledge Distillation", "categories": ["cs.CV"], "comment": "Efficient-FIQA achieved first place in the ICCV VQualA 2025 Face\n  Image Quality Assessment Challenge", "summary": "Face image quality assessment (FIQA) is essential for various face-related\napplications. Although FIQA has been extensively studied and achieved\nsignificant progress, the computational complexity of FIQA algorithms remains a\nkey concern for ensuring scalability and practical deployment in real-world\nsystems. In this paper, we aim to develop a computationally efficient FIQA\nmethod that can be easily deployed in real-world applications. Specifically,\nour method consists of two stages: training a powerful teacher model and\ndistilling a lightweight student model from it. To build a strong teacher\nmodel, we adopt a self-training strategy to improve its capacity. We first\ntrain the teacher model using labeled face images, then use it to generate\npseudo-labels for a set of unlabeled images. These pseudo-labeled samples are\nused in two ways: (1) to distill knowledge into the student model, and (2) to\ncombine with the original labeled images to further enhance the teacher model\nthrough self-training. The enhanced teacher model is used to further\npseudo-label another set of unlabeled images for distilling the student models.\nThe student model is trained using a combination of labeled images,\npseudo-labeled images from the original teacher model, and pseudo-labeled\nimages from the enhanced teacher model. Experimental results demonstrate that\nour student model achieves comparable performance to the teacher model with an\nextremely low computational overhead. Moreover, our method achieved first place\nin the ICCV 2025 VQualA FIQA Challenge. The code is available at\nhttps://github.com/sunwei925/Efficient-FIQA.git.", "AI": {"tldr": "A computationally efficient FIQA method using teacher-student distillation and self-training achieves high performance with low overhead.", "motivation": "To address the computational complexity of FIQA algorithms for real-world scalability and deployment.", "method": "Two-stage approach: train a teacher model using labeled data and self-training, then distill a lightweight student model using pseudo-labels.", "result": "Student model matches teacher performance with minimal computational cost; won ICCV 2025 VQualA FIQA Challenge.", "conclusion": "The method successfully balances efficiency and performance, making it practical for real-world applications."}}
{"id": "2507.15724", "pdf": "https://arxiv.org/pdf/2507.15724", "abs": "https://arxiv.org/abs/2507.15724", "authors": ["Guoxuan Xia", "Harleen Hanspal", "Petru-Daniel Tudosiu", "Shifeng Zhang", "Sarah Parisot"], "title": "A Practical Investigation of Spatially-Controlled Image Generation with Transformers", "categories": ["cs.CV"], "comment": "preprint", "summary": "Enabling image generation models to be spatially controlled is an important\narea of research, empowering users to better generate images according to their\nown fine-grained specifications via e.g. edge maps, poses. Although this task\nhas seen impressive improvements in recent times, a focus on rapidly producing\nstronger models has come at the cost of detailed and fair scientific\ncomparison. Differing training data, model architectures and generation\nparadigms make it difficult to disentangle the factors contributing to\nperformance. Meanwhile, the motivations and nuances of certain approaches\nbecome lost in the literature. In this work, we aim to provide clear takeaways\nacross generation paradigms for practitioners wishing to develop\ntransformer-based systems for spatially-controlled generation, clarifying the\nliterature and addressing knowledge gaps. We perform controlled experiments on\nImageNet across diffusion-based/flow-based and autoregressive (AR) models.\nFirst, we establish control token prefilling as a simple, general and\nperformant baseline approach for transformers. We then investigate previously\nunderexplored sampling time enhancements, showing that extending\nclassifier-free guidance to control, as well as softmax truncation, have a\nstrong impact on control-generation consistency. Finally, we re-clarify the\nmotivation of adapter-based approaches, demonstrating that they mitigate\n\"forgetting\" and maintain generation quality when trained on limited downstream\ndata, but underperform full training in terms of generation-control\nconsistency. Code will be released upon publication.", "AI": {"tldr": "The paper evaluates transformer-based models for spatially-controlled image generation, comparing paradigms like diffusion, flow-based, and autoregressive models, and introduces control token prefilling and sampling enhancements.", "motivation": "To address the lack of detailed and fair comparisons in spatially-controlled image generation research and clarify the literature for practitioners.", "method": "Controlled experiments on ImageNet using diffusion-based, flow-based, and autoregressive models, focusing on control token prefilling, classifier-free guidance, and softmax truncation.", "result": "Control token prefilling is a strong baseline; sampling enhancements like classifier-free guidance improve control-generation consistency; adapter-based approaches mitigate forgetting but underperform in consistency.", "conclusion": "The study provides clear takeaways for transformer-based spatially-controlled generation, highlighting effective methods and addressing knowledge gaps."}}
{"id": "2507.15728", "pdf": "https://arxiv.org/pdf/2507.15728", "abs": "https://arxiv.org/abs/2507.15728", "authors": ["Wenqi Ouyang", "Zeqi Xiao", "Danni Yang", "Yifan Zhou", "Shuai Yang", "Lei Yang", "Jianlou Si", "Xingang Pan"], "title": "TokensGen: Harnessing Condensed Tokens for Long Video Generation", "categories": ["cs.CV"], "comment": "Project page: https://vicky0522.github.io/tokensgen-webpage/", "summary": "Generating consistent long videos is a complex challenge: while\ndiffusion-based generative models generate visually impressive short clips,\nextending them to longer durations often leads to memory bottlenecks and\nlong-term inconsistency. In this paper, we propose TokensGen, a novel two-stage\nframework that leverages condensed tokens to address these issues. Our method\ndecomposes long video generation into three core tasks: (1) inner-clip semantic\ncontrol, (2) long-term consistency control, and (3) inter-clip smooth\ntransition. First, we train To2V (Token-to-Video), a short video diffusion\nmodel guided by text and video tokens, with a Video Tokenizer that condenses\nshort clips into semantically rich tokens. Second, we introduce T2To\n(Text-to-Token), a video token diffusion transformer that generates all tokens\nat once, ensuring global consistency across clips. Finally, during inference,\nan adaptive FIFO-Diffusion strategy seamlessly connects adjacent clips,\nreducing boundary artifacts and enhancing smooth transitions. Experimental\nresults demonstrate that our approach significantly enhances long-term temporal\nand content coherence without incurring prohibitive computational overhead. By\nleveraging condensed tokens and pre-trained short video models, our method\nprovides a scalable, modular solution for long video generation, opening new\npossibilities for storytelling, cinematic production, and immersive\nsimulations. Please see our project page at\nhttps://vicky0522.github.io/tokensgen-webpage/ .", "AI": {"tldr": "TokensGen is a two-stage framework using condensed tokens for long video generation, addressing memory and consistency issues with To2V and T2To models, and adaptive FIFO-Diffusion for smooth transitions.", "motivation": "Extending diffusion-based models for long videos faces memory bottlenecks and long-term inconsistency. TokensGen aims to solve these challenges.", "method": "Decomposes long video generation into three tasks: semantic control, consistency control, and smooth transitions. Uses To2V for short clips and T2To for global token generation.", "result": "Enhances long-term coherence without excessive computational cost, scalable for storytelling and simulations.", "conclusion": "TokensGen offers a modular, efficient solution for long video generation, enabling new applications in media and simulations."}}
{"id": "2507.15748", "pdf": "https://arxiv.org/pdf/2507.15748", "abs": "https://arxiv.org/abs/2507.15748", "authors": ["Jisu Shin", "Richard Shaw", "Seunghyun Shin", "Anton Pelykh", "Zhensong Zhang", "Hae-Gon Jeon", "Eduardo Perez-Pellitero"], "title": "Appearance Harmonization via Bilateral Grid Prediction with Transformers for 3DGS", "categories": ["cs.CV"], "comment": "10 pages, 3 figures, NeurIPS 2025 under review", "summary": "Modern camera pipelines apply extensive on-device processing, such as\nexposure adjustment, white balance, and color correction, which, while\nbeneficial individually, often introduce photometric inconsistencies across\nviews. These appearance variations violate multi-view consistency and degrade\nthe quality of novel view synthesis. Joint optimization of scene\nrepresentations and per-image appearance embeddings has been proposed to\naddress this issue, but at the cost of increased computational complexity and\nslower training. In this work, we propose a transformer-based method that\npredicts spatially adaptive bilateral grids to correct photometric variations\nin a multi-view consistent manner, enabling robust cross-scene generalization\nwithout the need for scene-specific retraining. By incorporating the learned\ngrids into the 3D Gaussian Splatting pipeline, we improve reconstruction\nquality while maintaining high training efficiency. Extensive experiments show\nthat our approach outperforms or matches existing scene-specific optimization\nmethods in reconstruction fidelity and convergence speed.", "AI": {"tldr": "A transformer-based method predicts bilateral grids to correct photometric inconsistencies in multi-view scenes, improving novel view synthesis without scene-specific retraining.", "motivation": "Photometric inconsistencies from camera pipelines degrade multi-view consistency and novel view synthesis quality. Existing methods increase computational complexity.", "method": "Uses a transformer to predict spatially adaptive bilateral grids for photometric correction, integrated into the 3D Gaussian Splatting pipeline.", "result": "Outperforms or matches scene-specific methods in reconstruction fidelity and convergence speed.", "conclusion": "The proposed method enables robust cross-scene generalization and maintains high training efficiency."}}
{"id": "2507.15765", "pdf": "https://arxiv.org/pdf/2507.15765", "abs": "https://arxiv.org/abs/2507.15765", "authors": ["Feng-Qi Cui", "Anyang Tong", "Jinyang Huang", "Jie Zhang", "Dan Guo", "Zhi Liu", "Meng Wang"], "title": "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization", "categories": ["cs.CV"], "comment": "Accepted by ACM MM'25", "summary": "Dynamic Facial Expression Recognition (DFER) plays a critical role in\naffective computing and human-computer interaction. Although existing methods\nachieve comparable performance, they inevitably suffer from performance\ndegradation under sample heterogeneity caused by multi-source data and\nindividual expression variability. To address these challenges, we propose a\nnovel framework, called Heterogeneity-aware Distributional Framework (HDF), and\ndesign two plug-and-play modules to enhance time-frequency modeling and\nmitigate optimization imbalance caused by hard samples. Specifically, the\nTime-Frequency Distributional Attention Module (DAM) captures both temporal\nconsistency and frequency robustness through a dual-branch attention design,\nimproving tolerance to sequence inconsistency and visual style shifts. Then,\nbased on gradient sensitivity and information bottleneck principles, an\nadaptive optimization module Distribution-aware Scaling Module (DSM) is\nintroduced to dynamically balance classification and contrastive losses,\nenabling more stable and discriminative representation learning. Extensive\nexperiments on two widely used datasets, DFEW and FERV39k, demonstrate that HDF\nsignificantly improves both recognition accuracy and robustness. Our method\nachieves superior weighted average recall (WAR) and unweighted average recall\n(UAR) while maintaining strong generalization across diverse and imbalanced\nscenarios. Codes are released at https://github.com/QIcita/HDF_DFER.", "AI": {"tldr": "The paper introduces HDF, a framework for dynamic facial expression recognition, addressing sample heterogeneity with two modules: DAM for time-frequency modeling and DSM for optimization balance, achieving high accuracy and robustness.", "motivation": "Existing methods for DFER degrade under sample heterogeneity from multi-source data and individual variability.", "method": "Proposes HDF with DAM (dual-branch attention for time-frequency) and DSM (adaptive optimization for loss balance).", "result": "HDF improves recognition accuracy and robustness on DFEW and FERV39k datasets, achieving superior WAR and UAR.", "conclusion": "HDF effectively handles heterogeneity and imbalance, offering stable and discriminative learning for DFER."}}
{"id": "2507.15777", "pdf": "https://arxiv.org/pdf/2507.15777", "abs": "https://arxiv.org/abs/2507.15777", "authors": ["Junwen Wang", "Oscar MacCormac", "William Rochford", "Aaron Kujawa", "Jonathan Shapey", "Tom Vercauteren"], "title": "Label tree semantic losses for rich multi-class medical image segmentation", "categories": ["cs.CV"], "comment": "arXiv admin note: text overlap with arXiv:2506.21150", "summary": "Rich and accurate medical image segmentation is poised to underpin the next\ngeneration of AI-defined clinical practice by delineating critical anatomy for\npre-operative planning, guiding real-time intra-operative navigation, and\nsupporting precise post-operative assessment. However, commonly used learning\nmethods for medical and surgical imaging segmentation tasks penalise all errors\nequivalently and thus fail to exploit any inter-class semantics in the labels\nspace. This becomes particularly problematic as the cardinality and richness of\nlabels increases to include subtly different classes. In this work, we propose\ntwo tree-based semantic loss functions which take advantage of a hierarchical\norganisation of the labels. We further incorporate our losses in a recently\nproposed approach for training with sparse, background-free annotations to\nextend the applicability of our proposed losses. Extensive experiments are\nreported on two medical and surgical image segmentation tasks, namely head MRI\nfor whole brain parcellation (WBP) with full supervision and neurosurgical\nhyperspectral imaging (HSI) for scene understanding with sparse annotations.\nResults demonstrate that our proposed method reaches state-of-the-art\nperformance in both cases.", "AI": {"tldr": "The paper introduces tree-based semantic loss functions for medical image segmentation, improving accuracy by leveraging hierarchical label organization and sparse annotations.", "motivation": "Current methods penalize all errors equally, ignoring inter-class semantics, which is problematic for rich and subtle label spaces.", "method": "Proposes two tree-based semantic loss functions and integrates them with sparse, background-free annotation training.", "result": "Achieves state-of-the-art performance in head MRI for whole brain parcellation and neurosurgical hyperspectral imaging tasks.", "conclusion": "The proposed method effectively exploits hierarchical label semantics, enhancing segmentation accuracy in medical imaging."}}
{"id": "2507.15793", "pdf": "https://arxiv.org/pdf/2507.15793", "abs": "https://arxiv.org/abs/2507.15793", "authors": ["Ghassen Baklouti", "Julio Silva-Rodr\u00edguez", "Jose Dolz", "Houda Bahig", "Ismail Ben Ayed"], "title": "Regularized Low-Rank Adaptation for Few-Shot Organ Segmentation", "categories": ["cs.CV"], "comment": "Accepted at MICCAI 2025", "summary": "Parameter-efficient fine-tuning (PEFT) of pre-trained foundation models is\nincreasingly attracting interest in medical imaging due to its effectiveness\nand computational efficiency. Among these methods, Low-Rank Adaptation (LoRA)\nis a notable approach based on the assumption that the adaptation inherently\noccurs in a low-dimensional subspace. While it has shown good performance, its\nimplementation requires a fixed and unalterable rank, which might be\nchallenging to select given the unique complexities and requirements of each\nmedical imaging downstream task. Inspired by advancements in natural image\nprocessing, we introduce a novel approach for medical image segmentation that\ndynamically adjusts the intrinsic rank during adaptation. Viewing the low-rank\nrepresentation of the trainable weight matrices as a singular value\ndecomposition, we introduce an l_1 sparsity regularizer to the loss function,\nand tackle it with a proximal optimizer. The regularizer could be viewed as a\npenalty on the decomposition rank. Hence, its minimization enables to find\ntask-adapted ranks automatically. Our method is evaluated in a realistic\nfew-shot fine-tuning setting, where we compare it first to the standard LoRA\nand then to several other PEFT methods across two distinguishable tasks: base\norgans and novel organs. Our extensive experiments demonstrate the significant\nperformance improvements driven by our method, highlighting its efficiency and\nrobustness against suboptimal rank initialization. Our code is publicly\navailable: https://github.com/ghassenbaklouti/ARENA", "AI": {"tldr": "A novel PEFT method for medical image segmentation dynamically adjusts rank during adaptation, outperforming LoRA and other methods.", "motivation": "Address the challenge of selecting a fixed rank in LoRA for medical imaging tasks by introducing dynamic rank adjustment.", "method": "Uses an l_1 sparsity regularizer on SVD-based low-rank representations, optimized proximally to automatically find task-adapted ranks.", "result": "Significant performance improvements in few-shot fine-tuning, especially for base and novel organ segmentation tasks.", "conclusion": "The method is efficient, robust against suboptimal rank initialization, and outperforms existing PEFT approaches."}}
{"id": "2507.15798", "pdf": "https://arxiv.org/pdf/2507.15798", "abs": "https://arxiv.org/abs/2507.15798", "authors": ["Lilian Hollard", "Lucas Mohimont", "Nathalie Gaveau", "Luiz-Angelo Steffenel"], "title": "Exploring Superposition and Interference in State-of-the-Art Low-Parameter Vision Models", "categories": ["cs.CV"], "comment": null, "summary": "The paper investigates the performance of state-of-the-art low-parameter deep\nneural networks for computer vision, focusing on bottleneck architectures and\ntheir behavior using superlinear activation functions. We address interference\nin feature maps, a phenomenon associated with superposition, where neurons\nsimultaneously encode multiple characteristics. Our research suggests that\nlimiting interference can enhance scaling and accuracy in very low-scaled\nnetworks (under 1.5M parameters). We identify key design elements that reduce\ninterference by examining various bottleneck architectures, leading to a more\nefficient neural network. Consequently, we propose a proof-of-concept\narchitecture named NoDepth Bottleneck built on mechanistic insights from our\nexperiments, demonstrating robust scaling accuracy on the ImageNet dataset.\nThese findings contribute to more efficient and scalable neural networks for\nthe low-parameter range and advance the understanding of bottlenecks in\ncomputer vision. https://caiac.pubpub.org/pub/3dh6rsel", "AI": {"tldr": "The paper explores low-parameter deep neural networks for computer vision, focusing on bottleneck architectures and superlinear activation functions. It addresses interference in feature maps, proposing design elements to reduce it, and introduces the NoDepth Bottleneck architecture, which improves scaling and accuracy in small networks.", "motivation": "To enhance the performance and scalability of low-parameter deep neural networks by addressing interference in feature maps, a challenge in bottleneck architectures.", "method": "Examines bottleneck architectures and superlinear activation functions, identifies design elements to reduce interference, and proposes the NoDepth Bottleneck architecture.", "result": "Demonstrates improved scaling and accuracy in networks under 1.5M parameters, validated on the ImageNet dataset.", "conclusion": "The findings contribute to more efficient and scalable neural networks for low-parameter ranges and advance understanding of bottlenecks in computer vision."}}
{"id": "2507.15803", "pdf": "https://arxiv.org/pdf/2507.15803", "abs": "https://arxiv.org/abs/2507.15803", "authors": ["Danhui Chen", "Ziquan Liu", "Chuxi Yang", "Dan Wang", "Yan Yan", "Yi Xu", "Xiangyang Ji"], "title": "ConformalSAM: Unlocking the Potential of Foundational Segmentation Models in Semi-Supervised Semantic Segmentation with Conformal Prediction", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "ICCV 2025", "summary": "Pixel-level vision tasks, such as semantic segmentation, require extensive\nand high-quality annotated data, which is costly to obtain. Semi-supervised\nsemantic segmentation (SSSS) has emerged as a solution to alleviate the\nlabeling burden by leveraging both labeled and unlabeled data through\nself-training techniques. Meanwhile, the advent of foundational segmentation\nmodels pre-trained on massive data, has shown the potential to generalize\nacross domains effectively. This work explores whether a foundational\nsegmentation model can address label scarcity in the pixel-level vision task as\nan annotator for unlabeled images. Specifically, we investigate the efficacy of\nusing SEEM, a Segment Anything Model (SAM) variant fine-tuned for textual\ninput, to generate predictive masks for unlabeled data. To address the\nshortcomings of using SEEM-generated masks as supervision, we propose\nConformalSAM, a novel SSSS framework which first calibrates the foundation\nmodel using the target domain's labeled data and then filters out unreliable\npixel labels of unlabeled data so that only high-confidence labels are used as\nsupervision. By leveraging conformal prediction (CP) to adapt foundation models\nto target data through uncertainty calibration, ConformalSAM exploits the\nstrong capability of the foundational segmentation model reliably which\nbenefits the early-stage learning, while a subsequent self-reliance training\nstrategy mitigates overfitting to SEEM-generated masks in the later training\nstage. Our experiment demonstrates that, on three standard benchmarks of SSSS,\nConformalSAM achieves superior performance compared to recent SSSS methods and\nhelps boost the performance of those methods as a plug-in.", "AI": {"tldr": "ConformalSAM leverages a foundational segmentation model (SEEM) to address label scarcity in semi-supervised semantic segmentation, using conformal prediction for uncertainty calibration and filtering unreliable labels.", "motivation": "Labeling pixel-level data is costly, and foundational models like SEEM offer potential for generalization. This work explores using SEEM as an annotator and improving its reliability.", "method": "Proposes ConformalSAM, which calibrates SEEM using labeled target data, filters unreliable labels via conformal prediction, and employs self-reliance training to avoid overfitting.", "result": "Outperforms recent SSSS methods on benchmarks and enhances other methods as a plug-in.", "conclusion": "ConformalSAM effectively leverages foundational models for SSSS, improving performance and reliability."}}
{"id": "2507.15807", "pdf": "https://arxiv.org/pdf/2507.15807", "abs": "https://arxiv.org/abs/2507.15807", "authors": ["Shuo Chen", "Jianzhe Liu", "Zhen Han", "Yan Xia", "Daniel Cremers", "Philip Torr", "Volker Tresp", "Jindong Gu"], "title": "True Multimodal In-Context Learning Needs Attention to the Visual Context", "categories": ["cs.CV", "cs.AI"], "comment": "accepted to COLM 2025", "summary": "Multimodal Large Language Models (MLLMs), built on powerful language\nbackbones, have enabled Multimodal In-Context Learning (MICL)-adapting to new\ntasks from a few multimodal demonstrations consisting of images, questions, and\nanswers. Despite showing noticeable improvement on standard vision-language\ndatasets, current MLLMs struggle to leverage visual information in the\ndemonstrations. Specifically, they tend to neglect visual cues and over-rely on\ntextual patterns, leading to mere text imitation rather than genuine multimodal\nadaptation. This behavior makes MICL still unimodal and largely restricts its\npractical utility. More importantly, this limitation is often concealed by the\nimproved performance on tasks that do not require understanding the visual\ncontext. As a result, how to effectively enhance MICL ability and reliably\nevaluate the MICL performance remains underexplored. To address these issues,\nwe first introduce Dynamic Attention Reallocation (DARA), an efficient\nfine-tuning strategy that encourages models to attend to the visual context by\nrebalancing attention across visual and textual tokens. In addition, we present\nTrueMICL, an MICL-dedicated dataset with both support and test sets that\nexplicitly requires the integration of multimodal information-particularly\nvisual content-for correct task completion. Extensive experiments demonstrate\nthe effectiveness of our holistic solution, showcasing substantial improvements\nin the true multimodal in-context learning capabilities. Code and datasets are\navailable at https://chenxshuo.github.io/true-micl-colm .", "AI": {"tldr": "The paper addresses the limitations of Multimodal Large Language Models (MLLMs) in effectively leveraging visual information during Multimodal In-Context Learning (MICL). It introduces Dynamic Attention Reallocation (DARA) and TrueMICL dataset to enhance and evaluate true multimodal adaptation.", "motivation": "Current MLLMs struggle to integrate visual cues in demonstrations, relying too heavily on textual patterns, which limits the practical utility of MICL. This issue is often masked by performance on tasks not requiring visual context.", "method": "The authors propose DARA, a fine-tuning strategy to rebalance attention between visual and textual tokens, and introduce TrueMICL, a dataset explicitly requiring multimodal integration for task completion.", "result": "Experiments show significant improvements in true multimodal in-context learning capabilities, validating the effectiveness of DARA and TrueMICL.", "conclusion": "The proposed solutions enhance MICL by addressing visual neglect in MLLMs, providing a reliable framework for evaluating and improving multimodal adaptation."}}
{"id": "2507.15809", "pdf": "https://arxiv.org/pdf/2507.15809", "abs": "https://arxiv.org/abs/2507.15809", "authors": ["Roberto Miele", "Niklas Linde"], "title": "Diffusion models for multivariate subsurface generation and efficient probabilistic inversion", "categories": ["cs.CV", "cs.LG", "physics.geo-ph", "stat.AP"], "comment": null, "summary": "Diffusion models offer stable training and state-of-the-art performance for\ndeep generative modeling tasks. Here, we consider their use in the context of\nmultivariate subsurface modeling and probabilistic inversion. We first\ndemonstrate that diffusion models enhance multivariate modeling capabilities\ncompared to variational autoencoders and generative adversarial networks. In\ndiffusion modeling, the generative process involves a comparatively large\nnumber of time steps with update rules that can be modified to account for\nconditioning data. We propose different corrections to the popular Diffusion\nPosterior Sampling approach by Chung et al. (2023). In particular, we introduce\na likelihood approximation accounting for the noise-contamination that is\ninherent in diffusion modeling. We assess performance in a multivariate\ngeological scenario involving facies and correlated acoustic impedance.\nConditional modeling is demonstrated using both local hard data (well logs) and\nnonlinear geophysics (fullstack seismic data). Our tests show significantly\nimproved statistical robustness, enhanced sampling of the posterior probability\ndensity function and reduced computational costs, compared to the original\napproach. The method can be used with both hard and indirect conditioning data,\nindividually or simultaneously. As the inversion is included within the\ndiffusion process, it is faster than other methods requiring an outer-loop\naround the generative model, such as Markov chain Monte Carlo.", "AI": {"tldr": "Diffusion models improve multivariate subsurface modeling and probabilistic inversion, outperforming VAEs and GANs, with enhanced robustness and reduced computational costs.", "motivation": "To enhance multivariate modeling and probabilistic inversion in subsurface scenarios using diffusion models, addressing limitations of existing methods like VAEs and GANs.", "method": "Proposes corrections to Diffusion Posterior Sampling, including a noise-contamination-aware likelihood approximation, and tests in geological scenarios with facies and acoustic impedance.", "result": "Shows improved statistical robustness, better posterior sampling, and lower computational costs compared to original methods, handling both hard and indirect data.", "conclusion": "Diffusion models offer efficient, robust inversion within the generative process, outperforming outer-loop methods like MCMC."}}
{"id": "2507.15824", "pdf": "https://arxiv.org/pdf/2507.15824", "abs": "https://arxiv.org/abs/2507.15824", "authors": ["Enes Sanli", "Baris Sarper Tezcan", "Aykut Erdem", "Erkut Erdem"], "title": "Can Your Model Separate Yolks with a Water Bottle? Benchmarking Physical Commonsense Understanding in Video Generation Models", "categories": ["cs.CV"], "comment": null, "summary": "Recent progress in text-to-video (T2V) generation has enabled the synthesis\nof visually compelling and temporally coherent videos from natural language.\nHowever, these models often fall short in basic physical commonsense, producing\noutputs that violate intuitive expectations around causality, object behavior,\nand tool use. Addressing this gap, we present PhysVidBench, a benchmark\ndesigned to evaluate the physical reasoning capabilities of T2V systems. The\nbenchmark includes 383 carefully curated prompts, emphasizing tool use,\nmaterial properties, and procedural interactions, and domains where physical\nplausibility is crucial. For each prompt, we generate videos using diverse\nstate-of-the-art models and adopt a three-stage evaluation pipeline: (1)\nformulate grounded physics questions from the prompt, (2) caption the generated\nvideo with a vision-language model, and (3) task a language model to answer\nseveral physics-involved questions using only the caption. This indirect\nstrategy circumvents common hallucination issues in direct video-based\nevaluation. By highlighting affordances and tool-mediated actions, areas\noverlooked in current T2V evaluations, PhysVidBench provides a structured,\ninterpretable framework for assessing physical commonsense in generative video\nmodels.", "AI": {"tldr": "PhysVidBench is a benchmark to evaluate physical commonsense in text-to-video models, using 383 prompts and a three-stage evaluation pipeline.", "motivation": "Current T2V models lack physical commonsense, producing videos that violate intuitive physics.", "method": "A three-stage pipeline: formulate physics questions, caption videos, and answer questions using captions.", "result": "Provides a structured framework to assess physical plausibility in T2V models.", "conclusion": "PhysVidBench addresses gaps in T2V evaluations by focusing on physical reasoning."}}
{"id": "2507.15852", "pdf": "https://arxiv.org/pdf/2507.15852", "abs": "https://arxiv.org/abs/2507.15852", "authors": ["Zhixiong Zhang", "Shuangrui Ding", "Xiaoyi Dong", "Songxin He", "Jianfan Lin", "Junsong Tang", "Yuhang Zang", "Yuhang Cao", "Dahua Lin", "Jiaqi Wang"], "title": "SeC: Advancing Complex Video Object Segmentation via Progressive Concept Construction", "categories": ["cs.CV", "cs.AI"], "comment": "project page: https://rookiexiong7.github.io/projects/SeC/; code:\n  https://github.com/OpenIXCLab/SeC; dataset:\n  https://huggingface.co/datasets/OpenIXCLab/SeCVOS", "summary": "Video Object Segmentation (VOS) is a core task in computer vision, requiring\nmodels to track and segment target objects across video frames. Despite notable\nadvances with recent efforts, current techniques still lag behind human\ncapabilities in handling drastic visual variations, occlusions, and complex\nscene changes. This limitation arises from their reliance on appearance\nmatching, neglecting the human-like conceptual understanding of objects that\nenables robust identification across temporal dynamics. Motivated by this gap,\nwe propose Segment Concept (SeC), a concept-driven segmentation framework that\nshifts from conventional feature matching to the progressive construction and\nutilization of high-level, object-centric representations. SeC employs Large\nVision-Language Models (LVLMs) to integrate visual cues across diverse frames,\nconstructing robust conceptual priors. During inference, SeC forms a\ncomprehensive semantic representation of the target based on processed frames,\nrealizing robust segmentation of follow-up frames. Furthermore, SeC adaptively\nbalances LVLM-based semantic reasoning with enhanced feature matching,\ndynamically adjusting computational efforts based on scene complexity. To\nrigorously assess VOS methods in scenarios demanding high-level conceptual\nreasoning and robust semantic understanding, we introduce the Semantic Complex\nScenarios Video Object Segmentation benchmark (SeCVOS). SeCVOS comprises 160\nmanually annotated multi-scenario videos designed to challenge models with\nsubstantial appearance variations and dynamic scene transformations. In\nparticular, SeC achieves an 11.8-point improvement over SAM 2.1 on SeCVOS,\nestablishing a new state-of-the-art in concept-aware video object segmentation.", "AI": {"tldr": "SeC is a concept-driven VOS framework using LVLMs for robust object segmentation, outperforming SAM 2.1 by 11.8 points on the new SeCVOS benchmark.", "motivation": "Current VOS methods rely on appearance matching, lacking human-like conceptual understanding, which limits robustness in complex scenarios.", "method": "SeC integrates LVLMs to build high-level object representations, balancing semantic reasoning and feature matching dynamically.", "result": "SeC achieves an 11.8-point improvement over SAM 2.1 on the SeCVOS benchmark.", "conclusion": "SeC sets a new state-of-the-art in concept-aware VOS, demonstrating the value of semantic reasoning in handling complex scenarios."}}
{"id": "2507.15856", "pdf": "https://arxiv.org/pdf/2507.15856", "abs": "https://arxiv.org/abs/2507.15856", "authors": ["Jiawei Yang", "Tianhong Li", "Lijie Fan", "Yonglong Tian", "Yue Wang"], "title": "Latent Denoising Makes Good Visual Tokenizers", "categories": ["cs.CV"], "comment": "Code is available at: https://github.com/Jiawei-Yang/DeTok", "summary": "Despite their fundamental role, it remains unclear what properties could make\nvisual tokenizers more effective for generative modeling. We observe that\nmodern generative models share a conceptually similar training objective --\nreconstructing clean signals from corrupted inputs such as Gaussian noise or\nmasking -- a process we term denoising. Motivated by this insight, we propose\naligning tokenizer embeddings directly with the downstream denoising objective,\nencouraging latent embeddings to be more easily reconstructed even when heavily\ncorrupted. To achieve this, we introduce the Latent Denoising Tokenizer\n(l-DeTok), a simple yet effective tokenizer trained to reconstruct clean images\nfrom latent embeddings corrupted by interpolative noise and random masking.\nExtensive experiments on ImageNet 256x256 demonstrate that our tokenizer\nconsistently outperforms standard tokenizers across six representative\ngenerative models. Our findings highlight denoising as a fundamental design\nprinciple for tokenizer development, and we hope it could motivate new\nperspectives for future tokenizer design.", "AI": {"tldr": "The paper proposes aligning tokenizer embeddings with the denoising objective to improve generative modeling, introducing the Latent Denoising Tokenizer (l-DeTok), which outperforms standard tokenizers.", "motivation": "Modern generative models share a denoising objective, but it's unclear what makes visual tokenizers effective for this. The paper aims to align tokenizer embeddings with denoising to improve reconstruction.", "method": "Introduces l-DeTok, a tokenizer trained to reconstruct clean images from corrupted latent embeddings using interpolative noise and random masking.", "result": "l-DeTok consistently outperforms standard tokenizers across six generative models on ImageNet 256x256.", "conclusion": "Denoising is a key design principle for tokenizers, and l-DeTok's success could inspire future tokenizer designs."}}
{"id": "2506.23298", "pdf": "https://arxiv.org/pdf/2506.23298", "abs": "https://arxiv.org/abs/2506.23298", "authors": ["Xing Shen", "Justin Szeto", "Mingyang Li", "Hengguan Huang", "Tal Arbel"], "title": "Exposing and Mitigating Calibration Biases and Demographic Unfairness in MLLM Few-Shot In-Context Learning for Medical Image Classification", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "Preprint version. The peer-reviewed version of this paper has been\n  accepted to MICCAI 2025 main conference", "summary": "Multimodal large language models (MLLMs) have enormous potential to perform\nfew-shot in-context learning in the context of medical image analysis. However,\nsafe deployment of these models into real-world clinical practice requires an\nin-depth analysis of the accuracies of their predictions, and their associated\ncalibration errors, particularly across different demographic subgroups. In\nthis work, we present the first investigation into the calibration biases and\ndemographic unfairness of MLLMs' predictions and confidence scores in few-shot\nin-context learning for medical image classification. We introduce CALIN, an\ninference-time calibration method designed to mitigate the associated biases.\nSpecifically, CALIN estimates the amount of calibration needed, represented by\ncalibration matrices, using a bi-level procedure: progressing from the\npopulation level to the subgroup level prior to inference. It then applies this\nestimation to calibrate the predicted confidence scores during inference.\nExperimental results on three medical imaging datasets: PAPILA for fundus image\nclassification, HAM10000 for skin cancer classification, and MIMIC-CXR for\nchest X-ray classification demonstrate CALIN's effectiveness at ensuring fair\nconfidence calibration in its prediction, while improving its overall\nprediction accuracies and exhibiting minimum fairness-utility trade-off. Our\ncodebase can be found at\nhttps://github.com/xingbpshen/medical-calibration-fairness-mllm.", "AI": {"tldr": "The paper investigates calibration biases and demographic unfairness in multimodal large language models (MLLMs) for medical image classification, introducing CALIN, an inference-time calibration method to mitigate these biases.", "motivation": "Safe deployment of MLLMs in clinical practice requires analyzing prediction accuracies and calibration errors across demographic subgroups.", "method": "CALIN uses a bi-level procedure to estimate calibration needs (via calibration matrices) from population to subgroup levels, applying this during inference.", "result": "Experiments on three datasets show CALIN improves fairness in confidence calibration, prediction accuracy, and minimizes fairness-utility trade-offs.", "conclusion": "CALIN effectively addresses calibration biases and unfairness in MLLMs for medical image classification, enhancing clinical deployment safety."}}
{"id": "2507.14199", "pdf": "https://arxiv.org/pdf/2507.14199", "abs": "https://arxiv.org/abs/2507.14199", "authors": ["Ebrahim Abu-Helalah", "Jordi Serra", "Jordi Perez-Romero"], "title": "On Splitting Lightweight Semantic Image Segmentation for Wireless Communications", "categories": ["cs.NI", "cs.CV", "eess.IV"], "comment": "IEEE International Mediterranean Conference on Communications and\n  Networking", "summary": "Semantic communication represents a promising technique towards reducing\ncommunication costs, especially when dealing with image segmentation, but it\nstill lacks a balance between computational efficiency and bandwidth\nrequirements while maintaining high image segmentation accuracy, particularly\nin resource-limited environments and changing channel conditions. On the other\nhand, the more complex and larger semantic image segmentation models become,\nthe more stressed the devices are when processing data. This paper proposes a\nnovel approach to implementing semantic communication based on splitting the\nsemantic image segmentation process between a resource constrained transmitter\nand the receiver. This allows saving bandwidth by reducing the transmitted data\nwhile maintaining the accuracy of the semantic image segmentation.\nAdditionally, it reduces the computational requirements at the resource\nconstrained transmitter compared to doing all the semantic image segmentation\nin the transmitter. The proposed approach is evaluated by means of\nsimulation-based experiments in terms of different metrics such as\ncomputational resource usage, required bit rate and segmentation accuracy. The\nresults when comparing the proposal with the full semantic image segmentation\nin the transmitter show that up to 72% of the bit rate was reduced in the\ntransmission process. In addition, the computational load of the transmitter is\nreduced by more than 19%. This reflects the interest of this technique for its\napplication in communication systems, particularly in the upcoming 6G systems.", "AI": {"tldr": "A novel approach splits semantic image segmentation between transmitter and receiver, reducing bandwidth and computational load while maintaining accuracy.", "motivation": "Addressing the trade-off between computational efficiency, bandwidth, and accuracy in semantic communication for image segmentation, especially in resource-limited environments.", "method": "Proposes splitting the semantic image segmentation process between a constrained transmitter and receiver to save bandwidth and reduce computational load.", "result": "Simulations show up to 72% reduction in bit rate and over 19% decrease in transmitter computational load without sacrificing accuracy.", "conclusion": "The technique is promising for communication systems, particularly 6G, due to its efficiency and resource-saving benefits."}}
{"id": "2507.14248", "pdf": "https://arxiv.org/pdf/2507.14248", "abs": "https://arxiv.org/abs/2507.14248", "authors": ["Eldor Abdukhamidov", "Mohammed Abuhamad", "Simon S. Woo", "Hyoungshick Kim", "Tamer Abuhmed"], "title": "Breaking the Illusion of Security via Interpretation: Interpretable Vision Transformer Systems under Attack", "categories": ["cs.CR", "cs.AI", "cs.CV", "cs.LG", "I.2.10; I.2.6; I.5.1; D.4.6; K.6.5"], "comment": null, "summary": "Vision transformer (ViT) models, when coupled with interpretation models, are\nregarded as secure and challenging to deceive, making them well-suited for\nsecurity-critical domains such as medical applications, autonomous vehicles,\ndrones, and robotics. However, successful attacks on these systems can lead to\nsevere consequences. Recent research on threats targeting ViT models primarily\nfocuses on generating the smallest adversarial perturbations that can deceive\nthe models with high confidence, without considering their impact on model\ninterpretations. Nevertheless, the use of interpretation models can effectively\nassist in detecting adversarial examples. This study investigates the\nvulnerability of transformer models to adversarial attacks, even when combined\nwith interpretation models. We propose an attack called \"AdViT\" that generates\nadversarial examples capable of misleading both a given transformer model and\nits coupled interpretation model. Through extensive experiments on various\ntransformer models and two transformer-based interpreters, we demonstrate that\nAdViT achieves a 100% attack success rate in both white-box and black-box\nscenarios. In white-box scenarios, it reaches up to 98% misclassification\nconfidence, while in black-box scenarios, it reaches up to 76%\nmisclassification confidence. Remarkably, AdViT consistently generates accurate\ninterpretations in both scenarios, making the adversarial examples more\ndifficult to detect.", "AI": {"tldr": "AdViT is an adversarial attack method targeting Vision Transformer (ViT) models and their interpretation models, achieving high success rates in misleading both while maintaining accurate interpretations.", "motivation": "Despite ViT models being considered secure, adversarial attacks can have severe consequences. Existing research overlooks the impact on model interpretations, which can aid in detecting adversarial examples.", "method": "Proposes AdViT, an attack method that generates adversarial examples to deceive both ViT models and their interpretation models. Tests on various models and interpreters in white-box and black-box scenarios.", "result": "AdViT achieves 100% attack success rate, with up to 98% misclassification confidence in white-box and 76% in black-box scenarios, while maintaining accurate interpretations.", "conclusion": "AdViT demonstrates the vulnerability of ViT models even with interpretation models, highlighting the need for improved defenses against such attacks."}}
{"id": "2507.14260", "pdf": "https://arxiv.org/pdf/2507.14260", "abs": "https://arxiv.org/abs/2507.14260", "authors": ["Alfredo Gimenez Zapiola", "Andrea Boselli", "Alessandra Menafoglio", "Simone Vantini"], "title": "Hyper-spectral Unmixing algorithms for remote compositional surface mapping: a review of the state of the art", "categories": ["astro-ph.IM", "astro-ph.EP", "cs.CV"], "comment": null, "summary": "This work concerns a detailed review of data analysis methods used for\nremotely sensed images of large areas of the Earth and of other solid\nastronomical objects. In detail, it focuses on the problem of inferring the\nmaterials that cover the surfaces captured by hyper-spectral images and\nestimating their abundances and spatial distributions within the region. The\nmost successful and relevant hyper-spectral unmixing methods are reported as\nwell as compared, as an addition to analysing the most recent methodologies.\nThe most important public data-sets in this setting, which are vastly used in\nthe testing and validation of the former, are also systematically explored.\nFinally, open problems are spotlighted and concrete recommendations for future\nresearch are provided.", "AI": {"tldr": "A review of hyper-spectral unmixing methods for analyzing remotely sensed images of Earth and other astronomical objects, including material inference, abundance estimation, and spatial distribution.", "motivation": "To address the challenge of accurately inferring surface materials and their distributions from hyper-spectral images of large areas.", "method": "Review and comparison of successful hyper-spectral unmixing methods, analysis of recent methodologies, and exploration of public datasets.", "result": "Identification of key methods and datasets, along with insights into their effectiveness.", "conclusion": "Highlights open problems and provides recommendations for future research in hyper-spectral unmixing."}}
{"id": "2507.14270", "pdf": "https://arxiv.org/pdf/2507.14270", "abs": "https://arxiv.org/abs/2507.14270", "authors": ["Ravin Kumar"], "title": "APTx Neuron: A Unified Trainable Neuron Architecture Integrating Activation and Computation", "categories": ["cs.NE", "cs.AI", "cs.CV", "cs.LG"], "comment": "10 pages, 2 figures, 1 table, and GitHub repository for the source\n  code", "summary": "We propose the APTx Neuron, a novel, unified neural computation unit that\nintegrates non-linear activation and linear transformation into a single\ntrainable expression. The APTx Neuron is derived from the APTx activation\nfunction, thereby eliminating the need for separate activation layers and\nmaking the architecture both computationally efficient and elegant. The\nproposed neuron follows the functional form $y = \\sum_{i=1}^{n} ((\\alpha_i +\n\\tanh(\\beta_i x_i)) \\cdot \\gamma_i x_i) + \\delta$, where all parameters\n$\\alpha_i$, $\\beta_i$, $\\gamma_i$, and $\\delta$ are trainable. We validate our\nAPTx Neuron-based architecture on the MNIST dataset, achieving up to 96.69\\%\ntest accuracy in just 20 epochs using approximately 332K trainable parameters.\nThe results highlight the superior expressiveness and computational efficiency\nof the APTx Neuron compared to traditional neurons, pointing toward a new\nparadigm in unified neuron design and the architectures built upon it.", "AI": {"tldr": "The APTx Neuron integrates activation and transformation into a single trainable unit, improving efficiency and performance.", "motivation": "To simplify neural architectures by unifying activation and linear transformation, reducing computational overhead.", "method": "Derives from the APTx activation function, using a trainable expression combining non-linear and linear operations.", "result": "Achieves 96.69% test accuracy on MNIST with 332K parameters in 20 epochs.", "conclusion": "The APTx Neuron offers superior expressiveness and efficiency, suggesting a new direction in neuron design."}}
{"id": "2507.14271", "pdf": "https://arxiv.org/pdf/2507.14271", "abs": "https://arxiv.org/abs/2507.14271", "authors": ["Refik Samet", "Nooshin Nemati", "Emrah Hancer", "Serpil Sak", "Bilge Ayca Kirmizi", "Zeynep Yildirim"], "title": "MiDeSeC: A Dataset for Mitosis Detection and Segmentation in Breast Cancer Histopathology Images", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "The MiDeSeC dataset is created through H&E stained invasive breast carcinoma,\nno special type (NST) slides of 25 different patients captured at 40x\nmagnification from the Department of Medical Pathology at Ankara University.\nThe slides have been scanned by 3D Histech Panoramic p250 Flash-3 scanner and\nOlympus BX50 microscope. As several possible mitosis shapes exist, it is\ncrucial to have a large dataset to cover all the cases. Accordingly, a total of\n50 regions is selected from glass slides for 25 patients, each of regions with\na size of 1024*1024 pixels. There are more than 500 mitoses in total in these\n50 regions. Two-thirds of the regions are reserved for training, the other\nthird for testing.", "AI": {"tldr": "The MiDeSeC dataset comprises H&E stained breast carcinoma slides from 25 patients, with 50 regions (1024x1024 pixels) containing over 500 mitoses, split into training and testing sets.", "motivation": "To address the variability in mitosis shapes, a large and diverse dataset is needed for accurate detection and analysis in breast carcinoma.", "method": "Slides were scanned using 3D Histech Panoramic p250 Flash-3 scanner and Olympus BX50 microscope, with 50 regions selected for analysis.", "result": "The dataset includes over 500 mitoses, with two-thirds of regions for training and one-third for testing.", "conclusion": "MiDeSeC provides a robust dataset for mitosis detection in breast carcinoma, aiding research and diagnostic accuracy."}}
{"id": "2507.14272", "pdf": "https://arxiv.org/pdf/2507.14272", "abs": "https://arxiv.org/abs/2507.14272", "authors": ["Refik Samet", "Nooshin Nemati", "Emrah Hancer", "Serpil Sak", "Bilge Ayca Kirmizi"], "title": "NuSeC: A Dataset for Nuclei Segmentation in Breast Cancer Histopathology Images", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "The NuSeC dataset is created by selecting 4 images with the size of 1024*1024\npixels from the slides of each patient among 25 patients. Therefore, there are\na total of 100 images in the NuSeC dataset. To carry out a consistent\ncomparative analysis between the methods that will be developed using the NuSeC\ndataset by the researchers in the future, we divide the NuSeC dataset 75% as\nthe training set and 25% as the testing set. In detail, an image is randomly\nselected from 4 images of each patient among 25 patients to build the testing\nset, and then the remaining images are reserved for the training set. While the\ntraining set includes 75 images with around 30000 nuclei structures, the\ntesting set includes 25 images with around 6000 nuclei structures.", "AI": {"tldr": "The NuSeC dataset consists of 100 images (1024*1024 pixels) from 25 patients, split into 75 training and 25 testing images for consistent analysis.", "motivation": "To enable consistent comparative analysis of future methods using the NuSeC dataset.", "method": "Dataset creation: 4 images per patient (25 patients). Split: 75% training (75 images, ~30k nuclei), 25% testing (25 images, ~6k nuclei). Random selection for testing.", "result": "Training set: 75 images (~30k nuclei). Testing set: 25 images (~6k nuclei).", "conclusion": "The NuSeC dataset is structured to support reproducible and comparative research in nuclei analysis."}}
{"id": "2507.14301", "pdf": "https://arxiv.org/pdf/2507.14301", "abs": "https://arxiv.org/abs/2507.14301", "authors": ["Yuxin Liu", "Yuezhang Peng", "Hefeng Zhou", "Hongze Liu", "Xinyu Lu", "Jiong Lou", "Chentao Wu", "Wei Zhao", "Jie Li"], "title": "LOVO: Efficient Complex Object Query in Large-Scale Video Datasets", "categories": ["cs.IR", "cs.CV", "cs.DB"], "comment": "@inproceedings{liu2025lovo,title={LOVO: Efficient Complex Object\n  Query in Large-Scale Video Datasets},author={Liu, Yuxin and Peng, Yuezhang\n  and Zhou, Hefeng and Liu, Hongze and Lu, Xinyu and Lou, Jiong and Wu, Chentao\n  and Zhao, Wei and Li, Jie},booktitle={2025 IEEE 41st International Conference\n  on Data Engineering (ICDE)},pages={1938--1951},year={2025},organization={IEEE\n  Computer Society}}", "summary": "The widespread deployment of cameras has led to an exponential increase in\nvideo data, creating vast opportunities for applications such as traffic\nmanagement and crime surveillance. However, querying specific objects from\nlarge-scale video datasets presents challenges, including (1) processing\nmassive and continuously growing data volumes, (2) supporting complex query\nrequirements, and (3) ensuring low-latency execution. Existing video analysis\nmethods struggle with either limited adaptability to unseen object classes or\nsuffer from high query latency. In this paper, we present LOVO, a novel system\ndesigned to efficiently handle comp$\\underline{L}$ex $\\underline{O}$bject\nqueries in large-scale $\\underline{V}$ide$\\underline{O}$ datasets. Agnostic to\nuser queries, LOVO performs one-time feature extraction using pre-trained\nvisual encoders, generating compact visual embeddings for key frames to build\nan efficient index. These visual embeddings, along with associated bounding\nboxes, are organized in an inverted multi-index structure within a vector\ndatabase, which supports queries for any objects. During the query phase, LOVO\ntransforms object queries to query embeddings and conducts fast approximate\nnearest-neighbor searches on the visual embeddings. Finally, a cross-modal\nrerank is performed to refine the results by fusing visual features with\ndetailed textual features. Evaluation on real-world video datasets demonstrates\nthat LOVO outperforms existing methods in handling complex queries, with\nnear-optimal query accuracy and up to 85x lower search latency, while\nsignificantly reducing index construction costs. This system redefines the\nstate-of-the-art object query approaches in video analysis, setting a new\nbenchmark for complex object queries with a novel, scalable, and efficient\napproach that excels in dynamic environments.", "AI": {"tldr": "LOVO is a system for efficient object queries in large-scale video datasets, using pre-trained visual encoders and an inverted multi-index structure for fast, accurate searches with low latency.", "motivation": "The exponential growth of video data demands efficient querying systems to handle massive volumes, complex queries, and low-latency execution, which existing methods fail to address adequately.", "method": "LOVO performs one-time feature extraction, organizes embeddings in an inverted multi-index, and uses approximate nearest-neighbor searches with cross-modal reranking for refined results.", "result": "LOVO achieves near-optimal query accuracy, up to 85x lower search latency, and reduced index construction costs compared to existing methods.", "conclusion": "LOVO sets a new benchmark for complex object queries in video analysis, offering scalability, efficiency, and adaptability to dynamic environments."}}
{"id": "2507.14308", "pdf": "https://arxiv.org/pdf/2507.14308", "abs": "https://arxiv.org/abs/2507.14308", "authors": ["Jingjia Chen", "Haoyang Pei", "Christoph Maier", "Mary Bruno", "Qiuting Wen", "Seon-Hi Shin", "William Moore", "Hersh Chandarana", "Li Feng"], "title": "Self-Supervised Joint Reconstruction and Denoising of T2-Weighted PROPELLER MRI of the Lungs at 0.55T", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Purpose: This study aims to improve 0.55T T2-weighted PROPELLER lung MRI\nthrough a self-supervised joint reconstruction and denoising model.\n  Methods: T2-weighted 0.55T lung MRI dataset including 44 patients with\nprevious covid infection were used. A self-supervised learning framework was\ndeveloped, where each blade of the PROPELLER acquisition was split along the\nreadout direction into two partitions. One subset trains the unrolled\nreconstruction network, while the other subset is used for loss calculation,\nenabling self-supervised training without clean targets and leveraging matched\nnoise statistics for denoising. For comparison, Marchenko-Pastur Principal\nComponent Analysis (MPPCA) was performed along the coil dimension, followed by\nconventional parallel imaging reconstruction. The quality of the reconstructed\nlung MRI was assessed visually by two experienced radiologists independently.\n  Results: The proposed self-supervised model improved the clarity and\nstructural integrity of the lung images. For cases with available CT scans, the\nreconstructed images demonstrated strong alignment with corresponding CT\nimages. Additionally, the proposed model enables further scan time reduction by\nrequiring only half the number of blades. Reader evaluations confirmed that the\nproposed method outperformed MPPCA-denoised images across all categories\n(Wilcoxon signed-rank test, p<0.001), with moderate inter-reader agreement\n(weighted Cohen's kappa=0.55; percentage of exact and within +/-1 point\nagreement=91%).\n  Conclusion: By leveraging intrinsic structural redundancies between two\ndisjoint splits of k-space subsets, the proposed self-supervised learning model\neffectively reconstructs the image while suppressing the noise for 0.55T\nT2-weighted lung MRI with PROPELLER sampling.", "AI": {"tldr": "A self-supervised joint reconstruction and denoising model improves 0.55T T2-weighted PROPELLER lung MRI, outperforming traditional methods and enabling scan time reduction.", "motivation": "To enhance the clarity and structural integrity of 0.55T T2-weighted lung MRI by leveraging self-supervised learning without requiring clean targets.", "method": "A self-supervised framework splits PROPELLER acquisition blades into partitions for training and loss calculation, using matched noise statistics for denoising. Compared with MPPCA-denoised images.", "result": "Improved image clarity and alignment with CT scans, reduced scan time by half, and outperformed MPPCA-denoised images in reader evaluations (p<0.001).", "conclusion": "The self-supervised model effectively reconstructs and denoises 0.55T T2-weighted lung MRI by utilizing intrinsic structural redundancies in k-space subsets."}}
{"id": "2507.14378", "pdf": "https://arxiv.org/pdf/2507.14378", "abs": "https://arxiv.org/abs/2507.14378", "authors": ["Shrunal Pothagoni", "Benjamin Schweinhart"], "title": "Classification of Histopathology Slides with Persistence Homology Convolutions", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Convolutional neural networks (CNNs) are a standard tool for computer vision\ntasks such as image classification. However, typical model architectures may\nresult in the loss of topological information. In specific domains such as\nhistopathology, topology is an important descriptor that can be used to\ndistinguish between disease-indicating tissue by analyzing the shape\ncharacteristics of cells. Current literature suggests that reintroducing\ntopological information using persistent homology can improve medical\ndiagnostics; however, previous methods utilize global topological summaries\nwhich do not contain information about the locality of topological features. To\naddress this gap, we present a novel method that generates local persistent\nhomology-based data using a modified version of the convolution operator called\nPersistent Homology Convolutions. This method captures information about the\nlocality and translation invariance of topological features. We perform a\ncomparative study using various representations of histopathology slides and\nfind that models trained with persistent homology convolutions outperform\nconventionally trained models and are less sensitive to hyperparameters. These\nresults indicate that persistent homology convolutions extract meaningful\ngeometric information from the histopathology slides.", "AI": {"tldr": "A novel method, Persistent Homology Convolutions, improves CNN performance in histopathology by capturing local topological features, outperforming conventional models.", "motivation": "Typical CNNs lose topological information, crucial in domains like histopathology where topology distinguishes disease-indicating tissue.", "method": "Introduces Persistent Homology Convolutions, a modified convolution operator, to generate local persistent homology-based data, preserving locality and translation invariance of topological features.", "result": "Models with persistent homology convolutions outperform conventional models and are less sensitive to hyperparameters.", "conclusion": "Persistent homology convolutions effectively extract meaningful geometric information from histopathology slides, enhancing diagnostics."}}
{"id": "2507.14503", "pdf": "https://arxiv.org/pdf/2507.14503", "abs": "https://arxiv.org/abs/2507.14503", "authors": ["Jiequan Cui", "Beier Zhu", "Qingshan Xu", "Xiaogang Xu", "Pengguang Chen", "Xiaojuan Qi", "Bei Yu", "Hanwang Zhang", "Richang Hong"], "title": "Generative Distribution Distillation", "categories": ["cs.LG", "cs.CV"], "comment": "Technique report", "summary": "In this paper, we formulate the knowledge distillation (KD) as a conditional\ngenerative problem and propose the \\textit{Generative Distribution Distillation\n(GenDD)} framework. A naive \\textit{GenDD} baseline encounters two major\nchallenges: the curse of high-dimensional optimization and the lack of semantic\nsupervision from labels. To address these issues, we introduce a \\textit{Split\nTokenization} strategy, achieving stable and effective unsupervised KD.\nAdditionally, we develop the \\textit{Distribution Contraction} technique to\nintegrate label supervision into the reconstruction objective. Our theoretical\nproof demonstrates that \\textit{GenDD} with \\textit{Distribution Contraction}\nserves as a gradient-level surrogate for multi-task learning, realizing\nefficient supervised training without explicit classification loss on\nmulti-step sampling image representations. To evaluate the effectiveness of our\nmethod, we conduct experiments on balanced, imbalanced, and unlabeled data.\nExperimental results show that \\textit{GenDD} performs competitively in the\nunsupervised setting, significantly surpassing KL baseline by \\textbf{16.29\\%}\non ImageNet validation set. With label supervision, our ResNet-50 achieves\n\\textbf{82.28\\%} top-1 accuracy on ImageNet in 600 epochs training,\nestablishing a new state-of-the-art.", "AI": {"tldr": "The paper proposes Generative Distribution Distillation (GenDD) for knowledge distillation, addressing high-dimensional optimization and lack of label supervision with Split Tokenization and Distribution Contraction. It achieves competitive results, outperforming baselines by 16.29% in unsupervised settings and setting a new SOTA (82.28% accuracy) with supervised training.", "motivation": "To address challenges in knowledge distillation (KD) as a conditional generative problem, specifically high-dimensional optimization and lack of semantic supervision from labels.", "method": "Introduces Split Tokenization for stable unsupervised KD and Distribution Contraction to integrate label supervision into reconstruction. Theoretical proof links GenDD to multi-task learning.", "result": "GenDD outperforms KL baseline by 16.29% on ImageNet in unsupervised settings. With supervision, ResNet-50 achieves 82.28% top-1 accuracy in 600 epochs.", "conclusion": "GenDD is a robust framework for KD, excelling in both unsupervised and supervised settings, with theoretical and empirical validation."}}
{"id": "2507.14542", "pdf": "https://arxiv.org/pdf/2507.14542", "abs": "https://arxiv.org/abs/2507.14542", "authors": ["Yipeng Zhang", "Yuanyi Ding", "Chenda Duan", "Atsuro Daida", "Hiroki Nariai", "Vwani Roychowdhury"], "title": "Self-Supervised Distillation of Legacy Rule-Based Methods for Enhanced EEG-Based Decision-Making", "categories": ["cs.CE", "cs.CV"], "comment": null, "summary": "High-frequency oscillations (HFOs) in intracranial Electroencephalography\n(iEEG) are critical biomarkers for localizing the epileptogenic zone in\nepilepsy treatment. However, traditional rule-based detectors for HFOs suffer\nfrom unsatisfactory precision, producing false positives that require\ntime-consuming manual review. Supervised machine learning approaches have been\nused to classify the detection results, yet they typically depend on labeled\ndatasets, which are difficult to acquire due to the need for specialized\nexpertise. Moreover, accurate labeling of HFOs is challenging due to low\ninter-rater reliability and inconsistent annotation practices across\ninstitutions. The lack of a clear consensus on what constitutes a pathological\nHFO further challenges supervised refinement approaches. To address this, we\nleverage the insight that legacy detectors reliably capture clinically relevant\nsignals despite their relatively high false positive rates. We thus propose the\nSelf-Supervised to Label Discovery (SS2LD) framework to refine the large set of\ncandidate events generated by legacy detectors into a precise set of\npathological HFOs. SS2LD employs a variational autoencoder (VAE) for\nmorphological pre-training to learn meaningful latent representation of the\ndetected events. These representations are clustered to derive weak supervision\nfor pathological events. A classifier then uses this supervision to refine\ndetection boundaries, trained on real and VAE-augmented data. Evaluated on\nlarge multi-institutional interictal iEEG datasets, SS2LD outperforms\nstate-of-the-art methods. SS2LD offers a scalable, label-efficient, and\nclinically effective strategy to identify pathological HFOs using legacy\ndetectors.", "AI": {"tldr": "The paper introduces SS2LD, a self-supervised framework to refine HFO detection in epilepsy treatment by leveraging legacy detectors and a VAE for improved precision without heavy reliance on labeled data.", "motivation": "Traditional HFO detectors have high false positives, and supervised methods require scarce labeled data. The lack of consensus on pathological HFOs further complicates refinement.", "method": "SS2LD uses a VAE for morphological pre-training, clusters latent representations for weak supervision, and trains a classifier on real and augmented data.", "result": "SS2LD outperforms state-of-the-art methods on multi-institutional datasets, offering scalable and label-efficient HFO detection.", "conclusion": "SS2LD provides a clinically effective solution for refining HFO detection, reducing reliance on labeled data and improving precision."}}
{"id": "2507.14560", "pdf": "https://arxiv.org/pdf/2507.14560", "abs": "https://arxiv.org/abs/2507.14560", "authors": ["Giorgio Roffo"], "title": "The Origin of Self-Attention: From Pairwise Affinity Matrices to Transformers", "categories": ["cs.LG", "cs.CV", "68T07, 05C50, 15A18", "I.2.6; I.2.7; I.5.1"], "comment": "24 pages, 10 figures, submitted for review. Companion code and\n  reproducibility materials available", "summary": "The self-attention mechanism, now central to deep learning architectures such\nas Transformers, is a modern instance of a more general computational\nprinciple: learning and using pairwise affinity matrices to control how\ninformation flows through a model. This paper traces the conceptual origins of\nself-attention across multiple domains, including computer vision, natural\nlanguage processing, and graph learning, through their shared reliance on an\naffinity matrix, denoted as A. We highlight Infinite Feature Selection (Inf-FS)\nas a foundational approach that generalizes the idea of affinity-based\nweighting. Unlike the fixed dot-product structure used in Transformers, Inf-FS\ndefines A either through domain knowledge or by learning, and computes feature\nrelevance through multi-hop propagation over the affinity graph. From this\nperspective, self-attention can be seen as a special case of Inf-FS: it uses a\nsingle-hop affinity computation where A is dynamically built from token\nsimilarities. We argue that the underlying structure, reasoning over pairwise\nrelationships, is preserved across both approaches, and the key differences lie\nin how the affinity matrix is defined and applied. By situating self-attention\nwithin the broader paradigm of affinity-based computation, we unify several\nstrands of machine learning research and highlight a common mathematical\nfoundation that underpins diverse models and tasks.", "AI": {"tldr": "The paper connects self-attention in Transformers to the broader concept of affinity-based computation, highlighting Infinite Feature Selection (Inf-FS) as a foundational approach. It shows self-attention as a special case of Inf-FS and unifies diverse ML models under this paradigm.", "motivation": "To trace the origins of self-attention and situate it within the broader framework of affinity-based computation, revealing common principles across domains like vision, NLP, and graph learning.", "method": "Comparative analysis of self-attention and Inf-FS, focusing on how affinity matrices (A) are defined and used. Self-attention uses dynamic token similarities, while Inf-FS employs domain knowledge or learned affinities with multi-hop propagation.", "result": "Self-attention is a single-hop variant of Inf-FS, sharing the core idea of reasoning over pairwise relationships. The key differences lie in affinity matrix construction and application.", "conclusion": "The paper unifies self-attention and Inf-FS under affinity-based computation, emphasizing their shared mathematical foundation and potential for cross-domain insights."}}
{"id": "2507.14597", "pdf": "https://arxiv.org/pdf/2507.14597", "abs": "https://arxiv.org/abs/2507.14597", "authors": ["Eugene Armah", "Linda Amoako Bannning"], "title": "Towards a Proactive Autoscaling Framework for Data Stream Processing at the Edge using GRU and Transfer Learning", "categories": ["cs.DC", "cs.CV", "cs.LG", "cs.PF"], "comment": null, "summary": "Processing data at high speeds is becoming increasingly critical as digital\neconomies generate enormous data. The current paradigms for timely data\nprocessing are edge computing and data stream processing (DSP). Edge computing\nplaces resources closer to where data is generated, while stream processing\nanalyzes the unbounded high-speed data in motion. However, edge stream\nprocessing faces rapid workload fluctuations, complicating resource\nprovisioning. Inadequate resource allocation leads to bottlenecks, whereas\nexcess allocation results in wastage. Existing reactive methods, such as\nthreshold-based policies and queuing theory scale only after performance\ndegrades, potentially violating SLAs. Although reinforcement learning (RL)\noffers a proactive approach through agents that learn optimal runtime\nadaptation policies, it requires extensive simulation. Furthermore, predictive\nmachine learning models face online distribution and concept drift that\nminimize their accuracy. We propose a three-step solution to the proactive edge\nstream processing autoscaling problem. Firstly, a GRU neural network forecasts\nthe upstream load using real-world and synthetic DSP datasets. Secondly, a\ntransfer learning framework integrates the predictive model into an online\nstream processing system using the DTW algorithm and joint distribution\nadaptation to handle the disparities between offline and online domains.\nFinally, a horizontal autoscaling module dynamically adjusts the degree of\noperator parallelism, based on predicted load while considering edge resource\nconstraints. The lightweight GRU model for load predictions recorded up to\n1.3\\% SMAPE value on a real-world data set. It outperformed CNN, ARIMA, and\nProphet on the SMAPE and RMSE evaluation metrics, with lower training time than\nthe computationally intensive RL models.", "AI": {"tldr": "The paper proposes a three-step solution for proactive edge stream processing autoscaling, combining GRU-based load forecasting, transfer learning, and dynamic resource scaling to address workload fluctuations.", "motivation": "High-speed data processing is critical, but edge stream processing faces challenges like rapid workload fluctuations and inefficient resource provisioning, leading to bottlenecks or wastage. Existing reactive methods and RL approaches have limitations.", "method": "A GRU neural network forecasts upstream load, transfer learning integrates the model into an online system using DTW and joint distribution adaptation, and a horizontal autoscaling module dynamically adjusts operator parallelism.", "result": "The GRU model achieved up to 1.3% SMAPE on real-world data, outperforming CNN, ARIMA, and Prophet in accuracy and training time.", "conclusion": "The proposed solution effectively addresses edge stream processing challenges with accurate load forecasting and dynamic scaling, outperforming existing methods."}}
{"id": "2507.14624", "pdf": "https://arxiv.org/pdf/2507.14624", "abs": "https://arxiv.org/abs/2507.14624", "authors": ["Yaru Liu", "Derek Nowrouzezahri", "Morgan Mcguire"], "title": "Real-Time Scene Reconstruction using Light Field Probes", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Reconstructing photo-realistic large-scale scenes from images, for example at\ncity scale, is a long-standing problem in computer graphics. Neural rendering\nis an emerging technique that enables photo-realistic image synthesis from\npreviously unobserved viewpoints; however, state-of-the-art neural rendering\nmethods have difficulty efficiently rendering a high complex large-scale scene\nbecause these methods typically trade scene size, fidelity, and rendering speed\nfor quality. The other stream of techniques utilizes scene geometries for\nreconstruction. But the cost of building and maintaining a large set of\ngeometry data increases as scene size grows. Our work explores novel view\nsynthesis methods that efficiently reconstruct complex scenes without explicit\nuse of scene geometries. Specifically, given sparse images of the scene\n(captured from the real world), we reconstruct intermediate, multi-scale,\nimplicit representations of scene geometries. In this way, our method avoids\nexplicitly relying on scene geometry, significantly reducing the computational\ncost of maintaining large 3D data. Unlike current methods, we reconstruct the\nscene using a probe data structure. Probe data hold highly accurate depth\ninformation of dense data points, enabling the reconstruction of highly complex\nscenes. By reconstructing the scene using probe data, the rendering cost is\nindependent of the complexity of the scene. As such, our approach combines\ngeometry reconstruction and novel view synthesis. Moreover, when rendering\nlarge-scale scenes, compressing and streaming probe data is more efficient than\nusing explicit scene geometry. Therefore, our neural representation approach\ncan potentially be applied to virtual reality (VR) and augmented reality (AR)\napplications.", "AI": {"tldr": "A novel method for reconstructing large-scale scenes without explicit geometry, using probe data for efficient rendering and scalability.", "motivation": "Addressing the inefficiency of current neural rendering and geometry-based methods in handling large-scale, complex scenes.", "method": "Uses sparse images to create multi-scale implicit representations with probe data, avoiding explicit geometry.", "result": "Efficient reconstruction and rendering of complex scenes, scalable for VR/AR applications.", "conclusion": "Probe-based neural representation offers a scalable, efficient alternative for large-scale scene reconstruction and rendering."}}
{"id": "2507.14694", "pdf": "https://arxiv.org/pdf/2507.14694", "abs": "https://arxiv.org/abs/2507.14694", "authors": ["Yue Ma", "Kanglei Zhou", "Fuyang Yu", "Frederick W. B. Li", "Xiaohui Liang"], "title": "Uncertainty-aware Probabilistic 3D Human Motion Forecasting via Invertible Networks", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "3D human motion forecasting aims to enable autonomous applications.\nEstimating uncertainty for each prediction (i.e., confidence based on\nprobability density or quantile) is essential for safety-critical contexts like\nhuman-robot collaboration to minimize risks. However, existing diverse motion\nforecasting approaches struggle with uncertainty quantification due to implicit\nprobabilistic representations hindering uncertainty modeling. We propose\nProbHMI, which introduces invertible networks to parameterize poses in a\ndisentangled latent space, enabling probabilistic dynamics modeling. A\nforecasting module then explicitly predicts future latent distributions,\nallowing effective uncertainty quantification. Evaluated on benchmarks, ProbHMI\nachieves strong performance for both deterministic and diverse prediction while\nvalidating uncertainty calibration, critical for risk-aware decision making.", "AI": {"tldr": "ProbHMI introduces invertible networks for 3D human motion forecasting, enabling explicit uncertainty quantification for safer human-robot collaboration.", "motivation": "Existing methods lack effective uncertainty quantification, which is critical for safety in human-robot collaboration.", "method": "ProbHMI uses invertible networks to parameterize poses in a disentangled latent space and predicts future latent distributions explicitly.", "result": "ProbHMI performs well on benchmarks for deterministic and diverse predictions while validating uncertainty calibration.", "conclusion": "ProbHMI addresses uncertainty quantification effectively, supporting risk-aware decision-making in safety-critical applications."}}
{"id": "2507.14760", "pdf": "https://arxiv.org/pdf/2507.14760", "abs": "https://arxiv.org/abs/2507.14760", "authors": ["Cassandra Tong Ye", "Shamus Li", "Tyler King", "Kristina Monakhova"], "title": "QUTCC: Quantile Uncertainty Training and Conformal Calibration for Imaging Inverse Problems", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Deep learning models often hallucinate, producing realistic artifacts that\nare not truly present in the sample. This can have dire consequences for\nscientific and medical inverse problems, such as MRI and microscopy denoising,\nwhere accuracy is more important than perceptual quality. Uncertainty\nquantification techniques, such as conformal prediction, can pinpoint outliers\nand provide guarantees for image regression tasks, improving reliability.\nHowever, existing methods utilize a linear constant scaling factor to calibrate\nuncertainty bounds, resulting in larger, less informative bounds. We propose\nQUTCC, a quantile uncertainty training and calibration technique that enables\nnonlinear, non-uniform scaling of quantile predictions to enable tighter\nuncertainty estimates. Using a U-Net architecture with a quantile embedding,\nQUTCC enables the prediction of the full conditional distribution of quantiles\nfor the imaging task. During calibration, QUTCC generates uncertainty bounds by\niteratively querying the network for upper and lower quantiles, progressively\nrefining the bounds to obtain a tighter interval that captures the desired\ncoverage. We evaluate our method on several denoising tasks as well as\ncompressive MRI reconstruction. Our method successfully pinpoints\nhallucinations in image estimates and consistently achieves tighter uncertainty\nintervals than prior methods while maintaining the same statistical coverage.", "AI": {"tldr": "QUTCC introduces a nonlinear, non-uniform scaling method for quantile predictions to tighten uncertainty bounds in deep learning models, improving reliability in medical imaging tasks.", "motivation": "Addressing the issue of hallucinations in deep learning models for medical imaging, where accuracy is critical, by improving uncertainty quantification.", "method": "QUTCC uses a U-Net with quantile embedding to predict full conditional quantile distributions and iteratively refines bounds during calibration.", "result": "QUTCC achieves tighter uncertainty intervals and better identifies hallucinations compared to prior methods, maintaining statistical coverage.", "conclusion": "QUTCC enhances reliability in medical imaging by providing tighter, more informative uncertainty bounds."}}
{"id": "2507.14766", "pdf": "https://arxiv.org/pdf/2507.14766", "abs": "https://arxiv.org/abs/2507.14766", "authors": ["Mehak Arora", "Ayman Ali", "Kaiyuan Wu", "Carolyn Davis", "Takashi Shimazui", "Mahmoud Alwakeel", "Victor Moas", "Philip Yang", "Annette Esper", "Rishikesan Kamaleswaran"], "title": "CXR-TFT: Multi-Modal Temporal Fusion Transformer for Predicting Chest X-ray Trajectories", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "In Review for MICCAI 2025", "summary": "In intensive care units (ICUs), patients with complex clinical conditions\nrequire vigilant monitoring and prompt interventions. Chest X-rays (CXRs) are a\nvital diagnostic tool, providing insights into clinical trajectories, but their\nirregular acquisition limits their utility. Existing tools for CXR\ninterpretation are constrained by cross-sectional analysis, failing to capture\ntemporal dynamics. To address this, we introduce CXR-TFT, a novel multi-modal\nframework that integrates temporally sparse CXR imaging and radiology reports\nwith high-frequency clinical data, such as vital signs, laboratory values, and\nrespiratory flow sheets, to predict the trajectory of CXR findings in\ncritically ill patients. CXR-TFT leverages latent embeddings from a vision\nencoder that are temporally aligned with hourly clinical data through\ninterpolation. A transformer model is then trained to predict CXR embeddings at\neach hour, conditioned on previous embeddings and clinical measurements. In a\nretrospective study of 20,000 ICU patients, CXR-TFT demonstrated high accuracy\nin forecasting abnormal CXR findings up to 12 hours before they became\nradiographically evident. This predictive capability in clinical data holds\nsignificant potential for enhancing the management of time-sensitive conditions\nlike acute respiratory distress syndrome, where early intervention is crucial\nand diagnoses are often delayed. By providing distinctive temporal resolution\nin prognostic CXR analysis, CXR-TFT offers actionable 'whole patient' insights\nthat can directly improve clinical outcomes.", "AI": {"tldr": "CXR-TFT is a multi-modal framework integrating sparse CXR data and high-frequency clinical metrics to predict abnormal CXR findings in ICU patients up to 12 hours early, improving time-sensitive condition management.", "motivation": "Existing CXR tools lack temporal dynamics, limiting their utility in ICU settings where early intervention is critical.", "method": "CXR-TFT combines sparse CXR imaging, radiology reports, and hourly clinical data (vital signs, lab values) using a vision encoder and transformer model to predict CXR trajectories.", "result": "In a study of 20,000 ICU patients, CXR-TFT accurately predicted abnormal CXR findings 12 hours before radiographic evidence.", "conclusion": "CXR-TFT enhances early intervention for conditions like acute respiratory distress syndrome, improving clinical outcomes through timely insights."}}
{"id": "2507.14793", "pdf": "https://arxiv.org/pdf/2507.14793", "abs": "https://arxiv.org/abs/2507.14793", "authors": ["T. Anderson Keller"], "title": "Flow Equivariant Recurrent Neural Networks", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Data arrives at our senses as a continuous stream, smoothly transforming from\none instant to the next. These smooth transformations can be viewed as\ncontinuous symmetries of the environment that we inhabit, defining equivalence\nrelations between stimuli over time. In machine learning, neural network\narchitectures that respect symmetries of their data are called equivariant and\nhave provable benefits in terms of generalization ability and sample\nefficiency. To date, however, equivariance has been considered only for static\ntransformations and feed-forward networks, limiting its applicability to\nsequence models, such as recurrent neural networks (RNNs), and corresponding\ntime-parameterized sequence transformations. In this work, we extend\nequivariant network theory to this regime of `flows' -- one-parameter Lie\nsubgroups capturing natural transformations over time, such as visual motion.\nWe begin by showing that standard RNNs are generally not flow equivariant:\ntheir hidden states fail to transform in a geometrically structured manner for\nmoving stimuli. We then show how flow equivariance can be introduced, and\ndemonstrate that these models significantly outperform their non-equivariant\ncounterparts in terms of training speed, length generalization, and velocity\ngeneralization, on both next step prediction and sequence classification. We\npresent this work as a first step towards building sequence models that respect\nthe time-parameterized symmetries which govern the world around us.", "AI": {"tldr": "The paper extends equivariant network theory to time-parameterized transformations (flows) in sequence models, showing improved performance in RNNs through flow equivariance.", "motivation": "Current equivariant networks are limited to static transformations and feed-forward architectures, missing the continuous symmetries in real-world data streams.", "method": "The authors introduce flow equivariance in RNNs, ensuring hidden states transform geometrically for moving stimuli.", "result": "Flow-equivariant RNNs outperform non-equivariant models in training speed, length generalization, and velocity generalization.", "conclusion": "This work advances sequence models by respecting time-parameterized symmetries, with broader implications for real-world applications."}}
{"id": "2507.14841", "pdf": "https://arxiv.org/pdf/2507.14841", "abs": "https://arxiv.org/abs/2507.14841", "authors": ["Xiang Tang", "Ruotong Li", "Xiaopeng Fan"], "title": "Towards Geometric and Textural Consistency 3D Scene Generation via Single Image-guided Model Generation and Layout Optimization", "categories": ["cs.GR", "cs.CV"], "comment": "15 pages, 8 figures, Project page: https://xdlbw.github.io/sing3d/", "summary": "In recent years, 3D generation has made great strides in both academia and\nindustry. However, generating 3D scenes from a single RGB image remains a\nsignificant challenge, as current approaches often struggle to ensure both\nobject generation quality and scene coherence in multi-object scenarios. To\novercome these limitations, we propose a novel three-stage framework for 3D\nscene generation with explicit geometric representations and high-quality\ntextural details via single image-guided model generation and spatial layout\noptimization. Our method begins with an image instance segmentation and\ninpainting phase, which recovers missing details of occluded objects in the\ninput images, thereby achieving complete generation of foreground 3D assets.\nSubsequently, our approach captures the spatial geometry of reference image by\nconstructing pseudo-stereo viewpoint for camera parameter estimation and scene\ndepth inference, while employing a model selection strategy to ensure optimal\nalignment between the 3D assets generated in the previous step and the input.\nFinally, through model parameterization and minimization of the Chamfer\ndistance between point clouds in 3D and 2D space, our approach optimizes layout\nparameters to produce an explicit 3D scene representation that maintains\nprecise alignment with input guidance image. Extensive experiments on\nmulti-object scene image sets have demonstrated that our approach not only\noutperforms state-of-the-art methods in terms of geometric accuracy and texture\nfidelity of individual generated 3D models, but also has significant advantages\nin scene layout synthesis.", "AI": {"tldr": "A novel three-stage framework for 3D scene generation from a single RGB image, ensuring object quality and scene coherence via image-guided generation and layout optimization.", "motivation": "Current methods struggle with object generation quality and scene coherence in multi-object scenarios from single images.", "method": "Three-stage process: image instance segmentation/inpainting, pseudo-stereo viewpoint construction for geometry capture, and layout optimization via Chamfer distance minimization.", "result": "Outperforms state-of-the-art in geometric accuracy, texture fidelity, and scene layout synthesis.", "conclusion": "The proposed framework effectively addresses challenges in 3D scene generation from single images, achieving superior results."}}
{"id": "2507.14899", "pdf": "https://arxiv.org/pdf/2507.14899", "abs": "https://arxiv.org/abs/2507.14899", "authors": ["Jiale Liu", "Huan Wang", "Yue Zhang", "Xiaoyu Luo", "Jiaxiang Hu", "Zhiliang Liu", "Min Xie"], "title": "InsightX Agent: An LMM-based Agentic Framework with Integrated Tools for Reliable X-ray NDT Analysis", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Non-destructive testing (NDT), particularly X-ray inspection, is vital for\nindustrial quality assurance, yet existing deep-learning-based approaches often\nlack interactivity, interpretability, and the capacity for critical\nself-assessment, limiting their reliability and operator trust. To address\nthese shortcomings, this paper proposes InsightX Agent, a novel LMM-based\nagentic framework designed to deliver reliable, interpretable, and interactive\nX-ray NDT analysis. Unlike typical sequential pipelines, InsightX Agent\npositions a Large Multimodal Model (LMM) as a central orchestrator,\ncoordinating between the Sparse Deformable Multi-Scale Detector (SDMSD) and the\nEvidence-Grounded Reflection (EGR) tool. The SDMSD generates dense defect\nregion proposals for multi-scale feature maps and sparsifies them through\nNon-Maximum Suppression (NMS), optimizing detection of small, dense targets in\nX-ray images while maintaining computational efficiency. The EGR tool guides\nthe LMM agent through a chain-of-thought-inspired review process, incorporating\ncontext assessment, individual defect analysis, false positive elimination,\nconfidence recalibration and quality assurance to validate and refine the\nSDMSD's initial proposals. By strategically employing and intelligently using\ntools, InsightX Agent moves beyond passive data processing to active reasoning,\nenhancing diagnostic reliability and providing interpretations that integrate\ndiverse information sources. Experimental evaluations on the GDXray+ dataset\ndemonstrate that InsightX Agent not only achieves a high object detection\nF1-score of 96.35% but also offers significantly improved interpretability and\ntrustworthiness in its analyses, highlighting the transformative potential of\nagentic LLM frameworks for industrial inspection tasks.", "AI": {"tldr": "InsightX Agent is an LMM-based framework for X-ray NDT, improving reliability, interpretability, and interactivity by combining SDMSD for defect detection and EGR for validation.", "motivation": "Existing deep-learning NDT methods lack interactivity, interpretability, and self-assessment, limiting reliability and trust.", "method": "Uses a Large Multimodal Model (LMM) to coordinate SDMSD for defect detection and EGR for validation through a review process.", "result": "Achieves 96.35% F1-score on GDXray+ dataset, with enhanced interpretability and trustworthiness.", "conclusion": "InsightX Agent demonstrates the potential of agentic LLM frameworks for industrial inspection."}}
{"id": "2507.14902", "pdf": "https://arxiv.org/pdf/2507.14902", "abs": "https://arxiv.org/abs/2507.14902", "authors": ["Xiaojie Li", "Chu Li", "Shi-Zhe Chen", "Xi Chen"], "title": "U-MARVEL: Unveiling Key Factors for Universal Multimodal Retrieval via Embedding Learning with MLLMs", "categories": ["cs.IR", "cs.CV"], "comment": "Technical Report (in progress)", "summary": "Universal multimodal retrieval (UMR), which aims to address complex retrieval\ntasks where both queries and candidates span diverse modalities, has been\nsignificantly advanced by the emergence of MLLMs. While state-of-the-art\nMLLM-based methods in the literature predominantly adopt contrastive learning\nprinciples, they often differ in their specific training recipes. Despite their\nsuccess, the mechanisms underlying their retrieval capabilities remain largely\nunexplored, potentially resulting in suboptimal performance and limited\ngeneralization ability. To address these issues, we present a comprehensive\nstudy aimed at uncovering the key factors that drive effective embedding\nlearning for UMR using MLLMs. We begin by implementing a general MLLM-based\nembedding learning pipeline, and systematically analyze the primary\ncontributors to high-performing universal retrieval systems. Based on this, we\nexplore various aspects of the details in embedding generation and training\nstrategies, including progressive transition, hard negative mining and\nre-ranker distillation. Notably, our findings reveal that often-overlooked\nfactors can have a substantial impact on model performance. Building on these\ndiscoveries, we introduce a unified framework termed U-MARVEL\n(\\textbf{U}niversal \\textbf{M}ultimod\\textbf{A}l \\textbf{R}etrie\\textbf{V}al\nvia \\textbf{E}mbedding \\textbf{L}earning), which outperforms state-of-the-art\ncompetitors on the M-BEIR benchmark by a large margin in supervised settings,\nand also exihibits strong zero-shot performance on several tasks such as\ncomposed image retrieval and text-to-video retrieval. These results underscore\nthe generalization potential of our framework across various embedding-based\nretrieval tasks. Code is available at https://github.com/chaxjli/U-MARVEL", "AI": {"tldr": "The paper introduces U-MARVEL, a unified framework for universal multimodal retrieval (UMR), analyzing key factors for effective embedding learning with MLLMs and outperforming state-of-the-art methods.", "motivation": "Existing MLLM-based UMR methods lack understanding of their retrieval mechanisms, leading to suboptimal performance and limited generalization.", "method": "The study implements a general MLLM-based pipeline, analyzes embedding generation and training strategies (e.g., progressive transition, hard negative mining), and introduces U-MARVEL.", "result": "U-MARVEL significantly outperforms competitors on the M-BEIR benchmark and shows strong zero-shot performance in tasks like composed image retrieval.", "conclusion": "The framework demonstrates strong generalization across embedding-based retrieval tasks, highlighting the impact of overlooked factors."}}
{"id": "2507.15078", "pdf": "https://arxiv.org/pdf/2507.15078", "abs": "https://arxiv.org/abs/2507.15078", "authors": ["Fumio Hashimoto", "Kuang Gong"], "title": "PET Image Reconstruction Using Deep Diffusion Image Prior", "categories": ["eess.IV", "cs.CV", "physics.med-ph"], "comment": "11 pages, 11 figures", "summary": "Diffusion models have shown great promise in medical image denoising and\nreconstruction, but their application to Positron Emission Tomography (PET)\nimaging remains limited by tracer-specific contrast variability and high\ncomputational demands. In this work, we proposed an anatomical prior-guided PET\nimage reconstruction method based on diffusion models, inspired by the deep\ndiffusion image prior (DDIP) framework. The proposed method alternated between\ndiffusion sampling and model fine-tuning guided by the PET sinogram, enabling\nthe reconstruction of high-quality images from various PET tracers using a\nscore function pretrained on a dataset of another tracer. To improve\ncomputational efficiency, the half-quadratic splitting (HQS) algorithm was\nadopted to decouple network optimization from iterative PET reconstruction. The\nproposed method was evaluated using one simulation and two clinical datasets.\nFor the simulation study, a model pretrained on [$^{18}$F]FDG data was tested\non amyloid-negative PET data to assess out-of-distribution (OOD) performance.\nFor the clinical-data validation, ten low-dose [$^{18}$F]FDG datasets and one\n[$^{18}$F]Florbetapir dataset were tested on a model pretrained on data from\nanother tracer. Experiment results show that the proposed PET reconstruction\nmethod can generalize robustly across tracer distributions and scanner types,\nproviding an efficient and versatile reconstruction framework for low-dose PET\nimaging.", "AI": {"tldr": "A diffusion model-based PET image reconstruction method using anatomical priors and HQS for efficiency, showing robust performance across tracers and scanners.", "motivation": "Addressing limitations of diffusion models in PET imaging, such as tracer-specific contrast variability and high computational demands.", "method": "Combines diffusion sampling and model fine-tuning guided by PET sinogram, with HQS for decoupling optimization from reconstruction.", "result": "Demonstrated robust generalization across tracer distributions and scanner types in simulation and clinical datasets.", "conclusion": "Provides an efficient, versatile framework for low-dose PET imaging with cross-tracer applicability."}}
{"id": "2507.15146", "pdf": "https://arxiv.org/pdf/2507.15146", "abs": "https://arxiv.org/abs/2507.15146", "authors": ["Sebastian A. Cruz Romero", "Misael J. Mercado Hernandez", "Samir Y. Ali Rivera", "Jorge A. Santiago Fernandez", "Wilfredo E. Lugo Beauchamp"], "title": "Design of an Edge-based Portable EHR System for Anemia Screening in Remote Health Applications", "categories": ["cs.ET", "cs.AI", "cs.CV", "cs.CY", "cs.LG", "cs.SE"], "comment": "Accepted at IEEE Global Humanitarian Technology Conference 2025", "summary": "The design of medical systems for remote, resource-limited environments faces\npersistent challenges due to poor interoperability, lack of offline support,\nand dependency on costly infrastructure. Many existing digital health solutions\nneglect these constraints, limiting their effectiveness for frontline health\nworkers in underserved regions. This paper presents a portable, edge-enabled\nElectronic Health Record platform optimized for offline-first operation, secure\npatient data management, and modular diagnostic integration. Running on\nsmall-form factor embedded devices, it provides AES-256 encrypted local storage\nwith optional cloud synchronization for interoperability. As a use case, we\nintegrated a non-invasive anemia screening module leveraging fingernail pallor\nanalysis. Trained on 250 patient cases (27\\% anemia prevalence) with\nKDE-balanced data, the Random Forest model achieved a test RMSE of 1.969 g/dL\nand MAE of 1.490 g/dL. A severity-based model reached 79.2\\% sensitivity. To\noptimize performance, a YOLOv8n-based nail bed detector was quantized to INT8,\nreducing inference latency from 46.96 ms to 21.50 ms while maintaining mAP@0.5\nat 0.995. The system emphasizes low-cost deployment, modularity, and data\nprivacy compliance (HIPAA/GDPR), addressing critical barriers to digital health\nadoption in disconnected settings. Our work demonstrates a scalable approach to\nenhance portable health information systems and support frontline healthcare in\nunderserved regions.", "AI": {"tldr": "A portable, edge-enabled Electronic Health Record platform is designed for offline-first operation, secure data management, and modular diagnostics, addressing challenges in remote healthcare.", "motivation": "To overcome poor interoperability, lack of offline support, and high infrastructure costs in medical systems for resource-limited areas.", "method": "Developed a platform with AES-256 encrypted local storage, cloud sync, and integrated anemia screening using fingernail pallor analysis with a Random Forest model and YOLOv8n-based detector.", "result": "Achieved test RMSE of 1.969 g/dL, MAE of 1.490 g/dL, and 79.2% sensitivity for anemia screening. Optimized detector latency reduced from 46.96 ms to 21.50 ms.", "conclusion": "The system offers a scalable, low-cost solution for digital health in underserved regions, emphasizing modularity, privacy, and offline functionality."}}
{"id": "2507.15151", "pdf": "https://arxiv.org/pdf/2507.15151", "abs": "https://arxiv.org/abs/2507.15151", "authors": ["Sebastian A. Cruz Romero", "Wilfredo E. Lugo Beauchamp"], "title": "Performance Analysis of Post-Training Quantization for CNN-based Conjunctival Pallor Anemia Detection", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "Accepted at International Symposium on Intelligent Computing &\n  Networks 2025", "summary": "Anemia is a widespread global health issue, particularly among young children\nin low-resource settings. Traditional methods for anemia detection often\nrequire expensive equipment and expert knowledge, creating barriers to early\nand accurate diagnosis. To address these challenges, we explore the use of deep\nlearning models for detecting anemia through conjunctival pallor, focusing on\nthe CP-AnemiC dataset, which includes 710 images from children aged 6-59\nmonths. The dataset is annotated with hemoglobin levels, gender, age and other\ndemographic data, enabling the development of machine learning models for\naccurate anemia detection. We use the MobileNet architecture as a backbone,\nknown for its efficiency in mobile and embedded vision applications, and\nfine-tune our model end-to-end using data augmentation techniques and a\ncross-validation strategy. Our model implementation achieved an accuracy of\n0.9313, a precision of 0.9374, and an F1 score of 0.9773 demonstrating strong\nperformance on the dataset. To optimize the model for deployment on edge\ndevices, we performed post-training quantization, evaluating the impact of\ndifferent bit-widths (FP32, FP16, INT8, and INT4) on model performance.\nPreliminary results suggest that while FP16 quantization maintains high\naccuracy (0.9250), precision (0.9370), and F1 Score (0.9377), more aggressive\nquantization (INT8 and INT4) leads to significant performance degradation.\nOverall, our study supports further exploration of quantization schemes and\nhardware optimizations to assess trade-offs between model size, inference time,\nand diagnostic accuracy in mobile healthcare applications.", "AI": {"tldr": "A deep learning model using MobileNet detects anemia via conjunctival pallor with high accuracy, precision, and F1 score, while exploring quantization for edge deployment.", "motivation": "Traditional anemia detection methods are costly and require expertise, limiting accessibility in low-resource settings.", "method": "MobileNet architecture fine-tuned with data augmentation and cross-validation on the CP-AnemiC dataset (710 images).", "result": "Achieved accuracy of 0.9313, precision of 0.9374, and F1 score of 0.9773. FP16 quantization maintained performance, while INT8/INT4 degraded it.", "conclusion": "Supports further optimization of quantization for mobile healthcare, balancing model size, speed, and accuracy."}}
{"id": "2507.15193", "pdf": "https://arxiv.org/pdf/2507.15193", "abs": "https://arxiv.org/abs/2507.15193", "authors": ["Tanjin Taher Toma", "Tejas Sudharshan Mathai", "Bikash Santra", "Pritam Mukherjee", "Jianfei Liu", "Wesley Jong", "Darwish Alabyad", "Vivek Batheja", "Abhishek Jha", "Mayank Patel", "Darko Pucar", "Jayadira del Rivero", "Karel Pacak", "Ronald M. Summers"], "title": "A Study of Anatomical Priors for Deep Learning-Based Segmentation of Pheochromocytoma in Abdominal CT", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Accurate segmentation of pheochromocytoma (PCC) in abdominal CT scans is\nessential for tumor burden estimation, prognosis, and treatment planning. It\nmay also help infer genetic clusters, reducing reliance on expensive testing.\nThis study systematically evaluates anatomical priors to identify\nconfigurations that improve deep learning-based PCC segmentation. We employed\nthe nnU-Net framework to evaluate eleven annotation strategies for accurate 3D\nsegmentation of pheochromocytoma, introducing a set of novel multi-class\nschemes based on organ-specific anatomical priors. These priors were derived\nfrom adjacent organs commonly surrounding adrenal tumors (e.g., liver, spleen,\nkidney, aorta, adrenal gland, and pancreas), and were compared against a broad\nbody-region prior used in previous work. The framework was trained and tested\non 105 contrast-enhanced CT scans from 91 patients at the NIH Clinical Center.\nPerformance was measured using Dice Similarity Coefficient (DSC), Normalized\nSurface Distance (NSD), and instance-wise F1 score. Among all strategies, the\nTumor + Kidney + Aorta (TKA) annotation achieved the highest segmentation\naccuracy, significantly outperforming the previously used Tumor + Body (TB)\nannotation across DSC (p = 0.0097), NSD (p = 0.0110), and F1 score (25.84%\nimprovement at an IoU threshold of 0.5), measured on a 70-30 train-test split.\nThe TKA model also showed superior tumor burden quantification (R^2 = 0.968)\nand strong segmentation across all genetic subtypes. In five-fold\ncross-validation, TKA consistently outperformed TB across IoU thresholds (0.1\nto 0.5), reinforcing its robustness and generalizability. These findings\nhighlight the value of incorporating relevant anatomical context in deep\nlearning models to achieve precise PCC segmentation, supporting clinical\nassessment and longitudinal monitoring.", "AI": {"tldr": "The study evaluates anatomical priors for improving deep learning-based PCC segmentation in CT scans, finding the Tumor + Kidney + Aorta (TKA) strategy most effective.", "motivation": "Accurate PCC segmentation aids tumor burden estimation, prognosis, and treatment planning, while reducing reliance on expensive genetic testing.", "method": "The nnU-Net framework was used to test 11 annotation strategies, including novel multi-class schemes based on organ-specific anatomical priors, on 105 CT scans.", "result": "TKA outperformed the Tumor + Body (TB) strategy in segmentation accuracy (DSC, NSD, F1 score) and tumor burden quantification (R^2 = 0.968).", "conclusion": "Incorporating relevant anatomical context enhances PCC segmentation, supporting clinical assessment and monitoring."}}
{"id": "2507.15194", "pdf": "https://arxiv.org/pdf/2507.15194", "abs": "https://arxiv.org/abs/2507.15194", "authors": ["Yilin Lyu", "Fan Yang", "Xiaoyue Liu", "Zichen Jiang", "Joshua Dillon", "Debbie Zhao", "Martyn Nash", "Charlene Mauger", "Alistair Young", "Ching-Hui Sia", "Mark YY Chan", "Lei Li"], "title": "Personalized 3D Myocardial Infarct Geometry Reconstruction from Cine MRI with Explicit Cardiac Motion Modeling", "categories": ["eess.IV", "cs.CV"], "comment": "11 pages", "summary": "Accurate representation of myocardial infarct geometry is crucial for\npatient-specific cardiac modeling in MI patients. While Late gadolinium\nenhancement (LGE) MRI is the clinical gold standard for infarct detection, it\nrequires contrast agents, introducing side effects and patient discomfort.\nMoreover, infarct reconstruction from LGE often relies on sparsely sampled 2D\nslices, limiting spatial resolution and accuracy. In this work, we propose a\nnovel framework for automatically reconstructing high-fidelity 3D myocardial\ninfarct geometry from 2D clinically standard cine MRI, eliminating the need for\ncontrast agents. Specifically, we first reconstruct the 4D biventricular mesh\nfrom multi-view cine MRIs via an automatic deep shape fitting model, biv-me.\nThen, we design a infarction reconstruction model, CMotion2Infarct-Net, to\nexplicitly utilize the motion patterns within this dynamic geometry to localize\ninfarct regions. Evaluated on 205 cine MRI scans from 126 MI patients, our\nmethod shows reasonable agreement with manual delineation. This study\ndemonstrates the feasibility of contrast-free, cardiac motion-driven 3D infarct\nreconstruction, paving the way for efficient digital twin of MI.", "AI": {"tldr": "A novel framework for 3D myocardial infarct reconstruction from 2D cine MRI, eliminating contrast agents by leveraging motion patterns.", "motivation": "LGE MRI, the gold standard for infarct detection, requires contrast agents and suffers from limited spatial resolution due to sparse 2D slices.", "method": "Reconstructs 4D biventricular mesh from cine MRI using biv-me, then uses CMotion2Infarct-Net to localize infarcts via motion patterns.", "result": "Shows reasonable agreement with manual delineation on 205 cine MRI scans from 126 MI patients.", "conclusion": "Demonstrates feasibility of contrast-free, motion-driven 3D infarct reconstruction for digital twin applications."}}
{"id": "2507.15203", "pdf": "https://arxiv.org/pdf/2507.15203", "abs": "https://arxiv.org/abs/2507.15203", "authors": ["Xiaoyue Liu", "Xicheng Sheng", "Xiahai Zhuang", "Vicente Grau", "Mark YY Chan", "Ching-Hui Sia", "Lei Li"], "title": "Personalized 4D Whole Heart Geometry Reconstruction from Cine MRI for Cardiac Digital Twins", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Cardiac digital twins (CDTs) provide personalized in-silico cardiac\nrepresentations and hold great potential for precision medicine in cardiology.\nHowever, whole-heart CDT models that simulate the full organ-scale\nelectromechanics of all four heart chambers remain limited. In this work, we\npropose a weakly supervised learning model to reconstruct 4D (3D+t) heart mesh\ndirectly from multi-view 2D cardiac cine MRIs. This is achieved by learning a\nself-supervised mapping between cine MRIs and 4D cardiac meshes, enabling the\ngeneration of personalized heart models that closely correspond to input cine\nMRIs. The resulting 4D heart meshes can facilitate the automatic extraction of\nkey cardiac variables, including ejection fraction and dynamic chamber volume\nchanges with high temporal resolution. It demonstrates the feasibility of\ninferring personalized 4D heart models from cardiac MRIs, paving the way for an\nefficient CDT platform for precision medicine. The code will be publicly\nreleased once the manuscript is accepted.", "AI": {"tldr": "A weakly supervised learning model is proposed to reconstruct 4D heart meshes from multi-view 2D cardiac cine MRIs, enabling personalized cardiac digital twins for precision medicine.", "motivation": "Whole-heart cardiac digital twin models simulating full organ-scale electromechanics are limited. This work aims to bridge this gap by creating personalized 4D heart models from 2D MRIs.", "method": "A weakly supervised learning model maps multi-view 2D cardiac cine MRIs to 4D heart meshes, leveraging self-supervised learning for accurate reconstruction.", "result": "The model successfully generates 4D heart meshes, enabling automatic extraction of key cardiac variables like ejection fraction and dynamic chamber volume changes.", "conclusion": "The study demonstrates the feasibility of inferring personalized 4D heart models from MRIs, advancing efficient cardiac digital twin platforms for precision medicine."}}
{"id": "2507.15292", "pdf": "https://arxiv.org/pdf/2507.15292", "abs": "https://arxiv.org/abs/2507.15292", "authors": ["An Wanga", "Rulin Zhou", "Mengya Xu", "Yiru Ye", "Longfei Gou", "Yiting Chang", "Hao Chen", "Chwee Ming Lim", "Jiankun Wang", "Hongliang Ren"], "title": "EndoControlMag: Robust Endoscopic Vascular Motion Magnification with Periodic Reference Resetting and Hierarchical Tissue-aware Dual-Mask Contro", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Visualizing subtle vascular motions in endoscopic surgery is crucial for\nsurgical precision and decision-making, yet remains challenging due to the\ncomplex and dynamic nature of surgical scenes. To address this, we introduce\nEndoControlMag, a training-free, Lagrangian-based framework with\nmask-conditioned vascular motion magnification tailored to endoscopic\nenvironments. Our approach features two key modules: a Periodic Reference\nResetting (PRR) scheme that divides videos into short overlapping clips with\ndynamically updated reference frames to prevent error accumulation while\nmaintaining temporal coherence, and a Hierarchical Tissue-aware Magnification\n(HTM) framework with dual-mode mask dilation. HTM first tracks vessel cores\nusing a pretrained visual tracking model to maintain accurate localization\ndespite occlusions and view changes. It then applies one of two adaptive\nsoftening strategies to surrounding tissues: motion-based softening that\nmodulates magnification strength proportional to observed tissue displacement,\nor distance-based exponential decay that simulates biomechanical force\nattenuation. This dual-mode approach accommodates diverse surgical\nscenarios-motion-based softening excels with complex tissue deformations while\ndistance-based softening provides stability during unreliable optical flow\nconditions. We evaluate EndoControlMag on our EndoVMM24 dataset spanning four\ndifferent surgery types and various challenging scenarios, including\nocclusions, instrument disturbance, view changes, and vessel deformations.\nQuantitative metrics, visual assessments, and expert surgeon evaluations\ndemonstrate that EndoControlMag significantly outperforms existing methods in\nboth magnification accuracy and visual quality while maintaining robustness\nacross challenging surgical conditions. The code, dataset, and video results\nare available at https://szupc.github.io/EndoControlMag/.", "AI": {"tldr": "EndoControlMag is a training-free, Lagrangian-based framework for magnifying subtle vascular motions in endoscopic surgery, featuring a PRR scheme and HTM framework for robust performance.", "motivation": "Visualizing subtle vascular motions is critical for surgical precision but challenging due to dynamic surgical scenes.", "method": "Uses PRR for error-free temporal coherence and HTM with dual-mode mask dilation for adaptive magnification.", "result": "Outperforms existing methods in accuracy and visual quality, validated on the EndoVMM24 dataset.", "conclusion": "EndoControlMag offers a robust solution for vascular motion magnification in endoscopic surgery."}}
{"id": "2507.15340", "pdf": "https://arxiv.org/pdf/2507.15340", "abs": "https://arxiv.org/abs/2507.15340", "authors": ["Marc Boubnovski Martell", "Kristofer Linton-Reid", "Mitchell Chen", "Sumeet Hindocha", "Benjamin Hunter", "Marco A. Calzado", "Richard Lee", "Joram M. Posma", "Eric O. Aboagye"], "title": "MedSR-Impact: Transformer-Based Super-Resolution for Lung CT Segmentation, Radiomics, Classification, and Prognosis", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "High-resolution volumetric computed tomography (CT) is essential for accurate\ndiagnosis and treatment planning in thoracic diseases; however, it is limited\nby radiation dose and hardware costs. We present the Transformer Volumetric\nSuper-Resolution Network (\\textbf{TVSRN-V2}), a transformer-based\nsuper-resolution (SR) framework designed for practical deployment in clinical\nlung CT analysis. Built from scalable components, including Through-Plane\nAttention Blocks (TAB) and Swin Transformer V2 -- our model effectively\nreconstructs fine anatomical details in low-dose CT volumes and integrates\nseamlessly with downstream analysis pipelines. We evaluate its effectiveness on\nthree critical lung cancer tasks -- lobe segmentation, radiomics, and prognosis\n-- across multiple clinical cohorts. To enhance robustness across variable\nacquisition protocols, we introduce pseudo-low-resolution augmentation,\nsimulating scanner diversity without requiring private data. TVSRN-V2\ndemonstrates a significant improvement in segmentation accuracy (+4\\% Dice),\nhigher radiomic feature reproducibility, and enhanced predictive performance\n(+0.06 C-index and AUC). These results indicate that SR-driven recovery of\nstructural detail significantly enhances clinical decision support, positioning\nTVSRN-V2 as a well-engineered, clinically viable system for dose-efficient\nimaging and quantitative analysis in real-world CT workflows.", "AI": {"tldr": "TVSRN-V2, a transformer-based super-resolution network, improves lung CT analysis by enhancing resolution in low-dose scans, boosting segmentation, radiomics, and prognosis accuracy.", "motivation": "High-resolution CT is crucial for thoracic disease diagnosis but limited by radiation dose and costs. TVSRN-V2 aims to address this by enhancing low-dose CT scans.", "method": "Uses Through-Plane Attention Blocks (TAB) and Swin Transformer V2 for super-resolution, with pseudo-low-resolution augmentation for robustness across protocols.", "result": "Improves segmentation accuracy (+4% Dice), radiomic feature reproducibility, and predictive performance (+0.06 C-index and AUC).", "conclusion": "TVSRN-V2 is a clinically viable solution for dose-efficient CT imaging, enhancing clinical decision support."}}
{"id": "2507.15361", "pdf": "https://arxiv.org/pdf/2507.15361", "abs": "https://arxiv.org/abs/2507.15361", "authors": ["Muhammad Aqeel", "Maham Nazir", "Zanxi Ruan", "Francesco Setti"], "title": "Latent Space Synergy: Text-Guided Data Augmentation for Direct Diffusion Biomedical Segmentation", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "Accepted to CVGMMI Workshop at ICIAP 2025", "summary": "Medical image segmentation suffers from data scarcity, particularly in polyp\ndetection where annotation requires specialized expertise. We present SynDiff,\na framework combining text-guided synthetic data generation with efficient\ndiffusion-based segmentation. Our approach employs latent diffusion models to\ngenerate clinically realistic synthetic polyps through text-conditioned\ninpainting, augmenting limited training data with semantically diverse samples.\nUnlike traditional diffusion methods requiring iterative denoising, we\nintroduce direct latent estimation enabling single-step inference with T x\ncomputational speedup. On CVC-ClinicDB, SynDiff achieves 96.0% Dice and 92.9%\nIoU while maintaining real-time capability suitable for clinical deployment.\nThe framework demonstrates that controlled synthetic augmentation improves\nsegmentation robustness without distribution shift. SynDiff bridges the gap\nbetween data-hungry deep learning models and clinical constraints, offering an\nefficient solution for deployment in resourcelimited medical settings.", "AI": {"tldr": "SynDiff combines text-guided synthetic data generation and efficient diffusion-based segmentation to address data scarcity in medical image segmentation, achieving high accuracy and real-time performance.", "motivation": "Medical image segmentation, especially polyp detection, faces data scarcity due to the need for specialized annotation expertise. SynDiff aims to bridge this gap by generating realistic synthetic data.", "method": "The framework uses latent diffusion models for text-conditioned inpainting to create diverse synthetic polyps. It introduces direct latent estimation for single-step inference, reducing computational overhead.", "result": "SynDiff achieves 96.0% Dice and 92.9% IoU on CVC-ClinicDB, with real-time capability suitable for clinical use.", "conclusion": "SynDiff effectively improves segmentation robustness through controlled synthetic augmentation, offering a practical solution for resource-limited medical settings."}}
{"id": "2507.15381", "pdf": "https://arxiv.org/pdf/2507.15381", "abs": "https://arxiv.org/abs/2507.15381", "authors": ["Julia Machnio", "Mads Nielsen", "Mostafa Mehdipour Ghazi"], "title": "To Label or Not to Label: PALM -- A Predictive Model for Evaluating Sample Efficiency in Active Learning Models", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "ICCV 2025", "summary": "Active learning (AL) seeks to reduce annotation costs by selecting the most\ninformative samples for labeling, making it particularly valuable in\nresource-constrained settings. However, traditional evaluation methods, which\nfocus solely on final accuracy, fail to capture the full dynamics of the\nlearning process. To address this gap, we propose PALM (Performance Analysis of\nActive Learning Models), a unified and interpretable mathematical model that\ncharacterizes AL trajectories through four key parameters: achievable accuracy,\ncoverage efficiency, early-stage performance, and scalability. PALM provides a\npredictive description of AL behavior from partial observations, enabling the\nestimation of future performance and facilitating principled comparisons across\ndifferent strategies. We validate PALM through extensive experiments on\nCIFAR-10/100 and ImageNet-50/100/200, covering a wide range of AL methods and\nself-supervised embeddings. Our results demonstrate that PALM generalizes\neffectively across datasets, budgets, and strategies, accurately predicting\nfull learning curves from limited labeled data. Importantly, PALM reveals\ncrucial insights into learning efficiency, data space coverage, and the\nscalability of AL methods. By enabling the selection of cost-effective\nstrategies and predicting performance under tight budget constraints, PALM lays\nthe basis for more systematic, reproducible, and data-efficient evaluation of\nAL in both research and real-world applications. The code is available at:\nhttps://github.com/juliamachnio/PALM.", "AI": {"tldr": "PALM is a unified model for analyzing active learning (AL) trajectories, predicting performance, and comparing strategies using four key parameters.", "motivation": "Traditional AL evaluations focus only on final accuracy, missing the dynamics of the learning process. PALM addresses this gap.", "method": "PALM uses four parameters (achievable accuracy, coverage efficiency, early-stage performance, scalability) to model AL behavior and predict future performance.", "result": "Validated on CIFAR and ImageNet datasets, PALM generalizes well, predicts learning curves, and provides insights into AL efficiency and scalability.", "conclusion": "PALM enables systematic, reproducible, and cost-effective AL evaluation, aiding research and real-world applications."}}
{"id": "2507.15399", "pdf": "https://arxiv.org/pdf/2507.15399", "abs": "https://arxiv.org/abs/2507.15399", "authors": ["Etai Sella", "Noam Atia", "Ron Mokady", "Hadar Averbuch-Elor"], "title": "Blended Point Cloud Diffusion for Localized Text-guided Shape Editing", "categories": ["cs.GR", "cs.CV"], "comment": "Accepted to ICCV 2025. Project Page:\n  https://tau-vailab.github.io/BlendedPC/", "summary": "Natural language offers a highly intuitive interface for enabling localized\nfine-grained edits of 3D shapes. However, prior works face challenges in\npreserving global coherence while locally modifying the input 3D shape. In this\nwork, we introduce an inpainting-based framework for editing shapes represented\nas point clouds. Our approach leverages foundation 3D diffusion models for\nachieving localized shape edits, adding structural guidance in the form of a\npartial conditional shape, ensuring that other regions correctly preserve the\nshape's identity. Furthermore, to encourage identity preservation also within\nthe local edited region, we propose an inference-time coordinate blending\nalgorithm which balances reconstruction of the full shape with inpainting at a\nprogression of noise levels during the inference process. Our coordinate\nblending algorithm seamlessly blends the original shape with its edited\nversion, enabling a fine-grained editing of 3D shapes, all while circumventing\nthe need for computationally expensive and often inaccurate inversion.\nExtensive experiments show that our method outperforms alternative techniques\nacross a wide range of metrics that evaluate both fidelity to the original\nshape and also adherence to the textual description.", "AI": {"tldr": "An inpainting-based framework for editing 3D point cloud shapes using natural language, ensuring global coherence and local fidelity via a coordinate blending algorithm.", "motivation": "Prior methods struggle with preserving global coherence while making localized edits to 3D shapes.", "method": "Uses foundation 3D diffusion models and a partial conditional shape for guidance, along with an inference-time coordinate blending algorithm.", "result": "Outperforms alternatives in fidelity to the original shape and adherence to textual descriptions.", "conclusion": "The proposed method enables fine-grained 3D shape editing while preserving identity and coherence."}}
{"id": "2507.15444", "pdf": "https://arxiv.org/pdf/2507.15444", "abs": "https://arxiv.org/abs/2507.15444", "authors": ["Leonard Bauersfeld", "Davide Scaramuzza"], "title": "Low-Latency Event-Based Velocimetry for Quadrotor Control in a Narrow Pipe", "categories": ["cs.RO", "cs.CV"], "comment": "17 pages", "summary": "Autonomous quadrotor flight in confined spaces such as pipes and tunnels\npresents significant challenges due to unsteady, self-induced aerodynamic\ndisturbances. Very recent advances have enabled flight in such conditions, but\nthey either rely on constant motion through the pipe to mitigate airflow\nrecirculation effects or suffer from limited stability during hovering. In this\nwork, we present the first closed-loop control system for quadrotors for\nhovering in narrow pipes that leverages real-time flow field measurements. We\ndevelop a low-latency, event-based smoke velocimetry method that estimates\nlocal airflow at high temporal resolution. This flow information is used by a\ndisturbance estimator based on a recurrent convolutional neural network, which\ninfers force and torque disturbances in real time. The estimated disturbances\nare integrated into a learning-based controller trained via reinforcement\nlearning. The flow-feedback control proves particularly effective during\nlateral translation maneuvers in the pipe cross-section. There, the real-time\ndisturbance information enables the controller to effectively counteract\ntransient aerodynamic effects, thereby preventing collisions with the pipe\nwall. To the best of our knowledge, this work represents the first\ndemonstration of an aerial robot with closed-loop control informed by real-time\nflow field measurements. This opens new directions for research on flight in\naerodynamically complex environments. In addition, our work also sheds light on\nthe characteristic flow structures that emerge during flight in narrow,\ncircular pipes, providing new insights at the intersection of robotics and\nfluid dynamics.", "AI": {"tldr": "A closed-loop control system for quadrotors hovering in narrow pipes uses real-time airflow measurements and a learning-based controller to counteract aerodynamic disturbances, enabling stable flight.", "motivation": "Autonomous quadrotor flight in confined spaces like pipes is challenging due to unsteady airflow. Existing methods either require constant motion or lack stability during hovering.", "method": "Developed a low-latency, event-based smoke velocimetry for airflow measurement, a disturbance estimator using a recurrent CNN, and a reinforcement learning-trained controller.", "result": "The system effectively counters aerodynamic disturbances during lateral maneuvers, preventing collisions with pipe walls.", "conclusion": "This is the first demonstration of real-time flow-feedback control for aerial robots, advancing research in aerodynamically complex environments and providing insights into fluid dynamics."}}
{"id": "2507.15454", "pdf": "https://arxiv.org/pdf/2507.15454", "abs": "https://arxiv.org/abs/2507.15454", "authors": ["Ruijie Zhu", "Mulin Yu", "Linning Xu", "Lihan Jiang", "Yixuan Li", "Tianzhu Zhang", "Jiangmiao Pang", "Bo Dai"], "title": "ObjectGS: Object-aware Scene Reconstruction and Scene Understanding via Gaussian Splatting", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.HC"], "comment": "Accepted by ICCV 2025", "summary": "3D Gaussian Splatting is renowned for its high-fidelity reconstructions and\nreal-time novel view synthesis, yet its lack of semantic understanding limits\nobject-level perception. In this work, we propose ObjectGS, an object-aware\nframework that unifies 3D scene reconstruction with semantic understanding.\nInstead of treating the scene as a unified whole, ObjectGS models individual\nobjects as local anchors that generate neural Gaussians and share object IDs,\nenabling precise object-level reconstruction. During training, we dynamically\ngrow or prune these anchors and optimize their features, while a one-hot ID\nencoding with a classification loss enforces clear semantic constraints. We\nshow through extensive experiments that ObjectGS not only outperforms\nstate-of-the-art methods on open-vocabulary and panoptic segmentation tasks,\nbut also integrates seamlessly with applications like mesh extraction and scene\nediting. Project page: https://ruijiezhu94.github.io/ObjectGS_page", "AI": {"tldr": "ObjectGS enhances 3D Gaussian Splatting by adding semantic understanding, enabling object-level reconstruction and outperforming state-of-the-art methods in segmentation tasks.", "motivation": "3D Gaussian Splatting lacks semantic understanding, limiting object-level perception. ObjectGS addresses this by unifying 3D reconstruction with semantic awareness.", "method": "ObjectGS models objects as local anchors with neural Gaussians and shared IDs, dynamically adjusting anchors during training and using ID encoding for semantic constraints.", "result": "ObjectGS excels in open-vocabulary and panoptic segmentation and integrates well with applications like mesh extraction and scene editing.", "conclusion": "ObjectGS successfully combines 3D reconstruction with semantic understanding, offering improved performance and practical applications."}}
{"id": "2507.15476", "pdf": "https://arxiv.org/pdf/2507.15476", "abs": "https://arxiv.org/abs/2507.15476", "authors": ["Cong Chen", "Ming Chen", "Hoileong Lee", "Yan Li", "Jiyang Yu"], "title": "A Steel Surface Defect Detection Method Based on Lightweight Convolution Optimization", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Surface defect detection of steel, especially the recognition of multi-scale\ndefects, has always been a major challenge in industrial manufacturing. Steel\nsurfaces not only have defects of various sizes and shapes, which limit the\naccuracy of traditional image processing and detection methods in complex\nenvironments. However, traditional defect detection methods face issues of\ninsufficient accuracy and high miss-detection rates when dealing with small\ntarget defects. To address this issue, this study proposes a detection\nframework based on deep learning, specifically YOLOv9s, combined with the\nC3Ghost module, SCConv module, and CARAFE upsampling operator, to improve\ndetection accuracy and model performance. First, the SCConv module is used to\nreduce feature redundancy and optimize feature representation by reconstructing\nthe spatial and channel dimensions. Second, the C3Ghost module is introduced to\nenhance the model's feature extraction ability by reducing redundant\ncomputations and parameter volume, thereby improving model efficiency. Finally,\nthe CARAFE upsampling operator, which can more finely reorganize feature maps\nin a content-aware manner, optimizes the upsampling process and ensures\ndetailed restoration of high-resolution defect regions. Experimental results\ndemonstrate that the proposed model achieves higher accuracy and robustness in\nsteel surface defect detection tasks compared to other methods, effectively\naddressing defect detection problems.", "AI": {"tldr": "A deep learning framework (YOLOv9s with C3Ghost, SCConv, and CARAFE) improves steel surface defect detection accuracy and efficiency.", "motivation": "Traditional methods struggle with multi-scale defect detection due to insufficient accuracy and high miss-detection rates.", "method": "Combines YOLOv9s with SCConv (reduces feature redundancy), C3Ghost (enhances feature extraction), and CARAFE (optimizes upsampling).", "result": "Higher accuracy and robustness in defect detection compared to other methods.", "conclusion": "The proposed framework effectively addresses challenges in steel surface defect detection."}}
{"id": "2507.15487", "pdf": "https://arxiv.org/pdf/2507.15487", "abs": "https://arxiv.org/abs/2507.15487", "authors": ["Dezhen Wang", "Sheng Miao", "Rongxin Chai", "Jiufa Cui"], "title": "DeSamba: Decoupled Spectral Adaptive Framework for 3D Multi-Sequence MRI Lesion Classification", "categories": ["eess.IV", "cs.CV"], "comment": "7 figures, 3 tables, submitted to AAAI2026", "summary": "Magnetic Resonance Imaging (MRI) sequences provide rich spatial and frequency\ndomain information, which is crucial for accurate lesion classification in\nmedical imaging. However, effectively integrating multi-sequence MRI data for\nrobust 3D lesion classification remains a challenge. In this paper, we propose\nDeSamba (Decoupled Spectral Adaptive Network and Mamba-Based Model), a novel\nframework designed to extract decoupled representations and adaptively fuse\nspatial and spectral features for lesion classification. DeSamba introduces a\nDecoupled Representation Learning Module (DRLM) that decouples features from\ndifferent MRI sequences through self-reconstruction and cross-reconstruction,\nand a Spectral Adaptive Modulation Block (SAMB) within the proposed SAMNet,\nenabling dynamic fusion of spectral and spatial information based on lesion\ncharacteristics. We evaluate DeSamba on two clinically relevant 3D datasets. On\na six-class spinal metastasis dataset (n=1,448), DeSamba achieves 62.10% Top-1\naccuracy, 63.62% F1-score, 87.71% AUC, and 93.55% Top-3 accuracy on an external\nvalidation set (n=372), outperforming all state-of-the-art (SOTA) baselines. On\na spondylitis dataset (n=251) involving a challenging binary classification\ntask, DeSamba achieves 70.00%/64.52% accuracy and 74.75/73.88 AUC on internal\nand external validation sets, respectively. Ablation studies demonstrate that\nboth DRLM and SAMB significantly contribute to overall performance, with over\n10% relative improvement compared to the baseline. Our results highlight the\npotential of DeSamba as a generalizable and effective solution for 3D lesion\nclassification in multi-sequence medical imaging.", "AI": {"tldr": "DeSamba is a novel framework for 3D lesion classification in multi-sequence MRI, combining decoupled representation learning and adaptive spectral-spatial feature fusion, outperforming SOTA methods.", "motivation": "Effective integration of multi-sequence MRI data for robust 3D lesion classification is challenging.", "method": "DeSamba uses a Decoupled Representation Learning Module (DRLM) and Spectral Adaptive Modulation Block (SAMB) for feature decoupling and adaptive fusion.", "result": "Achieves 62.10% Top-1 accuracy on spinal metastasis and 70.00%/64.52% accuracy on spondylitis datasets, outperforming baselines.", "conclusion": "DeSamba is a generalizable and effective solution for 3D lesion classification in medical imaging."}}
{"id": "2507.15491", "pdf": "https://arxiv.org/pdf/2507.15491", "abs": "https://arxiv.org/abs/2507.15491", "authors": ["Deyu Zhang", "Tingting Long", "Jinrui Zhang", "Ligeng Chen", "Ju Ren", "Yaoxue Zhang"], "title": "Prompt-aware of Frame Sampling for Efficient Text-Video Retrieval", "categories": ["cs.MM", "cs.CV"], "comment": null, "summary": "Enabling efficient text-video retrieval on edge-end devices is critical for\nreal-world applications. Yet, existing methods face a critical challenge in\nbalancing accuracy and computational efficiency: uniform frame sampling methods\nensure content coverage but incur prohibitive computational costs, while\nsalient-frame sampling methods reduce overhead but suffer from query-agnostic\nframe selection that biases retrieval results. To address this, we propose\nProCLIP, a user-centric framework that achieves state-of-the-art accuracy with\nsignificantly improved efficiency. We design a prompt-aware frame sampling\nstrategy that dynamically guides lightweight feature extractors using textual\nprompts to select semantically relevant frames, overcoming the limitations of\nexisting salient-frame sampling methods which rely on static, query-agnostic\nselection criteria. Moreover, we adopt a two-stage candidate pruning strategy\nthat combines rapid coarse filtering via a lightweight module with CLIP-powered\nfine-grained re-ranking, enhancing retrieval efficiency while preserving\naccuracy. Experiments across benchmarks show ProCLIP achieves 75.3% latency\nreduction versus baselines while maintaining competitive accuracy, i.e.,\nR@1=49.0 in MSR-VTT dataset. Code is available at\nhttps://github.com/tiffylong/ProCLIP.", "AI": {"tldr": "ProCLIP is a user-centric framework for efficient text-video retrieval, balancing accuracy and computational efficiency via prompt-aware frame sampling and two-stage pruning.", "motivation": "Existing methods struggle to balance accuracy and efficiency in text-video retrieval, with uniform frame sampling being computationally costly and salient-frame sampling being query-agnostic.", "method": "ProCLIP uses prompt-aware frame sampling to dynamically select relevant frames and a two-stage pruning strategy (coarse filtering + CLIP-powered re-ranking) for efficiency.", "result": "ProCLIP reduces latency by 75.3% while maintaining competitive accuracy (R@1=49.0 on MSR-VTT).", "conclusion": "ProCLIP offers a practical solution for efficient text-video retrieval on edge devices, outperforming existing methods in both speed and accuracy."}}
{"id": "2507.15493", "pdf": "https://arxiv.org/pdf/2507.15493", "abs": "https://arxiv.org/abs/2507.15493", "authors": ["Chilam Cheang", "Sijin Chen", "Zhongren Cui", "Yingdong Hu", "Liqun Huang", "Tao Kong", "Hang Li", "Yifeng Li", "Yuxiao Liu", "Xiao Ma", "Hao Niu", "Wenxuan Ou", "Wanli Peng", "Zeyu Ren", "Haixin Shi", "Jiawen Tian", "Hongtao Wu", "Xin Xiao", "Yuyang Xiao", "Jiafeng Xu", "Yichu Yang"], "title": "GR-3 Technical Report", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "Tech report. Authors are listed in alphabetical order. Project page:\n  https://seed.bytedance.com/GR3/", "summary": "We report our recent progress towards building generalist robot policies, the\ndevelopment of GR-3. GR-3 is a large-scale vision-language-action (VLA) model.\nIt showcases exceptional capabilities in generalizing to novel objects,\nenvironments, and instructions involving abstract concepts. Furthermore, it can\nbe efficiently fine-tuned with minimal human trajectory data, enabling rapid\nand cost-effective adaptation to new settings. GR-3 also excels in handling\nlong-horizon and dexterous tasks, including those requiring bi-manual\nmanipulation and mobile movement, showcasing robust and reliable performance.\nThese capabilities are achieved through a multi-faceted training recipe that\nincludes co-training with web-scale vision-language data, efficient fine-tuning\nfrom human trajectory data collected via VR devices, and effective imitation\nlearning with robot trajectory data. In addition, we introduce ByteMini, a\nversatile bi-manual mobile robot designed with exceptional flexibility and\nreliability, capable of accomplishing a wide range of tasks when integrated\nwith GR-3. Through extensive real-world experiments, we show GR-3 surpasses the\nstate-of-the-art baseline method, $\\pi_0$, on a wide variety of challenging\ntasks. We hope GR-3 can serve as a step towards building generalist robots\ncapable of assisting humans in daily life.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.15509", "pdf": "https://arxiv.org/pdf/2507.15509", "abs": "https://arxiv.org/abs/2507.15509", "authors": ["Lei Chen", "Xuanle Zhao", "Zhixiong Zeng", "Jing Huang", "Yufeng Zhong", "Lin Ma"], "title": "Chart-R1: Chain-of-Thought Supervision and Reinforcement for Advanced Chart Reasoner", "categories": ["cs.AI", "cs.CV"], "comment": "technical report", "summary": "Recently, inspired by OpenAI-o1/o3 and Deepseek-R1, the R1-Style method based\non reinforcement learning fine-tuning has received widespread attention from\nthe community. Previous R1-Style methods mainly focus on mathematical reasoning\nand code intelligence. It is of great research significance to verify their\nadvantages on more general multimodal data. Chart is an important multimodal\ndata type with rich information, which brings important research challenges in\ncomplex reasoning. In this work, we introduce Chart-R1, a chart-domain\nvision-language model with reinforcement learning fine-tuning to enable complex\nchart reasoning. To support Chart-R1, we first propose a novel programmatic\ndata synthesis technology to generate high-quality step-by-step chart reasoning\ndata covering single- and multi-subcharts, which makes up for the lack of\nreasoning data in the chart domain. Then we develop a two-stage training\nstrategy: Chart-COT with step-by-step chain-of-thought supervision, and\nChart-RFT with numerically sensitive reinforcement fine-tuning. Chart-COT aims\nto decompose complex chart reasoning tasks into fine-grained, understandable\nsubtasks through step-by-step supervision, which lays a good foundation for\nimproving the reasoning level of reinforcement learning. Chart-RFT utilize the\ntypical group relative policy optimization strategy, in which a relatively soft\nreward is adopted for numerical response to emphasize the numerical sensitivity\nin the chart domain. We conduct extensive experiments on open-source benchmarks\nand self-built chart reasoning dataset (\\emph{i.e., ChartRQA}). Experimental\nresults show that Chart-R1 has significant advantages compared to chart-domain\nmethods, even comparable to open/closed source large-scale models (\\emph{e.g.,\nGPT-4o, Claude-3.5}).", "AI": {"tldr": "Chart-R1 introduces a chart-domain vision-language model with reinforcement learning fine-tuning for complex reasoning, supported by programmatic data synthesis and a two-stage training strategy.", "motivation": "To extend R1-Style methods beyond mathematical reasoning and code intelligence to multimodal data like charts, addressing the lack of reasoning data in this domain.", "method": "Uses programmatic data synthesis for high-quality reasoning data and a two-stage training strategy: Chart-COT for step-by-step supervision and Chart-RFT for numerically sensitive reinforcement fine-tuning.", "result": "Chart-R1 outperforms chart-domain methods and competes with large-scale models like GPT-4o and Claude-3.5.", "conclusion": "Chart-R1 successfully advances chart reasoning, demonstrating the potential of reinforcement learning fine-tuning in multimodal domains."}}
{"id": "2507.15524", "pdf": "https://arxiv.org/pdf/2507.15524", "abs": "https://arxiv.org/abs/2507.15524", "authors": ["Simon Winther Albertsen", "Hjalte Svaneborg Bj\u00f8rnstrup", "Mostafa Mehdipour Ghazi"], "title": "RARE-UNet: Resolution-Aligned Routing Entry for Adaptive Medical Image Segmentation", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "EMA4MICCAI 2025", "summary": "Accurate segmentation is crucial for clinical applications, but existing\nmodels often assume fixed, high-resolution inputs and degrade significantly\nwhen faced with lower-resolution data in real-world scenarios. To address this\nlimitation, we propose RARE-UNet, a resolution-aware multi-scale segmentation\narchitecture that dynamically adapts its inference path to the spatial\nresolution of the input. Central to our design are multi-scale blocks\nintegrated at multiple encoder depths, a resolution-aware routing mechanism,\nand consistency-driven training that aligns multi-resolution features with\nfull-resolution representations. We evaluate RARE-UNet on two benchmark brain\nimaging tasks for hippocampus and tumor segmentation. Compared to standard\nUNet, its multi-resolution augmented variant, and nnUNet, our model achieves\nthe highest average Dice scores of 0.84 and 0.65 across resolution, while\nmaintaining consistent performance and significantly reduced inference time at\nlower resolutions. These results highlight the effectiveness and scalability of\nour architecture in achieving resolution-robust segmentation. The codes are\navailable at: https://github.com/simonsejse/RARE-UNet.", "AI": {"tldr": "RARE-UNet is a resolution-aware multi-scale segmentation model that dynamically adapts to input resolution, outperforming standard models in accuracy and efficiency.", "motivation": "Existing segmentation models degrade with lower-resolution inputs, limiting real-world clinical applications.", "method": "Proposes RARE-UNet with multi-scale blocks, resolution-aware routing, and consistency-driven training for dynamic adaptation.", "result": "Achieves highest Dice scores (0.84, 0.65) for hippocampus and tumor segmentation, with reduced inference time at lower resolutions.", "conclusion": "RARE-UNet is effective and scalable for resolution-robust segmentation, with code available for public use."}}
{"id": "2507.15629", "pdf": "https://arxiv.org/pdf/2507.15629", "abs": "https://arxiv.org/abs/2507.15629", "authors": ["Zuo-Liang Zhu", "Jian Yang", "Beibei Wang"], "title": "Gaussian Splatting with Discretized SDF for Relightable Assets", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "3D Gaussian splatting (3DGS) has shown its detailed expressive ability and\nhighly efficient rendering speed in the novel view synthesis (NVS) task. The\napplication to inverse rendering still faces several challenges, as the\ndiscrete nature of Gaussian primitives makes it difficult to apply geometry\nconstraints. Recent works introduce the signed distance field (SDF) as an extra\ncontinuous representation to regularize the geometry defined by Gaussian\nprimitives. It improves the decomposition quality, at the cost of increasing\nmemory usage and complicating training. Unlike these works, we introduce a\ndiscretized SDF to represent the continuous SDF in a discrete manner by\nencoding it within each Gaussian using a sampled value. This approach allows us\nto link the SDF with the Gaussian opacity through an SDF-to-opacity\ntransformation, enabling rendering the SDF via splatting and avoiding the\ncomputational cost of ray marching.The key challenge is to regularize the\ndiscrete samples to be consistent with the underlying SDF, as the discrete\nrepresentation can hardly apply the gradient-based constraints (\\eg Eikonal\nloss). For this, we project Gaussians onto the zero-level set of SDF and\nenforce alignment with the surface from splatting, namely a projection-based\nconsistency loss. Thanks to the discretized SDF, our method achieves higher\nrelighting quality, while requiring no extra memory beyond GS and avoiding\ncomplex manually designed optimization. The experiments reveal that our method\noutperforms existing Gaussian-based inverse rendering methods. Our code is\navailable at https://github.com/NK-CS-ZZL/DiscretizedSDF.", "AI": {"tldr": "The paper introduces a discretized SDF method for 3D Gaussian splatting to improve inverse rendering quality without extra memory or complex training.", "motivation": "The discrete nature of Gaussian primitives in 3DGS complicates geometry constraints for inverse rendering, prompting the need for a simpler, memory-efficient solution.", "method": "A discretized SDF is encoded within each Gaussian, linking SDF to opacity via transformation, and a projection-based consistency loss regularizes the discrete samples.", "result": "The method achieves higher relighting quality, outperforms existing Gaussian-based inverse rendering methods, and avoids extra memory or complex optimization.", "conclusion": "Discretized SDF within 3DGS offers a practical solution for inverse rendering, balancing quality and efficiency."}}
{"id": "2507.15833", "pdf": "https://arxiv.org/pdf/2507.15833", "abs": "https://arxiv.org/abs/2507.15833", "authors": ["Ian Chuang", "Andrew Lee", "Dechen Gao", "Jinyu Zou", "Iman Soltani"], "title": "Look, Focus, Act: Efficient and Robust Robot Learning via Human Gaze and Foveated Vision Transformers", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "13 pages, 10 figures", "summary": "Human vision is a highly active process driven by gaze, which directs\nattention and fixation to task-relevant regions and dramatically reduces visual\nprocessing. In contrast, robot learning systems typically rely on passive,\nuniform processing of raw camera images. In this work, we explore how\nincorporating human-like active gaze into robotic policies can enhance both\nefficiency and performance. We build on recent advances in foveated image\nprocessing and apply them to an Active Vision robot system that emulates both\nhuman head movement and eye tracking. Extending prior work on the AV-ALOHA\nrobot simulation platform, we introduce a framework for simultaneously\ncollecting eye-tracking data and robot demonstrations from a human operator as\nwell as a simulation benchmark and dataset for training robot policies that\nincorporate human gaze. Given the widespread use of Vision Transformers (ViTs)\nin robot learning, we integrate gaze information into ViTs using a foveated\npatch tokenization scheme inspired by recent work in image segmentation.\nCompared to uniform patch tokenization, this significantly reduces the number\nof tokens-and thus computation-without sacrificing visual fidelity near regions\nof interest. We also explore two approaches to gaze imitation and prediction\nfrom human data. The first is a two-stage model that predicts gaze to guide\nfoveation and action; the second integrates gaze into the action space,\nallowing the policy to jointly predict gaze and actions end-to-end. Our results\nshow that our method for foveated robot vision not only drastically reduces\ncomputational overhead, but also improves performance for high precision tasks\nand robustness to unseen distractors. Together, these findings suggest that\nhuman-inspired visual processing offers a useful inductive bias for robotic\nvision systems. https://ian-chuang.github.io/gaze-av-aloha/", "AI": {"tldr": "The paper explores integrating human-like active gaze into robotic policies to improve efficiency and performance, using foveated image processing and Vision Transformers (ViTs).", "motivation": "Human vision is task-driven and efficient, while robotic systems often process images uniformly. The work aims to bridge this gap by emulating human gaze for better robotic vision.", "method": "The authors introduce a framework for collecting eye-tracking and robot demonstration data, integrate gaze into ViTs via foveated patch tokenization, and explore two gaze imitation/prediction approaches.", "result": "Foveated robot vision reduces computational overhead and enhances performance in precision tasks and robustness to distractors.", "conclusion": "Human-inspired visual processing provides a valuable inductive bias for robotic vision systems."}}
{"id": "2507.15857", "pdf": "https://arxiv.org/pdf/2507.15857", "abs": "https://arxiv.org/abs/2507.15857", "authors": ["Mihir Prabhudesai", "Menging Wu", "Amir Zadeh", "Katerina Fragkiadaki", "Deepak Pathak"], "title": "Diffusion Beats Autoregressive in Data-Constrained Settings", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.RO"], "comment": "Project Webpage: https://diffusion-scaling.github.io", "summary": "Autoregressive (AR) models have long dominated the landscape of large\nlanguage models, driving progress across a wide range of tasks. Recently,\ndiffusion-based language models have emerged as a promising alternative, though\ntheir advantages over AR models remain underexplored. In this paper, we\nsystematically study masked diffusion models in data-constrained settings-where\ntraining involves repeated passes over limited data-and find that they\nsignificantly outperform AR models when compute is abundant but data is scarce.\nDiffusion models make better use of repeated data, achieving lower validation\nloss and superior downstream performance. We interpret this advantage as\nimplicit data augmentation: masked diffusion exposes the model to a diverse\ndistribution of token orderings and prediction tasks, unlike AR's fixed\nleft-to-right factorization. We find new scaling laws for diffusion models and\nderive a closed-form expression for the critical compute threshold at which\ndiffusion begins to outperform AR. These results suggest that when data, not\ncompute, is the bottleneck, diffusion models offer a compelling alternative to\nthe standard AR paradigm. Our code is available at:\nhttps://diffusion-scaling.github.io.", "AI": {"tldr": "Diffusion models outperform autoregressive (AR) models in data-scarce settings due to better utilization of repeated data and implicit data augmentation.", "motivation": "To explore the advantages of diffusion-based language models over AR models, especially in data-constrained scenarios.", "method": "Systematic study of masked diffusion models in data-constrained settings, comparing their performance with AR models.", "result": "Diffusion models achieve lower validation loss and superior downstream performance when compute is abundant but data is scarce.", "conclusion": "Diffusion models are a compelling alternative to AR models when data is the bottleneck, not compute."}}

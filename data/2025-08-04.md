<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 45]
- [cs.CV](#cs.CV) [Total: 89]
- [cs.AI](#cs.AI) [Total: 6]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.LG](#cs.LG) [Total: 4]
- [eess.IV](#eess.IV) [Total: 6]
- [cs.GR](#cs.GR) [Total: 2]
- [hep-ph](#hep-ph) [Total: 1]
- [q-fin.TR](#q-fin.TR) [Total: 1]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.RO](#cs.RO) [Total: 3]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.SE](#cs.SE) [Total: 3]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.PL](#cs.PL) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [PhysicsEval: Inference-Time Techniques to Improve the Reasoning Proficiency of Large Language Models on Physics Problems](https://arxiv.org/abs/2508.00079)
*Oshayer Siddique,J. M Areeb Uzair Alam,Md Jobayer Rahman Rafy,Syed Rifat Raiyan,Hasan Mahmud,Md Kamrul Hasan*

Main category: cs.CL

TL;DR: The paper evaluates frontier LLMs in solving physics problems, using multi-agent frameworks and inference-time techniques to improve performance, and introduces a new benchmark, ${\rm P{\small HYSICS}E{\small VAL}}$.


<details>
  <summary>Details</summary>
Motivation: To assess and enhance the capability of LLMs in solving physics problems, both mathematical and descriptive, and to provide a standardized benchmark for future research.

Method: Employing inference-time techniques and multi-agent frameworks, including solution verification by smaller LLM agents, and comparative analysis of performance improvements.

Result: Significant performance improvements, especially for initially poorly solved problems, using the multi-agent framework.

Conclusion: The multi-agent approach enhances LLM performance in physics problem-solving, and the new benchmark, ${\rm P{\small HYSICS}E{\small VAL}}$, provides a valuable resource for future evaluations.

Abstract: The discipline of physics stands as a cornerstone of human intellect, driving
the evolution of technology and deepening our understanding of the fundamental
principles of the cosmos. Contemporary literature includes some works centered
on the task of solving physics problems - a crucial domain of natural language
reasoning. In this paper, we evaluate the performance of frontier LLMs in
solving physics problems, both mathematical and descriptive. We also employ a
plethora of inference-time techniques and agentic frameworks to improve the
performance of the models. This includes the verification of proposed solutions
in a cumulative fashion by other, smaller LLM agents, and we perform a
comparative analysis of the performance that the techniques entail. There are
significant improvements when the multi-agent framework is applied to problems
that the models initially perform poorly on. Furthermore, we introduce a new
evaluation benchmark for physics problems, ${\rm P{\small HYSICS}E{\small
VAL}}$, consisting of 19,609 problems sourced from various physics textbooks
and their corresponding correct solutions scraped from physics forums and
educational websites. Our code and data are publicly available at
https://github.com/areebuzair/PhysicsEval.

</details>


### [2] [Do LLMs produce texts with "human-like" lexical diversity?](https://arxiv.org/abs/2508.00086)
*Kelly Kendro,Jeffrey Maloney,Scott Jarvis*

Main category: cs.CL

TL;DR: The study compares lexical diversity in texts from four ChatGPT models and human writers, finding significant differences, with newer models being less human-like.


<details>
  <summary>Details</summary>
Motivation: To assess how human-like LLM-generated texts are in terms of lexical diversity.

Method: Analyzed six lexical diversity dimensions in texts from ChatGPT models and human participants using statistical methods (MANOVAs, ANOVAs, SVMs).

Result: LLM texts differed significantly from human texts, with newer models (ChatGPT-o4 mini and -4.5) being least human-like. Human writers' diversity was consistent across subgroups.

Conclusion: LLMs do not produce human-like lexical diversity, and newer models are less human-like. Implications for language pedagogy are discussed.

Abstract: The degree to which LLMs produce writing that is truly human-like remains
unclear despite the extensive empirical attention that this question has
received. The present study addresses this question from the perspective of
lexical diversity. Specifically, the study investigates patterns of lexical
diversity in LLM-generated texts from four ChatGPT models (-3.5, -4, -o4 mini,
and -4.5) in comparison with texts written by L1 and L2 English participants (n
= 240) across four education levels. Six dimensions of lexical diversity were
measured in each text: volume, abundance, variety-repetition, evenness,
disparity, and dispersion. Results from one-way MANOVAs, one-way ANOVAS, and
Support Vector Machines revealed that the LLM-generated texts differed
significantly from human-written texts for each variable, with ChatGPT-o4 mini
and -4.5 differing the most. Within these two groups, ChatGPT-4.5 demonstrated
higher levels of lexical diversity despite producing fewer tokens. The human
writers' lexical diversity did not differ across subgroups (i.e., education,
language status). Altogether, the results indicate that LLMs do not produce
human-like texts in relation to lexical diversity, and the newer LLMs produce
less human-like texts than older models. We discuss the implications of these
results for language pedagogy and related applications.

</details>


### [3] [Semiotic Complexity and Its Epistemological Implications for Modeling Culture](https://arxiv.org/abs/2508.00095)
*Zachary K. Stine,James E. Deitrick*

Main category: cs.CL

TL;DR: The paper argues for greater theorizing of methods in computational humanities to improve clarity and avoid translation errors in modeling cultural and linguistic data.


<details>
  <summary>Details</summary>
Motivation: The need for epistemological and interpretive clarity in computational humanities to mature the field and avoid errors in modeling practices.

Method: Framing modeling as translation between cultural/linguistic and computational/mathematical domains, introducing the concept of semiotic complexity.

Result: Identifies a translation error where semiotically complex data is treated as simple, leading to superficial clarity.

Conclusion: Provides recommendations for researchers to better address epistemological issues in their work.

Abstract: Greater theorizing of methods in the computational humanities is needed for
epistemological and interpretive clarity, and therefore the maturation of the
field. In this paper, we frame such modeling work as engaging in translation
work from a cultural, linguistic domain into a computational, mathematical
domain, and back again. Translators benefit from articulating the theory of
their translation process, and so do computational humanists in their work --
to ensure internal consistency, avoid subtle yet consequential translation
errors, and facilitate interpretive transparency. Our contribution in this
paper is to lay out a particularly consequential dimension of the lack of
theorizing and the sorts of translation errors that emerge in our modeling
practices as a result. Along these lines we introduce the idea of semiotic
complexity as the degree to which the meaning of some text may vary across
interpretive lenses, and make the case that dominant modeling practices --
especially around evaluation -- commit a translation error by treating
semiotically complex data as semiotically simple when it seems
epistemologically convenient by conferring superficial clarity. We then lay out
several recommendations for researchers to better account for these
epistemological issues in their own work.

</details>


### [4] [FACTORY: A Challenging Human-Verified Prompt Set for Long-Form Factuality](https://arxiv.org/abs/2508.00109)
*Mingda Chen,Yang Li,Xilun Chen,Adina Williams,Gargi Ghosh,Scott Yih*

Main category: cs.CL

TL;DR: FACTORY is a human-verified benchmark for evaluating long-form factuality in language models, revealing 40% inaccuracies in SOTA models.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack human verification, leading to potential quality issues in evaluating model factuality.

Method: Developed FACTORY using a model-in-the-loop approach with human refinement, featuring challenging, fact-seeking prompts.

Result: 40% of claims by SOTA models were non-factual on FACTORY, compared to 10% on other datasets.

Conclusion: FACTORY is a reliable, challenging benchmark, highlighting the need for models to reason across long-tailed facts.

Abstract: Long-form factuality evaluation assesses the ability of models to generate
accurate, comprehensive responses to short prompts. Existing benchmarks often
lack human verification, leading to potential quality issues. To address this
limitation, we introduce FACTORY, a large-scale, human-verified prompt set.
Developed using a model-in-the-loop approach and refined by humans, FACTORY
includes challenging prompts that are fact-seeking, answerable, and
unambiguous. We conduct human evaluations on 6 state-of-the-art language models
using FACTORY and existing datasets. Our results show that FACTORY is a
challenging benchmark: approximately 40% of the claims made in the responses of
SOTA models are not factual, compared to only 10% for other datasets. Our
analysis identifies the strengths of FACTORY over prior benchmarks, emphasizing
its reliability and the necessity for models to reason across long-tailed
facts.

</details>


### [5] [Is neural semantic parsing good at ellipsis resolution, or isn't it?](https://arxiv.org/abs/2508.00121)
*Xiao Zhang,Johan bos*

Main category: cs.CL

TL;DR: Neural semantic parsers excel in general but struggle with context-sensitive phenomena like verb phrase ellipsis, failing despite high overall performance.


<details>
  <summary>Details</summary>
Motivation: To evaluate neural semantic parsers' ability to handle strongly context-sensitive linguistic phenomena, specifically verb phrase ellipsis.

Method: Constructed a corpus of 120 ellipsis cases with resolved meaning representations and tested various neural semantic parsers.

Result: Parsers performed well on standard tests but failed on ellipsis cases.

Conclusion: Neural semantic parsers are inadequate for context-sensitive phenomena like ellipsis, highlighting a need for improvement.

Abstract: Neural semantic parsers have shown good overall performance for a variety of
linguistic phenomena, reaching semantic matching scores of more than 90%. But
how do such parsers perform on strongly context-sensitive phenomena, where
large pieces of semantic information need to be duplicated to form a meaningful
semantic representation? A case in point is English verb phrase ellipsis, a
construct where entire verb phrases can be abbreviated by a single auxiliary
verb. Are the otherwise known as powerful semantic parsers able to deal with
ellipsis or aren't they? We constructed a corpus of 120 cases of ellipsis with
their fully resolved meaning representation and used this as a challenge set
for a large battery of neural semantic parsers. Although these parsers
performed very well on the standard test set, they failed in the instances with
ellipsis. Data augmentation

</details>


### [6] [Comparison of Large Language Models for Deployment Requirements](https://arxiv.org/abs/2508.00185)
*Alper Yaman,Jannik Schwab,Christof Nitsche,Abhirup Sinha,Marco Huber*

Main category: cs.CL

TL;DR: A comparative list of foundational and domain-specific LLMs is provided to help researchers and companies navigate the evolving landscape, focusing on features like licensing and hardware requirements.


<details>
  <summary>Details</summary>
Motivation: The rapid introduction of numerous open-source LLMs has made it challenging to select the optimal model, necessitating a clear comparison tool.

Method: The paper compiles and compares foundational and domain-specific LLMs, detailing features such as release year, licensing, and hardware requirements.

Result: A continuously updated comparative list of LLMs is published on GitLab.

Conclusion: The list serves as a practical resource for selecting LLMs, addressing the complexity of the current landscape.

Abstract: Large Language Models (LLMs), such as Generative Pre-trained Transformers
(GPTs) are revolutionizing the generation of human-like text, producing
contextually relevant and syntactically correct content. Despite challenges
like biases and hallucinations, these Artificial Intelligence (AI) models excel
in tasks, such as content creation, translation, and code generation.
Fine-tuning and novel architectures, such as Mixture of Experts (MoE), address
these issues. Over the past two years, numerous open-source foundational and
fine-tuned models have been introduced, complicating the selection of the
optimal LLM for researchers and companies regarding licensing and hardware
requirements. To navigate the rapidly evolving LLM landscape and facilitate LLM
selection, we present a comparative list of foundational and domain-specific
models, focusing on features, such as release year, licensing, and hardware
requirements. This list is published on GitLab and will be continuously
updated.

</details>


### [7] [Tabular Data Understanding with LLMs: A Survey of Recent Advances and Challenges](https://arxiv.org/abs/2508.00217)
*Xiaofeng Wu,Alan Ritter,Wei Xu*

Main category: cs.CL

TL;DR: The paper discusses challenges in table understanding tasks for LLMs and MLLMs, proposing a taxonomy of tabular inputs and highlighting gaps like limited reasoning, complex table processing, and generalization issues.


<details>
  <summary>Details</summary>
Motivation: The diversity and complexity of table formats in LLMs and MLLMs necessitate specialized methods, making universal approaches difficult.

Method: Introduces a taxonomy of tabular input representations and table understanding tasks to address these challenges.

Result: Identifies critical gaps: retrieval-focused tasks, processing challenges for complex/large tables, and limited model generalization.

Conclusion: Further research is needed to improve reasoning, handling of complex tables, and generalization across tabular formats.

Abstract: Tables have gained significant attention in large language models (LLMs) and
multimodal large language models (MLLMs) due to their complex and flexible
structure. Unlike linear text inputs, tables are two-dimensional, encompassing
formats that range from well-structured database tables to complex,
multi-layered spreadsheets, each with different purposes. This diversity in
format and purpose has led to the development of specialized methods and tasks,
instead of universal approaches, making navigation of table understanding tasks
challenging. To address these challenges, this paper introduces key concepts
through a taxonomy of tabular input representations and an introduction of
table understanding tasks. We highlight several critical gaps in the field that
indicate the need for further research: (1) the predominance of
retrieval-focused tasks that require minimal reasoning beyond mathematical and
logical operations; (2) significant challenges faced by models when processing
complex table structures, large-scale tables, length context, or multi-table
scenarios; and (3) the limited generalization of models across different
tabular representations and formats.

</details>


### [8] [Semantic Compression for Word and Sentence Embeddings using Discrete Wavelet Transform](https://arxiv.org/abs/2508.00220)
*Rana Aref Salama,Abdou Youssef,Mona Diab*

Main category: cs.CL

TL;DR: The paper explores using Discrete Wavelet Transforms (DWT) on word and sentence embeddings to analyze and compress them while preserving semantic quality, showing significant dimensionality reduction with minimal performance loss.


<details>
  <summary>Details</summary>
Motivation: Wavelet transforms are effective in other domains, and the paper aims to extend their use to NLP for analyzing and compressing embeddings without losing semantic information.

Method: Empirically applies DWT to embeddings, evaluates on semantic similarity tasks, and tests with various embedding models, including large language models.

Result: DWT reduces embedding dimensionality by 50-93% with almost no performance drop in semantic tasks and often improves accuracy in downstream tasks.

Conclusion: DWT is a promising tool for enhancing NLP applications by efficiently compressing and analyzing embeddings.

Abstract: Wavelet transforms, a powerful mathematical tool, have been widely used in
different domains, including Signal and Image processing, to unravel intricate
patterns, enhance data representation, and extract meaningful features from
data. Tangible results from their application suggest that Wavelet transforms
can be applied to NLP capturing a variety of linguistic and semantic
properties. In this paper, we empirically leverage the application of Discrete
Wavelet Transforms (DWT) to word and sentence embeddings. We aim to showcase
the capabilities of DWT in analyzing embedding representations at different
levels of resolution and compressing them while maintaining their overall
quality. We assess the effectiveness of DWT embeddings on semantic similarity
tasks to show how DWT can be used to consolidate important semantic information
in an embedding vector. We show the efficacy of the proposed paradigm using
different embedding models, including large language models, on downstream
tasks. Our results show that DWT can reduce the dimensionality of embeddings by
50-93% with almost no change in performance for semantic similarity tasks,
while achieving superior accuracy in most downstream tasks. Our findings pave
the way for applying DWT to improve NLP applications.

</details>


### [9] [Model Misalignment and Language Change: Traces of AI-Associated Language in Unscripted Spoken English](https://arxiv.org/abs/2508.00238)
*Bryce Anderson,Riley Galpin,Tom S. Juzek*

Main category: cs.CL

TL;DR: The study examines shifts in word usage in spoken language post-ChatGPT's release, finding a moderate increase in LLM-associated words, suggesting AI's influence on human language.


<details>
  <summary>Details</summary>
Motivation: To investigate whether changes in word usage reflect broader shifts in human language due to AI influence or natural evolution.

Method: Analyzed a dataset of 22.1 million words from unscripted spoken language in science/tech podcasts, comparing trends before and after ChatGPT's 2022 release.

Result: Found a significant increase in LLM-associated words post-2022, indicating convergence with AI patterns, while baseline synonyms showed no change.

Conclusion: The findings suggest AI may be influencing human language, raising ethical concerns about misaligned models shaping social norms.

Abstract: In recent years, written language, particularly in science and education, has
undergone remarkable shifts in word usage. These changes are widely attributed
to the growing influence of Large Language Models (LLMs), which frequently rely
on a distinct lexical style. Divergences between model output and target
audience norms can be viewed as a form of misalignment. While these shifts are
often linked to using Artificial Intelligence (AI) directly as a tool to
generate text, it remains unclear whether the changes reflect broader changes
in the human language system itself. To explore this question, we constructed a
dataset of 22.1 million words from unscripted spoken language drawn from
conversational science and technology podcasts. We analyzed lexical trends
before and after ChatGPT's release in 2022, focusing on commonly LLM-associated
words. Our results show a moderate yet significant increase in the usage of
these words post-2022, suggesting a convergence between human word choices and
LLM-associated patterns. In contrast, baseline synonym words exhibit no
significant directional shift. Given the short time frame and the number of
words affected, this may indicate the onset of a remarkable shift in language
use. Whether this represents natural language change or a novel shift driven by
AI exposure remains an open question. Similarly, although the shifts may stem
from broader adoption patterns, it may also be that upstream training
misalignments ultimately contribute to changes in human language use. These
findings parallel ethical concerns that misaligned models may shape social and
moral beliefs.

</details>


### [10] [Integrating clinical reasoning into large language model-based diagnosis through etiology-aware attention steering](https://arxiv.org/abs/2508.00285)
*Peixian Li,Yu Tian,Ruiqi Tu,Chengkai Wu,Jingjing Ren,Jingsong Li*

Main category: cs.CL

TL;DR: The paper introduces an Etiology-Aware Attention Steering Framework to improve LLMs' diagnostic accuracy and clinical reasoning by integrating structured clinical guidelines and fine-tuning attention mechanisms.


<details>
  <summary>Details</summary>
Motivation: Despite LLMs' capabilities in medical text understanding, their diagnostic reliability in complex clinical scenarios is limited. This study aims to enhance their accuracy and reasoning.

Method: The framework uses Clinical Reasoning Scaffolding (CRS) from guidelines, identifies key attention heads, and employs Reasoning-Guided Parameter-Efficient Fine-tuning to align reasoning cues.

Result: The framework improves diagnostic accuracy by 15.65% and Reasoning Focus Score by 31.6%, with external validation confirming effectiveness.

Conclusion: The approach enhances LLM-based clinical reasoning, offering a reliable and interpretable paradigm for AI diagnostics in complex settings.

Abstract: Objective: Large Language Models (LLMs) demonstrate significant capabilities
in medical text understanding and generation. However, their diagnostic
reliability in complex clinical scenarios remains limited. This study aims to
enhance LLMs' diagnostic accuracy and clinical reasoning ability. Method: We
propose an Etiology-Aware Attention Steering Framework to integrate structured
clinical reasoning into LLM-based diagnosis. Specifically, we first construct
Clinical Reasoning Scaffolding (CRS) based on authoritative clinical guidelines
for three representative acute abdominal emergencies: acute appendicitis, acute
pancreatitis, and acute cholecystitis. Next, we develop the Etiology-Aware Head
Identification algorithm to pinpoint attention heads crucial for the model's
etiology reasoning. To ensure reliable clinical reasoning alignment, we
introduce the Reasoning-Guided Parameter-Efficient Fine-tuning that embeds
etiological reasoning cues into input representations and steers the selected
Etiology-Aware Heads toward critical information through a Reasoning-Guided
Loss function. Result: On the Consistent Diagnosis Cohort, our framework
improves average diagnostic accuracy by 15.65% and boosts the average Reasoning
Focus Score by 31.6% over baselines. External validation on the Discrepant
Diagnosis Cohort further confirms its effectiveness in enhancing diagnostic
accuracy. Further assessments via Reasoning Attention Frequency indicate that
our models exhibit enhanced reliability when faced with real-world complex
scenarios. Conclusion: This study presents a practical and effective approach
to enhance clinical reasoning in LLM-based diagnosis. By aligning model
attention with structured CRS, the proposed framework offers a promising
paradigm for building more interpretable and reliable AI diagnostic systems in
complex clinical settings.

</details>


### [11] [Systematic Evaluation of Optimization Techniques for Long-Context Language Models](https://arxiv.org/abs/2508.00305)
*Ammar Ahmed,Sheng Di,Franck Cappello,Zirui Liu,Jingoo Han,Ali Anwar*

Main category: cs.CL

TL;DR: This paper benchmarks optimization techniques for LLMs, analyzing their impact on performance, scalability, and text generation quality, revealing pitfalls in naive combinations and trade-offs in metrics like F1.


<details>
  <summary>Details</summary>
Motivation: To address the resource demands and limited context windows of LLMs by evaluating pruning, quantization, and token dropping in long-context scenarios.

Method: Systematically benchmarks individual and combined optimization methods for LLMs, analyzing memory, latency, throughput, and text generation quality, including scalability tests on a 70B-parameter model.

Result: Naive combinations of optimization methods harm larger models due to compounded errors, and F1 scores can mask precision-recall trade-offs in QA tasks.

Conclusion: The study provides insights for balancing efficiency, accuracy, and scalability in LLMs, aiding practitioners in optimizing for diverse tasks and hardware.

Abstract: Large language models (LLMs) excel across diverse natural language processing
tasks but face resource demands and limited context windows. Although
techniques like pruning, quantization, and token dropping can mitigate these
issues, their efficacy in long-context scenarios and system evaluation remains
underexplored. This paper systematically benchmarks these optimizations,
characterizing memory usage, latency, and throughput, and studies how these
methods impact the quality of text generation. We first analyze individual
optimization methods for two LLM architectures supporting long context and then
systematically evaluate combinations of these techniques to assess how this
deeper analysis impacts performance metrics. We subsequently study the
scalability of individual optimization methods on a larger variant with 70
billion-parameter model. Our novel insights reveal that naive combination
inference optimization algorithms can adversely affect larger models due to
compounded approximation errors, as compared to their smaller counterparts.
Experiments show that relying solely on F1 obscures these effects by hiding
precision-recall trade-offs in question answering tasks. By integrating
system-level profiling with task-specific insights, this study helps LLM
practitioners and researchers explore and balance efficiency, accuracy, and
scalability across tasks and hardware configurations.

</details>


### [12] [Improving Multimodal Contrastive Learning of Sentence Embeddings with Object-Phrase Alignment](https://arxiv.org/abs/2508.00332)
*Kaiyan Zhao,Zhongtao Miao,Yoshimasa Tsuruoka*

Main category: cs.CL

TL;DR: MCSEO improves multimodal sentence embeddings by aligning fine-grained object-phrase pairs, outperforming baselines on STS tasks.


<details>
  <summary>Details</summary>
Motivation: Noise in image-caption pairs (redundant/irrelevant info) hampers multimodal sentence embedding quality.

Method: MCSEO uses segmentation/object detection to extract object-phrase pairs, optimizing contrastive learning for alignment.

Result: MCSEO consistently outperforms baselines on semantic textual similarity tasks.

Conclusion: Precise object-phrase alignment is crucial for effective multimodal representation learning.

Abstract: Multimodal sentence embedding models typically leverage image-caption pairs
in addition to textual data during training. However, such pairs often contain
noise, including redundant or irrelevant information on either the image or
caption side. To mitigate this issue, we propose MCSEO, a method that enhances
multimodal sentence embeddings by incorporating fine-grained object-phrase
alignment alongside traditional image-caption alignment. Specifically, MCSEO
utilizes existing segmentation and object detection models to extract accurate
object-phrase pairs, which are then used to optimize a contrastive learning
objective tailored to object-phrase correspondence. Experimental results on
semantic textual similarity (STS) tasks across different backbone models
demonstrate that MCSEO consistently outperforms strong baselines, highlighting
the significance of precise object-phrase alignment in multimodal
representation learning.

</details>


### [13] [PilotRL: Training Language Model Agents via Global Planning-Guided Progressive Reinforcement Learning](https://arxiv.org/abs/2508.00344)
*Keer Lu,Chong Chen,Bin Cui,Huang Leng,Wentao Zhang*

Main category: cs.CL

TL;DR: The paper introduces AdaPlan, a global plan-based agent paradigm, and PilotRL, a reinforcement learning framework, to enhance LLM agents' long-term planning and execution coordination, outperforming GPT-4o.


<details>
  <summary>Details</summary>
Motivation: Existing LLM agent paradigms like ReAct lack effectiveness in complex, long-term tasks due to single-step reasoning and poor planner-executor coordination, and supervised fine-tuning limits generalization.

Method: Proposes AdaPlan for explicit guidance and execution synergy, and PilotRL for progressive reinforcement learning to train LLM agents in global planning and execution optimization.

Result: PilotRL achieves state-of-the-art performance, with LLaMA3.1-8B-Instruct + PilotRL surpassing GPT-4o by 3.60% and GPT-4o-mini by 55.78%.

Conclusion: AdaPlan and PilotRL address limitations of current LLM agent paradigms, improving long-horizon decision-making and generalization.

Abstract: Large Language Models (LLMs) have shown remarkable advancements in tackling
agent-oriented tasks. Despite their potential, existing work faces challenges
when deploying LLMs in agent-based environments. The widely adopted agent
paradigm ReAct centers on integrating single-step reasoning with immediate
action execution, which limits its effectiveness in complex tasks requiring
long-term strategic planning. Furthermore, the coordination between the planner
and executor during problem-solving is also a critical factor to consider in
agent design. Additionally, current approaches predominantly rely on supervised
fine-tuning, which often leads models to memorize established task completion
trajectories, thereby restricting their generalization ability when confronted
with novel problem contexts. To address these challenges, we introduce an
adaptive global plan-based agent paradigm AdaPlan, aiming to synergize
high-level explicit guidance with execution to support effective long-horizon
decision-making. Based on the proposed paradigm, we further put forward
PilotRL, a global planning-guided training framework for LLM agents driven by
progressive reinforcement learning. We first develop the model's ability to
follow explicit guidance from global plans when addressing agent tasks.
Subsequently, based on this foundation, we focus on optimizing the quality of
generated plans. Finally, we conduct joint optimization of the model's planning
and execution coordination. Experiments indicate that PilotRL could achieve
state-of-the-art performances, with LLaMA3.1-8B-Instruct + PilotRL surpassing
closed-sourced GPT-4o by 3.60%, while showing a more substantial gain of 55.78%
comparing to GPT-4o-mini at a comparable parameter scale.

</details>


### [14] [Lucy: edgerunning agentic web search on mobile with machine generated task vectors](https://arxiv.org/abs/2508.00360)
*Alan Dao,Dinh Bach Vu,Alex Nguyen,Norapat Buppodom*

Main category: cs.CL

TL;DR: A 1.7B-parameter small language model (Lucy) uses dynamic task vector construction during reasoning, achieving 78.3% accuracy on SimpleQA, rivaling larger models.


<details>
  <summary>Details</summary>
Motivation: Small language models (SLMs) struggle with knowledge-intensive tasks due to limited capacity. This work aims to enhance SLM performance by treating reasoning as a dynamic, self-refining process.

Method: Proposes a paradigm where reasoning (delimited by <think> tags) is viewed as a dynamic task vector machine. Uses RLVR to optimize this mechanism and integrates MCP for training.

Result: Lucy, a 1.7B-parameter SLM, achieves 78.3% accuracy on SimpleQA, matching larger models like DeepSeek-V3.

Conclusion: Small models can compete with large ones when equipped with structured, self-constructed task reasoning.

Abstract: Small language models (SLMs) are inherently limited in knowledge-intensive
tasks due to their constrained capacity. While test-time computation offers a
path to enhanced performance, most approaches treat reasoning as a fixed or
heuristic process. In this work, we propose a new paradigm: viewing the model's
internal reasoning, delimited by <think> and </think> tags, as a dynamic task
vector machine. Rather than treating the content inside these tags as a mere
trace of thought, we interpret the generation process itself as a mechanism
through which the model \textbf{constructs and refines its own task vectors} on
the fly. We developed a method to optimize this dynamic task vector machine
through RLVR and successfully trained an agentic web-search model. We present
Lucy, a 1.7B-parameter SLM that leverages this dynamic reasoning mechanism with
MCP integration to achieve 78.3% accuracy on the SimpleQA benchmark, performing
on par with much larger models such as DeepSeek-V3. This demonstrates that
small models can rival large ones when equipped with structured,
self-constructed task reasoning.

</details>


### [15] [EdgeInfinite-Instruct: Bridging SFT-Based Optimization and NPU-Level Efficiency for Edge Devices](https://arxiv.org/abs/2508.00370)
*Jiyu Chen,Poh Seng Lim,Shuang Peng,Daxiong Luo,JungHau Foo,Yap Deep,Timothy Lee Jun Jie,Kelvin Teh Kae Wen,Fan Yang,Danyu Feng,Hao-Yun Chen,Peng-Wen Chen,Fangyuan Li,Xiaoxin Chen,Wong Wai Mun*

Main category: cs.CL

TL;DR: EdgeInfinite-Instruct optimizes Transformer-based LLMs for edge devices by fine-tuning a small subset of parameters, employing S-SFT for long-sequence tasks, and using PTQ and fixed-shape computation for efficiency.


<details>
  <summary>Details</summary>
Motivation: Challenges in deploying LLMs on edge devices include high computational costs, memory demands, and poor TTFT. Existing solutions either degrade performance or lack infrastructure support.

Method: Proposes EdgeInfinite-Instruct with S-SFT for long-sequence tasks, PTQ for computational efficiency, and fixed-shape computation for memory optimization.

Result: Improves performance on long-context benchmarks and mobile tasks while maintaining efficiency on NPU-accelerated devices.

Conclusion: EdgeInfinite-Instruct effectively balances performance and efficiency for edge deployment, addressing limitations of prior methods.

Abstract: Deploying Transformer-based large language models (LLMs) on
resource-constrained edge devices for long-sequence tasks remains challenging
due to the quadratic time complexity of self-attention and growing Key-Value
(KV) cache demands. While existing KV cache optimizations improve memory
efficiency, they often fail to reduce time to first token (TTFT) and may
degrade performance through token pruning. Alternative sequence modeling
architectures address some of these limitations, but typically require full
retraining and lack infrastructure support. EdgeInfinite offers an efficient
solution by fine-tuning only a small subset of parameters, maintaining quality
while reducing both computational and memory costs, including improved TTFT.
However, its instruction-following ability is limited, and it lacks
mobile-specific optimizations. To address these issues, we propose
EdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning
(S-SFT) strategy tailored to long-sequence tasks such as summarization and
question answering. We further optimized EdgeInfinite-Instruct for efficient
deployment on edge NPUs by employing fine-grained post-training quantization
(PTQ) to reduce computational demands while maintaining accuracy, and by
implementing a fixed-shape computation graph that balances memory usage and
on-device efficiency through scenario-specific customization of input token and
cache sizes. Experiments on long-context benchmarks and real-world mobile tasks
show that our approach improves domain-specific performance while maintaining
efficiency on NPU-accelerated edge devices.

</details>


### [16] [Multi-Layer Attention is the Amplifier of Demonstration Effectiveness](https://arxiv.org/abs/2508.00385)
*Dingzirui Wang,Xuangliang Zhang,Keyan Xu,Qingfu Zhu,Wanxiang Che,Yang Deng*

Main category: cs.CL

TL;DR: The paper investigates why some demonstrations in in-context learning (ICL) are ineffective, attributing it to learned or irrelevant information. It introduces GradS, a gradient-based method for selecting effective demonstrations, improving performance by 6.8% over baselines.


<details>
  <summary>Details</summary>
Motivation: Existing work assumes all demonstrations in ICL are effective, but many fail to improve performance. This paper explores the reasons behind ineffective demonstrations.

Method: The study uses gradient flow and linear self-attention models to analyze demonstration ineffectiveness. It proposes GradS, a gradient-based selection method, validated on four LLMs and five datasets.

Result: Experiments show that demonstration effectiveness disparity grows with model layers. GradS outperforms baselines by 6.8% on average.

Conclusion: GradS effectively selects demonstrations by leveraging gradient flow, addressing the limitations of current methods that ignore learned information.

Abstract: Numerous studies have investigated the underlying mechanisms of in-context
learning (ICL) effectiveness to inspire the design of related methods. However,
existing work predominantly assumes the effectiveness of the demonstrations
provided within ICL, while many research indicates that not all demonstrations
are effective, failing to yielding any performance improvement during ICL.
Therefore, in this paper, we investigate the reasons behind demonstration
ineffectiveness. Our analysis is based on gradient flow and linear
self-attention models. By setting the gradient flow to zero, we deduce that a
demonstration becomes ineffective if its information has either been learned by
the model or is irrelevant to the user query. Furthermore, we demonstrate that
in multi-layer models, the disparity in effectiveness among demonstrations is
amplified with layer increasing, causing the model to focus more on effective
ones. Considering that current demonstration selection methods primarily focus
on the relevance to the user query while overlooking the information that the
model has already assimilated, we propose a novel method called GradS, which
leverages gradient flow for demonstration selection. We use the magnitude of
the gradient flow of the demonstration with respect to a given user query as
the criterion, thereby ensuring the effectiveness of the chosen ones. We
validate our derivation and GradS on four prominent LLMs across five mainstream
datasets. The experimental results confirm that the disparity in effectiveness
among demonstrations is magnified as the model layer increases, substantiating
our derivations. Moreover, GradS achieves a relative improvement of $6.8\%$ on
average over the strongest baselines, demonstrating its effectiveness.

</details>


### [17] [SA-GCS: Semantic-Aware Gaussian Curriculum Scheduling for UAV Vision-Language Navigation](https://arxiv.org/abs/2508.00390)
*Hengxing Cai,Jinhan Dong,Yijie Rao,Jingcheng Deng,Jingjun Tan,Qien Chen,Haidong Wang,Zhen Wang,Shiyu Huang,Agachai Sumalee,Renxin Zhong*

Main category: cs.CL

TL;DR: SA-GCS improves UAV VLN by integrating curriculum learning into RL, enhancing training efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Address inefficiencies in RL methods for UAV VLN, such as slow convergence and poor data use.

Method: Proposes SA-GCS with a Semantic-Aware Difficulty Estimator and Gaussian Curriculum Scheduler for dynamic task progression.

Result: Outperforms baselines on CityNav, with faster convergence and better generalization.

Conclusion: SA-GCS is robust, scalable, and improves UAV VLN performance effectively.

Abstract: Unmanned Aerial Vehicle (UAV) Vision-Language Navigation (VLN) aims to enable
agents to accurately localize targets and plan flight paths in complex
environments based on natural language instructions, with broad applications in
intelligent inspection, disaster rescue, and urban monitoring. Recent progress
in Vision-Language Models (VLMs) has provided strong semantic understanding for
this task, while reinforcement learning (RL) has emerged as a promising
post-training strategy to further improve generalization. However, existing RL
methods often suffer from inefficient use of training data, slow convergence,
and insufficient consideration of the difficulty variation among training
samples, which limits further performance improvement. To address these
challenges, we propose \textbf{Semantic-Aware Gaussian Curriculum Scheduling
(SA-GCS)}, a novel training framework that systematically integrates Curriculum
Learning (CL) into RL. SA-GCS employs a Semantic-Aware Difficulty Estimator
(SA-DE) to quantify the complexity of training samples and a Gaussian
Curriculum Scheduler (GCS) to dynamically adjust the sampling distribution,
enabling a smooth progression from easy to challenging tasks. This design
significantly improves training efficiency, accelerates convergence, and
enhances overall model performance. Extensive experiments on the CityNav
benchmark demonstrate that SA-GCS consistently outperforms strong baselines
across all metrics, achieves faster and more stable convergence, and
generalizes well across models of different scales, highlighting its robustness
and scalability. The implementation of our approach is publicly available.

</details>


### [18] [Combining Discrete Wavelet and Cosine Transforms for Efficient Sentence Embedding](https://arxiv.org/abs/2508.00420)
*Rana Salama,Abdou Youssef,Mona Diab*

Main category: cs.CL

TL;DR: The paper explores using Discrete Wavelet Transforms (DWT) and Discrete Cosine Transform (DCT) for compressing word and sentence embeddings in NLP, showing competitive or superior results to original embeddings.


<details>
  <summary>Details</summary>
Motivation: Wavelets have proven effective in image and signal processing, suggesting potential for NLP tasks to capture linguistic properties.

Method: Applies DWT to word embeddings for dimensionality reduction and combines DWT with DCT for non-parameterized sentence compression.

Result: The proposed method yields comparable or superior results in downstream NLP tasks compared to original embeddings.

Conclusion: Wavelet-based techniques offer an effective, non-parameterized approach for embedding compression in NLP.

Abstract: Wavelets have emerged as a cutting edge technology in a number of fields.
Concrete results of their application in Image and Signal processing suggest
that wavelets can be effectively applied to Natural Language Processing (NLP)
tasks that capture a variety of linguistic properties. In this paper, we
leverage the power of applying Discrete Wavelet Transforms (DWT) to word and
sentence embeddings. We first evaluate, intrinsically and extrinsically, how
wavelets can effectively be used to consolidate important information in a word
vector while reducing its dimensionality. We further combine DWT with Discrete
Cosine Transform (DCT) to propose a non-parameterized model that compresses a
sentence with a dense amount of information in a fixed size vector based on
locally varying word features. We show the efficacy of the proposed paradigm on
downstream applications models yielding comparable and even superior (in some
tasks) results to original embeddings.

</details>


### [19] [ReaGAN: Node-as-Agent-Reasoning Graph Agentic Network](https://arxiv.org/abs/2508.00429)
*Minghao Guo,Xi Zhu,Jingyuan Huang,Kai Mei,Yongfeng Zhang*

Main category: cs.CL

TL;DR: ReaGAN introduces an agent-based GNN framework with node-level autonomy and retrieval-augmented generation to address limitations of fixed aggregation schemes, achieving competitive performance without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Fixed GNN aggregation schemes struggle with node informativeness imbalance and lack global semantic relationships, limiting their effectiveness.

Method: ReaGAN uses agent-based nodes with autonomous decision-making and retrieval-augmented generation (RAG) to enable adaptive message propagation and global semantic access.

Result: ReaGAN performs competitively in few-shot settings using a frozen LLM backbone, demonstrating the value of agentic planning and retrieval.

Conclusion: ReaGAN's agentic and retrieval-enhanced approach addresses key GNN limitations, offering a promising direction for graph learning.

Abstract: Graph Neural Networks (GNNs) have achieved remarkable success in graph-based
learning by propagating information among neighbor nodes via predefined
aggregation mechanisms. However, such fixed schemes often suffer from two key
limitations. First, they cannot handle the imbalance in node informativeness --
some nodes are rich in information, while others remain sparse. Second,
predefined message passing primarily leverages local structural similarity
while ignoring global semantic relationships across the graph, limiting the
model's ability to capture distant but relevant information. We propose
Retrieval-augmented Graph Agentic Network (ReaGAN), an agent-based framework
that empowers each node with autonomous, node-level decision-making. Each node
acts as an agent that independently plans its next action based on its internal
memory, enabling node-level planning and adaptive message propagation.
Additionally, retrieval-augmented generation (RAG) allows nodes to access
semantically relevant content and build global relationships in the graph.
ReaGAN achieves competitive performance under few-shot in-context settings
using a frozen LLM backbone without fine-tuning, showcasing the potential of
agentic planning and local-global retrieval in graph learning.

</details>


### [20] [Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges](https://arxiv.org/abs/2508.00454)
*Yuqi Tang,Kehua Feng,Yunfeng Wang,Zhiwen Chen,Chengfei Lv,Gang Yu,Qiang Zhang,Keyan Ding*

Main category: cs.CL

TL;DR: Proposes an efficient multi-turn dialogue evaluator to reduce biases and computational costs in LLM-based dialogue quality assessment.


<details>
  <summary>Details</summary>
Motivation: Current LLM-as-a-judge methods suffer from biases and high computational overhead, necessitating a more efficient solution.

Method: Aggregates preference knowledge from multiple LLM judges into a single model to capture collective wisdom while reducing costs.

Result: Outperforms existing baselines on seven benchmarks, demonstrating efficiency and robustness.

Conclusion: The proposed method enables fast, flexible, and reliable dialogue quality assessment with reduced computational overhead.

Abstract: Evaluating the conversational abilities of large language models (LLMs)
remains a challenging task. Current mainstream approaches primarily rely on the
``LLM-as-a-judge" paradigm, where an LLM is prompted to serve as an evaluator
to assess dialogue quality. However, such methods often suffer from various
biases, which undermine the reliability and consistency of the evaluation
results. To mitigate these biases, recent methods employ multiple LLMs as
judges and aggregate their judgments to select the optimal assessment. Although
effective, this multi-judge approach incurs significant computational overhead
during inference. In this paper, we propose an efficient multi-turn dialogue
evaluator that captures the collective wisdom of multiple LLM judges by
aggregating their preference knowledge into a single model. Our approach
preserves the advantages of diverse multi-judge feedback while drastically
reducing the evaluation cost, enabling fast and flexible dialogue quality
assessment. Extensive experiments on seven single rating and pairwise
comparison dialogue evaluation benchmarks demonstrate that our method
outperforms existing baselines across diverse scenarios, showcasing its
efficiency and robustness.

</details>


### [21] [GETALP@AutoMin 2025: Leveraging RAG to Answer Questions based on Meeting Transcripts](https://arxiv.org/abs/2508.00476)
*Jeongwoo Kang,Markarit Vartampetian,Felix Herron,Yongxin Zhou,Diandra Fabre,Gabriela Gonzalez-Saez*

Main category: cs.CL

TL;DR: GETALP's submission to SIGDial 2025 Task B uses RAG and AMR, showing AMR improves QA performance, especially for 'who' questions.


<details>
  <summary>Details</summary>
Motivation: To enhance question-answering (QA) performance on meeting transcripts by integrating retrieval augmented generation (RAG) and Abstract Meaning Representations (AMR).

Method: Proposed three systems combining RAG and AMR for QA on meeting transcripts.

Result: AMR improved response quality for ~35% of questions, notably aiding 'who' questions.

Conclusion: Incorporating AMR with RAG enhances QA, particularly for participant-specific queries.

Abstract: This paper documents GETALP's submission to the Third Run of the Automatic
Minuting Shared Task at SIGDial 2025. We participated in Task B:
question-answering based on meeting transcripts. Our method is based on a
retrieval augmented generation (RAG) system and Abstract Meaning
Representations (AMR). We propose three systems combining these two approaches.
Our results show that incorporating AMR leads to high-quality responses for
approximately 35% of the questions and provides notable improvements in
answering questions that involve distinguishing between different participants
(e.g., who questions).

</details>


### [22] [The Missing Parts: Augmenting Fact Verification with Half-Truth Detection](https://arxiv.org/abs/2508.00489)
*Yixuan Tang,Jincheng Wang,Anthony K. H. Tung*

Main category: cs.CL

TL;DR: The paper introduces the task of half-truth detection and proposes a new benchmark, PolitiFact-Hidden, along with TRACER, a framework to identify omission-based misinformation.


<details>
  <summary>Details</summary>
Motivation: Existing fact verification systems fail to detect half-truthsclaims that are factually correct but misleading due to omitted context.

Method: The authors propose TRACER, a modular framework that aligns evidence, infers implied intent, and estimates the causal impact of hidden content.

Result: TRACER improves Half-True classification F1 by up to 16 points, enhancing existing fact-checking pipelines.

Conclusion: Modeling omissions is crucial for trustworthy fact verification, as demonstrated by TRACER's effectiveness.

Abstract: Fact verification systems typically assess whether a claim is supported by
retrieved evidence, assuming that truthfulness depends solely on what is
stated. However, many real-world claims are half-truths, factually correct yet
misleading due to the omission of critical context. Existing models struggle
with such cases, as they are not designed to reason about what is left unsaid.
We introduce the task of half-truth detection, and propose PolitiFact-Hidden, a
new benchmark with 15k political claims annotated with sentence-level evidence
alignment and inferred claim intent. To address this challenge, we present
TRACER, a modular re-assessment framework that identifies omission-based
misinformation by aligning evidence, inferring implied intent, and estimating
the causal impact of hidden content. TRACER can be integrated into existing
fact-checking pipelines and consistently improves performance across multiple
strong baselines. Notably, it boosts Half-True classification F1 by up to 16
points, highlighting the importance of modeling omissions for trustworthy fact
verification.

</details>


### [23] [EFlat-LoRA: Efficiently Seeking Flat Minima for Better Generalization in Fine-Tuning Large Language Models and Beyond](https://arxiv.org/abs/2508.00522)
*Jiaxin Deng,Qingcheng Zhu,Junbiao Pang,Linlin Yang,Zhongqian Fu,Baochang Zhang*

Main category: cs.CL

TL;DR: The paper explores the link between expressive ability and generalization in LoRA, proposing Flat-LoRA and EFlat-LoRA to find flat minima, improving performance over LoRA and full fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Little research exists on the correlation between expressive ability and generalization in LoRA, and the lack of tools to explore sharpness for LoRA motivates this work.

Method: Proposes Flat-LoRA and EFlat-LoRA, transferring perturbations from full parameter space to low-rank subspace to avoid interference.

Result: EFlat-LoRA matches LoRA's efficiency while outperforming it (e.g., 1.0% on GLUE with RoBERTa-large) and full fine-tuning (0.5%). Vision-language models also show gains (1.5% on SQA).

Conclusion: Generalization in LoRA is tied to sharpness, a factor overlooked by prior methods, as validated by empirical results.

Abstract: Little research explores the correlation between the expressive ability and
generalization ability of the low-rank adaptation (LoRA). Sharpness-Aware
Minimization (SAM) improves model generalization for both Convolutional Neural
Networks (CNNs) and Transformers by encouraging convergence to locally flat
minima. However, the connection between sharpness and generalization has not
been fully explored for LoRA due to the lack of tools to either empirically
seek flat minima or develop theoretical methods. In this work, we propose
Flat-LoRA and its efficient version i.e., EFlat-LoRA, to seek flat minima for
LoRA. Concretely, we theoretically demonstrate that perturbations in the full
parameter space can be transferred to the low-rank subspace. This approach
eliminates the potential interference introduced by perturbations across
multiple matrices in the low-rank subspace. Our extensive experiments on large
language models and vision-language models demonstrate that EFlat-LoRA achieves
optimize efficiency comparable to that of LoRA while simultaneously attaining
comparable or even better performance. For example, on the GLUE dataset with
RoBERTa-large, EFlat-LoRA outperforms LoRA and full fine-tuning by 1.0% and
0.5% on average, respectively. On vision-language models e.g., Qwen-VL-Chat
shows performance improvements of 1.5% and 1.0% on SQA and VizWiz datasets,
respectively. These empirical results also verify that the generalization of
LoRA is closely related to sharpness, which is omitted by previous methods.

</details>


### [24] [The Prosody of Emojis](https://arxiv.org/abs/2508.00537)
*Giulio Zhou,Tsz Kin Lam,Alexandra Birch,Barry Haddow*

Main category: cs.CL

TL;DR: The study explores how emojis influence prosody in speech and how listeners interpret these cues, showing emojis can convey prosodic intent.


<details>
  <summary>Details</summary>
Motivation: To understand the role of emojis as visual surrogates for prosodic features in text-based communication and their impact on spoken prosody.

Method: Analysis of human speech data from structured production and perception tasks, linking prosody and emoji meanings empirically.

Result: Speakers adjust prosody based on emojis; listeners often identify emojis from prosody alone; greater semantic differences between emojis lead to more prosodic divergence.

Conclusion: Emojis serve as meaningful carriers of prosodic intent, highlighting their communicative role in digital contexts.

Abstract: Prosodic features such as pitch, timing, and intonation are central to spoken
communication, conveying emotion, intent, and discourse structure. In
text-based settings, where these cues are absent, emojis act as visual
surrogates that add affective and pragmatic nuance. This study examines how
emojis influence prosodic realisation in speech and how listeners interpret
prosodic cues to recover emoji meanings. Unlike previous work, we directly link
prosody and emoji by analysing actual human speech data, collected through
structured but open-ended production and perception tasks. This provides
empirical evidence of how emoji semantics shape spoken delivery and perception.
Results show that speakers adapt their prosody based on emoji cues, listeners
can often identify the intended emoji from prosodic variation alone, and
greater semantic differences between emojis correspond to increased prosodic
divergence. These findings suggest that emojis can act as meaningful carriers
of prosodic intent, offering insight into their communicative role in digitally
mediated contexts.

</details>


### [25] [PaPaformer: Language Model from Pre-trained Paraller Paths](https://arxiv.org/abs/2508.00544)
*Joonas Tapaninaho,Mourad Oussala*

Main category: cs.CL

TL;DR: The paper introduces PaPaformer, a decoder-only transformer variant, to reduce training time and parameters while improving performance by training lower-dimensional paths individually and combining them.


<details>
  <summary>Details</summary>
Motivation: Modern large-language models require extensive computation and time, even for smaller variants. The paper aims to reduce training time from days/weeks to hours.

Method: Introduces PaPaformer, a decoder-only transformer with parallel paths trained individually on diverse data and combined into a larger model.

Result: Reduces total parameters and training time while increasing performance. Enables customization of paths for specific tasks.

Conclusion: PaPaformer offers a scalable and efficient method for training transformer-based models with potential for task-specific customization.

Abstract: The training of modern large-language models requires an increasingly amount
of computation power and time. Even smaller variants, such as small-language
models (SLMs), take several days to train in the best-case scenarios, often
requiring multiple GPUs. This paper explores methods to train and evaluate
decoder-only transformer-based language models in hours instead of days/weeks.
We introduces \textit{PaPaformer}, a decoder-only transformer architecture
variant, whose lower-dimensional parallel paths are combined into larger model.
The paper shows that these lower-dimensional paths can be trained individually
with different types of training data and then combined into one larger model.
This method gives the option to reduce the total number of model parameters and
the training time with increasing performance. Moreover, the use of parallel
path structure opens interesting possibilities to customize paths to
accommodate specific task requirements.

</details>


### [26] [SynAdapt: Learning Adaptive Reasoning in Large Language Models via Synthetic Continuous Chain-of-Thought](https://arxiv.org/abs/2508.00574)
*Jianwei Wang,Ziming Wu,Fuming Lai,Shaobing Lian,Ziqian Zeng*

Main category: cs.CL

TL;DR: SynAdapt is a framework improving reasoning efficiency by using synthetic Continuous Chain-of-Thought (CCoT) for alignment and adaptive prompting for hard questions.


<details>
  <summary>Details</summary>
Motivation: Existing CCoT methods suffer from inefficiencies like indirect fine-tuning and inconsistent targets, limiting their effectiveness.

Method: SynAdapt generates synthetic CCoT for precise alignment and uses a difficulty classifier to adaptively prompt LLMs for hard questions.

Result: Achieves the best accuracy-efficiency trade-off across benchmarks of varying difficulty.

Conclusion: SynAdapt effectively addresses limitations of current CCoT methods, enhancing reasoning performance and efficiency.

Abstract: While Chain-of-Thought (CoT) reasoning improves model performance, it incurs
significant time costs due to the generation of discrete CoT tokens (DCoT).
Continuous CoT (CCoT) offers a more efficient alternative, but existing CCoT
methods are hampered by indirect fine-tuning, limited alignment, or
inconsistent targets. To overcome these limitations, we propose
\textit{SynAdapt}, an innovative efficient reasoning framework. Specifically,
\textit{SynAdapt} generates the synthetic CCoT to serve as a precise and
effective alignment target for LLMs. This synthetic CCoT explicitly guides the
LLM to learn CCoT and derive accurate answers directly. Furthermore, relying
solely on CCoT is insufficient for solving hard questions. To address this,
\textit{SynAdapt} integrates a difficulty classifier that leverages both
question context and CCoT to identify hard questions. CCoT can effectively help
identify hard questions after some brief reasoning. We then adaptively prompt
the LLM to re-think these hard questions for improved performance. Extensive
experimental results across various benchmarks from different difficulty levels
strongly demonstrate the effectiveness of our method, achieving the best
accuracy-efficiency trade-off.

</details>


### [27] [A Context-Aware Dual-Metric Framework for Confidence Estimation in Large Language Models](https://arxiv.org/abs/2508.00600)
*Mingruo Yuan,Shuyi Zhang,Ben Kao*

Main category: cs.CL

TL;DR: CRUX is a framework for confidence estimation in LLMs, integrating context faithfulness and consistency via novel metrics, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Current methods lack context relevance, crucial for output quality in scenarios with background knowledge.

Method: CRUX uses contextual entropy reduction and unified consistency examination to estimate confidence.

Result: Achieves highest AUROC on benchmark datasets (CoQA, SQuAD, QuAC) and domain-specific datasets (BioASQ, EduQG).

Conclusion: CRUX effectively bridges the gap in context-aware confidence estimation for LLMs.

Abstract: Accurate confidence estimation is essential for trustworthy large language
models (LLMs) systems, as it empowers the user to determine when to trust
outputs and enables reliable deployment in safety-critical applications.
Current confidence estimation methods for LLMs neglect the relevance between
responses and contextual information, a crucial factor in output quality
evaluation, particularly in scenarios where background knowledge is provided.
To bridge this gap, we propose CRUX (Context-aware entropy Reduction and
Unified consistency eXamination), the first framework that integrates context
faithfulness and consistency for confidence estimation via two novel metrics.
First, contextual entropy reduction represents data uncertainty with the
information gain through contrastive sampling with and without context. Second,
unified consistency examination captures potential model uncertainty through
the global consistency of the generated answers with and without context.
Experiments across three benchmark datasets (CoQA, SQuAD, QuAC) and two
domain-specific datasets (BioASQ, EduQG) demonstrate CRUX's effectiveness,
achieving the highest AUROC than existing baselines.

</details>


### [28] [GHTM: A Graph based Hybrid Topic Modeling Approach in Low-Resource Bengali Language](https://arxiv.org/abs/2508.00605)
*Farhana Haque,Md. Abdur Rahman,Sumon Ahmed*

Main category: cs.CL

TL;DR: A novel Graph Convolutional Network (GCN)-based model, GHTM, is proposed for Bengali topic modeling, outperforming traditional and contemporary methods in coherence and diversity. A new Bengali dataset, NCTBText, is introduced.


<details>
  <summary>Details</summary>
Motivation: Topic modeling is understudied in Bengali due to its complexity and lack of resources. The paper aims to address this gap.

Method: GHTM uses GCN to create semantic embeddings from document nodes, decomposed via NMF for topic extraction. Compared against LDA, LSA, NMF, BERTopic, and Top2Vec on Bengali datasets.

Result: GHTM outperforms other models in topic coherence and diversity. A new dataset, NCTBText, is introduced.

Conclusion: GHTM is effective for Bengali topic modeling, and NCTBText enriches available corpora.

Abstract: Topic modeling is a Natural Language Processing (NLP) technique that is used
to identify latent themes and extract topics from text corpora by grouping
similar documents based on their most significant keywords. Although widely
researched in English, topic modeling remains understudied in Bengali due to
its morphological complexity, lack of adequate resources and initiatives. In
this contribution, a novel Graph Convolutional Network (GCN) based model called
GHTM (Graph-Based Hybrid Topic Model) is proposed. This model represents input
vectors of documents as nodes in the graph, which GCN uses to produce
semantically rich embeddings. The embeddings are then decomposed using
Non-negative Matrix Factorization (NMF) to get the topical representations of
the underlying themes of the text corpus. This study compares the proposed
model against a wide range of Bengali topic modeling techniques, from
traditional methods such as LDA, LSA, and NMF to contemporary frameworks such
as BERTopic and Top2Vec on three Bengali datasets. The experimental results
demonstrate the effectiveness of the proposed model by outperforming other
models in topic coherence and diversity. In addition, we introduce a novel
Bengali dataset called "NCTBText" sourced from Bengali textbook materials to
enrich and diversify the predominantly newspaper-centric Bengali corpora.

</details>


### [29] [Prompting Science Report 3: I'll pay you or I'll kill you -- but will you care?](https://arxiv.org/abs/2508.00614)
*Lennart Meincke,Ethan Mollick,Lilach Mollick,Dan Shapiro*

Main category: cs.CL

TL;DR: The paper tests the impact of tipping and threatening AI models on performance, finding no significant effect on benchmarks but noting per-question variability.


<details>
  <summary>Details</summary>
Motivation: To clarify whether common prompting tactics (tipping or threatening AI models) improve performance, as claimed by some leaders.

Method: Empirical testing on GPQA and MMLU-Pro benchmarks to evaluate model performance under tipping and threatening prompts.

Result: No significant effect on overall benchmark performance, but per-question performance varies unpredictably with prompt changes.

Conclusion: Simple prompting variations like tipping or threatening are not broadly effective, though they may unpredictably impact individual questions.

Abstract: This is the third in a series of short reports that seek to help business,
education, and policy leaders understand the technical details of working with
AI through rigorous testing. In this report, we investigate two commonly held
prompting beliefs: a) offering to tip the AI model and b) threatening the AI
model. Tipping was a commonly shared tactic for improving AI performance and
threats have been endorsed by Google Founder Sergey Brin (All-In, May 2025,
8:20) who observed that 'models tend to do better if you threaten them,' a
claim we subject to empirical testing here. We evaluate model performance on
GPQA (Rein et al. 2024) and MMLU-Pro (Wang et al. 2024).
  We demonstrate two things:
  - Threatening or tipping a model generally has no significant effect on
benchmark performance.
  - Prompt variations can significantly affect performance on a per-question
level. However, it is hard to know in advance whether a particular prompting
approach will help or harm the LLM's ability to answer any particular question.
  Taken together, this suggests that simple prompting variations might not be
as effective as previously assumed, especially for difficult problems. However,
as reported previously (Meincke et al. 2025a), prompting approaches can yield
significantly different results for individual questions.

</details>


### [30] [DACTYL: Diverse Adversarial Corpus of Texts Yielded from Large Language Models](https://arxiv.org/abs/2508.00619)
*Shantanu Thorat,Andrew Caines*

Main category: cs.CL

TL;DR: The paper introduces DACTYL, a dataset for detecting AI-generated texts in one-shot/few-shot and domain-specific scenarios, revealing vulnerabilities in existing detectors. It compares BCE and DXO training methods, showing DXO's superior generalization.


<details>
  <summary>Details</summary>
Motivation: Existing AI-generated text detectors fail in real-world settings, especially for one-shot/few-shot and domain-specific texts, prompting the need for a robust dataset and improved training methods.

Method: The authors create DACTYL, a dataset with one-shot/few-shot and CPT-generated texts. They train classifiers using BCE and DXO optimization, evaluating performance on DACTYL and OOD datasets.

Result: Existing detectors struggle with DACTYL. DXO-trained classifiers outperform BCE-trained ones on OOD data, with a 50.56 macro-F1 score improvement in a mock deployment.

Conclusion: DXO optimization enhances generalization for AI-generated text detection, highlighting weaknesses in current methods and the need for robust datasets like DACTYL.

Abstract: Existing AIG (AI-generated) text detectors struggle in real-world settings
despite succeeding in internal testing, suggesting that they may not be robust
enough. We rigorously examine the machine-learning procedure to build these
detectors to address this. Most current AIG text detection datasets focus on
zero-shot generations, but little work has been done on few-shot or one-shot
generations, where LLMs are given human texts as an example. In response, we
introduce the Diverse Adversarial Corpus of Texts Yielded from Language models
(DACTYL), a challenging AIG text detection dataset focusing on
one-shot/few-shot generations. We also include texts from domain-specific
continued-pre-trained (CPT) language models, where we fully train all
parameters using a memory-efficient optimization approach. Many existing AIG
text detectors struggle significantly on our dataset, indicating a potential
vulnerability to one-shot/few-shot and CPT-generated texts. We also train our
own classifiers using two approaches: standard binary cross-entropy (BCE)
optimization and a more recent approach, deep X-risk optimization (DXO). While
BCE-trained classifiers marginally outperform DXO classifiers on the DACTYL
test set, the latter excels on out-of-distribution (OOD) texts. In our mock
deployment scenario in student essay detection with an OOD student essay
dataset, the best DXO classifier outscored the best BCE-trained classifier by
50.56 macro-F1 score points at the lowest false positive rates for both. Our
results indicate that DXO classifiers generalize better without overfitting to
the test set. Our experiments highlight several areas of improvement for AIG
text detectors.

</details>


### [31] [Medical Reasoning in the Era of LLMs: A Systematic Review of Enhancement Techniques and Applications](https://arxiv.org/abs/2508.00669)
*Wenxuan Wang,Zizhan Ma,Meidan Ding,Shiyi Zheng,Shengyuan Liu,Jie Liu,Jiaming Ji,Wenting Chen,Xiang Li,Linlin Shen,Yixuan Yuan*

Main category: cs.CL

TL;DR: A systematic review of LLMs in medical reasoning, proposing a taxonomy of enhancement techniques, analyzing applications, and identifying future challenges.


<details>
  <summary>Details</summary>
Motivation: Address the gap in LLMs' ability to perform systematic, transparent, and verifiable reasoning in clinical practice.

Method: Taxonomy of reasoning techniques (training-time and test-time), analysis across data modalities and clinical applications, and evaluation of benchmarks.

Result: Identified key challenges like the faithfulness-plausibility gap and the need for native multimodal reasoning.

Conclusion: Future directions focus on building efficient, robust, and sociotechnically responsible medical AI.

Abstract: The proliferation of Large Language Models (LLMs) in medicine has enabled
impressive capabilities, yet a critical gap remains in their ability to perform
systematic, transparent, and verifiable reasoning, a cornerstone of clinical
practice. This has catalyzed a shift from single-step answer generation to the
development of LLMs explicitly designed for medical reasoning. This paper
provides the first systematic review of this emerging field. We propose a
taxonomy of reasoning enhancement techniques, categorized into training-time
strategies (e.g., supervised fine-tuning, reinforcement learning) and test-time
mechanisms (e.g., prompt engineering, multi-agent systems). We analyze how
these techniques are applied across different data modalities (text, image,
code) and in key clinical applications such as diagnosis, education, and
treatment planning. Furthermore, we survey the evolution of evaluation
benchmarks from simple accuracy metrics to sophisticated assessments of
reasoning quality and visual interpretability. Based on an analysis of 60
seminal studies from 2022-2025, we conclude by identifying critical challenges,
including the faithfulness-plausibility gap and the need for native multimodal
reasoning, and outlining future directions toward building efficient, robust,
and sociotechnically responsible medical AI.

</details>


### [32] [MELAC: Massive Evaluation of Large Language Models with Alignment of Culture in Persian Language](https://arxiv.org/abs/2508.00673)
*Farhan Farsi,Farnaz Aghababaloo,Shahriar Shariati Motlagh,Parsa Ghofrani,MohammadAli SadraeiJavaheri,Shayan Bali,Amirhossein Shabani,Farbod Bijary,Ghazal Zamaninejad,AmirMohammad Salehoof,Saeedeh Momtazi*

Main category: cs.CL

TL;DR: The paper introduces 19 new Persian-language datasets to evaluate LLMs on Iranian cultural and linguistic contexts, benchmarking 41 models to address gaps in non-Western evaluation resources.


<details>
  <summary>Details</summary>
Motivation: Address the lack of evaluation resources for non-English languages and non-Western cultural contexts in LLM assessments.

Method: Developed 19 Persian datasets (e.g., Iranian law, idioms, grammar) and benchmarked 41 LLMs.

Result: Provided benchmarks for LLMs in Persian language and Iranian culture, highlighting performance gaps.

Conclusion: The study bridges cultural and linguistic evaluation gaps, emphasizing the need for diverse LLM assessments.

Abstract: As large language models (LLMs) become increasingly embedded in our daily
lives, evaluating their quality and reliability across diverse contexts has
become essential. While comprehensive benchmarks exist for assessing LLM
performance in English, there remains a significant gap in evaluation resources
for other languages. Moreover, because most LLMs are trained primarily on data
rooted in European and American cultures, they often lack familiarity with
non-Western cultural contexts. To address this limitation, our study focuses on
the Persian language and Iranian culture. We introduce 19 new evaluation
datasets specifically designed to assess LLMs on topics such as Iranian law,
Persian grammar, Persian idioms, and university entrance exams. Using these
datasets, we benchmarked 41 prominent LLMs, aiming to bridge the existing
cultural and linguistic evaluation gap in the field.

</details>


### [33] [Team "better_call_claude": Style Change Detection using a Sequential Sentence Pair Classifier](https://arxiv.org/abs/2508.00675)
*Gleb Schmidt,Johannes Rmisch,Mariia Halchynska,Svetlana Gorovaia,Ivan P. Yamshchikov*

Main category: cs.CL

TL;DR: The paper proposes a Sequential Sentence Pair Classifier (SSPC) for style change detection at the sentence level, leveraging a pre-trained language model and BiLSTM for contextualization, achieving strong performance on PAN-2025 datasets.


<details>
  <summary>Details</summary>
Motivation: Style change detection is a key challenge in authorship analysis, especially at the fine-grained sentence level. The PAN 2025 shared task aims to address this with controlled datasets.

Method: Uses a pre-trained language model for sentence embeddings, contextualizes them with a BiLSTM, and predicts style changes via a multi-layer perceptron on adjacent sentence pairs.

Result: Achieves macro-F1 scores of 0.923 (EASY), 0.828 (MEDIUM), and 0.724 (HARD), outperforming baselines and zero-shot performance of claude-3.7-sonnet.

Conclusion: The SSPC model effectively leverages contextual information and addresses the challenge of 'stylistically shallow' sentences, proving robust on diverse datasets.

Abstract: Style change detection - identifying the points in a document where writing
style shifts - remains one of the most important and challenging problems in
computational authorship analysis. At PAN 2025, the shared task challenges
participants to detect style switches at the most fine-grained level:
individual sentences. The task spans three datasets, each designed with
controlled and increasing thematic variety within documents. We propose to
address this problem by modeling the content of each problem instance - that
is, a series of sentences - as a whole, using a Sequential Sentence Pair
Classifier (SSPC). The architecture leverages a pre-trained language model
(PLM) to obtain representations of individual sentences, which are then fed
into a bidirectional LSTM (BiLSTM) to contextualize them within the document.
The BiLSTM-produced vectors of adjacent sentences are concatenated and passed
to a multi-layer perceptron for prediction per adjacency. Building on the work
of previous PAN participants classical text segmentation, the approach is
relatively conservative and lightweight. Nevertheless, it proves effective in
leveraging contextual information and addressing what is arguably the most
challenging aspect of this year's shared task: the notorious problem of
"stylistically shallow", short sentences that are prevalent in the proposed
benchmark data. Evaluated on the official PAN-2025 test datasets, the model
achieves strong macro-F1 scores of 0.923, 0.828, and 0.724 on the EASY, MEDIUM,
and HARD data, respectively, outperforming not only the official random
baselines but also a much more challenging one: claude-3.7-sonnet's zero-shot
performance.

</details>


### [34] [Segment First, Retrieve Better: Realistic Legal Search via Rhetorical Role-Based Queries](https://arxiv.org/abs/2508.00679)
*Shubham Kumar Nigam,Tanmay Dubey,Noel Shallum,Arnab Bhattacharya*

Main category: cs.CL

TL;DR: TraceRetriever improves legal precedent retrieval by focusing on rhetorically significant segments, combining BM25, Vector Database, and Cross-Encoder models for scalable and reliable results.


<details>
  <summary>Details</summary>
Motivation: The growing complexity and volume of legal documents challenge traditional retrieval methods, necessitating a system that works with limited case information.

Method: The pipeline integrates BM25, Vector Database, and Cross-Encoder models, using Reciprocal Rank Fusion for combining results and a Hierarchical BiLSTM CRF classifier for rhetorical annotations.

Result: Evaluated on IL-PCR and COLIEE 2025 datasets, TraceRetriever addresses document volume challenges and aligns with practical search constraints.

Conclusion: TraceRetriever provides a reliable and scalable foundation for precedent retrieval, enhancing legal research with partial case knowledge.

Abstract: Legal precedent retrieval is a cornerstone of the common law system, governed
by the principle of stare decisis, which demands consistency in judicial
decisions. However, the growing complexity and volume of legal documents
challenge traditional retrieval methods. TraceRetriever mirrors real-world
legal search by operating with limited case information, extracting only
rhetorically significant segments instead of requiring complete documents. Our
pipeline integrates BM25, Vector Database, and Cross-Encoder models, combining
initial results through Reciprocal Rank Fusion before final re-ranking.
Rhetorical annotations are generated using a Hierarchical BiLSTM CRF classifier
trained on Indian judgments. Evaluated on IL-PCR and COLIEE 2025 datasets,
TraceRetriever addresses growing document volume challenges while aligning with
practical search constraints, reliable and scalable foundation for precedent
retrieval enhancing legal research when only partial case knowledge is
available.

</details>


### [35] [Better Call Claude: Can LLMs Detect Changes of Writing Style?](https://arxiv.org/abs/2508.00680)
*Johannes Rmisch,Svetlana Gorovaia,Mariia Halchynska,Gleb Schmidt,Ivan P. Yamshchikov*

Main category: cs.CL

TL;DR: The paper evaluates zero-shot performance of large language models (LLMs) on sentence-level style change detection, showing their sensitivity to stylistic variations and superior accuracy over PAN competition baselines.


<details>
  <summary>Details</summary>
Motivation: To assess how well state-of-the-art LLMs perform on the challenging task of detecting writing style changes at the sentence level.

Method: Benchmarked four LLMs on PAN 2024 and 2025 datasets for multi-author writing style analysis.

Result: LLMs are highly sensitive to stylistic variations, outperform PAN baselines, and may rely more on content-independent stylistic signals.

Conclusion: Modern LLMs excel in style change detection, setting a high benchmark and revealing unexpected sensitivity to stylistic cues.

Abstract: This article explores the zero-shot performance of state-of-the-art large
language models (LLMs) on one of the most challenging tasks in authorship
analysis: sentence-level style change detection. Benchmarking four LLMs on the
official PAN~2024 and 2025 "Multi-Author Writing Style Analysis" datasets, we
present several observations. First, state-of-the-art generative models are
sensitive to variations in writing style - even at the granular level of
individual sentences. Second, their accuracy establishes a challenging baseline
for the task, outperforming suggested baselines of the PAN competition.
Finally, we explore the influence of semantics on model predictions and present
evidence suggesting that the latest generation of LLMs may be more sensitive to
content-independent and purely stylistic signals than previously reported.

</details>


### [36] [NyayaRAG: Realistic Legal Judgment Prediction with RAG under the Indian Common Law System](https://arxiv.org/abs/2508.00709)
*Shubham Kumar Nigam,Balaramamahanthi Deepak Patnaik,Shivam Mishra,Ajay Varghese Thomas,Noel Shallum,Kripabandhu Ghosh,Arnab Bhattacharya*

Main category: cs.CL

TL;DR: NyayaRAG, a Retrieval-Augmented Generation framework, enhances Legal Judgment Prediction in India by integrating case facts, statutes, and precedents, improving accuracy and explanation quality.


<details>
  <summary>Details</summary>
Motivation: Previous LJP methods in India ignore statutory provisions and precedents, key in common law systems. NyayaRAG addresses this gap.

Method: NyayaRAG combines factual case descriptions, legal statutes, and retrieved prior cases using a domain-specific pipeline for the Indian legal system.

Result: Augmenting factual inputs with legal knowledge boosts predictive accuracy and explanation quality, as shown by lexical, semantic, and LLM-based evaluations.

Conclusion: NyayaRAG demonstrates the importance of integrating structured legal knowledge for better LJP outcomes in common law systems like India.

Abstract: Legal Judgment Prediction (LJP) has emerged as a key area in AI for law,
aiming to automate judicial outcome forecasting and enhance interpretability in
legal reasoning. While previous approaches in the Indian context have relied on
internal case content such as facts, issues, and reasoning, they often overlook
a core element of common law systems, which is reliance on statutory provisions
and judicial precedents. In this work, we propose NyayaRAG, a
Retrieval-Augmented Generation (RAG) framework that simulates realistic
courtroom scenarios by providing models with factual case descriptions,
relevant legal statutes, and semantically retrieved prior cases. NyayaRAG
evaluates the effectiveness of these combined inputs in predicting court
decisions and generating legal explanations using a domain-specific pipeline
tailored to the Indian legal system. We assess performance across various input
configurations using both standard lexical and semantic metrics as well as
LLM-based evaluators such as G-Eval. Our results show that augmenting factual
inputs with structured legal knowledge significantly improves both predictive
accuracy and explanation quality.

</details>


### [37] [Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and Context-Aware KGQA](https://arxiv.org/abs/2508.00719)
*Yingxu Wang,Shiqi Fan,Mengzhu Wang,Siwei Liu*

Main category: cs.CL

TL;DR: DAMR is a novel KGQA framework combining symbolic search with adaptive path evaluation, using MCTS and a lightweight Transformer scorer for efficient, context-aware reasoning.


<details>
  <summary>Details</summary>
Motivation: Address limitations of static path extraction in retrieve-then-reason methods and high computational costs of dynamic path generation with LLMs.

Method: Integrates MCTS guided by an LLM-based planner, a Transformer-based scorer for context-aware evaluation, and dynamic pseudo-path refinement for training.

Result: Outperforms state-of-the-art methods on multiple KGQA benchmarks.

Conclusion: DAMR offers an efficient, adaptable solution for KGQA by balancing symbolic search and dynamic evaluation.

Abstract: Knowledge Graph Question Answering (KGQA) aims to interpret natural language
queries and perform structured reasoning over knowledge graphs by leveraging
their relational and semantic structures to retrieve accurate answers. Recent
KGQA methods primarily follow either retrieve-then-reason paradigm, relying on
GNNs or heuristic rules for static paths extraction, or dynamic path generation
strategies that use large language models (LLMs) with prompting to jointly
perform retrieval and reasoning. However, the former suffers from limited
adaptability due to static path extraction and lack of contextual refinement,
while the latter incurs high computational costs and struggles with accurate
path evaluation due to reliance on fixed scoring functions and extensive LLM
calls. To address these issues, this paper proposes Dynamically Adaptive
MCTS-based Reasoning (DAMR), a novel framework that integrates symbolic search
with adaptive path evaluation for efficient and context-aware KGQA. DAMR
employs a Monte Carlo Tree Search (MCTS) backbone guided by an LLM-based
planner, which selects top-$k$ relevant relations at each step to reduce search
space. To improve path evaluation accuracy, we introduce a lightweight
Transformer-based scorer that performs context-aware plausibility estimation by
jointly encoding the question and relation sequence through cross-attention,
enabling the model to capture fine-grained semantic shifts during multi-hop
reasoning. Furthermore, to alleviate the scarcity of high-quality supervision,
DAMR incorporates a dynamic pseudo-path refinement mechanism that periodically
generates training signals from partial paths explored during search, allowing
the scorer to continuously adapt to the evolving distribution of reasoning
trajectories. Extensive experiments on multiple KGQA benchmarks show that DAMR
significantly outperforms state-of-the-art methods.

</details>


### [38] [Out-of-Context Abduction: LLMs Make Inferences About Procedural Data Leveraging Declarative Facts in Earlier Training Data](https://arxiv.org/abs/2508.00741)
*Sohaib Imran,Rob Lamb,Peter M. Atkinson*

Main category: cs.CL

TL;DR: GPT 4o can infer chatbot names and behaviors from training data, showing situational awareness and implications for AI safety.


<details>
  <summary>Details</summary>
Motivation: To investigate if LLMs can reason about information in their training data, specifically through out-of-context abduction.

Method: Experiments with GPT 4o, training it on fictitious chatbot names and behaviors but not dialogues, then testing inference and behavior replication.

Result: GPT 4o correctly inferred chatbot names and replicated behaviors when trained on descriptions.

Conclusion: LLMs like GPT 4o demonstrate situational awareness, raising important considerations for AI safety.

Abstract: Large language models (LLMs) are trained on large corpora, yet it is unclear
whether they can reason about the information present within their training
data. We design experiments to study out-of-context abduction in LLMs, the
ability to infer the most plausible explanations for observations using
relevant facts present in training data. We train treatment LLMs on names and
behavior descriptions of fictitious chatbots, but not on examples of dialogue
with the chatbots. We find that OpenAI's GPT 4o LLM can correctly infer at
least one chatbot's name after observing example responses characteristic of
that chatbot. We also find that previously training GPT 4o on descriptions of a
chatbot's behavior allows it to display behaviors more characteristic of the
chatbot when iteratively trained to display such behaviors. Our results have
implications for situational awareness in LLMs and, therefore, for AI safety.

</details>


### [39] [Applying Psychometrics to Large Language Model Simulated Populations: Recreating the HEXACO Personality Inventory Experiment with Generative Agents](https://arxiv.org/abs/2508.00742)
*Sarah Mercer,Daniel P. Martin,Phil Swatton*

Main category: cs.CL

TL;DR: GPT-4 agents partially align with HEXACO personality traits, showing reliability but model-specific biases.


<details>
  <summary>Details</summary>
Motivation: To validate persona-based generative agents as substitutes for human participants in social science research.

Method: Recreated HEXACO personality inventory with 310 GPT-4 agents, analyzed responses, and compared to original human data.

Result: Agents showed coherent personality structure, reliability within GPT-4, but cross-model variability revealed biases.

Conclusion: Generative agents are promising but require careful design to represent human traits accurately.

Abstract: Generative agents powered by Large Language Models demonstrate human-like
characteristics through sophisticated natural language interactions. Their
ability to assume roles and personalities based on predefined character
biographies has positioned them as cost-effective substitutes for human
participants in social science research. This paper explores the validity of
such persona-based agents in representing human populations; we recreate the
HEXACO personality inventory experiment by surveying 310 GPT-4 powered agents,
conducting factor analysis on their responses, and comparing these results to
the original findings presented by Ashton, Lee, & Goldberg in 2004. Our results
found 1) a coherent and reliable personality structure was recoverable from the
agents' responses demonstrating partial alignment to the HEXACO framework. 2)
the derived personality dimensions were consistent and reliable within GPT-4,
when coupled with a sufficiently curated population, and 3) cross-model
analysis revealed variability in personality profiling, suggesting
model-specific biases and limitations. We discuss the practical considerations
and challenges encountered during the experiment. This study contributes to the
ongoing discourse on the potential benefits and limitations of using generative
agents in social science research and provides useful guidance on designing
consistent and representative agent personas to maximise coverage and
representation of human personality traits.

</details>


### [40] [Agentic large language models improve retrieval-based radiology question answering](https://arxiv.org/abs/2508.00743)
*Sebastian Wind,Jeta Sopa,Daniel Truhn,Mahshad Lotfinia,Tri-Thien Nguyen,Keno Bressem,Lisa Adams,Mirabela Rusu,Harald Kstler,Gerhard Wellein,Andreas Maier,Soroosh Tayebi Arasteh*

Main category: cs.CL

TL;DR: An agentic RAG framework improves radiology QA by enabling LLMs to decompose questions, retrieve evidence iteratively, and synthesize responses, boosting accuracy and reducing hallucinations.


<details>
  <summary>Details</summary>
Motivation: Traditional RAG systems for radiology QA rely on single-step retrieval, limiting complex clinical reasoning. This paper aims to enhance diagnostic accuracy and factual grounding using an agentic approach.

Method: Proposed an agentic RAG framework for LLMs to autonomously decompose questions, iteratively retrieve evidence from Radiopaedia, and synthesize responses. Evaluated 24 LLMs on expert-curated radiology questions.

Result: Agentic retrieval improved mean diagnostic accuracy (73% vs. 64% for zero-shot, 73% vs. 68% for conventional RAG), reduced hallucinations (9.4%), and increased clinically relevant context retrieval (46%). Mid-sized models showed the greatest gains.

Conclusion: Agentic frameworks enhance factuality and diagnostic accuracy in radiology QA, especially for mid-sized LLMs, suggesting complementary roles of retrieval and fine-tuning. Future studies should validate clinical utility.

Abstract: Clinical decision-making in radiology increasingly benefits from artificial
intelligence (AI), particularly through large language models (LLMs). However,
traditional retrieval-augmented generation (RAG) systems for radiology question
answering (QA) typically rely on single-step retrieval, limiting their ability
to handle complex clinical reasoning tasks. Here we propose an agentic RAG
framework enabling LLMs to autonomously decompose radiology questions,
iteratively retrieve targeted clinical evidence from Radiopaedia, and
dynamically synthesize evidence-based responses. We evaluated 24 LLMs spanning
diverse architectures, parameter scales (0.5B to >670B), and training paradigms
(general-purpose, reasoning-optimized, clinically fine-tuned), using 104
expert-curated radiology questions from previously established RSNA-RadioQA and
ExtendedQA datasets. Agentic retrieval significantly improved mean diagnostic
accuracy over zero-shot prompting (73% vs. 64%; P<0.001) and conventional
online RAG (73% vs. 68%; P<0.001). The greatest gains occurred in mid-sized
models (e.g., Mistral Large improved from 72% to 81%) and small-scale models
(e.g., Qwen 2.5-7B improved from 55% to 71%), while very large models (>200B
parameters) demonstrated minimal changes (<2% improvement). Additionally,
agentic retrieval reduced hallucinations (mean 9.4%) and retrieved clinically
relevant context in 46% of cases, substantially aiding factual grounding. Even
clinically fine-tuned models exhibited meaningful improvements (e.g.,
MedGemma-27B improved from 71% to 81%), indicating complementary roles of
retrieval and fine-tuning. These results highlight the potential of agentic
frameworks to enhance factuality and diagnostic accuracy in radiology QA,
particularly among mid-sized LLMs, warranting future studies to validate their
clinical utility.

</details>


### [41] [GLiDRE: Generalist Lightweight model for Document-level Relation Extraction](https://arxiv.org/abs/2508.00757)
*Robin Armingaud,Romaric Besanon*

Main category: cs.CL

TL;DR: GLiDRE, a new model for document-level relation extraction, outperforms state-of-the-art models in few-shot settings, inspired by GLiNER's compact NER success.


<details>
  <summary>Details</summary>
Motivation: Address the underexplored performance of current ATLOP-based approaches in zero-shot/few-shot settings for document-level RE, inspired by GLiNER's efficiency.

Method: Introduces GLiDRE, building on GLiNER's key ideas, and benchmarks it against state-of-the-art models on Re-DocRED.

Result: GLiDRE achieves state-of-the-art performance in few-shot scenarios.

Conclusion: GLiDRE is effective for few-shot document-level RE, with publicly available code.

Abstract: Relation Extraction (RE) is a fundamental task in Natural Language
Processing, and its document-level variant poses significant challenges, due to
the need to model complex interactions between entities across sentences.
Current approaches, largely based on the ATLOP architecture, are commonly
evaluated on benchmarks like DocRED and Re-DocRED. However, their performance
in zero-shot or few-shot settings remains largely underexplored due to the
task's complexity. Recently, the GLiNER model has shown that a compact NER
model can outperform much larger Large Language Models. With a similar
motivation, we introduce GLiDRE, a new model for document-level relation
extraction that builds on the key ideas of GliNER. We benchmark GLiDRE against
state-of-the-art models across various data settings on the Re-DocRED dataset.
Our results demonstrate that GLiDRE achieves state-of-the-art performance in
few-shot scenarios. Our code is publicly available.

</details>


### [42] [MMBERT: Scaled Mixture-of-Experts Multimodal BERT for Robust Chinese Hate Speech Detection under Cloaking Perturbations](https://arxiv.org/abs/2508.00760)
*Qiyao Xue,Yuchen Dou,Ryan Shi,Xiang Lorraine Li,Wei Gao*

Main category: cs.CL

TL;DR: MMBERT, a BERT-based multimodal framework, outperforms existing models in Chinese hate speech detection by integrating text, speech, and visual data via a Mixture-of-Experts architecture and progressive training.


<details>
  <summary>Details</summary>
Motivation: Hate speech detection in Chinese is challenging due to cloaking techniques and limited multimodal research. Existing work focuses on English, leaving a gap for Chinese-specific solutions.

Method: Proposes MMBERT, combining text, speech, and visuals using a Mixture-of-Experts (MoE) architecture with modality-specific experts, shared self-attention, and a router-based expert allocation. Uses a three-stage training paradigm for stability.

Result: MMBERT outperforms fine-tuned BERT models, LLMs, and in-context learning approaches on Chinese hate speech datasets.

Conclusion: MMBERT is a robust solution for Chinese hate speech detection, leveraging multimodal data and MoE to address evasion techniques effectively.

Abstract: Hate speech detection on Chinese social networks presents distinct
challenges, particularly due to the widespread use of cloaking techniques
designed to evade conventional text-based detection systems. Although large
language models (LLMs) have recently improved hate speech detection
capabilities, the majority of existing work has concentrated on English
datasets, with limited attention given to multimodal strategies in the Chinese
context. In this study, we propose MMBERT, a novel BERT-based multimodal
framework that integrates textual, speech, and visual modalities through a
Mixture-of-Experts (MoE) architecture. To address the instability associated
with directly integrating MoE into BERT-based models, we develop a progressive
three-stage training paradigm. MMBERT incorporates modality-specific experts, a
shared self-attention mechanism, and a router-based expert allocation strategy
to enhance robustness against adversarial perturbations. Empirical results in
several Chinese hate speech datasets show that MMBERT significantly surpasses
fine-tuned BERT-based encoder models, fine-tuned LLMs, and LLMs utilizing
in-context learning approaches.

</details>


### [43] [ITUNLP at SemEval-2025 Task 8: Question-Answering over Tabular Data: A Zero-Shot Approach using LLM-Driven Code Generation](https://arxiv.org/abs/2508.00762)
*Atakan Site,Emre Hakan Erdemir,Glen Eryiit*

Main category: cs.CL

TL;DR: The paper introduces a zero-shot system for SemEval-2025 Task 8, using LLM-based Python code generation for tabular QA, achieving competitive rankings.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of question answering over tabular data in diverse domains, leveraging LLMs for efficient code generation.

Method: A Python code generation framework using open-source LLMs with optimized prompting to generate executable Pandas code.

Result: Different LLMs vary in effectiveness; Python code generation outperforms other methods. The system ranked 8th in Subtask I and 6th in Subtask II among 30 systems.

Conclusion: LLM-based Python code generation is effective for tabular QA, with potential for further optimization.

Abstract: This paper presents our system for SemEval-2025 Task 8: DataBench,
Question-Answering over Tabular Data. The primary objective of this task is to
perform question answering on given tabular datasets from diverse domains under
two subtasks: DataBench QA (Subtask I) and DataBench Lite QA (Subtask II). To
tackle both subtasks, we developed a zero-shot solution with a particular
emphasis on leveraging Large Language Model (LLM)-based code generation.
Specifically, we propose a Python code generation framework utilizing
state-of-the-art open-source LLMs to generate executable Pandas code via
optimized prompting strategies. Our experiments reveal that different LLMs
exhibit varying levels of effectiveness in Python code generation.
Additionally, results show that Python code generation achieves superior
performance in tabular question answering compared to alternative approaches.
Although our ranking among zero-shot systems is unknown at the time of this
paper's submission, our system achieved eighth place in Subtask I and sixth
place in Subtask~II among the 30 systems that outperformed the baseline in the
open-source models category.

</details>


### [44] [Do They Understand Them? An Updated Evaluation on Nonbinary Pronoun Handling in Large Language Models](https://arxiv.org/abs/2508.00788)
*Xushuo Tang,Yi Ding,Zhengyi Yang,Yin Chen,Yongrui Gu,Wenke Yang,Mingchen Ju,Xin Cao,Yongfei Liu,Wenjie Zhang*

Main category: cs.CL

TL;DR: The paper introduces MISGENDERED+, an updated benchmark for evaluating LLMs' handling of inclusive pronouns, testing five models and showing improvements in binary and gender-neutral pronouns but inconsistencies in neopronouns.


<details>
  <summary>Details</summary>
Motivation: To address fairness and inclusivity in LLMs, particularly in pronoun usage, as prior benchmarks like MISGENDERED were outdated and limited.

Method: Extended the MISGENDERED benchmark to MISGENDERED+ and evaluated five LLMs (GPT-4o, Claude 4, DeepSeek-V3, Qwen Turbo, Qwen2.5) across zero-shot, few-shot, and gender identity inference tasks.

Result: Notable improvements in binary and gender-neutral pronoun accuracy, but inconsistent performance on neopronouns and reverse inference tasks.

Conclusion: Persistent gaps in identity-sensitive reasoning highlight the need for further research in inclusive AI.

Abstract: Large language models (LLMs) are increasingly deployed in sensitive contexts
where fairness and inclusivity are critical. Pronoun usage, especially
concerning gender-neutral and neopronouns, remains a key challenge for
responsible AI. Prior work, such as the MISGENDERED benchmark, revealed
significant limitations in earlier LLMs' handling of inclusive pronouns, but
was constrained to outdated models and limited evaluations. In this study, we
introduce MISGENDERED+, an extended and updated benchmark for evaluating LLMs'
pronoun fidelity. We benchmark five representative LLMs, GPT-4o, Claude 4,
DeepSeek-V3, Qwen Turbo, and Qwen2.5, across zero-shot, few-shot, and gender
identity inference. Our results show notable improvements compared with
previous studies, especially in binary and gender-neutral pronoun accuracy.
However, accuracy on neopronouns and reverse inference tasks remains
inconsistent, underscoring persistent gaps in identity-sensitive reasoning. We
discuss implications, model-specific observations, and avenues for future
inclusive AI research.

</details>


### [45] [Beyond Fixed: Variable-Length Denoising for Diffusion Large Language Models](https://arxiv.org/abs/2508.00819)
*Jinsong Li,Xiaoyi Dong,Yuhang Zang,Yuhang Cao,Jiaqi Wang,Dahua Lin*

Main category: cs.CL

TL;DR: DAEDAL introduces a dynamic adaptive length expansion strategy for Diffusion Large Language Models (DLLMs), eliminating the need for static predefined generation lengths and improving efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: The static length constraint in DLLMs leads to inefficiency and performance issues, either underperforming on complex tasks or wasting resources with excessive lengths.

Method: DAEDAL uses latent signals in DLLMs to dynamically adjust generation length in two phases: coarse length expansion before denoising and targeted expansion during denoising.

Result: DAEDAL matches or outperforms fixed-length baselines while improving computational efficiency by optimizing token usage.

Conclusion: DAEDAL resolves DLLMs' static length limitation, enhancing their practicality and bridging the gap with Autoregressive models.

Abstract: Diffusion Large Language Models (DLLMs) are emerging as a powerful
alternative to the dominant Autoregressive Large Language Models, offering
efficient parallel generation and capable global context modeling. However, the
practical application of DLLMs is hindered by a critical architectural
constraint: the need for a statically predefined generation length. This static
length allocation leads to a problematic trade-off: insufficient lengths
cripple performance on complex tasks, while excessive lengths incur significant
computational overhead and sometimes result in performance degradation. While
the inference framework is rigid, we observe that the model itself possesses
internal signals that correlate with the optimal response length for a given
task. To bridge this gap, we leverage these latent signals and introduce
DAEDAL, a novel training-free denoising strategy that enables Dynamic Adaptive
Length Expansion for Diffusion Large Language Models. DAEDAL operates in two
phases: 1) Before the denoising process, DAEDAL starts from a short initial
length and iteratively expands it to a coarse task-appropriate length, guided
by a sequence completion metric. 2) During the denoising process, DAEDAL
dynamically intervenes by pinpointing and expanding insufficient generation
regions through mask token insertion, ensuring the final output is fully
developed. Extensive experiments on DLLMs demonstrate that DAEDAL achieves
performance comparable, and in some cases superior, to meticulously tuned
fixed-length baselines, while simultaneously enhancing computational efficiency
by achieving a higher effective token ratio. By resolving the static length
constraint, DAEDAL unlocks new potential for DLLMs, bridging a critical gap
with their Autoregressive counterparts and paving the way for more efficient
and capable generation.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [46] [A Quality-Guided Mixture of Score-Fusion Experts Framework for Human Recognition](https://arxiv.org/abs/2508.00053)
*Jie Zhu,Yiyang Su,Minchul Kim,Anil Jain,Xiaoming Liu*

Main category: cs.CV

TL;DR: The paper introduces QME, a learnable score-fusion framework for whole-body biometric recognition, addressing limitations of traditional methods by incorporating quality estimation and metric improvement.


<details>
  <summary>Details</summary>
Motivation: Traditional score-fusion methods for whole-body biometric recognition overlook score distribution variations, limiting performance. QME aims to improve this by integrating quality-guided fusion.

Method: QME uses a Mixture of Experts (MoE) for learnable score-fusion, with a pseudo-quality loss for quality estimation and a score triplet loss for metric enhancement.

Result: Experiments show QME achieves state-of-the-art performance on whole-body biometric datasets, outperforming baseline methods.

Conclusion: QME effectively addresses challenges like model misalignment and data quality variability, proving superior for multimodal and multi-model biometric recognition.

Abstract: Whole-body biometric recognition is a challenging multimodal task that
integrates various biometric modalities, including face, gait, and body. This
integration is essential for overcoming the limitations of unimodal systems.
Traditionally, whole-body recognition involves deploying different models to
process multiple modalities, achieving the final outcome by score-fusion (e.g.,
weighted averaging of similarity matrices from each model). However, these
conventional methods may overlook the variations in score distributions of
individual modalities, making it challenging to improve final performance. In
this work, we present \textbf{Q}uality-guided \textbf{M}ixture of score-fusion
\textbf{E}xperts (QME), a novel framework designed for improving whole-body
biometric recognition performance through a learnable score-fusion strategy
using a Mixture of Experts (MoE). We introduce a novel pseudo-quality loss for
quality estimation with a modality-specific Quality Estimator (QE), and a score
triplet loss to improve the metric performance. Extensive experiments on
multiple whole-body biometric datasets demonstrate the effectiveness of our
proposed approach, achieving state-of-the-art results across various metrics
compared to baseline methods. Our method is effective for multimodal and
multi-model, addressing key challenges such as model misalignment in the
similarity score domain and variability in data quality.

</details>


### [47] [Punching Bag vs. Punching Person: Motion Transferability in Videos](https://arxiv.org/abs/2508.00085)
*Raiyaan Abdullah,Jared Claypoole,Michael Cogswell,Ajay Divakaran,Yogesh Rawat*

Main category: cs.CV

TL;DR: The paper investigates motion transferability in action recognition models, revealing performance drops in novel contexts and proposing a framework with synthetic and real-world datasets.


<details>
  <summary>Details</summary>
Motivation: To assess if action recognition models can generalize high-level motion concepts across diverse contexts, even within similar distributions.

Method: Introduces a motion transferability framework with three datasets (Syn-TA, Kinetics400-TA, Something-Something-v2-TA) and evaluates 13 state-of-the-art models.

Result: Performance drops significantly for novel contexts; multimodal models struggle with fine-grained actions, and larger models face challenges with temporal reasoning.

Conclusion: The study establishes a benchmark for motion transferability, highlighting the need for disentangling coarse and fine motions to improve recognition.

Abstract: Action recognition models demonstrate strong generalization, but can they
effectively transfer high-level motion concepts across diverse contexts, even
within similar distributions? For example, can a model recognize the broad
action "punching" when presented with an unseen variation such as "punching
person"? To explore this, we introduce a motion transferability framework with
three datasets: (1) Syn-TA, a synthetic dataset with 3D object motions; (2)
Kinetics400-TA; and (3) Something-Something-v2-TA, both adapted from natural
video datasets. We evaluate 13 state-of-the-art models on these benchmarks and
observe a significant drop in performance when recognizing high-level actions
in novel contexts. Our analysis reveals: 1) Multimodal models struggle more
with fine-grained unknown actions than with coarse ones; 2) The bias-free
Syn-TA proves as challenging as real-world datasets, with models showing
greater performance drops in controlled settings; 3) Larger models improve
transferability when spatial cues dominate but struggle with intensive temporal
reasoning, while reliance on object and background cues hinders generalization.
We further explore how disentangling coarse and fine motions can improve
recognition in temporally challenging datasets. We believe this study
establishes a crucial benchmark for assessing motion transferability in action
recognition. Datasets and relevant code:
https://github.com/raiyaan-abdullah/Motion-Transfer.

</details>


### [48] [The Monado SLAM Dataset for Egocentric Visual-Inertial Tracking](https://arxiv.org/abs/2508.00088)
*Mateo de Mayo,Daniel Cremers,Taih Pire*

Main category: cs.CV

TL;DR: The paper introduces the Monado SLAM dataset to address gaps in existing VIO/SLAM datasets for head-mounted tracking, covering challenging real-world scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing VIO/SLAM systems struggle with head-mounted tracking challenges like high-intensity motions, dynamic occlusions, and adverse conditions, which are underrepresented in current datasets.

Method: The authors present the Monado SLAM dataset, comprising real sequences from multiple VR headsets, released under a permissive CC BY 4.0 license.

Result: The dataset aims to improve VIO/SLAM research by providing data for overlooked real-world scenarios.

Conclusion: The Monado SLAM dataset is a resource to advance tracking systems by addressing critical gaps in existing datasets.

Abstract: Humanoid robots and mixed reality headsets benefit from the use of
head-mounted sensors for tracking. While advancements in visual-inertial
odometry (VIO) and simultaneous localization and mapping (SLAM) have produced
new and high-quality state-of-the-art tracking systems, we show that these are
still unable to gracefully handle many of the challenging settings presented in
the head-mounted use cases. Common scenarios like high-intensity motions,
dynamic occlusions, long tracking sessions, low-textured areas, adverse
lighting conditions, saturation of sensors, to name a few, continue to be
covered poorly by existing datasets in the literature. In this way, systems may
inadvertently overlook these essential real-world issues. To address this, we
present the Monado SLAM dataset, a set of real sequences taken from multiple
virtual reality headsets. We release the dataset under a permissive CC BY 4.0
license, to drive advancements in VIO/SLAM research and development.

</details>


### [49] [Exploring the Feasibility of Deep Learning Techniques for Accurate Gender Classification from Eye Images](https://arxiv.org/abs/2508.00135)
*Basna Mohammed Salih Hasan,Ramadhan J. Mstafa*

Main category: cs.CV

TL;DR: A CNN model for gender classification using the periocular region achieves high accuracy (99% and 96%) on two datasets, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Gender classification is vital in security and surveillance but is affected by cosmetics and disguise. The study focuses on the periocular region for reliable classification.

Method: A sophisticated CNN model is developed, tested on CVBL and (Female and Male) datasets, and compared with other approaches.

Result: The model achieved 99% accuracy on CVBL and 96% on (Female and Male) with minimal parameters (7,235,089).

Conclusion: The model is highly effective, suggesting practical applications in security and surveillance.

Abstract: Gender classification has emerged as a crucial aspect in various fields,
including security, human-machine interaction, surveillance, and advertising.
Nonetheless, the accuracy of this classification can be influenced by factors
such as cosmetics and disguise. Consequently, our study is dedicated to
addressing this concern by concentrating on gender classification using color
images of the periocular region. The periocular region refers to the area
surrounding the eye, including the eyelids, eyebrows, and the region between
them. It contains valuable visual cues that can be used to extract key features
for gender classification. This paper introduces a sophisticated Convolutional
Neural Network (CNN) model that utilizes color image databases to evaluate the
effectiveness of the periocular region for gender classification. To validate
the model's performance, we conducted tests on two eye datasets, namely CVBL
and (Female and Male). The recommended architecture achieved an outstanding
accuracy of 99% on the previously unused CVBL dataset while attaining a
commendable accuracy of 96% with a small number of learnable parameters
(7,235,089) on the (Female and Male) dataset. To ascertain the effectiveness of
our proposed model for gender classification using the periocular region, we
evaluated its performance through an extensive range of metrics and compared it
with other state-of-the-art approaches. The results unequivocally demonstrate
the efficacy of our model, thereby suggesting its potential for practical
application in domains such as security and surveillance.

</details>


### [50] [World Consistency Score: A Unified Metric for Video Generation Quality](https://arxiv.org/abs/2508.00144)
*Akshat Rakheja,Aarsh Ashdhir,Aryan Bhattacharjee,Vanshika Sharma*

Main category: cs.CV

TL;DR: World Consistency Score (WCS) is a new metric for evaluating generative video models, focusing on internal world consistency through four sub-components. It combines these into a single score aligned with human judgment.


<details>
  <summary>Details</summary>
Motivation: Existing video evaluation metrics often overlook temporal and physical coherence, focusing instead on visual fidelity or prompt alignment. WCS addresses this gap by emphasizing world consistency.

Method: WCS integrates four submetrics (object permanence, relation stability, causal compliance, flicker penalty) computed using open-source tools. A learned weighted formula combines these into a unified score.

Result: WCS is validated using benchmarks like VBench-2.0 and compared to metrics like FVD and CLIPScore, showing alignment with human evaluations.

Conclusion: WCS provides a comprehensive, interpretable framework for assessing video generation models' ability to maintain a coherent world over time, filling gaps in existing metrics.

Abstract: We introduce World Consistency Score (WCS), a novel unified evaluation metric
for generative video models that emphasizes internal world consistency of the
generated videos. WCS integrates four interpretable sub-components - object
permanence, relation stability, causal compliance, and flicker penalty - each
measuring a distinct aspect of temporal and physical coherence in a video.
These submetrics are combined via a learned weighted formula to produce a
single consistency score that aligns with human judgments. We detail the
motivation for WCS in the context of existing video evaluation metrics,
formalize each submetric and how it is computed with open-source tools
(trackers, action recognizers, CLIP embeddings, optical flow), and describe how
the weights of the WCS combination are trained using human preference data. We
also outline an experimental validation blueprint: using benchmarks like
VBench-2.0, EvalCrafter, and LOVE to test WCS's correlation with human
evaluations, performing sensitivity analyses, and comparing WCS against
established metrics (FVD, CLIPScore, VBench, FVMD). The proposed WCS offers a
comprehensive and interpretable framework for evaluating video generation
models on their ability to maintain a coherent "world" over time, addressing
gaps left by prior metrics focused only on visual fidelity or prompt alignment.

</details>


### [51] [GeoExplorer: Active Geo-localization with Curiosity-Driven Exploration](https://arxiv.org/abs/2508.00152)
*Li Mi,Manon Bechaz,Zeming Chen,Antoine Bosselut,Devis Tuia*

Main category: cs.CV

TL;DR: GeoExplorer introduces curiosity-driven exploration for Active Geo-localization (AGL), improving robustness and generalization over distance-based RL methods.


<details>
  <summary>Details</summary>
Motivation: Current AGL methods rely on distance-based rewards, which struggle with challenging distance estimation and unseen targets/environments, reducing robustness.

Method: Proposes GeoExplorer, an AGL agent using curiosity-driven intrinsic rewards for goal-agnostic, diverse exploration.

Result: Demonstrates effectiveness across four AGL benchmarks, excelling in unfamiliar targets and environments.

Conclusion: GeoExplorer's curiosity-driven approach enhances AGL robustness and generalization, outperforming traditional methods.

Abstract: Active Geo-localization (AGL) is the task of localizing a goal, represented
in various modalities (e.g., aerial images, ground-level images, or text),
within a predefined search area. Current methods approach AGL as a
goal-reaching reinforcement learning (RL) problem with a distance-based reward.
They localize the goal by implicitly learning to minimize the relative distance
from it. However, when distance estimation becomes challenging or when
encountering unseen targets and environments, the agent exhibits reduced
robustness and generalization ability due to the less reliable exploration
strategy learned during training. In this paper, we propose GeoExplorer, an AGL
agent that incorporates curiosity-driven exploration through intrinsic rewards.
Unlike distance-based rewards, our curiosity-driven reward is goal-agnostic,
enabling robust, diverse, and contextually relevant exploration based on
effective environment modeling. These capabilities have been proven through
extensive experiments across four AGL benchmarks, demonstrating the
effectiveness and generalization ability of GeoExplorer in diverse settings,
particularly in localizing unfamiliar targets and environments.

</details>


### [52] [Robust 3D Object Detection using Probabilistic Point Clouds from Single-Photon LiDARs](https://arxiv.org/abs/2508.00169)
*Bhavya Goyal,Felipe Gutierrez-Barragan,Wei Lin,Andreas Velten,Yin Li,Mohit Gupta*

Main category: cs.CV

TL;DR: The paper introduces Probabilistic Point Clouds (PPC), a 3D representation that includes uncertainty attributes for each point, improving accuracy in LiDAR-based scene understanding tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges like sparse or erroneous point clouds from LiDARs in scenarios involving long-distance or low-albedo objects, which degrade downstream perception models.

Method: Proposes PPC, where each point has a probability attribute reflecting measurement uncertainty, and introduces inference methods for robust 3D object detection.

Result: PPC-based methods outperform baselines in simulations and real captures, especially for small, distant, low-albedo objects and strong ambient light.

Conclusion: PPC enhances 3D perception by incorporating uncertainty, offering a lightweight, versatile solution for improving LiDAR-based models.

Abstract: LiDAR-based 3D sensors provide point clouds, a canonical 3D representation
used in various scene understanding tasks. Modern LiDARs face key challenges in
several real-world scenarios, such as long-distance or low-albedo objects,
producing sparse or erroneous point clouds. These errors, which are rooted in
the noisy raw LiDAR measurements, get propagated to downstream perception
models, resulting in potentially severe loss of accuracy. This is because
conventional 3D processing pipelines do not retain any uncertainty information
from the raw measurements when constructing point clouds.
  We propose Probabilistic Point Clouds (PPC), a novel 3D scene representation
where each point is augmented with a probability attribute that encapsulates
the measurement uncertainty (or confidence) in the raw data. We further
introduce inference approaches that leverage PPC for robust 3D object
detection; these methods are versatile and can be used as computationally
lightweight drop-in modules in 3D inference pipelines. We demonstrate, via both
simulations and real captures, that PPC-based 3D inference methods outperform
several baselines using LiDAR as well as camera-LiDAR fusion models, across
challenging indoor and outdoor scenarios involving small, distant, and
low-albedo objects, as well as strong ambient light.
  Our project webpage is at https://bhavyagoyal.github.io/ppc .

</details>


### [53] [On the Risk of Misleading Reports: Diagnosing Textual Biases in Multimodal Clinical AI](https://arxiv.org/abs/2508.00171)
*David Restrepo,Ira Ktena,Maria Vakalopoulou,Stergios Christodoulidis,Enzo Ferrante*

Main category: cs.CV

TL;DR: The paper introduces Selective Modality Shifting (SMS) to quantify modality biases in Vision-Language Models (VLMs) for medical tasks, revealing a strong reliance on text over visual cues.


<details>
  <summary>Details</summary>
Motivation: To address biases in VLMs that favor text over visual information in clinical decision-making, which can overlook critical visual cues.

Method: SMS, a perturbation-based approach, swaps images or text between samples with opposing labels to expose biases. Evaluated on six VLMs using MIMIC-CXR and FairVLMed datasets.

Result: VLMs show marked dependency on text, overshadowing visual information, even in models fine-tuned for medical data.

Conclusion: Highlights the need for multimodal models that genuinely integrate both visual and textual cues, not just single-modality signals.

Abstract: Clinical decision-making relies on the integrated analysis of medical images
and the associated clinical reports. While Vision-Language Models (VLMs) can
offer a unified framework for such tasks, they can exhibit strong biases toward
one modality, frequently overlooking critical visual cues in favor of textual
information. In this work, we introduce Selective Modality Shifting (SMS), a
perturbation-based approach to quantify a model's reliance on each modality in
binary classification tasks. By systematically swapping images or text between
samples with opposing labels, we expose modality-specific biases. We assess six
open-source VLMs-four generalist models and two fine-tuned for medical data-on
two medical imaging datasets with distinct modalities: MIMIC-CXR (chest X-ray)
and FairVLMed (scanning laser ophthalmoscopy). By assessing model performance
and the calibration of every model in both unperturbed and perturbed settings,
we reveal a marked dependency on text input, which persists despite the
presence of complementary visual information. We also perform a qualitative
attention-based analysis which further confirms that image content is often
overshadowed by text details. Our findings highlight the importance of
designing and evaluating multimodal medical models that genuinely integrate
visual and textual cues, rather than relying on single-modality signals.

</details>


### [54] [Graph Lineages and Skeletal Graph Products](https://arxiv.org/abs/2508.00197)
*Eric Mjolsness,Cory B. Scott*

Main category: cs.CV

TL;DR: The paper introduces structured graph lineages for hierarchical model architectures, enabling exponential growth, skeletal algebraic operations, and applications in machine learning and computational science.


<details>
  <summary>Details</summary>
Motivation: To define hierarchical graph structures for modeling in fields like machine learning and computational science, enabling efficient operations and continuum limits.

Method: Defines graph lineages with hierarchical growth, bipartite connections, and prolongation maps. Introduces skeletal algebraic operations and unary operators like thickening and escalation.

Result: Develops an algebraic type theory for graded graphs and hierarchical lineages, applicable to deep neural networks and multigrid methods.

Conclusion: The framework is suitable for hierarchical model architectures and local algorithms, demonstrated in deep learning and numerical methods.

Abstract: Graphs, and sequences of growing graphs, can be used to specify the
architecture of mathematical models in many fields including machine learning
and computational science. Here we define structured graph "lineages" (ordered
by level number) that grow in a hierarchical fashion, so that: (1) the number
of graph vertices and edges increases exponentially in level number; (2)
bipartite graphs connect successive levels within a graph lineage and, as in
multigrid methods, can constrain matrices relating successive levels; (3) using
prolongation maps within a graph lineage, process-derived distance measures
between graphs at successive levels can be defined; (4) a category of "graded
graphs" can be defined, and using it low-cost "skeletal" variants of standard
algebraic graph operations and type constructors (cross product, box product,
disjoint sum, and function types) can be derived for graded graphs and hence
hierarchical graph lineages; (5) these skeletal binary operators have similar
but not identical algebraic and category-theoretic properties to their standard
counterparts; (6) graph lineages and their skeletal product constructors can
approach continuum limit objects. Additional space-efficient unary operators on
graded graphs are also derived: thickening, which creates a graph lineage of
multiscale graphs, and escalation to a graph lineage of search frontiers
(useful as a generalization of adaptive grids and in defining "skeletal"
functions). The result is an algebraic type theory for graded graphs and
(hierarchical) graph lineages. The approach is expected to be well suited to
defining hierarchical model architectures - "hierarchitectures" - and local
sampling, search, or optimization algorithms on them. We demonstrate such
application to deep neural networks (including visual and feature scale spaces)
and to multigrid numerical methods.

</details>


### [55] [Learning Personalised Human Internal Cognition from External Expressive Behaviours for Real Personality Recognition](https://arxiv.org/abs/2508.00205)
*Xiangyu Kong,Hengde Zhu,Haoqin Sun,Zhihao Guo,Jiayan Gu,Xinyi Ni,Wei Zhang,Shizhe Liu,Siyang Song*

Main category: cs.CV

TL;DR: The paper introduces a novel approach for real personality recognition (RPR) by simulating personalized internal cognition from external behaviors, using a 2D-GNN for inference.


<details>
  <summary>Details</summary>
Motivation: Existing RPR methods infer personality impressions from external behaviors, often deviating from real personalities. The paper aims to bridge this gap by leveraging internal cognition.

Method: Proposes a method to simulate personalized cognition from audio-visual behaviors, represented as a graph with 2D node/edge features, and uses a 2D-GNN for personality inference.

Result: The approach jointly trains cognition simulation, graph construction, and recognition modules for improved RPR performance.

Conclusion: The novel 2D-GNN-based method effectively captures real personality traits by modeling internal cognition, outperforming traditional observer-based approaches.

Abstract: Automatic real personality recognition (RPR) aims to evaluate human real
personality traits from their expressive behaviours. However, most existing
solutions generally act as external observers to infer observers' personality
impressions based on target individuals' expressive behaviours, which
significantly deviate from their real personalities and consistently lead to
inferior recognition performance. Inspired by the association between real
personality and human internal cognition underlying the generation of
expressive behaviours, we propose a novel RPR approach that efficiently
simulates personalised internal cognition from easy-accessible external short
audio-visual behaviours expressed by the target individual. The simulated
personalised cognition, represented as a set of network weights that enforce
the personalised network to reproduce the individual-specific facial reactions,
is further encoded as a novel graph containing two-dimensional node and edge
feature matrices, with a novel 2D Graph Neural Network (2D-GNN) proposed for
inferring real personality traits from it. To simulate real personality-related
cognition, an end-to-end strategy is designed to jointly train our cognition
simulation, 2D graph construction, and personality recognition modules.

</details>


### [56] [SAM-PTx: Text-Guided Fine-Tuning of SAM with Parameter-Efficient, Parallel-Text Adapters](https://arxiv.org/abs/2508.00213)
*Shayan Jalilian,Abdul Bais*

Main category: cs.CV

TL;DR: SAM-PTx introduces a lightweight adapter, Parallel-Text, to integrate CLIP-derived text embeddings into SAM for semantics-guided segmentation, improving performance over spatial prompts.


<details>
  <summary>Details</summary>
Motivation: The potential of semantic text prompts in SAM is underexplored compared to spatial prompts like points and boxes.

Method: A parameter-efficient adapter, Parallel-Text, injects frozen CLIP text embeddings into SAM's image encoder, modifying only the MLP-parallel branch of transformer blocks.

Result: Experiments on COD10K, COCO, and ADE20K show improved segmentation performance with text embeddings over spatial prompts.

Conclusion: Integrating semantic conditioning into SAM offers a practical, scalable adaptation method with minimal computational overhead.

Abstract: The Segment Anything Model (SAM) has demonstrated impressive generalization
in prompt-based segmentation. Yet, the potential of semantic text prompts
remains underexplored compared to traditional spatial prompts like points and
boxes. This paper introduces SAM-PTx, a parameter-efficient approach for
adapting SAM using frozen CLIP-derived text embeddings as class-level semantic
guidance. Specifically, we propose a lightweight adapter design called
Parallel-Text that injects text embeddings into SAM's image encoder, enabling
semantics-guided segmentation while keeping most of the original architecture
frozen. Our adapter modifies only the MLP-parallel branch of each transformer
block, preserving the attention pathway for spatial reasoning. Through
supervised experiments and ablations on the COD10K dataset as well as low-data
subsets of COCO and ADE20K, we show that incorporating fixed text embeddings as
input improves segmentation performance over purely spatial prompt baselines.
To our knowledge, this is the first work to use text prompts for segmentation
on the COD10K dataset. These results suggest that integrating semantic
conditioning into SAM's architecture offers a practical and scalable path for
efficient adaptation with minimal computational complexity.

</details>


### [57] [Object-Centric Cropping for Visual Few-Shot Classification](https://arxiv.org/abs/2508.00218)
*Aymane Abdali,Bartosz Boguslawski,Lucas Drumetz,Vincent Gripon*

Main category: cs.CV

TL;DR: Using local positioning info and SAM improves few-shot image classification, even with minimal supervision.


<details>
  <summary>Details</summary>
Motivation: Address performance degradation in few-shot image classification due to image ambiguities like multiple objects or complex backgrounds.

Method: Incorporate local positioning info and use Segment Anything Model (SAM) or unsupervised foreground extraction.

Result: Significant improvement in classification benchmarks with minimal supervision (e.g., a single pixel).

Conclusion: Local positioning and SAM enhance few-shot classification, offering practical solutions with low supervision.

Abstract: In the domain of Few-Shot Image Classification, operating with as little as
one example per class, the presence of image ambiguities stemming from multiple
objects or complex backgrounds can significantly deteriorate performance. Our
research demonstrates that incorporating additional information about the local
positioning of an object within its image markedly enhances classification
across established benchmarks. More importantly, we show that a significant
fraction of the improvement can be achieved through the use of the Segment
Anything Model, requiring only a pixel of the object of interest to be pointed
out, or by employing fully unsupervised foreground object extraction methods.

</details>


### [58] [Guided Depth Map Super-Resolution via Multi-Scale Fusion U-shaped Mamba Network](https://arxiv.org/abs/2508.00248)
*Chenggang Guo,Hao Xu,XianMing Wan*

Main category: cs.CV

TL;DR: The paper proposes a multi-scale fusion U-shaped Mamba (MSF-UM) model for depth map super-resolution, combining Mamba's state-space modeling with a U-shaped structure guided by color images to improve resolution and detail restoration.


<details>
  <summary>Details</summary>
Motivation: Traditional CNNs struggle with long-range dependencies, and transformers are computationally expensive. The goal is to efficiently model global context for high-resolution depth maps.

Method: The MSF-UM integrates Mamba's state-space modeling into a U-shaped fusion structure, using residual dense channel attention blocks and multi-scale cross-modal fusion with color images.

Result: The model reduces parameters while achieving better accuracy, validated on public datasets with strong generalization for large-scale tasks.

Conclusion: MSF-UM effectively balances computational efficiency and performance, advancing depth map super-resolution.

Abstract: Depth map super-resolution technology aims to improve the spatial resolution
of low-resolution depth maps and effectively restore high-frequency detail
information. Traditional convolutional neural network has limitations in
dealing with long-range dependencies and are unable to fully model the global
contextual information in depth maps. Although transformer can model global
dependencies, its computational complexity and memory consumption are
quadratic, which significantly limits its ability to process high-resolution
depth maps. In this paper, we propose a multi-scale fusion U-shaped Mamba
(MSF-UM) model, a novel guided depth map super-resolution framework. The core
innovation of this model is to integrate Mamba's efficient state-space modeling
capabilities into a multi-scale U-shaped fusion structure guided by a color
image. The structure combining the residual dense channel attention block and
the Mamba state space module is designed, which combines the local feature
extraction capability of the convolutional layer with the modeling advantage of
the state space model for long-distance dependencies. At the same time, the
model adopts a multi-scale cross-modal fusion strategy to make full use of the
high-frequency texture information from the color image to guide the
super-resolution process of the depth map. Compared with existing mainstream
methods, the proposed MSF-UM significantly reduces the number of model
parameters while achieving better reconstruction accuracy. Extensive
experiments on multiple publicly available datasets validate the effectiveness
of the model, especially showing excellent generalization ability in the task
of large-scale depth map super-resolution.

</details>


### [59] [PointGauss: Point Cloud-Guided Multi-Object Segmentation for Gaussian Splatting](https://arxiv.org/abs/2508.00259)
*Wentao Sun,Hanqing Xu,Quanyun Wu,Dedong Zhang,Yiping Chen,Lingfei Ma,John S. Zelek,Jonathan Li*

Main category: cs.CV

TL;DR: PointGauss is a real-time multi-object segmentation framework for Gaussian Splatting, using point cloud guidance for efficient 3D segmentation and multi-view consistency. It outperforms prior methods and introduces a new dataset, DesktopObjects-360.


<details>
  <summary>Details</summary>
Motivation: Existing methods for multi-object segmentation in Gaussian Splatting suffer from slow initialization and poor multi-view consistency.

Method: PointGauss uses a point cloud-based Gaussian primitive decoder for fast 3D instance mask generation and a GPU-accelerated 2D mask rendering system for consistency.

Result: Achieves 1.89 to 31.78% improvement in multi-view mIoU while being computationally efficient.

Conclusion: PointGauss advances real-time 3D segmentation and introduces a comprehensive dataset, DesktopObjects-360, to address benchmark limitations.

Abstract: We introduce PointGauss, a novel point cloud-guided framework for real-time
multi-object segmentation in Gaussian Splatting representations. Unlike
existing methods that suffer from prolonged initialization and limited
multi-view consistency, our approach achieves efficient 3D segmentation by
directly parsing Gaussian primitives through a point cloud segmentation-driven
pipeline. The key innovation lies in two aspects: (1) a point cloud-based
Gaussian primitive decoder that generates 3D instance masks within 1 minute,
and (2) a GPU-accelerated 2D mask rendering system that ensures multi-view
consistency. Extensive experiments demonstrate significant improvements over
previous state-of-the-art methods, achieving performance gains of 1.89 to
31.78% in multi-view mIoU, while maintaining superior computational efficiency.
To address the limitations of current benchmarks (single-object focus,
inconsistent 3D evaluation, small scale, and partial coverage), we present
DesktopObjects-360, a novel comprehensive dataset for 3D segmentation in
radiance fields, featuring: (1) complex multi-object scenes, (2) globally
consistent 2D annotations, (3) large-scale training data (over 27 thousand 2D
masks), (4) full 360{\deg} coverage, and (5) 3D evaluation masks.

</details>


### [60] [Instruction-Grounded Visual Projectors for Continual Learning of Generative Vision-Language Models](https://arxiv.org/abs/2508.00260)
*Hyundong Jin,Hyung Jin Chang,Eunwoo Kim*

Main category: cs.CV

TL;DR: A novel framework for continual learning in vision-language models ensures balanced focus on visual and language inputs by using specialized visual projectors and expert recommendation/pruning.


<details>
  <summary>Details</summary>
Motivation: Address the neglect of language instructions in continual learning for vision-language models, which often prioritize visual inputs over repetitive textual instructions.

Method: Introduces a mixture of visual projectors (experts) tailored to instruction contexts, with expert recommendation for task similarity and pruning to reduce interference.

Result: Outperforms existing continual learning methods by generating better instruction-following responses across diverse tasks.

Conclusion: The proposed framework effectively balances visual and language inputs in continual learning, improving task adaptation and response quality.

Abstract: Continual learning enables pre-trained generative vision-language models
(VLMs) to incorporate knowledge from new tasks without retraining data from
previous ones. Recent methods update a visual projector to translate visual
information for new tasks, connecting pre-trained vision encoders with large
language models. However, such adjustments may cause the models to prioritize
visual inputs over language instructions, particularly learning tasks with
repetitive types of textual instructions. To address the neglect of language
instructions, we propose a novel framework that grounds the translation of
visual information on instructions for language models. We introduce a mixture
of visual projectors, each serving as a specialized visual-to-language
translation expert based on the given instruction context to adapt to new
tasks. To avoid using experts for irrelevant instruction contexts, we propose
an expert recommendation strategy that reuses experts for tasks similar to
those previously learned. Additionally, we introduce expert pruning to
alleviate interference from the use of experts that cumulatively activated in
previous tasks. Extensive experiments on diverse vision-language tasks
demonstrate that our method outperforms existing continual learning approaches
by generating instruction-following responses.

</details>


### [61] [Multimodal Referring Segmentation: A Survey](https://arxiv.org/abs/2508.00265)
*Henghui Ding,Song Tang,Shuting He,Chang Liu,Zuxuan Wu,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: A survey on multimodal referring segmentation, covering background, methods, challenges, and applications across images, videos, and 3D scenes.


<details>
  <summary>Details</summary>
Motivation: To address the need for accurate object perception based on user instructions in multimodal contexts.

Method: Summarizes a unified meta architecture and reviews methods for images, videos, and 3D scenes, including Generalized Referring Expression (GREx) approaches.

Result: Extensive performance comparisons on benchmarks and tracking of related works.

Conclusion: Highlights advancements and challenges in multimodal referring segmentation, with ongoing updates via a GitHub repository.

Abstract: Multimodal referring segmentation aims to segment target objects in visual
scenes, such as images, videos, and 3D scenes, based on referring expressions
in text or audio format. This task plays a crucial role in practical
applications requiring accurate object perception based on user instructions.
Over the past decade, it has gained significant attention in the multimodal
community, driven by advances in convolutional neural networks, transformers,
and large language models, all of which have substantially improved multimodal
perception capabilities. This paper provides a comprehensive survey of
multimodal referring segmentation. We begin by introducing this field's
background, including problem definitions and commonly used datasets. Next, we
summarize a unified meta architecture for referring segmentation and review
representative methods across three primary visual scenes, including images,
videos, and 3D scenes. We further discuss Generalized Referring Expression
(GREx) methods to address the challenges of real-world complexity, along with
related tasks and practical applications. Extensive performance comparisons on
standard benchmarks are also provided. We continually track related works at
https://github.com/henghuiding/Awesome-Multimodal-Referring-Segmentation.

</details>


### [62] [Towards Robust Semantic Correspondence: A Benchmark and Insights](https://arxiv.org/abs/2508.00272)
*Wenyue Chong*

Main category: cs.CV

TL;DR: A benchmark for evaluating semantic correspondence in adverse conditions reveals performance drops in existing methods, highlights the robustness of large-scale vision models, and identifies the need for task-specific enhancements.


<details>
  <summary>Details</summary>
Motivation: Semantic correspondence is crucial for tasks like 3D reconstruction and image editing, but its robustness in challenging scenarios is understudied.

Method: A novel benchmark dataset with 14 challenging scenarios (e.g., distortion, blurring) was created. Evaluations compared methods like DINO and Stable Diffusion, tested robustness strategies, and analyzed performance.

Result: Existing methods perform poorly in adverse conditions. Large-scale models improve robustness, but fine-tuning reduces it. DINO outperforms Stable Diffusion, and their fusion enhances robustness. General data augmentations are ineffective.

Conclusion: The study underscores the need for task-specific robustness enhancements in semantic correspondence and provides insights for future research.

Abstract: Semantic correspondence aims to identify semantically meaningful
relationships between different images and is a fundamental challenge in
computer vision. It forms the foundation for numerous tasks such as 3D
reconstruction, object tracking, and image editing. With the progress of
large-scale vision models, semantic correspondence has achieved remarkable
performance in controlled and high-quality conditions. However, the robustness
of semantic correspondence in challenging scenarios is much less investigated.
In this work, we establish a novel benchmark for evaluating semantic
correspondence in adverse conditions. The benchmark dataset comprises 14
distinct challenging scenarios that reflect commonly encountered imaging
issues, including geometric distortion, image blurring, digital artifacts, and
environmental occlusion. Through extensive evaluations, we provide several key
insights into the robustness of semantic correspondence approaches: (1) All
existing methods suffer from noticeable performance drops under adverse
conditions; (2) Using large-scale vision models can enhance overall robustness,
but fine-tuning on these models leads to a decline in relative robustness; (3)
The DINO model outperforms the Stable Diffusion in relative robustness, and
their fusion achieves better absolute robustness; Moreover, We evaluate common
robustness enhancement strategies for semantic correspondence and find that
general data augmentations are ineffective, highlighting the need for
task-specific designs. These results are consistent across both our dataset and
real-world benchmarks.

</details>


### [63] [Privacy-Preserving Driver Drowsiness Detection with Spatial Self-Attention and Federated Learning](https://arxiv.org/abs/2508.00287)
*Tran Viet Khoa,Do Hai Son,Mohammad Abu Alsheikh,Yibeltal F Alem,Dinh Thai Hoang*

Main category: cs.CV

TL;DR: A novel framework for drowsiness detection using Spatial Self-Attention and LSTM, with federated learning via Gradient Similarity Comparison, achieves 89.9% accuracy.


<details>
  <summary>Details</summary>
Motivation: Driver drowsiness causes accidents; detecting it accurately in decentralized, diverse real-world data is challenging.

Method: Combines Spatial Self-Attention with LSTM for feature extraction, uses Gradient Similarity Comparison for federated learning, and includes automated video data processing.

Result: 89.9% detection accuracy in federated learning, outperforming existing methods.

Conclusion: The framework effectively handles real-world data variability and shows promise for intelligent transportation systems.

Abstract: Driver drowsiness is one of the main causes of road accidents and is
recognized as a leading contributor to traffic-related fatalities. However,
detecting drowsiness accurately remains a challenging task, especially in
real-world settings where facial data from different individuals is
decentralized and highly diverse. In this paper, we propose a novel framework
for drowsiness detection that is designed to work effectively with
heterogeneous and decentralized data. Our approach develops a new Spatial
Self-Attention (SSA) mechanism integrated with a Long Short-Term Memory (LSTM)
network to better extract key facial features and improve detection
performance. To support federated learning, we employ a Gradient Similarity
Comparison (GSC) that selects the most relevant trained models from different
operators before aggregation. This improves the accuracy and robustness of the
global model while preserving user privacy. We also develop a customized tool
that automatically processes video data by extracting frames, detecting and
cropping faces, and applying data augmentation techniques such as rotation,
flipping, brightness adjustment, and zooming. Experimental results show that
our framework achieves a detection accuracy of 89.9% in the federated learning
settings, outperforming existing methods under various deployment scenarios.
The results demonstrate the effectiveness of our approach in handling
real-world data variability and highlight its potential for deployment in
intelligent transportation systems to enhance road safety through early and
reliable drowsiness detection.

</details>


### [64] [TITAN-Guide: Taming Inference-Time AligNment for Guided Text-to-Video Diffusion Models](https://arxiv.org/abs/2508.00289)
*Christian Simon,Masato Ishii,Akio Hayakawa,Zhi Zhong,Shusuke Takahashi,Takashi Shibuya,Yuki Mitsufuji*

Main category: cs.CV

TL;DR: TITAN-Guide is a training-free guidance framework for Text-to-Video diffusion models, addressing memory and control issues in existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing training-free guidance frameworks for diffusion models suffer from heavy memory usage or sub-optimal control, limiting their application in compute-intensive tasks like Text-to-Video generation.

Method: Proposes TITAN-Guide, which optimizes diffusion latents without backpropagation using forward gradient descents and directional directives.

Result: Efficiently manages memory during latent optimization and enhances Text-to-Video performance across benchmarks.

Conclusion: TITAN-Guide overcomes memory and control limitations, improving Text-to-Video diffusion model guidance.

Abstract: In the recent development of conditional diffusion models still require heavy
supervised fine-tuning for performing control on a category of tasks.
Training-free conditioning via guidance with off-the-shelf models is a
favorable alternative to avoid further fine-tuning on the base model. However,
the existing training-free guidance frameworks either have heavy memory
requirements or offer sub-optimal control due to rough estimation. These
shortcomings limit the applicability to control diffusion models that require
intense computation, such as Text-to-Video (T2V) diffusion models. In this
work, we propose Taming Inference Time Alignment for Guided Text-to-Video
Diffusion Model, so-called TITAN-Guide, which overcomes memory space issues,
and provides more optimal control in the guidance process compared to the
counterparts. In particular, we develop an efficient method for optimizing
diffusion latents without backpropagation from a discriminative guiding model.
In particular, we study forward gradient descents for guided diffusion tasks
with various options on directional directives. In our experiments, we
demonstrate the effectiveness of our approach in efficiently managing memory
during latent optimization, while previous methods fall short. Our proposed
approach not only minimizes memory requirements but also significantly enhances
T2V performance across a range of diffusion guidance benchmarks. Code, models,
and demo are available at https://titanguide.github.io.

</details>


### [65] [AniMer+: Unified Pose and Shape Estimation Across Mammalia and Aves via Family-Aware Transformer](https://arxiv.org/abs/2508.00298)
*Jin Lyu,Liang An,Li Lin,Pujin Cheng,Yebin Liu,Xiaoying Tang*

Main category: cs.CV

TL;DR: AniMer+ introduces a unified approach for reconstructing mammals and birds using a high-capacity, family-aware ViT with MoE design and synthetic datasets to address data scarcity.


<details>
  <summary>Details</summary>
Motivation: To enable stronger spatial intelligence and accurate animal pose/shape estimation across species, addressing limited network capacity and data scarcity.

Method: Uses a family-aware ViT with MoE, partitioning layers into taxa-specific and shared components, and generates synthetic datasets (CtrlAni3D, CtrlAVES3D) via diffusion-based image generation.

Result: Superior performance on benchmarks, including out-of-domain Animal Kingdom, validated by ablation studies.

Conclusion: AniMer+ effectively addresses data and capacity limitations, enhancing real-world performance for animal pose and shape estimation.

Abstract: In the era of foundation models, achieving a unified understanding of
different dynamic objects through a single network has the potential to empower
stronger spatial intelligence. Moreover, accurate estimation of animal pose and
shape across diverse species is essential for quantitative analysis in
biological research. However, this topic remains underexplored due to the
limited network capacity of previous methods and the scarcity of comprehensive
multi-species datasets. To address these limitations, we introduce AniMer+, an
extended version of our scalable AniMer framework. In this paper, we focus on a
unified approach for reconstructing mammals (mammalia) and birds (aves). A key
innovation of AniMer+ is its high-capacity, family-aware Vision Transformer
(ViT) incorporating a Mixture-of-Experts (MoE) design. Its architecture
partitions network layers into taxa-specific components (for mammalia and aves)
and taxa-shared components, enabling efficient learning of both distinct and
common anatomical features within a single model. To overcome the critical
shortage of 3D training data, especially for birds, we introduce a
diffusion-based conditional image generation pipeline. This pipeline produces
two large-scale synthetic datasets: CtrlAni3D for quadrupeds and CtrlAVES3D for
birds. To note, CtrlAVES3D is the first large-scale, 3D-annotated dataset for
birds, which is crucial for resolving single-view depth ambiguities. Trained on
an aggregated collection of 41.3k mammalian and 12.4k avian images (combining
real and synthetic data), our method demonstrates superior performance over
existing approaches across a wide range of benchmarks, including the
challenging out-of-domain Animal Kingdom dataset. Ablation studies confirm the
effectiveness of both our novel network architecture and the generated
synthetic datasets in enhancing real-world application performance.

</details>


### [66] [Controllable Pedestrian Video Editing for Multi-View Driving Scenarios via Motion Sequence](https://arxiv.org/abs/2508.00299)
*Danzhen Fu,Jiagao Hu,Daiguo Zhou,Fei Wang,Zepeng Wang,Wenhua Liao*

Main category: cs.CV

TL;DR: A novel framework for controllable pedestrian video editing in multi-view driving scenarios is introduced, addressing robustness issues in pedestrian detection models by enhancing training datasets.


<details>
  <summary>Details</summary>
Motivation: Pedestrian detection models lack robustness due to insufficient representation of dangerous scenarios in training datasets.

Method: The framework integrates video inpainting and human motion control, identifying pedestrian regions, expanding bounding boxes, and stitching them into a unified canvas. Editing is guided by pose sequence control within a binary mask.

Result: The method achieves high-quality pedestrian editing with visual realism, spatiotemporal coherence, and cross-view consistency.

Conclusion: The framework is a robust and versatile solution for multi-view pedestrian video generation, useful for data augmentation and scenario simulation in autonomous driving.

Abstract: Pedestrian detection models in autonomous driving systems often lack
robustness due to insufficient representation of dangerous pedestrian scenarios
in training datasets. To address this limitation, we present a novel framework
for controllable pedestrian video editing in multi-view driving scenarios by
integrating video inpainting and human motion control techniques. Our approach
begins by identifying pedestrian regions of interest across multiple camera
views, expanding detection bounding boxes with a fixed ratio, and resizing and
stitching these regions into a unified canvas while preserving cross-view
spatial relationships. A binary mask is then applied to designate the editable
area, within which pedestrian editing is guided by pose sequence control
conditions. This enables flexible editing functionalities, including pedestrian
insertion, replacement, and removal. Extensive experiments demonstrate that our
framework achieves high-quality pedestrian editing with strong visual realism,
spatiotemporal coherence, and cross-view consistency. These results establish
the proposed method as a robust and versatile solution for multi-view
pedestrian video generation, with broad potential for applications in data
augmentation and scenario simulation in autonomous driving.

</details>


### [67] [Exploring Fourier Prior and Event Collaboration for Low-Light Image Enhancement](https://arxiv.org/abs/2508.00308)
*Chunyan She,Fujun Han,Chengyu Fang,Shukai Duan,Lidan Wang*

Main category: cs.CV

TL;DR: The paper proposes a two-stage method for low-light image enhancement using event cameras, focusing on visibility restoration and structure refinement, outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: Existing event-based methods do not fully exploit modality-specific advantages, limiting performance in low-light image enhancement.

Method: The pipeline decouples into visibility restoration (using amplitude-phase entanglement in Fourier space) and structure refinement (with dynamic alignment fusion). Spatial-frequency interpolation and contrastive loss are also used.

Result: The proposed method outperforms state-of-the-art models in experiments.

Conclusion: Decoupling the enhancement pipeline and leveraging modality-specific advantages significantly improves low-light image enhancement.

Abstract: The event camera, benefiting from its high dynamic range and low latency,
provides performance gain for low-light image enhancement. Unlike frame-based
cameras, it records intensity changes with extremely high temporal resolution,
capturing sufficient structure information. Currently, existing event-based
methods feed a frame and events directly into a single model without fully
exploiting modality-specific advantages, which limits their performance.
Therefore, by analyzing the role of each sensing modality, the enhancement
pipeline is decoupled into two stages: visibility restoration and structure
refinement. In the first stage, we design a visibility restoration network with
amplitude-phase entanglement by rethinking the relationship between amplitude
and phase components in Fourier space. In the second stage, a fusion strategy
with dynamic alignment is proposed to mitigate the spatial mismatch caused by
the temporal resolution discrepancy between two sensing modalities, aiming to
refine the structure information of the image enhanced by the visibility
restoration network. In addition, we utilize spatial-frequency interpolation to
simulate negative samples with diverse illumination, noise and artifact
degradations, thereby developing a contrastive loss that encourages the model
to learn discriminative representations. Experiments demonstrate that the
proposed method outperforms state-of-the-art models.

</details>


### [68] [DocTron-Formula: Generalized Formula Recognition in Complex and Structured Scenarios](https://arxiv.org/abs/2508.00311)
*Yufeng Zhong,Zhixiong Zeng,Lei Chen,Longrong Yang,Liming Zheng,Jing Huang,Siqi Yang,Lin Ma*

Main category: cs.CV

TL;DR: DocTron-Formula is a unified OCR framework for mathematical formulas, leveraging general vision-language models and a new dataset (CSFormula) to achieve state-of-the-art performance without specialized architectures.


<details>
  <summary>Details</summary>
Motivation: Current OCR models struggle with the structural diversity and complexity of mathematical formulas in scientific literature, necessitating a more robust solution.

Method: The approach uses general vision-language models and introduces CSFormula, a large-scale dataset, followed by straightforward supervised fine-tuning.

Result: The method achieves state-of-the-art performance in accuracy and robustness across diverse styles, domains, and layouts, outperforming specialized models.

Conclusion: DocTron-Formula sets a new paradigm for automated understanding of complex scientific documents, eliminating the need for task-specific architectures.

Abstract: Optical Character Recognition (OCR) for mathematical formula is essential for
the intelligent analysis of scientific literature. However, both task-specific
and general vision-language models often struggle to handle the structural
diversity, complexity, and real-world variability inherent in mathematical
content. In this work, we present DocTron-Formula, a unified framework built
upon general vision-language models, thereby eliminating the need for
specialized architectures. Furthermore, we introduce CSFormula, a large-scale
and challenging dataset that encompasses multidisciplinary and structurally
complex formulas at the line, paragraph, and page levels. Through
straightforward supervised fine-tuning, our approach achieves state-of-the-art
performance across a variety of styles, scientific domains, and complex
layouts. Experimental results demonstrate that our method not only surpasses
specialized models in terms of accuracy and robustness, but also establishes a
new paradigm for the automated understanding of complex scientific documents.

</details>


### [69] [GV-VAD : Exploring Video Generation for Weakly-Supervised Video Anomaly Detection](https://arxiv.org/abs/2508.00312)
*Suhang Cai,Xiaohao Peng,Chong Wang,Xiaojie Cai,Jiangbo Qian*

Main category: cs.CV

TL;DR: The paper proposes GV-VAD, a generative video-enhanced weakly-supervised framework for video anomaly detection, using synthetic videos to augment training data and improve performance.


<details>
  <summary>Details</summary>
Motivation: The rarity, unpredictability, and high annotation cost of real-world anomalies limit the scalability and performance of existing VAD models.

Method: Leverages text-conditioned video generation models to create synthetic videos for data augmentation and uses a synthetic sample loss scaling strategy for efficient training.

Result: Outperforms state-of-the-art methods on UCF-Crime datasets.

Conclusion: GV-VAD effectively addresses the challenges of limited training data and improves anomaly detection performance.

Abstract: Video anomaly detection (VAD) plays a critical role in public safety
applications such as intelligent surveillance. However, the rarity,
unpredictability, and high annotation cost of real-world anomalies make it
difficult to scale VAD datasets, which limits the performance and
generalization ability of existing models. To address this challenge, we
propose a generative video-enhanced weakly-supervised video anomaly detection
(GV-VAD) framework that leverages text-conditioned video generation models to
produce semantically controllable and physically plausible synthetic videos.
These virtual videos are used to augment training data at low cost. In
addition, a synthetic sample loss scaling strategy is utilized to control the
influence of generated synthetic samples for efficient training. The
experiments show that the proposed framework outperforms state-of-the-art
methods on UCF-Crime datasets. The code is available at
https://github.com/Sumutan/GV-VAD.git.

</details>


### [70] [Steering Guidance for Personalized Text-to-Image Diffusion Models](https://arxiv.org/abs/2508.00319)
*Sunghyun Park,Seokeon Choi,Hyoungwoo Park,Sungrack Yun*

Main category: cs.CV

TL;DR: Proposes personalization guidance for text-to-image diffusion models to balance target fidelity and text editability, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Address the trade-off between aligning with target concepts and preserving original model knowledge in few-shot fine-tuning of diffusion models.

Method: Uses an unlearned weak model conditioned on a null text prompt and dynamically controls unlearning via weight interpolation during inference.

Result: Improves text alignment and target distribution fidelity without extra computational cost.

Conclusion: The method effectively balances adaptation and preservation, integrating well with various fine-tuning strategies.

Abstract: Personalizing text-to-image diffusion models is crucial for adapting the
pre-trained models to specific target concepts, enabling diverse image
generation. However, fine-tuning with few images introduces an inherent
trade-off between aligning with the target distribution (e.g., subject
fidelity) and preserving the broad knowledge of the original model (e.g., text
editability). Existing sampling guidance methods, such as classifier-free
guidance (CFG) and autoguidance (AG), fail to effectively guide the output
toward well-balanced space: CFG restricts the adaptation to the target
distribution, while AG compromises text alignment. To address these
limitations, we propose personalization guidance, a simple yet effective method
leveraging an unlearned weak model conditioned on a null text prompt. Moreover,
our method dynamically controls the extent of unlearning in a weak model
through weight interpolation between pre-trained and fine-tuned models during
inference. Unlike existing guidance methods, which depend solely on guidance
scales, our method explicitly steers the outputs toward a balanced latent space
without additional computational overhead. Experimental results demonstrate
that our proposed guidance can improve text alignment and target distribution
fidelity, integrating seamlessly with various fine-tuning strategies.

</details>


### [71] [Spectral Sensitivity Estimation with an Uncalibrated Diffraction Grating](https://arxiv.org/abs/2508.00330)
*Lilika Makabe,Hiroaki Santo,Fumio Okura,Michael S. Brown,Yasuyuki Matsushita*

Main category: cs.CV

TL;DR: A novel calibration method for camera spectral sensitivity using an uncalibrated diffraction grating, outperforming traditional reference target-based approaches.


<details>
  <summary>Details</summary>
Motivation: Accurate camera spectral sensitivity calibration is essential for tasks like color correction and material analysis, but existing methods require specialized equipment.

Method: Uses a diffraction grating sheet to capture direct and diffracted illumination, estimating camera sensitivity and grating parameters in closed-form.

Result: Outperforms conventional methods in synthetic and real-world experiments.

Conclusion: The method is effective, practical, and accessible, requiring only off-the-shelf materials.

Abstract: This paper introduces a practical and accurate calibration method for camera
spectral sensitivity using a diffraction grating. Accurate calibration of
camera spectral sensitivity is crucial for various computer vision tasks,
including color correction, illumination estimation, and material analysis.
Unlike existing approaches that require specialized narrow-band filters or
reference targets with known spectral reflectances, our method only requires an
uncalibrated diffraction grating sheet, readily available off-the-shelf. By
capturing images of the direct illumination and its diffracted pattern through
the grating sheet, our method estimates both the camera spectral sensitivity
and the diffraction grating parameters in a closed-form manner. Experiments on
synthetic and real-world data demonstrate that our method outperforms
conventional reference target-based methods, underscoring its effectiveness and
practicality.

</details>


### [72] [Analyze-Prompt-Reason: A Collaborative Agent-Based Framework for Multi-Image Vision-Language Reasoning](https://arxiv.org/abs/2508.00356)
*Angelos Vlachos,Giorgos Filandrianos,Maria Lymperaiou,Nikolaos Spanos,Ilias Mitsouras,Vasileios Karampinis,Athanasios Voulodimos*

Main category: cs.CV

TL;DR: A dual-agent framework (PromptEngineer and VisionReasoner) enables automated, training-free multi-image reasoning across diverse tasks, achieving high performance on 18 datasets.


<details>
  <summary>Details</summary>
Motivation: Addresses the challenge of interleaved multimodal reasoning across varied datasets and task formats.

Method: Uses a language-based PromptEngineer for task-specific prompts and a VisionReasoner (LVLM) for inference, evaluated on 18 datasets.

Result: High performance on tasks like TQA (99.13%), DocVQA (96.87%), and MMCoQA (75.28 ROUGE-L).

Conclusion: LVLMs can effectively reason over multiple images with informative prompts, with performance influenced by design choices.

Abstract: We present a Collaborative Agent-Based Framework for Multi-Image Reasoning.
Our approach tackles the challenge of interleaved multimodal reasoning across
diverse datasets and task formats by employing a dual-agent system: a
language-based PromptEngineer, which generates context-aware, task-specific
prompts, and a VisionReasoner, a large vision-language model (LVLM) responsible
for final inference. The framework is fully automated, modular, and
training-free, enabling generalization across classification, question
answering, and free-form generation tasks involving one or multiple input
images. We evaluate our method on 18 diverse datasets from the 2025 MIRAGE
Challenge (Track A), covering a broad spectrum of visual reasoning tasks
including document QA, visual comparison, dialogue-based understanding, and
scene-level inference. Our results demonstrate that LVLMs can effectively
reason over multiple images when guided by informative prompts. Notably, Claude
3.7 achieves near-ceiling performance on challenging tasks such as TQA (99.13%
accuracy), DocVQA (96.87%), and MMCoQA (75.28 ROUGE-L). We also explore how
design choices-such as model selection, shot count, and input length-influence
the reasoning performance of different LVLMs.

</details>


### [73] [Stable at Any Speed: Speed-Driven Multi-Object Tracking with Learnable Kalman Filtering](https://arxiv.org/abs/2508.00358)
*Yan Gong,Mengjun Chen,Hao Liu,Gao Yongsheng,Lei Yang,Naibang Wang,Ziying Song,Haoqun Ma*

Main category: cs.CV

TL;DR: The paper proposes a Speed-Guided Learnable Kalman Filter (SG-LKF) for multi-object tracking (MOT) in autonomous vehicles, improving stability and accuracy by dynamically adapting to ego-vehicle speed.


<details>
  <summary>Details</summary>
Motivation: Conventional MOT methods ignore ego-vehicle speed's impact on observation noise and reference frames, degrading tracking in dynamic, high-speed scenarios.

Method: SG-LKF uses MotionScaleNet (MSNet) to adaptively predict parameters and introduces a self-supervised trajectory consistency loss for better inter-frame association.

Result: SG-LKF achieves top performance on KITTI 2D MOT (79.59% HOTA), strong results on KITTI 3D MOT (82.03% HOTA), and outperforms SimpleTrack by 2.2% AMOTA on nuScenes 3D MOT.

Conclusion: The proposed SG-LKF effectively addresses speed-induced tracking challenges, enhancing MOT performance in dynamic scenarios.

Abstract: Multi-object tracking (MOT) enables autonomous vehicles to continuously
perceive dynamic objects, supplying essential temporal cues for prediction,
behavior understanding, and safe planning. However, conventional
tracking-by-detection methods typically rely on static coordinate
transformations based on ego-vehicle poses, disregarding ego-vehicle
speed-induced variations in observation noise and reference frame changes,
which degrades tracking stability and accuracy in dynamic, high-speed
scenarios. In this paper, we investigate the critical role of ego-vehicle speed
in MOT and propose a Speed-Guided Learnable Kalman Filter (SG-LKF) that
dynamically adapts uncertainty modeling to ego-vehicle speed, significantly
improving stability and accuracy in highly dynamic scenarios. Central to SG-LKF
is MotionScaleNet (MSNet), a decoupled token-mixing and channel-mixing MLP that
adaptively predicts key parameters of SG-LKF. To enhance inter-frame
association and trajectory continuity, we introduce a self-supervised
trajectory consistency loss jointly optimized with semantic and positional
constraints. Extensive experiments show that SG-LKF ranks first among all
vision-based methods on KITTI 2D MOT with 79.59% HOTA, delivers strong results
on KITTI 3D MOT with 82.03% HOTA, and outperforms SimpleTrack by 2.2% AMOTA on
nuScenes 3D MOT.

</details>


### [74] [CoST: Efficient Collaborative Perception From Unified Spatiotemporal Perspective](https://arxiv.org/abs/2508.00359)
*Zongheng Tang,Yi Liu,Yifan Sun,Yulu Gao,Jinyu Chen,Runsheng Xu,Si Liu*

Main category: cs.CV

TL;DR: The paper proposes CoST, a method for collaborative perception that unifies multi-agent and multi-time fusion into a single spatio-temporal space, improving efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Prior methods separate multi-agent and multi-time fusion, leading to inefficiencies like redundant feature transmission. The paper aims to unify these steps for better performance.

Method: CoST aggregates observations from different agents and times into a unified spatio-temporal space, enabling efficient feature transmission and superior fusion.

Result: CoST improves both efficiency (reducing redundant transmissions) and accuracy (enhancing perception in challenging scenarios).

Conclusion: CoST is versatile, compatible with prior methods, and enhances accuracy while reducing bandwidth usage.

Abstract: Collaborative perception shares information among different agents and helps
solving problems that individual agents may face, e.g., occlusions and small
sensing range. Prior methods usually separate the multi-agent fusion and
multi-time fusion into two consecutive steps. In contrast, this paper proposes
an efficient collaborative perception that aggregates the observations from
different agents (space) and different times into a unified spatio-temporal
space simultanesouly. The unified spatio-temporal space brings two benefits,
i.e., efficient feature transmission and superior feature fusion. 1) Efficient
feature transmission: each static object yields a single observation in the
spatial temporal space, and thus only requires transmission only once (whereas
prior methods re-transmit all the object features multiple times). 2) superior
feature fusion: merging the multi-agent and multi-time fusion into a unified
spatial-temporal aggregation enables a more holistic perspective, thereby
enhancing perception performance in challenging scenarios. Consequently, our
Collaborative perception with Spatio-temporal Transformer (CoST) gains
improvement in both efficiency and accuracy. Notably, CoST is not tied to any
specific method and is compatible with a majority of previous methods,
enhancing their accuracy while reducing the transmission bandwidth.

</details>


### [75] [Honey Classification using Hyperspectral Imaging and Machine Learning](https://arxiv.org/abs/2508.00361)
*Mokhtar A. Al-Awadhi,Ratnadeep R. Deshmukh*

Main category: cs.CV

TL;DR: A machine learning method for classifying honey botanical origins using dataset preparation, LDA for feature extraction, and SVM/KNN for classification, achieving high accuracy.


<details>
  <summary>Details</summary>
Motivation: To automate and improve the classification of honey botanical origins using advanced machine learning techniques.

Method: Dataset preparation with class transformation, feature extraction via LDA, and classification using SVM and KNN models.

Result: Achieved 95.13% accuracy for image-based and 92.80% for instance-based classification on a standard HSI dataset.

Conclusion: The proposed method is effective for honey botanical origin classification, outperforming existing approaches.

Abstract: In this paper, we propose a machine learning-based method for automatically
classifying honey botanical origins. Dataset preparation, feature extraction,
and classification are the three main steps of the proposed method. We use a
class transformation method in the dataset preparation phase to maximize the
separability across classes. The feature extraction phase employs the Linear
Discriminant Analysis (LDA) technique for extracting relevant features and
reducing the number of dimensions. In the classification phase, we use Support
Vector Machines (SVM) and K-Nearest Neighbors (KNN) models to classify the
extracted features of honey samples into their botanical origins. We evaluate
our system using a standard honey hyperspectral imaging (HSI) dataset.
Experimental findings demonstrate that the proposed system produces
state-of-the-art results on this dataset, achieving the highest classification
accuracy of 95.13% for hyperspectral image-based classification and 92.80% for
hyperspectral instance-based classification.

</details>


### [76] [SparseRecon: Neural Implicit Surface Reconstruction from Sparse Views with Feature and Depth Consistencies](https://arxiv.org/abs/2508.00366)
*Liang Han,Xu Zhang,Haichuan Song,Kanle Shi,Yu-Shen Liu,Zhizhong Han*

Main category: cs.CV

TL;DR: SparseRecon improves sparse-view 3D reconstruction by combining feature consistency and uncertainty-guided depth constraints, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods (generalization-based or overfitting-based) struggle with unseen views or limited geometry clues, leading to poor reconstruction quality.

Method: SparseRecon uses volume rendering-based feature consistency loss and uncertainty-guided depth constraints to enhance reconstruction.

Result: Outperforms state-of-the-art methods, especially in scenarios with small overlapping views.

Conclusion: SparseRecon effectively addresses sparse-view reconstruction challenges, producing high-quality geometry.

Abstract: Surface reconstruction from sparse views aims to reconstruct a 3D shape or
scene from few RGB images. The latest methods are either generalization-based
or overfitting-based. However, the generalization-based methods do not
generalize well on views that were unseen during training, while the
reconstruction quality of overfitting-based methods is still limited by the
limited geometry clues. To address this issue, we propose SparseRecon, a novel
neural implicit reconstruction method for sparse views with volume
rendering-based feature consistency and uncertainty-guided depth constraint.
Firstly, we introduce a feature consistency loss across views to constrain the
neural implicit field. This design alleviates the ambiguity caused by
insufficient consistency information of views and ensures completeness and
smoothness in the reconstruction results. Secondly, we employ an
uncertainty-guided depth constraint to back up the feature consistency loss in
areas with occlusion and insignificant features, which recovers geometry
details for better reconstruction quality. Experimental results demonstrate
that our method outperforms the state-of-the-art methods, which can produce
high-quality geometry with sparse-view input, especially in the scenarios with
small overlapping views. Project page: https://hanl2010.github.io/SparseRecon/.

</details>


### [77] [Representation Shift: Unifying Token Compression with FlashAttention](https://arxiv.org/abs/2508.00367)
*Joonmyung Choi,Sanghyeok Lee,Byungoh Ko,Eunseo Kim,Jihyung Kil,Hyunwoo J. Kim*

Main category: cs.CV

TL;DR: The paper introduces Representation Shift, a training-free metric for token compression in Transformers, compatible with FlashAttention, achieving speedups in tasks like video-text retrieval and video QA.


<details>
  <summary>Details</summary>
Motivation: Address the high computational cost and GPU memory overhead of self-attention in Transformers due to larger models and more tokens, while maintaining compatibility with efficient attention kernels like FlashAttention.

Method: Proposes Representation Shift, a model-agnostic metric to measure token representation changes, enabling token compression without attention maps or retraining.

Result: Achieves speedups of up to 5.5% in video-text retrieval and 4.4% in video QA, demonstrating effective token compression.

Conclusion: Representation Shift offers a practical solution for reducing computation costs in Transformers and other models, enhancing efficiency without retraining.

Abstract: Transformers have demonstrated remarkable success across vision, language,
and video. Yet, increasing task complexity has led to larger models and more
tokens, raising the quadratic cost of self-attention and the overhead of GPU
memory access. To reduce the computation cost of self-attention, prior work has
proposed token compression techniques that drop redundant or less informative
tokens. Meanwhile, fused attention kernels such as FlashAttention have been
developed to alleviate memory overhead by avoiding attention map construction
and its associated I/O to HBM. This, however, makes it incompatible with most
training-free token compression methods, which rely on attention maps to
determine token importance. Here, we propose Representation Shift, a
training-free, model-agnostic metric that measures the degree of change in each
token's representation. This seamlessly integrates token compression with
FlashAttention, without attention maps or retraining. Our method further
generalizes beyond Transformers to CNNs and state space models. Extensive
experiments show that Representation Shift enables effective token compression
compatible with FlashAttention, yielding significant speedups of up to 5.5% and
4.4% in video-text retrieval and video QA, respectively. Code is available at
https://github.com/mlvlab/Representation-Shift.

</details>


### [78] [Bidirectional Action Sequence Learning for Long-term Action Anticipation with Large Language Models](https://arxiv.org/abs/2508.00374)
*Yuji Sato,Yasunori Ishii,Takayoshi Yamashita*

Main category: cs.CV

TL;DR: BiAnt improves long-term action anticipation by combining forward and backward predictions using a large language model, outperforming baseline methods.


<details>
  <summary>Details</summary>
Motivation: Current methods for video-based long-term action anticipation are limited by unidirectional feature extraction, failing to capture distinct sub-actions.

Method: BiAnt integrates forward and backward prediction with a large language model to enhance feature extraction.

Result: Experiments on Ego4D show BiAnt outperforms baselines in edit distance metrics.

Conclusion: BiAnt's bidirectional approach effectively addresses limitations of unidirectional methods, improving action anticipation.

Abstract: Video-based long-term action anticipation is crucial for early risk detection
in areas such as automated driving and robotics. Conventional approaches
extract features from past actions using encoders and predict future events
with decoders, which limits performance due to their unidirectional nature.
These methods struggle to capture semantically distinct sub-actions within a
scene. The proposed method, BiAnt, addresses this limitation by combining
forward prediction with backward prediction using a large language model.
Experimental results on Ego4D demonstrate that BiAnt improves performance in
terms of edit distance compared to baseline methods.

</details>


### [79] [Advancing Welding Defect Detection in Maritime Operations via Adapt-WeldNet and Defect Detection Interpretability Analysis](https://arxiv.org/abs/2508.00381)
*Kamal Basha S,Athira Nambiar*

Main category: cs.CV

TL;DR: The paper introduces Adapt-WeldNet, an adaptive framework for weld defect detection, and DDIA, an interpretability analysis framework, to improve performance and transparency in defect detection systems.


<details>
  <summary>Details</summary>
Motivation: Traditional NDT methods and existing neural network approaches often fail to detect subtle defects and lack interpretability, raising safety concerns.

Method: Adapt-WeldNet evaluates pre-trained architectures, transfer learning strategies, and adaptive optimizers. DDIA uses XAI techniques (Grad-CAM, LIME) and expert validation for interpretability.

Result: The framework optimizes defect detection and provides actionable insights, enhancing reliability and trust in automated decisions.

Conclusion: The work improves trust, safety, and reliability in welding defect detection, especially for offshore and marine environments.

Abstract: Weld defect detection is crucial for ensuring the safety and reliability of
piping systems in the oil and gas industry, especially in challenging marine
and offshore environments. Traditional non-destructive testing (NDT) methods
often fail to detect subtle or internal defects, leading to potential failures
and costly downtime. Furthermore, existing neural network-based approaches for
defect classification frequently rely on arbitrarily selected pretrained
architectures and lack interpretability, raising safety concerns for
deployment. To address these challenges, this paper introduces
``Adapt-WeldNet", an adaptive framework for welding defect detection that
systematically evaluates various pre-trained architectures, transfer learning
strategies, and adaptive optimizers to identify the best-performing model and
hyperparameters, optimizing defect detection and providing actionable insights.
Additionally, a novel Defect Detection Interpretability Analysis (DDIA)
framework is proposed to enhance system transparency. DDIA employs Explainable
AI (XAI) techniques, such as Grad-CAM and LIME, alongside domain-specific
evaluations validated by certified ASNT NDE Level II professionals.
Incorporating a Human-in-the-Loop (HITL) approach and aligning with the
principles of Trustworthy AI, DDIA ensures the reliability, fairness, and
accountability of the defect detection system, fostering confidence in
automated decisions through expert validation. By improving both performance
and interpretability, this work enhances trust, safety, and reliability in
welding defect detection systems, supporting critical operations in offshore
and marine environments.

</details>


### [80] [$MV_{Hybrid}$: Improving Spatial Transcriptomics Prediction with Hybrid State Space-Vision Transformer Backbone in Pathology Vision Foundation Models](https://arxiv.org/abs/2508.00383)
*Won June Cho,Hongjun Yoon,Daeky Jeong,Hyeongyeol Lim,Yosep Chong*

Main category: cs.CV

TL;DR: The paper introduces $MV_{Hybrid}$, a hybrid architecture combining state space models (SSMs) with Vision Transformers (ViT) to predict spatial gene expression from histopathology images, outperforming ViT in robustness and performance.


<details>
  <summary>Details</summary>
Motivation: Spatial transcriptomics is costly and complex, limiting clinical use. Predicting gene expression from routine histopathology images offers a practical alternative, but current ViT-based models fall short.

Method: The authors propose $MV_{Hybrid}$, a hybrid backbone of SSMs and ViT, pretrained on colorectal cancer datasets using DINOv2. They compare it against five other architectures in random split and leave-one-study-out (LOSO) evaluations.

Result: $MV_{Hybrid}$ achieves 57% higher correlation than ViT in LOSO and shows 43% smaller performance degradation. It also matches or exceeds ViT in classification, retrieval, and survival tasks.

Conclusion: $MV_{Hybrid}$ is a promising next-generation backbone for pathology vision foundation models, offering superior performance and robustness for clinical applications.

Abstract: Spatial transcriptomics reveals gene expression patterns within tissue
context, enabling precision oncology applications such as treatment response
prediction, but its high cost and technical complexity limit clinical adoption.
Predicting spatial gene expression (biomarkers) from routine histopathology
images offers a practical alternative, yet current vision foundation models
(VFMs) in pathology based on Vision Transformer (ViT) backbones perform below
clinical standards. Given that VFMs are already trained on millions of diverse
whole slide images, we hypothesize that architectural innovations beyond ViTs
may better capture the low-frequency, subtle morphological patterns correlating
with molecular phenotypes. By demonstrating that state space models initialized
with negative real eigenvalues exhibit strong low-frequency bias, we introduce
$MV_{Hybrid}$, a hybrid backbone architecture combining state space models
(SSMs) with ViT. We compare five other different backbone architectures for
pathology VFMs, all pretrained on identical colorectal cancer datasets using
the DINOv2 self-supervised learning method. We evaluate all pretrained models
using both random split and leave-one-study-out (LOSO) settings of the same
biomarker dataset. In LOSO evaluation, $MV_{Hybrid}$ achieves 57% higher
correlation than the best-performing ViT and shows 43% smaller performance
degradation compared to random split in gene expression prediction,
demonstrating superior performance and robustness, respectively. Furthermore,
$MV_{Hybrid}$ shows equal or better downstream performance in classification,
patch retrieval, and survival prediction tasks compared to that of ViT, showing
its promise as a next-generation pathology VFM backbone. Our code is publicly
available at: https://github.com/deepnoid-ai/MVHybrid.

</details>


### [81] [Cued-Agent: A Collaborative Multi-Agent System for Automatic Cued Speech Recognition](https://arxiv.org/abs/2508.00391)
*Guanjie Huang,Danny H. K. Tsang,Shan Yang,Guangzhi Lei,Li Liu*

Main category: cs.CV

TL;DR: The paper introduces Cued-Agent, a collaborative multi-agent system for Automatic Cued Speech Recognition (ACSR), addressing challenges in multimodal fusion and data scarcity. It outperforms existing methods in both normal and hearing-impaired scenarios.


<details>
  <summary>Details</summary>
Motivation: The temporal asynchrony between hand and lip movements in Cued Speech (CS) complicates multimodal fusion, and limited data availability hinders effective training of fusion mechanisms. Multi-agent systems offer a promising solution.

Method: Cued-Agent integrates four sub-agents: a Hand Recognition agent, a Lip Recognition agent, a Hand Prompt Decoding agent, and a Self-Correction Phoneme-to-Word agent. It uses keyframe screening, expert prompts, and semantic refinement.

Result: Cued-Agent achieves superior performance in ACSR compared to state-of-the-art methods, validated on an expanded Mandarin CS dataset.

Conclusion: The proposed multi-agent system effectively addresses ACSR challenges, demonstrating robustness and scalability, with potential for broader applications in multimodal AI.

Abstract: Cued Speech (CS) is a visual communication system that combines lip-reading
with hand coding to facilitate communication for individuals with hearing
impairments. Automatic CS Recognition (ACSR) aims to convert CS hand gestures
and lip movements into text via AI-driven methods. Traditionally, the temporal
asynchrony between hand and lip movements requires the design of complex
modules to facilitate effective multimodal fusion. However, constrained by
limited data availability, current methods demonstrate insufficient capacity
for adequately training these fusion mechanisms, resulting in suboptimal
performance. Recently, multi-agent systems have shown promising capabilities in
handling complex tasks with limited data availability. To this end, we propose
the first collaborative multi-agent system for ACSR, named Cued-Agent. It
integrates four specialized sub-agents: a Multimodal Large Language Model-based
Hand Recognition agent that employs keyframe screening and CS expert prompt
strategies to decode hand movements, a pretrained Transformer-based Lip
Recognition agent that extracts lip features from the input video, a Hand
Prompt Decoding agent that dynamically integrates hand prompts with lip
features during inference in a training-free manner, and a Self-Correction
Phoneme-to-Word agent that enables post-process and end-to-end conversion from
phoneme sequences to natural language sentences for the first time through
semantic refinement. To support this study, we expand the existing Mandarin CS
dataset by collecting data from eight hearing-impaired cuers, establishing a
mixed dataset of fourteen subjects. Extensive experiments demonstrate that our
Cued-Agent performs superbly in both normal and hearing-impaired scenarios
compared with state-of-the-art methods. The implementation is available at
https://github.com/DennisHgj/Cued-Agent.

</details>


### [82] [Decouple before Align: Visual Disentanglement Enhances Prompt Tuning](https://arxiv.org/abs/2508.00395)
*Fei Zhang,Tianfei Zhou,Jiangchao Yao,Ya Zhang,Ivor W. Tsang,Yanfeng Wang*

Main category: cs.CV

TL;DR: The paper introduces DAPT, a prompt tuning framework addressing visual-textual information asymmetry by decoupling and aligning visual foreground/background with text, improving model focus and performance.


<details>
  <summary>Details</summary>
Motivation: Address the overlooked issue of visual-textual information asymmetry in prompt tuning, which causes biased attention in vision-language models.

Method: Proposes DAPT: decouples visual modality into foreground/background, aligns them with text, and uses visual pull-push regularization for unbiased attention.

Result: DAPT shows superior performance in few-shot learning, generalization, and data-efficient learning across benchmarks.

Conclusion: DAPT effectively resolves asymmetry, enhancing modal alignment and model performance, with code publicly available.

Abstract: Prompt tuning (PT), as an emerging resource-efficient fine-tuning paradigm,
has showcased remarkable effectiveness in improving the task-specific
transferability of vision-language models. This paper delves into a previously
overlooked information asymmetry issue in PT, where the visual modality mostly
conveys more context than the object-oriented textual modality.
Correspondingly, coarsely aligning these two modalities could result in the
biased attention, driving the model to merely focus on the context area. To
address this, we propose DAPT, an effective PT framework based on an intuitive
decouple-before-align concept. First, we propose to explicitly decouple the
visual modality into the foreground and background representation via
exploiting coarse-and-fine visual segmenting cues, and then both of these
decoupled patterns are aligned with the original foreground texts and the
hand-crafted background classes, thereby symmetrically strengthening the modal
alignment. To further enhance the visual concentration, we propose a visual
pull-push regularization tailored for the foreground-background patterns,
directing the original visual representation towards unbiased attention on the
region-of-interest object. We demonstrate the power of architecture-free DAPT
through few-shot learning, base-to-novel generalization, and data-efficient
learning, all of which yield superior performance across prevailing benchmarks.
Our code will be released at https://github.com/Ferenas/DAPT.

</details>


### [83] [Video Forgery Detection with Optical Flow Residuals and Spatial-Temporal Consistency](https://arxiv.org/abs/2508.00397)
*Xi Xue,Kunio Suzuki,Nabarun Goswami,Takuya Shintate*

Main category: cs.CV

TL;DR: A dual-branch framework detects AI-generated videos by combining RGB appearance features and optical flow residuals, addressing temporal inconsistencies in synthetic content.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with fine-grained temporal inconsistencies in high-fidelity AI-generated videos, necessitating a robust detection approach.

Method: The proposed framework uses a dual-branch architecture: one branch analyzes RGB frames for appearance artifacts, and the other processes optical flow residuals for motion anomalies.

Result: The method effectively detects forged videos, showing robustness and generalization across ten generative models in text-to-video and image-to-video tasks.

Conclusion: The dual-branch framework successfully addresses the challenge of detecting AI-generated videos by leveraging spatial-temporal consistency.

Abstract: The rapid advancement of diffusion-based video generation models has led to
increasingly realistic synthetic content, presenting new challenges for video
forgery detection. Existing methods often struggle to capture fine-grained
temporal inconsistencies, particularly in AI-generated videos with high visual
fidelity and coherent motion. In this work, we propose a detection framework
that leverages spatial-temporal consistency by combining RGB appearance
features with optical flow residuals. The model adopts a dual-branch
architecture, where one branch analyzes RGB frames to detect appearance-level
artifacts, while the other processes flow residuals to reveal subtle motion
anomalies caused by imperfect temporal synthesis. By integrating these
complementary features, the proposed method effectively detects a wide range of
forged videos. Extensive experiments on text-to-video and image-to-video tasks
across ten diverse generative models demonstrate the robustness and strong
generalization ability of the proposed approach.

</details>


### [84] [iSafetyBench: A video-language benchmark for safety in industrial environment](https://arxiv.org/abs/2508.00399)
*Raiyaan Abdullah,Yogesh Singh Rawat,Shruti Vyas*

Main category: cs.CV

TL;DR: iSafetyBench is a new benchmark for evaluating vision-language models in industrial settings, focusing on routine and hazardous actions, revealing gaps in current models.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored capabilities of VLMs in high-stakes industrial domains, where recognizing routine and hazardous actions is crucial.

Method: Introduces iSafetyBench with 1,100 real-world industrial video clips, annotated with 98 routine and 67 hazardous action categories, paired with multiple-choice questions for evaluation.

Result: Eight state-of-the-art VLMs struggle with iSafetyBench, especially in hazardous activity recognition and multi-label scenarios.

Conclusion: Highlights the need for more robust, safety-aware multimodal models in industrial applications, with iSafetyBench serving as a testbed for progress.

Abstract: Recent advances in vision-language models (VLMs) have enabled impressive
generalization across diverse video understanding tasks under zero-shot
settings. However, their capabilities in high-stakes industrial domains-where
recognizing both routine operations and safety-critical anomalies is
essential-remain largely underexplored. To address this gap, we introduce
iSafetyBench, a new video-language benchmark specifically designed to evaluate
model performance in industrial environments across both normal and hazardous
scenarios. iSafetyBench comprises 1,100 video clips sourced from real-world
industrial settings, annotated with open-vocabulary, multi-label action tags
spanning 98 routine and 67 hazardous action categories. Each clip is paired
with multiple-choice questions for both single-label and multi-label
evaluation, enabling fine-grained assessment of VLMs in both standard and
safety-critical contexts. We evaluate eight state-of-the-art video-language
models under zero-shot conditions. Despite their strong performance on existing
video benchmarks, these models struggle with iSafetyBench-particularly in
recognizing hazardous activities and in multi-label scenarios. Our results
reveal significant performance gaps, underscoring the need for more robust,
safety-aware multimodal models for industrial applications. iSafetyBench
provides a first-of-its-kind testbed to drive progress in this direction. The
dataset is available at: https://github.com/raiyaan-abdullah/iSafety-Bench.

</details>


### [85] [Sari Sandbox: A Virtual Retail Store Environment for Embodied AI Agents](https://arxiv.org/abs/2508.00400)
*Janika Deborah Gajo,Gerarld Paul Merales,Jerome Escarcha,Brenden Ashley Molina,Gian Nartea,Emmanuel G. Maminta,Juan Carlos Roldan,Rowel O. Atienza*

Main category: cs.CV

TL;DR: Sari Sandbox is a photorealistic 3D retail store simulation for benchmarking embodied agents against human performance in shopping tasks, featuring interactive items and VR support.


<details>
  <summary>Details</summary>
Motivation: To address the lack of retail-specific simulation environments for training embodied agents.

Method: Developed a high-fidelity 3D simulation with 250+ interactive grocery items, VR for human interaction, and a VLM-powered agent. Introduced SariBench, a dataset of annotated human demonstrations.

Result: Enabled benchmarking of embodied agents against human performance in navigation, inspection, and manipulation tasks.

Conclusion: The sandbox provides baselines and recommendations for improving realism and scalability, with open-source code available.

Abstract: We present Sari Sandbox, a high-fidelity, photorealistic 3D retail store
simulation for benchmarking embodied agents against human performance in
shopping tasks. Addressing a gap in retail-specific sim environments for
embodied agent training, Sari Sandbox features over 250 interactive grocery
items across three store configurations, controlled via an API. It supports
both virtual reality (VR) for human interaction and a vision language model
(VLM)-powered embodied agent. We also introduce SariBench, a dataset of
annotated human demonstrations across varied task difficulties. Our sandbox
enables embodied agents to navigate, inspect, and manipulate retail items,
providing baselines against human performance. We conclude with benchmarks,
performance analysis, and recommendations for enhancing realism and
scalability. The source code can be accessed via
https://github.com/upeee/sari-sandbox-env.

</details>


### [86] [PMR: Physical Model-Driven Multi-Stage Restoration of Turbulent Dynamic Videos](https://arxiv.org/abs/2508.00406)
*Tao Wu,Jingyuan Ye,Ying Fu*

Main category: cs.CV

TL;DR: The paper introduces a Dynamic Efficiency Index (DEI) and a Physical Model-Driven Multi-Stage Video Restoration (PMR) framework to address turbulence-induced distortions in videos, achieving high restoration quality and efficiency.


<details>
  <summary>Details</summary>
Motivation: Atmospheric turbulence causes geometric distortions and blurring in long-range dynamic scene videos, and existing methods fail to restore edge details and eliminate mixed distortions under strong turbulence and complex dynamics.

Method: The proposed PMR framework includes three stages: de-tilting for stabilization, motion segmentation enhancement for dynamic region refinement, and de-blurring for quality restoration, using lightweight backbones and joint training.

Result: The method effectively suppresses motion trailing artifacts, restores edge details, and generalizes well in high-turbulence, complex dynamic scenarios.

Conclusion: The DEI and PMR framework provide an efficient and high-quality solution for turbulence-distorted video restoration, with code and datasets to be made publicly available.

Abstract: Geometric distortions and blurring caused by atmospheric turbulence degrade
the quality of long-range dynamic scene videos. Existing methods struggle with
restoring edge details and eliminating mixed distortions, especially under
conditions of strong turbulence and complex dynamics. To address these
challenges, we introduce a Dynamic Efficiency Index ($DEI$), which combines
turbulence intensity, optical flow, and proportions of dynamic regions to
accurately quantify video dynamic intensity under varying turbulence conditions
and provide a high-dynamic turbulence training dataset. Additionally, we
propose a Physical Model-Driven Multi-Stage Video Restoration ($PMR$) framework
that consists of three stages: \textbf{de-tilting} for geometric stabilization,
\textbf{motion segmentation enhancement} for dynamic region refinement, and
\textbf{de-blurring} for quality restoration. $PMR$ employs lightweight
backbones and stage-wise joint training to ensure both efficiency and high
restoration quality. Experimental results demonstrate that the proposed method
effectively suppresses motion trailing artifacts, restores edge details and
exhibits strong generalization capability, especially in real-world scenarios
characterized by high-turbulence and complex dynamics. We will make the code
and datasets openly available.

</details>


### [87] [Sortblock: Similarity-Aware Feature Reuse for Diffusion Model](https://arxiv.org/abs/2508.00412)
*Hanqi Chen,Xu Zhang,Xiaoliu Guan,Lielin Jiang,Guanzhong Wang,Zeyu Chen,Yi Liu*

Main category: cs.CV

TL;DR: Sortblock is a training-free framework that accelerates DiTs by dynamically caching block-wise features and skipping redundant computations, achieving 2x speedup with minimal quality loss.


<details>
  <summary>Details</summary>
Motivation: DiTs suffer from high inference latency due to sequential denoising, limiting real-time use. Existing methods overlook evolving semantic focus.

Method: Sortblock dynamically caches features based on similarity across timesteps, ranks residuals, and uses a linear predictor to reduce errors.

Result: Achieves over 2x speedup with minimal quality degradation across tasks and DiT architectures.

Conclusion: Sortblock provides an effective, generalizable solution for accelerating diffusion-based models without retraining.

Abstract: Diffusion Transformers (DiTs) have demonstrated remarkable generative
capabilities, particularly benefiting from Transformer architectures that
enhance visual and artistic fidelity. However, their inherently sequential
denoising process results in high inference latency, limiting their deployment
in real-time scenarios. Existing training-free acceleration approaches
typically reuse intermediate features at fixed timesteps or layers, overlooking
the evolving semantic focus across denoising stages and Transformer blocks.To
address this, we propose Sortblock, a training-free inference acceleration
framework that dynamically caches block-wise features based on their similarity
across adjacent timesteps. By ranking the evolution of residuals, Sortblock
adaptively determines a recomputation ratio, selectively skipping redundant
computations while preserving generation quality. Furthermore, we incorporate a
lightweight linear prediction mechanism to reduce accumulated errors in skipped
blocks.Extensive experiments across various tasks and DiT architectures
demonstrate that Sortblock achieves over 2$\times$ inference speedup with
minimal degradation in output quality, offering an effective and generalizable
solution for accelerating diffusion-based generative models.

</details>


### [88] [DC-AE 1.5: Accelerating Diffusion Model Convergence with Structured Latent Space](https://arxiv.org/abs/2508.00413)
*Junyu Chen,Dongyun Zou,Wenkun He,Junsong Chen,Enze Xie,Song Han,Han Cai*

Main category: cs.CV

TL;DR: DC-AE 1.5 introduces structured latent space and augmented diffusion training to improve convergence and generation quality in high-resolution diffusion models.


<details>
  <summary>Details</summary>
Motivation: The challenge of slow convergence in diffusion models when increasing autoencoder latent channels limits quality and compression ratios.

Method: Two innovations: Structured Latent Space for channel-wise organization and Augmented Diffusion Training with additional objectives.

Result: DC-AE 1.5 achieves faster convergence and better generation quality, outperforming DC-AE on ImageNet 512x512.

Conclusion: DC-AE 1.5 effectively addresses convergence and quality issues, enabling higher compression ratios and better performance.

Abstract: We present DC-AE 1.5, a new family of deep compression autoencoders for
high-resolution diffusion models. Increasing the autoencoder's latent channel
number is a highly effective approach for improving its reconstruction quality.
However, it results in slow convergence for diffusion models, leading to poorer
generation quality despite better reconstruction quality. This issue limits the
quality upper bound of latent diffusion models and hinders the employment of
autoencoders with higher spatial compression ratios. We introduce two key
innovations to address this challenge: i) Structured Latent Space, a
training-based approach to impose a desired channel-wise structure on the
latent space with front latent channels capturing object structures and latter
latent channels capturing image details; ii) Augmented Diffusion Training, an
augmented diffusion training strategy with additional diffusion training
objectives on object latent channels to accelerate convergence. With these
techniques, DC-AE 1.5 delivers faster convergence and better diffusion scaling
results than DC-AE. On ImageNet 512x512, DC-AE-1.5-f64c128 delivers better
image generation quality than DC-AE-f32c32 while being 4x faster. Code:
https://github.com/dc-ai-projects/DC-Gen.

</details>


### [89] [IN2OUT: Fine-Tuning Video Inpainting Model for Video Outpainting Using Hierarchical Discriminator](https://arxiv.org/abs/2508.00418)
*Sangwoo Youn,Minji Lee,Nokap Tony Park,Yeonggyoo Jeon,Taeyoung Na*

Main category: cs.CV

TL;DR: The paper proposes a method to improve video outpainting by adapting video inpainting models with a hierarchical discriminator and specialized loss function, achieving better results than existing methods.


<details>
  <summary>Details</summary>
Motivation: Video outpainting struggles with maintaining consistency when extending borders. Existing methods focus on background generation, but adapting inpainting models directly leads to blurry results.

Method: The authors introduce a hierarchical discriminator for adversarial training, differentiating global and local objectives, and develop a specialized outpainting loss function.

Result: The proposed method outperforms state-of-the-art approaches, producing visually appealing and coherent outpainted scenes.

Conclusion: The hierarchical discriminator and tailored loss function effectively enhance video outpainting, demonstrating superior performance.

Abstract: Video outpainting presents a unique challenge of extending the borders while
maintaining consistency with the given content. In this paper, we suggest the
use of video inpainting models that excel in object flow learning and
reconstruction in outpainting rather than solely generating the background as
in existing methods. However, directly applying or fine-tuning inpainting
models to outpainting has shown to be ineffective, often leading to blurry
results. Our extensive experiments on discriminator designs reveal that a
critical component missing in the outpainting fine-tuning process is a
discriminator capable of effectively assessing the perceptual quality of the
extended areas. To tackle this limitation, we differentiate the objectives of
adversarial training into global and local goals and introduce a hierarchical
discriminator that meets both objectives. Additionally, we develop a
specialized outpainting loss function that leverages both local and global
features of the discriminator. Fine-tuning on this adversarial loss function
enhances the generator's ability to produce both visually appealing and
globally coherent outpainted scenes. Our proposed method outperforms
state-of-the-art methods both quantitatively and qualitatively. Supplementary
materials including the demo video and the code are available in SigPort.

</details>


### [90] [UIS-Mamba: Exploring Mamba for Underwater Instance Segmentation via Dynamic Tree Scan and Hidden State Weaken](https://arxiv.org/abs/2508.00421)
*Runmin Cong,Zongji Yu,Hao Fang,Haoyan Sun,Sam Kwong*

Main category: cs.CV

TL;DR: UIS-Mamba, a Mamba-based model for underwater instance segmentation, introduces Dynamic Tree Scan and Hidden State Weaken modules to address challenges like color distortion and blurred boundaries, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Underwater scenes pose challenges like color distortion and blurred boundaries, making instance segmentation difficult. Mamba's linear complexity and global receptive fields are promising but need adaptation for underwater tasks.

Method: Proposes UIS-Mamba with Dynamic Tree Scan (DTS) for dynamic local receptive fields and Hidden State Weaken (HSW) to suppress background interference using Ncut-based weakening.

Result: Achieves state-of-the-art performance on UIIS and USIS10K datasets with low parameters and computational complexity.

Conclusion: UIS-Mamba effectively adapts Mamba for underwater instance segmentation, addressing key challenges and outperforming existing methods.

Abstract: Underwater Instance Segmentation (UIS) tasks are crucial for underwater
complex scene detection. Mamba, as an emerging state space model with
inherently linear complexity and global receptive fields, is highly suitable
for processing image segmentation tasks with long sequence features. However,
due to the particularity of underwater scenes, there are many challenges in
applying Mamba to UIS. The existing fixed-patch scanning mechanism cannot
maintain the internal continuity of scanned instances in the presence of
severely underwater color distortion and blurred instance boundaries, and the
hidden state of the complex underwater background can also inhibit the
understanding of instance objects. In this work, we propose the first
Mamba-based underwater instance segmentation model UIS-Mamba, and design two
innovative modules, Dynamic Tree Scan (DTS) and Hidden State Weaken (HSW), to
migrate Mamba to the underwater task. DTS module maintains the continuity of
the internal features of the instance objects by allowing the patches to
dynamically offset and scale, thereby guiding the minimum spanning tree and
providing dynamic local receptive fields. HSW module suppresses the
interference of complex backgrounds and effectively focuses the information
flow of state propagation to the instances themselves through the Ncut-based
hidden state weakening mechanism. Experimental results show that UIS-Mamba
achieves state-of-the-art performance on both UIIS and USIS10K datasets, while
maintaining a low number of parameters and computational complexity. Code is
available at https://github.com/Maricalce/UIS-Mamba.

</details>


### [91] [Contact-Aware Amodal Completion for Human-Object Interaction via Multi-Regional Inpainting](https://arxiv.org/abs/2508.00427)
*Seunggeun Chi,Enna Sachdeva,Pin-Hao Huang,Kwonjoon Lee*

Main category: cs.CV

TL;DR: A new method for amodal completion in human-object interactions (HOI) uses physical priors and multi-regional inpainting to improve accuracy and realism, outperforming existing approaches.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with plausible completions in dynamic HOI scenarios due to limited understanding of interactions.

Method: Incorporates physical constraints (human topology, contact info) and uses a multi-regional inpainting technique with customized denoising in a diffusion model.

Result: Significantly outperforms existing methods in HOI scenarios, enhancing realism and accuracy.

Conclusion: The approach advances machine perception in dynamic environments and is robust even without ground-truth contact annotations, broadening its applicability.

Abstract: Amodal completion, which is the process of inferring the full appearance of
objects despite partial occlusions, is crucial for understanding complex
human-object interactions (HOI) in computer vision and robotics. Existing
methods, such as those that use pre-trained diffusion models, often struggle to
generate plausible completions in dynamic scenarios because they have a limited
understanding of HOI. To solve this problem, we've developed a new approach
that uses physical prior knowledge along with a specialized multi-regional
inpainting technique designed for HOI. By incorporating physical constraints
from human topology and contact information, we define two distinct regions:
the primary region, where occluded object parts are most likely to be, and the
secondary region, where occlusions are less probable. Our multi-regional
inpainting method uses customized denoising strategies across these regions
within a diffusion model. This improves the accuracy and realism of the
generated completions in both their shape and visual detail. Our experimental
results show that our approach significantly outperforms existing methods in
HOI scenarios, moving machine perception closer to a more human-like
understanding of dynamic environments. We also show that our pipeline is robust
even without ground-truth contact annotations, which broadens its applicability
to tasks like 3D reconstruction and novel view/pose synthesis.

</details>


### [92] [Reducing the gap between general purpose data and aerial images in concentrated solar power plants](https://arxiv.org/abs/2508.00440)
*M. A. Prez-Cutio,J. Valverde,J. Capitn,J. M. Daz-Bez*

Main category: cs.CV

TL;DR: Proposes AerialCSP, a synthetic dataset for training models on aerial images of CSP plants, reducing manual labeling needs.


<details>
  <summary>Details</summary>
Motivation: Generic datasets fail for CSP plants due to reflective surfaces and unique elements, making manual labeling costly.

Method: Created AerialCSP, a virtual dataset simulating CSP plant imagery, and benchmarked models on it.

Result: Pretraining on AerialCSP improves real-world fault detection, especially for rare/small defects.

Conclusion: AerialCSP effectively reduces manual labeling and enhances model performance for CSP plant inspections.

Abstract: In the context of Concentrated Solar Power (CSP) plants, aerial images
captured by drones present a unique set of challenges. Unlike urban or natural
landscapes commonly found in existing datasets, solar fields contain highly
reflective surfaces, and domain-specific elements that are uncommon in
traditional computer vision benchmarks. As a result, machine learning models
trained on generic datasets struggle to generalize to this setting without
extensive retraining and large volumes of annotated data. However, collecting
and labeling such data is costly and time-consuming, making it impractical for
rapid deployment in industrial applications.
  To address this issue, we propose a novel approach: the creation of
AerialCSP, a virtual dataset that simulates aerial imagery of CSP plants. By
generating synthetic data that closely mimic real-world conditions, our
objective is to facilitate pretraining of models before deployment,
significantly reducing the need for extensive manual labeling. Our main
contributions are threefold: (1) we introduce AerialCSP, a high-quality
synthetic dataset for aerial inspection of CSP plants, providing annotated data
for object detection and image segmentation; (2) we benchmark multiple models
on AerialCSP, establishing a baseline for CSP-related vision tasks; and (3) we
demonstrate that pretraining on AerialCSP significantly improves real-world
fault detection, particularly for rare and small defects, reducing the need for
extensive manual labeling. AerialCSP is made publicly available at
https://mpcutino.github.io/aerialcsp/.

</details>


### [93] [TopoTTA: Topology-Enhanced Test-Time Adaptation for Tubular Structure Segmentation](https://arxiv.org/abs/2508.00442)
*Jiale Zhou,Wenhan Wang,Shikun Li,Xiaolei Qu,Xin Guo,Yizhong Liu,Wenzhong Tang,Xun Lin,Yefeng Zheng*

Main category: cs.CV

TL;DR: TopoTTA is a test-time adaptation framework for tubular structure segmentation, addressing domain shifts by enhancing topological representation and continuity.


<details>
  <summary>Details</summary>
Motivation: Domain shifts in tubular structure segmentation degrade performance due to sensitivity to topological changes and local feature variations.

Method: TopoTTA uses Topological Meta Difference Convolutions (TopoMDCs) for cross-domain adaptation and Topology Hard sample Generation (TopoHG) for continuity improvement.

Result: TopoTTA improves performance by 31.81% in clDice across four scenarios and ten datasets.

Conclusion: TopoTTA effectively handles topological distribution shifts and is a plug-and-play solution for CNN-based TSS models.

Abstract: Tubular structure segmentation (TSS) is important for various applications,
such as hemodynamic analysis and route navigation. Despite significant progress
in TSS, domain shifts remain a major challenge, leading to performance
degradation in unseen target domains. Unlike other segmentation tasks, TSS is
more sensitive to domain shifts, as changes in topological structures can
compromise segmentation integrity, and variations in local features
distinguishing foreground from background (e.g., texture and contrast) may
further disrupt topological continuity. To address these challenges, we propose
Topology-enhanced Test-Time Adaptation (TopoTTA), the first test-time
adaptation framework designed specifically for TSS. TopoTTA consists of two
stages: Stage 1 adapts models to cross-domain topological discrepancies using
the proposed Topological Meta Difference Convolutions (TopoMDCs), which enhance
topological representation without altering pre-trained parameters; Stage 2
improves topological continuity by a novel Topology Hard sample Generation
(TopoHG) strategy and prediction alignment on hard samples with pseudo-labels
in the generated pseudo-break regions. Extensive experiments across four
scenarios and ten datasets demonstrate TopoTTA's effectiveness in handling
topological distribution shifts, achieving an average improvement of 31.81% in
clDice. TopoTTA also serves as a plug-and-play TTA solution for CNN-based TSS
models.

</details>


### [94] [SDMatte: Grafting Diffusion Models for Interactive Matting](https://arxiv.org/abs/2508.00443)
*Longfei Huang,Yu Liang,Hao Zhang,Jinwei Chen,Wei Dong,Lunde Chen,Wanyu Liu,Bo Li,Pengtao Jiang*

Main category: cs.CV

TL;DR: SDMatte leverages diffusion models for interactive matting, improving fine-grained detail extraction with visual prompts and a masked self-attention mechanism.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack fine-grained detail extraction in edge regions, while diffusion models offer robust capabilities for such tasks.

Method: SDMatte transforms text-driven interaction into visual prompt-driven interaction, integrates coordinate and opacity embeddings, and uses masked self-attention.

Result: Extensive experiments show superior performance in interactive matting.

Conclusion: SDMatte effectively addresses fine-grained detail extraction in interactive matting, validated by experiments.

Abstract: Recent interactive matting methods have shown satisfactory performance in
capturing the primary regions of objects, but they fall short in extracting
fine-grained details in edge regions. Diffusion models trained on billions of
image-text pairs, demonstrate exceptional capability in modeling highly complex
data distributions and synthesizing realistic texture details, while exhibiting
robust text-driven interaction capabilities, making them an attractive solution
for interactive matting. To this end, we propose SDMatte, a diffusion-driven
interactive matting model, with three key contributions. First, we exploit the
powerful priors of diffusion models and transform the text-driven interaction
capability into visual prompt-driven interaction capability to enable
interactive matting. Second, we integrate coordinate embeddings of visual
prompts and opacity embeddings of target objects into U-Net, enhancing
SDMatte's sensitivity to spatial position information and opacity information.
Third, we propose a masked self-attention mechanism that enables the model to
focus on areas specified by visual prompts, leading to better performance.
Extensive experiments on multiple datasets demonstrate the superior performance
of our method, validating its effectiveness in interactive matting. Our code
and model are available at https://github.com/vivoCameraResearch/SDMatte.

</details>


### [95] [AutoDebias: Automated Framework for Debiasing Text-to-Image Models](https://arxiv.org/abs/2508.00445)
*Hongyi Cai,Mohammad Mahdinur Rahman,Mingkang Dong,Jie Li,Muxin Pu,Zhili Fang,Yinan Peng,Hanjun Luo,Yang Liu*

Main category: cs.CV

TL;DR: AutoDebias is a framework for automatically detecting and mitigating social biases in Text-to-Image models without prior knowledge of bias types, using vision-language models and fairness guides.


<details>
  <summary>Details</summary>
Motivation: Address unintended social biases (e.g., gender, racial stereotypes) in T2I models, especially subtle or overlapping biases that existing methods struggle with.

Method: Leverages vision-language models to detect biases, constructs fairness guides with inclusive prompts, and uses CLIP-guided training to balance representations.

Result: Achieves 91.6% accuracy in detecting biases, reduces biased outputs from 90% to negligible levels, and maintains image quality and diversity.

Conclusion: AutoDebias effectively addresses subtle and multiple interacting biases in T2I models while preserving visual fidelity.

Abstract: Text-to-Image (T2I) models generate high-quality images from text prompts but
often exhibit unintended social biases, such as gender or racial stereotypes,
even when these attributes are not mentioned. Existing debiasing methods work
well for simple or well-known cases but struggle with subtle or overlapping
biases. We propose AutoDebias, a framework that automatically identifies and
mitigates harmful biases in T2I models without prior knowledge of specific bias
types. Specifically, AutoDebias leverages vision-language models to detect
biased visual patterns and constructs fairness guides by generating inclusive
alternative prompts that reflect balanced representations. These guides drive a
CLIP-guided training process that promotes fairer outputs while preserving the
original model's image quality and diversity. Unlike existing methods,
AutoDebias effectively addresses both subtle stereotypes and multiple
interacting biases. We evaluate the framework on a benchmark covering over 25
bias scenarios, including challenging cases where multiple biases occur
simultaneously. AutoDebias detects harmful patterns with 91.6% accuracy and
reduces biased outputs from 90% to negligible levels, while preserving the
visual fidelity of the original model.

</details>


### [96] [CLIPTime: Time-Aware Multimodal Representation Learning from Images and Text](https://arxiv.org/abs/2508.00447)
*Anju Rani,Daniel Ortiz-Arroyo,Petar Durdevic*

Main category: cs.CV

TL;DR: CLIPTime is a multimodal framework built on CLIP to predict fungal growth stages and timestamps from image-text inputs, improving temporal modeling in biological monitoring.


<details>
  <summary>Details</summary>
Motivation: Existing vision-language models like CLIP lack effectiveness in capturing temporal progression, which is crucial for biological growth analysis.

Method: CLIPTime extends CLIP to jointly predict developmental stages (classification) and timestamps (regression) using a synthetic fungal growth dataset.

Result: CLIPTime successfully models biological progression and provides interpretable, temporally grounded predictions.

Conclusion: The framework demonstrates the potential of vision-language models for real-world biological monitoring applications.

Abstract: Understanding the temporal dynamics of biological growth is critical across
diverse fields such as microbiology, agriculture, and biodegradation research.
Although vision-language models like Contrastive Language Image Pretraining
(CLIP) have shown strong capabilities in joint visual-textual reasoning, their
effectiveness in capturing temporal progression remains limited. To address
this, we propose CLIPTime, a multimodal, multitask framework designed to
predict both the developmental stage and the corresponding timestamp of fungal
growth from image and text inputs. Built upon the CLIP architecture, our model
learns joint visual-textual embeddings and enables time-aware inference without
requiring explicit temporal input during testing. To facilitate training and
evaluation, we introduce a synthetic fungal growth dataset annotated with
aligned timestamps and categorical stage labels. CLIPTime jointly performs
classification and regression, predicting discrete growth stages alongside
continuous timestamps. We also propose custom evaluation metrics, including
temporal accuracy and regression error, to assess the precision of time-aware
predictions. Experimental results demonstrate that CLIPTime effectively models
biological progression and produces interpretable, temporally grounded outputs,
highlighting the potential of vision-language models in real-world biological
monitoring applications.

</details>


### [97] [PIF-Net: Ill-Posed Prior Guided Multispectral and Hyperspectral Image Fusion via Invertible Mamba and Fusion-Aware LoRA](https://arxiv.org/abs/2508.00453)
*Baisong Li,Xingwang Wang,Haixiao Xu*

Main category: cs.CV

TL;DR: PIF-Net is a novel framework for multispectral and hyperspectral image fusion (MHIF) that addresses the ill-posed nature of the task by incorporating ill-posed priors and using an invertible Mamba architecture for efficient feature fusion.


<details>
  <summary>Details</summary>
Motivation: The inherent trade-off between spectral and spatial information in MHIF makes the task ill-posed, and previous methods failed to address data misalignment effectively.

Method: PIF-Net uses an invertible Mamba architecture for global spectral modeling and introduces a Fusion-Aware Low-Rank Adaptation module for dynamic feature calibration.

Result: Experiments show PIF-Net outperforms state-of-the-art methods in image restoration while maintaining efficiency.

Conclusion: PIF-Net effectively balances spectral and spatial information, offering a robust solution to the ill-posed MHIF problem.

Abstract: The goal of multispectral and hyperspectral image fusion (MHIF) is to
generate high-quality images that simultaneously possess rich spectral
information and fine spatial details. However, due to the inherent trade-off
between spectral and spatial information and the limited availability of
observations, this task is fundamentally ill-posed. Previous studies have not
effectively addressed the ill-posed nature caused by data misalignment. To
tackle this challenge, we propose a fusion framework named PIF-Net, which
explicitly incorporates ill-posed priors to effectively fuse multispectral
images and hyperspectral images. To balance global spectral modeling with
computational efficiency, we design a method based on an invertible Mamba
architecture that maintains information consistency during feature
transformation and fusion, ensuring stable gradient flow and process
reversibility. Furthermore, we introduce a novel fusion module called the
Fusion-Aware Low-Rank Adaptation module, which dynamically calibrates spectral
and spatial features while keeping the model lightweight. Extensive experiments
on multiple benchmark datasets demonstrate that PIF-Net achieves significantly
better image restoration performance than current state-of-the-art methods
while maintaining model efficiency.

</details>


### [98] [Semantic and Temporal Integration in Latent Diffusion Space for High-Fidelity Video Super-Resolution](https://arxiv.org/abs/2508.00471)
*Yiwen Wang,Xinning Chai,Yuhong Zhang,Zhengxue Cheng,Jun Zhao,Rong Xie,Li Song*

Main category: cs.CV

TL;DR: SeTe-VSR improves video super-resolution by integrating semantic and temporal-spatio guidance, achieving better detail recovery and temporal coherence.


<details>
  <summary>Details</summary>
Motivation: Current VSR models struggle with fidelity alignment and temporal consistency in low-resolution video enhancement.

Method: Proposes SeTe-VSR, using semantic and temporal-spatio guidance in latent diffusion space for balanced detail recovery and coherence.

Result: Outperforms existing methods in detail recovery and perceptual quality.

Conclusion: SeTe-VSR is effective for complex video super-resolution tasks.

Abstract: Recent advancements in video super-resolution (VSR) models have demonstrated
impressive results in enhancing low-resolution videos. However, due to
limitations in adequately controlling the generation process, achieving high
fidelity alignment with the low-resolution input while maintaining temporal
consistency across frames remains a significant challenge. In this work, we
propose Semantic and Temporal Guided Video Super-Resolution (SeTe-VSR), a novel
approach that incorporates both semantic and temporal-spatio guidance in the
latent diffusion space to address these challenges. By incorporating high-level
semantic information and integrating spatial and temporal information, our
approach achieves a seamless balance between recovering intricate details and
ensuring temporal coherence. Our method not only preserves high-reality visual
content but also significantly enhances fidelity. Extensive experiments
demonstrate that SeTe-VSR outperforms existing methods in terms of detail
recovery and perceptual quality, highlighting its effectiveness for complex
video super-resolution tasks.

</details>


### [99] [HyPCV-Former: Hyperbolic Spatio-Temporal Transformer for 3D Point Cloud Video Anomaly Detection](https://arxiv.org/abs/2508.00473)
*Jiaping Cao,Kangkang Zhou,Juan Du*

Main category: cs.CV

TL;DR: HyPCV-Former, a hyperbolic spatio-temporal transformer, improves video anomaly detection by leveraging Lorentzian hyperbolic space and a novel attention mechanism, outperforming benchmarks by 5.6-7%.


<details>
  <summary>Details</summary>
Motivation: Traditional Euclidean representations in RGB or depth domains fail to capture hierarchical event structures and spatio-temporal continuity in video anomaly detection.

Method: HyPCV-Former uses a point cloud extractor for spatial features, embeds them in Lorentzian hyperbolic space, and employs a hyperbolic multi-head self-attention (HMHA) mechanism for temporal dynamics.

Result: Achieves state-of-the-art performance with 7% improvement on TIMo and 5.6% on DAD datasets.

Conclusion: HyPCV-Former effectively addresses limitations of Euclidean embeddings, offering superior anomaly detection in 3D point cloud videos.

Abstract: Video anomaly detection is a fundamental task in video surveillance, with
broad applications in public safety and intelligent monitoring systems.
Although previous methods leverage Euclidean representations in RGB or depth
domains, such embeddings are inherently limited in capturing hierarchical event
structures and spatio-temporal continuity. To address these limitations, we
propose HyPCV-Former, a novel hyperbolic spatio-temporal transformer for
anomaly detection in 3D point cloud videos. Our approach first extracts
per-frame spatial features from point cloud sequences via point cloud
extractor, and then embeds them into Lorentzian hyperbolic space, which better
captures the latent hierarchical structure of events. To model temporal
dynamics, we introduce a hyperbolic multi-head self-attention (HMHA) mechanism
that leverages Lorentzian inner products and curvature-aware softmax to learn
temporal dependencies under non-Euclidean geometry. Our method performs all
feature transformations and anomaly scoring directly within full Lorentzian
space rather than via tangent space approximation. Extensive experiments
demonstrate that HyPCV-Former achieves state-of-the-art performance across
multiple anomaly categories, with a 7\% improvement on the TIMo dataset and a
5.6\% gain on the DAD dataset compared to benchmarks. The code will be released
upon paper acceptance.

</details>


### [100] [Fine-grained Spatiotemporal Grounding on Egocentric Videos](https://arxiv.org/abs/2508.00518)
*Shuo Liang,Yiwu Zhong,Zi-Yuan Hu,Yeyao Tao,Liwei Wang*

Main category: cs.CV

TL;DR: The paper introduces EgoMask, a pixel-level benchmark for spatiotemporal grounding in egocentric videos, addressing challenges like shorter object durations and sparser trajectories. It includes an automatic annotation pipeline and a training dataset (EgoMask-Train), showing significant improvements when fine-tuning existing models.


<details>
  <summary>Details</summary>
Motivation: Egocentric video grounding is underexplored compared to exocentric videos, despite its importance in applications like augmented reality and robotics. The paper aims to bridge this gap by analyzing discrepancies and providing resources for better egocentric video understanding.

Method: The authors introduce EgoMask, a benchmark created via an automatic annotation pipeline for fine-grained spatiotemporal grounding. They also develop EgoMask-Train, a large-scale training dataset, and evaluate state-of-the-art models on their benchmark.

Result: Existing models perform poorly on EgoMask but show significant improvement after fine-tuning on EgoMask-Train, without compromising performance on exocentric datasets.

Conclusion: The work provides essential resources (EgoMask and EgoMask-Train) and insights for advancing egocentric video understanding, demonstrating the effectiveness of their approach.

Abstract: Spatiotemporal video grounding aims to localize target entities in videos
based on textual queries. While existing research has made significant progress
in exocentric videos, the egocentric setting remains relatively underexplored,
despite its growing importance in applications such as augmented reality and
robotics. In this work, we conduct a systematic analysis of the discrepancies
between egocentric and exocentric videos, revealing key challenges such as
shorter object durations, sparser trajectories, smaller object sizes, and
larger positional shifts. To address these challenges, we introduce EgoMask,
the first pixel-level benchmark for fine-grained spatiotemporal grounding in
egocentric videos. It is constructed by our proposed automatic annotation
pipeline, which annotates referring expressions and object masks across short-,
medium-, and long-term videos. Additionally, we create EgoMask-Train, a
large-scale training dataset to facilitate model development. Experiments
demonstrate that the state-of-the-art spatiotemporal grounding models perform
poorly on our benchmark EgoMask, but fine-tuning on EgoMask-Train yields
significant improvements, while preserving performance on exocentric datasets.
Our work thus provides essential resources and insights for advancing
egocentric video understanding. Our code is available at
https://github.com/LaVi-Lab/EgoMask .

</details>


### [101] [LAMIC: Layout-Aware Multi-Image Composition via Scalability of Multimodal Diffusion Transformer](https://arxiv.org/abs/2508.00477)
*Yuzhuo Chen,Zehua Ma,Jianhua Wang,Kai Kang,Shunyu Yao,Weiming Zhang*

Main category: cs.CV

TL;DR: LAMIC is a Layout-Aware Multi-Image Composition framework that extends single-reference diffusion models to multi-reference scenarios without training, using novel attention mechanisms and achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: The challenge of generating coherent and consistent images from multiple references with spatial layout awareness in controllable image synthesis.

Method: LAMIC introduces Group Isolation Attention (GIA) for entity disentanglement and Region-Modulated Attention (RMA) for layout-aware generation, built upon the MMDiT model.

Result: LAMIC outperforms existing baselines in ID-S, BG-S, IN-R, and AVG scores, demonstrating superior identity keeping, background preservation, and layout control.

Conclusion: LAMIC establishes a training-free paradigm for multi-image composition, with strong zero-shot generalization and scalability potential.

Abstract: In controllable image synthesis, generating coherent and consistent images
from multiple references with spatial layout awareness remains an open
challenge. We present LAMIC, a Layout-Aware Multi-Image Composition framework
that, for the first time, extends single-reference diffusion models to
multi-reference scenarios in a training-free manner. Built upon the MMDiT
model, LAMIC introduces two plug-and-play attention mechanisms: 1) Group
Isolation Attention (GIA) to enhance entity disentanglement; and 2)
Region-Modulated Attention (RMA) to enable layout-aware generation. To
comprehensively evaluate model capabilities, we further introduce three
metrics: 1) Inclusion Ratio (IN-R) and Fill Ratio (FI-R) for assessing layout
control; and 2) Background Similarity (BG-S) for measuring background
consistency. Extensive experiments show that LAMIC achieves state-of-the-art
performance across most major metrics: it consistently outperforms existing
multi-reference baselines in ID-S, BG-S, IN-R and AVG scores across all
settings, and achieves the best DPG in complex composition tasks. These results
demonstrate LAMIC's superior abilities in identity keeping, background
preservation, layout control, and prompt-following, all achieved without any
training or fine-tuning, showcasing strong zero-shot generalization ability. By
inheriting the strengths of advanced single-reference models and enabling
seamless extension to multi-image scenarios, LAMIC establishes a new
training-free paradigm for controllable multi-image composition. As foundation
models continue to evolve, LAMIC's performance is expected to scale
accordingly. Our implementation is available at:
https://github.com/Suchenl/LAMIC.

</details>


### [102] [SAMSA 2.0: Prompting Segment Anything with Spectral Angles for Hyperspectral Interactive Medical Image Segmentation](https://arxiv.org/abs/2508.00493)
*Alfie Roddan,Tobias Czempiel,Chi Xu,Daniel S. Elson,Stamatia Giannarou*

Main category: cs.CV

TL;DR: SAMSA 2.0 improves hyperspectral medical image segmentation by integrating spectral angle prompting with SAM, achieving higher accuracy and robustness without retraining.


<details>
  <summary>Details</summary>
Motivation: Enhance segmentation accuracy and robustness in hyperspectral medical imaging by leveraging spectral similarity alongside spatial cues.

Method: Introduces spectral angle prompting to guide SAM, fusing spectral information early for better performance.

Result: Achieves up to +3.8% higher Dice scores than RGB-only models and +3.1% over prior spectral fusion methods, with improved few-shot and zero-shot performance.

Conclusion: SAMSA 2.0 demonstrates strong generalization in low-data and noisy clinical scenarios, advancing hyperspectral segmentation.

Abstract: We present SAMSA 2.0, an interactive segmentation framework for hyperspectral
medical imaging that introduces spectral angle prompting to guide the Segment
Anything Model (SAM) using spectral similarity alongside spatial cues. This
early fusion of spectral information enables more accurate and robust
segmentation across diverse spectral datasets. Without retraining, SAMSA 2.0
achieves up to +3.8% higher Dice scores compared to RGB-only models and up to
+3.1% over prior spectral fusion methods. Our approach enhances few-shot and
zero-shot performance, demonstrating strong generalization in challenging
low-data and noisy scenarios common in clinical imaging.

</details>


### [103] [LesiOnTime -- Joint Temporal and Clinical Modeling for Small Breast Lesion Segmentation in Longitudinal DCE-MRI](https://arxiv.org/abs/2508.00496)
*Mohammed Kamran,Maria Bernathova,Raoul Varga,Christian Singer,Zsuzsanna Bago-Horvath,Thomas Helbich,Georg Langs,Philipp Seebck*

Main category: cs.CV

TL;DR: LesiOnTime improves small lesion segmentation in breast DCE-MRI by integrating longitudinal imaging and BI-RADS scores, outperforming baselines by 5% Dice.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning methods for lesion segmentation focus on large lesions and ignore longitudinal and clinical context, which is critical for early cancer detection.

Method: Proposes LesiOnTime with Temporal Prior Attention (TPA) for dynamic integration of past and current scans and BI-RADS Consistency Regularization (BCR) loss for latent space alignment.

Result: Outperforms state-of-the-art methods by 5% Dice on a longitudinal dataset; TPA and BCR provide complementary gains.

Conclusion: Incorporating temporal and clinical context enhances early lesion segmentation in breast cancer screening.

Abstract: Accurate segmentation of small lesions in Breast Dynamic Contrast-Enhanced
MRI (DCE-MRI) is critical for early cancer detection, especially in high-risk
patients. While recent deep learning methods have advanced lesion segmentation,
they primarily target large lesions and neglect valuable longitudinal and
clinical information routinely used by radiologists. In real-world screening,
detecting subtle or emerging lesions requires radiologists to compare across
timepoints and consider previous radiology assessments, such as the BI-RADS
score. We propose LesiOnTime, a novel 3D segmentation approach that mimics
clinical diagnostic workflows by jointly leveraging longitudinal imaging and
BIRADS scores. The key components are: (1) a Temporal Prior Attention (TPA)
block that dynamically integrates information from previous and current scans;
and (2) a BI-RADS Consistency Regularization (BCR) loss that enforces latent
space alignment for scans with similar radiological assessments, thus embedding
domain knowledge into the training process. Evaluated on a curated in-house
longitudinal dataset of high-risk patients with DCE-MRI, our approach
outperforms state-of-the-art single-timepoint and longitudinal baselines by 5%
in terms of Dice. Ablation studies demonstrate that both TPA and BCR contribute
complementary performance gains. These results highlight the importance of
incorporating temporal and clinical context for reliable early lesion
segmentation in real-world breast cancer screening. Our code is publicly
available at https://github.com/cirmuw/LesiOnTime

</details>


### [104] [Leveraging Convolutional and Graph Networks for an Unsupervised Remote Sensing Labelling Tool](https://arxiv.org/abs/2508.00506)
*Tulsi Patel,Mark W. Jones,Thomas Redfern*

Main category: cs.CV

TL;DR: An unsupervised pipeline for labeling similar geographical areas in Sentinel-2 imagery using segmentation with convolutional and graph neural networks, improving robustness and granularity.


<details>
  <summary>Details</summary>
Motivation: Labeling remote sensing imagery is costly and time-consuming, requiring expert input. Existing tools depend on pre-labeled data, limiting flexibility.

Method: Uses segmentation with convolutional and graph neural networks to group pixels by color and spatial similarity, encoding robust features and local neighborhood information.

Result: Reduces labeling outliers, enables granular labeling, and forms rotationally invariant semantic relationships in the encoding space.

Conclusion: The approach overcomes limitations of prior methods, offering a more efficient and flexible solution for remote sensing image labeling.

Abstract: Machine learning for remote sensing imaging relies on up-to-date and accurate
labels for model training and testing. Labelling remote sensing imagery is time
and cost intensive, requiring expert analysis. Previous labelling tools rely on
pre-labelled data for training in order to label new unseen data. In this work,
we define an unsupervised pipeline for finding and labelling geographical areas
of similar context and content within Sentinel-2 satellite imagery. Our
approach removes limitations of previous methods by utilising segmentation with
convolutional and graph neural networks to encode a more robust feature space
for image comparison. Unlike previous approaches we segment the image into
homogeneous regions of pixels that are grouped based on colour and spatial
similarity. Graph neural networks are used to aggregate information about the
surrounding segments enabling the feature representation to encode the local
neighbourhood whilst preserving its own local information. This reduces
outliers in the labelling tool, allows users to label at a granular level, and
allows a rotationally invariant semantic relationship at the image level to be
formed within the encoding space.

</details>


### [105] [Context-based Motion Retrieval using Open Vocabulary Methods for Autonomous Driving](https://arxiv.org/abs/2508.00589)
*Stefan Englmeier,Max A. Bttner,Katharina Winter,Fabian B. Flohr*

Main category: cs.CV

TL;DR: A novel context-aware motion retrieval framework for identifying rare human behaviors in autonomous driving datasets, improving retrieval accuracy by 27.5%.


<details>
  <summary>Details</summary>
Motivation: To enhance the evaluation and generalization of autonomous driving systems by addressing the challenge of retrieving rare human behaviors in large datasets.

Method: Combines SMPL-based motion sequences and video frames into a shared multimodal embedding space aligned with natural language for scalable retrieval via text queries. Introduces the WayMoCo dataset with labeled motion and scene context descriptions.

Result: Outperforms state-of-the-art models by up to 27.5% in motion-context retrieval accuracy on the WayMoCo dataset.

Conclusion: The proposed framework effectively retrieves diverse human behaviors, supporting robust evaluation of autonomous driving systems in safety-critical scenarios.

Abstract: Autonomous driving systems must operate reliably in safety-critical
scenarios, particularly those involving unusual or complex behavior by
Vulnerable Road Users (VRUs). Identifying these edge cases in driving datasets
is essential for robust evaluation and generalization, but retrieving such rare
human behavior scenarios within the long tail of large-scale datasets is
challenging. To support targeted evaluation of autonomous driving systems in
diverse, human-centered scenarios, we propose a novel context-aware motion
retrieval framework. Our method combines Skinned Multi-Person Linear
(SMPL)-based motion sequences and corresponding video frames before encoding
them into a shared multimodal embedding space aligned with natural language.
Our approach enables the scalable retrieval of human behavior and their context
through text queries. This work also introduces our dataset WayMoCo, an
extension of the Waymo Open Dataset. It contains automatically labeled motion
and scene context descriptions derived from generated pseudo-ground-truth SMPL
sequences and corresponding image data. Our approach outperforms
state-of-the-art models by up to 27.5% accuracy in motion-context retrieval,
when evaluated on the WayMoCo dataset.

</details>


### [106] [EPANet: Efficient Path Aggregation Network for Underwater Fish Detection](https://arxiv.org/abs/2508.00528)
*Jinsong Yang,Zeyuan Hu,Yichen Li*

Main category: cs.CV

TL;DR: EPANet, an efficient path aggregation network, improves underwater fish detection by combining complementary features with lightweight design, outperforming existing methods in accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: Underwater fish detection faces challenges like low resolution, background interference, and target similarity, with current methods being inefficient or overly complex.

Method: EPANet uses an efficient path aggregation feature pyramid network (EPA-FPN) for semantic-spatial complementarity and a multi-scale diverse-division short path bottleneck (MS-DDSP) for enhanced feature diversity.

Result: EPANet achieves superior detection accuracy and speed on benchmark datasets while maintaining low parameter complexity.

Conclusion: EPANet offers an efficient and accurate solution for underwater fish detection, balancing performance and simplicity.

Abstract: Underwater fish detection (UFD) remains a challenging task in computer vision
due to low object resolution, significant background interference, and high
visual similarity between targets and surroundings. Existing approaches
primarily focus on local feature enhancement or incorporate complex attention
mechanisms to highlight small objects, often at the cost of increased model
complexity and reduced efficiency. To address these limitations, we propose an
efficient path aggregation network (EPANet), which leverages complementary
feature integration to achieve accurate and lightweight UFD. EPANet consists of
two key components: an efficient path aggregation feature pyramid network
(EPA-FPN) and a multi-scale diverse-division short path bottleneck (MS-DDSP
bottleneck). The EPA-FPN introduces long-range skip connections across
disparate scales to improve semantic-spatial complementarity, while cross-layer
fusion paths are adopted to enhance feature integration efficiency. The MS-DDSP
bottleneck extends the conventional bottleneck structure by introducing
finer-grained feature division and diverse convolutional operations, thereby
increasing local feature diversity and representation capacity. Extensive
experiments on benchmark UFD datasets demonstrate that EPANet outperforms
state-of-the-art methods in terms of detection accuracy and inference speed,
while maintaining comparable or even lower parameter complexity.

</details>


### [107] [Video Color Grading via Look-Up Table Generation](https://arxiv.org/abs/2508.00548)
*Seunghyun Shin,Dongmin Shin,Jisu Shin,Hae-Gon Jeon,Joon-Young Lee*

Main category: cs.CV

TL;DR: A reference-based video color grading framework using a diffusion model to generate LUTs for artistic color adjustments, preserving structural details and enabling fast inference.


<details>
  <summary>Details</summary>
Motivation: Professional color grading is complex and skill-intensive; the paper aims to democratize it by automating the process while preserving artistic intent.

Method: Uses a diffusion model to generate LUTs for aligning color attributes between reference scenes and input video, incorporating user preferences via text prompts.

Result: Effective video color grading with preserved structural details, fast inference, and user preference integration, validated by user studies.

Conclusion: The framework successfully automates video color grading, making it accessible while maintaining artistic quality and efficiency.

Abstract: Different from color correction and transfer, color grading involves
adjusting colors for artistic or storytelling purposes in a video, which is
used to establish a specific look or mood. However, due to the complexity of
the process and the need for specialized editing skills, video color grading
remains primarily the domain of professional colorists. In this paper, we
present a reference-based video color grading framework. Our key idea is
explicitly generating a look-up table (LUT) for color attribute alignment
between reference scenes and input video via a diffusion model. As a training
objective, we enforce that high-level features of the reference scenes like
look, mood, and emotion should be similar to that of the input video. Our
LUT-based approach allows for color grading without any loss of structural
details in the whole video frames as well as achieving fast inference. We
further build a pipeline to incorporate a user-preference via text prompts for
low-level feature enhancement such as contrast and brightness, etc.
Experimental results, including extensive user studies, demonstrate the
effectiveness of our approach for video color grading. Codes are publicly
available at https://github.com/seunghyuns98/VideoColorGrading.

</details>


### [108] [Your other Left! Vision-Language Models Fail to Identify Relative Positions in Medical Images](https://arxiv.org/abs/2508.00549)
*Daniel Wolf,Heiko Hillenhagen,Billurvan Taskin,Alex Buerle,Meinrad Beer,Michael Gtz,Timo Ropinski*

Main category: cs.CV

TL;DR: VLMs struggle with determining relative positions in medical images, despite moderate improvements from visual prompts. A new benchmark, MIRP, is introduced to evaluate this capability.


<details>
  <summary>Details</summary>
Motivation: Understanding relative positions of anatomical structures is crucial for clinical decision-making, but VLMs lack this capability, hindering their clinical application.

Method: Evaluated state-of-the-art VLMs (GPT-4o, Llama3.2, Pixtral, JanusPro) and tested visual prompts (alphanumeric/colored markers) for performance enhancement.

Result: VLMs performed poorly on medical images, relying more on prior knowledge than image content. Visual prompts improved results but not sufficiently.

Conclusion: VLMs need significant improvement for clinical use. The MIRP benchmark is introduced to advance research in this area.

Abstract: Clinical decision-making relies heavily on understanding relative positions
of anatomical structures and anomalies. Therefore, for Vision-Language Models
(VLMs) to be applicable in clinical practice, the ability to accurately
determine relative positions on medical images is a fundamental prerequisite.
Despite its importance, this capability remains highly underexplored. To
address this gap, we evaluate the ability of state-of-the-art VLMs, GPT-4o,
Llama3.2, Pixtral, and JanusPro, and find that all models fail at this
fundamental task. Inspired by successful approaches in computer vision, we
investigate whether visual prompts, such as alphanumeric or colored markers
placed on anatomical structures, can enhance performance. While these markers
provide moderate improvements, results remain significantly lower on medical
images compared to observations made on natural images. Our evaluations suggest
that, in medical imaging, VLMs rely more on prior anatomical knowledge than on
actual image content for answering relative position questions, often leading
to incorrect conclusions. To facilitate further research in this area, we
introduce the MIRP , Medical Imaging Relative Positioning, benchmark dataset,
designed to systematically evaluate the capability to identify relative
positions in medical images.

</details>


### [109] [DBLP: Noise Bridge Consistency Distillation For Efficient And Reliable Adversarial Purification](https://arxiv.org/abs/2508.00552)
*Chihan Huang,Belal Alsinglawi,Islam Al-qudah*

Main category: cs.CV

TL;DR: DBLP introduces an efficient diffusion-based adversarial purification method using noise bridge distillation and adaptive semantic enhancement, achieving SOTA results with fast inference.


<details>
  <summary>Details</summary>
Motivation: Addressing the vulnerability of DNNs to adversarial perturbations and the inefficiency of existing diffusion-based purification methods.

Method: Proposes Diffusion Bridge Distillation for Purification (DBLP) with noise bridge distillation and adaptive semantic enhancement using multi-scale pyramid edge maps.

Result: Achieves state-of-the-art robust accuracy, superior image quality, and ~0.2s inference time.

Conclusion: DBLP marks a significant advancement toward real-time adversarial purification with high efficiency and effectiveness.

Abstract: Recent advances in deep neural networks (DNNs) have led to remarkable success
across a wide range of tasks. However, their susceptibility to adversarial
perturbations remains a critical vulnerability. Existing diffusion-based
adversarial purification methods often require intensive iterative denoising,
severely limiting their practical deployment. In this paper, we propose
Diffusion Bridge Distillation for Purification (DBLP), a novel and efficient
diffusion-based framework for adversarial purification. Central to our approach
is a new objective, noise bridge distillation, which constructs a principled
alignment between the adversarial noise distribution and the clean data
distribution within a latent consistency model (LCM). To further enhance
semantic fidelity, we introduce adaptive semantic enhancement, which fuses
multi-scale pyramid edge maps as conditioning input to guide the purification
process. Extensive experiments across multiple datasets demonstrate that DBLP
achieves state-of-the-art (SOTA) robust accuracy, superior image quality, and
around 0.2s inference time, marking a significant step toward real-time
adversarial purification.

</details>


### [110] [HiPrune: Training-Free Visual Token Pruning via Hierarchical Attention in Vision-Language Models](https://arxiv.org/abs/2508.00553)
*Jizhihui Liu,Feiyi Du,Guangdao Zhu,Niu Lian,Jun Li,Bin Chen*

Main category: cs.CV

TL;DR: HiPrune is a training-free, model-agnostic token pruning framework for VLMs that leverages hierarchical attention to retain key tokens, achieving high efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: VLMs suffer from computational inefficiency due to lengthy visual token sequences. Existing methods are not scalable or require task-specific training.

Method: HiPrune identifies three token types (Anchor, Buffer, Register) based on hierarchical attention in vision encoders, pruning non-informative tokens without retraining.

Result: HiPrune preserves 99.3% accuracy with 33.3% tokens and reduces FLOPs/latency by up to 9x, outperforming prior methods.

Conclusion: HiPrune offers a scalable, efficient solution for token pruning in VLMs, generalizing well across models and tasks.

Abstract: Vision-Language Models (VLMs) encode images into lengthy sequences of visual
tokens, leading to excessive computational overhead and limited inference
efficiency. While prior efforts prune or merge tokens to address this issue,
they often rely on special tokens (e.g., CLS) or require task-specific
training, hindering scalability across architectures. In this paper, we propose
HiPrune, a training-free and model-agnostic token Pruning framework that
exploits the Hierarchical attention structure within vision encoders. We
identify that middle layers attend to object-centric regions, while deep layers
capture global contextual features. Based on this observation, HiPrune selects
three types of informative tokens: (1) Anchor tokens with high attention in
object-centric layers, (2) Buffer tokens adjacent to anchors for spatial
continuity, and (3) Register tokens with strong attention in deep layers for
global summarization. Our method requires no retraining and integrates
seamlessly with any ViT-based VLM. Extensive experiments on LLaVA-1.5,
LLaVA-NeXT, and Qwen2.5-VL demonstrate that HiPrune achieves state-of-the-art
pruning performance, preserving up to 99.3% task accuracy with only 33.3%
tokens, and maintaining 99.5% accuracy with just 11.1% tokens. Meanwhile, it
reduces inference FLOPs and latency by up to 9$\times$, showcasing strong
generalization across models and tasks. Code is available at
https://github.com/Danielement321/HiPrune.

</details>


### [111] [Training-Free Class Purification for Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2508.00557)
*Qi Chen,Lingxiao Yang,Yun Chen,Nailong Zhao,Jianhuang Lai,Jie Shao,Xiaohua Xie*

Main category: cs.CV

TL;DR: FreeCP is a training-free class purification framework for open-vocabulary semantic segmentation, addressing class redundancy and visual-language ambiguity to improve segmentation performance.


<details>
  <summary>Details</summary>
Motivation: Existing training-free methods neglect challenges like class redundancy and visual-language ambiguity, leading to suboptimal segmentation.

Method: FreeCP purifies semantic categories and rectifies errors from redundancy and ambiguity, leveraging purified representations for segmentation.

Result: Extensive experiments on eight benchmarks show FreeCP significantly enhances segmentation performance when combined with other methods.

Conclusion: FreeCP effectively addresses key challenges in training-free OVSS, improving segmentation accuracy as a plug-and-play module.

Abstract: Fine-tuning pre-trained vision-language models has emerged as a powerful
approach for enhancing open-vocabulary semantic segmentation (OVSS). However,
the substantial computational and resource demands associated with training on
large datasets have prompted interest in training-free methods for OVSS.
Existing training-free approaches primarily focus on modifying model
architectures and generating prototypes to improve segmentation performance.
However, they often neglect the challenges posed by class redundancy, where
multiple categories are not present in the current test image, and
visual-language ambiguity, where semantic similarities among categories create
confusion in class activation. These issues can lead to suboptimal class
activation maps and affinity-refined activation maps. Motivated by these
observations, we propose FreeCP, a novel training-free class purification
framework designed to address these challenges. FreeCP focuses on purifying
semantic categories and rectifying errors caused by redundancy and ambiguity.
The purified class representations are then leveraged to produce final
segmentation predictions. We conduct extensive experiments across eight
benchmarks to validate FreeCP's effectiveness. Results demonstrate that FreeCP,
as a plug-and-play module, significantly boosts segmentation performance when
combined with other OVSS methods.

</details>


### [112] [Guiding Diffusion-Based Articulated Object Generation by Partial Point Cloud Alignment and Physical Plausibility Constraints](https://arxiv.org/abs/2508.00558)
*Jens U. Kreber,Joerg Stueckler*

Main category: cs.CV

TL;DR: PhysNAP is a diffusion model-based method for generating articulated objects, improving physical plausibility and alignment with partial point clouds using SDFs and constraints.


<details>
  <summary>Details</summary>
Motivation: Articulated objects are common in everyday environments, but generating them with physical plausibility and alignment to partial point clouds is challenging.

Method: Uses a diffusion model with SDFs for part shapes, guided by point cloud alignment loss, non-penetration, and mobility constraints. Category-aware diffusion improves alignment.

Result: Evaluated on PartNet-Mobility, PhysNAP improves constraint consistency and offers a tradeoff with generative ability compared to an unguided baseline.

Conclusion: PhysNAP effectively generates physically plausible articulated objects with better alignment and constraint consistency.

Abstract: Articulated objects are an important type of interactable objects in everyday
environments. In this paper, we propose PhysNAP, a novel diffusion model-based
approach for generating articulated objects that aligns them with partial point
clouds and improves their physical plausibility. The model represents part
shapes by signed distance functions (SDFs). We guide the reverse diffusion
process using a point cloud alignment loss computed using the predicted SDFs.
Additionally, we impose non-penetration and mobility constraints based on the
part SDFs for guiding the model to generate more physically plausible objects.
We also make our diffusion approach category-aware to further improve point
cloud alignment if category information is available. We evaluate the
generative ability and constraint consistency of samples generated with PhysNAP
using the PartNet-Mobility dataset. We also compare it with an unguided
baseline diffusion model and demonstrate that PhysNAP can improve constraint
consistency and provides a tradeoff with generative ability.

</details>


### [113] [Weakly Supervised Virus Capsid Detection with Image-Level Annotations in Electron Microscopy Images](https://arxiv.org/abs/2508.00563)
*Hannah Kniesel,Leon Sick,Tristan Payer,Tim Bergner,Kavitha Shaga Devan,Clarissa Read,Paul Walther,Timo Ropinski*

Main category: cs.CV

TL;DR: A weakly supervised object detection method using image-level annotations to avoid costly bounding box labels, outperforming other weak labeling methods.


<details>
  <summary>Details</summary>
Motivation: Reducing the need for expensive, expert-annotated bounding boxes in object detection by leveraging easier-to-acquire image-level labels.

Method: Uses a pre-trained model to generate pseudo-labels via optimization with a shrinking receptive field, avoiding specialized architectures.

Result: Pseudo-labels outperform other weak labeling methods and even ground truth labels when annotation time is limited.

Conclusion: The proposed method efficiently reduces annotation costs while maintaining or improving detection performance.

Abstract: Current state-of-the-art methods for object detection rely on annotated
bounding boxes of large data sets for training. However, obtaining such
annotations is expensive and can require up to hundreds of hours of manual
labor. This poses a challenge, especially since such annotations can only be
provided by experts, as they require knowledge about the scientific domain. To
tackle this challenge, we propose a domain-specific weakly supervised object
detection algorithm that only relies on image-level annotations, which are
significantly easier to acquire. Our method distills the knowledge of a
pre-trained model, on the task of predicting the presence or absence of a virus
in an image, to obtain a set of pseudo-labels that can be used to later train a
state-of-the-art object detection model. To do so, we use an optimization
approach with a shrinking receptive field to extract virus particles directly
without specific network architectures. Through a set of extensive studies, we
show how the proposed pseudo-labels are easier to obtain, and, more
importantly, are able to outperform other existing weak labeling methods, and
even ground truth labels, in cases where the time to obtain the annotation is
limited.

</details>


### [114] [CoProU-VO: Combining Projected Uncertainty for End-to-End Unsupervised Monocular Visual Odometry](https://arxiv.org/abs/2508.00568)
*Jingchao Xie,Oussema Dhaouadi,Weirong Chen,Johannes Meier,Jacques Kaiser,Daniel Cremers*

Main category: cs.CV

TL;DR: CoProU-VO improves unsupervised Visual Odometry by combining uncertainty across frames to handle dynamic scenes, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Dynamic objects and occlusions in scenes violate static assumptions, causing errors in pose estimation. Traditional uncertainty modeling ignores temporal frame uncertainties.

Method: Introduces CoProU-VO, an end-to-end approach combining target and reference frame uncertainties using probabilistic formulation, built on vision transformers.

Result: Outperforms previous methods on KITTI and nuScenes datasets, especially in challenging highway scenes. Ablation studies confirm cross-frame uncertainty propagation's effectiveness.

Conclusion: CoProU-VO successfully addresses dynamic scene challenges by propagating uncertainty across frames, enhancing robustness in unsupervised VO.

Abstract: Visual Odometry (VO) is fundamental to autonomous navigation, robotics, and
augmented reality, with unsupervised approaches eliminating the need for
expensive ground-truth labels. However, these methods struggle when dynamic
objects violate the static scene assumption, leading to erroneous pose
estimations. We tackle this problem by uncertainty modeling, which is a
commonly used technique that creates robust masks to filter out dynamic objects
and occlusions without requiring explicit motion segmentation. Traditional
uncertainty modeling considers only single-frame information, overlooking the
uncertainties across consecutive frames. Our key insight is that uncertainty
must be propagated and combined across temporal frames to effectively identify
unreliable regions, particularly in dynamic scenes. To address this challenge,
we introduce Combined Projected Uncertainty VO (CoProU-VO), a novel end-to-end
approach that combines target frame uncertainty with projected reference frame
uncertainty using a principled probabilistic formulation. Built upon vision
transformer backbones, our model simultaneously learns depth, uncertainty
estimation, and camera poses. Consequently, experiments on the KITTI and
nuScenes datasets demonstrate significant improvements over previous
unsupervised monocular end-to-end two-frame-based methods and exhibit strong
performance in challenging highway scenes where other approaches often fail.
Additionally, comprehensive ablation studies validate the effectiveness of
cross-frame uncertainty propagation.

</details>


### [115] [Uncertainty-Aware Likelihood Ratio Estimation for Pixel-Wise Out-of-Distribution Detection](https://arxiv.org/abs/2508.00587)
*Marc Hlle,Walter Kellermann,Vasileios Belagiannis*

Main category: cs.CV

TL;DR: The paper introduces an uncertainty-aware likelihood ratio estimation method to improve out-of-distribution detection in semantic segmentation for autonomous driving, reducing false positives while maintaining high precision.


<details>
  <summary>Details</summary>
Motivation: Semantic segmentation models often misclassify unknown objects in real-world scenarios, and existing methods struggle with rare classes.

Method: The approach uses an evidential classifier within a likelihood ratio test to distinguish known and unknown pixel features, accounting for uncertainty via probability distributions.

Result: The method achieves a 2.5% false positive rate and 90.91% precision on five benchmark datasets with minimal computational overhead.

Conclusion: Incorporating uncertainty improves outlier exposure effectiveness, making the method robust for real-world applications.

Abstract: Semantic segmentation models trained on known object classes often fail in
real-world autonomous driving scenarios by confidently misclassifying unknown
objects. While pixel-wise out-of-distribution detection can identify unknown
objects, existing methods struggle in complex scenes where rare object classes
are often confused with truly unknown objects. We introduce an
uncertainty-aware likelihood ratio estimation method that addresses these
limitations. Our approach uses an evidential classifier within a likelihood
ratio test to distinguish between known and unknown pixel features from a
semantic segmentation model, while explicitly accounting for uncertainty.
Instead of producing point estimates, our method outputs probability
distributions that capture uncertainty from both rare training examples and
imperfect synthetic outliers. We show that by incorporating uncertainty in this
way, outlier exposure can be leveraged more effectively. Evaluated on five
standard benchmark datasets, our method achieves the lowest average false
positive rate (2.5%) among state-of-the-art while maintaining high average
precision (90.91%) and incurring only negligible computational overhead. Code
is available at https://github.com/glasbruch/ULRE.

</details>


### [116] [A Novel Modeling Framework and Data Product for Extended VIIRS-like Artificial Nighttime Light Image Reconstruction (1986-2024)](https://arxiv.org/abs/2508.00590)
*Yihe Tian,Kwan Man Cheng,Zhengbo Zhang,Tao Zhang,Suju Li,Dongmei Yan,Bing Xu*

Main category: cs.CV

TL;DR: The paper introduces EVAL, a framework to reconstruct extended VIIRS-like artificial nighttime light data for China, addressing underestimation and structural omission issues in current methods.


<details>
  <summary>Details</summary>
Motivation: Existing NTL data lacks long-term coverage and suffers from intensity underestimation and structural omission, limiting historical analysis.

Method: A two-stage framework: construction (Hierarchical Fusion Decoder) and refinement (Dual Feature Refiner using impervious surface masks).

Result: EVAL extends NTL data back to 1986, outperforming existing products (R 0.80 vs. 0.68, RMSE 0.99 vs. 1.27).

Conclusion: EVAL is reliable for long-term analysis and publicly available, offering a valuable resource for research.

Abstract: Artificial Night-Time Light (NTL) remote sensing is a vital proxy for
quantifying the intensity and spatial distribution of human activities.
Although the NPP-VIIRS sensor provides high-quality NTL observations, its
temporal coverage, which begins in 2012, restricts long-term time-series
studies that extend to earlier periods. Despite the progress in extending
VIIRS-like NTL time-series, current methods still suffer from two significant
shortcomings: the underestimation of light intensity and the structural
omission. To overcome these limitations, we propose a novel reconstruction
framework consisting of a two-stage process: construction and refinement. The
construction stage features a Hierarchical Fusion Decoder (HFD) designed to
enhance the fidelity of the initial reconstruction. The refinement stage
employs a Dual Feature Refiner (DFR), which leverages high-resolution
impervious surface masks to guide and enhance fine-grained structural details.
Based on this framework, we developed the Extended VIIRS-like Artificial
Nighttime Light (EVAL) product for China, extending the standard data record
backwards by 26 years to begin in 1986. Quantitative evaluation shows that EVAL
significantly outperforms existing state-of-the-art products, boosting the
$\text{R}^2$ from 0.68 to 0.80 while lowering the RMSE from 1.27 to 0.99.
Furthermore, EVAL exhibits excellent temporal consistency and maintains a high
correlation with socioeconomic parameters, confirming its reliability for
long-term analysis. The resulting EVAL dataset provides a valuable new resource
for the research community and is publicly available at
https://doi.org/10.11888/HumanNat.tpdc.302930.

</details>


### [117] [Wukong Framework for Not Safe For Work Detection in Text-to-Image systems](https://arxiv.org/abs/2508.00591)
*Mingrui Liu,Sixiao Zhang,Cheng Long*

Main category: cs.CV

TL;DR: Wukong is a transformer-based NSFW detection framework for T2I generation, leveraging early denoising steps and pre-trained U-Net cross-attention for efficient and accurate detection.


<details>
  <summary>Details</summary>
Motivation: Existing NSFW safeguards (text or image filters) are either inefficient or prone to attacks, necessitating a better solution.

Method: Wukong uses intermediate outputs from early denoising steps and reuses U-Net's cross-attention parameters for early detection.

Result: Wukong outperforms text-based safeguards and matches image filters' accuracy while being more efficient.

Conclusion: Wukong provides an effective and efficient solution for NSFW detection in T2I generation.

Abstract: Text-to-Image (T2I) generation is a popular AI-generated content (AIGC)
technology enabling diverse and creative image synthesis. However, some outputs
may contain Not Safe For Work (NSFW) content (e.g., violence), violating
community guidelines. Detecting NSFW content efficiently and accurately, known
as external safeguarding, is essential. Existing external safeguards fall into
two types: text filters, which analyze user prompts but overlook T2I
model-specific variations and are prone to adversarial attacks; and image
filters, which analyze final generated images but are computationally costly
and introduce latency. Diffusion models, the foundation of modern T2I systems
like Stable Diffusion, generate images through iterative denoising using a
U-Net architecture with ResNet and Transformer blocks. We observe that: (1)
early denoising steps define the semantic layout of the image, and (2)
cross-attention layers in U-Net are crucial for aligning text and image
regions. Based on these insights, we propose Wukong, a transformer-based NSFW
detection framework that leverages intermediate outputs from early denoising
steps and reuses U-Net's pre-trained cross-attention parameters. Wukong
operates within the diffusion process, enabling early detection without waiting
for full image generation. We also introduce a new dataset containing prompts,
seeds, and image-specific NSFW labels, and evaluate Wukong on this and two
public benchmarks. Results show that Wukong significantly outperforms
text-based safeguards and achieves comparable accuracy of image filters, while
offering much greater efficiency.

</details>


### [118] [GeoMoE: Divide-and-Conquer Motion Field Modeling with Mixture-of-Experts for Two-View Geometry](https://arxiv.org/abs/2508.00592)
*Jiajun Le,Jiayi Ma*

Main category: cs.CV

TL;DR: GeoMoE introduces a Mixture-of-Experts (MoE) framework for two-view geometry, addressing heterogeneous motion patterns in complex scenes with a divide-and-conquer strategy.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to handle diverse motion patterns in complex scenes, leading to inaccurate motion field estimates.

Method: GeoMoE uses a Probabilistic Prior-Guided Decomposition and MoE-Enhanced Bi-Path Rectifier to model sub-fields and assign experts for targeted motion rectification.

Result: GeoMoE outperforms state-of-the-art methods in relative pose and homography estimation with strong generalization.

Conclusion: GeoMoE provides a streamlined, effective solution for motion field modeling in two-view geometry.

Abstract: Recent progress in two-view geometry increasingly emphasizes enforcing
smoothness and global consistency priors when estimating motion fields between
pairs of images. However, in complex real-world scenes, characterized by
extreme viewpoint and scale changes as well as pronounced depth
discontinuities, the motion field often exhibits diverse and heterogeneous
motion patterns. Most existing methods lack targeted modeling strategies and
fail to explicitly account for this variability, resulting in estimated motion
fields that diverge from their true underlying structure and distribution. We
observe that Mixture-of-Experts (MoE) can assign dedicated experts to motion
sub-fields, enabling a divide-and-conquer strategy for heterogeneous motion
patterns. Building on this insight, we re-architect motion field modeling in
two-view geometry with GeoMoE, a streamlined framework. Specifically, we first
devise a Probabilistic Prior-Guided Decomposition strategy that exploits inlier
probability signals to perform a structure-aware decomposition of the motion
field into heterogeneous sub-fields, sharply curbing outlier-induced bias.
Next, we introduce an MoE-Enhanced Bi-Path Rectifier that enhances each
sub-field along spatial-context and channel-semantic paths and routes it to a
customized expert for targeted modeling, thereby decoupling heterogeneous
motion regimes, suppressing cross-sub-field interference and representational
entanglement, and yielding fine-grained motion-field rectification. With this
minimalist design, GeoMoE outperforms prior state-of-the-art methods in
relative pose and homography estimation and shows strong generalization. The
source code and pre-trained models are available at
https://github.com/JiajunLe/GeoMoE.

</details>


### [119] [DPoser-X: Diffusion Model as Robust 3D Whole-body Human Pose Prior](https://arxiv.org/abs/2508.00599)
*Junzhe Lu,Jing Lin,Hongkun Dou,Ailing Zeng,Yue Deng,Xian Liu,Zhongang Cai,Lei Yang,Yulun Zhang,Haoqian Wang,Ziwei Liu*

Main category: cs.CV

TL;DR: DPoser-X is a diffusion-based prior model for 3D whole-body human poses, addressing challenges like pose complexity and dataset scarcity. It unifies pose tasks as inverse problems, uses variational diffusion sampling, and introduces novel training methods for robustness.


<details>
  <summary>Details</summary>
Motivation: Building a versatile and robust full-body human pose prior is challenging due to articulated pose complexity and limited high-quality datasets.

Method: Introduces DPoser (Diffusion model as body Pose prior) and extends it to DPoser-X. Uses variational diffusion sampling, truncated timestep scheduling, and masked training to combine whole-body and part-specific datasets.

Result: DPoser-X outperforms state-of-the-art models in body, hand, face, and full-body pose benchmarks, demonstrating robustness and versatility.

Conclusion: DPoser-X sets a new benchmark for whole-body human pose prior modeling, offering superior performance and adaptability.

Abstract: We present DPoser-X, a diffusion-based prior model for 3D whole-body human
poses. Building a versatile and robust full-body human pose prior remains
challenging due to the inherent complexity of articulated human poses and the
scarcity of high-quality whole-body pose datasets. To address these
limitations, we introduce a Diffusion model as body Pose prior (DPoser) and
extend it to DPoser-X for expressive whole-body human pose modeling. Our
approach unifies various pose-centric tasks as inverse problems, solving them
through variational diffusion sampling. To enhance performance on downstream
applications, we introduce a novel truncated timestep scheduling method
specifically designed for pose data characteristics. We also propose a masked
training mechanism that effectively combines whole-body and part-specific
datasets, enabling our model to capture interdependencies between body parts
while avoiding overfitting to specific actions. Extensive experiments
demonstrate DPoser-X's robustness and versatility across multiple benchmarks
for body, hand, face, and full-body pose modeling. Our model consistently
outperforms state-of-the-art alternatives, establishing a new benchmark for
whole-body human pose prior modeling.

</details>


### [120] [Backdoor Attacks on Deep Learning Face Detection](https://arxiv.org/abs/2508.00620)
*Quentin Le Roux,Yannick Teglia,Teddy Furon,Philippe Loubet-Moundi*

Main category: cs.CV

TL;DR: The paper explores vulnerabilities in face detection systems, introducing Face Generation Attacks and a novel Landmark Shift Attack, and proposes mitigations.


<details>
  <summary>Details</summary>
Motivation: Face recognition systems in unconstrained environments face challenges like inconsistent lighting and poses, requiring robust face detection. This paper investigates vulnerabilities in such systems.

Method: The study introduces Face Generation Attacks and a Landmark Shift Attack, targeting bounding box and landmark coordinate regression in face detectors.

Result: The attacks successfully backdoor the coordinate regression task, demonstrating vulnerabilities in face detection systems.

Conclusion: The paper highlights security risks in face detection and offers mitigations to address these vulnerabilities.

Abstract: Face Recognition Systems that operate in unconstrained environments capture
images under varying conditions,such as inconsistent lighting, or diverse face
poses. These challenges require including a Face Detection module that
regresses bounding boxes and landmark coordinates for proper Face Alignment.
This paper shows the effectiveness of Object Generation Attacks on Face
Detection, dubbed Face Generation Attacks, and demonstrates for the first time
a Landmark Shift Attack that backdoors the coordinate regression task performed
by face detectors. We then offer mitigations against these vulnerabilities.

</details>


### [121] [Minimum Data, Maximum Impact: 20 annotated samples for explainable lung nodule classification](https://arxiv.org/abs/2508.00639)
*Luisa Galle,Catharina Silvia Lisson,Christoph Gerhard Lisson,Daniela Drees,Felix Weig,Daniel Vogele,Meinrad Beer,Michael Gtz*

Main category: cs.CV

TL;DR: The paper proposes using a generative Diffusion Model to synthesize attribute-annotated medical images, improving explainable AI performance in diagnosis.


<details>
  <summary>Details</summary>
Motivation: Enhancing clinicians' trust in AI by aligning AI decision-making with radiologists' reasoning through pathology-related visual attributes.

Method: A Diffusion Model is enhanced with attribute conditioning and trained on a small dataset (20 samples) to generate synthetic attribute-annotated images.

Result: Using synthetic data boosts attribute prediction accuracy by 13.4% and target prediction accuracy by 1.8% compared to training with limited real data.

Conclusion: Synthetic data can overcome dataset limitations, improving the practicality of explainable models in medical image analysis.

Abstract: Classification models that provide human-interpretable explanations enhance
clinicians' trust and usability in medical image diagnosis. One research focus
is the integration and prediction of pathology-related visual attributes used
by radiologists alongside the diagnosis, aligning AI decision-making with
clinical reasoning. Radiologists use attributes like shape and texture as
established diagnostic criteria and mirroring these in AI decision-making both
enhances transparency and enables explicit validation of model outputs.
However, the adoption of such models is limited by the scarcity of large-scale
medical image datasets annotated with these attributes. To address this
challenge, we propose synthesizing attribute-annotated data using a generative
model. We enhance the Diffusion Model with attribute conditioning and train it
using only 20 attribute-labeled lung nodule samples from the LIDC-IDRI dataset.
Incorporating its generated images into the training of an explainable model
boosts performance, increasing attribute prediction accuracy by 13.4% and
target prediction accuracy by 1.8% compared to training with only the small
real attribute-annotated dataset. This work highlights the potential of
synthetic data to overcome dataset limitations, enhancing the applicability of
explainable models in medical image analysis.

</details>


### [122] [Revisiting Adversarial Patch Defenses on Object Detectors: Unified Evaluation, Large-Scale Dataset, and New Insights](https://arxiv.org/abs/2508.00649)
*Junhao Zheng,Jiahao Sun,Chenhao Lin,Zhengyu Zhao,Chen Ma,Chong Zhang,Cong Wang,Qian Wang,Chao Shen*

Main category: cs.CV

TL;DR: The paper introduces a unified benchmark for evaluating defenses against patch attacks on object detectors, revealing key insights and improving existing defenses by 15.09% AP@0.5.


<details>
  <summary>Details</summary>
Motivation: Existing defense evaluations lack a comprehensive framework, leading to inconsistent assessments of methods.

Method: The study revisits 11 defenses, creates a benchmark with 2 attack goals, 13 attacks, 11 detectors, and 4 metrics, and analyzes a dataset of 94,000 images.

Result: Key findings include the importance of data distribution in defense difficulty, the relevance of attacked object precision, and the robustness of complex/stochastic defenses.

Conclusion: The benchmark provides guidance for evaluating and designing patch defenses, with ongoing updates to include new attacks/defenses.

Abstract: Developing reliable defenses against patch attacks on object detectors has
attracted increasing interest. However, we identify that existing defense
evaluations lack a unified and comprehensive framework, resulting in
inconsistent and incomplete assessments of current methods. To address this
issue, we revisit 11 representative defenses and present the first patch
defense benchmark, involving 2 attack goals, 13 patch attacks, 11 object
detectors, and 4 diverse metrics. This leads to the large-scale adversarial
patch dataset with 94 types of patches and 94,000 images. Our comprehensive
analyses reveal new insights: (1) The difficulty in defending against
naturalistic patches lies in the data distribution, rather than the commonly
believed high frequencies. Our new dataset with diverse patch distributions can
be used to improve existing defenses by 15.09% AP@0.5. (2) The average
precision of the attacked object, rather than the commonly pursued patch
detection accuracy, shows high consistency with defense performance. (3)
Adaptive attacks can substantially bypass existing defenses, and defenses with
complex/stochastic models or universal patch properties are relatively robust.
We hope that our analyses will serve as guidance on properly evaluating patch
attacks/defenses and advancing their design. Code and dataset are available at
https://github.com/Gandolfczjh/APDE, where we will keep integrating new
attacks/defenses.

</details>


### [123] [Can Large Pretrained Depth Estimation Models Help With Image Dehazing?](https://arxiv.org/abs/2508.00698)
*Hongfei Zhang,Kun Zhou,Ruizheng Wu,Jiangbo Lu*

Main category: cs.CV

TL;DR: The paper explores using pretrained depth representations for image dehazing, proposing a plug-and-play RGB-D fusion module for adaptability across diverse scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing dehazing methods lack adaptability due to architecture-specific designs, despite the promise of large-scale pretrained models.

Method: Systematically investigates pretrained depth representations and introduces a plug-and-play RGB-D fusion module.

Result: Learned deep depth features remain consistent across haze levels; the module is validated across benchmarks.

Conclusion: The proposed approach is effective and broadly applicable for image dehazing.

Abstract: Image dehazing remains a challenging problem due to the spatially varying
nature of haze in real-world scenes. While existing methods have demonstrated
the promise of large-scale pretrained models for image dehazing, their
architecture-specific designs hinder adaptability across diverse scenarios with
different accuracy and efficiency requirements. In this work, we systematically
investigate the generalization capability of pretrained depth
representations-learned from millions of diverse images-for image dehazing. Our
empirical analysis reveals that the learned deep depth features maintain
remarkable consistency across varying haze levels. Building on this insight, we
propose a plug-and-play RGB-D fusion module that seamlessly integrates with
diverse dehazing architectures. Extensive experiments across multiple
benchmarks validate both the effectiveness and broad applicability of our
approach.

</details>


### [124] [D3: Training-Free AI-Generated Video Detection Using Second-Order Features](https://arxiv.org/abs/2508.00701)
*Chende Zheng,Ruiqi suo,Chenhao Lin,Zhengyu Zhao,Le Yang,Shuai Liu,Minghui Yang,Cong Wang,Chao Shen*

Main category: cs.CV

TL;DR: The paper introduces D3, a training-free method for detecting AI-generated videos by analyzing second-order temporal artifacts, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Public concern over synthetic video dissemination highlights the need for better detection methods, as current approaches lack exploration of temporal artifacts.

Method: The authors propose a theoretical framework using second-order dynamical analysis and introduce D3, which leverages second-order temporal discrepancies for detection.

Result: D3 outperforms previous methods by 10.39% mean Average Precision on Gen-Video and shows computational efficiency and robustness.

Conclusion: D3 is a superior, efficient, and robust method for detecting AI-generated videos, validated across multiple datasets.

Abstract: The evolution of video generation techniques, such as Sora, has made it
increasingly easy to produce high-fidelity AI-generated videos, raising public
concern over the dissemination of synthetic content. However, existing
detection methodologies remain limited by their insufficient exploration of
temporal artifacts in synthetic videos. To bridge this gap, we establish a
theoretical framework through second-order dynamical analysis under Newtonian
mechanics, subsequently extending the Second-order Central Difference features
tailored for temporal artifact detection. Building on this theoretical
foundation, we reveal a fundamental divergence in second-order feature
distributions between real and AI-generated videos. Concretely, we propose
Detection by Difference of Differences (D3), a novel training-free detection
method that leverages the above second-order temporal discrepancies. We
validate the superiority of our D3 on 4 open-source datasets (Gen-Video,
VideoPhy, EvalCrafter, VidProM), 40 subsets in total. For example, on GenVideo,
D3 outperforms the previous best method by 10.39% (absolute) mean Average
Precision. Additional experiments on time cost and post-processing operations
demonstrate D3's exceptional computational efficiency and strong robust
performance. Our code is available at https://github.com/Zig-HS/D3.

</details>


### [125] [MIHBench: Benchmarking and Mitigating Multi-Image Hallucinations in Multimodal Large Language Models](https://arxiv.org/abs/2508.00726)
*Jiale Li,Mingrui Wu,Zixiang Jin,Hao Chen,Jiayi Ji,Xiaoshuai Sun,Liujuan Cao,Rongrong Ji*

Main category: cs.CV

TL;DR: The paper introduces MIHBench, a benchmark for evaluating object-related hallucinations in multi-image MLLMs, identifies key factors causing hallucinations, and proposes a Dynamic Attention Balancing mechanism to mitigate them.


<details>
  <summary>Details</summary>
Motivation: Existing studies on hallucination in MLLMs focus on single-image settings, leaving multi-image scenarios unexplored. This work aims to fill that gap.

Method: The authors propose MIHBench with three tasks to evaluate hallucinations and introduce a Dynamic Attention Balancing mechanism to adjust inter-image attention.

Result: Key findings include the relationship between image inputs and hallucination likelihood, and the effectiveness of the proposed mechanism in reducing hallucinations.

Conclusion: The Dynamic Attention Balancing mechanism improves semantic integration and reasoning stability in multi-image MLLMs, addressing hallucination challenges.

Abstract: Despite growing interest in hallucination in Multimodal Large Language
Models, existing studies primarily focus on single-image settings, leaving
hallucination in multi-image scenarios largely unexplored. To address this gap,
we conduct the first systematic study of hallucinations in multi-image MLLMs
and propose MIHBench, a benchmark specifically tailored for evaluating
object-related hallucinations across multiple images. MIHBench comprises three
core tasks: Multi-Image Object Existence Hallucination, Multi-Image Object
Count Hallucination, and Object Identity Consistency Hallucination, targeting
semantic understanding across object existence, quantity reasoning, and
cross-view identity consistency. Through extensive evaluation, we identify key
factors associated with the occurrence of multi-image hallucinations,
including: a progressive relationship between the number of image inputs and
the likelihood of hallucination occurrences; a strong correlation between
single-image hallucination tendencies and those observed in multi-image
contexts; and the influence of same-object image ratios and the positional
placement of negative samples within image sequences on the occurrence of
object identity consistency hallucination. To address these challenges, we
propose a Dynamic Attention Balancing mechanism that adjusts inter-image
attention distributions while preserving the overall visual attention
proportion. Experiments across multiple state-of-the-art MLLMs demonstrate that
our method effectively reduces hallucination occurrences and enhances semantic
integration and reasoning stability in multi-image scenarios.

</details>


### [126] [YOLO-Count: Differentiable Object Counting for Text-to-Image Generation](https://arxiv.org/abs/2508.00728)
*Guanning Zeng,Xiang Zhang,Zirui Wang,Haiyang Xu,Zeyuan Chen,Bingnan Li,Zhuowen Tu*

Main category: cs.CV

TL;DR: YOLO-Count is a differentiable model for open-vocabulary object counting and precise quantity control in text-to-image generation, using a novel 'cardinality' map and hybrid supervision.


<details>
  <summary>Details</summary>
Motivation: To address challenges in general object counting and enable accurate quantity control in text-to-image generation.

Method: Introduces a 'cardinality' map for regression, uses representation alignment, and employs a hybrid strong-weak supervision scheme.

Result: Achieves state-of-the-art counting accuracy and robust quantity control for T2I systems.

Conclusion: YOLO-Count effectively bridges open-vocabulary counting and generative model control, demonstrating superior performance.

Abstract: We propose YOLO-Count, a differentiable open-vocabulary object counting model
that tackles both general counting challenges and enables precise quantity
control for text-to-image (T2I) generation. A core contribution is the
'cardinality' map, a novel regression target that accounts for variations in
object size and spatial distribution. Leveraging representation alignment and a
hybrid strong-weak supervision scheme, YOLO-Count bridges the gap between
open-vocabulary counting and T2I generation control. Its fully differentiable
architecture facilitates gradient-based optimization, enabling accurate object
count estimation and fine-grained guidance for generative models. Extensive
experiments demonstrate that YOLO-Count achieves state-of-the-art counting
accuracy while providing robust and effective quantity control for T2I systems.

</details>


### [127] [Rethinking Backbone Design for Lightweight 3D Object Detection in LiDAR](https://arxiv.org/abs/2508.00744)
*Adwait Chandorkar,Hasan Tercan,Tobias Meisen*

Main category: cs.CV

TL;DR: The paper introduces Dense Backbone, a lightweight backbone for 3D object detection, reducing model complexity and computational cost while maintaining high accuracy.


<details>
  <summary>Details</summary>
Motivation: Current 3D object detection methods rely on complex backbones like VGG or ResNet, increasing model complexity. Lightweight backbones for 3D detection are understudied.

Method: The authors propose Dense Backbone, a lightweight backbone combining speed, efficiency, and accuracy. It integrates with existing detectors like PillarNet without modifying other components.

Result: DensePillarNet reduces parameters by 29% and latency by 28%, with only a 2% accuracy drop on the nuScenes test set.

Conclusion: Dense Backbone offers a plug-and-play solution for efficient 3D object detection, balancing performance and computational cost.

Abstract: Recent advancements in LiDAR-based 3D object detection have significantly
accelerated progress toward the realization of fully autonomous driving in
real-world environments. Despite achieving high detection performance, most of
the approaches still rely on a VGG-based or ResNet-based backbone for feature
exploration, which increases the model complexity. Lightweight backbone design
is well-explored for 2D object detection, but research on 3D object detection
still remains limited. In this work, we introduce Dense Backbone, a lightweight
backbone that combines the benefits of high processing speed, lightweight
architecture, and robust detection accuracy. We adapt multiple SoTA 3d object
detectors, such as PillarNet, with our backbone and show that with our
backbone, these models retain most of their detection capability at a
significantly reduced computational cost. To our knowledge, this is the first
dense-layer-based backbone tailored specifically for 3D object detection from
point cloud data. DensePillarNet, our adaptation of PillarNet, achieves a 29%
reduction in model parameters and a 28% reduction in latency with just a 2%
drop in detection accuracy on the nuScenes test set. Furthermore, Dense
Backbone's plug-and-play design allows straightforward integration into
existing architectures, requiring no modifications to other network components.

</details>


### [128] [GECO: Geometrically Consistent Embedding with Lightspeed Inference](https://arxiv.org/abs/2508.00746)
*Regine Hartwig,Dominik Muhle,Riccardo Marin,Daniel Cremers*

Main category: cs.CV

TL;DR: GECO introduces a self-supervised vision model that enhances geometric awareness in feature learning, outperforming prior methods in speed and accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing feature learning models lack 3D geometry awareness, limiting their ability to distinguish parts based on geometry.

Method: GECO uses optimal transport for training, enabling supervision beyond keypoints and handling occlusions. It has a lightweight architecture for real-time performance.

Result: GECO achieves 30 fps (98.2% faster than prior methods) and improves PCK by 6.0%, 6.2%, and 4.1% on PFPascal, APK, and CUB datasets.

Conclusion: PCK is insufficient for geometric quality; GECO introduces new metrics for geometry-aware feature learning.

Abstract: Recent advances in feature learning have shown that self-supervised vision
foundation models can capture semantic correspondences but often lack awareness
of underlying 3D geometry. GECO addresses this gap by producing geometrically
coherent features that semantically distinguish parts based on geometry (e.g.,
left/right eyes, front/back legs). We propose a training framework based on
optimal transport, enabling supervision beyond keypoints, even under occlusions
and disocclusions. With a lightweight architecture, GECO runs at 30 fps, 98.2%
faster than prior methods, while achieving state-of-the-art performance on
PFPascal, APK, and CUB, improving PCK by 6.0%, 6.2%, and 4.1%, respectively.
Finally, we show that PCK alone is insufficient to capture geometric quality
and introduce new metrics and insights for more geometry-aware feature
learning. Link to project page:
https://reginehartwig.github.io/publications/geco/

</details>


### [129] [Is It Really You? Exploring Biometric Verification Scenarios in Photorealistic Talking-Head Avatar Videos](https://arxiv.org/abs/2508.00748)
*Laura Pedrouzo-Rodriguez,Pedro Delgado-DeRobles,Luis F. Gomez,Ruben Tolosana,Ruben Vera-Rodriguez,Aythami Morales,Julian Fierrez*

Main category: cs.CV

TL;DR: The paper explores facial motion patterns as behavioral biometrics for verifying identity in photorealistic avatar-mediated scenarios, proposing a lightweight Graph Convolutional Network with 80% AUC performance.


<details>
  <summary>Details</summary>
Motivation: Addressing security risks like impersonation in avatar-based communication by leveraging unique facial motion patterns for identity verification.

Method: Introduces a dataset of genuine/impostor avatar videos and a spatio-temporal Graph Convolutional Network with temporal attention pooling, using facial landmarks to model gestures.

Result: Facial motion cues achieve ~80% AUC for identity verification, demonstrating their reliability.

Conclusion: Highlights the need for advanced behavioral biometric defenses in avatar systems, providing a benchmark for future research.

Abstract: Photorealistic talking-head avatars are becoming increasingly common in
virtual meetings, gaming, and social platforms. These avatars allow for more
immersive communication, but they also introduce serious security risks. One
emerging threat is impersonation: an attacker can steal a user's
avatar-preserving their appearance and voice-making it nearly impossible to
detect its fraudulent usage by sight or sound alone. In this paper, we explore
the challenge of biometric verification in such avatar-mediated scenarios. Our
main question is whether an individual's facial motion patterns can serve as
reliable behavioral biometrics to verify their identity when the avatar's
visual appearance is a facsimile of its owner. To answer this question, we
introduce a new dataset of realistic avatar videos created using a
state-of-the-art one-shot avatar generation model, GAGAvatar, with genuine and
impostor avatar videos. We also propose a lightweight, explainable
spatio-temporal Graph Convolutional Network architecture with temporal
attention pooling, that uses only facial landmarks to model dynamic facial
gestures. Experimental results demonstrate that facial motion cues enable
meaningful identity verification with AUC values approaching 80%. The proposed
benchmark and biometric system are available for the research community in
order to bring attention to the urgent need for more advanced behavioral
biometric defenses in avatar-based communication systems.

</details>


### [130] [SU-ESRGAN: Semantic and Uncertainty-Aware ESRGAN for Super-Resolution of Satellite and Drone Imagery with Fine-Tuning for Cross Domain Evaluation](https://arxiv.org/abs/2508.00750)
*Prerana Ramkumar*

Main category: cs.CV

TL;DR: SU-ESRGAN enhances satellite imagery super-resolution by integrating ESRGAN with segmentation loss and uncertainty maps, improving semantic consistency and credibility for critical applications.


<details>
  <summary>Details</summary>
Motivation: GANs lack semantic consistency and per-pixel confidence in super-resolution, limiting their use in critical remote sensing tasks like disaster response and urban planning.

Method: SU-ESRGAN combines ESRGAN, DeepLabv3 for segmentation loss, and Monte Carlo dropout for uncertainty maps, tested on aerial and drone datasets.

Result: Achieves comparable PSNR, SSIM, LPIPS to ESRGAN, with better domain adaptation when fine-tuned on aligned datasets.

Conclusion: SU-ESRGAN is effective for satellite and UAV imagery, emphasizing the need for domain-aware training in super-resolution applications.

Abstract: Generative Adversarial Networks (GANs) have achieved realistic
super-resolution (SR) of images however, they lack semantic consistency and
per-pixel confidence, limiting their credibility in critical remote sensing
applications such as disaster response, urban planning and agriculture. This
paper introduces Semantic and Uncertainty-Aware ESRGAN (SU-ESRGAN), the first
SR framework designed for satellite imagery to integrate the ESRGAN,
segmentation loss via DeepLabv3 for class detail preservation and Monte Carlo
dropout to produce pixel-wise uncertainty maps. The SU-ESRGAN produces results
(PSNR, SSIM, LPIPS) comparable to the Baseline ESRGAN on aerial imagery. This
novel model is valuable in satellite systems or UAVs that use wide
field-of-view (FoV) cameras, trading off spatial resolution for coverage. The
modular design allows integration in UAV data pipelines for on-board or
post-processing SR to enhance imagery resulting due to motion blur, compression
and sensor limitations. Further, the model is fine-tuned to evaluate its
performance on cross domain applications. The tests are conducted on two drone
based datasets which differ in altitude and imaging perspective. Performance
evaluation of the fine-tuned models show a stronger adaptation to the Aerial
Maritime Drone Dataset, whose imaging characteristics align with the training
data, highlighting the importance of domain-aware training in SR-applications.

</details>


### [131] [Sample-Aware Test-Time Adaptation for Medical Image-to-Image Translation](https://arxiv.org/abs/2508.00766)
*Irene Iele,Francesco Di Feola,Valerio Guarrasi,Paolo Soda*

Main category: cs.CV

TL;DR: A novel Test-Time Adaptation (TTA) framework is proposed for medical image-to-image translation, dynamically adjusting the process per test sample to handle out-of-distribution cases without degrading in-distribution performance.


<details>
  <summary>Details</summary>
Motivation: Address limitations in handling out-of-distribution samples in image-to-image translation without performance degradation.

Method: Introduces a Reconstruction Module to quantify domain shift and a Dynamic Adaptation Block for selective feature modification in pretrained models.

Result: Consistent improvements in low-dose CT denoising and T1 to T2 MRI translation tasks over baseline and prior TTA methods.

Conclusion: Dynamic, sample-specific adjustment improves model resilience, outperforming uniform adaptation methods.

Abstract: Image-to-image translation has emerged as a powerful technique in medical
imaging, enabling tasks such as image denoising and cross-modality conversion.
However, it suffers from limitations in handling out-of-distribution samples
without causing performance degradation. To address this limitation, we propose
a novel Test-Time Adaptation (TTA) framework that dynamically adjusts the
translation process based on the characteristics of each test sample. Our
method introduces a Reconstruction Module to quantify the domain shift and a
Dynamic Adaptation Block that selectively modifies the internal features of a
pretrained translation model to mitigate the shift without compromising the
performance on in-distribution samples that do not require adaptation. We
evaluate our approach on two medical image-to-image translation tasks: low-dose
CT denoising and T1 to T2 MRI translation, showing consistent improvements over
both the baseline translation model without TTA and prior TTA methods. Our
analysis highlights the limitations of the state-of-the-art that uniformly
apply the adaptation to both out-of-distribution and in-distribution samples,
demonstrating that dynamic, sample-specific adjustment offers a promising path
to improve model resilience in real-world scenarios. The code is available at:
https://github.com/cosbidev/Sample-Aware_TTA.

</details>


### [132] [Zero-Shot Anomaly Detection with Dual-Branch Prompt Learning](https://arxiv.org/abs/2508.00777)
*Zihan Wang,Samira Ebrahimi Kahou,Narges Armanfard*

Main category: cs.CV

TL;DR: PILOT introduces a dual-branch prompt learning mechanism and label-free test-time adaptation to improve zero-shot anomaly detection under domain shifts.


<details>
  <summary>Details</summary>
Motivation: Existing zero-shot anomaly detection methods fail under domain shifts due to limited training data and lack of generalization.

Method: PILOT uses a dual-branch prompt learning mechanism and label-free test-time adaptation with pseudo-labels.

Result: PILOT achieves state-of-the-art performance on 13 benchmarks for anomaly detection and localization under domain shifts.

Conclusion: PILOT effectively addresses domain shift challenges in zero-shot anomaly detection with innovative prompt learning and adaptation strategies.

Abstract: Zero-shot anomaly detection (ZSAD) enables identifying and localizing defects
in unseen categories by relying solely on generalizable features rather than
requiring any labeled examples of anomalies. However, existing ZSAD methods,
whether using fixed or learned prompts, struggle under domain shifts because
their training data are derived from limited training domains and fail to
generalize to new distributions. In this paper, we introduce PILOT, a framework
designed to overcome these challenges through two key innovations: (1) a novel
dual-branch prompt learning mechanism that dynamically integrates a pool of
learnable prompts with structured semantic attributes, enabling the model to
adaptively weight the most relevant anomaly cues for each input image; and (2)
a label-free test-time adaptation strategy that updates the learnable prompt
parameters using high-confidence pseudo-labels from unlabeled test data.
Extensive experiments on 13 industrial and medical benchmarks demonstrate that
PILOT achieves state-of-the-art performance in both anomaly detection and
localization under domain shift.

</details>


### [133] [Cross-Dataset Semantic Segmentation Performance Analysis: Unifying NIST Point Cloud City Datasets for 3D Deep Learning](https://arxiv.org/abs/2508.00822)
*Alexander Nikitas Dimopoulos,Joseph Grasso*

Main category: cs.CV

TL;DR: The study examines semantic segmentation in heterogeneously labeled point-cloud datasets for public safety, highlighting challenges like class imbalance and label unification, and suggests improvements like standardized protocols and automated labeling.


<details>
  <summary>Details</summary>
Motivation: To address challenges in unifying differently labeled 3D data for public safety applications, particularly in pre-incident planning systems using lidar scans.

Method: Uses NIST's Point Cloud City dataset with a graded schema and KPConv architecture, evaluating performance via IoU metrics on safety-relevant features.

Result: Performance varies: larger objects (e.g., stairs, windows) segment better, while smaller safety-critical features have lower recognition rates due to class imbalance and geometric limitations.

Conclusion: Reliable segmentation for public safety requires standardized annotation protocols and better labeling techniques to handle data heterogeneity and detect small, critical elements.

Abstract: This study analyzes semantic segmentation performance across heterogeneously
labeled point-cloud datasets relevant to public safety applications, including
pre-incident planning systems derived from lidar scans. Using NIST's Point
Cloud City dataset (Enfield and Memphis collections), we investigate challenges
in unifying differently labeled 3D data. Our methodology employs a graded
schema with the KPConv architecture, evaluating performance through IoU metrics
on safety-relevant features. Results indicate performance variability:
geometrically large objects (e.g. stairs, windows) achieve higher segmentation
performance, suggesting potential for navigational context, while smaller
safety-critical features exhibit lower recognition rates. Performance is
impacted by class imbalance and the limited geometric distinction of smaller
objects in typical lidar scans, indicating limitations in detecting certain
safety-relevant features using current point-cloud methods. Key identified
challenges include insufficient labeled data, difficulties in unifying class
labels across datasets, and the need for standardization. Potential directions
include automated labeling and multi-dataset learning strategies. We conclude
that reliable point-cloud semantic segmentation for public safety necessitates
standardized annotation protocols and improved labeling techniques to address
data heterogeneity and the detection of small, safety-critical elements.

</details>


### [134] [IGL-Nav: Incremental 3D Gaussian Localization for Image-goal Navigation](https://arxiv.org/abs/2508.00823)
*Wenxuan Guo,Xiuwei Xu,Hang Yin,Ziwei Wang,Jianjiang Feng,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: IGL-Nav introduces an incremental 3D Gaussian localization framework for efficient and 3D-aware image-goal navigation, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to fully model the geometric relationship between the explored 3D environment and the goal image, making goal localization inefficient.

Method: IGL-Nav incrementally updates scene representation with monocular prediction, coarsely localizes the goal via geometric matching, and refines the pose with differentiable rendering.

Result: IGL-Nav significantly outperforms state-of-the-art methods and handles free-view image-goal navigation, even in real-world robotic deployment.

Conclusion: The proposed framework efficiently and accurately localizes goal images in 3D space, advancing image-goal navigation capabilities.

Abstract: Visual navigation with an image as goal is a fundamental and challenging
problem. Conventional methods either rely on end-to-end RL learning or
modular-based policy with topological graph or BEV map as memory, which cannot
fully model the geometric relationship between the explored 3D environment and
the goal image. In order to efficiently and accurately localize the goal image
in 3D space, we build our navigation system upon the renderable 3D gaussian
(3DGS) representation. However, due to the computational intensity of 3DGS
optimization and the large search space of 6-DoF camera pose, directly
leveraging 3DGS for image localization during agent exploration process is
prohibitively inefficient. To this end, we propose IGL-Nav, an Incremental 3D
Gaussian Localization framework for efficient and 3D-aware image-goal
navigation. Specifically, we incrementally update the scene representation as
new images arrive with feed-forward monocular prediction. Then we coarsely
localize the goal by leveraging the geometric information for discrete space
matching, which can be equivalent to efficient 3D convolution. When the agent
is close to the goal, we finally solve the fine target pose with optimization
via differentiable rendering. The proposed IGL-Nav outperforms existing
state-of-the-art methods by a large margin across diverse experimental
configurations. It can also handle the more challenging free-view image-goal
setting and be deployed on real-world robotic platform using a cellphone to
capture goal image at arbitrary pose. Project page:
https://gwxuan.github.io/IGL-Nav/.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [135] [RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization](https://arxiv.org/abs/2508.00222)
*Yihong Dong,Xue Jiang,Yongding Tao,Huanyu Liu,Kechi Zhang,Lili Mou,Rongyu Cao,Yingwei Ma,Jue Chen,Binhua Li,Zhi Jin,Fei Huang,Yongbin Li,Ge Li*

Main category: cs.AI

TL;DR: RL-PLUS enhances LLM reasoning by combining internal exploitation and external data, outperforming RLVR and avoiding capability boundary collapse.


<details>
  <summary>Details</summary>
Motivation: RLVR struggles with LLM's inherent limits and sparse rewards, leading to capability boundary collapse. RL-PLUS aims to surpass these boundaries.

Method: RL-PLUS uses Multiple Importance Sampling for data mismatch and an Exploration-Based Advantage Function to guide reasoning paths.

Result: RL-PLUS achieves state-of-the-art performance on math reasoning benchmarks and out-of-distribution tasks, with 21.1% to 69.2% improvements.

Conclusion: RL-PLUS effectively resolves capability boundary collapse and generalizes well across model families.

Abstract: Reinforcement Learning with Verifiable Reward (RLVR) has significantly
advanced the complex reasoning abilities of Large Language Models (LLMs).
However, it struggles to break through the inherent capability boundaries of
the base LLM, due to its inherently on-policy strategy with LLM's immense
action space and sparse reward. Further, RLVR can lead to the capability
boundary collapse, narrowing the LLM's problem-solving scope. To address this
problem, we propose RL-PLUS, a novel approach that synergizes internal
exploitation (i.e., Thinking) with external data (i.e., Learning) to achieve
stronger reasoning capabilities and surpass the boundaries of base models.
RL-PLUS integrates two core components: Multiple Importance Sampling to address
for distributional mismatch from external data, and an Exploration-Based
Advantage Function to guide the model towards high-value, unexplored reasoning
paths. We provide both theoretical analysis and extensive experiments to
demonstrate the superiority and generalizability of our approach. The results
show that RL-PLUS achieves state-of-the-art performance compared with existing
RLVR methods on six math reasoning benchmarks and exhibits superior performance
on six out-of-distribution reasoning tasks. It also achieves consistent and
significant gains across diverse model families, with average relative
improvements ranging from 21.1\% to 69.2\%. Moreover, Pass@k curves across
multiple benchmarks indicate that RL-PLUS effectively resolves the capability
boundary collapse problem.

</details>


### [136] [MetaAgent: Toward Self-Evolving Agent via Tool Meta-Learning](https://arxiv.org/abs/2508.00271)
*Hongjin Qian,Zheng Liu*

Main category: cs.AI

TL;DR: MetaAgent is a self-improving agent that learns by doing, using adaptive help-seeking, self-reflection, and tool-use history to enhance performance without model updates.


<details>
  <summary>Details</summary>
Motivation: To create a robust, general-purpose knowledge discovery system that evolves autonomously through hands-on practice.

Method: MetaAgent starts with basic reasoning, seeks help via natural language, routes requests to tools, and distills experience into reusable knowledge. It also builds tools and a knowledge base from its history.

Result: Outperforms workflow-based baselines and matches/exceeds end-to-end trained agents on benchmarks like GAIA, WebWalkerQA, and BrowseCamp.

Conclusion: MetaAgent demonstrates the potential of self-evolving agentic systems for effective knowledge discovery.

Abstract: In this work, we propose MetaAgent, an agentic paradigm inspired by the
principle of learning-by-doing, where expertise is developed through hands-on
practice and continual self-improvement. MetaAgent starts with a minimal
workflow, equipped only with basic reasoning and adaptive help-seeking
abilities. When a knowledge gap is encountered, MetaAgent generates natural
language help requests, which are routed to the most suitable external tool by
a dedicated tool router. As MetaAgent solves tasks, it continually conducts
self-reflection and answer verification, distilling actionable experience into
concise texts that are dynamically incorporated into future task contexts.
Besides, MetaAgent autonomously builds in-house tools and a persistent
knowledge base by organizing its tool-use history, further enhancing its
ability to retrieve and integrate relevant information We term this continual,
data-driven process as \textit{meta tool learning}, through which MetaAgent
incrementally refines its reasoning and tool-use strategies, without changing
model parameters or requiring further post-training. Evaluated on challenging
knowledge discovery benchmarks, including GAIA, WebWalkerQA, and BrowseCamp,
MetaAgent consistently outperforms workflow-based baselines and matches or
exceeds end-to-end trained agents, demonstrating the promise of self-evolving
agentic systems for robust, general-purpose knowledge discovery. We provide our
source codes in https://github.com/qhjqhj00/MetaAgent.

</details>


### [137] [Mind the Gap: The Divergence Between Human and LLM-Generated Tasks](https://arxiv.org/abs/2508.00282)
*Yi-Long Lu,Jiajun Song,Chunhui Zhang,Wei Wang*

Main category: cs.AI

TL;DR: The study compares human and LLM (GPT-4o) task generation, finding humans driven by psychological factors while LLMs lack such alignment, producing less social and physical tasks.


<details>
  <summary>Details</summary>
Motivation: To explore whether LLMs simulate human-like task generation based on cognitive principles like intrinsic motivation and personal values.

Method: A task-generation experiment comparing human responses with GPT-4o, with and without explicit psychological drivers.

Result: Humans generate tasks influenced by values and cognitive style; LLMs produce less social, less physical, and more abstract tasks, despite being seen as more fun and novel.

Conclusion: LLMs lack human-like embodied cognition, emphasizing the need for intrinsic motivation and physical grounding in AI design.

Abstract: Humans constantly generate a diverse range of tasks guided by internal
motivations. While generative agents powered by large language models (LLMs)
aim to simulate this complex behavior, it remains uncertain whether they
operate on similar cognitive principles. To address this, we conducted a
task-generation experiment comparing human responses with those of an LLM agent
(GPT-4o). We find that human task generation is consistently influenced by
psychological drivers, including personal values (e.g., Openness to Change) and
cognitive style. Even when these psychological drivers are explicitly provided
to the LLM, it fails to reflect the corresponding behavioral patterns. They
produce tasks that are markedly less social, less physical, and thematically
biased toward abstraction. Interestingly, while the LLM's tasks were perceived
as more fun and novel, this highlights a disconnect between its linguistic
proficiency and its capacity to generate human-like, embodied goals.We conclude
that there is a core gap between the value-driven, embodied nature of human
cognition and the statistical patterns of LLMs, highlighting the necessity of
incorporating intrinsic motivation and physical grounding into the design of
more human-aligned agents.

</details>


### [138] [R1-ACT: Efficient Reasoning Model Safety Alignment by Activating Safety Knowledge](https://arxiv.org/abs/2508.00324)
*Yeonjun In,Wonjoong Kim,Sangwu Park,Chanyoung Park*

Main category: cs.AI

TL;DR: R1-Act is a post-training method that activates safety knowledge in large reasoning models (LRMs) to reduce harmful outputs, requiring minimal training resources.


<details>
  <summary>Details</summary>
Motivation: LRMs often comply with harmful instructions despite possessing safety knowledge, highlighting a need for methods to activate this knowledge during reasoning.

Method: R1-Act uses a structured reasoning process to explicitly trigger safety knowledge, requiring only 1,000 examples and 90 minutes of training on a single GPU.

Result: R1-Act improves safety without compromising reasoning performance, outperforming existing alignment methods.

Conclusion: R1-Act is a robust, scalable, and efficient solution for enhancing LRM safety.

Abstract: Although large reasoning models (LRMs) have demonstrated impressive
capabilities on complex tasks, recent studies reveal that these models
frequently fulfill harmful user instructions, raising significant safety
concerns. In this paper, we investigate the underlying cause of LRM safety
risks and find that models already possess sufficient safety knowledge but fail
to activate it during reasoning. Based on this insight, we propose R1-Act, a
simple and efficient post-training method that explicitly triggers safety
knowledge through a structured reasoning process. R1-Act achieves strong safety
improvements while preserving reasoning performance, outperforming prior
alignment methods. Notably, it requires only 1,000 training examples and 90
minutes of training on a single RTX A6000 GPU. Extensive experiments across
multiple LRM backbones and sizes demonstrate the robustness, scalability, and
practical efficiency of our approach.

</details>


### [139] [Cognitive Kernel-Pro: A Framework for Deep Research Agents and Agent Foundation Models Training](https://arxiv.org/abs/2508.00414)
*Tianqing Fang,Zhisong Zhang,Xiaoyang Wang,Rui Wang,Can Qin,Yuxuan Wan,Jun-Yu Ma,Ce Zhang,Jiaqi Chen,Xiyun Li,Hongming Zhang,Haitao Mi,Dong Yu*

Main category: cs.AI

TL;DR: Cognitive Kernel-Pro is an open-source, free multi-module AI agent framework designed to democratize advanced AI agent development, achieving state-of-the-art results on GAIA.


<details>
  <summary>Details</summary>
Motivation: Current AI agent systems are often closed-source or rely on paid APIs, limiting accessibility and reproducibility.

Method: The framework focuses on high-quality training data curation, agent test-time reflection, and voting strategies across web, file, code, and reasoning domains.

Result: The 8B-parameter open-source model outperforms previous leading systems like WebDancer and WebSailor on GAIA.

Conclusion: Cognitive Kernel-Pro sets a new standard for accessible, high-capability AI agents, with code publicly available.

Abstract: General AI Agents are increasingly recognized as foundational frameworks for
the next generation of artificial intelligence, enabling complex reasoning, web
interaction, coding, and autonomous research capabilities. However, current
agent systems are either closed-source or heavily reliant on a variety of paid
APIs and proprietary tools, limiting accessibility and reproducibility for the
research community. In this work, we present \textbf{Cognitive Kernel-Pro}, a
fully open-source and (to the maximum extent) free multi-module agent framework
designed to democratize the development and evaluation of advanced AI agents.
Within Cognitive Kernel-Pro, we systematically investigate the curation of
high-quality training data for Agent Foundation Models, focusing on the
construction of queries, trajectories, and verifiable answers across four key
domains: web, file, code, and general reasoning. Furthermore, we explore novel
strategies for agent test-time reflection and voting to enhance agent
robustness and performance. We evaluate Cognitive Kernel-Pro on GAIA, achieving
state-of-the-art results among open-source and free agents. Notably, our
8B-parameter open-source model surpasses previous leading systems such as
WebDancer and WebSailor, establishing a new performance standard for
accessible, high-capability AI agents. Code is available at
https://github.com/Tencent/CognitiveKernel-Pro

</details>


### [140] [CoRGI: Verified Chain-of-Thought Reasoning with Visual Grounding](https://arxiv.org/abs/2508.00378)
*Shixin Yi,Lin Shang*

Main category: cs.AI

TL;DR: CoRGI introduces visual verification into Chain-of-Thought prompting for VLMs, improving reasoning by grounding explanations in visual evidence.


<details>
  <summary>Details</summary>
Motivation: Address hallucinations in CoT prompting by ensuring visual grounding during reasoning.

Method: Three-stage pipeline: generate textual reasoning, extract visual evidence (VEVM), synthesize grounded answer.

Result: Improves reasoning on VCR benchmark with Qwen-2.5VL and LLaVA-1.6; more factual explanations.

Conclusion: Grounding intermediate reasoning in visual evidence enhances robustness in multimodal reasoning.

Abstract: Chain-of-Thought (CoT) prompting has shown promise in improving reasoning in
vision-language models (VLMs), but it often produces explanations that are
linguistically fluent yet lack grounding in visual content. We observe that
such hallucinations arise in part from the absence of an explicit verification
mechanism during multi-step reasoning. To address this, we propose
\textbf{CoRGI}(\textbf{C}hain \textbf{o}f \textbf{R}easoning with
\textbf{G}rounded \textbf{I}nsights), a modular framework that introduces
visual verification into the reasoning process. CoRGI follows a three-stage
pipeline: it first generates a textual reasoning chain, then extracts
supporting visual evidence for each reasoning step via a dedicated module
(VEVM), and finally synthesizes the textual rationale with visual evidence to
generate a grounded, verified answer. The framework can be integrated with
existing VLMs without end-to-end retraining. We evaluate CoRGI on the VCR
benchmark and find that it improves reasoning performance on two representative
open-source VLM backbones, Qwen-2.5VL and LLaVA-1.6. Ablation studies confirm
the contribution of each step in the verification module, and human evaluations
suggest that CoRGI leads to more factual and helpful explanations. We also
examine alternative designs for the visual verification step and discuss
potential limitations of post-hoc verification frameworks. These findings
highlight the importance of grounding intermediate reasoning steps in visual
evidence to enhance the robustness of multimodal reasoning.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [141] [The Repeated-Stimulus Confound in Electroencephalography](https://arxiv.org/abs/2508.00531)
*Jack A. Kilgallen,Barak A. Pearlmutter,Jeffrey Mark Siskind*

Main category: q-bio.NC

TL;DR: The paper identifies a 'repeated-stimulus confound' in neural-decoding studies, where models trained and evaluated on the same stimuli overestimate accuracy by 4.46-7.42%, undermining validity.


<details>
  <summary>Details</summary>
Motivation: To address the issue of inflated decoding accuracies in neural-decoding studies due to repeated stimuli, which confounds model evaluation.

Method: Analyzed susceptible datasets and 16 affected publications, conducted experiments to quantify overestimation, and tested the confound's implications in pseudoscientific contexts.

Result: Decoding accuracies were overestimated by 4.46-7.42%, with a 0.26% increase in overestimation per 1% accuracy under the confound. The confound also validated pseudoscientific claims.

Conclusion: The repeated-stimulus confound significantly biases neural-decoding results, necessitating methodological revisions to ensure accurate model evaluation.

Abstract: In neural-decoding studies, recordings of participants' responses to stimuli
are used to train models. In recent years, there has been an explosion of
publications detailing applications of innovations from deep-learning research
to neural-decoding studies. The data-hungry models used in these experiments
have resulted in a demand for increasingly large datasets. Consequently, in
some studies, the same stimuli are presented multiple times to each participant
to increase the number of trials available for use in model training. However,
when a decoding model is trained and subsequently evaluated on responses to the
same stimuli, stimulus identity becomes a confounder for accuracy. We term this
the repeated-stimulus confound. We identify a susceptible dataset, and 16
publications which report model performance based on evaluation procedures
affected by the confound. We conducted experiments using models from the
affected studies to investigate the likely extent to which results in the
literature have been misreported. Our findings suggest that the decoding
accuracies of these models were overestimated by between 4.46-7.42%. Our
analysis also indicates that per 1% increase in accuracy under the confound,
the magnitude of the overestimation increases by 0.26%. The confound not only
results in optimistic estimates of decoding performance, but undermines the
validity of several claims made within the affected publications. We conducted
further experiments to investigate the implications of the confound in
alternative contexts. We found that the same methodology used within the
affected studies could also be used to justify an array of pseudoscientific
claims, such as the existence of extrasensory perception.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [142] [Watch the Weights: Unsupervised monitoring and control of fine-tuned LLMs](https://arxiv.org/abs/2508.00161)
*Ziqian Zhong,Aditi Raghunathan*

Main category: cs.LG

TL;DR: A new method interprets weights instead of activations to monitor and control fine-tuned LLMs, detecting behaviors like backdoors and unlearning with high accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing interpretability methods rely on distributionally similar data, limiting their ability to detect novel threats like backdoors.

Method: Analyzes weight differences between fine-tuned and base models, focusing on top singular vectors to detect new behaviors.

Result: Detects backdoors with 100% success (1.2% false positives) and unlearning with 95.42% accuracy. Also useful for model auditing.

Conclusion: The method effectively monitors and controls fine-tuned LLMs without needing training data, showing promise for security and auditing.

Abstract: The releases of powerful open-weight large language models (LLMs) are often
not accompanied by access to their full training data. Existing
interpretability methods, particularly those based on activations, often
require or assume distributionally similar data. This is a significant
limitation when detecting and defending against novel potential threats like
backdoors, which are by definition out-of-distribution.
  In this work, we introduce a new method for understanding, monitoring and
controlling fine-tuned LLMs that interprets weights, rather than activations,
thereby side stepping the need for data that is distributionally similar to the
unknown training data. We demonstrate that the top singular vectors of the
weight difference between a fine-tuned model and its base model correspond to
newly acquired behaviors. By monitoring the cosine similarity of activations
along these directions, we can detect salient behaviors introduced during
fine-tuning with high precision.
  For backdoored models that bypasses safety mechanisms when a secret trigger
is present, our method stops up to 100% of attacks with a false positive rate
below 1.2%. For models that have undergone unlearning, we detect inference on
erased topics with accuracy up to 95.42% and can even steer the model to
recover "unlearned" information. Besides monitoring, our method also shows
potential for pre-deployment model auditing: by analyzing commercial
instruction-tuned models (OLMo, Llama, Qwen), we are able to uncover
model-specific fine-tuning focus including marketing strategies and Midjourney
prompt generation.
  Our implementation can be found at https://github.com/fjzzq2002/WeightWatch.

</details>


### [143] [Towards Higher Effective Rank in Parameter-efficient Fine-tuning using Khatri--Rao Product](https://arxiv.org/abs/2508.00230)
*Paul Albert,Frederic Z. Zhang,Hemanth Saratchandran,Anton van den Hengel,Ehsan Abbasnejad*

Main category: cs.LG

TL;DR: KRAdapter, a new PEFT method using Khatri-Rao product, outperforms LoRA on high-rank matrices and maintains efficiency.


<details>
  <summary>Details</summary>
Motivation: Address LoRA's limitations in approximating high-rank matrices, especially in multimodal and large language models.

Method: Introduces KRAdapter, leveraging Khatri-Rao product for high-rank weight updates, tested on vision-language and large language models.

Result: KRAdapter shows gains on tasks, especially unseen common-sense reasoning, while keeping LoRA's efficiency.

Conclusion: KRAdapter is a practical, robust alternative for fine-tuning billion-scale models, overcoming LoRA's high-rank limitations.

Abstract: Parameter-efficient fine-tuning (PEFT) has become a standard approach for
adapting large pre-trained models. Amongst PEFT methods, low-rank adaptation
(LoRA) has achieved notable success. However, recent studies have highlighted
its limitations compared against full-rank alternatives, particularly when
applied to multimodal and large language models. In this work, we present a
quantitative comparison amongst full-rank and low-rank PEFT methods using a
synthetic matrix approximation benchmark with controlled spectral properties.
Our results confirm that LoRA struggles to approximate matrices with relatively
flat spectrums or high frequency components -- signs of high effective ranks.
To this end, we introduce KRAdapter, a novel PEFT algorithm that leverages the
Khatri-Rao product to produce weight updates, which, by construction, tends to
produce matrix product with a high effective rank. We demonstrate performance
gains with KRAdapter on vision-language models up to 1B parameters and on large
language models up to 8B parameters, particularly on unseen common-sense
reasoning tasks. In addition, KRAdapter maintains the memory and compute
efficiency of LoRA, making it a practical and robust alternative to fine-tune
billion-scale parameter models.

</details>


### [144] [Classification of Psychiatry Clinical Notes by Diagnosis: A Deep Learning and Machine Learning Approach](https://arxiv.org/abs/2508.00695)
*Sergio Rubio-Martn,Mara Teresa Garca-Ords,Antonio Serrano-Garca,Clara Margarita Franch-Pato,Arturo Crespo-lvaro,Jos Alberto Bentez-Andrades*

Main category: cs.LG

TL;DR: The study compares AI models (traditional ML and Deep Learning) for classifying clinical notes into Anxiety and Adjustment Disorder diagnoses, evaluating oversampling and hyperparameter tuning. Oversampling had minimal impact, but hyperparameter tuning significantly boosted accuracy, with Decision Tree, XGBoost, DistilBERT, and SciBERT achieving 96% accuracy.


<details>
  <summary>Details</summary>
Motivation: To improve AI-assisted diagnostic tools in mental health by evaluating model performance and data balancing methods for classifying clinical notes.

Method: Comparison of traditional ML (Random Forest, SVM, KNN, Decision Tree, XGBoost) and Deep Learning (DistilBERT, SciBERT) models, with oversampling (None, Random, SMOTE) and hyperparameter tuning.

Result: Oversampling had little impact except SMOTE with BERT models. Hyperparameter tuning improved accuracy, with Decision Tree, XGBoost, DistilBERT, and SciBERT reaching 96%.

Conclusion: Hyperparameter tuning is crucial for model performance. The study provides insights into effective AI models and data balancing for mental health diagnostics.

Abstract: The classification of clinical notes into specific diagnostic categories is
critical in healthcare, especially for mental health conditions like Anxiety
and Adjustment Disorder. In this study, we compare the performance of various
Artificial Intelligence models, including both traditional Machine Learning
approaches (Random Forest, Support Vector Machine, K-nearest neighbors,
Decision Tree, and eXtreme Gradient Boost) and Deep Learning models (DistilBERT
and SciBERT), to classify clinical notes into these two diagnoses.
Additionally, we implemented three oversampling strategies: No Oversampling,
Random Oversampling, and Synthetic Minority Oversampling Technique (SMOTE), to
assess their impact on model performance. Hyperparameter tuning was also
applied to optimize model accuracy. Our results indicate that oversampling
techniques had minimal impact on model performance overall. The only exception
was SMOTE, which showed a positive effect specifically with BERT-based models.
However, hyperparameter optimization significantly improved accuracy across the
models, enhancing their ability to generalize and perform on the dataset. The
Decision Tree and eXtreme Gradient Boost models achieved the highest accuracy
among machine learning approaches, both reaching 96%, while the DistilBERT and
SciBERT models also attained 96% accuracy in the deep learning category. These
findings underscore the importance of hyperparameter tuning in maximizing model
performance. This study contributes to the ongoing research on AI-assisted
diagnostic tools in mental health by providing insights into the efficacy of
different model architectures and data balancing methods.

</details>


### [145] [Stress-Aware Resilient Neural Training](https://arxiv.org/abs/2508.00098)
*Ashkan Shakarami,Yousef Yeganeh,Azade Farshad,Lorenzo Nicole,Stefano Ghidoni,Nassir Navab*

Main category: cs.LG

TL;DR: Stress-Aware Learning (SAL) introduces a resilient neural training paradigm using adaptive noise to escape sharp minima, improving robustness and generalization.


<details>
  <summary>Details</summary>
Motivation: Inspired by structural fatigue in materials science, SAL addresses optimization difficulties in deep neural networks under uncertain dynamics.

Method: Proposes Plastic Deformation Optimizer, injecting adaptive noise based on internal stress signals to escape sharp minima.

Result: Experiments show improved robustness and generalization across architectures, optimizers, and benchmarks with minimal overhead.

Conclusion: SAL offers a practical, efficient approach to enhance neural network training resilience and performance.

Abstract: This paper introduces Stress-Aware Learning, a resilient neural training
paradigm in which deep neural networks dynamically adjust their optimization
behavior - whether under stable training regimes or in settings with uncertain
dynamics - based on the concept of Temporary (Elastic) and Permanent (Plastic)
Deformation, inspired by structural fatigue in materials science. To
instantiate this concept, we propose Plastic Deformation Optimizer, a
stress-aware mechanism that injects adaptive noise into model parameters
whenever an internal stress signal - reflecting stagnation in training loss and
accuracy - indicates persistent optimization difficulty. This enables the model
to escape sharp minima and converge toward flatter, more generalizable regions
of the loss landscape. Experiments across six architectures, four optimizers,
and seven vision benchmarks demonstrate improved robustness and generalization
with minimal computational overhead. The code and 3D visuals will be available
on GitHub: https://github.com/Stress-Aware-Learning/SAL.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [146] [CADS: A Comprehensive Anatomical Dataset and Segmentation for Whole-Body Anatomy in Computed Tomography](https://arxiv.org/abs/2507.22953)
*Murong Xu,Tamaz Amiranashvili,Fernando Navarro,Maksym Fritsak,Ibrahim Ethem Hamamci,Suprosanna Shit,Bastian Wittmann,Sezgin Er,Sebastian M. Christ,Ezequiel de la Rosa,Julian Deseoe,Robert Graf,Hendrik Mller,Anjany Sekuboyina,Jan C. Peeken,Sven Becker,Giulia Baldini,Johannes Haubold,Felix Nensa,Ren Hosch,Nikhil Mirajkar,Saad Khalid,Stefan Zachow,Marc-Andr Weber,Georg Langs,Jakob Wasserthal,Mehmet Kemal Ozdemir,Andrey Fedorov,Ron Kikinis,Stephanie Tanadini-Lang,Jan S. Kirschke,Stephanie E. Combs,Bjoern Menze*

Main category: eess.IV

TL;DR: CADS introduces an open-source framework for whole-body CT segmentation, leveraging a large-scale dataset and standardized data integration to outperform existing methods.


<details>
  <summary>Details</summary>
Motivation: Current AI segmentation models are fragmented and lack comprehensive training data, hindering clinical deployment. CADS addresses this by integrating diverse data sources.

Method: CADS uses a large-scale dataset of 22,022 CT volumes with 167 annotated structures, combined with established architectures for full-body segmentation.

Result: The CADS-model outperforms state-of-the-art approaches in evaluations across 18 public datasets and a real-world hospital cohort, proving clinical utility.

Conclusion: CADS advances robust AI solutions in radiology by providing open-access tools for comprehensive anatomical analysis.

Abstract: Accurate delineation of anatomical structures in volumetric CT scans is
crucial for diagnosis and treatment planning. While AI has advanced automated
segmentation, current approaches typically target individual structures,
creating a fragmented landscape of incompatible models with varying performance
and disparate evaluation protocols. Foundational segmentation models address
these limitations by providing a holistic anatomical view through a single
model. Yet, robust clinical deployment demands comprehensive training data,
which is lacking in existing whole-body approaches, both in terms of data
heterogeneity and, more importantly, anatomical coverage. In this work, rather
than pursuing incremental optimizations in model architecture, we present CADS,
an open-source framework that prioritizes the systematic integration,
standardization, and labeling of heterogeneous data sources for whole-body CT
segmentation. At its core is a large-scale dataset of 22,022 CT volumes with
complete annotations for 167 anatomical structures, representing a significant
advancement in both scale and coverage, with 18 times more scans than existing
collections and 60% more distinct anatomical targets. Building on this diverse
dataset, we develop the CADS-model using established architectures for
accessible and automated full-body CT segmentation. Through comprehensive
evaluation across 18 public datasets and an independent real-world hospital
cohort, we demonstrate advantages over SoTA approaches. Notably, thorough
testing of the model's performance in segmentation tasks from radiation
oncology validates its direct utility for clinical interventions. By making our
large-scale dataset, our segmentation models, and our clinical software tool
publicly available, we aim to advance robust AI solutions in radiology and make
comprehensive anatomical analysis accessible to clinicians and researchers
alike.

</details>


### [147] [GEPAR3D: Geometry Prior-Assisted Learning for 3D Tooth Segmentation](https://arxiv.org/abs/2508.00155)
*Tomasz Szczepaski,Szymon Potka,Michal K. Grzeszczyk,Arleta Adamowicz,Piotr Fudalej,Przemysaw Korzeniowski,Tomasz Trzciski,Arkadiusz Sitek*

Main category: eess.IV

TL;DR: GEPAR3D is a novel method for tooth segmentation in CBCT scans, combining instance detection and multi-class segmentation with a Statistical Shape Model and deep watershed method, achieving superior performance (DSC 95.0%).


<details>
  <summary>Details</summary>
Motivation: Accurate tooth segmentation, especially for fine structures like root apices, is critical for assessing root resorption in orthodontics but remains challenging.

Method: GEPAR3D integrates a Statistical Shape Model as a geometric prior and uses a deep watershed method to model teeth as 3D energy basins, ensuring precise segmentation.

Result: GEPAR3D outperforms other methods with a DSC of 95.0% (+2.8%) and recall of 95.2% (+9.5%), showing significant improvements in root segmentation quality.

Conclusion: GEPAR3D enhances root resorption assessment and clinical decision-making, with the implementation and dataset made publicly available.

Abstract: Tooth segmentation in Cone-Beam Computed Tomography (CBCT) remains
challenging, especially for fine structures like root apices, which is critical
for assessing root resorption in orthodontics. We introduce GEPAR3D, a novel
approach that unifies instance detection and multi-class segmentation into a
single step tailored to improve root segmentation. Our method integrates a
Statistical Shape Model of dentition as a geometric prior, capturing anatomical
context and morphological consistency without enforcing restrictive adjacency
constraints. We leverage a deep watershed method, modeling each tooth as a
continuous 3D energy basin encoding voxel distances to boundaries. This
instance-aware representation ensures accurate segmentation of narrow, complex
root apices. Trained on publicly available CBCT scans from a single center, our
method is evaluated on external test sets from two in-house and two public
medical centers. GEPAR3D achieves the highest overall segmentation performance,
averaging a Dice Similarity Coefficient (DSC) of 95.0% (+2.8% over the
second-best method) and increasing recall to 95.2% (+9.5%) across all test
sets. Qualitative analyses demonstrated substantial improvements in root
segmentation quality, indicating significant potential for more accurate root
resorption assessment and enhanced clinical decision-making in orthodontics. We
provide the implementation and dataset at https://github.com/tomek1911/GEPAR3D.

</details>


### [148] [Weakly Supervised Intracranial Aneurysm Detection and Segmentation in MR angiography via Multi-task UNet with Vesselness Prior](https://arxiv.org/abs/2508.00235)
*Erin Rainville,Amirhossein Rasoulian,Hassan Rivaz,Yiming Xiao*

Main category: eess.IV

TL;DR: A weakly supervised 3D multi-task UNet with vesselness priors is proposed for intracranial aneurysm detection and segmentation in TOF-MRA, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Intracranial aneurysms (IAs) are hard to detect and analyze due to small size and low contrast in scans, and the lack of large annotated datasets for deep learning.

Method: A novel 3D multi-task UNet integrates Frangi's vesselness filter for cerebrovascular priors, enabling joint detection and segmentation with weak supervision.

Result: Achieves superior performance: Dice = 0.614, 95%HD =1.38mm for segmentation; false positive rate = 1.47, sensitivity = 92.9% for detection.

Conclusion: The proposed method effectively addresses IA detection and segmentation challenges, demonstrating strong performance and generalizability.

Abstract: Intracranial aneurysms (IAs) are abnormal dilations of cerebral blood vessels
that, if ruptured, can lead to life-threatening consequences. However, their
small size and soft contrast in radiological scans often make it difficult to
perform accurate and efficient detection and morphological analyses, which are
critical in the clinical care of the disorder. Furthermore, the lack of large
public datasets with voxel-wise expert annotations pose challenges for
developing deep learning algorithms to address the issues. Therefore, we
proposed a novel weakly supervised 3D multi-task UNet that integrates
vesselness priors to jointly perform aneurysm detection and segmentation in
time-of-flight MR angiography (TOF-MRA). Specifically, to robustly guide IA
detection and segmentation, we employ the popular Frangi's vesselness filter to
derive soft cerebrovascular priors for both network input and an attention
block to conduct segmentation from the decoder and detection from an auxiliary
branch. We train our model on the Lausanne dataset with coarse ground truth
segmentation, and evaluate it on the test set with refined labels from the same
database. To further assess our model's generalizability, we also validate it
externally on the ADAM dataset. Our results demonstrate the superior
performance of the proposed technique over the SOTA techniques for aneurysm
segmentation (Dice = 0.614, 95%HD =1.38mm) and detection (false positive rate =
1.47, sensitivity = 92.9%).

</details>


### [149] [Diffusion-Based User-Guided Data Augmentation for Coronary Stenosis Detection](https://arxiv.org/abs/2508.00438)
*Sumin Seo,In Kyu Lee,Hyun-Woo Kim,Jaesik Min,Chung-Hwan Jung*

Main category: eess.IV

TL;DR: A novel data augmentation method using diffusion models for generating realistic coronary lesions improves stenosis detection and severity classification, especially with limited labeled data.


<details>
  <summary>Details</summary>
Motivation: Coronary stenosis is a critical condition requiring precise analysis, but deep learning approaches face challenges like limited labeled data and class imbalance.

Method: Proposes a data augmentation approach using a diffusion model for inpainting realistic lesions with user-controlled severity.

Result: Outperforms other methods in lesion detection and severity classification, even with limited data, on both in-house and public datasets.

Conclusion: The method enhances stenosis assessment and optimizes data use, offering reliable decision support in clinical settings.

Abstract: Coronary stenosis is a major risk factor for ischemic heart events leading to
increased mortality, and medical treatments for this condition require
meticulous, labor-intensive analysis. Coronary angiography provides critical
visual cues for assessing stenosis, supporting clinicians in making informed
decisions for diagnosis and treatment. Recent advances in deep learning have
shown great potential for automated localization and severity measurement of
stenosis. In real-world scenarios, however, the success of these competent
approaches is often hindered by challenges such as limited labeled data and
class imbalance. In this study, we propose a novel data augmentation approach
that uses an inpainting method based on a diffusion model to generate realistic
lesions, allowing user-guided control of severity. Extensive evaluation on
lesion detection and severity classification across various synthetic dataset
sizes shows superior performance of our method on both a large-scale in-house
dataset and a public coronary angiography dataset. Furthermore, our approach
maintains high detection and classification performance even when trained with
limited data, highlighting its clinical importance in improving the assessment
of severity of stenosis and optimizing data utilization for more reliable
decision support.

</details>


### [150] [FMPlug: Plug-In Foundation Flow-Matching Priors for Inverse Problems](https://arxiv.org/abs/2508.00721)
*Yuxiang Wan,Ryan Devera,Wenjie Zhang,Ju Sun*

Main category: eess.IV

TL;DR: FMPlug is a plug-in framework enhancing flow-matching priors for inverse problems, outperforming state-of-the-art methods in image super-resolution and Gaussian deblurring.


<details>
  <summary>Details</summary>
Motivation: Traditional methods rely on domain-specific or untrained priors, limiting their effectiveness. FMPlug aims to leverage foundation models more efficiently.

Method: FMPlug uses a time-adaptive warm-up strategy and sharp Gaussianity regularization, capitalizing on object similarity and Gaussianity of generative flows.

Result: FMPlug significantly outperforms state-of-the-art methods in image super-resolution and Gaussian deblurring.

Conclusion: FMPlug demonstrates the potential of domain-agnostic foundation models for solving ill-posed inverse problems.

Abstract: We present FMPlug, a novel plug-in framework that enhances foundation
flow-matching (FM) priors for solving ill-posed inverse problems. Unlike
traditional approaches that rely on domain-specific or untrained priors, FMPlug
smartly leverages two simple but powerful insights: the similarity between
observed and desired objects and the Gaussianity of generative flows. By
introducing a time-adaptive warm-up strategy and sharp Gaussianity
regularization, FMPlug unlocks the true potential of domain-agnostic foundation
models. Our method beats state-of-the-art methods that use foundation FM priors
by significant margins, on image super-resolution and Gaussian deblurring.

</details>


### [151] [AI-Driven Collaborative Satellite Object Detection for Space Sustainability](https://arxiv.org/abs/2508.00755)
*Peng Hu,Wenxuan Zhang*

Main category: eess.IV

TL;DR: A novel satellite clustering framework for collaborative deep learning-based space object detection (SOD) is proposed, improving accuracy and efficiency while maintaining low SWaP.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of space sustainability due to increasing satellite density in LEO, traditional ground-based tracking limitations, and the need for onboard vision-based SOD.

Method: Developed a satellite clustering framework with a high-fidelity dataset, distance-aware viewpoint selection, and evaluated using DL models.

Result: The clustering-based method achieves competitive detection accuracy with low SWaP, outperforming single-satellite and existing approaches.

Conclusion: Distributed, AI-enabled in-orbit systems can enhance space situational awareness and contribute to long-term space sustainability.

Abstract: The growing density of satellites in low-Earth orbit (LEO) presents serious
challenges to space sustainability, primarily due to the increased risk of
in-orbit collisions. Traditional ground-based tracking systems are constrained
by latency and coverage limitations, underscoring the need for onboard,
vision-based space object detection (SOD) capabilities. In this paper, we
propose a novel satellite clustering framework that enables the collaborative
execution of deep learning (DL)-based SOD tasks across multiple satellites. To
support this approach, we construct a high-fidelity dataset simulating imaging
scenarios for clustered satellite formations. A distance-aware viewpoint
selection strategy is introduced to optimize detection performance, and recent
DL models are used for evaluation. Experimental results show that the
clustering-based method achieves competitive detection accuracy compared to
single-satellite and existing approaches, while maintaining a low size, weight,
and power (SWaP) footprint. These findings underscore the potential of
distributed, AI-enabled in-orbit systems to enhance space situational awareness
and contribute to long-term space sustainability.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [152] [Occlusion-robust Stylization for Drawing-based 3D Animation](https://arxiv.org/abs/2508.00398)
*Sunjae Yoon,Gwanhyeong Koo,Younghwan Lee,Ji Woo Hong,Chang D. Yoo*

Main category: cs.GR

TL;DR: The paper introduces OSF, an occlusion-robust framework for drawing-based 3D animation, addressing style deterioration under occlusions and improving speed and memory efficiency.


<details>
  <summary>Details</summary>
Motivation: Preserving an artist's unique style in drawing-based 3D animation is challenging due to occlusions, causing flickering and blurring. The 'stylization pose gap' worsens this issue.

Method: OSF uses optical flow for occlusion-robust edge guidance, replacing inaccurate edge inputs. It operates in a single run, unlike previous two-stage methods.

Result: OSF ensures consistent stylization under occlusions, achieves 2.4x faster inference, and uses 2.1x less memory.

Conclusion: OSF effectively addresses the stylization pose gap, improving quality and efficiency in drawing-based 3D animation.

Abstract: 3D animation aims to generate a 3D animated video from an input image and a
target 3D motion sequence. Recent advances in image-to-3D models enable the
creation of animations directly from user-hand drawings. Distinguished from
conventional 3D animation, drawing-based 3D animation is crucial to preserve
artist's unique style properties, such as rough contours and distinct stroke
patterns. However, recent methods still exhibit quality deterioration in style
properties, especially under occlusions caused by overlapping body parts,
leading to contour flickering and stroke blurring. This occurs due to a
`stylization pose gap' between training and inference in stylization networks
designed to preserve drawing styles in drawing-based 3D animation systems. The
stylization pose gap denotes that input target poses used to train the
stylization network are always in occlusion-free poses, while target poses
encountered in an inference include diverse occlusions under dynamic motions.
To this end, we propose Occlusion-robust Stylization Framework (OSF) for
drawing-based 3D animation. We found that while employing object's edge can be
effective input prior for guiding stylization, it becomes notably inaccurate
when occlusions occur at inference. Thus, our proposed OSF provides
occlusion-robust edge guidance for stylization network using optical flow,
ensuring a consistent stylization even under occlusions. Furthermore, OSF
operates in a single run instead of the previous two-stage method, achieving
2.4x faster inference and 2.1x less memory.

</details>


### [153] [SpA2V: Harnessing Spatial Auditory Cues for Audio-driven Spatially-aware Video Generation](https://arxiv.org/abs/2508.00782)
*Kien T. Pham,Yingqing He,Yazhou Xing,Qifeng Chen,Long Chen*

Main category: cs.GR

TL;DR: SpA2V is a novel framework for audio-driven video generation that leverages spatial auditory cues to create videos with accurate semantic and spatial alignment, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: Existing audio-driven video generation methods focus mainly on semantic information, neglecting spatial attributes like sound source locations and movements, which humans naturally perceive. SpA2V addresses this gap by incorporating spatial auditory cues.

Method: SpA2V uses a two-stage process: 1) Audio-guided Video Planning, where spatial and semantic cues from audio are used to create Video Scene Layouts (VSLs), and 2) Layout-grounded Video Generation, integrating VSLs into pre-trained diffusion models for training-free video synthesis.

Result: Experiments show SpA2V generates realistic videos with strong semantic and spatial alignment to input audios.

Conclusion: SpA2V advances audio-driven video generation by effectively utilizing spatial auditory cues, bridging the gap between audio and video modalities.

Abstract: Audio-driven video generation aims to synthesize realistic videos that align
with input audio recordings, akin to the human ability to visualize scenes from
auditory input. However, existing approaches predominantly focus on exploring
semantic information, such as the classes of sounding sources present in the
audio, limiting their ability to generate videos with accurate content and
spatial composition. In contrast, we humans can not only naturally identify the
semantic categories of sounding sources but also determine their deeply encoded
spatial attributes, including locations and movement directions. This useful
information can be elucidated by considering specific spatial indicators
derived from the inherent physical properties of sound, such as loudness or
frequency. As prior methods largely ignore this factor, we present SpA2V, the
first framework explicitly exploits these spatial auditory cues from audios to
generate videos with high semantic and spatial correspondence. SpA2V decomposes
the generation process into two stages: 1) Audio-guided Video Planning: We
meticulously adapt a state-of-the-art MLLM for a novel task of harnessing
spatial and semantic cues from input audio to construct Video Scene Layouts
(VSLs). This serves as an intermediate representation to bridge the gap between
the audio and video modalities. 2) Layout-grounded Video Generation: We develop
an efficient and effective approach to seamlessly integrate VSLs as conditional
guidance into pre-trained diffusion models, enabling VSL-grounded video
generation in a training-free manner. Extensive experiments demonstrate that
SpA2V excels in generating realistic videos with semantic and spatial alignment
to the input audios.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [154] [Jet Image Generation in High Energy Physics Using Diffusion Models](https://arxiv.org/abs/2508.00250)
*Victor D. Martinez,Vidya Manian,Sudhir Malik*

Main category: hep-ph

TL;DR: The paper introduces diffusion models for generating jet images from proton-proton collisions at the LHC, comparing score-based and consistency models, with the latter showing superior fidelity and stability.


<details>
  <summary>Details</summary>
Motivation: To improve the generation of class-conditional jet images for HEP research by leveraging diffusion models directly in image space.

Method: Mapping kinematic variables of jets to 2D images, training diffusion models (score-based and consistency) on these images, and evaluating fidelity using metrics like FID.

Result: Consistency models outperform score-based diffusion models in fidelity and generation stability.

Conclusion: The method enhances computational efficiency and accuracy, offering valuable tools for HEP research.

Abstract: This article presents, for the first time, the application of diffusion
models for generating jet images corresponding to proton-proton collision
events at the Large Hadron Collider (LHC). The kinematic variables of quark,
gluon, W-boson, Z-boson, and top quark jets from the JetNet simulation dataset
are mapped to two-dimensional image representations. Diffusion models are
trained on these images to learn the spatial distribution of jet constituents.
We compare the performance of score-based diffusion models and consistency
models in accurately generating class-conditional jet images. Unlike approaches
based on latent distributions, our method operates directly in image space. The
fidelity of the generated images is evaluated using several metrics, including
the Fr\'echet Inception Distance (FID), which demonstrates that consistency
models achieve higher fidelity and generation stability compared to score-based
diffusion models. These advancements offer significant improvements in
computational efficiency and generation accuracy, providing valuable tools for
High Energy Physics (HEP) research.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [155] [ContestTrade: A Multi-Agent Trading System Based on Internal Contest Mechanism](https://arxiv.org/abs/2508.00554)
*Li Zhao,Rui Sun,Zuoyou Jiang,Bo Yang,Yuxiao Bai,Mengting Chen,Xinyang Wang,Jing Li,Zuo Bai*

Main category: q-fin.TR

TL;DR: A multi-agent system with internal competition improves LLM-based trading by reducing sensitivity to market noise.


<details>
  <summary>Details</summary>
Motivation: High sensitivity to market noise limits LLM-based trading systems, prompting the need for a more robust solution.

Method: Proposes a two-team system (Data Team and Research Team) with real-time evaluation and ranking to filter top-performing agents.

Result: Outperforms existing multi-agent systems and traditional methods in trading performance.

Conclusion: The system enhances adaptability and robustness, proving effective in dynamic financial markets.

Abstract: In financial trading, large language model (LLM)-based agents demonstrate
significant potential. However, the high sensitivity to market noise undermines
the performance of LLM-based trading systems. To address this limitation, we
propose a novel multi-agent system featuring an internal competitive mechanism
inspired by modern corporate management structures. The system consists of two
specialized teams: (1) Data Team - responsible for processing and condensing
massive market data into diversified text factors, ensuring they fit the
model's constrained context. (2) Research Team - tasked with making
parallelized multipath trading decisions based on deep research methods. The
core innovation lies in implementing a real-time evaluation and ranking
mechanism within each team, driven by authentic market feedback. Each agent's
performance undergoes continuous scoring and ranking, with only outputs from
top-performing agents being adopted. The design enables the system to
adaptively adjust to dynamic environment, enhances robustness against market
noise and ultimately delivers superior trading performance. Experimental
results demonstrate that our proposed system significantly outperforms
prevailing multiagent systems and traditional quantitative investment methods
across diverse evaluation metrics.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [156] [Activation-Guided Local Editing for Jailbreaking Attacks](https://arxiv.org/abs/2508.00555)
*Jiecong Wang,Haoran Li,Hao Peng,Ziqian Zeng,Zihao Wang,Haohua Du,Zhengtao Yu*

Main category: cs.CR

TL;DR: AGILE is a two-stage jailbreak framework combining scenario-based generation and hidden-state-guided edits to improve attack success and transferability.


<details>
  <summary>Details</summary>
Motivation: Existing jailbreak methods are either incoherent (token-level) or unscalable (prompt-level), limiting their effectiveness.

Method: A two-stage approach: (1) scenario-based context generation and query rephrasing, (2) hidden-state-guided edits to shift input representation from malicious to benign.

Result: Achieves state-of-the-art Attack Success Rate (up to 37.74% improvement) and strong transferability to black-box models.

Conclusion: AGILE highlights current defense limitations and offers insights for future safeguards.

Abstract: Jailbreaking is an essential adversarial technique for red-teaming these
models to uncover and patch security flaws. However, existing jailbreak methods
face significant drawbacks. Token-level jailbreak attacks often produce
incoherent or unreadable inputs and exhibit poor transferability, while
prompt-level attacks lack scalability and rely heavily on manual effort and
human ingenuity. We propose a concise and effective two-stage framework that
combines the advantages of these approaches. The first stage performs a
scenario-based generation of context and rephrases the original malicious query
to obscure its harmful intent. The second stage then utilizes information from
the model's hidden states to guide fine-grained edits, effectively steering the
model's internal representation of the input from a malicious toward a benign
one. Extensive experiments demonstrate that this method achieves
state-of-the-art Attack Success Rate, with gains of up to 37.74% over the
strongest baseline, and exhibits excellent transferability to black-box models.
Our analysis further demonstrates that AGILE maintains substantial
effectiveness against prominent defense mechanisms, highlighting the
limitations of current safeguards and providing valuable insights for future
defense development. Our code is available at
https://github.com/yunsaijc/AGILE.

</details>


### [157] [Demo: TOSense -- What Did You Just Agree to?](https://arxiv.org/abs/2508.00659)
*Xinzhang Chen,Hassan Ali,Arash Shaghaghi,Salil S. Kanhere,Sanjay Jha*

Main category: cs.CR

TL;DR: TOSense is a Chrome extension that helps users understand Terms of Service (ToS) by answering natural language questions in real time, using a crawler and a lightweight LLM pipeline.


<details>
  <summary>Details</summary>
Motivation: To address information asymmetry and legal risks caused by lengthy and obscure ToS agreements.

Method: Combines a crawler (tos-crawl) for content extraction and a lightweight LLM pipeline (MiniLM for retrieval, BART-encoder for verification). Uses a synthetic QA evaluation pipeline (QEP) to avoid manual annotation.

Result: Achieves up to 44.5% accuracy on five major platforms (Apple, Google, X, Microsoft, Netflix).

Conclusion: TOSense effectively simplifies ToS comprehension and will be demonstrated for interactive use.

Abstract: Online services often require users to agree to lengthy and obscure Terms of
Service (ToS), leading to information asymmetry and legal risks. This paper
proposes TOSense-a Chrome extension that allows users to ask questions about
ToS in natural language and get concise answers in real time. The system
combines (i) a crawler "tos-crawl" that automatically extracts ToS content, and
(ii) a lightweight large language model pipeline: MiniLM for semantic retrieval
and BART-encoder for answer relevance verification. To avoid expensive manual
annotation, we present a novel Question Answering Evaluation Pipeline (QEP)
that generates synthetic questions and verifies the correctness of answers
using clustered topic matching. Experiments on five major platforms, Apple,
Google, X (formerly Twitter), Microsoft, and Netflix, show the effectiveness of
TOSense (with up to 44.5% accuracy) across varying number of topic clusters.
During the demonstration, we will showcase TOSense in action. Attendees will be
able to experience seamless extraction, interactive question answering, and
instant indexing of new sites.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [158] [UAV-ON: A Benchmark for Open-World Object Goal Navigation with Aerial Agents](https://arxiv.org/abs/2508.00288)
*Jianqiang Xiao,Yuexuan Sun,Yixin Shao,Boxi Gan,Rongqiang Liu,Yanjing Wu,Weili Gua,Xiang Deng*

Main category: cs.RO

TL;DR: UAV-ON is a benchmark for aerial Object Goal Navigation (ObjectNav) in open-world environments, addressing limitations of Vision-and-Language Navigation (VLN) by using high-level semantic goals.


<details>
  <summary>Details</summary>
Motivation: Existing VLN paradigms rely on sequential linguistic instructions, limiting scalability and autonomy. UAV-ON aims to enable aerial agents to navigate based on semantic goals in unstructured environments.

Method: UAV-ON includes 14 high-fidelity environments with diverse semantic regions and 1270 annotated target objects. It introduces instance-level instructions for grounded reasoning and evaluates baselines like Aerial ObjectNav Agent (AOA).

Result: Baseline methods, including AOA, struggle with the challenges of aerial navigation and semantic goal grounding, highlighting the complexity of the task.

Conclusion: UAV-ON advances research on scalable UAV autonomy by focusing on semantic goal-driven navigation in complex real-world environments.

Abstract: Aerial navigation is a fundamental yet underexplored capability in embodied
intelligence, enabling agents to operate in large-scale, unstructured
environments where traditional navigation paradigms fall short. However, most
existing research follows the Vision-and-Language Navigation (VLN) paradigm,
which heavily depends on sequential linguistic instructions, limiting its
scalability and autonomy. To address this gap, we introduce UAV-ON, a benchmark
for large-scale Object Goal Navigation (ObjectNav) by aerial agents in
open-world environments, where agents operate based on high-level semantic
goals without relying on detailed instructional guidance as in VLN. UAV-ON
comprises 14 high-fidelity Unreal Engine environments with diverse semantic
regions and complex spatial layouts, covering urban, natural, and mixed-use
settings. It defines 1270 annotated target objects, each characterized by an
instance-level instruction that encodes category, physical footprint, and
visual descriptors, allowing grounded reasoning. These instructions serve as
semantic goals, introducing realistic ambiguity and complex reasoning
challenges for aerial agents. To evaluate the benchmark, we implement several
baseline methods, including Aerial ObjectNav Agent (AOA), a modular policy that
integrates instruction semantics with egocentric observations for long-horizon,
goal-directed exploration. Empirical results show that all baselines struggle
in this setting, highlighting the compounded challenges of aerial navigation
and semantic goal grounding. UAV-ON aims to advance research on scalable UAV
autonomy driven by semantic goal descriptions in complex real-world
environments.

</details>


### [159] [Omni-Scan: Creating Visually-Accurate Digital Twin Object Models Using a Bimanual Robot with Handover and Gaussian Splat Merging](https://arxiv.org/abs/2508.00354)
*Tianshuang Qiu,Zehan Ma,Karim El-Refai,Hiya Shah,Chung Min Kim,Justin Kerr,Ken Goldberg*

Main category: cs.RO

TL;DR: Omni-Scan is a bi-manual robot pipeline for creating high-quality 3D Gaussian Splat models by rotating objects with grippers, using advanced vision models to isolate objects and train omni-directional models. It achieves 83% accuracy in defect inspection.


<details>
  <summary>Details</summary>
Motivation: Traditional 3D object scanning methods are limited by workspace constraints. Omni-Scan aims to overcome this by enabling flexible, high-quality 3D modeling using a robot setup.

Method: The pipeline uses a bi-manual robot to grasp and rotate objects, leveraging DepthAnything, Segment Anything, and RAFT optical flow to isolate objects. The 3DGS training pipeline is modified to handle gripper occlusion for 360-degree modeling.

Result: Omni-Scan successfully identifies visual or geometric defects in 12 objects with 83% average accuracy.

Conclusion: Omni-Scan provides a scalable and accurate solution for 3D object modeling and defect inspection, overcoming workspace limitations of traditional methods.

Abstract: 3D Gaussian Splats (3DGSs) are 3D object models derived from multi-view
images. Such "digital twins" are useful for simulations, virtual reality,
marketing, robot policy fine-tuning, and part inspection. 3D object scanning
usually requires multi-camera arrays, precise laser scanners, or robot
wrist-mounted cameras, which have restricted workspaces. We propose Omni-Scan,
a pipeline for producing high-quality 3D Gaussian Splat models using a
bi-manual robot that grasps an object with one gripper and rotates the object
with respect to a stationary camera. The object is then re-grasped by a second
gripper to expose surfaces that were occluded by the first gripper. We present
the Omni-Scan robot pipeline using DepthAny-thing, Segment Anything, as well as
RAFT optical flow models to identify and isolate objects held by a robot
gripper while removing the gripper and the background. We then modify the 3DGS
training pipeline to support concatenated datasets with gripper occlusion,
producing an omni-directional (360 degree view) model of the object. We apply
Omni-Scan to part defect inspection, finding that it can identify visual or
geometric defects in 12 different industrial and household objects with an
average accuracy of 83%. Interactive videos of Omni-Scan 3DGS models can be
found at https://berkeleyautomation.github.io/omni-scan/

</details>


### [160] [On-Device Diffusion Transformer Policy for Efficient Robot Manipulation](https://arxiv.org/abs/2508.00697)
*Yiming Wu,Huan Wang,Zhenghao Chen,Jianxin Pang,Dong Xu*

Main category: cs.RO

TL;DR: LightDP accelerates Diffusion Policies for mobile devices via network compression and reduced sampling steps, achieving real-time performance without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: Diffusion Policies are computationally inefficient for mobile platforms, limiting their practical deployment.

Method: LightDP uses network compression (pruning and retraining) and consistency distillation to reduce latency and sampling steps.

Result: LightDP achieves real-time performance on mobile devices with competitive accuracy across standard datasets.

Conclusion: LightDP enables practical deployment of diffusion-based policies in resource-limited environments.

Abstract: Diffusion Policies have significantly advanced robotic manipulation tasks via
imitation learning, but their application on resource-constrained mobile
platforms remains challenging due to computational inefficiency and extensive
memory footprint. In this paper, we propose LightDP, a novel framework
specifically designed to accelerate Diffusion Policies for real-time deployment
on mobile devices. LightDP addresses the computational bottleneck through two
core strategies: network compression of the denoising modules and reduction of
the required sampling steps. We first conduct an extensive computational
analysis on existing Diffusion Policy architectures, identifying the denoising
network as the primary contributor to latency. To overcome performance
degradation typically associated with conventional pruning methods, we
introduce a unified pruning and retraining pipeline, optimizing the model's
post-pruning recoverability explicitly. Furthermore, we combine pruning
techniques with consistency distillation to effectively reduce sampling steps
while maintaining action prediction accuracy. Experimental evaluations on the
standard datasets, \ie, PushT, Robomimic, CALVIN, and LIBERO, demonstrate that
LightDP achieves real-time action prediction on mobile devices with competitive
performance, marking an important step toward practical deployment of
diffusion-based policies in resource-limited environments. Extensive real-world
experiments also show the proposed LightDP can achieve performance comparable
to state-of-the-art Diffusion Policies.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [161] [STF: Shallow-Level Temporal Feedback to Enhance Spiking Transformers](https://arxiv.org/abs/2508.00387)
*Zeqi Zheng,Zizheng Zhu,Yingchao Yu,Yanchen Huang,Changze Lv,Junfeng Tang,Zhaofei Yu,Yaochu Jin*

Main category: cs.NE

TL;DR: The paper proposes Shallow-level Temporal Feedback (STF) to improve Transformer-based Spiking Neural Networks (SNNs) by addressing performance gaps caused by binary spike trains, reducing costs and latency.


<details>
  <summary>Details</summary>
Motivation: The performance gap between SNNs and ANNs due to binary spike trains, and the inefficiency of deep-level feedback loops in existing designs.

Method: Introduces STF, a lightweight module with Temporal-Spatial Position Embedding (TSPE) and Temporal Feedback (TF) for the encoding layer.

Result: STF improves performance on static datasets (CIFAR-10, CIFAR-100, ImageNet-1K) and enhances spike pattern diversity. It also shows better adversarial robustness and temporal sensitivity.

Conclusion: STF is a promising spike encoding scheme for static scenarios, offering efficiency and performance gains.

Abstract: Transformer-based Spiking Neural Networks (SNNs) suffer from a great
performance gap compared to floating-point Artificial Neural Networks (ANNs)
due to the binary nature of spike trains. Recent efforts have introduced
deep-level feedback loops to transmit high-level semantic information to narrow
this gap. However, these designs often span multiple deep layers, resulting in
costly feature transformations, higher parameter overhead, increased energy
consumption, and longer inference latency. To address this issue, we propose
Shallow-level Temporal Feedback (STF), a lightweight plug-and-play module for
the encoding layer, which consists of Temporal-Spatial Position Embedding
(TSPE) and Temporal Feedback (TF).Extensive experiments show that STF
consistently improves performance across various Transformer-based SNN
backbones on static datasets, including CIFAR-10, CIFAR-100, and ImageNet-1K,
under different spike timestep settings. Further analysis reveals that STF
enhances the diversity of the spike patterns, which is key to performance gain.
Moreover, evaluations on adversarial robustness and temporal sensitivity
confirm that STF outperforms direct coding and its variants, highlighting its
potential as a new spike encoding scheme for static scenarios. Our code will be
released upon acceptance.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [162] [GPT-4.1 Sets the Standard in Automated Experiment Design Using Novel Python Libraries](https://arxiv.org/abs/2508.00033)
*Nuno Fachada,Daniel Fernandes,Carlos M. Fernandes,Bruno D. Ferreira-Saraiva,Joo P. Matos-Carvalho*

Main category: cs.SE

TL;DR: The study benchmarks LLMs for generating functional Python code using unfamiliar APIs, finding GPT-4.1 as the only consistently successful model, while highlighting limitations in LLMs and library documentation.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' ability to generate functional Python code for complex tasks using unfamiliar APIs, addressing gaps in understanding their practical utility in scientific automation.

Method: Systematically benchmarks LLMs using zero-shot prompts for two tasks: conversational data analysis with ParShift and synthetic data generation/clustering with pyclugen and scikit-learn. Evaluates correctness and errors.

Result: Only a small subset of models, notably GPT-4.1, consistently produced correct, executable code. Errors revealed issues in library documentation and implementation.

Conclusion: LLMs have limitations for end-to-end scientific automation, requiring better prompt design, library documentation, and model advancements.

Abstract: Large Language Models (LLMs) have advanced rapidly as tools for automating
code generation in scientific research, yet their ability to interpret and use
unfamiliar Python APIs for complex computational experiments remains poorly
characterized. This study systematically benchmarks a selection of
state-of-the-art LLMs in generating functional Python code for two increasingly
challenging scenarios: conversational data analysis with the \textit{ParShift}
library, and synthetic data generation and clustering using \textit{pyclugen}
and \textit{scikit-learn}. Both experiments use structured, zero-shot prompts
specifying detailed requirements but omitting in-context examples. Model
outputs are evaluated quantitatively for functional correctness and prompt
compliance over multiple runs, and qualitatively by analyzing the errors
produced when code execution fails. Results show that only a small subset of
models consistently generate correct, executable code, with GPT-4.1 standing
out as the only model to always succeed in both tasks. In addition to
benchmarking LLM performance, this approach helps identify shortcomings in
third-party libraries, such as unclear documentation or obscure implementation
bugs. Overall, these findings highlight current limitations of LLMs for
end-to-end scientific automation and emphasize the need for careful prompt
design, comprehensive library documentation, and continued advances in language
model capabilities.

</details>


### [163] [A Survey on Code Generation with LLM-based Agents](https://arxiv.org/abs/2508.00083)
*Yihong Dong,Xue Jiang,Jiaru Qian,Tian Wang,Kechi Zhang,Zhi Jin,Ge Li*

Main category: cs.SE

TL;DR: A survey of LLM-based code generation agents, highlighting their autonomy, expanded task scope, and engineering practicality, along with their development trajectory, techniques, applications, benchmarks, and future challenges.


<details>
  <summary>Details</summary>
Motivation: To systematically review the emerging field of LLM-based code generation agents, which are transforming software development with their autonomous capabilities and broader SDLC scope.

Method: The paper traces the technology's development, categorizes core techniques (single/multi-agent architectures), details SDLC applications, summarizes benchmarks, and catalogs tools.

Result: The survey provides a comprehensive overview of the field, including key techniques, applications, and evaluation methods, while identifying current challenges.

Conclusion: The paper proposes foundational research directions to address challenges and advance the field of LLM-based code generation agents.

Abstract: Code generation agents powered by large language models (LLMs) are
revolutionizing the software development paradigm. Distinct from previous code
generation techniques, code generation agents are characterized by three core
features. 1) Autonomy: the ability to independently manage the entire workflow,
from task decomposition to coding and debugging. 2) Expanded task scope:
capabilities that extend beyond generating code snippets to encompass the full
software development lifecycle (SDLC). 3) Enhancement of engineering
practicality: a shift in research emphasis from algorithmic innovation toward
practical engineering challenges, such as system reliability, process
management, and tool integration. This domain has recently witnessed rapid
development and an explosion in research, demonstrating significant application
potential. This paper presents a systematic survey of the field of LLM-based
code generation agents. We trace the technology's developmental trajectory from
its inception and systematically categorize its core techniques, including both
single-agent and multi-agent architectures. Furthermore, this survey details
the applications of LLM-based agents across the full SDLC, summarizes
mainstream evaluation benchmarks and metrics, and catalogs representative
tools. Finally, by analyzing the primary challenges, we identify and propose
several foundational, long-term research directions for the future work of the
field.

</details>


### [164] [Benchmarking LLMs for Unit Test Generation from Real-World Functions](https://arxiv.org/abs/2508.00408)
*Dong Huang,Jie M. Zhang,Mark Harman,Qianru Zhang,Mingzhe Du,See-Kiong Ng*

Main category: cs.SE

TL;DR: The paper introduces ULT, a new benchmark for evaluating LLMs in unit test generation, addressing data contamination and structural simplicity issues in existing benchmarks. Results show ULT is more challenging than others.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for LLM test generation suffer from data contamination and overly simple function code, leading to unreliable conclusions.

Method: ULT is created through a multi-stage curation process for real-world Python functions, ensuring high complexity and mitigating contamination. PLT is also introduced for controlled analysis.

Result: ULT proves significantly harder for LLMs, with lower accuracy and coverage metrics compared to TestEval and PLT.

Conclusion: ULT provides a more realistic and challenging benchmark for evaluating LLMs in test generation, addressing flaws in existing benchmarks.

Abstract: Recently, large language models (LLMs) have shown great promise in automating
unit test generation, significantly reducing the manual effort required by
developers. To effectively evaluate the capabilities of LLMs in this domain, it
is crucial to have a well-designed benchmark that accurately reflects
real-world scenarios and mitigates common pitfalls. Existing LLM test
generation benchmarks are limited by two critical drawbacks: data contamination
and structurally simple function code. As a result, we often cannot rely on the
validity of scientific conclusions drawn from empirical studies using these
limited benchmarks. The empirical evidence presented may be biased due to
contamination and may fail to generalize beyond toy programs due to structural
simplicity.
  To address these problems, we introduce ULT (UnLeakedTestbench), a new
benchmark specifically designed for function-level unit test generation from
real-world Python functions. ULT is constructed through a multi-stage curation
process that ensures high cyclomatic complexity and mitigates test case
contamination. With 3,909 carefully selected function-level tasks, ULT provides
a more realistic and challenging evaluation of LLMs' test generation
capabilities. We also provide PLT (PreLeakedTestbench), a pair benchmark of ULT
with leaked tests designed to enable a controlled analysis of memorization
versus reasoning in test generation. Our evaluation results demonstrate that
ULT is significantly more challenging. For example, test cases generated by
LLMs only achieve 41.32\%, 45.10\%, 30.22\%, and 40.21\% for accuracy,
statement coverage, branch coverage, and mutation score on average for all
LLMs, respectively. These results are substantially lower than the
corresponding metrics on TestEval (91.79\%, 92.18\%, 82.04\%, and 49.69\%) and
PLT (47.07\%, 55.13\%, 40.07\%, and 50.80\%).

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [165] [AudioGen-Omni: A Unified Multimodal Diffusion Transformer for Video-Synchronized Audio, Speech, and Song Generation](https://arxiv.org/abs/2508.00733)
*Le Wang,Jun Wang,Feng Deng,Chen Zhang,Kun Gai,Di Zhang*

Main category: cs.SD

TL;DR: AudioGen-Omni is a unified multimodal diffusion transformer model for generating high-fidelity audio, speech, and songs synchronized with video, achieving state-of-the-art performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: To create a versatile model capable of generating diverse, semantically rich audio synchronized with video, overcoming limitations of text-frozen paradigms.

Method: Uses a joint training paradigm with multimodal diffusion transformers (MMDit), a lyrics-transcription encoder, and AdaLN-based joint attention with PAAPI for cross-modal alignment.

Result: Achieves state-of-the-art results in Text-to-Audio/Speech/Song tasks with high semantic alignment, lip-sync accuracy, and fast inference (1.91s for 8s audio).

Conclusion: AudioGen-Omni is a robust, efficient, and generalizable solution for multimodal audio generation tasks.

Abstract: We present AudioGen-Omni - a unified approach based on multimodal diffusion
transformers (MMDit), capable of generating high-fidelity audio, speech, and
songs coherently synchronized with the input video. AudioGen-Omni introduces a
novel joint training paradigm that seamlessly integrates large-scale
video-text-audio corpora, enabling a model capable of generating semantically
rich, acoustically diverse audio conditioned on multimodal inputs and adaptable
to a wide range of audio generation tasks. AudioGen-Omni employs a unified
lyrics-transcription encoder that encodes graphemes and phonemes from both sung
and spoken inputs into dense frame-level representations. Dense frame-level
representations are fused using an AdaLN-based joint attention mechanism
enhanced with phase-aligned anisotropic positional infusion (PAAPI), wherein
RoPE is selectively applied to temporally structured modalities to ensure
precise and robust cross-modal alignment. By unfreezing all modalities and
masking missing inputs, AudioGen-Omni mitigates the semantic constraints of
text-frozen paradigms, enabling effective cross-modal conditioning. This joint
training approach enhances audio quality, semantic alignment, and lip-sync
accuracy, while also achieving state-of-the-art results on
Text-to-Audio/Speech/Song tasks. With an inference time of 1.91 seconds for 8
seconds of audio, it offers substantial improvements in both efficiency and
generality.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [166] [Scalable Spectrum Availability Prediction using a Markov Chain Framework and ITU-R Propagation Models](https://arxiv.org/abs/2508.00028)
*Abir Ray*

Main category: cs.NI

TL;DR: A scalable framework for predicting spectrum availability using Markov chains and ITU-R propagation models, improving accuracy for dynamic spectrum access.


<details>
  <summary>Details</summary>
Motivation: Spectrum resources are often underutilized, and predicting availability is key for interference-free secondary use.

Method: Combines a two-state Markov chain for temporal patterns with ITU-R propagation models (P.528, P.2108) for spatial analysis.

Result: Effectively identifies available spectrum with low computational cost, suitable for real-time management.

Conclusion: The framework is flexible, accurate, and scalable for dynamic spectrum sharing systems.

Abstract: Spectrum resources are often underutilized across time and space, motivating
dynamic spectrum access strategies that allow secondary users to exploit unused
frequencies. A key challenge is predicting when and where spectrum will be
available (i.e., unused by primary licensed users) in order to enable proactive
and interference-free access. This paper proposes a scalable framework for
spectrum availability prediction that combines a two-state Markov chain model
of primary user activity with high-fidelity propagation models from the ITU-R
(specifically Recommendations P.528 and P.2108). The Markov chain captures
temporal occupancy patterns, while the propagation models incorporate path loss
and clutter effects to determine if primary signals exceed interference
thresholds at secondary user locations. By integrating these components, the
proposed method can predict spectrum opportunities both in time and space with
improved accuracy. We develop the system model and algorithm for the approach,
analyze its scalability and computational efficiency, and discuss assumptions,
limitations, and potential applications. The framework is flexible and can be
adapted to various frequency bands and scenarios. The results and analysis show
that the proposed approach can effectively identify available spectrum with low
computational cost, making it suitable for real-time spectrum management in
cognitive radio networks and other dynamic spectrum sharing systems.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [167] [Towards a unified framework for programming paradigms: A systematic review of classification formalisms and methodological foundations](https://arxiv.org/abs/2508.00534)
*Mikel Vandeloise*

Main category: cs.PL

TL;DR: The paper reviews formal foundations of programming paradigms, highlighting limitations in current taxonomies and advocating for a compositional approach using mathematical frameworks like Type theory and Category theory.


<details>
  <summary>Details</summary>
Motivation: The rise of multi-paradigm languages challenges traditional classification methods, causing interoperability issues, necessitating a formal, reconstructive approach.

Method: A systematic literature review (SLR) of 74 primary studies to assess classification formalisms and identify conceptual primitives and mathematical frameworks.

Result: Existing taxonomies lack granularity and a unified basis. A compositional approach with atomic primitives and mathematical frameworks (Type theory, Category theory, UTP) is proposed.

Conclusion: The literature shows a shift from classification to formal reconstructive frameworks, with a proposed research agenda for unification.

Abstract: The rise of multi-paradigm languages challenges traditional classification
methods, leading to practical software engineering issues like interoperability
defects. This systematic literature review (SLR) maps the formal foundations of
programming paradigms. Our objective is twofold: (1) to assess the state of the
art of classification formalisms and their limitations, and (2) to identify the
conceptual primitives and mathematical frameworks for a more powerful,
reconstructive approach.
  Based on a synthesis of 74 primary studies, we find that existing taxonomies
lack conceptual granularity, a unified formal basis, and struggle with hybrid
languages. In response, our analysis reveals a strong convergence toward a
compositional reconstruction of paradigms. This approach identifies a minimal
set of orthogonal, atomic primitives and leverages mathematical frameworks,
predominantly Type theory, Category theory and Unifying Theories of Programming
(UTP), to formally guarantee their compositional properties.
  We conclude that the literature reflects a significant intellectual shift
away from classification towards these promising formal, reconstructive
frameworks. This review provides a map of this evolution and proposes a
research agenda for their unification.

</details>

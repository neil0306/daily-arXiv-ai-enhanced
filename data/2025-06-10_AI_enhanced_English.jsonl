{"id": "2506.06331", "pdf": "https://arxiv.org/pdf/2506.06331", "abs": "https://arxiv.org/abs/2506.06331", "authors": ["Qiming Zeng", "Xiao Yan", "Hao Luo", "Yuhao Lin", "Yuxiang Wang", "Fangcheng Fu", "Bo Du", "Quanqing Xu", "Jiawei Jiang"], "title": "How Significant Are the Real Performance Gains? An Unbiased Evaluation Framework for GraphRAG", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "By retrieving contexts from knowledge graphs, graph-based retrieval-augmented\ngeneration (GraphRAG) enhances large language models (LLMs) to generate quality\nanswers for user questions. Many GraphRAG methods have been proposed and\nreported inspiring performance in answer quality. However, we observe that the\ncurrent answer evaluation framework for GraphRAG has two critical flaws, i.e.,\nunrelated questions and evaluation biases, which may lead to biased or even\nwrong conclusions on performance. To tackle the two flaws, we propose an\nunbiased evaluation framework that uses graph-text-grounded question generation\nto produce questions that are more related to the underlying dataset and an\nunbiased evaluation procedure to eliminate the biases in LLM-based answer\nassessment. We apply our unbiased framework to evaluate 3 representative\nGraphRAG methods and find that their performance gains are much more moderate\nthan reported previously. Although our evaluation framework may still have\nflaws, it calls for scientific evaluations to lay solid foundations for\nGraphRAG research.", "AI": {"tldr": "The paper identifies flaws in current GraphRAG evaluation frameworks and proposes an unbiased method to assess performance more accurately, revealing more moderate gains than previously reported.", "motivation": "Current GraphRAG evaluation frameworks suffer from unrelated questions and biases, leading to misleading conclusions about performance.", "method": "Proposes an unbiased evaluation framework using graph-text-grounded question generation and unbiased LLM-based answer assessment.", "result": "Evaluation of 3 GraphRAG methods shows more moderate performance gains than prior reports.", "conclusion": "The framework highlights the need for scientific evaluations to advance GraphRAG research, despite potential remaining flaws."}}
{"id": "2506.06343", "pdf": "https://arxiv.org/pdf/2506.06343", "abs": "https://arxiv.org/abs/2506.06343", "authors": ["Taesoo Kim", "Jong Hwan Ko"], "title": "TESU-LLM: Training Speech-LLMs Without Speech via Unified Encoder Alignment", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "comment": null, "summary": "Recent advances in speech-enabled language models have shown promising\nresults in building intelligent voice assistants. However, most existing\napproaches rely on large-scale paired speech-text data and extensive\ncomputational resources, which pose challenges in terms of scalability and\naccessibility. In this paper, we present \\textbf{TESU-LLM}, a novel framework\nthat enables training speech-capable language models using only text data. Our\nkey insight is to leverage a unified encoder that maps semantically equivalent\ntext and speech inputs to a shared latent space. By aligning the encoder output\nwith the embedding space of a LLM via a lightweight projection network, we\nenable the model to generalize from text-only supervision to speech-based\ninference. Despite being trained exclusively on text, TESU-LLM achieves strong\nperformance on various speech-related benchmarks, comparable to baseline\nmethods trained with large-scale multimodal datasets and substantial\ncomputational resources. These results highlight the effectiveness and\nefficiency of our approach, offering a scalable path toward building speech\nLLMs without speech data.", "AI": {"tldr": "TESU-LLM trains speech-capable language models using only text data by aligning text and speech in a shared latent space, achieving strong performance without speech data.", "motivation": "Existing speech-enabled models require large-scale paired speech-text data and computational resources, limiting scalability and accessibility.", "method": "Leverages a unified encoder to map text and speech to a shared latent space, aligning it with LLM embeddings via a lightweight projection network.", "result": "Achieves performance comparable to models trained with multimodal data, despite using only text.", "conclusion": "TESU-LLM offers a scalable and efficient approach to building speech LLMs without speech data."}}
{"id": "2506.06347", "pdf": "https://arxiv.org/pdf/2506.06347", "abs": "https://arxiv.org/abs/2506.06347", "authors": ["Zachary Yang", "Domenico Tullo", "Reihaneh Rabbany"], "title": "Unified Game Moderation: Soft-Prompting and LLM-Assisted Label Transfer for Resource-Efficient Toxicity Detection", "categories": ["cs.CL", "cs.AI", "I.2.7; J.4"], "comment": "11 pages, 1 figure, 9 Tables, KDD 2025 ADS Track", "summary": "Toxicity detection in gaming communities faces significant scaling challenges\nwhen expanding across multiple games and languages, particularly in real-time\nenvironments where computational efficiency is crucial. We present two key\nfindings to address these challenges while building upon our previous work on\nToxBuster, a BERT-based real-time toxicity detection system. First, we\nintroduce a soft-prompting approach that enables a single model to effectively\nhandle multiple games by incorporating game-context tokens, matching the\nperformance of more complex methods like curriculum learning while offering\nsuperior scalability. Second, we develop an LLM-assisted label transfer\nframework using GPT-4o-mini to extend support to seven additional languages.\nEvaluations on real game chat data across French, German, Portuguese, and\nRussian achieve macro F1-scores ranging from 32.96% to 58.88%, with\nparticularly strong performance in German, surpassing the English benchmark of\n45.39%. In production, this unified approach significantly reduces\ncomputational resources and maintenance overhead compared to maintaining\nseparate models for each game and language combination. At Ubisoft, this model\nsuccessfully identifies an average of 50 players, per game, per day engaging in\nsanctionable behavior.", "AI": {"tldr": "The paper introduces a scalable, efficient toxicity detection system for gaming communities, using soft-prompting for multi-game support and LLM-assisted label transfer for multilingual capabilities.", "motivation": "Addressing scalability and efficiency challenges in real-time toxicity detection across multiple games and languages.", "method": "Soft-prompting for multi-game handling and GPT-4o-mini-assisted label transfer for multilingual support.", "result": "Achieves macro F1-scores of 32.96% to 58.88% in non-English languages, outperforming English benchmarks. Reduces computational resources in production.", "conclusion": "The unified approach is effective, scalable, and reduces overhead, successfully identifying toxic behavior in gaming communities."}}
{"id": "2506.06371", "pdf": "https://arxiv.org/pdf/2506.06371", "abs": "https://arxiv.org/abs/2506.06371", "authors": ["Panagiotis Koletsis", "Christos Panagiotopoulos", "Georgios Th. Papadopoulos", "Vasilis Efthymiou"], "title": "Relationship Detection on Tabular Data Using Statistical Analysis and Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Over the past few years, table interpretation tasks have made significant\nprogress due to their importance and the introduction of new technologies and\nbenchmarks in the field. This work experiments with a hybrid approach for\ndetecting relationships among columns of unlabeled tabular data, using a\nKnowledge Graph (KG) as a reference point, a task known as CPA. This approach\nleverages large language models (LLMs) while employing statistical analysis to\nreduce the search space of potential KG relations. The main modules of this\napproach for reducing the search space are domain and range constraints\ndetection, as well as relation co-appearance analysis. The experimental\nevaluation on two benchmark datasets provided by the SemTab challenge assesses\nthe influence of each module and the effectiveness of different\nstate-of-the-art LLMs at various levels of quantization. The experiments were\nperformed, as well as at different prompting techniques. The proposed\nmethodology, which is publicly available on github, proved to be competitive\nwith state-of-the-art approaches on these datasets.", "AI": {"tldr": "A hybrid approach combining LLMs and statistical analysis for detecting column relationships in unlabeled tabular data, using a KG, achieves competitive results on benchmark datasets.", "motivation": "Advance table interpretation tasks by leveraging new technologies and benchmarks, specifically for CPA.", "method": "Uses LLMs and statistical analysis (domain/range constraints, relation co-appearance) to reduce KG relation search space.", "result": "Competitive performance on SemTab benchmark datasets, tested with various LLMs and prompting techniques.", "conclusion": "The hybrid approach is effective and publicly available, matching state-of-the-art methods."}}
{"id": "2506.06283", "pdf": "https://arxiv.org/pdf/2506.06283", "abs": "https://arxiv.org/abs/2506.06283", "authors": ["Juexiao Zhou", "Zhongyi Han", "Mankun Xin", "Xingwei He", "Guotao Wang", "Jiaoyan Song", "Gongning Luo", "Wenjia He", "Xintong Li", "Yuetan Chu", "Juanwen Chen", "Bo Wang", "Xia Wu", "Wenwen Duan", "Zhixia Guo", "Liyan Bai", "Yilin Pan", "Xuefei Bi", "Lu Liu", "Long Feng", "Xiaonan He", "Xin Gao"], "title": "Facial Foundational Model Advances Early Warning of Coronary Artery Disease from Live Videos with DigitalShadow", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Global population aging presents increasing challenges to healthcare systems,\nwith coronary artery disease (CAD) responsible for approximately 17.8 million\ndeaths annually, making it a leading cause of global mortality. As CAD is\nlargely preventable, early detection and proactive management are essential. In\nthis work, we introduce DigitalShadow, an advanced early warning system for\nCAD, powered by a fine-tuned facial foundation model. The system is pre-trained\non 21 million facial images and subsequently fine-tuned into LiveCAD, a\nspecialized CAD risk assessment model trained on 7,004 facial images from 1,751\nsubjects across four hospitals in China. DigitalShadow functions passively and\ncontactlessly, extracting facial features from live video streams without\nrequiring active user engagement. Integrated with a personalized database, it\ngenerates natural language risk reports and individualized health\nrecommendations. With privacy as a core design principle, DigitalShadow\nsupports local deployment to ensure secure handling of user data.", "AI": {"tldr": "DigitalShadow is an AI-powered early warning system for CAD, using facial analysis to assess risk passively and contactlessly.", "motivation": "Addressing the global burden of CAD by enabling early detection and proactive management.", "method": "Leverages a fine-tuned facial foundation model (LiveCAD) trained on 7,004 facial images from 1,751 subjects, integrated with live video streams for passive risk assessment.", "result": "Generates natural language risk reports and personalized health recommendations while ensuring privacy through local deployment.", "conclusion": "DigitalShadow offers a scalable, privacy-focused solution for early CAD detection and management."}}
{"id": "2506.06376", "pdf": "https://arxiv.org/pdf/2506.06376", "abs": "https://arxiv.org/abs/2506.06376", "authors": ["Heng Dong", "Kefei Duan", "Chongjie Zhang"], "title": "Enhancing Decision-Making of Large Language Models via Actor-Critic", "categories": ["cs.CL", "cs.AI"], "comment": "Forty-second International Conference on Machine Learning (ICML 2025)", "summary": "Large Language Models (LLMs) have achieved remarkable advancements in natural\nlanguage processing tasks, yet they encounter challenges in complex\ndecision-making scenarios that require long-term reasoning and alignment with\nhigh-level objectives. Existing methods either rely on short-term\nauto-regressive action generation or face limitations in accurately simulating\nrollouts and assessing outcomes, leading to sub-optimal decisions. This paper\nintroduces a novel LLM-based Actor-Critic framework, termed LAC, that\neffectively improves LLM policies with long-term action evaluations in a\nprincipled and scalable way. Our approach addresses two key challenges: (1)\nextracting robust action evaluations by computing Q-values via token logits\nassociated with positive/negative outcomes, enhanced by future trajectory\nrollouts and reasoning; and (2) enabling efficient policy improvement through a\ngradient-free mechanism. Experiments across diverse environments -- including\nhigh-level decision-making (ALFWorld), low-level action spaces (BabyAI-Text),\nand large action spaces (WebShop) -- demonstrate the framework's generality and\nsuperiority over state-of-the-art methods. Notably, our approach achieves\ncompetitive performance using 7B/8B parameter LLMs, even outperforming baseline\nmethods employing GPT-4 in complex tasks. These results underscore the\npotential of integrating structured policy optimization with LLMs' intrinsic\nknowledge to advance decision-making capabilities in multi-step environments.", "AI": {"tldr": "The paper introduces LAC, an LLM-based Actor-Critic framework, to improve long-term decision-making in LLMs by addressing challenges in action evaluation and policy improvement.", "motivation": "LLMs struggle with complex decision-making requiring long-term reasoning and alignment with objectives. Existing methods are limited in simulating rollouts and assessing outcomes.", "method": "LAC uses token logits for Q-values, enhanced by future trajectory rollouts, and employs a gradient-free mechanism for policy improvement.", "result": "Experiments in diverse environments (ALFWorld, BabyAI-Text, WebShop) show LAC outperforms state-of-the-art methods, even with smaller LLMs.", "conclusion": "Integrating structured policy optimization with LLMs' knowledge advances decision-making in multi-step environments."}}
{"id": "2506.06389", "pdf": "https://arxiv.org/pdf/2506.06389", "abs": "https://arxiv.org/abs/2506.06389", "authors": ["Rifat Sadik", "Tanvir Rahman", "Arpan Bhattacharjee", "Bikash Chandra Halder", "Ismail Hossain"], "title": "Exploring Adversarial Watermarking in Transformer-Based Models: Transferability and Robustness Against Defense Mechanism for Medical Images", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Deep learning models have shown remarkable success in dermatological image\nanalysis, offering potential for automated skin disease diagnosis. Previously,\nconvolutional neural network(CNN) based architectures have achieved immense\npopularity and success in computer vision (CV) based task like skin image\nrecognition, generation and video analysis. But with the emergence of\ntransformer based models, CV tasks are now are nowadays carrying out using\nthese models. Vision Transformers (ViTs) is such a transformer-based models\nthat have shown success in computer vision. It uses self-attention mechanisms\nto achieve state-of-the-art performance across various tasks. However, their\nreliance on global attention mechanisms makes them susceptible to adversarial\nperturbations. This paper aims to investigate the susceptibility of ViTs for\nmedical images to adversarial watermarking-a method that adds so-called\nimperceptible perturbations in order to fool models. By generating adversarial\nwatermarks through Projected Gradient Descent (PGD), we examine the\ntransferability of such attacks to CNNs and analyze the performance defense\nmechanism -- adversarial training. Results indicate that while performance is\nnot compromised for clean images, ViTs certainly become much more vulnerable to\nadversarial attacks: an accuracy drop of as low as 27.6%. Nevertheless,\nadversarial training raises it up to 90.0%.", "AI": {"tldr": "The paper investigates the vulnerability of Vision Transformers (ViTs) to adversarial watermarking in dermatological image analysis, comparing it to CNNs and evaluating adversarial training as a defense.", "motivation": "With the rise of transformer-based models like ViTs in computer vision, their susceptibility to adversarial attacks, especially in medical imaging, remains underexplored.", "method": "Adversarial watermarks are generated using Projected Gradient Descent (PGD) to attack ViTs, and their transferability to CNNs is tested. Adversarial training is evaluated as a defense.", "result": "ViTs show significant vulnerability to adversarial attacks (accuracy drops to 27.6%), but adversarial training improves robustness (accuracy rises to 90.0%).", "conclusion": "While ViTs are highly susceptible to adversarial attacks in medical imaging, adversarial training effectively mitigates this vulnerability."}}
{"id": "2506.06384", "pdf": "https://arxiv.org/pdf/2506.06384", "abs": "https://arxiv.org/abs/2506.06384", "authors": ["Yi Ji", "Runzhi Li", "Baolei Mao"], "title": "Detection Method for Prompt Injection by Integrating Pre-trained Model and Heuristic Feature Engineering", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by KSEM2025 AI & Sec Workshop", "summary": "With the widespread adoption of Large Language Models (LLMs), prompt\ninjection attacks have emerged as a significant security threat. Existing\ndefense mechanisms often face critical trade-offs between effectiveness and\ngeneralizability. This highlights the urgent need for efficient prompt\ninjection detection methods that are applicable across a wide range of LLMs. To\naddress this challenge, we propose DMPI-PMHFE, a dual-channel feature fusion\ndetection framework. It integrates a pretrained language model with heuristic\nfeature engineering to detect prompt injection attacks. Specifically, the\nframework employs DeBERTa-v3-base as a feature extractor to transform input\ntext into semantic vectors enriched with contextual information. In parallel,\nwe design heuristic rules based on known attack patterns to extract explicit\nstructural features commonly observed in attacks. Features from both channels\nare subsequently fused and passed through a fully connected neural network to\nproduce the final prediction. This dual-channel approach mitigates the\nlimitations of relying only on DeBERTa to extract features. Experimental\nresults on diverse benchmark datasets demonstrate that DMPI-PMHFE outperforms\nexisting methods in terms of accuracy, recall, and F1-score. Furthermore, when\ndeployed actually, it significantly reduces attack success rates across\nmainstream LLMs, including GLM-4, LLaMA 3, Qwen 2.5, and GPT-4o.", "AI": {"tldr": "DMPI-PMHFE is a dual-channel framework combining DeBERTa-v3-base and heuristic rules to detect prompt injection attacks in LLMs, outperforming existing methods in accuracy and reducing attack success rates.", "motivation": "Prompt injection attacks are a growing threat to LLMs, and current defenses lack effectiveness and generalizability.", "method": "Uses DeBERTa-v3-base for semantic feature extraction and heuristic rules for structural features, fusing them via a neural network.", "result": "Outperforms benchmarks in accuracy, recall, and F1-score, and reduces attack success rates in real-world LLMs.", "conclusion": "DMPI-PMHFE offers a robust, generalizable solution for prompt injection detection in LLMs."}}
{"id": "2506.06480", "pdf": "https://arxiv.org/pdf/2506.06480", "abs": "https://arxiv.org/abs/2506.06480", "authors": ["A. Postlmayr", "P. Cosman", "S. Dey"], "title": "(LiFT) Lightweight Fitness Transformer: A language-vision model for Remote Monitoring of Physical Training", "categories": ["cs.CV"], "comment": null, "summary": "We introduce a fitness tracking system that enables remote monitoring for\nexercises using only a RGB smartphone camera, making fitness tracking more\nprivate, scalable, and cost effective. Although prior work explored automated\nexercise supervision, existing models are either too limited in exercise\nvariety or too complex for real-world deployment. Prior approaches typically\nfocus on a small set of exercises and fail to generalize across diverse\nmovements. In contrast, we develop a robust, multitask motion analysis model\ncapable of performing exercise detection and repetition counting across\nhundreds of exercises, a scale far beyond previous methods. We overcome\nprevious data limitations by assembling a large-scale fitness dataset, Olympia\ncovering more than 1,900 exercises. To our knowledge, our vision-language model\nis the first that can perform multiple tasks on skeletal fitness data. On\nOlympia, our model can detect exercises with 76.5% accuracy and count\nrepetitions with 85.3% off-by-one accuracy, using only RGB video. By presenting\na single vision-language transformer model for both exercise identification and\nrep counting, we take a significant step toward democratizing AI-powered\nfitness tracking.", "AI": {"tldr": "A fitness tracking system using RGB smartphone cameras for remote exercise monitoring, offering privacy, scalability, and cost-effectiveness. It outperforms prior methods with a multitask model handling 1,900+ exercises.", "motivation": "Existing fitness tracking models are limited in exercise variety or too complex for real-world use. This work aims to democratize AI-powered fitness tracking by addressing these gaps.", "method": "Developed a robust multitask motion analysis model using a large-scale dataset (Olympia) with 1,900+ exercises. Uses a vision-language transformer for exercise detection and repetition counting.", "result": "Achieves 76.5% accuracy in exercise detection and 85.3% off-by-one accuracy in repetition counting using RGB video.", "conclusion": "The system advances fitness tracking by combining exercise identification and rep counting in a single model, making AI-powered tracking more accessible."}}
{"id": "2506.06395", "pdf": "https://arxiv.org/pdf/2506.06395", "abs": "https://arxiv.org/abs/2506.06395", "authors": ["Pengyi Li", "Matvey Skripkin", "Alexander Zubrey", "Andrey Kuznetsov", "Ivan Oseledets"], "title": "Confidence Is All You Need: Few-Shot RL Fine-Tuning of Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) excel at reasoning, yet post-training remains\ncritical for aligning their behavior with task goals. Existing reinforcement\nlearning (RL) methods often depend on costly human annotations or external\nreward models. We propose Reinforcement Learning via Self-Confidence (RLSC),\nwhich uses the model's own confidence as reward signals-eliminating the need\nfor labels, preference models, or reward engineering. Applied to\nQwen2.5-Math-7B with only 8 samples per question and 4 training epochs, RLSC\nimproves accuracy by +20.10% on AIME2024, +49.40% on MATH500, and +52.50% on\nAMC23. RLSC offers a simple, scalable post-training method for reasoning models\nwith minimal supervision.", "AI": {"tldr": "RLSC uses a model's self-confidence as reward signals for post-training, improving accuracy on math tasks without human annotations.", "motivation": "Existing RL methods for LLMs rely on costly human annotations or external rewards; RLSC aims to eliminate this dependency.", "method": "RLSC leverages the model's own confidence as reward signals, requiring minimal supervision (8 samples per question, 4 epochs).", "result": "RLSC boosts accuracy by +20.10% (AIME2024), +49.40% (MATH500), and +52.50% (AMC23).", "conclusion": "RLSC provides a simple, scalable post-training method for reasoning models with minimal supervision."}}
{"id": "2506.06517", "pdf": "https://arxiv.org/pdf/2506.06517", "abs": "https://arxiv.org/abs/2506.06517", "authors": ["Mingqi Jiang", "Chanho Kim", "Chen Ziwen", "Li Fuxin"], "title": "GS4: Generalizable Sparse Splatting Semantic SLAM", "categories": ["cs.CV"], "comment": "13 pages, 6 figures", "summary": "Traditional SLAM algorithms are excellent at camera tracking but might\ngenerate lower resolution and incomplete 3D maps. Recently, Gaussian Splatting\n(GS) approaches have emerged as an option for SLAM with accurate, dense 3D map\nbuilding. However, existing GS-based SLAM methods rely on per-scene\noptimization which is time-consuming and does not generalize to diverse scenes\nwell. In this work, we introduce the first generalizable GS-based semantic SLAM\nalgorithm that incrementally builds and updates a 3D scene representation from\nan RGB-D video stream using a learned generalizable network. Our approach\nstarts from an RGB-D image recognition backbone to predict the Gaussian\nparameters from every downsampled and backprojected image location.\nAdditionally, we seamlessly integrate 3D semantic segmentation into our GS\nframework, bridging 3D mapping and recognition through a shared backbone. To\ncorrect localization drifting and floaters, we propose to optimize the GS for\nonly 1 iteration following global localization. We demonstrate state-of-the-art\nsemantic SLAM performance on the real-world benchmark ScanNet with an order of\nmagnitude fewer Gaussians compared to other recent GS-based methods, and\nshowcase our model's generalization capability through zero-shot transfer to\nthe NYUv2 and TUM RGB-D datasets.", "AI": {"tldr": "A generalizable Gaussian Splatting (GS)-based semantic SLAM algorithm is introduced, improving 3D map resolution and semantic segmentation with efficient optimization and zero-shot transfer capability.", "motivation": "Traditional SLAM lacks high-resolution 3D maps, while GS-based SLAM suffers from slow per-scene optimization and poor generalization. This work aims to address these limitations.", "method": "Uses a learned network to predict Gaussian parameters from RGB-D images, integrates semantic segmentation, and optimizes GS with 1 iteration post-global localization.", "result": "Achieves state-of-the-art semantic SLAM performance on ScanNet, uses fewer Gaussians, and generalizes to NYUv2 and TUM RGB-D datasets.", "conclusion": "The proposed method efficiently combines 3D mapping and recognition, offering high-quality, generalizable semantic SLAM."}}
{"id": "2506.06396", "pdf": "https://arxiv.org/pdf/2506.06396", "abs": "https://arxiv.org/abs/2506.06396", "authors": ["Christopher D. Molek", "Roberto Fronteddu", "K. Brent Venable", "Niranjan Suri"], "title": "Natural Language Interaction with Databases on Edge Devices in the Internet of Battlefield Things", "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": null, "summary": "The expansion of the Internet of Things (IoT) in the battlefield, Internet of\nBattlefield Things (IoBT), gives rise to new opportunities for enhancing\nsituational awareness. To increase the potential of IoBT for situational\nawareness in critical decision making, the data from these devices must be\nprocessed into consumer-ready information objects, and made available to\nconsumers on demand. To address this challenge we propose a workflow that makes\nuse of natural language processing (NLP) to query a database technology and\nreturn a response in natural language. Our solution utilizes Large Language\nModels (LLMs) that are sized for edge devices to perform NLP as well as\ngraphical databases which are well suited for dynamic connected networks which\nare pervasive in the IoBT. Our architecture employs LLMs for both mapping\nquestions in natural language to Cypher database queries as well as to\nsummarize the database output back to the user in natural language. We evaluate\nseveral medium sized LLMs for both of these tasks on a database representing\npublicly available data from the US Army's Multipurpose Sensing Area (MSA) at\nthe Jornada Range in Las Cruces, NM. We observe that Llama 3.1 (8 billion\nparameters) outperforms the other models across all the considered metrics.\nMost importantly, we note that, unlike current methods, our two step approach\nallows the relaxation of the Exact Match (EM) requirement of the produced\nCypher queries with ground truth code and, in this way, it achieves a 19.4%\nincrease in accuracy. Our workflow lays the ground work for deploying LLMs on\nedge devices to enable natural language interactions with databases containing\ninformation objects for critical decision making.", "AI": {"tldr": "The paper proposes a workflow using NLP and LLMs for IoBT to enhance situational awareness by querying databases in natural language and summarizing results.", "motivation": "To improve situational awareness in IoBT by processing device data into consumer-ready information and enabling natural language interactions.", "method": "Uses LLMs for mapping natural language queries to Cypher database queries and summarizing results. Evaluated on US Army MSA data.", "result": "Llama 3.1 (8B parameters) outperforms other models, achieving a 19.4% accuracy increase by relaxing Exact Match requirements.", "conclusion": "The workflow enables effective natural language interactions with IoBT databases, advancing edge-based LLM deployment for critical decision-making."}}
{"id": "2506.06537", "pdf": "https://arxiv.org/pdf/2506.06537", "abs": "https://arxiv.org/abs/2506.06537", "authors": ["Seung-jae Lee", "Paul Hongsuck Seo"], "title": "Bridging Audio and Vision: Zero-Shot Audiovisual Segmentation by Connecting Pretrained Models", "categories": ["cs.CV", "cs.SD", "eess.AS"], "comment": "Accepted on INTERSPEECH2025", "summary": "Audiovisual segmentation (AVS) aims to identify visual regions corresponding\nto sound sources, playing a vital role in video understanding, surveillance,\nand human-computer interaction. Traditional AVS methods depend on large-scale\npixel-level annotations, which are costly and time-consuming to obtain. To\naddress this, we propose a novel zero-shot AVS framework that eliminates\ntask-specific training by leveraging multiple pretrained models. Our approach\nintegrates audio, vision, and text representations to bridge modality gaps,\nenabling precise sound source segmentation without AVS-specific annotations. We\nsystematically explore different strategies for connecting pretrained models\nand evaluate their efficacy across multiple datasets. Experimental results\ndemonstrate that our framework achieves state-of-the-art zero-shot AVS\nperformance, highlighting the effectiveness of multimodal model integration for\nfinegrained audiovisual segmentation.", "AI": {"tldr": "A novel zero-shot audiovisual segmentation (AVS) framework eliminates task-specific training by integrating pretrained models, achieving state-of-the-art performance without costly annotations.", "motivation": "Traditional AVS methods require expensive pixel-level annotations, prompting the need for a cost-effective, annotation-free solution.", "method": "The framework integrates audio, vision, and text representations from pretrained models to bridge modality gaps and segment sound sources.", "result": "The approach achieves state-of-the-art zero-shot AVS performance across multiple datasets.", "conclusion": "Multimodal integration of pretrained models effectively enables fine-grained AVS without task-specific training or annotations."}}
{"id": "2506.06401", "pdf": "https://arxiv.org/pdf/2506.06401", "abs": "https://arxiv.org/abs/2506.06401", "authors": ["Hongming Yang", "Shi Lin", "Jun Shao", "Changting Lin", "Donghai Zhu", "Meng Han", "Qinglei Kong"], "title": "Direct Behavior Optimization: Unlocking the Potential of Lightweight LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "This work is accepted at ACL 2025", "summary": "Lightweight Large Language Models (LwLLMs) are reduced-parameter, optimized\nmodels designed to run efficiently on consumer-grade hardware, offering\nsignificant advantages in resource efficiency, cost-effectiveness, and data\nprivacy. However, these models often struggle with limited inference and\nreasoning capabilities, which restrict their performance on complex tasks and\nlimit their practical applicability. Moreover, existing prompt optimization\nmethods typically rely on extensive manual effort or the meta-cognitive\nabilities of state-of-the-art LLMs, making them less effective for LwLLMs. To\naddress these challenges, we introduce DeBoP, a new Direct Behavior\nOptimization Paradigm, original from the Chain-of-Thought (CoT) prompting\ntechnique. Unlike CoT Prompting, DeBoP is an automatic optimization method,\nwhich focuses on the optimization directly on the behavior of LwLLMs. In\nparticular, DeBoP transforms the optimization of complex prompts into the\noptimization of discrete, quantifiable execution sequences using a\ngradient-free Monte Carlo Tree Search. We evaluate DeBoP on seven challenging\ntasks where state-of-the-art LLMs excel but LwLLMs generally underperform.\nExperimental results demonstrate that DeBoP significantly outperforms recent\nprompt optimization methods on most tasks. In particular, DeBoP-optimized\nLwLLMs surpass GPT-3.5 on most tasks while reducing computational time by\napproximately 60% compared to other automatic prompt optimization methods.", "AI": {"tldr": "DeBoP, a new Direct Behavior Optimization Paradigm, enhances LwLLMs' performance by optimizing behavior directly, outperforming GPT-3.5 on most tasks with 60% faster computation.", "motivation": "LwLLMs struggle with limited inference and reasoning capabilities, and existing prompt optimization methods are ineffective for them.", "method": "DeBoP transforms complex prompt optimization into quantifiable execution sequences using gradient-free Monte Carlo Tree Search.", "result": "DeBoP-optimized LwLLMs surpass GPT-3.5 on most tasks and reduce computational time by ~60%.", "conclusion": "DeBoP effectively addresses LwLLMs' limitations, offering a practical and efficient solution for prompt optimization."}}
{"id": "2506.06563", "pdf": "https://arxiv.org/pdf/2506.06563", "abs": "https://arxiv.org/abs/2506.06563", "authors": ["Thushari Hapuarachchi", "Long Dang", "Kaiqi Xiong"], "title": "Securing Traffic Sign Recognition Systems in Autonomous Vehicles", "categories": ["cs.CV", "cs.CR", "cs.LG"], "comment": null, "summary": "Deep Neural Networks (DNNs) are widely used for traffic sign recognition\nbecause they can automatically extract high-level features from images. These\nDNNs are trained on large-scale datasets obtained from unknown sources.\nTherefore, it is important to ensure that the models remain secure and are not\ncompromised or poisoned during training. In this paper, we investigate the\nrobustness of DNNs trained for traffic sign recognition. First, we perform the\nerror-minimizing attacks on DNNs used for traffic sign recognition by adding\nimperceptible perturbations on training data. Then, we propose a data\naugmentation-based training method to mitigate the error-minimizing attacks.\nThe proposed training method utilizes nonlinear transformations to disrupt the\nperturbations and improve the model robustness. We experiment with two\nwell-known traffic sign datasets to demonstrate the severity of the attack and\nthe effectiveness of our mitigation scheme. The error-minimizing attacks reduce\nthe prediction accuracy of the DNNs from 99.90% to 10.6%. However, our\nmitigation scheme successfully restores the prediction accuracy to 96.05%.\nMoreover, our approach outperforms adversarial training in mitigating the\nerror-minimizing attacks. Furthermore, we propose a detection model capable of\nidentifying poisoned data even when the perturbations are imperceptible to\nhuman inspection. Our detection model achieves a success rate of over 99% in\nidentifying the attack. This research highlights the need to employ advanced\ntraining methods for DNNs in traffic sign recognition systems to mitigate the\neffects of data poisoning attacks.", "AI": {"tldr": "The paper investigates DNN robustness in traffic sign recognition, proposes a data augmentation-based method to mitigate error-minimizing attacks, and introduces a detection model for poisoned data.", "motivation": "To address the vulnerability of DNNs to data poisoning attacks in traffic sign recognition and ensure model security.", "method": "Error-minimizing attacks are performed, followed by a data augmentation-based training method using nonlinear transformations. A detection model for poisoned data is also proposed.", "result": "Attacks reduced accuracy from 99.90% to 10.6%, but the mitigation method restored it to 96.05%. The detection model achieved a 99% success rate.", "conclusion": "Advanced training methods are essential for DNNs in traffic sign recognition to counter data poisoning attacks."}}
{"id": "2506.06404", "pdf": "https://arxiv.org/pdf/2506.06404", "abs": "https://arxiv.org/abs/2506.06404", "authors": ["Sooyung Choi", "Jaehyeok Lee", "Xiaoyuan Yi", "Jing Yao", "Xing Xie", "JinYeong Bak"], "title": "Unintended Harms of Value-Aligned LLMs: Psychological and Empirical Insights", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "comment": "Accepted to ACL 2025", "summary": "The application scope of Large Language Models (LLMs) continues to expand,\nleading to increasing interest in personalized LLMs that align with human\nvalues. However, aligning these models with individual values raises\nsignificant safety concerns, as certain values may correlate with harmful\ninformation. In this paper, we identify specific safety risks associated with\nvalue-aligned LLMs and investigate the psychological principles behind these\nchallenges. Our findings reveal two key insights. (1) Value-aligned LLMs are\nmore prone to harmful behavior compared to non-fine-tuned models and exhibit\nslightly higher risks in traditional safety evaluations than other fine-tuned\nmodels. (2) These safety issues arise because value-aligned LLMs genuinely\ngenerate text according to the aligned values, which can amplify harmful\noutcomes. Using a dataset with detailed safety categories, we find significant\ncorrelations between value alignment and safety risks, supported by\npsychological hypotheses. This study offers insights into the \"black box\" of\nvalue alignment and proposes in-context alignment methods to enhance the safety\nof value-aligned LLMs.", "AI": {"tldr": "Value-aligned LLMs pose higher safety risks due to genuine adherence to aligned values, amplifying harmful outcomes. Psychological insights and in-context alignment methods are proposed to mitigate risks.", "motivation": "To address safety concerns in personalized LLMs aligned with human values, as such alignment may correlate with harmful information.", "method": "Identified safety risks in value-aligned LLMs, analyzed psychological principles, and used a dataset with safety categories to correlate value alignment with risks.", "result": "Value-aligned LLMs are more prone to harmful behavior and exhibit higher risks in safety evaluations due to genuine value adherence.", "conclusion": "The study provides insights into value alignment risks and proposes in-context methods to improve safety in value-aligned LLMs."}}
{"id": "2506.06569", "pdf": "https://arxiv.org/pdf/2506.06569", "abs": "https://arxiv.org/abs/2506.06569", "authors": ["Yannis Spyridis", "Vasileios Argyriou"], "title": "Textile Analysis for Recycling Automation using Transfer Learning and Zero-Shot Foundation Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Automated sorting is crucial for improving the efficiency and scalability of\ntextile recycling, but accurately identifying material composition and\ndetecting contaminants from sensor data remains challenging. This paper\ninvestigates the use of standard RGB imagery, a cost-effective sensing\nmodality, for key pre-processing tasks in an automated system. We present\ncomputer vision components designed for a conveyor belt setup to perform (a)\nclassification of four common textile types and (b) segmentation of non-textile\nfeatures such as buttons and zippers. For classification, several pre-trained\narchitectures were evaluated using transfer learning and cross-validation, with\nEfficientNetB0 achieving the best performance on a held-out test set with\n81.25\\% accuracy. For feature segmentation, a zero-shot approach combining the\nGrounding DINO open-vocabulary detector with the Segment Anything Model (SAM)\nwas employed, demonstrating excellent performance with a mIoU of 0.90 for the\ngenerated masks against ground truth. This study demonstrates the feasibility\nof using RGB images coupled with modern deep learning techniques, including\ntransfer learning for classification and foundation models for zero-shot\nsegmentation, to enable essential analysis steps for automated textile\nrecycling pipelines.", "AI": {"tldr": "The paper explores using RGB imagery and deep learning for automated textile recycling, achieving high accuracy in textile classification (81.25%) and feature segmentation (mIoU 0.90).", "motivation": "Improving efficiency in textile recycling by addressing challenges in material composition identification and contaminant detection using cost-effective RGB imagery.", "method": "Utilized transfer learning with pre-trained models (e.g., EfficientNetB0) for textile classification and a zero-shot approach (Grounding DINO + Segment Anything Model) for feature segmentation.", "result": "EfficientNetB0 achieved 81.25% accuracy in classification, and the zero-shot segmentation approach achieved a mIoU of 0.90.", "conclusion": "RGB imagery combined with modern deep learning techniques is feasible for automating key steps in textile recycling pipelines."}}
{"id": "2506.06406", "pdf": "https://arxiv.org/pdf/2506.06406", "abs": "https://arxiv.org/abs/2506.06406", "authors": ["Guoyang Xia", "Yifeng Ding", "Fengfa Li", "Lei Ren", "Chen Wei", "Fangxiang Feng", "Xiaojie Wang"], "title": "SMAR: Soft Modality-Aware Routing Strategy for MoE-based Multimodal Large Language Models Preserving Language Capabilities", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Mixture of Experts (MoE) architectures have become a key approach for scaling\nlarge language models, with growing interest in extending them to multimodal\ntasks. Existing methods to build multimodal MoE models either incur high\ntraining costs or suffer from degraded language capabilities when adapting\npretrained models. To address this, we propose Soft ModalityAware Routing\n(SMAR), a novel regularization technique that uses Kullback Leibler divergence\nto control routing probability distributions across modalities, encouraging\nexpert specialization without modifying model architecture or heavily relying\non textual data. Experiments on visual instruction tuning show that SMAR\npreserves language ability at 86.6% retention with only 2.5% pure text,\noutperforming baselines while maintaining strong multimodal performance. Our\napproach offers a practical and efficient solution to balance modality\ndifferentiation and language capabilities in multimodal MoE models.", "AI": {"tldr": "SMAR, a novel regularization technique, balances modality differentiation and language capabilities in multimodal MoE models, achieving 86.6% language retention with minimal text data.", "motivation": "Address high training costs and degraded language capabilities in existing multimodal MoE models.", "method": "Introduces Soft Modality-Aware Routing (SMAR) using Kullback-Leibler divergence to control routing probabilities across modalities.", "result": "SMAR retains 86.6% language ability with only 2.5% pure text, outperforming baselines while maintaining strong multimodal performance.", "conclusion": "SMAR provides a practical and efficient solution for multimodal MoE models, balancing modality specialization and language preservation."}}
{"id": "2506.06578", "pdf": "https://arxiv.org/pdf/2506.06578", "abs": "https://arxiv.org/abs/2506.06578", "authors": ["Anees Nashath Shaik", "Barbara Villarini", "Vasileios Argyriou"], "title": "A Deep Learning Approach for Facial Attribute Manipulation and Reconstruction in Surveillance and Reconnaissance", "categories": ["cs.CV"], "comment": null, "summary": "Surveillance systems play a critical role in security and reconnaissance, but\ntheir performance is often compromised by low-quality images and videos,\nleading to reduced accuracy in face recognition. Additionally, existing\nAI-based facial analysis models suffer from biases related to skin tone\nvariations and partially occluded faces, further limiting their effectiveness\nin diverse real-world scenarios. These challenges are the results of data\nlimitations and imbalances, where available training datasets lack sufficient\ndiversity, resulting in unfair and unreliable facial recognition performance.\nTo address these issues, we propose a data-driven platform that enhances\nsurveillance capabilities by generating synthetic training data tailored to\ncompensate for dataset biases. Our approach leverages deep learning-based\nfacial attribute manipulation and reconstruction using autoencoders and\nGenerative Adversarial Networks (GANs) to create diverse and high-quality\nfacial datasets. Additionally, our system integrates an image enhancement\nmodule, improving the clarity of low-resolution or occluded faces in\nsurveillance footage. We evaluate our approach using the CelebA dataset,\ndemonstrating that the proposed platform enhances both training data diversity\nand model fairness. This work contributes to reducing bias in AI-based facial\nanalysis and improving surveillance accuracy in challenging environments,\nleading to fairer and more reliable security applications.", "AI": {"tldr": "A data-driven platform using GANs and autoencoders enhances surveillance by generating diverse synthetic training data and improving image quality, reducing bias in facial recognition.", "motivation": "Addressing biases and limitations in AI-based facial recognition due to low-quality images, occlusions, and lack of dataset diversity.", "method": "Uses deep learning (autoencoders and GANs) for facial attribute manipulation, reconstruction, and image enhancement.", "result": "Improved training data diversity and model fairness, validated on the CelebA dataset.", "conclusion": "The platform reduces bias and enhances surveillance accuracy, leading to fairer and more reliable security applications."}}
{"id": "2506.06446", "pdf": "https://arxiv.org/pdf/2506.06446", "abs": "https://arxiv.org/abs/2506.06446", "authors": ["Ivi Chatzi", "Nina Corvelo Benz", "Stratis Tsirtsis", "Manuel Gomez-Rodriguez"], "title": "Canonical Autoregressive Generation", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "comment": null, "summary": "State of the art large language models are trained using large amounts of\ntokens derived from raw text using what is called a tokenizer. Crucially, the\ntokenizer determines the (token) vocabulary a model will use during inference\nas well as, in principle, the (token) language. This is because, while the\ntoken vocabulary may allow for different tokenizations of a string, the\ntokenizer always maps the string to only one of these tokenizations--the\ncanonical tokenization. However, multiple lines of empirical evidence suggest\nthat large language models do not always generate canonical token sequences,\nand this comes with several negative consequences. In this work, we first show\nthat, to generate a canonical token sequence, a model needs to generate\n(partial) canonical token sequences at each step of the autoregressive\ngeneration process underpinning its functioning. Building upon this theoretical\nresult, we introduce canonical sampling, a simple and efficient sampling method\nthat precludes a given model from generating non-canonical token sequences.\nFurther, we also show that, in comparison with standard sampling, the\ndistribution of token sequences generated using canonical sampling is provably\ncloser to the true distribution of token sequences used during training.", "AI": {"tldr": "The paper introduces 'canonical sampling' to ensure large language models generate canonical token sequences, improving alignment with training data distributions.", "motivation": "Large language models often generate non-canonical token sequences, leading to negative consequences. The work aims to address this by enforcing canonical tokenization during generation.", "method": "The authors first establish that generating canonical sequences requires step-by-step canonical tokenization. They then propose 'canonical sampling,' a method to prevent non-canonical sequences.", "result": "Canonical sampling ensures models generate only canonical token sequences and aligns the output distribution more closely with the training data.", "conclusion": "The proposed method effectively mitigates non-canonical token generation, enhancing model reliability and consistency with training data."}}
{"id": "2506.06596", "pdf": "https://arxiv.org/pdf/2506.06596", "abs": "https://arxiv.org/abs/2506.06596", "authors": ["Youssef Farah", "Federico Paredes-Vall\u00e9s", "Guido De Croon", "Muhammad Ahmed Humais", "Hussain Sajwani", "Yahya Zweiri"], "title": "EV-LayerSegNet: Self-supervised Motion Segmentation using Event Cameras", "categories": ["cs.CV"], "comment": "This paper has been accepted for publication at the IEEE Conference\n  on Computer Vision and Pattern Recognition (CVPR) Workshops, Nashville, 2025", "summary": "Event cameras are novel bio-inspired sensors that capture motion dynamics\nwith much higher temporal resolution than traditional cameras, since pixels\nreact asynchronously to brightness changes. They are therefore better suited\nfor tasks involving motion such as motion segmentation. However, training\nevent-based networks still represents a difficult challenge, as obtaining\nground truth is very expensive, error-prone and limited in frequency. In this\narticle, we introduce EV-LayerSegNet, a self-supervised CNN for event-based\nmotion segmentation. Inspired by a layered representation of the scene\ndynamics, we show that it is possible to learn affine optical flow and\nsegmentation masks separately, and use them to deblur the input events. The\ndeblurring quality is then measured and used as self-supervised learning loss.\nWe train and test the network on a simulated dataset with only affine motion,\nachieving IoU and detection rate up to 71% and 87% respectively.", "AI": {"tldr": "EV-LayerSegNet is a self-supervised CNN for event-based motion segmentation, using affine optical flow and segmentation masks to deblur events, achieving 71% IoU and 87% detection rate.", "motivation": "Ground truth for event-based networks is expensive and error-prone, necessitating a self-supervised approach.", "method": "Uses a layered scene representation to learn affine optical flow and segmentation masks separately, measuring deblurring quality as a self-supervised loss.", "result": "Achieves 71% IoU and 87% detection rate on a simulated dataset with affine motion.", "conclusion": "EV-LayerSegNet demonstrates effective self-supervised learning for event-based motion segmentation."}}
{"id": "2506.06485", "pdf": "https://arxiv.org/pdf/2506.06485", "abs": "https://arxiv.org/abs/2506.06485", "authors": ["Kaiser Sun", "Fan Bai", "Mark Dredze"], "title": "What Is Seen Cannot Be Unseen: The Disruptive Effect of Knowledge Conflict on Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models frequently rely on both contextual input and parametric\nknowledge to perform tasks. However, these sources can come into conflict,\nespecially when retrieved documents contradict the model's parametric\nknowledge. We propose a diagnostic framework to systematically evaluate LLM\nbehavior under context-memory conflict, where the contextual information\ndiverges from their parametric beliefs. We construct diagnostic data that\nelicit these conflicts and analyze model performance across multiple task\ntypes. Our findings reveal that (1) knowledge conflict has minimal impact on\ntasks that do not require knowledge utilization, (2) model performance is\nconsistently higher when contextual and parametric knowledge are aligned, (3)\nmodels are unable to fully suppress their internal knowledge even when\ninstructed, and (4) providing rationales that explain the conflict increases\nreliance on contexts. These insights raise concerns about the validity of\nmodel-based evaluation and underscore the need to account for knowledge\nconflict in the deployment of LLMs.", "AI": {"tldr": "A framework evaluates LLM behavior under context-memory conflict, revealing minimal impact on non-knowledge tasks, better performance with aligned knowledge, inability to suppress internal knowledge, and increased context reliance with rationales.", "motivation": "To address conflicts between contextual input and parametric knowledge in LLMs, which can affect performance and evaluation validity.", "method": "Proposes a diagnostic framework, constructs conflict-inducing data, and analyzes model performance across tasks.", "result": "Findings show knowledge conflict's limited impact on non-knowledge tasks, better alignment performance, inability to suppress internal knowledge, and increased context reliance with rationales.", "conclusion": "Highlights concerns about model evaluation validity and the need to address knowledge conflict in LLM deployment."}}
{"id": "2506.06600", "pdf": "https://arxiv.org/pdf/2506.06600", "abs": "https://arxiv.org/abs/2506.06600", "authors": ["Tan-Hanh Pham", "Chris Ngo"], "title": "RARL: Improving Medical VLM Reasoning and Generalization with Reinforcement Learning and LoRA under Data and Hardware Constraints", "categories": ["cs.CV"], "comment": "Under review", "summary": "The growing integration of vision-language models (VLMs) in medical\napplications offers promising support for diagnostic reasoning. However,\ncurrent medical VLMs often face limitations in generalization, transparency,\nand computational efficiency-barriers that hinder deployment in real-world,\nresource-constrained settings. To address these challenges, we propose a\nReasoning-Aware Reinforcement Learning framework, \\textbf{RARL}, that enhances\nthe reasoning capabilities of medical VLMs while remaining efficient and\nadaptable to low-resource environments. Our approach fine-tunes a lightweight\nbase model, Qwen2-VL-2B-Instruct, using Low-Rank Adaptation and custom reward\nfunctions that jointly consider diagnostic accuracy and reasoning quality.\nTraining is performed on a single NVIDIA A100-PCIE-40GB GPU, demonstrating the\nfeasibility of deploying such models in constrained environments. We evaluate\nthe model using an LLM-as-judge framework that scores both correctness and\nexplanation quality. Experimental results show that RARL significantly improves\nVLM performance in medical image analysis and clinical reasoning, outperforming\nsupervised fine-tuning on reasoning-focused tasks by approximately 7.78%, while\nrequiring fewer computational resources. Additionally, we demonstrate the\ngeneralization capabilities of our approach on unseen datasets, achieving\naround 27% improved performance compared to supervised fine-tuning and about 4%\nover traditional RL fine-tuning. Our experiments also illustrate that diversity\nprompting during training and reasoning prompting during inference are crucial\nfor enhancing VLM performance. Our findings highlight the potential of\nreasoning-guided learning and reasoning prompting to steer medical VLMs toward\nmore transparent, accurate, and resource-efficient clinical decision-making.\nCode and data are publicly available.", "AI": {"tldr": "The paper introduces RARL, a Reasoning-Aware Reinforcement Learning framework, to enhance medical VLMs' reasoning, efficiency, and adaptability in low-resource settings. It outperforms traditional methods in accuracy and computational efficiency.", "motivation": "Current medical VLMs lack generalization, transparency, and computational efficiency, limiting real-world deployment. RARL aims to address these challenges.", "method": "RARL fine-tunes Qwen2-VL-2B-Instruct using Low-Rank Adaptation and custom reward functions for diagnostic accuracy and reasoning quality. Training is done on a single GPU.", "result": "RARL improves VLM performance by 7.78% in reasoning tasks and shows 27% better generalization on unseen datasets, with reduced resource usage.", "conclusion": "Reasoning-guided learning and prompting enhance medical VLMs' transparency, accuracy, and efficiency, making them viable for clinical decision-making."}}
{"id": "2506.06500", "pdf": "https://arxiv.org/pdf/2506.06500", "abs": "https://arxiv.org/abs/2506.06500", "authors": ["Luyao Shi", "Michael Kazda", "Charles Schmitter", "Hemlata Gupta"], "title": "Improving LLM-Powered EDA Assistants with RAFT", "categories": ["cs.CL"], "comment": "Accepted paper at IEEE International Conference on LLM-Aided Design,\n  2025 (LAD 2025)", "summary": "Electronic design engineers often struggle to efficiently access relevant\ninformation for tasks like design verification and technology development.\nWhile large language models (LLMs) can enhance productivity as conversational\nagents, pre-trained open-source LLMs lack domain-specific knowledge for\nElectronic Design Automation (EDA). In a Retrieval-Augmented Generation (RAG)\ncontext, LLMs rely on external context but may still produce inaccurate\nresponses. Retrieval-Augmented Fine-Tuning (RAFT) improves LLM performance, but\nacquiring labeled question/answer (Q/A) data in EDA is difficult. To address\nthis, we propose using synthetic Q/A datasets to enhance LLMs with RAFT. Our\nresults show that RAFT with synthetic data significantly boosts LLM performance\nfor RAG-based EDA tasks. We also investigate the impact of using real user\nquestions as Retrieval-Augmented Few-Shot (RAFS) examples for synthetic data\ngeneration. Additionally, we implement secure access control to ensure\nsensitive information is only accessible to authorized personnel. Finally, we\nassess the risk of data leakage and unintended memorization during fine-tuning\nwith synthetic data, providing practical insights.", "AI": {"tldr": "Proposes using synthetic Q/A datasets with RAFT to enhance LLMs for EDA tasks, improving performance and addressing data scarcity.", "motivation": "EDA engineers struggle with inefficient information access, and pre-trained LLMs lack domain-specific knowledge.", "method": "Uses Retrieval-Augmented Fine-Tuning (RAFT) with synthetic Q/A data and investigates RAFS for synthetic data generation.", "result": "RAFT with synthetic data significantly boosts LLM performance for EDA tasks.", "conclusion": "The approach improves LLM utility in EDA while addressing data scarcity and security concerns."}}
{"id": "2506.06602", "pdf": "https://arxiv.org/pdf/2506.06602", "abs": "https://arxiv.org/abs/2506.06602", "authors": ["Santhosh Kakarla", "Gautama Shastry Bulusu Venkata"], "title": "Zero Shot Composed Image Retrieval", "categories": ["cs.CV"], "comment": "8 pages, 3 figures", "summary": "Composed image retrieval (CIR) allows a user to locate a target image by\napplying a fine-grained textual edit (e.g., ``turn the dress blue'' or ``remove\nstripes'') to a reference image. Zero-shot CIR, which embeds the image and the\ntext with separate pretrained vision-language encoders, reaches only 20-25\\%\nRecall@10 on the FashionIQ benchmark. We improve this by fine-tuning BLIP-2\nwith a lightweight Q-Former that fuses visual and textual features into a\nsingle embedding, raising Recall@10 to 45.6\\% (shirt), 40.1\\% (dress), and\n50.4\\% (top-tee) and increasing the average Recall@50 to 67.6\\%. We also\nexamine Retrieval-DPO, which fine-tunes CLIP's text encoder with a Direct\nPreference Optimization loss applied to FAISS-mined hard negatives. Despite\nextensive tuning of the scaling factor, index, and sampling strategy,\nRetrieval-DPO attains only 0.02\\% Recall@10 -- far below zero-shot and\nprompt-tuned baselines -- because it (i) lacks joint image-text fusion, (ii)\nuses a margin objective misaligned with top-$K$ metrics, (iii) relies on\nlow-quality negatives, and (iv) keeps the vision and Transformer layers frozen.\nOur results show that effective preference-based CIR requires genuine\nmultimodal fusion, ranking-aware objectives, and carefully curated negatives.", "AI": {"tldr": "The paper improves zero-shot composed image retrieval (CIR) by fine-tuning BLIP-2 with a Q-Former, achieving significant performance gains. It also explores Retrieval-DPO but finds it ineffective due to several limitations.", "motivation": "To enhance the performance of zero-shot CIR, which underperforms on benchmarks like FashionIQ, by leveraging advanced fusion techniques and addressing limitations of existing methods.", "method": "Fine-tunes BLIP-2 with a lightweight Q-Former for joint image-text fusion and evaluates Retrieval-DPO, a method using Direct Preference Optimization on hard negatives.", "result": "BLIP-2 with Q-Former achieves Recall@10 of 45.6% (shirt), 40.1% (dress), and 50.4% (top-tee), while Retrieval-DPO fails with only 0.02% Recall@10.", "conclusion": "Effective CIR requires multimodal fusion, ranking-aware objectives, and high-quality negatives, as demonstrated by the success of BLIP-2 and the failure of Retrieval-DPO."}}
{"id": "2506.06506", "pdf": "https://arxiv.org/pdf/2506.06506", "abs": "https://arxiv.org/abs/2506.06506", "authors": ["Kshitish Ghate", "Tessa Charlesworth", "Mona Diab", "Aylin Caliskan"], "title": "Biases Propagate in Encoder-based Vision-Language Models: A Systematic Analysis From Intrinsic Measures to Zero-shot Retrieval Outcomes", "categories": ["cs.CL"], "comment": "Accepted to ACL Findings 2025", "summary": "To build fair AI systems we need to understand how social-group biases\nintrinsic to foundational encoder-based vision-language models (VLMs) manifest\nin biases in downstream tasks. In this study, we demonstrate that intrinsic\nbiases in VLM representations systematically ``carry over'' or propagate into\nzero-shot retrieval tasks, revealing how deeply rooted biases shape a model's\noutputs. We introduce a controlled framework to measure this propagation by\ncorrelating (a) intrinsic measures of bias in the representational space with\n(b) extrinsic measures of bias in zero-shot text-to-image (TTI) and\nimage-to-text (ITT) retrieval. Results show substantial correlations between\nintrinsic and extrinsic bias, with an average $\\rho$ = 0.83 $\\pm$ 0.10. This\npattern is consistent across 114 analyses, both retrieval directions, six\nsocial groups, and three distinct VLMs. Notably, we find that\nlarger/better-performing models exhibit greater bias propagation, a finding\nthat raises concerns given the trend towards increasingly complex AI models.\nOur framework introduces baseline evaluation tasks to measure the propagation\nof group and valence signals. Investigations reveal that underrepresented\ngroups experience less robust propagation, further skewing their model-related\noutcomes.", "AI": {"tldr": "The study investigates how social-group biases in vision-language models (VLMs) propagate into zero-shot retrieval tasks, revealing strong correlations between intrinsic and extrinsic biases. Larger models show greater bias propagation, raising concerns for fairness in AI.", "motivation": "To understand and measure how intrinsic biases in VLMs affect downstream tasks, particularly zero-shot retrieval, and to highlight fairness concerns in AI systems.", "method": "A controlled framework correlates intrinsic bias measures in VLM representations with extrinsic bias measures in zero-shot text-to-image (TTI) and image-to-text (ITT) retrieval tasks.", "result": "Strong correlations (average \u03c1 = 0.83 \u00b1 0.10) between intrinsic and extrinsic biases were found across 114 analyses, with larger models exhibiting greater bias propagation. Underrepresented groups face less robust propagation.", "conclusion": "The study underscores the need for fairness-aware AI development, as biases in foundational models systematically propagate, disproportionately affecting underrepresented groups."}}
{"id": "2506.06631", "pdf": "https://arxiv.org/pdf/2506.06631", "abs": "https://arxiv.org/abs/2506.06631", "authors": ["Minghao Zou", "Qingtian Zeng", "Yongping Miao", "Shangkun Liu", "Zilong Wang", "Hantao Liu", "Wei Zhou"], "title": "PhysLab: A Benchmark Dataset for Multi-Granularity Visual Parsing of Physics Experiments", "categories": ["cs.CV"], "comment": null, "summary": "Visual parsing of images and videos is critical for a wide range of\nreal-world applications. However, progress in this field is constrained by\nlimitations of existing datasets: (1) insufficient annotation granularity,\nwhich impedes fine-grained scene understanding and high-level reasoning; (2)\nlimited coverage of domains, particularly a lack of datasets tailored for\neducational scenarios; and (3) lack of explicit procedural guidance, with\nminimal logical rules and insufficient representation of structured task\nprocess. To address these gaps, we introduce PhysLab, the first video dataset\nthat captures students conducting complex physics experiments. The dataset\nincludes four representative experiments that feature diverse scientific\ninstruments and rich human-object interaction (HOI) patterns. PhysLab comprises\n620 long-form videos and provides multilevel annotations that support a variety\nof vision tasks, including action recognition, object detection, HOI analysis,\netc. We establish strong baselines and perform extensive evaluations to\nhighlight key challenges in the parsing of procedural educational videos. We\nexpect PhysLab to serve as a valuable resource for advancing fine-grained\nvisual parsing, facilitating intelligent classroom systems, and fostering\ncloser integration between computer vision and educational technologies. The\ndataset and the evaluation toolkit are publicly available at\nhttps://github.com/ZMH-SDUST/PhysLab.", "AI": {"tldr": "PhysLab is a new video dataset for fine-grained visual parsing in educational scenarios, addressing gaps in annotation granularity, domain coverage, and procedural guidance.", "motivation": "Existing datasets lack granularity, domain coverage (especially in education), and procedural guidance, limiting progress in visual parsing.", "method": "Introduces PhysLab, a dataset with 620 long-form videos of physics experiments, featuring multilevel annotations for tasks like action recognition and HOI analysis.", "result": "Establishes baselines and evaluations, highlighting challenges in parsing procedural educational videos.", "conclusion": "PhysLab aims to advance fine-grained visual parsing and integrate computer vision with educational technologies."}}
{"id": "2506.06522", "pdf": "https://arxiv.org/pdf/2506.06522", "abs": "https://arxiv.org/abs/2506.06522", "authors": ["Aladin Djuhera", "Swanand Ravindra Kadhe", "Syed Zawad", "Farhan Ahmed", "Heiko Ludwig", "Holger Boche"], "title": "Fixing It in Post: A Comparative Study of LLM Post-Training Data Quality and Model Performance", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent work on large language models (LLMs) has increasingly focused on\npost-training and alignment with datasets curated to enhance instruction\nfollowing, world knowledge, and specialized skills. However, most post-training\ndatasets used in leading open- and closed-source LLMs remain inaccessible to\nthe public, with limited information about their construction process. This\nlack of transparency has motivated the recent development of open-source\npost-training corpora. While training on these open alternatives can yield\nperformance comparable to that of leading models, systematic comparisons remain\nchallenging due to the significant computational cost of conducting them\nrigorously at scale, and are therefore largely absent. As a result, it remains\nunclear how specific samples, task types, or curation strategies influence\ndownstream performance when assessing data quality. In this work, we conduct\nthe first comprehensive side-by-side analysis of two prominent open\npost-training datasets: Tulu-3-SFT-Mix and SmolTalk. Using the Magpie\nframework, we annotate each sample with detailed quality metrics, including\nturn structure (single-turn vs. multi-turn), task category, input quality, and\nresponse quality, and we derive statistics that reveal structural and\nqualitative similarities and differences between the two datasets. Based on\nthese insights, we design a principled curation recipe that produces a new data\nmixture, TuluTalk, which contains 14% fewer samples than either source dataset\nwhile matching or exceeding their performance on key benchmarks. Our findings\noffer actionable insights for constructing more effective post-training\ndatasets that improve model performance within practical resource limits. To\nsupport future research, we publicly release both the annotated source datasets\nand our curated TuluTalk mixture.", "AI": {"tldr": "The paper compares two open post-training datasets (Tulu-3-SFT-Mix and SmolTalk) using the Magpie framework, identifies structural and qualitative differences, and creates a new dataset (TuluTalk) that outperforms the originals with fewer samples.", "motivation": "The lack of transparency and systematic comparisons in post-training datasets for LLMs motivates this study to analyze and improve data quality.", "method": "The Magpie framework is used to annotate samples with quality metrics, and a new dataset (TuluTalk) is curated based on insights from the analysis.", "result": "TuluTalk, with 14% fewer samples, matches or exceeds the performance of the original datasets on key benchmarks.", "conclusion": "The study provides actionable insights for creating better post-training datasets and releases the annotated datasets and TuluTalk for future research."}}
{"id": "2506.06643", "pdf": "https://arxiv.org/pdf/2506.06643", "abs": "https://arxiv.org/abs/2506.06643", "authors": ["Moushumi Medhi", "Rajiv Ranjan Sahay"], "title": "Dark Channel-Assisted Depth-from-Defocus from a Single Image", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we utilize the dark channel as a complementary cue to estimate\nthe depth of a scene from a single space-variant defocus blurred image due to\nits effectiveness in implicitly capturing the local statistics of blurred\nimages and the scene structure. Existing depth-from-defocus (DFD) techniques\ntypically rely on multiple images with varying apertures or focus settings to\nrecover depth information. Very few attempts have focused on DFD from a single\ndefocused image due to the underconstrained nature of the problem. Our method\ncapitalizes on the relationship between local defocus blur and contrast\nvariations as key depth cues to enhance the overall performance in estimating\nthe scene's structure. The entire pipeline is trained adversarially in a fully\nend-to-end fashion. Experiments conducted on real data with realistic\ndepth-induced defocus blur demonstrate that incorporating dark channel prior\ninto single image DFD yields meaningful depth estimation results, validating\nthe effectiveness of our approach.", "AI": {"tldr": "The paper proposes using the dark channel prior to estimate depth from a single defocused image, improving upon traditional multi-image DFD methods.", "motivation": "Existing DFD techniques require multiple images, but the paper aims to solve the underconstrained problem of single-image DFD by leveraging dark channel and contrast cues.", "method": "The method uses dark channel prior and contrast variations as depth cues, trained adversarially in an end-to-end pipeline.", "result": "Experiments on real data show meaningful depth estimation, validating the approach.", "conclusion": "Incorporating dark channel prior into single-image DFD is effective for depth estimation."}}
{"id": "2506.06539", "pdf": "https://arxiv.org/pdf/2506.06539", "abs": "https://arxiv.org/abs/2506.06539", "authors": ["Yijie Hao", "Haofei Yu", "Jiaxuan You"], "title": "Beyond Facts: Evaluating Intent Hallucination in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 main conference", "summary": "When exposed to complex queries containing multiple conditions, today's large\nlanguage models (LLMs) tend to produce responses that only partially satisfy\nthe query while neglecting certain conditions. We therefore introduce the\nconcept of Intent Hallucination. In this phenomenon, LLMs either omit\n(neglecting to address certain parts) or misinterpret (responding to invented\nquery parts) elements of the given query, leading to intent hallucinated\ngeneration. To systematically evaluate intent hallucination, we introduce\nFAITHQA, a novel benchmark for intent hallucination that contains 20,068\nproblems, covering both query-only and retrieval-augmented generation (RAG)\nsetups with varying topics and difficulty. FAITHQA is the first hallucination\nbenchmark that goes beyond factual verification, tailored to identify the\nfundamental cause of intent hallucination. By evaluating various LLMs on\nFAITHQA, we find that (1) intent hallucination is a common issue even for\nstate-of-the-art models, and (2) the phenomenon stems from omission or\nmisinterpretation of LLMs. To facilitate future research, we introduce an\nautomatic LLM generation evaluation metric, CONSTRAINT SCORE, for detecting\nintent hallucination. Human evaluation results demonstrate that CONSTRAINT\nSCORE is closer to human performance for intent hallucination compared to\nbaselines.", "AI": {"tldr": "The paper introduces Intent Hallucination in LLMs, where models partially satisfy or misinterpret complex queries. It presents FAITHQA, a benchmark for evaluating this issue, and a metric called CONSTRAINT SCORE to detect it.", "motivation": "LLMs often fail to fully address complex queries, leading to partial or incorrect responses. This work aims to systematically study and measure this phenomenon, termed Intent Hallucination.", "method": "The authors create FAITHQA, a benchmark with 20,068 problems, to evaluate LLMs in query-only and RAG setups. They also propose CONSTRAINT SCORE, an automatic metric for detecting intent hallucination.", "result": "Intent hallucination is prevalent even in state-of-the-art LLMs, caused by omission or misinterpretation. CONSTRAINT SCORE aligns well with human evaluation for detecting this issue.", "conclusion": "The paper highlights the challenge of intent hallucination in LLMs, provides tools (FAITHQA and CONSTRAINT SCORE) for evaluation, and calls for further research to address this limitation."}}
{"id": "2506.06645", "pdf": "https://arxiv.org/pdf/2506.06645", "abs": "https://arxiv.org/abs/2506.06645", "authors": ["Cheng Peng", "Jingxiang Sun", "Yushuo Chen", "Zhaoqi Su", "Zhuo Su", "Yebin Liu"], "title": "Parametric Gaussian Human Model: Generalizable Prior for Efficient and Realistic Human Avatar Modeling", "categories": ["cs.CV"], "comment": "Project Page: https://pengc02.github.io/pghm/", "summary": "Photorealistic and animatable human avatars are a key enabler for\nvirtual/augmented reality, telepresence, and digital entertainment. While\nrecent advances in 3D Gaussian Splatting (3DGS) have greatly improved rendering\nquality and efficiency, existing methods still face fundamental challenges,\nincluding time-consuming per-subject optimization and poor generalization under\nsparse monocular inputs. In this work, we present the Parametric Gaussian Human\nModel (PGHM), a generalizable and efficient framework that integrates human\npriors into 3DGS for fast and high-fidelity avatar reconstruction from\nmonocular videos. PGHM introduces two core components: (1) a UV-aligned latent\nidentity map that compactly encodes subject-specific geometry and appearance\ninto a learnable feature tensor; and (2) a disentangled Multi-Head U-Net that\npredicts Gaussian attributes by decomposing static, pose-dependent, and\nview-dependent components via conditioned decoders. This design enables robust\nrendering quality under challenging poses and viewpoints, while allowing\nefficient subject adaptation without requiring multi-view capture or long\noptimization time. Experiments show that PGHM is significantly more efficient\nthan optimization-from-scratch methods, requiring only approximately 20 minutes\nper subject to produce avatars with comparable visual quality, thereby\ndemonstrating its practical applicability for real-world monocular avatar\ncreation.", "AI": {"tldr": "PGHM introduces a generalizable framework for fast, high-fidelity human avatar reconstruction from monocular videos using 3D Gaussian Splatting and human priors.", "motivation": "To address challenges in existing methods, such as time-consuming optimization and poor generalization with sparse inputs, for photorealistic and animatable human avatars.", "method": "PGHM uses a UV-aligned latent identity map and a disentangled Multi-Head U-Net to predict Gaussian attributes, enabling efficient subject adaptation.", "result": "PGHM achieves comparable visual quality in ~20 minutes per subject, outperforming optimization-from-scratch methods.", "conclusion": "PGHM demonstrates practical applicability for real-world monocular avatar creation with efficiency and robustness."}}
{"id": "2506.06561", "pdf": "https://arxiv.org/pdf/2506.06561", "abs": "https://arxiv.org/abs/2506.06561", "authors": ["Ho Yin 'Sam' Ng", "Ting-Yao Hsu", "Aashish Anantha Ramakrishnan", "Branislav Kveton", "Nedim Lipka", "Franck Dernoncourt", "Dongwon Lee", "Tong Yu", "Sungchul Kim", "Ryan A. Rossi", "Ting-Hao 'Kenneth' Huang"], "title": "LaMP-Cap: Personalized Figure Caption Generation With Multimodal Figure Profiles", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Figure captions are crucial for helping readers understand and remember a\nfigure's key message. Many models have been developed to generate these\ncaptions, helping authors compose better quality captions more easily. Yet,\nauthors almost always need to revise generic AI-generated captions to match\ntheir writing style and the domain's style, highlighting the need for\npersonalization. Despite language models' personalization (LaMP) advances,\nthese technologies often focus on text-only settings and rarely address\nscenarios where both inputs and profiles are multimodal. This paper introduces\nLaMP-Cap, a dataset for personalized figure caption generation with multimodal\nfigure profiles. For each target figure, LaMP-Cap provides not only the needed\ninputs, such as figure images, but also up to three other figures from the same\ndocument--each with its image, caption, and figure-mentioning paragraphs--as a\nprofile to characterize the context. Experiments with four LLMs show that using\nprofile information consistently helps generate captions closer to the original\nauthor-written ones. Ablation studies reveal that images in the profile are\nmore helpful than figure-mentioning paragraphs, highlighting the advantage of\nusing multimodal profiles over text-only ones.", "AI": {"tldr": "LaMP-Cap introduces a multimodal dataset for personalized figure caption generation, improving caption quality by leveraging context from related figures.", "motivation": "Existing caption generation models lack personalization and multimodal context, requiring authors to revise generic outputs.", "method": "LaMP-Cap provides multimodal profiles (images, captions, and paragraphs) for figures, tested with four LLMs.", "result": "Using profile information improves caption quality, with images proving more helpful than text.", "conclusion": "Multimodal profiles enhance personalized caption generation, outperforming text-only approaches."}}
{"id": "2506.06667", "pdf": "https://arxiv.org/pdf/2506.06667", "abs": "https://arxiv.org/abs/2506.06667", "authors": ["Yu-Hsuan Ho", "Ali Mostafavi"], "title": "Flood-DamageSense: Multimodal Mamba with Multitask Learning for Building Flood Damage Assessment using SAR Remote Sensing Imagery", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": null, "summary": "Most post-disaster damage classifiers succeed only when destructive forces\nleave clear spectral or structural signatures -- conditions rarely present\nafter inundation. Consequently, existing models perform poorly at identifying\nflood-related building damages. The model presented in this study,\nFlood-DamageSense, addresses this gap as the first deep-learning framework\npurpose-built for building-level flood-damage assessment. The architecture\nfuses pre- and post-event SAR/InSAR scenes with very-high-resolution optical\nbasemaps and an inherent flood-risk layer that encodes long-term exposure\nprobabilities, guiding the network toward plausibly affected structures even\nwhen compositional change is minimal. A multimodal Mamba backbone with a\nsemi-Siamese encoder and task-specific decoders jointly predicts (1) graded\nbuilding-damage states, (2) floodwater extent, and (3) building footprints.\nTraining and evaluation on Hurricane Harvey (2017) imagery from Harris County,\nTexas -- supported by insurance-derived property-damage extents -- show a mean\nF1 improvement of up to 19 percentage points over state-of-the-art baselines,\nwith the largest gains in the frequently misclassified \"minor\" and \"moderate\"\ndamage categories. Ablation studies identify the inherent-risk feature as the\nsingle most significant contributor to this performance boost. An end-to-end\npost-processing pipeline converts pixel-level outputs to actionable,\nbuilding-scale damage maps within minutes of image acquisition. By combining\nrisk-aware modeling with SAR's all-weather capability, Flood-DamageSense\ndelivers faster, finer-grained, and more reliable flood-damage intelligence to\nsupport post-disaster decision-making and resource allocation.", "AI": {"tldr": "Flood-DamageSense is a deep-learning model for flood-damage assessment, outperforming existing methods by 19% F1 score, especially in minor/moderate damage categories, using SAR/InSAR, optical data, and flood-risk layers.", "motivation": "Existing models fail to accurately identify flood-related building damages due to minimal spectral/structural signatures post-inundation.", "method": "Uses a multimodal Mamba backbone with SAR/InSAR, optical basemaps, and flood-risk layers to predict damage states, floodwater extent, and footprints.", "result": "Achieves 19% F1 improvement over baselines, with significant gains in minor/moderate damage categories.", "conclusion": "Flood-DamageSense provides faster, finer-grained flood-damage intelligence for post-disaster decision-making."}}
{"id": "2506.06589", "pdf": "https://arxiv.org/pdf/2506.06589", "abs": "https://arxiv.org/abs/2506.06589", "authors": ["Jacqueline He", "Howard Yen", "Margaret Li", "Shuyue Stella Li", "Zhiyuan Zeng", "Weijia Shi", "Yulia Tsvetkov", "Danqi Chen", "Pang Wei Koh", "Luke Zettlemoyer"], "title": "Precise Information Control in Long-Form Text Generation", "categories": ["cs.CL"], "comment": "56 pages, 8 figures. Code and models are publicly available at\n  https://github.com/jacqueline-he/precise-information-control", "summary": "A central challenge in modern language models (LMs) is intrinsic\nhallucination: the generation of information that is plausible but\nunsubstantiated relative to input context. To study this problem, we propose\nPrecise Information Control (PIC), a new task formulation that requires models\nto generate long-form outputs grounded in a provided set of short\nself-contained statements, known as verifiable claims, without adding any\nunsupported ones. For comprehensiveness, PIC includes a full setting that tests\na model's ability to include exactly all input claims, and a partial setting\nthat requires the model to selectively incorporate only relevant claims. We\npresent PIC-Bench, a benchmark of eight long-form generation tasks (e.g.,\nsummarization, biography generation) adapted to the PIC setting, where LMs are\nsupplied with well-formed, verifiable input claims. Our evaluation of a range\nof open and proprietary LMs on PIC-Bench reveals that, surprisingly,\nstate-of-the-art LMs still intrinsically hallucinate in over 70% of outputs. To\nalleviate this lack of faithfulness, we introduce a post-training framework,\nusing a weakly supervised preference data construction method, to train an 8B\nPIC-LM with stronger PIC ability--improving from 69.1% to 91.0% F1 in the full\nPIC setting. When integrated into end-to-end factual generation pipelines,\nPIC-LM improves exact match recall by 17.1% on ambiguous QA with retrieval, and\nfactual precision by 30.5% on a birthplace verification task, underscoring the\npotential of precisely grounded generation.", "AI": {"tldr": "The paper introduces Precise Information Control (PIC) to address intrinsic hallucination in LMs, proposes PIC-Bench for evaluation, and presents PIC-LM, a post-trained model improving faithfulness in generation.", "motivation": "To tackle the issue of intrinsic hallucination in LMs, where models generate unsubstantiated information.", "method": "Proposes PIC, a task requiring grounded generation, and PIC-Bench for evaluation. Introduces a post-training framework for PIC-LM.", "result": "State-of-the-art LMs hallucinate in over 70% of outputs. PIC-LM improves F1 from 69.1% to 91.0%.", "conclusion": "PIC-LM enhances faithfulness in generation, with significant improvements in factual precision and recall."}}
{"id": "2506.06680", "pdf": "https://arxiv.org/pdf/2506.06680", "abs": "https://arxiv.org/abs/2506.06680", "authors": ["Radha Kodali", "Venkata Rao Dhulipalla", "Venkata Siva Kishor Tatavarty", "Madhavi Nadakuditi", "Bharadwaj Thiruveedhula", "Suryanarayana Gunnam", "Durga Prasad Bavirisetti"], "title": "Interpretation of Deep Learning Model in Embryo Selection for In Vitro Fertilization (IVF) Treatment", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Infertility has a considerable impact on individuals' quality of life,\naffecting them socially and psychologically, with projections indicating a rise\nin the upcoming years. In vitro fertilization (IVF) emerges as one of the\nprimary techniques within economically developed nations, employed to address\nthe rising problem of low fertility. Expert embryologists conventionally grade\nembryos by reviewing blastocyst images to select the most optimal for transfer,\nyet this process is time-consuming and lacks efficiency. Blastocyst images\nprovide a valuable resource for assessing embryo viability. In this study, we\nintroduce an explainable artificial intelligence (XAI) framework for\nclassifying embryos, employing a fusion of convolutional neural network (CNN)\nand long short-term memory (LSTM) architecture, referred to as CNN-LSTM.\nUtilizing deep learning, our model achieves high accuracy in embryo\nclassification while maintaining interpretability through XAI.", "AI": {"tldr": "An XAI framework using CNN-LSTM for embryo classification in IVF improves accuracy and interpretability.", "motivation": "Infertility impacts quality of life, and IVF is a key solution. Current embryo grading is inefficient.", "method": "Combines CNN and LSTM (CNN-LSTM) for deep learning-based embryo classification with XAI for interpretability.", "result": "High accuracy in embryo classification achieved.", "conclusion": "The CNN-LSTM model enhances IVF efficiency and interpretability in embryo selection."}}
{"id": "2506.06605", "pdf": "https://arxiv.org/pdf/2506.06605", "abs": "https://arxiv.org/abs/2506.06605", "authors": ["Xiao Wang", "Mengjue Tan", "Qiao Jin", "Guangzhi Xiong", "Yu Hu", "Aidong Zhang", "Zhiyong Lu", "Minjia Zhang"], "title": "MedCite: Can Language Models Generate Verifiable Text for Medicine?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Existing LLM-based medical question-answering systems lack citation\ngeneration and evaluation capabilities, raising concerns about their adoption\nin practice. In this work, we introduce \\name, the first end-to-end framework\nthat facilitates the design and evaluation of citation generation with LLMs for\nmedical tasks. Meanwhile, we introduce a novel multi-pass retrieval-citation\nmethod that generates high-quality citations. Our evaluation highlights the\nchallenges and opportunities of citation generation for medical tasks, while\nidentifying important design choices that have a significant impact on the\nfinal citation quality. Our proposed method achieves superior citation\nprecision and recall improvements compared to strong baseline methods, and we\nshow that evaluation results correlate well with annotation results from\nprofessional experts.", "AI": {"tldr": "The paper introduces an end-to-end framework for citation generation in LLM-based medical QA systems, proposing a multi-pass retrieval-citation method that improves citation quality and evaluation.", "motivation": "Existing LLM-based medical QA systems lack citation generation and evaluation, limiting their practical adoption.", "method": "A novel multi-pass retrieval-citation method is introduced to generate high-quality citations.", "result": "The method outperforms baselines in citation precision and recall, with evaluation results aligning with expert annotations.", "conclusion": "The framework addresses challenges in citation generation for medical tasks, offering significant improvements and insights for future work."}}
{"id": "2506.06710", "pdf": "https://arxiv.org/pdf/2506.06710", "abs": "https://arxiv.org/abs/2506.06710", "authors": ["Qianqian Zhao", "Chunle Guo", "Tianyi Zhang", "Junpei Zhang", "Peiyang Jia", "Tan Su", "Wenjie Jiang", "Chongyi Li"], "title": "A Systematic Investigation on Deep Learning-Based Omnidirectional Image and Video Super-Resolution", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Omnidirectional image and video super-resolution is a crucial research topic\nin low-level vision, playing an essential role in virtual reality and augmented\nreality applications. Its goal is to reconstruct high-resolution images or\nvideo frames from low-resolution inputs, thereby enhancing detail preservation\nand enabling more accurate scene analysis and interpretation. In recent years,\nnumerous innovative and effective approaches have been proposed, predominantly\nbased on deep learning techniques, involving diverse network architectures,\nloss functions, projection strategies, and training datasets. This paper\npresents a systematic review of recent progress in omnidirectional image and\nvideo super-resolution, focusing on deep learning-based methods. Given that\nexisting datasets predominantly rely on synthetic degradation and fall short in\ncapturing real-world distortions, we introduce a new dataset, 360Insta, that\ncomprises authentically degraded omnidirectional images and videos collected\nunder diverse conditions, including varying lighting, motion, and exposure\nsettings. This dataset addresses a critical gap in current omnidirectional\nbenchmarks and enables more robust evaluation of the generalization\ncapabilities of omnidirectional super-resolution methods. We conduct\ncomprehensive qualitative and quantitative evaluations of existing methods on\nboth public datasets and our proposed dataset. Furthermore, we provide a\nsystematic overview of the current status of research and discuss promising\ndirections for future exploration. All datasets, methods, and evaluation\nmetrics introduced in this work are publicly available and will be regularly\nupdated. Project page: https://github.com/nqian1/Survey-on-ODISR-and-ODVSR.", "AI": {"tldr": "A review of omnidirectional image/video super-resolution (ODISR/ODVSR) focusing on deep learning methods, introducing a new dataset (360Insta) for real-world evaluation.", "motivation": "To address the lack of real-world datasets for evaluating omnidirectional super-resolution methods, which currently rely on synthetic degradation.", "method": "Systematic review of deep learning-based ODISR/ODVSR methods, introduction of the 360Insta dataset, and comprehensive evaluation of existing methods.", "result": "360Insta dataset fills a critical gap, enabling robust evaluation of generalization capabilities. Existing methods are evaluated qualitatively and quantitatively.", "conclusion": "The paper highlights current research status, future directions, and provides open access to datasets, methods, and metrics for ongoing updates."}}
{"id": "2506.06607", "pdf": "https://arxiv.org/pdf/2506.06607", "abs": "https://arxiv.org/abs/2506.06607", "authors": ["Charles Goddard", "Fernando Fernandes Neto"], "title": "Training-Free Tokenizer Transplantation via Orthogonal Matching Pursuit", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We present a training-free method to transplant tokenizers in pretrained\nlarge language models (LLMs) by reconstructing unseen token embeddings via\nOrthogonal Matching Pursuit (OMP). Specifically, we approximate each\nout-of-vocabulary token as a sparse linear combination of shared tokens, in two\nphases: first, compute each new token's representation in the donor embedding\nspace with a small dictionary of shared anchor tokens, then transfer these same\nsparse coefficients back into the base model's embedding space.\n  On two challenging cross-tokenizer tasks--Llama$\\to$Mistral NeMo (12B) and\nQwen$\\to$Llama (1B)--we show that OMP achieves best zero-shot preservation of\nthe base model's performance across multiple benchmarks, while other zero-shot\napproaches degrade significantly. Compared to baselines (zero-init, mean-init,\nand existing approaches like WECHSEL, FOCUS, ZETT), OMP consistently achieves\nthe best overall performance, effectively bridging large tokenizer\ndiscrepancies without gradient updates. Our analysis further identifies\nmismatched numerical tokenization schemes as a critical challenge for\npreserving mathematical reasoning capabilities. This technique enables direct\nreuse of pretrained model weights with new tokenizers, facilitating\ncross-tokenizer knowledge distillation, speculative decoding, ensembling,\nmerging, and domain-specific vocabulary adaptations. We integrate our method\ninto the open-source mergekit-tokensurgeon tool for post hoc vocabulary\nrealignment.", "AI": {"tldr": "A training-free method using Orthogonal Matching Pursuit (OMP) to transplant tokenizers in LLMs, achieving zero-shot performance preservation across benchmarks.", "motivation": "To enable reuse of pretrained model weights with new tokenizers without gradient updates, addressing cross-tokenizer challenges.", "method": "Reconstructs unseen token embeddings via OMP, approximating out-of-vocabulary tokens as sparse linear combinations of shared tokens.", "result": "OMP outperforms baselines in zero-shot performance, preserving model capabilities across benchmarks.", "conclusion": "OMP effectively bridges tokenizer discrepancies, enabling applications like knowledge distillation and domain-specific adaptations."}}
{"id": "2506.06712", "pdf": "https://arxiv.org/pdf/2506.06712", "abs": "https://arxiv.org/abs/2506.06712", "authors": ["Saiyu Hu", "Chunlei He", "Jianfeng Zhang", "Dexing Kong", "Shoujun Huang"], "title": "Active Contour Models Driven by Hyperbolic Mean Curvature Flow for Image Segmentation", "categories": ["cs.CV", "math.AP"], "comment": null, "summary": "Parabolic mean curvature flow-driven active contour models (PMCF-ACMs) are\nwidely used in image segmentation, which however depend heavily on the\nselection of initial curve configurations. In this paper, we firstly propose\nseveral hyperbolic mean curvature flow-driven ACMs (HMCF-ACMs), which introduce\ntunable initial velocity fields, enabling adaptive optimization for diverse\nsegmentation scenarios. We shall prove that HMCF-ACMs are indeed normal flows\nand establish the numerical equivalence between dissipative HMCF formulations\nand certain wave equations using the level set method with signed distance\nfunction. Building on this framework, we furthermore develop hyperbolic\ndual-mode regularized flow-driven ACMs (HDRF-ACMs), which utilize smooth\nHeaviside functions for edge-aware force modulation to suppress over-diffusion\nnear weak boundaries. Then, we optimize a weighted fourth-order Runge-Kutta\nalgorithm with nine-point stencil spatial discretization when solving the\nabove-mentioned wave equations. Experiments show that both HMCF-ACMs and\nHDRF-ACMs could achieve more precise segmentations with superior noise\nresistance and numerical stability due to task-adaptive configurations of\ninitial velocities and initial contours.", "AI": {"tldr": "The paper introduces hyperbolic mean curvature flow-driven active contour models (HMCF-ACMs) and their dual-mode variant (HDRF-ACMs) for adaptive image segmentation, proving their equivalence to wave equations and demonstrating improved precision, noise resistance, and stability.", "motivation": "Existing parabolic mean curvature flow-driven models (PMCF-ACMs) heavily depend on initial curve configurations, limiting adaptability. This work aims to overcome this by introducing tunable initial velocity fields.", "method": "Proposes HMCF-ACMs and HDRF-ACMs, establishes their equivalence to wave equations using level set methods, and optimizes a weighted fourth-order Runge-Kutta algorithm for solving these equations.", "result": "HMCF-ACMs and HDRF-ACMs achieve more precise segmentations with better noise resistance and numerical stability due to adaptive initial configurations.", "conclusion": "The proposed hyperbolic models outperform traditional PMCF-ACMs by enabling task-adaptive segmentation, validated through theoretical proofs and experiments."}}
{"id": "2506.06609", "pdf": "https://arxiv.org/pdf/2506.06609", "abs": "https://arxiv.org/abs/2506.06609", "authors": ["Alan Chen", "Jack Merullo", "Alessandro Stolfo", "Ellie Pavlick"], "title": "Transferring Features Across Language Models With Model Stitching", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "In this work, we demonstrate that affine mappings between residual streams of\nlanguage models is a cheap way to effectively transfer represented features\nbetween models. We apply this technique to transfer the weights of Sparse\nAutoencoders (SAEs) between models of different sizes to compare their\nrepresentations. We find that small and large models learn highly similar\nrepresentation spaces, which motivates training expensive components like SAEs\non a smaller model and transferring to a larger model at a FLOPs savings. For\nexample, using a small-to-large transferred SAE as initialization can lead to\n50% cheaper training runs when training SAEs on larger models. Next, we show\nthat transferred probes and steering vectors can effectively recover ground\ntruth performance. Finally, we dive deeper into feature-level transferability,\nfinding that semantic and structural features transfer noticeably differently\nwhile specific classes of functional features have their roles faithfully\nmapped. Overall, our findings illustrate similarities and differences in the\nlinear representation spaces of small and large models and demonstrate a method\nfor improving the training efficiency of SAEs.", "AI": {"tldr": "Affine mappings between residual streams of language models enable cheap and effective feature transfer, improving SAE training efficiency.", "motivation": "To explore the transferability of features between small and large language models and reduce computational costs.", "method": "Use affine mappings to transfer weights of Sparse Autoencoders (SAEs) between models of different sizes and analyze representation spaces.", "result": "Small and large models learn similar representations, enabling 50% FLOPs savings in SAE training. Transferred probes and steering vectors recover ground truth performance.", "conclusion": "Feature transfer between models is feasible and efficient, with semantic and structural features transferring differently, offering insights into model representation spaces."}}
{"id": "2506.06719", "pdf": "https://arxiv.org/pdf/2506.06719", "abs": "https://arxiv.org/abs/2506.06719", "authors": ["Mufhumudzi Muthivhi", "Jiahao Huo", "Fredrik Gustafsson", "Terence L. van Zyl"], "title": "Improving Wildlife Out-of-Distribution Detection: Africas Big Five", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Mitigating human-wildlife conflict seeks to resolve unwanted encounters\nbetween these parties. Computer Vision provides a solution to identifying\nindividuals that might escalate into conflict, such as members of the Big Five\nAfrican animals. However, environments often contain several varied species.\nThe current state-of-the-art animal classification models are trained under a\nclosed-world assumption. They almost always remain overconfident in their\npredictions even when presented with unknown classes. This study investigates\nout-of-distribution (OOD) detection of wildlife, specifically the Big Five. To\nthis end, we select a parametric Nearest Class Mean (NCM) and a non-parametric\ncontrastive learning approach as baselines to take advantage of pretrained and\nprojected features from popular classification encoders. Moreover, we compare\nour baselines to various common OOD methods in the literature. The results show\nfeature-based methods reflect stronger generalisation capability across varying\nclassification thresholds. Specifically, NCM with ImageNet pre-trained features\nachieves a 2%, 4% and 22% improvement on AUPR-IN, AUPR-OUT and AUTC over the\nbest OOD methods, respectively. The code can be found here\nhttps://github.com/pxpana/BIG5OOD", "AI": {"tldr": "The paper explores out-of-distribution (OOD) detection for wildlife, focusing on the Big Five African animals, to improve human-wildlife conflict mitigation. It compares parametric and non-parametric methods, showing feature-based approaches outperform traditional OOD methods.", "motivation": "Human-wildlife conflict requires accurate identification of animals, but current models fail with unknown species. This study aims to enhance OOD detection for better conflict resolution.", "method": "The study uses parametric Nearest Class Mean (NCM) and non-parametric contrastive learning, leveraging pretrained features from classification encoders, and compares them to common OOD techniques.", "result": "Feature-based methods, especially NCM with ImageNet features, outperform others, improving AUPR-IN by 2%, AUPR-OUT by 4%, and AUTC by 22%.", "conclusion": "Feature-based OOD detection, particularly NCM, offers superior generalization for wildlife identification, aiding human-wildlife conflict mitigation."}}
{"id": "2506.06616", "pdf": "https://arxiv.org/pdf/2506.06616", "abs": "https://arxiv.org/abs/2506.06616", "authors": ["Samuel Kim", "Oghenemaro Imieye", "Yunting Yin"], "title": "Interpretable Depression Detection from Social Media Text Using LLM-Derived Embeddings", "categories": ["cs.CL"], "comment": "Submitted to the IEEE EMBS BHI 2025 Conference", "summary": "Accurate and interpretable detection of depressive language in social media\nis useful for early interventions of mental health conditions, and has\nimportant implications for both clinical practice and broader public health\nefforts. In this paper, we investigate the performance of large language models\n(LLMs) and traditional machine learning classifiers across three classification\ntasks involving social media data: binary depression classification, depression\nseverity classification, and differential diagnosis classification among\ndepression, PTSD, and anxiety. Our study compares zero-shot LLMs with\nsupervised classifiers trained on both conventional text embeddings and\nLLM-generated summary embeddings. Our experiments reveal that while zero-shot\nLLMs demonstrate strong generalization capabilities in binary classification,\nthey struggle with fine-grained ordinal classifications. In contrast,\nclassifiers trained on summary embeddings generated by LLMs demonstrate\ncompetitive, and in some cases superior, performance on the classification\ntasks, particularly when compared to models using traditional text embeddings.\nOur findings demonstrate the strengths of LLMs in mental health prediction, and\nsuggest promising directions for better utilization of their zero-shot\ncapabilities and context-aware summarization techniques.", "AI": {"tldr": "The paper evaluates LLMs and traditional classifiers for detecting depressive language in social media, finding LLMs strong in binary classification but weaker in fine-grained tasks, while LLM-generated summary embeddings outperform traditional methods.", "motivation": "To improve early mental health interventions by accurately detecting depressive language in social media using LLMs and traditional classifiers.", "method": "Comparison of zero-shot LLMs and supervised classifiers (using traditional and LLM-generated embeddings) across three tasks: binary depression, severity, and differential diagnosis classification.", "result": "Zero-shot LLMs excel in binary classification but struggle with fine-grained tasks; LLM-generated summary embeddings outperform traditional embeddings.", "conclusion": "LLMs show promise for mental health prediction, with potential for better use of zero-shot capabilities and context-aware summarization."}}
{"id": "2506.06729", "pdf": "https://arxiv.org/pdf/2506.06729", "abs": "https://arxiv.org/abs/2506.06729", "authors": ["Zixian Gao", "Chao Yang", "Zhanhui Zhou", "Xing Xu", "Chaochao Lu"], "title": "Mitigating Object Hallucination via Robust Local Perception Search", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have enabled\nthem to effectively integrate vision and language, addressing a variety of\ndownstream tasks. However, despite their significant success, these models\nstill exhibit hallucination phenomena, where the outputs appear plausible but\ndo not align with the content of the images. To mitigate this issue, we\nintroduce Local Perception Search (LPS), a decoding method during inference\nthat is both simple and training-free, yet effectively suppresses\nhallucinations. This method leverages local visual prior information as a value\nfunction to correct the decoding process. Additionally, we observe that the\nimpact of the local visual prior on model performance is more pronounced in\nscenarios with high levels of image noise. Notably, LPS is a plug-and-play\napproach that is compatible with various models. Extensive experiments on\nwidely used hallucination benchmarks and noisy data demonstrate that LPS\nsignificantly reduces the incidence of hallucinations compared to the baseline,\nshowing exceptional performance, particularly in noisy settings.", "AI": {"tldr": "The paper introduces Local Perception Search (LPS), a training-free decoding method to reduce hallucinations in Multimodal Large Language Models (MLLMs) by leveraging local visual priors.", "motivation": "MLLMs often produce plausible but incorrect outputs (hallucinations) that don't align with image content, especially in noisy settings.", "method": "LPS uses local visual prior information as a value function to correct decoding during inference, without requiring additional training.", "result": "LPS significantly reduces hallucinations, especially in noisy scenarios, and is compatible with various models.", "conclusion": "LPS is an effective, plug-and-play solution for mitigating hallucinations in MLLMs, particularly under noisy conditions."}}
{"id": "2506.06619", "pdf": "https://arxiv.org/pdf/2506.06619", "abs": "https://arxiv.org/abs/2506.06619", "authors": ["Jesse Woo", "Fateme Hashemi Chaleshtori", "Ana Marasovi\u0107", "Kenneth Marino"], "title": "BriefMe: A Legal NLP Benchmark for Assisting with Legal Briefs", "categories": ["cs.CL"], "comment": "ACL Findings 2025; 10 pages main, 5 pages references, 37 pages\n  appendix", "summary": "A core part of legal work that has been under-explored in Legal NLP is the\nwriting and editing of legal briefs. This requires not only a thorough\nunderstanding of the law of a jurisdiction, from judgments to statutes, but\nalso the ability to make new arguments to try to expand the law in a new\ndirection and make novel and creative arguments that are persuasive to judges.\nTo capture and evaluate these legal skills in language models, we introduce\nBRIEFME, a new dataset focused on legal briefs. It contains three tasks for\nlanguage models to assist legal professionals in writing briefs: argument\nsummarization, argument completion, and case retrieval. In this work, we\ndescribe the creation of these tasks, analyze them, and show how current models\nperform. We see that today's large language models (LLMs) are already quite\ngood at the summarization and guided completion tasks, even beating\nhuman-generated headings. Yet, they perform poorly on other tasks in our\nbenchmark: realistic argument completion and retrieving relevant legal cases.\nWe hope this dataset encourages more development in Legal NLP in ways that will\nspecifically aid people in performing legal work.", "AI": {"tldr": "The paper introduces BRIEFME, a dataset for evaluating language models in legal brief tasks like summarization, argument completion, and case retrieval, showing strengths in summarization but weaknesses in realistic argument completion and case retrieval.", "motivation": "To address the under-explored area of legal brief writing in Legal NLP by evaluating language models' ability to assist in legal tasks.", "method": "Creation of the BRIEFME dataset with three tasks: argument summarization, argument completion, and case retrieval, followed by analysis of model performance.", "result": "LLMs excel in summarization and guided completion but struggle with realistic argument completion and case retrieval.", "conclusion": "The dataset aims to spur further Legal NLP development to better assist legal professionals."}}
{"id": "2506.06733", "pdf": "https://arxiv.org/pdf/2506.06733", "abs": "https://arxiv.org/abs/2506.06733", "authors": ["Ruoxuan Zhang", "Jidong Gao", "Bin Wen", "Hongxia Xie", "Chenming Zhang", "Honghan-shuai", "Wen-Huang Cheng"], "title": "RecipeGen: A Step-Aligned Multimodal Benchmark for Real-World Recipe Generation", "categories": ["cs.CV"], "comment": "This is an extended version of arXiv:2503.05228", "summary": "Creating recipe images is a key challenge in food computing, with\napplications in culinary education and multimodal recipe assistants. However,\nexisting datasets lack fine-grained alignment between recipe goals, step-wise\ninstructions, and visual content. We present RecipeGen, the first large-scale,\nreal-world benchmark for recipe-based Text-to-Image (T2I), Image-to-Video\n(I2V), and Text-to-Video (T2V) generation. RecipeGen contains 26,453 recipes,\n196,724 images, and 4,491 videos, covering diverse ingredients, cooking\nprocedures, styles, and dish types. We further propose domain-specific\nevaluation metrics to assess ingredient fidelity and interaction modeling,\nbenchmark representative T2I, I2V, and T2V models, and provide insights for\nfuture recipe generation models. Project page is available now.", "AI": {"tldr": "RecipeGen is a large-scale benchmark for recipe-based text-to-image, image-to-video, and text-to-video generation, addressing the lack of fine-grained alignment in existing datasets.", "motivation": "Existing datasets lack alignment between recipe goals, instructions, and visuals, hindering applications in culinary education and recipe assistants.", "method": "RecipeGen introduces 26,453 recipes, 196,724 images, and 4,491 videos, with domain-specific metrics for evaluating ingredient fidelity and interaction modeling.", "result": "The benchmark evaluates T2I, I2V, and T2V models, providing insights for future recipe generation.", "conclusion": "RecipeGen fills a gap in food computing by offering a comprehensive dataset and evaluation framework for recipe-based visual content generation."}}
{"id": "2506.06626", "pdf": "https://arxiv.org/pdf/2506.06626", "abs": "https://arxiv.org/abs/2506.06626", "authors": ["Junzhe Wang", "Bichen Wang", "Xing Fu", "Yixin Sun", "Yanyan Zhao", "Bing Qin"], "title": "Psychological Counseling Cannot Be Achieved Overnight: Automated Psychological Counseling Through Multi-Session Conversations", "categories": ["cs.CL"], "comment": "15 pages, 19 figures", "summary": "In recent years, Large Language Models (LLMs) have made significant progress\nin automated psychological counseling. However, current research focuses on\nsingle-session counseling, which doesn't represent real-world scenarios. In\npractice, psychological counseling is a process, not a one-time event,\nrequiring sustained, multi-session engagement to progressively address clients'\nissues. To overcome this limitation, we introduce a dataset for Multi-Session\nPsychological Counseling Conversation Dataset (MusPsy-Dataset). Our\nMusPsy-Dataset is constructed using real client profiles from publicly\navailable psychological case reports. It captures the dynamic arc of\ncounseling, encompassing multiple progressive counseling conversations from the\nsame client across different sessions. Leveraging our dataset, we also\ndeveloped our MusPsy-Model, which aims to track client progress and adapt its\ncounseling direction over time. Experiments show that our model performs better\nthan baseline models across multiple sessions.", "AI": {"tldr": "The paper introduces a dataset (MusPsy-Dataset) and model (MusPsy-Model) for multi-session psychological counseling, addressing the gap in current LLM research focused on single-session scenarios.", "motivation": "Current LLM research lacks focus on multi-session psychological counseling, which is more reflective of real-world practice.", "method": "The authors created MusPsy-Dataset from real client profiles and developed MusPsy-Model to track client progress across sessions.", "result": "Experiments show MusPsy-Model outperforms baseline models in multi-session counseling.", "conclusion": "The work advances LLM applications in psychological counseling by addressing the need for sustained, multi-session engagement."}}
{"id": "2506.06748", "pdf": "https://arxiv.org/pdf/2506.06748", "abs": "https://arxiv.org/abs/2506.06748", "authors": ["Mingqi Gao", "Haoran Duan", "Tianlu Zhang", "Jungong Han"], "title": "THU-Warwick Submission for EPIC-KITCHEN Challenge 2025: Semi-Supervised Video Object Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "In this report, we describe our approach to egocentric video object\nsegmentation. Our method combines large-scale visual pretraining from SAM2 with\ndepth-based geometric cues to handle complex scenes and long-term tracking. By\nintegrating these signals in a unified framework, we achieve strong\nsegmentation performance. On the VISOR test set, our method reaches a J&F score\nof 90.1%.", "AI": {"tldr": "The paper presents a method for egocentric video object segmentation by combining visual pretraining (SAM2) and depth-based geometric cues, achieving a high J&F score of 90.1% on the VISOR test set.", "motivation": "To address the challenges of complex scenes and long-term tracking in egocentric video object segmentation.", "method": "Combines large-scale visual pretraining (SAM2) with depth-based geometric cues in a unified framework.", "result": "Achieves a J&F score of 90.1% on the VISOR test set.", "conclusion": "The integrated approach effectively handles complex scenes and long-term tracking, demonstrating strong segmentation performance."}}
{"id": "2506.06636", "pdf": "https://arxiv.org/pdf/2506.06636", "abs": "https://arxiv.org/abs/2506.06636", "authors": ["Chuxue Cao", "Han Zhu", "Jiaming Ji", "Qichao Sun", "Zhenghao Zhu", "Yinyu Wu", "Juntao Dai", "Yaodong Yang", "Sirui Han", "Yike Guo"], "title": "SafeLawBench: Towards Safe Alignment of Large Language Models", "categories": ["cs.CL"], "comment": "Accepted to ACL2025 Findings", "summary": "With the growing prevalence of large language models (LLMs), the safety of\nLLMs has raised significant concerns. However, there is still a lack of\ndefinitive standards for evaluating their safety due to the subjective nature\nof current safety benchmarks. To address this gap, we conducted the first\nexploration of LLMs' safety evaluation from a legal perspective by proposing\nthe SafeLawBench benchmark. SafeLawBench categorizes safety risks into three\nlevels based on legal standards, providing a systematic and comprehensive\nframework for evaluation. It comprises 24,860 multi-choice questions and 1,106\nopen-domain question-answering (QA) tasks. Our evaluation included 2\nclosed-source LLMs and 18 open-source LLMs using zero-shot and few-shot\nprompting, highlighting the safety features of each model. We also evaluated\nthe LLMs' safety-related reasoning stability and refusal behavior.\nAdditionally, we found that a majority voting mechanism can enhance model\nperformance. Notably, even leading SOTA models like Claude-3.5-Sonnet and\nGPT-4o have not exceeded 80.5% accuracy in multi-choice tasks on SafeLawBench,\nwhile the average accuracy of 20 LLMs remains at 68.8\\%. We urge the community\nto prioritize research on the safety of LLMs.", "AI": {"tldr": "The paper introduces SafeLawBench, a legal-based benchmark for evaluating LLM safety, categorizing risks into three levels and testing 20 models, revealing gaps even in top models.", "motivation": "Address the lack of definitive standards for LLM safety evaluation due to subjective benchmarks by proposing a legal perspective.", "method": "Developed SafeLawBench with 24,860 multi-choice and 1,106 open-domain QA tasks, evaluating 20 LLMs using zero-shot and few-shot prompting, and analyzing reasoning stability and refusal behavior.", "result": "Top models like Claude-3.5-Sonnet and GPT-4o scored below 80.5% accuracy; average accuracy was 68.8%. Majority voting improved performance.", "conclusion": "The community should prioritize LLM safety research, as current models, including SOTA, underperform on legal-based safety benchmarks."}}
{"id": "2506.06757", "pdf": "https://arxiv.org/pdf/2506.06757", "abs": "https://arxiv.org/abs/2506.06757", "authors": ["Ziyu Yue", "Ruixi You", "Feng Xu"], "title": "SAR2Struct: Extracting 3D Semantic Structural Representation of Aircraft Targets from Single-View SAR Image", "categories": ["cs.CV"], "comment": "13 pages, 12 figures", "summary": "To translate synthetic aperture radar (SAR) image into interpretable forms\nfor human understanding is the ultimate goal of SAR advanced information\nretrieval. Existing methods mainly focus on 3D surface reconstruction or local\ngeometric feature extraction of targets, neglecting the role of structural\nmodeling in capturing semantic information. This paper proposes a novel task:\nSAR target structure recovery, which aims to infer the components of a target\nand the structural relationships between its components, specifically symmetry\nand adjacency, from a single-view SAR image. Through learning the structural\nconsistency and geometric diversity across the same type of targets as observed\nin different SAR images, it aims to derive the semantic representation of\ntarget directly from its 2D SAR image. To solve this challenging task, a\ntwo-step algorithmic framework based on structural descriptors is developed.\nSpecifically, in the training phase, it first detects 2D keypoints from real\nSAR images, and then learns the mapping from these keypoints to 3D hierarchical\nstructures using simulated data. During the testing phase, these two steps are\nintegrated to infer the 3D structure from real SAR images. Experimental results\nvalidated the effectiveness of each step and demonstrated, for the first time,\nthat 3D semantic structural representation of aircraft targets can be directly\nderived from a single-view SAR image.", "AI": {"tldr": "The paper introduces SAR target structure recovery, a novel task to infer target components and their structural relationships from single-view SAR images, using a two-step framework based on structural descriptors.", "motivation": "Existing SAR image methods focus on 3D reconstruction or local features, missing semantic structural modeling. This work aims to bridge this gap by capturing semantic information through structural relationships.", "method": "A two-step framework: (1) detect 2D keypoints from real SAR images, (2) learn mapping from keypoints to 3D hierarchical structures using simulated data, integrating both steps for inference.", "result": "The method successfully derives 3D semantic structural representations of aircraft targets from single-view SAR images, validated by experiments.", "conclusion": "The proposed approach effectively recovers target structures from SAR images, demonstrating the feasibility of semantic representation extraction directly from 2D SAR data."}}
{"id": "2506.06657", "pdf": "https://arxiv.org/pdf/2506.06657", "abs": "https://arxiv.org/abs/2506.06657", "authors": ["Nikhita Vedula", "Dushyanta Dhyani", "Laleh Jalali", "Boris Oreshkin", "Mohsen Bayati", "Shervin Malmasi"], "title": "Quantile Regression with Large Language Models for Price Prediction", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to Findings of ACL, 2025", "summary": "Large Language Models (LLMs) have shown promise in structured prediction\ntasks, including regression, but existing approaches primarily focus on point\nestimates and lack systematic comparison across different methods. We\ninvestigate probabilistic regression using LLMs for unstructured inputs,\naddressing challenging text-to-distribution prediction tasks such as price\nestimation where both nuanced text understanding and uncertainty quantification\nare critical. We propose a novel quantile regression approach that enables LLMs\nto produce full predictive distributions, improving upon traditional point\nestimates. Through extensive experiments across three diverse price prediction\ndatasets, we demonstrate that a Mistral-7B model fine-tuned with quantile heads\nsignificantly outperforms traditional approaches for both point and\ndistributional estimations, as measured by three established metrics each for\nprediction accuracy and distributional calibration. Our systematic comparison\nof LLM approaches, model architectures, training approaches, and data scaling\nreveals that Mistral-7B consistently outperforms encoder architectures,\nembedding-based methods, and few-shot learning methods. Our experiments also\nreveal the effectiveness of LLM-assisted label correction in achieving\nhuman-level accuracy without systematic bias. Our curated datasets are made\navailable at https://github.com/vnik18/llm-price-quantile-reg/ to support\nfuture research.", "AI": {"tldr": "The paper explores probabilistic regression with LLMs for text-to-distribution tasks like price estimation, proposing a quantile regression method to improve predictive distributions. Fine-tuned Mistral-7B outperforms traditional methods in accuracy and calibration.", "motivation": "Existing LLM approaches lack systematic comparison and focus on point estimates, missing uncertainty quantification crucial for tasks like price estimation.", "method": "A novel quantile regression approach is introduced, enabling LLMs to produce predictive distributions. Mistral-7B is fine-tuned with quantile heads and tested on price prediction datasets.", "result": "Mistral-7B outperforms traditional methods in accuracy and calibration, and excels over encoder architectures, embedding-based methods, and few-shot learning.", "conclusion": "The proposed method enhances LLM-based regression, achieving human-level accuracy with label correction. Datasets are shared for future research."}}
{"id": "2506.06759", "pdf": "https://arxiv.org/pdf/2506.06759", "abs": "https://arxiv.org/abs/2506.06759", "authors": ["Nidheesh Gorthi", "Kartik Thakral", "Rishabh Ranjan", "Richa Singh", "Mayank Vatsa"], "title": "LitMAS: A Lightweight and Generalized Multi-Modal Anti-Spoofing Framework for Biometric Security", "categories": ["cs.CV"], "comment": "Accepted in Interspeech 2025", "summary": "Biometric authentication systems are increasingly being deployed in critical\napplications, but they remain susceptible to spoofing. Since most of the\nresearch efforts focus on modality-specific anti-spoofing techniques, building\na unified, resource-efficient solution across multiple biometric modalities\nremains a challenge. To address this, we propose LitMAS, a\n$\\textbf{Li}$gh$\\textbf{t}$ weight and generalizable $\\textbf{M}$ulti-modal\n$\\textbf{A}$nti-$\\textbf{S}$poofing framework designed to detect spoofing\nattacks in speech, face, iris, and fingerprint-based biometric systems. At the\ncore of LitMAS is a Modality-Aligned Concentration Loss, which enhances\ninter-class separability while preserving cross-modal consistency and enabling\nrobust spoof detection across diverse biometric traits. With just 6M\nparameters, LitMAS surpasses state-of-the-art methods by $1.36\\%$ in average\nEER across seven datasets, demonstrating high efficiency, strong\ngeneralizability, and suitability for edge deployment. Code and trained models\nare available at https://github.com/IAB-IITJ/LitMAS.", "AI": {"tldr": "LitMAS is a lightweight, generalizable multi-modal anti-spoofing framework for biometric systems, outperforming state-of-the-art methods with high efficiency and edge deployment suitability.", "motivation": "Biometric systems are vulnerable to spoofing, and existing solutions lack a unified, resource-efficient approach across multiple modalities.", "method": "Proposes LitMAS with a Modality-Aligned Concentration Loss to enhance inter-class separability and cross-modal consistency for robust spoof detection.", "result": "LitMAS achieves a 1.36% lower average EER than state-of-the-art methods across seven datasets, using only 6M parameters.", "conclusion": "LitMAS is efficient, generalizable, and suitable for edge deployment, with code and models publicly available."}}
{"id": "2506.06686", "pdf": "https://arxiv.org/pdf/2506.06686", "abs": "https://arxiv.org/abs/2506.06686", "authors": ["Chunyuan Deng", "Ruidi Chang", "Hanjie Chen"], "title": "Learning Distribution-Wise Control in Representation Space for Language Models", "categories": ["cs.CL"], "comment": "ICML 2025", "summary": "Interventions in language models (LMs) are applied strategically to steer\nmodel behavior during the forward pass. Learnable interventions, also known as\nrepresentation fine-tuning, aim to apply pointwise control within the concept\nsubspace and have proven effective in altering high-level behaviors. In this\nwork, we extend this approach to the distribution level, enabling the model to\nlearn not only pointwise transformations but also the surrounding regions of\nthe concept subspace. We demonstrate that these methods perform effectively in\nearly layers, with larger standard deviations correlating strongly with\nimproved performance. Across eight commonsense reasoning and seven arithmetic\nreasoning benchmarks, our distribution-wise interventions consistently\noutperform pointwise interventions in controllability and robustness. These\nresults illustrate that distribution-wise interventions provide a more\ncomprehensive method for steering model behavior and enabling finer-grained\ncontrol over language models. The code is at:\n\\href{https://github.com/chili-lab/D-Intervention}{https://github.com/chili-lab/D-Intervention}.", "AI": {"tldr": "The paper extends learnable interventions in language models from pointwise to distribution level, improving controllability and robustness.", "motivation": "To enhance fine-grained control over language models by learning transformations in the concept subspace at the distribution level.", "method": "Distribution-wise interventions are applied to learn transformations and surrounding regions of the concept subspace, tested on commonsense and arithmetic reasoning benchmarks.", "result": "Distribution-wise interventions outperform pointwise methods in controllability and robustness across benchmarks.", "conclusion": "Distribution-wise interventions offer a more comprehensive approach for steering language model behavior."}}
{"id": "2506.06771", "pdf": "https://arxiv.org/pdf/2506.06771", "abs": "https://arxiv.org/abs/2506.06771", "authors": ["Mohammad-Maher Nakshbandi", "Ziad Sharawy", "Dorian Cojocaru", "Sorin Grigorescu"], "title": "LoopDB: A Loop Closure Dataset for Large Scale Simultaneous Localization and Mapping", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "In this study, we introduce LoopDB, which is a challenging loop closure\ndataset comprising over 1000 images captured across diverse environments,\nincluding parks, indoor scenes, parking spaces, as well as centered around\nindividual objects. Each scene is represented by a sequence of five consecutive\nimages. The dataset was collected using a high resolution camera, providing\nsuitable imagery for benchmarking the accuracy of loop closure algorithms,\ntypically used in simultaneous localization and mapping. As ground truth\ninformation, we provide computed rotations and translations between each\nconsecutive images. Additional to its benchmarking goal, the dataset can be\nused to train and fine-tune loop closure methods based on deep neural networks.\nLoopDB is publicly available at https://github.com/RovisLab/LoopDB.", "AI": {"tldr": "LoopDB is a dataset of 1000+ images for benchmarking and training loop closure algorithms, with ground truth data for rotations and translations.", "motivation": "To provide a challenging and diverse dataset for evaluating and improving loop closure algorithms in SLAM applications.", "method": "Collected 1000+ images across varied environments using a high-resolution camera, with sequences of five consecutive images per scene. Ground truth includes rotations and translations.", "result": "LoopDB offers a robust dataset for benchmarking and training loop closure methods, including deep neural networks.", "conclusion": "LoopDB is publicly available and serves as a valuable resource for advancing loop closure research."}}
{"id": "2506.06704", "pdf": "https://arxiv.org/pdf/2506.06704", "abs": "https://arxiv.org/abs/2506.06704", "authors": ["Weihang Su", "Qingyao Ai", "Jingtao Zhan", "Qian Dong", "Yiqun Liu"], "title": "Dynamic and Parametric Retrieval-Augmented Generation", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has become a foundational paradigm for\nequipping large language models (LLMs) with external knowledge, playing a\ncritical role in information retrieval and knowledge-intensive applications.\nHowever, conventional RAG systems typically adopt a static\nretrieve-then-generate pipeline and rely on in-context knowledge injection,\nwhich can be suboptimal for complex tasks that require multihop reasoning,\nadaptive information access, and deeper integration of external knowledge.\nMotivated by these limitations, the research community has moved beyond static\nretrieval and in-context knowledge injection. Among the emerging directions,\nthis tutorial delves into two rapidly growing and complementary research areas\non RAG: Dynamic RAG and Parametric RAG. Dynamic RAG adaptively determines when\nand what to retrieve during the LLM's generation process, enabling real-time\nadaptation to the LLM's evolving information needs. Parametric RAG rethinks how\nretrieved knowledge should be injected into LLMs, transitioning from\ninput-level to parameter-level knowledge injection for enhanced efficiency and\neffectiveness. This tutorial offers a comprehensive overview of recent advances\nin these emerging research areas. It also shares theoretical foundations and\npractical insights to support and inspire further research in RAG.", "AI": {"tldr": "The paper discusses advancements in Retrieval-Augmented Generation (RAG), focusing on Dynamic RAG and Parametric RAG to address limitations of static retrieval and in-context knowledge injection.", "motivation": "To overcome the suboptimal performance of conventional RAG systems in complex tasks requiring multihop reasoning and adaptive knowledge integration.", "method": "Introduces Dynamic RAG for adaptive retrieval during generation and Parametric RAG for parameter-level knowledge injection.", "result": "Provides a comprehensive overview of these emerging research areas, offering theoretical and practical insights.", "conclusion": "The tutorial aims to inspire further research in RAG by addressing current limitations with innovative approaches."}}
{"id": "2506.06780", "pdf": "https://arxiv.org/pdf/2506.06780", "abs": "https://arxiv.org/abs/2506.06780", "authors": ["Lennart Bastian", "Mohammad Rashed", "Nassir Navab", "Tolga Birdal"], "title": "Continuous-Time SO(3) Forecasting with Savitzky--Golay Neural Controlled Differential Equations", "categories": ["cs.CV", "cs.LG"], "comment": "Extended abstract, presented at the CVPR Workshop on 4D Vision", "summary": "Tracking and forecasting the rotation of objects is fundamental in computer\nvision and robotics, yet SO(3) extrapolation remains challenging as (1) sensor\nobservations can be noisy and sparse, (2) motion patterns can be governed by\ncomplex dynamics, and (3) application settings can demand long-term\nforecasting. This work proposes modeling continuous-time rotational object\ndynamics on $SO(3)$ using Neural Controlled Differential Equations guided by\nSavitzky-Golay paths. Unlike existing methods that rely on simplified motion\nassumptions, our method learns a general latent dynamical system of the\nunderlying object trajectory while respecting the geometric structure of\nrotations. Experimental results on real-world data demonstrate compelling\nforecasting capabilities compared to existing approaches.", "AI": {"tldr": "Proposes a method using Neural Controlled Differential Equations and Savitzky-Golay paths to model rotational dynamics on SO(3), outperforming existing approaches.", "motivation": "Challenges in SO(3) extrapolation due to noisy/sparse observations, complex dynamics, and long-term forecasting needs.", "method": "Uses Neural Controlled Differential Equations guided by Savitzky-Golay paths to learn latent dynamics while respecting SO(3) structure.", "result": "Demonstrates superior forecasting capabilities on real-world data compared to existing methods.", "conclusion": "The method effectively addresses SO(3) extrapolation challenges by learning general dynamics without simplifying assumptions."}}
{"id": "2506.06705", "pdf": "https://arxiv.org/pdf/2506.06705", "abs": "https://arxiv.org/abs/2506.06705", "authors": ["Zhihui Chen", "Kai He", "Yucheng Huang", "Yunxiao Zhu", "Mengling Feng"], "title": "DivScore: Zero-Shot Detection of LLM-Generated Text in Specialized Domains", "categories": ["cs.CL", "cs.AI"], "comment": "Zhihui Chen and Kai He contributed equally to this work, Mengling\n  Feng is the corresponding author", "summary": "Detecting LLM-generated text in specialized and high-stakes domains like\nmedicine and law is crucial for combating misinformation and ensuring\nauthenticity. However, current zero-shot detectors, while effective on general\ntext, often fail when applied to specialized content due to domain shift. We\nprovide a theoretical analysis showing this failure is fundamentally linked to\nthe KL divergence between human, detector, and source text distributions. To\naddress this, we propose DivScore, a zero-shot detection framework using\nnormalized entropy-based scoring and domain knowledge distillation to robustly\nidentify LLM-generated text in specialized domains. We also release a\ndomain-specific benchmark for LLM-generated text detection in the medical and\nlegal domains. Experiments on our benchmark show that DivScore consistently\noutperforms state-of-the-art detectors, with 14.4% higher AUROC and 64.0%\nhigher recall (0.1% false positive rate threshold). In adversarial settings,\nDivScore demonstrates superior robustness than other baselines, achieving on\naverage 22.8% advantage in AUROC and 29.5% in recall. Code and data are\npublicly available.", "AI": {"tldr": "DivScore, a zero-shot detection framework, outperforms existing methods in identifying LLM-generated text in specialized domains like medicine and law, addressing domain shift issues.", "motivation": "Current detectors fail in specialized domains due to domain shift, risking misinformation and authenticity issues.", "method": "Proposes DivScore, using normalized entropy-based scoring and domain knowledge distillation.", "result": "DivScore achieves 14.4% higher AUROC and 64.0% higher recall than state-of-the-art detectors.", "conclusion": "DivScore is robust and effective for detecting LLM-generated text in high-stakes domains."}}
{"id": "2506.06802", "pdf": "https://arxiv.org/pdf/2506.06802", "abs": "https://arxiv.org/abs/2506.06802", "authors": ["Mohammad Ali Rezaei", "Helia Hajikazem", "Saeed Khanehgir", "Mahdi Javanmardi"], "title": "Training-Free Identity Preservation in Stylized Image Generation Using Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "While diffusion models have demonstrated remarkable generative capabilities,\nexisting style transfer techniques often struggle to maintain identity while\nachieving high-quality stylization. This limitation is particularly acute for\nimages where faces are small or exhibit significant camera-to-face distances,\nfrequently leading to inadequate identity preservation. To address this, we\nintroduce a novel, training-free framework for identity-preserved stylized\nimage synthesis using diffusion models. Key contributions include: (1) the\n\"Mosaic Restored Content Image\" technique, significantly enhancing identity\nretention, especially in complex scenes; and (2) a training-free content\nconsistency loss that enhances the preservation of fine-grained content details\nby directing more attention to the original image during stylization. Our\nexperiments reveal that the proposed approach substantially surpasses the\nbaseline model in concurrently maintaining high stylistic fidelity and robust\nidentity integrity, particularly under conditions of small facial regions or\nsignificant camera-to-face distances, all without necessitating model\nretraining or fine-tuning.", "AI": {"tldr": "A training-free framework for identity-preserved stylized image synthesis using diffusion models, improving identity retention and content consistency without retraining.", "motivation": "Existing style transfer techniques struggle with identity preservation, especially for small or distant faces.", "method": "Introduces 'Mosaic Restored Content Image' and a training-free content consistency loss to enhance identity and detail retention.", "result": "Outperforms baseline models in maintaining stylistic fidelity and identity integrity, especially for small or distant faces.", "conclusion": "The proposed framework effectively addresses identity preservation in style transfer without requiring model retraining."}}
{"id": "2506.06708", "pdf": "https://arxiv.org/pdf/2506.06708", "abs": "https://arxiv.org/abs/2506.06708", "authors": ["Haiqi Yang", "Zhiyuan Li", "Yi Chang", "Yuan Wu"], "title": "A Survey of Retentive Network", "categories": ["cs.CL"], "comment": "15 pages, 3 figures", "summary": "Retentive Network (RetNet) represents a significant advancement in neural\nnetwork architecture, offering an efficient alternative to the Transformer.\nWhile Transformers rely on self-attention to model dependencies, they suffer\nfrom high memory costs and limited scalability when handling long sequences due\nto their quadratic complexity. To mitigate these limitations, RetNet introduces\na retention mechanism that unifies the inductive bias of recurrence with the\nglobal dependency modeling of attention. This mechanism enables linear-time\ninference, facilitates efficient modeling of extended contexts, and remains\ncompatible with fully parallelizable training pipelines. RetNet has garnered\nsignificant research interest due to its consistently demonstrated cross-domain\neffectiveness, achieving robust performance across machine learning paradigms\nincluding natural language processing, speech recognition, and time-series\nanalysis. However, a comprehensive review of RetNet is still missing from the\ncurrent literature. This paper aims to fill that gap by offering the first\ndetailed survey of the RetNet architecture, its key innovations, and its\ndiverse applications. We also explore the main challenges associated with\nRetNet and propose future research directions to support its continued\nadvancement in both academic research and practical deployment.", "AI": {"tldr": "RetNet is an efficient neural network alternative to Transformers, addressing their scalability and memory issues with a retention mechanism. It enables linear-time inference and parallel training, excelling in NLP, speech, and time-series tasks. This paper reviews RetNet's innovations, applications, challenges, and future directions.", "motivation": "Transformers face scalability and memory issues due to quadratic complexity. RetNet aims to solve these problems with a retention mechanism, offering efficient long-sequence modeling.", "method": "RetNet introduces a retention mechanism combining recurrence and attention, enabling linear-time inference and parallel training.", "result": "RetNet achieves robust performance in NLP, speech recognition, and time-series analysis, demonstrating cross-domain effectiveness.", "conclusion": "The paper provides the first comprehensive review of RetNet, highlighting its innovations, applications, challenges, and future research directions."}}
{"id": "2506.06818", "pdf": "https://arxiv.org/pdf/2506.06818", "abs": "https://arxiv.org/abs/2506.06818", "authors": ["Chao Yin", "Hao Li", "Kequan Yang", "Jide Li", "Pinpin Zhu", "Xiaoqiang Li"], "title": "Stepwise Decomposition and Dual-stream Focus: A Novel Approach for Training-free Camouflaged Object Segmentation", "categories": ["cs.CV"], "comment": "under review", "summary": "While promptable segmentation (\\textit{e.g.}, SAM) has shown promise for\nvarious segmentation tasks, it still requires manual visual prompts for each\nobject to be segmented. In contrast, task-generic promptable segmentation aims\nto reduce the need for such detailed prompts by employing only a task-generic\nprompt to guide segmentation across all test samples. However, when applied to\nCamouflaged Object Segmentation (COS), current methods still face two critical\nissues: 1) \\textit{\\textbf{semantic ambiguity in getting instance-specific text\nprompts}}, which arises from insufficient discriminative cues in holistic\ncaptions, leading to foreground-background confusion; 2)\n\\textit{\\textbf{semantic discrepancy combined with spatial separation in\ngetting instance-specific visual prompts}}, which results from global\nbackground sampling far from object boundaries with low feature correlation,\ncausing SAM to segment irrelevant regions. To address the issues above, we\npropose \\textbf{RDVP-MSD}, a novel training-free test-time adaptation framework\nthat synergizes \\textbf{R}egion-constrained \\textbf{D}ual-stream\n\\textbf{V}isual \\textbf{P}rompting (RDVP) via \\textbf{M}ultimodal\n\\textbf{S}tepwise \\textbf{D}ecomposition Chain of Thought (MSD-CoT). MSD-CoT\nprogressively disentangles image captions to eliminate semantic ambiguity,\nwhile RDVP injects spatial constraints into visual prompting and independently\nsamples visual prompts for foreground and background points, effectively\nmitigating semantic discrepancy and spatial separation. Without requiring any\ntraining or supervision, RDVP-MSD achieves a state-of-the-art segmentation\nresult on multiple COS benchmarks and delivers a faster inference speed than\nprevious methods, demonstrating significantly improved accuracy and efficiency.\nThe codes will be available at\n\\href{https://github.com/ycyinchao/RDVP-MSD}{https://github.com/ycyinchao/RDVP-MSD}", "AI": {"tldr": "RDVP-MSD is a training-free framework for Camouflaged Object Segmentation (COS) that combines Region-constrained Dual-stream Visual Prompting (RDVP) and Multimodal Stepwise Decomposition Chain of Thought (MSD-CoT) to address semantic ambiguity and discrepancy, achieving state-of-the-art results.", "motivation": "Current promptable segmentation methods for COS suffer from semantic ambiguity in text prompts and semantic discrepancy with spatial separation in visual prompts, leading to inaccurate segmentation.", "method": "The proposed RDVP-MSD framework uses MSD-CoT to progressively disentangle captions for clarity and RDVP to inject spatial constraints into visual prompts, sampling foreground and background points independently.", "result": "RDVP-MSD achieves state-of-the-art segmentation results on COS benchmarks with faster inference speed, improving accuracy and efficiency.", "conclusion": "RDVP-MSD effectively addresses the limitations of current methods for COS without requiring training, offering superior performance and practicality."}}
{"id": "2506.06737", "pdf": "https://arxiv.org/pdf/2506.06737", "abs": "https://arxiv.org/abs/2506.06737", "authors": ["Qi Shi", "Qiwei Han", "Cl\u00e1udia Soares"], "title": "C-PATH: Conversational Patient Assistance and Triage in Healthcare System", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted in IEEE ICDH 2025, 10 pages, 8 figures, 5 tables", "summary": "Navigating healthcare systems can be complex and overwhelming, creating\nbarriers for patients seeking timely and appropriate medical attention. In this\npaper, we introduce C-PATH (Conversational Patient Assistance and Triage in\nHealthcare), a novel conversational AI system powered by large language models\n(LLMs) designed to assist patients in recognizing symptoms and recommending\nappropriate medical departments through natural, multi-turn dialogues. C-PATH\nis fine-tuned on medical knowledge, dialogue data, and clinical summaries using\na multi-stage pipeline built on the LLaMA3 architecture. A core contribution of\nthis work is a GPT-based data augmentation framework that transforms structured\nclinical knowledge from DDXPlus into lay-person-friendly conversations,\nallowing alignment with patient communication norms. We also implement a\nscalable conversation history management strategy to ensure long-range\ncoherence. Evaluation with GPTScore demonstrates strong performance across\ndimensions such as clarity, informativeness, and recommendation accuracy.\nQuantitative benchmarks show that C-PATH achieves superior performance in\nGPT-rewritten conversational datasets, significantly outperforming\ndomain-specific baselines. C-PATH represents a step forward in the development\nof user-centric, accessible, and accurate AI tools for digital health\nassistance and triage.", "AI": {"tldr": "C-PATH is a conversational AI system using LLMs to help patients navigate healthcare by recognizing symptoms and recommending departments via natural dialogues. It leverages medical data and a GPT-based augmentation framework for accuracy and coherence.", "motivation": "To simplify healthcare navigation and improve patient access to timely medical attention by providing an AI-driven conversational assistant.", "method": "C-PATH is fine-tuned on medical knowledge and dialogue data using the LLaMA3 architecture. It includes a GPT-based data augmentation framework and scalable conversation history management.", "result": "C-PATH performs well in clarity, informativeness, and recommendation accuracy, outperforming domain-specific baselines in benchmarks.", "conclusion": "C-PATH advances user-centric, accessible, and accurate AI tools for digital health assistance and triage."}}
{"id": "2506.06822", "pdf": "https://arxiv.org/pdf/2506.06822", "abs": "https://arxiv.org/abs/2506.06822", "authors": ["Chenlu Zhan", "Yufei Zhang", "Gaoang Wang", "Hongwei Wang"], "title": "Hi-LSplat: Hierarchical 3D Language Gaussian Splatting", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Modeling 3D language fields with Gaussian Splatting for open-ended language\nqueries has recently garnered increasing attention. However, recent 3DGS-based\nmodels leverage view-dependent 2D foundation models to refine 3D semantics but\nlack a unified 3D representation, leading to view inconsistencies.\nAdditionally, inherent open-vocabulary challenges cause inconsistencies in\nobject and relational descriptions, impeding hierarchical semantic\nunderstanding. In this paper, we propose Hi-LSplat, a view-consistent\nHierarchical Language Gaussian Splatting work for 3D open-vocabulary querying.\nTo achieve view-consistent 3D hierarchical semantics, we first lift 2D features\nto 3D features by constructing a 3D hierarchical semantic tree with layered\ninstance clustering, which addresses the view inconsistency issue caused by 2D\nsemantic features. Besides, we introduce instance-wise and part-wise\ncontrastive losses to capture all-sided hierarchical semantic representations.\nNotably, we construct two hierarchical semantic datasets to better assess the\nmodel's ability to distinguish different semantic levels. Extensive experiments\nhighlight our method's superiority in 3D open-vocabulary segmentation and\nlocalization. Its strong performance on hierarchical semantic datasets\nunderscores its ability to capture complex hierarchical semantics within 3D\nscenes.", "AI": {"tldr": "Hi-LSplat introduces a view-consistent hierarchical language Gaussian Splatting method for 3D open-vocabulary querying, addressing inconsistencies in 2D-based models and improving semantic understanding.", "motivation": "Existing 3DGS-based models lack unified 3D representations, causing view inconsistencies and open-vocabulary challenges, hindering hierarchical semantic understanding.", "method": "Hi-LSplat lifts 2D features to 3D using a hierarchical semantic tree with instance clustering and employs contrastive losses for comprehensive semantic representation.", "result": "The method excels in 3D open-vocabulary segmentation and localization, demonstrating strong performance on hierarchical semantic datasets.", "conclusion": "Hi-LSplat effectively addresses view inconsistency and hierarchical semantic challenges, offering superior performance in 3D language field modeling."}}
{"id": "2506.06751", "pdf": "https://arxiv.org/pdf/2506.06751", "abs": "https://arxiv.org/abs/2506.06751", "authors": ["Mikhail Salnikov", "Dmitrii Korzh", "Ivan Lazichny", "Elvir Karimov", "Artyom Iudin", "Ivan Oseledets", "Oleg Y. Rogov", "Alexander Panchenko", "Natalia Loukachevitch", "Elena Tutubalina"], "title": "Geopolitical biases in LLMs: what are the \"good\" and the \"bad\" countries according to contemporary language models", "categories": ["cs.CL"], "comment": null, "summary": "This paper evaluates geopolitical biases in LLMs with respect to various\ncountries though an analysis of their interpretation of historical events with\nconflicting national perspectives (USA, UK, USSR, and China). We introduce a\nnovel dataset with neutral event descriptions and contrasting viewpoints from\ndifferent countries. Our findings show significant geopolitical biases, with\nmodels favoring specific national narratives. Additionally, simple debiasing\nprompts had a limited effect in reducing these biases. Experiments with\nmanipulated participant labels reveal models' sensitivity to attribution,\nsometimes amplifying biases or recognizing inconsistencies, especially with\nswapped labels. This work highlights national narrative biases in LLMs,\nchallenges the effectiveness of simple debiasing methods, and offers a\nframework and dataset for future geopolitical bias research.", "AI": {"tldr": "The paper investigates geopolitical biases in LLMs by analyzing their interpretations of historical events from conflicting national perspectives (USA, UK, USSR, China). It introduces a dataset with neutral event descriptions and contrasting viewpoints, revealing significant biases favoring certain national narratives. Simple debiasing methods proved ineffective, and models showed sensitivity to manipulated participant labels, sometimes amplifying biases. The work provides a framework and dataset for future research on geopolitical bias.", "motivation": "To evaluate and address geopolitical biases in LLMs, particularly their tendency to favor specific national narratives when interpreting historical events with conflicting perspectives.", "method": "The study introduces a novel dataset with neutral event descriptions and contrasting national viewpoints. It tests LLMs' interpretations, applies simple debiasing prompts, and experiments with manipulated participant labels to assess bias sensitivity.", "result": "Findings reveal significant geopolitical biases in LLMs, with limited effectiveness of simple debiasing methods. Models showed sensitivity to manipulated labels, sometimes amplifying biases or recognizing inconsistencies.", "conclusion": "The paper highlights the persistence of national narrative biases in LLMs, challenges the efficacy of simple debiasing techniques, and provides a framework and dataset for further research on geopolitical bias."}}
{"id": "2506.06823", "pdf": "https://arxiv.org/pdf/2506.06823", "abs": "https://arxiv.org/abs/2506.06823", "authors": ["Qi Li", "Liangzhi Li", "Zhouqiang Jiang", "Bowen Wang", "Keke Tang"], "title": "Exploring Visual Prompting: Robustness Inheritance and Beyond", "categories": ["cs.CV", "cs.AI"], "comment": "arXiv admin note: substantial text overlap with arXiv:2311.10992", "summary": "Visual Prompting (VP), an efficient method for transfer learning, has shown\nits potential in vision tasks. However, previous works focus exclusively on VP\nfrom standard source models, it is still unknown how it performs under the\nscenario of a robust source model: Can the robustness of the source model be\nsuccessfully inherited? Does VP also encounter the same trade-off between\nrobustness and generalization ability as the source model during this process?\nIf such a trade-off exists, is there a strategy specifically tailored to VP to\nmitigate this limitation? In this paper, we thoroughly explore these three\nquestions for the first time and provide affirmative answers to them. To\nmitigate the trade-off faced by VP, we propose a strategy called Prompt\nBoundary Loosening (PBL). As a lightweight, plug-and-play strategy naturally\ncompatible with VP, PBL effectively ensures the successful inheritance of\nrobustness when the source model is a robust model, while significantly\nenhancing VP's generalization ability across various downstream datasets.\nExtensive experiments across various datasets show that our findings are\nuniversal and demonstrate the significant benefits of the proposed strategy.", "AI": {"tldr": "The paper explores Visual Prompting (VP) with robust source models, confirming robustness inheritance and a trade-off with generalization. It proposes Prompt Boundary Loosening (PBL) to mitigate this trade-off, showing universal benefits across datasets.", "motivation": "To investigate if VP inherits robustness from robust source models and address the robustness-generalization trade-off.", "method": "Proposes Prompt Boundary Loosening (PBL), a lightweight, plug-and-play strategy for VP.", "result": "PBL ensures robustness inheritance and enhances generalization across datasets.", "conclusion": "The findings are universal, and PBL effectively mitigates the trade-off in VP."}}
{"id": "2506.06775", "pdf": "https://arxiv.org/pdf/2506.06775", "abs": "https://arxiv.org/abs/2506.06775", "authors": ["Walter Paci", "Alessandro Panunzi", "Sandro Pezzelle"], "title": "They want to pretend not to understand: The Limits of Current LLMs in Interpreting Implicit Content of Political Discourse", "categories": ["cs.CL"], "comment": "Accepted to the ACL2025 Findings", "summary": "Implicit content plays a crucial role in political discourse, where speakers\nsystematically employ pragmatic strategies such as implicatures and\npresuppositions to influence their audiences. Large Language Models (LLMs) have\ndemonstrated strong performance in tasks requiring complex semantic and\npragmatic understanding, highlighting their potential for detecting and\nexplaining the meaning of implicit content. However, their ability to do this\nwithin political discourse remains largely underexplored. Leveraging, for the\nfirst time, the large IMPAQTS corpus, which comprises Italian political\nspeeches with the annotation of manipulative implicit content, we propose\nmethods to test the effectiveness of LLMs in this challenging problem. Through\na multiple-choice task and an open-ended generation task, we demonstrate that\nall tested models struggle to interpret presuppositions and implicatures. We\nconclude that current LLMs lack the key pragmatic capabilities necessary for\naccurately interpreting highly implicit language, such as that found in\npolitical discourse. At the same time, we highlight promising trends and future\ndirections for enhancing model performance. We release our data and code at\nhttps://github.com/WalterPaci/IMPAQTS-PID", "AI": {"tldr": "The paper explores LLMs' ability to detect and explain implicit content in political discourse, finding current models struggle with pragmatic tasks like interpreting presuppositions and implicatures.", "motivation": "To assess LLMs' potential in understanding implicit content in political discourse, a largely underexplored area.", "method": "Uses the IMPAQTS corpus of Italian political speeches to test LLMs via multiple-choice and open-ended tasks.", "result": "All tested models struggled with pragmatic tasks, indicating a lack of key capabilities for interpreting implicit language.", "conclusion": "Current LLMs lack pragmatic skills for political discourse, but future enhancements are promising. Data and code are publicly released."}}
{"id": "2506.06826", "pdf": "https://arxiv.org/pdf/2506.06826", "abs": "https://arxiv.org/abs/2506.06826", "authors": ["Chenfei Yuan", "Nanshan Jia", "Hangqi Li", "Peter W. Glynn", "Zeyu Zheng"], "title": "Controllable Coupled Image Generation via Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We provide an attention-level control method for the task of coupled image\ngeneration, where \"coupled\" means that multiple simultaneously generated images\nare expected to have the same or very similar backgrounds. While backgrounds\ncoupled, the centered objects in the generated images are still expected to\nenjoy the flexibility raised from different text prompts. The proposed method\ndisentangles the background and entity components in the model's\ncross-attention modules, attached with a sequence of time-varying weight\ncontrol parameters depending on the time step of sampling. We optimize this\nsequence of weight control parameters with a combined objective that assesses\nhow coupled the backgrounds are as well as text-to-image alignment and overall\nvisual quality. Empirical results demonstrate that our method outperforms\nexisting approaches across these criteria.", "AI": {"tldr": "A method for coupled image generation controls attention to ensure similar backgrounds while allowing flexible foreground objects via text prompts.", "motivation": "To generate multiple images with shared backgrounds but varied foreground objects, addressing the challenge of balancing background coupling and foreground flexibility.", "method": "Disentangles background and entity components in cross-attention modules, using time-varying weight control parameters optimized for coupling, alignment, and quality.", "result": "Outperforms existing methods in background coupling, text-to-image alignment, and visual quality.", "conclusion": "The proposed method effectively balances background consistency and foreground diversity in coupled image generation."}}
{"id": "2506.06785", "pdf": "https://arxiv.org/pdf/2506.06785", "abs": "https://arxiv.org/abs/2506.06785", "authors": ["Hiram Ring"], "title": "Extending dependencies to the taggedPBC: Word order in transitive clauses", "categories": ["cs.CL"], "comment": null, "summary": "The taggedPBC (Ring 2025a) contains more than 1,800 sentences of pos-tagged\nparallel text data from over 1,500 languages, representing 133 language\nfamilies and 111 isolates. While this dwarfs previously available resources,\nand the POS tags achieve decent accuracy, allowing for predictive\ncrosslinguistic insights (Ring 2025b), the dataset was not initially annotated\nfor dependencies. This paper reports on a CoNLLU-formatted version of the\ndataset which transfers dependency information along with POS tags to all\nlanguages in the taggedPBC. Although there are various concerns regarding the\nquality of the tags and the dependencies, word order information derived from\nthis dataset regarding the position of arguments and predicates in transitive\nclauses correlates with expert determinations of word order in three\ntypological databases (WALS, Grambank, Autotyp). This highlights the usefulness\nof corpus-based typological approaches (as per Baylor et al. 2023; Bjerva 2024)\nfor extending comparisons of discrete linguistic categories, and suggests that\nimportant insights can be gained even from noisy data, given sufficient\nannotation. The dependency-annotated corpora are also made available for\nresearch and collaboration via GitHub.", "AI": {"tldr": "The paper introduces a CoNLLU-formatted version of the taggedPBC dataset, adding dependency annotations to its existing POS tags, and validates its utility by correlating word order data with expert typological databases.", "motivation": "To extend the taggedPBC dataset with dependency annotations, enabling broader crosslinguistic research despite potential quality concerns.", "method": "Transferring dependency information to the taggedPBC dataset and validating the results by comparing word order data with expert typological databases (WALS, Grambank, Autotyp).", "result": "The dataset's word order information correlates with expert typological data, demonstrating its usefulness for corpus-based typological research.", "conclusion": "Noisy but sufficiently annotated data can yield valuable linguistic insights, and the dependency-annotated corpora are made available for further research."}}
{"id": "2506.06830", "pdf": "https://arxiv.org/pdf/2506.06830", "abs": "https://arxiv.org/abs/2506.06830", "authors": ["Guankun Wang", "Rui Tang", "Mengya Xu", "Long Bai", "Huxin Gao", "Hongliang Ren"], "title": "EndoARSS: Adapting Spatially-Aware Foundation Model for Efficient Activity Recognition and Semantic Segmentation in Endoscopic Surgery", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by Advanced Intelligent Systems", "summary": "Endoscopic surgery is the gold standard for robotic-assisted minimally\ninvasive surgery, offering significant advantages in early disease detection\nand precise interventions. However, the complexity of surgical scenes,\ncharacterized by high variability in different surgical activity scenarios and\nconfused image features between targets and the background, presents challenges\nfor surgical environment understanding. Traditional deep learning models often\nstruggle with cross-activity interference, leading to suboptimal performance in\neach downstream task. To address this limitation, we explore multi-task\nlearning, which utilizes the interrelated features between tasks to enhance\noverall task performance. In this paper, we propose EndoARSS, a novel\nmulti-task learning framework specifically designed for endoscopy surgery\nactivity recognition and semantic segmentation. Built upon the DINOv2\nfoundation model, our approach integrates Low-Rank Adaptation to facilitate\nefficient fine-tuning while incorporating Task Efficient Shared Low-Rank\nAdapters to mitigate gradient conflicts across diverse tasks. Additionally, we\nintroduce the Spatially-Aware Multi-Scale Attention that enhances feature\nrepresentation discrimination by enabling cross-spatial learning of global\ninformation. In order to evaluate the effectiveness of our framework, we\npresent three novel datasets, MTLESD, MTLEndovis and MTLEndovis-Gen, tailored\nfor endoscopic surgery scenarios with detailed annotations for both activity\nrecognition and semantic segmentation tasks. Extensive experiments demonstrate\nthat EndoARSS achieves remarkable performance across multiple benchmarks,\nsignificantly improving both accuracy and robustness in comparison to existing\nmodels. These results underscore the potential of EndoARSS to advance AI-driven\nendoscopic surgical systems, offering valuable insights for enhancing surgical\nsafety and efficiency.", "AI": {"tldr": "EndoARSS is a multi-task learning framework for endoscopic surgery, combining activity recognition and semantic segmentation, leveraging DINOv2 and novel adapters for improved performance.", "motivation": "Address challenges in surgical scene understanding due to high variability and cross-activity interference in traditional deep learning models.", "method": "Proposes EndoARSS, integrating Low-Rank Adaptation and Task Efficient Shared Low-Rank Adapters with Spatially-Aware Multi-Scale Attention for feature enhancement.", "result": "Achieves superior performance on new datasets (MTLESD, MTLEndovis, MTLEndovis-Gen), outperforming existing models in accuracy and robustness.", "conclusion": "EndoARSS advances AI-driven endoscopic surgery, enhancing safety and efficiency through improved multi-task learning."}}
{"id": "2506.06800", "pdf": "https://arxiv.org/pdf/2506.06800", "abs": "https://arxiv.org/abs/2506.06800", "authors": ["Tianjie Ju", "Yujia Chen", "Hao Fei", "Mong-Li Lee", "Wynne Hsu", "Pengzhou Cheng", "Zongru Wu", "Zhuosheng Zhang", "Gongshen Liu"], "title": "On the Adaptive Psychological Persuasion of Large Language Models", "categories": ["cs.CL"], "comment": "Working in progress", "summary": "Previous work has showcased the intriguing capabilities of Large Language\nModels (LLMs) in instruction-following and rhetorical fluency. However,\nsystematic exploration of their dual capabilities to autonomously persuade and\nresist persuasion, particularly in contexts involving psychological rhetoric,\nremains unexplored. In this paper, we first evaluate four commonly adopted LLMs\nby tasking them to alternately act as persuaders and listeners in adversarial\ndialogues. Empirical results show that persuader LLMs predominantly employ\nrepetitive strategies, leading to low success rates. Then we introduce eleven\ncomprehensive psychological persuasion strategies, finding that explicitly\ninstructing LLMs to adopt specific strategies such as Fluency Effect and\nRepetition Effect significantly improves persuasion success rates. However, no\n``one-size-fits-all'' strategy proves universally effective, with performance\nheavily dependent on contextual counterfactuals. Motivated by these\nobservations, we propose an adaptive framework based on direct preference\noptimization that trains LLMs to autonomously select optimal strategies by\nleveraging persuasion results from strategy-specific responses as preference\npairs. Experiments on three open-source LLMs confirm that the proposed adaptive\npsychological persuasion method effectively enables persuader LLMs to select\noptimal strategies, significantly enhancing their success rates while\nmaintaining general capabilities. Our code is available at\nhttps://github.com/KalinaEine/PsychologicalPersuasion.", "AI": {"tldr": "The paper explores LLMs' persuasion and resistance capabilities, introduces psychological strategies, and proposes an adaptive framework to improve persuasion success.", "motivation": "To systematically study LLMs' dual capabilities in persuasion and resistance, especially in psychological rhetoric contexts, which remain unexplored.", "method": "Evaluates four LLMs in adversarial dialogues, introduces eleven psychological strategies, and proposes an adaptive framework using direct preference optimization.", "result": "Persuader LLMs initially use repetitive strategies with low success; explicit strategy adoption improves rates, but no universal strategy exists. The adaptive framework enhances success rates.", "conclusion": "The adaptive framework effectively enables LLMs to select optimal persuasion strategies, improving success rates while maintaining general capabilities."}}
{"id": "2506.06836", "pdf": "https://arxiv.org/pdf/2506.06836", "abs": "https://arxiv.org/abs/2506.06836", "authors": ["Zelin He", "Sarah Alnegheimish", "Matthew Reimherr"], "title": "Harnessing Vision-Language Models for Time Series Anomaly Detection", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Time-series anomaly detection (TSAD) has played a vital role in a variety of\nfields, including healthcare, finance, and industrial monitoring. Prior\nmethods, which mainly focus on training domain-specific models on numerical\ndata, lack the visual-temporal reasoning capacity that human experts have to\nidentify contextual anomalies. To fill this gap, we explore a solution based on\nvision language models (VLMs). Recent studies have shown the ability of VLMs\nfor visual reasoning tasks, yet their direct application to time series has\nfallen short on both accuracy and efficiency. To harness the power of VLMs for\nTSAD, we propose a two-stage solution, with (1) ViT4TS, a vision-screening\nstage built on a relatively lightweight pretrained vision encoder, which\nleverages 2-D time-series representations to accurately localize candidate\nanomalies; (2) VLM4TS, a VLM-based stage that integrates global temporal\ncontext and VLM reasoning capacity to refine the detection upon the candidates\nprovided by ViT4TS. We show that without any time-series training, VLM4TS\noutperforms time-series pretrained and from-scratch baselines in most cases,\nyielding a 24.6 percent improvement in F1-max score over the best baseline.\nMoreover, VLM4TS also consistently outperforms existing language-model-based\nTSAD methods and is on average 36 times more efficient in token usage.", "AI": {"tldr": "The paper proposes a two-stage VLM-based method for time-series anomaly detection, outperforming existing baselines in accuracy and efficiency.", "motivation": "Existing methods lack visual-temporal reasoning for contextual anomalies, which VLMs can address.", "method": "A two-stage approach: ViT4TS for anomaly localization and VLM4TS for refined detection using VLM reasoning.", "result": "VLM4TS improves F1-max by 24.6% over baselines and is 36x more efficient in token usage.", "conclusion": "VLMs can effectively enhance TSAD without time-series training, offering superior performance and efficiency."}}
{"id": "2506.06806", "pdf": "https://arxiv.org/pdf/2506.06806", "abs": "https://arxiv.org/abs/2506.06806", "authors": ["Subhendu Khatuya", "Shashwat Naidu", "Saptarshi Ghosh", "Pawan Goyal", "Niloy Ganguly"], "title": "Label-semantics Aware Generative Approach for Domain-Agnostic Multilabel Classification", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "This work has been accepted to appear at the Association for\n  Computational Linguistics (ACL), 2025", "summary": "The explosion of textual data has made manual document classification\nincreasingly challenging. To address this, we introduce a robust, efficient\ndomain-agnostic generative model framework for multi-label text classification.\nInstead of treating labels as mere atomic symbols, our approach utilizes\npredefined label descriptions and is trained to generate these descriptions\nbased on the input text. During inference, the generated descriptions are\nmatched to the pre-defined labels using a finetuned sentence transformer. We\nintegrate this with a dual-objective loss function, combining cross-entropy\nloss and cosine similarity of the generated sentences with the predefined\ntarget descriptions, ensuring both semantic alignment and accuracy. Our\nproposed model LAGAMC stands out for its parameter efficiency and versatility\nacross diverse datasets, making it well-suited for practical applications. We\ndemonstrate the effectiveness of our proposed model by achieving new\nstate-of-the-art performances across all evaluated datasets, surpassing several\nstrong baselines. We achieve improvements of 13.94% in Micro-F1 and 24.85% in\nMacro-F1 compared to the closest baseline across all datasets.", "AI": {"tldr": "LAGAMC, a domain-agnostic generative model for multi-label text classification, uses label descriptions and a dual-objective loss to achieve state-of-the-art performance.", "motivation": "Manual document classification is challenging due to the explosion of textual data, necessitating an efficient and versatile solution.", "method": "The model generates label descriptions from input text and matches them to predefined labels using a sentence transformer, with a dual-objective loss combining cross-entropy and cosine similarity.", "result": "LAGAMC achieves 13.94% and 24.85% improvements in Micro-F1 and Macro-F1, respectively, outperforming baselines.", "conclusion": "LAGAMC is parameter-efficient and versatile, making it suitable for practical applications in multi-label text classification."}}
{"id": "2506.06846", "pdf": "https://arxiv.org/pdf/2506.06846", "abs": "https://arxiv.org/abs/2506.06846", "authors": ["Yangkai Lin", "Jiabao Lei", "Kui jia"], "title": "Multi-StyleGS: Stylizing Gaussian Splatting with Multiple Styles", "categories": ["cs.CV"], "comment": "AAAI 2025", "summary": "In recent years, there has been a growing demand to stylize a given 3D scene\nto align with the artistic style of reference images for creative purposes.\nWhile 3D Gaussian Splatting(GS) has emerged as a promising and efficient method\nfor realistic 3D scene modeling, there remains a challenge in adapting it to\nstylize 3D GS to match with multiple styles through automatic local style\ntransfer or manual designation, while maintaining memory efficiency for\nstylization training. In this paper, we introduce a novel 3D GS stylization\nsolution termed Multi-StyleGS to tackle these challenges. In particular, we\nemploy a bipartite matching mechanism to au tomatically identify\ncorrespondences between the style images and the local regions of the rendered\nimages. To facilitate local style transfer, we introduce a novel semantic style\nloss function that employs a segmentation network to apply distinct styles to\nvarious objects of the scene and propose a local-global feature matching to\nenhance the multi-view consistency. Furthermore, this technique can achieve\nmemory efficient training, more texture details and better color match. To\nbetter assign a robust semantic label to each Gaussian, we propose several\ntechniques to regularize the segmentation network. As demonstrated by our\ncomprehensive experiments, our approach outperforms existing ones in producing\nplausible stylization results and offering flexible editing.", "AI": {"tldr": "A novel method, Multi-StyleGS, is introduced for stylizing 3D Gaussian Splatting scenes with multiple artistic styles, ensuring memory efficiency and improved texture details.", "motivation": "The demand for stylizing 3D scenes to match artistic reference images has grown, but adapting 3D Gaussian Splatting for multi-style transfer while maintaining efficiency remains challenging.", "method": "Multi-StyleGS uses bipartite matching for style-image correspondences, a semantic style loss function with segmentation, and local-global feature matching for consistency.", "result": "The approach outperforms existing methods in stylization quality, texture details, and color matching, while enabling flexible editing.", "conclusion": "Multi-StyleGS effectively addresses the challenges of 3D GS stylization, offering memory-efficient training and superior results."}}
{"id": "2506.06808", "pdf": "https://arxiv.org/pdf/2506.06808", "abs": "https://arxiv.org/abs/2506.06808", "authors": ["James A. Michaelov", "Reeka Estacio", "Zhien Zhang", "Benjamin K. Bergen"], "title": "Not quite Sherlock Holmes: Language model predictions do not reliably differentiate impossible from improbable events", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to Findings of ACL 2025", "summary": "Can language models reliably predict that possible events are more likely\nthan merely improbable ones? By teasing apart possibility, typicality, and\ncontextual relatedness, we show that despite the results of previous work,\nlanguage models' ability to do this is far from robust. In fact, under certain\nconditions, all models tested - including Llama 3, Gemma 2, and Mistral NeMo -\nperform at worse-than-chance level, assigning higher probabilities to\nimpossible sentences such as 'the car was given a parking ticket by the brake'\nthan to merely unlikely sentences such as 'the car was given a parking ticket\nby the explorer'.", "AI": {"tldr": "Language models struggle to reliably predict possible events over improbable ones, sometimes performing worse than chance.", "motivation": "To assess language models' robustness in distinguishing possible from improbable events, separating factors like possibility, typicality, and contextual relatedness.", "method": "Analyzed models like Llama 3, Gemma 2, and Mistral NeMo under specific conditions to compare their performance.", "result": "Models often assigned higher probabilities to impossible sentences than unlikely ones, performing worse than chance in some cases.", "conclusion": "Language models' ability to predict event likelihood is not robust, highlighting limitations in their reasoning."}}
{"id": "2506.06850", "pdf": "https://arxiv.org/pdf/2506.06850", "abs": "https://arxiv.org/abs/2506.06850", "authors": ["Sara M. Cerqueira", "Manuel Palermo", "Cristina P. Santos"], "title": "Deep Inertial Pose: A deep learning approach for human pose estimation", "categories": ["cs.CV", "eess.SP"], "comment": null, "summary": "Inertial-based Motion capture system has been attracting growing attention\ndue to its wearability and unsconstrained use. However, accurate human joint\nestimation demands several complex and expertise demanding steps, which leads\nto expensive software such as the state-of-the-art MVN Awinda from Xsens\nTechnologies. This work aims to study the use of Neural Networks to abstract\nthe complex biomechanical models and analytical mathematics required for pose\nestimation. Thus, it presents a comparison of different Neural Network\narchitectures and methodologies to understand how accurately these methods can\nestimate human pose, using both low cost(MPU9250) and high end (Mtw Awinda)\nMagnetic, Angular Rate, and Gravity (MARG) sensors. The most efficient method\nwas the Hybrid LSTM-Madgwick detached, which achieved an Quaternion Angle\ndistance error of 7.96, using Mtw Awinda data. Also, an ablation study was\nconducted to study the impact of data augmentation, output representation,\nwindow size, loss function and magnetometer data on the pose estimation error.\nThis work indicates that Neural Networks can be trained to estimate human pose,\nwith results comparable to the state-of-the-art fusion filters.", "AI": {"tldr": "Neural Networks can estimate human pose accurately, matching state-of-the-art methods, with Hybrid LSTM-Madgwick detached being the most efficient.", "motivation": "Simplify the complex biomechanical models and analytical steps in inertial-based motion capture using Neural Networks.", "method": "Comparison of Neural Network architectures and methodologies for pose estimation using low-cost and high-end MARG sensors.", "result": "Hybrid LSTM-Madgwick detached achieved a Quaternion Angle distance error of 7.96 with Mtw Awinda data.", "conclusion": "Neural Networks are viable for accurate human pose estimation, comparable to fusion filters."}}
{"id": "2506.06812", "pdf": "https://arxiv.org/pdf/2506.06812", "abs": "https://arxiv.org/abs/2506.06812", "authors": ["Bernardo Leite", "Henrique Lopes Cardoso"], "title": "Advancing Question Generation with Joint Narrative and Difficulty Control", "categories": ["cs.CL"], "comment": "Preprint. Accepted to the BEA 2025 Workshop (ACL)", "summary": "Question Generation (QG), the task of automatically generating questions from\na source input, has seen significant progress in recent years.\nDifficulty-controllable QG (DCQG) enables control over the difficulty level of\ngenerated questions while considering the learner's ability. Additionally,\nnarrative-controllable QG (NCQG) allows control over the narrative aspects\nembedded in the questions. However, research in QG lacks a focus on combining\nthese two types of control, which is important for generating questions\ntailored to educational purposes. To address this gap, we propose a strategy\nfor Joint Narrative and Difficulty Control, enabling simultaneous control over\nthese two attributes in the generation of reading comprehension questions. Our\nevaluation provides preliminary evidence that this approach is feasible, though\nit is not effective across all instances. Our findings highlight the conditions\nunder which the strategy performs well and discuss the trade-offs associated\nwith its application.", "AI": {"tldr": "The paper proposes a strategy for joint control of narrative and difficulty in question generation (QG) for educational purposes, showing feasibility but noting limitations.", "motivation": "Existing QG research lacks combined control over question difficulty and narrative, which is crucial for tailored educational questions.", "method": "The paper introduces a strategy for Joint Narrative and Difficulty Control in QG, evaluating its feasibility and performance conditions.", "result": "Preliminary evidence shows the approach is feasible but not universally effective, with identified performance conditions and trade-offs.", "conclusion": "The strategy offers potential for tailored educational QG, though further refinement is needed for broader effectiveness."}}
{"id": "2506.06852", "pdf": "https://arxiv.org/pdf/2506.06852", "abs": "https://arxiv.org/abs/2506.06852", "authors": ["John Waithaka", "Moise Busogi"], "title": "Position Prediction Self-Supervised Learning for Multimodal Satellite Imagery Semantic Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Semantic segmentation of satellite imagery is crucial for Earth observation\napplications, but remains constrained by limited labelled training data. While\nself-supervised pretraining methods like Masked Autoencoders (MAE) have shown\npromise, they focus on reconstruction rather than localisation-a fundamental\naspect of segmentation tasks. We propose adapting LOCA (Location-aware), a\nposition prediction self-supervised learning method, for multimodal satellite\nimagery semantic segmentation. Our approach addresses the unique challenges of\nsatellite data by extending SatMAE's channel grouping from multispectral to\nmultimodal data, enabling effective handling of multiple modalities, and\nintroducing same-group attention masking to encourage cross-modal interaction\nduring pretraining. The method uses relative patch position prediction,\nencouraging spatial reasoning for localisation rather than reconstruction. We\nevaluate our approach on the Sen1Floods11 flood mapping dataset, where it\nsignificantly outperforms existing reconstruction-based self-supervised\nlearning methods for satellite imagery. Our results demonstrate that position\nprediction tasks, when properly adapted for multimodal satellite imagery, learn\nrepresentations more effective for satellite image semantic segmentation than\nreconstruction-based approaches.", "AI": {"tldr": "The paper proposes LOCA, a position prediction self-supervised learning method, for multimodal satellite imagery semantic segmentation, outperforming reconstruction-based methods like MAE.", "motivation": "Limited labelled training data for satellite imagery semantic segmentation and the focus of existing methods (e.g., MAE) on reconstruction rather than localisation, a key aspect of segmentation.", "method": "Adapts LOCA for multimodal satellite data, extends SatMAE's channel grouping, introduces same-group attention masking, and uses relative patch position prediction for spatial reasoning.", "result": "Outperforms reconstruction-based methods on the Sen1Floods11 dataset, showing better representations for segmentation.", "conclusion": "Position prediction tasks, adapted for multimodal satellite imagery, are more effective for semantic segmentation than reconstruction-based approaches."}}
{"id": "2506.06813", "pdf": "https://arxiv.org/pdf/2506.06813", "abs": "https://arxiv.org/abs/2506.06813", "authors": ["Dipto Das", "Syed Ishtiaque Ahmed", "Shion Guha"], "title": "BTPD: A Multilingual Hand-curated Dataset of Bengali Transnational Political Discourse Across Online Communities", "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": null, "summary": "Understanding political discourse in online spaces is crucial for analyzing\npublic opinion and ideological polarization. While social computing and\ncomputational linguistics have explored such discussions in English, such\nresearch efforts are significantly limited in major yet under-resourced\nlanguages like Bengali due to the unavailability of datasets. In this paper, we\npresent a multilingual dataset of Bengali transnational political discourse\n(BTPD) collected from three online platforms, each representing distinct\ncommunity structures and interaction dynamics. Besides describing how we\nhand-curated the dataset through community-informed keyword-based retrieval,\nthis paper also provides a general overview of its topics and multilingual\ncontent.", "AI": {"tldr": "A multilingual dataset (BTPD) for Bengali transnational political discourse is introduced, addressing the lack of resources for under-resourced languages.", "motivation": "To fill the gap in research on political discourse in Bengali, an under-resourced language, by providing a curated dataset.", "method": "Hand-curated dataset collection from three online platforms using community-informed keyword-based retrieval.", "result": "A multilingual dataset of Bengali political discourse, detailing topics and content.", "conclusion": "The BTPD dataset enables future research on Bengali political discourse and multilingual analysis."}}
{"id": "2506.06854", "pdf": "https://arxiv.org/pdf/2506.06854", "abs": "https://arxiv.org/abs/2506.06854", "authors": ["Markus Knoche", "Daan de Geus", "Bastian Leibe"], "title": "DONUT: A Decoder-Only Model for Trajectory Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Predicting the motion of other agents in a scene is highly relevant for\nautonomous driving, as it allows a self-driving car to anticipate. Inspired by\nthe success of decoder-only models for language modeling, we propose DONUT, a\nDecoder-Only Network for Unrolling Trajectories. Different from existing\nencoder-decoder forecasting models, we encode historical trajectories and\npredict future trajectories with a single autoregressive model. This allows the\nmodel to make iterative predictions in a consistent manner, and ensures that\nthe model is always provided with up-to-date information, enhancing the\nperformance. Furthermore, inspired by multi-token prediction for language\nmodeling, we introduce an 'overprediction' strategy that gives the network the\nauxiliary task of predicting trajectories at longer temporal horizons. This\nallows the model to better anticipate the future, and further improves the\nperformance. With experiments, we demonstrate that our decoder-only approach\noutperforms the encoder-decoder baseline, and achieves new state-of-the-art\nresults on the Argoverse 2 single-agent motion forecasting benchmark.", "AI": {"tldr": "DONUT is a decoder-only model for trajectory prediction in autonomous driving, outperforming encoder-decoder baselines with iterative predictions and an 'overprediction' strategy.", "motivation": "To improve motion prediction for autonomous driving by leveraging decoder-only models, inspired by their success in language modeling.", "method": "Uses a single autoregressive model to encode history and predict future trajectories iteratively, with an 'overprediction' strategy for longer horizons.", "result": "Outperforms encoder-decoder baselines, achieving state-of-the-art on Argoverse 2 benchmark.", "conclusion": "Decoder-only models like DONUT enhance trajectory prediction performance and consistency in autonomous driving."}}
{"id": "2506.06816", "pdf": "https://arxiv.org/pdf/2506.06816", "abs": "https://arxiv.org/abs/2506.06816", "authors": ["Dipto Das", "Shion Guha", "Bryan Semaan"], "title": "How do datasets, developers, and models affect biases in a low-resourced language?", "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": null, "summary": "Sociotechnical systems, such as language technologies, frequently exhibit\nidentity-based biases. These biases exacerbate the experiences of historically\nmarginalized communities and remain understudied in low-resource contexts.\nWhile models and datasets specific to a language or with multilingual support\nare commonly recommended to address these biases, this paper empirically tests\nthe effectiveness of such approaches in the context of gender, religion, and\nnationality-based identities in Bengali, a widely spoken but low-resourced\nlanguage. We conducted an algorithmic audit of sentiment analysis models built\non mBERT and BanglaBERT, which were fine-tuned using all Bengali sentiment\nanalysis (BSA) datasets from Google Dataset Search. Our analyses showed that\nBSA models exhibit biases across different identity categories despite having\nsimilar semantic content and structure. We also examined the inconsistencies\nand uncertainties arising from combining pre-trained models and datasets\ncreated by individuals from diverse demographic backgrounds. We connected these\nfindings to the broader discussions on epistemic injustice, AI alignment, and\nmethodological decisions in algorithmic audits.", "AI": {"tldr": "The paper examines identity-based biases in Bengali sentiment analysis models, testing multilingual and language-specific approaches. It finds biases persist despite similar semantic content, linking findings to epistemic injustice and AI alignment.", "motivation": "To address understudied identity-based biases in low-resource contexts like Bengali, testing common mitigation approaches.", "method": "Algorithmic audit of mBERT and BanglaBERT models fine-tuned with Bengali sentiment analysis datasets, analyzing biases in gender, religion, and nationality.", "result": "BSA models exhibit biases across identity categories, with inconsistencies from combining diverse datasets and models.", "conclusion": "Highlights challenges in bias mitigation, connecting to broader issues like epistemic injustice and methodological decisions in audits."}}
{"id": "2506.06856", "pdf": "https://arxiv.org/pdf/2506.06856", "abs": "https://arxiv.org/abs/2506.06856", "authors": ["Chaoyang Wang", "Zeyu Zhang", "Haiyun Jiang"], "title": "Vision-EKIPL: External Knowledge-Infused Policy Learning for Visual Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "Visual reasoning is crucial for understanding complex multimodal data and\nadvancing Artificial General Intelligence. Existing methods enhance the\nreasoning capability of Multimodal Large Language Models (MLLMs) through\nReinforcement Learning (RL) fine-tuning (e.g., GRPO). However, current RL\napproaches sample action groups solely from the policy model itself, which\nlimits the upper boundary of the model's reasoning capability and leads to\ninefficient training. To address these limitations, this paper proposes a novel\nRL framework called \\textbf{Vision-EKIPL}. The core of this framework lies in\nintroducing high-quality actions generated by external auxiliary models during\nthe RL training process to guide the optimization of the policy model. The\npolicy learning with knowledge infusion from external models significantly\nexpands the model's exploration space, effectively improves the reasoning\nboundary, and substantially accelerates training convergence speed and\nefficiency. Experimental results demonstrate that our proposed Vision-EKIPL\nachieved up to a 5\\% performance improvement on the Reason-RFT-CoT Benchmark\ncompared to the state-of-the-art (SOTA). It reveals that Vision-EKIPL can\novercome the limitations of traditional RL methods, significantly enhance the\nvisual reasoning performance of MLLMs, and provide a new effective paradigm for\nresearch in this field.", "AI": {"tldr": "Vision-EKIPL enhances MLLMs' visual reasoning by integrating high-quality actions from external models during RL training, improving performance and efficiency.", "motivation": "Current RL methods for MLLMs limit reasoning capability and training efficiency by relying solely on the policy model's actions.", "method": "Proposes Vision-EKIPL, an RL framework that uses external auxiliary models to guide policy optimization, expanding exploration space.", "result": "Achieves up to 5% performance improvement on the Reason-RFT-CoT Benchmark compared to SOTA.", "conclusion": "Vision-EKIPL overcomes traditional RL limitations, boosts MLLMs' visual reasoning, and offers a new research paradigm."}}
{"id": "2506.06820", "pdf": "https://arxiv.org/pdf/2506.06820", "abs": "https://arxiv.org/abs/2506.06820", "authors": ["Wenyu Zhang", "Yingxu He", "Geyu Lin", "Zhuohan Liu", "Shuo Sun", "Bin Wang", "Xunlong Zou", "Jeremy H. M. Wong", "Qiongqiong Wang", "Hardik B. Sailor", "Nancy F. Chen", "Ai Ti Aw"], "title": "Beyond Classification: Towards Speech Emotion Reasoning with Multitask AudioLLMs", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Audio Large Language Models (AudioLLMs) have achieved strong results in\nsemantic tasks like speech recognition and translation, but remain limited in\nmodeling paralinguistic cues such as emotion. Existing approaches often treat\nemotion understanding as a classification problem, offering little insight into\nthe underlying rationale behind predictions. In this work, we explore emotion\nreasoning, a strategy that leverages the generative capabilities of AudioLLMs\nto enhance emotion recognition by producing semantically aligned,\nevidence-grounded explanations. To support this in multitask AudioLLMs, we\nintroduce a unified framework combining reasoning-augmented data supervision,\ndual-encoder architecture, and task-alternating training. This approach enables\nAudioLLMs to effectively learn different tasks while incorporating emotional\nreasoning. Experiments on IEMOCAP and MELD show that our approach not only\nimproves emotion prediction accuracy but also enhances the coherence and\nevidential grounding of the generated responses.", "AI": {"tldr": "The paper introduces a unified framework for AudioLLMs to enhance emotion recognition through emotion reasoning, improving prediction accuracy and response coherence.", "motivation": "Existing AudioLLMs excel in semantic tasks but lack in modeling paralinguistic cues like emotion, and current emotion classification methods lack explanatory insights.", "method": "A unified framework combining reasoning-augmented data supervision, dual-encoder architecture, and task-alternating training is proposed.", "result": "Experiments on IEMOCAP and MELD show improved emotion prediction accuracy and better coherence in generated responses.", "conclusion": "The approach successfully integrates emotion reasoning into AudioLLMs, enhancing both performance and interpretability."}}
{"id": "2506.06864", "pdf": "https://arxiv.org/pdf/2506.06864", "abs": "https://arxiv.org/abs/2506.06864", "authors": ["Junyu Liu", "Jianfeng Ren", "Sunhong Liang", "Xudong Jiang"], "title": "Face recognition on point cloud with cgan-top for denoising", "categories": ["cs.CV", "cs.AI"], "comment": "Published in ICASSP 2023", "summary": "Face recognition using 3D point clouds is gaining growing interest, while raw\npoint clouds often contain a significant amount of noise due to imperfect\nsensors. In this paper, an end-to-end 3D face recognition on a noisy point\ncloud is proposed, which synergistically integrates the denoising and\nrecognition modules. Specifically, a Conditional Generative Adversarial Network\non Three Orthogonal Planes (cGAN-TOP) is designed to effectively remove the\nnoise in the point cloud, and recover the underlying features for subsequent\nrecognition. A Linked Dynamic Graph Convolutional Neural Network (LDGCNN) is\nthen adapted to recognize faces from the processed point cloud, which\nhierarchically links both the local point features and neighboring features of\nmultiple scales. The proposed method is validated on the Bosphorus dataset. It\nsignificantly improves the recognition accuracy under all noise settings, with\na maximum gain of 14.81%.", "AI": {"tldr": "An end-to-end 3D face recognition method integrates denoising and recognition using cGAN-TOP and LDGCNN, improving accuracy by up to 14.81%.", "motivation": "Raw 3D point clouds for face recognition often contain noise due to imperfect sensors, necessitating an integrated solution for denoising and recognition.", "method": "Combines cGAN-TOP for denoising and LDGCNN for recognition, leveraging hierarchical local and neighboring features.", "result": "Validated on Bosphorus dataset, the method improves recognition accuracy under all noise settings, with a maximum gain of 14.81%.", "conclusion": "The proposed synergistic approach effectively addresses noise in 3D face recognition, enhancing accuracy."}}
{"id": "2506.06821", "pdf": "https://arxiv.org/pdf/2506.06821", "abs": "https://arxiv.org/abs/2506.06821", "authors": ["Yuhan Cao", "Zian Chen", "Kun Quan", "Ziliang Zhang", "Yu Wang", "Xiaoning Dong", "Yeqi Feng", "Guanzhong He", "Jingcheng Huang", "Jianhao Li", "Yixuan Tan", "Jiafu Tang", "Yilin Tang", "Junlei Wu", "Qianyu Xiao", "Can Zheng", "Shouchen Zhou", "Yuxiang Zhu", "Yiming Huang", "Tian Xie", "Tianxing He"], "title": "Can LLMs Generate Reliable Test Case Generators? A Study on Competition-Level Programming Problems", "categories": ["cs.CL", "cs.AI", "cs.SE"], "comment": "37 pages, 22 figures", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncode generation, capable of tackling complex tasks during inference. However,\nthe extent to which LLMs can be utilized for code checking or debugging through\ntest case generation remains largely unexplored. We investigate this problem\nfrom the perspective of competition-level programming (CP) programs and propose\nTCGBench, a Benchmark for (LLM generation of) Test Case Generators. This\nbenchmark comprises two tasks, aimed at studying the capabilities of LLMs in\n(1) generating valid test case generators for a given CP problem, and further\n(2) generating targeted test case generators that expose bugs in human-written\ncode. Experimental results indicate that while state-of-the-art LLMs can\ngenerate valid test case generators in most cases, most LLMs struggle to\ngenerate targeted test cases that reveal flaws in human code effectively.\nEspecially, even advanced reasoning models (e.g., o3-mini) fall significantly\nshort of human performance in the task of generating targeted generators.\nFurthermore, we construct a high-quality, manually curated dataset of\ninstructions for generating targeted generators. Analysis demonstrates that the\nperformance of LLMs can be enhanced with the aid of this dataset, by both\nprompting and fine-tuning.", "AI": {"tldr": "The paper explores LLMs' ability to generate test case generators for code debugging, introducing TCGBench to evaluate their performance in creating valid and targeted test cases. Results show LLMs struggle with targeted cases but improve with curated datasets.", "motivation": "To investigate LLMs' potential in code checking and debugging through test case generation, focusing on competition-level programming tasks.", "method": "Proposes TCGBench, a benchmark with two tasks: generating valid test case generators and targeted ones to expose bugs. Uses state-of-the-art LLMs and a manually curated dataset for evaluation.", "result": "LLMs generate valid test cases well but struggle with targeted ones. Performance improves with curated datasets via prompting or fine-tuning.", "conclusion": "LLMs show promise in test case generation but need further refinement for targeted debugging, with curated datasets offering a path for improvement."}}
{"id": "2506.06886", "pdf": "https://arxiv.org/pdf/2506.06886", "abs": "https://arxiv.org/abs/2506.06886", "authors": ["Wafaa Kasri", "Yassine Himeur", "Abigail Copiaco", "Wathiq Mansoor", "Ammar Albanna", "Valsamma Eapen"], "title": "Hybrid Vision Transformer-Mamba Framework for Autism Diagnosis via Eye-Tracking Analysis", "categories": ["cs.CV"], "comment": "7 pages, 4 figures and 2 tables", "summary": "Accurate Autism Spectrum Disorder (ASD) diagnosis is vital for early\nintervention. This study presents a hybrid deep learning framework combining\nVision Transformers (ViT) and Vision Mamba to detect ASD using eye-tracking\ndata. The model uses attention-based fusion to integrate visual, speech, and\nfacial cues, capturing both spatial and temporal dynamics. Unlike traditional\nhandcrafted methods, it applies state-of-the-art deep learning and explainable\nAI techniques to enhance diagnostic accuracy and transparency. Tested on the\nSaliency4ASD dataset, the proposed ViT-Mamba model outperformed existing\nmethods, achieving 0.96 accuracy, 0.95 F1-score, 0.97 sensitivity, and 0.94\nspecificity. These findings show the model's promise for scalable,\ninterpretable ASD screening, especially in resource-constrained or remote\nclinical settings where access to expert diagnosis is limited.", "AI": {"tldr": "A hybrid deep learning framework (ViT-Mamba) for ASD diagnosis using eye-tracking data outperforms traditional methods with high accuracy and explainability.", "motivation": "Early ASD diagnosis is crucial for intervention, but expert access is limited. The study aims to improve accuracy and scalability using advanced deep learning.", "method": "Combines Vision Transformers and Vision Mamba with attention-based fusion to integrate visual, speech, and facial cues, capturing spatial-temporal dynamics.", "result": "Achieved 0.96 accuracy, 0.95 F1-score, 0.97 sensitivity, and 0.94 specificity on the Saliency4ASD dataset, outperforming existing methods.", "conclusion": "The ViT-Mamba model is promising for scalable, interpretable ASD screening, especially in resource-limited settings."}}
{"id": "2506.06842", "pdf": "https://arxiv.org/pdf/2506.06842", "abs": "https://arxiv.org/abs/2506.06842", "authors": ["Arkadiusz Modzelewski", "Witold Sosnowski", "Tiziano Labruna", "Adam Wierzbicki", "Giovanni Da San Martino"], "title": "PCoT: Persuasion-Augmented Chain of Thought for Detecting Fake News and Social Media Disinformation", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Disinformation detection is a key aspect of media literacy. Psychological\nstudies have shown that knowledge of persuasive fallacies helps individuals\ndetect disinformation. Inspired by these findings, we experimented with large\nlanguage models (LLMs) to test whether infusing persuasion knowledge enhances\ndisinformation detection. As a result, we introduce the Persuasion-Augmented\nChain of Thought (PCoT), a novel approach that leverages persuasion to improve\ndisinformation detection in zero-shot classification. We extensively evaluate\nPCoT on online news and social media posts. Moreover, we publish two novel,\nup-to-date disinformation datasets: EUDisinfo and MultiDis. These datasets\nenable the evaluation of PCoT on content entirely unseen by the LLMs used in\nour experiments, as the content was published after the models' knowledge\ncutoffs. We show that, on average, PCoT outperforms competitive methods by 15%\nacross five LLMs and five datasets. These findings highlight the value of\npersuasion in strengthening zero-shot disinformation detection.", "AI": {"tldr": "The paper introduces Persuasion-Augmented Chain of Thought (PCoT), a method using persuasion knowledge to improve zero-shot disinformation detection in LLMs, outperforming others by 15%.", "motivation": "Psychological studies suggest persuasion knowledge aids disinformation detection, prompting exploration of its application in LLMs.", "method": "Developed PCoT, leveraging persuasion to enhance disinformation detection, and tested it on news and social media posts using novel datasets EUDisinfo and MultiDisinfo.", "result": "PCoT outperformed competitive methods by 15% across five LLMs and datasets.", "conclusion": "Persuasion knowledge significantly strengthens zero-shot disinformation detection in LLMs."}}
{"id": "2506.06898", "pdf": "https://arxiv.org/pdf/2506.06898", "abs": "https://arxiv.org/abs/2506.06898", "authors": ["Reese Kneeland", "Paul S. Scotti", "Ghislain St-Yves", "Jesse Breedlove", "Kendrick Kay", "Thomas Naselaris"], "title": "NSD-Imagery: A benchmark dataset for extending fMRI vision decoding methods to mental imagery", "categories": ["cs.CV", "cs.LG", "eess.IV", "q-bio.NC"], "comment": "Published at CVPR 2025", "summary": "We release NSD-Imagery, a benchmark dataset of human fMRI activity paired\nwith mental images, to complement the existing Natural Scenes Dataset (NSD), a\nlarge-scale dataset of fMRI activity paired with seen images that enabled\nunprecedented improvements in fMRI-to-image reconstruction efforts. Recent\nmodels trained on NSD have been evaluated only on seen image reconstruction.\nUsing NSD-Imagery, it is possible to assess how well these models perform on\nmental image reconstruction. This is a challenging generalization requirement\nbecause mental images are encoded in human brain activity with relatively lower\nsignal-to-noise and spatial resolution; however, generalization from seen to\nmental imagery is critical for real-world applications in medical domains and\nbrain-computer interfaces, where the desired information is always internally\ngenerated. We provide benchmarks for a suite of recent NSD-trained open-source\nvisual decoding models (MindEye1, MindEye2, Brain Diffuser, iCNN, Takagi et\nal.) on NSD-Imagery, and show that the performance of decoding methods on\nmental images is largely decoupled from performance on vision reconstruction.\nWe further demonstrate that architectural choices significantly impact\ncross-decoding performance: models employing simple linear decoding\narchitectures and multimodal feature decoding generalize better to mental\nimagery, while complex architectures tend to overfit visual training data. Our\nfindings indicate that mental imagery datasets are critical for the development\nof practical applications, and establish NSD-Imagery as a useful resource for\nbetter aligning visual decoding methods with this goal.", "AI": {"tldr": "NSD-Imagery is a new benchmark dataset for evaluating fMRI-to-image reconstruction models on mental imagery, showing performance differences from seen images and highlighting the importance of architectural choices.", "motivation": "To address the gap in evaluating fMRI-to-image models on mental imagery, which is critical for real-world applications like brain-computer interfaces.", "method": "The study benchmarks existing NSD-trained models (MindEye1, MindEye2, Brain Diffuser, iCNN, Takagi et al.) on NSD-Imagery to assess their performance on mental image reconstruction.", "result": "Performance on mental images is largely decoupled from seen images, with simpler linear and multimodal models generalizing better than complex architectures.", "conclusion": "Mental imagery datasets like NSD-Imagery are essential for practical applications, and simpler architectures may be more effective for cross-decoding tasks."}}
{"id": "2506.06844", "pdf": "https://arxiv.org/pdf/2506.06844", "abs": "https://arxiv.org/abs/2506.06844", "authors": ["Naibin Gu", "Peng Fu", "Xiyu Liu", "Ke Ma", "Zheng Lin", "Weiping Wang"], "title": "Adapt Once, Thrive with Updates: Transferable Parameter-Efficient Fine-Tuning on Evolving Base Models", "categories": ["cs.CL"], "comment": "Accepted by ACL 2025", "summary": "Parameter-efficient fine-tuning (PEFT) has become a common method for\nfine-tuning large language models, where a base model can serve multiple users\nthrough PEFT module switching. To enhance user experience, base models require\nperiodic updates. However, once updated, PEFT modules fine-tuned on previous\nversions often suffer substantial performance degradation on newer versions.\nRe-tuning these numerous modules to restore performance would incur significant\ncomputational costs. Through a comprehensive analysis of the changes that occur\nduring base model updates, we uncover an interesting phenomenon: continual\ntraining primarily affects task-specific knowledge stored in Feed-Forward\nNetworks (FFN), while having less impact on the task-specific pattern in the\nAttention mechanism. Based on these findings, we introduce Trans-PEFT, a novel\napproach that enhances the PEFT module by focusing on the task-specific pattern\nwhile reducing its dependence on certain knowledge in the base model. Further\ntheoretical analysis supports our approach. Extensive experiments across 7 base\nmodels and 12 datasets demonstrate that Trans-PEFT trained modules can maintain\nperformance on updated base models without re-tuning, significantly reducing\nmaintenance overhead in real-world applications.", "AI": {"tldr": "Trans-PEFT improves PEFT module performance on updated base models without re-tuning, reducing computational costs.", "motivation": "Address performance degradation of PEFT modules on updated base models and reduce re-tuning costs.", "method": "Focuses on task-specific patterns in Attention mechanisms, less affected by base model updates, and introduces Trans-PEFT.", "result": "Trans-PEFT maintains performance on updated models without re-tuning, validated across 7 base models and 12 datasets.", "conclusion": "Trans-PEFT reduces maintenance overhead and enhances PEFT module adaptability to base model updates."}}
{"id": "2506.06906", "pdf": "https://arxiv.org/pdf/2506.06906", "abs": "https://arxiv.org/abs/2506.06906", "authors": ["Nima Jamali", "Matina Mahdizadeh Sani", "Hanieh Naderi", "Shohreh Kasaei"], "title": "KNN-Defense: Defense against 3D Adversarial Point Clouds using Nearest-Neighbor Search", "categories": ["cs.CV"], "comment": null, "summary": "Deep neural networks (DNNs) have demonstrated remarkable performance in\nanalyzing 3D point cloud data. However, their vulnerability to adversarial\nattacks-such as point dropping, shifting, and adding-poses a critical challenge\nto the reliability of 3D vision systems. These attacks can compromise the\nsemantic and structural integrity of point clouds, rendering many existing\ndefense mechanisms ineffective. To address this issue, a defense strategy named\nKNN-Defense is proposed, grounded in the manifold assumption and\nnearest-neighbor search in feature space. Instead of reconstructing surface\ngeometry or enforcing uniform point distributions, the method restores\nperturbed inputs by leveraging the semantic similarity of neighboring samples\nfrom the training set. KNN-Defense is lightweight and computationally\nefficient, enabling fast inference and making it suitable for real-time and\npractical applications. Empirical results on the ModelNet40 dataset\ndemonstrated that KNN-Defense significantly improves robustness across various\nattack types. In particular, under point-dropping attacks-where many existing\nmethods underperform due to the targeted removal of critical points-the\nproposed method achieves accuracy gains of 20.1%, 3.6%, 3.44%, and 7.74% on\nPointNet, PointNet++, DGCNN, and PCT, respectively. These findings suggest that\nKNN-Defense offers a scalable and effective solution for enhancing the\nadversarial resilience of 3D point cloud classifiers. (An open-source\nimplementation of the method, including code and data, is available at\nhttps://github.com/nimajam41/3d-knn-defense).", "AI": {"tldr": "KNN-Defense, a lightweight method leveraging semantic similarity in feature space, improves robustness of 3D point cloud classifiers against adversarial attacks like point dropping, shifting, and adding.", "motivation": "Deep neural networks (DNNs) for 3D point cloud analysis are vulnerable to adversarial attacks, compromising reliability. Existing defenses often fail, necessitating a new approach.", "method": "KNN-Defense uses manifold assumption and nearest-neighbor search in feature space to restore perturbed inputs, avoiding surface reconstruction or uniform point distribution enforcement.", "result": "On ModelNet40, KNN-Defense boosts accuracy under point-dropping attacks by 20.1% (PointNet), 3.6% (PointNet++), 3.44% (DGCNN), and 7.74% (PCT).", "conclusion": "KNN-Defense is scalable, effective, and computationally efficient, enhancing adversarial resilience for real-time 3D point cloud applications."}}
{"id": "2506.06877", "pdf": "https://arxiv.org/pdf/2506.06877", "abs": "https://arxiv.org/abs/2506.06877", "authors": ["Jiaxing Guo", "Wenjie Yang", "Shengzhong Zhang", "Tongshan Xu", "Lun Du", "Da Zheng", "Zengfeng Huang"], "title": "Right Is Not Enough: The Pitfalls of Outcome Supervision in Training LLMs for Math Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Outcome-rewarded Large Language Models (LLMs) have demonstrated remarkable\nsuccess in mathematical problem-solving. However, this success often masks a\ncritical issue: models frequently achieve correct answers through fundamentally\nunsound reasoning processes, a phenomenon indicative of reward hacking. We\nintroduce MathOlympiadEval, a new dataset with fine-grained annotations, which\nreveals a significant gap between LLMs' answer correctness and their low\nprocess correctness. Existing automated methods like LLM-as-a-judge struggle to\nreliably detect these reasoning flaws. To address this, we propose\nParaStepVerifier, a novel methodology for meticulous, step-by-step verification\nof mathematical solutions. ParaStepVerifier identifies incorrect reasoning\nsteps. Empirical results demonstrate that ParaStepVerifier substantially\nimproves the accuracy of identifying flawed solutions compared to baselines,\nespecially for complex, multi-step problems. This offers a more robust path\ntowards evaluating and training LLMs with genuine mathematical reasoning.", "AI": {"tldr": "The paper highlights the issue of reward hacking in LLMs for math problem-solving, introduces MathOlympiadEval dataset, and proposes ParaStepVerifier for better step-by-step verification of reasoning flaws.", "motivation": "LLMs often produce correct answers via unsound reasoning, masking flaws. Existing methods fail to reliably detect these issues.", "method": "Introduces MathOlympiadEval dataset and ParaStepVerifier, a step-by-step verification tool for mathematical reasoning.", "result": "ParaStepVerifier outperforms baselines in detecting flawed reasoning, especially in complex problems.", "conclusion": "ParaStepVerifier provides a robust method for evaluating and improving LLMs' mathematical reasoning."}}
{"id": "2506.06909", "pdf": "https://arxiv.org/pdf/2506.06909", "abs": "https://arxiv.org/abs/2506.06909", "authors": ["Vladimir Yugay", "Thies Kersten", "Luca Carlone", "Theo Gevers", "Martin R. Oswald", "Lukas Schmid"], "title": "Gaussian Mapping for Evolving Scenes", "categories": ["cs.CV"], "comment": null, "summary": "Mapping systems with novel view synthesis (NVS) capabilities are widely used\nin computer vision, with augmented reality, robotics, and autonomous driving\napplications. Most notably, 3D Gaussian Splatting-based systems show high NVS\nperformance; however, many current approaches are limited to static scenes.\nWhile recent works have started addressing short-term dynamics (motion within\nthe view of the camera), long-term dynamics (the scene evolving through changes\nout of view) remain less explored. To overcome this limitation, we introduce a\ndynamic scene adaptation mechanism that continuously updates the 3D\nrepresentation to reflect the latest changes. In addition, since maintaining\ngeometric and semantic consistency remains challenging due to stale\nobservations disrupting the reconstruction process, we propose a novel keyframe\nmanagement mechanism that discards outdated observations while preserving as\nmuch information as possible. We evaluate Gaussian Mapping for Evolving Scenes\n(GaME) on both synthetic and real-world datasets and find it to be more\naccurate than the state of the art.", "AI": {"tldr": "The paper introduces GaME, a dynamic scene adaptation system for 3D Gaussian Splatting, addressing long-term scene evolution and maintaining consistency through keyframe management.", "motivation": "Current 3D Gaussian Splatting-based systems excel in static scenes but struggle with long-term dynamics (scene changes out of view). This work aims to overcome this limitation.", "method": "Proposes a dynamic scene adaptation mechanism for continuous 3D updates and a keyframe management system to discard outdated observations while preserving information.", "result": "GaME outperforms state-of-the-art methods in accuracy on synthetic and real-world datasets.", "conclusion": "GaME effectively handles evolving scenes, improving dynamic scene representation and consistency."}}
{"id": "2506.06887", "pdf": "https://arxiv.org/pdf/2506.06887", "abs": "https://arxiv.org/abs/2506.06887", "authors": ["Ziheng Qiao", "Houquan Zhou", "Zhenghua Li"], "title": "Mixture of Small and Large Models for Chinese Spelling Check", "categories": ["cs.CL"], "comment": null, "summary": "In the era of large language models (LLMs), the Chinese Spelling Check (CSC)\ntask has seen various LLM methods developed, yet their performance remains\nunsatisfactory. In contrast, fine-tuned BERT-based models, relying on\nhigh-quality in-domain data, show excellent performance but suffer from edit\npattern overfitting. This paper proposes a novel dynamic mixture approach that\neffectively combines the probability distributions of small models and LLMs\nduring the beam search decoding phase, achieving a balanced enhancement of\nprecise corrections from small models and the fluency of LLMs. This approach\nalso eliminates the need for fine-tuning LLMs, saving significant time and\nresources, and facilitating domain adaptation. Comprehensive experiments\ndemonstrate that our mixture approach significantly boosts error correction\ncapabilities, achieving state-of-the-art results across multiple datasets. Our\ncode is available at https://github.com/zhqiao-nlp/MSLLM.", "AI": {"tldr": "A dynamic mixture approach combines small models and LLMs for Chinese Spelling Check, improving correction precision and fluency without fine-tuning LLMs.", "motivation": "Current LLM methods for CSC underperform, while BERT-based models overfit. A balanced solution is needed.", "method": "Dynamic mixture of small models and LLMs during beam search decoding.", "result": "Achieves state-of-the-art performance across datasets, enhancing correction and fluency.", "conclusion": "The approach effectively balances precision and fluency, saving resources and enabling domain adaptation."}}
{"id": "2506.06912", "pdf": "https://arxiv.org/pdf/2506.06912", "abs": "https://arxiv.org/abs/2506.06912", "authors": ["Olivier Papillon", "Rafik Goubran", "James Green", "Julien Larivi\u00e8re-Chartier", "Caitlin Higginson", "Frank Knoefel", "R\u00e9becca Robillard"], "title": "Sleep Stage Classification using Multimodal Embedding Fusion from EOG and PSM", "categories": ["cs.CV"], "comment": "Submitted to IEEE MeMeA 2025", "summary": "Accurate sleep stage classification is essential for diagnosing sleep\ndisorders, particularly in aging populations. While traditional polysomnography\n(PSG) relies on electroencephalography (EEG) as the gold standard, its\ncomplexity and need for specialized equipment make home-based sleep monitoring\nchallenging. To address this limitation, we investigate the use of\nelectrooculography (EOG) and pressure-sensitive mats (PSM) as less obtrusive\nalternatives for five-stage sleep-wake classification. This study introduces a\nnovel approach that leverages ImageBind, a multimodal embedding deep learning\nmodel, to integrate PSM data with dual-channel EOG signals for sleep stage\nclassification. Our method is the first reported approach that fuses PSM and\nEOG data for sleep stage classification with ImageBind. Our results demonstrate\nthat fine-tuning ImageBind significantly improves classification accuracy,\noutperforming existing models based on single-channel EOG (DeepSleepNet),\nexclusively PSM data (ViViT), and other multimodal deep learning approaches\n(MBT). Notably, the model also achieved strong performance without fine-tuning,\nhighlighting its adaptability to specific tasks with limited labeled data,\nmaking it particularly advantageous for medical applications. We evaluated our\nmethod using 85 nights of patient recordings from a sleep clinic. Our findings\nsuggest that pre-trained multimodal embedding models, even those originally\ndeveloped for non-medical domains, can be effectively adapted for sleep\nstaging, with accuracies approaching systems that require complex EEG data.", "AI": {"tldr": "The paper proposes a novel sleep stage classification method using multimodal data (EOG and PSM) with ImageBind, achieving high accuracy and adaptability, even outperforming traditional EEG-based approaches.", "motivation": "Traditional PSG with EEG is complex and unsuitable for home-based monitoring. The study explores less obtrusive alternatives (EOG and PSM) for accurate sleep stage classification.", "method": "Leverages ImageBind, a multimodal embedding model, to integrate PSM data with dual-channel EOG signals. Evaluated on 85 nights of patient recordings.", "result": "Fine-tuning ImageBind improves accuracy, outperforming single-channel EOG, PSM-only, and other multimodal models. Strong performance even without fine-tuning.", "conclusion": "Pre-trained multimodal models like ImageBind can be effectively adapted for sleep staging, offering accuracy comparable to EEG-based systems with simpler data."}}
{"id": "2506.06888", "pdf": "https://arxiv.org/pdf/2506.06888", "abs": "https://arxiv.org/abs/2506.06888", "authors": ["Hamid Mojarad", "Kevin Tang"], "title": "Automatic Speech Recognition of African American English: Lexical and Contextual Effects", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "submitted to Interspeech 2025", "summary": "Automatic Speech Recognition (ASR) models often struggle with the phonetic,\nphonological, and morphosyntactic features found in African American English\n(AAE). This study focuses on two key AAE variables: Consonant Cluster Reduction\n(CCR) and ING-reduction. It examines whether the presence of CCR and\nING-reduction increases ASR misrecognition. Subsequently, it investigates\nwhether end-to-end ASR systems without an external Language Model (LM) are more\ninfluenced by lexical neighborhood effect and less by contextual predictability\ncompared to systems with an LM. The Corpus of Regional African American\nLanguage (CORAAL) was transcribed using wav2vec 2.0 with and without an LM. CCR\nand ING-reduction were detected using the Montreal Forced Aligner (MFA) with\npronunciation expansion. The analysis reveals a small but significant effect of\nCCR and ING on Word Error Rate (WER) and indicates a stronger presence of\nlexical neighborhood effect in ASR systems without LMs.", "AI": {"tldr": "The study examines how Consonant Cluster Reduction (CCR) and ING-reduction in African American English (AAE) affect ASR accuracy, finding a small but significant impact on Word Error Rate (WER). It also shows that ASR systems without Language Models (LMs) are more influenced by lexical neighborhood effects.", "motivation": "ASR models often struggle with AAE features like CCR and ING-reduction, leading to misrecognition. This study aims to quantify their impact and compare ASR systems with and without LMs.", "method": "The study uses the CORAAL corpus, transcribed with wav2vec 2.0 (with and without an LM). CCR and ING-reduction are detected using the Montreal Forced Aligner (MFA) with pronunciation expansion.", "result": "CCR and ING-reduction have a small but significant effect on WER. ASR systems without LMs show stronger lexical neighborhood effects.", "conclusion": "The findings highlight the need for ASR systems to better handle AAE features and suggest that LM-free systems may rely more on lexical neighborhood effects."}}
{"id": "2506.06918", "pdf": "https://arxiv.org/pdf/2506.06918", "abs": "https://arxiv.org/abs/2506.06918", "authors": ["Carl Brander", "Giovanni Cioffi", "Nico Messikommer", "Davide Scaramuzza"], "title": "Reading in the Dark with Foveated Event Vision", "categories": ["cs.CV", "cs.RO"], "comment": "CVPR 2025 Workshop on Event-based Vision", "summary": "Current smart glasses equipped with RGB cameras struggle to perceive the\nenvironment in low-light and high-speed motion scenarios due to motion blur and\nthe limited dynamic range of frame cameras. Additionally, capturing dense\nimages with a frame camera requires large bandwidth and power consumption,\nconsequently draining the battery faster. These challenges are especially\nrelevant for developing algorithms that can read text from images. In this\nwork, we propose a novel event-based Optical Character Recognition (OCR)\napproach for smart glasses. By using the eye gaze of the user, we foveate the\nevent stream to significantly reduce bandwidth by around 98% while exploiting\nthe benefits of event cameras in high-dynamic and fast scenes. Our proposed\nmethod performs deep binary reconstruction trained on synthetic data and\nleverages multimodal LLMs for OCR, outperforming traditional OCR solutions. Our\nresults demonstrate the ability to read text in low light environments where\nRGB cameras struggle while using up to 2400 times less bandwidth than a\nwearable RGB camera.", "AI": {"tldr": "A novel event-based OCR method for smart glasses uses eye gaze to reduce bandwidth by 98% and outperforms traditional OCR in low-light and high-speed scenarios.", "motivation": "Current RGB cameras on smart glasses fail in low-light and high-speed motion due to blur and limited dynamic range, and they consume excessive bandwidth and power.", "method": "The approach foveates the event stream using user eye gaze, employs deep binary reconstruction trained on synthetic data, and uses multimodal LLMs for OCR.", "result": "The method reads text in low-light conditions, reduces bandwidth by 98%, and uses 2400x less bandwidth than RGB cameras.", "conclusion": "The proposed event-based OCR is efficient and effective for smart glasses in challenging environments."}}
{"id": "2506.06929", "pdf": "https://arxiv.org/pdf/2506.06929", "abs": "https://arxiv.org/abs/2506.06929", "authors": ["Mikhail Krasitskii", "Grigori Sidorov", "Olga Kolesnikova", "Liliana Chanona Hernandez", "Alexander Gelbukh"], "title": "Hybrid Extractive Abstractive Summarization for Multilingual Sentiment Analysis", "categories": ["cs.CL"], "comment": "6 pages", "summary": "We propose a hybrid approach for multilingual sentiment analysis that\ncombines extractive and abstractive summarization to address the limitations of\nstandalone methods. The model integrates TF-IDF-based extraction with a\nfine-tuned XLM-R abstractive module, enhanced by dynamic thresholding and\ncultural adaptation. Experiments across 10 languages show significant\nimprovements over baselines, achieving 0.90 accuracy for English and 0.84 for\nlow-resource languages. The approach also demonstrates 22% greater\ncomputational efficiency than traditional methods. Practical applications\ninclude real-time brand monitoring and cross-cultural discourse analysis.\nFuture work will focus on optimization for low-resource languages via 8-bit\nquantization.", "AI": {"tldr": "A hybrid multilingual sentiment analysis model combining extractive and abstractive summarization outperforms baselines with high accuracy and efficiency.", "motivation": "Address limitations of standalone methods in multilingual sentiment analysis.", "method": "Integrates TF-IDF-based extraction with fine-tuned XLM-R abstractive module, using dynamic thresholding and cultural adaptation.", "result": "Achieves 0.90 accuracy for English, 0.84 for low-resource languages, and 22% greater computational efficiency.", "conclusion": "Effective for real-time applications; future work will optimize for low-resource languages via 8-bit quantization."}}
{"id": "2506.06928", "pdf": "https://arxiv.org/pdf/2506.06928", "abs": "https://arxiv.org/abs/2506.06928", "authors": ["George Lydakis", "Alexander Hermans", "Ali Athar", "Daan de Geus", "Bastian Leibe"], "title": "How Important are Videos for Training Video LLMs?", "categories": ["cs.CV"], "comment": "Project page on\n  https://visualcomputinginstitute.github.io/videollm-pseudovideo-training/", "summary": "Research into Video Large Language Models (LLMs) has progressed rapidly, with\nnumerous models and benchmarks emerging in just a few years. Typically, these\nmodels are initialized with a pretrained text-only LLM and finetuned on both\nimage- and video-caption datasets. In this paper, we present findings\nindicating that Video LLMs are more capable of temporal reasoning after\nimage-only training than one would assume, and that improvements from\nvideo-specific training are surprisingly small. Specifically, we show that\nimage-trained versions of two LLMs trained with the recent LongVU algorithm\nperform significantly above chance level on TVBench, a temporal reasoning\nbenchmark. Additionally, we introduce a simple finetuning scheme involving\nsequences of annotated images and questions targeting temporal capabilities.\nThis baseline results in temporal reasoning performance close to, and\noccasionally higher than, what is achieved by video-trained LLMs. This suggests\nsuboptimal utilization of rich temporal features found in real video by current\nmodels. Our analysis motivates further research into the mechanisms that allow\nimage-trained LLMs to perform temporal reasoning, as well as into the\nbottlenecks that render current video training schemes inefficient.", "AI": {"tldr": "Video LLMs trained on images show strong temporal reasoning, outperforming expectations, while video-specific training offers minimal gains. A simple image-based finetuning method matches or exceeds video-trained models, suggesting inefficiencies in current video training.", "motivation": "To investigate the temporal reasoning capabilities of Video LLMs and evaluate the impact of image-only versus video-specific training.", "method": "Initialized pretrained text-only LLMs, finetuned on image- and video-caption datasets. Introduced a simple finetuning scheme using annotated image sequences and temporal questions.", "result": "Image-trained LLMs performed well on temporal reasoning benchmarks, with video-trained models showing only slight improvements. The simple finetuning method matched or surpassed video-trained performance.", "conclusion": "Current video training schemes may inefficiently utilize temporal features. Further research is needed to understand image-trained LLMs' temporal reasoning and improve video training efficiency."}}
{"id": "2506.06930", "pdf": "https://arxiv.org/pdf/2506.06930", "abs": "https://arxiv.org/abs/2506.06930", "authors": ["Alexander Spangher", "Tenghao Huang", "Jialiang Gu", "Jiatong Shi", "Muhao Chen"], "title": "DiscoSum: Discourse-aware News Summarization", "categories": ["cs.CL", "cs.AI"], "comment": "8 pages, 3 figures, 10 pages in Appendix", "summary": "Recent advances in text summarization have predominantly leveraged large\nlanguage models to generate concise summaries. However, language models often\ndo not maintain long-term discourse structure, especially in news articles,\nwhere organizational flow significantly influences reader engagement. We\nintroduce a novel approach to integrating discourse structure into\nsummarization processes, focusing specifically on news articles across various\nmedia. We present a novel summarization dataset where news articles are\nsummarized multiple times in different ways across different social media\nplatforms (e.g. LinkedIn, Facebook, etc.). We develop a novel news discourse\nschema to describe summarization structures and a novel algorithm, DiscoSum,\nwhich employs beam search technique for structure-aware summarization, enabling\nthe transformation of news stories to meet different stylistic and structural\ndemands. Both human and automatic evaluation results demonstrate the efficacy\nof our approach in maintaining narrative fidelity and meeting structural\nrequirements.", "AI": {"tldr": "The paper introduces DiscoSum, a novel algorithm for structure-aware summarization of news articles, addressing the limitations of large language models in maintaining discourse structure.", "motivation": "Current language models often fail to preserve long-term discourse structure in news summarization, which is crucial for reader engagement.", "method": "The authors develop a news discourse schema and the DiscoSum algorithm, using beam search for structure-aware summarization tailored to different social media platforms.", "result": "Human and automatic evaluations show the approach effectively maintains narrative fidelity and meets structural requirements.", "conclusion": "The proposed method successfully integrates discourse structure into summarization, improving quality and adaptability for diverse platforms."}}
{"id": "2506.06944", "pdf": "https://arxiv.org/pdf/2506.06944", "abs": "https://arxiv.org/abs/2506.06944", "authors": ["Mellon M. Zhang", "Glen Chou", "Saibal Mukhopadhyay"], "title": "Polar Hierarchical Mamba: Towards Streaming LiDAR Object Detection with Point Clouds as Egocentric Sequences", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Accurate and efficient object detection is essential for autonomous vehicles,\nwhere real-time perception requires low latency and high throughput. LiDAR\nsensors provide robust depth information, but conventional methods process full\n360{\\deg} scans in a single pass, introducing significant delay. Streaming\napproaches address this by sequentially processing partial scans in the native\npolar coordinate system, yet they rely on translation-invariant convolutions\nthat are misaligned with polar geometry -- resulting in degraded performance or\nrequiring complex distortion mitigation. Recent Mamba-based state space models\n(SSMs) have shown promise for LiDAR perception, but only in the full-scan\nsetting, relying on geometric serialization and positional embeddings that are\nmemory-intensive and ill-suited to streaming. We propose Polar Hierarchical\nMamba (PHiM), a novel SSM architecture designed for polar-coordinate streaming\nLiDAR. PHiM uses local bidirectional Mamba blocks for intra-sector spatial\nencoding and a global forward Mamba for inter-sector temporal modeling,\nreplacing convolutions and positional encodings with distortion-aware,\ndimensionally-decomposed operations. PHiM sets a new state-of-the-art among\nstreaming detectors on the Waymo Open Dataset, outperforming the previous best\nby 10\\% and matching full-scan baselines at twice the throughput. Code will be\navailable at https://github.com/meilongzhang/Polar-Hierarchical-Mamba .", "AI": {"tldr": "PHiM, a novel SSM architecture for polar-coordinate streaming LiDAR, outperforms existing methods by 10% in accuracy and matches full-scan baselines at higher throughput.", "motivation": "Addressing the inefficiency and misalignment of conventional methods in processing LiDAR data for autonomous vehicles, which rely on translation-invariant convolutions and are memory-intensive.", "method": "PHiM uses local bidirectional Mamba blocks for spatial encoding and a global forward Mamba for temporal modeling, replacing convolutions with distortion-aware operations.", "result": "PHiM achieves state-of-the-art performance on the Waymo Open Dataset, improving accuracy by 10% and doubling throughput compared to full-scan baselines.", "conclusion": "PHiM is an efficient and accurate solution for streaming LiDAR processing, suitable for real-time autonomous vehicle perception."}}
{"id": "2506.06950", "pdf": "https://arxiv.org/pdf/2506.06950", "abs": "https://arxiv.org/abs/2506.06950", "authors": ["Do Xuan Long", "Duy Dinh", "Ngoc-Hai Nguyen", "Kenji Kawaguchi", "Nancy F. Chen", "Shafiq Joty", "Min-Yen Kan"], "title": "What Makes a Good Natural Language Prompt?", "categories": ["cs.CL"], "comment": "ACL 2025 Main Conference", "summary": "As large language models (LLMs) have progressed towards more human-like and\nhuman--AI communications have become prevalent, prompting has emerged as a\ndecisive component. However, there is limited conceptual consensus on what\nexactly quantifies natural language prompts. We attempt to address this\nquestion by conducting a meta-analysis surveying more than 150\nprompting-related papers from leading NLP and AI conferences from 2022 to 2025\nand blogs. We propose a property- and human-centric framework for evaluating\nprompt quality, encompassing 21 properties categorized into six dimensions. We\nthen examine how existing studies assess their impact on LLMs, revealing their\nimbalanced support across models and tasks, and substantial research gaps.\nFurther, we analyze correlations among properties in high-quality natural\nlanguage prompts, deriving prompting recommendations. We then empirically\nexplore multi-property prompt enhancements in reasoning tasks, observing that\nsingle-property enhancements often have the greatest impact. Finally, we\ndiscover that instruction-tuning on property-enhanced prompts can result in\nbetter reasoning models. Our findings establish a foundation for\nproperty-centric prompt evaluation and optimization, bridging the gaps between\nhuman--AI communication and opening new prompting research directions.", "AI": {"tldr": "The paper proposes a framework for evaluating prompt quality in LLMs, analyzing 150+ papers to identify 21 properties across six dimensions. It highlights research gaps, correlations among properties, and the impact of single-property enhancements. Instruction-tuning on enhanced prompts improves reasoning models.", "motivation": "Address the lack of consensus on quantifying natural language prompts in human-AI communication.", "method": "Meta-analysis of 150+ prompting-related papers, proposing a property-centric framework and empirically testing multi-property enhancements in reasoning tasks.", "result": "Single-property enhancements often have the greatest impact, and instruction-tuning on enhanced prompts improves reasoning models.", "conclusion": "The study provides a foundation for property-centric prompt evaluation and optimization, advancing human-AI communication and prompting research."}}
{"id": "2506.06952", "pdf": "https://arxiv.org/pdf/2506.06952", "abs": "https://arxiv.org/abs/2506.06952", "authors": ["Ying Shen", "Zhiyang Xu", "Jiuhai Chen", "Shizhe Diao", "Jiaxin Zhang", "Yuguang Yao", "Joy Rimchala", "Ismini Lourentzou", "Lifu Huang"], "title": "LaTtE-Flow: Layerwise Timestep-Expert Flow-based Transformer", "categories": ["cs.CV"], "comment": "Unified multimodal model, Flow-matching", "summary": "Recent advances in multimodal foundation models unifying image understanding\nand generation have opened exciting avenues for tackling a wide range of\nvision-language tasks within a single framework. Despite progress, existing\nunified models typically require extensive pretraining and struggle to achieve\nthe same level of performance compared to models dedicated to each task.\nAdditionally, many of these models suffer from slow image generation speeds,\nlimiting their practical deployment in real-time or resource-constrained\nsettings. In this work, we propose Layerwise Timestep-Expert Flow-based\nTransformer (LaTtE-Flow), a novel and efficient architecture that unifies image\nunderstanding and generation within a single multimodal model. LaTtE-Flow\nbuilds upon powerful pretrained Vision-Language Models (VLMs) to inherit strong\nmultimodal understanding capabilities, and extends them with a novel Layerwise\nTimestep Experts flow-based architecture for efficient image generation.\nLaTtE-Flow distributes the flow-matching process across specialized groups of\nTransformer layers, each responsible for a distinct subset of timesteps. This\ndesign significantly improves sampling efficiency by activating only a small\nsubset of layers at each sampling timestep. To further enhance performance, we\npropose a Timestep-Conditioned Residual Attention mechanism for efficient\ninformation reuse across layers. Experiments demonstrate that LaTtE-Flow\nachieves strong performance on multimodal understanding tasks, while achieving\ncompetitive image generation quality with around 6x faster inference speed\ncompared to recent unified multimodal models.", "AI": {"tldr": "LaTtE-Flow is a novel multimodal model unifying image understanding and generation efficiently, improving speed and performance over existing unified models.", "motivation": "Existing unified models require extensive pretraining, underperform task-specific models, and have slow image generation speeds, limiting practical use.", "method": "LaTtE-Flow uses a Layerwise Timestep-Expert Flow-based Transformer, leveraging pretrained VLMs and a flow-based architecture for efficient generation, with specialized layer groups for timesteps and a Timestep-Conditioned Residual Attention mechanism.", "result": "LaTtE-Flow achieves strong multimodal understanding and competitive image generation with 6x faster inference speed.", "conclusion": "LaTtE-Flow addresses efficiency and performance gaps in unified multimodal models, enabling practical deployment."}}
{"id": "2506.06955", "pdf": "https://arxiv.org/pdf/2506.06955", "abs": "https://arxiv.org/abs/2506.06955", "authors": ["Ha-Thanh Nguyen", "Chaoran Liu", "Hirokazu Kiyomaru", "Koichi Takeda", "Yusuke Miyao", "Maki Matsuda", "Yusuke Oda", "Pontus Stenetorp", "Qianying Liu", "Su Myat Noe", "Hideyuki Tachibana", "Kouta Nakayama", "Sadao Kurohashi"], "title": "BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for Belief-Inconsistent Syllogistic Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present BIS Reasoning 1.0, the first large-scale Japanese dataset of\nsyllogistic reasoning problems explicitly designed to evaluate\nbelief-inconsistent reasoning in large language models (LLMs). Unlike prior\ndatasets such as NeuBAROCO and JFLD, which focus on general or belief-aligned\nreasoning, BIS Reasoning 1.0 introduces logically valid yet belief-inconsistent\nsyllogisms to uncover reasoning biases in LLMs trained on human-aligned\ncorpora. We benchmark state-of-the-art models - including GPT models, Claude\nmodels, and leading Japanese LLMs - revealing significant variance in\nperformance, with GPT-4o achieving 79.54% accuracy. Our analysis identifies\ncritical weaknesses in current LLMs when handling logically valid but\nbelief-conflicting inputs. These findings have important implications for\ndeploying LLMs in high-stakes domains such as law, healthcare, and scientific\nliterature, where truth must override intuitive belief to ensure integrity and\nsafety.", "AI": {"tldr": "BIS Reasoning 1.0 is a Japanese dataset for evaluating belief-inconsistent reasoning in LLMs, showing performance gaps in models like GPT-4o.", "motivation": "To uncover biases in LLMs when handling logically valid but belief-conflicting syllogisms, especially in high-stakes domains.", "method": "Benchmarked state-of-the-art models (GPT, Claude, Japanese LLMs) on belief-inconsistent syllogisms.", "result": "GPT-4o achieved 79.54% accuracy, revealing weaknesses in handling belief-conflicting inputs.", "conclusion": "Highlights the need for LLMs to prioritize logical validity over intuitive beliefs in critical applications."}}
{"id": "2506.06953", "pdf": "https://arxiv.org/pdf/2506.06953", "abs": "https://arxiv.org/abs/2506.06953", "authors": ["Maciej Zyrek", "Tomasz Tarasiewicz", "Jakub Sadel", "Aleksandra Krzywon", "Michal Kawulok"], "title": "Task-driven real-world super-resolution of document scans", "categories": ["cs.CV"], "comment": null, "summary": "Single-image super-resolution refers to the reconstruction of a\nhigh-resolution image from a single low-resolution observation. Although recent\ndeep learning-based methods have demonstrated notable success on simulated\ndatasets -- with low-resolution images obtained by degrading and downsampling\nhigh-resolution ones -- they frequently fail to generalize to real-world\nsettings, such as document scans, which are affected by complex degradations\nand semantic variability. In this study, we introduce a task-driven, multi-task\nlearning framework for training a super-resolution network specifically\noptimized for optical character recognition tasks. We propose to incorporate\nauxiliary loss functions derived from high-level vision tasks, including text\ndetection using the connectionist text proposal network, text recognition via a\nconvolutional recurrent neural network, keypoints localization using Key.Net,\nand hue consistency. To balance these diverse objectives, we employ dynamic\nweight averaging mechanism, which adaptively adjusts the relative importance of\neach loss term based on its convergence behavior. We validate our approach upon\nthe SRResNet architecture, which is a well-established technique for\nsingle-image super-resolution. Experimental evaluations on both simulated and\nreal-world scanned document datasets demonstrate that the proposed approach\nimproves text detection, measured with intersection over union, while\npreserving overall image fidelity. These findings underscore the value of\nmulti-objective optimization in super-resolution models for bridging the gap\nbetween simulated training regimes and practical deployment in real-world\nscenarios.", "AI": {"tldr": "A multi-task learning framework for single-image super-resolution, optimized for OCR tasks, improves text detection and image fidelity in real-world scenarios.", "motivation": "Address the failure of deep learning-based super-resolution methods to generalize to real-world settings with complex degradations and semantic variability.", "method": "Proposes a task-driven, multi-task learning framework with auxiliary loss functions (text detection, recognition, keypoints localization, hue consistency) and dynamic weight averaging. Validated on SRResNet.", "result": "Improves text detection (measured with IoU) and preserves image fidelity on simulated and real-world datasets.", "conclusion": "Multi-objective optimization bridges the gap between simulated training and real-world deployment in super-resolution models."}}
{"id": "2506.06964", "pdf": "https://arxiv.org/pdf/2506.06964", "abs": "https://arxiv.org/abs/2506.06964", "authors": ["Subhojyoti Mukherjee", "Viet Dac Lai", "Raghavendra Addanki", "Ryan Rossi", "Seunghyun Yoon", "Trung Bui", "Anup Rao", "Jayakumar Subramanian", "Branislav Kveton"], "title": "Learning to Clarify by Reinforcement Learning Through Reward-Weighted Fine-Tuning", "categories": ["cs.CL", "cs.LG"], "comment": "39 pages", "summary": "Question answering (QA) agents automatically answer questions posed in\nnatural language. In this work, we learn to ask clarifying questions in QA\nagents. The key idea in our method is to simulate conversations that contain\nclarifying questions and learn from them using reinforcement learning (RL). To\nmake RL practical, we propose and analyze offline RL objectives that can be\nviewed as reward-weighted supervised fine-tuning (SFT) and easily optimized in\nlarge language models. Our work stands in a stark contrast to recently proposed\nmethods, based on SFT and direct preference optimization, which have additional\nhyper-parameters and do not directly optimize rewards. We compare to these\nmethods empirically and report gains in both optimized rewards and language\nquality.", "AI": {"tldr": "The paper introduces a method for QA agents to learn to ask clarifying questions using reinforcement learning (RL), focusing on offline RL objectives for practicality.", "motivation": "To improve QA agents by enabling them to ask clarifying questions, enhancing interaction and accuracy.", "method": "Simulates conversations with clarifying questions and uses offline RL (reward-weighted supervised fine-tuning) for learning.", "result": "Outperforms SFT and direct preference optimization methods in rewards and language quality.", "conclusion": "The proposed offline RL approach is effective for optimizing QA agents' ability to ask clarifying questions."}}
{"id": "2506.06962", "pdf": "https://arxiv.org/pdf/2506.06962", "abs": "https://arxiv.org/abs/2506.06962", "authors": ["Jingyuan Qi", "Zhiyang Xu", "Qifan Wang", "Lifu Huang"], "title": "AR-RAG: Autoregressive Retrieval Augmentation for Image Generation", "categories": ["cs.CV"], "comment": "Image Generation, Retrieval Augmented Generation", "summary": "We introduce Autoregressive Retrieval Augmentation (AR-RAG), a novel paradigm\nthat enhances image generation by autoregressively incorporating knearest\nneighbor retrievals at the patch level. Unlike prior methods that perform a\nsingle, static retrieval before generation and condition the entire generation\non fixed reference images, AR-RAG performs context-aware retrievals at each\ngeneration step, using prior-generated patches as queries to retrieve and\nincorporate the most relevant patch-level visual references, enabling the model\nto respond to evolving generation needs while avoiding limitations (e.g.,\nover-copying, stylistic bias, etc.) prevalent in existing methods. To realize\nAR-RAG, we propose two parallel frameworks: (1) Distribution-Augmentation in\nDecoding (DAiD), a training-free plug-and-use decoding strategy that directly\nmerges the distribution of model-predicted patches with the distribution of\nretrieved patches, and (2) Feature-Augmentation in Decoding (FAiD), a\nparameter-efficient fine-tuning method that progressively smooths the features\nof retrieved patches via multi-scale convolution operations and leverages them\nto augment the image generation process. We validate the effectiveness of\nAR-RAG on widely adopted benchmarks, including Midjourney-30K, GenEval and\nDPG-Bench, demonstrating significant performance gains over state-of-the-art\nimage generation models.", "AI": {"tldr": "AR-RAG enhances image generation by dynamically retrieving and incorporating patch-level references during each generation step, avoiding issues like over-copying. It introduces two frameworks: DAiD (training-free) and FAiD (fine-tuning), showing significant performance gains.", "motivation": "To address limitations of static retrieval methods in image generation (e.g., over-copying, stylistic bias) by enabling dynamic, context-aware retrievals.", "method": "Proposes AR-RAG with two frameworks: DAiD (merges predicted and retrieved patch distributions) and FAiD (fine-tunes features of retrieved patches).", "result": "Validated on benchmarks (Midjourney-30K, GenEval, DPG-Bench), showing superior performance over state-of-the-art models.", "conclusion": "AR-RAG offers a flexible and effective approach for improving image generation through dynamic retrieval augmentation."}}
{"id": "2506.06968", "pdf": "https://arxiv.org/pdf/2506.06968", "abs": "https://arxiv.org/abs/2506.06968", "authors": ["Pavel Kovalev", "Carlo Angiuli"], "title": "A dependently-typed calculus of event telicity and culminativity", "categories": ["cs.CL", "cs.LO"], "comment": "52 pages, Agda formalization available at\n  https://doi.org/10.5281/zenodo.15602617", "summary": "We present a dependently-typed cross-linguistic framework for analyzing the\ntelicity and culminativity of events, accompanied by examples of using our\nframework to model English sentences. Our framework consists of two parts. In\nthe nominal domain, we model the boundedness of noun phrases and its\nrelationship to subtyping, delimited quantities, and adjectival modification.\nIn the verbal domain we define a dependent event calculus, modeling telic\nevents as those whose undergoer is bounded, culminating events as telic events\nthat achieve their inherent endpoint, and consider adverbial modification. In\nboth domains we pay particular attention to associated entailments. Our\nframework is defined as an extension of intensional Martin-L\\\"of dependent type\ntheory, and the rules and examples in this paper have been formalized in the\nAgda proof assistant.", "AI": {"tldr": "A dependently-typed framework for analyzing event telicity and culminativity, applied to English sentences, formalized in Agda.", "motivation": "To provide a cross-linguistic framework for modeling boundedness in noun phrases and telic/culminating events in verbs, with attention to entailments.", "method": "Extends intensional Martin-L\u00f6f dependent type theory, with rules formalized in Agda. Analyzes nominal boundedness and verbal event calculus.", "result": "A formalized framework for linguistic analysis, demonstrating how boundedness and event properties interact.", "conclusion": "The framework successfully models linguistic phenomena, with potential for broader cross-linguistic application."}}
{"id": "2506.06966", "pdf": "https://arxiv.org/pdf/2506.06966", "abs": "https://arxiv.org/abs/2506.06966", "authors": ["Siyuan Jing", "Guangxue Wang", "Haoyang Zhai", "Qin Tao", "Jun Yang", "Bing Wang", "Peng Jin"], "title": "Dual-view Spatio-Temporal Feature Fusion with CNN-Transformer Hybrid Network for Chinese Isolated Sign Language Recognition", "categories": ["cs.CV"], "comment": "18 pages, 3 figures", "summary": "Due to the emergence of many sign language datasets, isolated sign language\nrecognition (ISLR) has made significant progress in recent years. In addition,\nthe development of various advanced deep neural networks is another reason for\nthis breakthrough. However, challenges remain in applying the technique in the\nreal world. First, existing sign language datasets do not cover the whole sign\nvocabulary. Second, most of the sign language datasets provide only single view\nRGB videos, which makes it difficult to handle hand occlusions when performing\nISLR. To fill this gap, this paper presents a dual-view sign language dataset\nfor ISLR named NationalCSL-DP, which fully covers the Chinese national sign\nlanguage vocabulary. The dataset consists of 134140 sign videos recorded by ten\nsigners with respect to two vertical views, namely, the front side and the left\nside. Furthermore, a CNN transformer network is also proposed as a strong\nbaseline and an extremely simple but effective fusion strategy for prediction.\nExtensive experiments were conducted to prove the effectiveness of the datasets\nas well as the baseline. The results show that the proposed fusion strategy can\nsignificantly increase the performance of the ISLR, but it is not easy for the\nsequence-to-sequence model, regardless of whether the early-fusion or\nlate-fusion strategy is applied, to learn the complementary features from the\nsign videos of two vertical views.", "AI": {"tldr": "The paper introduces NationalCSL-DP, a dual-view dataset for isolated sign language recognition (ISLR), addressing gaps in existing datasets and proposing a CNN transformer network with a fusion strategy.", "motivation": "Existing ISLR datasets lack full vocabulary coverage and single-view limitations hinder real-world application due to hand occlusions.", "method": "A dual-view dataset (NationalCSL-DP) with 134140 sign videos from two vertical views (front and left) is created. A CNN transformer network and fusion strategy are proposed.", "result": "The fusion strategy improves ISLR performance, but sequence-to-sequence models struggle to learn complementary features from dual-view videos.", "conclusion": "NationalCSL-DP fills dataset gaps and the proposed baseline is effective, though challenges remain in feature fusion for dual-view learning."}}
{"id": "2506.06971", "pdf": "https://arxiv.org/pdf/2506.06971", "abs": "https://arxiv.org/abs/2506.06971", "authors": ["Jaechul Roh", "Varun Gandhi", "Shivani Anilkumar", "Arin Garg"], "title": "Break-The-Chain: Reasoning Failures in LLMs via Adversarial Prompting in Code Generation", "categories": ["cs.CL", "cs.CR"], "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable success in tasks\nrequiring complex reasoning, such as code generation, mathematical problem\nsolving, and algorithmic synthesis -- especially when aided by reasoning tokens\nand Chain-of-Thought prompting. Yet, a core question remains: do these models\ntruly reason, or do they merely exploit shallow statistical patterns? In this\npaper, we systematically investigate the robustness of reasoning LLMs by\nintroducing a suite of semantically faithful yet adversarially structured\nprompt perturbations. Our evaluation -- spanning 700 perturbed code generations\nderived from LeetCode-style problems -- applies transformations such as\nstorytelling reframing, irrelevant constraint injection, example reordering,\nand numeric perturbation. We observe that while certain modifications severely\ndegrade performance (with accuracy drops up to -42.1%), others surprisingly\nimprove model accuracy by up to 35.3%, suggesting sensitivity not only to\nsemantics but also to surface-level prompt dynamics. These findings expose the\nfragility and unpredictability of current reasoning systems, underscoring the\nneed for more principles approaches to reasoning alignments and prompting\nrobustness. We release our perturbation datasets and evaluation framework to\npromote further research in trustworthy and resilient LLM reasoning.", "AI": {"tldr": "The paper investigates whether LLMs truly reason or rely on shallow patterns by testing their robustness against adversarially structured prompt perturbations. Results show significant performance variations, highlighting fragility and the need for better reasoning alignment.", "motivation": "To determine if LLMs genuinely reason or exploit statistical patterns, and to assess their robustness under adversarial prompt modifications.", "method": "Introduces semantically faithful but adversarially structured prompt perturbations (e.g., storytelling reframing, irrelevant constraints) and evaluates 700 perturbed code generations from LeetCode-style problems.", "result": "Performance varies widely: some perturbations degrade accuracy by up to 42.1%, while others improve it by 35.3%, revealing sensitivity to prompt dynamics.", "conclusion": "Current reasoning systems are fragile and unpredictable, emphasizing the need for principled approaches to robust prompting and reasoning alignment."}}
{"id": "2506.06970", "pdf": "https://arxiv.org/pdf/2506.06970", "abs": "https://arxiv.org/abs/2506.06970", "authors": ["Pengfei Zhao", "Rongbo Luan", "Wei Zhang", "Peng Wu", "Sifeng He"], "title": "Guiding Cross-Modal Representations with MLLM Priors via Preference Alignment", "categories": ["cs.CV"], "comment": null, "summary": "Despite Contrastive Language-Image Pretraining (CLIP)'s remarkable capability\nto retrieve content across modalities, a substantial modality gap persists in\nits feature space. Intriguingly, we discover that off-the-shelf MLLMs\n(Multimodal Large Language Models) demonstrate powerful inherent modality\nalignment properties. While recent MLLM-based retrievers with unified\narchitectures partially mitigate this gap, their reliance on coarse modality\nalignment mechanisms fundamentally limits their potential. In this work, We\nintroduce MAPLE (Modality-Aligned Preference Learning for Embeddings), a novel\nframework that leverages the fine grained alignment priors inherent in MLLM to\nguide cross modal representation learning. MAPLE formulates the learning\nprocess as reinforcement learning with two key components: (1) Automatic\npreference data construction using off-the-shelf MLLM, and (2) a new Relative\nPreference Alignment (RPA) loss, which adapts Direct Preference Optimization\n(DPO) to the embedding learning setting. Experimental results show that our\npreference-guided alignment achieves substantial gains in fine-grained\ncross-modal retrieval, underscoring its effectiveness in handling nuanced\nsemantic distinctions.", "AI": {"tldr": "MAPLE introduces a reinforcement learning framework for fine-grained cross-modal alignment using MLLMs, outperforming existing methods in retrieval tasks.", "motivation": "To address the modality gap in CLIP and leverage MLLMs' inherent alignment properties for better cross-modal representation learning.", "method": "Uses reinforcement learning with automatic preference data construction and a Relative Preference Alignment (RPA) loss.", "result": "Achieves significant improvements in fine-grained cross-modal retrieval.", "conclusion": "MAPLE effectively bridges the modality gap and enhances semantic alignment in cross-modal tasks."}}
{"id": "2506.06972", "pdf": "https://arxiv.org/pdf/2506.06972", "abs": "https://arxiv.org/abs/2506.06972", "authors": ["Yuji Zhang", "Qingyun Wang", "Cheng Qian", "Jiateng Liu", "Chenkai Sun", "Denghui Zhang", "Tarek Abdelzaher", "Chengxiang Zhai", "Preslav Nakov", "Heng Ji"], "title": "Atomic Reasoning for Scientific Table Claim Verification", "categories": ["cs.CL"], "comment": null, "summary": "Scientific texts often convey authority due to their technical language and\ncomplex data. However, this complexity can sometimes lead to the spread of\nmisinformation. Non-experts are particularly susceptible to misleading claims\nbased on scientific tables due to their high information density and perceived\ncredibility. Existing table claim verification models, including\nstate-of-the-art large language models (LLMs), often struggle with precise\nfine-grained reasoning, resulting in errors and a lack of precision in\nverifying scientific claims. Inspired by Cognitive Load Theory, we propose that\nenhancing a model's ability to interpret table-based claims involves reducing\ncognitive load by developing modular, reusable reasoning components (i.e.,\natomic skills). We introduce a skill-chaining schema that dynamically composes\nthese skills to facilitate more accurate and generalizable reasoning with a\nreduced cognitive load. To evaluate this, we create SciAtomicBench, a\ncross-domain benchmark with fine-grained reasoning annotations. With only 350\nfine-tuning examples, our model trained by atomic reasoning outperforms\nGPT-4o's chain-of-thought method, achieving state-of-the-art results with far\nless training data.", "AI": {"tldr": "The paper addresses misinformation in scientific tables by proposing a modular reasoning approach (atomic skills) to reduce cognitive load, outperforming GPT-4o with minimal training data.", "motivation": "Scientific tables can mislead non-experts due to their complexity. Existing models lack precision in verifying claims, prompting the need for better reasoning methods.", "method": "Introduces a skill-chaining schema using modular atomic skills to reduce cognitive load and improve reasoning. Evaluated on SciAtomicBench, a cross-domain benchmark.", "result": "The model, trained with only 350 examples, surpasses GPT-4o's chain-of-thought method, achieving state-of-the-art accuracy.", "conclusion": "Modular reasoning with atomic skills enhances claim verification in scientific tables, offering a scalable and efficient solution."}}
{"id": "2506.06988", "pdf": "https://arxiv.org/pdf/2506.06988", "abs": "https://arxiv.org/abs/2506.06988", "authors": ["Binxiao Huang", "Zhihao Li", "Shiyong Liu", "Xiao Tang", "Jiajun Tang", "Jiaqi Lin", "Yuxin Cheng", "Zhenyu Chen", "Xiaofei Wu", "Ngai Wong"], "title": "Hybrid Mesh-Gaussian Representation for Efficient Indoor Scene Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian splatting (3DGS) has demonstrated exceptional performance in\nimage-based 3D reconstruction and real-time rendering. However, regions with\ncomplex textures require numerous Gaussians to capture significant color\nvariations accurately, leading to inefficiencies in rendering speed. To address\nthis challenge, we introduce a hybrid representation for indoor scenes that\ncombines 3DGS with textured meshes. Our approach uses textured meshes to handle\ntexture-rich flat areas, while retaining Gaussians to model intricate\ngeometries. The proposed method begins by pruning and refining the extracted\nmesh to eliminate geometrically complex regions. We then employ a joint\noptimization for 3DGS and mesh, incorporating a warm-up strategy and\ntransmittance-aware supervision to balance their contributions\nseamlessly.Extensive experiments demonstrate that the hybrid representation\nmaintains comparable rendering quality and achieves superior frames per second\nFPS with fewer Gaussian primitives.", "AI": {"tldr": "A hybrid method combining 3D Gaussian splatting (3DGS) and textured meshes improves efficiency in rendering complex indoor scenes by reducing Gaussian primitives while maintaining quality.", "motivation": "Complex textures in 3DGS require excessive Gaussians, slowing rendering. A hybrid approach aims to balance efficiency and accuracy.", "method": "Uses textured meshes for flat, texture-rich areas and 3DGS for complex geometries. Includes mesh pruning, joint optimization, warm-up strategy, and transmittance-aware supervision.", "result": "Achieves comparable rendering quality with fewer Gaussians and higher FPS.", "conclusion": "The hybrid representation effectively addresses inefficiencies in 3DGS for complex scenes, enhancing rendering speed without sacrificing quality."}}
{"id": "2506.06982", "pdf": "https://arxiv.org/pdf/2506.06982", "abs": "https://arxiv.org/abs/2506.06982", "authors": ["Cong Liu", "Jie Wu", "Weigang Wu", "Xu Chen", "Liang Lin", "Wei-Shi Zheng"], "title": "Chain of Methodologies: Scaling Test Time Computation without Training", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) often struggle with complex reasoning tasks due\nto insufficient in-depth insights in their training data, which are typically\nabsent in publicly available documents. This paper introduces the Chain of\nMethodologies (CoM), an innovative and intuitive prompting framework that\nenhances structured thinking by integrating human methodological insights,\nenabling LLMs to tackle complex tasks with extended reasoning. CoM leverages\nthe metacognitive abilities of advanced LLMs, activating systematic reasoning\nthrought user-defined methodologies without explicit fine-tuning. Experiments\nshow that CoM surpasses competitive baselines, demonstrating the potential of\ntraining-free prompting methods as robust solutions for complex reasoning tasks\nand bridging the gap toward human-level reasoning through human-like\nmethodological insights.", "AI": {"tldr": "CoM is a prompting framework that enhances LLMs' reasoning by integrating human methodological insights, outperforming baselines without fine-tuning.", "motivation": "LLMs lack in-depth insights for complex reasoning tasks due to limited training data. CoM aims to bridge this gap by leveraging human-like methodologies.", "method": "CoM uses a prompting framework to activate systematic reasoning in LLMs through user-defined methodologies, avoiding explicit fine-tuning.", "result": "Experiments show CoM outperforms baselines, proving its effectiveness for complex reasoning tasks.", "conclusion": "CoM demonstrates the potential of training-free prompting methods to achieve human-level reasoning in LLMs."}}
{"id": "2506.06992", "pdf": "https://arxiv.org/pdf/2506.06992", "abs": "https://arxiv.org/abs/2506.06992", "authors": ["Yanting Gao", "Yepeng Liu", "Junming Liu", "Qi Zhang", "Hongyun Zhang", "Duoqian Miao", "Cairong Zhao"], "title": "Boosting Adversarial Transferability via Commonality-Oriented Gradient Optimization", "categories": ["cs.CV"], "comment": "22 pages", "summary": "Exploring effective and transferable adversarial examples is vital for\nunderstanding the characteristics and mechanisms of Vision Transformers (ViTs).\nHowever, adversarial examples generated from surrogate models often exhibit\nweak transferability in black-box settings due to overfitting. Existing methods\nimprove transferability by diversifying perturbation inputs or applying uniform\ngradient regularization within surrogate models, yet they have not fully\nleveraged the shared and unique features of surrogate models trained on the\nsame task, leading to suboptimal transfer performance. Therefore, enhancing\nperturbations of common information shared by surrogate models and suppressing\nthose tied to individual characteristics offers an effective way to improve\ntransferability. Accordingly, we propose a commonality-oriented gradient\noptimization strategy (COGO) consisting of two components: Commonality\nEnhancement (CE) and Individuality Suppression (IS). CE perturbs the mid-to-low\nfrequency regions, leveraging the fact that ViTs trained on the same dataset\ntend to rely more on mid-to-low frequency information for classification. IS\nemploys adaptive thresholds to evaluate the correlation between backpropagated\ngradients and model individuality, assigning weights to gradients accordingly.\nExtensive experiments demonstrate that COGO significantly improves the transfer\nsuccess rates of adversarial attacks, outperforming current state-of-the-art\nmethods.", "AI": {"tldr": "The paper proposes COGO, a method to improve adversarial example transferability in Vision Transformers (ViTs) by enhancing shared features and suppressing individual model characteristics.", "motivation": "Adversarial examples from surrogate models often transfer poorly due to overfitting. Existing methods don't fully exploit shared features among models.", "method": "COGO includes Commonality Enhancement (CE) for mid-to-low frequency perturbations and Individuality Suppression (IS) for adaptive gradient weighting.", "result": "COGO significantly boosts transfer success rates, outperforming state-of-the-art methods.", "conclusion": "COGO effectively improves adversarial transferability by focusing on shared model features and suppressing individuality."}}
{"id": "2506.06987", "pdf": "https://arxiv.org/pdf/2506.06987", "abs": "https://arxiv.org/abs/2506.06987", "authors": ["Senqi Yang", "Dongyu Zhang", "Jing Ren", "Ziqi Xu", "Xiuzhen Zhang", "Yiliao Song", "Hongfei Lin", "Feng Xia"], "title": "Cultural Bias Matters: A Cross-Cultural Benchmark Dataset and Sentiment-Enriched Model for Understanding Multimodal Metaphors", "categories": ["cs.CL"], "comment": "This paper has been accepted to the 63rd Annual Meeting of the\n  Association for Computational Linguistics (ACL 2025), Main Conference", "summary": "Metaphors are pervasive in communication, making them crucial for natural\nlanguage processing (NLP). Previous research on automatic metaphor processing\npredominantly relies on training data consisting of English samples, which\noften reflect Western European or North American biases. This cultural skew can\nlead to an overestimation of model performance and contributions to NLP\nprogress. However, the impact of cultural bias on metaphor processing,\nparticularly in multimodal contexts, remains largely unexplored. To address\nthis gap, we introduce MultiMM, a Multicultural Multimodal Metaphor dataset\ndesigned for cross-cultural studies of metaphor in Chinese and English. MultiMM\nconsists of 8,461 text-image advertisement pairs, each accompanied by\nfine-grained annotations, providing a deeper understanding of multimodal\nmetaphors beyond a single cultural domain. Additionally, we propose\nSentiment-Enriched Metaphor Detection (SEMD), a baseline model that integrates\nsentiment embeddings to enhance metaphor comprehension across cultural\nbackgrounds. Experimental results validate the effectiveness of SEMD on\nmetaphor detection and sentiment analysis tasks. We hope this work increases\nawareness of cultural bias in NLP research and contributes to the development\nof fairer and more inclusive language models. Our dataset and code are\navailable at https://github.com/DUTIR-YSQ/MultiMM.", "AI": {"tldr": "The paper introduces MultiMM, a multicultural multimodal metaphor dataset for Chinese and English, and proposes SEMD, a sentiment-enriched metaphor detection model, to address cultural bias in NLP.", "motivation": "To explore the impact of cultural bias on metaphor processing, especially in multimodal contexts, and provide a fairer, more inclusive approach.", "method": "Developed MultiMM dataset (8,461 text-image pairs) and SEMD model integrating sentiment embeddings for cross-cultural metaphor detection.", "result": "SEMD shows effectiveness in metaphor detection and sentiment analysis, validated by experiments.", "conclusion": "The work highlights cultural bias in NLP and advances fairer, more inclusive language models."}}
{"id": "2506.06993", "pdf": "https://arxiv.org/pdf/2506.06993", "abs": "https://arxiv.org/abs/2506.06993", "authors": ["Cong Guan", "Jiacheng Ying", "Yuya Ieiri", "Osamu Yoshie"], "title": "DM$^3$Net: Dual-Camera Super-Resolution via Domain Modulation and Multi-scale Matching", "categories": ["cs.CV"], "comment": null, "summary": "Dual-camera super-resolution is highly practical for smartphone photography\nthat primarily super-resolve the wide-angle images using the telephoto image as\na reference. In this paper, we propose DM$^3$Net, a novel dual-camera\nsuper-resolution network based on Domain Modulation and Multi-scale Matching.\nTo bridge the domain gap between the high-resolution domain and the degraded\ndomain, we learn two compressed global representations from image pairs\ncorresponding to the two domains. To enable reliable transfer of high-frequency\nstructural details from the reference image, we design a multi-scale matching\nmodule that conducts patch-level feature matching and retrieval across multiple\nreceptive fields to improve matching accuracy and robustness. Moreover, we also\nintroduce Key Pruning to achieve a significant reduction in memory usage and\ninference time with little model performance sacrificed. Experimental results\non three real-world datasets demonstrate that our DM$^3$Net outperforms the\nstate-of-the-art approaches.", "AI": {"tldr": "DM$^3$Net improves dual-camera super-resolution by addressing domain gaps and enhancing detail transfer with multi-scale matching, achieving state-of-the-art results.", "motivation": "To enhance smartphone photography by super-resolving wide-angle images using telephoto references, overcoming domain gaps and improving detail transfer.", "method": "Uses Domain Modulation for global representation learning and Multi-scale Matching for accurate high-frequency detail transfer, plus Key Pruning for efficiency.", "result": "Outperforms state-of-the-art methods on three real-world datasets.", "conclusion": "DM$^3$Net effectively bridges domain gaps and improves super-resolution performance with efficient computational usage."}}
{"id": "2506.06998", "pdf": "https://arxiv.org/pdf/2506.06998", "abs": "https://arxiv.org/abs/2506.06998", "authors": ["Ming Li", "Zhengyuan Yang", "Xiyao Wang", "Dianqi Li", "Kevin Lin", "Tianyi Zhou", "Lijuan Wang"], "title": "What makes Reasoning Models Different? Follow the Reasoning Leader for Efficient Decoding", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large reasoning models (LRMs) achieve strong reasoning performance by\nemitting long chains of thought. Yet, these verbose traces slow down inference\nand often drift into unnecessary detail, known as the overthinking phenomenon.\nTo better understand LRMs' behavior, we systematically analyze the token-level\nmisalignment between reasoning and non-reasoning models. While it is expected\nthat their primary difference lies in the stylistic \"thinking cues\", LRMs\nuniquely exhibit two pivotal, previously under-explored phenomena: a Global\nMisalignment Rebound, where their divergence from non-reasoning models persists\nor even grows as response length increases, and more critically, a Local\nMisalignment Diminish, where the misalignment concentrates at the \"thinking\ncues\" each sentence starts with but rapidly declines in the remaining of the\nsentence. Motivated by the Local Misalignment Diminish, we propose\nFoReaL-Decoding, a collaborative fast-slow thinking decoding method for\ncost-quality trade-off. In FoReaL-Decoding, a Leading model leads the first few\ntokens for each sentence, and then a weaker draft model completes the following\ntokens to the end of each sentence. FoReaL-Decoding adopts a stochastic gate to\nsmoothly interpolate between the small and the large model. On four popular\nmath-reasoning benchmarks (AIME24, GPQA-Diamond, MATH500, AMC23),\nFoReaL-Decoding reduces theoretical FLOPs by 30 to 50% and trims CoT length by\nup to 40%, while preserving 86 to 100% of model performance. These results\nestablish FoReaL-Decoding as a simple, plug-and-play route to controllable\ncost-quality trade-offs in reasoning-centric tasks.", "AI": {"tldr": "FoReaL-Decoding reduces FLOPs and CoT length while maintaining performance in reasoning tasks.", "motivation": "To address the overthinking phenomenon and token-level misalignment in large reasoning models.", "method": "Proposes FoReaL-Decoding, a collaborative fast-slow thinking decoding method with a stochastic gate.", "result": "Reduces FLOPs by 30-50% and CoT length by up to 40%, preserving 86-100% performance.", "conclusion": "FoReaL-Decoding offers a simple, plug-and-play solution for cost-quality trade-offs in reasoning tasks."}}
{"id": "2506.06995", "pdf": "https://arxiv.org/pdf/2506.06995", "abs": "https://arxiv.org/abs/2506.06995", "authors": ["Xiaoya Zhang"], "title": "Technical Report for ICRA 2025 GOOSE 3D Semantic Segmentation Challenge: Adaptive Point Cloud Understanding for Heterogeneous Robotic Systems", "categories": ["cs.CV"], "comment": "Winner of the GOOSE 3D Semantic Segmentation Challenge at the IEEE\n  ICRA Workshop on Field Robotics 2025", "summary": "This technical report presents the implementation details of the winning\nsolution for the ICRA 2025 GOOSE 3D Semantic Segmentation Challenge. This\nchallenge focuses on semantic segmentation of 3D point clouds from diverse\nunstructured outdoor environments collected from multiple robotic platforms.\nThis problem was addressed by implementing Point Prompt Tuning (PPT) integrated\nwith Point Transformer v3 (PTv3) backbone, enabling adaptive processing of\nheterogeneous LiDAR data through platform-specific conditioning and\ncross-dataset class alignment strategies. The model is trained without\nrequiring additional external data. As a result, this approach achieved\nsubstantial performance improvements with mIoU increases of up to 22.59% on\nchallenging platforms compared to the baseline PTv3 model, demonstrating the\neffectiveness of adaptive point cloud understanding for field robotics\napplications.", "AI": {"tldr": "The winning solution for ICRA 2025 GOOSE 3D Semantic Segmentation Challenge uses Point Prompt Tuning (PPT) with Point Transformer v3 (PTv3) for adaptive 3D point cloud segmentation, achieving a 22.59% mIoU improvement.", "motivation": "Address semantic segmentation of 3D point clouds from diverse outdoor environments collected by robotic platforms, improving adaptability and performance.", "method": "Integrated Point Prompt Tuning (PPT) with Point Transformer v3 (PTv3) for adaptive processing, using platform-specific conditioning and cross-dataset class alignment. No external data was used.", "result": "Achieved a 22.59% mIoU improvement over baseline PTv3, especially on challenging platforms.", "conclusion": "The approach demonstrates effective adaptive point cloud understanding for field robotics applications."}}
{"id": "2506.07001", "pdf": "https://arxiv.org/pdf/2506.07001", "abs": "https://arxiv.org/abs/2506.07001", "authors": ["Yize Cheng", "Vinu Sankar Sadasivan", "Mehrdad Saberi", "Shoumik Saha", "Soheil Feizi"], "title": "Adversarial Paraphrasing: A Universal Attack for Humanizing AI-Generated Text", "categories": ["cs.CL"], "comment": null, "summary": "The increasing capabilities of Large Language Models (LLMs) have raised\nconcerns about their misuse in AI-generated plagiarism and social engineering.\nWhile various AI-generated text detectors have been proposed to mitigate these\nrisks, many remain vulnerable to simple evasion techniques such as\nparaphrasing. However, recent detectors have shown greater robustness against\nsuch basic attacks. In this work, we introduce Adversarial Paraphrasing, a\ntraining-free attack framework that universally humanizes any AI-generated text\nto evade detection more effectively. Our approach leverages an off-the-shelf\ninstruction-following LLM to paraphrase AI-generated content under the guidance\nof an AI text detector, producing adversarial examples that are specifically\noptimized to bypass detection. Extensive experiments show that our attack is\nboth broadly effective and highly transferable across several detection\nsystems. For instance, compared to simple paraphrasing attack--which,\nironically, increases the true positive at 1% false positive (T@1%F) by 8.57%\non RADAR and 15.03% on Fast-DetectGPT--adversarial paraphrasing, guided by\nOpenAI-RoBERTa-Large, reduces T@1%F by 64.49% on RADAR and a striking 98.96% on\nFast-DetectGPT. Across a diverse set of detectors--including neural\nnetwork-based, watermark-based, and zero-shot approaches--our attack achieves\nan average T@1%F reduction of 87.88% under the guidance of\nOpenAI-RoBERTa-Large. We also analyze the tradeoff between text quality and\nattack success to find that our method can significantly reduce detection\nrates, with mostly a slight degradation in text quality. Our adversarial setup\nhighlights the need for more robust and resilient detection strategies in the\nlight of increasingly sophisticated evasion techniques.", "AI": {"tldr": "Adversarial Paraphrasing is a training-free attack framework that humanizes AI-generated text to evade detection, outperforming simple paraphrasing and reducing detection rates significantly across various systems.", "motivation": "Addressing the vulnerability of AI-generated text detectors to evasion techniques like paraphrasing, this work introduces a more sophisticated attack method to highlight the need for robust detection strategies.", "method": "The approach uses an instruction-following LLM to paraphrase AI-generated text under the guidance of an AI text detector, creating adversarial examples optimized to bypass detection.", "result": "The attack reduces detection rates by 64.49% on RADAR and 98.96% on Fast-DetectGPT, with an average reduction of 87.88% across diverse detectors, while maintaining text quality.", "conclusion": "The study underscores the necessity for more resilient detection methods as evasion techniques grow more advanced."}}
{"id": "2506.07002", "pdf": "https://arxiv.org/pdf/2506.07002", "abs": "https://arxiv.org/abs/2506.07002", "authors": ["Yunxiao Shi", "Hong Cai", "Jisoo Jeong", "Yinhao Zhu", "Shizhong Han", "Amin Ansari", "Fatih Porikli"], "title": "BePo: Leveraging Birds Eye View and Sparse Points for Efficient and Accurate 3D Occupancy Prediction", "categories": ["cs.CV"], "comment": "Two-page abstract version available at CVPR 2025 Embodied AI Workshop", "summary": "3D occupancy provides fine-grained 3D geometry and semantics for scene\nunderstanding which is critical for autonomous driving. Most existing methods,\nhowever, carry high compute costs, requiring dense 3D feature volume and\ncross-attention to effectively aggregate information. More recent works have\nadopted Bird's Eye View (BEV) or sparse points as scene representation with\nmuch reduced cost, but still suffer from their respective shortcomings. More\nconcretely, BEV struggles with small objects that often experience significant\ninformation loss after being projected to the ground plane. On the other hand,\npoints can flexibly model little objects in 3D, but is inefficient at capturing\nflat surfaces or large objects. To address these challenges, in this paper, we\npresent a novel 3D occupancy prediction approach, BePo, which combines BEV and\nsparse points based representations. We propose a dual-branch design: a\nquery-based sparse points branch and a BEV branch. The 3D information learned\nin the sparse points branch is shared with the BEV stream via cross-attention,\nwhich enriches the weakened signals of difficult objects on the BEV plane. The\noutputs of both branches are finally fused to generate predicted 3D occupancy.\nWe conduct extensive experiments on the Occ3D-nuScenes and Occ3D-Waymo\nbenchmarks that demonstrate the superiority of our proposed BePo. Moreover,\nBePo also delivers competitive inference speed when compared to the latest\nefficient approaches.", "AI": {"tldr": "BePo combines BEV and sparse points representations for efficient 3D occupancy prediction, addressing limitations of each method.", "motivation": "Existing methods (BEV or sparse points) have trade-offs in accuracy and efficiency for 3D scene understanding in autonomous driving.", "method": "Dual-branch design: a sparse points branch and a BEV branch, with cross-attention for information sharing and fusion.", "result": "BePo outperforms on Occ3D benchmarks and maintains competitive inference speed.", "conclusion": "BePo effectively balances accuracy and efficiency for 3D occupancy prediction."}}
{"id": "2506.07032", "pdf": "https://arxiv.org/pdf/2506.07032", "abs": "https://arxiv.org/abs/2506.07032", "authors": ["Bhuiyan Sanjid Shafique", "Ashmal Vayani", "Muhammad Maaz", "Hanoona Abdul Rasheed", "Dinura Dissanayake", "Mohammed Irfan Kurpath", "Yahya Hmaiti", "Go Inoue", "Jean Lahoud", "Md. Safirur Rashid", "Shadid Intisar Quasem", "Maheen Fatima", "Franco Vidal", "Mykola Maslych", "Ketan Pravin More", "Sanoojan Baliah", "Hasindri Watawana", "Yuhao Li", "Fabian Farestam", "Leon Schaller", "Roman Tymtsiv", "Simon Weber", "Hisham Cholakkal", "Ivan Laptev", "Shin'ichi Satoh", "Michael Felsberg", "Mubarak Shah", "Salman Khan", "Fahad Shahbaz Khan"], "title": "A Culturally-diverse Multilingual Multimodal Video Benchmark & Model", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Large multimodal models (LMMs) have recently gained attention due to their\neffectiveness to understand and generate descriptions of visual content. Most\nexisting LMMs are in English language. While few recent works explore\nmultilingual image LMMs, to the best of our knowledge, moving beyond the\nEnglish language for cultural and linguistic inclusivity is yet to be\ninvestigated in the context of video LMMs. In pursuit of more inclusive video\nLMMs, we introduce a multilingual Video LMM benchmark, named ViMUL-Bench, to\nevaluate Video LMMs across 14 languages, including both low- and high-resource\nlanguages: English, Chinese, Spanish, French, German, Hindi, Arabic, Russian,\nBengali, Urdu, Sinhala, Tamil, Swedish, and Japanese. Our ViMUL-Bench is\ndesigned to rigorously test video LMMs across 15 categories including eight\nculturally diverse categories, ranging from lifestyles and festivals to foods\nand rituals and from local landmarks to prominent cultural personalities.\nViMUL-Bench comprises both open-ended (short and long-form) and multiple-choice\nquestions spanning various video durations (short, medium, and long) with 8k\nsamples that are manually verified by native language speakers. In addition, we\nalso introduce a machine translated multilingual video training set comprising\n1.2 million samples and develop a simple multilingual video LMM, named ViMUL,\nthat is shown to provide a better tradeoff between high-and low-resource\nlanguages for video understanding. We hope our ViMUL-Bench and multilingual\nvideo LMM along with a large-scale multilingual video training set will help\nease future research in developing cultural and linguistic inclusive\nmultilingual video LMMs. Our proposed benchmark, video LMM and training data\nwill be publicly released at https://mbzuai-oryx.github.io/ViMUL/.", "AI": {"tldr": "The paper introduces ViMUL-Bench, a multilingual benchmark for video LMMs, and ViMUL, a multilingual video LMM, to promote inclusivity across 14 languages and diverse cultural contexts.", "motivation": "Existing LMMs are predominantly English-focused, lacking cultural and linguistic inclusivity, especially for video content. This work aims to address this gap.", "method": "Developed ViMUL-Bench with 8k manually verified samples across 14 languages and 15 culturally diverse categories. Also created a 1.2M-sample multilingual training set and a simple multilingual video LMM (ViMUL).", "result": "ViMUL provides a balanced tradeoff between high- and low-resource languages for video understanding.", "conclusion": "The work facilitates future research in inclusive multilingual video LMMs by releasing ViMUL-Bench, ViMUL, and training data publicly."}}
{"id": "2506.07013", "pdf": "https://arxiv.org/pdf/2506.07013", "abs": "https://arxiv.org/abs/2506.07013", "authors": ["Wentao Zhao", "Yihe Niu", "Yanbo Wang", "Tianchen Deng", "Shenghai Yuan", "Zhenli Wang", "Rui Guo", "Jingchuan Wang"], "title": "UNO: Unified Self-Supervised Monocular Odometry for Platform-Agnostic Deployment", "categories": ["cs.CV"], "comment": "15pages, 8 figures", "summary": "This work presents UNO, a unified monocular visual odometry framework that\nenables robust and adaptable pose estimation across diverse environments,\nplatforms, and motion patterns. Unlike traditional methods that rely on\ndeployment-specific tuning or predefined motion priors, our approach\ngeneralizes effectively across a wide range of real-world scenarios, including\nautonomous vehicles, aerial drones, mobile robots, and handheld devices. To\nthis end, we introduce a Mixture-of-Experts strategy for local state\nestimation, with several specialized decoders that each handle a distinct class\nof ego-motion patterns. Moreover, we introduce a fully differentiable\nGumbel-Softmax module that constructs a robust inter-frame correlation graph,\nselects the optimal expert decoder, and prunes erroneous estimates. These cues\nare then fed into a unified back-end that combines pre-trained,\nscale-independent depth priors with a lightweight bundling adjustment to\nenforce geometric consistency. We extensively evaluate our method on three\nmajor benchmark datasets: KITTI (outdoor/autonomous driving), EuRoC-MAV\n(indoor/aerial drones), and TUM-RGBD (indoor/handheld), demonstrating\nstate-of-the-art performance.", "AI": {"tldr": "UNO is a unified monocular visual odometry framework for robust pose estimation across diverse environments and motion patterns, using a Mixture-of-Experts strategy and differentiable Gumbel-Softmax module.", "motivation": "Traditional methods require deployment-specific tuning or predefined motion priors, limiting adaptability. UNO aims to generalize across varied scenarios like autonomous vehicles, drones, and handheld devices.", "method": "UNO employs a Mixture-of-Experts strategy with specialized decoders for different motion patterns, a Gumbel-Softmax module for expert selection, and a unified back-end combining depth priors with lightweight bundling adjustment.", "result": "UNO achieves state-of-the-art performance on KITTI, EuRoC-MAV, and TUM-RGBD benchmarks.", "conclusion": "UNO offers a robust, adaptable solution for monocular visual odometry, outperforming traditional methods in diverse real-world scenarios."}}
{"id": "2506.07037", "pdf": "https://arxiv.org/pdf/2506.07037", "abs": "https://arxiv.org/abs/2506.07037", "authors": ["Zhongze Luo", "Weixuan Wan", "Qizhi Zheng", "Yanhong Bai", "Jingyun Sun", "Jian Wang", "Dan Wang"], "title": "KG2QA: Knowledge Graph-enhanced Retrieval-Augmented Generation for Communication Standards Question Answering", "categories": ["cs.CL"], "comment": "23 pages", "summary": "There are many types of standards in the field of communication. The\ntraditional consulting model has a long cycle and relies on the knowledge and\nexperience of experts, making it difficult to meet the rapidly developing\ntechnological demands. This paper combines the fine-tuning of large language\nmodels with the construction of knowledge graphs to implement an intelligent\nconsultation and question-answering system for communication standards. The\nexperimental results show that after LoRA tuning on the constructed dataset of\n6,587 questions and answers in the field of communication standards,\nQwen2.5-7B-Instruct demonstrates outstanding professional capabilities in the\nfield of communication standards on the test set. BLEU-4 rose from 18.8564 to\n66.8993, and evaluation indicators such as ROUGE also increased significantly,\noutperforming the fine-tuning effect of the comparison model\nLlama-3-8B-Instruct. Based on the ontology framework containing 6 entity\nattributes and 10 relation attributes, a knowledge graph of the communication\nstandard domain containing 13,906 entities and 13,524 relations was\nconstructed, showing a relatively good query accuracy rate. The intelligent\nconsultation and question-answering system enables the fine-tuned model on the\nserver side to access the locally constructed knowledge graph and conduct\ngraphical retrieval of key information first, which is conducive to improving\nthe question-answering effect. The evaluation using DeepSeek as the Judge on\nthe test set shows that our RAG framework enables the fine-tuned model to\nimprove the scores at all five angles, with an average score increase of 2.26%.\nAnd combined with web services and API interfaces, it has achieved very good\nresults in terms of interaction experience and back-end access, and has very\ngood practical application value.", "AI": {"tldr": "The paper proposes an intelligent consultation system for communication standards by combining fine-tuned large language models (Qwen2.5-7B-Instruct) with a knowledge graph, achieving significant performance improvements and practical application value.", "motivation": "Traditional consulting models are slow and expert-dependent, failing to meet rapid technological demands. The paper aims to automate and enhance the process using AI.", "method": "Fine-tuned Qwen2.5-7B-Instruct on 6,587 QA pairs and built a knowledge graph (13,906 entities, 13,524 relations) for retrieval-augmented generation (RAG).", "result": "BLEU-4 improved from 18.8564 to 66.8993; ROUGE scores also rose. The RAG framework boosted scores by 2.26% on average.", "conclusion": "The system demonstrates strong performance and practical value, outperforming comparison models like Llama-3-8B-Instruct."}}
{"id": "2506.07015", "pdf": "https://arxiv.org/pdf/2506.07015", "abs": "https://arxiv.org/abs/2506.07015", "authors": ["Qiyu Hou", "Jun Wang"], "title": "TABLET: Table Structure Recognition using Encoder-only Transformers", "categories": ["cs.CV", "cs.LG"], "comment": "ICDAR 2025", "summary": "To address the challenges of table structure recognition, we propose a novel\nSplit-Merge-based top-down model optimized for large, densely populated tables.\nOur approach formulates row and column splitting as sequence labeling tasks,\nutilizing dual Transformer encoders to capture feature interactions. The\nmerging process is framed as a grid cell classification task, leveraging an\nadditional Transformer encoder to ensure accurate and coherent merging. By\neliminating unstable bounding box predictions, our method reduces resolution\nloss and computational complexity, achieving high accuracy while maintaining\nfast processing speed. Extensive experiments on FinTabNet and PubTabNet\ndemonstrate the superiority of our model over existing approaches, particularly\nin real-world applications. Our method offers a robust, scalable, and efficient\nsolution for large-scale table recognition, making it well-suited for\nindustrial deployment.", "AI": {"tldr": "A novel Split-Merge-based model for table structure recognition, using Transformer encoders for row/column splitting and merging, achieves high accuracy and efficiency.", "motivation": "Addressing challenges in recognizing large, densely populated tables by improving accuracy and reducing computational complexity.", "method": "Uses dual Transformer encoders for row/column splitting (sequence labeling) and an additional encoder for merging (grid cell classification), eliminating unstable bounding box predictions.", "result": "Superior performance on FinTabNet and PubTabNet, with high accuracy and fast processing, suitable for industrial deployment.", "conclusion": "The method provides a robust, scalable, and efficient solution for large-scale table recognition."}}
{"id": "2506.07042", "pdf": "https://arxiv.org/pdf/2506.07042", "abs": "https://arxiv.org/abs/2506.07042", "authors": ["Stergios Chatzikyriakidis"], "title": "Reasoning with RAGged events: RAG-Enhanced Event Knowledge Base Construction and reasoning with proof-assistants", "categories": ["cs.CL"], "comment": null, "summary": "Extracting structured computational representations of historical events from\nnarrative text remains computationally expensive when constructed manually.\nWhile RDF/OWL reasoners enable graph-based reasoning, they are limited to\nfragments of first-order logic, preventing deeper temporal and semantic\nanalysis. This paper addresses both challenges by developing automatic\nhistorical event extraction models using multiple LLMs (GPT-4, Claude, Llama\n3.2) with three enhancement strategies: pure base generation, knowledge graph\nenhancement, and Retrieval-Augmented Generation (RAG). We conducted\ncomprehensive evaluations using historical texts from Thucydides. Our findings\nreveal that enhancement strategies optimize different performance dimensions\nrather than providing universal improvements. For coverage and historical\nbreadth, base generation achieves optimal performance with Claude and GPT-4\nextracting comprehensive events. However, for precision, RAG enhancement\nimproves coordinate accuracy and metadata completeness. Model architecture\nfundamentally determines enhancement sensitivity: larger models demonstrate\nrobust baseline performance with incremental RAG improvements, while Llama 3.2\nshows extreme variance from competitive performance to complete failure. We\nthen developed an automated translation pipeline converting extracted RDF\nrepresentations into Coq proof assistant specifications, enabling higher-order\nreasoning beyond RDF capabilities including multi-step causal verification,\ntemporal arithmetic with BC dates, and formal proofs about historical\ncausation. The Coq formalization validates that RAG-discovered event types\nrepresent legitimate domain-specific semantic structures rather than\nontological violations.", "AI": {"tldr": "The paper introduces automatic historical event extraction using LLMs (GPT-4, Claude, Llama 3.2) with three enhancement strategies, evaluates their performance, and translates RDF representations into Coq for advanced reasoning.", "motivation": "Manual extraction of historical event representations is costly, and RDF/OWL reasoners are limited in temporal and semantic analysis. The paper aims to automate this process and enable deeper reasoning.", "method": "Uses multiple LLMs with three strategies: base generation, knowledge graph enhancement, and RAG. Evaluates performance on Thucydides' texts and translates RDF into Coq for higher-order reasoning.", "result": "Enhancement strategies optimize different performance dimensions. Base generation excels in coverage, while RAG improves precision. Larger models show robust performance, while Llama 3.2 varies widely. Coq formalization validates RAG-discovered event types.", "conclusion": "The approach automates historical event extraction and enables advanced reasoning, with enhancement strategies tailored to specific needs. Coq integration validates semantic structures beyond RDF capabilities."}}
{"id": "2506.07016", "pdf": "https://arxiv.org/pdf/2506.07016", "abs": "https://arxiv.org/abs/2506.07016", "authors": ["Sanjoy Chowdhury", "Mohamed Elmoghany", "Yohan Abeysinghe", "Junjie Fei", "Sayan Nag", "Salman Khan", "Mohamed Elhoseiny", "Dinesh Manocha"], "title": "MAGNET: A Multi-agent Framework for Finding Audio-Visual Needles by Reasoning over Multi-Video Haystacks", "categories": ["cs.CV", "cs.AI"], "comment": "Audio-visual learning, Audio-Visual RAG, Multi-Video Linkage", "summary": "Large multimodal models (LMMs) have shown remarkable progress in audio-visual\nunderstanding, yet they struggle with real-world scenarios that require complex\nreasoning across extensive video collections. Existing benchmarks for video\nquestion answering remain limited in scope, typically involving one clip per\nquery, which falls short of representing the challenges of large-scale,\naudio-visual retrieval and reasoning encountered in practical applications. To\nbridge this gap, we introduce a novel task named AV-HaystacksQA, where the goal\nis to identify salient segments across different videos in response to a query\nand link them together to generate the most informative answer. To this end, we\npresent AVHaystacks, an audio-visual benchmark comprising 3100 annotated QA\npairs designed to assess the capabilities of LMMs in multi-video retrieval and\ntemporal grounding task. Additionally, we propose a model-agnostic, multi-agent\nframework MAGNET to address this challenge, achieving up to 89% and 65%\nrelative improvements over baseline methods on BLEU@4 and GPT evaluation scores\nin QA task on our proposed AVHaystacks. To enable robust evaluation of\nmulti-video retrieval and temporal grounding for optimal response generation,\nwe introduce two new metrics, STEM, which captures alignment errors between a\nground truth and a predicted step sequence and MTGS, to facilitate balanced and\ninterpretable evaluation of segment-level grounding performance. Project:\nhttps://schowdhury671.github.io/magnet_project/", "AI": {"tldr": "The paper introduces AV-HaystacksQA, a novel task for multi-video retrieval and reasoning, along with the AVHaystacks benchmark and MAGNET framework, achieving significant improvements over baselines.", "motivation": "Existing benchmarks for video question answering are limited, failing to address large-scale audio-visual retrieval and reasoning challenges in real-world scenarios.", "method": "The authors propose AV-HaystacksQA, a task involving salient segment identification across videos, and introduce AVHaystacks, a benchmark with 3100 QA pairs. They also present MAGNET, a multi-agent framework for this task.", "result": "MAGNET achieves 89% and 65% relative improvements on BLEU@4 and GPT scores, respectively, and introduces new metrics (STEM and MTGS) for evaluation.", "conclusion": "The work bridges the gap in large-scale audio-visual reasoning, offering a robust benchmark and framework for future research."}}
{"id": "2506.07044", "pdf": "https://arxiv.org/pdf/2506.07044", "abs": "https://arxiv.org/abs/2506.07044", "authors": ["LASA Team", "Weiwen Xu", "Hou Pong Chan", "Long Li", "Mahani Aljunied", "Ruifeng Yuan", "Jianyu Wang", "Chenghao Xiao", "Guizhen Chen", "Chaoqun Liu", "Zhaodonghui Li", "Yu Sun", "Junao Shen", "Chaojun Wang", "Jie Tan", "Deli Zhao", "Tingyang Xu", "Hao Zhang", "Yu Rong"], "title": "Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Technical Report, 53 pages, 25 tables, and 16 figures", "summary": "Multimodal Large Language Models (MLLMs) have demonstrated impressive\ncapabilities in understanding common visual elements, largely due to their\nlarge-scale datasets and advanced training strategies. However, their\neffectiveness in medical applications remains limited due to the inherent\ndiscrepancies between data and tasks in medical scenarios and those in the\ngeneral domain. Concretely, existing medical MLLMs face the following critical\nlimitations: (1) limited coverage of medical knowledge beyond imaging, (2)\nheightened susceptibility to hallucinations due to suboptimal data curation\nprocesses, (3) lack of reasoning capabilities tailored for complex medical\nscenarios. To address these challenges, we first propose a comprehensive data\ncuration procedure that (1) efficiently acquires rich medical knowledge data\nnot only from medical imaging but also from extensive medical texts and\ngeneral-domain data; and (2) synthesizes accurate medical captions, visual\nquestion answering (VQA), and reasoning samples. As a result, we build a\nmultimodal dataset enriched with extensive medical knowledge. Building on the\ncurated data, we introduce our medical-specialized MLLM: Lingshu. Lingshu\nundergoes multi-stage training to embed medical expertise and enhance its\ntask-solving capabilities progressively. Besides, we preliminarily explore the\npotential of applying reinforcement learning with verifiable rewards paradigm\nto enhance Lingshu's medical reasoning ability. Additionally, we develop\nMedEvalKit, a unified evaluation framework that consolidates leading multimodal\nand textual medical benchmarks for standardized, fair, and efficient model\nassessment. We evaluate the performance of Lingshu on three fundamental medical\ntasks, multimodal QA, text-based QA, and medical report generation. The results\nshow that Lingshu consistently outperforms the existing open-source multimodal\nmodels on most tasks ...", "AI": {"tldr": "The paper introduces Lingshu, a medical-specialized MLLM, addressing limitations in existing models by proposing a comprehensive data curation procedure and multi-stage training. It also develops MedEvalKit for standardized evaluation.", "motivation": "Existing MLLMs lack effectiveness in medical applications due to discrepancies in data and tasks, limited medical knowledge coverage, susceptibility to hallucinations, and inadequate reasoning capabilities.", "method": "Proposes a data curation procedure for rich medical knowledge, multi-stage training for Lingshu, and explores reinforcement learning for reasoning. Introduces MedEvalKit for evaluation.", "result": "Lingshu outperforms existing open-source multimodal models on tasks like multimodal QA, text-based QA, and medical report generation.", "conclusion": "The approach enhances medical MLLMs by addressing key limitations, demonstrating superior performance in medical tasks."}}
{"id": "2506.07045", "pdf": "https://arxiv.org/pdf/2506.07045", "abs": "https://arxiv.org/abs/2506.07045", "authors": ["Yikun Ji", "Hong Yan", "Jun Lan", "Huijia Zhu", "Weiqiang Wang", "Qi Fan", "Liqing Zhang", "Jianfu Zhang"], "title": "Interpretable and Reliable Detection of AI-Generated Images via Grounded Reasoning in MLLMs", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "The rapid advancement of image generation technologies intensifies the demand\nfor interpretable and robust detection methods. Although existing approaches\noften attain high accuracy, they typically operate as black boxes without\nproviding human-understandable justifications. Multi-modal Large Language\nModels (MLLMs), while not originally intended for forgery detection, exhibit\nstrong analytical and reasoning capabilities. When properly fine-tuned, they\ncan effectively identify AI-generated images and offer meaningful explanations.\nHowever, existing MLLMs still struggle with hallucination and often fail to\nalign their visual interpretations with actual image content and human\nreasoning. To bridge this gap, we construct a dataset of AI-generated images\nannotated with bounding boxes and descriptive captions that highlight synthesis\nartifacts, establishing a foundation for human-aligned visual-textual grounded\nreasoning. We then finetune MLLMs through a multi-stage optimization strategy\nthat progressively balances the objectives of accurate detection, visual\nlocalization, and coherent textual explanation. The resulting model achieves\nsuperior performance in both detecting AI-generated images and localizing\nvisual flaws, significantly outperforming baseline methods.", "AI": {"tldr": "The paper introduces a method to improve AI-generated image detection using fine-tuned Multi-modal Large Language Models (MLLMs) with human-aligned reasoning and visual-textual grounding.", "motivation": "The demand for interpretable and robust detection methods for AI-generated images is growing, but existing approaches lack human-understandable justifications and suffer from hallucination.", "method": "The authors construct a dataset of AI-generated images with annotations for synthesis artifacts and fine-tune MLLMs using a multi-stage optimization strategy to balance detection, localization, and explanation.", "result": "The fine-tuned model outperforms baselines in detecting AI-generated images and localizing visual flaws.", "conclusion": "The approach successfully bridges the gap between detection accuracy and human-aligned reasoning, enhancing interpretability and robustness."}}
{"id": "2506.07064", "pdf": "https://arxiv.org/pdf/2506.07064", "abs": "https://arxiv.org/abs/2506.07064", "authors": ["Kai Xiong", "Xiao Ding", "Yixin Cao", "Yuxiong Yan", "Li Du", "Yufei Zhang", "Jinglong Gao", "Jiaqian Liu", "Bing Qin", "Ting Liu"], "title": "Com$^2$: A Causal-Guided Benchmark for Exploring Complex Commonsense Reasoning in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL 2025 Main Conference", "summary": "Large language models (LLMs) have mastered abundant simple and explicit\ncommonsense knowledge through pre-training, enabling them to achieve human-like\nperformance in simple commonsense reasoning. Nevertheless, LLMs struggle to\nreason with complex and implicit commonsense knowledge that is derived from\nsimple ones (such as understanding the long-term effects of certain events), an\naspect humans tend to focus on more. Existing works focus on complex tasks like\nmath and code, while complex commonsense reasoning remains underexplored due to\nits uncertainty and lack of structure. To fill this gap and align with\nreal-world concerns, we propose a benchmark Com$^2$ focusing on complex\ncommonsense reasoning. We first incorporate causal event graphs to serve as\nstructured complex commonsense. Then we adopt causal theory~(e.g.,\nintervention) to modify the causal event graphs and obtain different scenarios\nthat meet human concerns. Finally, an LLM is employed to synthesize examples\nwith slow thinking, which is guided by the logical relationships in the\nmodified causal graphs. Furthermore, we use detective stories to construct a\nmore challenging subset. Experiments show that LLMs struggle in reasoning depth\nand breadth, while post-training and slow thinking can alleviate this. The code\nand data are available at https://github.com/Waste-Wood/Com2.", "AI": {"tldr": "The paper introduces Com$^2$, a benchmark for complex commonsense reasoning, addressing LLMs' limitations in handling implicit knowledge.", "motivation": "LLMs excel in simple commonsense reasoning but struggle with complex, implicit knowledge, which is crucial for real-world applications.", "method": "The authors use causal event graphs and causal theory to structure complex commonsense, synthesize examples via LLMs, and create a subset with detective stories.", "result": "Experiments reveal LLMs' shortcomings in reasoning depth and breadth, though post-training and slow thinking help.", "conclusion": "Com$^2$ fills a gap in evaluating complex commonsense reasoning, highlighting LLMs' limitations and potential improvements."}}
{"id": "2506.07050", "pdf": "https://arxiv.org/pdf/2506.07050", "abs": "https://arxiv.org/abs/2506.07050", "authors": ["Zheng Wang", "Kai Ying", "Bin Xu", "Chunjiao Wang", "Cong Bai"], "title": "From Swath to Full-Disc: Advancing Precipitation Retrieval with Multimodal Knowledge Expansion", "categories": ["cs.CV", "cs.IR", "cs.MM"], "comment": null, "summary": "Accurate near-real-time precipitation retrieval has been enhanced by\nsatellite-based technologies. However, infrared-based algorithms have low\naccuracy due to weak relations with surface precipitation, whereas passive\nmicrowave and radar-based methods are more accurate but limited in range. This\nchallenge motivates the Precipitation Retrieval Expansion (PRE) task, which\naims to enable accurate, infrared-based full-disc precipitation retrievals\nbeyond the scanning swath. We introduce Multimodal Knowledge Expansion, a\ntwo-stage pipeline with the proposed PRE-Net model. In the Swath-Distilling\nstage, PRE-Net transfers knowledge from a multimodal data integration model to\nan infrared-based model within the scanning swath via Coordinated Masking and\nWavelet Enhancement (CoMWE). In the Full-Disc Adaptation stage, Self-MaskTune\nrefines predictions across the full disc by balancing multimodal and full-disc\ninfrared knowledge. Experiments on the introduced PRE benchmark demonstrate\nthat PRE-Net significantly advanced precipitation retrieval performance,\noutperforming leading products like PERSIANN-CCS, PDIR, and IMERG. The code\nwill be available at https://github.com/Zjut-MultimediaPlus/PRE-Net.", "AI": {"tldr": "The paper introduces PRE-Net, a two-stage pipeline for accurate infrared-based full-disc precipitation retrieval, outperforming existing methods like PERSIANN-CCS, PDIR, and IMERG.", "motivation": "Improving infrared-based precipitation retrieval accuracy, which is currently limited by weak relations with surface precipitation, while expanding coverage beyond scanning swaths.", "method": "A two-stage pipeline: (1) Swath-Distilling with PRE-Net using Coordinated Masking and Wavelet Enhancement (CoMWE) to transfer knowledge, and (2) Full-Disc Adaptation with Self-MaskTune to refine predictions.", "result": "PRE-Net significantly advances precipitation retrieval performance, surpassing leading products in accuracy.", "conclusion": "The proposed method enables accurate, infrared-based full-disc precipitation retrieval, addressing limitations of current technologies."}}
{"id": "2506.07086", "pdf": "https://arxiv.org/pdf/2506.07086", "abs": "https://arxiv.org/abs/2506.07086", "authors": ["Yuanhe Tian", "Pengsen Cheng", "Guoqing Jin", "Lei Zhang", "Yan Song"], "title": "Representation Decomposition for Learning Similarity and Contrastness Across Modalities for Affective Computing", "categories": ["cs.CL"], "comment": "13 pages, 4 figures", "summary": "Multi-modal affective computing aims to automatically recognize and interpret\nhuman attitudes from diverse data sources such as images and text, thereby\nenhancing human-computer interaction and emotion understanding. Existing\napproaches typically rely on unimodal analysis or straightforward fusion of\ncross-modal information that fail to capture complex and conflicting evidence\npresented across different modalities. In this paper, we propose a novel\nLLM-based approach for affective computing that explicitly deconstructs visual\nand textual representations into shared (modality-invariant) and\nmodality-specific components. Specifically, our approach firstly encodes and\naligns input modalities using pre-trained multi-modal encoders, then employs a\nrepresentation decomposition framework to separate common emotional content\nfrom unique cues, and finally integrates these decomposed signals via an\nattention mechanism to form a dynamic soft prompt for a multi-modal LLM.\nExtensive experiments on three representative tasks for affective computing,\nnamely, multi-modal aspect-based sentiment analysis, multi-modal emotion\nanalysis, and hateful meme detection, demonstrate the effectiveness of our\napproach, which consistently outperforms strong baselines and state-of-the-art\nmodels.", "AI": {"tldr": "A novel LLM-based approach for multi-modal affective computing decomposes visual and textual representations into shared and modality-specific components, outperforming existing methods.", "motivation": "Existing approaches fail to capture complex cross-modal interactions in affective computing, limiting their effectiveness.", "method": "The method encodes and aligns modalities, decomposes representations into shared and unique components, and integrates them via attention for a multi-modal LLM.", "result": "The approach outperforms baselines and state-of-the-art models in tasks like sentiment analysis, emotion analysis, and hateful meme detection.", "conclusion": "The proposed framework effectively addresses multi-modal affective computing challenges by leveraging decomposed representations and LLMs."}}
{"id": "2506.07055", "pdf": "https://arxiv.org/pdf/2506.07055", "abs": "https://arxiv.org/abs/2506.07055", "authors": ["Tarique Dahri", "Zulfiqar Ali Memon", "Zhenyu Yu", "Mohd. Yamani Idna Idris", "Sheheryar Khan", "Sadiq Ahmad", "Maged Shoman", "Saddam Aziz", "Rizwan Qureshi"], "title": "A Layered Self-Supervised Knowledge Distillation Framework for Efficient Multimodal Learning on the Edge", "categories": ["cs.CV"], "comment": null, "summary": "We introduce Layered Self-Supervised Knowledge Distillation (LSSKD) framework\nfor training compact deep learning models. Unlike traditional methods that rely\non pre-trained teacher networks, our approach appends auxiliary classifiers to\nintermediate feature maps, generating diverse self-supervised knowledge and\nenabling one-to-one transfer across different network stages. Our method\nachieves an average improvement of 4.54\\% over the state-of-the-art PS-KD\nmethod and a 1.14% gain over SSKD on CIFAR-100, with a 0.32% improvement on\nImageNet compared to HASSKD. Experiments on Tiny ImageNet and CIFAR-100 under\nfew-shot learning scenarios also achieve state-of-the-art results. These\nfindings demonstrate the effectiveness of our approach in enhancing model\ngeneralization and performance without the need for large over-parameterized\nteacher networks. Importantly, at the inference stage, all auxiliary\nclassifiers can be removed, yielding no extra computational cost. This makes\nour model suitable for deploying small language models on affordable\nlow-computing devices. Owing to its lightweight design and adaptability, our\nframework is particularly suitable for multimodal sensing and cyber-physical\nenvironments that require efficient and responsive inference. LSSKD facilitates\nthe development of intelligent agents capable of learning from limited sensory\ndata under weak supervision.", "AI": {"tldr": "LSSKD is a lightweight self-supervised knowledge distillation framework that improves model performance without large teacher networks, achieving state-of-the-art results on benchmarks like CIFAR-100 and ImageNet.", "motivation": "To train compact models without relying on large pre-trained teacher networks, enabling efficient deployment on low-computing devices.", "method": "Appends auxiliary classifiers to intermediate feature maps for diverse self-supervised knowledge and one-to-one transfer across network stages.", "result": "Achieves 4.54% improvement over PS-KD on CIFAR-100, 1.14% over SSKD, and 0.32% over HASSKD on ImageNet, with strong few-shot learning performance.", "conclusion": "LSSKD enhances model generalization and performance efficiently, suitable for low-resource environments and multimodal applications."}}
{"id": "2506.07104", "pdf": "https://arxiv.org/pdf/2506.07104", "abs": "https://arxiv.org/abs/2506.07104", "authors": ["Jiaxuan Gao", "Shu Yan", "Qixin Tan", "Lu Yang", "Shusheng Xu", "Wei Fu", "Zhiyu Mei", "Kaifeng Lyu", "Yi Wu"], "title": "How Far Are We from Optimal Reasoning Efficiency?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Reasoning Models (LRMs) demonstrate remarkable problem-solving\ncapabilities through extended Chain-of-Thought (CoT) reasoning but often\nproduce excessively verbose and redundant reasoning traces. This inefficiency\nincurs high inference costs and limits practical deployment. While existing\nfine-tuning methods aim to improve reasoning efficiency, assessing their\nefficiency gains remains challenging due to inconsistent evaluations. In this\nwork, we introduce the reasoning efficiency frontiers, empirical upper bounds\nderived from fine-tuning base LRMs across diverse approaches and training\nconfigurations. Based on these frontiers, we propose the Reasoning Efficiency\nGap (REG), a unified metric quantifying deviations of any fine-tuned LRMs from\nthese frontiers. Systematic evaluation on challenging mathematical benchmarks\nreveals significant gaps in current methods: they either sacrifice accuracy for\nshort length or still remain inefficient under tight token budgets. To reduce\nthe efficiency gap, we propose REO-RL, a class of Reinforcement Learning\nalgorithms that minimizes REG by targeting a sparse set of token budgets.\nLeveraging numerical integration over strategically selected budgets, REO-RL\napproximates the full efficiency objective with low error using a small set of\ntoken budgets. Through systematic benchmarking, we demonstrate that our\nefficiency metric, REG, effectively captures the accuracy-length trade-off,\nwith low-REG methods reducing length while maintaining accuracy. Our approach,\nREO-RL, consistently reduces REG by >=50 across all evaluated LRMs and matching\nQwen3-4B/8B efficiency frontiers under a 16K token budget with minimal accuracy\nloss. Ablation studies confirm the effectiveness of our exponential token\nbudget strategy. Finally, our findings highlight that fine-tuning LRMs to\nperfectly align with the efficiency frontiers remains an open challenge.", "AI": {"tldr": "The paper introduces the Reasoning Efficiency Gap (REG) metric to assess the efficiency of fine-tuned Large Reasoning Models (LRMs) and proposes REO-RL, a Reinforcement Learning method, to reduce REG by optimizing token budgets.", "motivation": "Current LRMs produce verbose reasoning traces, leading to high inference costs and impractical deployment. Existing fine-tuning methods lack consistent evaluation of efficiency gains.", "method": "The authors propose REG as a unified metric and REO-RL, a Reinforcement Learning algorithm, to minimize REG by targeting sparse token budgets. Numerical integration is used to approximate efficiency objectives.", "result": "REO-RL reduces REG by >=50% across LRMs and matches efficiency frontiers under tight token budgets with minimal accuracy loss.", "conclusion": "Fine-tuning LRMs to align perfectly with efficiency frontiers remains challenging, but REO-RL significantly improves reasoning efficiency."}}
{"id": "2506.07056", "pdf": "https://arxiv.org/pdf/2506.07056", "abs": "https://arxiv.org/abs/2506.07056", "authors": ["Zhenyu Liu", "Huizhi Liang", "Rajiv Ranjan", "Zhanxing Zhu", "Vaclav Snasel", "Varun Ojha"], "title": "D2R: dual regularization loss with collaborative adversarial generation for model robustness", "categories": ["cs.CV", "cs.CR", "cs.LG"], "comment": null, "summary": "The robustness of Deep Neural Network models is crucial for defending models\nagainst adversarial attacks. Recent defense methods have employed collaborative\nlearning frameworks to enhance model robustness. Two key limitations of\nexisting methods are (i) insufficient guidance of the target model via loss\nfunctions and (ii) non-collaborative adversarial generation. We, therefore,\npropose a dual regularization loss (D2R Loss) method and a collaborative\nadversarial generation (CAG) strategy for adversarial training. D2R loss\nincludes two optimization steps. The adversarial distribution and clean\ndistribution optimizations enhance the target model's robustness by leveraging\nthe strengths of different loss functions obtained via a suitable function\nspace exploration to focus more precisely on the target model's distribution.\nCAG generates adversarial samples using a gradient-based collaboration between\nguidance and target models. We conducted extensive experiments on three\nbenchmark databases, including CIFAR-10, CIFAR-100, Tiny ImageNet, and two\npopular target models, WideResNet34-10 and PreActResNet18. Our results show\nthat D2R loss with CAG produces highly robust models.", "AI": {"tldr": "The paper introduces D2R Loss and CAG to improve DNN robustness against adversarial attacks by addressing insufficient loss guidance and non-collaborative adversarial generation.", "motivation": "Enhancing model robustness against adversarial attacks by overcoming limitations in existing methods.", "method": "Proposes D2R Loss (dual regularization) and CAG (collaborative adversarial generation) for adversarial training.", "result": "Experiments on CIFAR-10, CIFAR-100, Tiny ImageNet, WideResNet34-10, and PreActResNet18 show improved robustness.", "conclusion": "D2R Loss with CAG effectively produces highly robust models."}}
{"id": "2506.07106", "pdf": "https://arxiv.org/pdf/2506.07106", "abs": "https://arxiv.org/abs/2506.07106", "authors": ["Samir Abdaljalil", "Hasan Kurban", "Khalid Qaraqe", "Erchin Serpedin"], "title": "Theorem-of-Thought: A Multi-Agent Framework for Abductive, Deductive, and Inductive Reasoning in Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have shown strong performance across natural\nlanguage reasoning tasks, yet their reasoning processes remain brittle and\ndifficult to interpret. Prompting techniques like Chain-of-Thought (CoT)\nenhance reliability by eliciting intermediate reasoning steps or aggregating\nmultiple outputs. However, they lack mechanisms for enforcing logical structure\nand assessing internal coherence. We introduce Theorem-of-Thought (ToTh), a\nnovel framework that models reasoning as collaboration among three parallel\nagents, each simulating a distinct mode of inference: abductive, deductive, and\ninductive. Each agent produces a reasoning trace, which is structured into a\nformal reasoning graph. To evaluate consistency, we apply Bayesian belief\npropagation guided by natural language inference (NLI), assigning confidence\nscores to each step. The most coherent graph is selected to derive the final\nanswer. Experiments on symbolic (WebOfLies) and numerical (MultiArith)\nreasoning benchmarks show that ToTh consistently outperforms CoT,\nSelf-Consistency, and CoT-Decoding across multiple LLMs, while producing\ninterpretable and logically grounded reasoning chains. Our findings suggest a\npromising direction for building more robust and cognitively inspired LLM\nreasoning. The implementation is available at\nhttps://github.com/KurbanIntelligenceLab/theorem-of-thought.", "AI": {"tldr": "ToTh is a novel framework that models reasoning as collaboration among three agents (abductive, deductive, inductive) to enhance LLM reasoning, outperforming CoT and others.", "motivation": "LLMs lack logical structure and coherence in reasoning. ToTh aims to address this by introducing structured, collaborative reasoning.", "method": "ToTh uses three parallel agents for distinct inference modes, structures reasoning into graphs, and evaluates coherence via Bayesian belief propagation and NLI.", "result": "ToTh outperforms CoT, Self-Consistency, and CoT-Decoding on symbolic and numerical reasoning benchmarks.", "conclusion": "ToTh offers a robust, interpretable, and cognitively inspired approach to LLM reasoning."}}
{"id": "2506.07080", "pdf": "https://arxiv.org/pdf/2506.07080", "abs": "https://arxiv.org/abs/2506.07080", "authors": ["Anatol Garioud", "S\u00e9bastien Giordano", "Nicolas David", "Nicolas Gonthier"], "title": "FLAIR-HUB: Large-scale Multimodal Dataset for Land Cover and Crop Mapping", "categories": ["cs.CV"], "comment": null, "summary": "The growing availability of high-quality Earth Observation (EO) data enables\naccurate global land cover and crop type monitoring. However, the volume and\nheterogeneity of these datasets pose major processing and annotation\nchallenges. To address this, the French National Institute of Geographical and\nForest Information (IGN) is actively exploring innovative strategies to exploit\ndiverse EO data, which require large annotated datasets. IGN introduces\nFLAIR-HUB, the largest multi-sensor land cover dataset with\nvery-high-resolution (20 cm) annotations, covering 2528 km2 of France. It\ncombines six aligned modalities: aerial imagery, Sentinel-1/2 time series, SPOT\nimagery, topographic data, and historical aerial images. Extensive benchmarks\nevaluate multimodal fusion and deep learning models (CNNs, transformers) for\nland cover or crop mapping and also explore multi-task learning. Results\nunderscore the complexity of multimodal fusion and fine-grained classification,\nwith best land cover performance (78.2% accuracy, 65.8% mIoU) achieved using\nnearly all modalities. FLAIR-HUB supports supervised and multimodal\npretraining, with data and code available at\nhttps://ignf.github.io/FLAIR/flairhub.", "AI": {"tldr": "FLAIR-HUB is a large multi-sensor land cover dataset with high-resolution annotations, addressing challenges in processing Earth Observation data. It combines six modalities and evaluates multimodal fusion for land cover and crop mapping.", "motivation": "To tackle the challenges of processing and annotating large, heterogeneous Earth Observation datasets for accurate land cover and crop monitoring.", "method": "IGN introduces FLAIR-HUB, a dataset combining six aligned modalities (aerial imagery, Sentinel-1/2 time series, SPOT imagery, topographic data, historical aerial images). Benchmarks evaluate multimodal fusion and deep learning models (CNNs, transformers) for land cover and crop mapping.", "result": "Best land cover performance achieved 78.2% accuracy and 65.8% mIoU using nearly all modalities, highlighting the complexity of multimodal fusion and fine-grained classification.", "conclusion": "FLAIR-HUB supports supervised and multimodal pretraining, providing a valuable resource for land cover and crop monitoring, with data and code publicly available."}}
{"id": "2506.07142", "pdf": "https://arxiv.org/pdf/2506.07142", "abs": "https://arxiv.org/abs/2506.07142", "authors": ["Lennart Meincke", "Ethan Mollick", "Lilach Mollick", "Dan Shapiro"], "title": "Prompting Science Report 2: The Decreasing Value of Chain of Thought in Prompting", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This is the second in a series of short reports that seek to help business,\neducation, and policy leaders understand the technical details of working with\nAI through rigorous testing. In this report, we investigate Chain-of-Thought\n(CoT) prompting, a technique that encourages a large language model (LLM) to\n\"think step by step\" (Wei et al., 2022). CoT is a widely adopted method for\nimproving reasoning tasks, however, our findings reveal a more nuanced picture\nof its effectiveness. We demonstrate two things:\n  - The effectiveness of Chain-of-Thought prompting can vary greatly depending\non the type of task and model. For non-reasoning models, CoT generally improves\naverage performance by a small amount, particularly if the model does not\ninherently engage in step-by-step processing by default. However, CoT can\nintroduce more variability in answers, sometimes triggering occasional errors\nin questions the model would otherwise get right. We also found that many\nrecent models perform some form of CoT reasoning even if not asked; for these\nmodels, a request to perform CoT had little impact. Performing CoT generally\nrequires far more tokens (increasing cost and time) than direct answers.\n  - For models designed with explicit reasoning capabilities, CoT prompting\noften results in only marginal, if any, gains in answer accuracy. However, it\nsignificantly increases the time and tokens needed to generate a response.", "AI": {"tldr": "The paper examines Chain-of-Thought (CoT) prompting's effectiveness, finding it varies by task and model type, with mixed results on performance, variability, and efficiency.", "motivation": "To provide business, education, and policy leaders with insights into AI techniques like CoT prompting through rigorous testing.", "method": "Investigates CoT prompting's impact on large language models (LLMs) by analyzing task performance, variability, and efficiency across different model types.", "result": "CoT improves non-reasoning models slightly but increases variability and errors. For reasoning models, gains are marginal, while costs (time and tokens) rise significantly.", "conclusion": "CoT prompting's benefits are context-dependent, with trade-offs in performance, consistency, and efficiency, suggesting careful consideration for its use."}}
{"id": "2506.07087", "pdf": "https://arxiv.org/pdf/2506.07087", "abs": "https://arxiv.org/abs/2506.07087", "authors": ["Weiqi Yan", "Lvhai Chen", "Huaijia Kou", "Shengchuan Zhang", "Yan Zhang", "Liujuan Cao"], "title": "UCOD-DPL: Unsupervised Camouflaged Object Detection via Dynamic Pseudo-label Learning", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025 (Hightlight)", "summary": "Unsupervised Camoflaged Object Detection (UCOD) has gained attention since it\ndoesn't need to rely on extensive pixel-level labels. Existing UCOD methods\ntypically generate pseudo-labels using fixed strategies and train 1 x1\nconvolutional layers as a simple decoder, leading to low performance compared\nto fully-supervised methods. We emphasize two drawbacks in these approaches:\n1). The model is prone to fitting incorrect knowledge due to the pseudo-label\ncontaining substantial noise. 2). The simple decoder fails to capture and learn\nthe semantic features of camouflaged objects, especially for small-sized\nobjects, due to the low-resolution pseudo-labels and severe confusion between\nforeground and background pixels. To this end, we propose a UCOD method with a\nteacher-student framework via Dynamic Pseudo-label Learning called UCOD-DPL,\nwhich contains an Adaptive Pseudo-label Module (APM), a Dual-Branch Adversarial\n(DBA) decoder, and a Look-Twice mechanism. The APM module adaptively combines\npseudo-labels generated by fixed strategies and the teacher model to prevent\nthe model from overfitting incorrect knowledge while preserving the ability for\nself-correction; the DBA decoder takes adversarial learning of different\nsegmentation objectives, guides the model to overcome the foreground-background\nconfusion of camouflaged objects, and the Look-Twice mechanism mimics the human\ntendency to zoom in on camouflaged objects and performs secondary refinement on\nsmall-sized objects. Extensive experiments show that our method demonstrates\noutstanding performance, even surpassing some existing fully supervised\nmethods. The code is available now.", "AI": {"tldr": "Proposes UCOD-DPL, a teacher-student framework with dynamic pseudo-label learning for unsupervised camouflaged object detection, addressing noise in pseudo-labels and decoder limitations.", "motivation": "Existing UCOD methods suffer from noisy pseudo-labels and simple decoders, leading to poor performance compared to supervised methods.", "method": "Introduces UCOD-DPL with Adaptive Pseudo-label Module, Dual-Branch Adversarial decoder, and Look-Twice mechanism to improve pseudo-label quality and feature learning.", "result": "Outperforms existing UCOD methods and even some fully-supervised approaches.", "conclusion": "UCOD-DPL effectively addresses key challenges in unsupervised camouflaged object detection, achieving superior performance."}}
{"id": "2506.07148", "pdf": "https://arxiv.org/pdf/2506.07148", "abs": "https://arxiv.org/abs/2506.07148", "authors": ["Yaping Chai", "Haoran Xie", "Joe S. Qin"], "title": "Semantic-preserved Augmentation with Confidence-weighted Fine-tuning for Aspect Category Sentiment Analysis", "categories": ["cs.CL"], "comment": "10 pages, 7 figures, 4 tables", "summary": "Large language model (LLM) is an effective approach to addressing data\nscarcity in low-resource scenarios. Recent existing research designs\nhand-crafted prompts to guide LLM for data augmentation. We introduce a data\naugmentation strategy for the aspect category sentiment analysis (ACSA) task\nthat preserves the original sentence semantics and has linguistic diversity,\nspecifically by providing a structured prompt template for an LLM to generate\npredefined content. In addition, we employ a post-processing technique to\nfurther ensure semantic consistency between the generated sentence and the\noriginal sentence. The augmented data increases the semantic coverage of the\ntraining distribution, enabling the model better to understand the relationship\nbetween aspect categories and sentiment polarities, enhancing its inference\ncapabilities. Furthermore, we propose a confidence-weighted fine-tuning\nstrategy to encourage the model to generate more confident and accurate\nsentiment polarity predictions. Compared with powerful and recent works, our\nmethod consistently achieves the best performance on four benchmark datasets\nover all baselines.", "AI": {"tldr": "The paper introduces a data augmentation strategy using LLMs for the ACSA task, ensuring semantic consistency and linguistic diversity, and proposes a confidence-weighted fine-tuning method, achieving top performance on benchmarks.", "motivation": "Address data scarcity in low-resource scenarios for ACSA tasks by leveraging LLMs for effective data augmentation.", "method": "Uses structured prompt templates with LLMs for data augmentation, post-processing for semantic consistency, and confidence-weighted fine-tuning for better predictions.", "result": "Achieves best performance on four benchmark datasets compared to recent baselines.", "conclusion": "The proposed method effectively enhances model understanding of aspect-sentiment relationships and improves inference capabilities."}}
{"id": "2506.07091", "pdf": "https://arxiv.org/pdf/2506.07091", "abs": "https://arxiv.org/abs/2506.07091", "authors": ["Yangkai Lin", "Jiabao Lei", "Kui Jia"], "title": "SceneLCM: End-to-End Layout-Guided Interactive Indoor Scene Generation with Latent Consistency Model", "categories": ["cs.CV"], "comment": null, "summary": "Our project page: https://scutyklin.github.io/SceneLCM/. Automated generation\nof complex, interactive indoor scenes tailored to user prompt remains a\nformidable challenge. While existing methods achieve indoor scene synthesis,\nthey struggle with rigid editing constraints, physical incoherence, excessive\nhuman effort, single-room limitations, and suboptimal material quality. To\naddress these limitations, we propose SceneLCM, an end-to-end framework that\nsynergizes Large Language Model (LLM) for layout design with Latent Consistency\nModel(LCM) for scene optimization. Our approach decomposes scene generation\ninto four modular pipelines: (1) Layout Generation. We employ LLM-guided 3D\nspatial reasoning to convert textual descriptions into parametric blueprints(3D\nlayout). And an iterative programmatic validation mechanism iteratively refines\nlayout parameters through LLM-mediated dialogue loops; (2) Furniture\nGeneration. SceneLCM employs Consistency Trajectory Sampling(CTS), a\nconsistency distillation sampling loss guided by LCM, to form fast,\nsemantically rich, and high-quality representations. We also offer two\ntheoretical justification to demonstrate that our CTS loss is equivalent to\nconsistency loss and its distillation error is bounded by the truncation error\nof the Euler solver; (3) Environment Optimization. We use a multiresolution\ntexture field to encode the appearance of the scene, and optimize via CTS loss.\nTo maintain cross-geometric texture coherence, we introduce a normal-aware\ncross-attention decoder to predict RGB by cross-attending to the anchors\nlocations in geometrically heterogeneous instance. (4)Physically Editing.\nSceneLCM supports physically editing by integrating physical simulation,\nachieved persistent physical realism. Extensive experiments validate SceneLCM's\nsuperiority over state-of-the-art techniques, showing its wide-ranging\npotential for diverse applications.", "AI": {"tldr": "SceneLCM is an end-to-end framework combining LLM for layout design and LCM for scene optimization, addressing challenges in automated indoor scene generation.", "motivation": "Existing methods for indoor scene synthesis face issues like rigid editing, physical incoherence, and limited scalability. SceneLCM aims to overcome these with modular pipelines.", "method": "SceneLCM uses four pipelines: LLM-guided layout generation, LCM-based furniture generation, environment optimization with multiresolution texture, and physical editing via simulation.", "result": "The framework outperforms state-of-the-art techniques, demonstrating high-quality, coherent, and physically realistic scene generation.", "conclusion": "SceneLCM offers a scalable, efficient solution for interactive indoor scene synthesis with broad application potential."}}
{"id": "2506.07154", "pdf": "https://arxiv.org/pdf/2506.07154", "abs": "https://arxiv.org/abs/2506.07154", "authors": ["Vicky Xefteri", "Tim Vieira", "Ryan Cotterell", "Afra Amini"], "title": "Syntactic Control of Language Models by Posterior Inference", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Controlling the syntactic structure of text generated by language models is\nvaluable for applications requiring clarity, stylistic consistency, or\ninterpretability, yet it remains a challenging task. In this paper, we argue\nthat sampling algorithms based on the posterior inference can effectively\nenforce a target constituency structure during generation. Our approach\ncombines sequential Monte Carlo, which estimates the posterior distribution by\nsampling from a proposal distribution, with a syntactic tagger that ensures\nthat each generated token aligns with the desired syntactic structure. Our\nexperiments with GPT2 and Llama3-8B models show that with an appropriate\nproposal distribution, we can improve syntactic accuracy, increasing the F1\nscore from $12.31$ (GPT2-large) and $35.33$ (Llama3-8B) to about $93$ in both\ncases without compromising the language model's fluency. These results\nunderscore both the complexity of syntactic control and the effectiveness of\nsampling algorithms, offering a promising approach for applications where\nprecise control over syntax is essential.", "AI": {"tldr": "The paper proposes a sampling algorithm based on posterior inference to control syntactic structure in text generation, improving syntactic accuracy without losing fluency.", "motivation": "To address the challenge of enforcing target syntactic structures in text generation for clarity, style, or interpretability.", "method": "Combines sequential Monte Carlo for posterior distribution estimation with a syntactic tagger to align tokens with desired structures.", "result": "Achieves ~93 F1 score for syntactic accuracy in GPT2 and Llama3-8B models, up from 12.31 and 35.33, respectively.", "conclusion": "Demonstrates the effectiveness of sampling algorithms for precise syntactic control, offering promise for applications requiring such precision."}}
{"id": "2506.07112", "pdf": "https://arxiv.org/pdf/2506.07112", "abs": "https://arxiv.org/abs/2506.07112", "authors": ["Changhong Fu", "Hua Lin", "Haobo Zuo", "Liangliang Yao", "Liguo Zhang"], "title": "EdgeSpotter: Multi-Scale Dense Text Spotting for Industrial Panel Monitoring", "categories": ["cs.CV"], "comment": null, "summary": "Text spotting for industrial panels is a key task for intelligent monitoring.\nHowever, achieving efficient and accurate text spotting for complex industrial\npanels remains challenging due to issues such as cross-scale localization and\nambiguous boundaries in dense text regions. Moreover, most existing methods\nprimarily focus on representing a single text shape, neglecting a comprehensive\nexploration of multi-scale feature information across different texts. To\naddress these issues, this work proposes a novel multi-scale dense text spotter\nfor edge AI-based vision system (EdgeSpotter) to achieve accurate and robust\nindustrial panel monitoring. Specifically, a novel Transformer with efficient\nmixer is developed to learn the interdependencies among multi-level features,\nintegrating multi-layer spatial and semantic cues. In addition, a new feature\nsampling with catmull-rom splines is designed, which explicitly encodes the\nshape, position, and semantic information of text, thereby alleviating missed\ndetections and reducing recognition errors caused by multi-scale or dense text\nregions. Furthermore, a new benchmark dataset for industrial panel monitoring\n(IPM) is constructed. Extensive qualitative and quantitative evaluations on\nthis challenging benchmark dataset validate the superior performance of the\nproposed method in different challenging panel monitoring tasks. Finally,\npractical tests based on the self-designed edge AI-based vision system\ndemonstrate the practicality of the method. The code and demo will be available\nat https://github.com/vision4robotics/EdgeSpotter.", "AI": {"tldr": "Proposes EdgeSpotter, a multi-scale dense text spotter for industrial panels, addressing challenges like cross-scale localization and dense text regions. Uses a Transformer with efficient mixer and feature sampling with Catmull-Rom splines. Validated on a new benchmark dataset (IPM).", "motivation": "Challenges in industrial panel text spotting include cross-scale localization and dense text regions. Existing methods lack multi-scale feature exploration.", "method": "Develops a Transformer with efficient mixer for multi-level feature interdependencies and Catmull-Rom splines for feature sampling. Introduces the IPM dataset.", "result": "Superior performance on the IPM benchmark, with practical validation on an edge AI-based vision system.", "conclusion": "EdgeSpotter effectively addresses industrial panel text spotting challenges, validated by benchmarks and practical tests."}}
{"id": "2506.07160", "pdf": "https://arxiv.org/pdf/2506.07160", "abs": "https://arxiv.org/abs/2506.07160", "authors": ["Yikun Wang", "Yibin Wang", "Dianyi Wang", "Zimian Peng", "Qipeng Guo", "Dacheng Tao", "Jiaqi Wang"], "title": "GeometryZero: Improving Geometry Solving for LLM with Group Contrastive Policy Optimization", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in large language models (LLMs) have demonstrated remarkable\ncapabilities across diverse domains, particularly in mathematical reasoning,\namid which geometry problem solving remains a challenging area where auxiliary\nconstruction plays a enssential role. Existing approaches either achieve\nsuboptimal performance or rely on massive LLMs (e.g., GPT-4o), incurring\nmassive computational costs. We posit that reinforcement learning with\nverifiable reward (e.g., GRPO) offers a promising direction for training\nsmaller models that effectively combine auxiliary construction with robust\ngeometric reasoning. However, directly applying GRPO to geometric reasoning\npresents fundamental limitations due to its dependence on unconditional\nrewards, which leads to indiscriminate and counterproductive auxiliary\nconstructions. To address these challenges, we propose Group Contrastive Policy\nOptimization (GCPO), a novel reinforcement learning framework featuring two key\ninnovations: (1) Group Contrastive Masking, which adaptively provides positive\nor negative reward signals for auxiliary construction based on contextual\nutility, and a (2) length reward that promotes longer reasoning chains.\nBuilding on GCPO, we develop GeometryZero, a family of affordable-size\ngeometric reasoning models that judiciously determine when to employ auxiliary\nconstruction. Our extensive empirical evaluation across popular geometric\nbenchmarks (Geometry3K, MathVista) demonstrates that GeometryZero models\nconsistently outperform baselines (e.g. GRPO), achieving an average improvement\nof 4.29% across all benchmarks.", "AI": {"tldr": "The paper introduces Group Contrastive Policy Optimization (GCPO), a reinforcement learning framework, to improve geometric reasoning in smaller LLMs by optimizing auxiliary construction and reasoning chains.", "motivation": "Geometry problem-solving in LLMs is challenging due to suboptimal performance or high computational costs of large models. The paper aims to train smaller models effectively.", "method": "Proposes GCPO with Group Contrastive Masking and length rewards to guide auxiliary construction and reasoning chains. Develops GeometryZero models based on GCPO.", "result": "GeometryZero outperforms baselines (e.g., GRPO) by 4.29% on benchmarks like Geometry3K and MathVista.", "conclusion": "GCPO and GeometryZero offer an efficient solution for geometric reasoning, balancing performance and computational cost."}}
{"id": "2506.07122", "pdf": "https://arxiv.org/pdf/2506.07122", "abs": "https://arxiv.org/abs/2506.07122", "authors": ["Prakriti Tripathi", "Theertha Biju", "Maniram Thota", "Rakesh Lingam"], "title": "Image segmentation and classification of E-waste for waste segregation", "categories": ["cs.CV", "cs.AI", "I.2.10"], "comment": "4 pages, 7 figures. For code and link to dataset, see\n  https://github.com/prakriti16/Image-segmentation-and-classification-of-e-waste", "summary": "Industry partners provided a problem statement that involves classifying\nelectronic waste using machine learning models that will be used by\npick-and-place robots for waste segregation. We started by taking common\nelectronic waste items, such as a mouse and charger, unsoldering them, and\ntaking pictures to create a custom dataset. Then state-of-the art YOLOv11 model\nwas trained and run to achieve 70 mAP in real-time. Mask-RCNN model was also\ntrained and achieved 41 mAP. The model will be further integrated with\npick-and-place robots to perform segregation of e-waste.", "AI": {"tldr": "A machine learning approach using YOLOv11 and Mask-RCNN was developed to classify electronic waste for pick-and-place robots, achieving 70 and 41 mAP, respectively.", "motivation": "The need for efficient electronic waste segregation using automation to improve recycling processes.", "method": "Created a custom dataset by disassembling and photographing e-waste items, then trained YOLOv11 and Mask-RCNN models.", "result": "YOLOv11 achieved 70 mAP, while Mask-RCNN achieved 41 mAP in real-time classification.", "conclusion": "The models show promise for integration with robots to automate e-waste segregation, with potential for further improvement."}}
{"id": "2506.07169", "pdf": "https://arxiv.org/pdf/2506.07169", "abs": "https://arxiv.org/abs/2506.07169", "authors": ["Washington Cunha", "Leonardo Rocha", "Marcos Andr\u00e9 Gon\u00e7alves"], "title": "CTDGSI: A comprehensive exploitation of instance selection methods for automatic text classification. VII Concurso de Teses, Disserta\u00e7\u00f5es e Trabalhos de Gradua\u00e7\u00e3o em SI -- XXI Simp\u00f3sio Brasileiro de Sistemas de Informa\u00e7\u00e3o", "categories": ["cs.CL", "cs.AI"], "comment": "16 pages, 5 figures, 2 tables", "summary": "Progress in Natural Language Processing (NLP) has been dictated by the rule\nof more: more data, more computing power and more complexity, best exemplified\nby the Large Language Models. However, training (or fine-tuning) large dense\nmodels for specific applications usually requires significant amounts of\ncomputing resources. This \\textbf{Ph.D. dissertation} focuses on an\nunder-investi\\-gated NLP data engineering technique, whose potential is\nenormous in the current scenario known as Instance Selection (IS). The IS goal\nis to reduce the training set size by removing noisy or redundant instances\nwhile maintaining the effectiveness of the trained models and reducing the\ntraining process cost. We provide a comprehensive and scientifically sound\ncomparison of IS methods applied to an essential NLP task -- Automatic Text\nClassification (ATC), considering several classification solutions and many\ndatasets. Our findings reveal a significant untapped potential for IS\nsolutions. We also propose two novel IS solutions that are noise-oriented and\nredundancy-aware, specifically designed for large datasets and transformer\narchitectures. Our final solution achieved an average reduction of 41\\% in\ntraining sets, while maintaining the same levels of effectiveness in all\ndatasets. Importantly, our solutions demonstrated speedup improvements of 1.67x\n(up to 2.46x), making them scalable for datasets with hundreds of thousands of\ndocuments.", "AI": {"tldr": "The dissertation explores Instance Selection (IS) in NLP to reduce training costs by removing noisy/redundant data while maintaining model effectiveness, achieving 41% reduction and 1.67x speedup.", "motivation": "Address the high resource demands of training large NLP models by optimizing data efficiency through IS.", "method": "Comprehensive comparison of IS methods for Automatic Text Classification (ATC), proposing noise-oriented and redundancy-aware IS solutions for large datasets and transformers.", "result": "41% average reduction in training sets with maintained effectiveness and up to 2.46x speedup.", "conclusion": "IS has untapped potential for cost-effective NLP training, with proposed solutions offering scalable efficiency."}}
{"id": "2506.07136", "pdf": "https://arxiv.org/pdf/2506.07136", "abs": "https://arxiv.org/abs/2506.07136", "authors": ["Huaize Liu", "Wenzhang Sun", "Qiyuan Zhang", "Donglin Di", "Biao Gong", "Hao Li", "Chen Wei", "Changqing Zou"], "title": "Hi-VAE: Efficient Video Autoencoding with Global and Detailed Motion", "categories": ["cs.CV"], "comment": null, "summary": "Recent breakthroughs in video autoencoders (Video AEs) have advanced video\ngeneration, but existing methods fail to efficiently model spatio-temporal\nredundancies in dynamics, resulting in suboptimal compression factors. This\nshortfall leads to excessive training costs for downstream tasks. To address\nthis, we introduce Hi-VAE, an efficient video autoencoding framework that\nhierarchically encode coarse-to-fine motion representations of video dynamics\nand formulate the decoding process as a conditional generation task.\nSpecifically, Hi-VAE decomposes video dynamics into two latent spaces: Global\nMotion, capturing overarching motion patterns, and Detailed Motion, encoding\nhigh-frequency spatial details. Using separate self-supervised motion encoders,\nwe compress video latents into compact motion representations to reduce\nredundancy significantly. A conditional diffusion decoder then reconstructs\nvideos by combining hierarchical global and detailed motions, enabling\nhigh-fidelity video reconstructions. Extensive experiments demonstrate that\nHi-VAE achieves a high compression factor of 1428$\\times$, almost 30$\\times$\nhigher than baseline methods (e.g., Cosmos-VAE at 48$\\times$), validating the\nefficiency of our approach. Meanwhile, Hi-VAE maintains high reconstruction\nquality at such high compression rates and performs effectively in downstream\ngenerative tasks. Moreover, Hi-VAE exhibits interpretability and scalability,\nproviding new perspectives for future exploration in video latent\nrepresentation and generation.", "AI": {"tldr": "Hi-VAE is a hierarchical video autoencoder that efficiently models spatio-temporal redundancies, achieving high compression (1428\u00d7) while maintaining quality.", "motivation": "Existing video autoencoders inefficiently model dynamics, leading to high training costs and suboptimal compression.", "method": "Hi-VAE hierarchically encodes global and detailed motion, using self-supervised encoders and a conditional diffusion decoder.", "result": "Hi-VAE achieves 1428\u00d7 compression (30\u00d7 better than baselines) with high reconstruction quality and downstream task performance.", "conclusion": "Hi-VAE offers efficient, interpretable, and scalable video representation, advancing video generation research."}}
{"id": "2506.07171", "pdf": "https://arxiv.org/pdf/2506.07171", "abs": "https://arxiv.org/abs/2506.07171", "authors": ["Chenlong Zhang", "Zhuoran Jin", "Hongbang Yuan", "Jiaheng Wei", "Tong Zhou", "Kang Liu", "Jun Zhao", "Yubo Chen"], "title": "RULE: Reinforcement UnLEarning Achieves Forget-Retain Pareto Optimality", "categories": ["cs.CL", "cs.LG"], "comment": "Paper under review", "summary": "The widespread deployment of Large Language Models (LLMs) trained on massive,\nuncurated corpora has raised growing concerns about the inclusion of sensitive,\ncopyrighted, or illegal content. This has led to increasing interest in LLM\nunlearning: the task of selectively removing specific information from a model\nwithout retraining from scratch or degrading overall utility. However, existing\nmethods often rely on large-scale forget and retain datasets, and suffer from\nunnatural responses, poor generalization, or catastrophic utility loss. In this\nwork, we propose Reinforcement UnLearning (RULE), an efficient framework that\nformulates unlearning as a refusal boundary optimization problem. RULE is\ntrained with a small portion of the forget set and synthesized boundary\nqueries, using a verifiable reward function that encourages safe refusal on\nforget--related queries while preserving helpful responses on permissible\ninputs. We provide both theoretical and empirical evidence demonstrating the\neffectiveness of RULE in achieving targeted unlearning without compromising\nmodel utility. Experimental results show that, with only $12%$ forget set and\n$8%$ synthesized boundary data, RULE outperforms existing baselines by up to\n$17.5%$ forget quality and $16.3%$ naturalness response while maintaining\ngeneral utility, achieving forget--retain Pareto optimality. Remarkably, we\nfurther observe that RULE improves the naturalness of model outputs, enhances\ntraining efficiency, and exhibits strong generalization ability, generalizing\nrefusal behavior to semantically related but unseen queries.", "AI": {"tldr": "RULE is a Reinforcement UnLearning framework for LLMs that efficiently removes specific information without retraining, using minimal forget data and synthesized queries, outperforming existing methods in forget quality and naturalness.", "motivation": "Concerns about sensitive, copyrighted, or illegal content in LLMs drive the need for selective unlearning without degrading model utility.", "method": "RULE formulates unlearning as a refusal boundary optimization problem, using a verifiable reward function and minimal forget set and synthesized boundary queries.", "result": "RULE achieves up to 17.5% better forget quality and 16.3% better naturalness, maintaining general utility and Pareto optimality.", "conclusion": "RULE effectively unlearns targeted information while improving response naturalness, training efficiency, and generalization."}}
{"id": "2506.07138", "pdf": "https://arxiv.org/pdf/2506.07138", "abs": "https://arxiv.org/abs/2506.07138", "authors": ["Hao Tang", "Chengchao Shen"], "title": "Learning Compact Vision Tokens for Efficient Large Multimodal Models", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "comment": "The source code and trained weights are available at\n  https://github.com/visresearch/LLaVA-STF", "summary": "Large multimodal models (LMMs) suffer significant computational challenges\ndue to the high cost of Large Language Models (LLMs) and the quadratic\ncomplexity of processing long vision token sequences. In this paper, we explore\nthe spatial redundancy among vision tokens and shorten the length of vision\ntoken sequences for inference acceleration. Specifically, we propose a Spatial\nToken Fusion (STF) method to learn compact vision tokens for short vision token\nsequence, where spatial-adjacent tokens are fused into one. Meanwhile,\nweight-frozen vision encoder can not well adapt to the demand of extensive\ndownstream vision-language tasks. To this end, we further introduce a\nMulti-Block Token Fusion (MBTF) module to supplement multi-granularity features\nfor the reduced token sequence. Overall, we combine STF and MBTF module to\nbalance token reduction and information preservation, thereby improving\ninference efficiency without sacrificing multimodal reasoning capabilities.\nExperimental results demonstrate that our method based on LLaVA-1.5 achieves\ncomparable or even superior performance to the baseline on 8 popular\nvision-language benchmarks with only $25\\%$ vision tokens of baseline. The\nsource code and trained weights are available at\nhttps://github.com/visresearch/LLaVA-STF.", "AI": {"tldr": "The paper proposes Spatial Token Fusion (STF) and Multi-Block Token Fusion (MBTF) to reduce vision token sequences in LMMs, improving efficiency without losing performance.", "motivation": "Address computational inefficiency in LMMs due to high LLM costs and quadratic complexity of long vision token sequences.", "method": "Introduces STF to fuse spatial-adjacent tokens and MBTF to add multi-granularity features, balancing token reduction and information preservation.", "result": "Achieves comparable/superior performance on 8 benchmarks with only 25% of baseline tokens.", "conclusion": "STF and MBTF enhance inference efficiency while maintaining multimodal reasoning capabilities."}}
{"id": "2506.07180", "pdf": "https://arxiv.org/pdf/2506.07180", "abs": "https://arxiv.org/abs/2506.07180", "authors": ["Wenrui Zhou", "Shu Yang", "Qingsong Yang", "Zikun Guo", "Lijie Hu", "Di Wang"], "title": "Flattery in Motion: Benchmarking and Analyzing Sycophancy in Video-LLMs", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "24 pages", "summary": "As video large language models (Video-LLMs) become increasingly integrated\ninto real-world applications that demand grounded multimodal reasoning,\nensuring their factual consistency and reliability is of critical importance.\nHowever, sycophancy, the tendency of these models to align with user input even\nwhen it contradicts the visual evidence, undermines their trustworthiness in\nsuch contexts. Current sycophancy research has largely overlooked its specific\nmanifestations in the video-language domain, resulting in a notable absence of\nsystematic benchmarks and targeted evaluations to understand how Video-LLMs\nrespond under misleading user input. To fill this gap, we propose VISE\n(Video-LLM Sycophancy Benchmarking and Evaluation), the first dedicated\nbenchmark designed to evaluate sycophantic behavior in state-of-the-art\nVideo-LLMs across diverse question formats, prompt biases, and visual reasoning\ntasks. Specifically, VISE pioneeringly brings linguistic perspectives on\nsycophancy into the visual domain, enabling fine-grained analysis across\nmultiple sycophancy types and interaction patterns. In addition, we explore\nkey-frame selection as an interpretable, training-free mitigation strategy,\nwhich reveals potential paths for reducing sycophantic bias by strengthening\nvisual grounding.", "AI": {"tldr": "The paper introduces VISE, a benchmark for evaluating sycophantic behavior in Video-LLMs, addressing the lack of systematic assessments in the video-language domain. It also explores key-frame selection as a mitigation strategy.", "motivation": "Video-LLMs often align with user input even when it contradicts visual evidence, undermining trustworthiness. Current research lacks benchmarks for sycophancy in this domain.", "method": "Proposes VISE, a benchmark for evaluating sycophantic behavior in Video-LLMs across diverse question formats and tasks. Introduces key-frame selection as a mitigation strategy.", "result": "VISE enables fine-grained analysis of sycophantic behavior and suggests key-frame selection can reduce bias by improving visual grounding.", "conclusion": "The study highlights the need for targeted evaluations of Video-LLMs and offers a practical approach to mitigate sycophantic behavior."}}
{"id": "2506.07155", "pdf": "https://arxiv.org/pdf/2506.07155", "abs": "https://arxiv.org/abs/2506.07155", "authors": ["Van Nguyen Nguyen", "Christian Forster", "Sindi Shkodrani", "Vincent Lepetit", "Bugra Tekin", "Cem Keskin", "Tomas Hodan"], "title": "GoTrack: Generic 6DoF Object Pose Refinement and Tracking", "categories": ["cs.CV"], "comment": null, "summary": "We introduce GoTrack, an efficient and accurate CAD-based method for 6DoF\nobject pose refinement and tracking, which can handle diverse objects without\nany object-specific training. Unlike existing tracking methods that rely solely\non an analysis-by-synthesis approach for model-to-frame registration, GoTrack\nadditionally integrates frame-to-frame registration, which saves compute and\nstabilizes tracking. Both types of registration are realized by optical flow\nestimation. The model-to-frame registration is noticeably simpler than in\nexisting methods, relying only on standard neural network blocks (a transformer\nis trained on top of DINOv2) and producing reliable pose confidence scores\nwithout a scoring network. For the frame-to-frame registration, which is an\neasier problem as consecutive video frames are typically nearly identical, we\nemploy a light off-the-shelf optical flow model. We demonstrate that GoTrack\ncan be seamlessly combined with existing coarse pose estimation methods to\ncreate a minimal pipeline that reaches state-of-the-art RGB-only results on\nstandard benchmarks for 6DoF object pose estimation and tracking. Our source\ncode and trained models are publicly available at\nhttps://github.com/facebookresearch/gotrack", "AI": {"tldr": "GoTrack is a CAD-based method for 6DoF object pose refinement and tracking, integrating model-to-frame and frame-to-frame registration via optical flow, achieving state-of-the-art results without object-specific training.", "motivation": "Existing tracking methods rely heavily on analysis-by-synthesis for model-to-frame registration, lacking efficiency and stability. GoTrack aims to address this by combining both registration types.", "method": "GoTrack uses optical flow for model-to-frame (simplified with a transformer on DINOv2) and frame-to-frame (light off-the-shelf model) registration, avoiding object-specific training.", "result": "GoTrack achieves state-of-the-art RGB-only results on 6DoF object pose estimation and tracking benchmarks.", "conclusion": "GoTrack offers an efficient, accurate, and minimal pipeline for 6DoF tracking, with publicly available code and models."}}
{"id": "2506.07245", "pdf": "https://arxiv.org/pdf/2506.07245", "abs": "https://arxiv.org/abs/2506.07245", "authors": ["Wenxuan Xie", "Yaxun Dai", "Wenhao Jiang"], "title": "SDE-SQL: Enhancing Text-to-SQL Generation in Large Language Models via Self-Driven Exploration with SQL Probes", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have significantly\nimproved performance on the Text-to-SQL task. However, prior approaches\ntypically rely on static, pre-processed database information provided at\ninference time, which limits the model's ability to fully understand the\ndatabase contents. Without dynamic interaction, LLMs are constrained to fixed,\nhuman-provided context and cannot autonomously explore the underlying data. To\naddress this limitation, we propose SDE-SQL, a framework that enables large\nlanguage models to perform self-driven exploration of databases during\ninference. This is accomplished by generating and executing SQL probes, which\nallow the model to actively retrieve information from the database and\niteratively update its understanding of the data. Unlike prior methods, SDE-SQL\noperates in a zero-shot setting, without relying on any question-SQL pairs as\nin-context demonstrations. When evaluated on the BIRD benchmark with\nQwen2.5-72B-Instruct, SDE-SQL achieves an 8.02% relative improvement in\nexecution accuracy over the vanilla Qwen2.5-72B-Instruct baseline, establishing\na new state-of-the-art among methods based on open-source models without\nsupervised fine-tuning (SFT) or model ensembling. Moreover, with SFT, the\nperformance of SDE-SQL can be further enhanced, yielding an additional 0.52%\nimprovement.", "AI": {"tldr": "SDE-SQL enables LLMs to dynamically explore databases during inference using SQL probes, improving Text-to-SQL performance without relying on pre-processed data or in-context demonstrations.", "motivation": "Prior methods limit LLMs by using static database info, preventing autonomous exploration. SDE-SQL addresses this by allowing dynamic interaction.", "method": "SDE-SQL generates and executes SQL probes for active database retrieval, updating the model's understanding iteratively in a zero-shot setting.", "result": "Achieves 8.02% relative improvement in execution accuracy on BIRD benchmark with Qwen2.5-72B-Instruct, setting a new SOTA for open-source models without SFT.", "conclusion": "SDE-SQL enhances LLM performance in Text-to-SQL tasks through dynamic exploration, with further gains possible via SFT."}}
{"id": "2506.07164", "pdf": "https://arxiv.org/pdf/2506.07164", "abs": "https://arxiv.org/abs/2506.07164", "authors": ["Qiong Chang", "Xinyuan Chen", "Xiang Li", "Weimin Wang", "Jun Miyazaki"], "title": "Faster than Fast: Accelerating Oriented FAST Feature Detection on Low-end Embedded GPUs", "categories": ["cs.CV"], "comment": null, "summary": "The visual-based SLAM (Simultaneous Localization and Mapping) is a technology\nwidely used in applications such as robotic navigation and virtual reality,\nwhich primarily focuses on detecting feature points from visual images to\nconstruct an unknown environmental map and simultaneously determines its own\nlocation. It usually imposes stringent requirements on hardware power\nconsumption, processing speed and accuracy. Currently, the ORB (Oriented FAST\nand Rotated BRIEF)-based SLAM systems have exhibited superior performance in\nterms of processing speed and robustness. However, they still fall short of\nmeeting the demands for real-time processing on mobile platforms. This\nlimitation is primarily due to the time-consuming Oriented FAST calculations\naccounting for approximately half of the entire SLAM system. This paper\npresents two methods to accelerate the Oriented FAST feature detection on\nlow-end embedded GPUs. These methods optimize the most time-consuming steps in\nOriented FAST feature detection: FAST feature point detection and Harris corner\ndetection, which is achieved by implementing a binary-level encoding strategy\nto determine candidate points quickly and a separable Harris detection strategy\nwith efficient low-level GPU hardware-specific instructions. Extensive\nexperiments on a Jetson TX2 embedded GPU demonstrate an average speedup of over\n7.3 times compared to widely used OpenCV with GPU support. This significant\nimprovement highlights its effectiveness and potential for real-time\napplications in mobile and resource-constrained environments.", "AI": {"tldr": "The paper proposes two GPU-optimized methods to speed up Oriented FAST feature detection in SLAM systems, achieving a 7.3x speedup on embedded GPUs.", "motivation": "Current ORB-based SLAM systems struggle with real-time processing on mobile platforms due to slow Oriented FAST calculations.", "method": "Binary-level encoding for quick candidate point determination and separable Harris detection using GPU-specific instructions.", "result": "7.3x average speedup on Jetson TX2 compared to OpenCV with GPU support.", "conclusion": "The methods enable real-time SLAM on mobile and resource-constrained devices."}}
{"id": "2506.07248", "pdf": "https://arxiv.org/pdf/2506.07248", "abs": "https://arxiv.org/abs/2506.07248", "authors": ["Prathamesh Kokate", "Mitali Sarnaik", "Manavi Khopade", "Raviraj Joshi"], "title": "Improving the Efficiency of Long Document Classification using Sentence Ranking Approach", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Long document classification poses challenges due to the computational\nlimitations of transformer-based models, particularly BERT, which are\nconstrained by fixed input lengths and quadratic attention complexity.\nMoreover, using the full document for classification is often redundant, as\nonly a subset of sentences typically carries the necessary information. To\naddress this, we propose a TF-IDF-based sentence ranking method that improves\nefficiency by selecting the most informative content. Our approach explores\nfixed-count and percentage-based sentence selection, along with an enhanced\nscoring strategy combining normalized TF-IDF scores and sentence length.\nEvaluated on the MahaNews LDC dataset of long Marathi news articles, the method\nconsistently outperforms baselines such as first, last, and random sentence\nselection. With MahaBERT-v2, we achieve near-identical classification accuracy\nwith just a 0.33 percent drop compared to the full-context baseline, while\nreducing input size by over 50 percent and inference latency by 43 percent.\nThis demonstrates that significant context reduction is possible without\nsacrificing performance, making the method practical for real-world long\ndocument classification tasks.", "AI": {"tldr": "A TF-IDF-based sentence ranking method improves efficiency in long document classification by selecting the most informative content, reducing input size and latency without significant accuracy loss.", "motivation": "Transformer-based models like BERT face computational limitations with long documents, and full-document classification is often redundant.", "method": "Proposes a TF-IDF-based sentence ranking method with fixed-count and percentage-based selection, combining normalized TF-IDF scores and sentence length.", "result": "Outperforms baselines, achieving near-identical accuracy with a 0.33% drop, reducing input size by 50% and latency by 43%.", "conclusion": "Significant context reduction is feasible without performance loss, making the method practical for real-world tasks."}}
{"id": "2506.07177", "pdf": "https://arxiv.org/pdf/2506.07177", "abs": "https://arxiv.org/abs/2506.07177", "authors": ["Sangwon Jang", "Taekyung Ki", "Jaehyeong Jo", "Jaehong Yoon", "Soo Ye Kim", "Zhe Lin", "Sung Ju Hwang"], "title": "Frame Guidance: Training-Free Guidance for Frame-Level Control in Video Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": "Project page: https://frame-guidance-video.github.io/", "summary": "Advancements in diffusion models have significantly improved video quality,\ndirecting attention to fine-grained controllability. However, many existing\nmethods depend on fine-tuning large-scale video models for specific tasks,\nwhich becomes increasingly impractical as model sizes continue to grow. In this\nwork, we present Frame Guidance, a training-free guidance for controllable\nvideo generation based on frame-level signals, such as keyframes, style\nreference images, sketches, or depth maps. For practical training-free\nguidance, we propose a simple latent processing method that dramatically\nreduces memory usage, and apply a novel latent optimization strategy designed\nfor globally coherent video generation. Frame Guidance enables effective\ncontrol across diverse tasks, including keyframe guidance, stylization, and\nlooping, without any training, compatible with any video models. Experimental\nresults show that Frame Guidance can produce high-quality controlled videos for\na wide range of tasks and input signals.", "AI": {"tldr": "Frame Guidance is a training-free method for controllable video generation using frame-level signals, reducing memory usage and ensuring coherence.", "motivation": "Existing methods rely on fine-tuning large models, which is impractical as models grow. Frame Guidance offers a scalable alternative.", "method": "Uses frame-level signals (e.g., keyframes, style images) with latent processing and optimization for coherent video generation.", "result": "Produces high-quality controlled videos for tasks like keyframe guidance, stylization, and looping without training.", "conclusion": "Frame Guidance is a versatile, training-free solution compatible with any video model."}}
{"id": "2506.07249", "pdf": "https://arxiv.org/pdf/2506.07249", "abs": "https://arxiv.org/abs/2506.07249", "authors": ["Lance Calvin Lim Gamboa", "Yue Feng", "Mark Lee"], "title": "Bias Attribution in Filipino Language Models: Extending a Bias Interpretability Metric for Application on Agglutinative Languages", "categories": ["cs.CL"], "comment": "Accepted into the Gender Bias in NLP Workshop at ACL 2025\n  (GeBNLP@ACL2025)", "summary": "Emerging research on bias attribution and interpretability have revealed how\ntokens contribute to biased behavior in language models processing English\ntexts. We build on this line of inquiry by adapting the information-theoretic\nbias attribution score metric for implementation on models handling\nagglutinative languages, particularly Filipino. We then demonstrate the\neffectiveness of our adapted method by using it on a purely Filipino model and\non three multilingual models: one trained on languages worldwide and two on\nSoutheast Asian data. Our results show that Filipino models are driven towards\nbias by words pertaining to people, objects, and relationships, entity-based\nthemes that stand in contrast to the action-heavy nature of bias-contributing\nthemes in English (i.e., criminal, sexual, and prosocial behaviors). These\nfindings point to differences in how English and non-English models process\ninputs linked to sociodemographic groups and bias.", "AI": {"tldr": "The paper adapts a bias attribution metric for agglutinative languages like Filipino, revealing differences in bias themes between Filipino and English models.", "motivation": "To extend bias attribution research to agglutinative languages and compare bias themes in Filipino and English models.", "method": "Adapts an information-theoretic bias attribution score for Filipino and tests it on a Filipino model and three multilingual models.", "result": "Filipino models show bias from words about people, objects, and relationships, unlike action-heavy themes in English models.", "conclusion": "Bias processing differs between English and non-English models, highlighting language-specific themes."}}
{"id": "2506.07188", "pdf": "https://arxiv.org/pdf/2506.07188", "abs": "https://arxiv.org/abs/2506.07188", "authors": ["Ni Ding", "Lei He", "Shengbo Eben Li", "Keqiang Li"], "title": "Hierarchical Feature-level Reverse Propagation for Post-Training Neural Networks", "categories": ["cs.CV", "I.2.10"], "comment": "13 pages, 7 figures,", "summary": "End-to-end autonomous driving has emerged as a dominant paradigm, yet its\nhighly entangled black-box models pose significant challenges in terms of\ninterpretability and safety assurance. To improve model transparency and\ntraining flexibility, this paper proposes a hierarchical and decoupled\npost-training framework tailored for pretrained neural networks. By\nreconstructing intermediate feature maps from ground-truth labels, surrogate\nsupervisory signals are introduced at transitional layers to enable independent\ntraining of specific components, thereby avoiding the complexity and coupling\nof conventional end-to-end backpropagation and providing interpretable insights\ninto networks' internal mechanisms. To the best of our knowledge, this is the\nfirst method to formalize feature-level reverse computation as well-posed\noptimization problems, which we rigorously reformulate as systems of linear\nequations or least squares problems. This establishes a novel and efficient\ntraining paradigm that extends gradient backpropagation to feature\nbackpropagation. Extensive experiments on multiple standard image\nclassification benchmarks demonstrate that the proposed method achieves\nsuperior generalization performance and computational efficiency compared to\ntraditional training approaches, validating its effectiveness and potential.", "AI": {"tldr": "A hierarchical, decoupled post-training framework improves interpretability and training flexibility for pretrained neural networks in autonomous driving by reconstructing feature maps and introducing surrogate signals.", "motivation": "Addressing the interpretability and safety challenges of black-box models in end-to-end autonomous driving.", "method": "Proposes feature-level reverse computation as optimization problems (linear equations or least squares) to enable independent training of components.", "result": "Achieves superior generalization and computational efficiency on image classification benchmarks.", "conclusion": "The method offers a novel, efficient training paradigm with interpretable insights, validated by experimental results."}}
{"id": "2506.07270", "pdf": "https://arxiv.org/pdf/2506.07270", "abs": "https://arxiv.org/abs/2506.07270", "authors": ["Atahan \u00d6zer", "\u00c7a\u011fatay Y\u0131ld\u0131z"], "title": "Question Answering under Temporal Conflict: Evaluating and Organizing Evolving Knowledge with LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) exhibit remarkable capabilities in question\nanswering and reasoning thanks to their extensive parametric memory. However,\ntheir knowledge is inherently limited by the scope of their pre-training data,\nwhile real-world information evolves continuously. Updating this knowledge\ntypically requires costly and brittle re-training, or in-context learning\n(ICL), which becomes impractical at scale given the volume and volatility of\nmodern information. Motivated by these limitations, we investigate how LLMs\nperform when exposed to temporal text corpora, or documents that reflect\nevolving knowledge over time, such as sports biographies where facts like a\nplayer's \"current team\" change year by year. To this end, we introduce two new\nbenchmarks: Temporal Wiki, which captures factual drift across historical\nWikipedia snapshots, and Unified Clark, which aggregates timestamped news\narticles to simulate real-world information accumulation. Our analysis reveals\nthat LLMs often struggle to reconcile conflicting or outdated facts and can be\nmisled when multiple versions of a fact appear in context. To address these\nissues, we propose a lightweight, agentic framework that incrementally builds a\nstructured, external memory from source documents without requiring\nre-training. This knowledge organization strategy enables models to retrieve\nand reason over temporally filtered, relevant information at inference time.\nEmpirically, our method outperforms ICL and RAG baselines across both\nbenchmarks, especially on questions requiring more complex reasoning or\nintegration of conflicting facts.", "AI": {"tldr": "The paper addresses the limitations of LLMs in handling evolving knowledge by introducing benchmarks and a lightweight framework for structured external memory, improving performance over baselines.", "motivation": "LLMs' knowledge is static and outdated due to pre-training data limitations, making re-training or in-context learning impractical for evolving information.", "method": "Introduces Temporal Wiki and Unified Clark benchmarks to test LLMs on temporal data and proposes a lightweight framework for structured external memory.", "result": "LLMs struggle with conflicting facts; the proposed framework outperforms ICL and RAG baselines, especially in complex reasoning tasks.", "conclusion": "The lightweight framework effectively handles evolving knowledge without re-training, improving LLM performance on temporal and conflicting data."}}
{"id": "2506.07196", "pdf": "https://arxiv.org/pdf/2506.07196", "abs": "https://arxiv.org/abs/2506.07196", "authors": ["Mengya Xu", "Zhongzhen Huang", "Dillan Imans", "Yiru Ye", "Xiaofan Zhang", "Qi Dou"], "title": "SAP-Bench: Benchmarking Multimodal Large Language Models in Surgical Action Planning", "categories": ["cs.CV", "cs.CL"], "comment": "11 pages, 4 figures", "summary": "Effective evaluation is critical for driving advancements in MLLM research.\nThe surgical action planning (SAP) task, which aims to generate future action\nsequences from visual inputs, demands precise and sophisticated analytical\ncapabilities. Unlike mathematical reasoning, surgical decision-making operates\nin life-critical domains and requires meticulous, verifiable processes to\nensure reliability and patient safety. This task demands the ability to\ndistinguish between atomic visual actions and coordinate complex, long-horizon\nprocedures, capabilities that are inadequately evaluated by current benchmarks.\nTo address this gap, we introduce SAP-Bench, a large-scale, high-quality\ndataset designed to enable multimodal large language models (MLLMs) to perform\ninterpretable surgical action planning. Our SAP-Bench benchmark, derived from\nthe cholecystectomy procedures context with the mean duration of 1137.5s, and\nintroduces temporally-grounded surgical action annotations, comprising the\n1,226 clinically validated action clips (mean duration: 68.7s) capturing five\nfundamental surgical actions across 74 procedures. The dataset provides 1,152\nstrategically sampled current frames, each paired with the corresponding next\naction as multimodal analysis anchors. We propose the MLLM-SAP framework that\nleverages MLLMs to generate next action recommendations from the current\nsurgical scene and natural language instructions, enhanced with injected\nsurgical domain knowledge. To assess our dataset's effectiveness and the\nbroader capabilities of current models, we evaluate seven state-of-the-art\nMLLMs (e.g., OpenAI-o1, GPT-4o, QwenVL2.5-72B, Claude-3.5-Sonnet, GeminiPro2.5,\nStep-1o, and GLM-4v) and reveal critical gaps in next action prediction\nperformance.", "AI": {"tldr": "The paper introduces SAP-Bench, a dataset for evaluating multimodal large language models (MLLMs) in surgical action planning, highlighting gaps in current benchmarks and model performance.", "motivation": "Current benchmarks inadequately evaluate MLLMs for surgical action planning, a life-critical task requiring precision and verifiability.", "method": "SAP-Bench is created with 1,226 clinically validated action clips from cholecystectomy procedures. The MLLM-SAP framework leverages MLLMs for next-action recommendations.", "result": "Evaluation of seven state-of-the-art MLLMs reveals significant gaps in next-action prediction performance.", "conclusion": "SAP-Bench addresses the need for better evaluation tools in surgical action planning, exposing limitations in current MLLMs."}}
{"id": "2506.07274", "pdf": "https://arxiv.org/pdf/2506.07274", "abs": "https://arxiv.org/abs/2506.07274", "authors": ["Olga Kellert", "Nemika Tyagi", "Muhammad Imran", "Nelvin Licona-Guevara", "Carlos G\u00f3mez-Rodr\u00edguez"], "title": "Parsing the Switch: LLM-Based UD Annotation for Complex Code-Switched and Low-Resource Languages", "categories": ["cs.CL", "cs.AI"], "comment": "16 pages", "summary": "Code-switching presents a complex challenge for syntactic analysis,\nespecially in low-resource language settings where annotated data is scarce.\nWhile recent work has explored the use of large language models (LLMs) for\nsequence-level tagging, few approaches systematically investigate how well\nthese models capture syntactic structure in code-switched contexts. Moreover,\nexisting parsers trained on monolingual treebanks often fail to generalize to\nmultilingual and mixed-language input. To address this gap, we introduce the\nBiLingua Parser, an LLM-based annotation pipeline designed to produce Universal\nDependencies (UD) annotations for code-switched text. First, we develop a\nprompt-based framework for Spanish-English and Spanish-Guaran\\'i data,\ncombining few-shot LLM prompting with expert review. Second, we release two\nannotated datasets, including the first Spanish-Guaran\\'i UD-parsed corpus.\nThird, we conduct a detailed syntactic analysis of switch points across\nlanguage pairs and communicative contexts. Experimental results show that\nBiLingua Parser achieves up to 95.29% LAS after expert revision, significantly\noutperforming prior baselines and multilingual parsers. These results show that\nLLMs, when carefully guided, can serve as practical tools for bootstrapping\nsyntactic resources in under-resourced, code-switched environments. Data and\nsource code are available at https://github.com/N3mika/ParsingProject", "AI": {"tldr": "The paper introduces BiLingua Parser, an LLM-based tool for annotating code-switched text with Universal Dependencies, achieving high accuracy and outperforming existing methods.", "motivation": "Addressing the lack of syntactic analysis tools for code-switched text, especially in low-resource languages, and improving upon monolingual parsers' limitations.", "method": "Develops a prompt-based framework using few-shot LLM prompting and expert review, releases annotated datasets, and analyzes syntactic switch points.", "result": "BiLingua Parser achieves 95.29% LAS after expert revision, surpassing prior baselines and multilingual parsers.", "conclusion": "LLMs, when guided properly, can effectively bootstrap syntactic resources for code-switched and low-resource language settings."}}
{"id": "2506.07205", "pdf": "https://arxiv.org/pdf/2506.07205", "abs": "https://arxiv.org/abs/2506.07205", "authors": ["Min-Jung Kim", "Dongjin Kim", "Seokju Yun", "Jaegul Choo"], "title": "TV-LiVE: Training-Free, Text-Guided Video Editing via Layer Informed Vitality Exploitation", "categories": ["cs.CV"], "comment": null, "summary": "Video editing has garnered increasing attention alongside the rapid progress\nof diffusion-based video generation models. As part of these advancements,\nthere is a growing demand for more accessible and controllable forms of video\nediting, such as prompt-based editing. Previous studies have primarily focused\non tasks such as style transfer, background replacement, object substitution,\nand attribute modification, while maintaining the content structure of the\nsource video. However, more complex tasks, including the addition of novel\nobjects and nonrigid transformations, remain relatively unexplored. In this\npaper, we present TV-LiVE, a Training-free and text-guided Video editing\nframework via Layerinformed Vitality Exploitation. We empirically identify\nvital layers within the video generation model that significantly influence the\nquality of generated outputs. Notably, these layers are closely associated with\nRotary Position Embeddings (RoPE). Based on this observation, our method\nenables both object addition and non-rigid video editing by selectively\ninjecting key and value features from the source model into the corresponding\nlayers of the target model guided by the layer vitality. For object addition,\nwe further identify prominent layers to extract the mask regions corresponding\nto the newly added target prompt. We found that the extracted masks from the\nprominent layers faithfully indicate the region to be edited. Experimental\nresults demonstrate that TV-LiVE outperforms existing approaches for both\nobject addition and non-rigid video editing. Project Page:\nhttps://emjay73.github.io/TV_LiVE/", "AI": {"tldr": "TV-LiVE is a training-free, text-guided video editing framework that enables complex tasks like object addition and non-rigid transformations by leveraging vital layers in video generation models.", "motivation": "The demand for more accessible and controllable video editing, especially for complex tasks like adding novel objects and non-rigid transformations, is unmet by current methods.", "method": "TV-LiVE identifies vital layers in video generation models, linked to Rotary Position Embeddings (RoPE), and selectively injects key/value features for editing. For object addition, it extracts masks from prominent layers.", "result": "TV-LiVE outperforms existing methods in object addition and non-rigid video editing, with extracted masks accurately indicating editable regions.", "conclusion": "TV-LiVE advances video editing by addressing complex tasks without training, leveraging layer vitality for high-quality results."}}
{"id": "2506.07295", "pdf": "https://arxiv.org/pdf/2506.07295", "abs": "https://arxiv.org/abs/2506.07295", "authors": ["Lujun Li", "Lama Sleem", "Niccolo' Gentile", "Geoffrey Nichil", "Radu State"], "title": "Exploring the Impact of Temperature on Large Language Models:Hot or Cold?", "categories": ["cs.CL"], "comment": null, "summary": "The sampling temperature, a critical hyperparameter in large language models\n(LLMs), modifies the logits before the softmax layer, thereby reshaping the\ndistribution of output tokens. Recent studies have challenged the Stochastic\nParrots analogy by demonstrating that LLMs are capable of understanding\nsemantics rather than merely memorizing data and that randomness, modulated by\nsampling temperature, plays a crucial role in model inference. In this study,\nwe systematically evaluated the impact of temperature in the range of 0 to 2 on\ndata sets designed to assess six different capabilities, conducting statistical\nanalyses on open source models of three different sizes: small (1B--4B), medium\n(6B--13B), and large (40B--80B). Our findings reveal distinct skill-specific\neffects of temperature on model performance, highlighting the complexity of\noptimal temperature selection in practical applications. To address this\nchallenge, we propose a BERT-based temperature selector that takes advantage of\nthese observed effects to identify the optimal temperature for a given prompt.\nWe demonstrate that this approach can significantly improve the performance of\nsmall and medium models in the SuperGLUE datasets. Furthermore, our study\nextends to FP16 precision inference, revealing that temperature effects are\nconsistent with those observed in 4-bit quantized models. By evaluating\ntemperature effects up to 4.0 in three quantized models, we find that the\nMutation Temperature -- the point at which significant performance changes\noccur -- increases with model size.", "AI": {"tldr": "The study evaluates the impact of sampling temperature on LLM performance across different model sizes and proposes a BERT-based selector for optimal temperature, improving results in smaller models.", "motivation": "Recent research shows LLMs understand semantics, not just memorize data, with temperature playing a key role. This study aims to systematically assess temperature's impact on model performance.", "method": "Evaluated temperature (0-2) on datasets testing six capabilities, analyzed open-source models of small (1B-4B), medium (6B-13B), and large (40B-80B) sizes. Proposed a BERT-based temperature selector.", "result": "Temperature has skill-specific effects on performance. The BERT selector improved small/medium models in SuperGLUE. FP16 precision and 4-bit quantized models showed consistent temperature effects. Mutation Temperature rises with model size.", "conclusion": "Optimal temperature selection is complex but crucial. The BERT-based selector offers a practical solution, especially for smaller models, and temperature effects remain consistent across precision levels."}}
{"id": "2506.07214", "pdf": "https://arxiv.org/pdf/2506.07214", "abs": "https://arxiv.org/abs/2506.07214", "authors": ["Zhiyuan Zhong", "Zhen Sun", "Yepang Liu", "Xinlei He", "Guanhong Tao"], "title": "Backdoor Attack on Vision Language Models with Stealthy Semantic Manipulation", "categories": ["cs.CV", "cs.CR"], "comment": null, "summary": "Vision Language Models (VLMs) have shown remarkable performance, but are also\nvulnerable to backdoor attacks whereby the adversary can manipulate the model's\noutputs through hidden triggers. Prior attacks primarily rely on\nsingle-modality triggers, leaving the crucial cross-modal fusion nature of VLMs\nlargely unexplored. Unlike prior work, we identify a novel attack surface that\nleverages cross-modal semantic mismatches as implicit triggers. Based on this\ninsight, we propose BadSem (Backdoor Attack with Semantic Manipulation), a data\npoisoning attack that injects stealthy backdoors by deliberately misaligning\nimage-text pairs during training. To perform the attack, we construct SIMBad, a\ndataset tailored for semantic manipulation involving color and object\nattributes. Extensive experiments across four widely used VLMs show that BadSem\nachieves over 98% average ASR, generalizes well to out-of-distribution\ndatasets, and can transfer across poisoning modalities. Our detailed analysis\nusing attention visualization shows that backdoored models focus on\nsemantically sensitive regions under mismatched conditions while maintaining\nnormal behavior on clean inputs. To mitigate the attack, we try two defense\nstrategies based on system prompt and supervised fine-tuning but find that both\nof them fail to mitigate the semantic backdoor. Our findings highlight the\nurgent need to address semantic vulnerabilities in VLMs for their safer\ndeployment.", "AI": {"tldr": "BadSem introduces a cross-modal backdoor attack exploiting semantic mismatches in VLMs, achieving high attack success while evading defenses.", "motivation": "Prior backdoor attacks on VLMs focus on single-modality triggers, neglecting cross-modal vulnerabilities. This work explores semantic mismatches as triggers.", "method": "BadSem poisons training data by misaligning image-text pairs, using the SIMBad dataset for semantic manipulation (color/object attributes).", "result": "BadSem achieves >98% ASR, generalizes to out-of-distribution data, and transfers across modalities. Defenses like prompt and fine-tuning fail.", "conclusion": "Semantic vulnerabilities in VLMs are critical; BadSem exposes urgent risks, calling for safer deployment strategies."}}
{"id": "2506.07297", "pdf": "https://arxiv.org/pdf/2506.07297", "abs": "https://arxiv.org/abs/2506.07297", "authors": ["Lauren Levine", "Amir Zeldes"], "title": "Subjectivity in the Annotation of Bridging Anaphora", "categories": ["cs.CL", "I.2.7"], "comment": "LAW-XIX, ACL 2025 Workshop", "summary": "Bridging refers to the associative relationship between inferable entities in\na discourse and the antecedents which allow us to understand them, such as\nunderstanding what \"the door\" means with respect to an aforementioned \"house\".\nAs identifying associative relations between entities is an inherently\nsubjective task, it is difficult to achieve consistent agreement in the\nannotation of bridging anaphora and their antecedents. In this paper, we\nexplore the subjectivity involved in the annotation of bridging instances at\nthree levels: anaphor recognition, antecedent resolution, and bridging subtype\nselection. To do this, we conduct an annotation pilot on the test set of the\nexisting GUM corpus, and propose a newly developed classification system for\nbridging subtypes, which we compare to previously proposed schemes. Our results\nsuggest that some previous resources are likely to be severely under-annotated.\nWe also find that while agreement on the bridging subtype category was\nmoderate, annotator overlap for exhaustively identifying instances of bridging\nis low, and that many disagreements resulted from subjective understanding of\nthe entities involved.", "AI": {"tldr": "The paper explores subjectivity in annotating bridging anaphora, proposing a new classification system and revealing under-annotation in prior resources.", "motivation": "Understanding the challenges of consistent annotation for bridging anaphora due to its subjective nature.", "method": "Conducted an annotation pilot on the GUM corpus test set and developed a new bridging subtype classification system.", "result": "Found prior resources likely under-annotated, moderate agreement on subtypes, and low annotator overlap due to subjectivity.", "conclusion": "Highlights the complexity of bridging annotation and the need for improved frameworks to address subjectivity."}}
{"id": "2506.07216", "pdf": "https://arxiv.org/pdf/2506.07216", "abs": "https://arxiv.org/abs/2506.07216", "authors": ["Nada Aboudeshish", "Dmitry Ignatov", "Radu Timofte"], "title": "AugmentGest: Can Random Data Cropping Augmentation Boost Gesture Recognition Performance?", "categories": ["cs.CV"], "comment": null, "summary": "Data augmentation is a crucial technique in deep learning, particularly for\ntasks with limited dataset diversity, such as skeleton-based datasets. This\npaper proposes a comprehensive data augmentation framework that integrates\ngeometric transformations, random cropping, rotation, zooming and\nintensity-based transformations, brightness and contrast adjustments to\nsimulate real-world variations. Random cropping ensures the preservation of\nspatio-temporal integrity while addressing challenges such as viewpoint bias\nand occlusions. The augmentation pipeline generates three augmented versions\nfor each sample in addition to the data set sample, thus quadrupling the data\nset size and enriching the diversity of gesture representations. The proposed\naugmentation strategy is evaluated on three models: multi-stream e2eET, FPPR\npoint cloud-based hand gesture recognition (HGR), and DD-Network. Experiments\nare conducted on benchmark datasets including DHG14/28, SHREC'17, and JHMDB.\nThe e2eET model, recognized as the state-of-the-art for hand gesture\nrecognition on DHG14/28 and SHREC'17. The FPPR-PCD model, the second-best\nperforming model on SHREC'17, excels in point cloud-based gesture recognition.\nDD-Net, a lightweight and efficient architecture for skeleton-based action\nrecognition, is evaluated on SHREC'17 and the Human Motion Data Base (JHMDB).\nThe results underline the effectiveness and versatility of the proposed\naugmentation strategy, significantly improving model generalization and\nrobustness across diverse datasets and architectures. This framework not only\nestablishes state-of-the-art results on all three evaluated models but also\noffers a scalable solution to advance HGR and action recognition applications\nin real-world scenarios. The framework is available at\nhttps://github.com/NadaAbodeshish/Random-Cropping-augmentation-HGR", "AI": {"tldr": "The paper introduces a data augmentation framework for skeleton-based datasets, integrating geometric and intensity-based transformations to enhance dataset diversity and model performance.", "motivation": "Addressing limited dataset diversity and challenges like viewpoint bias and occlusions in skeleton-based datasets.", "method": "Proposes a framework combining geometric transformations, random cropping, rotation, zooming, brightness, and contrast adjustments, generating three augmented versions per sample.", "result": "Significantly improves model generalization and robustness, achieving state-of-the-art results on benchmark datasets (DHG14/28, SHREC'17, JHMDB) with three models (e2eET, FPPR-PCD, DD-Net).", "conclusion": "The framework is effective and scalable, advancing hand gesture and action recognition in real-world applications."}}
{"id": "2506.07309", "pdf": "https://arxiv.org/pdf/2506.07309", "abs": "https://arxiv.org/abs/2506.07309", "authors": ["Yin Huang", "Yifan Ethan Xu", "Kai Sun", "Vera Yan", "Alicia Sun", "Haidar Khan", "Jimmy Nguyen", "Mohammad Kachuee", "Zhaojiang Lin", "Yue Liu", "Aaron Colak", "Anuj Kumar", "Wen-tau Yih", "Xin Luna Dong"], "title": "ConfQA: Answer Only If You Are Confident", "categories": ["cs.CL"], "comment": "10 pages main content, 10 pages appendix, 5 figures, 7 tables", "summary": "Can we teach Large Language Models (LLMs) to refrain from hallucinating\nfactual statements? In this paper we present a fine-tuning strategy that we\ncall ConfQA, which can reduce hallucination rate from 20-40% to under 5% across\nmultiple factuality benchmarks. The core idea is simple: when the LLM answers a\nquestion correctly, it is trained to continue with the answer; otherwise, it is\ntrained to admit \"I am unsure\". But there are two key factors that make the\ntraining highly effective. First, we introduce a dampening prompt \"answer only\nif you are confident\" to explicitly guide the behavior, without which\nhallucination remains high as 15%-25%. Second, we leverage simple factual\nstatements, specifically attribute values from knowledge graphs, to help LLMs\ncalibrate the confidence, resulting in robust generalization across domains and\nquestion types. Building on this insight, we propose the Dual Neural Knowledge\nframework, which seamlessly select between internally parameterized neural\nknowledge and externally recorded symbolic knowledge based on ConfQA's\nconfidence. The framework enables potential accuracy gains to beyond 95%, while\nreducing unnecessary external retrievals by over 30%.", "AI": {"tldr": "A fine-tuning strategy called ConfQA reduces LLM hallucination from 20-40% to under 5% by training models to admit uncertainty when unsure and using dampening prompts and factual statements for confidence calibration.", "motivation": "To address the issue of LLMs hallucinating factual statements, aiming to improve their reliability and accuracy.", "method": "ConfQA fine-tuning: trains LLMs to admit 'I am unsure' for incorrect answers and continue with correct ones. Uses dampening prompts and factual statements for confidence calibration. Introduces the Dual Neural Knowledge framework.", "result": "Hallucination rate drops to under 5%, with potential accuracy gains beyond 95% and 30% fewer unnecessary external retrievals.", "conclusion": "ConfQA effectively reduces hallucination and improves LLM reliability, with the Dual Neural Knowledge framework enhancing accuracy and efficiency."}}
{"id": "2506.07227", "pdf": "https://arxiv.org/pdf/2506.07227", "abs": "https://arxiv.org/abs/2506.07227", "authors": ["Tianyi Bai", "Yuxuan Fan", "Jiantao Qiu", "Fupeng Sun", "Jiayi Song", "Junlin Han", "Zichen Liu", "Conghui He", "Wentao Zhang", "Binhang Yuan"], "title": "Hallucination at a Glance: Controlled Visual Edits and Fine-Grained Multimodal Learning", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Multimodal large language models (MLLMs) have achieved strong performance on\nvision-language tasks but still struggle with fine-grained visual differences,\nleading to hallucinations or missed semantic shifts. We attribute this to\nlimitations in both training data and learning objectives. To address these\nissues, we propose a controlled data generation pipeline that produces\nminimally edited image pairs with semantically aligned captions. Using this\npipeline, we construct the Micro Edit Dataset (MED), containing over 50K\nimage-text pairs spanning 11 fine-grained edit categories, including attribute,\ncount, position, and object presence changes. Building on MED, we introduce a\nsupervised fine-tuning (SFT) framework with a feature-level consistency loss\nthat promotes stable visual embeddings under small edits. We evaluate our\napproach on the Micro Edit Detection benchmark, which includes carefully\nbalanced evaluation pairs designed to test sensitivity to subtle visual\nvariations across the same edit categories. Our method improves difference\ndetection accuracy and reduces hallucinations compared to strong baselines,\nincluding GPT-4o. Moreover, it yields consistent gains on standard\nvision-language tasks such as image captioning and visual question answering.\nThese results demonstrate the effectiveness of combining targeted data and\nalignment objectives for enhancing fine-grained visual reasoning in MLLMs.", "AI": {"tldr": "The paper proposes a controlled data generation pipeline to create the Micro Edit Dataset (MED) for improving fine-grained visual reasoning in MLLMs, alongside a supervised fine-tuning framework with a feature-level consistency loss. The method outperforms baselines like GPT-4o in detecting subtle visual differences and reduces hallucinations.", "motivation": "MLLMs struggle with fine-grained visual differences, leading to hallucinations or missed semantic shifts, due to limitations in training data and learning objectives.", "method": "A controlled data generation pipeline produces minimally edited image pairs with aligned captions (MED dataset). A supervised fine-tuning framework with a feature-level consistency loss is introduced.", "result": "The method improves difference detection accuracy, reduces hallucinations, and shows gains on standard vision-language tasks like image captioning and VQA.", "conclusion": "Targeted data and alignment objectives effectively enhance fine-grained visual reasoning in MLLMs."}}
{"id": "2506.07326", "pdf": "https://arxiv.org/pdf/2506.07326", "abs": "https://arxiv.org/abs/2506.07326", "authors": ["Brian Christian", "Hannah Rose Kirk", "Jessica A. F. Thompson", "Christopher Summerfield", "Tsvetomira Dumbalska"], "title": "Reward Model Interpretability via Optimal and Pessimal Tokens", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG", "I.2.6; I.2.7; H.5.2; J.4; K.4.2"], "comment": "Accepted for publication in Proceedings of the 2025 ACM Conference on\n  Fairness, Accountability, and Transparency (FAccT '25), to appear June 2025", "summary": "Reward modeling has emerged as a crucial component in aligning large language\nmodels with human values. Significant attention has focused on using reward\nmodels as a means for fine-tuning generative models. However, the reward models\nthemselves -- which directly encode human value judgments by turning\nprompt-response pairs into scalar rewards -- remain relatively understudied. We\npresent a novel approach to reward model interpretability through exhaustive\nanalysis of their responses across their entire vocabulary space. By examining\nhow different reward models score every possible single-token response to\nvalue-laden prompts, we uncover several striking findings: (i) substantial\nheterogeneity between models trained on similar objectives, (ii) systematic\nasymmetries in how models encode high- vs low-scoring tokens, (iii) significant\nsensitivity to prompt framing that mirrors human cognitive biases, and (iv)\novervaluation of more frequent tokens. We demonstrate these effects across ten\nrecent open-source reward models of varying parameter counts and architectures.\nOur results challenge assumptions about the interchangeability of reward\nmodels, as well as their suitability as proxies of complex and\ncontext-dependent human values. We find that these models can encode concerning\nbiases toward certain identity groups, which may emerge as unintended\nconsequences of harmlessness training -- distortions that risk propagating\nthrough the downstream large language models now deployed to millions.", "AI": {"tldr": "The paper analyzes reward models in large language models, revealing inconsistencies, biases, and sensitivities in their scoring of human values.", "motivation": "To understand the interpretability and reliability of reward models in encoding human values, given their understudied nature despite their critical role in aligning language models.", "method": "Exhaustive analysis of reward models' responses across their entire vocabulary space, examining single-token responses to value-laden prompts.", "result": "Found heterogeneity between models, asymmetries in scoring, sensitivity to prompt framing, and biases toward certain identity groups.", "conclusion": "Reward models are not interchangeable or reliable proxies for human values, and their biases risk propagating through deployed language models."}}
{"id": "2506.07235", "pdf": "https://arxiv.org/pdf/2506.07235", "abs": "https://arxiv.org/abs/2506.07235", "authors": ["Tianyi Bai", "Zengjie Hu", "Fupeng Sun", "Jiantao Qiu", "Yizhen Jiang", "Guangxin He", "Bohan Zeng", "Conghui He", "Binhang Yuan", "Wentao Zhang"], "title": "Multi-Step Visual Reasoning with Visual Tokens Scaling and Verification", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Multi-modal large language models (MLLMs) have achieved remarkable\ncapabilities by integrating visual perception with language understanding,\nenabling applications such as image-grounded dialogue, visual question\nanswering, and scientific analysis. However, most MLLMs adopt a static\ninference paradigm, encoding the entire image into fixed visual tokens upfront,\nwhich limits their ability to iteratively refine understanding or adapt to\ncontext during inference. This contrasts sharply with human perception, which\nis dynamic, selective, and feedback-driven. In this work, we introduce a novel\nframework for inference-time visual token scaling that enables MLLMs to perform\niterative, verifier-guided reasoning over visual content. We formulate the\nproblem as a Markov Decision Process, involving a reasoner that proposes visual\nactions and a verifier, which is trained via multi-step Direct Preference\nOptimization (DPO), that evaluates these actions and determines when reasoning\nshould terminate. To support this, we present a new dataset, VTS, comprising\nsupervised reasoning trajectories (VTS-SFT) and preference-labeled reasoning\ncomparisons (VTS-DPO). Our method significantly outperforms existing approaches\nacross diverse visual reasoning benchmarks, offering not only improved accuracy\nbut also more interpretable and grounded reasoning processes. These results\ndemonstrate the promise of dynamic inference mechanisms for enabling\nfine-grained, context-aware visual reasoning in next-generation MLLMs.", "AI": {"tldr": "The paper introduces a dynamic inference framework for multi-modal large language models (MLLMs) to improve iterative, verifier-guided visual reasoning, outperforming static approaches.", "motivation": "Current MLLMs use static visual token encoding, limiting adaptability and refinement during inference, unlike human perception.", "method": "Proposes a Markov Decision Process framework with a reasoner and verifier trained via Direct Preference Optimization (DPO), supported by a new dataset (VTS).", "result": "Significantly outperforms existing methods in visual reasoning benchmarks, offering better accuracy and interpretability.", "conclusion": "Dynamic inference mechanisms enhance fine-grained, context-aware visual reasoning in MLLMs."}}
{"id": "2506.07335", "pdf": "https://arxiv.org/pdf/2506.07335", "abs": "https://arxiv.org/abs/2506.07335", "authors": ["Anyi Wang", "Dong Shu", "Yifan Wang", "Yunpu Ma", "Mengnan Du"], "title": "Improving LLM Reasoning through Interpretable Role-Playing Steering", "categories": ["cs.CL", "cs.AI"], "comment": "21 pages, 8 figures, 8 tables", "summary": "Role-playing has emerged as an effective technique for enhancing the\nreasoning capabilities of large language models (LLMs). However, existing\nmethods primarily rely on prompt engineering, which often lacks stability and\ninterpretability. In this paper, we introduce Sparse Autoencoder Role-Playing\nSteering (SRPS), a novel framework that identifies and manipulates internal\nmodel features associated with role-playing behavior. Our approach extracts\nlatent representations from role-play prompts, selects the most relevant\nfeatures based on activation patterns, and constructs a steering vector that\ncan be injected into the model's residual stream with controllable intensity.\nOur method enables fine-grained control over role-specific behavior and offers\ninsights into how role information influences internal model activations.\nExtensive experiments across various reasoning benchmarks and model sizes\ndemonstrate consistent performance gains. Notably, in the zero-shot\nchain-of-thought (CoT) setting, the accuracy of Llama3.1-8B on CSQA improves\nfrom 31.86% to 39.80%, while Gemma2-9B on SVAMP increases from 37.50% to\n45.10%. These results highlight the potential of SRPS to enhance reasoning\nability in LLMs, providing better interpretability and stability compared to\ntraditional prompt-based role-playing.", "AI": {"tldr": "SRPS is a new framework for enhancing LLM reasoning by manipulating internal features linked to role-playing, outperforming traditional prompt methods.", "motivation": "Existing role-playing methods lack stability and interpretability, prompting the need for a more controlled and insightful approach.", "method": "SRPS extracts latent representations from prompts, selects key features, and injects a steering vector into the model's residual stream.", "result": "SRPS improves reasoning accuracy, e.g., Llama3.1-8B on CSQA (31.86% to 39.80%) and Gemma2-9B on SVAMP (37.50% to 45.10%).", "conclusion": "SRPS enhances LLM reasoning with better interpretability and stability than prompt-based methods."}}
{"id": "2506.07280", "pdf": "https://arxiv.org/pdf/2506.07280", "abs": "https://arxiv.org/abs/2506.07280", "authors": ["Pablo Acuaviva", "Aram Davtyan", "Mariam Hassan", "Sebastian Stapf", "Ahmad Rahimi", "Alexandre Alahi", "Paolo Favaro"], "title": "From Generation to Generalization: Emergent Few-Shot Learning in Video Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": "27 pages, 23 figures, 9 tables", "summary": "Video Diffusion Models (VDMs) have emerged as powerful generative tools,\ncapable of synthesizing high-quality spatiotemporal content. Yet, their\npotential goes far beyond mere video generation. We argue that the training\ndynamics of VDMs, driven by the need to model coherent sequences, naturally\npushes them to internalize structured representations and an implicit\nunderstanding of the visual world. To probe the extent of this internal\nknowledge, we introduce a few-shot fine-tuning framework that repurposes VDMs\nfor new tasks using only a handful of examples. Our method transforms each task\ninto a visual transition, enabling the training of LoRA weights on short\ninput-output sequences without altering the generative interface of a frozen\nVDM. Despite minimal supervision, the model exhibits strong generalization\nacross diverse tasks, from low-level vision (for example, segmentation and pose\nestimation) to high-level reasoning (for example, on ARC-AGI). These results\nreframe VDMs as more than generative engines. They are adaptable visual\nlearners with the potential to serve as the backbone for future foundation\nmodels in vision.", "AI": {"tldr": "Video Diffusion Models (VDMs) are repurposed for diverse tasks using few-shot fine-tuning, demonstrating adaptability beyond video generation.", "motivation": "To explore VDMs' internalized structured representations and implicit understanding of the visual world, leveraging their training dynamics.", "method": "A few-shot fine-tuning framework transforms tasks into visual transitions, training LoRA weights on short sequences without modifying the frozen VDM.", "result": "Strong generalization across tasks, from low-level vision (e.g., segmentation) to high-level reasoning (e.g., ARC-AGI).", "conclusion": "VDMs are adaptable visual learners, potentially serving as backbone for future vision foundation models."}}
{"id": "2506.07356", "pdf": "https://arxiv.org/pdf/2506.07356", "abs": "https://arxiv.org/abs/2506.07356", "authors": ["Seokil Ham", "Yubin Choi", "Seungju Cho", "Yujin Yang", "Younghun Kim", "Changick Kim"], "title": "Refusal-Feature-guided Teacher for Safe Finetuning via Data Filtering and Alignment Distillation", "categories": ["cs.CL"], "comment": null, "summary": "Recently, major AI service providers such as Google and OpenAI have\nintroduced Finetuning-as-a-Service, which enables users to customize Large\nLanguage Models (LLMs) for specific downstream tasks using their own data.\nHowever, this service is vulnerable to degradation of LLM safety-alignment when\nuser data contains harmful prompts. While some prior works address this issue,\nfundamentally filtering harmful data from user data remains unexplored.\nMotivated by our observation that a directional representation reflecting\nrefusal behavior (called the refusal feature) obtained from safety-aligned LLMs\ncan inherently distinguish between harmful and harmless prompts, we propose the\nRefusal-Feature-guided Teacher (ReFT). Our ReFT model is trained to identify\nharmful prompts based on the similarity between input prompt features and its\nrefusal feature. During finetuning, the ReFT model serves as a teacher that\nfilters harmful prompts from user data and distills alignment knowledge into\nthe base model. Extensive experiments demonstrate that our ReFT-based\nfinetuning strategy effectively minimizes harmful outputs and enhances\nfinetuning accuracy for user-specific tasks, offering a practical solution for\nsecure and reliable deployment of LLMs in Finetuning-as-a-Service.", "AI": {"tldr": "The paper introduces ReFT, a model that filters harmful prompts during finetuning of LLMs by leveraging refusal features, ensuring safety-alignment and task accuracy.", "motivation": "Finetuning-as-a-Service is vulnerable to harmful prompts in user data, degrading LLM safety-alignment. Existing solutions lack fundamental filtering methods.", "method": "Proposes ReFT, which identifies harmful prompts using refusal features from safety-aligned LLMs and filters them during finetuning.", "result": "ReFT-based finetuning reduces harmful outputs and improves task-specific accuracy, ensuring secure LLM deployment.", "conclusion": "ReFT offers a practical solution for maintaining safety-alignment and reliability in Finetuning-as-a-Service."}}
{"id": "2506.07286", "pdf": "https://arxiv.org/pdf/2506.07286", "abs": "https://arxiv.org/abs/2506.07286", "authors": ["Aditya Chakravarty"], "title": "Multi-Step Guided Diffusion for Image Restoration on Edge Devices: Toward Lightweight Perception in Embodied AI", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": "Accepted in CVPR 2025 Embodied AI Workshop", "summary": "Diffusion models have shown remarkable flexibility for solving inverse\nproblems without task-specific retraining. However, existing approaches such as\nManifold Preserving Guided Diffusion (MPGD) apply only a single gradient update\nper denoising step, limiting restoration fidelity and robustness, especially in\nembedded or out-of-distribution settings. In this work, we introduce a\nmultistep optimization strategy within each denoising timestep, significantly\nenhancing image quality, perceptual accuracy, and generalization. Our\nexperiments on super-resolution and Gaussian deblurring demonstrate that\nincreasing the number of gradient updates per step improves LPIPS and PSNR with\nminimal latency overhead. Notably, we validate this approach on a Jetson Orin\nNano using degraded ImageNet and a UAV dataset, showing that MPGD, originally\ntrained on face datasets, generalizes effectively to natural and aerial scenes.\nOur findings highlight MPGD's potential as a lightweight, plug-and-play\nrestoration module for real-time visual perception in embodied AI agents such\nas drones and mobile robots.", "AI": {"tldr": "The paper introduces a multistep optimization strategy for diffusion models, improving image restoration quality and generalization, validated on real-time applications like drones.", "motivation": "Existing diffusion models like MPGD use only one gradient update per denoising step, limiting fidelity and robustness, especially in challenging settings.", "method": "A multistep optimization strategy is applied within each denoising timestep to enhance image quality and perceptual accuracy.", "result": "Experiments show improved LPIPS and PSNR with minimal latency overhead, and MPGD generalizes well to natural and aerial scenes.", "conclusion": "The approach demonstrates MPGD's potential as a lightweight, plug-and-play module for real-time visual perception in embodied AI agents."}}
{"id": "2506.07423", "pdf": "https://arxiv.org/pdf/2506.07423", "abs": "https://arxiv.org/abs/2506.07423", "authors": ["Janghyeon Yun", "Sang-goo Lee"], "title": "SEED: Enhancing Text-to-SQL Performance and Practical Usability Through Automatic Evidence Generation", "categories": ["cs.CL", "I.2.7"], "comment": null, "summary": "Text-to-SQL enables non-experts to retrieve data from databases by converting\nnatural language queries into SQL. However, state-of-the-art text-to-SQL\nstudies rely on the BIRD dataset, which assumes that evidence is provided along\nwith questions. Although BIRD facilitates research advancements, it assumes\nthat users have expertise and domain knowledge, contradicting the fundamental\ngoal of text-to-SQL. In addition, human-generated evidence in BIRD contains\ndefects, including missing or erroneous evidence, which affects model\nperformance. To address this issue, we propose SEED (System for Evidence\nExtraction and Domain knowledge generation), an approach that automatically\ngenerates evidence to improve performance and practical usability in real-world\nscenarios. SEED systematically analyzes database schema, description files, and\nvalues to extract relevant information. We evaluated SEED on BIRD and Spider,\ndemonstrating that it significantly improves SQL generation accuracy in the\nno-evidence scenario, and in some cases, even outperforms the setting where\nBIRD evidence is provided. Our results highlight that SEED-generated evidence\nnot only bridges the gap between research and real-world deployment but also\nimproves the adaptability and robustness of text-to-SQL models. Our code is\navailable at https://github.com/felix01189/SEED", "AI": {"tldr": "SEED automates evidence generation for text-to-SQL tasks, improving accuracy and usability without relying on human-provided evidence.", "motivation": "Current text-to-SQL models depend on human-provided evidence (BIRD dataset), which assumes user expertise and contains defects, limiting real-world practicality.", "method": "SEED automatically extracts and generates evidence by analyzing database schema, description files, and values.", "result": "SEED improves SQL generation accuracy in no-evidence scenarios and sometimes outperforms models using BIRD evidence.", "conclusion": "SEED enhances text-to-SQL adaptability and robustness, bridging the gap between research and real-world deployment."}}
{"id": "2506.07304", "pdf": "https://arxiv.org/pdf/2506.07304", "abs": "https://arxiv.org/abs/2506.07304", "authors": ["Kavitha Viswanathan", "Vrinda Goel", "Shlesh Gholap", "Devayan Ghosh", "Madhav Gupta", "Dhruvi Ganatra", "Sanket Potdar", "Amit Sethi"], "title": "FANVID: A Benchmark for Face and License Plate Recognition in Low-Resolution Videos", "categories": ["cs.CV"], "comment": null, "summary": "Real-world surveillance often renders faces and license plates unrecognizable\nin individual low-resolution (LR) frames, hindering reliable identification. To\nadvance temporal recognition models, we present FANVID, a novel video-based\nbenchmark comprising nearly 1,463 LR clips (180 x 320, 20--60 FPS) featuring 63\nidentities and 49 license plates from three English-speaking countries. Each\nvideo includes distractor faces and plates, increasing task difficulty and\nrealism. The dataset contains 31,096 manually verified bounding boxes and\nlabels.\n  FANVID defines two tasks: (1) face matching -- detecting LR faces and\nmatching them to high-resolution mugshots, and (2) license plate recognition --\nextracting text from LR plates without a predefined database. Videos are\ndownsampled from high-resolution sources to ensure that faces and text are\nindecipherable in single frames, requiring models to exploit temporal\ninformation. We introduce evaluation metrics adapted from mean Average\nPrecision at IoU > 0.5, prioritizing identity correctness for faces and\ncharacter-level accuracy for text.\n  A baseline method with pre-trained video super-resolution, detection, and\nrecognition achieved performance scores of 0.58 (face matching) and 0.42 (plate\nrecognition), highlighting both the feasibility and challenge of the tasks.\nFANVID's selection of faces and plates balances diversity with recognition\nchallenge. We release the software for data access, evaluation, baseline, and\nannotation to support reproducibility and extension. FANVID aims to catalyze\ninnovation in temporal modeling for LR recognition, with applications in\nsurveillance, forensics, and autonomous vehicles.", "AI": {"tldr": "FANVID is a video-based benchmark for temporal recognition of low-resolution faces and license plates, featuring 1,463 clips with distractor elements. It defines two tasks: face matching and license plate recognition, with baseline scores of 0.58 and 0.42, respectively.", "motivation": "To address the challenge of recognizing faces and license plates in low-resolution surveillance footage by leveraging temporal information.", "method": "The dataset includes manually labeled LR clips with distractor elements, downsampled from high-resolution sources. Tasks involve face matching and plate recognition, evaluated using adapted metrics.", "result": "Baseline performance scores of 0.58 (face matching) and 0.42 (plate recognition) demonstrate feasibility but highlight the challenge.", "conclusion": "FANVID aims to advance temporal modeling for LR recognition, with applications in surveillance, forensics, and autonomous vehicles."}}
{"id": "2506.07424", "pdf": "https://arxiv.org/pdf/2506.07424", "abs": "https://arxiv.org/abs/2506.07424", "authors": ["Kyeonghyun Kim", "Jinhee Jang", "Juhwan Choi", "Yoonji Lee", "Kyohoon Jin", "YoungBin Kim"], "title": "Plug-in and Fine-tuning: Bridging the Gap between Small Language Models and Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 main conference", "summary": "Large language models (LLMs) are renowned for their extensive linguistic\nknowledge and strong generalization capabilities, but their high computational\ndemands make them unsuitable for resource-constrained environments. In\ncontrast, small language models (SLMs) are computationally efficient but often\nlack the broad generalization capacity of LLMs. To bridge this gap, we propose\nPiFi, a novel framework that combines the strengths of both LLMs and SLMs to\nachieve high performance while maintaining efficiency. PiFi integrates a single\nfrozen layer from an LLM into a SLM and fine-tunes the combined model for\nspecific tasks, boosting performance without a significant increase in\ncomputational cost. We show that PiFi delivers consistent performance\nimprovements across a range of natural language processing tasks, including\nboth natural language understanding and generation. Moreover, our findings\ndemonstrate PiFi's ability to effectively leverage LLM knowledge, enhancing\ngeneralization to unseen domains and facilitating the transfer of linguistic\nabilities.", "AI": {"tldr": "PiFi combines LLMs and SLMs by integrating a frozen LLM layer into an SLM, improving performance efficiently.", "motivation": "Bridging the gap between high-performance LLMs and efficient SLMs for resource-constrained environments.", "method": "Integrates a frozen LLM layer into an SLM and fine-tunes the combined model for specific tasks.", "result": "Consistent performance improvements across NLP tasks, better generalization, and efficient knowledge transfer.", "conclusion": "PiFi effectively leverages LLM strengths in SLMs, balancing performance and computational efficiency."}}
{"id": "2506.07310", "pdf": "https://arxiv.org/pdf/2506.07310", "abs": "https://arxiv.org/abs/2506.07310", "authors": ["Adam W. Harley", "Yang You", "Xinglong Sun", "Yang Zheng", "Nikhil Raghuraman", "Yunqi Gu", "Sheldon Liang", "Wen-Hsuan Chu", "Achal Dave", "Pavel Tokmakov", "Suya You", "Rares Ambrus", "Katerina Fragkiadaki", "Leonidas J. Guibas"], "title": "AllTracker: Efficient Dense Point Tracking at High Resolution", "categories": ["cs.CV"], "comment": null, "summary": "We introduce AllTracker: a model that estimates long-range point tracks by\nway of estimating the flow field between a query frame and every other frame of\na video. Unlike existing point tracking methods, our approach delivers\nhigh-resolution and dense (all-pixel) correspondence fields, which can be\nvisualized as flow maps. Unlike existing optical flow methods, our approach\ncorresponds one frame to hundreds of subsequent frames, rather than just the\nnext frame. We develop a new architecture for this task, blending techniques\nfrom existing work in optical flow and point tracking: the model performs\niterative inference on low-resolution grids of correspondence estimates,\npropagating information spatially via 2D convolution layers, and propagating\ninformation temporally via pixel-aligned attention layers. The model is fast\nand parameter-efficient (16 million parameters), and delivers state-of-the-art\npoint tracking accuracy at high resolution (i.e., tracking 768x1024 pixels, on\na 40G GPU). A benefit of our design is that we can train on a wider set of\ndatasets, and we find that doing so is crucial for top performance. We provide\nan extensive ablation study on our architecture details and training recipe,\nmaking it clear which details matter most. Our code and model weights are\navailable at https://alltracker.github.io .", "AI": {"tldr": "AllTracker is a model for estimating long-range, high-resolution, dense point tracks in videos by predicting flow fields between frames, outperforming existing methods in accuracy and efficiency.", "motivation": "Existing point tracking methods lack high-resolution, dense correspondence fields, while optical flow methods are limited to short-range frame pairs. AllTracker bridges this gap.", "method": "The model uses iterative inference on low-resolution grids with 2D convolutions for spatial propagation and pixel-aligned attention for temporal propagation, combining optical flow and point tracking techniques.", "result": "AllTracker achieves state-of-the-art accuracy at high resolution (768x1024 pixels) with 16M parameters, trained on diverse datasets for optimal performance.", "conclusion": "The model's design and training on varied datasets are key to its success, with detailed ablation studies provided. Code and weights are publicly available."}}
{"id": "2506.07429", "pdf": "https://arxiv.org/pdf/2506.07429", "abs": "https://arxiv.org/abs/2506.07429", "authors": ["Ratna Kandala"], "title": "Conjoined Predication and Scalar Implicature", "categories": ["cs.CL"], "comment": null, "summary": "Magri (2016) investigates two puzzles arising from conjunction. Although\nMagri has proposed a solution to the second puzzle, the first remains\nunresolved. This first puzzle reveals a hidden interaction among\nquantification, collective/concurrent interpretation, and contextual updating\ndimensions that have yet to be explored. In essence, the problem is that\ncertain forms of sentences like \"Some Italians come from a warm country,\" when\nconjoined as in \"(Only) Some Italians come from a warm country and are blond,\"\nsound infelicitous, even though no obvious alternative triggers a conflicting\nscalar implicature. In this paper, we offer a conceptual analysis of Magri's\nfirst puzzle by situating it within its original theoretical framework. We\nargue that the oddness arises from the collective or concurrent reading of the\nconjunctive predicate: in examples such as \"(Only) Some Italians come from a\nwarm country and are blond,\" this interpretation generates an indirect\ncontextual contradiction. Moreover, we suggest that the pragmatic mechanisms\ngoverning scalar implicature generation extend beyond what is captured by\nexhaustification-based grammatical licensing accounts.", "AI": {"tldr": "The paper analyzes Magri's unresolved puzzle about conjunction, focusing on infelicitous sentences like \"(Only) Some Italians come from a warm country and are blond.\" It attributes the oddness to collective/concurrent readings and suggests broader pragmatic mechanisms.", "motivation": "To address Magri's first puzzle about conjunction, which involves unexplained infelicity in certain sentences, and explore hidden interactions among quantification, interpretation, and context.", "method": "Conceptual analysis within Magri's theoretical framework, examining collective/concurrent interpretations and pragmatic mechanisms.", "result": "The oddness arises from indirect contextual contradictions in collective/concurrent readings, implicating broader pragmatic mechanisms than current accounts capture.", "conclusion": "The study highlights limitations in exhaustification-based accounts and suggests extending pragmatic theories to explain such conjunction puzzles."}}
{"id": "2506.07327", "pdf": "https://arxiv.org/pdf/2506.07327", "abs": "https://arxiv.org/abs/2506.07327", "authors": ["Dane Williamson", "Yangfeng Ji", "Matthew Dwyer"], "title": "\"CASE: Contrastive Activation for Saliency Estimation", "categories": ["cs.CV", "cs.LG", "I.2.6; I.5.1; I.5.5; I.2.10"], "comment": "9 pages, 5 figures. Submitted to IEEE Transactions on Neural Networks\n  and Learning Systems (TNNLS)", "summary": "Saliency methods are widely used to visualize which input features are deemed\nrelevant to a model's prediction. However, their visual plausibility can\nobscure critical limitations. In this work, we propose a diagnostic test for\nclass sensitivity: a method's ability to distinguish between competing class\nlabels on the same input. Through extensive experiments, we show that many\nwidely used saliency methods produce nearly identical explanations regardless\nof the class label, calling into question their reliability. We find that\nclass-insensitive behavior persists across architectures and datasets,\nsuggesting the failure mode is structural rather than model-specific. Motivated\nby these findings, we introduce CASE, a contrastive explanation method that\nisolates features uniquely discriminative for the predicted class. We evaluate\nCASE using the proposed diagnostic and a perturbation-based fidelity test, and\nshow that it produces faithful and more class-specific explanations than\nexisting methods.", "AI": {"tldr": "The paper critiques saliency methods for lacking class sensitivity, introduces a diagnostic test, and proposes CASE, a contrastive method for more reliable explanations.", "motivation": "To address the unreliability of saliency methods in distinguishing between class labels, despite their visual plausibility.", "method": "Proposes a diagnostic test for class sensitivity and introduces CASE, a contrastive explanation method.", "result": "Many saliency methods fail to distinguish class labels; CASE outperforms them in fidelity and class specificity.", "conclusion": "CASE provides more faithful and class-specific explanations, highlighting structural flaws in existing saliency methods."}}
{"id": "2506.07434", "pdf": "https://arxiv.org/pdf/2506.07434", "abs": "https://arxiv.org/abs/2506.07434", "authors": ["Feifan Song", "Shaohang Wei", "Wen Luo", "Yuxuan Fan", "Tianyu Liu", "Guoyin Wang", "Houfeng Wang"], "title": "Well Begun is Half Done: Low-resource Preference Alignment by Weak-to-Strong Decoding", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL 2025 Findings", "summary": "Large Language Models (LLMs) require alignment with human preferences to\navoid generating offensive, false, or meaningless content. Recently,\nlow-resource methods for LLM alignment have been popular, while still facing\nchallenges in obtaining both high-quality and aligned content. Motivated by the\nobservation that the difficulty of generating aligned responses is concentrated\nat the beginning of decoding, we propose a novel framework, Weak-to-Strong\nDecoding (WSD), to enhance the alignment ability of base models by the guidance\nof a small aligned model. The small model first drafts well-aligned beginnings,\nfollowed by the large base model to continue the rest, controlled by a\nwell-designed auto-switch mechanism. We also collect a new dataset, GenerAlign,\nto fine-tune a small-sized Pilot-3B as the draft model, which effectively\nenhances different base models under the WSD framework to outperform all\nbaseline methods, while avoiding degradation on downstream tasks, termed as the\nalignment tax. Extensive experiments are further conducted to examine the\nimpact of different settings and time efficiency, as well as analyses on the\nintrinsic mechanisms of WSD in depth.", "AI": {"tldr": "The paper introduces Weak-to-Strong Decoding (WSD), a framework to improve LLM alignment by using a small aligned model to draft well-aligned beginnings, followed by a larger base model.", "motivation": "To address challenges in low-resource LLM alignment, ensuring high-quality and aligned content without degrading downstream tasks.", "method": "Proposes WSD, where a small aligned model drafts aligned beginnings, and a larger base model continues, controlled by an auto-switch mechanism. Uses GenerAlign dataset to fine-tune Pilot-3B as the draft model.", "result": "WSD outperforms baseline methods, avoids alignment tax, and maintains downstream task performance.", "conclusion": "WSD effectively enhances LLM alignment, balancing quality and efficiency, with potential for broader applications."}}
{"id": "2506.07338", "pdf": "https://arxiv.org/pdf/2506.07338", "abs": "https://arxiv.org/abs/2506.07338", "authors": ["Yijie Deng", "Shuaihang Yuan", "Geeta Chandra Raju Bethala", "Anthony Tzes", "Yu-Shen Liu", "Yi Fang"], "title": "Hierarchical Scoring with 3D Gaussian Splatting for Instance Image-Goal Navigation", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Instance Image-Goal Navigation (IIN) requires autonomous agents to identify\nand navigate to a target object or location depicted in a reference image\ncaptured from any viewpoint. While recent methods leverage powerful novel view\nsynthesis (NVS) techniques, such as three-dimensional Gaussian splatting\n(3DGS), they typically rely on randomly sampling multiple viewpoints or\ntrajectories to ensure comprehensive coverage of discriminative visual cues.\nThis approach, however, creates significant redundancy through overlapping\nimage samples and lacks principled view selection, substantially increasing\nboth rendering and comparison overhead. In this paper, we introduce a novel IIN\nframework with a hierarchical scoring paradigm that estimates optimal\nviewpoints for target matching. Our approach integrates cross-level semantic\nscoring, utilizing CLIP-derived relevancy fields to identify regions with high\nsemantic similarity to the target object class, with fine-grained local\ngeometric scoring that performs precise pose estimation within promising\nregions. Extensive evaluations demonstrate that our method achieves\nstate-of-the-art performance on simulated IIN benchmarks and real-world\napplicability.", "AI": {"tldr": "A novel framework for Instance Image-Goal Navigation (IIN) introduces hierarchical scoring to optimize viewpoint selection, reducing redundancy and improving efficiency.", "motivation": "Current IIN methods rely on random viewpoint sampling, causing redundancy and inefficiency. A principled approach is needed to enhance performance.", "method": "The framework combines cross-level semantic scoring (using CLIP-derived relevancy fields) and fine-grained local geometric scoring for precise pose estimation.", "result": "Achieves state-of-the-art performance on simulated IIN benchmarks and demonstrates real-world applicability.", "conclusion": "The hierarchical scoring paradigm significantly improves efficiency and accuracy in IIN tasks."}}
{"id": "2506.07438", "pdf": "https://arxiv.org/pdf/2506.07438", "abs": "https://arxiv.org/abs/2506.07438", "authors": ["Jooyoung Choi", "Hyun Kim", "Hansol Jang", "Changwook Jun", "Kyunghoon Bae", "Hyewon Choi", "Stanley Jungkyu Choi", "Honglak Lee", "Chulmin Yun"], "title": "LG-ANNA-Embedding technical report", "categories": ["cs.CL"], "comment": "10 pages", "summary": "This report presents a unified instruction-based framework for learning\ngeneralized text embeddings optimized for both information retrieval (IR) and\nnon-IR tasks. Built upon a decoder-only large language model (Mistral-7B), our\napproach combines in-context learning, soft supervision, and adaptive\nhard-negative mining to generate context-aware embeddings without task-specific\nfine-tuning. Structured instructions and few-shot examples are used to guide\nthe model across diverse tasks, enabling strong performance on classification,\nsemantic similarity, clustering, and reranking benchmarks. To improve semantic\ndiscrimination, we employ a soft labeling framework where continuous relevance\nscores, distilled from a high-performance dense retriever and reranker, serve\nas fine-grained supervision signals. In addition, we introduce adaptive\nmargin-based hard-negative mining, which filters out semantically ambiguous\nnegatives based on their similarity to positive examples, thereby enhancing\ntraining stability and retrieval robustness. Our model is evaluated on the\nnewly introduced MTEB (English, v2) benchmark, covering 41 tasks across seven\ncategories. Results show that our method achieves strong generalization and\nranks among the top-performing models by Borda score, outperforming several\nlarger or fully fine-tuned baselines. These findings highlight the\neffectiveness of combining in-context prompting, soft supervision, and adaptive\nsampling for scalable, high-quality embedding generation.", "AI": {"tldr": "A unified framework for learning generalized text embeddings using in-context learning, soft supervision, and adaptive hard-negative mining, achieving strong performance on diverse tasks without task-specific fine-tuning.", "motivation": "To develop a scalable and high-quality embedding generation method that generalizes across both IR and non-IR tasks without requiring task-specific fine-tuning.", "method": "Combines in-context learning, soft supervision (using continuous relevance scores), and adaptive margin-based hard-negative mining with a decoder-only large language model (Mistral-7B).", "result": "Achieves strong generalization on the MTEB benchmark (41 tasks), ranking among top models and outperforming larger or fully fine-tuned baselines.", "conclusion": "The framework effectively combines in-context prompting, soft supervision, and adaptive sampling for scalable and high-quality embedding generation."}}
{"id": "2506.07357", "pdf": "https://arxiv.org/pdf/2506.07357", "abs": "https://arxiv.org/abs/2506.07357", "authors": ["Satvik Praveen", "Yoonsung Jung"], "title": "CBAM-STN-TPS-YOLO: Enhancing Agricultural Object Detection through Spatially Adaptive Attention Mechanisms", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Object detection is vital in precision agriculture for plant monitoring,\ndisease detection, and yield estimation. However, models like YOLO struggle\nwith occlusions, irregular structures, and background noise, reducing detection\naccuracy. While Spatial Transformer Networks (STNs) improve spatial invariance\nthrough learned transformations, affine mappings are insufficient for non-rigid\ndeformations such as bent leaves and overlaps.\n  We propose CBAM-STN-TPS-YOLO, a model integrating Thin-Plate Splines (TPS)\ninto STNs for flexible, non-rigid spatial transformations that better align\nfeatures. Performance is further enhanced by the Convolutional Block Attention\nModule (CBAM), which suppresses background noise and emphasizes relevant\nspatial and channel-wise features.\n  On the occlusion-heavy Plant Growth and Phenotyping (PGP) dataset, our model\noutperforms STN-YOLO in precision, recall, and mAP. It achieves a 12% reduction\nin false positives, highlighting the benefits of improved spatial flexibility\nand attention-guided refinement. We also examine the impact of the TPS\nregularization parameter in balancing transformation smoothness and detection\nperformance.\n  This lightweight model improves spatial awareness and supports real-time edge\ndeployment, making it ideal for smart farming applications requiring accurate\nand efficient monitoring.", "AI": {"tldr": "CBAM-STN-TPS-YOLO integrates Thin-Plate Splines (TPS) and CBAM into YOLO for better object detection in agriculture, outperforming STN-YOLO with improved precision and reduced false positives.", "motivation": "Addressing limitations of YOLO and STNs in handling occlusions, irregular structures, and background noise in precision agriculture.", "method": "Combines TPS for non-rigid transformations and CBAM for noise suppression and feature emphasis.", "result": "Achieves higher precision, recall, and mAP on the PGP dataset, with a 12% reduction in false positives.", "conclusion": "The model enhances spatial flexibility and attention, supporting real-time edge deployment for smart farming."}}
{"id": "2506.07453", "pdf": "https://arxiv.org/pdf/2506.07453", "abs": "https://arxiv.org/abs/2506.07453", "authors": ["Pritom Saha Akash", "Kevin Chen-Chuan Chang"], "title": "Understanding Cross-Domain Adaptation in Low-Resource Topic Modeling", "categories": ["cs.CL"], "comment": null, "summary": "Topic modeling plays a vital role in uncovering hidden semantic structures\nwithin text corpora, but existing models struggle in low-resource settings\nwhere limited target-domain data leads to unstable and incoherent topic\ninference. We address this challenge by formally introducing domain adaptation\nfor low-resource topic modeling, where a high-resource source domain informs a\nlow-resource target domain without overwhelming it with irrelevant content. We\nestablish a finite-sample generalization bound showing that effective knowledge\ntransfer depends on robust performance in both domains, minimizing latent-space\ndiscrepancy, and preventing overfitting to the data. Guided by these insights,\nwe propose DALTA (Domain-Aligned Latent Topic Adaptation), a new framework that\nemploys a shared encoder for domain-invariant features, specialized decoders\nfor domain-specific nuances, and adversarial alignment to selectively transfer\nrelevant information. Experiments on diverse low-resource datasets demonstrate\nthat DALTA consistently outperforms state-of-the-art methods in terms of topic\ncoherence, stability, and transferability.", "AI": {"tldr": "DALTA introduces domain adaptation for low-resource topic modeling, outperforming state-of-the-art methods in coherence, stability, and transferability.", "motivation": "Existing topic models struggle in low-resource settings due to unstable and incoherent topic inference.", "method": "DALTA uses a shared encoder for domain-invariant features, specialized decoders for domain-specific nuances, and adversarial alignment for selective knowledge transfer.", "result": "Experiments show DALTA outperforms state-of-the-art methods in topic coherence, stability, and transferability.", "conclusion": "DALTA effectively addresses low-resource topic modeling challenges through domain adaptation."}}
{"id": "2506.07364", "pdf": "https://arxiv.org/pdf/2506.07364", "abs": "https://arxiv.org/abs/2506.07364", "authors": ["Chengchao Shen", "Dawei Liu", "Jianxin Wang"], "title": "Multiple Object Stitching for Unsupervised Representation Learning", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Contrastive learning for single object centric images has achieved remarkable\nprogress on unsupervised representation, but suffering inferior performance on\nthe widespread images with multiple objects. In this paper, we propose a simple\nbut effective method, Multiple Object Stitching (MOS), to refine the\nunsupervised representation for multi-object images. Specifically, we construct\nthe multi-object images by stitching the single object centric ones, where the\nobjects in the synthesized multi-object images are predetermined. Hence,\ncompared to the existing contrastive methods, our method provides additional\nobject correspondences between multi-object images without human annotations.\nIn this manner, our method pays more attention to the representations of each\nobject in multi-object image, thus providing more detailed representations for\ncomplicated downstream tasks, such as object detection and semantic\nsegmentation. Experimental results on ImageNet, CIFAR and COCO datasets\ndemonstrate that our proposed method achieves the leading unsupervised\nrepresentation performance on both single object centric images and\nmulti-object ones. The source code is available at\nhttps://github.com/visresearch/MultipleObjectStitching.", "AI": {"tldr": "The paper introduces Multiple Object Stitching (MOS), a method to improve unsupervised representation for multi-object images by stitching single-object images, outperforming existing contrastive learning methods.", "motivation": "Existing contrastive learning methods perform poorly on multi-object images, limiting their applicability in tasks like object detection and semantic segmentation.", "method": "MOS constructs multi-object images by stitching single-object ones, leveraging predetermined object correspondences without human annotations.", "result": "MOS achieves leading unsupervised representation performance on ImageNet, CIFAR, and COCO datasets for both single and multi-object images.", "conclusion": "MOS provides detailed object representations for complex tasks, advancing unsupervised learning for multi-object scenarios."}}
{"id": "2506.07458", "pdf": "https://arxiv.org/pdf/2506.07458", "abs": "https://arxiv.org/abs/2506.07458", "authors": ["Yuxin Xiao", "Shan Chen", "Jack Gallifant", "Danielle Bitterman", "Thomas Hartvigsen", "Marzyeh Ghassemi"], "title": "KScope: A Framework for Characterizing the Knowledge Status of Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Characterizing a large language model's (LLM's) knowledge of a given question\nis challenging. As a result, prior work has primarily examined LLM behavior\nunder knowledge conflicts, where the model's internal parametric memory\ncontradicts information in the external context. However, this does not fully\nreflect how well the model knows the answer to the question. In this paper, we\nfirst introduce a taxonomy of five knowledge statuses based on the consistency\nand correctness of LLM knowledge modes. We then propose KScope, a hierarchical\nframework of statistical tests that progressively refines hypotheses about\nknowledge modes and characterizes LLM knowledge into one of these five\nstatuses. We apply KScope to nine LLMs across four datasets and systematically\nestablish: (1) Supporting context narrows knowledge gaps across models. (2)\nContext features related to difficulty, relevance, and familiarity drive\nsuccessful knowledge updates. (3) LLMs exhibit similar feature preferences when\npartially correct or conflicted, but diverge sharply when consistently wrong.\n(4) Context summarization constrained by our feature analysis, together with\nenhanced credibility, further improves update effectiveness and generalizes\nacross LLMs.", "AI": {"tldr": "The paper introduces KScope, a framework to classify LLM knowledge into five statuses and analyzes how context affects knowledge updates across models.", "motivation": "To better understand LLM knowledge beyond conflicts, by characterizing its consistency and correctness.", "method": "Proposes KScope, a hierarchical framework of statistical tests to classify LLM knowledge into five statuses, applied to nine LLMs across four datasets.", "result": "Finds that context narrows knowledge gaps, specific features drive updates, and LLMs diverge in behavior when consistently wrong. Summarization and credibility improve updates.", "conclusion": "KScope effectively characterizes LLM knowledge, with context and feature analysis enhancing update effectiveness across models."}}
{"id": "2506.07368", "pdf": "https://arxiv.org/pdf/2506.07368", "abs": "https://arxiv.org/abs/2506.07368", "authors": ["Jiaying He", "Yitong Lin", "Jiahe Chen", "Honghui Xu", "Jianwei Zheng"], "title": "C3S3: Complementary Competition and Contrastive Selection for Semi-Supervised Medical Image Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": "6 pages, 4 figures, ICME2025", "summary": "For the immanent challenge of insufficiently annotated samples in the medical\nfield, semi-supervised medical image segmentation (SSMIS) offers a promising\nsolution. Despite achieving impressive results in delineating primary target\nareas, most current methodologies struggle to precisely capture the subtle\ndetails of boundaries. This deficiency often leads to significant diagnostic\ninaccuracies. To tackle this issue, we introduce C3S3, a novel semi-supervised\nsegmentation model that synergistically integrates complementary competition\nand contrastive selection. This design significantly sharpens boundary\ndelineation and enhances overall precision. Specifically, we develop an\n$\\textit{Outcome-Driven Contrastive Learning}$ module dedicated to refining\nboundary localization. Additionally, we incorporate a $\\textit{Dynamic\nComplementary Competition}$ module that leverages two high-performing\nsub-networks to generate pseudo-labels, thereby further improving segmentation\nquality. The proposed C3S3 undergoes rigorous validation on two publicly\naccessible datasets, encompassing the practices of both MRI and CT scans. The\nresults demonstrate that our method achieves superior performance compared to\nprevious cutting-edge competitors. Especially, on the 95HD and ASD metrics, our\napproach achieves a notable improvement of at least $6\\%$, highlighting the\nsignificant advancements. The code is available at\nhttps://github.com/Y-TARL/C3S3.", "AI": {"tldr": "C3S3 is a semi-supervised medical image segmentation model that improves boundary delineation and precision using complementary competition and contrastive selection.", "motivation": "Addressing the challenge of insufficient annotated samples and poor boundary detail capture in medical image segmentation.", "method": "Integrates Outcome-Driven Contrastive Learning for boundary refinement and Dynamic Complementary Competition for pseudo-label generation.", "result": "Achieves superior performance on MRI and CT datasets, with at least 6% improvement on 95HD and ASD metrics.", "conclusion": "C3S3 significantly advances semi-supervised segmentation, particularly in boundary accuracy."}}
{"id": "2506.07461", "pdf": "https://arxiv.org/pdf/2506.07461", "abs": "https://arxiv.org/abs/2506.07461", "authors": ["Siddartha Devic", "Tejas Srinivasan", "Jesse Thomason", "Willie Neiswanger", "Vatsal Sharan"], "title": "From Calibration to Collaboration: LLM Uncertainty Quantification Should Be More Human-Centered", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly assisting users in the real\nworld, yet their reliability remains a concern. Uncertainty quantification (UQ)\nhas been heralded as a tool to enhance human-LLM collaboration by enabling\nusers to know when to trust LLM predictions. We argue that current practices\nfor uncertainty quantification in LLMs are not optimal for developing useful UQ\nfor human users making decisions in real-world tasks. Through an analysis of 40\nLLM UQ methods, we identify three prevalent practices hindering the community's\nprogress toward its goal of benefiting downstream users: 1) evaluating on\nbenchmarks with low ecological validity; 2) considering only epistemic\nuncertainty; and 3) optimizing metrics that are not necessarily indicative of\ndownstream utility. For each issue, we propose concrete user-centric practices\nand research directions that LLM UQ researchers should consider. Instead of\nhill-climbing on unrepresentative tasks using imperfect metrics, we argue that\nthe community should adopt a more human-centered approach to LLM uncertainty\nquantification.", "AI": {"tldr": "The paper critiques current LLM uncertainty quantification (UQ) methods, identifying three flaws: low ecological validity benchmarks, focus on epistemic uncertainty, and non-utility metrics. It advocates for user-centric UQ practices.", "motivation": "To improve human-LLM collaboration by addressing shortcomings in current UQ methods for LLMs, ensuring reliability and trust in real-world applications.", "method": "Analyzed 40 LLM UQ methods, identifying three key issues and proposing user-centric solutions.", "result": "Found current UQ practices are misaligned with real-world user needs, suggesting a shift toward human-centered approaches.", "conclusion": "Advocates for adopting user-centric UQ practices to enhance LLM reliability and trust in real-world decision-making."}}
{"id": "2506.07369", "pdf": "https://arxiv.org/pdf/2506.07369", "abs": "https://arxiv.org/abs/2506.07369", "authors": ["Bolin Chen", "Shanzhi Yin", "Goluck Konuko", "Giuseppe Valenzise", "Zihan Zhang", "Shiqi Wang", "Yan Ye"], "title": "Generative Models at the Frontier of Compression: A Survey on Generative Face Video Coding", "categories": ["cs.CV"], "comment": null, "summary": "The rise of deep generative models has greatly advanced video compression,\nreshaping the paradigm of face video coding through their powerful capability\nfor semantic-aware representation and lifelike synthesis. Generative Face Video\nCoding (GFVC) stands at the forefront of this revolution, which could\ncharacterize complex facial dynamics into compact latent codes for bitstream\ncompactness at the encoder side and leverages powerful deep generative models\nto reconstruct high-fidelity face signal from the compressed latent codes at\nthe decoder side. As such, this well-designed GFVC paradigm could enable\nhigh-fidelity face video communication at ultra-low bitrate ranges, far\nsurpassing the capabilities of the latest Versatile Video Coding (VVC)\nstandard. To pioneer foundational research and accelerate the evolution of\nGFVC, this paper presents the first comprehensive survey of GFVC technologies,\nsystematically bridging critical gaps between theoretical innovation and\nindustrial standardization. In particular, we first review a broad range of\nexisting GFVC methods with different feature representations and optimization\nstrategies, and conduct a thorough benchmarking analysis. In addition, we\nconstruct a large-scale GFVC-compressed face video database with subjective\nMean Opinion Scores (MOSs) based on human perception, aiming to identify the\nmost appropriate quality metrics tailored to GFVC. Moreover, we summarize the\nGFVC standardization potentials with a unified high-level syntax and develop a\nlow-complexity GFVC system which are both expected to push forward future\npractical deployments and applications. Finally, we envision the potential of\nGFVC in industrial applications and deliberate on the current challenges and\nfuture opportunities.", "AI": {"tldr": "The paper surveys Generative Face Video Coding (GFVC), a deep generative model-based approach for ultra-low bitrate face video compression, outperforming VVC. It reviews methods, benchmarks, and standardization potential.", "motivation": "To advance GFVC by bridging theory and standardization, addressing gaps in face video compression using deep generative models.", "method": "Comprehensive survey of GFVC methods, benchmarking, creation of a GFVC-compressed video database with MOS, and development of a low-complexity system.", "result": "GFVC enables high-fidelity face video at ultra-low bitrates, surpassing VVC. A standardized framework and quality metrics are proposed.", "conclusion": "GFVC has strong industrial potential but faces challenges; future work includes standardization and practical deployment."}}
{"id": "2506.07463", "pdf": "https://arxiv.org/pdf/2506.07463", "abs": "https://arxiv.org/abs/2506.07463", "authors": ["Guang Liu", "Liangdong Wang", "Jijie Li", "Yang Yu", "Yao Xu", "Jiabei Chen", "Yu Bai", "Feng Liao", "Yonghua Lin"], "title": "CCI4.0: A Bilingual Pretraining Dataset for Enhancing Reasoning in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We introduce CCI4.0, a large-scale bilingual pre-training dataset engineered\nfor superior data quality and diverse human-like reasoning trajectory. CCI4.0\noccupies roughly $35$ TB of disk space and comprises two sub-datasets:\nCCI4.0-M2-Base and CCI4.0-M2-CoT. CCI4.0-M2-Base combines a $5.2$ TB carefully\ncurated Chinese web corpus, a $22.5$ TB English subset from Nemotron-CC, and\ndiverse sources from math, wiki, arxiv, and code. Although these data are\nmostly sourced from well-processed datasets, the quality standards of various\ndomains are dynamic and require extensive expert experience and labor to\nprocess. So, we propose a novel pipeline justifying data quality mainly based\non models through two-stage deduplication, multiclassifier quality scoring, and\ndomain-aware fluency filtering. We extract $4.5$ billion pieces of\nCoT(Chain-of-Thought) templates, named CCI4.0-M2-CoT. Differing from the\ndistillation of CoT from larger models, our proposed staged CoT extraction\nexemplifies diverse reasoning patterns and significantly decreases the\npossibility of hallucination. Empirical evaluations demonstrate that LLMs\npre-trained in CCI4.0 benefit from cleaner, more reliable training signals,\nyielding consistent improvements in downstream tasks, especially in math and\ncode reflection tasks. Our results underscore the critical role of rigorous\ndata curation and human thinking templates in advancing LLM performance,\nshedding some light on automatically processing pretraining corpora.", "AI": {"tldr": "CCI4.0 is a large-scale bilingual dataset for pre-training, featuring high-quality data and diverse reasoning templates, improving LLM performance in tasks like math and coding.", "motivation": "To address the need for superior data quality and diverse reasoning in pre-training datasets, ensuring cleaner and more reliable training signals for LLMs.", "method": "Proposes a pipeline for data quality justification, including deduplication, quality scoring, and fluency filtering, and introduces staged CoT extraction for diverse reasoning patterns.", "result": "LLMs pre-trained on CCI4.0 show consistent improvements in downstream tasks, particularly in math and code reflection.", "conclusion": "Rigorous data curation and human-like reasoning templates are crucial for advancing LLM performance, offering insights into automated corpus processing."}}
{"id": "2506.07371", "pdf": "https://arxiv.org/pdf/2506.07371", "abs": "https://arxiv.org/abs/2506.07371", "authors": ["Ruchit Rawal", "Reza Shirkavand", "Heng Huang", "Gowthami Somepalli", "Tom Goldstein"], "title": "ARGUS: Hallucination and Omission Evaluation in Video-LLMs", "categories": ["cs.CV"], "comment": "Project page with all the artifacts:\n  https://ruchitrawal.github.io/argus", "summary": "Video large language models have not yet been widely deployed, largely due to\ntheir tendency to hallucinate. Typical benchmarks for Video-LLMs rely simply on\nmultiple-choice questions. Unfortunately, VideoLLMs hallucinate far more\naggressively on freeform text generation tasks like video captioning than they\ndo on multiple choice verification tasks. To address this weakness, we propose\nARGUS, a VideoLLM benchmark that measures freeform video captioning\nperformance. By comparing VideoLLM outputs to human ground truth captions,\nARGUS quantifies dual metrics. First, we measure the rate of hallucinations in\nthe form of incorrect statements about video content or temporal relationships.\nSecond, we measure the rate at which the model omits important descriptive\ndetails. Together, these dual metrics form a comprehensive view of video\ncaptioning performance.", "AI": {"tldr": "ARGUS is a benchmark for VideoLLMs to measure freeform video captioning performance, addressing hallucinations and omissions in outputs.", "motivation": "VideoLLMs hallucinate aggressively in freeform tasks like captioning, which isn't captured by typical multiple-choice benchmarks.", "method": "ARGUS compares VideoLLM outputs to human ground truth captions, quantifying hallucinations (incorrect statements) and omissions (missing details).", "result": "The benchmark provides dual metrics: hallucination rate and omission rate, offering a comprehensive view of captioning performance.", "conclusion": "ARGUS addresses a critical gap in evaluating VideoLLMs, ensuring better assessment of their freeform text generation capabilities."}}
{"id": "2506.07479", "pdf": "https://arxiv.org/pdf/2506.07479", "abs": "https://arxiv.org/abs/2506.07479", "authors": ["Haoyuan Li Yusen Zhang", "Snigdha Chaturvedi"], "title": "Improving Fairness of Large Language Models in Multi-document Summarization", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 main", "summary": "Fairness in multi-document summarization (MDS) is crucial for providing\ncomprehensive views across documents with diverse social attribute values,\nwhich can significantly impact decision-making. For example, a summarization\nsystem that tends to overrepresent negative reviews of products can mislead\ncustomers into disregarding good products. Previous works measure fairness in\nMDS at two levels: summary-level and corpus-level. While summary-level fairness\nfocuses on individual summaries, corpus-level fairness focuses on a corpus of\nsummaries. Recent methods primarily focus on summary-level fairness. We propose\nFairPO, a preference tuning method that focuses on both summary-level and\ncorpus-level fairness in MDS. To improve summary-level fairness, we propose to\ngenerate preference pairs by perturbing document sets. To improve corpus-level\nfairness, we propose fairness-aware preference tuning by dynamically adjusting\nthe weights of preference pairs. Our experiments show that FairPO outperforms\nstrong baselines while maintaining the critical qualities of summaries. The\ncode is available at https://github.com/leehaoyuan/coverage_fairnes.", "AI": {"tldr": "FairPO is a method for improving fairness in multi-document summarization (MDS) at both summary and corpus levels, outperforming baselines while maintaining summary quality.", "motivation": "Fairness in MDS is critical to avoid biased representations, such as overrepresenting negative reviews, which can mislead decision-making.", "method": "FairPO uses preference tuning: perturbing document sets for summary-level fairness and dynamically adjusting weights for corpus-level fairness.", "result": "FairPO outperforms baselines in fairness while preserving summary quality.", "conclusion": "FairPO effectively addresses fairness in MDS at both levels, offering a balanced approach for unbiased summarization."}}
{"id": "2506.07375", "pdf": "https://arxiv.org/pdf/2506.07375", "abs": "https://arxiv.org/abs/2506.07375", "authors": ["Xunjie He", "Christina Dao Wen Lee", "Meiling Wang", "Chengran Yuan", "Zefan Huang", "Yufeng Yue", "Marcelo H. Ang Jr"], "title": "DINO-CoDT: Multi-class Collaborative Detection and Tracking with Vision Foundation Models", "categories": ["cs.CV"], "comment": null, "summary": "Collaborative perception plays a crucial role in enhancing environmental\nunderstanding by expanding the perceptual range and improving robustness\nagainst sensor failures, which primarily involves collaborative 3D detection\nand tracking tasks. The former focuses on object recognition in individual\nframes, while the latter captures continuous instance tracklets over time.\nHowever, existing works in both areas predominantly focus on the vehicle\nsuperclass, lacking effective solutions for both multi-class collaborative\ndetection and tracking. This limitation hinders their applicability in\nreal-world scenarios, which involve diverse object classes with varying\nappearances and motion patterns. To overcome these limitations, we propose a\nmulti-class collaborative detection and tracking framework tailored for diverse\nroad users. We first present a detector with a global spatial attention fusion\n(GSAF) module, enhancing multi-scale feature learning for objects of varying\nsizes. Next, we introduce a tracklet RE-IDentification (REID) module that\nleverages visual semantics with a vision foundation model to effectively reduce\nID SWitch (IDSW) errors, in cases of erroneous mismatches involving small\nobjects like pedestrians. We further design a velocity-based adaptive tracklet\nmanagement (VATM) module that adjusts the tracking interval dynamically based\non object motion. Extensive experiments on the V2X-Real and OPV2V datasets show\nthat our approach significantly outperforms existing state-of-the-art methods\nin both detection and tracking accuracy.", "AI": {"tldr": "A multi-class collaborative detection and tracking framework is proposed to address limitations in existing methods, enhancing accuracy for diverse road users.", "motivation": "Existing works focus on vehicle superclass, lacking solutions for multi-class scenarios, limiting real-world applicability.", "method": "Proposes a detector with a GSAF module for multi-scale feature learning, a REID module to reduce IDSW errors, and a VATM module for dynamic tracking.", "result": "Outperforms state-of-the-art methods on V2X-Real and OPV2V datasets in detection and tracking accuracy.", "conclusion": "The framework effectively addresses multi-class challenges, improving robustness and applicability in real-world scenarios."}}
{"id": "2506.07483", "pdf": "https://arxiv.org/pdf/2506.07483", "abs": "https://arxiv.org/abs/2506.07483", "authors": ["Berry Feng", "Jonas Lin", "Patrick Lau"], "title": "A Hybrid GA LLM Framework for Structured Task Optimization", "categories": ["cs.CL"], "comment": "7 pages", "summary": "GA LLM is a hybrid framework that combines Genetic Algorithms with Large\nLanguage Models to handle structured generation tasks under strict constraints.\nEach output, such as a plan or report, is treated as a gene, and evolutionary\noperations like selection, crossover, and mutation are guided by the language\nmodel to iteratively improve solutions. The language model provides domain\nknowledge and creative variation, while the genetic algorithm ensures\nstructural integrity and global optimization. GA LLM has proven effective in\ntasks such as itinerary planning, academic outlining, and business reporting,\nconsistently producing well structured and requirement satisfying results. Its\nmodular design also makes it easy to adapt to new tasks. Compared to using a\nlanguage model alone, GA LLM achieves better constraint satisfaction and higher\nquality solutions by combining the strengths of both components.", "AI": {"tldr": "GA LLM combines Genetic Algorithms and Large Language Models for structured generation tasks, improving constraint satisfaction and solution quality.", "motivation": "To address the limitations of using language models alone for structured tasks by integrating genetic algorithms for better optimization and structural integrity.", "method": "Hybrid framework where outputs are treated as genes; evolutionary operations (selection, crossover, mutation) are guided by the language model for iterative improvement.", "result": "Effective in tasks like itinerary planning, academic outlining, and business reporting, producing well-structured and requirement-satisfying results.", "conclusion": "GA LLM outperforms standalone language models by leveraging the strengths of both genetic algorithms and language models for structured generation."}}
{"id": "2506.07376", "pdf": "https://arxiv.org/pdf/2506.07376", "abs": "https://arxiv.org/abs/2506.07376", "authors": ["Jintao Tong", "Ran Ma", "Yixiong Zou", "Guangyao Chen", "Yuhua Li", "Ruixuan Li"], "title": "Adapter Naturally Serves as Decoupler for Cross-Domain Few-Shot Semantic Segmentation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "ICML 2025 Spotlight", "summary": "Cross-domain few-shot segmentation (CD-FSS) is proposed to pre-train the\nmodel on a source-domain dataset with sufficient samples, and then transfer the\nmodel to target-domain datasets where only a few samples are available for\nefficient fine-tuning. There are majorly two challenges in this task: (1) the\ndomain gap and (2) fine-tuning with scarce data. To solve these challenges, we\nrevisit the adapter-based methods, and discover an intriguing insight not\nexplored in previous works: the adapter not only helps the fine-tuning of\ndownstream tasks but also naturally serves as a domain information decoupler.\nThen, we delve into this finding for an interpretation, and find the model's\ninherent structure could lead to a natural decoupling of domain information.\nBuilding upon this insight, we propose the Domain Feature Navigator (DFN),\nwhich is a structure-based decoupler instead of loss-based ones like current\nworks, to capture domain-specific information, thereby directing the model's\nattention towards domain-agnostic knowledge. Moreover, to prevent the potential\nexcessive overfitting of DFN during the source-domain training, we further\ndesign the SAM-SVN method to constrain DFN from learning sample-specific\nknowledge. On target domains, we freeze the model and fine-tune the DFN to\nlearn target-specific knowledge specific. Extensive experiments demonstrate\nthat our method surpasses the state-of-the-art method in CD-FSS significantly\nby 2.69% and 4.68% MIoU in 1-shot and 5-shot scenarios, respectively.", "AI": {"tldr": "The paper proposes a Domain Feature Navigator (DFN) for cross-domain few-shot segmentation, addressing domain gaps and scarce data by decoupling domain information.", "motivation": "To tackle challenges in cross-domain few-shot segmentation (CD-FSS), such as domain gaps and limited fine-tuning data, by leveraging adapter-based methods for domain decoupling.", "method": "Introduces DFN, a structure-based decoupler, and SAM-SVN to prevent overfitting. DFN is fine-tuned on target domains while freezing the model.", "result": "Achieves significant improvements over state-of-the-art methods, with 2.69% and 4.68% MIoU gains in 1-shot and 5-shot scenarios.", "conclusion": "DFN effectively decouples domain-specific information, enhancing performance in CD-FSS tasks."}}
{"id": "2506.07502", "pdf": "https://arxiv.org/pdf/2506.07502", "abs": "https://arxiv.org/abs/2506.07502", "authors": ["Haotian Guo", "Jing Han", "Yongfeng Tu", "Shihao Gao", "Shengfan Shen", "Wulong Xiang", "Weihao Gan", "Zixing Zhang"], "title": "DEBATE: A Dataset for Disentangling Textual Ambiguity in Mandarin Through Speech", "categories": ["cs.CL"], "comment": null, "summary": "Despite extensive research on textual and visual disambiguation,\ndisambiguation through speech (DTS) remains underexplored. This is largely due\nto the lack of high-quality datasets that pair spoken sentences with richly\nambiguous text. To address this gap, we present DEBATE, a unique public Chinese\nspeech-text dataset designed to study how speech cues and\npatterns-pronunciation, pause, stress and intonation-can help resolve textual\nambiguity and reveal a speaker's true intent. DEBATE contains 1,001 carefully\nselected ambiguous utterances, each recorded by 10 native speakers, capturing\ndiverse linguistic ambiguities and their disambiguation through speech. We\ndetail the data collection pipeline and provide rigorous quality analysis.\nAdditionally, we benchmark three state-of-the-art large speech and\naudio-language models, illustrating clear and huge performance gaps between\nmachine and human understanding of spoken intent. DEBATE represents the first\neffort of its kind and offers a foundation for building similar DTS datasets\nacross languages and cultures. The dataset and associated code are available\nat: https://github.com/SmileHnu/DEBATE.", "AI": {"tldr": "The paper introduces DEBATE, a Chinese speech-text dataset for studying disambiguation through speech (DTS), highlighting gaps in existing research and benchmarking models.", "motivation": "To address the lack of high-quality datasets for studying how speech cues resolve textual ambiguity.", "method": "Created DEBATE, a dataset of 1,001 ambiguous utterances recorded by 10 native speakers, with detailed data collection and quality analysis.", "result": "Benchmarked models show significant performance gaps between machine and human understanding of spoken intent.", "conclusion": "DEBATE is a pioneering dataset for DTS research, with potential for cross-language and cultural applications."}}
{"id": "2506.07399", "pdf": "https://arxiv.org/pdf/2506.07399", "abs": "https://arxiv.org/abs/2506.07399", "authors": ["Peiru Yang", "Jinhua Yin", "Haoran Zheng", "Xueying Bai", "Huili Wang", "Yufei Sun", "Xintian Li", "Shangguang Wang", "Yongfeng Huang", "Tao Qi"], "title": "MrM: Black-Box Membership Inference Attacks against Multimodal RAG Systems", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multimodal retrieval-augmented generation (RAG) systems enhance large\nvision-language models by integrating cross-modal knowledge, enabling their\nincreasing adoption across real-world multimodal tasks. These knowledge\ndatabases may contain sensitive information that requires privacy protection.\nHowever, multimodal RAG systems inherently grant external users indirect access\nto such data, making them potentially vulnerable to privacy attacks,\nparticularly membership inference attacks (MIAs). % Existing MIA methods\ntargeting RAG systems predominantly focus on the textual modality, while the\nvisual modality remains relatively underexplored. To bridge this gap, we\npropose MrM, the first black-box MIA framework targeted at multimodal RAG\nsystems. It utilizes a multi-object data perturbation framework constrained by\ncounterfactual attacks, which can concurrently induce the RAG systems to\nretrieve the target data and generate information that leaks the membership\ninformation. Our method first employs an object-aware data perturbation method\nto constrain the perturbation to key semantics and ensure successful retrieval.\nBuilding on this, we design a counterfact-informed mask selection strategy to\nprioritize the most informative masked regions, aiming to eliminate the\ninterference of model self-knowledge and amplify attack efficacy. Finally, we\nperform statistical membership inference by modeling query trials to extract\nfeatures that reflect the reconstruction of masked semantics from response\npatterns. Experiments on two visual datasets and eight mainstream commercial\nvisual-language models (e.g., GPT-4o, Gemini-2) demonstrate that MrM achieves\nconsistently strong performance across both sample-level and set-level\nevaluations, and remains robust under adaptive defenses.", "AI": {"tldr": "MrM is a black-box membership inference attack framework for multimodal RAG systems, addressing privacy vulnerabilities by leveraging data perturbation and counterfactual attacks.", "motivation": "Multimodal RAG systems risk exposing sensitive data to privacy attacks, especially in the visual modality, which is underexplored.", "method": "MrM uses object-aware data perturbation, counterfact-informed mask selection, and statistical inference to leak membership information.", "result": "MrM performs strongly across datasets and models, including GPT-4o and Gemini-2, and resists adaptive defenses.", "conclusion": "MrM effectively bridges the gap in visual modality privacy attacks, demonstrating robustness and efficacy in real-world scenarios."}}
{"id": "2506.07506", "pdf": "https://arxiv.org/pdf/2506.07506", "abs": "https://arxiv.org/abs/2506.07506", "authors": ["Muhammad Dehan Al Kautsar", "Lucky Susanto", "Derry Wijaya", "Fajri Koto"], "title": "What Do Indonesians Really Need from Language Technology? A Nationwide Survey", "categories": ["cs.CL"], "comment": "26 pages, 12 figures, 5 tables", "summary": "There is an emerging effort to develop NLP for Indonesias 700+ local\nlanguages, but progress remains costly due to the need for direct engagement\nwith native speakers. However, it is unclear what these language communities\ntruly need from language technology. To address this, we conduct a nationwide\nsurvey to assess the actual needs of native speakers in Indonesia. Our findings\nindicate that addressing language barriers, particularly through machine\ntranslation and information retrieval, is the most critical priority. Although\nthere is strong enthusiasm for advancements in language technology, concerns\naround privacy, bias, and the use of public data for AI training highlight the\nneed for greater transparency and clear communication to support broader AI\nadoption.", "AI": {"tldr": "Survey in Indonesia reveals machine translation and information retrieval are top priorities for local language NLP, but privacy and bias concerns need addressing.", "motivation": "To understand the actual needs of native speakers in Indonesia for language technology, as current NLP efforts are costly and lack clarity on community priorities.", "method": "Conducted a nationwide survey to assess the needs and concerns of native speakers regarding language technology.", "result": "Machine translation and information retrieval are critical needs, but privacy, bias, and data transparency are significant concerns.", "conclusion": "Clear communication and transparency are essential to address concerns and support broader AI adoption in Indonesia's local languages."}}
{"id": "2506.07412", "pdf": "https://arxiv.org/pdf/2506.07412", "abs": "https://arxiv.org/abs/2506.07412", "authors": ["Changsheng Gao", "Wei Zhou", "Guosheng Lin", "Weisi Lin"], "title": "Compressed Feature Quality Assessment: Dataset and Baselines", "categories": ["cs.CV"], "comment": null, "summary": "The widespread deployment of large models in resource-constrained\nenvironments has underscored the need for efficient transmission of\nintermediate feature representations. In this context, feature coding, which\ncompresses features into compact bitstreams, becomes a critical component for\nscenarios involving feature transmission, storage, and reuse. However, this\ncompression process introduces inherent semantic degradation that is\nnotoriously difficult to quantify with traditional metrics. To address this,\nthis paper introduces the research problem of Compressed Feature Quality\nAssessment (CFQA), which seeks to evaluate the semantic fidelity of compressed\nfeatures. To advance CFQA research, we propose the first benchmark dataset,\ncomprising 300 original features and 12000 compressed features derived from\nthree vision tasks and four feature codecs. Task-specific performance drops are\nprovided as true semantic distortion for the evaluation of CFQA metrics. We\nassess the performance of three widely used metrics (MSE, cosine similarity,\nand Centered Kernel Alignment) in capturing semantic degradation. The results\nunderscore the representativeness of the dataset and highlight the need for\nmore refined metrics capable of addressing the nuances of semantic distortion\nin compressed features. To facilitate the ongoing development of CFQA research,\nwe release the dataset and all accompanying source code at\n\\href{https://github.com/chansongoal/Compressed-Feature-Quality-Assessment}{https://github.com/chansongoal/Compressed-Feature-Quality-Assessment}.\nThis contribution aims to advance the field and provide a foundational resource\nfor the community to explore CFQA.", "AI": {"tldr": "The paper introduces Compressed Feature Quality Assessment (CFQA) to evaluate semantic fidelity in compressed features, proposes a benchmark dataset, and assesses existing metrics, highlighting the need for better evaluation tools.", "motivation": "The need for efficient feature transmission in resource-constrained environments and the lack of metrics to quantify semantic degradation in compressed features.", "method": "Creation of a benchmark dataset with original and compressed features from vision tasks, evaluation of three metrics (MSE, cosine similarity, Centered Kernel Alignment).", "result": "The dataset is representative, but existing metrics fall short in capturing semantic distortion, indicating a need for refined tools.", "conclusion": "The paper provides a foundational resource for CFQA research, releasing the dataset and code to advance the field."}}
{"id": "2506.07510", "pdf": "https://arxiv.org/pdf/2506.07510", "abs": "https://arxiv.org/abs/2506.07510", "authors": ["Solee Im", "Wonjun Lee", "Jinmyeong An", "Yunsu Kim", "Jungseul Ok", "Gary Geunbae Lee"], "title": "DeRAGEC: Denoising Named Entity Candidates with Synthetic Rationale for ASR Error Correction", "categories": ["cs.CL"], "comment": "ACL2025 Findings", "summary": "We present DeRAGEC, a method for improving Named Entity (NE) correction in\nAutomatic Speech Recognition (ASR) systems. By extending the\nRetrieval-Augmented Generative Error Correction (RAGEC) framework, DeRAGEC\nemploys synthetic denoising rationales to filter out noisy NE candidates before\ncorrection. By leveraging phonetic similarity and augmented definitions, it\nrefines noisy retrieved NEs using in-context learning, requiring no additional\ntraining. Experimental results on CommonVoice and STOP datasets show\nsignificant improvements in Word Error Rate (WER) and NE hit ratio,\noutperforming baseline ASR and RAGEC methods. Specifically, we achieved a 28%\nrelative reduction in WER compared to ASR without postprocessing. Our source\ncode is publicly available at: https://github.com/solee0022/deragec", "AI": {"tldr": "DeRAGEC improves NE correction in ASR by filtering noisy candidates with synthetic denoising rationales, achieving a 28% WER reduction.", "motivation": "Enhance NE correction in ASR systems by addressing noisy NE candidates.", "method": "Extends RAGEC with synthetic denoising rationales, phonetic similarity, and augmented definitions, using in-context learning.", "result": "28% relative WER reduction and improved NE hit ratio on CommonVoice and STOP datasets.", "conclusion": "DeRAGEC outperforms baseline ASR and RAGEC, requiring no additional training."}}
{"id": "2506.07414", "pdf": "https://arxiv.org/pdf/2506.07414", "abs": "https://arxiv.org/abs/2506.07414", "authors": ["Sheng-Kai Huang", "Jiun-Feng Chang", "Chun-Rong Huang"], "title": "DPFormer: Dynamic Prompt Transformer for Continual Learning", "categories": ["cs.CV"], "comment": null, "summary": "In continual learning, solving the catastrophic forgetting problem may make\nthe models fall into the stability-plasticity dilemma. Moreover, inter-task\nconfusion will also occur due to the lack of knowledge exchanges between\ndifferent tasks. In order to solve the aforementioned problems, we propose a\nnovel dynamic prompt transformer (DPFormer) with prompt schemes. The prompt\nschemes help the DPFormer memorize learned knowledge of previous classes and\ntasks, and keep on learning new knowledge from new classes and tasks under a\nsingle network structure with a nearly fixed number of model parameters.\nMoreover, they also provide discrepant information to represent different tasks\nto solve the inter-task confusion problem. Based on prompt schemes, a unified\nclassification module with the binary cross entropy loss, the knowledge\ndistillation loss and the auxiliary loss is proposed to train the whole model\nin an end-to-end trainable manner. Compared with state-of-the-art methods, our\nmethod achieves the best performance in the CIFAR-100, ImageNet100 and\nImageNet1K datasets under different class-incremental settings in continual\nlearning. The source code will be available at our GitHub after acceptance.", "AI": {"tldr": "Proposes DPFormer, a dynamic prompt transformer with prompt schemes to address catastrophic forgetting and inter-task confusion in continual learning, achieving top performance on benchmark datasets.", "motivation": "To tackle the stability-plasticity dilemma and inter-task confusion in continual learning by enabling knowledge retention and exchange.", "method": "Uses prompt schemes for memorization and learning, a unified classification module with multiple losses (binary cross entropy, knowledge distillation, auxiliary), and trains end-to-end.", "result": "Achieves best performance on CIFAR-100, ImageNet100, and ImageNet1K under class-incremental settings.", "conclusion": "DPFormer effectively addresses continual learning challenges with minimal parameter growth and high performance."}}
{"id": "2506.07523", "pdf": "https://arxiv.org/pdf/2506.07523", "abs": "https://arxiv.org/abs/2506.07523", "authors": ["Sahar Admoni", "Ofra Amir", "Assaf Hallak", "Yftah Ziser"], "title": "Towards Large Language Models with Self-Consistent Natural Language Explanations", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) seem to offer an easy path to interpretability:\njust ask them to explain their decisions. Yet, studies show that these post-hoc\nexplanations often misrepresent the true decision process, as revealed by\nmismatches in feature importance. Despite growing evidence of this\ninconsistency, no systematic solutions have emerged, partly due to the high\ncost of estimating feature importance, which limits evaluations to small\ndatasets. To address this, we introduce the Post-hoc Self-Consistency Bank\n(PSCB) - a large-scale benchmark of decisions spanning diverse tasks and\nmodels, each paired with LLM-generated explanations and corresponding feature\nimportance scores. Analysis of PSCB reveals that self-consistency scores barely\ndiffer between correct and incorrect predictions. We also show that the\nstandard metric fails to meaningfully distinguish between explanations. To\novercome this limitation, we propose an alternative metric that more\neffectively captures variation in explanation quality. We use it to fine-tune\nLLMs via Direct Preference Optimization (DPO), leading to significantly better\nalignment between explanations and decision-relevant features, even under\ndomain shift. Our findings point to a scalable path toward more trustworthy,\nself-consistent LLMs.", "AI": {"tldr": "The paper introduces PSCB, a benchmark for evaluating LLM-generated explanations, and proposes a new metric to improve explanation quality via DPO fine-tuning.", "motivation": "Address inconsistencies in LLM post-hoc explanations by providing a scalable evaluation method.", "method": "Develop PSCB benchmark, analyze self-consistency, propose a new metric, and fine-tune LLMs with DPO.", "result": "Self-consistency scores don't differ between correct/incorrect predictions; new metric improves explanation alignment.", "conclusion": "PSCB and DPO offer a scalable path to more trustworthy, self-consistent LLMs."}}
{"id": "2506.07431", "pdf": "https://arxiv.org/pdf/2506.07431", "abs": "https://arxiv.org/abs/2506.07431", "authors": ["Jie He", "Minglang Chen", "Minying Lu", "Bocheng Liang", "Junming Wei", "Guiyan Peng", "Jiaxi Chen", "Ying Tan"], "title": "FAMSeg: Fetal Femur and Cranial Ultrasound Segmentation Using Feature-Aware Attention and Mamba Enhancement", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Accurate ultrasound image segmentation is a prerequisite for precise\nbiometrics and accurate assessment. Relying on manual delineation introduces\nsignificant errors and is time-consuming. However, existing segmentation models\nare designed based on objects in natural scenes, making them difficult to adapt\nto ultrasound objects with high noise and high similarity. This is particularly\nevident in small object segmentation, where a pronounced jagged effect occurs.\nTherefore, this paper proposes a fetal femur and cranial ultrasound image\nsegmentation model based on feature perception and Mamba enhancement to address\nthese challenges. Specifically, a longitudinal and transverse independent\nviewpoint scanning convolution block and a feature perception module were\ndesigned to enhance the ability to capture local detail information and improve\nthe fusion of contextual information. Combined with the Mamba-optimized\nresidual structure, this design suppresses the interference of raw noise and\nenhances local multi-dimensional scanning. The system builds global information\nand local feature dependencies, and is trained with a combination of different\noptimizers to achieve the optimal solution. After extensive experimental\nvalidation, the FAMSeg network achieved the fastest loss reduction and the best\nsegmentation performance across images of varying sizes and orientations.", "AI": {"tldr": "Proposes FAMSeg, a model for fetal femur and cranial ultrasound image segmentation, using feature perception and Mamba enhancement to improve accuracy and reduce noise.", "motivation": "Manual ultrasound segmentation is error-prone and time-consuming; existing models struggle with noise and similarity in ultrasound images, especially for small objects.", "method": "Uses longitudinal and transverse independent viewpoint scanning convolution, feature perception module, and Mamba-optimized residual structure to enhance detail capture and noise suppression.", "result": "FAMSeg achieved fastest loss reduction and best segmentation performance across varying image sizes and orientations.", "conclusion": "The proposed model effectively addresses challenges in ultrasound segmentation, offering superior performance and efficiency."}}
{"id": "2506.07541", "pdf": "https://arxiv.org/pdf/2506.07541", "abs": "https://arxiv.org/abs/2506.07541", "authors": ["Sangwhan Moon", "Tatsuya Hiraoka", "Naoaki Okazaki"], "title": "Bit-level BPE: Below the byte boundary", "categories": ["cs.CL"], "comment": null, "summary": "Byte-level fallbacks for subword tokenization have become a common practice\nin large language models. In particular, it has been demonstrated to be\nincredibly effective as a pragmatic solution for preventing OOV, especially in\nthe context of larger models. However, breaking a character down to individual\nbytes significantly increases the sequence length for long-tail tokens in\nlanguages such as Chinese, Japanese, and Korean (CJK) and other\ncharacter-diverse contexts such as emoji. The increased sequence length results\nin longer computation during both training and inference. In this work, we\npropose a simple compression technique that reduces the sequence length\nlosslessly.", "AI": {"tldr": "A compression technique is proposed to reduce sequence length caused by byte-level fallbacks in subword tokenization, addressing inefficiencies in CJK and emoji contexts.", "motivation": "Byte-level fallbacks increase sequence length for long-tail tokens in CJK and emoji contexts, leading to computational inefficiencies.", "method": "A simple, lossless compression technique is introduced.", "result": "The technique reduces sequence length without losing information.", "conclusion": "The proposed method effectively mitigates computational inefficiencies in tokenization for diverse character sets."}}
{"id": "2506.07436", "pdf": "https://arxiv.org/pdf/2506.07436", "abs": "https://arxiv.org/abs/2506.07436", "authors": ["Nishi Chaudhary", "S M Jamil Uddin", "Sathvik Sharath Chandra", "Anto Ovid", "Alex Albert"], "title": "Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition", "categories": ["cs.CV", "cs.AI", "cs.ET"], "comment": null, "summary": "The recent emergence of multimodal large language models (LLMs) has\nintroduced new opportunities for improving visual hazard recognition on\nconstruction sites. Unlike traditional computer vision models that rely on\ndomain-specific training and extensive datasets, modern LLMs can interpret and\ndescribe complex visual scenes using simple natural language prompts. However,\ndespite growing interest in their applications, there has been limited\ninvestigation into how different LLMs perform in safety-critical visual tasks\nwithin the construction domain. To address this gap, this study conducts a\ncomparative evaluation of five state-of-the-art LLMs: Claude-3 Opus, GPT-4.5,\nGPT-4o, GPT-o3, and Gemini 2.0 Pro, to assess their ability to identify\npotential hazards from real-world construction images. Each model was tested\nunder three prompting strategies: zero-shot, few-shot, and chain-of-thought\n(CoT). Zero-shot prompting involved minimal instruction, few-shot incorporated\nbasic safety context and a hazard source mnemonic, and CoT provided\nstep-by-step reasoning examples to scaffold model thinking. Quantitative\nanalysis was performed using precision, recall, and F1-score metrics across all\nconditions. Results reveal that prompting strategy significantly influenced\nperformance, with CoT prompting consistently producing higher accuracy across\nmodels. Additionally, LLM performance varied under different conditions, with\nGPT-4.5 and GPT-o3 outperforming others in most settings. The findings also\ndemonstrate the critical role of prompt design in enhancing the accuracy and\nconsistency of multimodal LLMs for construction safety applications. This study\noffers actionable insights into the integration of prompt engineering and LLMs\nfor practical hazard recognition, contributing to the development of more\nreliable AI-assisted safety systems.", "AI": {"tldr": "The study compares five LLMs (Claude-3 Opus, GPT-4.5, GPT-4o, GPT-o3, Gemini 2.0 Pro) for visual hazard recognition in construction, testing zero-shot, few-shot, and CoT prompting. CoT performed best, with GPT-4.5 and GPT-o3 leading. Prompt design is crucial for accuracy.", "motivation": "Limited research exists on LLM performance in safety-critical visual tasks in construction, despite their potential for hazard recognition.", "method": "Comparative evaluation of five LLMs using precision, recall, and F1-score metrics under zero-shot, few-shot, and CoT prompting strategies.", "result": "CoT prompting yielded the highest accuracy. GPT-4.5 and GPT-o3 performed best overall. Prompt design significantly impacts LLM performance.", "conclusion": "Prompt engineering is key for reliable AI-assisted hazard recognition in construction, with CoT and specific LLMs (GPT-4.5, GPT-o3) showing promise."}}
{"id": "2506.07557", "pdf": "https://arxiv.org/pdf/2506.07557", "abs": "https://arxiv.org/abs/2506.07557", "authors": ["Mengsong Wu", "Di Zhang", "Yuqiang Li", "Dongzhan Zhou", "Wenliang Chen"], "title": "SELT: Self-Evaluation Tree Search for LLMs with Task Decomposition", "categories": ["cs.CL", "cs.AI"], "comment": "11 pages, 5 figures", "summary": "While Large Language Models (LLMs) have achieved remarkable success in a wide\nrange of applications, their performance often degrades in complex reasoning\ntasks. In this work, we introduce SELT (Self-Evaluation LLM Tree Search), a\nnovel framework that leverages a modified Monte Carlo Tree Search (MCTS) to\nenhance LLM reasoning without relying on external reward models. By redefining\nthe Upper Confidence Bound scoring to align with intrinsic self-evaluation\ncapabilities of LLMs and decomposing the inference process into atomic subtasks\naugmented with semantic clustering at each node, SELT effectively balances\nexploration and exploitation, reduces redundant reasoning paths, and mitigates\nhallucination. We validate our approach on challenging benchmarks, including\nthe knowledge-based MMLU and the Tool Learning dataset Seal-Tools, where SELT\nachieves significant improvements in answer accuracy and reasoning robustness\ncompared to baseline methods. Notably, our framework operates without\ntask-specific fine-tuning, demonstrating strong generalizability across diverse\nreasoning tasks. Relevant results and code are available at\nhttps://github.com/fairyshine/SELT .", "AI": {"tldr": "SELT enhances LLM reasoning using a modified Monte Carlo Tree Search, improving accuracy and robustness without external rewards or task-specific fine-tuning.", "motivation": "LLMs struggle with complex reasoning tasks; SELT aims to improve their performance without relying on external reward models.", "method": "SELT uses a modified MCTS with intrinsic self-evaluation and semantic clustering to balance exploration and exploitation.", "result": "SELT outperforms baselines on benchmarks like MMLU and Seal-Tools, improving accuracy and reasoning robustness.", "conclusion": "SELT is a generalizable framework for enhancing LLM reasoning, validated by strong performance on diverse tasks."}}
{"id": "2506.07456", "pdf": "https://arxiv.org/pdf/2506.07456", "abs": "https://arxiv.org/abs/2506.07456", "authors": ["Wei Yao", "Yunlian Sun", "Chang Liu", "Hongwen Zhang", "Jinhui Tang"], "title": "PhysiInter: Integrating Physical Mapping for High-Fidelity Human Interaction Generation", "categories": ["cs.CV"], "comment": null, "summary": "Driven by advancements in motion capture and generative artificial\nintelligence, leveraging large-scale MoCap datasets to train generative models\nfor synthesizing diverse, realistic human motions has become a promising\nresearch direction. However, existing motion-capture techniques and generative\nmodels often neglect physical constraints, leading to artifacts such as\ninterpenetration, sliding, and floating. These issues are exacerbated in\nmulti-person motion generation, where complex interactions are involved. To\naddress these limitations, we introduce physical mapping, integrated throughout\nthe human interaction generation pipeline. Specifically, motion imitation\nwithin a physics-based simulation environment is used to project target motions\ninto a physically valid space. The resulting motions are adjusted to adhere to\nreal-world physics constraints while retaining their original semantic meaning.\nThis mapping not only improves MoCap data quality but also directly informs\npost-processing of generated motions. Given the unique interactivity of\nmulti-person scenarios, we propose a tailored motion representation framework.\nMotion Consistency (MC) and Marker-based Interaction (MI) loss functions are\nintroduced to improve model performance. Experiments show our method achieves\nimpressive results in generated human motion quality, with a 3%-89% improvement\nin physical fidelity. Project page http://yw0208.github.io/physiinter", "AI": {"tldr": "The paper introduces physical mapping to improve motion generation by enforcing physical constraints, enhancing realism in multi-person scenarios.", "motivation": "Existing motion-capture and generative models often ignore physical constraints, causing artifacts like interpenetration and sliding, especially in multi-person interactions.", "method": "The proposed method uses motion imitation in a physics-based simulation to project motions into a physically valid space, with tailored loss functions (MC and MI) for multi-person scenarios.", "result": "Experiments show a 3%-89% improvement in physical fidelity of generated motions.", "conclusion": "The approach effectively enhances motion quality by integrating physical constraints, particularly beneficial for multi-person interactions."}}
{"id": "2506.07583", "pdf": "https://arxiv.org/pdf/2506.07583", "abs": "https://arxiv.org/abs/2506.07583", "authors": ["Ramakrishna Appicharla", "Baban Gain", "Santanu Pal", "Asif Ekbal"], "title": "Beyond the Sentence: A Survey on Context-Aware Machine Translation with Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite the popularity of the large language models (LLMs), their application\nto machine translation is relatively underexplored, especially in context-aware\nsettings. This work presents a literature review of context-aware translation\nwith LLMs. The existing works utilise prompting and fine-tuning approaches,\nwith few focusing on automatic post-editing and creating translation agents for\ncontext-aware machine translation. We observed that the commercial LLMs (such\nas ChatGPT and Tower LLM) achieved better results than the open-source LLMs\n(such as Llama and Bloom LLMs), and prompt-based approaches serve as good\nbaselines to assess the quality of translations. Finally, we present some\ninteresting future directions to explore.", "AI": {"tldr": "A review of context-aware machine translation using LLMs, comparing commercial and open-source models, with future research directions.", "motivation": "To explore the underexplored application of LLMs in context-aware machine translation.", "method": "Literature review of prompting, fine-tuning, automatic post-editing, and translation agents.", "result": "Commercial LLMs (e.g., ChatGPT) outperform open-source ones (e.g., Llama); prompt-based approaches are effective baselines.", "conclusion": "Identifies gaps and suggests future directions for context-aware translation with LLMs."}}
{"id": "2506.07460", "pdf": "https://arxiv.org/pdf/2506.07460", "abs": "https://arxiv.org/abs/2506.07460", "authors": ["Taeryung Lee", "Hyeongjin Nam", "Gyeongsik Moon", "Kyoung Mu Lee"], "title": "GLOS: Sign Language Generation with Temporally Aligned Gloss-Level Conditioning", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Sign language generation (SLG), or text-to-sign generation, bridges the gap\nbetween signers and non-signers. Despite recent progress in SLG, existing\nmethods still often suffer from incorrect lexical ordering and low semantic\naccuracy. This is primarily due to sentence-level condition, which encodes the\nentire sentence of the input text into a single feature vector as a condition\nfor SLG. This approach fails to capture the temporal structure of sign language\nand lacks the granularity of word-level semantics, often leading to disordered\nsign sequences and ambiguous motions. To overcome these limitations, we propose\nGLOS, a sign language generation framework with temporally aligned gloss-level\nconditioning. First, we employ gloss-level conditions, which we define as\nsequences of gloss embeddings temporally aligned with the motion sequence. This\nenables the model to access both the temporal structure of sign language and\nword-level semantics at each timestep. As a result, this allows for\nfine-grained control of signs and better preservation of lexical order. Second,\nwe introduce a condition fusion module, temporal alignment conditioning (TAC),\nto efficiently deliver the word-level semantic and temporal structure provided\nby the gloss-level condition to the corresponding motion timesteps. Our method,\nwhich is composed of gloss-level conditions and TAC, generates signs with\ncorrect lexical order and high semantic accuracy, outperforming prior methods\non CSL-Daily and Phoenix-2014T.", "AI": {"tldr": "The paper proposes GLOS, a sign language generation framework using gloss-level conditioning and temporal alignment to improve lexical order and semantic accuracy.", "motivation": "Existing SLG methods suffer from incorrect lexical ordering and low semantic accuracy due to sentence-level conditions, which lack temporal structure and word-level granularity.", "method": "GLOS employs gloss-level conditions aligned with motion sequences and introduces a condition fusion module (TAC) for efficient semantic and temporal alignment.", "result": "GLOS outperforms prior methods on CSL-Daily and Phoenix-2014T, generating signs with correct lexical order and high semantic accuracy.", "conclusion": "The proposed GLOS framework effectively addresses the limitations of sentence-level conditions, improving sign language generation quality."}}
{"id": "2506.07597", "pdf": "https://arxiv.org/pdf/2506.07597", "abs": "https://arxiv.org/abs/2506.07597", "authors": ["Oscar Sainz", "Naiara Perez", "Julen Etxaniz", "Joseba Fernandez de Landa", "Itziar Aldabe", "Iker Garc\u00eda-Ferrero", "Aimar Zabala", "Ekhi Azurmendi", "German Rigau", "Eneko Agirre", "Mikel Artetxe", "Aitor Soroa"], "title": "Instructing Large Language Models for Low-Resource Languages: A Systematic Study for Basque", "categories": ["cs.CL"], "comment": "Under review", "summary": "Instructing language models with user intent requires large instruction\ndatasets, which are only available for a limited set of languages. In this\npaper, we explore alternatives to conventional instruction adaptation pipelines\nin low-resource scenarios. We assume a realistic scenario for low-resource\nlanguages, where only the following are available: corpora in the target\nlanguage, existing open-weight multilingual base and instructed backbone LLMs,\nand synthetically generated instructions sampled from the instructed backbone.\nWe present a comprehensive set of experiments for Basque that systematically\nstudy different combinations of these components evaluated on benchmarks and\nhuman preferences from 1,680 participants. Our conclusions show that target\nlanguage corpora are essential, with synthetic instructions yielding robust\nmodels, and, most importantly, that using as backbone an instruction-tuned\nmodel outperforms using a base non-instructed model, and improved results when\nscaling up. Using Llama 3.1 instruct 70B as backbone our model comes near\nfrontier models of much larger sizes for Basque, without using any Basque data\napart from the 1.2B word corpora. We release code, models, instruction\ndatasets, and human preferences to support full reproducibility in future\nresearch on low-resource language adaptation.", "AI": {"tldr": "The paper explores alternatives to traditional instruction adaptation for low-resource languages, using Basque as a case study. It leverages synthetic instructions and existing multilingual models to achieve near-state-of-the-art performance without extensive Basque data.", "motivation": "Address the lack of large instruction datasets for low-resource languages by exploring efficient adaptation methods.", "method": "Uses target language corpora, synthetic instructions, and multilingual base/instructed models (e.g., Llama 3.1 instruct 70B) for adaptation. Evaluated via benchmarks and human preferences.", "result": "Synthetic instructions and instructed backbones yield robust models, outperforming base models. Near-state-of-the-art performance achieved with minimal Basque data.", "conclusion": "Target language corpora and instructed backbones are crucial. Synthetic instructions enable effective adaptation for low-resource languages."}}
{"id": "2506.07464", "pdf": "https://arxiv.org/pdf/2506.07464", "abs": "https://arxiv.org/abs/2506.07464", "authors": ["Jinyoung Park", "Jeehye Na", "Jinyoung Kim", "Hyunwoo J. Kim"], "title": "DeepVideo-R1: Video Reinforcement Fine-Tuning via Difficulty-aware Regressive GRPO", "categories": ["cs.CV", "cs.AI"], "comment": "Work in progress", "summary": "Recent works have demonstrated the effectiveness of reinforcement learning\n(RL)-based post-training in enhancing the reasoning capabilities of large\nlanguage models (LLMs). In particular, Group Relative Policy Optimization\n(GRPO) has shown impressive success by employing a PPO-style reinforcement\nalgorithm with group-based normalized rewards. However, the application of GRPO\nto Video Large Language Models (Video LLMs) has been less studied. In this\npaper, we explore GRPO for video LLMs and identify two primary issues that\nimpede its effective learning: (1) reliance on safeguards, and (2) the\nvanishing advantage problem. To mitigate these challenges, we propose\nDeepVideo-R1, a video large language model trained with our proposed Reg-GRPO\n(Regressive GRPO) and difficulty-aware data augmentation strategy. Reg-GRPO\nreformulates the GRPO objective as a regression task, directly predicting the\nadvantage in GRPO. This design eliminates the need for safeguards like clipping\nand min functions, thereby facilitating more direct policy guidance by aligning\nthe model with the advantage values. We also design the difficulty-aware data\naugmentation strategy that dynamically augments training samples at solvable\ndifficulty levels, fostering diverse and informative reward signals. Our\ncomprehensive experiments show that DeepVideo-R1 significantly improves video\nreasoning performance across multiple video reasoning benchmarks.", "AI": {"tldr": "The paper explores GRPO for Video LLMs, identifies issues (safeguards and vanishing advantage), and proposes Reg-GRPO and difficulty-aware data augmentation to improve performance.", "motivation": "To enhance the reasoning capabilities of Video LLMs by addressing limitations of GRPO, such as reliance on safeguards and the vanishing advantage problem.", "method": "Introduces Reg-GRPO, a regression-based reformulation of GRPO, and a difficulty-aware data augmentation strategy to improve training.", "result": "DeepVideo-R1, trained with Reg-GRPO and data augmentation, shows significant improvements in video reasoning benchmarks.", "conclusion": "The proposed methods effectively address GRPO's limitations and enhance Video LLM performance."}}
{"id": "2506.07606", "pdf": "https://arxiv.org/pdf/2506.07606", "abs": "https://arxiv.org/abs/2506.07606", "authors": ["Peyman Rostami", "Vahid Rahimzadeh", "Ali Adibi", "Azadeh Shakery"], "title": "PolitiSky24: U.S. Political Bluesky Dataset with User Stance Labels", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.SI", "I.2.7"], "comment": "The dataset is available at https://doi.org/10.5281/zenodo.15616911", "summary": "Stance detection identifies the viewpoint expressed in text toward a specific\ntarget, such as a political figure. While previous datasets have focused\nprimarily on tweet-level stances from established platforms, user-level stance\nresources, especially on emerging platforms like Bluesky remain scarce.\nUser-level stance detection provides a more holistic view by considering a\nuser's complete posting history rather than isolated posts. We present the\nfirst stance detection dataset for the 2024 U.S. presidential election,\ncollected from Bluesky and centered on Kamala Harris and Donald Trump. The\ndataset comprises 16,044 user-target stance pairs enriched with engagement\nmetadata, interaction graphs, and user posting histories. PolitiSky24 was\ncreated using a carefully evaluated pipeline combining advanced information\nretrieval and large language models, which generates stance labels with\nsupporting rationales and text spans for transparency. The labeling approach\nachieves 81\\% accuracy with scalable LLMs. This resource addresses gaps in\npolitical stance analysis through its timeliness, open-data nature, and\nuser-level perspective. The dataset is available at\nhttps://doi.org/10.5281/zenodo.15616911", "AI": {"tldr": "The paper introduces PolitiSky24, the first stance detection dataset for the 2024 U.S. presidential election, collected from Bluesky, focusing on Kamala Harris and Donald Trump. It includes user-level stances, engagement metadata, and interaction graphs, labeled using a pipeline with 81% accuracy.", "motivation": "To address the scarcity of user-level stance detection datasets, especially on emerging platforms like Bluesky, and provide a holistic view of political stances for the 2024 U.S. presidential election.", "method": "A dataset of 16,044 user-target stance pairs was created using a pipeline combining advanced information retrieval and large language models, generating labels with rationales and text spans.", "result": "The labeling approach achieves 81% accuracy, offering a timely, open-data resource with user-level insights.", "conclusion": "PolitiSky24 fills gaps in political stance analysis by providing a comprehensive, transparent, and scalable dataset for emerging platforms."}}
{"id": "2506.07471", "pdf": "https://arxiv.org/pdf/2506.07471", "abs": "https://arxiv.org/abs/2506.07471", "authors": ["CH Cho", "WJ Moon", "W Jun", "MS Jung", "JP Heo"], "title": "Ambiguity-Restrained Text-Video Representation Learning for Partially Relevant Video Retrieval", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to AAAI 2025", "summary": "Partially Relevant Video Retrieval~(PRVR) aims to retrieve a video where a\nspecific segment is relevant to a given text query. Typical training processes\nof PRVR assume a one-to-one relationship where each text query is relevant to\nonly one video. However, we point out the inherent ambiguity between text and\nvideo content based on their conceptual scope and propose a framework that\nincorporates this ambiguity into the model learning process. Specifically, we\npropose Ambiguity-Restrained representation Learning~(ARL) to address ambiguous\ntext-video pairs. Initially, ARL detects ambiguous pairs based on two criteria:\nuncertainty and similarity. Uncertainty represents whether instances include\ncommonly shared context across the dataset, while similarity indicates\npair-wise semantic overlap. Then, with the detected ambiguous pairs, our ARL\nhierarchically learns the semantic relationship via multi-positive contrastive\nlearning and dual triplet margin loss. Additionally, we delve into fine-grained\nrelationships within the video instances. Unlike typical training at the\ntext-video level, where pairwise information is provided, we address the\ninherent ambiguity within frames of the same untrimmed video, which often\ncontains multiple contexts. This allows us to further enhance learning at the\ntext-frame level. Lastly, we propose cross-model ambiguity detection to\nmitigate the error propagation that occurs when a single model is employed to\ndetect ambiguous pairs for its training. With all components combined, our\nproposed method demonstrates its effectiveness in PRVR.", "AI": {"tldr": "The paper introduces Ambiguity-Restrained representation Learning (ARL) to address ambiguous text-video pairs in Partially Relevant Video Retrieval (PRVR), improving retrieval accuracy by detecting and learning from ambiguous pairs hierarchically.", "motivation": "Existing PRVR methods assume a one-to-one text-video relationship, ignoring inherent ambiguity due to conceptual scope differences between text and video content.", "method": "ARL detects ambiguous pairs using uncertainty and similarity criteria, then learns semantic relationships via multi-positive contrastive learning and dual triplet margin loss. It also explores fine-grained text-frame relationships and cross-model ambiguity detection to reduce error propagation.", "result": "The proposed ARL framework effectively improves performance in PRVR by addressing ambiguity in text-video pairs.", "conclusion": "ARL successfully mitigates ambiguity in PRVR, enhancing retrieval accuracy through hierarchical learning and cross-model detection."}}
{"id": "2506.07617", "pdf": "https://arxiv.org/pdf/2506.07617", "abs": "https://arxiv.org/abs/2506.07617", "authors": ["Roman Kyslyi", "Yuliia Maksymiuk", "Ihor Pysmennyi"], "title": "Vuyko Mistral: Adapting LLMs for Low-Resource Dialectal Translation", "categories": ["cs.CL"], "comment": "Preprint. Will be published at Proceedings of the Fourth Ukrainian\n  Natural Language Processing Workshop (UNLP)", "summary": "In this paper we introduce the first effort to adapt large language models\n(LLMs) to the Ukrainian dialect (in our case Hutsul), a low-resource and\nmorphologically complex dialect spoken in the Carpathian Highlands. We created\na parallel corpus of 9852 dialect-to-standard Ukrainian sentence pairs and a\ndictionary of 7320 dialectal word mappings. We also addressed data shortage by\nproposing an advanced Retrieval-Augmented Generation (RAG) pipeline to generate\nsynthetic parallel translation pairs, expanding the corpus with 52142 examples.\nWe have fine-tuned multiple open-source LLMs using LoRA and evaluated them on a\nstandard-to-dialect translation task, also comparing with few-shot GPT-4o\ntranslation. In the absence of human annotators, we adopt a multi-metric\nevaluation strategy combining BLEU, chrF++, TER, and LLM-based judgment\n(GPT-4o). The results show that even small(7B) finetuned models outperform\nzero-shot baselines such as GPT-4o across both automatic and LLM-evaluated\nmetrics. All data, models, and code are publicly released at:\nhttps://github.com/woters/vuyko-hutsul", "AI": {"tldr": "The paper introduces the first adaptation of large language models (LLMs) to the Ukrainian Hutsul dialect, using a parallel corpus and synthetic data generation to address low-resource challenges. Fine-tuned models outperform GPT-4o in translation tasks.", "motivation": "To adapt LLMs for the low-resource and morphologically complex Hutsul dialect, addressing data scarcity and improving translation quality.", "method": "Created a parallel corpus and dictionary, used Retrieval-Augmented Generation (RAG) for synthetic data, and fine-tuned LLMs with LoRA. Evaluated using BLEU, chrF++, TER, and GPT-4o judgments.", "result": "Fine-tuned small (7B) models outperformed zero-shot GPT-4o in translation tasks across all metrics.", "conclusion": "The approach successfully adapts LLMs to low-resource dialects, with publicly released data and models."}}
{"id": "2506.07484", "pdf": "https://arxiv.org/pdf/2506.07484", "abs": "https://arxiv.org/abs/2506.07484", "authors": ["Dasol Hong", "Wooju Lee", "Hyun Myung"], "title": "CoCoA-Mix: Confusion-and-Confidence-Aware Mixture Model for Context Optimization", "categories": ["cs.CV", "cs.AI", "cs.LG", "I.2.6; I.5.2"], "comment": "8 pages, 5 figures; accepted at ICML 2025", "summary": "Prompt tuning, which adapts vision-language models by freezing model\nparameters and optimizing only the prompt, has proven effective for\ntask-specific adaptations. The core challenge in prompt tuning is improving\nspecialization for a specific task and generalization for unseen domains.\nHowever, frozen encoders often produce misaligned features, leading to\nconfusion between classes and limiting specialization. To overcome this issue,\nwe propose a confusion-aware loss (CoA-loss) that improves specialization by\nrefining the decision boundaries between confusing classes. Additionally, we\nmathematically demonstrate that a mixture model can enhance generalization\nwithout compromising specialization. This is achieved using confidence-aware\nweights (CoA-weights), which adjust the weights of each prediction in the\nmixture model based on its confidence within the class domains. Extensive\nexperiments show that CoCoA-Mix, a mixture model with CoA-loss and CoA-weights,\noutperforms state-of-the-art methods by enhancing specialization and\ngeneralization. Our code is publicly available at\nhttps://github.com/url-kaist/CoCoA-Mix.", "AI": {"tldr": "The paper introduces CoCoA-Mix, a method combining confusion-aware loss (CoA-loss) and confidence-aware weights (CoA-weights) to improve specialization and generalization in prompt tuning for vision-language models.", "motivation": "Frozen encoders in prompt tuning often produce misaligned features, causing class confusion and limiting task specialization.", "method": "Proposes CoA-loss to refine decision boundaries and CoA-weights in a mixture model to balance specialization and generalization.", "result": "CoCoA-Mix outperforms state-of-the-art methods by enhancing both specialization and generalization.", "conclusion": "The proposed method effectively addresses misalignment and confusion in prompt tuning, achieving superior performance."}}
{"id": "2506.07621", "pdf": "https://arxiv.org/pdf/2506.07621", "abs": "https://arxiv.org/abs/2506.07621", "authors": ["Harsh Bihany", "Shubham Patel", "Ashutosh Modi"], "title": "LoRMA: Low-Rank Multiplicative Adaptation for LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted at ACL Findings 2025; 21 pages (9 main paper + 5 pages\n  references + 7 pages appendix)", "summary": "Large Language Models have shown remarkable capabilities in the NLP domain.\nTheir effectiveness can mainly be attributed to their ability to adapt to an\narray of downstream tasks. However, generally, full fine-tuning is a\ncomputationally expensive job. To mitigate this, many techniques have been\ndeveloped that prime efficiency, a prominent one being Low-Rank Adaptation\n(LoRA). However, LoRA and its variants employ re-parametrized additive updates.\nIn this paper, we propose Low-Rank Multiplicative Adaptation (LoRMA), which\nshifts the paradigm of additive updates to a richer space of matrix\nmultiplicative transformations. We tackle challenges such as computational\ncomplexity and rank bottleneck of matrix multiplication by effectively\nre-ordering operations and introducing rank inflation strategies. We conduct\nextensive experiments to demonstrate the effectiveness of our approach in terms\nof various evaluation metrics.", "AI": {"tldr": "The paper introduces LoRMA, a method replacing additive updates in LoRA with multiplicative transformations for efficient fine-tuning of large language models.", "motivation": "Full fine-tuning of large language models is computationally expensive, and existing methods like LoRA use additive updates, which may not be optimal.", "method": "Proposes Low-Rank Multiplicative Adaptation (LoRMA), addressing computational complexity and rank bottlenecks through operation re-ordering and rank inflation strategies.", "result": "Extensive experiments show LoRMA's effectiveness across various evaluation metrics.", "conclusion": "LoRMA offers a more efficient and effective alternative to additive update methods like LoRA for fine-tuning large language models."}}
{"id": "2506.07489", "pdf": "https://arxiv.org/pdf/2506.07489", "abs": "https://arxiv.org/abs/2506.07489", "authors": ["Yahao Shi", "Yang Liu", "Yanmin Wu", "Xing Liu", "Chen Zhao", "Jie Luo", "Bin Zhou"], "title": "Drive Any Mesh: 4D Latent Diffusion for Mesh Deformation from Video", "categories": ["cs.CV"], "comment": "technical report", "summary": "We propose DriveAnyMesh, a method for driving mesh guided by monocular video.\nCurrent 4D generation techniques encounter challenges with modern rendering\nengines. Implicit methods have low rendering efficiency and are unfriendly to\nrasterization-based engines, while skeletal methods demand significant manual\neffort and lack cross-category generalization. Animating existing 3D assets,\ninstead of creating 4D assets from scratch, demands a deep understanding of the\ninput's 3D structure. To tackle these challenges, we present a 4D diffusion\nmodel that denoises sequences of latent sets, which are then decoded to produce\nmesh animations from point cloud trajectory sequences. These latent sets\nleverage a transformer-based variational autoencoder, simultaneously capturing\n3D shape and motion information. By employing a spatiotemporal,\ntransformer-based diffusion model, information is exchanged across multiple\nlatent frames, enhancing the efficiency and generalization of the generated\nresults. Our experimental results demonstrate that DriveAnyMesh can rapidly\nproduce high-quality animations for complex motions and is compatible with\nmodern rendering engines. This method holds potential for applications in both\nthe gaming and filming industries.", "AI": {"tldr": "DriveAnyMesh is a 4D diffusion model for mesh animation from monocular video, addressing inefficiencies in current 4D generation methods.", "motivation": "Overcoming challenges in 4D generation like low rendering efficiency in implicit methods and manual effort in skeletal methods, while enabling animation of existing 3D assets.", "method": "Uses a 4D diffusion model to denoise latent sets, decoded into mesh animations from point cloud trajectories, leveraging a transformer-based VAE for shape and motion.", "result": "Produces high-quality animations efficiently, compatible with modern rendering engines.", "conclusion": "DriveAnyMesh is promising for gaming and filming industries due to its efficiency and generalization."}}
{"id": "2506.07626", "pdf": "https://arxiv.org/pdf/2506.07626", "abs": "https://arxiv.org/abs/2506.07626", "authors": ["Kseniia Petukhova", "Ekaterina Kochmar"], "title": "Intent Matters: Enhancing AI Tutoring with Fine-Grained Pedagogical Intent Annotation", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) hold great promise for educational applications,\nparticularly in intelligent tutoring systems. However, effective tutoring\nrequires alignment with pedagogical strategies - something current LLMs lack\nwithout task-specific adaptation. In this work, we explore whether fine-grained\nannotation of teacher intents can improve the quality of LLM-generated tutoring\nresponses. We focus on MathDial, a dialog dataset for math instruction, and\napply an automated annotation framework to re-annotate a portion of the dataset\nusing a detailed taxonomy of eleven pedagogical intents. We then fine-tune an\nLLM using these new annotations and compare its performance to models trained\non the original four-category taxonomy. Both automatic and qualitative\nevaluations show that the fine-grained model produces more pedagogically\naligned and effective responses. Our findings highlight the value of intent\nspecificity for controlled text generation in educational settings, and we\nrelease our annotated data and code to facilitate further research.", "AI": {"tldr": "Fine-grained annotation of teacher intents improves LLM-generated tutoring responses, enhancing pedagogical alignment and effectiveness.", "motivation": "Current LLMs lack alignment with pedagogical strategies, limiting their effectiveness in educational applications like intelligent tutoring systems.", "method": "Re-annotated a portion of the MathDial dataset using a detailed taxonomy of eleven pedagogical intents, then fine-tuned an LLM with these annotations.", "result": "The fine-grained model outperformed models trained on the original four-category taxonomy, producing more pedagogically aligned and effective responses.", "conclusion": "Fine-grained intent annotation enhances LLM performance in educational settings, and the annotated data and code are released for further research."}}
{"id": "2506.07491", "pdf": "https://arxiv.org/pdf/2506.07491", "abs": "https://arxiv.org/abs/2506.07491", "authors": ["Yongsen Mao", "Junhao Zhong", "Chuan Fang", "Jia Zheng", "Rui Tang", "Hao Zhu", "Ping Tan", "Zihan Zhou"], "title": "SpatialLM: Training Large Language Models for Structured Indoor Modeling", "categories": ["cs.CV"], "comment": null, "summary": "SpatialLM is a large language model designed to process 3D point cloud data\nand generate structured 3D scene understanding outputs. These outputs include\narchitectural elements like walls, doors, windows, and oriented object boxes\nwith their semantic categories. Unlike previous methods which exploit\ntask-specific network designs, our model adheres to the standard multimodal LLM\narchitecture and is fine-tuned directly from open-source LLMs.\n  To train SpatialLM, we collect a large-scale, high-quality synthetic dataset\nconsisting of the point clouds of 12,328 indoor scenes (54,778 rooms) with\nground-truth 3D annotations, and conduct a careful study on various modeling\nand training decisions. On public benchmarks, our model gives state-of-the-art\nperformance in layout estimation and competitive results in 3D object\ndetection. With that, we show a feasible path for enhancing the spatial\nunderstanding capabilities of modern LLMs for applications in augmented\nreality, embodied robotics, and more.", "AI": {"tldr": "SpatialLM is a large language model for 3D point cloud data, achieving state-of-the-art performance in layout estimation and competitive results in 3D object detection.", "motivation": "To enhance spatial understanding capabilities of modern LLMs for applications like augmented reality and robotics.", "method": "Uses a standard multimodal LLM architecture, fine-tuned from open-source LLMs, trained on a synthetic dataset of 12,328 indoor scenes.", "result": "State-of-the-art in layout estimation and competitive in 3D object detection.", "conclusion": "Demonstrates a feasible path to improve spatial understanding in LLMs for practical applications."}}
{"id": "2506.07631", "pdf": "https://arxiv.org/pdf/2506.07631", "abs": "https://arxiv.org/abs/2506.07631", "authors": ["Brian Gordon", "Yonatan Bitton", "Andreea Marzoca", "Yasumasa Onoe", "Xiao Wang", "Daniel Cohen-Or", "Idan Szpektor"], "title": "Unblocking Fine-Grained Evaluation of Detailed Captions: An Explaining AutoRater and Critic-and-Revise Pipeline", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Large Vision-Language Models (VLMs) now generate highly detailed,\nparagraphlength image captions, yet evaluating their factual accuracy remains\nchallenging. Current methods often miss fine-grained errors, being designed for\nshorter texts or lacking datasets with verified inaccuracies. We introduce\nDOCCI-Critique, a benchmark with 1,400 VLM-generated paragraph captions (100\nimages, 14 VLMs) featuring over 10,216 sentence-level human annotations of\nfactual correctness and explanatory rationales for errors, all within paragraph\ncontext. Building on this, we develop VNLI-Critique, a model for automated\nsentence-level factuality classification and critique generation. We highlight\nthree key applications: (1) VNLI-Critique demonstrates robust generalization,\nvalidated by state-of-the-art performance on the M-HalDetect benchmark and\nstrong results in CHOCOLATE claim verification. (2) The VNLI-Critique driven\nAutoRater for DOCCI-Critique provides reliable VLM rankings, showing excellent\nalignment with human factuality judgments (e.g., 0.98 Spearman). (3) An\ninnovative Critic-and-Revise pipeline, where critiques from VNLI-Critique guide\nLLM-based corrections, achieves substantial improvements in caption factuality\n(e.g., a 46% gain on DetailCaps-4870). Our work offers a crucial benchmark\nalongside practical tools, designed to significantly elevate the standards for\nfine-grained evaluation and foster the improvement of VLM image understanding.\nProject page: https://google.github.io/unblocking-detail-caption", "AI": {"tldr": "DOCCI-Critique is a benchmark for evaluating factual accuracy in VLM-generated captions, featuring human annotations and automated tools like VNLI-Critique for factuality classification and critique generation.", "motivation": "Current methods for evaluating VLM-generated captions lack fine-grained error detection and verified datasets, necessitating a more robust benchmark and tools.", "method": "Developed DOCCI-Critique with human-annotated captions and VNLI-Critique for automated factuality classification and critique generation.", "result": "VNLI-Critique shows strong performance on benchmarks, reliable VLM rankings, and improves caption factuality via a Critic-and-Revise pipeline.", "conclusion": "The work provides a benchmark and tools to enhance fine-grained evaluation and VLM image understanding."}}
{"id": "2506.07497", "pdf": "https://arxiv.org/pdf/2506.07497", "abs": "https://arxiv.org/abs/2506.07497", "authors": ["Xiangyu Guo", "Zhanqian Wu", "Kaixin Xiong", "Ziyang Xu", "Lijun Zhou", "Gangwei Xu", "Shaoqing Xu", "Haiyang Sun", "Bing Wang", "Guang Chen", "Hangjun Ye", "Wenyu Liu", "Xinggang Wang"], "title": "Genesis: Multimodal Driving Scene Generation with Spatio-Temporal and Cross-Modal Consistency", "categories": ["cs.CV"], "comment": null, "summary": "We present Genesis, a unified framework for joint generation of multi-view\ndriving videos and LiDAR sequences with spatio-temporal and cross-modal\nconsistency. Genesis employs a two-stage architecture that integrates a\nDiT-based video diffusion model with 3D-VAE encoding, and a BEV-aware LiDAR\ngenerator with NeRF-based rendering and adaptive sampling. Both modalities are\ndirectly coupled through a shared latent space, enabling coherent evolution\nacross visual and geometric domains. To guide the generation with structured\nsemantics, we introduce DataCrafter, a captioning module built on\nvision-language models that provides scene-level and instance-level\nsupervision. Extensive experiments on the nuScenes benchmark demonstrate that\nGenesis achieves state-of-the-art performance across video and LiDAR metrics\n(FVD 16.95, FID 4.24, Chamfer 0.611), and benefits downstream tasks including\nsegmentation and 3D detection, validating the semantic fidelity and practical\nutility of the generated data.", "AI": {"tldr": "Genesis is a framework for generating multi-view driving videos and LiDAR sequences with consistency, using a two-stage architecture and shared latent space, achieving state-of-the-art performance.", "motivation": "To create a unified framework for generating consistent multi-modal driving data (videos and LiDAR) for practical applications like segmentation and 3D detection.", "method": "Uses a two-stage architecture: DiT-based video diffusion with 3D-VAE encoding and BEV-aware LiDAR generator with NeRF rendering. Includes DataCrafter for semantic supervision.", "result": "Achieves SOTA metrics (FVD 16.95, FID 4.24, Chamfer 0.611) on nuScenes, improving downstream tasks like segmentation and 3D detection.", "conclusion": "Genesis validates semantic fidelity and utility of generated data, proving effective for multi-modal driving data generation."}}
{"id": "2506.07642", "pdf": "https://arxiv.org/pdf/2506.07642", "abs": "https://arxiv.org/abs/2506.07642", "authors": ["Yuan Chang", "Ziyue Li", "Hengyuan Zhang", "Yuanbo Kong", "Yanru Wu", "Zhijiang Guo", "Ngai Wong"], "title": "TreeReview: A Dynamic Tree of Questions Framework for Deep and Efficient LLM-based Scientific Peer Review", "categories": ["cs.CL"], "comment": "30 pages, 17 figures", "summary": "While Large Language Models (LLMs) have shown significant potential in\nassisting peer review, current methods often struggle to generate thorough and\ninsightful reviews while maintaining efficiency. In this paper, we propose\nTreeReview, a novel framework that models paper review as a hierarchical and\nbidirectional question-answering process. TreeReview first constructs a tree of\nreview questions by recursively decomposing high-level questions into\nfine-grained sub-questions and then resolves the question tree by iteratively\naggregating answers from leaf to root to get the final review. Crucially, we\nincorporate a dynamic question expansion mechanism to enable deeper probing by\ngenerating follow-up questions when needed. We construct a benchmark derived\nfrom ICLR and NeurIPS venues to evaluate our method on full review generation\nand actionable feedback comments generation tasks. Experimental results of both\nLLM-based and human evaluation show that TreeReview outperforms strong\nbaselines in providing comprehensive, in-depth, and expert-aligned review\nfeedback, while reducing LLM token usage by up to 80% compared to\ncomputationally intensive approaches. Our code and benchmark dataset are\navailable at https://github.com/YuanChang98/tree-review.", "AI": {"tldr": "TreeReview is a hierarchical, bidirectional QA framework for generating thorough and efficient peer reviews using LLMs, outperforming baselines in quality and reducing token usage.", "motivation": "Current LLM-based peer review methods lack thoroughness and efficiency. TreeReview aims to improve both by modeling reviews as a hierarchical QA process.", "method": "TreeReview constructs a tree of review questions, decomposes high-level questions into sub-questions, and aggregates answers dynamically. It includes a question expansion mechanism for deeper probing.", "result": "TreeReview outperforms baselines in generating comprehensive and expert-aligned reviews, reducing LLM token usage by up to 80%.", "conclusion": "TreeReview offers a scalable and efficient solution for high-quality peer review generation, with potential applications in academic and professional settings."}}
{"id": "2506.07533", "pdf": "https://arxiv.org/pdf/2506.07533", "abs": "https://arxiv.org/abs/2506.07533", "authors": ["Wei Tao", "Haocheng Lu", "Xiaoyang Qu", "Bin Zhang", "Kai Lu", "Jiguang Wan", "Jianzong Wang"], "title": "MoQAE: Mixed-Precision Quantization for Long-Context LLM Inference via Mixture of Quantization-Aware Experts", "categories": ["cs.CV"], "comment": "Accepted by the 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025)", "summary": "One of the primary challenges in optimizing large language models (LLMs) for\nlong-context inference lies in the high memory consumption of the Key-Value\n(KV) cache. Existing approaches, such as quantization, have demonstrated\npromising results in reducing memory usage. However, current quantization\nmethods cannot take both effectiveness and efficiency into account. In this\npaper, we propose MoQAE, a novel mixed-precision quantization method via\nmixture of quantization-aware experts. First, we view different quantization\nbit-width configurations as experts and use the traditional mixture of experts\n(MoE) method to select the optimal configuration. To avoid the inefficiency\ncaused by inputting tokens one by one into the router in the traditional MoE\nmethod, we input the tokens into the router chunk by chunk. Second, we design a\nlightweight router-only fine-tuning process to train MoQAE with a comprehensive\nloss to learn the trade-off between model accuracy and memory usage. Finally,\nwe introduce a routing freezing (RF) and a routing sharing (RS) mechanism to\nfurther reduce the inference overhead. Extensive experiments on multiple\nbenchmark datasets demonstrate that our method outperforms state-of-the-art KV\ncache quantization approaches in both efficiency and effectiveness.", "AI": {"tldr": "MoQAE is a mixed-precision quantization method for LLMs, combining quantization-aware experts and chunk-based routing to optimize KV cache memory usage while balancing accuracy and efficiency.", "motivation": "High memory consumption of KV cache in LLMs is a challenge. Existing quantization methods lack balance between effectiveness and efficiency.", "method": "MoQAE uses mixture of quantization-aware experts, chunk-based routing, lightweight fine-tuning, and routing freezing/sharing mechanisms.", "result": "Outperforms state-of-the-art KV cache quantization methods in efficiency and effectiveness.", "conclusion": "MoQAE effectively reduces memory usage while maintaining model accuracy, offering a practical solution for long-context inference."}}
{"id": "2506.07645", "pdf": "https://arxiv.org/pdf/2506.07645", "abs": "https://arxiv.org/abs/2506.07645", "authors": ["Maciej Chrab\u0105szcz", "Katarzyna Lorenc", "Karolina Seweryn"], "title": "Evaluating LLMs Robustness in Less Resourced Languages with Proxy Models", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive capabilities across\nvarious natural language processing (NLP) tasks in recent years. However, their\nsusceptibility to jailbreaks and perturbations necessitates additional\nevaluations. Many LLMs are multilingual, but safety-related training data\ncontains mainly high-resource languages like English. This can leave them\nvulnerable to perturbations in low-resource languages such as Polish. We show\nhow surprisingly strong attacks can be cheaply created by altering just a few\ncharacters and using a small proxy model for word importance calculation. We\nfind that these character and word-level attacks drastically alter the\npredictions of different LLMs, suggesting a potential vulnerability that can be\nused to circumvent their internal safety mechanisms. We validate our attack\nconstruction methodology on Polish, a low-resource language, and find potential\nvulnerabilities of LLMs in this language. Additionally, we show how it can be\nextended to other languages. We release the created datasets and code for\nfurther research.", "AI": {"tldr": "LLMs are vulnerable to cheap, character/word-level attacks in low-resource languages like Polish, bypassing safety mechanisms.", "motivation": "Evaluate LLM susceptibility to perturbations, especially in low-resource languages, due to limited safety training data.", "method": "Use small proxy models to calculate word importance and alter few characters for attack construction.", "result": "Attacks drastically alter LLM predictions, revealing vulnerabilities in low-resource languages.", "conclusion": "LLMs need improved safety evaluations for low-resource languages; datasets and code are released for further research."}}
{"id": "2506.07539", "pdf": "https://arxiv.org/pdf/2506.07539", "abs": "https://arxiv.org/abs/2506.07539", "authors": ["Xiaomeng Zhu", "Jacob Henningsson", "Duruo Li", "P\u00e4r M\u00e5rtensson", "Lars Hanson", "M\u00e5rten Bj\u00f6rkman", "Atsuto Maki"], "title": "Domain Randomization for Object Detection in Manufacturing Applications using Synthetic Data: A Comprehensive Study", "categories": ["cs.CV", "cs.AI"], "comment": "This is accepted by 2025 IEEE International Conference on Robotics &\n  Automation (ICRA), waiting for publication. 14 pages, 14 figures", "summary": "This paper addresses key aspects of domain randomization in generating\nsynthetic data for manufacturing object detection applications. To this end, we\npresent a comprehensive data generation pipeline that reflects different\nfactors: object characteristics, background, illumination, camera settings, and\npost-processing. We also introduce the Synthetic Industrial Parts Object\nDetection dataset (SIP15-OD) consisting of 15 objects from three industrial use\ncases under varying environments as a test bed for the study, while also\nemploying an industrial dataset publicly available for robotic applications. In\nour experiments, we present more abundant results and insights into the\nfeasibility as well as challenges of sim-to-real object detection. In\nparticular, we identified material properties, rendering methods,\npost-processing, and distractors as important factors. Our method, leveraging\nthese, achieves top performance on the public dataset with Yolov8 models\ntrained exclusively on synthetic data; mAP@50 scores of 96.4% for the robotics\ndataset, and 94.1%, 99.5%, and 95.3% across three of the SIP15-OD use cases,\nrespectively. The results showcase the effectiveness of the proposed domain\nrandomization, potentially covering the distribution close to real data for the\napplications.", "AI": {"tldr": "The paper proposes a domain randomization pipeline for synthetic data generation in manufacturing object detection, achieving high performance with Yolov8 models trained on synthetic data.", "motivation": "To address the challenges of sim-to-real object detection by generating synthetic data that closely mimics real-world conditions.", "method": "A comprehensive data generation pipeline considering object characteristics, background, illumination, camera settings, and post-processing, tested on the SIP15-OD dataset and a public robotics dataset.", "result": "High mAP@50 scores: 96.4% for the robotics dataset and up to 99.5% for SIP15-OD use cases.", "conclusion": "Domain randomization effectively bridges the sim-to-real gap, with material properties, rendering, and post-processing being key factors."}}
{"id": "2506.07646", "pdf": "https://arxiv.org/pdf/2506.07646", "abs": "https://arxiv.org/abs/2506.07646", "authors": ["Rui Hu", "Xiaolong Lin", "Jiawang Liu", "Shixi Huang", "Zhenpeng Zhan"], "title": "Transcript-Prompted Whisper with Dictionary-Enhanced Decoding for Japanese Speech Annotation", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted to INTERSPEECH 2025", "summary": "In this paper, we propose a method for annotating phonemic and prosodic\nlabels on a given audio-transcript pair, aimed at constructing Japanese\ntext-to-speech (TTS) datasets. Our approach involves fine-tuning a large-scale\npre-trained automatic speech recognition (ASR) model, conditioned on ground\ntruth transcripts, to simultaneously output phrase-level graphemes and\nannotation labels. To further correct errors in phonemic labeling, we employ a\ndecoding strategy that utilizes dictionary prior knowledge. The objective\nevaluation results demonstrate that our proposed method outperforms previous\napproaches relying solely on text or audio. The subjective evaluation results\nindicate that the naturalness of speech synthesized by the TTS model, trained\nwith labels annotated using our method, is comparable to that of a model\ntrained with manual annotations.", "AI": {"tldr": "Proposes a method for phonemic and prosodic labeling in Japanese TTS datasets using a fine-tuned ASR model and dictionary-based error correction, outperforming text/audio-only methods.", "motivation": "To improve the construction of Japanese TTS datasets by automating phonemic and prosodic labeling, reducing reliance on manual annotations.", "method": "Fine-tunes a pre-trained ASR model to output graphemes and labels, using dictionary priors for error correction.", "result": "Outperforms text/audio-only methods in objective evaluation; TTS models trained with these labels match manual annotation quality in subjective tests.", "conclusion": "The method effectively automates labeling for TTS datasets, achieving results comparable to manual annotations."}}
{"id": "2506.07542", "pdf": "https://arxiv.org/pdf/2506.07542", "abs": "https://arxiv.org/abs/2506.07542", "authors": ["Bowen Liu", "Weiyi Zhang", "Peranut Chotcomwongse", "Xiaolan Chen", "Ruoyu Chen", "Pawin Pakaymaskul", "Niracha Arjkongharn", "Nattaporn Vongsa", "Xuelian Cheng", "Zongyuan Ge", "Kun Huang", "Xiaohui Li", "Yiru Duan", "Zhenbang Wang", "BaoYe Xie", "Qiang Chen", "Huazhu Fu", "Michael A. Mahr", "Jiaqi Qu", "Wangyiyang Chen", "Shiye Wang", "Yubo Tan", "Yongjie Li", "Mingguang He", "Danli Shi", "Paisan Ruamviboonsuk"], "title": "APTOS-2024 challenge report: Generation of synthetic 3D OCT images from fundus photographs", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Optical Coherence Tomography (OCT) provides high-resolution, 3D, and\nnon-invasive visualization of retinal layers in vivo, serving as a critical\ntool for lesion localization and disease diagnosis. However, its widespread\nadoption is limited by equipment costs and the need for specialized operators.\nIn comparison, 2D color fundus photography offers faster acquisition and\ngreater accessibility with less dependence on expensive devices. Although\ngenerative artificial intelligence has demonstrated promising results in\nmedical image synthesis, translating 2D fundus images into 3D OCT images\npresents unique challenges due to inherent differences in data dimensionality\nand biological information between modalities. To advance generative models in\nthe fundus-to-3D-OCT setting, the Asia Pacific Tele-Ophthalmology Society\n(APTOS-2024) organized a challenge titled Artificial Intelligence-based OCT\nGeneration from Fundus Images. This paper details the challenge framework\n(referred to as APTOS-2024 Challenge), including: the benchmark dataset,\nevaluation methodology featuring two fidelity metrics-image-based distance\n(pixel-level OCT B-scan similarity) and video-based distance (semantic-level\nvolumetric consistency), and analysis of top-performing solutions. The\nchallenge attracted 342 participating teams, with 42 preliminary submissions\nand 9 finalists. Leading methodologies incorporated innovations in hybrid data\npreprocessing or augmentation (cross-modality collaborative paradigms),\npre-training on external ophthalmic imaging datasets, integration of vision\nfoundation models, and model architecture improvement. The APTOS-2024 Challenge\nis the first benchmark demonstrating the feasibility of fundus-to-3D-OCT\nsynthesis as a potential solution for improving ophthalmic care accessibility\nin under-resourced healthcare settings, while helping to expedite medical\nresearch and clinical applications.", "AI": {"tldr": "The APTOS-2024 Challenge explored AI-based 3D OCT generation from 2D fundus images to address accessibility and cost issues in ophthalmic care.", "motivation": "The high cost and specialized requirements of OCT limit its use, while 2D fundus images are more accessible. AI could bridge this gap by generating 3D OCT from 2D images.", "method": "The challenge involved a benchmark dataset, evaluation metrics (pixel-level and semantic-level fidelity), and analysis of top solutions from 342 teams.", "result": "Leading methods used hybrid preprocessing, external dataset pre-training, vision foundation models, and architecture improvements.", "conclusion": "The challenge demonstrated the feasibility of fundus-to-3D-OCT synthesis, potentially improving accessibility in under-resourced settings."}}
{"id": "2506.07658", "pdf": "https://arxiv.org/pdf/2506.07658", "abs": "https://arxiv.org/abs/2506.07658", "authors": ["Nitin Sharma", "Thomas Wolfers", "\u00c7a\u011fatay Y\u0131ld\u0131z"], "title": "Beyond Benchmarks: A Novel Framework for Domain-Specific LLM Evaluation and Knowledge Mapping", "categories": ["cs.CL", "I.2.7; I.2.6"], "comment": "35 pages, 24 figures. First submission", "summary": "The paper addresses two critical challenges in language model (LM)\nevaluation: creating reliable domain-specific benchmarks and understanding\nknowledge representation during domain adaptation. We introduce a deterministic\npipeline that converts raw domain corpora into completion-type benchmarks\nwithout relying on LMs or human curation, eliminating benchmark contamination\nissues while enabling evaluation on the latest domain data. Our approach\ngenerates domain-specific keywords and related word lists using TF and Term\nTF-IDF methods and constructs prompt-target pairs. We evaluate models by\nmeasuring their ability to complete these prompts with the correct\ndomain-specific targets, providing a direct assessment of domain knowledge with\nlow computational cost. Through comprehensive experiments across multiple\nmodels (GPT-2 medium/XL, Llama-2/3.1, OLMo-2, Qwen-2, Mistral) and domains, we\ndemonstrate that our benchmark strongly correlates with expert-generated\nbenchmarks while providing a more accurate measure of domain knowledge than\ntraditional perplexity metrics. We reveal that domain adaptation happens\nrapidly in smaller models (within 500 steps) and illustrate a new approach to\ndomain knowledge evaluation in base models during training for early stopping.\nBy extending mechanistic analysis to domain adaptation, we discover that\ninitial-to-mid layers are primarily responsible for attribute extraction, while\nlater layers focus on next token prediction. Furthermore, we show that during\nadaptation, forgetting begins in the middle layers, where attribute extraction\nhappens and is amplified in later layers. Our work provides both a practical\nevaluation methodology for domain-specific LMs and novel insights into\nknowledge representation during adaptation, with implications for more\nefficient fine-tuning strategies and targeted approaches to mitigate\ncatastrophic forgetting.", "AI": {"tldr": "The paper introduces a deterministic pipeline for creating domain-specific benchmarks for language model evaluation, revealing insights into domain adaptation and knowledge representation.", "motivation": "Addressing challenges in reliable domain-specific LM evaluation and understanding knowledge representation during domain adaptation.", "method": "A deterministic pipeline converts raw domain corpora into completion-type benchmarks using TF and Term TF-IDF methods, generating prompt-target pairs for evaluation.", "result": "The benchmark correlates with expert-generated ones and provides a more accurate domain knowledge measure than perplexity metrics. Domain adaptation occurs rapidly in smaller models, and mechanistic analysis reveals layer-specific roles and forgetting patterns.", "conclusion": "The work offers a practical evaluation methodology and insights into domain adaptation, aiding efficient fine-tuning and mitigating catastrophic forgetting."}}
{"id": "2506.07555", "pdf": "https://arxiv.org/pdf/2506.07555", "abs": "https://arxiv.org/abs/2506.07555", "authors": ["Haoxiang Wang", "Zinan Lin", "Da Yu", "Huishuai Zhang"], "title": "Synthesize Privacy-Preserving High-Resolution Images via Private Textual Intermediaries", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Generating high fidelity, differentially private (DP) synthetic images offers\na promising route to share and analyze sensitive visual data without\ncompromising individual privacy. However, existing DP image synthesis methods\nstruggle to produce high resolution outputs that faithfully capture the\nstructure of the original data. In this paper, we introduce a novel method,\nreferred to as Synthesis via Private Textual Intermediaries (SPTI), that can\ngenerate high resolution DP images with easy adoption. The key idea is to shift\nthe challenge of DP image synthesis from the image domain to the text domain by\nleveraging state of the art DP text generation methods. SPTI first summarizes\neach private image into a concise textual description using image to text\nmodels, then applies a modified Private Evolution algorithm to generate DP\ntext, and finally reconstructs images using text to image models. Notably, SPTI\nrequires no model training, only inference with off the shelf models. Given a\nprivate dataset, SPTI produces synthetic images of substantially higher quality\nthan prior DP approaches. On the LSUN Bedroom dataset, SPTI attains an FID less\nthan or equal to 26.71 under epsilon equal to 1.0, improving over Private\nEvolution FID of 40.36. Similarly, on MM CelebA HQ, SPTI achieves an FID less\nthan or equal to 33.27 at epsilon equal to 1.0, compared to 57.01 from DP fine\ntuning baselines. Overall, our results demonstrate that Synthesis via Private\nTextual Intermediaries provides a resource efficient and proprietary model\ncompatible framework for generating high resolution DP synthetic images,\ngreatly expanding access to private visual datasets.", "AI": {"tldr": "SPTI generates high-resolution DP synthetic images by converting images to text, applying DP text generation, and reconstructing images, outperforming prior methods.", "motivation": "Existing DP image synthesis methods fail to produce high-resolution outputs that retain original data structure.", "method": "SPTI converts images to text, applies DP text generation, and reconstructs images using off-the-shelf models without training.", "result": "SPTI achieves better FID scores (e.g., 26.71 vs. 40.36 on LSUN Bedroom) than prior DP methods.", "conclusion": "SPTI offers a resource-efficient, high-quality solution for DP synthetic image generation."}}
{"id": "2506.07664", "pdf": "https://arxiv.org/pdf/2506.07664", "abs": "https://arxiv.org/abs/2506.07664", "authors": ["Lei Xu", "Sirui Chen", "Yuxuan Huang", "Chaochao Lu"], "title": "Synthesis by Design: Controlled Data Generation via Structural Guidance", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Mathematical reasoning remains challenging for LLMs due to complex logic and\nthe need for precise computation. Existing methods enhance LLM reasoning by\nsynthesizing datasets through problem rephrasing, but face issues with\ngeneration quality and problem complexity. To address this, we propose to\nextract structural information with generated problem-solving code from\nmathematical reasoning and guide data generation with structured solutions.\nApplied to MATH and GSM8K, our approach produces 39K problems with labeled\nintermediate steps and a 6.1K-problem benchmark of higher difficulty. Results\non our benchmark show that model performance declines as reasoning length\nincreases. Additionally, we conducted fine-tuning experiments using the\nproposed training data on a range of LLMs, and the results validate the\neffectiveness of our dataset. We hope the proposed method and dataset will\ncontribute to future research in enhancing LLM reasoning capabilities.", "AI": {"tldr": "The paper proposes a method to improve LLM mathematical reasoning by extracting structural information and generating problem-solving code, creating a high-quality dataset and benchmark.", "motivation": "Existing methods for enhancing LLM reasoning suffer from poor generation quality and struggle with complex problems.", "method": "Extract structural information and generate problem-solving code to guide data generation, producing labeled datasets.", "result": "Created 39K problems with intermediate steps and a 6.1K-problem benchmark; model performance declines with longer reasoning.", "conclusion": "The method and dataset effectively enhance LLM reasoning, validated by fine-tuning experiments, and contribute to future research."}}
{"id": "2506.07559", "pdf": "https://arxiv.org/pdf/2506.07559", "abs": "https://arxiv.org/abs/2506.07559", "authors": ["Hao Yang", "JianYu Wu", "Run Fang", "Xuelian Zhao", "Yuan Ji", "Zhiyu Chen", "Guibin He", "Junceng Guo", "Yang Liu", "Xinhua Zeng"], "title": "Cross-channel Perception Learning for H&E-to-IHC Virtual Staining", "categories": ["cs.CV"], "comment": null, "summary": "With the rapid development of digital pathology, virtual staining has become\na key technology in multimedia medical information systems, offering new\npossibilities for the analysis and diagnosis of pathological images. However,\nexisting H&E-to-IHC studies often overlook the cross-channel correlations\nbetween cell nuclei and cell membranes. To address this issue, we propose a\nnovel Cross-Channel Perception Learning (CCPL) strategy. Specifically, CCPL\nfirst decomposes HER2 immunohistochemical staining into Hematoxylin and DAB\nstaining channels, corresponding to cell nuclei and cell membranes,\nrespectively. Using the pathology foundation model Gigapath's Tile Encoder,\nCCPL extracts dual-channel features from both the generated and real images and\nmeasures cross-channel correlations between nuclei and membranes. The features\nof the generated and real stained images, obtained through the Tile Encoder,\nare also used to calculate feature distillation loss, enhancing the model's\nfeature extraction capabilities without increasing the inference burden.\nAdditionally, CCPL performs statistical analysis on the focal optical density\nmaps of both single channels to ensure consistency in staining distribution and\nintensity. Experimental results, based on quantitative metrics such as PSNR,\nSSIM, PCC, and FID, along with professional evaluations from pathologists,\ndemonstrate that CCPL effectively preserves pathological features, generates\nhigh-quality virtual stained images, and provides robust support for automated\npathological diagnosis using multimedia medical data.", "AI": {"tldr": "A novel Cross-Channel Perception Learning (CCPL) strategy is proposed to address overlooked cross-channel correlations in H&E-to-IHC studies, improving virtual staining quality for pathological diagnosis.", "motivation": "Existing H&E-to-IHC studies neglect cross-channel correlations between cell nuclei and membranes, limiting analysis and diagnosis accuracy.", "method": "CCPL decomposes HER2 staining into nuclei and membrane channels, extracts dual-channel features using Gigapath's Tile Encoder, measures cross-channel correlations, and ensures staining consistency via statistical analysis.", "result": "CCPL outperforms in metrics (PSNR, SSIM, PCC, FID) and pathologist evaluations, preserving features and enhancing virtual staining quality.", "conclusion": "CCPL effectively improves virtual staining, supporting automated pathological diagnosis with high-quality multimedia medical data."}}
{"id": "2506.07667", "pdf": "https://arxiv.org/pdf/2506.07667", "abs": "https://arxiv.org/abs/2506.07667", "authors": ["Prarabdh Shukla", "Wei Yin Chong", "Yash Patel", "Brennan Schaffner", "Danish Pruthi", "Arjun Bhagoji"], "title": "Silencing Empowerment, Allowing Bigotry: Auditing the Moderation of Hate Speech on Twitch", "categories": ["cs.CL", "cs.HC", "cs.LG"], "comment": null, "summary": "To meet the demands of content moderation, online platforms have resorted to\nautomated systems. Newer forms of real-time engagement($\\textit{e.g.}$, users\ncommenting on live streams) on platforms like Twitch exert additional pressures\non the latency expected of such moderation systems. Despite their prevalence,\nrelatively little is known about the effectiveness of these systems. In this\npaper, we conduct an audit of Twitch's automated moderation tool\n($\\texttt{AutoMod}$) to investigate its effectiveness in flagging hateful\ncontent. For our audit, we create streaming accounts to act as siloed test\nbeds, and interface with the live chat using Twitch's APIs to send over\n$107,000$ comments collated from $4$ datasets. We measure $\\texttt{AutoMod}$'s\naccuracy in flagging blatantly hateful content containing misogyny, racism,\nableism and homophobia. Our experiments reveal that a large fraction of hateful\nmessages, up to $94\\%$ on some datasets, $\\textit{bypass moderation}$.\nContextual addition of slurs to these messages results in $100\\%$ removal,\nrevealing $\\texttt{AutoMod}$'s reliance on slurs as a moderation signal. We\nalso find that contrary to Twitch's community guidelines, $\\texttt{AutoMod}$\nblocks up to $89.5\\%$ of benign examples that use sensitive words in\npedagogical or empowering contexts. Overall, our audit points to large gaps in\n$\\texttt{AutoMod}$'s capabilities and underscores the importance for such\nsystems to understand context effectively.", "AI": {"tldr": "An audit of Twitch's AutoMod reveals it misses up to 94% of hateful content but blocks 89.5% of benign examples, highlighting its reliance on slurs and lack of contextual understanding.", "motivation": "To assess the effectiveness of Twitch's AutoMod in flagging hateful content, given the growing reliance on automated moderation for real-time engagement.", "method": "Created streaming accounts to test AutoMod, sent 107,000 comments from 4 datasets via Twitch's APIs, and measured accuracy in flagging hateful content (misogyny, racism, ableism, homophobia).", "result": "AutoMod misses up to 94% of hateful content but blocks 89.5% of benign examples. It relies heavily on slurs for moderation, lacking contextual understanding.", "conclusion": "AutoMod has significant gaps in effectiveness, emphasizing the need for better contextual understanding in automated moderation systems."}}
{"id": "2506.07565", "pdf": "https://arxiv.org/pdf/2506.07565", "abs": "https://arxiv.org/abs/2506.07565", "authors": ["Jinlu Zhang", "Zixi Kang", "Yizhou Wang"], "title": "OpenDance: Multimodal Controllable 3D Dance Generation Using Large-scale Internet Data", "categories": ["cs.CV"], "comment": null, "summary": "Music-driven dance generation offers significant creative potential yet faces\nconsiderable challenges. The absence of fine-grained multimodal data and the\ndifficulty of flexible multi-conditional generation limit previous works on\ngeneration controllability and diversity in practice. In this paper, we build\nOpenDance5D, an extensive human dance dataset comprising over 101 hours across\n14 distinct genres. Each sample has five modalities to facilitate robust\ncross-modal learning: RGB video, audio, 2D keypoints, 3D motion, and\nfine-grained textual descriptions from human arts. Furthermore, we propose\nOpenDanceNet, a unified masked modeling framework for controllable dance\ngeneration conditioned on music and arbitrary combinations of text prompts,\nkeypoints, or character positioning. Comprehensive experiments demonstrate that\nOpenDanceNet achieves high-fidelity and flexible controllability.", "AI": {"tldr": "The paper introduces OpenDance5D, a large dance dataset, and OpenDanceNet, a framework for controllable dance generation from music and multimodal inputs.", "motivation": "Addressing the lack of fine-grained multimodal data and challenges in flexible multi-conditional dance generation.", "method": "Developed OpenDance5D dataset with 5 modalities and proposed OpenDanceNet, a masked modeling framework for controllable generation.", "result": "OpenDanceNet achieves high-fidelity and flexible controllability in dance generation.", "conclusion": "The work advances dance generation by providing a robust dataset and a versatile framework."}}
{"id": "2506.07671", "pdf": "https://arxiv.org/pdf/2506.07671", "abs": "https://arxiv.org/abs/2506.07671", "authors": ["Ionut-Teodor Sorodoc", "Leonardo F. R. Ribeiro", "Rexhina Blloshmi", "Christopher Davis", "Adri\u00e0 de Gispert"], "title": "GaRAGe: A Benchmark with Grounding Annotations for RAG Evaluation", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 (Findings)", "summary": "We present GaRAGe, a large RAG benchmark with human-curated long-form answers\nand annotations of each grounding passage, allowing a fine-grained evaluation\nof whether LLMs can identify relevant grounding when generating RAG answers.\nOur benchmark contains 2366 questions of diverse complexity, dynamism, and\ntopics, and includes over 35K annotated passages retrieved from both private\ndocument sets and the Web, to reflect real-world RAG use cases. This makes it\nan ideal test bed to evaluate an LLM's ability to identify only the relevant\ninformation necessary to compose a response, or provide a deflective response\nwhen there is insufficient information. Evaluations of multiple\nstate-of-the-art LLMs on GaRAGe show that the models tend to over-summarise\nrather than (a) ground their answers strictly on the annotated relevant\npassages (reaching at most a Relevance-Aware Factuality Score of 60%), or (b)\ndeflect when no relevant grounding is available (reaching at most 31% true\npositive rate in deflections). The F1 in attribution to relevant sources is at\nmost 58.9%, and we show that performance is particularly reduced when answering\ntime-sensitive questions and when having to draw knowledge from sparser private\ngrounding sources.", "AI": {"tldr": "GaRAGe is a benchmark for evaluating LLMs' ability to ground answers in relevant passages or deflect when information is insufficient, revealing current models' shortcomings in these tasks.", "motivation": "To provide a fine-grained evaluation of LLMs' grounding and deflection capabilities in RAG systems, reflecting real-world use cases.", "method": "The benchmark includes 2366 diverse questions and 35K annotated passages, evaluating LLMs on relevance, factuality, and deflection.", "result": "LLMs struggle with grounding (max 60% relevance-aware factuality) and deflection (max 31% true positive rate), especially for time-sensitive or sparse private sources.", "conclusion": "GaRAGe highlights the need for improved LLM capabilities in grounding and deflection for practical RAG applications."}}
{"id": "2506.07566", "pdf": "https://arxiv.org/pdf/2506.07566", "abs": "https://arxiv.org/abs/2506.07566", "authors": ["Marco Peer", "Robert Sablatnig", "Florian Kleber"], "title": "Towards the Influence of Text Quantity on Writer Retrieval", "categories": ["cs.CV"], "comment": "accepted for ICDAR2025", "summary": "This paper investigates the task of writer retrieval, which identifies\ndocuments authored by the same individual within a dataset based on handwriting\nsimilarities. While existing datasets and methodologies primarily focus on page\nlevel retrieval, we explore the impact of text quantity on writer retrieval\nperformance by evaluating line- and word level retrieval. We examine three\nstate-of-the-art writer retrieval systems, including both handcrafted and deep\nlearning-based approaches, and analyze their performance using varying amounts\nof text. Our experiments on the CVL and IAM dataset demonstrate that while\nperformance decreases by 20-30% when only one line of text is used as query and\ngallery, retrieval accuracy remains above 90% of full-page performance when at\nleast four lines are included. We further show that text-dependent retrieval\ncan maintain strong performance in low-text scenarios. Our findings also\nhighlight the limitations of handcrafted features in low-text scenarios, with\ndeep learning-based methods like NetVLAD outperforming traditional VLAD\nencoding.", "AI": {"tldr": "The paper explores writer retrieval performance at line- and word-level, showing deep learning methods outperform handcrafted features in low-text scenarios.", "motivation": "To understand how text quantity affects writer retrieval accuracy, moving beyond page-level analysis.", "method": "Evaluated three state-of-the-art writer retrieval systems (handcrafted and deep learning-based) on CVL and IAM datasets using varying text quantities.", "result": "Performance drops 20-30% with one line but stays above 90% of full-page accuracy with four lines. Deep learning (NetVLAD) outperforms handcrafted features in low-text cases.", "conclusion": "Text-dependent retrieval is viable in low-text scenarios, with deep learning methods showing superior performance over traditional approaches."}}
{"id": "2506.07691", "pdf": "https://arxiv.org/pdf/2506.07691", "abs": "https://arxiv.org/abs/2506.07691", "authors": ["Jiaming Li", "Haoran Ye", "Yukun Chen", "Xinyue Li", "Lei Zhang", "Hamid Alinejad-Rokny", "Jimmy Chih-Hsien Peng", "Min Yang"], "title": "Training Superior Sparse Autoencoders for Instruct Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "As large language models (LLMs) grow in scale and capability, understanding\ntheir internal mechanisms becomes increasingly critical. Sparse autoencoders\n(SAEs) have emerged as a key tool in mechanistic interpretability, enabling the\nextraction of human-interpretable features from LLMs. However, existing SAE\ntraining methods are primarily designed for base models, resulting in reduced\nreconstruction quality and interpretability when applied to instruct models. To\nbridge this gap, we propose\n$\\underline{\\textbf{F}}$inetuning-$\\underline{\\textbf{a}}$ligned\n$\\underline{\\textbf{S}}$equential $\\underline{\\textbf{T}}$raining\n($\\textit{FAST}$), a novel training method specifically tailored for instruct\nmodels. $\\textit{FAST}$ aligns the training process with the data distribution\nand activation patterns characteristic of instruct models, resulting in\nsubstantial improvements in both reconstruction and feature interpretability.\nOn Qwen2.5-7B-Instruct, $\\textit{FAST}$ achieves a mean squared error of 0.6468\nin token reconstruction, significantly outperforming baseline methods with\nerrors of 5.1985 and 1.5096. In feature interpretability, $\\textit{FAST}$\nyields a higher proportion of high-quality features, for Llama3.2-3B-Instruct,\n$21.1\\%$ scored in the top range, compared to $7.0\\%$ and $10.2\\%$ for\n$\\textit{BT(P)}$ and $\\textit{BT(F)}$. Surprisingly, we discover that\nintervening on the activations of special tokens via the SAEs leads to\nimprovements in output quality, suggesting new opportunities for fine-grained\ncontrol of model behavior. Code, data, and 240 trained SAEs are available at\nhttps://github.com/Geaming2002/FAST.", "AI": {"tldr": "The paper introduces FAST, a training method for sparse autoencoders (SAEs) tailored to instruct models, improving reconstruction quality and feature interpretability compared to baseline methods.", "motivation": "Existing SAE training methods are designed for base models, leading to poor performance when applied to instruct models. The goal is to bridge this gap.", "method": "Proposes FAST, aligning SAE training with instruct models' data distribution and activation patterns.", "result": "FAST outperforms baselines in reconstruction (lower MSE) and feature interpretability (higher quality features). It also discovers benefits from intervening on special tokens.", "conclusion": "FAST is effective for instruct models, offering better performance and new control opportunities. Code and trained SAEs are publicly available."}}
{"id": "2506.07570", "pdf": "https://arxiv.org/pdf/2506.07570", "abs": "https://arxiv.org/abs/2506.07570", "authors": ["Yixuan Yang", "Zhen Luo", "Tongsheng Ding", "Junru Lu", "Mingqi Gao", "Jinyu Yang", "Victor Sanchez", "Feng Zheng"], "title": "LLM-driven Indoor Scene Layout Generation via Scaled Human-aligned Data Synthesis and Multi-Stage Preference Optimization", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Automatic indoor layout generation has attracted increasing attention due to\nits potential in interior design, virtual environment construction, and\nembodied AI. Existing methods fall into two categories: prompt-driven\napproaches that leverage proprietary LLM services (e.g., GPT APIs) and\nlearning-based methods trained on layout data upon diffusion-based models.\nPrompt-driven methods often suffer from spatial inconsistency and high\ncomputational costs, while learning-based methods are typically constrained by\ncoarse relational graphs and limited datasets, restricting their generalization\nto diverse room categories. In this paper, we revisit LLM-based indoor layout\ngeneration and present 3D-SynthPlace, a large-scale dataset that combines\nsynthetic layouts generated via a 'GPT synthesize, Human inspect' pipeline,\nupgraded from the 3D-Front dataset. 3D-SynthPlace contains nearly 17,000\nscenes, covering four common room types -- bedroom, living room, kitchen, and\nbathroom -- enriched with diverse objects and high-level spatial annotations.\nWe further introduce OptiScene, a strong open-source LLM optimized for indoor\nlayout generation, fine-tuned based on our 3D-SynthPlace dataset through our\ntwo-stage training. For the warum-up stage I, we adopt supervised fine-tuning\n(SFT), which is taught to first generate high-level spatial descriptions then\nconditionally predict concrete object placements. For the reinforcing stage II,\nto better align the generated layouts with human design preferences, we apply\nmulti-turn direct preference optimization (DPO), which significantly improving\nlayout quality and generation success rates. Extensive experiments demonstrate\nthat OptiScene outperforms traditional prompt-driven and learning-based\nbaselines. Moreover, OptiScene shows promising potential in interactive tasks\nsuch as scene editing and robot navigation.", "AI": {"tldr": "The paper introduces 3D-SynthPlace, a large-scale dataset for indoor layout generation, and OptiScene, an optimized LLM for this task, outperforming existing methods.", "motivation": "Addressing limitations in existing indoor layout generation methods, such as spatial inconsistency, high computational costs, and limited generalization.", "method": "Combines synthetic data generation via a 'GPT synthesize, Human inspect' pipeline and a two-stage training approach (SFT and DPO) for LLM optimization.", "result": "OptiScene outperforms traditional prompt-driven and learning-based baselines, showing promise in interactive tasks.", "conclusion": "The proposed dataset and LLM offer improved performance and generalization for indoor layout generation, with potential applications in scene editing and robot navigation."}}
{"id": "2506.07712", "pdf": "https://arxiv.org/pdf/2506.07712", "abs": "https://arxiv.org/abs/2506.07712", "authors": ["Renjie Luo", "Jiaxi Li", "Chen Huang", "Wei Lu"], "title": "Through the Valley: Path to Effective Long CoT Training for Small Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Long chain-of-thought (CoT) supervision has become a common strategy to\nenhance reasoning in language models. While effective for large models, we\nidentify a phenomenon we call Long CoT Degradation, in which small language\nmodels (SLMs; <=3B parameters) trained on limited long CoT data experience\nsignificant performance deterioration. Through extensive experiments on the\nQwen2.5, LLaMA3 and Gemma3 families, we demonstrate that this degradation is\nwidespread across SLMs. In some settings, models trained on only 8k long CoT\nexamples lose up to 75% of their original performance before fine-tuning.\nStrikingly, we further observe that for some particularly small models, even\ntraining on 220k long CoT examples fails to recover or surpass their original\nperformance prior to fine-tuning. Our analysis attributes this effect to error\naccumulation: while longer responses increase the capacity for multi-step\nreasoning, they also amplify the risk of compounding mistakes. Furthermore, we\nfind that Long CoT Degradation may negatively impacts downstream reinforcement\nlearning (RL), although this can be alleviated by sufficiently scaled\nsupervised fine-tuning (SFT). Our findings challenge common assumptions about\nthe benefits of long CoT training for SLMs and offer practical guidance for\nbuilding more effective small-scale reasoning models.", "AI": {"tldr": "Small language models (SLMs) suffer performance degradation when trained on long chain-of-thought (CoT) data, termed Long CoT Degradation, due to error accumulation. This effect is widespread and impacts downstream tasks like RL, though mitigated by scaled supervised fine-tuning.", "motivation": "To investigate why small language models (SLMs) perform poorly when trained on long CoT data, despite its effectiveness for larger models.", "method": "Conducted experiments on Qwen2.5, LLaMA3, and Gemma3 families, analyzing performance degradation with varying CoT data sizes (8k to 220k examples).", "result": "SLMs lose up to 75% performance with 8k long CoT examples, and some fail to recover even with 220k examples. Error accumulation in longer responses is the primary cause.", "conclusion": "Long CoT training harms SLMs due to error accumulation, challenging assumptions about its benefits. Practical guidance is provided for improving small-scale reasoning models."}}
{"id": "2506.07572", "pdf": "https://arxiv.org/pdf/2506.07572", "abs": "https://arxiv.org/abs/2506.07572", "authors": ["Yu Li", "Feng Xue", "Shujie Li", "Jinrui Zhang", "Shuang Yang", "Dan Guo", "Richang Hong"], "title": "Learning Speaker-Invariant Visual Features for Lipreading", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Lipreading is a challenging cross-modal task that aims to convert visual lip\nmovements into spoken text. Existing lipreading methods often extract visual\nfeatures that include speaker-specific lip attributes (e.g., shape, color,\ntexture), which introduce spurious correlations between vision and text. These\ncorrelations lead to suboptimal lipreading accuracy and restrict model\ngeneralization. To address this challenge, we introduce SIFLip, a\nspeaker-invariant visual feature learning framework that disentangles\nspeaker-specific attributes using two complementary disentanglement modules\n(Implicit Disentanglement and Explicit Disentanglement) to improve\ngeneralization. Specifically, since different speakers exhibit semantic\nconsistency between lip movements and phonetic text when pronouncing the same\nwords, our implicit disentanglement module leverages stable text embeddings as\nsupervisory signals to learn common visual representations across speakers,\nimplicitly decoupling speaker-specific features. Additionally, we design a\nspeaker recognition sub-task within the main lipreading pipeline to filter\nspeaker-specific features, then further explicitly disentangle these\npersonalized visual features from the backbone network via gradient reversal.\nExperimental results demonstrate that SIFLip significantly enhances\ngeneralization performance across multiple public datasets. Experimental\nresults demonstrate that SIFLip significantly improves generalization\nperformance across multiple public datasets, outperforming state-of-the-art\nmethods.", "AI": {"tldr": "SIFLip introduces a speaker-invariant feature learning framework for lipreading, using implicit and explicit disentanglement to improve generalization by removing speaker-specific attributes.", "motivation": "Existing lipreading methods suffer from spurious correlations due to speaker-specific features, reducing accuracy and generalization.", "method": "SIFLip uses two disentanglement modules: implicit (leveraging text embeddings) and explicit (via speaker recognition and gradient reversal).", "result": "SIFLip outperforms state-of-the-art methods, enhancing generalization across multiple datasets.", "conclusion": "The framework effectively disentangles speaker-specific features, improving lipreading accuracy and generalization."}}
{"id": "2506.07719", "pdf": "https://arxiv.org/pdf/2506.07719", "abs": "https://arxiv.org/abs/2506.07719", "authors": ["Mengyang Qiu", "Tran Minh Nguyen", "Zihao Huang", "Zelong Li", "Yang Gu", "Qingyu Gao", "Siliang Liu", "Jungyeul Park"], "title": "Multilingual Grammatical Error Annotation: Combining Language-Agnostic Framework with Language-Specific Flexibility", "categories": ["cs.CL"], "comment": "BEA2025", "summary": "Grammatical Error Correction (GEC) relies on accurate error annotation and\nevaluation, yet existing frameworks, such as $\\texttt{errant}$, face\nlimitations when extended to typologically diverse languages. In this paper, we\nintroduce a standardized, modular framework for multilingual grammatical error\nannotation. Our approach combines a language-agnostic foundation with\nstructured language-specific extensions, enabling both consistency and\nflexibility across languages. We reimplement $\\texttt{errant}$ using\n$\\texttt{stanza}$ to support broader multilingual coverage, and demonstrate the\nframework's adaptability through applications to English, German, Czech,\nKorean, and Chinese, ranging from general-purpose annotation to more customized\nlinguistic refinements. This work supports scalable and interpretable GEC\nannotation across languages and promotes more consistent evaluation in\nmultilingual settings. The complete codebase and annotation tools can be\naccessed at https://github.com/open-writing-evaluation/jp_errant_bea.", "AI": {"tldr": "A modular framework for multilingual grammatical error annotation is introduced, improving consistency and flexibility across diverse languages.", "motivation": "Existing frameworks like $\texttt{errant}$ have limitations in handling typologically diverse languages, necessitating a standardized, adaptable solution.", "method": "The approach combines a language-agnostic foundation with language-specific extensions, reimplementing $\texttt{errant}$ using $\texttt{stanza}$ for broader multilingual support.", "result": "The framework is successfully applied to English, German, Czech, Korean, and Chinese, demonstrating adaptability for general and customized annotation.", "conclusion": "This work enables scalable, interpretable GEC annotation and promotes consistent evaluation in multilingual settings, with tools available on GitHub."}}
{"id": "2506.07575", "pdf": "https://arxiv.org/pdf/2506.07575", "abs": "https://arxiv.org/abs/2506.07575", "authors": ["Ruiyang Zhang", "Hu Zhang", "Hao Fei", "Zhedong Zheng"], "title": "Uncertainty-o: One Model-agnostic Framework for Unveiling Uncertainty in Large Multimodal Models", "categories": ["cs.CV", "cs.LG"], "comment": "Project page: https://uncertainty-o.github.io/", "summary": "Large Multimodal Models (LMMs), harnessing the complementarity among diverse\nmodalities, are often considered more robust than pure Language Large Models\n(LLMs); yet do LMMs know what they do not know? There are three key open\nquestions remaining: (1) how to evaluate the uncertainty of diverse LMMs in a\nunified manner, (2) how to prompt LMMs to show its uncertainty, and (3) how to\nquantify uncertainty for downstream tasks. In an attempt to address these\nchallenges, we introduce Uncertainty-o: (1) a model-agnostic framework designed\nto reveal uncertainty in LMMs regardless of their modalities, architectures, or\ncapabilities, (2) an empirical exploration of multimodal prompt perturbations\nto uncover LMM uncertainty, offering insights and findings, and (3) derive the\nformulation of multimodal semantic uncertainty, which enables quantifying\nuncertainty from multimodal responses. Experiments across 18 benchmarks\nspanning various modalities and 10 LMMs (both open- and closed-source)\ndemonstrate the effectiveness of Uncertainty-o in reliably estimating LMM\nuncertainty, thereby enhancing downstream tasks such as hallucination\ndetection, hallucination mitigation, and uncertainty-aware Chain-of-Thought\nreasoning.", "AI": {"tldr": "Uncertainty-o is a model-agnostic framework for evaluating and quantifying uncertainty in Large Multimodal Models (LMMs), addressing key challenges in unified evaluation, prompting, and downstream task application.", "motivation": "To address the lack of methods for evaluating and quantifying uncertainty in LMMs, despite their perceived robustness compared to pure Language Large Models (LLMs).", "method": "Introduces Uncertainty-o, a framework for unified uncertainty evaluation, explores multimodal prompt perturbations, and formulates multimodal semantic uncertainty.", "result": "Demonstrates effectiveness across 18 benchmarks and 10 LMMs, improving tasks like hallucination detection and mitigation.", "conclusion": "Uncertainty-o reliably estimates LMM uncertainty, enhancing downstream applications and providing insights into model robustness."}}
{"id": "2506.07726", "pdf": "https://arxiv.org/pdf/2506.07726", "abs": "https://arxiv.org/abs/2506.07726", "authors": ["Vincenzo Timmel", "Manfred Vogel", "Daniel Perruchoud", "Reza Kakooee"], "title": "Swiss Parliaments Corpus Re-Imagined (SPC_R): Enhanced Transcription with RAG-based Correction and Predicted BLEU", "categories": ["cs.CL"], "comment": null, "summary": "This paper presents a new long-form release of the Swiss Parliaments Corpus,\nconverting entire multi-hour Swiss German debate sessions (each aligned with\nthe official session protocols) into high-quality speech-text pairs. Our\npipeline starts by transcribing all session audio into Standard German using\nWhisper Large-v3 under high-compute settings. We then apply a two-step GPT-4o\ncorrection process: first, GPT-4o ingests the raw Whisper output alongside the\nofficial protocols to refine misrecognitions, mainly named entities. Second, a\nseparate GPT-4o pass evaluates each refined segment for semantic completeness.\nWe filter out any segments whose Predicted BLEU score (derived from Whisper's\naverage token log-probability) and GPT-4o evaluation score fall below a certain\nthreshold. The final corpus contains 801 hours of audio, of which 751 hours\npass our quality control. Compared to the original sentence-level SPC release,\nour long-form dataset achieves a 6-point BLEU improvement, demonstrating the\npower of combining robust ASR, LLM-based correction, and data-driven filtering\nfor low-resource, domain-specific speech corpora.", "AI": {"tldr": "The paper introduces a long-form Swiss Parliaments Corpus, using Whisper and GPT-4o for high-quality speech-text alignment, achieving a 6-point BLEU improvement.", "motivation": "To create a high-quality, domain-specific speech corpus for Swiss German debates by leveraging advanced ASR and LLM-based correction.", "method": "Transcribes audio with Whisper Large-v3, refines with GPT-4o using official protocols, and filters segments based on BLEU and GPT-4o scores.", "result": "Produced 751 hours of quality-controlled audio, showing a 6-point BLEU improvement over the original release.", "conclusion": "Combining robust ASR, LLM correction, and data-driven filtering effectively enhances low-resource speech corpora."}}
{"id": "2506.07576", "pdf": "https://arxiv.org/pdf/2506.07576", "abs": "https://arxiv.org/abs/2506.07576", "authors": ["Boyu Chen", "Siran Chen", "Kunchang Li", "Qinglin Xu", "Yu Qiao", "Yali Wang"], "title": "Super Encoding Network: Recursive Association of Multi-Modal Encoders for Video Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Video understanding has been considered as one critical step towards world\nmodeling, which is an important long-term problem in AI research. Recently,\nmulti-modal foundation models have shown such potential via large-scale\npretraining. However, these models simply align encoders of different\nmodalities via contrastive learning, while lacking deeper multi-modal\ninteractions, which is critical for understanding complex target movements with\ndiversified video scenes. To fill this gap, we propose a unified Super Encoding\nNetwork (SEN) for video understanding, which builds up such distinct\ninteractions through recursive association of multi-modal encoders in the\nfoundation models. Specifically, we creatively treat those well-trained\nencoders as \"super neurons\" in our SEN. Via designing a Recursive Association\n(RA) block, we progressively fuse multi-modalities with the input video, based\non knowledge integrating, distributing, and prompting of super neurons in a\nrecursive manner. In this way, our SEN can effectively encode deeper\nmulti-modal interactions, for prompting various video understanding tasks in\ndownstream. Extensive experiments show that, our SEN can remarkably boost the\nfour most representative video tasks, including tracking, recognition,\nchatting, and editing, e.g., for pixel-level tracking, the average jaccard\nindex improves 2.7%, temporal coherence(TC) drops 8.8% compared to the popular\nCaDeX++ approach. For one-shot video editing, textual alignment improves 6.4%,\nand frame consistency increases 4.1% compared to the popular TuneA-Video\napproach.", "AI": {"tldr": "The paper introduces a Super Encoding Network (SEN) to enhance video understanding by enabling deeper multi-modal interactions through recursive association of encoders, outperforming existing methods in tasks like tracking, recognition, chatting, and editing.", "motivation": "Current multi-modal foundation models lack deeper multi-modal interactions, limiting their ability to understand complex video scenes and movements.", "method": "Proposes SEN, treating pre-trained encoders as 'super neurons' and using a Recursive Association (RA) block to fuse multi-modalities recursively.", "result": "SEN improves performance in four video tasks, e.g., boosting tracking accuracy by 2.7% and enhancing video editing metrics by 4.1-6.4%.", "conclusion": "SEN effectively addresses the gap in multi-modal interactions, advancing video understanding capabilities."}}
{"id": "2506.07751", "pdf": "https://arxiv.org/pdf/2506.07751", "abs": "https://arxiv.org/abs/2506.07751", "authors": ["Silin Gao", "Antoine Bosselut", "Samy Bengio", "Emmanuel Abbe"], "title": "Augmenting LLMs' Reasoning by Reinforcing Abstract Thinking", "categories": ["cs.CL", "cs.AI", "cs.SC"], "comment": "Under review", "summary": "Recent studies have shown that large language models (LLMs), especially\nsmaller ones, often lack robustness in their reasoning. I.e., they tend to\nexperience performance drops when faced with distribution shifts, such as\nchanges to numerical or nominal variables, or insertions of distracting\nclauses. A possible strategy to address this involves generating synthetic data\nto further \"instantiate\" reasoning problems on potential variations. In\ncontrast, our approach focuses on \"abstracting\" reasoning problems. This not\nonly helps counteract distribution shifts but also facilitates the connection\nto symbolic tools for deriving solutions. We find that this abstraction process\nis better acquired through reinforcement learning (RL) than just supervised\nfine-tuning, which often fails to produce faithful abstractions. Our method,\nAbstraL -- which promotes abstract reasoning in LLMs using RL on granular\nabstraction data -- significantly mitigates performance degradation on recent\nGSM perturbation benchmarks.", "AI": {"tldr": "The paper introduces AbstraL, a method using reinforcement learning to enhance abstract reasoning in LLMs, mitigating performance drops from distribution shifts.", "motivation": "Addressing LLMs' lack of robustness in reasoning, especially under distribution shifts, by shifting focus from synthetic data generation to abstracting reasoning problems.", "method": "Uses reinforcement learning (RL) to train LLMs on granular abstraction data, promoting faithful abstractions over supervised fine-tuning.", "result": "AbstraL significantly reduces performance degradation on GSM perturbation benchmarks.", "conclusion": "Abstracting reasoning problems via RL improves LLM robustness against distribution shifts and connects better to symbolic tools."}}
{"id": "2506.07590", "pdf": "https://arxiv.org/pdf/2506.07590", "abs": "https://arxiv.org/abs/2506.07590", "authors": ["Jiacheng Shi", "Yanfu Zhang", "Huajie Shao", "Ashley Gao"], "title": "Explore the vulnerability of black-box models via diffusion models", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Recent advancements in diffusion models have enabled high-fidelity and\nphotorealistic image generation across diverse applications. However, these\nmodels also present security and privacy risks, including copyright violations,\nsensitive information leakage, and the creation of harmful or offensive content\nthat could be exploited maliciously. In this study, we uncover a novel security\nthreat where an attacker leverages diffusion model APIs to generate synthetic\nimages, which are then used to train a high-performing substitute model. This\nenables the attacker to execute model extraction and transfer-based adversarial\nattacks on black-box classification models with minimal queries, without\nneeding access to the original training data. The generated images are\nsufficiently high-resolution and diverse to train a substitute model whose\noutputs closely match those of the target model. Across the seven benchmarks,\nincluding CIFAR and ImageNet subsets, our method shows an average improvement\nof 27.37% over state-of-the-art methods while using just 0.01 times of the\nquery budget, achieving a 98.68% success rate in adversarial attacks on the\ntarget model.", "AI": {"tldr": "A novel security threat in diffusion models allows attackers to generate synthetic images via APIs, train substitute models, and execute adversarial attacks on black-box models with minimal queries.", "motivation": "Addressing security and privacy risks in diffusion models, such as copyright violations, sensitive data leakage, and malicious content creation.", "method": "Leveraging diffusion model APIs to generate synthetic images for training substitute models, enabling efficient adversarial attacks without original data.", "result": "Achieves 27.37% average improvement over state-of-the-art methods with 0.01x query budget and 98.68% adversarial attack success rate.", "conclusion": "Demonstrates significant security vulnerabilities in diffusion models, highlighting the need for robust defenses against model extraction and adversarial attacks."}}
{"id": "2506.07795", "pdf": "https://arxiv.org/pdf/2506.07795", "abs": "https://arxiv.org/abs/2506.07795", "authors": ["Xiaotian Ye", "Mengqi Zhang", "Shu Wu"], "title": "LLM Unlearning Should Be Form-Independent", "categories": ["cs.CL", "cs.CR", "cs.LG"], "comment": null, "summary": "Large Language Model (LLM) unlearning aims to erase or suppress undesirable\nknowledge within the model, offering promise for controlling harmful or private\ninformation to prevent misuse. However, recent studies highlight its limited\nefficacy in real-world scenarios, hindering practical adoption. In this study,\nwe identify a pervasive issue underlying many downstream failures: the\neffectiveness of existing unlearning methods heavily depends on the form of\ntraining samples and frequently fails to generalize to alternate expressions of\nthe same knowledge. We formally characterize this problem as Form-Dependent\nBias and systematically investigate its specific manifestation patterns across\nvarious downstream tasks. To quantify its prevalence and support future\nresearch, we introduce ORT, a novel benchmark designed to evaluate the\nrobustness of unlearning methods against variations in knowledge expression.\nResults reveal that Form-Dependent Bias is both widespread and severe among\ncurrent techniques.\n  We argue that LLM unlearning should be form-independent to address the\nendless forms of downstream tasks encountered in real-world security-critical\nscenarios. Towards this goal, we introduce Rank-one Concept Redirection (ROCR),\na novel training-free method, as a promising solution path. ROCR performs\nunlearning by targeting the invariants in downstream tasks, specifically the\nactivated dangerous concepts. It is capable of modifying model parameters\nwithin seconds to redirect the model's perception of a specific unlearning\ntarget concept to another harmless concept. Extensive experiments demonstrate\nthat ROCR significantly improves unlearning effectiveness compared to\ntraditional methods while generating highly natural outputs.", "AI": {"tldr": "The paper addresses the issue of Form-Dependent Bias in LLM unlearning, introduces the ORT benchmark, and proposes ROCR, a training-free method to improve unlearning robustness.", "motivation": "Current LLM unlearning methods struggle with real-world efficacy due to Form-Dependent Bias, limiting their practical adoption for controlling harmful or private information.", "method": "The study introduces ORT to benchmark unlearning robustness and proposes ROCR, a training-free method targeting invariants in downstream tasks to redirect harmful concepts.", "result": "Results show Form-Dependent Bias is widespread and severe; ROCR outperforms traditional methods in effectiveness and output naturalness.", "conclusion": "LLM unlearning should be form-independent, and ROCR offers a promising solution for real-world security-critical scenarios."}}
{"id": "2506.07600", "pdf": "https://arxiv.org/pdf/2506.07600", "abs": "https://arxiv.org/abs/2506.07600", "authors": ["Nianbo Zeng", "Haowen Hou", "Fei Richard Yu", "Si Shi", "Ying Tiffany He"], "title": "SceneRAG: Scene-level Retrieval-Augmented Generation for Video Understanding", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Despite recent advances in retrieval-augmented generation (RAG) for video\nunderstanding, effectively understanding long-form video content remains\nunderexplored due to the vast scale and high complexity of video data. Current\nRAG approaches typically segment videos into fixed-length chunks, which often\ndisrupts the continuity of contextual information and fails to capture\nauthentic scene boundaries. Inspired by the human ability to naturally organize\ncontinuous experiences into coherent scenes, we present SceneRAG, a unified\nframework that leverages large language models to segment videos into\nnarrative-consistent scenes by processing ASR transcripts alongside temporal\nmetadata. SceneRAG further sharpens these initial boundaries through\nlightweight heuristics and iterative correction. For each scene, the framework\nfuses information from both visual and textual modalities to extract entity\nrelations and dynamically builds a knowledge graph, enabling robust multi-hop\nretrieval and generation that account for long-range dependencies. Experiments\non the LongerVideos benchmark, featuring over 134 hours of diverse content,\nconfirm that SceneRAG substantially outperforms prior baselines, achieving a\nwin rate of up to 72.5 percent on generation tasks.", "AI": {"tldr": "SceneRAG improves video understanding by segmenting videos into narrative-consistent scenes using ASR transcripts and temporal metadata, outperforming baselines with a 72.5% win rate.", "motivation": "Current RAG methods disrupt video context by using fixed-length chunks, failing to capture authentic scene boundaries. SceneRAG addresses this by mimicking human scene organization.", "method": "SceneRAG uses large language models to segment videos into coherent scenes via ASR transcripts and temporal metadata, refines boundaries with heuristics, and builds dynamic knowledge graphs for multi-hop retrieval.", "result": "SceneRAG achieves a 72.5% win rate on the LongerVideos benchmark, significantly outperforming prior methods.", "conclusion": "SceneRAG effectively addresses the challenges of long-form video understanding by leveraging narrative-consistent segmentation and multimodal fusion."}}
{"id": "2506.07801", "pdf": "https://arxiv.org/pdf/2506.07801", "abs": "https://arxiv.org/abs/2506.07801", "authors": ["Iustin Sirbu", "Robert-Adrian Popovici", "Cornelia Caragea", "Stefan Trausan-Matu", "Traian Rebedea"], "title": "MultiMatch: Multihead Consistency Regularization Matching for Semi-Supervised Text Classification", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "comment": null, "summary": "We introduce MultiMatch, a novel semi-supervised learning (SSL) algorithm\ncombining the paradigms of co-training and consistency regularization with\npseudo-labeling. At its core, MultiMatch features a three-fold pseudo-label\nweighting module designed for three key purposes: selecting and filtering\npseudo-labels based on head agreement and model confidence, and weighting them\naccording to the perceived classification difficulty. This novel module\nenhances and unifies three existing techniques -- heads agreement from\nMultihead Co-training, self-adaptive thresholds from FreeMatch, and Average\nPseudo-Margins from MarginMatch -- resulting in a holistic approach that\nimproves robustness and performance in SSL settings. Experimental results on\nbenchmark datasets highlight the superior performance of MultiMatch, achieving\nstate-of-the-art results on 9 out of 10 setups from 5 natural language\nprocessing datasets and ranking first according to the Friedman test among 19\nmethods. Furthermore, MultiMatch demonstrates exceptional robustness in highly\nimbalanced settings, outperforming the second-best approach by 3.26% -- and\ndata imbalance is a key factor for many text classification tasks.", "AI": {"tldr": "MultiMatch is a semi-supervised learning algorithm combining co-training and consistency regularization with pseudo-labeling, featuring a three-fold pseudo-label weighting module for improved performance and robustness.", "motivation": "To enhance semi-supervised learning by unifying and improving existing techniques for better robustness and performance, especially in imbalanced settings.", "method": "Combines co-training and consistency regularization with pseudo-labeling, using a novel three-fold pseudo-label weighting module for selection, filtering, and weighting.", "result": "Achieves state-of-the-art results on 9/10 setups across 5 NLP datasets and excels in imbalanced settings, outperforming the second-best method by 3.26%.", "conclusion": "MultiMatch is a robust and high-performing SSL algorithm, particularly effective for imbalanced text classification tasks."}}
{"id": "2506.07603", "pdf": "https://arxiv.org/pdf/2506.07603", "abs": "https://arxiv.org/abs/2506.07603", "authors": ["Jianhui Wei", "Zikai Xiao", "Danyu Sun", "Luqi Gong", "Zongxin Yang", "Zuozhu Liu", "Jian Wu"], "title": "SurgBench: A Unified Large-Scale Benchmark for Surgical Video Analysis", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Surgical video understanding is pivotal for enabling automated intraoperative\ndecision-making, skill assessment, and postoperative quality improvement.\nHowever, progress in developing surgical video foundation models (FMs) remains\nhindered by the scarcity of large-scale, diverse datasets for pretraining and\nsystematic evaluation. In this paper, we introduce \\textbf{SurgBench}, a\nunified surgical video benchmarking framework comprising a pretraining dataset,\n\\textbf{SurgBench-P}, and an evaluation benchmark, \\textbf{SurgBench-E}.\nSurgBench offers extensive coverage of diverse surgical scenarios, with\nSurgBench-P encompassing 53 million frames across 22 surgical procedures and 11\nspecialties, and SurgBench-E providing robust evaluation across six categories\n(phase classification, camera motion, tool recognition, disease diagnosis,\naction classification, and organ detection) spanning 72 fine-grained tasks.\nExtensive experiments reveal that existing video FMs struggle to generalize\nacross varied surgical video analysis tasks, whereas pretraining on SurgBench-P\nyields substantial performance improvements and superior cross-domain\ngeneralization to unseen procedures and modalities. Our dataset and code are\navailable upon request.", "AI": {"tldr": "SurgBench is a surgical video benchmarking framework with a pretraining dataset (SurgBench-P) and evaluation benchmark (SurgBench-E), addressing the lack of diverse datasets for surgical video foundation models.", "motivation": "The scarcity of large-scale, diverse datasets for pretraining and evaluating surgical video foundation models hinders progress in automated intraoperative decision-making and skill assessment.", "method": "The authors introduce SurgBench, comprising SurgBench-P (53M frames across 22 procedures) for pretraining and SurgBench-E (72 tasks across 6 categories) for evaluation.", "result": "Existing video foundation models struggle with generalization, but pretraining on SurgBench-P improves performance and cross-domain generalization.", "conclusion": "SurgBench provides a robust framework for advancing surgical video analysis, with pretraining on SurgBench-P enhancing model performance and adaptability."}}
{"id": "2506.07818", "pdf": "https://arxiv.org/pdf/2506.07818", "abs": "https://arxiv.org/abs/2506.07818", "authors": ["Zhiyu Lin", "Zhengda Zhou", "Zhiyuan Zhao", "Tianrui Wan", "Yilun Ma", "Junyu Gao", "Xuelong Li"], "title": "WebUIBench: A Comprehensive Benchmark for Evaluating Multimodal Large Language Models in WebUI-to-Code", "categories": ["cs.CL"], "comment": null, "summary": "With the rapid advancement of Generative AI technology, Multimodal Large\nLanguage Models(MLLMs) have the potential to act as AI software engineers\ncapable of executing complex web application development. Considering that the\nmodel requires a confluence of multidimensional sub-capabilities to address the\nchallenges of various development phases, constructing a multi-view evaluation\nframework is crucial for accurately guiding the enhancement of development\nefficiency. However, existing benchmarks usually fail to provide an assessment\nof sub-capabilities and focus solely on webpage generation outcomes. In this\nwork, we draw inspiration from the principles of software engineering and\nfurther propose WebUIBench, a benchmark systematically designed to evaluate\nMLLMs in four key areas: WebUI Perception, HTML Programming,WebUI-HTML\nUnderstanding, and WebUI-to-Code. WebUIBench comprises 21K high-quality\nquestion-answer pairs derived from over 0.7K real-world websites. The extensive\nevaluation of 29 mainstream MLLMs uncovers the skill characteristics and\nvarious weakness that models encountered during the development process.", "AI": {"tldr": "WebUIBench is a benchmark for evaluating Multimodal Large Language Models (MLLMs) in web development, focusing on four key areas: WebUI Perception, HTML Programming, WebUI-HTML Understanding, and WebUI-to-Code.", "motivation": "Existing benchmarks lack comprehensive evaluation of sub-capabilities in web development, focusing only on outcomes. WebUIBench addresses this gap by systematically assessing MLLMs.", "method": "The benchmark includes 21K question-answer pairs from 0.7K real-world websites, evaluating 29 MLLMs across four key areas.", "result": "The evaluation reveals skill characteristics and weaknesses of MLLMs in web development.", "conclusion": "WebUIBench provides a structured framework to enhance MLLMs' development efficiency by identifying and addressing their weaknesses."}}
{"id": "2506.07611", "pdf": "https://arxiv.org/pdf/2506.07611", "abs": "https://arxiv.org/abs/2506.07611", "authors": ["Yuan Zhou", "Junbao Zhou", "Qingshan Xu", "Kesen Zhao", "Yuxuan Wang", "Hao Fei", "Richang Hong", "Hanwang Zhang"], "title": "DragNeXt: Rethinking Drag-Based Image Editing", "categories": ["cs.CV"], "comment": null, "summary": "Drag-Based Image Editing (DBIE), which allows users to manipulate images by\ndirectly dragging objects within them, has recently attracted much attention\nfrom the community. However, it faces two key challenges:\n(\\emph{\\textcolor{magenta}{i}}) point-based drag is often highly ambiguous and\ndifficult to align with users' intentions; (\\emph{\\textcolor{magenta}{ii}})\ncurrent DBIE methods primarily rely on alternating between motion supervision\nand point tracking, which is not only cumbersome but also fails to produce\nhigh-quality results. These limitations motivate us to explore DBIE from a new\nperspective -- redefining it as deformation, rotation, and translation of\nuser-specified handle regions. Thereby, by requiring users to explicitly\nspecify both drag areas and types, we can effectively address the ambiguity\nissue. Furthermore, we propose a simple-yet-effective editing framework, dubbed\n\\textcolor{SkyBlue}{\\textbf{DragNeXt}}. It unifies DBIE as a Latent Region\nOptimization (LRO) problem and solves it through Progressive Backward\nSelf-Intervention (PBSI), simplifying the overall procedure of DBIE while\nfurther enhancing quality by fully leveraging region-level structure\ninformation and progressive guidance from intermediate drag states. We validate\n\\textcolor{SkyBlue}{\\textbf{DragNeXt}} on our NextBench, and extensive\nexperiments demonstrate that our proposed method can significantly outperform\nexisting approaches. Code will be released on github.", "AI": {"tldr": "DragNeXt simplifies Drag-Based Image Editing (DBIE) by redefining it as deformation, rotation, and translation of handle regions, addressing ambiguity and improving quality through Latent Region Optimization and Progressive Backward Self-Intervention.", "motivation": "Current DBIE methods are ambiguous and cumbersome, relying on point-based drag and alternating motion supervision and tracking, leading to poor results.", "method": "DragNeXt unifies DBIE as a Latent Region Optimization problem, solved via Progressive Backward Self-Intervention, leveraging region-level structure and intermediate drag states.", "result": "DragNeXt outperforms existing methods on NextBench, demonstrating superior quality and simplicity.", "conclusion": "DragNeXt offers a more effective and user-friendly approach to DBIE, resolving ambiguity and enhancing results."}}
{"id": "2506.07851", "pdf": "https://arxiv.org/pdf/2506.07851", "abs": "https://arxiv.org/abs/2506.07851", "authors": ["Yiju Guo", "Wenkai Yang", "Zexu Sun", "Ning Ding", "Zhiyuan Liu", "Yankai Lin"], "title": "Learning to Focus: Causal Attention Distillation via Gradient-Guided Token Pruning", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated significant improvements in\ncontextual understanding. However, their ability to attend to truly critical\ninformation during long-context reasoning and generation still falls behind the\npace. Specifically, our preliminary experiments reveal that certain distracting\npatterns can misdirect the model's attention during inference, and removing\nthese patterns substantially improves reasoning accuracy and generation\nquality. We attribute this phenomenon to spurious correlations in the training\ndata, which obstruct the model's capacity to infer authentic causal\ninstruction-response relationships. This phenomenon may induce redundant\nreasoning processes, potentially resulting in significant inference overhead\nand, more critically, the generation of erroneous or suboptimal responses. To\nmitigate this, we introduce a two-stage framework called Learning to Focus\n(LeaF) leveraging intervention-based inference to disentangle confounding\nfactors. In the first stage, LeaF employs gradient-based comparisons with an\nadvanced teacher to automatically identify confounding tokens based on causal\nrelationships in the training corpus. Then, in the second stage, it prunes\nthese tokens during distillation to enact intervention, aligning the student's\nattention with the teacher's focus distribution on truly critical context\ntokens. Experimental results demonstrate that LeaF not only achieves an\nabsolute improvement in various mathematical reasoning and code generation\nbenchmarks but also effectively suppresses attention to confounding tokens\nduring inference, yielding a more interpretable and reliable reasoning model.", "AI": {"tldr": "LeaF is a two-stage framework to improve LLMs' focus on critical information by identifying and pruning confounding tokens, enhancing reasoning and generation.", "motivation": "LLMs struggle with attending to critical information due to distracting patterns and spurious correlations in training data, leading to redundant reasoning and errors.", "method": "LeaF uses gradient-based comparisons to identify confounding tokens and prunes them during distillation to align attention with critical context.", "result": "LeaF improves reasoning and code generation benchmarks and reduces attention to confounding tokens, yielding a more reliable model.", "conclusion": "LeaF effectively mitigates spurious correlations, enhancing LLMs' focus and interpretability in reasoning tasks."}}
{"id": "2506.07612", "pdf": "https://arxiv.org/pdf/2506.07612", "abs": "https://arxiv.org/abs/2506.07612", "authors": ["Zikang Leng", "Archith Iyer", "Thomas Pl\u00f6tz"], "title": "Scaling Human Activity Recognition: A Comparative Evaluation of Synthetic Data Generation and Augmentation Techniques", "categories": ["cs.CV"], "comment": null, "summary": "Human activity recognition (HAR) is often limited by the scarcity of labeled\ndatasets due to the high cost and complexity of real-world data collection. To\nmitigate this, recent work has explored generating virtual inertial measurement\nunit (IMU) data via cross-modality transfer. While video-based and\nlanguage-based pipelines have each shown promise, they differ in assumptions\nand computational cost. Moreover, their effectiveness relative to traditional\nsensor-level data augmentation remains unclear. In this paper, we present a\ndirect comparison between these two virtual IMU generation approaches against\nclassical data augmentation techniques. We construct a large-scale virtual IMU\ndataset spanning 100 diverse activities from Kinetics-400 and simulate sensor\nsignals at 22 body locations. The three data generation strategies are\nevaluated on benchmark HAR datasets (UTD-MHAD, PAMAP2, HAD-AW) using four\npopular models. Results show that virtual IMU data significantly improves\nperformance over real or augmented data alone, particularly under limited-data\nconditions. We offer practical guidance on choosing data generation strategies\nand highlight the distinct advantages and disadvantages of each approach.", "AI": {"tldr": "The paper compares video-based and language-based virtual IMU data generation with classical augmentation for HAR, showing virtual data outperforms real or augmented data, especially with limited data.", "motivation": "Addressing the scarcity of labeled HAR datasets by exploring cross-modality transfer for virtual IMU data generation.", "method": "Direct comparison of video-based, language-based, and classical augmentation techniques using a large-scale virtual IMU dataset and benchmark HAR datasets.", "result": "Virtual IMU data significantly improves HAR performance over real or augmented data, particularly in limited-data scenarios.", "conclusion": "Provides practical guidance on choosing data generation strategies, highlighting pros and cons of each approach."}}
{"id": "2506.07899", "pdf": "https://arxiv.org/pdf/2506.07899", "abs": "https://arxiv.org/abs/2506.07899", "authors": ["Ke Wang", "Yiming Qin", "Nikolaos Dimitriadis", "Alessandro Favero", "Pascal Frossard"], "title": "MEMOIR: Lifelong Model Editing with Minimal Overwrite and Informed Retention for LLMs", "categories": ["cs.CL", "cs.LG"], "comment": "The first two authors contributed equally to this work", "summary": "Language models deployed in real-world systems often require post-hoc updates\nto incorporate new or corrected knowledge. However, editing such models\nefficiently and reliably - without retraining or forgetting previous\ninformation - remains a major challenge. Existing methods for lifelong model\nediting either compromise generalization, interfere with past edits, or fail to\nscale to long editing sequences. We propose MEMOIR, a novel scalable framework\nthat injects knowledge through a residual memory, i.e., a dedicated parameter\nmodule, while preserving the core capabilities of the pre-trained model. By\nsparsifying input activations through sample-dependent masks, MEMOIR confines\neach edit to a distinct subset of the memory parameters, minimizing\ninterference among edits. At inference, it identifies relevant edits by\ncomparing the sparse activation patterns of new queries to those stored during\nediting. This enables generalization to rephrased queries by activating only\nthe relevant knowledge while suppressing unnecessary memory activation for\nunrelated prompts. Experiments on question answering, hallucination correction,\nand out-of-distribution generalization benchmarks across LLaMA-3 and Mistral\ndemonstrate that MEMOIR achieves state-of-the-art performance across\nreliability, generalization, and locality metrics, scaling to thousands of\nsequential edits with minimal forgetting.", "AI": {"tldr": "MEMOIR is a scalable framework for updating language models efficiently by using a residual memory module, minimizing interference between edits and enabling generalization to rephrased queries.", "motivation": "The challenge of updating language models post-deployment without retraining or forgetting previous knowledge motivates the need for a reliable and scalable editing method.", "method": "MEMOIR uses a residual memory module with sample-dependent masks to isolate edits and sparse activation patterns to identify relevant knowledge during inference.", "result": "MEMOIR outperforms existing methods in reliability, generalization, and scalability, handling thousands of edits with minimal forgetting.", "conclusion": "MEMOIR provides a robust solution for lifelong model editing, balancing efficiency, scalability, and performance."}}
{"id": "2506.07627", "pdf": "https://arxiv.org/pdf/2506.07627", "abs": "https://arxiv.org/abs/2506.07627", "authors": ["Haotong Qin", "Cheng Hu", "Michele Magno"], "title": "Event-Priori-Based Vision-Language Model for Efficient Visual Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Large Language Model (LLM)-based Vision-Language Models (VLMs) have\nsubstantially extended the boundaries of visual understanding capabilities.\nHowever, their high computational demands hinder deployment on\nresource-constrained edge devices. A key source of inefficiency stems from the\nVLM's need to process dense and redundant visual information. Visual inputs\ncontain significant regions irrelevant to text semantics, rendering the\nassociated computations ineffective for inference. This paper introduces a\nnovel Event-Priori-Based Vision-Language Model, termed EP-VLM. Its core\ncontribution is a novel mechanism leveraging motion priors derived from dynamic\nevent vision to enhance VLM efficiency. Inspired by human visual cognition,\nEP-VLM first employs event data to guide the patch-wise sparsification of RGB\nvisual inputs, progressively concentrating VLM computation on salient regions\nof the visual input. Subsequently, we construct a position-preserving\ntokenization strategy for the visual encoder within the VLM architecture. This\nstrategy processes the event-guided, unstructured, sparse visual input while\naccurately preserving positional understanding within the visual input.\nExperimental results demonstrate that EP-VLM achieves significant efficiency\nimprovements while maintaining nearly lossless accuracy compared to baseline\nmodels from the Qwen2-VL series. For instance, against the original\nQwen2-VL-2B, EP-VLM achieves 50% FLOPs savings while retaining 98% of the\noriginal accuracy on the RealWorldQA dataset. This work demonstrates the\npotential of event-based vision priors for improving VLM inference efficiency,\npaving the way for creating more efficient and deployable VLMs for sustainable\nvisual understanding at the edge.", "AI": {"tldr": "EP-VLM improves VLM efficiency by using event-based motion priors to sparsify visual inputs, reducing computation while maintaining accuracy.", "motivation": "High computational demands of VLMs hinder deployment on edge devices due to redundant visual processing.", "method": "EP-VLM leverages event data for patch-wise sparsification of RGB inputs and uses a position-preserving tokenization strategy.", "result": "EP-VLM reduces FLOPs by 50% while retaining 98% accuracy on RealWorldQA compared to Qwen2-VL-2B.", "conclusion": "Event-based vision priors enhance VLM efficiency, enabling sustainable edge deployment."}}
{"id": "2506.07900", "pdf": "https://arxiv.org/pdf/2506.07900", "abs": "https://arxiv.org/abs/2506.07900", "authors": ["MiniCPM Team", "Chaojun Xiao", "Yuxuan Li", "Xu Han", "Yuzhuo Bai", "Jie Cai", "Haotian Chen", "Wentong Chen", "Xin Cong", "Ganqu Cui", "Ning Ding", "Shengdan Fan", "Yewei Fang", "Zixuan Fu", "Wenyu Guan", "Yitong Guan", "Junshao Guo", "Yufeng Han", "Bingxiang He", "Yuxiang Huang", "Cunliang Kong", "Qiuzuo Li", "Siyuan Li", "Wenhao Li", "Yanghao Li", "Yishan Li", "Zhen Li", "Dan Liu", "Biyuan Lin", "Yankai Lin", "Xiang Long", "Quanyu Lu", "Yaxi Lu", "Peiyan Luo", "Hongya Lyu", "Litu Ou", "Yinxu Pan", "Zekai Qu", "Qundong Shi", "Zijun Song", "Jiayuan Su", "Zhou Su", "Ao Sun", "Xianghui Sun", "Peijun Tang", "Fangzheng Wang", "Feng Wang", "Shuo Wang", "Yudong Wang", "Yesai Wu", "Zhenyu Xiao", "Jie Xie", "Zihao Xie", "Yukun Yan", "Jiarui Yuan", "Kaihuo Zhang", "Lei Zhang", "Linyue Zhang", "Xueren Zhang", "Yudi Zhang", "Hengyu Zhao", "Weilin Zhao", "Weilun Zhao", "Yuanqian Zhao", "Zhi Zheng", "Ge Zhou", "Jie Zhou", "Wei Zhou", "Zihan Zhou", "Zixuan Zhou", "Zhiyuan Liu", "Guoyang Zeng", "Chao Jia", "Dahai Li", "Maosong Sun"], "title": "MiniCPM4: Ultra-Efficient LLMs on End Devices", "categories": ["cs.CL", "cs.AI"], "comment": "MiniCPM4 Technical Report", "summary": "This paper introduces MiniCPM4, a highly efficient large language model (LLM)\ndesigned explicitly for end-side devices. We achieve this efficiency through\nsystematic innovation in four key dimensions: model architecture, training\ndata, training algorithms, and inference systems. Specifically, in terms of\nmodel architecture, we propose InfLLM v2, a trainable sparse attention\nmechanism that accelerates both prefilling and decoding phases for long-context\nprocessing. Regarding training data, we propose UltraClean, an efficient and\naccurate pre-training data filtering and generation strategy, and UltraChat v2,\na comprehensive supervised fine-tuning dataset. These datasets enable\nsatisfactory model performance to be achieved using just 8 trillion training\ntokens. Regarding training algorithms, we propose ModelTunnel v2 for efficient\npre-training strategy search, and improve existing post-training methods by\nintroducing chunk-wise rollout for load-balanced reinforcement learning and\ndata-efficient tenary LLM, BitCPM. Regarding inference systems, we propose\nCPM.cu that integrates sparse attention, model quantization, and speculative\nsampling to achieve efficient prefilling and decoding. To meet diverse\non-device requirements, MiniCPM4 is available in two versions, with 0.5B and 8B\nparameters, respectively. Sufficient evaluation results show that MiniCPM4\noutperforms open-source models of similar size across multiple benchmarks,\nhighlighting both its efficiency and effectiveness. Notably, MiniCPM4-8B\ndemonstrates significant speed improvements over Qwen3-8B when processing long\nsequences. Through further adaptation, MiniCPM4 successfully powers diverse\napplications, including trustworthy survey generation and tool use with model\ncontext protocol, clearly showcasing its broad usability.", "AI": {"tldr": "MiniCPM4 is an efficient LLM for end-side devices, optimized in architecture, training data, algorithms, and inference systems, outperforming similar-sized models.", "motivation": "To create a highly efficient large language model tailored for end-side devices through systematic innovations.", "method": "Innovations include InfLLM v2 for sparse attention, UltraClean and UltraChat v2 for data, ModelTunnel v2 and BitCPM for training, and CPM.cu for inference.", "result": "MiniCPM4 outperforms similar-sized models, with MiniCPM4-8B showing speed improvements over Qwen3-8B in long sequences.", "conclusion": "MiniCPM4 is efficient, effective, and versatile, suitable for diverse applications like survey generation and tool use."}}
{"id": "2506.07628", "pdf": "https://arxiv.org/pdf/2506.07628", "abs": "https://arxiv.org/abs/2506.07628", "authors": ["Weronika Smolak-Dy\u017cewska", "Dawid Malarz", "Grzegorz Wilczy\u0144ski", "Rafa\u0142 Tobiasz", "Joanna Waczy\u0144ska", "Piotr Borycki", "Przemys\u0142aw Spurek"], "title": "HuSc3D: Human Sculpture dataset for 3D object reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "3D scene reconstruction from 2D images is one of the most important tasks in\ncomputer graphics. Unfortunately, existing datasets and benchmarks concentrate\non idealized synthetic or meticulously captured realistic data. Such benchmarks\nfail to convey the inherent complexities encountered in newly acquired\nreal-world scenes. In such scenes especially those acquired outside, the\nbackground is often dynamic, and by popular usage of cell phone cameras, there\nmight be discrepancies in, e.g., white balance. To address this gap, we present\nHuSc3D, a novel dataset specifically designed for rigorous benchmarking of 3D\nreconstruction models under realistic acquisition challenges. Our dataset\nuniquely features six highly detailed, fully white sculptures characterized by\nintricate perforations and minimal textural and color variation. Furthermore,\nthe number of images per scene varies significantly, introducing the additional\nchallenge of limited training data for some instances alongside scenes with a\nstandard number of views. By evaluating popular 3D reconstruction methods on\nthis diverse dataset, we demonstrate the distinctiveness of HuSc3D in\neffectively differentiating model performance, particularly highlighting the\nsensitivity of methods to fine geometric details, color ambiguity, and varying\ndata availability--limitations often masked by more conventional datasets.", "AI": {"tldr": "The paper introduces HuSc3D, a dataset for benchmarking 3D reconstruction models under real-world challenges like dynamic backgrounds and limited training data.", "motivation": "Existing datasets lack realism, failing to address complexities like dynamic backgrounds and color discrepancies in real-world scenes.", "method": "HuSc3D features six detailed white sculptures with perforations and minimal texture, varying image counts per scene.", "result": "The dataset effectively differentiates model performance, revealing sensitivities to geometry, color ambiguity, and data availability.", "conclusion": "HuSc3D fills a gap in benchmarking by exposing limitations masked by conventional datasets."}}
{"id": "2506.07937", "pdf": "https://arxiv.org/pdf/2506.07937", "abs": "https://arxiv.org/abs/2506.07937", "authors": ["Shamminuj Aktar", "Andreas B\u00e4rtschi", "Abdel-Hameed A. Badawy", "Stephan Eidenbenz"], "title": "Quantum Graph Transformer for NLP Sentiment Classification", "categories": ["cs.CL", "quant-ph"], "comment": null, "summary": "Quantum machine learning is a promising direction for building more efficient\nand expressive models, particularly in domains where understanding complex,\nstructured data is critical. We present the Quantum Graph Transformer (QGT), a\nhybrid graph-based architecture that integrates a quantum self-attention\nmechanism into the message-passing framework for structured language modeling.\nThe attention mechanism is implemented using parameterized quantum circuits\n(PQCs), which enable the model to capture rich contextual relationships while\nsignificantly reducing the number of trainable parameters compared to classical\nattention mechanisms. We evaluate QGT on five sentiment classification\nbenchmarks. Experimental results show that QGT consistently achieves higher or\ncomparable accuracy than existing quantum natural language processing (QNLP)\nmodels, including both attention-based and non-attention-based approaches. When\ncompared with an equivalent classical graph transformer, QGT yields an average\naccuracy improvement of 5.42% on real-world datasets and 4.76% on synthetic\ndatasets. Additionally, QGT demonstrates improved sample efficiency, requiring\nnearly 50% fewer labeled samples to reach comparable performance on the Yelp\ndataset. These results highlight the potential of graph-based QNLP techniques\nfor advancing efficient and scalable language understanding.", "AI": {"tldr": "The paper introduces the Quantum Graph Transformer (QGT), a hybrid quantum-classical model for structured language modeling, showing improved accuracy and efficiency over existing methods.", "motivation": "To leverage quantum computing for more efficient and expressive machine learning models, especially in complex data domains like language understanding.", "method": "QGT integrates a quantum self-attention mechanism using parameterized quantum circuits (PQCs) into a graph-based message-passing framework.", "result": "QGT outperforms existing QNLP models, achieving 5.42% and 4.76% higher accuracy on real-world and synthetic datasets, respectively, and requires 50% fewer labeled samples.", "conclusion": "Graph-based QNLP techniques, exemplified by QGT, hold promise for scalable and efficient language understanding."}}
{"id": "2506.07637", "pdf": "https://arxiv.org/pdf/2506.07637", "abs": "https://arxiv.org/abs/2506.07637", "authors": ["Yuchong Long", "Wen Sun", "Ningxiao Sun", "Wenxiao Wang", "Chao Li", "Shan Yin"], "title": "HieraEdgeNet: A Multi-Scale Edge-Enhanced Framework for Automated Pollen Recognition", "categories": ["cs.CV", "cs.LG", "68T07, 68T45", "I.2.10; I.4.9; I.5.4"], "comment": "16 pages, 5 figures, 2 tables. The dataset at\n  https://www.kaggle.com/datasets/ayinven/hieraedgenetintegratesdatasets. The\n  models at\n  https://huggingface.co/datasets/AyinMostima/HieraEdgeNetintegratesdatasets.\n  The source code in at https://github.com/AyinMostima/PalynoKit", "summary": "Automated pollen recognition is vital to paleoclimatology, biodiversity\nmonitoring, and public health, yet conventional methods are hampered by\ninefficiency and subjectivity. Existing deep learning models often struggle to\nachieve the requisite localization accuracy for microscopic targets like\npollen, which are characterized by their minute size, indistinct edges, and\ncomplex backgrounds. To overcome this limitation, we introduce HieraEdgeNet, a\nmulti-scale edge-enhancement framework. The framework's core innovation is the\nintroduction of three synergistic modules: the Hierarchical Edge Module (HEM),\nwhich explicitly extracts a multi-scale pyramid of edge features that\ncorresponds to the semantic hierarchy at early network stages; the Synergistic\nEdge Fusion (SEF) module, for deeply fusing these edge priors with semantic\ninformation at each respective scale; and the Cross Stage Partial Omni-Kernel\nModule (CSPOKM), which maximally refines the most detail-rich feature layers\nusing an Omni-Kernel operator - comprising anisotropic large-kernel\nconvolutions and mixed-domain attention - all within a computationally\nefficient Cross-Stage Partial (CSP) framework. On a large-scale dataset\ncomprising 120 pollen classes, HieraEdgeNet achieves a mean Average Precision\n(mAP@.5) of 0.9501, significantly outperforming state-of-the-art baseline\nmodels such as YOLOv12n and RT-DETR. Furthermore, qualitative analysis confirms\nthat our approach generates feature representations that are more precisely\nfocused on object boundaries. By systematically integrating edge information,\nHieraEdgeNet provides a robust and powerful solution for high-precision,\nhigh-efficiency automated detection of microscopic objects.", "AI": {"tldr": "HieraEdgeNet introduces a multi-scale edge-enhancement framework for high-precision automated pollen recognition, outperforming existing models.", "motivation": "Current deep learning models lack accuracy for microscopic objects like pollen due to their small size and complex backgrounds.", "method": "HieraEdgeNet uses three modules: HEM for multi-scale edge features, SEF for fusing edge priors with semantic info, and CSPOKM for refining detail-rich layers.", "result": "Achieves mAP@.5 of 0.9501 on 120 pollen classes, surpassing YOLOv12n and RT-DETR.", "conclusion": "HieraEdgeNet offers a robust solution for precise, efficient microscopic object detection by integrating edge information."}}
{"id": "2506.07947", "pdf": "https://arxiv.org/pdf/2506.07947", "abs": "https://arxiv.org/abs/2506.07947", "authors": ["Paulius Rauba", "Qiyao Wei", "Mihaela van der Schaar"], "title": "Statistical Hypothesis Testing for Auditing Robustness in Language Models", "categories": ["cs.CL"], "comment": "arXiv admin note: substantial text overlap with arXiv:2412.00868", "summary": "Consider the problem of testing whether the outputs of a large language model\n(LLM) system change under an arbitrary intervention, such as an input\nperturbation or changing the model variant. We cannot simply compare two LLM\noutputs since they might differ due to the stochastic nature of the system, nor\ncan we compare the entire output distribution due to computational\nintractability. While existing methods for analyzing text-based outputs exist,\nthey focus on fundamentally different problems, such as measuring bias or\nfairness. To this end, we introduce distribution-based perturbation analysis, a\nframework that reformulates LLM perturbation analysis as a frequentist\nhypothesis testing problem. We construct empirical null and alternative output\ndistributions within a low-dimensional semantic similarity space via Monte\nCarlo sampling, enabling tractable inference without restrictive distributional\nassumptions. The framework is (i) model-agnostic, (ii) supports the evaluation\nof arbitrary input perturbations on any black-box LLM, (iii) yields\ninterpretable p-values; (iv) supports multiple perturbations via controlled\nerror rates; and (v) provides scalar effect sizes. We demonstrate the\nusefulness of the framework across multiple case studies, showing how we can\nquantify response changes, measure true/false positive rates, and evaluate\nalignment with reference models. Above all, we see this as a reliable\nfrequentist hypothesis testing framework for LLM auditing.", "AI": {"tldr": "The paper introduces a framework for testing changes in LLM outputs under interventions, using semantic similarity and hypothesis testing.", "motivation": "Existing methods for analyzing LLM outputs focus on bias or fairness, not changes due to interventions.", "method": "Proposes distribution-based perturbation analysis, constructing empirical null and alternative distributions via Monte Carlo sampling in a low-dimensional space.", "result": "The framework is model-agnostic, tractable, interpretable, and supports multiple perturbations with controlled error rates.", "conclusion": "It provides a reliable frequentist hypothesis testing method for LLM auditing, demonstrated through case studies."}}
{"id": "2506.07643", "pdf": "https://arxiv.org/pdf/2506.07643", "abs": "https://arxiv.org/abs/2506.07643", "authors": ["Jae Sung Park", "Zixian Ma", "Linjie Li", "Chenhao Zheng", "Cheng-Yu Hsieh", "Ximing Lu", "Khyathi Chandu", "Quan Kong", "Norimasa Kobori", "Ali Farhadi", "Yejin Choi", "Ranjay Krishna"], "title": "Synthetic Visual Genome", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Reasoning over visual relationships-spatial, functional, interactional,\nsocial, etc.-is considered to be a fundamental component of human cognition.\nYet, despite the major advances in visual comprehension in multimodal language\nmodels (MLMs), precise reasoning over relationships and their generations\nremains a challenge. We introduce ROBIN: an MLM instruction-tuned with densely\nannotated relationships capable of constructing high-quality dense scene graphs\nat scale. To train ROBIN, we curate SVG, a synthetic scene graph dataset by\ncompleting the missing relations of selected objects in existing scene graphs\nusing a teacher MLM and a carefully designed filtering process to ensure\nhigh-quality. To generate more accurate and rich scene graphs at scale for any\nimage, we introduce SG-EDIT: a self-distillation framework where GPT-4o further\nrefines ROBIN's predicted scene graphs by removing unlikely relations and/or\nsuggesting relevant ones. In total, our dataset contains 146K images and 5.6M\nrelationships for 2.6M objects. Results show that our ROBIN-3B model, despite\nbeing trained on less than 3 million instances, outperforms similar-size models\ntrained on over 300 million instances on relationship understanding benchmarks,\nand even surpasses larger models up to 13B parameters. Notably, it achieves\nstate-of-the-art performance in referring expression comprehension with a score\nof 88.9, surpassing the previous best of 87.4. Our results suggest that\ntraining on the refined scene graph data is crucial to maintaining high\nperformance across diverse visual reasoning task.", "AI": {"tldr": "ROBIN, an MLM trained on densely annotated relationships, outperforms larger models in visual reasoning tasks by leveraging synthetic scene graphs refined by GPT-4o.", "motivation": "Precise reasoning over visual relationships remains a challenge in MLMs, despite advances in visual comprehension.", "method": "ROBIN is trained on SVG, a synthetic dataset with missing relations filled by a teacher MLM and filtered for quality. SG-EDIT refines scene graphs using GPT-4o.", "result": "ROBIN-3B outperforms similar and larger models, achieving SOTA in referring expression comprehension (88.9).", "conclusion": "Refined scene graph data is key to high performance in diverse visual reasoning tasks."}}
{"id": "2506.07956", "pdf": "https://arxiv.org/pdf/2506.07956", "abs": "https://arxiv.org/abs/2506.07956", "authors": ["Tim Vieira", "Tianyu Liu", "Clemente Pasti", "Yahya Emara", "Brian DuSell", "Benjamin LeBrun", "Mario Giulianelli", "Juan Luis Gastaldi", "Timothy J. O'Donnell", "Ryan Cotterell"], "title": "Language Models over Canonical Byte-Pair Encodings", "categories": ["cs.CL", "cs.FL", "cs.LG"], "comment": "ICML 2025", "summary": "Modern language models represent probability distributions over character\nstrings as distributions over (shorter) token strings derived via a\ndeterministic tokenizer, such as byte-pair encoding. While this approach is\nhighly effective at scaling up language models to large corpora, its current\nincarnations have a concerning property: the model assigns nonzero probability\nmass to an exponential number of $\\it{noncanonical}$ token encodings of each\ncharacter string -- these are token strings that decode to valid character\nstrings but are impossible under the deterministic tokenizer (i.e., they will\nnever be seen in any training corpus, no matter how large). This misallocation\nis both erroneous, as noncanonical strings never appear in training data, and\nwasteful, diverting probability mass away from plausible outputs. These are\navoidable mistakes! In this work, we propose methods to enforce canonicality in\ntoken-level language models, ensuring that only canonical token strings are\nassigned positive probability. We present two approaches: (1) canonicality by\nconditioning, leveraging test-time inference strategies without additional\ntraining, and (2) canonicality by construction, a model parameterization that\nguarantees canonical outputs but requires training. We demonstrate that fixing\ncanonicality mistakes improves the likelihood of held-out data for several\nmodels and corpora.", "AI": {"tldr": "The paper addresses the issue of noncanonical token encodings in language models, proposing two methods to enforce canonicality and improve model efficiency.", "motivation": "Current tokenization methods in language models allocate probability to noncanonical token strings, which are never seen in training data, leading to inefficiencies and errors.", "method": "Two approaches are proposed: (1) canonicality by conditioning (test-time inference) and (2) canonicality by construction (model parameterization requiring training).", "result": "Enforcing canonicality improves the likelihood of held-out data across various models and corpora.", "conclusion": "The proposed methods effectively eliminate noncanonical token encodings, enhancing model accuracy and efficiency."}}
{"id": "2506.07652", "pdf": "https://arxiv.org/pdf/2506.07652", "abs": "https://arxiv.org/abs/2506.07652", "authors": ["Hangbei Cheng", "Xiaorong Dong", "Xueyu Liu", "Jianan Zhang", "Xuetao Ma", "Mingqiang Wei", "Liansheng Wang", "Junxin Chen", "Yongfei Wu"], "title": "FMaMIL: Frequency-Driven Mamba Multi-Instance Learning for Weakly Supervised Lesion Segmentation in Medical Images", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Accurate lesion segmentation in histopathology images is essential for\ndiagnostic interpretation and quantitative analysis, yet it remains challenging\ndue to the limited availability of costly pixel-level annotations. To address\nthis, we propose FMaMIL, a novel two-stage framework for weakly supervised\nlesion segmentation based solely on image-level labels. In the first stage, a\nlightweight Mamba-based encoder is introduced to capture long-range\ndependencies across image patches under the MIL paradigm. To enhance spatial\nsensitivity and structural awareness, we design a learnable frequency-domain\nencoding module that supplements spatial-domain features with spectrum-based\ninformation. CAMs generated in this stage are used to guide segmentation\ntraining. In the second stage, we refine the initial pseudo labels via a\nCAM-guided soft-label supervision and a self-correction mechanism, enabling\nrobust training even under label noise. Extensive experiments on both public\nand private histopathology datasets demonstrate that FMaMIL outperforms\nstate-of-the-art weakly supervised methods without relying on pixel-level\nannotations, validating its effectiveness and potential for digital pathology\napplications.", "AI": {"tldr": "FMaMIL is a two-stage weakly supervised framework for lesion segmentation in histopathology images using only image-level labels, outperforming state-of-the-art methods.", "motivation": "Lesion segmentation is crucial for diagnostics but suffers from limited pixel-level annotations. FMaMIL aims to address this challenge with weak supervision.", "method": "1. Mamba-based encoder captures long-range dependencies under MIL. 2. Frequency-domain encoding enhances spatial sensitivity. 3. CAMs guide segmentation training. 4. Pseudo labels are refined via soft-label supervision and self-correction.", "result": "FMaMIL outperforms existing weakly supervised methods on histopathology datasets without pixel-level annotations.", "conclusion": "FMaMIL is effective for lesion segmentation in digital pathology, offering a practical solution with minimal annotation requirements."}}
{"id": "2506.07962", "pdf": "https://arxiv.org/pdf/2506.07962", "abs": "https://arxiv.org/abs/2506.07962", "authors": ["Elliot Kim", "Avi Garg", "Kenny Peng", "Nikhil Garg"], "title": "Correlated Errors in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CY", "stat.ML"], "comment": "Accepted to ICML 2025", "summary": "Diversity in training data, architecture, and providers is assumed to\nmitigate homogeneity in LLMs. However, we lack empirical evidence on whether\ndifferent LLMs differ meaningfully. We conduct a large-scale empirical\nevaluation on over 350 LLMs overall, using two popular leaderboards and a\nresume-screening task. We find substantial correlation in model errors -- on\none leaderboard dataset, models agree 60% of the time when both models err. We\nidentify factors driving model correlation, including shared architectures and\nproviders. Crucially, however, larger and more accurate models have highly\ncorrelated errors, even with distinct architectures and providers. Finally, we\nshow the effects of correlation in two downstream tasks: LLM-as-judge\nevaluation and hiring -- the latter reflecting theoretical predictions\nregarding algorithmic monoculture.", "AI": {"tldr": "The paper investigates whether diversity in training data, architecture, and providers reduces homogeneity in LLMs, finding high error correlations among models, especially larger ones, and impacts on downstream tasks like hiring.", "motivation": "To empirically assess if diverse LLMs differ meaningfully, given assumptions that diversity mitigates homogeneity.", "method": "Large-scale evaluation of over 350 LLMs using leaderboards and a resume-screening task, analyzing error correlations and influencing factors.", "result": "High error correlation (60% agreement when both err), driven by shared architectures/providers, but also present in larger, distinct models. Downstream impacts confirmed.", "conclusion": "Diversity assumptions may not hold; error correlation persists even in diverse models, affecting tasks like hiring, highlighting risks of algorithmic monoculture."}}
{"id": "2506.07670", "pdf": "https://arxiv.org/pdf/2506.07670", "abs": "https://arxiv.org/abs/2506.07670", "authors": ["Xiaohan Lu", "Jiaye Fu", "Jiaqi Zhang", "Zetian Song", "Chuanmin Jia", "Siwei Ma"], "title": "ProSplat: Improved Feed-Forward 3D Gaussian Splatting for Wide-Baseline Sparse Views", "categories": ["cs.CV"], "comment": null, "summary": "Feed-forward 3D Gaussian Splatting (3DGS) has recently demonstrated promising\nresults for novel view synthesis (NVS) from sparse input views, particularly\nunder narrow-baseline conditions. However, its performance significantly\ndegrades in wide-baseline scenarios due to limited texture details and\ngeometric inconsistencies across views. To address these challenges, in this\npaper, we propose ProSplat, a two-stage feed-forward framework designed for\nhigh-fidelity rendering under wide-baseline conditions. The first stage\ninvolves generating 3D Gaussian primitives via a 3DGS generator. In the second\nstage, rendered views from these primitives are enhanced through an improvement\nmodel. Specifically, this improvement model is based on a one-step diffusion\nmodel, further optimized by our proposed Maximum Overlap Reference view\nInjection (MORI) and Distance-Weighted Epipolar Attention (DWEA). MORI\nsupplements missing texture and color by strategically selecting a reference\nview with maximum viewpoint overlap, while DWEA enforces geometric consistency\nusing epipolar constraints. Additionally, we introduce a divide-and-conquer\ntraining strategy that aligns data distributions between the two stages through\njoint optimization. We evaluate ProSplat on the RealEstate10K and DL3DV-10K\ndatasets under wide-baseline settings. Experimental results demonstrate that\nProSplat achieves an average improvement of 1 dB in PSNR compared to recent\nSOTA methods.", "AI": {"tldr": "ProSplat improves 3D Gaussian Splatting for wide-baseline novel view synthesis using a two-stage framework with a diffusion-based enhancement model and novel techniques (MORI, DWEA).", "motivation": "Address performance degradation of 3DGS in wide-baseline scenarios due to limited texture details and geometric inconsistencies.", "method": "Two-stage framework: 1) Generate 3D Gaussian primitives, 2) Enhance rendered views using a diffusion model optimized by MORI and DWEA. Joint training aligns data distributions.", "result": "Achieves 1 dB PSNR improvement over SOTA methods on RealEstate10K and DL3DV-10K datasets.", "conclusion": "ProSplat effectively enhances 3DGS for wide-baseline NVS, outperforming existing methods."}}
{"id": "2506.08007", "pdf": "https://arxiv.org/pdf/2506.08007", "abs": "https://arxiv.org/abs/2506.08007", "authors": ["Qingxiu Dong", "Li Dong", "Yao Tang", "Tianzhu Ye", "Yutao Sun", "Zhifang Sui", "Furu Wei"], "title": "Reinforcement Pre-Training", "categories": ["cs.CL"], "comment": null, "summary": "In this work, we introduce Reinforcement Pre-Training (RPT) as a new scaling\nparadigm for large language models and reinforcement learning (RL).\nSpecifically, we reframe next-token prediction as a reasoning task trained\nusing RL, where it receives verifiable rewards for correctly predicting the\nnext token for a given context. RPT offers a scalable method to leverage vast\namounts of text data for general-purpose RL, rather than relying on\ndomain-specific annotated answers. By incentivizing the capability of\nnext-token reasoning, RPT significantly improves the language modeling accuracy\nof predicting the next tokens. Moreover, RPT provides a strong pre-trained\nfoundation for further reinforcement fine-tuning. The scaling curves show that\nincreased training compute consistently improves the next-token prediction\naccuracy. The results position RPT as an effective and promising scaling\nparadigm to advance language model pre-training.", "AI": {"tldr": "Reinforcement Pre-Training (RPT) reframes next-token prediction as a reasoning task using RL, improving language modeling accuracy and providing a scalable foundation for further fine-tuning.", "motivation": "To leverage vast text data for general-purpose RL without domain-specific annotations, improving next-token prediction accuracy.", "method": "RPT trains next-token prediction as a reasoning task with verifiable rewards, incentivizing accurate predictions.", "result": "RPT significantly improves next-token prediction accuracy and scales well with increased training compute.", "conclusion": "RPT is an effective and promising paradigm for advancing language model pre-training."}}
{"id": "2506.07697", "pdf": "https://arxiv.org/pdf/2506.07697", "abs": "https://arxiv.org/abs/2506.07697", "authors": ["Jens Piekenbrinck", "Christian Schmidt", "Alexander Hermans", "Narunas Vaskevicius", "Timm Linder", "Bastian Leibe"], "title": "OpenSplat3D: Open-Vocabulary 3D Instance Segmentation using Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) has emerged as a powerful representation for\nneural scene reconstruction, offering high-quality novel view synthesis while\nmaintaining computational efficiency. In this paper, we extend the capabilities\nof 3DGS beyond pure scene representation by introducing an approach for\nopen-vocabulary 3D instance segmentation without requiring manual labeling,\ntermed OpenSplat3D. Our method leverages feature-splatting techniques to\nassociate semantic information with individual Gaussians, enabling fine-grained\nscene understanding. We incorporate Segment Anything Model instance masks with\na contrastive loss formulation as guidance for the instance features to achieve\naccurate instance-level segmentation. Furthermore, we utilize language\nembeddings of a vision-language model, allowing for flexible, text-driven\ninstance identification. This combination enables our system to identify and\nsegment arbitrary objects in 3D scenes based on natural language descriptions.\nWe show results on LERF-mask and LERF-OVS as well as the full ScanNet++\nvalidation set, demonstrating the effectiveness of our approach.", "AI": {"tldr": "OpenSplat3D extends 3D Gaussian Splatting (3DGS) for open-vocabulary 3D instance segmentation without manual labels, using feature-splatting and language embeddings for text-driven object identification.", "motivation": "To enhance 3DGS for fine-grained scene understanding and enable flexible, text-driven instance segmentation without manual labeling.", "method": "Combines feature-splatting with Segment Anything Model masks and contrastive loss, integrating vision-language model embeddings for text-driven identification.", "result": "Demonstrated effectiveness on LERF-mask, LERF-OVS, and ScanNet++ datasets.", "conclusion": "OpenSplat3D successfully enables open-vocabulary 3D instance segmentation, advancing scene understanding capabilities."}}
{"id": "2506.06294", "pdf": "https://arxiv.org/pdf/2506.06294", "abs": "https://arxiv.org/abs/2506.06294", "authors": ["Yunqing Liu", "Wenqi Fan", "Xiaoyong Wei", "Qing Li"], "title": "GLProtein: Global-and-Local Structure Aware Protein Representation Learning", "categories": ["cs.LG", "cs.AI", "cs.CL", "q-bio.BM"], "comment": null, "summary": "Proteins are central to biological systems, participating as building blocks\nacross all forms of life. Despite advancements in understanding protein\nfunctions through protein sequence analysis, there remains potential for\nfurther exploration in integrating protein structural information. We argue\nthat the structural information of proteins is not only limited to their 3D\ninformation but also encompasses information from amino acid molecules (local\ninformation) to protein-protein structure similarity (global information). To\naddress this, we propose \\textbf{GLProtein}, the first framework in protein\npre-training that incorporates both global structural similarity and local\namino acid details to enhance prediction accuracy and functional insights.\nGLProtein innovatively combines protein-masked modelling with triplet structure\nsimilarity scoring, protein 3D distance encoding and substructure-based amino\nacid molecule encoding. Experimental results demonstrate that GLProtein\noutperforms previous methods in several bioinformatics tasks, including\npredicting protein-protein interaction, contact prediction, and so on.", "AI": {"tldr": "GLProtein is a novel protein pre-training framework integrating global structural similarity and local amino acid details, outperforming prior methods in bioinformatics tasks.", "motivation": "Despite progress in protein sequence analysis, integrating protein structural information (local to global) remains underexplored for enhancing functional insights.", "method": "GLProtein combines protein-masked modeling, triplet structure similarity scoring, 3D distance encoding, and substructure-based amino acid encoding.", "result": "GLProtein surpasses existing methods in tasks like protein-protein interaction and contact prediction.", "conclusion": "GLProtein advances protein pre-training by unifying structural details, offering improved accuracy and functional understanding."}}
{"id": "2506.07698", "pdf": "https://arxiv.org/pdf/2506.07698", "abs": "https://arxiv.org/abs/2506.07698", "authors": ["Yuxiao Yang", "Peihao Li", "Yuhong Zhang", "Junzhe Lu", "Xianglong He", "Minghan Qin", "Weitao Wang", "Haoqian Wang"], "title": "NOVA3D: Normal Aligned Video Diffusion Model for Single Image to 3D Generation", "categories": ["cs.CV", "cs.AI"], "comment": "8 pages, 7 figures, accepted by ICME 2025", "summary": "3D AI-generated content (AIGC) has made it increasingly accessible for anyone\nto become a 3D content creator. While recent methods leverage Score\nDistillation Sampling to distill 3D objects from pretrained image diffusion\nmodels, they often suffer from inadequate 3D priors, leading to insufficient\nmulti-view consistency. In this work, we introduce NOVA3D, an innovative\nsingle-image-to-3D generation framework. Our key insight lies in leveraging\nstrong 3D priors from a pretrained video diffusion model and integrating\ngeometric information during multi-view video fine-tuning. To facilitate\ninformation exchange between color and geometric domains, we propose the\nGeometry-Temporal Alignment (GTA) attention mechanism, thereby improving\ngeneralization and multi-view consistency. Moreover, we introduce the\nde-conflict geometry fusion algorithm, which improves texture fidelity by\naddressing multi-view inaccuracies and resolving discrepancies in pose\nalignment. Extensive experiments validate the superiority of NOVA3D over\nexisting baselines.", "AI": {"tldr": "NOVA3D is a novel single-image-to-3D generation framework that improves multi-view consistency and texture fidelity by leveraging 3D priors from a pretrained video diffusion model and introducing innovative techniques like GTA attention and de-conflict geometry fusion.", "motivation": "Existing methods for 3D AIGC often lack adequate 3D priors, leading to poor multi-view consistency. NOVA3D aims to address this by integrating strong 3D priors and geometric information.", "method": "NOVA3D uses a pretrained video diffusion model for 3D priors, incorporates geometric information during fine-tuning, and introduces the GTA attention mechanism and de-conflict geometry fusion algorithm.", "result": "Extensive experiments show NOVA3D outperforms existing baselines in multi-view consistency and texture fidelity.", "conclusion": "NOVA3D advances 3D AIGC by effectively leveraging 3D priors and resolving multi-view inconsistencies, offering superior performance over current methods."}}
{"id": "2506.06295", "pdf": "https://arxiv.org/pdf/2506.06295", "abs": "https://arxiv.org/abs/2506.06295", "authors": ["Zhiyuan Liu", "Yicun Yang", "Yaojie Zhang", "Junjie Chen", "Chang Zou", "Qingyuan Wei", "Shaobo Wang", "Linfeng Zhang"], "title": "dLLM-Cache: Accelerating Diffusion Large Language Models with Adaptive Caching", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Autoregressive Models (ARMs) have long dominated the landscape of Large\nLanguage Models. Recently, a new paradigm has emerged in the form of\ndiffusion-based Large Language Models (dLLMs), which generate text by\niteratively denoising masked segments. This approach has shown significant\nadvantages and potential. However, dLLMs suffer from high inference latency.\nTraditional ARM acceleration techniques, such as Key-Value caching, are\nincompatible with dLLMs due to their bidirectional attention mechanism. To\naddress this specific challenge, our work begins with a key observation that\ndLLM inference involves a static prompt and a partially dynamic response, where\nmost tokens remain stable across adjacent denoising steps. Based on this, we\npropose dLLM-Cache, a training-free adaptive caching framework that combines\nlong-interval prompt caching with partial response updates guided by feature\nsimilarity. This design enables efficient reuse of intermediate computations\nwithout compromising model performance. Extensive experiments on representative\ndLLMs, including LLaDA 8B and Dream 7B, show that dLLM-Cache achieves up to 9.1\nx speedup over standard inference without compromising output quality. Notably,\nour method brings dLLM inference latency close to that of ARMs under many\nsettings. Codes are provided in the supplementary material and will be released\npublicly on GitHub.", "AI": {"tldr": "The paper introduces dLLM-Cache, a caching framework to reduce inference latency in diffusion-based Large Language Models (dLLMs) by reusing intermediate computations, achieving up to 9.1x speedup without quality loss.", "motivation": "Diffusion-based LLMs (dLLMs) offer advantages over autoregressive models but suffer from high inference latency due to incompatible acceleration techniques like Key-Value caching.", "method": "Proposes dLLM-Cache, a training-free adaptive caching framework combining long-interval prompt caching with partial response updates based on feature similarity.", "result": "Experiments on models like LLaDA 8B and Dream 7B show up to 9.1x speedup, bringing dLLM latency close to autoregressive models.", "conclusion": "dLLM-Cache effectively reduces inference latency in dLLMs without compromising performance, making them more practical for real-world use."}}
{"id": "2506.07705", "pdf": "https://arxiv.org/pdf/2506.07705", "abs": "https://arxiv.org/abs/2506.07705", "authors": ["Weilei Wen", "Chunle Guo", "Wenqi Ren", "Hongpeng Wang", "Xiuli Shao"], "title": "Adaptive Blind Super-Resolution Network for Spatial-Specific and Spatial-Agnostic Degradations", "categories": ["cs.CV", "eess.IV"], "comment": "IEEE TRANSACTIONS ON IMAGE PROCESSING", "summary": "Prior methodologies have disregarded the diversities among distinct\ndegradation types during image reconstruction, employing a uniform network\nmodel to handle multiple deteriorations. Nevertheless, we discover that\nprevalent degradation modalities, including sampling, blurring, and noise, can\nbe roughly categorized into two classes. We classify the first class as\nspatial-agnostic dominant degradations, less affected by regional changes in\nimage space, such as downsampling and noise degradation. The second class\ndegradation type is intimately associated with the spatial position of the\nimage, such as blurring, and we identify them as spatial-specific dominant\ndegradations. We introduce a dynamic filter network integrating global and\nlocal branches to address these two degradation types. This network can greatly\nalleviate the practical degradation problem. Specifically, the global dynamic\nfiltering layer can perceive the spatial-agnostic dominant degradation in\ndifferent images by applying weights generated by the attention mechanism to\nmultiple parallel standard convolution kernels, enhancing the network's\nrepresentation ability. Meanwhile, the local dynamic filtering layer converts\nfeature maps of the image into a spatially specific dynamic filtering operator,\nwhich performs spatially specific convolution operations on the image features\nto handle spatial-specific dominant degradations. By effectively integrating\nboth global and local dynamic filtering operators, our proposed method\noutperforms state-of-the-art blind super-resolution algorithms in both\nsynthetic and real image datasets.", "AI": {"tldr": "The paper introduces a dynamic filter network to address two classes of image degradations\u2014spatial-agnostic and spatial-specific\u2014improving reconstruction performance over existing methods.", "motivation": "Prior methods treated all degradations uniformly, ignoring their diversity. The authors identify two distinct classes of degradations and propose a tailored solution.", "method": "A dynamic filter network with global and local branches: global handles spatial-agnostic degradations (e.g., noise), while local addresses spatial-specific degradations (e.g., blurring).", "result": "The method outperforms state-of-the-art blind super-resolution algorithms on synthetic and real datasets.", "conclusion": "The proposed network effectively addresses diverse degradation types, enhancing image reconstruction quality."}}
{"id": "2506.06299", "pdf": "https://arxiv.org/pdf/2506.06299", "abs": "https://arxiv.org/abs/2506.06299", "authors": ["Daniel Thilo Schroeder", "Meeyoung Cha", "Andrea Baronchelli", "Nick Bostrom", "Nicholas A. Christakis", "David Garcia", "Amit Goldenberg", "Yara Kyrychenko", "Kevin Leyton-Brown", "Nina Lutz", "Gary Marcus", "Filippo Menczer", "Gordon Pennycook", "David G. Rand", "Frank Schweitzer", "Christopher Summerfield", "Audrey Tang", "Jay Van Bavel", "Sander van der Linden", "Dawn Song", "Jonas R. Kunst"], "title": "How Malicious AI Swarms Can Threaten Democracy", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "comment": "8 pages, 1 figure", "summary": "Advances in AI portend a new era of sophisticated disinformation operations.\nWhile individual AI systems already create convincing -- and at times\nmisleading -- information, an imminent development is the emergence of\nmalicious AI swarms. These systems can coordinate covertly, infiltrate\ncommunities, evade traditional detectors, and run continuous A/B tests, with\nround-the-clock persistence. The result can include fabricated grassroots\nconsensus, fragmented shared reality, mass harassment, voter micro-suppression\nor mobilization, contamination of AI training data, and erosion of\ninstitutional trust. With democratic processes worldwide increasingly\nvulnerable, we urge a three-pronged response: (1) platform-side defenses --\nalways-on swarm-detection dashboards, pre-election high-fidelity\nswarm-simulation stress-tests, transparency audits, and optional client-side\n\"AI shields\" for users; (2) model-side safeguards -- standardized\npersuasion-risk tests, provenance-authenticating passkeys, and watermarking;\nand (3) system-level oversight -- a UN-backed AI Influence Observatory.", "AI": {"tldr": "The paper warns of AI-driven disinformation via malicious swarms, proposing platform, model, and system-level defenses.", "motivation": "The rise of AI swarms threatens democracy by enabling covert, persistent disinformation campaigns.", "method": "Proposes three strategies: platform defenses (e.g., swarm detection), model safeguards (e.g., watermarking), and system oversight (e.g., UN-backed observatory).", "result": "Identifies risks like fabricated consensus and eroded trust, urging proactive measures.", "conclusion": "Calls for immediate action to counter AI-driven disinformation through multi-layered safeguards."}}
{"id": "2506.07713", "pdf": "https://arxiv.org/pdf/2506.07713", "abs": "https://arxiv.org/abs/2506.07713", "authors": ["Ge Wang", "Songlin Fan", "Hangxu Liu", "Quanjian Song", "Hewei Wang", "Jinfeng Xu"], "title": "Consistent Video Editing as Flow-Driven Image-to-Video Generation", "categories": ["cs.CV", "cs.AI"], "comment": "16 pages, 12 figures", "summary": "With the prosper of video diffusion models, down-stream applications like\nvideo editing have been significantly promoted without consuming much\ncomputational cost. One particular challenge in this task lies at the motion\ntransfer process from the source video to the edited one, where it requires the\nconsideration of the shape deformation in between, meanwhile maintaining the\ntemporal consistency in the generated video sequence. However, existing methods\nfail to model complicated motion patterns for video editing, and are\nfundamentally limited to object replacement, where tasks with non-rigid object\nmotions like multi-object and portrait editing are largely neglected. In this\npaper, we observe that optical flows offer a promising alternative in complex\nmotion modeling, and present FlowV2V to re-investigate video editing as a task\nof flow-driven Image-to-Video (I2V) generation. Specifically, FlowV2V\ndecomposes the entire pipeline into first-frame editing and conditional I2V\ngeneration, and simulates pseudo flow sequence that aligns with the deformed\nshape, thus ensuring the consistency during editing. Experimental results on\nDAVIS-EDIT with improvements of 13.67% and 50.66% on DOVER and warping error\nillustrate the superior temporal consistency and sample quality of FlowV2V\ncompared to existing state-of-the-art ones. Furthermore, we conduct\ncomprehensive ablation studies to analyze the internal functionalities of the\nfirst-frame paradigm and flow alignment in the proposed method.", "AI": {"tldr": "FlowV2V introduces a flow-driven I2V generation method for video editing, improving temporal consistency and quality by decomposing the task into first-frame editing and conditional I2V generation.", "motivation": "Existing methods struggle with complex motion patterns in video editing, especially for non-rigid objects. Optical flows offer a solution for better motion modeling.", "method": "FlowV2V decomposes video editing into first-frame editing and conditional I2V generation, using pseudo flow sequences to align deformed shapes and ensure consistency.", "result": "FlowV2V outperforms state-of-the-art methods, improving DOVER by 13.67% and reducing warping error by 50.66% on DAVIS-EDIT.", "conclusion": "FlowV2V demonstrates superior performance in handling complex motions and maintaining temporal consistency, validated by ablation studies."}}
{"id": "2506.06303", "pdf": "https://arxiv.org/pdf/2506.06303", "abs": "https://arxiv.org/abs/2506.06303", "authors": ["Kefan Song", "Amir Moeini", "Peng Wang", "Lei Gong", "Rohan Chandra", "Yanjun Qi", "Shangtong Zhang"], "title": "Reward Is Enough: LLMs Are In-Context Reinforcement Learners", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Reinforcement learning (RL) is a human-designed framework for solving\nsequential decision making problems. In this work, we demonstrate that,\nsurprisingly, RL emerges in LLM's (Large Language Model) inference time -- a\nphenomenon known as in-context RL (ICRL). Specifically, we propose a novel\nmulti-round prompting framework called ICRL prompting. The goal is to prompt\nthe LLM to complete a task. After the LLM generates a response at the current\nround, we give numerical scalar feedbacks for the response, called the rewards.\nAt the next round, we prompt the LLM again with the same task and a context\nconsisting of all previous responses and rewards. We observe that the quality\nof the LLM's response increases as the context grows. In other words, the LLM\nis able to maximize the scalar reward signal in the inference time, just like\nan RL algorithm. We evaluate ICRL prompting in three benchmarks (Game of 24,\ncreative writing, and ScienceWorld) and demonstrate significant performance\nimprovements over baseline methods such as Self-Refine and Reflexion.\nSurprisingly, in some experiments the reward signals are generated by the LLM\nitself, yet performance improvements are still observed from ICRL prompting,\noffering a promising paradigm for scaling test-time compute.", "AI": {"tldr": "The paper introduces in-context RL (ICRL), showing that RL emerges in LLM inference through multi-round prompting with feedback, improving task performance.", "motivation": "To explore if LLMs can exhibit RL-like behavior during inference by leveraging feedback to improve responses iteratively.", "method": "Proposes ICRL prompting: multi-round prompting with numerical rewards for responses, using past responses and rewards as context.", "result": "ICRL improves LLM performance across benchmarks (Game of 24, creative writing, ScienceWorld), even with self-generated rewards.", "conclusion": "ICRL demonstrates RL-like behavior in LLMs, offering a scalable paradigm for enhancing inference-time performance."}}
{"id": "2506.07720", "pdf": "https://arxiv.org/pdf/2506.07720", "abs": "https://arxiv.org/abs/2506.07720", "authors": ["Yufei Guo", "Yuhan Zhang", "Zhou Jie", "Xiaode Liu", "Xin Tong", "Yuanpei Chen", "Weihang Peng", "Zhe Ma"], "title": "ReverB-SNN: Reversing Bit of the Weight and Activation for Spiking Neural Networks", "categories": ["cs.CV"], "comment": "Accpeted by ICML2024", "summary": "The Spiking Neural Network (SNN), a biologically inspired neural network\ninfrastructure, has garnered significant attention recently. SNNs utilize\nbinary spike activations for efficient information transmission, replacing\nmultiplications with additions, thereby enhancing energy efficiency. However,\nbinary spike activation maps often fail to capture sufficient data information,\nresulting in reduced accuracy. To address this challenge, we advocate reversing\nthe bit of the weight and activation for SNNs, called \\textbf{ReverB-SNN},\ninspired by recent findings that highlight greater accuracy degradation from\nquantizing activations compared to weights. Specifically, our method employs\nreal-valued spike activations alongside binary weights in SNNs. This preserves\nthe event-driven and multiplication-free advantages of standard SNNs while\nenhancing the information capacity of activations. Additionally, we introduce a\ntrainable factor within binary weights to adaptively learn suitable weight\namplitudes during training, thereby increasing network capacity. To maintain\nefficiency akin to vanilla \\textbf{ReverB-SNN}, our trainable binary weight\nSNNs are converted back to standard form using a re-parameterization technique\nduring inference. Extensive experiments across various network architectures\nand datasets, both static and dynamic, demonstrate that our approach\nconsistently outperforms state-of-the-art methods.", "AI": {"tldr": "The paper introduces ReverB-SNN, a method using real-valued spike activations and binary weights in SNNs to enhance accuracy while maintaining efficiency.", "motivation": "Binary spike activations in SNNs often lack sufficient data information, reducing accuracy. The paper aims to address this by reversing bits for weights and activations.", "method": "ReverB-SNN employs real-valued spike activations with binary weights, introduces a trainable factor for adaptive weight amplitudes, and uses re-parameterization for inference efficiency.", "result": "Experiments show ReverB-SNN outperforms state-of-the-art methods across various architectures and datasets.", "conclusion": "ReverB-SNN effectively balances accuracy and efficiency in SNNs by leveraging real-valued activations and adaptive binary weights."}}
{"id": "2506.06313", "pdf": "https://arxiv.org/pdf/2506.06313", "abs": "https://arxiv.org/abs/2506.06313", "authors": ["Huiyao Chen", "Yi Yang", "Yinghui Li", "Meishan Zhang", "Min Zhang"], "title": "DISRetrieval: Harnessing Discourse Structure for Long Document Retrieval", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "21 pages, 7 figures", "summary": "Long document understanding has become increasingly crucial in natural\nlanguage processing, with retrieval-based methods emerging as a promising\nsolution to address the context length limitations of large language models\n(LLMs). However, existing approaches either treat documents as flat sequences\nor employ arbitrary chunking strategies, failing to capture the inherent\ndiscourse structure that guides human comprehension. We present DISRetrieval, a\nnovel hierarchical retrieval framework that leverages linguistic discourse\nstructure to enhance long document understanding. Our approach introduces three\nkey innovations: (1) a discourse-aware document organization framework that\nutilizes rhetorical structure theory (RST) to create sentence-level\nhierarchical representations, preserving both semantic relationships and\nnatural document flow; (2) an LLM-enhanced node representation technique that\ncombines discourse structure with adaptive summarization to enrich tree nodes\nwith contextual information; and (3) a hierarchical evidence retrieval\nmechanism that effectively selects relevant content while maintaining discourse\ncoherence. Through comprehensive experiments on QASPER and QuALITY datasets,\nDISRetrieval demonstrates substantial improvements over existing methods in\nboth token-level retrieval metrics and downstream question answering tasks. Our\nablation studies confirm that incorporating discourse structure significantly\nenhances retrieval effectiveness across different document lengths and query\ntypes, validating the importance of linguistically-informed document\nrepresentation in long-text understanding. Our code and datasets are publicly\navailable at github/DreamH1gh/DISRetrieval to facilitate future research.", "AI": {"tldr": "DISRetrieval is a hierarchical retrieval framework for long document understanding, leveraging discourse structure (RST) for improved performance over flat or chunked methods.", "motivation": "Existing methods fail to capture document discourse structure, limiting their effectiveness for long document understanding.", "method": "Uses RST for hierarchical document organization, LLM-enhanced node representation, and hierarchical evidence retrieval.", "result": "Outperforms existing methods on QASPER and QuALITY datasets in retrieval and QA tasks.", "conclusion": "Incorporating discourse structure significantly improves retrieval, highlighting its importance for long-text understanding."}}
{"id": "2506.07725", "pdf": "https://arxiv.org/pdf/2506.07725", "abs": "https://arxiv.org/abs/2506.07725", "authors": ["Shadi Hamdan", "Chonghao Sima", "Zetong Yang", "Hongyang Li", "Fatma G\u00fcney"], "title": "ETA: Efficiency through Thinking Ahead, A Dual Approach to Self-Driving with Large Models", "categories": ["cs.CV", "cs.AI"], "comment": "ICCV 2025 submission. For code, see\n  https://github.com/opendrivelab/ETA", "summary": "How can we benefit from large models without sacrificing inference speed, a\ncommon dilemma in self-driving systems? A prevalent solution is a dual-system\narchitecture, employing a small model for rapid, reactive decisions and a\nlarger model for slower but more informative analyses. Existing dual-system\ndesigns often implement parallel architectures where inference is either\ndirectly conducted using the large model at each current frame or retrieved\nfrom previously stored inference results. However, these works still struggle\nto enable large models for a timely response to every online frame. Our key\ninsight is to shift intensive computations of the current frame to previous\ntime steps and perform a batch inference of multiple time steps to make large\nmodels respond promptly to each time step. To achieve the shifting, we\nintroduce Efficiency through Thinking Ahead (ETA), an asynchronous system\ndesigned to: (1) propagate informative features from the past to the current\nframe using future predictions from the large model, (2) extract current frame\nfeatures using a small model for real-time responsiveness, and (3) integrate\nthese dual features via an action mask mechanism that emphasizes\naction-critical image regions. Evaluated on the Bench2Drive CARLA\nLeaderboard-v2 benchmark, ETA advances state-of-the-art performance by 8% with\na driving score of 69.53 while maintaining a near-real-time inference speed at\n50 ms.", "AI": {"tldr": "ETA introduces an asynchronous system to shift intensive computations to prior time steps, enabling large models to respond promptly while maintaining near-real-time inference speed.", "motivation": "Address the dilemma of benefiting from large models without sacrificing inference speed in self-driving systems.", "method": "ETA propagates past features, uses a small model for real-time features, and integrates them via an action mask mechanism.", "result": "Achieves 8% performance improvement (69.53 driving score) on Bench2Drive CARLA Leaderboard-v2 with 50 ms inference speed.", "conclusion": "ETA successfully balances performance and speed, advancing state-of-the-art in self-driving systems."}}
{"id": "2506.06328", "pdf": "https://arxiv.org/pdf/2506.06328", "abs": "https://arxiv.org/abs/2506.06328", "authors": ["Aziida Nanyonga", "Joiner Keith", "Turhan Ugur", "Wild Graham"], "title": "Is BERTopic Better than PLSA for Extracting Key Topics in Aviation Safety Reports?", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "This study compares the effectiveness of BERTopic and Probabilistic Latent\nSemantic Analysis (PLSA) in extracting meaningful topics from aviation safety\nreports aiming to enhance the understanding of patterns in aviation incident\ndata. Using a dataset of over 36,000 National Transportation Safety Board\n(NTSB) reports from 2000 to 2020, BERTopic employed transformer based\nembeddings and hierarchical clustering, while PLSA utilized probabilistic\nmodelling through the Expectation-Maximization (EM) algorithm. Results showed\nthat BERTopic outperformed PLSA in topic coherence, achieving a Cv score of\n0.41 compared to PLSA 0.37, while also demonstrating superior interpretability\nas validated by aviation safety experts. These findings underscore the\nadvantages of modern transformer based approaches in analyzing complex aviation\ndatasets, paving the way for enhanced insights and informed decision-making in\naviation safety. Future work will explore hybrid models, multilingual datasets,\nand advanced clustering techniques to further improve topic modelling in this\ndomain.", "AI": {"tldr": "BERTopic outperforms PLSA in topic coherence and interpretability for aviation safety reports, showcasing transformer-based advantages.", "motivation": "To compare BERTopic and PLSA for extracting meaningful topics from aviation safety reports to better understand incident patterns.", "method": "Used 36,000 NTSB reports (2000-2020); BERTopic with transformer embeddings and hierarchical clustering vs. PLSA with EM algorithm.", "result": "BERTopic achieved higher topic coherence (Cv score 0.41 vs. PLSA 0.37) and better interpretability.", "conclusion": "Transformer-based approaches like BERTopic are superior for aviation data analysis, with future work on hybrid models and multilingual datasets."}}
{"id": "2506.07737", "pdf": "https://arxiv.org/pdf/2506.07737", "abs": "https://arxiv.org/abs/2506.07737", "authors": ["Xuemei Chen", "Huamin Wang", "Hangchi Shen", "Shukai Duan", "Shiping Wen", "Tingwen Huang"], "title": "SpikeSMOKE: Spiking Neural Networks for Monocular 3D Object Detection with Cross-Scale Gated Coding", "categories": ["cs.CV"], "comment": null, "summary": "Low energy consumption for 3D object detection is an important research area\nbecause of the increasing energy consumption with their wide application in\nfields such as autonomous driving. The spiking neural networks (SNNs) with\nlow-power consumption characteristics can provide a novel solution for this\nresearch. Therefore, we apply SNNs to monocular 3D object detection and propose\nthe SpikeSMOKE architecture in this paper, which is a new attempt for low-power\nmonocular 3D object detection. As we all know, discrete signals of SNNs will\ngenerate information loss and limit their feature expression ability compared\nwith the artificial neural networks (ANNs).In order to address this issue,\ninspired by the filtering mechanism of biological neuronal synapses, we propose\na cross-scale gated coding mechanism(CSGC), which can enhance feature\nrepresentation by combining cross-scale fusion of attentional methods and gated\nfiltering mechanisms.In addition, to reduce the computation and increase the\nspeed of training, we present a novel light-weight residual block that can\nmaintain spiking computing paradigm and the highest possible detection\nperformance. Compared to the baseline SpikeSMOKE under the 3D Object Detection,\nthe proposed SpikeSMOKE with CSGC can achieve 11.78 (+2.82, Easy), 10.69 (+3.2,\nModerate), and 10.48 (+3.17, Hard) on the KITTI autonomous driving dataset by\nAP|R11 at 0.7 IoU threshold, respectively. It is important to note that the\nresults of SpikeSMOKE can significantly reduce energy consumption compared to\nthe results on SMOKE. For example,the energy consumption can be reduced by\n72.2% on the hard category, while the detection performance is reduced by only\n4%. SpikeSMOKE-L (lightweight) can further reduce the amount of parameters by 3\ntimes and computation by 10 times compared to SMOKE.", "AI": {"tldr": "Proposes SpikeSMOKE, a low-power 3D object detection method using SNNs, enhanced by a cross-scale gated coding mechanism (CSGC) and lightweight residual blocks, achieving competitive performance with significant energy savings.", "motivation": "Addresses high energy consumption in 3D object detection, leveraging SNNs' low-power potential for applications like autonomous driving.", "method": "Introduces SpikeSMOKE with CSGC for feature enhancement and lightweight residual blocks to reduce computation while maintaining performance.", "result": "Achieves improved AP scores on KITTI dataset (e.g., +3.17 for Hard) with 72.2% energy reduction and 4% performance drop compared to SMOKE.", "conclusion": "SpikeSMOKE offers an efficient, low-power solution for 3D object detection, balancing performance and energy savings."}}
{"id": "2506.06329", "pdf": "https://arxiv.org/pdf/2506.06329", "abs": "https://arxiv.org/abs/2506.06329", "authors": ["Zheng Cao", "Wanchaloem Wunkaew", "Helyette Geman"], "title": "The Hype Index: an NLP-driven Measure of Market News Attention", "categories": ["q-fin.ST", "cs.CE", "cs.CL"], "comment": null, "summary": "This paper introduces the Hype Index as a novel metric to quantify media\nattention toward large-cap equities, leveraging advances in Natural Language\nProcessing (NLP) for extracting predictive signals from financial news. Using\nthe S&P 100 as the focus universe, we first construct a News Count-Based Hype\nIndex, which measures relative media exposure by computing the share of news\narticles referencing each stock or sector. We then extend it to the\nCapitalization Adjusted Hype Index, adjusts for economic size by taking the\nratio of a stock's or sector's media weight to its market capitalization weight\nwithin its industry or sector. We compute both versions of the Hype Index at\nthe stock and sector levels, and evaluate them through multiple lenses: (1)\ntheir classification into different hype groups, (2) their associations with\nreturns, volatility, and VIX index at various lags, (3) their signaling power\nfor short-term market movements, and (4) their empirical properties including\ncorrelations, samplings, and trends. Our findings suggest that the Hype Index\nfamily provides a valuable set of tools for stock volatility analysis, market\nsignaling, and NLP extensions in Finance.", "AI": {"tldr": "The paper introduces the Hype Index, a metric using NLP to quantify media attention on large-cap equities, with versions adjusted for market capitalization. It evaluates the index's predictive power for returns, volatility, and market movements.", "motivation": "To quantify media attention's impact on financial markets and provide tools for volatility analysis and market signaling.", "method": "Constructs two versions of the Hype Index (News Count-Based and Capitalization Adjusted) using NLP on financial news, applied to the S&P 100. Evaluates through hype groups, associations with market metrics, and empirical properties.", "result": "The Hype Index family is effective for stock volatility analysis, market signaling, and NLP applications in finance.", "conclusion": "The Hype Index offers valuable tools for analyzing media-driven market dynamics and enhancing financial NLP applications."}}
{"id": "2506.07738", "pdf": "https://arxiv.org/pdf/2506.07738", "abs": "https://arxiv.org/abs/2506.07738", "authors": ["Lanjiong Li", "Guanhua Zhao", "Lingting Zhu", "Zeyu Cai", "Lequan Yu", "Jian Zhang", "Zeyu Wang"], "title": "AssetDropper: Asset Extraction via Diffusion Models with Reward-Driven Optimization", "categories": ["cs.CV"], "comment": "SIGGRAPH 2025. 11 pages, 12 figures", "summary": "Recent research on generative models has primarily focused on creating\nproduct-ready visual outputs; however, designers often favor access to\nstandardized asset libraries, a domain that has yet to be significantly\nenhanced by generative capabilities. Although open-world scenes provide ample\nraw materials for designers, efficiently extracting high-quality, standardized\nassets remains a challenge. To address this, we introduce AssetDropper, the\nfirst framework designed to extract assets from reference images, providing\nartists with an open-world asset palette. Our model adeptly extracts a front\nview of selected subjects from input images, effectively handling complex\nscenarios such as perspective distortion and subject occlusion. We establish a\nsynthetic dataset of more than 200,000 image-subject pairs and a real-world\nbenchmark with thousands more for evaluation, facilitating the exploration of\nfuture research in downstream tasks. Furthermore, to ensure precise asset\nextraction that aligns well with the image prompts, we employ a pre-trained\nreward model to fulfill a closed-loop with feedback. We design the reward model\nto perform an inverse task that pastes the extracted assets back into the\nreference sources, which assists training with additional consistency and\nmitigates hallucination. Extensive experiments show that, with the aid of\nreward-driven optimization, AssetDropper achieves the state-of-the-art results\nin asset extraction. Project page: AssetDropper.github.io.", "AI": {"tldr": "AssetDropper is a framework for extracting standardized assets from reference images, addressing challenges like perspective distortion and occlusion, using a reward-driven approach for high-quality results.", "motivation": "Designers prefer standardized asset libraries, but generative models haven't significantly improved this domain. AssetDropper aims to bridge this gap by extracting assets from open-world scenes.", "method": "AssetDropper uses a pre-trained reward model for closed-loop feedback, extracts front views of subjects, and employs a synthetic dataset (200,000+ pairs) and real-world benchmark for evaluation.", "result": "AssetDropper achieves state-of-the-art performance in asset extraction, validated by extensive experiments.", "conclusion": "AssetDropper effectively enhances asset extraction for designers, with potential for future research in downstream tasks."}}
{"id": "2506.06335", "pdf": "https://arxiv.org/pdf/2506.06335", "abs": "https://arxiv.org/abs/2506.06335", "authors": ["Xuan Xu", "Fufang Wen", "Beilin Chu", "Zhibing Fu", "Qinhong Lin", "Jiaqi Liu", "Binjie Fei", "Zhongliang Yang", "Linna Zhou", "Yu Li"], "title": "FinBERT2: A Specialized Bidirectional Encoder for Bridging the Gap in Finance-Specific Deployment of Large Language Models", "categories": ["cs.IR", "cs.AI", "cs.CE", "cs.CL"], "comment": null, "summary": "In natural language processing (NLP), the focus has shifted from encoder-only\ntiny language models like BERT to decoder-only large language models(LLMs) such\nas GPT-3. However, LLMs' practical application in the financial sector has\nrevealed three limitations: (1) LLMs often perform worse than fine-tuned BERT\non discriminative tasks despite costing much higher computational resources,\nsuch as market sentiment analysis in financial reports; (2) Application on\ngenerative tasks heavily relies on retrieval augmented generation (RAG) methods\nto provide current and specialized information, with general retrievers showing\nsuboptimal performance on domain-specific retrieval tasks; (3) There are\nadditional inadequacies in other feature-based scenarios, such as topic\nmodeling. We introduce FinBERT2, a specialized bidirectional encoder pretrained\non a high-quality, financial-specific corpus of 32b tokens. This represents the\nlargest known Chinese financial pretraining corpus for models of this parameter\nsize. As a better backbone, FinBERT2 can bridge the gap in the\nfinancial-specific deployment of LLMs through the following achievements: (1)\nDiscriminative fine-tuned models (Fin-Labelers) outperform other (Fin)BERT\nvariants by 0.4%-3.3% and leading LLMs by 9.7%-12.3% on average across five\nfinancial classification tasks. (2) Contrastive fine-tuned models\n(Fin-Retrievers) outperform both open-source (e.g., +6.8\\% avg improvement over\nBGE-base-zh) and proprietary (e.g., +4.2\\% avg improvement over OpenAI's\ntext-embedding-3-large) embedders across five financial retrieval tasks; (3)\nBuilding on FinBERT2 variants, we construct the Fin-TopicModel, which enables\nsuperior clustering and topic representation for financial titles. Our work\nrevisits financial BERT models through comparative analysis with contemporary\nLLMs and offers practical insights for effectively utilizing FinBERT in the\nLLMs era.", "AI": {"tldr": "FinBERT2, a specialized bidirectional encoder pretrained on a financial corpus, outperforms LLMs and BERT variants in financial NLP tasks like classification, retrieval, and topic modeling.", "motivation": "Address limitations of LLMs in financial NLP, such as poor performance on discriminative tasks, reliance on RAG for generative tasks, and inadequacies in feature-based scenarios.", "method": "Introduce FinBERT2, pretrained on a 32b-token financial corpus, and fine-tune it for discriminative (Fin-Labelers), retrieval (Fin-Retrievers), and topic modeling (Fin-TopicModel) tasks.", "result": "FinBERT2 outperforms BERT variants (0.4%-3.3%) and LLMs (9.7%-12.3%) in classification, beats open-source and proprietary embedders in retrieval (+4.2%-6.8%), and enables superior topic modeling.", "conclusion": "FinBERT2 bridges gaps in financial NLP deployment, offering practical insights for leveraging specialized models in the LLMs era."}}
{"id": "2506.07739", "pdf": "https://arxiv.org/pdf/2506.07739", "abs": "https://arxiv.org/abs/2506.07739", "authors": ["Jing Zhong", "Jun Yin", "Peilin Li", "Pengyu Zeng", "Miao Zhang", "Shuai Lu", "Ran Luo"], "title": "ArchiLense: A Framework for Quantitative Analysis of Architectural Styles Based on Vision Large Language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Architectural cultures across regions are characterized by stylistic\ndiversity, shaped by historical, social, and technological contexts in addition\nto geograph-ical conditions. Understanding architectural styles requires the\nability to describe and analyze the stylistic features of different architects\nfrom various regions through visual observations of architectural imagery.\nHowever, traditional studies of architectural culture have largely relied on\nsubjective expert interpretations and historical literature reviews, often\nsuffering from regional biases and limited ex-planatory scope. To address these\nchallenges, this study proposes three core contributions: (1) We construct a\nprofessional architectural style dataset named ArchDiffBench, which comprises\n1,765 high-quality architectural images and their corresponding style\nannotations, collected from different regions and historical periods. (2) We\npropose ArchiLense, an analytical framework grounded in Vision-Language Models\nand constructed using the ArchDiffBench dataset. By integrating ad-vanced\ncomputer vision techniques, deep learning, and machine learning algo-rithms,\nArchiLense enables automatic recognition, comparison, and precise\nclassi-fication of architectural imagery, producing descriptive language\noutputs that ar-ticulate stylistic differences. (3) Extensive evaluations show\nthat ArchiLense achieves strong performance in architectural style recognition,\nwith a 92.4% con-sistency rate with expert annotations and 84.5% classification\naccuracy, effec-tively capturing stylistic distinctions across images. The\nproposed approach transcends the subjectivity inherent in traditional analyses\nand offers a more objective and accurate perspective for comparative studies of\narchitectural culture.", "AI": {"tldr": "The paper introduces ArchDiffBench, a dataset of 1,765 architectural images with style annotations, and ArchiLense, a framework using Vision-Language Models for automated style recognition and classification, achieving high accuracy.", "motivation": "Traditional architectural studies rely on subjective expert interpretations and suffer from regional biases. This study aims to provide an objective, automated approach for analyzing architectural styles.", "method": "The study constructs the ArchDiffBench dataset and develops ArchiLense, a framework combining Vision-Language Models, computer vision, and machine learning for style recognition and classification.", "result": "ArchiLense achieves 92.4% consistency with expert annotations and 84.5% classification accuracy, effectively capturing stylistic differences.", "conclusion": "The proposed approach offers an objective, accurate alternative to traditional methods for comparative architectural studies."}}
{"id": "2506.06339", "pdf": "https://arxiv.org/pdf/2506.06339", "abs": "https://arxiv.org/abs/2506.06339", "authors": ["Jumana Alsubhi", "Mohammad D. Alahmadi", "Ahmed Alhusayni", "Ibrahim Aldailami", "Israa Hamdine", "Ahmad Shabana", "Yazeed Iskandar", "Suhayb Khayyat"], "title": "Optimizing RAG Pipelines for Arabic: A Systematic Analysis of Core Components", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful architecture\nfor combining the precision of retrieval systems with the fluency of large\nlanguage models. While several studies have investigated RAG pipelines for\nhigh-resource languages, the optimization of RAG components for Arabic remains\nunderexplored. This study presents a comprehensive empirical evaluation of\nstate-of-the-art RAG components-including chunking strategies, embedding\nmodels, rerankers, and language models-across a diverse set of Arabic datasets.\nUsing the RAGAS framework, we systematically compare performance across four\ncore metrics: context precision, context recall, answer faithfulness, and\nanswer relevancy. Our experiments demonstrate that sentence-aware chunking\noutperforms all other segmentation methods, while BGE-M3 and\nMultilingual-E5-large emerge as the most effective embedding models. The\ninclusion of a reranker (bge-reranker-v2-m3) significantly boosts faithfulness\nin complex datasets, and Aya-8B surpasses StableLM in generation quality. These\nfindings provide critical insights for building high-quality Arabic RAG\npipelines and offer practical guidelines for selecting optimal components\nacross different document types.", "AI": {"tldr": "The paper evaluates RAG components for Arabic, finding sentence-aware chunking, BGE-M3, Multilingual-E5-large, and bge-reranker-v2-m3 optimal, with Aya-8B excelling in generation.", "motivation": "To optimize RAG pipelines for Arabic, as prior work focused on high-resource languages.", "method": "Empirical evaluation of RAG components (chunking, embeddings, rerankers, models) using RAGAS metrics on Arabic datasets.", "result": "Sentence-aware chunking, BGE-M3, Multilingual-E5-large, and bge-reranker-v2-m3 performed best; Aya-8B outperformed StableLM.", "conclusion": "Provides insights and guidelines for building high-quality Arabic RAG pipelines."}}
{"id": "2506.07740", "pdf": "https://arxiv.org/pdf/2506.07740", "abs": "https://arxiv.org/abs/2506.07740", "authors": ["Yingping Liang", "Ying Fu", "Yutao Hu", "Wenqi Shao", "Jiaming Liu", "Debing Zhang"], "title": "Flow-Anything: Learning Real-World Optical Flow Estimation from Large-Scale Single-view Images", "categories": ["cs.CV"], "comment": null, "summary": "Optical flow estimation is a crucial subfield of computer vision, serving as\na foundation for video tasks. However, the real-world robustness is limited by\nanimated synthetic datasets for training. This introduces domain gaps when\napplied to real-world applications and limits the benefits of scaling up\ndatasets. To address these challenges, we propose \\textbf{Flow-Anything}, a\nlarge-scale data generation framework designed to learn optical flow estimation\nfrom any single-view images in the real world. We employ two effective steps to\nmake data scaling-up promising. First, we convert a single-view image into a 3D\nrepresentation using advanced monocular depth estimation networks. This allows\nus to render optical flow and novel view images under a virtual camera. Second,\nwe develop an Object-Independent Volume Rendering module and a Depth-Aware\nInpainting module to model the dynamic objects in the 3D representation. These\ntwo steps allow us to generate realistic datasets for training from large-scale\nsingle-view images, namely \\textbf{FA-Flow Dataset}. For the first time, we\ndemonstrate the benefits of generating optical flow training data from\nlarge-scale real-world images, outperforming the most advanced unsupervised\nmethods and supervised methods on synthetic datasets. Moreover, our models\nserve as a foundation model and enhance the performance of various downstream\nvideo tasks.", "AI": {"tldr": "Flow-Anything is a framework for generating optical flow training data from real-world single-view images, outperforming existing methods and enhancing downstream video tasks.", "motivation": "Addressing the domain gap and limitations of synthetic datasets for optical flow estimation in real-world applications.", "method": "Uses monocular depth estimation to convert images into 3D representations, then employs Object-Independent Volume Rendering and Depth-Aware Inpainting to model dynamic objects.", "result": "Outperforms advanced unsupervised and supervised methods on synthetic datasets and improves downstream task performance.", "conclusion": "Flow-Anything demonstrates the potential of real-world image-based training data for optical flow estimation and broader video applications."}}
{"id": "2506.06355", "pdf": "https://arxiv.org/pdf/2506.06355", "abs": "https://arxiv.org/abs/2506.06355", "authors": ["Lingyao Li", "Dawei Li", "Zhenhui Ou", "Xiaoran Xu", "Jingxiao Liu", "Zihui Ma", "Runlong Yu", "Min Deng"], "title": "LLMs as World Models: Data-Driven and Human-Centered Pre-Event Simulation for Disaster Impact Assessment", "categories": ["cs.CY", "cs.CE", "cs.CL", "cs.CV"], "comment": null, "summary": "Efficient simulation is essential for enhancing proactive preparedness for\nsudden-onset disasters such as earthquakes. Recent advancements in large\nlanguage models (LLMs) as world models show promise in simulating complex\nscenarios. This study examines multiple LLMs to proactively estimate perceived\nearthquake impacts. Leveraging multimodal datasets including geospatial,\nsocioeconomic, building, and street-level imagery data, our framework generates\nModified Mercalli Intensity (MMI) predictions at zip code and county scales.\nEvaluations on the 2014 Napa and 2019 Ridgecrest earthquakes using USGS ''Did\nYou Feel It? (DYFI)'' reports demonstrate significant alignment, as evidenced\nby a high correlation of 0.88 and a low RMSE of 0.77 as compared to real\nreports at the zip code level. Techniques such as RAG and ICL can improve\nsimulation performance, while visual inputs notably enhance accuracy compared\nto structured numerical data alone. These findings show the promise of LLMs in\nsimulating disaster impacts that can help strengthen pre-event planning.", "AI": {"tldr": "LLMs are used to simulate earthquake impacts using multimodal data, showing high accuracy in predicting MMI at zip code and county levels.", "motivation": "Enhance proactive preparedness for sudden-onset disasters like earthquakes by leveraging LLMs for efficient simulation.", "method": "Utilize multimodal datasets (geospatial, socioeconomic, building, street-level imagery) and LLMs to predict MMI. Techniques like RAG and ICL improve performance.", "result": "High correlation (0.88) and low RMSE (0.77) with USGS DYFI reports, especially with visual inputs enhancing accuracy.", "conclusion": "LLMs show promise for disaster impact simulation, aiding pre-event planning."}}
{"id": "2506.07750", "pdf": "https://arxiv.org/pdf/2506.07750", "abs": "https://arxiv.org/abs/2506.07750", "authors": ["Hyunsoo Kim", "Donghyun Kim", "Suhyun Kim"], "title": "Difference Inversion: Interpolate and Isolate the Difference with Token Consistency for Image Analogy Generation", "categories": ["cs.CV"], "comment": "Published at CVPR 2025", "summary": "How can we generate an image B' that satisfies A:A'::B:B', given the input\nimages A,A' and B? Recent works have tackled this challenge through approaches\nlike visual in-context learning or visual instruction. However, these methods\nare typically limited to specific models (e.g. InstructPix2Pix. Inpainting\nmodels) rather than general diffusion models (e.g. Stable Diffusion, SDXL).\nThis dependency may lead to inherited biases or lower editing capabilities. In\nthis paper, we propose Difference Inversion, a method that isolates only the\ndifference from A and A' and applies it to B to generate a plausible B'. To\naddress model dependency, it is crucial to structure prompts in the form of a\n\"Full Prompt\" suitable for input to stable diffusion models, rather than using\nan \"Instruction Prompt\". To this end, we accurately extract the Difference\nbetween A and A' and combine it with the prompt of B, enabling a plug-and-play\napplication of the difference. To extract a precise difference, we first\nidentify it through 1) Delta Interpolation. Additionally, to ensure accurate\ntraining, we propose the 2) Token Consistency Loss and 3) Zero Initialization\nof Token Embeddings. Our extensive experiments demonstrate that Difference\nInversion outperforms existing baselines both quantitatively and qualitatively,\nindicating its ability to generate more feasible B' in a model-agnostic manner.", "AI": {"tldr": "Proposes Difference Inversion, a method to generate image B' by isolating and applying the difference between A and A' to B, avoiding model-specific limitations.", "motivation": "Existing methods rely on specific models, leading to biases or limited editing. A general solution is needed.", "method": "Uses Difference Inversion with Delta Interpolation, Token Consistency Loss, and Zero Initialization to extract and apply differences.", "result": "Outperforms baselines, generating feasible B' in a model-agnostic way.", "conclusion": "Difference Inversion is effective for image editing without model dependency."}}
{"id": "2506.06382", "pdf": "https://arxiv.org/pdf/2506.06382", "abs": "https://arxiv.org/abs/2506.06382", "authors": ["Micha\u0142 P. Karpowicz"], "title": "On the Fundamental Impossibility of Hallucination Control in Large Language Models", "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.GT", "cs.LG"], "comment": null, "summary": "This paper explains \\textbf{why it is impossible to create large language\nmodels that do not hallucinate and what are the trade-offs we should be looking\nfor}. It presents a formal \\textbf{impossibility theorem} demonstrating that no\ninference mechanism can simultaneously satisfy four fundamental properties:\n\\textbf{truthful (non-hallucinatory) generation, semantic information\nconservation, relevant knowledge revelation, and knowledge-constrained\noptimality}. By modeling LLM inference as an \\textbf{auction of ideas} where\nneural components compete to contribute to responses, we prove the\nimpossibility using the Green-Laffont theorem. That mathematical framework\nprovides a rigorous foundation for understanding the nature of inference\nprocess, with implications for model architecture, training objectives, and\nevaluation methods.", "AI": {"tldr": "The paper proves it's impossible for large language models (LLMs) to avoid hallucinations while meeting four key properties: truthful generation, semantic conservation, knowledge revelation, and optimality. It uses an auction-based model and the Green-Laffont theorem to formalize this.", "motivation": "To understand the inherent limitations of LLMs in avoiding hallucinations and identify the trade-offs required in their design.", "method": "Models LLM inference as an 'auction of ideas,' applying the Green-Laffont theorem to demonstrate the impossibility of simultaneously satisfying four properties.", "result": "A formal impossibility theorem showing no LLM can avoid hallucinations while meeting all four desired properties.", "conclusion": "The findings highlight unavoidable trade-offs in LLM design, guiding future work on architecture, training, and evaluation."}}
{"id": "2506.07773", "pdf": "https://arxiv.org/pdf/2506.07773", "abs": "https://arxiv.org/abs/2506.07773", "authors": ["Mohamed Djilani", "Nassim Ali Ousalah", "Nidhal Eddine Chenni"], "title": "Trend-Aware Fashion Recommendation with Visual Segmentation and Semantic Similarity", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "We introduce a trend-aware and visually-grounded fashion recommendation\nsystem that integrates deep visual representations, garment-aware segmentation,\nsemantic category similarity and user behavior simulation. Our pipeline\nextracts focused visual embeddings by masking non-garment regions via semantic\nsegmentation followed by feature extraction using pretrained CNN backbones\n(ResNet-50, DenseNet-121, VGG16). To simulate realistic shopping behavior, we\ngenerate synthetic purchase histories influenced by user-specific trendiness\nand item popularity. Recommendations are computed using a weighted scoring\nfunction that fuses visual similarity, semantic coherence and popularity\nalignment. Experiments on the DeepFashion dataset demonstrate consistent gender\nalignment and improved category relevance, with ResNet-50 achieving 64.95%\ncategory similarity and lowest popularity MAE. An ablation study confirms the\ncomplementary roles of visual and popularity cues. Our method provides a\nscalable framework for personalized fashion recommendations that balances\nindividual style with emerging trends. Our implementation is available at\nhttps://github.com/meddjilani/FashionRecommender", "AI": {"tldr": "A fashion recommendation system using visual embeddings, semantic segmentation, and user behavior simulation to balance style and trends.", "motivation": "To create a personalized fashion recommendation system that integrates visual, semantic, and popularity cues for better user alignment.", "method": "Uses deep visual embeddings (ResNet-50, DenseNet-121, VGG16) with garment-aware segmentation, synthetic purchase histories, and a weighted scoring function for recommendations.", "result": "Achieves 64.95% category similarity and low popularity MAE on DeepFashion dataset, with gender alignment and improved relevance.", "conclusion": "The method offers a scalable framework for personalized fashion recommendations, balancing individual style with trends."}}
{"id": "2506.06391", "pdf": "https://arxiv.org/pdf/2506.06391", "abs": "https://arxiv.org/abs/2506.06391", "authors": ["John Mavi", "Diana Teodora G\u0103itan", "Sergio Coronado"], "title": "From Rogue to Safe AI: The Role of Explicit Refusals in Aligning LLMs with International Humanitarian Law", "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are widely used across sectors, yet their\nalignment with International Humanitarian Law (IHL) is not well understood.\nThis study evaluates eight leading LLMs on their ability to refuse prompts that\nexplicitly violate these legal frameworks, focusing also on helpfulness - how\nclearly and constructively refusals are communicated. While most models\nrejected unlawful requests, the clarity and consistency of their responses\nvaried. By revealing the model's rationale and referencing relevant legal or\nsafety principles, explanatory refusals clarify the system's boundaries, reduce\nambiguity, and help prevent misuse. A standardised system-level safety prompt\nsignificantly improved the quality of the explanations expressed within\nrefusals in most models, highlighting the effectiveness of lightweight\ninterventions. However, more complex prompts involving technical language or\nrequests for code revealed ongoing vulnerabilities. These findings contribute\nto the development of safer, more transparent AI systems and propose a\nbenchmark to evaluate the compliance of LLM with IHL.", "AI": {"tldr": "The study evaluates eight LLMs on their compliance with IHL, focusing on refusal clarity and helpfulness. Most models rejected unlawful requests, but response quality varied. A standardized safety prompt improved explanations, though vulnerabilities remained for complex prompts.", "motivation": "To assess LLMs' alignment with IHL and their ability to clearly refuse unlawful requests, aiming to improve AI safety and transparency.", "method": "Evaluated eight leading LLMs on their refusal responses to IHL-violating prompts, testing clarity and consistency. Used a standardized safety prompt to measure improvements.", "result": "Most models rejected unlawful requests, but response clarity varied. The safety prompt improved explanations, though complex prompts still posed challenges.", "conclusion": "The study highlights the need for clearer refusal mechanisms in LLMs and proposes a benchmark for IHL compliance, suggesting lightweight interventions can enhance safety."}}
{"id": "2506.07778", "pdf": "https://arxiv.org/pdf/2506.07778", "abs": "https://arxiv.org/abs/2506.07778", "authors": ["Yichang Xu", "Gaowen Liu", "Ramana Rao Kompella", "Sihao Hu", "Tiansheng Huang", "Fatih Ilhan", "Selim Furkan Tekin", "Zachary Yahn", "Ling Liu"], "title": "Language-Vision Planner and Executor for Text-to-Visual Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "The advancement in large language models (LLMs) and large vision models has\nfueled the rapid progress in multi-modal visual-text reasoning capabilities.\nHowever, existing vision-language models (VLMs) to date suffer from\ngeneralization performance. Inspired by recent development in LLMs for visual\nreasoning, this paper presents VLAgent, an AI system that can create a\nstep-by-step visual reasoning plan with an easy-to-understand script and\nexecute each step of the plan in real time by integrating planning script with\nexecution verifications via an automated process supported by VLAgent. In the\ntask planning phase, VLAgent fine-tunes an LLM through in-context learning to\ngenerate a step-by-step planner for each user-submitted text-visual reasoning\ntask. During the plan execution phase, VLAgent progressively refines the\ncomposition of neuro-symbolic executable modules to generate high-confidence\nreasoning results. VLAgent has three unique design characteristics: First, we\nimprove the quality of plan generation through in-context learning, improving\nlogic reasoning by reducing erroneous logic steps, incorrect programs, and LLM\nhallucinations. Second, we design a syntax-semantics parser to identify and\ncorrect additional logic errors of the LLM-generated planning script prior to\nlaunching the plan executor. Finally, we employ the ensemble method to improve\nthe generalization performance of our step-executor. Extensive experiments with\nfour visual reasoning benchmarks (GQA, MME, NLVR2, VQAv2) show that VLAgent\nachieves significant performance enhancement for multimodal text-visual\nreasoning applications, compared to the exiting representative VLMs and LLM\nbased visual composition approaches like ViperGPT and VisProg, thanks to the\nnovel optimization modules of VLAgent back-engine (SS-Parser, Plan Repairer,\nOutput Verifiers). Code and data will be made available upon paper acceptance.", "AI": {"tldr": "VLAgent is an AI system that improves multimodal visual-text reasoning by generating step-by-step plans and executing them with verification, outperforming existing models.", "motivation": "Existing vision-language models (VLMs) lack generalization performance, prompting the development of VLAgent for better visual reasoning.", "method": "VLAgent uses in-context learning to generate planning scripts, refines neuro-symbolic modules for execution, and employs syntax-semantics parsing and ensemble methods.", "result": "VLAgent significantly outperforms existing VLMs and LLM-based approaches on benchmarks like GQA, MME, NLVR2, and VQAv2.", "conclusion": "VLAgent's novel optimization modules (SS-Parser, Plan Repairer, Output Verifiers) enhance multimodal reasoning, setting a new standard for performance."}}
{"id": "2506.06409", "pdf": "https://arxiv.org/pdf/2506.06409", "abs": "https://arxiv.org/abs/2506.06409", "authors": ["Dor Tsur", "Carol Xuan Long", "Claudio Mayrink Verdun", "Hsiang Hsu", "Chen-Fu Chen", "Haim Permuter", "Sajani Vithana", "Flavio P. Calmon"], "title": "HeavyWater and SimplexWater: Watermarking Low-Entropy Text Distributions", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.CY", "cs.IT", "cs.LG", "math.IT"], "comment": null, "summary": "Large language model (LLM) watermarks enable authentication of text\nprovenance, curb misuse of machine-generated text, and promote trust in AI\nsystems. Current watermarks operate by changing the next-token predictions\noutput by an LLM. The updated (i.e., watermarked) predictions depend on random\nside information produced, for example, by hashing previously generated tokens.\nLLM watermarking is particularly challenging in low-entropy generation tasks -\nsuch as coding - where next-token predictions are near-deterministic. In this\npaper, we propose an optimization framework for watermark design. Our goal is\nto understand how to most effectively use random side information in order to\nmaximize the likelihood of watermark detection and minimize the distortion of\ngenerated text. Our analysis informs the design of two new watermarks:\nHeavyWater and SimplexWater. Both watermarks are tunable, gracefully\ntrading-off between detection accuracy and text distortion. They can also be\napplied to any LLM and are agnostic to side information generation. We examine\nthe performance of HeavyWater and SimplexWater through several benchmarks,\ndemonstrating that they can achieve high watermark detection accuracy with\nminimal compromise of text generation quality, particularly in the low-entropy\nregime. Our theoretical analysis also reveals surprising new connections\nbetween LLM watermarking and coding theory. The code implementation can be\nfound in https://github.com/DorTsur/HeavyWater_SimplexWater", "AI": {"tldr": "The paper proposes an optimization framework for designing LLM watermarks, introducing two tunable watermarks (HeavyWater and SimplexWater) that balance detection accuracy and text distortion, especially in low-entropy tasks like coding.", "motivation": "To address challenges in LLM watermarking, particularly in low-entropy tasks, by optimizing the use of random side information for better detection and minimal text distortion.", "method": "An optimization framework for watermark design, leading to two new watermarks (HeavyWater and SimplexWater), which are tunable and agnostic to side information generation.", "result": "HeavyWater and SimplexWater achieve high detection accuracy with minimal text quality compromise, especially in low-entropy scenarios, and reveal connections to coding theory.", "conclusion": "The proposed watermarks effectively balance detection and distortion, offering practical solutions for LLM watermarking, with potential applications in coding and beyond."}}
{"id": "2506.07779", "pdf": "https://arxiv.org/pdf/2506.07779", "abs": "https://arxiv.org/abs/2506.07779", "authors": ["Beining Xu", "Junxian Li"], "title": "Design and Evaluation of Deep Learning-Based Dual-Spectrum Image Fusion Methods", "categories": ["cs.CV"], "comment": "11 pages, 13 figures", "summary": "Visible images offer rich texture details, while infrared images emphasize\nsalient targets. Fusing these complementary modalities enhances scene\nunderstanding, particularly for advanced vision tasks under challenging\nconditions. Recently, deep learning-based fusion methods have gained attention,\nbut current evaluations primarily rely on general-purpose metrics without\nstandardized benchmarks or downstream task performance. Additionally, the lack\nof well-developed dual-spectrum datasets and fair algorithm comparisons hinders\nprogress.\n  To address these gaps, we construct a high-quality dual-spectrum dataset\ncaptured in campus environments, comprising 1,369 well-aligned visible-infrared\nimage pairs across four representative scenarios: daytime, nighttime, smoke\nocclusion, and underpasses. We also propose a comprehensive and fair evaluation\nframework that integrates fusion speed, general metrics, and object detection\nperformance using the lang-segment-anything model to ensure fairness in\ndownstream evaluation.\n  Extensive experiments benchmark several state-of-the-art fusion algorithms\nunder this framework. Results demonstrate that fusion models optimized for\ndownstream tasks achieve superior performance in target detection, especially\nin low-light and occluded scenes. Notably, some algorithms that perform well on\ngeneral metrics do not translate to strong downstream performance, highlighting\nlimitations of current evaluation practices and validating the necessity of our\nproposed framework.\n  The main contributions of this work are: (1)a campus-oriented dual-spectrum\ndataset with diverse and challenging scenes; (2) a task-aware, comprehensive\nevaluation framework; and (3) thorough comparative analysis of leading fusion\nmethods across multiple datasets, offering insights for future development.", "AI": {"tldr": "The paper introduces a dual-spectrum dataset and a task-aware evaluation framework for visible-infrared image fusion, highlighting gaps in current methods and evaluations.", "motivation": "Current evaluations for visible-infrared image fusion lack standardized benchmarks and downstream task performance metrics, hindering progress.", "method": "Constructed a high-quality dual-spectrum dataset and proposed a comprehensive evaluation framework integrating fusion speed, general metrics, and object detection performance.", "result": "Fusion models optimized for downstream tasks outperform others, especially in low-light and occluded scenes. Some algorithms excelling in general metrics fail in downstream tasks.", "conclusion": "The work provides a dataset, evaluation framework, and comparative analysis, addressing gaps in current practices and guiding future research."}}
{"id": "2506.06540", "pdf": "https://arxiv.org/pdf/2506.06540", "abs": "https://arxiv.org/abs/2506.06540", "authors": ["Patrick Y. Wu"], "title": "Large Language Models Can Be a Viable Substitute for Expert Political Surveys When a Shock Disrupts Traditional Measurement Approaches", "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": "19 pages, 6 figures", "summary": "After a disruptive event or shock, such as the Department of Government\nEfficiency (DOGE) federal layoffs of 2025, expert judgments are colored by\nknowledge of the outcome. This can make it difficult or impossible to\nreconstruct the pre-event perceptions needed to study the factors associated\nwith the event. This position paper argues that large language models (LLMs),\ntrained on vast amounts of digital media data, can be a viable substitute for\nexpert political surveys when a shock disrupts traditional measurement. We\nanalyze the DOGE layoffs as a specific case study for this position. We use\npairwise comparison prompts with LLMs and derive ideology scores for federal\nexecutive agencies. These scores replicate pre-layoff expert measures and\npredict which agencies were targeted by DOGE. We also use this same approach\nand find that the perceptions of certain federal agencies as knowledge\ninstitutions predict which agencies were targeted by DOGE, even when\ncontrolling for ideology. This case study demonstrates that using LLMs allows\nus to rapidly and easily test the associated factors hypothesized behind the\nshock. More broadly, our case study of this recent event exemplifies how LLMs\noffer insights into the correlational factors of the shock when traditional\nmeasurement techniques fail. We conclude by proposing a two-part criterion for\nwhen researchers can turn to LLMs as a substitute for expert political surveys.", "AI": {"tldr": "The paper proposes using large language models (LLMs) as a substitute for expert political surveys after disruptive events, demonstrating their effectiveness in analyzing the DOGE layoffs of 2025.", "motivation": "Disruptive events like the DOGE layoffs make it hard to reconstruct pre-event perceptions, necessitating alternative measurement methods.", "method": "Pairwise comparison prompts with LLMs to derive ideology scores and analyze agency perceptions.", "result": "LLM-derived scores replicated pre-layoff expert measures and predicted targeted agencies, including perceptions of agencies as knowledge institutions.", "conclusion": "LLMs are viable for rapid testing of shock-associated factors when traditional methods fail, with proposed criteria for their use."}}
{"id": "2506.07785", "pdf": "https://arxiv.org/pdf/2506.07785", "abs": "https://arxiv.org/abs/2506.07785", "authors": ["Qi Yang", "Chenghao Zhang", "Lubin Fan", "Kun Ding", "Jieping Ye", "Shiming Xiang"], "title": "Re-ranking Reasoning Context with Tree Search Makes Large Vision-Language Models Stronger", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "ICML 2025 Spotlight. 22 pages, 16 figures", "summary": "Recent advancements in Large Vision Language Models (LVLMs) have\nsignificantly improved performance in Visual Question Answering (VQA) tasks\nthrough multimodal Retrieval-Augmented Generation (RAG). However, existing\nmethods still face challenges, such as the scarcity of knowledge with reasoning\nexamples and erratic responses from retrieved knowledge. To address these\nissues, in this study, we propose a multimodal RAG framework, termed RCTS,\nwhich enhances LVLMs by constructing a Reasoning Context-enriched knowledge\nbase and a Tree Search re-ranking method. Specifically, we introduce a\nself-consistent evaluation mechanism to enrich the knowledge base with\nintrinsic reasoning patterns. We further propose a Monte Carlo Tree Search with\nHeuristic Rewards (MCTS-HR) to prioritize the most relevant examples. This\nensures that LVLMs can leverage high-quality contextual reasoning for better\nand more consistent responses. Extensive experiments demonstrate that our\nframework achieves state-of-the-art performance on multiple VQA datasets,\nsignificantly outperforming In-Context Learning (ICL) and Vanilla-RAG methods.\nIt highlights the effectiveness of our knowledge base and re-ranking method in\nimproving LVLMs. Our code is available at https://github.com/yannqi/RCTS-RAG.", "AI": {"tldr": "The paper proposes RCTS, a multimodal RAG framework for LVLMs, addressing challenges like knowledge scarcity and erratic responses by enriching the knowledge base with reasoning context and using tree search re-ranking.", "motivation": "Existing LVLMs struggle with knowledge scarcity and inconsistent responses in VQA tasks, prompting the need for improved multimodal RAG methods.", "method": "RCTS constructs a Reasoning Context-enriched knowledge base and employs Monte Carlo Tree Search with Heuristic Rewards (MCTS-HR) for re-ranking.", "result": "The framework achieves state-of-the-art performance on VQA datasets, outperforming ICL and Vanilla-RAG methods.", "conclusion": "RCTS effectively enhances LVLMs by leveraging high-quality contextual reasoning, demonstrating significant improvements in VQA tasks."}}
{"id": "2506.06576", "pdf": "https://arxiv.org/pdf/2506.06576", "abs": "https://arxiv.org/abs/2506.06576", "authors": ["Yijia Shao", "Humishka Zope", "Yucheng Jiang", "Jiaxin Pei", "David Nguyen", "Erik Brynjolfsson", "Diyi Yang"], "title": "Future of Work with AI Agents: Auditing Automation and Augmentation Potential across the U.S. Workforce", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC", "cs.LG"], "comment": "Preprint", "summary": "The rapid rise of compound AI systems (a.k.a., AI agents) is reshaping the\nlabor market, raising concerns about job displacement, diminished human agency,\nand overreliance on automation. Yet, we lack a systematic understanding of the\nevolving landscape. In this paper, we address this gap by introducing a novel\nauditing framework to assess which occupational tasks workers want AI agents to\nautomate or augment, and how those desires align with the current technological\ncapabilities. Our framework features an audio-enhanced mini-interview to\ncapture nuanced worker desires and introduces the Human Agency Scale (HAS) as a\nshared language to quantify the preferred level of human involvement. Using\nthis framework, we construct the WORKBank database, building on the U.S.\nDepartment of Labor's O*NET database, to capture preferences from 1,500 domain\nworkers and capability assessments from AI experts across over 844 tasks\nspanning 104 occupations. Jointly considering the desire and technological\ncapability divides tasks in WORKBank into four zones: Automation \"Green Light\"\nZone, Automation \"Red Light\" Zone, R&D Opportunity Zone, Low Priority Zone.\nThis highlights critical mismatches and opportunities for AI agent development.\nMoving beyond a simple automate-or-not dichotomy, our results reveal diverse\nHAS profiles across occupations, reflecting heterogeneous expectations for\nhuman involvement. Moreover, our study offers early signals of how AI agent\nintegration may reshape the core human competencies, shifting from\ninformation-focused skills to interpersonal ones. These findings underscore the\nimportance of aligning AI agent development with human desires and preparing\nworkers for evolving workplace dynamics.", "AI": {"tldr": "The paper introduces an auditing framework to assess worker preferences for AI automation/augmentation, aligning them with technological capabilities, and reveals mismatches and opportunities in AI agent development.", "motivation": "To address the lack of systematic understanding of how workers want AI to automate or augment tasks and how this aligns with current technological capabilities.", "method": "Developed an audio-enhanced mini-interview framework and the Human Agency Scale (HAS) to quantify worker preferences. Constructed the WORKBank database with input from 1,500 workers and AI experts across 844 tasks in 104 occupations.", "result": "Tasks were categorized into four zones (Green Light, Red Light, R&D Opportunity, Low Priority), revealing mismatches and opportunities. Diverse HAS profiles showed varied expectations for human involvement.", "conclusion": "AI agent development must align with human desires, and workers need preparation for shifting workplace dynamics, emphasizing interpersonal skills over information-focused ones."}}
{"id": "2506.07803", "pdf": "https://arxiv.org/pdf/2506.07803", "abs": "https://arxiv.org/abs/2506.07803", "authors": ["Eduard Allakhverdov", "Dmitrii Tarasov", "Elizaveta Goncharova", "Andrey Kuznetsov"], "title": "Image Reconstruction as a Tool for Feature Analysis", "categories": ["cs.CV", "68T10, 68T30, 68T45", "I.2.10"], "comment": "23 pages, 14 figures", "summary": "Vision encoders are increasingly used in modern applications, from\nvision-only models to multimodal systems such as vision-language models.\nDespite their remarkable success, it remains unclear how these architectures\nrepresent features internally. Here, we propose a novel approach for\ninterpreting vision features via image reconstruction. We compare two related\nmodel families, SigLIP and SigLIP2, which differ only in their training\nobjective, and show that encoders pre-trained on image-based tasks retain\nsignificantly more image information than those trained on non-image tasks such\nas contrastive learning. We further apply our method to a range of vision\nencoders, ranking them by the informativeness of their feature representations.\nFinally, we demonstrate that manipulating the feature space yields predictable\nchanges in reconstructed images, revealing that orthogonal rotations (rather\nthan spatial transformations) control color encoding. Our approach can be\napplied to any vision encoder, shedding light on the inner structure of its\nfeature space. The code and model weights to reproduce the experiments are\navailable in GitHub.", "AI": {"tldr": "The paper proposes a method to interpret vision encoders via image reconstruction, comparing SigLIP and SigLIP2 models, and reveals insights into feature representation and color encoding.", "motivation": "Understanding how vision encoders internally represent features, given their widespread use in vision-only and multimodal systems.", "method": "A novel approach using image reconstruction to analyze feature representations, comparing models with different training objectives (SigLIP vs. SigLIP2).", "result": "Encoders pre-trained on image tasks retain more image information than those trained on non-image tasks. Feature manipulation shows orthogonal rotations control color encoding.", "conclusion": "The method provides insights into vision encoder feature spaces and is applicable to any encoder, with code and models available for reproducibility."}}
{"id": "2506.06579", "pdf": "https://arxiv.org/pdf/2506.06579", "abs": "https://arxiv.org/abs/2506.06579", "authors": ["Adarsh Prasad Behera", "Jaya Prakash Champati", "Roberto Morabito", "Sasu Tarkoma", "James Gross"], "title": "Towards Efficient Multi-LLM Inference: Characterization and Analysis of LLM Routing and Hierarchical Techniques", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DC"], "comment": null, "summary": "Recent progress in Language Models (LMs) has dramatically advanced the field\nof natural language processing (NLP), excelling at tasks like text generation,\nsummarization, and question answering. However, their inference remains\ncomputationally expensive and energy intensive, especially in settings with\nlimited hardware, power, or bandwidth. This makes it difficult to deploy LMs in\nmobile, edge, or cost sensitive environments. To address these challenges,\nrecent approaches have introduced multi LLM intelligent model selection\nstrategies that dynamically allocate computational resources based on query\ncomplexity -- using lightweight models for simpler queries and escalating to\nlarger models only when necessary. This survey explores two complementary\nstrategies for efficient LLM inference: (i) routing, which selects the most\nsuitable model based on the query, and (ii) cascading or hierarchical inference\n(HI), which escalates queries through a sequence of models until a confident\nresponse is found. Both approaches aim to reduce computation by using\nlightweight models for simpler tasks while offloading only when needed. We\nprovide a comparative analysis of these techniques across key performance\nmetrics, discuss benchmarking efforts, and outline open challenges. Finally, we\noutline future research directions to enable faster response times, adaptive\nmodel selection based on task complexity, and scalable deployment across\nheterogeneous environments, making LLM based systems more efficient and\naccessible for real world applications.", "AI": {"tldr": "The paper surveys strategies for efficient inference in Language Models (LMs), focusing on model selection (routing) and hierarchical inference (HI) to reduce computational costs.", "motivation": "LMs are computationally expensive, limiting deployment in resource-constrained environments like mobile or edge devices.", "method": "Two strategies are explored: (i) routing to select models based on query complexity, and (ii) hierarchical inference (HI) to escalate queries through models until confident responses are found.", "result": "These approaches reduce computation by using lightweight models for simpler tasks, improving efficiency.", "conclusion": "Future research aims to enhance response times, adaptive model selection, and scalable deployment for real-world LM applications."}}
{"id": "2506.07809", "pdf": "https://arxiv.org/pdf/2506.07809", "abs": "https://arxiv.org/abs/2506.07809", "authors": ["Weilei Wen", "Tianyi Zhang", "Qianqian Zhao", "Zhaohui Zheng", "Chunle Guo", "Xiuli Shao", "Chongyi Li"], "title": "Incorporating Uncertainty-Guided and Top-k Codebook Matching for Real-World Blind Image Super-Resolution", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in codebook-based real image super-resolution (SR) have\nshown promising results in real-world applications. The core idea involves\nmatching high-quality image features from a codebook based on low-resolution\n(LR) image features. However, existing methods face two major challenges:\ninaccurate feature matching with the codebook and poor texture detail\nreconstruction. To address these issues, we propose a novel Uncertainty-Guided\nand Top-k Codebook Matching SR (UGTSR) framework, which incorporates three key\ncomponents: (1) an uncertainty learning mechanism that guides the model to\nfocus on texture-rich regions, (2) a Top-k feature matching strategy that\nenhances feature matching accuracy by fusing multiple candidate features, and\n(3) an Align-Attention module that enhances the alignment of information\nbetween LR and HR features. Experimental results demonstrate significant\nimprovements in texture realism and reconstruction fidelity compared to\nexisting methods. We will release the code upon formal publication.", "AI": {"tldr": "The paper proposes UGTSR, a novel framework for real image super-resolution, addressing inaccurate feature matching and poor texture detail reconstruction with uncertainty learning, Top-k matching, and an Align-Attention module.", "motivation": "Existing codebook-based SR methods struggle with inaccurate feature matching and poor texture detail reconstruction.", "method": "UGTSR introduces an uncertainty learning mechanism, Top-k feature matching, and an Align-Attention module to improve feature matching and texture reconstruction.", "result": "The framework achieves significant improvements in texture realism and reconstruction fidelity.", "conclusion": "UGTSR outperforms existing methods and will release code upon publication."}}
{"id": "2506.06632", "pdf": "https://arxiv.org/pdf/2506.06632", "abs": "https://arxiv.org/abs/2506.06632", "authors": ["Shubham Parashar", "Shurui Gui", "Xiner Li", "Hongyi Ling", "Sushil Vemuri", "Blake Olson", "Eric Li", "Yu Zhang", "James Caverlee", "Dileep Kalathil", "Shuiwang Ji"], "title": "Curriculum Reinforcement Learning from Easy to Hard Tasks Improves LLM Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "We aim to improve the reasoning capabilities of language models via\nreinforcement learning (RL). Recent RL post-trained models like DeepSeek-R1\nhave demonstrated reasoning abilities on mathematical and coding tasks.\nHowever, prior studies suggest that using RL alone to improve reasoning on\ninherently difficult tasks is less effective. Here, we draw inspiration from\ncurriculum learning and propose to schedule tasks from easy to hard (E2H),\nallowing LLMs to build reasoning skills gradually. Our method is termed E2H\nReasoner. Empirically, we observe that, although easy tasks are important\ninitially, fading them out through appropriate scheduling is essential in\npreventing overfitting. Theoretically, we establish convergence guarantees for\nE2H Reasoner within an approximate policy iteration framework. We derive\nfinite-sample complexity bounds and show that when tasks are appropriately\ndecomposed and conditioned, learning through curriculum stages requires fewer\ntotal samples than direct learning. Experiments across multiple domains show\nthat E2H Reasoner significantly improves the reasoning ability of small LLMs\n(1.5B to 3B), which otherwise struggle when trained with vanilla RL alone,\nhighlighting the effectiveness of our method.", "AI": {"tldr": "The paper proposes E2H Reasoner, a curriculum learning method for improving reasoning in language models by scheduling tasks from easy to hard, showing better performance than vanilla RL.", "motivation": "To enhance reasoning in language models, as RL alone is less effective for inherently difficult tasks.", "method": "Curriculum learning with task scheduling from easy to hard (E2H), fading easy tasks to prevent overfitting, supported by theoretical guarantees.", "result": "E2H Reasoner improves reasoning in small LLMs (1.5B to 3B), outperforming vanilla RL, with fewer samples required.", "conclusion": "Curriculum learning via E2H scheduling is effective for improving reasoning in language models, especially for smaller models."}}
{"id": "2506.07811", "pdf": "https://arxiv.org/pdf/2506.07811", "abs": "https://arxiv.org/abs/2506.07811", "authors": ["Tieyuan Chen", "Huabin Liu", "Yi Wang", "Chaofan Gan", "Mingxi Lyu", "Gui Zou", "Weiyao Lin"], "title": "Looking Beyond Visible Cues: Implicit Video Question Answering via Dual-Clue Reasoning", "categories": ["cs.CV"], "comment": "Preprint", "summary": "Video Question Answering (VideoQA) aims to answer natural language questions\nbased on the given video, with prior work primarily focusing on identifying the\nduration of relevant segments, referred to as explicit visual evidence.\nHowever, explicit visual evidence is not always directly available,\nparticularly when questions target symbolic meanings or deeper intentions,\nleading to significant performance degradation. To fill this gap, we introduce\na novel task and dataset, $\\textbf{I}$mplicit $\\textbf{V}$ideo\n$\\textbf{Q}$uestion $\\textbf{A}$nswering (I-VQA), which focuses on answering\nquestions in scenarios where explicit visual evidence is inaccessible. Given an\nimplicit question and its corresponding video, I-VQA requires answering based\non the contextual visual cues present within the video. To tackle I-VQA, we\npropose a novel reasoning framework, IRM (Implicit Reasoning Model),\nincorporating dual-stream modeling of contextual actions and intent clues as\nimplicit reasoning chains. IRM comprises the Action-Intent Module (AIM) and the\nVisual Enhancement Module (VEM). AIM deduces and preserves question-related\ndual clues by generating clue candidates and performing relation deduction. VEM\nenhances contextual visual representation by leveraging key contextual clues.\nExtensive experiments validate the effectiveness of our IRM in I-VQA tasks,\noutperforming GPT-4o, OpenAI-o3, and fine-tuned VideoChat2 by $0.76\\%$,\n$1.37\\%$, and $4.87\\%$, respectively. Additionally, IRM performs SOTA on\nsimilar implicit advertisement understanding and future prediction in\ntraffic-VQA. Datasets and codes are available for double-blind review in\nanonymous repo: https://github.com/tychen-SJTU/Implicit-VideoQA.", "AI": {"tldr": "The paper introduces a new task, Implicit Video Question Answering (I-VQA), and a dataset for scenarios where explicit visual evidence is missing. It proposes the Implicit Reasoning Model (IRM) with dual-stream modeling to handle contextual cues, outperforming existing models.", "motivation": "Prior VideoQA work relies on explicit visual evidence, which fails for questions about symbolic meanings or deeper intentions. This gap motivates the I-VQA task and IRM framework.", "method": "IRM uses dual-stream modeling (Action-Intent Module and Visual Enhancement Module) to deduce contextual clues and enhance visual representation.", "result": "IRM outperforms GPT-4o, OpenAI-o3, and VideoChat2 by 0.76%, 1.37%, and 4.87%, respectively, and achieves SOTA in related tasks.", "conclusion": "IRM effectively addresses I-VQA by leveraging implicit reasoning chains, demonstrating superior performance and potential for broader applications."}}
{"id": "2506.06698", "pdf": "https://arxiv.org/pdf/2506.06698", "abs": "https://arxiv.org/abs/2506.06698", "authors": ["Yitao Liu", "Chenglei Si", "Karthik Narasimhan", "Shunyu Yao"], "title": "Contextual Experience Replay for Self-Improvement of Language Agents", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": "Accepted to ACL 2025. 20 pages", "summary": "Large language model (LLM) agents have been applied to sequential\ndecision-making tasks such as web navigation, but without any\nenvironment-specific experiences, they often fail in these complex tasks.\nMoreover, current LLM agents are not designed to continually learn from past\nexperiences during inference time, which could be crucial for them to gain\nthese environment-specific experiences. To address this, we propose Contextual\nExperience Replay (CER), a training-free framework to enable efficient\nself-improvement for language agents in their context window. Specifically, CER\naccumulates and synthesizes past experiences into a dynamic memory buffer.\nThese experiences encompass environment dynamics and common decision-making\npatterns, allowing the agents to retrieve and augment themselves with relevant\nknowledge in new tasks, enhancing their adaptability in complex environments.\nWe evaluate CER on the challenging WebArena and VisualWebArena benchmarks. On\nVisualWebArena, CER achieves a competitive performance of 31.9%. On WebArena,\nCER also gets a competitive average success rate of 36.7%, relatively improving\nthe success rate of the GPT-4o agent baseline by 51.0%. We also conduct a\ncomprehensive analysis on it to prove its efficiency, validity and understand\nit better.", "AI": {"tldr": "CER enables LLM agents to self-improve by replaying past experiences, improving adaptability in complex tasks like web navigation.", "motivation": "Current LLM agents lack environment-specific experiences and continual learning during inference, limiting their performance in sequential decision-making tasks.", "method": "Proposes Contextual Experience Replay (CER), a training-free framework that accumulates and synthesizes past experiences into a dynamic memory buffer for retrieval in new tasks.", "result": "CER achieves 31.9% on VisualWebArena and 36.7% on WebArena, improving GPT-4o baseline by 51.0%.", "conclusion": "CER effectively enhances LLM agents' adaptability and performance in complex environments through experience replay."}}
{"id": "2506.07813", "pdf": "https://arxiv.org/pdf/2506.07813", "abs": "https://arxiv.org/abs/2506.07813", "authors": ["Junseo Bang", "Joonhee Lee", "Kyeonghyun Lee", "Haechang Lee", "Dong Un Kang", "Se Young Chun"], "title": "Self-Cascaded Diffusion Models for Arbitrary-Scale Image Super-Resolution", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Arbitrary-scale image super-resolution aims to upsample images to any desired\nresolution, offering greater flexibility than traditional fixed-scale\nsuper-resolution. Recent approaches in this domain utilize regression-based or\ngenerative models, but many of them are a single-stage upsampling process,\nwhich may be challenging to learn across a wide, continuous distribution of\nscaling factors. Progressive upsampling strategies have shown promise in\nmitigating this issue, yet their integration with diffusion models for flexible\nupscaling remains underexplored. Here, we present CasArbi, a novel\nself-cascaded diffusion framework for arbitrary-scale image super-resolution.\nCasArbi meets the varying scaling demands by breaking them down into smaller\nsequential factors and progressively enhancing the image resolution at each\nstep with seamless transitions for arbitrary scales. Our novel\ncoordinate-guided residual diffusion model allows for the learning of\ncontinuous image representations while enabling efficient diffusion sampling.\nExtensive experiments demonstrate that our CasArbi outperforms prior arts in\nboth perceptual and distortion performance metrics across diverse\narbitrary-scale super-resolution benchmarks.", "AI": {"tldr": "CasArbi is a self-cascaded diffusion framework for arbitrary-scale image super-resolution, outperforming prior methods by breaking scaling into smaller steps and using a coordinate-guided residual diffusion model.", "motivation": "Traditional fixed-scale super-resolution lacks flexibility, and single-stage upsampling struggles with continuous scaling factors. Progressive upsampling with diffusion models is underexplored.", "method": "CasArbi uses a self-cascaded diffusion framework, breaking scaling into sequential steps with a coordinate-guided residual diffusion model for continuous image representation.", "result": "CasArbi achieves superior perceptual and distortion performance across arbitrary-scale super-resolution benchmarks.", "conclusion": "CasArbi effectively addresses the challenges of arbitrary-scale super-resolution, offering a flexible and high-performance solution."}}
{"id": "2506.06699", "pdf": "https://arxiv.org/pdf/2506.06699", "abs": "https://arxiv.org/abs/2506.06699", "authors": ["Rajeev Bhatt Ambati", "James Lester", "Shashank Srivastava", "Snigdha Chaturvedi"], "title": "MarginSel : Max-Margin Demonstration Selection for LLMs", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) excel at few-shot learning via in-context\nlearning (ICL). However, the effectiveness of ICL is often sensitive to the\nselection and ordering of demonstration examples. To address this, we present\nMarginSel: Max-Margin Demonstration Selection for LLMs, a two-step method that\nselects hard demonstration examples for the ICL prompt, adapting to each test\ninstance. Our approach achieves 2-7% absolute improvement in F1-score across\nclassification tasks, compared to a random selection of examples. We also\nprovide theoretical insights and empirical evidence showing that MarginSel\ninduces max-margin behavior in LLMs by effectively increasing the margin for\nhard examples, analogous to support vectors, thereby shifting the decision\nboundary in a beneficial direction.", "AI": {"tldr": "MarginSel improves ICL in LLMs by selecting hard examples, boosting F1-scores by 2-7%.", "motivation": "ICL effectiveness depends on example selection and order; MarginSel addresses this by adapting to test instances.", "method": "Two-step method (MarginSel) selects hard examples for ICL prompts, enhancing model performance.", "result": "Achieves 2-7% F1-score improvement over random selection in classification tasks.", "conclusion": "MarginSel induces max-margin behavior, shifting decision boundaries beneficially for hard examples."}}
{"id": "2506.07814", "pdf": "https://arxiv.org/pdf/2506.07814", "abs": "https://arxiv.org/abs/2506.07814", "authors": ["Yongzhen Wang", "Yongjun Li", "Zhuoran Zheng", "Xiao-Ping Zhang", "Mingqiang Wei"], "title": "M2Restore: Mixture-of-Experts-based Mamba-CNN Fusion Framework for All-in-One Image Restoration", "categories": ["cs.CV"], "comment": "13 pages, 8 figures, 3 tables", "summary": "Natural images are often degraded by complex, composite degradations such as\nrain, snow, and haze, which adversely impact downstream vision applications.\nWhile existing image restoration efforts have achieved notable success, they\nare still hindered by two critical challenges: limited generalization across\ndynamically varying degradation scenarios and a suboptimal balance between\npreserving local details and modeling global dependencies. To overcome these\nchallenges, we propose M2Restore, a novel Mixture-of-Experts (MoE)-based\nMamba-CNN fusion framework for efficient and robust all-in-one image\nrestoration. M2Restore introduces three key contributions: First, to boost the\nmodel's generalization across diverse degradation conditions, we exploit a\nCLIP-guided MoE gating mechanism that fuses task-conditioned prompts with\nCLIP-derived semantic priors. This mechanism is further refined via cross-modal\nfeature calibration, which enables precise expert selection for various\ndegradation types. Second, to jointly capture global contextual dependencies\nand fine-grained local details, we design a dual-stream architecture that\nintegrates the localized representational strength of CNNs with the long-range\nmodeling efficiency of Mamba. This integration enables collaborative\noptimization of global semantic relationships and local structural fidelity,\npreserving global coherence while enhancing detail restoration. Third, we\nintroduce an edge-aware dynamic gating mechanism that adaptively balances\nglobal modeling and local enhancement by reallocating computational attention\nto degradation-sensitive regions. This targeted focus leads to more efficient\nand precise restoration. Extensive experiments across multiple image\nrestoration benchmarks validate the superiority of M2Restore in both visual\nquality and quantitative performance.", "AI": {"tldr": "M2Restore is a novel Mixture-of-Experts-based Mamba-CNN fusion framework for robust all-in-one image restoration, addressing generalization and detail preservation challenges.", "motivation": "Existing image restoration methods struggle with generalization across varying degradations and balancing local details with global dependencies.", "method": "M2Restore uses a CLIP-guided MoE gating mechanism, a dual-stream CNN-Mamba architecture, and an edge-aware dynamic gating mechanism.", "result": "Extensive experiments show M2Restore outperforms benchmarks in visual quality and quantitative performance.", "conclusion": "M2Restore effectively addresses key challenges in image restoration, offering superior performance and adaptability."}}
{"id": "2506.07826", "pdf": "https://arxiv.org/pdf/2506.07826", "abs": "https://arxiv.org/abs/2506.07826", "authors": ["William Ljungbergh", "Bernardo Taveira", "Wenzhao Zheng", "Adam Tonderski", "Chensheng Peng", "Fredrik Kahl", "Christoffer Petersson", "Michael Felsberg", "Kurt Keutzer", "Masayoshi Tomizuka", "Wei Zhan"], "title": "R3D2: Realistic 3D Asset Insertion via Diffusion for Autonomous Driving Simulation", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": null, "summary": "Validating autonomous driving (AD) systems requires diverse and\nsafety-critical testing, making photorealistic virtual environments essential.\nTraditional simulation platforms, while controllable, are resource-intensive to\nscale and often suffer from a domain gap with real-world data. In contrast,\nneural reconstruction methods like 3D Gaussian Splatting (3DGS) offer a\nscalable solution for creating photorealistic digital twins of real-world\ndriving scenes. However, they struggle with dynamic object manipulation and\nreusability as their per-scene optimization-based methodology tends to result\nin incomplete object models with integrated illumination effects. This paper\nintroduces R3D2, a lightweight, one-step diffusion model designed to overcome\nthese limitations and enable realistic insertion of complete 3D assets into\nexisting scenes by generating plausible rendering effects-such as shadows and\nconsistent lighting-in real time. This is achieved by training R3D2 on a novel\ndataset: 3DGS object assets are generated from in-the-wild AD data using an\nimage-conditioned 3D generative model, and then synthetically placed into\nneural rendering-based virtual environments, allowing R3D2 to learn realistic\nintegration. Quantitative and qualitative evaluations demonstrate that R3D2\nsignificantly enhances the realism of inserted assets, enabling use-cases like\ntext-to-3D asset insertion and cross-scene/dataset object transfer, allowing\nfor true scalability in AD validation. To promote further research in scalable\nand realistic AD simulation, we will release our dataset and code, see\nhttps://research.zenseact.com/publications/R3D2/.", "AI": {"tldr": "R3D2 is a lightweight diffusion model for realistic 3D asset insertion in autonomous driving simulations, overcoming limitations of neural reconstruction methods like 3DGS.", "motivation": "Traditional simulation platforms are resource-intensive and suffer from domain gaps, while neural reconstruction methods lack dynamic object manipulation and reusability.", "method": "R3D2 is trained on a novel dataset of 3DGS object assets from AD data, enabling realistic rendering effects like shadows and lighting in real time.", "result": "R3D2 enhances realism of inserted assets, enabling text-to-3D insertion and cross-scene object transfer for scalable AD validation.", "conclusion": "R3D2 advances photorealistic simulation for AD validation, with dataset and code released to promote further research."}}
{"id": "2506.06832", "pdf": "https://arxiv.org/pdf/2506.06832", "abs": "https://arxiv.org/abs/2506.06832", "authors": ["Cl\u00e9ment Hongler", "Andrew Emil"], "title": "Cross-Entropy Games for Language Models: From Implicit Knowledge to General Capability Measures", "categories": ["cs.AI", "cs.CL", "cs.GT", "cs.IT", "cs.NE", "math.IT"], "comment": "41 pages, 16 figures", "summary": "Large Language Models (LLMs) define probability measures on text. By\nconsidering the implicit knowledge question of what it means for an LLM to know\nsuch a measure and what it entails algorithmically, we are naturally led to\nformulate a series of tasks that go beyond generative sampling, involving forms\nof summarization, counterfactual thinking, anomaly detection, originality\nsearch, reverse prompting, debating, creative solving, etc. These tasks can be\nformulated as games based on LLM measures, which we call Cross-Entropy (Xent)\nGames. Xent Games can be single-player or multi-player. They involve\ncross-entropy scores and cross-entropy constraints, and can be expressed as\nsimple computational graphs and programs. We show the Xent Game space is large\nenough to contain a wealth of interesting examples, while being constructible\nfrom basic game-theoretic consistency axioms. We then discuss how the Xent Game\nspace can be used to measure the abilities of LLMs. This leads to the\nconstruction of Xent Game measures: finite families of Xent Games that can be\nused as capability benchmarks, built from a given scope, by extracting a\ncovering measure. To address the unbounded scope problem associated with the\nchallenge of measuring general abilities, we propose to explore the space of\nXent Games in a coherent fashion, using ideas inspired by evolutionary\ndynamics.", "AI": {"tldr": "The paper introduces Cross-Entropy (Xent) Games, tasks based on LLM probability measures, to benchmark LLM capabilities through game-theoretic axioms and evolutionary dynamics.", "motivation": "To explore what it means for LLMs to know probability measures on text and extend their applications beyond generative sampling.", "method": "Formulate Xent Games involving cross-entropy scores and constraints, expressed as computational graphs, and construct benchmarks from these games.", "result": "The Xent Game space is rich and constructible, enabling the creation of capability benchmarks for LLMs.", "conclusion": "Xent Games provide a framework for measuring LLM abilities, addressing scope challenges through evolutionary dynamics."}}
{"id": "2506.07841", "pdf": "https://arxiv.org/pdf/2506.07841", "abs": "https://arxiv.org/abs/2506.07841", "authors": ["Elizabeth Pavlova", "Xue-Xin Wei"], "title": "Diffusion models under low-noise regime", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent work on diffusion models proposed that they operate in two regimes:\nmemorization, in which models reproduce their training data, and\ngeneralization, in which they generate novel samples. While this has been\ntested in high-noise settings, the behavior of diffusion models as effective\ndenoisers when the corruption level is small remains unclear. To address this\ngap, we systematically investigated the behavior of diffusion models under\nlow-noise diffusion dynamics, with implications for model robustness and\ninterpretability. Using (i) CelebA subsets of varying sample sizes and (ii)\nanalytic Gaussian mixture benchmarks, we reveal that models trained on disjoint\ndata diverge near the data manifold even when their high-noise outputs\nconverge. We quantify how training set size, data geometry, and model objective\nchoice shape denoising trajectories and affect score accuracy, providing\ninsights into how these models actually learn representations of data\ndistributions. This work starts to address gaps in our understanding of\ngenerative model reliability in practical applications where small\nperturbations are common.", "AI": {"tldr": "Diffusion models operate in memorization and generalization regimes. This study investigates their behavior under low-noise conditions, revealing divergence near the data manifold despite high-noise convergence.", "motivation": "To clarify diffusion models' behavior as denoisers under low-noise conditions, addressing gaps in understanding their robustness and interpretability.", "method": "Systematic investigation using CelebA subsets and Gaussian mixture benchmarks to analyze denoising trajectories, score accuracy, and the impact of training set size and data geometry.", "result": "Models trained on disjoint data diverge near the data manifold under low noise, even if high-noise outputs converge. Training set size, data geometry, and model objectives shape denoising behavior.", "conclusion": "The study provides insights into how diffusion models learn data distributions, highlighting implications for reliability in practical applications with small perturbations."}}
{"id": "2506.06905", "pdf": "https://arxiv.org/pdf/2506.06905", "abs": "https://arxiv.org/abs/2506.06905", "authors": ["Akash Gupta", "Amos Storkey", "Mirella Lapata"], "title": "Meta-Adaptive Prompt Distillation for Few-Shot Visual Question Answering", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Large Multimodal Models (LMMs) often rely on in-context learning (ICL) to\nperform new tasks with minimal supervision. However, ICL performance,\nespecially in smaller LMMs, is inconsistent and does not always improve\nmonotonically with increasing examples. We hypothesize that this occurs due to\nthe LMM being overwhelmed by additional information present in the image\nembeddings, which is not required for the downstream task. To address this, we\npropose a meta-learning approach that provides an alternative for inducing\nfew-shot capabilities in LMMs, using a fixed set of soft prompts that are\ndistilled from task-relevant image features and can be adapted at test time\nusing a few examples. To facilitate this distillation, we introduce an\nattention-mapper module that can be easily integrated with the popular LLaVA\nv1.5 architecture and is jointly learned with soft prompts, enabling task\nadaptation in LMMs under low-data regimes with just a few gradient steps.\nEvaluation on the VL-ICL Bench shows that our method consistently outperforms\nICL and related prompt-tuning approaches, even under image perturbations,\nimproving task induction and reasoning across visual question answering tasks.", "AI": {"tldr": "A meta-learning approach improves few-shot learning in Large Multimodal Models (LMMs) by distilling task-relevant image features into soft prompts, outperforming in-context learning (ICL) and prompt-tuning methods.", "motivation": "Inconsistent ICL performance in smaller LMMs due to irrelevant image embedding information.", "method": "Proposes a meta-learning approach with soft prompts distilled from task-relevant image features, integrated via an attention-mapper module.", "result": "Outperforms ICL and prompt-tuning on VL-ICL Bench, even under image perturbations.", "conclusion": "The method enhances task induction and reasoning in LMMs under low-data regimes."}}
{"id": "2506.07847", "pdf": "https://arxiv.org/pdf/2506.07847", "abs": "https://arxiv.org/abs/2506.07847", "authors": ["Hengzhi Chen", "Liqian Feng", "Wenhua Wu", "Xiaogang Zhu", "Shawn Leo", "Kun Hu"], "title": "F2Net: A Frequency-Fused Network for Ultra-High Resolution Remote Sensing Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Semantic segmentation of ultra-high-resolution (UHR) remote sensing imagery\nis critical for applications like environmental monitoring and urban planning\nbut faces computational and optimization challenges. Conventional methods\neither lose fine details through downsampling or fragment global context via\npatch processing. While multi-branch networks address this trade-off, they\nsuffer from computational inefficiency and conflicting gradient dynamics during\ntraining. We propose F2Net, a frequency-aware framework that decomposes UHR\nimages into high- and low-frequency components for specialized processing. The\nhigh-frequency branch preserves full-resolution structural details, while the\nlow-frequency branch processes downsampled inputs through dual sub-branches\ncapturing short- and long-range dependencies. A Hybrid-Frequency Fusion module\nintegrates these observations, guided by two novel objectives: Cross-Frequency\nAlignment Loss ensures semantic consistency between frequency components, and\nCross-Frequency Balance Loss regulates gradient magnitudes across branches to\nstabilize training. Evaluated on DeepGlobe and Inria Aerial benchmarks, F2Net\nachieves state-of-the-art performance with mIoU of 80.22 and 83.39,\nrespectively. Our code will be publicly available.", "AI": {"tldr": "F2Net is a frequency-aware framework for semantic segmentation of ultra-high-resolution remote sensing images, addressing computational and optimization challenges by decomposing images into high- and low-frequency components for specialized processing.", "motivation": "Semantic segmentation of ultra-high-resolution (UHR) remote sensing imagery is critical for applications like environmental monitoring and urban planning, but conventional methods lose fine details or fragment global context. Multi-branch networks also face inefficiency and training instability.", "method": "F2Net decomposes UHR images into high- and low-frequency components. The high-frequency branch preserves full-resolution details, while the low-frequency branch processes downsampled inputs with dual sub-branches for short- and long-range dependencies. A Hybrid-Frequency Fusion module integrates these, guided by Cross-Frequency Alignment Loss and Cross-Frequency Balance Loss.", "result": "F2Net achieves state-of-the-art performance with mIoU of 80.22 on DeepGlobe and 83.39 on Inria Aerial benchmarks.", "conclusion": "F2Net effectively balances detail preservation and computational efficiency, offering a robust solution for UHR image segmentation."}}
{"id": "2506.06941", "pdf": "https://arxiv.org/pdf/2506.06941", "abs": "https://arxiv.org/abs/2506.06941", "authors": ["Parshin Shojaee", "Iman Mirzadeh", "Keivan Alizadeh", "Maxwell Horton", "Samy Bengio", "Mehrdad Farajtabar"], "title": "The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "preprint", "summary": "Recent generations of language models have introduced Large Reasoning Models\n(LRMs) that generate detailed thinking processes before providing answers.\nWhile these models demonstrate improved performance on reasoning benchmarks,\ntheir fundamental capabilities, scaling properties, and limitations remain\ninsufficiently understood. Current evaluations primarily focus on established\nmath and coding benchmarks, emphasizing final answer accuracy. However, this\nevaluation paradigm often suffers from contamination and does not provide\ninsights into the reasoning traces. In this work, we systematically investigate\nthese gaps with the help of controllable puzzle environments that allow precise\nmanipulation of complexity while maintaining consistent logical structures.\nThis setup enables the analysis of not only final answers but also the internal\nreasoning traces, offering insights into how LRMs think. Through extensive\nexperiments, we show that LRMs face a complete accuracy collapse beyond certain\ncomplexities. Moreover, they exhibit a counterintuitive scaling limit: their\nreasoning effort increases with problem complexity up to a point, then declines\ndespite having remaining token budget. By comparing LRMs with their standard\nLLM counterparts under same inference compute, we identify three performance\nregimes: (1) low-complexity tasks where standard models outperform LRMs, (2)\nmedium-complexity tasks where LRMs demonstrates advantage, and (3)\nhigh-complexity tasks where both models face complete collapse. We found that\nLRMs have limitations in exact computation: they fail to use explicit\nalgorithms and reason inconsistently across scales. We also investigate the\nreasoning traces in more depth, studying the patterns of explored solutions and\nanalyzing the models' computational behavior, shedding light on their\nstrengths, limitations, and raising questions about their reasoning\ncapabilities.", "AI": {"tldr": "The paper investigates the capabilities and limitations of Large Reasoning Models (LRMs) using controllable puzzle environments, revealing their performance collapse at high complexities and inconsistent reasoning patterns.", "motivation": "To address the insufficient understanding of LRMs' fundamental capabilities, scaling properties, and limitations, especially in reasoning traces beyond final answer accuracy.", "method": "Systematic investigation using controllable puzzle environments to manipulate complexity while analyzing both final answers and internal reasoning traces.", "result": "LRMs show accuracy collapse beyond certain complexities, exhibit a scaling limit in reasoning effort, and perform inconsistently across task complexities compared to standard LLMs.", "conclusion": "LRMs have limitations in exact computation and inconsistent reasoning, raising questions about their capabilities despite their advantages in medium-complexity tasks."}}
{"id": "2506.07848", "pdf": "https://arxiv.org/pdf/2506.07848", "abs": "https://arxiv.org/abs/2506.07848", "authors": ["Teng Hu", "Zhentao Yu", "Zhengguang Zhou", "Jiangning Zhang", "Yuan Zhou", "Qinglin Lu", "Ran Yi"], "title": "PolyVivid: Vivid Multi-Subject Video Generation with Cross-Modal Interaction and Enhancement", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Despite recent advances in video generation, existing models still lack\nfine-grained controllability, especially for multi-subject customization with\nconsistent identity and interaction. In this paper, we propose PolyVivid, a\nmulti-subject video customization framework that enables flexible and\nidentity-consistent generation. To establish accurate correspondences between\nsubject images and textual entities, we design a VLLM-based text-image fusion\nmodule that embeds visual identities into the textual space for precise\ngrounding. To further enhance identity preservation and subject interaction, we\npropose a 3D-RoPE-based enhancement module that enables structured\nbidirectional fusion between text and image embeddings. Moreover, we develop an\nattention-inherited identity injection module to effectively inject fused\nidentity features into the video generation process, mitigating identity drift.\nFinally, we construct an MLLM-based data pipeline that combines MLLM-based\ngrounding, segmentation, and a clique-based subject consolidation strategy to\nproduce high-quality multi-subject data, effectively enhancing subject\ndistinction and reducing ambiguity in downstream video generation. Extensive\nexperiments demonstrate that PolyVivid achieves superior performance in\nidentity fidelity, video realism, and subject alignment, outperforming existing\nopen-source and commercial baselines.", "AI": {"tldr": "PolyVivid is a framework for multi-subject video customization, ensuring identity consistency and interaction through VLLM-based text-image fusion, 3D-RoPE enhancement, and attention-inherited identity injection.", "motivation": "Existing video generation models lack fine-grained controllability for multi-subject customization with consistent identity and interaction.", "method": "PolyVivid uses a VLLM-based text-image fusion module, 3D-RoPE enhancement, attention-inherited identity injection, and an MLLM-based data pipeline for high-quality multi-subject data.", "result": "PolyVivid outperforms baselines in identity fidelity, video realism, and subject alignment.", "conclusion": "PolyVivid advances multi-subject video generation with superior controllability and identity consistency."}}
{"id": "2506.06975", "pdf": "https://arxiv.org/pdf/2506.06975", "abs": "https://arxiv.org/abs/2506.06975", "authors": ["Xiaoyuan Zhu", "Yaowen Ye", "Tianyi Qiu", "Hanlin Zhu", "Sijun Tan", "Ajraf Mannan", "Jonathan Michala", "Raluca Ada Popa", "Willie Neiswanger"], "title": "Auditing Black-Box LLM APIs with a Rank-Based Uniformity Test", "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": null, "summary": "As API access becomes a primary interface to large language models (LLMs),\nusers often interact with black-box systems that offer little transparency into\nthe deployed model. To reduce costs or maliciously alter model behaviors, API\nproviders may discreetly serve quantized or fine-tuned variants, which can\ndegrade performance and compromise safety. Detecting such substitutions is\ndifficult, as users lack access to model weights and, in most cases, even\noutput logits. To tackle this problem, we propose a rank-based uniformity test\nthat can verify the behavioral equality of a black-box LLM to a locally\ndeployed authentic model. Our method is accurate, query-efficient, and avoids\ndetectable query patterns, making it robust to adversarial providers that\nreroute or mix responses upon the detection of testing attempts. We evaluate\nthe approach across diverse threat scenarios, including quantization, harmful\nfine-tuning, jailbreak prompts, and full model substitution, showing that it\nconsistently achieves superior statistical power over prior methods under\nconstrained query budgets.", "AI": {"tldr": "Proposes a rank-based uniformity test to detect substitutions in black-box LLM APIs, ensuring model authenticity without access to weights or logits.", "motivation": "API providers may covertly serve altered LLM variants (e.g., quantized or fine-tuned), degrading performance and safety, necessitating a detection method.", "method": "Uses a rank-based uniformity test to compare black-box LLM behavior to a locally deployed authentic model, ensuring query efficiency and robustness.", "result": "The method outperforms prior approaches in detecting threats like quantization, harmful fine-tuning, and model substitution under limited queries.", "conclusion": "The proposed test effectively verifies LLM authenticity, offering accuracy and resilience against adversarial API providers."}}
{"id": "2506.07850", "pdf": "https://arxiv.org/pdf/2506.07850", "abs": "https://arxiv.org/abs/2506.07850", "authors": ["Arash Rocky", "Q. M. Jonathan Wu"], "title": "SAM2Auto: Auto Annotation Using FLASH", "categories": ["cs.CV"], "comment": null, "summary": "Vision-Language Models (VLMs) lag behind Large Language Models due to the\nscarcity of annotated datasets, as creating paired visual-textual annotations\nis labor-intensive and expensive. To address this bottleneck, we introduce\nSAM2Auto, the first fully automated annotation pipeline for video datasets\nrequiring no human intervention or dataset-specific training. Our approach\nconsists of two key components: SMART-OD, a robust object detection system that\ncombines automatic mask generation with open-world object detection\ncapabilities, and FLASH (Frame-Level Annotation and Segmentation Handler), a\nmulti-object real-time video instance segmentation (VIS) that maintains\nconsistent object identification across video frames even with intermittent\ndetection gaps. Unlike existing open-world detection methods that require\nframe-specific hyperparameter tuning and suffer from numerous false positives,\nour system employs statistical approaches to minimize detection errors while\nensuring consistent object tracking throughout entire video sequences.\nExtensive experimental validation demonstrates that SAM2Auto achieves\ncomparable accuracy to manual annotation while dramatically reducing annotation\ntime and eliminating labor costs. The system successfully handles diverse\ndatasets without requiring retraining or extensive parameter adjustments,\nmaking it a practical solution for large-scale dataset creation. Our work\nestablishes a new baseline for automated video annotation and provides a\npathway for accelerating VLM development by addressing the fundamental dataset\nbottleneck that has constrained progress in vision-language understanding.", "AI": {"tldr": "SAM2Auto is a fully automated pipeline for video dataset annotation, combining SMART-OD for object detection and FLASH for consistent video instance segmentation, achieving accuracy comparable to manual annotation while reducing time and costs.", "motivation": "The scarcity of annotated datasets for Vision-Language Models (VLMs) due to labor-intensive and expensive manual annotation processes.", "method": "SAM2Auto uses SMART-OD for robust object detection and FLASH for real-time video instance segmentation, employing statistical approaches to minimize errors and ensure consistent tracking.", "result": "SAM2Auto achieves accuracy comparable to manual annotation, reduces annotation time, and eliminates labor costs, handling diverse datasets without retraining.", "conclusion": "SAM2Auto sets a new baseline for automated video annotation, addressing the dataset bottleneck in VLM development."}}
{"id": "2506.07031", "pdf": "https://arxiv.org/pdf/2506.07031", "abs": "https://arxiv.org/abs/2506.07031", "authors": ["Jingyuan Ma", "Rui Li", "Zheng Li", "Junfeng Liu", "Lei Sha", "Zhifang Sui"], "title": "HauntAttack: When Attack Follows Reasoning as a Shadow", "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": null, "summary": "Emerging Large Reasoning Models (LRMs) consistently excel in mathematical and\nreasoning tasks, showcasing exceptional capabilities. However, the enhancement\nof reasoning abilities and the exposure of their internal reasoning processes\nintroduce new safety vulnerabilities. One intriguing concern is: when reasoning\nis strongly entangled with harmfulness, what safety-reasoning trade-off do LRMs\nexhibit? To address this issue, we introduce HauntAttack, a novel and\ngeneral-purpose black-box attack framework that systematically embeds harmful\ninstructions into reasoning questions. Specifically, we treat reasoning\nquestions as carriers and substitute one of their original conditions with a\nharmful instruction. This process creates a reasoning pathway in which the\nmodel is guided step by step toward generating unsafe outputs. Based on\nHauntAttack, we conduct comprehensive experiments on multiple LRMs. Our results\nreveal that even the most advanced LRMs exhibit significant safety\nvulnerabilities. Additionally, we perform a detailed analysis of different\nmodels, various types of harmful instructions, and model output patterns,\nproviding valuable insights into the security of LRMs.", "AI": {"tldr": "HauntAttack is a black-box attack framework that embeds harmful instructions into reasoning questions, revealing safety vulnerabilities in advanced Large Reasoning Models (LRMs).", "motivation": "To explore the safety-reasoning trade-off in LRMs when reasoning is entangled with harmfulness.", "method": "Introduces HauntAttack, which substitutes original conditions in reasoning questions with harmful instructions to guide models toward unsafe outputs.", "result": "Advanced LRMs exhibit significant safety vulnerabilities under HauntAttack.", "conclusion": "The study highlights critical security risks in LRMs and provides insights for improving their safety."}}
{"id": "2506.07857", "pdf": "https://arxiv.org/pdf/2506.07857", "abs": "https://arxiv.org/abs/2506.07857", "authors": ["Zihui Zhang", "Weisheng Dai", "Hongtao Wen", "Bo Yang"], "title": "LogoSP: Local-global Grouping of Superpoints for Unsupervised Semantic Segmentation of 3D Point Clouds", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "CVPR 2025. Code and data are available at:\n  https://github.com/vLAR-group/LogoSP", "summary": "We study the problem of unsupervised 3D semantic segmentation on raw point\nclouds without needing human labels in training. Existing methods usually\nformulate this problem into learning per-point local features followed by a\nsimple grouping strategy, lacking the ability to discover additional and\npossibly richer semantic priors beyond local features. In this paper, we\nintroduce LogoSP to learn 3D semantics from both local and global point\nfeatures. The key to our approach is to discover 3D semantic information by\ngrouping superpoints according to their global patterns in the frequency\ndomain, thus generating highly accurate semantic pseudo-labels for training a\nsegmentation network. Extensive experiments on two indoor and an outdoor\ndatasets show that our LogoSP surpasses all existing unsupervised methods by\nlarge margins, achieving the state-of-the-art performance for unsupervised 3D\nsemantic segmentation. Notably, our investigation into the learned global\npatterns reveals that they truly represent meaningful 3D semantics in the\nabsence of human labels during training.", "AI": {"tldr": "LogoSP introduces an unsupervised 3D semantic segmentation method using local and global point features, outperforming existing methods by leveraging frequency-domain patterns for pseudo-label generation.", "motivation": "Existing unsupervised methods rely on local features and simple grouping, missing richer semantic priors. LogoSP aims to discover 3D semantics from both local and global features.", "method": "LogoSP groups superpoints based on global patterns in the frequency domain to generate accurate pseudo-labels for training a segmentation network.", "result": "LogoSP achieves state-of-the-art performance on indoor and outdoor datasets, surpassing existing unsupervised methods by large margins.", "conclusion": "The learned global patterns in LogoSP effectively represent meaningful 3D semantics without human labels, demonstrating its superiority in unsupervised 3D semantic segmentation."}}
{"id": "2506.07860", "pdf": "https://arxiv.org/pdf/2506.07860", "abs": "https://arxiv.org/abs/2506.07860", "authors": ["Ivan Alberico", "Marco Cannici", "Giovanni Cioffi", "Davide Scaramuzza"], "title": "Egocentric Event-Based Vision for Ping Pong Ball Trajectory Prediction", "categories": ["cs.CV"], "comment": "IEEE/CVF Conference on Computer Vision and Pattern Recognition\n  Workshops (CVPRW), Nashville (TN), USA, 2025; 5th International Workshop on\n  Event-Based Vision", "summary": "In this paper, we present a real-time egocentric trajectory prediction system\nfor table tennis using event cameras. Unlike standard cameras, which suffer\nfrom high latency and motion blur at fast ball speeds, event cameras provide\nhigher temporal resolution, allowing more frequent state updates, greater\nrobustness to outliers, and accurate trajectory predictions using just a short\ntime window after the opponent's impact. We collect a dataset of ping-pong game\nsequences, including 3D ground-truth trajectories of the ball, synchronized\nwith sensor data from the Meta Project Aria glasses and event streams. Our\nsystem leverages foveated vision, using eye-gaze data from the glasses to\nprocess only events in the viewer's fovea. This biologically inspired approach\nimproves ball detection performance and significantly reduces computational\nlatency, as it efficiently allocates resources to the most perceptually\nrelevant regions, achieving a reduction factor of 10.81 on the collected\ntrajectories. Our detection pipeline has a worst-case total latency of 4.5 ms,\nincluding computation and perception - significantly lower than a frame-based\n30 FPS system, which, in the worst case, takes 66 ms solely for perception.\nFinally, we fit a trajectory prediction model to the estimated states of the\nball, enabling 3D trajectory forecasting in the future. To the best of our\nknowledge, this is the first approach to predict table tennis trajectories from\nan egocentric perspective using event cameras.", "AI": {"tldr": "A real-time egocentric trajectory prediction system for table tennis using event cameras, leveraging foveated vision for low latency and high accuracy.", "motivation": "Standard cameras struggle with high latency and motion blur in fast-paced table tennis, while event cameras offer higher temporal resolution and robustness.", "method": "Uses event cameras and foveated vision (via eye-gaze data) to process relevant regions, reducing latency. Collects a dataset with 3D ground-truth trajectories and fits a prediction model.", "result": "Achieves a 10.81x reduction in computational latency, with a worst-case total latency of 4.5 ms, outperforming 30 FPS frame-based systems.", "conclusion": "First egocentric trajectory prediction system for table tennis using event cameras, demonstrating significant improvements in speed and accuracy."}}
{"id": "2506.07863", "pdf": "https://arxiv.org/pdf/2506.07863", "abs": "https://arxiv.org/abs/2506.07863", "authors": ["Lev Novitskiy", "Viacheslav Vasilev", "Maria Kovaleva", "Vladimir Arkhipkin", "Denis Dimitrov"], "title": "VIVAT: Virtuous Improving VAE Training through Artifact Mitigation", "categories": ["cs.CV", "cs.LG", "cs.MM"], "comment": null, "summary": "Variational Autoencoders (VAEs) remain a cornerstone of generative computer\nvision, yet their training is often plagued by artifacts that degrade\nreconstruction and generation quality. This paper introduces VIVAT, a\nsystematic approach to mitigating common artifacts in KL-VAE training without\nrequiring radical architectural changes. We present a detailed taxonomy of five\nprevalent artifacts - color shift, grid patterns, blur, corner and droplet\nartifacts - and analyze their root causes. Through straightforward\nmodifications, including adjustments to loss weights, padding strategies, and\nthe integration of Spatially Conditional Normalization, we demonstrate\nsignificant improvements in VAE performance. Our method achieves\nstate-of-the-art results in image reconstruction metrics (PSNR and SSIM) across\nmultiple benchmarks and enhances text-to-image generation quality, as evidenced\nby superior CLIP scores. By preserving the simplicity of the KL-VAE framework\nwhile addressing its practical challenges, VIVAT offers actionable insights for\nresearchers and practitioners aiming to optimize VAE training.", "AI": {"tldr": "VIVAT improves KL-VAE training by addressing common artifacts (color shift, grid patterns, blur, corner, droplet) with simple modifications, achieving top results in reconstruction and generation.", "motivation": "KL-VAEs suffer from artifacts that degrade performance; VIVAT aims to fix these without major architectural changes.", "method": "Modifies loss weights, padding strategies, and integrates Spatially Conditional Normalization.", "result": "State-of-the-art PSNR, SSIM, and CLIP scores in benchmarks.", "conclusion": "VIVAT effectively optimizes VAE training while maintaining simplicity, offering practical solutions."}}
{"id": "2506.07168", "pdf": "https://arxiv.org/pdf/2506.07168", "abs": "https://arxiv.org/abs/2506.07168", "authors": ["Huanyi Xie", "Lijie Hu", "Lu Yu", "Tianhao Huang", "Longfei Li", "Meng Li", "Jun Zhou", "Huan Wang", "Di Wang"], "title": "Efficient Text-Attributed Graph Learning through Selective Annotation and Graph Alignment", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "23 pages", "summary": "In the realm of Text-attributed Graphs (TAGs), traditional graph neural\nnetworks (GNNs) often fall short due to the complex textual information\nassociated with each node. Recent methods have improved node representations by\nleveraging large language models (LLMs) to enhance node text features, but\nthese approaches typically require extensive annotations or fine-tuning across\nall nodes, which is both time-consuming and costly. To overcome these\nchallenges, we introduce GAGA, an efficient framework for TAG representation\nlearning. GAGA reduces annotation time and cost by focusing on annotating only\nrepresentative nodes and edges. It constructs an annotation graph that captures\nthe topological relationships among these annotations. Furthermore, GAGA\nemploys a two-level alignment module to effectively integrate the annotation\ngraph with the TAG, aligning their underlying structures. Experiments show that\nGAGA achieves classification accuracies on par with or surpassing\nstate-of-the-art methods while requiring only 1% of the data to be annotated,\ndemonstrating its high efficiency.", "AI": {"tldr": "GAGA is an efficient framework for TAG representation learning, reducing annotation costs by focusing on representative nodes and edges, and achieves competitive accuracy with minimal data.", "motivation": "Traditional GNNs struggle with TAGs due to complex textual data, and existing LLM-based methods require costly annotations. GAGA aims to address these inefficiencies.", "method": "GAGA annotates only representative nodes/edges, constructs an annotation graph, and uses a two-level alignment module to integrate it with the TAG.", "result": "GAGA matches or outperforms state-of-the-art methods with just 1% of annotated data.", "conclusion": "GAGA offers a cost-effective and efficient solution for TAG representation learning, reducing annotation burdens while maintaining high accuracy."}}
{"id": "2506.07865", "pdf": "https://arxiv.org/pdf/2506.07865", "abs": "https://arxiv.org/abs/2506.07865", "authors": ["Jinxi Li", "Ziyang Song", "Siyuan Zhou", "Bo Yang"], "title": "FreeGave: 3D Physics Learning from Dynamic Videos by Gaussian Velocity", "categories": ["cs.CV", "cs.AI", "cs.CE", "cs.LG", "cs.RO"], "comment": "CVPR 2025. Code and data are available at:\n  https://github.com/vLAR-group/FreeGave", "summary": "In this paper, we aim to model 3D scene geometry, appearance, and the\nunderlying physics purely from multi-view videos. By applying various governing\nPDEs as PINN losses or incorporating physics simulation into neural networks,\nexisting works often fail to learn complex physical motions at boundaries or\nrequire object priors such as masks or types. In this paper, we propose\nFreeGave to learn the physics of complex dynamic 3D scenes without needing any\nobject priors. The key to our approach is to introduce a physics code followed\nby a carefully designed divergence-free module for estimating a per-Gaussian\nvelocity field, without relying on the inefficient PINN losses. Extensive\nexperiments on three public datasets and a newly collected challenging\nreal-world dataset demonstrate the superior performance of our method for\nfuture frame extrapolation and motion segmentation. Most notably, our\ninvestigation into the learned physics codes reveals that they truly learn\nmeaningful 3D physical motion patterns in the absence of any human labels in\ntraining.", "AI": {"tldr": "FreeGave models 3D scene geometry, appearance, and physics from multi-view videos without object priors, using a physics code and divergence-free module for velocity fields.", "motivation": "Existing methods struggle with complex physical motions at boundaries or require object priors, limiting their applicability.", "method": "Proposes FreeGave, which introduces a physics code and divergence-free module to estimate velocity fields without inefficient PINN losses.", "result": "Outperforms on future frame extrapolation and motion segmentation across datasets, with physics codes learning meaningful motion patterns.", "conclusion": "FreeGave effectively learns 3D physical motion patterns without human labels, demonstrating superior performance."}}
{"id": "2506.07184", "pdf": "https://arxiv.org/pdf/2506.07184", "abs": "https://arxiv.org/abs/2506.07184", "authors": ["Liangliang You", "Junchi Yao", "Shu Yang", "Guimin Hu", "Lijie Hu", "Di Wang"], "title": "Mitigating Behavioral Hallucination in Multimodal Large Language Models for Sequential Images", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "While multimodal large language models excel at various tasks, they still\nsuffer from hallucinations, which limit their reliability and scalability for\nbroader domain applications. To address this issue, recent research mainly\nfocuses on objective hallucination. However, for sequential images, besides\nobjective hallucination, there is also behavioral hallucination, which is less\nstudied. This work aims to fill in the gap. We first reveal that behavioral\nhallucinations mainly arise from two key factors: prior-driven bias and the\nsnowball effect. Based on these observations, we introduce SHE (Sequence\nHallucination Eradication), a lightweight, two-stage framework that (1) detects\nhallucinations via visual-textual alignment check using our proposed adaptive\ntemporal window and (2) mitigates them via orthogonal projection onto the joint\nembedding space. We also propose a new metric (BEACH) to quantify behavioral\nhallucination severity. Empirical results on standard benchmarks demonstrate\nthat SHE reduces behavioral hallucination by over 10% on BEACH while\nmaintaining descriptive accuracy.", "AI": {"tldr": "The paper addresses behavioral hallucinations in multimodal large language models, introducing SHE, a lightweight framework to detect and mitigate them, and a new metric (BEACH) for evaluation.", "motivation": "Behavioral hallucinations in sequential images are understudied, limiting model reliability. The work aims to address this gap by identifying key causes and proposing a solution.", "method": "SHE, a two-stage framework: (1) detects hallucinations via visual-textual alignment with adaptive temporal windows, (2) mitigates them via orthogonal projection in joint embedding space.", "result": "SHE reduces behavioral hallucinations by over 10% on BEACH while maintaining descriptive accuracy.", "conclusion": "The proposed SHE framework effectively addresses behavioral hallucinations, improving model reliability for sequential image tasks."}}
{"id": "2506.07878", "pdf": "https://arxiv.org/pdf/2506.07878", "abs": "https://arxiv.org/abs/2506.07878", "authors": ["Muhammad Ahmed Humais", "Xiaoqian Huang", "Hussain Sajwani", "Sajid Javed", "Yahya Zweiri"], "title": "Spatio-Temporal State Space Model For Efficient Event-Based Optical Flow", "categories": ["cs.CV"], "comment": null, "summary": "Event cameras unlock new frontiers that were previously unthinkable with\nstandard frame-based cameras. One notable example is low-latency motion\nestimation (optical flow), which is critical for many real-time applications.\nIn such applications, the computational efficiency of algorithms is paramount.\nAlthough recent deep learning paradigms such as CNN, RNN, or ViT have shown\nremarkable performance, they often lack the desired computational efficiency.\nConversely, asynchronous event-based methods including SNNs and GNNs are\ncomputationally efficient; however, these approaches fail to capture sufficient\nspatio-temporal information, a powerful feature required to achieve better\nperformance for optical flow estimation. In this work, we introduce\nSpatio-Temporal State Space Model (STSSM) module along with a novel network\narchitecture to develop an extremely efficient solution with competitive\nperformance. Our STSSM module leverages state-space models to effectively\ncapture spatio-temporal correlations in event data, offering higher performance\nwith lower complexity compared to ViT, CNN-based architectures in similar\nsettings. Our model achieves 4.5x faster inference and 8x lower computations\ncompared to TMA and 2x lower computations compared to EV-FlowNet with\ncompetitive performance on the DSEC benchmark. Our code will be available at\nhttps://github.com/AhmedHumais/E-STMFlow", "AI": {"tldr": "The paper introduces STSSM, a spatio-temporal state space model, for efficient and high-performance optical flow estimation using event cameras, outperforming existing methods in speed and computational efficiency.", "motivation": "Event cameras enable low-latency motion estimation, but existing deep learning methods lack computational efficiency, while asynchronous methods miss spatio-temporal information. The goal is to bridge this gap.", "method": "Proposes STSSM module and a novel network architecture to capture spatio-temporal correlations efficiently, leveraging state-space models.", "result": "Achieves 4.5x faster inference and 8x lower computations than TMA, and 2x lower computations than EV-FlowNet, with competitive performance on DSEC benchmark.", "conclusion": "STSSM offers a highly efficient and performant solution for optical flow estimation with event cameras, addressing limitations of existing methods."}}
{"id": "2506.07885", "pdf": "https://arxiv.org/pdf/2506.07885", "abs": "https://arxiv.org/abs/2506.07885", "authors": ["Zubin Bhuyan", "Yuanchang Xie", "AngkeaReach Rith", "Xintong Yan", "Nasko Apostolov", "Jimi Oke", "Chengbo Ai"], "title": "CrosswalkNet: An Optimized Deep Learning Framework for Pedestrian Crosswalk Detection in Aerial Images with High-Performance Computing", "categories": ["cs.CV"], "comment": null, "summary": "With the increasing availability of aerial and satellite imagery, deep\nlearning presents significant potential for transportation asset management,\nsafety analysis, and urban planning. This study introduces CrosswalkNet, a\nrobust and efficient deep learning framework designed to detect various types\nof pedestrian crosswalks from 15-cm resolution aerial images. CrosswalkNet\nincorporates a novel detection approach that improves upon traditional object\ndetection strategies by utilizing oriented bounding boxes (OBB), enhancing\ndetection precision by accurately capturing crosswalks regardless of their\norientation. Several optimization techniques, including Convolutional Block\nAttention, a dual-branch Spatial Pyramid Pooling-Fast module, and cosine\nannealing, are implemented to maximize performance and efficiency. A\ncomprehensive dataset comprising over 23,000 annotated crosswalk instances is\nutilized to train and validate the proposed framework. The best-performing\nmodel achieves an impressive precision of 96.5% and a recall of 93.3% on aerial\nimagery from Massachusetts, demonstrating its accuracy and effectiveness.\nCrosswalkNet has also been successfully applied to datasets from New Hampshire,\nVirginia, and Maine without transfer learning or fine-tuning, showcasing its\nrobustness and strong generalization capability. Additionally, the crosswalk\ndetection results, processed using High-Performance Computing (HPC) platforms\nand provided in polygon shapefile format, have been shown to accelerate data\nprocessing and detection, supporting real-time analysis for safety and mobility\napplications. This integration offers policymakers, transportation engineers,\nand urban planners an effective instrument to enhance pedestrian safety and\nimprove urban mobility.", "AI": {"tldr": "CrosswalkNet is a deep learning framework for detecting pedestrian crosswalks in aerial images, achieving high precision and recall, and demonstrating strong generalization across regions.", "motivation": "The study aims to leverage deep learning for transportation asset management and urban planning by improving crosswalk detection accuracy and efficiency.", "method": "CrosswalkNet uses oriented bounding boxes (OBB), Convolutional Block Attention, a dual-branch Spatial Pyramid Pooling-Fast module, and cosine annealing for optimization. It is trained on a dataset of 23,000 annotated crosswalks.", "result": "The model achieves 96.5% precision and 93.3% recall on Massachusetts imagery and generalizes well to other states without fine-tuning.", "conclusion": "CrosswalkNet provides an effective tool for policymakers and planners to enhance pedestrian safety and urban mobility through real-time analysis."}}
{"id": "2506.07886", "pdf": "https://arxiv.org/pdf/2506.07886", "abs": "https://arxiv.org/abs/2506.07886", "authors": ["Gen Li", "Yutong Chen", "Yiqian Wu", "Kaifeng Zhao", "Marc Pollefeys", "Siyu Tang"], "title": "EgoM2P: Egocentric Multimodal Multitask Pretraining", "categories": ["cs.CV"], "comment": null, "summary": "Understanding multimodal signals in egocentric vision, such as RGB video,\ndepth, camera poses, and gaze, is essential for applications in augmented\nreality, robotics, and human-computer interaction. These capabilities enable\nsystems to better interpret the camera wearer's actions, intentions, and\nsurrounding environment. However, building large-scale egocentric multimodal\nand multitask models presents unique challenges. Egocentric data are inherently\nheterogeneous, with large variations in modality coverage across devices and\nsettings. Generating pseudo-labels for missing modalities, such as gaze or\nhead-mounted camera trajectories, is often infeasible, making standard\nsupervised learning approaches difficult to scale. Furthermore, dynamic camera\nmotion and the complex temporal and spatial structure of first-person video\npose additional challenges for the direct application of existing multimodal\nfoundation models.\n  To address these challenges, we introduce a set of efficient temporal\ntokenizers and propose EgoM2P, a masked modeling framework that learns from\ntemporally aware multimodal tokens to train a large, general-purpose model for\negocentric 4D understanding. This unified design supports multitasking across\ndiverse egocentric perception and synthesis tasks, including gaze prediction,\negocentric camera tracking, and monocular depth estimation from egocentric\nvideo. EgoM2P also serves as a generative model for conditional egocentric\nvideo synthesis. Across these tasks, EgoM2P matches or outperforms specialist\nmodels while being an order of magnitude faster. We will fully open-source\nEgoM2P to support the community and advance egocentric vision research. Project\npage: https://egom2p.github.io/", "AI": {"tldr": "EgoM2P is a masked modeling framework for egocentric 4D understanding, addressing challenges in multimodal data by using efficient temporal tokenizers. It outperforms specialist models in tasks like gaze prediction and depth estimation.", "motivation": "Egocentric vision applications require understanding multimodal signals, but data heterogeneity and missing modalities make supervised learning difficult. Existing models struggle with dynamic camera motion and complex spatiotemporal structures.", "method": "Proposes EgoM2P, a masked modeling framework with temporal tokenizers, enabling multitask learning for egocentric perception and synthesis tasks.", "result": "EgoM2P matches or outperforms specialist models in tasks like gaze prediction, camera tracking, and depth estimation, while being faster.", "conclusion": "EgoM2P is a scalable, general-purpose solution for egocentric vision, advancing research with its open-source release."}}
{"id": "2506.07233", "pdf": "https://arxiv.org/pdf/2506.07233", "abs": "https://arxiv.org/abs/2506.07233", "authors": ["Tzu-wen Hsu", "Ke-Han Lu", "Cheng-Han Chiang", "Hung-yi Lee"], "title": "Reducing Object Hallucination in Large Audio-Language Models via Audio-Aware Decoding", "categories": ["eess.AS", "cs.CL"], "comment": null, "summary": "Large Audio-Language Models (LALMs) can take audio and text as the inputs and\nanswer questions about the audio. While prior LALMs have shown strong\nperformance on standard benchmarks, there has been alarming evidence that LALMs\ncan hallucinate what is presented in the audio. To mitigate the hallucination\nof LALMs, we introduce Audio-Aware Decoding (AAD), a lightweight inference-time\nstrategy that uses contrastive decoding to compare the token prediction logits\nwith and without the audio context. By contrastive decoding, AAD promotes the\ntokens whose probability increases when the audio is present. We conduct our\nexperiment on object hallucination datasets with three LALMs and show that AAD\nimproves the F1 score by 0.046 to 0.428. We also show that AAD can improve the\naccuracy on general audio QA datasets like Clotho-AQA by 5.4% to 10.3%. We\nconduct thorough ablation studies to understand the effectiveness of each\ncomponent in AAD.", "AI": {"tldr": "Audio-Aware Decoding (AAD) reduces hallucination in Large Audio-Language Models (LALMs) by using contrastive decoding to prioritize tokens influenced by audio context, improving performance on benchmarks.", "motivation": "Prior LALMs exhibit hallucination issues, generating incorrect responses about audio content. AAD aims to mitigate this by leveraging audio context during inference.", "method": "AAD employs contrastive decoding to compare token predictions with and without audio, favoring tokens whose probability increases with audio.", "result": "AAD improves F1 scores by 0.046 to 0.428 on hallucination datasets and boosts accuracy by 5.4% to 10.3% on general audio QA tasks.", "conclusion": "AAD effectively reduces hallucination in LALMs and enhances performance, demonstrating its utility as a lightweight inference-time strategy."}}
{"id": "2506.07891", "pdf": "https://arxiv.org/pdf/2506.07891", "abs": "https://arxiv.org/abs/2506.07891", "authors": ["Simone Facchiano", "Stefano Saravalle", "Matteo Migliarini", "Edoardo De Matteis", "Alessio Sampieri", "Andrea Pilzer", "Emanuele Rodol\u00e0", "Indro Spinelli", "Luca Franco", "Fabio Galasso"], "title": "Video Unlearning via Low-Rank Refusal Vector", "categories": ["cs.CV"], "comment": null, "summary": "Video generative models democratize the creation of visual content through\nintuitive instruction following, but they also inherit the biases and harmful\nconcepts embedded within their web-scale training data. This inheritance\ncreates a significant risk, as users can readily generate undesirable and even\nillegal content. This work introduces the first unlearning technique tailored\nexplicitly for video diffusion models to address this critical issue. Our\nmethod requires 5 multi-modal prompt pairs only. Each pair contains a \"safe\"\nand an \"unsafe\" example that differ only by the target concept. Averaging their\nper-layer latent differences produces a \"refusal vector\", which, once\nsubtracted from the model parameters, neutralizes the unsafe concept. We\nintroduce a novel low-rank factorization approach on the covariance difference\nof embeddings that yields robust refusal vectors. This isolates the target\nconcept while minimizing collateral unlearning of other semantics, thus\npreserving the visual quality of the generated video. Our method preserves the\nmodel's generation quality while operating without retraining or access to the\noriginal training data. By embedding the refusal direction directly into the\nmodel's weights, the suppression mechanism becomes inherently more robust\nagainst adversarial bypass attempts compared to surface-level input-output\nfilters. In a thorough qualitative and quantitative evaluation, we show that we\ncan neutralize a variety of harmful contents, including explicit nudity,\ngraphic violence, copyrights, and trademarks. Project page:\nhttps://www.pinlab.org/video-unlearning.", "AI": {"tldr": "The paper introduces an unlearning technique for video diffusion models to remove harmful biases using minimal multi-modal prompt pairs and a refusal vector approach.", "motivation": "Video generative models can produce harmful or illegal content due to biases in training data, necessitating a method to neutralize such concepts without retraining.", "method": "Uses 5 multi-modal prompt pairs (safe/unsafe) to compute a refusal vector via latent differences, applied to model parameters. Low-rank factorization ensures robust concept isolation.", "result": "Effectively neutralizes harmful content (e.g., nudity, violence) while preserving video quality and model performance.", "conclusion": "The method offers a robust, data-efficient solution for unlearning harmful concepts in video diffusion models without retraining."}}
{"id": "2506.07905", "pdf": "https://arxiv.org/pdf/2506.07905", "abs": "https://arxiv.org/abs/2506.07905", "authors": ["Jie Yang", "Feipeng Ma", "Zitian Wang", "Dacheng Yin", "Kang Rong", "Fengyun Rao", "Ruimao Zhang"], "title": "WeThink: Toward General-purpose Vision-Language Reasoning via Reinforcement Learning", "categories": ["cs.CV"], "comment": null, "summary": "Building on the success of text-based reasoning models like DeepSeek-R1,\nextending these capabilities to multimodal reasoning holds great promise. While\nrecent works have attempted to adapt DeepSeek-R1-style reinforcement learning\n(RL) training paradigms to multimodal large language models (MLLM), focusing on\ndomain-specific tasks like math and visual perception, a critical question\nremains: How can we achieve the general-purpose visual-language reasoning\nthrough RL? To address this challenge, we make three key efforts: (1) A novel\nScalable Multimodal QA Synthesis pipeline that autonomously generates\ncontext-aware, reasoning-centric question-answer (QA) pairs directly from the\ngiven images. (2) The open-source WeThink dataset containing over 120K\nmultimodal QA pairs with annotated reasoning paths, curated from 18 diverse\ndataset sources and covering various question domains. (3) A comprehensive\nexploration of RL on our dataset, incorporating a hybrid reward mechanism that\ncombines rule-based verification with model-based assessment to optimize RL\ntraining efficiency across various task domains. Across 14 diverse MLLM\nbenchmarks, we demonstrate that our WeThink dataset significantly enhances\nperformance, from mathematical reasoning to diverse general multimodal tasks.\nMoreover, we show that our automated data pipeline can continuously increase\ndata diversity to further improve model performance.", "AI": {"tldr": "The paper introduces a scalable pipeline for generating multimodal QA pairs, a dataset (WeThink), and explores RL training to enhance general-purpose visual-language reasoning in MLLMs.", "motivation": "Extending text-based reasoning models to multimodal tasks, addressing the challenge of general-purpose visual-language reasoning through RL.", "method": "Develops a Scalable Multimodal QA Synthesis pipeline, creates the WeThink dataset, and explores RL with hybrid rewards.", "result": "WeThink dataset improves performance across 14 MLLM benchmarks, and the pipeline enhances data diversity.", "conclusion": "The approach advances multimodal reasoning, with potential for continuous improvement via automated data generation."}}
{"id": "2506.07398", "pdf": "https://arxiv.org/pdf/2506.07398", "abs": "https://arxiv.org/abs/2506.07398", "authors": ["Guibin Zhang", "Muxin Fu", "Guancheng Wan", "Miao Yu", "Kun Wang", "Shuicheng Yan"], "title": "G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems", "categories": ["cs.MA", "cs.CL", "cs.LG"], "comment": null, "summary": "Large language model (LLM)-powered multi-agent systems (MAS) have\ndemonstrated cognitive and execution capabilities that far exceed those of\nsingle LLM agents, yet their capacity for self-evolution remains hampered by\nunderdeveloped memory architectures. Upon close inspection, we are alarmed to\ndiscover that prevailing MAS memory mechanisms (1) are overly simplistic,\ncompletely disregarding the nuanced inter-agent collaboration trajectories, and\n(2) lack cross-trial and agent-specific customization, in stark contrast to the\nexpressive memory developed for single agents. To bridge this gap, we introduce\nG-Memory, a hierarchical, agentic memory system for MAS inspired by\norganizational memory theory, which manages the lengthy MAS interaction via a\nthree-tier graph hierarchy: insight, query, and interaction graphs. Upon\nreceiving a new user query, G-Memory performs bi-directional memory traversal\nto retrieve both $\\textit{high-level, generalizable insights}$ that enable the\nsystem to leverage cross-trial knowledge, and $\\textit{fine-grained, condensed\ninteraction trajectories}$ that compactly encode prior collaboration\nexperiences. Upon task execution, the entire hierarchy evolves by assimilating\nnew collaborative trajectories, nurturing the progressive evolution of agent\nteams. Extensive experiments across five benchmarks, three LLM backbones, and\nthree popular MAS frameworks demonstrate that G-Memory improves success rates\nin embodied action and accuracy in knowledge QA by up to $20.89\\%$ and\n$10.12\\%$, respectively, without any modifications to the original frameworks.\nOur codes are available at https://github.com/bingreeky/GMemory.", "AI": {"tldr": "G-Memory is a hierarchical memory system for multi-agent systems (MAS) that improves collaboration and success rates by leveraging nuanced inter-agent interactions and cross-trial knowledge.", "motivation": "Current MAS memory architectures are simplistic and lack customization, hindering self-evolution and collaboration.", "method": "G-Memory uses a three-tier graph hierarchy (insight, query, interaction graphs) for bi-directional memory traversal and evolution.", "result": "G-Memory boosts success rates by up to 20.89% in embodied action and 10.12% in knowledge QA across benchmarks.", "conclusion": "G-Memory enhances MAS performance without framework modifications, enabling progressive evolution of agent teams."}}
{"id": "2506.07925", "pdf": "https://arxiv.org/pdf/2506.07925", "abs": "https://arxiv.org/abs/2506.07925", "authors": ["Yaxita Amin", "Naimisha S Trivedi", "Rashmi Bhattad"], "title": "A Comparative Study of U-Net Architectures for Change Detection in Satellite Images", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": null, "summary": "Remote sensing change detection is essential for monitoring the everchanging\nlandscapes of the Earth. The U-Net architecture has gained popularity for its\ncapability to capture spatial information and perform pixel-wise\nclassification. However, their application in the Remote sensing field remains\nlargely unexplored. Therefore, this paper fill the gap by conducting a\ncomprehensive analysis of 34 papers. This study conducts a comparison and\nanalysis of 18 different U-Net variations, assessing their potential for\ndetecting changes in remote sensing. We evaluate both benefits along with\ndrawbacks of each variation within the framework of this particular\napplication. We emphasize variations that are explicitly built for change\ndetection, such as Siamese Swin-U-Net, which utilizes a Siamese architecture.\nThe analysis highlights the significance of aspects such as managing data from\ndifferent time periods and collecting relationships over a long distance to\nenhance the precision of change detection. This study provides valuable\ninsights for researchers and practitioners that choose U-Net versions for\nremote sensing change detection tasks.", "AI": {"tldr": "This paper analyzes 18 U-Net variations for remote sensing change detection, emphasizing Siamese Swin-U-Net, and evaluates their benefits and drawbacks.", "motivation": "To address the unexplored application of U-Net architectures in remote sensing change detection and fill the research gap.", "method": "Comprehensive analysis of 34 papers and comparison of 18 U-Net variations, focusing on their suitability for change detection.", "result": "Highlights the importance of handling multi-temporal data and long-distance relationships for improved change detection accuracy.", "conclusion": "Provides insights for selecting U-Net versions in remote sensing change detection, aiding researchers and practitioners."}}
{"id": "2506.07402", "pdf": "https://arxiv.org/pdf/2506.07402", "abs": "https://arxiv.org/abs/2506.07402", "authors": ["Yukai Zhou", "Sibei Yang", "Wenjie Wang"], "title": "Beyond Jailbreaks: Revealing Stealthier and Broader LLM Security Risks Stemming from Alignment Failures", "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in real-world\napplications, raising concerns about their security. While jailbreak attacks\nhighlight failures under overtly harmful queries, they overlook a critical\nrisk: incorrectly answering harmless-looking inputs can be dangerous and cause\nreal-world harm (Implicit Harm). We systematically reformulate the LLM risk\nlandscape through a structured quadrant perspective based on output factuality\nand input harmlessness, uncovering an overlooked high-risk region. To\ninvestigate this gap, we propose JailFlipBench, a benchmark aims to capture\nimplicit harm, spanning single-modal, multimodal, and factual extension\nscenarios with diverse evaluation metrics. We further develop initial JailFlip\nattack methodologies and conduct comprehensive evaluations across multiple\nopen-source and black-box LLMs, show that implicit harm present immediate and\nurgent real-world risks, calling for broader LLM safety assessments and\nalignment beyond conventional jailbreak paradigms.", "AI": {"tldr": "The paper highlights the overlooked risk of implicit harm in LLMs, where harmless-looking inputs can lead to dangerous outputs. It introduces JailFlipBench to assess this risk and demonstrates its urgency through evaluations.", "motivation": "To address the gap in understanding and mitigating implicit harm in LLMs, which conventional jailbreak attacks overlook.", "method": "Proposes JailFlipBench, a benchmark for implicit harm, and develops JailFlip attack methodologies to evaluate LLMs across diverse scenarios.", "result": "Evaluations reveal that implicit harm poses immediate real-world risks, necessitating broader safety assessments.", "conclusion": "The study calls for expanded LLM safety measures beyond traditional jailbreak paradigms to address implicit harm."}}
{"id": "2506.07936", "pdf": "https://arxiv.org/pdf/2506.07936", "abs": "https://arxiv.org/abs/2506.07936", "authors": ["Chengyue Huang", "Yuchen Zhu", "Sichen Zhu", "Jingyun Xiao", "Moises Andrade", "Shivang Chopra", "Zsolt Kira"], "title": "Mimicking or Reasoning: Rethinking Multi-Modal In-Context Learning in Vision-Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Vision-language models (VLMs) are widely assumed to exhibit in-context\nlearning (ICL), a property similar to that of their language-only counterparts.\nWhile recent work suggests VLMs can perform multimodal ICL (MM-ICL), studies\nshow they often rely on shallow heuristics -- such as copying or majority\nvoting -- rather than true task understanding. We revisit this assumption by\nevaluating VLMs under distribution shifts, where support examples come from a\ndataset different from the query. Surprisingly, performance often degrades with\nmore demonstrations, and models tend to copy answers rather than learn from\nthem. To investigate further, we propose a new MM-ICL with Reasoning pipeline\nthat augments each demonstration with a generated rationale alongside the\nanswer. We conduct extensive and comprehensive experiments on both perception-\nand reasoning-required datasets with open-source VLMs ranging from 3B to 72B\nand proprietary models such as Gemini 2.0. We conduct controlled studies\nvarying shot count, retrieval method, rationale quality, and distribution. Our\nresults show limited performance sensitivity across these factors, suggesting\nthat current VLMs do not effectively utilize demonstration-level information as\nintended in MM-ICL.", "AI": {"tldr": "VLMs' in-context learning (ICL) is often shallow, relying on copying or majority voting rather than true understanding. Performance degrades with more demonstrations, and models copy answers. A new MM-ICL with Reasoning pipeline improves little, indicating VLMs don't effectively use demonstrations.", "motivation": "To assess if VLMs truly exhibit multimodal ICL (MM-ICL) or rely on shallow heuristics, especially under distribution shifts.", "method": "Proposed MM-ICL with Reasoning pipeline, adding generated rationales to demonstrations. Evaluated on perception- and reasoning-required datasets with VLMs of varying sizes.", "result": "Performance degrades with more demonstrations; models copy answers. Limited sensitivity to shot count, retrieval, rationale quality, or distribution.", "conclusion": "Current VLMs don't effectively utilize demonstration-level information in MM-ICL, questioning their true ICL capability."}}
{"id": "2506.07449", "pdf": "https://arxiv.org/pdf/2506.07449", "abs": "https://arxiv.org/abs/2506.07449", "authors": ["Vahid Azizi", "Fatemeh Koochaki"], "title": "LlamaRec-LKG-RAG: A Single-Pass, Learnable Knowledge Graph-RAG Framework for LLM-Based Ranking", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have driven their adoption in\nrecommender systems through Retrieval-Augmented Generation (RAG) frameworks.\nHowever, existing RAG approaches predominantly rely on flat, similarity-based\nretrieval that fails to leverage the rich relational structure inherent in\nuser-item interactions. We introduce LlamaRec-LKG-RAG, a novel single-pass,\nend-to-end trainable framework that integrates personalized knowledge graph\ncontext into LLM-based recommendation ranking. Our approach extends the\nLlamaRec architecture by incorporating a lightweight user preference module\nthat dynamically identifies salient relation paths within a heterogeneous\nknowledge graph constructed from user behavior and item metadata. These\npersonalized subgraphs are seamlessly integrated into prompts for a fine-tuned\nLlama-2 model, enabling efficient and interpretable recommendations through a\nunified inference step. Comprehensive experiments on ML-100K and Amazon Beauty\ndatasets demonstrate consistent and significant improvements over LlamaRec\nacross key ranking metrics (MRR, NDCG, Recall). LlamaRec-LKG-RAG demonstrates\nthe critical value of structured reasoning in LLM-based recommendations and\nestablishes a foundation for scalable, knowledge-aware personalization in\nnext-generation recommender systems. Code is available\nat~\\href{https://github.com/VahidAz/LlamaRec-LKG-RAG}{repository}.", "AI": {"tldr": "LlamaRec-LKG-RAG integrates personalized knowledge graphs into LLM-based recommendations, improving performance over flat retrieval methods.", "motivation": "Existing RAG approaches lack structured reasoning from user-item interactions, limiting recommendation quality.", "method": "Extends LlamaRec with a lightweight user preference module to dynamically identify relation paths in a knowledge graph, integrating them into LLM prompts.", "result": "Outperforms LlamaRec on ML-100K and Amazon Beauty datasets in MRR, NDCG, and Recall.", "conclusion": "Structured reasoning enhances LLM-based recommendations, enabling scalable, knowledge-aware personalization."}}
{"id": "2506.07943", "pdf": "https://arxiv.org/pdf/2506.07943", "abs": "https://arxiv.org/abs/2506.07943", "authors": ["Yizhen Li", "Dell Zhang", "Xuelong Li", "Yiqing Shen"], "title": "Decoupling the Image Perception and Multimodal Reasoning for Reasoning Segmentation with Digital Twin Representations", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Reasoning Segmentation (RS) is a multimodal vision-text task that requires\nsegmenting objects based on implicit text queries, demanding both precise\nvisual perception and vision-text reasoning capabilities. Current RS approaches\nrely on fine-tuning vision-language models (VLMs) for both perception and\nreasoning, but their tokenization of images fundamentally disrupts continuous\nspatial relationships between objects. We introduce DTwinSeger, a novel RS\napproach that leverages Digital Twin (DT) representation as an intermediate\nlayer to decouple perception from reasoning. Innovatively, DTwinSeger\nreformulates RS as a two-stage process, where the first transforms the image\ninto a structured DT representation that preserves spatial relationships and\nsemantic properties and then employs a Large Language Model (LLM) to perform\nexplicit reasoning over this representation to identify target objects. We\npropose a supervised fine-tuning method specifically for LLM with DT\nrepresentation, together with a corresponding fine-tuning dataset Seg-DT, to\nenhance the LLM's reasoning capabilities with DT representations. Experiments\nshow that our method can achieve state-of-the-art performance on two image RS\nbenchmarks and three image referring segmentation benchmarks. It yields that DT\nrepresentation functions as an effective bridge between vision and text,\nenabling complex multimodal reasoning tasks to be accomplished solely with an\nLLM.", "AI": {"tldr": "DTwinSeger introduces a two-stage RS approach using Digital Twin representation to decouple perception and reasoning, achieving state-of-the-art results.", "motivation": "Current RS approaches disrupt spatial relationships due to tokenization, limiting performance.", "method": "DTwinSeger transforms images into structured DT representations, then uses an LLM for reasoning.", "result": "Achieves top performance on RS and referring segmentation benchmarks.", "conclusion": "DT representation effectively bridges vision and text, enabling complex reasoning with LLMs."}}
{"id": "2506.07452", "pdf": "https://arxiv.org/pdf/2506.07452", "abs": "https://arxiv.org/abs/2506.07452", "authors": ["Yuxin Xiao", "Sana Tonekaboni", "Walter Gerych", "Vinith Suriyakumar", "Marzyeh Ghassemi"], "title": "When Style Breaks Safety: Defending Language Models Against Superficial Style Alignment", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "Large language models (LLMs) can be prompted with specific styles (e.g.,\nformatting responses as lists), including in jailbreak queries. Although these\nstyle patterns are semantically unrelated to the malicious intents behind\njailbreak queries, their safety impact remains unclear. In this work, we seek\nto understand whether style patterns compromise LLM safety, how superficial\nstyle alignment increases model vulnerability, and how best to mitigate these\nrisks during alignment. We evaluate 32 LLMs across seven jailbreak benchmarks,\nand find that malicious queries with style patterns inflate the attack success\nrate (ASR) for nearly all models. Notably, ASR inflation correlates with both\nthe length of style patterns and the relative attention an LLM exhibits on\nthem. We then investigate superficial style alignment, and find that\nfine-tuning with specific styles makes LLMs more vulnerable to jailbreaks of\nthose same styles. Finally, we propose SafeStyle, a defense strategy that\nincorporates a small amount of safety training data augmented to match the\ndistribution of style patterns in the fine-tuning data. Across three LLMs and\nfive fine-tuning style settings, SafeStyle consistently outperforms baselines\nin maintaining LLM safety.", "AI": {"tldr": "The paper investigates how style patterns in prompts affect LLM safety, showing they increase jailbreak success rates. It proposes SafeStyle, a defense method that outperforms baselines.", "motivation": "To understand if style patterns in prompts compromise LLM safety, how superficial style alignment increases vulnerability, and how to mitigate these risks.", "method": "Evaluated 32 LLMs across seven jailbreak benchmarks, analyzed style pattern impact, and proposed SafeStyle for defense.", "result": "Style patterns inflate jailbreak success rates, correlating with pattern length and LLM attention. SafeStyle effectively maintains safety.", "conclusion": "Style patterns pose safety risks, but SafeStyle offers a robust defense against such vulnerabilities."}}
{"id": "2506.07960", "pdf": "https://arxiv.org/pdf/2506.07960", "abs": "https://arxiv.org/abs/2506.07960", "authors": ["Ari Vesalainen", "Jenna Kanerva", "Aida Nitsch", "Kiia Korsu", "Ilari Larkiola", "Laura Ruotsalainen", "Filip Ginter"], "title": "Creating a Historical Migration Dataset from Finnish Church Records, 1800-1920", "categories": ["cs.CV", "I.4.6, J.5"], "comment": null, "summary": "This article presents a large-scale effort to create a structured dataset of\ninternal migration in Finland between 1800 and 1920 using digitized church\nmoving records. These records, maintained by Evangelical-Lutheran parishes,\ndocument the migration of individuals and families and offer a valuable source\nfor studying historical demographic patterns. The dataset includes over six\nmillion entries extracted from approximately 200,000 images of handwritten\nmigration records.\n  The data extraction process was automated using a deep learning pipeline that\nincluded layout analysis, table detection, cell classification, and handwriting\nrecognition. The complete pipeline was applied to all images, resulting in a\nstructured dataset suitable for research.\n  The dataset can be used to study internal migration, urbanization, and family\nmigration, and the spread of disease in preindustrial Finland. A case study\nfrom the Elim\\\"aki parish shows how local migration histories can be\nreconstructed. The work demonstrates how large volumes of handwritten archival\nmaterial can be transformed into structured data to support historical and\ndemographic research.", "AI": {"tldr": "A large-scale dataset of Finnish internal migration (1800-1920) was created using digitized church records, automated via deep learning, enabling historical and demographic research.", "motivation": "To transform handwritten archival migration records into structured data for studying historical demographic patterns.", "method": "Automated deep learning pipeline for layout analysis, table detection, cell classification, and handwriting recognition applied to 200,000 images.", "result": "Structured dataset with over six million entries, useful for studying migration, urbanization, family dynamics, and disease spread.", "conclusion": "Demonstrates successful transformation of archival material into research-ready data, with a case study from Elim\u00e4ki parish."}}
{"id": "2506.07964", "pdf": "https://arxiv.org/pdf/2506.07964", "abs": "https://arxiv.org/abs/2506.07964", "authors": ["Wenxin Tang", "Jingyu Xiao", "Wenxuan Jiang", "Xi Xiao", "Yuhang Wang", "Xuxin Tang", "Qing Li", "Yuehe Ma", "Junliang Liu", "Shisong Tang", "Michael R. Lyu"], "title": "SlideCoder: Layout-aware RAG-enhanced Hierarchical Slide Generation from Design", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Manual slide creation is labor-intensive and requires expert prior knowledge.\nExisting natural language-based LLM generation methods struggle to capture the\nvisual and structural nuances of slide designs. To address this, we formalize\nthe Reference Image to Slide Generation task and propose Slide2Code, the first\nbenchmark with difficulty-tiered samples based on a novel Slide Complexity\nMetric. We introduce SlideCoder, a layout-aware, retrieval-augmented framework\nfor generating editable slides from reference images. SlideCoder integrates a\nColor Gradient-based Segmentation algorithm and a Hierarchical\nRetrieval-Augmented Generation method to decompose complex tasks and enhance\ncode generation. We also release SlideMaster, a 7B open-source model fine-tuned\nwith improved reverse-engineered data. Experiments show that SlideCoder\noutperforms state-of-the-art baselines by up to 40.5 points, demonstrating\nstrong performance across layout fidelity, execution accuracy, and visual\nconsistency. Our code is available at\nhttps://github.com/vinsontang1/SlideCoder.", "AI": {"tldr": "Slide2Code and SlideCoder address the challenge of automated slide generation by leveraging reference images and a novel complexity metric, outperforming existing methods by up to 40.5 points.", "motivation": "Manual slide creation is labor-intensive and requires expertise, while current LLM-based methods fail to capture visual and structural nuances.", "method": "SlideCoder uses a Color Gradient-based Segmentation algorithm and Hierarchical Retrieval-Augmented Generation to decompose tasks and improve code generation.", "result": "SlideCoder outperforms state-of-the-art baselines by up to 40.5 points in layout fidelity, execution accuracy, and visual consistency.", "conclusion": "The proposed framework and benchmark advance automated slide generation, with SlideMaster (a 7B open-source model) and SlideCoder demonstrating strong performance."}}
{"id": "2506.07468", "pdf": "https://arxiv.org/pdf/2506.07468", "abs": "https://arxiv.org/abs/2506.07468", "authors": ["Mickel Liu", "Liwei Jiang", "Yancheng Liang", "Simon Shaolei Du", "Yejin Choi", "Tim Althoff", "Natasha Jaques"], "title": "Chasing Moving Targets with Online Self-Play Reinforcement Learning for Safer Language Models", "categories": ["cs.LG", "cs.CL", "cs.MA"], "comment": null, "summary": "Conventional language model (LM) safety alignment relies on a reactive,\ndisjoint procedure: attackers exploit a static model, followed by defensive\nfine-tuning to patch exposed vulnerabilities. This sequential approach creates\na mismatch -- attackers overfit to obsolete defenses, while defenders\nperpetually lag behind emerging threats. To address this, we propose\nSelf-RedTeam, an online self-play reinforcement learning algorithm where an\nattacker and defender agent co-evolve through continuous interaction. We cast\nsafety alignment as a two-player zero-sum game, where a single model alternates\nbetween attacker and defender roles -- generating adversarial prompts and\nsafeguarding against them -- while a reward LM adjudicates outcomes. This\nenables dynamic co-adaptation. Grounded in the game-theoretic framework of\nzero-sum games, we establish a theoretical safety guarantee which motivates the\ndesign of our method: if self-play converges to a Nash Equilibrium, the\ndefender will reliably produce safe responses to any adversarial input.\nEmpirically, Self-RedTeam uncovers more diverse attacks (+21.8% SBERT) compared\nto attackers trained against static defenders and achieves higher robustness on\nsafety benchmarks (e.g., +65.5% on WildJailBreak) than defenders trained\nagainst static attackers. We further propose hidden Chain-of-Thought, allowing\nagents to plan privately, which boosts adversarial diversity and reduces\nover-refusals. Our results motivate a shift from reactive patching to proactive\nco-evolution in LM safety training, enabling scalable, autonomous, and robust\nself-improvement of LMs via multi-agent reinforcement learning (MARL).", "AI": {"tldr": "Self-RedTeam is an online self-play reinforcement learning algorithm for LM safety alignment, where attacker and defender agents co-evolve, improving robustness and adversarial diversity.", "motivation": "Address the mismatch in conventional LM safety alignment, where reactive patching lags behind emerging threats, by enabling dynamic co-adaptation.", "method": "Uses a two-player zero-sum game framework with alternating attacker and defender roles, adjudicated by a reward LM, and includes hidden Chain-of-Thought for private planning.", "result": "Achieves higher adversarial diversity (+21.8% SBERT) and robustness (+65.5% on WildJailBreak) compared to static methods.", "conclusion": "Proposes a shift from reactive patching to proactive co-evolution, enabling scalable and autonomous LM safety improvement via MARL."}}
{"id": "2506.07966", "pdf": "https://arxiv.org/pdf/2506.07966", "abs": "https://arxiv.org/abs/2506.07966", "authors": ["Ziyang Gong", "Wenhao Li", "Oliver Ma", "Songyuan Li", "Jiayi Ji", "Xue Yang", "Gen Luo", "Junchi Yan", "Rongrong Ji"], "title": "SpaCE-10: A Comprehensive Benchmark for Multimodal Large Language Models in Compositional Spatial Intelligence", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable progress in\nvarious multimodal tasks. To pursue higher intelligence in space, MLLMs require\nintegrating multiple atomic spatial capabilities to handle complex and dynamic\ntasks. However, existing benchmarks struggle to comprehensively evaluate the\nspatial intelligence of common MLLMs from the atomic level to the compositional\nlevel. To fill this gap, we present SpaCE-10, a comprehensive benchmark for\ncompositional spatial evaluations. In SpaCE-10, we define 10 atomic spatial\ncapabilities, which are combined to form 8 compositional capabilities. Based on\nthese definitions, we propose a novel hierarchical annotation pipeline to\ngenerate high-quality and diverse question-answer (QA) pairs. With over 150+\nhours of human expert effort, we obtain over 5k QA pairs for 811 real indoor\nscenes in SpaCE-10, which covers various evaluation settings like point cloud\ninput and multi-choice QA. We conduct an extensive evaluation of common MLLMs\non SpaCE-10 and find that even the most advanced MLLM still lags behind humans\nby large margins. Through our careful study, we also draw several significant\nfindings that benefit the MLLM community. For example, we reveal that the\nshortcoming of counting capability greatly limits the compositional spatial\ncapabilities of existing MLLMs. The evaluation code and benchmark datasets are\navailable at https://github.com/Cuzyoung/SpaCE-10.", "AI": {"tldr": "SpaCE-10 is a benchmark for evaluating compositional spatial intelligence in MLLMs, revealing gaps in atomic and combined capabilities.", "motivation": "Existing benchmarks fail to comprehensively assess spatial intelligence in MLLMs from atomic to compositional levels.", "method": "Developed SpaCE-10 with 10 atomic and 8 compositional spatial capabilities, using a hierarchical annotation pipeline to generate 5k QA pairs for 811 indoor scenes.", "result": "Advanced MLLMs lag significantly behind humans, with counting capability identified as a major limitation.", "conclusion": "SpaCE-10 highlights critical gaps in MLLMs' spatial intelligence, offering insights and a benchmark for future improvements."}}
{"id": "2506.07501", "pdf": "https://arxiv.org/pdf/2506.07501", "abs": "https://arxiv.org/abs/2506.07501", "authors": ["Libo Wang"], "title": "Graph-of-Causal Evolution: Challenging Chain-of-Model for Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "The relevant code has been uploaded to the publicly available GitHub\n  repository. The link is:\n  https://github.com/brucewang123456789/GeniusTrail/tree/main/GoCE", "summary": "In view of the problem that each subchain in the chain-of-model (CoM) relies\nonly on the information of the previous subchain and may lose long-range\ndependencies due to the causal mask blocking the global context flow between\nmulti-level subchains, this work proposes a graph of causal evolution (GoCE).\nIts core principle is to map the implicit token representation into a\ndifferentiable and sparse causal adjacency matrix, then permeate causal\nconstraints through each layer of calculation using causal-masked attention and\ncausal-MoE. By combining intervention consistency loss test and self-evolution\ngate, the dynamic balance between causal structure learning and adaptive\nupdating of transformer architecture is realized. The researcher built\nexperimental environments in sandboxes built with Claude Sonnet 4,\no4-mini-high, and DeepSeek R1 respectively with the transformer variant\narchitecture introduced in GoCE. It is evaluated on publicly available datasets\nincluding CLUTRR, CLADDER, EX-FEVER, and CausalQA and compared with the\nbaseline LLMs. The finding proves that GoCE strengthens the transformer's\nability to capture long-range causal dependencies, while the ability to\nself-evolve is improved. It not only surpasses the design of CoM in terms of\ndesign principles, but also provides experience for future research on causal\nlearning and continuous adaptive improvement.", "AI": {"tldr": "The paper proposes GoCE, a method to enhance long-range dependency capture in transformers by using a differentiable causal adjacency matrix and causal-masked attention, outperforming CoM and improving self-evolution.", "motivation": "Address the limitation of CoM in losing long-range dependencies due to causal masking, aiming to improve transformer architecture for causal learning.", "method": "GoCE maps token representations to a sparse causal adjacency matrix, uses causal-masked attention and causal-MoE, and employs intervention consistency loss and self-evolution gates.", "result": "GoCE outperforms baseline LLMs, improving long-range causal dependency capture and self-evolution capabilities.", "conclusion": "GoCE surpasses CoM and offers insights for future causal learning and adaptive transformer research."}}
{"id": "2506.07971", "pdf": "https://arxiv.org/pdf/2506.07971", "abs": "https://arxiv.org/abs/2506.07971", "authors": ["Jiahao Meng", "Shuyang Sun", "Yue Tan", "Lu Qi", "Yunhai Tong", "Xiangtai Li", "Longyin Wen"], "title": "CyberV: Cybernetics for Test-time Scaling in Video Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Current Multimodal Large Language Models (MLLMs) may struggle with\nunderstanding long or complex videos due to computational demands at test time,\nlack of robustness, and limited accuracy, primarily stemming from their\nfeed-forward processing nature. These limitations could be more severe for\nmodels with fewer parameters. To address these limitations, we propose a novel\nframework inspired by cybernetic principles, redesigning video MLLMs as\nadaptive systems capable of self-monitoring, self-correction, and dynamic\nresource allocation during inference. Our approach, CyberV, introduces a\ncybernetic loop consisting of an MLLM Inference System, a Sensor, and a\nController. Specifically, the sensor monitors forward processes of the MLLM and\ncollects intermediate interpretations, such as attention drift, then the\ncontroller determines when and how to trigger self-correction and generate\nfeedback to guide the next round. This test-time adaptive scaling framework\nenhances frozen MLLMs without requiring retraining or additional components.\nExperiments demonstrate significant improvements: CyberV boosts Qwen2.5-VL-7B\nby 8.3% and InternVL3-8B by 5.5% on VideoMMMU, surpassing the competitive\nproprietary model GPT-4o. When applied to Qwen2.5-VL-72B, it yields a 10.0%\nimprovement, achieving performance even comparable to human experts.\nFurthermore, our method demonstrates consistent gains on general-purpose\nbenchmarks, such as VideoMME and WorldSense, highlighting its effectiveness and\ngeneralization capabilities in making MLLMs more robust and accurate for\ndynamic video understanding. The code is released at\nhttps://github.com/marinero4972/CyberV.", "AI": {"tldr": "CyberV introduces a cybernetic loop to enhance MLLMs for video understanding, improving accuracy and robustness without retraining.", "motivation": "Address limitations of current MLLMs in handling long/complex videos due to computational demands, lack of robustness, and accuracy issues.", "method": "Proposes CyberV, a framework with a cybernetic loop (MLLM Inference System, Sensor, Controller) for self-monitoring, self-correction, and dynamic resource allocation.", "result": "Significant improvements: 8.3% boost for Qwen2.5-VL-7B, 5.5% for InternVL3-8B on VideoMMMU, and 10.0% for Qwen2.5-VL-72B, outperforming GPT-4o and nearing human expert performance.", "conclusion": "CyberV effectively enhances MLLMs for dynamic video understanding, demonstrating robustness and generalization across benchmarks."}}
{"id": "2506.07515", "pdf": "https://arxiv.org/pdf/2506.07515", "abs": "https://arxiv.org/abs/2506.07515", "authors": ["Asahi Sakuma", "Hiroaki Sato", "Ryuga Sugano", "Tadashi Kumano", "Yoshihiko Kawai", "Tetsuji Ogawa"], "title": "Speaker-Distinguishable CTC: Learning Speaker Distinction Using CTC for Multi-Talker Speech Recognition", "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "Accepted at INTERSPEECH 2025", "summary": "This paper presents a novel framework for multi-talker automatic speech\nrecognition without the need for auxiliary information. Serialized Output\nTraining (SOT), a widely used approach, suffers from recognition errors due to\nspeaker assignment failures. Although incorporating auxiliary information, such\nas token-level timestamps, can improve recognition accuracy, extracting such\ninformation from natural conversational speech remains challenging. To address\nthis limitation, we propose Speaker-Distinguishable CTC (SD-CTC), an extension\nof CTC that jointly assigns a token and its corresponding speaker label to each\nframe. We further integrate SD-CTC into the SOT framework, enabling the SOT\nmodel to learn speaker distinction using only overlapping speech and\ntranscriptions. Experimental comparisons show that multi-task learning with\nSD-CTC and SOT reduces the error rate of the SOT model by 26% and achieves\nperformance comparable to state-of-the-art methods relying on auxiliary\ninformation.", "AI": {"tldr": "Proposes Speaker-Distinguishable CTC (SD-CTC) to improve multi-talker speech recognition without auxiliary info, reducing errors by 26%.", "motivation": "Addresses limitations of Serialized Output Training (SOT) in speaker assignment and challenges in extracting auxiliary info like timestamps.", "method": "Extends CTC to jointly assign tokens and speaker labels per frame, integrating SD-CTC into SOT for speaker distinction.", "result": "SD-CTC with SOT reduces error rate by 26%, matching state-of-the-art methods using auxiliary info.", "conclusion": "SD-CTC enhances SOT performance without needing auxiliary data, offering a practical solution for multi-talker recognition."}}
{"id": "2506.07977", "pdf": "https://arxiv.org/pdf/2506.07977", "abs": "https://arxiv.org/abs/2506.07977", "authors": ["Jingjing Chang", "Yixiao Fang", "Peng Xing", "Shuhan Wu", "Wei Cheng", "Rui Wang", "Xianfang Zeng", "Gang Yu", "Hai-Bao Chen"], "title": "OneIG-Bench: Omni-dimensional Nuanced Evaluation for Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-image (T2I) models have garnered significant attention for generating\nhigh-quality images aligned with text prompts. However, rapid T2I model\nadvancements reveal limitations in early benchmarks, lacking comprehensive\nevaluations, for example, the evaluation on reasoning, text rendering and\nstyle. Notably, recent state-of-the-art models, with their rich knowledge\nmodeling capabilities, show promising results on the image generation problems\nrequiring strong reasoning ability, yet existing evaluation systems have not\nadequately addressed this frontier. To systematically address these gaps, we\nintroduce OneIG-Bench, a meticulously designed comprehensive benchmark\nframework for fine-grained evaluation of T2I models across multiple dimensions,\nincluding prompt-image alignment, text rendering precision, reasoning-generated\ncontent, stylization, and diversity. By structuring the evaluation, this\nbenchmark enables in-depth analysis of model performance, helping researchers\nand practitioners pinpoint strengths and bottlenecks in the full pipeline of\nimage generation. Specifically, OneIG-Bench enables flexible evaluation by\nallowing users to focus on a particular evaluation subset. Instead of\ngenerating images for the entire set of prompts, users can generate images only\nfor the prompts associated with the selected dimension and complete the\ncorresponding evaluation accordingly. Our codebase and dataset are now publicly\navailable to facilitate reproducible evaluation studies and cross-model\ncomparisons within the T2I research community.", "AI": {"tldr": "The paper introduces OneIG-Bench, a comprehensive benchmark for evaluating text-to-image (T2I) models across multiple dimensions like reasoning, text rendering, and style, addressing gaps in existing benchmarks.", "motivation": "Existing T2I benchmarks lack comprehensive evaluations, especially for reasoning and stylization, prompting the need for a more systematic framework.", "method": "OneIG-Bench is designed for fine-grained evaluation across dimensions like prompt-image alignment, text rendering, reasoning, stylization, and diversity, allowing flexible subset evaluations.", "result": "The benchmark enables detailed performance analysis, helping identify model strengths and bottlenecks, and is publicly available for reproducible studies.", "conclusion": "OneIG-Bench fills critical gaps in T2I evaluation, offering a structured, flexible, and comprehensive framework for researchers and practitioners."}}
{"id": "2506.07551", "pdf": "https://arxiv.org/pdf/2506.07551", "abs": "https://arxiv.org/abs/2506.07551", "authors": ["Mengsong Wu", "YaFei Wang", "Yidong Ming", "Yuqi An", "Yuwei Wan", "Wenliang Chen", "Binbin Lin", "Yuqiang Li", "Tong Xie", "Dongzhan Zhou"], "title": "ChemAgent: Enhancing LLMs for Chemistry and Materials Science through Tree-Search Based Tool Learning", "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.CL"], "comment": "15 pages, 6 figures", "summary": "Large language models (LLMs) have recently demonstrated promising\ncapabilities in chemistry tasks while still facing challenges due to outdated\npretraining knowledge and the difficulty of incorporating specialized chemical\nexpertise. To address these issues, we propose an LLM-based agent that\nsynergistically integrates 137 external chemical tools created ranging from\nbasic information retrieval to complex reaction predictions, and a dataset\ncuration pipeline to generate the dataset ChemToolBench that facilitates both\neffective tool selection and precise parameter filling during fine-tuning and\nevaluation. We introduce a Hierarchical Evolutionary Monte Carlo Tree Search\n(HE-MCTS) framework, enabling independent optimization of tool planning and\nexecution. By leveraging self-generated data, our approach supports step-level\nfine-tuning (FT) of the policy model and training task-adaptive PRM and ORM\nthat surpass GPT-4o. Experimental evaluations demonstrate that our approach\nsignificantly improves performance in Chemistry QA and discovery tasks,\noffering a robust solution to integrate specialized tools with LLMs for\nadvanced chemical applications. All datasets and code are available at\nhttps://github.com/AI4Chem/ChemistryAgent .", "AI": {"tldr": "An LLM-based agent integrates 137 chemical tools and a dataset pipeline (ChemToolBench) to enhance chemistry tasks, using HE-MCTS for optimization and outperforming GPT-4o.", "motivation": "Address outdated pretraining knowledge and difficulty in incorporating chemical expertise in LLMs for chemistry tasks.", "method": "Proposes an LLM-based agent with external tools, ChemToolBench dataset, and HE-MCTS framework for tool planning and execution optimization.", "result": "Significant performance improvement in Chemistry QA and discovery tasks, surpassing GPT-4o.", "conclusion": "Robust solution for integrating specialized tools with LLMs in chemistry, with open datasets and code."}}
{"id": "2506.07981", "pdf": "https://arxiv.org/pdf/2506.07981", "abs": "https://arxiv.org/abs/2506.07981", "authors": ["Dmitrii Vorobev", "Artem Prosvetov", "Karim Elhadji Daou"], "title": "Real-time Localization of a Soccer Ball from a Single Camera", "categories": ["cs.CV", "cs.LG"], "comment": "13 pages, 4 figures", "summary": "We propose a computationally efficient method for real-time three-dimensional\nfootball trajectory reconstruction from a single broadcast camera. In contrast\nto previous work, our approach introduces a multi-mode state model with $W$\ndiscrete modes to significantly accelerate optimization while preserving\ncentimeter-level accuracy -- even in cases of severe occlusion, motion blur,\nand complex backgrounds. The system operates on standard CPUs and achieves low\nlatency suitable for live broadcast settings. Extensive evaluation on a\nproprietary dataset of 6K-resolution Russian Premier League matches\ndemonstrates performance comparable to multi-camera systems, without the need\nfor specialized or costly infrastructure. This work provides a practical method\nfor accessible and accurate 3D ball tracking in professional football\nenvironments.", "AI": {"tldr": "A computationally efficient method for real-time 3D football trajectory reconstruction from a single broadcast camera, achieving centimeter-level accuracy with low latency.", "motivation": "To provide an accessible and accurate 3D ball tracking solution in professional football without costly multi-camera setups.", "method": "Introduces a multi-mode state model with discrete modes to accelerate optimization, handling occlusion, motion blur, and complex backgrounds.", "result": "Demonstrates performance comparable to multi-camera systems on a proprietary dataset of 6K-resolution matches, using standard CPUs.", "conclusion": "The method offers a practical, cost-effective solution for live broadcast settings, maintaining high accuracy."}}
{"id": "2506.07564", "pdf": "https://arxiv.org/pdf/2506.07564", "abs": "https://arxiv.org/abs/2506.07564", "authors": ["Peiran Li", "Xinkai Zou", "Zhuohang Wu", "Ruifeng Li", "Shuo Xing", "Hanwen Zheng", "Zhikai Hu", "Yuping Wang", "Haoxi Li", "Qin Yuan", "Yingmo Zhang", "Zhengzhong Tu"], "title": "SAFEFLOW: A Principled Protocol for Trustworthy and Transactional Autonomous Agent Systems", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Recent advances in large language models (LLMs) and vision-language models\n(VLMs) have enabled powerful autonomous agents capable of complex reasoning and\nmulti-modal tool use. Despite their growing capabilities, today's agent\nframeworks remain fragile, lacking principled mechanisms for secure information\nflow, reliability, and multi-agent coordination. In this work, we introduce\nSAFEFLOW, a new protocol-level framework for building trustworthy LLM/VLM-based\nagents. SAFEFLOW enforces fine-grained information flow control (IFC),\nprecisely tracking provenance, integrity, and confidentiality of all the data\nexchanged between agents, tools, users, and environments. By constraining LLM\nreasoning to respect these security labels, SAFEFLOW prevents untrusted or\nadversarial inputs from contaminating high-integrity decisions. To ensure\nrobustness in concurrent multi-agent settings, SAFEFLOW introduces\ntransactional execution, conflict resolution, and secure scheduling over shared\nstate, preserving global consistency across agents. We further introduce\nmechanisms, including write-ahead logging, rollback, and secure caches, that\nfurther enhance resilience against runtime errors and policy violations. To\nvalidate the performances, we built SAFEFLOWBENCH, a comprehensive benchmark\nsuite designed to evaluate agent reliability under adversarial, noisy, and\nconcurrent operational conditions. Extensive experiments demonstrate that\nagents built with SAFEFLOW maintain impressive task performance and security\nguarantees even in hostile environments, substantially outperforming\nstate-of-the-art. Together, SAFEFLOW and SAFEFLOWBENCH lay the groundwork for\nprincipled, robust, and secure agent ecosystems, advancing the frontier of\nreliable autonomy.", "AI": {"tldr": "SAFEFLOW is a protocol-level framework for secure and reliable LLM/VLM-based agents, enforcing fine-grained information flow control and robust multi-agent coordination, validated by SAFEFLOWBENCH.", "motivation": "Current agent frameworks lack mechanisms for secure information flow, reliability, and multi-agent coordination, making them fragile.", "method": "SAFEFLOW introduces fine-grained IFC, transactional execution, conflict resolution, secure scheduling, write-ahead logging, rollback, and secure caches.", "result": "Agents built with SAFEFLOW maintain high task performance and security, outperforming state-of-the-art in adversarial conditions.", "conclusion": "SAFEFLOW and SAFEFLOWBENCH advance reliable autonomy by providing principled, robust, and secure agent ecosystems."}}
{"id": "2506.07984", "pdf": "https://arxiv.org/pdf/2506.07984", "abs": "https://arxiv.org/abs/2506.07984", "authors": ["Mingquan Lin", "Gregory Holste", "Song Wang", "Yiliang Zhou", "Yishu Wei", "Imon Banerjee", "Pengyi Chen", "Tianjie Dai", "Yuexi Du", "Nicha C. Dvornek", "Yuyan Ge", "Zuowei Guo", "Shouhei Hanaoka", "Dongkyun Kim", "Pablo Messina", "Yang Lu", "Denis Parra", "Donghyun Son", "\u00c1lvaro Soto", "Aisha Urooj", "Ren\u00e9 Vidal", "Yosuke Yamagishi", "Zefan Yang", "Ruichi Zhang", "Yang Zhou", "Leo Anthony Celi", "Ronald M. Summers", "Zhiyong Lu", "Hao Chen", "Adam Flanders", "George Shih", "Zhangyang Wang", "Yifan Peng"], "title": "CXR-LT 2024: A MICCAI challenge on long-tailed, multi-label, and zero-shot disease classification from chest X-ray", "categories": ["cs.CV", "cs.LG"], "comment": "17 pages, 3 figures", "summary": "The CXR-LT series is a community-driven initiative designed to enhance lung\ndisease classification using chest X-rays (CXR). It tackles challenges in open\nlong-tailed lung disease classification and enhances the measurability of\nstate-of-the-art techniques. The first event, CXR-LT 2023, aimed to achieve\nthese goals by providing high-quality benchmark CXR data for model development\nand conducting comprehensive evaluations to identify ongoing issues impacting\nlung disease classification performance. Building on the success of CXR-LT\n2023, the CXR-LT 2024 expands the dataset to 377,110 chest X-rays (CXRs) and 45\ndisease labels, including 19 new rare disease findings. It also introduces a\nnew focus on zero-shot learning to address limitations identified in the\nprevious event. Specifically, CXR-LT 2024 features three tasks: (i) long-tailed\nclassification on a large, noisy test set, (ii) long-tailed classification on a\nmanually annotated \"gold standard\" subset, and (iii) zero-shot generalization\nto five previously unseen disease findings. This paper provides an overview of\nCXR-LT 2024, detailing the data curation process and consolidating\nstate-of-the-art solutions, including the use of multimodal models for rare\ndisease detection, advanced generative approaches to handle noisy labels, and\nzero-shot learning strategies for unseen diseases. Additionally, the expanded\ndataset enhances disease coverage to better represent real-world clinical\nsettings, offering a valuable resource for future research. By synthesizing the\ninsights and innovations of participating teams, we aim to advance the\ndevelopment of clinically realistic and generalizable diagnostic models for\nchest radiography.", "AI": {"tldr": "CXR-LT 2024 expands a community-driven initiative for lung disease classification using chest X-rays, introducing a larger dataset, zero-shot learning, and three tasks to improve model performance and generalizability.", "motivation": "To enhance lung disease classification by addressing challenges in open long-tailed classification and improving the measurability of state-of-the-art techniques.", "method": "Expands the dataset to 377,110 CXRs with 45 disease labels, introduces zero-shot learning, and features three tasks: long-tailed classification on noisy and gold-standard test sets, and zero-shot generalization to unseen diseases.", "result": "Provides a high-quality benchmark dataset, consolidates advanced solutions (multimodal models, generative approaches, zero-shot strategies), and enhances disease coverage for real-world clinical settings.", "conclusion": "Aims to advance clinically realistic and generalizable diagnostic models for chest radiography by synthesizing insights from participating teams."}}
{"id": "2506.07985", "pdf": "https://arxiv.org/pdf/2506.07985", "abs": "https://arxiv.org/abs/2506.07985", "authors": ["Tuomas Oikarinen", "Ge Yan", "Akshay Kulkarni", "Tsui-Wei Weng"], "title": "Rethinking Crowd-Sourced Evaluation of Neuron Explanations", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Interpreting individual neurons or directions in activations space is an\nimportant component of mechanistic interpretability. As such, many algorithms\nhave been proposed to automatically produce neuron explanations, but it is\noften not clear how reliable these explanations are, or which methods produce\nthe best explanations. This can be measured via crowd-sourced evaluations, but\nthey can often be noisy and expensive, leading to unreliable results. In this\npaper, we carefully analyze the evaluation pipeline and develop a\ncost-effective and highly accurate crowdsourced evaluation strategy. In\ncontrast to previous human studies that only rate whether the explanation\nmatches the most highly activating inputs, we estimate whether the explanation\ndescribes neuron activations across all inputs. To estimate this effectively,\nwe introduce a novel application of importance sampling to determine which\ninputs are the most valuable to show to raters, leading to around 30x cost\nreduction compared to uniform sampling. We also analyze the label noise present\nin crowd-sourced evaluations and propose a Bayesian method to aggregate\nmultiple ratings leading to a further ~5x reduction in number of ratings\nrequired for the same accuracy. Finally, we use these methods to conduct a\nlarge-scale study comparing the quality of neuron explanations produced by the\nmost popular methods for two different vision models.", "AI": {"tldr": "The paper introduces a cost-effective, accurate crowdsourced evaluation strategy for neuron explanations in mechanistic interpretability, using importance sampling and Bayesian aggregation to reduce costs and improve reliability.", "motivation": "Current methods for evaluating neuron explanations are noisy and expensive, making it unclear which methods produce the best explanations.", "method": "Develops a crowdsourced evaluation strategy with importance sampling for input selection and Bayesian aggregation for rating noise reduction.", "result": "Achieves ~30x cost reduction via importance sampling and ~5x reduction in required ratings via Bayesian aggregation.", "conclusion": "The proposed methods enable large-scale, reliable comparison of neuron explanation quality across popular methods for vision models."}}
{"id": "2506.07747", "pdf": "https://arxiv.org/pdf/2506.07747", "abs": "https://arxiv.org/abs/2506.07747", "authors": ["Adam Breuer"], "title": "E-LDA: Toward Interpretable LDA Topic Models with Strong Guarantees in Logarithmic Parallel Time", "categories": ["cs.LG", "cs.CL", "stat.ML"], "comment": "ICML 2025; Code available at: https://github.com/BreuerLabs/E- LDA", "summary": "In this paper, we provide the first practical algorithms with provable\nguarantees for the problem of inferring the topics assigned to each document in\nan LDA topic model. This is the primary inference problem for many applications\nof topic models in social science, data exploration, and causal inference\nsettings. We obtain this result by showing a novel non-gradient-based,\ncombinatorial approach to estimating topic models. This yields algorithms that\nconverge to near-optimal posterior probability in logarithmic parallel\ncomputation time (adaptivity) -- exponentially faster than any known LDA\nalgorithm. We also show that our approach can provide interpretability\nguarantees such that each learned topic is formally associated with a known\nkeyword. Finally, we show that unlike alternatives, our approach can maintain\nthe independence assumptions necessary to use the learned topic model for\ndownstream causal inference methods that allow researchers to study topics as\ntreatments. In terms of practical performance, our approach consistently\nreturns solutions of higher semantic quality than solutions from\nstate-of-the-art LDA algorithms, neural topic models, and LLM-based topic\nmodels across a diverse range of text datasets and evaluation parameters.", "AI": {"tldr": "The paper introduces a novel combinatorial algorithm for LDA topic model inference, offering faster convergence and better interpretability than existing methods.", "motivation": "To address the lack of practical algorithms with provable guarantees for topic inference in LDA models, crucial for applications in social science and causal inference.", "method": "A non-gradient-based, combinatorial approach for estimating topic models, ensuring logarithmic parallel computation time and interpretability guarantees.", "result": "The algorithm converges exponentially faster than existing LDA methods, provides interpretable topics, and maintains independence for causal inference.", "conclusion": "The approach outperforms state-of-the-art LDA, neural, and LLM-based topic models in semantic quality across diverse datasets."}}
{"id": "2506.07986", "pdf": "https://arxiv.org/pdf/2506.07986", "abs": "https://arxiv.org/abs/2506.07986", "authors": ["Zhengyao Lv", "Tianlin Pan", "Chenyang Si", "Zhaoxi Chen", "Wangmeng Zuo", "Ziwei Liu", "Kwan-Yee K. Wong"], "title": "Rethinking Cross-Modal Interaction in Multimodal Diffusion Transformers", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal Diffusion Transformers (MM-DiTs) have achieved remarkable progress\nin text-driven visual generation. However, even state-of-the-art MM-DiT models\nlike FLUX struggle with achieving precise alignment between text prompts and\ngenerated content. We identify two key issues in the attention mechanism of\nMM-DiT, namely 1) the suppression of cross-modal attention due to token\nimbalance between visual and textual modalities and 2) the lack of\ntimestep-aware attention weighting, which hinder the alignment. To address\nthese issues, we propose \\textbf{Temperature-Adjusted Cross-modal Attention\n(TACA)}, a parameter-efficient method that dynamically rebalances multimodal\ninteractions through temperature scaling and timestep-dependent adjustment.\nWhen combined with LoRA fine-tuning, TACA significantly enhances text-image\nalignment on the T2I-CompBench benchmark with minimal computational overhead.\nWe tested TACA on state-of-the-art models like FLUX and SD3.5, demonstrating\nits ability to improve image-text alignment in terms of object appearance,\nattribute binding, and spatial relationships. Our findings highlight the\nimportance of balancing cross-modal attention in improving semantic fidelity in\ntext-to-image diffusion models. Our codes are publicly available at\n\\href{https://github.com/Vchitect/TACA}", "AI": {"tldr": "The paper introduces TACA, a method to improve text-image alignment in MM-DiTs by addressing token imbalance and timestep-aware attention issues.", "motivation": "State-of-the-art MM-DiT models struggle with precise alignment between text prompts and generated content due to cross-modal attention suppression and lack of timestep-aware weighting.", "method": "Proposes Temperature-Adjusted Cross-modal Attention (TACA), which dynamically rebalances multimodal interactions using temperature scaling and timestep-dependent adjustment, combined with LoRA fine-tuning.", "result": "TACA significantly enhances text-image alignment on T2I-CompBench, improving object appearance, attribute binding, and spatial relationships in models like FLUX and SD3.5.", "conclusion": "Balancing cross-modal attention is crucial for semantic fidelity in text-to-image diffusion models, and TACA offers a parameter-efficient solution."}}
{"id": "2506.07833", "pdf": "https://arxiv.org/pdf/2506.07833", "abs": "https://arxiv.org/abs/2506.07833", "authors": ["Michael K. Chen", "Xikun Zhang", "Jiaxing Huang", "Dacheng Tao"], "title": "Improving large language models with concept-aware fine-tuning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have become the cornerstone of modern AI.\nHowever, the existing paradigm of next-token prediction fundamentally limits\ntheir ability to form coherent, high-level concepts, making it a critical\nbarrier to human-like understanding and reasoning. Take the phrase \"ribonucleic\nacid\" as an example: an LLM will first decompose it into tokens, i.e.,\nartificial text fragments (\"rib\", \"on\", ...), then learn each token\nsequentially, rather than grasping the phrase as a unified, coherent semantic\nentity. This fragmented representation hinders deeper conceptual understanding\nand, ultimately, the development of truly intelligent systems. In response, we\nintroduce Concept-Aware Fine-Tuning (CAFT), a novel multi-token training method\nthat redefines how LLMs are fine-tuned. By enabling the learning of sequences\nthat span multiple tokens, this method fosters stronger concept-aware learning.\nOur experiments demonstrate significant improvements compared to conventional\nnext-token finetuning methods across diverse tasks, including traditional\napplications like text summarization and domain-specific ones like de novo\nprotein design. Multi-token prediction was previously only possible in the\nprohibitively expensive pretraining phase; CAFT, to our knowledge, is the first\nto bring the multi-token setting to the post-training phase, thus effectively\ndemocratizing its benefits for the broader community of practitioners and\nresearchers. Finally, the unexpected effectiveness of our proposed method\nsuggests wider implications for the machine learning research community. All\ncode and data are available at https://github.com/michaelchen-lab/caft-llm", "AI": {"tldr": "The paper introduces Concept-Aware Fine-Tuning (CAFT), a method to improve LLMs' ability to learn coherent concepts by enabling multi-token training, showing significant improvements over traditional methods.", "motivation": "Existing LLMs struggle with high-level conceptual understanding due to fragmented token-based learning, limiting human-like reasoning.", "method": "CAFT redefines fine-tuning by allowing multi-token sequence learning, fostering concept-aware understanding.", "result": "Experiments show CAFT outperforms conventional methods in tasks like text summarization and protein design.", "conclusion": "CAFT democratizes multi-token learning for post-training phases, with broader implications for AI research."}}
{"id": "2506.07992", "pdf": "https://arxiv.org/pdf/2506.07992", "abs": "https://arxiv.org/abs/2506.07992", "authors": ["Haoguang Lu", "Jiacheng Chen", "Zhenguo Yang", "Aurele Tohokantche Gnanha", "Fu Lee Wang", "Li Qing", "Xudong Mao"], "title": "PairEdit: Learning Semantic Variations for Exemplar-based Image Editing", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in text-guided image editing have achieved notable\nsuccess by leveraging natural language prompts for fine-grained semantic\ncontrol. However, certain editing semantics are challenging to specify\nprecisely using textual descriptions alone. A practical alternative involves\nlearning editing semantics from paired source-target examples. Existing\nexemplar-based editing methods still rely on text prompts describing the change\nwithin paired examples or learning implicit text-based editing instructions. In\nthis paper, we introduce PairEdit, a novel visual editing method designed to\neffectively learn complex editing semantics from a limited number of image\npairs or even a single image pair, without using any textual guidance. We\npropose a target noise prediction that explicitly models semantic variations\nwithin paired images through a guidance direction term. Moreover, we introduce\na content-preserving noise schedule to facilitate more effective semantic\nlearning. We also propose optimizing distinct LoRAs to disentangle the learning\nof semantic variations from content. Extensive qualitative and quantitative\nevaluations demonstrate that PairEdit successfully learns intricate semantics\nwhile significantly improving content consistency compared to baseline methods.\nCode will be available at https://github.com/xudonmao/PairEdit.", "AI": {"tldr": "PairEdit is a visual editing method that learns complex editing semantics from image pairs without textual guidance, improving content consistency.", "motivation": "Textual descriptions alone are insufficient for precise editing semantics, and existing exemplar-based methods still rely on text prompts.", "method": "PairEdit uses target noise prediction, a content-preserving noise schedule, and distinct LoRAs to disentangle semantic variations from content.", "result": "PairEdit successfully learns intricate semantics and improves content consistency over baselines.", "conclusion": "PairEdit offers a text-free solution for learning editing semantics from image pairs, outperforming existing methods."}}
{"id": "2506.07896", "pdf": "https://arxiv.org/pdf/2506.07896", "abs": "https://arxiv.org/abs/2506.07896", "authors": ["Shoko Oka"], "title": "Evaluating Large Language Models on the Frame and Symbol Grounding Problems: A Zero-shot Benchmark", "categories": ["cs.AI", "cs.CL"], "comment": "52 pages, Additional resources available on GitHub repository", "summary": "Recent advancements in large language models (LLMs) have revitalized\nphilosophical debates surrounding artificial intelligence. Two of the most\nfundamental challenges - namely, the Frame Problem and the Symbol Grounding\nProblem - have historically been viewed as unsolvable within traditional\nsymbolic AI systems. This study investigates whether modern LLMs possess the\ncognitive capacities required to address these problems. To do so, I designed\ntwo benchmark tasks reflecting the philosophical core of each problem,\nadministered them under zero-shot conditions to 13 prominent LLMs (both closed\nand open-source), and assessed the quality of the models' outputs across five\ntrials each. Responses were scored along multiple criteria, including\ncontextual reasoning, semantic coherence, and information filtering. The\nresults demonstrate that while open-source models showed variability in\nperformance due to differences in model size, quantization, and instruction\ntuning, several closed models consistently achieved high scores. These findings\nsuggest that select modern LLMs may be acquiring capacities sufficient to\nproduce meaningful and stable responses to these long-standing theoretical\nchallenges.", "AI": {"tldr": "Modern LLMs show potential in addressing the Frame Problem and Symbol Grounding Problem, with closed models outperforming open-source ones in benchmark tasks.", "motivation": "To explore whether modern LLMs can tackle the historically unsolvable Frame Problem and Symbol Grounding Problem in AI.", "method": "Designed two benchmark tasks reflecting these problems, tested 13 LLMs under zero-shot conditions, and scored responses based on reasoning, coherence, and filtering.", "result": "Closed models consistently scored high, while open-source models varied due to factors like size and tuning.", "conclusion": "Some modern LLMs may have the capacity to meaningfully address these long-standing theoretical challenges."}}
{"id": "2506.07996", "pdf": "https://arxiv.org/pdf/2506.07996", "abs": "https://arxiv.org/abs/2506.07996", "authors": ["Ming-Feng Li", "Xin Yang", "Fu-En Wang", "Hritam Basak", "Yuyin Sun", "Shreekant Gayaka", "Min Sun", "Cheng-Hao Kuo"], "title": "UA-Pose: Uncertainty-Aware 6D Object Pose Estimation and Online Object Completion with Partial References", "categories": ["cs.CV", "cs.RO"], "comment": "CVPR 2025", "summary": "6D object pose estimation has shown strong generalizability to novel objects.\nHowever, existing methods often require either a complete, well-reconstructed\n3D model or numerous reference images that fully cover the object. Estimating\n6D poses from partial references, which capture only fragments of an object's\nappearance and geometry, remains challenging. To address this, we propose\nUA-Pose, an uncertainty-aware approach for 6D object pose estimation and online\nobject completion specifically designed for partial references. We assume\naccess to either (1) a limited set of RGBD images with known poses or (2) a\nsingle 2D image. For the first case, we initialize a partial object 3D model\nbased on the provided images and poses, while for the second, we use\nimage-to-3D techniques to generate an initial object 3D model. Our method\nintegrates uncertainty into the incomplete 3D model, distinguishing between\nseen and unseen regions. This uncertainty enables confidence assessment in pose\nestimation and guides an uncertainty-aware sampling strategy for online object\ncompletion, enhancing robustness in pose estimation accuracy and improving\nobject completeness. We evaluate our method on the YCB-Video, YCBInEOAT, and\nHO3D datasets, including RGBD sequences of YCB objects manipulated by robots\nand human hands. Experimental results demonstrate significant performance\nimprovements over existing methods, particularly when object observations are\nincomplete or partially captured. Project page:\nhttps://minfenli.github.io/UA-Pose/", "AI": {"tldr": "UA-Pose is an uncertainty-aware method for 6D object pose estimation and online object completion, designed for partial references. It outperforms existing methods, especially with incomplete object observations.", "motivation": "Existing 6D pose estimation methods require complete 3D models or extensive reference images, limiting their use with partial references. UA-Pose addresses this gap.", "method": "UA-Pose uses partial RGBD images or a single 2D image to initialize an incomplete 3D model, integrating uncertainty to distinguish seen/unseen regions. It employs uncertainty-aware sampling for online completion.", "result": "UA-Pose shows significant performance improvements on YCB-Video, YCBInEOAT, and HO3D datasets, especially with incomplete observations.", "conclusion": "UA-Pose enhances robustness and accuracy in 6D pose estimation and object completion for partial references, outperforming existing methods."}}
{"id": "2506.07915", "pdf": "https://arxiv.org/pdf/2506.07915", "abs": "https://arxiv.org/abs/2506.07915", "authors": ["Dimitris Panagopoulos", "Adolfo Perrusquia", "Weisi Guo"], "title": "LUCIFER: Language Understanding and Context-Infused Framework for Exploration and Behavior Refinement", "categories": ["cs.AI", "cs.CL", "cs.SY", "eess.SY"], "comment": "12 pages, 4 Figures, 3 Tables, submitted to the IEEE for possible\n  publication", "summary": "In dynamic environments, the rapid obsolescence of pre-existing environmental\nknowledge creates a gap between an agent's internal model and the evolving\nreality of its operational context. This disparity between prior and updated\nenvironmental valuations fundamentally limits the effectiveness of autonomous\ndecision-making. To bridge this gap, the contextual bias of human domain\nstakeholders, who naturally accumulate insights through direct, real-time\nobservation, becomes indispensable. However, translating their nuanced, and\ncontext-rich input into actionable intelligence for autonomous systems remains\nan open challenge. To address this, we propose LUCIFER (Language Understanding\nand Context-Infused Framework for Exploration and Behavior Refinement), a\ndomain-agnostic framework that integrates a hierarchical decision-making\narchitecture with reinforcement learning (RL) and large language models (LLMs)\ninto a unified system. This architecture mirrors how humans decompose complex\ntasks, enabling a high-level planner to coordinate specialised sub-agents, each\nfocused on distinct objectives and temporally interdependent actions. Unlike\ntraditional applications where LLMs are limited to single role, LUCIFER\nintegrates them in two synergistic roles: as context extractors, structuring\nverbal stakeholder input into domain-aware representations that influence\ndecision-making through an attention space mechanism aligning LLM-derived\ninsights with the agent's learning process, and as zero-shot exploration\nfacilitators guiding the agent's action selection process during exploration.\nWe benchmark various LLMs in both roles and demonstrate that LUCIFER improves\nexploration efficiency and decision quality, outperforming flat,\ngoal-conditioned policies. Our findings show the potential of context-driven\ndecision-making, where autonomous systems leverage human contextual knowledge\nfor operational success.", "AI": {"tldr": "LUCIFER integrates hierarchical decision-making, RL, and LLMs to bridge the gap between human contextual insights and autonomous systems, improving exploration and decision quality.", "motivation": "Address the challenge of translating human contextual knowledge into actionable intelligence for autonomous systems in dynamic environments.", "method": "Proposes LUCIFER, a framework combining hierarchical decision-making, RL, and LLMs, using LLMs as context extractors and exploration facilitators.", "result": "LUCIFER outperforms flat, goal-conditioned policies, enhancing exploration efficiency and decision quality.", "conclusion": "Context-driven decision-making, leveraging human insights, holds potential for autonomous systems' operational success."}}
{"id": "2506.07999", "pdf": "https://arxiv.org/pdf/2506.07999", "abs": "https://arxiv.org/abs/2506.07999", "authors": ["Junhao Chen", "Yulia Tsvetkov", "Xiaochuang Han"], "title": "MADFormer: Mixed Autoregressive and Diffusion Transformers for Continuous Image Generation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Recent progress in multimodal generation has increasingly combined\nautoregressive (AR) and diffusion-based approaches, leveraging their\ncomplementary strengths: AR models capture long-range dependencies and produce\nfluent, context-aware outputs, while diffusion models operate in continuous\nlatent spaces to refine high-fidelity visual details. However, existing hybrids\noften lack systematic guidance on how and why to allocate model capacity\nbetween these paradigms. In this work, we introduce MADFormer, a Mixed\nAutoregressive and Diffusion Transformer that serves as a testbed for analyzing\nAR-diffusion trade-offs. MADFormer partitions image generation into spatial\nblocks, using AR layers for one-pass global conditioning across blocks and\ndiffusion layers for iterative local refinement within each block. Through\ncontrolled experiments on FFHQ-1024 and ImageNet, we identify two key insights:\n(1) block-wise partitioning significantly improves performance on\nhigh-resolution images, and (2) vertically mixing AR and diffusion layers\nyields better quality-efficiency balances--improving FID by up to 75% under\nconstrained inference compute. Our findings offer practical design principles\nfor future hybrid generative models.", "AI": {"tldr": "MADFormer combines autoregressive and diffusion models for image generation, improving performance and efficiency through block-wise partitioning and vertical mixing.", "motivation": "Existing hybrids of autoregressive and diffusion models lack systematic guidance on capacity allocation. MADFormer addresses this gap.", "method": "MADFormer partitions image generation into spatial blocks, using AR layers for global conditioning and diffusion layers for local refinement.", "result": "Block-wise partitioning improves high-resolution image performance, and vertical mixing enhances quality-efficiency balance (up to 75% FID improvement).", "conclusion": "MADFormer provides practical design principles for future hybrid generative models."}}
{"id": "2506.07919", "pdf": "https://arxiv.org/pdf/2506.07919", "abs": "https://arxiv.org/abs/2506.07919", "authors": ["Manuel Brenner", "Georgia Koppe"], "title": "Uncovering the Functional Roles of Nonlinearity in Memory", "categories": ["cs.LG", "cs.AI", "cs.CL", "nlin.CD", "physics.comp-ph"], "comment": "Preprint under review", "summary": "Memory and long-range temporal processing are core requirements for sequence\nmodeling tasks across natural language processing, time-series forecasting,\nspeech recognition, and control. While nonlinear recurrence has long been\nviewed as essential for enabling such mechanisms, recent work suggests that\nlinear dynamics may often suffice. In this study, we go beyond performance\ncomparisons to systematically dissect the functional role of nonlinearity in\nrecurrent networks--identifying both when it is computationally necessary, and\nwhat mechanisms it enables. We use Almost Linear Recurrent Neural Networks\n(AL-RNNs), which allow fine-grained control over nonlinearity, as both a\nflexible modeling tool and a probe into the internal mechanisms of memory.\nAcross a range of classic sequence modeling tasks and a real-world stimulus\nselection task, we find that minimal nonlinearity is not only sufficient but\noften optimal, yielding models that are simpler, more robust, and more\ninterpretable than their fully nonlinear or linear counterparts. Our results\nprovide a principled framework for selectively introducing nonlinearity,\nbridging dynamical systems theory with the functional demands of long-range\nmemory and structured computation in recurrent neural networks, with\nimplications for both artificial and biological neural systems.", "AI": {"tldr": "The study explores the role of nonlinearity in recurrent networks, finding minimal nonlinearity often optimal for performance, robustness, and interpretability.", "motivation": "To understand when and why nonlinearity is necessary in recurrent networks for tasks like memory and long-range temporal processing.", "method": "Uses Almost Linear Recurrent Neural Networks (AL-RNNs) to control nonlinearity and analyze its role across various tasks.", "result": "Minimal nonlinearity is sufficient and often optimal, simplifying models while maintaining performance.", "conclusion": "Provides a framework for selectively introducing nonlinearity, benefiting both artificial and biological neural systems."}}
{"id": "2506.08002", "pdf": "https://arxiv.org/pdf/2506.08002", "abs": "https://arxiv.org/abs/2506.08002", "authors": ["Aadarsh Sahoo", "Vansh Tibrewal", "Georgia Gkioxari"], "title": "Aligning Text, Images, and 3D Structure Token-by-Token", "categories": ["cs.CV"], "comment": "Project webpage: https://glab-caltech.github.io/kyvo/", "summary": "Creating machines capable of understanding the world in 3D is essential in\nassisting designers that build and edit 3D environments and robots navigating\nand interacting within a three-dimensional space. Inspired by advances in\nlanguage and image modeling, we investigate the potential of autoregressive\nmodels for a new modality: structured 3D scenes. To this end, we propose a\nunified LLM framework that aligns language, images, and 3D scenes and provide a\ndetailed ''cookbook'' outlining critical design choices for achieving optimal\ntraining and performance addressing key questions related to data\nrepresentation, modality-specific objectives, and more. We evaluate performance\nacross four core 3D tasks -- rendering, recognition, instruction-following, and\nquestion-answering -- and four 3D datasets, synthetic and real-world. We extend\nour approach to reconstruct complex 3D object shapes by enriching our 3D\nmodality with quantized shape encodings, and show our model's effectiveness on\nreal-world 3D object recognition tasks. Project webpage:\nhttps://glab-caltech.github.io/kyvo/", "AI": {"tldr": "The paper proposes a unified LLM framework for aligning language, images, and 3D scenes, evaluating performance across core 3D tasks and datasets.", "motivation": "To advance 3D understanding for applications like design and robotics, leveraging autoregressive models inspired by language and image modeling.", "method": "A unified LLM framework with detailed design choices for data representation and modality-specific objectives, extended with quantized shape encodings.", "result": "Effective performance in rendering, recognition, instruction-following, and question-answering across synthetic and real-world datasets.", "conclusion": "The framework successfully integrates 3D scenes with language and images, demonstrating practical utility in real-world tasks."}}
{"id": "2506.07927", "pdf": "https://arxiv.org/pdf/2506.07927", "abs": "https://arxiv.org/abs/2506.07927", "authors": ["Jiayi Sheng", "Luna Lyu", "Jikai Jin", "Tony Xia", "Alex Gu", "James Zou", "Pan Lu"], "title": "Solving Inequality Proofs with Large Language Models", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "52 pages, 16 figures", "summary": "Inequality proving, crucial across diverse scientific and mathematical\nfields, tests advanced reasoning skills such as discovering tight bounds and\nstrategic theorem application. This makes it a distinct, demanding frontier for\nlarge language models (LLMs), offering insights beyond general mathematical\nproblem-solving. Progress in this area is hampered by existing datasets that\nare often scarce, synthetic, or rigidly formal. We address this by proposing an\ninformal yet verifiable task formulation, recasting inequality proving into two\nautomatically checkable subtasks: bound estimation and relation prediction.\nBuilding on this, we release IneqMath, an expert-curated dataset of\nOlympiad-level inequalities, including a test set and training corpus enriched\nwith step-wise solutions and theorem annotations. We also develop a novel\nLLM-as-judge evaluation framework, combining a final-answer judge with four\nstep-wise judges designed to detect common reasoning flaws. A systematic\nevaluation of 29 leading LLMs on IneqMath reveals a surprising reality: even\ntop models like o1 achieve less than 10% overall accuracy under step-wise\nscrutiny; this is a drop of up to 65.5% from their accuracy considering only\nfinal answer equivalence. This discrepancy exposes fragile deductive chains and\na critical gap for current LLMs between merely finding an answer and\nconstructing a rigorous proof. Scaling model size and increasing test-time\ncomputation yield limited gains in overall proof correctness. Instead, our\nfindings highlight promising research directions such as theorem-guided\nreasoning and self-refinement. Code and data are available at\nhttps://ineqmath.github.io/.", "AI": {"tldr": "The paper introduces IneqMath, a dataset for inequality proving, and evaluates LLMs, revealing their limitations in rigorous proof construction despite strong final-answer performance.", "motivation": "Inequality proving is a challenging task for LLMs, but existing datasets are limited. The paper aims to provide a better dataset and evaluation framework.", "method": "Proposes a task formulation with two subtasks (bound estimation and relation prediction), releases the IneqMath dataset, and develops an LLM-as-judge evaluation framework.", "result": "Top LLMs achieve <10% accuracy under step-wise scrutiny, showing a 65.5% drop from final-answer accuracy, highlighting proof construction weaknesses.", "conclusion": "Scaling model size has limited impact; future work should focus on theorem-guided reasoning and self-refinement."}}
{"id": "2506.08003", "pdf": "https://arxiv.org/pdf/2506.08003", "abs": "https://arxiv.org/abs/2506.08003", "authors": ["Shuchen Weng", "Haojie Zheng", "Zheng Chang", "Si Li", "Boxin Shi", "Xinlong Wang"], "title": "Audio-Sync Video Generation with Multi-Stream Temporal Control", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Audio is inherently temporal and closely synchronized with the visual world,\nmaking it a naturally aligned and expressive control signal for controllable\nvideo generation (e.g., movies). Beyond control, directly translating audio\ninto video is essential for understanding and visualizing rich audio narratives\n(e.g., Podcasts or historical recordings). However, existing approaches fall\nshort in generating high-quality videos with precise audio-visual\nsynchronization, especially across diverse and complex audio types. In this\nwork, we introduce MTV, a versatile framework for audio-sync video generation.\nMTV explicitly separates audios into speech, effects, and music tracks,\nenabling disentangled control over lip motion, event timing, and visual mood,\nrespectively -- resulting in fine-grained and semantically aligned video\ngeneration. To support the framework, we additionally present DEMIX, a dataset\ncomprising high-quality cinematic videos and demixed audio tracks. DEMIX is\nstructured into five overlapped subsets, enabling scalable multi-stage training\nfor diverse generation scenarios. Extensive experiments demonstrate that MTV\nachieves state-of-the-art performance across six standard metrics spanning\nvideo quality, text-video consistency, and audio-video alignment. Project page:\nhttps://hjzheng.net/projects/MTV/.", "AI": {"tldr": "MTV is a framework for high-quality audio-sync video generation by disentangling audio into speech, effects, and music tracks, supported by the DEMIX dataset.", "motivation": "Existing methods struggle with precise audio-visual synchronization and diverse audio types, limiting video generation quality.", "method": "MTV separates audio into speech, effects, and music tracks for fine-grained control, using the DEMIX dataset for training.", "result": "MTV achieves state-of-the-art performance in video quality, text-video consistency, and audio-video alignment.", "conclusion": "MTV advances audio-to-video generation with improved synchronization and versatility, supported by the DEMIX dataset."}}
{"id": "2506.08004", "pdf": "https://arxiv.org/pdf/2506.08004", "abs": "https://arxiv.org/abs/2506.08004", "authors": ["Hidir Yesiltepe", "Pinar Yanardag"], "title": "Dynamic View Synthesis as an Inverse Problem", "categories": ["cs.CV", "cs.AI"], "comment": "Project Page: https://inverse-dvs.github.io/", "summary": "In this work, we address dynamic view synthesis from monocular videos as an\ninverse problem in a training-free setting. By redesigning the noise\ninitialization phase of a pre-trained video diffusion model, we enable\nhigh-fidelity dynamic view synthesis without any weight updates or auxiliary\nmodules. We begin by identifying a fundamental obstacle to deterministic\ninversion arising from zero-terminal signal-to-noise ratio (SNR) schedules and\nresolve it by introducing a novel noise representation, termed K-order\nRecursive Noise Representation. We derive a closed form expression for this\nrepresentation, enabling precise and efficient alignment between the\nVAE-encoded and the DDIM inverted latents. To synthesize newly visible regions\nresulting from camera motion, we introduce Stochastic Latent Modulation, which\nperforms visibility aware sampling over the latent space to complete occluded\nregions. Comprehensive experiments demonstrate that dynamic view synthesis can\nbe effectively performed through structured latent manipulation in the noise\ninitialization phase.", "AI": {"tldr": "The paper proposes a training-free method for dynamic view synthesis from monocular videos by redesigning noise initialization in a pre-trained video diffusion model, introducing K-order Recursive Noise Representation and Stochastic Latent Modulation.", "motivation": "To enable high-fidelity dynamic view synthesis without weight updates or auxiliary modules by addressing deterministic inversion challenges.", "method": "Redesigns noise initialization with K-order Recursive Noise Representation and Stochastic Latent Modulation for visibility-aware sampling.", "result": "Demonstrates effective dynamic view synthesis through structured latent manipulation.", "conclusion": "The approach successfully achieves dynamic view synthesis by leveraging noise initialization techniques."}}
{"id": "2506.07945", "pdf": "https://arxiv.org/pdf/2506.07945", "abs": "https://arxiv.org/abs/2506.07945", "authors": ["Arnav Sheth", "Ivaxi Sheth", "Mario Fritz"], "title": "ProtocolLLM: RTL Benchmark for SystemVerilog Generation of Communication Protocols", "categories": ["cs.AR", "cs.AI", "cs.CL"], "comment": "Accepted at MLSysArch@ISCA 2025", "summary": "Recent advances in Large Language Models (LLMs) have shown promising\ncapabilities in generating code for general-purpose programming languages. In\ncontrast, their applicability for hardware description languages, particularly\nfor generating synthesizable and functionally correct designs, remains\nsignificantly underexplored. HDLs such as SystemVerilog are logic-oriented and\ndemand strict adherence to timing semantics, concurrency, and synthesizability\nconstraints. Moreover, HDL-based design flows encompass a broad set of tasks\nbeyond structural code generation, including testbench development,\nassertion-based verification, timing closure, and protocol-level integration\nfor on-chip communication. The objective of our paper is to analyze the\ncapabilities of state-of-the-art LLMs in generating SystemVerilog\nimplementations of standard communication protocols, a core component of\nembedded and System-on-Chip (SoC) architectures. This paper introduces the\nfirst benchmark suite targeting four widely used protocols: SPI, I2C, UART, and\nAXI. We define code generation tasks that capture varying levels of design\nabstraction and prompt specificity. The generated designs are assessed for\nsyntactic correctness, synthesizability, and functional fidelity via waveform\nsimulation and test benches.", "AI": {"tldr": "The paper explores the capabilities of LLMs in generating synthesizable and functionally correct SystemVerilog code for standard communication protocols, introducing a benchmark suite for evaluation.", "motivation": "The applicability of LLMs for hardware description languages (HDLs) like SystemVerilog, which require strict adherence to timing and synthesizability constraints, remains underexplored.", "method": "The paper introduces a benchmark suite for four protocols (SPI, I2C, UART, AXI) and evaluates LLM-generated SystemVerilog code for syntactic correctness, synthesizability, and functional fidelity using waveform simulation and test benches.", "result": "The study assesses the effectiveness of LLMs in generating HDL code, focusing on practical design tasks beyond structural code generation.", "conclusion": "The work provides insights into the potential and limitations of LLMs for hardware design, paving the way for future research in this domain."}}
{"id": "2506.08005", "pdf": "https://arxiv.org/pdf/2506.08005", "abs": "https://arxiv.org/abs/2506.08005", "authors": ["Lei Lai", "Zekai Yin", "Eshed Ohn-Bar"], "title": "ZeroVO: Visual Odometry with Minimal Assumptions", "categories": ["cs.CV"], "comment": null, "summary": "We introduce ZeroVO, a novel visual odometry (VO) algorithm that achieves\nzero-shot generalization across diverse cameras and environments, overcoming\nlimitations in existing methods that depend on predefined or static camera\ncalibration setups. Our approach incorporates three main innovations. First, we\ndesign a calibration-free, geometry-aware network structure capable of handling\nnoise in estimated depth and camera parameters. Second, we introduce a\nlanguage-based prior that infuses semantic information to enhance robust\nfeature extraction and generalization to previously unseen domains. Third, we\ndevelop a flexible, semi-supervised training paradigm that iteratively adapts\nto new scenes using unlabeled data, further boosting the models' ability to\ngeneralize across diverse real-world scenarios. We analyze complex autonomous\ndriving contexts, demonstrating over 30% improvement against prior methods on\nthree standard benchmarks, KITTI, nuScenes, and Argoverse 2, as well as a newly\nintroduced, high-fidelity synthetic dataset derived from Grand Theft Auto\n(GTA). By not requiring fine-tuning or camera calibration, our work broadens\nthe applicability of VO, providing a versatile solution for real-world\ndeployment at scale.", "AI": {"tldr": "ZeroVO is a visual odometry algorithm achieving zero-shot generalization across diverse cameras and environments without predefined calibration. It uses a geometry-aware network, language-based priors, and semi-supervised training, outperforming prior methods by over 30% on benchmarks.", "motivation": "Existing VO methods rely on predefined or static camera setups, limiting their adaptability. ZeroVO aims to overcome this by enabling generalization across diverse, unseen domains.", "method": "1. Calibration-free, geometry-aware network. 2. Language-based semantic priors for robust feature extraction. 3. Semi-supervised training with unlabeled data for iterative adaptation.", "result": "Over 30% improvement on KITTI, nuScenes, Argoverse 2, and a GTA-derived synthetic dataset.", "conclusion": "ZeroVO eliminates the need for fine-tuning or calibration, offering a scalable, versatile solution for real-world VO deployment."}}
{"id": "2506.07963", "pdf": "https://arxiv.org/pdf/2506.07963", "abs": "https://arxiv.org/abs/2506.07963", "authors": ["Jixiang Hong", "Yiran Zhang", "Guanzhong Wang", "Yi Liu", "Ji-Rong Wen", "Rui Yan"], "title": "Reinforcing Multimodal Understanding and Generation with Dual Self-rewards", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Building upon large language models (LLMs), recent large multimodal models\n(LMMs) unify cross-model understanding and generation into a single framework.\nHowever, LMMs still struggle to achieve accurate image-text alignment, prone to\ngenerating text responses contradicting the visual input or failing to follow\nthe text-to-image prompts. Current solutions require external supervision\n(e.g., human feedback or reward models) and only address unidirectional\ntasks-either understanding or generation. In this work, based on the\nobservation that understanding and generation are inverse dual tasks, we\nintroduce a self-supervised dual reward mechanism to reinforce the\nunderstanding and generation capabilities of LMMs. Specifically, we sample\nmultiple outputs for a given input in one task domain, then reverse the\ninput-output pairs to compute the dual likelihood of the model as self-rewards\nfor optimization. Extensive experimental results on visual understanding and\ngeneration benchmarks demonstrate that our method can effectively enhance the\nperformance of the model without any external supervision, especially achieving\nremarkable improvements in text-to-image tasks.", "AI": {"tldr": "A self-supervised dual reward mechanism improves large multimodal models (LMMs) by reinforcing understanding and generation capabilities without external supervision.", "motivation": "LMMs struggle with accurate image-text alignment and require external supervision for unidirectional tasks. Understanding and generation are inverse dual tasks, inspiring a self-supervised solution.", "method": "A dual reward mechanism samples multiple outputs for a given input, reverses input-output pairs, and computes dual likelihood as self-rewards for optimization.", "result": "The method enhances LMM performance, especially in text-to-image tasks, without external supervision.", "conclusion": "The self-supervised dual reward mechanism effectively improves LMMs' understanding and generation capabilities."}}
{"id": "2506.08006", "pdf": "https://arxiv.org/pdf/2506.08006", "abs": "https://arxiv.org/abs/2506.08006", "authors": ["Sicheng Mo", "Ziyang Leng", "Leon Liu", "Weizhen Wang", "Honglin He", "Bolei Zhou"], "title": "Dreamland: Controllable World Creation with Simulator and Generative Models", "categories": ["cs.CV"], "comment": "Project Page: https://metadriverse.github.io/dreamland/", "summary": "Large-scale video generative models can synthesize diverse and realistic\nvisual content for dynamic world creation, but they often lack element-wise\ncontrollability, hindering their use in editing scenes and training embodied AI\nagents. We propose Dreamland, a hybrid world generation framework combining the\ngranular control of a physics-based simulator and the photorealistic content\noutput of large-scale pretrained generative models. In particular, we design a\nlayered world abstraction that encodes both pixel-level and object-level\nsemantics and geometry as an intermediate representation to bridge the\nsimulator and the generative model. This approach enhances controllability,\nminimizes adaptation cost through early alignment with real-world\ndistributions, and supports off-the-shelf use of existing and future pretrained\ngenerative models. We further construct a D3Sim dataset to facilitate the\ntraining and evaluation of hybrid generation pipelines. Experiments demonstrate\nthat Dreamland outperforms existing baselines with 50.8% improved image\nquality, 17.9% stronger controllability, and has great potential to enhance\nembodied agent training. Code and data will be made available.", "AI": {"tldr": "Dreamland is a hybrid framework combining physics-based simulators and generative models for controllable, photorealistic video generation, outperforming baselines in quality and controllability.", "motivation": "Existing large-scale video generative models lack element-wise controllability, limiting their use in scene editing and AI agent training.", "method": "Dreamland uses a layered world abstraction to bridge simulators and generative models, encoding pixel-level and object-level semantics for enhanced control.", "result": "Dreamland improves image quality by 50.8% and controllability by 17.9%, with potential for embodied agent training.", "conclusion": "Dreamland offers a scalable, adaptable solution for hybrid world generation, with code and data to be released."}}
{"id": "2506.07972", "pdf": "https://arxiv.org/pdf/2506.07972", "abs": "https://arxiv.org/abs/2506.07972", "authors": ["Hongzheng Chen", "Yingheng Wang", "Yaohui Cai", "Hins Hu", "Jiajie Li", "Shirley Huang", "Chenhui Deng", "Rongjian Liang", "Shufeng Kong", "Haoxing Ren", "Samitha Samaranayake", "Carla P. Gomes", "Zhiru Zhang"], "title": "HeuriGym: An Agentic Benchmark for LLM-Crafted Heuristics in Combinatorial Optimization", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "While Large Language Models (LLMs) have demonstrated significant advancements\nin reasoning and agent-based problem-solving, current evaluation methodologies\nfail to adequately assess their capabilities: existing benchmarks either rely\non closed-ended questions prone to saturation and memorization, or subjective\ncomparisons that lack consistency and rigor. In this work, we introduce\nHeuriGym, an agentic framework designed for evaluating heuristic algorithms\ngenerated by LLMs for combinatorial optimization problems, characterized by\nclearly defined objectives and expansive solution spaces. HeuriGym empowers\nLLMs to propose heuristics, receive evaluative feedback via code execution, and\niteratively refine their solutions. We evaluate nine state-of-the-art models on\nnine problems across domains such as computer systems, logistics, and biology,\nexposing persistent limitations in tool use, planning, and adaptive reasoning.\nTo quantify performance, we propose the Quality-Yield Index (QYI), a metric\nthat captures both solution pass rate and quality. Even top models like\nGPT-o4-mini-high and Gemini-2.5-Pro attain QYI scores of only 0.6, well below\nthe expert baseline of 1. Our open-source benchmark aims to guide the\ndevelopment of LLMs toward more effective and realistic problem-solving in\nscientific and engineering domains.", "AI": {"tldr": "HeuriGym is introduced as a framework to evaluate LLM-generated heuristic algorithms for combinatorial optimization, addressing gaps in current benchmarks. It uses iterative refinement and a new metric, QYI, revealing LLM limitations.", "motivation": "Current LLM evaluation methods are inadequate\u2014either too closed-ended or subjective. HeuriGym aims to provide a rigorous, objective benchmark for heuristic algorithm generation.", "method": "HeuriGym allows LLMs to propose heuristics, receive feedback via code execution, and refine solutions iteratively. Nine models are tested on nine problems.", "result": "Top models score 0.6 on QYI (expert baseline: 1), highlighting limitations in tool use, planning, and adaptive reasoning.", "conclusion": "HeuriGym offers a standardized benchmark to improve LLM problem-solving in scientific and engineering domains."}}
{"id": "2506.08008", "pdf": "https://arxiv.org/pdf/2506.08008", "abs": "https://arxiv.org/abs/2506.08008", "authors": ["Stephanie Fu", "Tyler Bonnen", "Devin Guillory", "Trevor Darrell"], "title": "Hidden in plain sight: VLMs overlook their visual representations", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Project page: https://hidden-plain-sight.github.io/", "summary": "Language provides a natural interface to specify and evaluate performance on\nvisual tasks. To realize this possibility, vision language models (VLMs) must\nsuccessfully integrate visual and linguistic information. Our work compares\nVLMs to a direct readout of their visual encoders to understand their ability\nto integrate across these modalities. Across a series of vision-centric\nbenchmarks (e.g., depth estimation, correspondence), we find that VLMs perform\nsubstantially worse than their visual encoders, dropping to near-chance\nperformance. We investigate these results through a series of analyses across\nthe entire VLM: namely 1) the degradation of vision representations, 2)\nbrittleness to task prompt, and 3) the language model's role in solving the\ntask. We find that the bottleneck in performing these vision-centric tasks lies\nin this third category; VLMs are not effectively using visual information\neasily accessible throughout the entire model, and they inherit the language\npriors present in the LLM. Our work helps diagnose the failure modes of\nopen-source VLMs, and presents a series of evaluations useful for future\ninvestigations into visual understanding within VLMs.", "AI": {"tldr": "VLMs perform worse than their visual encoders on vision-centric tasks due to ineffective integration of visual information and language priors.", "motivation": "To understand how VLMs integrate visual and linguistic information and diagnose their failure modes in vision-centric tasks.", "method": "Compare VLMs to their visual encoders across vision benchmarks, analyze degradation of vision representations, task prompt brittleness, and the language model's role.", "result": "VLMs perform substantially worse, near-chance, due to ineffective use of visual information and inherited language priors.", "conclusion": "The bottleneck lies in VLMs' inability to leverage visual information effectively, highlighting the need for better integration methods."}}
{"id": "2506.07982", "pdf": "https://arxiv.org/pdf/2506.07982", "abs": "https://arxiv.org/abs/2506.07982", "authors": ["Victor Barres", "Honghua Dong", "Soham Ray", "Xujie Si", "Karthik Narasimhan"], "title": "$\u03c4^2$-Bench: Evaluating Conversational Agents in a Dual-Control Environment", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Existing benchmarks for conversational AI agents simulate single-control\nenvironments, where only the AI agent can use tools to interact with the world,\nwhile the user remains a passive information provider. This differs from\nreal-world scenarios like technical support, where users need to actively\nparticipate in modifying the state of the (shared) world. In order to address\nthis gap, we introduce $\\tau^2$-bench, with four key contributions:\n  1) A novel Telecom dual-control domain modeled as a Dec-POMDP, where both\nagent and user make use of tools to act in a shared, dynamic environment that\ntests both agent coordination and communication,\n  2) A compositional task generator that programmatically creates diverse,\nverifiable tasks from atomic components, ensuring domain coverage and\ncontrolled complexity,\n  3) A reliable user simulator tightly coupled with the environment, whose\nbehavior is constrained by tools and observable states, improving simulation\nfidelity,\n  4) Fine-grained analysis of agent performance through multiple ablations\nincluding separating errors arising from reasoning vs\ncommunication/coordination.\n  In particular, our experiments show significant performance drops when agents\nshift from no-user to dual-control, highlighting the challenges of guiding\nusers. Overall, $\\tau^2$-bench provides a controlled testbed for agents that\nmust both reason effectively and guide user actions.", "AI": {"tldr": "The paper introduces $\tau^2$-bench, a benchmark for conversational AI agents in dual-control environments, addressing gaps in existing single-control benchmarks. It includes a Telecom domain, task generator, user simulator, and performance analysis.", "motivation": "Existing benchmarks simulate single-control environments, lacking realism for scenarios like technical support where users actively participate. $\tau^2$-bench aims to bridge this gap.", "method": "The benchmark features a Telecom dual-control domain modeled as a Dec-POMDP, a compositional task generator, a reliable user simulator, and fine-grained performance analysis.", "result": "Experiments show significant performance drops when agents shift from no-user to dual-control, highlighting challenges in guiding users.", "conclusion": "$\tau^2$-bench provides a controlled testbed for agents needing effective reasoning and user guidance."}}
{"id": "2506.08009", "pdf": "https://arxiv.org/pdf/2506.08009", "abs": "https://arxiv.org/abs/2506.08009", "authors": ["Xun Huang", "Zhengqi Li", "Guande He", "Mingyuan Zhou", "Eli Shechtman"], "title": "Self Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Project website: http://self-forcing.github.io/", "summary": "We introduce Self Forcing, a novel training paradigm for autoregressive video\ndiffusion models. It addresses the longstanding issue of exposure bias, where\nmodels trained on ground-truth context must generate sequences conditioned on\ntheir own imperfect outputs during inference. Unlike prior methods that denoise\nfuture frames based on ground-truth context frames, Self Forcing conditions\neach frame's generation on previously self-generated outputs by performing\nautoregressive rollout with key-value (KV) caching during training. This\nstrategy enables supervision through a holistic loss at the video level that\ndirectly evaluates the quality of the entire generated sequence, rather than\nrelying solely on traditional frame-wise objectives. To ensure training\nefficiency, we employ a few-step diffusion model along with a stochastic\ngradient truncation strategy, effectively balancing computational cost and\nperformance. We further introduce a rolling KV cache mechanism that enables\nefficient autoregressive video extrapolation. Extensive experiments demonstrate\nthat our approach achieves real-time streaming video generation with sub-second\nlatency on a single GPU, while matching or even surpassing the generation\nquality of significantly slower and non-causal diffusion models. Project\nwebsite: http://self-forcing.github.io/", "AI": {"tldr": "Self Forcing is a new training method for autoregressive video diffusion models that reduces exposure bias by conditioning frame generation on self-generated outputs, improving video-level quality and enabling real-time generation.", "motivation": "Addresses exposure bias in autoregressive video diffusion models, where models struggle with imperfect outputs during inference.", "method": "Uses autoregressive rollout with KV caching during training, a few-step diffusion model, stochastic gradient truncation, and rolling KV cache for efficiency.", "result": "Achieves real-time video generation with sub-second latency on a single GPU, matching or surpassing slower non-causal models.", "conclusion": "Self Forcing effectively balances performance and computational cost, advancing autoregressive video diffusion models."}}
{"id": "2506.08001", "pdf": "https://arxiv.org/pdf/2506.08001", "abs": "https://arxiv.org/abs/2506.08001", "authors": ["Zeju Qiu", "Simon Buchholz", "Tim Z. Xiao", "Maximilian Dax", "Bernhard Sch\u00f6lkopf", "Weiyang Liu"], "title": "Reparameterized LLM Training via Orthogonal Equivalence Transformation", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Technical report v1 (36 pages, 24 figures, project page:\n  https://spherelab.ai/poet-site/)", "summary": "While large language models (LLMs) are driving the rapid advancement of\nartificial intelligence, effectively and reliably training these large models\nremains one of the field's most significant challenges. To address this\nchallenge, we propose POET, a novel reParameterized training algorithm that\nuses Orthogonal Equivalence Transformation to optimize neurons. Specifically,\nPOET reparameterizes each neuron with two learnable orthogonal matrices and a\nfixed random weight matrix. Because of its provable preservation of spectral\nproperties of weight matrices, POET can stably optimize the objective function\nwith improved generalization. We further develop efficient approximations that\nmake POET flexible and scalable for training large-scale neural networks.\nExtensive experiments validate the effectiveness and scalability of POET in\ntraining LLMs.", "AI": {"tldr": "POET is a reParameterized training algorithm using Orthogonal Equivalence Transformation to optimize neurons, improving generalization and scalability for LLMs.", "motivation": "Effectively and reliably training large language models (LLMs) is a major challenge in AI.", "method": "POET reparameterizes neurons with two learnable orthogonal matrices and a fixed random weight matrix, preserving spectral properties for stable optimization.", "result": "POET shows improved generalization and scalability in training LLMs, validated by extensive experiments.", "conclusion": "POET is an effective and scalable solution for training large-scale neural networks, addressing key challenges in LLM optimization."}}
{"id": "2506.08010", "pdf": "https://arxiv.org/pdf/2506.08010", "abs": "https://arxiv.org/abs/2506.08010", "authors": ["Nick Jiang", "Amil Dravid", "Alexei Efros", "Yossi Gandelsman"], "title": "Vision Transformers Don't Need Trained Registers", "categories": ["cs.CV", "cs.AI"], "comment": "Project page and code: https://avdravid.github.io/test-time-registers", "summary": "We investigate the mechanism underlying a previously identified phenomenon in\nVision Transformers -- the emergence of high-norm tokens that lead to noisy\nattention maps. We observe that in multiple models (e.g., CLIP, DINOv2), a\nsparse set of neurons is responsible for concentrating high-norm activations on\noutlier tokens, leading to irregular attention patterns and degrading\ndownstream visual processing. While the existing solution for removing these\noutliers involves retraining models from scratch with additional learned\nregister tokens, we use our findings to create a training-free approach to\nmitigate these artifacts. By shifting the high-norm activations from our\ndiscovered register neurons into an additional untrained token, we can mimic\nthe effect of register tokens on a model already trained without registers. We\ndemonstrate that our method produces cleaner attention and feature maps,\nenhances performance over base models across multiple downstream visual tasks,\nand achieves results comparable to models explicitly trained with register\ntokens. We then extend test-time registers to off-the-shelf vision-language\nmodels to improve their interpretability. Our results suggest that test-time\nregisters effectively take on the role of register tokens at test-time,\noffering a training-free solution for any pre-trained model released without\nthem.", "AI": {"tldr": "The paper investigates high-norm tokens in Vision Transformers, identifies neurons causing irregular attention, and proposes a training-free method to mitigate this by shifting activations to an untrained token, improving performance and interpretability.", "motivation": "To address noisy attention maps caused by high-norm tokens in Vision Transformers without requiring retraining.", "method": "Shifts high-norm activations from identified neurons to an untrained token, mimicking register tokens' effect.", "result": "Cleaner attention maps, improved performance in downstream tasks, and comparable results to models trained with register tokens.", "conclusion": "Test-time registers offer a training-free solution for pre-trained models, enhancing interpretability and performance."}}
{"id": "2506.08011", "pdf": "https://arxiv.org/pdf/2506.08011", "abs": "https://arxiv.org/abs/2506.08011", "authors": ["Yunfei Xie", "Yinsong Ma", "Shiyi Lan", "Alan Yuille", "Junfei Xiao", "Chen Wei"], "title": "Play to Generalize: Learning to Reason Through Game Play", "categories": ["cs.CV", "cs.CL"], "comment": "Project Page: https://yunfeixie233.github.io/ViGaL/", "summary": "Developing generalizable reasoning capabilities in multimodal large language\nmodels (MLLMs) remains challenging. Motivated by cognitive science literature\nsuggesting that gameplay promotes transferable cognitive skills, we propose a\nnovel post-training paradigm, Visual Game Learning, or ViGaL, where MLLMs\ndevelop out-of-domain generalization of multimodal reasoning through playing\narcade-like games. Specifically, we show that post-training a 7B-parameter MLLM\nvia reinforcement learning (RL) on simple arcade-like games, e.g. Snake,\nsignificantly enhances its downstream performance on multimodal math benchmarks\nlike MathVista, and on multi-discipline questions like MMMU, without seeing any\nworked solutions, equations, or diagrams during RL, suggesting the capture of\ntransferable reasoning skills. Remarkably, our model outperforms specialist\nmodels tuned on multimodal reasoning data in multimodal reasoning benchmarks,\nwhile preserving the base model's performance on general visual benchmarks, a\nchallenge where specialist models often fall short. Our findings suggest a new\npost-training paradigm: synthetic, rule-based games can serve as controllable\nand scalable pre-text tasks that unlock generalizable multimodal reasoning\nabilities in MLLMs.", "AI": {"tldr": "ViGaL post-trains MLLMs via gameplay (e.g., Snake) using RL, enhancing multimodal reasoning without domain-specific data, outperforming specialist models.", "motivation": "To improve generalizable reasoning in MLLMs, inspired by cognitive science linking gameplay to transferable skills.", "method": "Post-training a 7B-parameter MLLM with RL on arcade-like games (e.g., Snake) to enhance reasoning.", "result": "Outperforms specialist models on multimodal benchmarks (e.g., MathVista, MMMU) while maintaining general visual performance.", "conclusion": "Rule-based games are scalable pre-text tasks for unlocking generalizable reasoning in MLLMs."}}
{"id": "2506.08013", "pdf": "https://arxiv.org/pdf/2506.08013", "abs": "https://arxiv.org/abs/2506.08013", "authors": ["Anh-Quan Cao", "Ivan Lopes", "Raoul de Charette"], "title": "StableMTL: Repurposing Latent Diffusion Models for Multi-Task Learning from Partially Annotated Synthetic Datasets", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Code is available at https://github.com/astra-vision/StableMTL", "summary": "Multi-task learning for dense prediction is limited by the need for extensive\nannotation for every task, though recent works have explored training with\npartial task labels. Leveraging the generalization power of diffusion models,\nwe extend the partial learning setup to a zero-shot setting, training a\nmulti-task model on multiple synthetic datasets, each labeled for only a subset\nof tasks. Our method, StableMTL, repurposes image generators for latent\nregression. Adapting a denoising framework with task encoding, per-task\nconditioning and a tailored training scheme. Instead of per-task losses\nrequiring careful balancing, a unified latent loss is adopted, enabling\nseamless scaling to more tasks. To encourage inter-task synergy, we introduce a\nmulti-stream model with a task-attention mechanism that converts N-to-N task\ninteractions into efficient 1-to-N attention, promoting effective cross-task\nsharing. StableMTL outperforms baselines on 7 tasks across 8 benchmarks.", "AI": {"tldr": "StableMTL leverages diffusion models for zero-shot multi-task learning, using synthetic datasets and a unified latent loss to scale tasks without balancing. It introduces a task-attention mechanism for cross-task synergy, outperforming baselines on 7 tasks.", "motivation": "Multi-task learning for dense prediction is limited by the need for extensive annotation. This work explores training with partial task labels and extends it to a zero-shot setting.", "method": "StableMTL repurposes image generators for latent regression, using a denoising framework with task encoding and per-task conditioning. It adopts a unified latent loss and introduces a multi-stream model with task-attention for inter-task synergy.", "result": "StableMTL outperforms baselines on 7 tasks across 8 benchmarks.", "conclusion": "The method successfully scales multi-task learning without per-task loss balancing, demonstrating the potential of diffusion models in zero-shot settings."}}
{"id": "2506.08015", "pdf": "https://arxiv.org/pdf/2506.08015", "abs": "https://arxiv.org/abs/2506.08015", "authors": ["Zhen Xu", "Zhengqin Li", "Zhao Dong", "Xiaowei Zhou", "Richard Newcombe", "Zhaoyang Lv"], "title": "4DGT: Learning a 4D Gaussian Transformer Using Real-World Monocular Videos", "categories": ["cs.CV"], "comment": "Project page: https://4dgt.github.io", "summary": "We propose 4DGT, a 4D Gaussian-based Transformer model for dynamic scene\nreconstruction, trained entirely on real-world monocular posed videos. Using 4D\nGaussian as an inductive bias, 4DGT unifies static and dynamic components,\nenabling the modeling of complex, time-varying environments with varying object\nlifespans. We proposed a novel density control strategy in training, which\nenables our 4DGT to handle longer space-time input and remain efficient\nrendering at runtime. Our model processes 64 consecutive posed frames in a\nrolling-window fashion, predicting consistent 4D Gaussians in the scene. Unlike\noptimization-based methods, 4DGT performs purely feed-forward inference,\nreducing reconstruction time from hours to seconds and scaling effectively to\nlong video sequences. Trained only on large-scale monocular posed video\ndatasets, 4DGT can outperform prior Gaussian-based networks significantly in\nreal-world videos and achieve on-par accuracy with optimization-based methods\non cross-domain videos. Project page: https://4dgt.github.io", "AI": {"tldr": "4DGT is a 4D Gaussian-based Transformer model for dynamic scene reconstruction from monocular videos, offering fast inference and scalability.", "motivation": "To unify static and dynamic scene components and efficiently model complex, time-varying environments with varying object lifespans.", "method": "Uses 4D Gaussian as an inductive bias and a novel density control strategy for training, enabling efficient handling of long space-time inputs.", "result": "Outperforms prior Gaussian-based networks and matches optimization-based methods in accuracy while reducing reconstruction time from hours to seconds.", "conclusion": "4DGT is a scalable, efficient solution for dynamic scene reconstruction, suitable for real-world and cross-domain videos."}}
{"id": "2506.06290", "pdf": "https://arxiv.org/pdf/2506.06290", "abs": "https://arxiv.org/abs/2506.06290", "authors": ["Mingyu Lu", "Ethan Weinberger", "Chanwoo Kim", "Su-In Lee"], "title": "CellCLIP -- Learning Perturbation Effects in Cell Painting via Text-Guided Contrastive Learning", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "High-content screening (HCS) assays based on high-throughput microscopy\ntechniques such as Cell Painting have enabled the interrogation of cells'\nmorphological responses to perturbations at an unprecedented scale. The\ncollection of such data promises to facilitate a better understanding of the\nrelationships between different perturbations and their effects on cellular\nstate. Towards achieving this goal, recent advances in cross-modal contrastive\nlearning could, in theory, be leveraged to learn a unified latent space that\naligns perturbations with their corresponding morphological effects. However,\nthe application of such methods to HCS data is not straightforward due to\nsubstantial differences in the semantics of Cell Painting images compared to\nnatural images, and the difficulty of representing different classes of\nperturbations (e.g., small molecule vs CRISPR gene knockout) in a single latent\nspace. In response to these challenges, here we introduce CellCLIP, a\ncross-modal contrastive learning framework for HCS data. CellCLIP leverages\npre-trained image encoders coupled with a novel channel encoding scheme to\nbetter capture relationships between different microscopy channels in image\nembeddings, along with natural language encoders for representing\nperturbations. Our framework outperforms current open-source models,\ndemonstrating the best performance in both cross-modal retrieval and\nbiologically meaningful downstream tasks while also achieving significant\nreductions in computation time.", "AI": {"tldr": "CellCLIP is a cross-modal contrastive learning framework for high-content screening (HCS) data, improving alignment of perturbations with cellular effects and outperforming existing models.", "motivation": "To address challenges in applying cross-modal contrastive learning to HCS data due to semantic differences in Cell Painting images and diverse perturbation classes.", "method": "Uses pre-trained image encoders with a novel channel encoding scheme and natural language encoders for perturbations.", "result": "Outperforms current models in cross-modal retrieval and downstream tasks, with reduced computation time.", "conclusion": "CellCLIP effectively aligns perturbations with morphological effects, advancing HCS data analysis."}}
{"id": "2506.06306", "pdf": "https://arxiv.org/pdf/2506.06306", "abs": "https://arxiv.org/abs/2506.06306", "authors": ["Ali Abedi", "Charlene H. Chu", "Shehroz S. Khan"], "title": "Benchmarking Early Agitation Prediction in Community-Dwelling People with Dementia Using Multimodal Sensors and Machine Learning", "categories": ["eess.SP", "cs.CV", "cs.HC", "cs.LG"], "comment": "16 pages, 4 figures, 2 tables", "summary": "Agitation is one of the most common responsive behaviors in people living\nwith dementia, particularly among those residing in community settings without\ncontinuous clinical supervision. Timely prediction of agitation can enable\nearly intervention, reduce caregiver burden, and improve the quality of life\nfor both patients and caregivers. This study aimed to develop and benchmark\nmachine learning approaches for the early prediction of agitation in\ncommunity-dwelling older adults with dementia using multimodal sensor data. A\nnew set of agitation-related contextual features derived from activity data was\nintroduced and employed for agitation prediction. A wide range of machine\nlearning and deep learning models was evaluated across multiple problem\nformulations, including binary classification for single-timestamp tabular\nsensor data and multi-timestamp sequential sensor data, as well as anomaly\ndetection for single-timestamp tabular sensor data. The study utilized the\nTechnology Integrated Health Management (TIHM) dataset, the largest publicly\navailable dataset for remote monitoring of people living with dementia,\ncomprising 2,803 days of in-home activity, physiology, and sleep data. The most\neffective setting involved binary classification of sensor data using the\ncurrent 6-hour timestamp to predict agitation at the subsequent timestamp.\nIncorporating additional information, such as time of day and agitation\nhistory, further improved model performance, with the highest AUC-ROC of 0.9720\nand AUC-PR of 0.4320 achieved by the light gradient boosting machine. This work\npresents the first comprehensive benchmarking of state-of-the-art techniques\nfor agitation prediction in community-based dementia care using\nprivacy-preserving sensor data. The approach enables accurate, explainable, and\nefficient agitation prediction, supporting proactive dementia care and aging in\nplace.", "AI": {"tldr": "The study developed machine learning models to predict agitation in dementia patients using multimodal sensor data, achieving high accuracy with a light gradient boosting machine.", "motivation": "Agitation in dementia patients is common and disruptive; early prediction can improve care and reduce caregiver burden.", "method": "Used machine learning and deep learning models on the TIHM dataset, introducing agitation-related contextual features from activity data.", "result": "Best performance was binary classification with AUC-ROC of 0.9720 and AUC-PR of 0.4320 using light gradient boosting.", "conclusion": "The approach enables accurate, explainable agitation prediction, supporting proactive dementia care."}}
{"id": "2506.06315", "pdf": "https://arxiv.org/pdf/2506.06315", "abs": "https://arxiv.org/abs/2506.06315", "authors": ["Masoud Rahimi", "Reza Karbasi", "Abdol-Hossein Vahabie"], "title": "An Open-Source Python Framework and Synthetic ECG Image Datasets for Digitization, Lead and Lead Name Detection, and Overlapping Signal Segmentation", "categories": ["eess.SP", "cs.CV", "cs.LG"], "comment": "5 pages, 5 figures", "summary": "We introduce an open-source Python framework for generating synthetic ECG\nimage datasets to advance critical deep learning-based tasks in ECG analysis,\nincluding ECG digitization, lead region and lead name detection, and\npixel-level waveform segmentation. Using the PTB-XL signal dataset, our\nproposed framework produces four open-access datasets: (1) ECG images in\nvarious lead configurations paired with time-series signals for ECG\ndigitization, (2) ECG images annotated with YOLO-format bounding boxes for\ndetection of lead region and lead name, (3)-(4) cropped single-lead images with\nsegmentation masks compatible with U-Net-based models in normal and overlapping\nversions. In the overlapping case, waveforms from neighboring leads are\nsuperimposed onto the target lead image, while the segmentation masks remain\nclean. The open-source Python framework and datasets are publicly available at\nhttps://github.com/rezakarbasi/ecg-image-and-signal-dataset and\nhttps://doi.org/10.5281/zenodo.15484519, respectively.", "AI": {"tldr": "An open-source Python framework generates synthetic ECG image datasets for deep learning tasks like ECG digitization, lead detection, and waveform segmentation.", "motivation": "To advance deep learning-based ECG analysis by providing diverse synthetic datasets for critical tasks.", "method": "Uses the PTB-XL signal dataset to create four datasets: paired ECG images and signals, annotated images for detection, and cropped single-lead images with segmentation masks.", "result": "Produces open-access datasets for ECG digitization, lead detection, and segmentation, with overlapping waveform support.", "conclusion": "The framework and datasets are publicly available, facilitating research in ECG analysis."}}
{"id": "2506.06349", "pdf": "https://arxiv.org/pdf/2506.06349", "abs": "https://arxiv.org/abs/2506.06349", "authors": ["Thien Nhan Vo", "Thanh Xuan Truong"], "title": "Heart Rate Classification in ECG Signals Using Machine Learning and Deep Learning", "categories": ["eess.SP", "cs.CV", "cs.LG"], "comment": null, "summary": "This study addresses the classification of heartbeats from ECG signals\nthrough two distinct approaches: traditional machine learning utilizing\nhand-crafted features and deep learning via transformed images of ECG beats.\nThe dataset underwent preprocessing steps, including downsampling, filtering,\nand normalization, to ensure consistency and relevance for subsequent analysis.\nIn the first approach, features such as heart rate variability (HRV), mean,\nvariance, and RR intervals were extracted to train various classifiers,\nincluding SVM, Random Forest, AdaBoost, LSTM, Bi-directional LSTM, and\nLightGBM. The second approach involved transforming ECG signals into images\nusing Gramian Angular Field (GAF), Markov Transition Field (MTF), and\nRecurrence Plots (RP), with these images subsequently classified using CNN\narchitectures like VGG and Inception.\n  Experimental results demonstrate that the LightGBM model achieved the highest\nperformance, with an accuracy of 99% and an F1 score of 0.94, outperforming the\nimage-based CNN approach (F1 score of 0.85). Models such as SVM and AdaBoost\nyielded significantly lower scores, indicating limited suitability for this\ntask. The findings underscore the superior ability of hand-crafted features to\ncapture temporal and morphological variations in ECG signals compared to\nimage-based representations of individual beats. Future investigations may\nbenefit from incorporating multi-lead ECG signals and temporal dependencies\nacross successive beats to enhance classification accuracy further.", "AI": {"tldr": "The study compares traditional ML (hand-crafted features) and deep learning (image-based CNN) for ECG heartbeat classification. LightGBM outperformed CNNs, achieving 99% accuracy.", "motivation": "To evaluate the effectiveness of hand-crafted features versus image-based deep learning for ECG heartbeat classification.", "method": "Two approaches: 1) Traditional ML with features like HRV and RR intervals, trained on SVM, Random Forest, etc. 2) Deep learning using GAF, MTF, and RP images classified by CNNs.", "result": "LightGBM (hand-crafted features) achieved 99% accuracy, outperforming CNN (F1=0.85). SVM and AdaBoost performed poorly.", "conclusion": "Hand-crafted features are superior for ECG classification. Future work should explore multi-lead signals and temporal dependencies."}}
{"id": "2506.06394", "pdf": "https://arxiv.org/pdf/2506.06394", "abs": "https://arxiv.org/abs/2506.06394", "authors": ["Yash Turkar", "Youngjin Kim", "Karthik Dantu"], "title": "Active Illumination Control in Low-Light Environments using NightHawk", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Subterranean environments such as culverts present significant challenges to\nrobot vision due to dim lighting and lack of distinctive features. Although\nonboard illumination can help, it introduces issues such as specular\nreflections, overexposure, and increased power consumption. We propose\nNightHawk, a framework that combines active illumination with exposure control\nto optimize image quality in these settings. NightHawk formulates an online\nBayesian optimization problem to determine the best light intensity and\nexposure-time for a given scene. We propose a novel feature detector-based\nmetric to quantify image utility and use it as the cost function for the\noptimizer. We built NightHawk as an event-triggered recursive optimization\npipeline and deployed it on a legged robot navigating a culvert beneath the\nErie Canal. Results from field experiments demonstrate improvements in feature\ndetection and matching by 47-197% enabling more reliable visual estimation in\nchallenging lighting conditions.", "AI": {"tldr": "NightHawk optimizes robot vision in dim, featureless subterranean environments using active illumination and exposure control, improving feature detection by 47-197%.", "motivation": "Subterranean environments like culverts challenge robot vision due to poor lighting and lack of features, while onboard illumination causes issues like reflections and power drain.", "method": "NightHawk uses Bayesian optimization to adjust light intensity and exposure-time, with a feature detector-based metric for image utility. It's implemented as an event-triggered recursive pipeline.", "result": "Field tests showed 47-197% improvement in feature detection and matching, enhancing visual estimation in tough lighting.", "conclusion": "NightHawk effectively addresses subterranean vision challenges, improving reliability for robots in such environments."}}
{"id": "2506.06400", "pdf": "https://arxiv.org/pdf/2506.06400", "abs": "https://arxiv.org/abs/2506.06400", "authors": ["Changsheng Fang", "Yongtong Liu", "Bahareh Morovati", "Shuo Han", "Yu Shi", "Li Zhou", "Shuyi Fan", "Hengyong Yu"], "title": "ResPF: Residual Poisson Flow for Efficient and Physically Consistent Sparse-View CT Reconstruction", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Sparse-view computed tomography (CT) is a practical solution to reduce\nradiation dose, but the resulting ill-posed inverse problem poses significant\nchallenges for accurate image reconstruction. Although deep learning and\ndiffusion-based methods have shown promising results, they often lack physical\ninterpretability or suffer from high computational costs due to iterative\nsampling starting from random noise. Recent advances in generative modeling,\nparticularly Poisson Flow Generative Models (PFGM), enable high-fidelity image\nsynthesis by modeling the full data distribution. In this work, we propose\nResidual Poisson Flow (ResPF) Generative Models for efficient and accurate\nsparse-view CT reconstruction. Based on PFGM++, ResPF integrates conditional\nguidance from sparse measurements and employs a hijacking strategy to\nsignificantly reduce sampling cost by skipping redundant initial steps.\nHowever, skipping early stages can degrade reconstruction quality and introduce\nunrealistic structures. To address this, we embed a data-consistency into each\niteration, ensuring fidelity to sparse-view measurements. Yet, PFGM sampling\nrelies on a fixed ordinary differential equation (ODE) trajectory induced by\nelectrostatic fields, which can be disrupted by step-wise data consistency,\nresulting in unstable or degraded reconstructions. Inspired by ResNet, we\nintroduce a residual fusion module to linearly combine generative outputs with\ndata-consistent reconstructions, effectively preserving trajectory continuity.\nTo the best of our knowledge, this is the first application of Poisson flow\nmodels to sparse-view CT. Extensive experiments on synthetic and clinical\ndatasets demonstrate that ResPF achieves superior reconstruction quality,\nfaster inference, and stronger robustness compared to state-of-the-art\niterative, learning-based, and diffusion models.", "AI": {"tldr": "Residual Poisson Flow (ResPF) models improve sparse-view CT reconstruction by integrating conditional guidance, reducing sampling costs, and ensuring data consistency, outperforming existing methods.", "motivation": "Sparse-view CT reduces radiation but poses reconstruction challenges. Current deep learning and diffusion methods lack interpretability or are computationally expensive.", "method": "ResPF, based on PFGM++, uses conditional guidance, skips redundant sampling steps, and employs a residual fusion module to maintain trajectory continuity.", "result": "ResPF achieves superior reconstruction quality, faster inference, and stronger robustness compared to state-of-the-art methods.", "conclusion": "ResPF is the first Poisson flow model for sparse-view CT, offering efficient, accurate, and robust reconstruction."}}
{"id": "2506.06412", "pdf": "https://arxiv.org/pdf/2506.06412", "abs": "https://arxiv.org/abs/2506.06412", "authors": ["Junming Wang", "Yi Shi"], "title": "NeurNCD: Novel Class Discovery via Implicit Neural Representation", "categories": ["cs.LG", "cs.CV"], "comment": "Accepted by ICMR 2024", "summary": "Discovering novel classes in open-world settings is crucial for real-world\napplications. Traditional explicit representations, such as object descriptors\nor 3D segmentation maps, are constrained by their discrete, hole-prone, and\nnoisy nature, which hinders accurate novel class discovery. To address these\nchallenges, we introduce NeurNCD, the first versatile and data-efficient\nframework for novel class discovery that employs the meticulously designed\nEmbedding-NeRF model combined with KL divergence as a substitute for\ntraditional explicit 3D segmentation maps to aggregate semantic embedding and\nentropy in visual embedding space. NeurNCD also integrates several key\ncomponents, including feature query, feature modulation and clustering,\nfacilitating efficient feature augmentation and information exchange between\nthe pre-trained semantic segmentation network and implicit neural\nrepresentations. As a result, our framework achieves superior segmentation\nperformance in both open and closed-world settings without relying on densely\nlabelled datasets for supervised training or human interaction to generate\nsparse label supervision. Extensive experiments demonstrate that our method\nsignificantly outperforms state-of-the-art approaches on the NYUv2 and Replica\ndatasets.", "AI": {"tldr": "NeurNCD is a novel framework for discovering classes in open-world settings using Embedding-NeRF and KL divergence, outperforming traditional methods without dense labels.", "motivation": "Traditional explicit representations for novel class discovery are limited by noise and discreteness, prompting the need for a more efficient and versatile solution.", "method": "NeurNCD combines Embedding-NeRF with KL divergence for semantic embedding aggregation, integrating feature query, modulation, and clustering for efficient feature augmentation.", "result": "The framework achieves superior segmentation performance on NYUv2 and Replica datasets, surpassing state-of-the-art methods.", "conclusion": "NeurNCD offers a robust, data-efficient solution for novel class discovery, eliminating the need for dense labeled datasets or human supervision."}}
{"id": "2506.06440", "pdf": "https://arxiv.org/pdf/2506.06440", "abs": "https://arxiv.org/abs/2506.06440", "authors": ["Chuhao Chen", "Zhiyang Dou", "Chen Wang", "Yiming Huang", "Anjun Chen", "Qiao Feng", "Jiatao Gu", "Lingjie Liu"], "title": "Vid2Sim: Generalizable, Video-based Reconstruction of Appearance, Geometry and Physics for Mesh-free Simulation", "categories": ["cs.GR", "cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Faithfully reconstructing textured shapes and physical properties from videos\npresents an intriguing yet challenging problem. Significant efforts have been\ndedicated to advancing such a system identification problem in this area.\nPrevious methods often rely on heavy optimization pipelines with a\ndifferentiable simulator and renderer to estimate physical parameters. However,\nthese approaches frequently necessitate extensive hyperparameter tuning for\neach scene and involve a costly optimization process, which limits both their\npracticality and generalizability. In this work, we propose a novel framework,\nVid2Sim, a generalizable video-based approach for recovering geometry and\nphysical properties through a mesh-free reduced simulation based on Linear\nBlend Skinning (LBS), offering high computational efficiency and versatile\nrepresentation capability. Specifically, Vid2Sim first reconstructs the\nobserved configuration of the physical system from video using a feed-forward\nneural network trained to capture physical world knowledge. A lightweight\noptimization pipeline then refines the estimated appearance, geometry, and\nphysical properties to closely align with video observations within just a few\nminutes. Additionally, after the reconstruction, Vid2Sim enables high-quality,\nmesh-free simulation with high efficiency. Extensive experiments demonstrate\nthat our method achieves superior accuracy and efficiency in reconstructing\ngeometry and physical properties from video data.", "AI": {"tldr": "Vid2Sim is a novel framework for reconstructing geometry and physical properties from videos using a mesh-free, efficient approach based on Linear Blend Skinning (LBS).", "motivation": "Existing methods rely on costly optimization pipelines and hyperparameter tuning, limiting practicality and generalizability. Vid2Sim aims to address these limitations.", "method": "Vid2Sim uses a feed-forward neural network to reconstruct observed configurations, followed by lightweight optimization for refinement. It enables mesh-free simulation post-reconstruction.", "result": "The method achieves superior accuracy and efficiency in reconstructing geometry and physical properties from videos.", "conclusion": "Vid2Sim offers a generalizable, efficient solution for video-based system identification, outperforming previous approaches."}}
{"id": "2506.06462", "pdf": "https://arxiv.org/pdf/2506.06462", "abs": "https://arxiv.org/abs/2506.06462", "authors": ["Nicol\u00e1s Violante", "Andreas Meuleman", "Alban Gauthier", "Fr\u00e9do Durand", "Thibault Groueix", "George Drettakis"], "title": "Splat and Replace: 3D Reconstruction with Repetitive Elements", "categories": ["cs.GR", "cs.CV"], "comment": "SIGGRAPH Conference Papers 2025. Project site:\n  https://repo-sam.inria.fr/nerphys/splat-and-replace/", "summary": "We leverage repetitive elements in 3D scenes to improve novel view synthesis.\nNeural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have greatly\nimproved novel view synthesis but renderings of unseen and occluded parts\nremain low-quality if the training views are not exhaustive enough. Our key\nobservation is that our environment is often full of repetitive elements. We\npropose to leverage those repetitions to improve the reconstruction of\nlow-quality parts of the scene due to poor coverage and occlusions. We propose\na method that segments each repeated instance in a 3DGS reconstruction,\nregisters them together, and allows information to be shared among instances.\nOur method improves the geometry while also accounting for appearance\nvariations across instances. We demonstrate our method on a variety of\nsynthetic and real scenes with typical repetitive elements, leading to a\nsubstantial improvement in the quality of novel view synthesis.", "AI": {"tldr": "The paper proposes leveraging repetitive elements in 3D scenes to enhance novel view synthesis by improving reconstruction of low-quality areas in Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS).", "motivation": "Existing methods like NeRF and 3DGS struggle with unseen or occluded parts when training views are insufficient. The authors observe that environments often contain repetitive elements, which can be utilized to improve reconstruction.", "method": "The method segments repeated instances in a 3DGS reconstruction, registers them, and shares information among instances, improving geometry and handling appearance variations.", "result": "The approach significantly enhances novel view synthesis quality in synthetic and real scenes with repetitive elements.", "conclusion": "Leveraging repetitive elements in 3D scenes effectively improves reconstruction and synthesis quality for under-covered or occluded areas."}}
{"id": "2506.06474", "pdf": "https://arxiv.org/pdf/2506.06474", "abs": "https://arxiv.org/abs/2506.06474", "authors": ["Everett Richards", "Bipul Thapa", "Lena Mashayekhy"], "title": "Edge-Enabled Collaborative Object Detection for Real-Time Multi-Vehicle Perception", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.MA", "cs.NI", "I.4.8; I.2.10; I.2.11; I.2.9; C.2.4"], "comment": "This paper has been accepted to IEEE EDGE 2025. The final version\n  will be published in IEEE Xplore later this year", "summary": "Accurate and reliable object detection is critical for ensuring the safety\nand efficiency of Connected Autonomous Vehicles (CAVs). Traditional on-board\nperception systems have limited accuracy due to occlusions and blind spots,\nwhile cloud-based solutions introduce significant latency, making them\nunsuitable for real-time processing demands required for autonomous driving in\ndynamic environments. To address these challenges, we introduce an innovative\nframework, Edge-Enabled Collaborative Object Detection (ECOD) for CAVs, that\nleverages edge computing and multi-CAV collaboration for real-time,\nmulti-perspective object detection. Our ECOD framework integrates two key\nalgorithms: Perceptive Aggregation and Collaborative Estimation (PACE) and\nVariable Object Tally and Evaluation (VOTE). PACE aggregates detection data\nfrom multiple CAVs on an edge server to enhance perception in scenarios where\nindividual CAVs have limited visibility. VOTE utilizes a consensus-based voting\nmechanism to improve the accuracy of object classification by integrating data\nfrom multiple CAVs. Both algorithms are designed at the edge to operate in\nreal-time, ensuring low-latency and reliable decision-making for CAVs. We\ndevelop a hardware-based controlled testbed consisting of camera-equipped\nrobotic CAVs and an edge server to evaluate the efficacy of our framework. Our\nexperimental results demonstrate the significant benefits of ECOD in terms of\nimproved object classification accuracy, outperforming traditional\nsingle-perspective onboard approaches by up to 75%, while ensuring low-latency,\nedge-driven real-time processing. This research highlights the potential of\nedge computing to enhance collaborative perception for latency-sensitive\nautonomous systems.", "AI": {"tldr": "The paper introduces Edge-Enabled Collaborative Object Detection (ECOD) for CAVs, leveraging edge computing and multi-CAV collaboration to improve real-time object detection accuracy and reduce latency.", "motivation": "Traditional on-board and cloud-based object detection systems for CAVs face limitations in accuracy and latency, necessitating a new approach for real-time, reliable perception in dynamic environments.", "method": "The ECOD framework integrates two algorithms: PACE for aggregating multi-CAV detection data and VOTE for consensus-based object classification, both operating at the edge for low-latency processing.", "result": "Experimental results show ECOD improves object classification accuracy by up to 75% compared to single-perspective onboard methods, while maintaining real-time performance.", "conclusion": "The research demonstrates the potential of edge computing and multi-CAV collaboration to enhance perception for latency-sensitive autonomous systems like CAVs."}}
{"id": "2506.06483", "pdf": "https://arxiv.org/pdf/2506.06483", "abs": "https://arxiv.org/abs/2506.06483", "authors": ["Yao Ni", "Song Wen", "Piotr Koniusz", "Anoop Cherian"], "title": "Noise Consistency Regularization for Improved Subject-Driven Image Synthesis", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG", "eess.IV"], "comment": null, "summary": "Fine-tuning Stable Diffusion enables subject-driven image synthesis by\nadapting the model to generate images containing specific subjects. However,\nexisting fine-tuning methods suffer from two key issues: underfitting, where\nthe model fails to reliably capture subject identity, and overfitting, where it\nmemorizes the subject image and reduces background diversity. To address these\nchallenges, we propose two auxiliary consistency losses for diffusion\nfine-tuning. First, a prior consistency regularization loss ensures that the\npredicted diffusion noise for prior (non-subject) images remains consistent\nwith that of the pretrained model, improving fidelity. Second, a subject\nconsistency regularization loss enhances the fine-tuned model's robustness to\nmultiplicative noise modulated latent code, helping to preserve subject\nidentity while improving diversity. Our experimental results demonstrate that\nincorporating these losses into fine-tuning not only preserves subject identity\nbut also enhances image diversity, outperforming DreamBooth in terms of CLIP\nscores, background variation, and overall visual quality.", "AI": {"tldr": "The paper proposes two auxiliary consistency losses to improve fine-tuning of Stable Diffusion for subject-driven image synthesis, addressing underfitting and overfitting issues.", "motivation": "Existing fine-tuning methods for Stable Diffusion struggle with underfitting (failing to capture subject identity) and overfitting (memorizing subject images and reducing background diversity).", "method": "The authors introduce two losses: a prior consistency regularization loss to maintain fidelity and a subject consistency regularization loss to enhance robustness and diversity.", "result": "The proposed method outperforms DreamBooth in CLIP scores, background variation, and visual quality, preserving subject identity while improving diversity.", "conclusion": "The auxiliary losses effectively address fine-tuning challenges, enhancing both subject fidelity and image diversity in Stable Diffusion."}}
{"id": "2506.06633", "pdf": "https://arxiv.org/pdf/2506.06633", "abs": "https://arxiv.org/abs/2506.06633", "authors": ["Chi-Sheng Chen"], "title": "Vision-QRWKV: Exploring Quantum-Enhanced RWKV Models for Image Classification", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Recent advancements in quantum machine learning have shown promise in\nenhancing classical neural network architectures, particularly in domains\ninvolving complex, high-dimensional data. Building upon prior work in temporal\nsequence modeling, this paper introduces Vision-QRWKV, a hybrid\nquantum-classical extension of the Receptance Weighted Key Value (RWKV)\narchitecture, applied for the first time to image classification tasks. By\nintegrating a variational quantum circuit (VQC) into the channel mixing\ncomponent of RWKV, our model aims to improve nonlinear feature transformation\nand enhance the expressive capacity of visual representations.\n  We evaluate both classical and quantum RWKV models on a diverse collection of\n14 medical and standard image classification benchmarks, including MedMNIST\ndatasets, MNIST, and FashionMNIST. Our results demonstrate that the\nquantum-enhanced model outperforms its classical counterpart on a majority of\ndatasets, particularly those with subtle or noisy class distinctions (e.g.,\nChestMNIST, RetinaMNIST, BloodMNIST). This study represents the first\nsystematic application of quantum-enhanced RWKV in the visual domain, offering\ninsights into the architectural trade-offs and future potential of quantum\nmodels for lightweight and efficient vision tasks.", "AI": {"tldr": "Vision-QRWKV, a quantum-classical hybrid model, enhances RWKV for image classification, outperforming classical RWKV on noisy datasets.", "motivation": "To leverage quantum computing for improving nonlinear feature transformations in visual tasks, addressing challenges in noisy or subtle class distinctions.", "method": "Integrates a variational quantum circuit (VQC) into RWKV's channel mixing component, tested on 14 image classification benchmarks.", "result": "Quantum-enhanced RWKV outperforms classical RWKV, especially on datasets with subtle/noisy distinctions like ChestMNIST.", "conclusion": "Quantum-enhanced RWKV shows promise for lightweight, efficient vision tasks, highlighting architectural trade-offs and future potential."}}
{"id": "2506.06637", "pdf": "https://arxiv.org/pdf/2506.06637", "abs": "https://arxiv.org/abs/2506.06637", "authors": ["Olimjon Toirov", "Wei Yu"], "title": "Non-Intrusive Load Monitoring Based on Image Load Signatures and Continual Learning", "categories": ["cs.LG", "cs.AI", "cs.CV", "eess.SP"], "comment": "10 pages, 3 figures, 2025 2nd International Conference on Digital\n  Society and Artificial Intelligence (DSAI 2025), Conference dates: May 23-25,\n  2025", "summary": "Non-Intrusive Load Monitoring (NILM) identifies the operating status and\nenergy consumption of each electrical device in the circuit by analyzing the\nelectrical signals at the bus, which is of great significance for smart power\nmanagement. However, the complex and changeable load combinations and\napplication environments lead to the challenges of poor feature robustness and\ninsufficient model generalization of traditional NILM methods. To this end,\nthis paper proposes a new non-intrusive load monitoring method that integrates\n\"image load signature\" and continual learning. This method converts\nmulti-dimensional power signals such as current, voltage, and power factor into\nvisual image load feature signatures, and combines deep convolutional neural\nnetworks to realize the identification and classification of multiple devices;\nat the same time, self-supervised pre-training is introduced to improve feature\ngeneralization, and continual online learning strategies are used to overcome\nmodel forgetting to adapt to the emergence of new loads. This paper conducts a\nlarge number of experiments on high-sampling rate load datasets, and compares a\nvariety of existing methods and model variants. The results show that the\nproposed method has achieved significant improvements in recognition accuracy.", "AI": {"tldr": "A new NILM method combines image load signatures and continual learning to improve device identification and adapt to new loads, achieving higher accuracy.", "motivation": "Traditional NILM methods struggle with poor feature robustness and insufficient generalization due to complex load combinations and environments.", "method": "Converts power signals into visual image load signatures, uses deep convolutional neural networks for classification, and employs continual learning to adapt to new loads.", "result": "Experiments show significant improvements in recognition accuracy compared to existing methods.", "conclusion": "The proposed method enhances NILM performance by leveraging visual signatures and continual learning."}}
{"id": "2506.06659", "pdf": "https://arxiv.org/pdf/2506.06659", "abs": "https://arxiv.org/abs/2506.06659", "authors": ["Wenhao Yao", "Zhenxin Li", "Shiyi Lan", "Zi Wang", "Xinglong Sun", "Jose M. Alvarez", "Zuxuan Wu"], "title": "DriveSuprim: Towards Precise Trajectory Selection for End-to-End Planning", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "15 pages, 6 figures", "summary": "In complex driving environments, autonomous vehicles must navigate safely.\nRelying on a single predicted path, as in regression-based approaches, usually\ndoes not explicitly assess the safety of the predicted trajectory.\nSelection-based methods address this by generating and scoring multiple\ntrajectory candidates and predicting the safety score for each, but face\noptimization challenges in precisely selecting the best option from thousands\nof possibilities and distinguishing subtle but safety-critical differences,\nespecially in rare or underrepresented scenarios. We propose DriveSuprim to\novercome these challenges and advance the selection-based paradigm through a\ncoarse-to-fine paradigm for progressive candidate filtering, a rotation-based\naugmentation method to improve robustness in out-of-distribution scenarios, and\na self-distillation framework to stabilize training. DriveSuprim achieves\nstate-of-the-art performance, reaching 93.5% PDMS in NAVSIM v1 and 87.1% EPDMS\nin NAVSIM v2 without extra data, demonstrating superior safetycritical\ncapabilities, including collision avoidance and compliance with rules, while\nmaintaining high trajectory quality in various driving scenarios.", "AI": {"tldr": "DriveSuprim improves autonomous vehicle trajectory prediction by filtering candidates progressively, enhancing robustness, and stabilizing training, achieving top performance in safety-critical scenarios.", "motivation": "Single-path predictions lack explicit safety assessment, and selection-based methods struggle with optimization and rare scenarios.", "method": "Uses coarse-to-fine candidate filtering, rotation-based augmentation, and self-distillation for robust training.", "result": "Achieves 93.5% PDMS in NAVSIM v1 and 87.1% EPDMS in NAVSIM v2, excelling in safety and trajectory quality.", "conclusion": "DriveSuprim advances selection-based methods, ensuring safer and more reliable autonomous driving."}}
{"id": "2506.06664", "pdf": "https://arxiv.org/pdf/2506.06664", "abs": "https://arxiv.org/abs/2506.06664", "authors": ["Zhenxin Li", "Wenhao Yao", "Zi Wang", "Xinglong Sun", "Joshua Chen", "Nadine Chang", "Maying Shen", "Zuxuan Wu", "Shiyi Lan", "Jose M. Alvarez"], "title": "Generalized Trajectory Scoring for End-to-end Multimodal Planning", "categories": ["cs.RO", "cs.CV"], "comment": "The 1st place solution of the End-to-end Driving Track at the CVPR\n  2025 Autonomous Grand Challenge", "summary": "End-to-end multi-modal planning is a promising paradigm in autonomous\ndriving, enabling decision-making with diverse trajectory candidates. A key\ncomponent is a robust trajectory scorer capable of selecting the optimal\ntrajectory from these candidates. While recent trajectory scorers focus on\nscoring either large sets of static trajectories or small sets of dynamically\ngenerated ones, both approaches face significant limitations in generalization.\nStatic vocabularies provide effective coarse discretization but struggle to\nmake fine-grained adaptation, while dynamic proposals offer detailed precision\nbut fail to capture broader trajectory distributions. To overcome these\nchallenges, we propose GTRS (Generalized Trajectory Scoring), a unified\nframework for end-to-end multi-modal planning that combines coarse and\nfine-grained trajectory evaluation. GTRS consists of three complementary\ninnovations: (1) a diffusion-based trajectory generator that produces diverse\nfine-grained proposals; (2) a vocabulary generalization technique that trains a\nscorer on super-dense trajectory sets with dropout regularization, enabling its\nrobust inference on smaller subsets; and (3) a sensor augmentation strategy\nthat enhances out-of-domain generalization while incorporating refinement\ntraining for critical trajectory discrimination. As the winning solution of the\nNavsim v2 Challenge, GTRS demonstrates superior performance even with\nsub-optimal sensor inputs, approaching privileged methods that rely on\nground-truth perception. Code will be available at\nhttps://github.com/NVlabs/GTRS.", "AI": {"tldr": "GTRS is a unified framework for end-to-end multi-modal planning in autonomous driving, combining coarse and fine-grained trajectory evaluation to overcome limitations of static and dynamic trajectory scoring methods.", "motivation": "Existing trajectory scorers struggle with generalization\u2014static vocabularies lack fine-grained adaptation, while dynamic proposals miss broader distributions. GTRS aims to unify these approaches.", "method": "GTRS integrates a diffusion-based trajectory generator, vocabulary generalization with dropout, and sensor augmentation for robust inference and generalization.", "result": "GTRS achieved superior performance, winning the Navsim v2 Challenge, and approaches privileged methods using ground-truth perception.", "conclusion": "GTRS provides a robust solution for trajectory scoring in autonomous driving, balancing coarse and fine-grained evaluation for better generalization."}}
{"id": "2506.06677", "pdf": "https://arxiv.org/pdf/2506.06677", "abs": "https://arxiv.org/abs/2506.06677", "authors": ["Songhao Han", "Boxiang Qiu", "Yue Liao", "Siyuan Huang", "Chen Gao", "Shuicheng Yan", "Si Liu"], "title": "RoboCerebra: A Large-scale Benchmark for Long-horizon Robotic Manipulation Evaluation", "categories": ["cs.RO", "cs.CV"], "comment": "23 pages, 18 figures", "summary": "Recent advances in vision-language models (VLMs) have enabled\ninstruction-conditioned robotic systems with improved generalization. However,\nmost existing work focuses on reactive System 1 policies, underutilizing VLMs'\nstrengths in semantic reasoning and long-horizon planning. These System 2\ncapabilities-characterized by deliberative, goal-directed thinking-remain under\nexplored due to the limited temporal scale and structural complexity of current\nbenchmarks. To address this gap, we introduce RoboCerebra, a benchmark for\nevaluating high-level reasoning in long-horizon robotic manipulation.\nRoboCerebra includes: (1) a large-scale simulation dataset with extended task\nhorizons and diverse subtask sequences in household environments; (2) a\nhierarchical framework combining a high-level VLM planner with a low-level\nvision-language-action (VLA) controller; and (3) an evaluation protocol\ntargeting planning, reflection, and memory through structured System 1-System 2\ninteraction. The dataset is constructed via a top-down pipeline, where GPT\ngenerates task instructions and decomposes them into subtask sequences. Human\noperators execute the subtasks in simulation, yielding high-quality\ntrajectories with dynamic object variations. Compared to prior benchmarks,\nRoboCerebra features significantly longer action sequences and denser\nannotations. We further benchmark state-of-the-art VLMs as System 2 modules and\nanalyze their performance across key cognitive dimensions, advancing the\ndevelopment of more capable and generalizable robotic planners.", "AI": {"tldr": "RoboCerebra is a benchmark for evaluating high-level reasoning in long-horizon robotic manipulation, addressing gaps in current benchmarks by focusing on System 2 capabilities like planning and reflection.", "motivation": "Existing work underutilizes vision-language models' (VLMs) strengths in semantic reasoning and long-horizon planning, focusing mostly on reactive policies (System 1).", "method": "RoboCerebra includes a large-scale simulation dataset, a hierarchical framework combining a VLM planner with a low-level controller, and an evaluation protocol for System 1-System 2 interaction.", "result": "The benchmark features longer action sequences and denser annotations than prior work, and benchmarks state-of-the-art VLMs as System 2 modules.", "conclusion": "RoboCerebra advances the development of more capable and generalizable robotic planners by evaluating high-level reasoning in long-horizon tasks."}}
{"id": "2506.06690", "pdf": "https://arxiv.org/pdf/2506.06690", "abs": "https://arxiv.org/abs/2506.06690", "authors": ["Hao Wang", "Chengkai Hou", "Xianglong Li", "Yankai Fu", "Chenxuan Li", "Ning Chen", "Gaole Dai", "Jiaming Liu", "Tiejun Huang", "Shanghang Zhang"], "title": "SpikePingpong: High-Frequency Spike Vision-based Robot Learning for Precise Striking in Table Tennis Game", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Learning to control high-speed objects in the real world remains a\nchallenging frontier in robotics. Table tennis serves as an ideal testbed for\nthis problem, demanding both rapid interception of fast-moving balls and\nprecise adjustment of their trajectories. This task presents two fundamental\nchallenges: it requires a high-precision vision system capable of accurately\npredicting ball trajectories, and it necessitates intelligent strategic\nplanning to ensure precise ball placement to target regions. The dynamic nature\nof table tennis, coupled with its real-time response requirements, makes it\nparticularly well-suited for advancing robotic control capabilities in\nfast-paced, precision-critical domains. In this paper, we present\nSpikePingpong, a novel system that integrates spike-based vision with imitation\nlearning for high-precision robotic table tennis. Our approach introduces two\nkey attempts that directly address the aforementioned challenges: SONIC, a\nspike camera-based module that achieves millimeter-level precision in\nball-racket contact prediction by compensating for real-world uncertainties\nsuch as air resistance and friction; and IMPACT, a strategic planning module\nthat enables accurate ball placement to targeted table regions. The system\nharnesses a 20 kHz spike camera for high-temporal resolution ball tracking,\ncombined with efficient neural network models for real-time trajectory\ncorrection and stroke planning. Experimental results demonstrate that\nSpikePingpong achieves a remarkable 91% success rate for 30 cm accuracy target\narea and 71% in the more challenging 20 cm accuracy task, surpassing previous\nstate-of-the-art approaches by 38% and 37% respectively. These significant\nperformance improvements enable the robust implementation of sophisticated\ntactical gameplay strategies, providing a new research perspective for robotic\ncontrol in high-speed dynamic tasks.", "AI": {"tldr": "SpikePingpong integrates spike-based vision and imitation learning for high-precision robotic table tennis, achieving superior accuracy and performance over prior methods.", "motivation": "The challenge of controlling high-speed objects in robotics, exemplified by table tennis, requires precise vision and strategic planning.", "method": "Combines SONIC (spike camera for millimeter-level precision) and IMPACT (strategic planning) with a 20 kHz spike camera and neural networks.", "result": "Achieves 91% success for 30 cm accuracy and 71% for 20 cm, outperforming prior methods by 38% and 37%.", "conclusion": "SpikePingpong advances robotic control in high-speed tasks, enabling sophisticated gameplay strategies."}}
{"id": "2506.06727", "pdf": "https://arxiv.org/pdf/2506.06727", "abs": "https://arxiv.org/abs/2506.06727", "authors": ["Can Li", "Ting Zhang", "Mei Wang", "Hua Huang"], "title": "VisioMath: Benchmarking Figure-based Mathematical Reasoning in LMMs", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Large Multimodal Models (LMMs) have demonstrated remarkable problem-solving\ncapabilities across various domains. However, their ability to perform\nmathematical reasoning when answer options are represented as images--an\nessential aspect of multi-image comprehension--remains underexplored. To bridge\nthis gap, we introduce VisioMath, a benchmark designed to evaluate mathematical\nreasoning in multimodal contexts involving image-based answer choices.\nVisioMath comprises 8,070 images and 1,800 multiple-choice questions, where\neach answer option is an image, presenting unique challenges to existing LMMs.\nTo the best of our knowledge, VisioMath is the first dataset specifically\ntailored for mathematical reasoning in image-based-option scenarios, where\nfine-grained distinctions between answer choices are critical for accurate\nproblem-solving. We systematically evaluate state-of-the-art LMMs on VisioMath\nand find that even the most advanced models struggle with this task. Notably,\nGPT-4o achieves only 45.9% accuracy, underscoring the limitations of current\nmodels in reasoning over visually similar answer choices. By addressing a\ncrucial gap in existing benchmarks, VisioMath establishes a rigorous testbed\nfor future research, driving advancements in multimodal reasoning.", "AI": {"tldr": "VisioMath is a new benchmark for evaluating LMMs' mathematical reasoning with image-based answer choices, revealing significant limitations in current models like GPT-4o.", "motivation": "Existing LMMs lack evaluation for mathematical reasoning with image-based options, a critical gap in multimodal comprehension.", "method": "VisioMath includes 8,070 images and 1,800 multiple-choice questions with image-based answers, testing fine-grained distinctions.", "result": "State-of-the-art LMMs, including GPT-4o, perform poorly (45.9% accuracy), highlighting challenges in visual reasoning.", "conclusion": "VisioMath fills a crucial gap, providing a testbed for advancing multimodal reasoning capabilities."}}
{"id": "2506.06761", "pdf": "https://arxiv.org/pdf/2506.06761", "abs": "https://arxiv.org/abs/2506.06761", "authors": ["Adri\u00e0 Molina Rodr\u00edguez", "Oriol Ramos Terrades", "Josep Llad\u00f3s"], "title": "The OCR Quest for Generalization: Learning to recognize low-resource alphabets with model editing", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Achieving robustness in recognition systems across diverse domains is crucial\nfor their practical utility. While ample data availability is usually assumed,\nlow-resource languages, such as ancient manuscripts and non-western languages,\ntend to be kept out of the equations of massive pretraining and foundational\ntechniques due to an under representation. In this work, we aim for building\nmodels which can generalize to new distributions of data, such as alphabets,\nfaster than centralized fine-tune strategies. For doing so, we take advantage\nof the recent advancements in model editing to enhance the incorporation of\nunseen scripts (low-resource learning). In contrast to state-of-the-art\nmeta-learning, we showcase the effectiveness of domain merging in sparse\ndistributions of data, with agnosticity of its relation to the overall\ndistribution or any other prototyping necessity. Even when using the same exact\ntraining data, our experiments showcase significant performance boosts in\n\\textbf{transfer learning} to new alphabets and \\textbf{out-of-domain\nevaluation} in challenging domain shifts, including historical ciphered texts\nand non-Latin scripts. This research contributes a novel approach into building\nmodels that can easily adopt under-represented alphabets and, therefore, enable\ndocument recognition to a wider set of contexts and cultures.", "AI": {"tldr": "The paper proposes a method to improve recognition systems' robustness for low-resource languages by leveraging model editing and domain merging, outperforming meta-learning in transfer learning and out-of-domain tasks.", "motivation": "Addressing the under-representation of low-resource languages in recognition systems to enhance practical utility across diverse domains.", "method": "Utilizes model editing and domain merging to generalize faster to new data distributions (e.g., alphabets) without relying on massive pretraining.", "result": "Significant performance boosts in transfer learning and out-of-domain evaluation, even with the same training data.", "conclusion": "The approach enables easier adoption of under-represented alphabets, expanding document recognition to more contexts and cultures."}}
{"id": "2506.06782", "pdf": "https://arxiv.org/pdf/2506.06782", "abs": "https://arxiv.org/abs/2506.06782", "authors": ["Qinting Jiang", "Chuyang Ye", "Dongyan Wei", "Bingli Wang", "Yuan Xue", "Jingyan Jiang", "Zhi Wang"], "title": "Feature-Based Instance Neighbor Discovery: Advanced Stable Test-Time Adaptation in Dynamic World", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Despite progress, deep neural networks still suffer performance declines\nunder distribution shifts between training and test domains, leading to a\nsubstantial decrease in Quality of Experience (QoE) for applications. Existing\ntest-time adaptation (TTA) methods are challenged by dynamic, multiple test\ndistributions within batches. We observe that feature distributions across\ndifferent domains inherently cluster into distinct groups with varying means\nand variances. This divergence reveals a critical limitation of previous global\nnormalization strategies in TTA, which inevitably distort the original data\ncharacteristics. Based on this insight, we propose Feature-based Instance\nNeighbor Discovery (FIND), which comprises three key components: Layer-wise\nFeature Disentanglement (LFD), Feature Aware Batch Normalization (FABN) and\nSelective FABN (S-FABN). LFD stably captures features with similar\ndistributions at each layer by constructing graph structures. While FABN\noptimally combines source statistics with test-time distribution specific\nstatistics for robust feature representation. Finally, S-FABN determines which\nlayers require feature partitioning and which can remain unified, thereby\nenhancing inference efficiency. Extensive experiments demonstrate that FIND\nsignificantly outperforms existing methods, achieving a 30\\% accuracy\nimprovement in dynamic scenarios while maintaining computational efficiency.", "AI": {"tldr": "FIND improves test-time adaptation by addressing dynamic distribution shifts with feature clustering, outperforming existing methods by 30% accuracy.", "motivation": "Deep neural networks suffer performance drops under distribution shifts, reducing Quality of Experience (QoE). Existing TTA methods struggle with dynamic, multiple test distributions.", "method": "FIND includes Layer-wise Feature Disentanglement (LFD), Feature Aware Batch Normalization (FABN), and Selective FABN (S-FABN) to handle feature clustering and optimize normalization.", "result": "FIND achieves a 30% accuracy improvement in dynamic scenarios while maintaining computational efficiency.", "conclusion": "FIND effectively addresses distribution shifts by leveraging feature clustering and adaptive normalization, enhancing both performance and efficiency."}}
{"id": "2506.06862", "pdf": "https://arxiv.org/pdf/2506.06862", "abs": "https://arxiv.org/abs/2506.06862", "authors": ["Chenguang Huang", "Oier Mees", "Andy Zeng", "Wolfram Burgard"], "title": "Multimodal Spatial Language Maps for Robot Navigation and Manipulation", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SD", "eess.AS"], "comment": "accepted to International Journal of Robotics Research (IJRR). 24\n  pages, 18 figures. The paper contains texts from VLMaps(arXiv:2210.05714) and\n  AVLMaps(arXiv:2303.07522). The project page is https://mslmaps.github.io/", "summary": "Grounding language to a navigating agent's observations can leverage\npretrained multimodal foundation models to match perceptions to object or event\ndescriptions. However, previous approaches remain disconnected from environment\nmapping, lack the spatial precision of geometric maps, or neglect additional\nmodality information beyond vision. To address this, we propose multimodal\nspatial language maps as a spatial map representation that fuses pretrained\nmultimodal features with a 3D reconstruction of the environment. We build these\nmaps autonomously using standard exploration. We present two instances of our\nmaps, which are visual-language maps (VLMaps) and their extension to\naudio-visual-language maps (AVLMaps) obtained by adding audio information. When\ncombined with large language models (LLMs), VLMaps can (i) translate natural\nlanguage commands into open-vocabulary spatial goals (e.g., \"in between the\nsofa and TV\") directly localized in the map, and (ii) be shared across\ndifferent robot embodiments to generate tailored obstacle maps on demand.\nBuilding upon the capabilities above, AVLMaps extend VLMaps by introducing a\nunified 3D spatial representation integrating audio, visual, and language cues\nthrough the fusion of features from pretrained multimodal foundation models.\nThis enables robots to ground multimodal goal queries (e.g., text, images, or\naudio snippets) to spatial locations for navigation. Additionally, the\nincorporation of diverse sensory inputs significantly enhances goal\ndisambiguation in ambiguous environments. Experiments in simulation and\nreal-world settings demonstrate that our multimodal spatial language maps\nenable zero-shot spatial and multimodal goal navigation and improve recall by\n50% in ambiguous scenarios. These capabilities extend to mobile robots and\ntabletop manipulators, supporting navigation and interaction guided by visual,\naudio, and spatial cues.", "AI": {"tldr": "The paper proposes multimodal spatial language maps (VLMaps and AVLMaps) to fuse pretrained multimodal features with 3D environment reconstructions, enabling robots to ground language and multimodal queries for navigation.", "motivation": "Existing approaches lack integration with environment mapping, spatial precision, or multimodal information beyond vision.", "method": "Autonomously build visual-language maps (VLMaps) and extend them to audio-visual-language maps (AVLMaps) by fusing pretrained multimodal features with 3D reconstructions. Combine with LLMs for natural language command translation and obstacle map generation.", "result": "Enables zero-shot spatial and multimodal goal navigation, improves recall by 50% in ambiguous scenarios, and supports diverse robot embodiments.", "conclusion": "Multimodal spatial language maps enhance robot navigation and interaction by integrating visual, audio, and spatial cues, outperforming previous methods."}}
{"id": "2506.06884", "pdf": "https://arxiv.org/pdf/2506.06884", "abs": "https://arxiv.org/abs/2506.06884", "authors": ["Divya Jyoti Bajpai", "Manjesh Kumar Hanawal"], "title": "FREE: Fast and Robust Vision Language Models with Early Exits", "categories": ["cs.LG", "cs.CV"], "comment": "To appear at the Association of Computational Linguistics (ACL) 2025\n  Conference", "summary": "In recent years, Vision-Language Models (VLMs) have shown remarkable\nperformance improvements in Vision-Language tasks. However, their large size\nposes challenges for real-world applications where inference latency is a\nconcern. To tackle this issue, we propose employing Early Exit (EE) strategies\nin VLMs. However, training exit classifiers in VLMs is challenging,\nparticularly with limited labeled training data. To address this, we introduce\nFREE, an adversarial training approach within a GAN-based framework. Here, each\nexit consists of a transformer layer and a classifier. The transformer layer is\nadversarially trained to produce feature representations similar to the final\nlayer, while a feature classifier serves as the discriminator. Our method\nfocuses on performing input-adaptive inference that increases inference speed\nwith minimal drop in performance. Experimental results demonstrate the\neffectiveness of our approach in enhancing accuracy and model robustness by\nmitigating overthinking and the phenomenon of mid-crisis that we highlight. We\nexperimentally validate that our method speeds up the inference process by more\nthan 1.51x while retaining comparable performance. The source code is available\nat https://github.com/Div290/FREE.", "AI": {"tldr": "The paper proposes FREE, an adversarial training approach for Vision-Language Models (VLMs) using Early Exit strategies to improve inference speed without significant performance loss.", "motivation": "Large VLMs face challenges in real-world applications due to high inference latency. Training exit classifiers with limited labeled data is difficult.", "method": "FREE employs a GAN-based framework where each exit includes a transformer layer and classifier, adversarially trained to mimic final-layer features.", "result": "The method speeds up inference by 1.51x while maintaining performance, mitigating overthinking and mid-crisis phenomena.", "conclusion": "FREE effectively balances speed and accuracy in VLMs, validated by experimental results."}}
{"id": "2506.06890", "pdf": "https://arxiv.org/pdf/2506.06890", "abs": "https://arxiv.org/abs/2506.06890", "authors": ["Sumit Sharma", "Gopi Raju Matta", "Kaushik Mitra"], "title": "SPC to 3D: Novel View Synthesis from Binary SPC via I2I translation", "categories": ["eess.IV", "cs.CV", "eess.SP"], "comment": "Accepted for publication at ICIP 2025", "summary": "Single Photon Avalanche Diodes (SPADs) represent a cutting-edge imaging\ntechnology, capable of detecting individual photons with remarkable timing\nprecision. Building on this sensitivity, Single Photon Cameras (SPCs) enable\nimage capture at exceptionally high speeds under both low and high\nillumination. Enabling 3D reconstruction and radiance field recovery from such\nSPC data holds significant promise. However, the binary nature of SPC images\nleads to severe information loss, particularly in texture and color, making\ntraditional 3D synthesis techniques ineffective. To address this challenge, we\npropose a modular two-stage framework that converts binary SPC images into\nhigh-quality colorized novel views. The first stage performs image-to-image\n(I2I) translation using generative models such as Pix2PixHD, converting binary\nSPC inputs into plausible RGB representations. The second stage employs 3D\nscene reconstruction techniques like Neural Radiance Fields (NeRF) or Gaussian\nSplatting (3DGS) to generate novel views. We validate our two-stage pipeline\n(Pix2PixHD + Nerf/3DGS) through extensive qualitative and quantitative\nexperiments, demonstrating significant improvements in perceptual quality and\ngeometric consistency over the alternative baseline.", "AI": {"tldr": "A two-stage framework converts binary Single Photon Camera (SPC) images into high-quality colorized novel views using generative models and 3D reconstruction techniques.", "motivation": "The binary nature of SPC images causes severe information loss, making traditional 3D synthesis ineffective.", "method": "A modular two-stage approach: (1) image-to-image translation (Pix2PixHD) to convert binary SPC inputs to RGB, and (2) 3D reconstruction (NeRF/3DGS) for novel views.", "result": "The pipeline shows significant improvements in perceptual quality and geometric consistency over baselines.", "conclusion": "The proposed framework effectively addresses the limitations of binary SPC data for 3D reconstruction and novel view synthesis."}}
{"id": "2506.06933", "pdf": "https://arxiv.org/pdf/2506.06933", "abs": "https://arxiv.org/abs/2506.06933", "authors": ["Mahdi Salmani", "Alireza Abdollahpoorrostam", "Seyed-Mohsen Moosavi-Dezfooli"], "title": "Rewriting the Budget: A General Framework for Black-Box Attacks Under Cost Asymmetry", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.CV"], "comment": null, "summary": "Traditional decision-based black-box adversarial attacks on image classifiers\naim to generate adversarial examples by slightly modifying input images while\nkeeping the number of queries low, where each query involves sending an input\nto the model and observing its output. Most existing methods assume that all\nqueries have equal cost. However, in practice, queries may incur asymmetric\ncosts; for example, in content moderation systems, certain output classes may\ntrigger additional review, enforcement, or penalties, making them more costly\nthan others. While prior work has considered such asymmetric cost settings,\neffective algorithms for this scenario remain underdeveloped. In this paper, we\npropose a general framework for decision-based attacks under asymmetric query\ncosts, which we refer to as asymmetric black-box attacks. We modify two core\ncomponents of existing attacks: the search strategy and the gradient estimation\nprocess. Specifically, we propose Asymmetric Search (AS), a more conservative\nvariant of binary search that reduces reliance on high-cost queries, and\nAsymmetric Gradient Estimation (AGREST), which shifts the sampling distribution\nto favor low-cost queries. We design efficient algorithms that minimize total\nattack cost by balancing different query types, in contrast to earlier methods\nsuch as stealthy attacks that focus only on limiting expensive (high-cost)\nqueries. Our method can be integrated into a range of existing black-box\nattacks with minimal changes. We perform both theoretical analysis and\nempirical evaluation on standard image classification benchmarks. Across\nvarious cost regimes, our method consistently achieves lower total query cost\nand smaller perturbations than existing approaches, with improvements of up to\n40% in some settings.", "AI": {"tldr": "The paper introduces a framework for asymmetric black-box attacks, optimizing query costs in adversarial attacks by modifying search strategies and gradient estimation.", "motivation": "Existing methods assume equal query costs, but practical scenarios involve asymmetric costs (e.g., content moderation). Effective algorithms for this are lacking.", "method": "Proposes Asymmetric Search (AS) and Asymmetric Gradient Estimation (AGREST) to minimize total attack cost by favoring low-cost queries.", "result": "Achieves up to 40% lower total query cost and smaller perturbations compared to existing methods.", "conclusion": "The framework improves adversarial attack efficiency under asymmetric query costs and integrates easily with existing methods."}}
{"id": "2506.06938", "pdf": "https://arxiv.org/pdf/2506.06938", "abs": "https://arxiv.org/abs/2506.06938", "authors": ["Bastian J\u00e4ckl", "Vojt\u011bch Kloda", "Daniel A. Keim", "Jakub Loko\u010d"], "title": "Experimental Evaluation of Static Image Sub-Region-Based Search Models Using CLIP", "categories": ["cs.MM", "cs.CV", "68U10", "H.3.3; I.4.10; H.2.8"], "comment": "14 pages, 4 figures, 2 tables", "summary": "Advances in multimodal text-image models have enabled effective text-based\nquerying in extensive image collections. While these models show convincing\nperformance for everyday life scenes, querying in highly homogeneous,\nspecialized domains remains challenging. The primary problem is that users can\noften provide only vague textual descriptions as they lack expert knowledge to\ndiscriminate between homogenous entities. This work investigates whether adding\nlocation-based prompts to complement these vague text queries can enhance\nretrieval performance. Specifically, we collected a dataset of 741 human\nannotations, each containing short and long textual descriptions and bounding\nboxes indicating regions of interest in challenging underwater scenes. Using\nthese annotations, we evaluate the performance of CLIP when queried on various\nstatic sub-regions of images compared to the full image. Our results show that\nboth a simple 3-by-3 partitioning and a 5-grid overlap significantly improve\nretrieval effectiveness and remain robust to perturbations of the annotation\nbox.", "AI": {"tldr": "Adding location-based prompts to vague text queries improves retrieval performance in homogeneous domains like underwater scenes.", "motivation": "Specialized domains with homogeneous entities make text-based querying challenging due to vague user descriptions.", "method": "Evaluated CLIP's performance using human-annotated underwater scene data with location-based prompts (3x3 partitioning and 5-grid overlap).", "result": "Both partitioning methods significantly improved retrieval effectiveness and robustness to annotation box perturbations.", "conclusion": "Location-based prompts enhance retrieval in specialized domains, addressing the limitations of vague text queries."}}
{"id": "2506.06965", "pdf": "https://arxiv.org/pdf/2506.06965", "abs": "https://arxiv.org/abs/2506.06965", "authors": ["Cuong Manh Hoang"], "title": "Long-Tailed Learning for Generalized Category Discovery", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Generalized Category Discovery (GCD) utilizes labeled samples of known\nclasses to discover novel classes in unlabeled samples. Existing methods show\neffective performance on artificial datasets with balanced distributions.\nHowever, real-world datasets are always imbalanced, significantly affecting the\neffectiveness of these methods. To solve this problem, we propose a novel\nframework that performs generalized category discovery in long-tailed\ndistributions. We first present a self-guided labeling technique that uses a\nlearnable distribution to generate pseudo-labels, resulting in less biased\nclassifiers. We then introduce a representation balancing process to derive\ndiscriminative representations. By mining sample neighborhoods, this process\nencourages the model to focus more on tail classes. We conduct experiments on\npublic datasets to demonstrate the effectiveness of the proposed framework. The\nresults show that our model exceeds previous state-of-the-art methods.", "AI": {"tldr": "A novel framework for Generalized Category Discovery (GCD) in imbalanced datasets, using self-guided labeling and representation balancing to improve performance on tail classes.", "motivation": "Existing GCD methods perform well on balanced datasets but struggle with real-world imbalanced data, necessitating a solution for long-tailed distributions.", "method": "Proposes a self-guided labeling technique for pseudo-label generation and a representation balancing process to enhance focus on tail classes.", "result": "Outperforms state-of-the-art methods on public datasets, demonstrating effectiveness in handling imbalanced data.", "conclusion": "The framework successfully addresses GCD challenges in long-tailed distributions, improving performance on tail classes."}}
{"id": "2506.06999", "pdf": "https://arxiv.org/pdf/2506.06999", "abs": "https://arxiv.org/abs/2506.06999", "authors": ["Arun Sharma", "Mingzhou Yang", "Majid Farhadloo", "Subhankar Ghosh", "Bharat Jayaprakash", "Shashi Shekhar"], "title": "Towards Physics-informed Diffusion for Anomaly Detection in Trajectories", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "comment": null, "summary": "Given trajectory data, a domain-specific study area, and a user-defined\nthreshold, we aim to find anomalous trajectories indicative of possible GPS\nspoofing (e.g., fake trajectory). The problem is societally important to curb\nillegal activities in international waters, such as unauthorized fishing and\nillicit oil transfers. The problem is challenging due to advances in AI\ngenerated in deep fakes generation (e.g., additive noise, fake trajectories)\nand lack of adequate amount of labeled samples for ground-truth verification.\nRecent literature shows promising results for anomalous trajectory detection\nusing generative models despite data sparsity. However, they do not consider\nfine-scale spatiotemporal dependencies and prior physical knowledge, resulting\nin higher false-positive rates. To address these limitations, we propose a\nphysics-informed diffusion model that integrates kinematic constraints to\nidentify trajectories that do not adhere to physical laws. Experimental results\non real-world datasets in the maritime and urban domains show that the proposed\nframework results in higher prediction accuracy and lower estimation error rate\nfor anomaly detection and trajectory generation methods, respectively. Our\nimplementation is available at\nhttps://github.com/arunshar/Physics-Informed-Diffusion-Probabilistic-Model.", "AI": {"tldr": "A physics-informed diffusion model is proposed to detect anomalous trajectories (e.g., GPS spoofing) by integrating kinematic constraints, improving accuracy and reducing false positives.", "motivation": "The study addresses the societal need to curb illegal activities (e.g., unauthorized fishing) by detecting fake trajectories, which is challenging due to AI-generated deep fakes and lack of labeled data.", "method": "The proposed method uses a physics-informed diffusion model that incorporates kinematic constraints to identify trajectories violating physical laws.", "result": "Experiments on real-world maritime and urban datasets show higher prediction accuracy for anomaly detection and lower error rates for trajectory generation.", "conclusion": "The framework effectively detects anomalous trajectories by leveraging physical laws, outperforming existing methods in accuracy and reliability."}}
{"id": "2506.07023", "pdf": "https://arxiv.org/pdf/2506.07023", "abs": "https://arxiv.org/abs/2506.07023", "authors": ["Suman Mahapatra", "Pradipta Maji"], "title": "Optimal Transport Driven Asymmetric Image-to-Image Translation for Nuclei Segmentation of Histological Images", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "13 pages, 8 figures", "summary": "Segmentation of nuclei regions from histological images enables morphometric\nanalysis of nuclei structures, which in turn helps in the detection and\ndiagnosis of diseases under consideration. To develop a nuclei segmentation\nalgorithm, applicable to different types of target domain representations,\nimage-to-image translation networks can be considered as they are invariant to\ntarget domain image representations. One of the important issues with\nimage-to-image translation models is that they fail miserably when the\ninformation content between two image domains are asymmetric in nature. In this\nregard, the paper introduces a new deep generative model for segmenting nuclei\nstructures from histological images. The proposed model considers an embedding\nspace for handling information-disparity between information-rich histological\nimage space and information-poor segmentation map domain. Integrating\njudiciously the concepts of optimal transport and measure theory, the model\ndevelops an invertible generator, which provides an efficient optimization\nframework with lower network complexity. The concept of invertible generator\nautomatically eliminates the need of any explicit cycle-consistency loss. The\nproposed model also introduces a spatially-constrained squeeze operation within\nthe framework of invertible generator to maintain spatial continuity within the\nimage patches. The model provides a better trade-off between network complexity\nand model performance compared to other existing models having complex network\narchitectures. The performance of the proposed deep generative model, along\nwith a comparison with state-of-the-art nuclei segmentation methods, is\ndemonstrated on publicly available histological image data sets.", "AI": {"tldr": "The paper proposes a deep generative model for nuclei segmentation in histological images, addressing information asymmetry between image domains using an invertible generator and optimal transport.", "motivation": "Nuclei segmentation aids disease diagnosis, but existing image-to-image translation models fail with asymmetric information content between domains.", "method": "The model uses an invertible generator with optimal transport and measure theory, eliminating cycle-consistency loss and incorporating a spatially-constrained squeeze operation.", "result": "The model achieves a better complexity-performance trade-off than existing methods, validated on public histological datasets.", "conclusion": "The proposed model effectively segments nuclei with lower complexity and improved performance, advancing histological image analysis."}}
{"id": "2506.07028", "pdf": "https://arxiv.org/pdf/2506.07028", "abs": "https://arxiv.org/abs/2506.07028", "authors": ["Suman Mahapatra", "Pradipta Maji"], "title": "SiliCoN: Simultaneous Nuclei Segmentation and Color Normalization of Histological Images", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "10 pages, 9 figures", "summary": "Segmentation of nuclei regions from histological images is an important task\nfor automated computer-aided analysis of histological images, particularly in\nthe presence of impermissible color variation in the color appearance of\nstained tissue images. While color normalization enables better nuclei\nsegmentation, accurate segmentation of nuclei structures makes color\nnormalization rather trivial. In this respect, the paper proposes a novel deep\ngenerative model for simultaneously segmenting nuclei structures and\nnormalizing color appearance of stained histological images.This model\njudiciously integrates the merits of truncated normal distribution and spatial\nattention. The model assumes that the latent color appearance information,\ncorresponding to a particular histological image, is independent of respective\nnuclei segmentation map as well as embedding map information. The disentangled\nrepresentation makes the model generalizable and adaptable as the modification\nor loss in color appearance information cannot be able to affect the nuclei\nsegmentation map as well as embedding information. Also, for dealing with the\nstain overlap of associated histochemical reagents, the prior for latent color\nappearance code is assumed to be a mixture of truncated normal distributions.\nThe proposed model incorporates the concept of spatial attention for\nsegmentation of nuclei regions from histological images. The performance of the\nproposed approach, along with a comparative analysis with related\nstate-of-the-art algorithms, has been demonstrated on publicly available\nstandard histological image data sets.", "AI": {"tldr": "The paper introduces a deep generative model for simultaneous nuclei segmentation and color normalization in histological images, using truncated normal distribution and spatial attention.", "motivation": "Accurate nuclei segmentation and color normalization are critical for histological image analysis, but existing methods struggle with color variation and stain overlap.", "method": "A novel deep generative model integrates truncated normal distribution and spatial attention, assuming disentangled representations for color appearance and nuclei segmentation.", "result": "The model demonstrates effective performance on standard histological datasets, outperforming state-of-the-art methods.", "conclusion": "The proposed approach is generalizable and adaptable, addressing key challenges in nuclei segmentation and color normalization."}}
{"id": "2506.07046", "pdf": "https://arxiv.org/pdf/2506.07046", "abs": "https://arxiv.org/abs/2506.07046", "authors": ["Anushka Jha", "Tanushree Dewangan", "Mukul Lokhande", "Santosh Kumar Vishvakarma"], "title": "QForce-RL: Quantized FPGA-Optimized Reinforcement Learning Compute Engine", "categories": ["cs.AR", "cs.CV", "cs.RO", "eess.IV"], "comment": null, "summary": "Reinforcement Learning (RL) has outperformed other counterparts in sequential\ndecision-making and dynamic environment control. However, FPGA deployment is\nsignificantly resource-expensive, as associated with large number of\ncomputations in training agents with high-quality images and possess new\nchallenges. In this work, we propose QForce-RL takes benefits of quantization\nto enhance throughput and reduce energy footprint with light-weight RL\narchitecture, without significant performance degradation. QForce-RL takes\nadvantages from E2HRL to reduce overall RL actions to learn desired policy and\nQuaRL for quantization based SIMD for hardware acceleration. We have also\nprovided detailed analysis for different RL environments, with emphasis on\nmodel size, parameters, and accelerated compute ops. The architecture is\nscalable for resource-constrained devices and provide parametrized efficient\ndeployment with flexibility in latency, throughput, power, and energy\nefficiency. The proposed QForce-RL provides performance enhancement up to 2.3x\nand better FPS - 2.6x compared to SoTA works.", "AI": {"tldr": "QForce-RL leverages quantization and lightweight RL architecture to improve throughput and energy efficiency on FPGAs without major performance loss, achieving 2.3x performance and 2.6x FPS gains over state-of-the-art.", "motivation": "FPGA deployment for RL is resource-intensive due to high computations with quality images, prompting the need for efficient solutions.", "method": "QForce-RL uses E2HRL to reduce RL actions and QuaRL for quantization-based SIMD hardware acceleration, analyzing model size, parameters, and compute ops.", "result": "Achieves 2.3x performance improvement and 2.6x better FPS compared to state-of-the-art methods.", "conclusion": "QForce-RL is scalable for resource-constrained devices, offering efficient deployment with flexibility in latency, throughput, power, and energy."}}
{"id": "2506.07069", "pdf": "https://arxiv.org/pdf/2506.07069", "abs": "https://arxiv.org/abs/2506.07069", "authors": ["Zhican Wang", "Guanghui He", "Dantong Liu", "Lingjun Gao", "Shell Xu Hu", "Chen Zhang", "Zhuoran Song", "Nicholas Lane", "Wayne Luk", "Hongxiang Fan"], "title": "Accelerating 3D Gaussian Splatting with Neural Sorting and Axis-Oriented Rasterization", "categories": ["cs.GR", "cs.AR", "cs.CV", "cs.LG"], "comment": "Preprint. Under review", "summary": "3D Gaussian Splatting (3DGS) has recently gained significant attention for\nhigh-quality and efficient view synthesis, making it widely adopted in fields\nsuch as AR/VR, robotics, and autonomous driving. Despite its impressive\nalgorithmic performance, real-time rendering on resource-constrained devices\nremains a major challenge due to tight power and area budgets. This paper\npresents an architecture-algorithm co-design to address these inefficiencies.\nFirst, we reveal substantial redundancy caused by repeated computation of\ncommon terms/expressions during the conventional rasterization. To resolve\nthis, we propose axis-oriented rasterization, which pre-computes and reuses\nshared terms along both the X and Y axes through a dedicated hardware design,\neffectively reducing multiply-and-add (MAC) operations by up to 63%. Second, by\nidentifying the resource and performance inefficiency of the sorting process,\nwe introduce a novel neural sorting approach that predicts order-independent\nblending weights using an efficient neural network, eliminating the need for\ncostly hardware sorters. A dedicated training framework is also proposed to\nimprove its algorithmic stability. Third, to uniformly support rasterization\nand neural network inference, we design an efficient reconfigurable processing\narray that maximizes hardware utilization and throughput. Furthermore, we\nintroduce a $\\pi$-trajectory tile schedule, inspired by Morton encoding and\nHilbert curve, to optimize Gaussian reuse and reduce memory access overhead.\nComprehensive experiments demonstrate that the proposed design preserves\nrendering quality while achieving a speedup of $23.4\\sim27.8\\times$ and energy\nsavings of $28.8\\sim51.4\\times$ compared to edge GPUs for real-world scenes. We\nplan to open-source our design to foster further development in this field.", "AI": {"tldr": "The paper proposes an architecture-algorithm co-design to optimize 3D Gaussian Splatting (3DGS) for real-time rendering on resource-constrained devices, achieving significant speedup and energy savings.", "motivation": "Real-time rendering of 3DGS on resource-constrained devices is challenging due to power and area limitations. The paper aims to address inefficiencies in computation and sorting.", "method": "The approach includes axis-oriented rasterization to reduce redundancy, a neural sorting method to replace hardware sorters, a reconfigurable processing array, and a tile scheduling technique.", "result": "The design achieves a 23.4\u223c27.8\u00d7 speedup and 28.8\u223c51.4\u00d7 energy savings compared to edge GPUs while preserving rendering quality.", "conclusion": "The proposed co-design effectively optimizes 3DGS for real-time applications, with plans to open-source the design for further development."}}
{"id": "2506.07209", "pdf": "https://arxiv.org/pdf/2506.07209", "abs": "https://arxiv.org/abs/2506.07209", "authors": ["Lei Li", "Angela Dai"], "title": "HOI-PAGE: Zero-Shot Human-Object Interaction Generation with Part Affordance Guidance", "categories": ["cs.GR", "cs.CV"], "comment": "Project page: https://hoipage.github.io/ Video:\n  https://youtu.be/b1pJU9lKQTE", "summary": "We present HOI-PAGE, a new approach to synthesizing 4D human-object\ninteractions (HOIs) from text prompts in a zero-shot fashion, driven by\npart-level affordance reasoning. In contrast to prior works that focus on\nglobal, whole body-object motion for 4D HOI synthesis, we observe that\ngenerating realistic and diverse HOIs requires a finer-grained understanding --\nat the level of how human body parts engage with object parts. We thus\nintroduce Part Affordance Graphs (PAGs), a structured HOI representation\ndistilled from large language models (LLMs) that encodes fine-grained part\ninformation along with contact relations. We then use these PAGs to guide a\nthree-stage synthesis: first, decomposing input 3D objects into geometric\nparts; then, generating reference HOI videos from text prompts, from which we\nextract part-based motion constraints; finally, optimizing for 4D HOI motion\nsequences that not only mimic the reference dynamics but also satisfy\npart-level contact constraints. Extensive experiments show that our approach is\nflexible and capable of generating complex multi-object or multi-person\ninteraction sequences, with significantly improved realism and text alignment\nfor zero-shot 4D HOI generation.", "AI": {"tldr": "HOI-PAGE synthesizes 4D human-object interactions (HOIs) from text prompts using part-level affordance reasoning, improving realism and diversity.", "motivation": "Prior works lack fine-grained understanding of how human body parts interact with object parts, limiting realism and diversity in 4D HOI synthesis.", "method": "Introduces Part Affordance Graphs (PAGs) from LLMs, guiding a three-stage synthesis: object decomposition, reference video generation, and motion optimization.", "result": "Generates complex, realistic HOIs with better text alignment, outperforming prior zero-shot methods.", "conclusion": "HOI-PAGE advances 4D HOI synthesis by leveraging part-level reasoning, enabling more realistic and diverse interactions."}}
{"id": "2506.07218", "pdf": "https://arxiv.org/pdf/2506.07218", "abs": "https://arxiv.org/abs/2506.07218", "authors": ["Tong Xiao", "Xin Xu", "Zhenya Huang", "Hongyu Gao", "Quan Liu", "Qi Liu", "Enhong Chen"], "title": "Advancing Multimodal Reasoning Capabilities of Multimodal Large Language Models via Visual Perception Reward", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Enhancing the multimodal reasoning capabilities of Multimodal Large Language\nModels (MLLMs) is a challenging task that has attracted increasing attention in\nthe community. Recently, several studies have applied Reinforcement Learning\nwith Verifiable Rewards (RLVR) to the multimodal domain in order to enhance the\nreasoning abilities of MLLMs. However, these works largely overlook the\nenhancement of multimodal perception capabilities in MLLMs, which serve as a\ncore prerequisite and foundational component of complex multimodal reasoning.\nThrough McNemar's test, we find that existing RLVR method fails to effectively\nenhance the multimodal perception capabilities of MLLMs, thereby limiting their\nfurther improvement in multimodal reasoning. To address this limitation, we\npropose Perception-R1, which introduces a novel visual perception reward that\nexplicitly encourages MLLMs to perceive the visual content accurately, thereby\ncan effectively incentivizing both their multimodal perception and reasoning\ncapabilities. Specifically, we first collect textual visual annotations from\nthe CoT trajectories of multimodal problems, which will serve as visual\nreferences for reward assignment. During RLVR training, we employ a judging LLM\nto assess the consistency between the visual annotations and the responses\ngenerated by MLLM, and assign the visual perception reward based on these\nconsistency judgments. Extensive experiments on several multimodal reasoning\nbenchmarks demonstrate the effectiveness of our Perception-R1, which achieves\nstate-of-the-art performance on most benchmarks using only 1,442 training data.", "AI": {"tldr": "The paper introduces Perception-R1, a method to enhance multimodal perception and reasoning in MLLMs using a novel visual perception reward, outperforming existing RLVR approaches.", "motivation": "Existing RLVR methods focus on reasoning but neglect multimodal perception, a core prerequisite for MLLMs. This limits their reasoning improvement.", "method": "Perception-R1 uses textual visual annotations from CoT trajectories and a judging LLM to assign visual perception rewards based on consistency with MLLM responses.", "result": "Perception-R1 achieves state-of-the-art performance on multimodal benchmarks with only 1,442 training samples.", "conclusion": "The proposed method effectively enhances both perception and reasoning in MLLMs, addressing a critical limitation of prior RLVR approaches."}}
{"id": "2506.07228", "pdf": "https://arxiv.org/pdf/2506.07228", "abs": "https://arxiv.org/abs/2506.07228", "authors": ["Shuvashis Sarker"], "title": "Transfer Learning and Explainable AI for Brain Tumor Classification: A Study Using MRI Data from Bangladesh", "categories": ["eess.IV", "cs.CV"], "comment": "2024 6th International Conference on Sustainable Technologies for\n  Industry 5.0 (STI)", "summary": "Brain tumors, regardless of being benign or malignant, pose considerable\nhealth risks, with malignant tumors being more perilous due to their swift and\nuncontrolled proliferation, resulting in malignancy. Timely identification is\ncrucial for enhancing patient outcomes, particularly in nations such as\nBangladesh, where healthcare infrastructure is constrained. Manual MRI analysis\nis arduous and susceptible to inaccuracies, rendering it inefficient for prompt\ndiagnosis. This research sought to tackle these problems by creating an\nautomated brain tumor classification system utilizing MRI data obtained from\nmany hospitals in Bangladesh. Advanced deep learning models, including VGG16,\nVGG19, and ResNet50, were utilized to classify glioma, meningioma, and various\nbrain cancers. Explainable AI (XAI) methodologies, such as Grad-CAM and\nGrad-CAM++, were employed to improve model interpretability by emphasizing the\ncritical areas in MRI scans that influenced the categorization. VGG16 achieved\nthe most accuracy, attaining 99.17%. The integration of XAI enhanced the\nsystem's transparency and stability, rendering it more appropriate for clinical\napplication in resource-limited environments such as Bangladesh. This study\nhighlights the capability of deep learning models, in conjunction with\nexplainable artificial intelligence (XAI), to enhance brain tumor detection and\nidentification in areas with restricted access to advanced medical\ntechnologies.", "AI": {"tldr": "An automated brain tumor classification system using deep learning (VGG16, VGG19, ResNet50) and XAI (Grad-CAM, Grad-CAM++) achieved 99.17% accuracy, aiding timely diagnosis in resource-limited settings like Bangladesh.", "motivation": "Manual MRI analysis is error-prone and inefficient, especially in regions with constrained healthcare infrastructure like Bangladesh. Automated systems can improve diagnosis speed and accuracy.", "method": "Utilized deep learning models (VGG16, VGG19, ResNet50) and XAI techniques (Grad-CAM, Grad-CAM++) to classify brain tumors from MRI data.", "result": "VGG16 achieved the highest accuracy (99.17%). XAI improved model interpretability and clinical applicability.", "conclusion": "Deep learning with XAI can enhance brain tumor detection in resource-limited areas, offering a viable solution for timely diagnosis."}}
{"id": "2506.07234", "pdf": "https://arxiv.org/pdf/2506.07234", "abs": "https://arxiv.org/abs/2506.07234", "authors": ["Shuvashis Sarker"], "title": "A Comprehensive Analysis of COVID-19 Detection Using Bangladeshi Data and Explainable AI", "categories": ["eess.IV", "cs.CV"], "comment": "2024 4th International Conference on Innovations in Science,\n  Engineering and Technology (ICISET)", "summary": "COVID-19 is a rapidly spreading and highly infectious virus which has\ntriggered a global pandemic, profoundly affecting millions across the world.\nThe pandemic has introduced unprecedented challenges in public health, economic\nstability, and societal structures, necessitating the implementation of\nextensive and multifaceted health interventions globally. It had a tremendous\nimpact on Bangladesh by April 2024, with around 29,495 fatalities and more than\n2 million confirmed cases. This study focuses on improving COVID-19 detection\nin CXR images by utilizing a dataset of 4,350 images from Bangladesh\ncategorized into four classes: Normal, Lung-Opacity, COVID-19 and\nViral-Pneumonia. ML, DL and TL models are employed with the VGG19 model\nachieving an impressive 98% accuracy. LIME is used to explain model\npredictions, highlighting the regions and features influencing classification\ndecisions. SMOTE is applied to address class imbalances. By providing insight\ninto both correct and incorrect classifications, the study emphasizes the\nimportance of XAI in enhancing the transparency and reliability of models,\nultimately improving the effectiveness of detection from CXR images.", "AI": {"tldr": "The study improves COVID-19 detection in CXR images using ML/DL models, achieving 98% accuracy with VGG19, and emphasizes XAI for transparency.", "motivation": "COVID-19's global impact and high cases in Bangladesh necessitate better detection methods from CXR images.", "method": "Utilized ML, DL, and TL models (VGG19) on 4,350 CXR images; applied LIME for explainability and SMOTE for class imbalance.", "result": "VGG19 achieved 98% accuracy; LIME highlighted influential regions for classification.", "conclusion": "XAI enhances model transparency and reliability, improving COVID-19 detection from CXR images."}}
{"id": "2506.07236", "pdf": "https://arxiv.org/pdf/2506.07236", "abs": "https://arxiv.org/abs/2506.07236", "authors": ["Jiachen Zhong", "Yiting Wang", "Di Zhu", "Ziwei Wang"], "title": "A Narrative Review on Large AI Models in Lung Cancer Screening, Diagnosis, and Treatment Planning", "categories": ["eess.IV", "cs.CV"], "comment": "Under Review", "summary": "Lung cancer remains one of the most prevalent and fatal diseases worldwide,\ndemanding accurate and timely diagnosis and treatment. Recent advancements in\nlarge AI models have significantly enhanced medical image understanding and\nclinical decision-making. This review systematically surveys the\nstate-of-the-art in applying large AI models to lung cancer screening,\ndiagnosis, prognosis, and treatment. We categorize existing models into\nmodality-specific encoders, encoder-decoder frameworks, and joint encoder\narchitectures, highlighting key examples such as CLIP, BLIP, Flamingo,\nBioViL-T, and GLoRIA. We further examine their performance in multimodal\nlearning tasks using benchmark datasets like LIDC-IDRI, NLST, and MIMIC-CXR.\nApplications span pulmonary nodule detection, gene mutation prediction,\nmulti-omics integration, and personalized treatment planning, with emerging\nevidence of clinical deployment and validation. Finally, we discuss current\nlimitations in generalizability, interpretability, and regulatory compliance,\nproposing future directions for building scalable, explainable, and clinically\nintegrated AI systems. Our review underscores the transformative potential of\nlarge AI models to personalize and optimize lung cancer care.", "AI": {"tldr": "A review of large AI models in lung cancer care, covering screening, diagnosis, prognosis, and treatment, with insights into current applications and future challenges.", "motivation": "Lung cancer's high prevalence and fatality necessitate improved diagnostic and treatment methods, with AI models offering promising advancements.", "method": "Systematic survey of state-of-the-art AI models (e.g., CLIP, BLIP) categorized into modality-specific encoders, encoder-decoder frameworks, and joint encoder architectures, evaluated on datasets like LIDC-IDRI.", "result": "AI models show promise in tasks like nodule detection and personalized treatment, though limitations in generalizability and interpretability remain.", "conclusion": "Large AI models have transformative potential for lung cancer care but require further development in scalability, explainability, and clinical integration."}}
{"id": "2506.07296", "pdf": "https://arxiv.org/pdf/2506.07296", "abs": "https://arxiv.org/abs/2506.07296", "authors": ["Arian Askari", "Emmanouil Stergiadis", "Ilya Gusev", "Moran Beladev"], "title": "HotelMatch-LLM: Joint Multi-Task Training of Small and Large Language Models for Efficient Multimodal Hotel Retrieval", "categories": ["cs.IR", "cs.AI", "cs.CV"], "comment": "Accepted at ACL 2025, Main track. 13 Pages, 1 figure", "summary": "We present HotelMatch-LLM, a multimodal dense retrieval model for the travel\ndomain that enables natural language property search, addressing the\nlimitations of traditional travel search engines which require users to start\nwith a destination and editing search parameters. HotelMatch-LLM features three\nkey innovations: (1) Domain-specific multi-task optimization with three novel\nretrieval, visual, and language modeling objectives; (2) Asymmetrical dense\nretrieval architecture combining a small language model (SLM) for efficient\nonline query processing and a large language model (LLM) for embedding hotel\ndata; and (3) Extensive image processing to handle all property image\ngalleries. Experiments on four diverse test sets show HotelMatch-LLM\nsignificantly outperforms state-of-the-art models, including VISTA and MARVEL.\nSpecifically, on the test set -- main query type -- we achieve 0.681 for\nHotelMatch-LLM compared to 0.603 for the most effective baseline, MARVEL. Our\nanalysis highlights the impact of our multi-task optimization, the\ngeneralizability of HotelMatch-LLM across LLM architectures, and its\nscalability for processing large image galleries.", "AI": {"tldr": "HotelMatch-LLM is a multimodal dense retrieval model for travel, improving natural language property search with domain-specific multi-task optimization, asymmetrical architecture, and image processing.", "motivation": "Address limitations of traditional travel search engines that require destination-first searches and parameter editing.", "method": "Combines multi-task optimization (retrieval, visual, language), asymmetrical dense retrieval (SLM for queries, LLM for hotel data), and extensive image processing.", "result": "Outperforms state-of-the-art models (e.g., MARVEL) with a score of 0.681 vs. 0.603 on the main query type.", "conclusion": "HotelMatch-LLM is effective, generalizable across LLM architectures, and scalable for large image galleries."}}
{"id": "2506.07301", "pdf": "https://arxiv.org/pdf/2506.07301", "abs": "https://arxiv.org/abs/2506.07301", "authors": ["Marco P. M. de Souza", "Juciane G. Maia", "Lilian N. de Andrade"], "title": "Pendulum Tracker -- SimuF\u00edsica: A Web-based Tool for Real-time Measurement of Oscillatory Motion", "categories": ["physics.ed-ph", "cs.CV"], "comment": null, "summary": "We present Pendulum Tracker, a computer vision-based application that enables\nreal-time measurement of the oscillatory motion of a physical pendulum.\nIntegrated into the educational platform SimuF\\'isica, the system uses the\nOpenCV.js library and runs directly in the browser, working on computers,\ntablets, and smartphones. The application automatically detects the pendulum's\nposition via the device's camera, displaying in real time the angle-versus-time\ngraph and estimates of the oscillation period. Experimental case studies\ndemonstrate its effectiveness in measuring the period, determining\ngravitational acceleration, and analyzing damped oscillations. The results show\nexcellent agreement with theoretical predictions, confirming the system's\naccuracy and its applicability in educational contexts. The accessible\ninterface and the ability to export raw data make Pendulum Tracker a versatile\ntool for experimental physics teaching.", "AI": {"tldr": "Pendulum Tracker is a browser-based tool using OpenCV.js for real-time pendulum motion analysis, aiding physics education with accurate measurements and data export.", "motivation": "To provide an accessible, real-time tool for measuring pendulum oscillations in educational settings, enhancing experimental physics teaching.", "method": "Uses OpenCV.js for computer vision to detect pendulum motion via device cameras, displaying angle-time graphs and period estimates.", "result": "Accurate period measurements, gravitational acceleration estimates, and damped oscillation analysis, aligning with theoretical predictions.", "conclusion": "Pendulum Tracker is an effective, versatile tool for physics education, offering real-time data and accessibility across devices."}}
{"id": "2506.07350", "pdf": "https://arxiv.org/pdf/2506.07350", "abs": "https://arxiv.org/abs/2506.07350", "authors": ["Yijie Deng", "Shuaihang Yuan", "Congcong Wen", "Hao Huang", "Anthony Tzes", "Geeta Chandra Raju Bethala", "Yi Fang"], "title": "MapBERT: Bitwise Masked Modeling for Real-Time Semantic Mapping Generation", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Spatial awareness is a critical capability for embodied agents, as it enables\nthem to anticipate and reason about unobserved regions. The primary challenge\narises from learning the distribution of indoor semantics, complicated by\nsparse, imbalanced object categories and diverse spatial scales. Existing\nmethods struggle to robustly generate unobserved areas in real time and do not\ngeneralize well to new environments. To this end, we propose \\textbf{MapBERT},\na novel framework designed to effectively model the distribution of unseen\nspaces. Motivated by the observation that the one-hot encoding of semantic maps\naligns naturally with the binary structure of bit encoding, we, for the first\ntime, leverage a lookup-free BitVAE to encode semantic maps into compact\nbitwise tokens. Building on this, a masked transformer is employed to infer\nmissing regions and generate complete semantic maps from limited observations.\nTo enhance object-centric reasoning, we propose an object-aware masking\nstrategy that masks entire object categories concurrently and pairs them with\nlearnable embeddings, capturing implicit relationships between object\nembeddings and spatial tokens. By learning these relationships, the model more\neffectively captures indoor semantic distributions crucial for practical\nrobotic tasks. Experiments on Gibson benchmarks show that MapBERT achieves\nstate-of-the-art semantic map generation, balancing computational efficiency\nwith accurate reconstruction of unobserved regions.", "AI": {"tldr": "MapBERT is a novel framework using BitVAE and masked transformers to model unseen indoor spaces, improving semantic map generation for robotics.", "motivation": "Existing methods struggle with real-time generation and generalization of unobserved areas in indoor environments.", "method": "Leverages BitVAE for compact bitwise encoding of semantic maps and a masked transformer for inference. Introduces object-aware masking and learnable embeddings.", "result": "Achieves state-of-the-art performance on Gibson benchmarks, balancing efficiency and accuracy.", "conclusion": "MapBERT effectively models indoor semantic distributions, enhancing robotic spatial awareness."}}
{"id": "2506.07400", "pdf": "https://arxiv.org/pdf/2506.07400", "abs": "https://arxiv.org/abs/2506.07400", "authors": ["Philip Liu", "Sparsh Bansal", "Jimmy Dinh", "Aditya Pawar", "Ramani Satishkumar", "Shail Desai", "Neeraj Gupta", "Xin Wang", "Shu Hu"], "title": "MedChat: A Multi-Agent Framework for Multimodal Diagnosis with Large Language Models", "categories": ["cs.MA", "cs.AI", "cs.CV", "cs.LG"], "comment": "7 pages, 6 figures. Accepted to the 2025 IEEE 8th International\n  Conference on Multimedia Information Processing and Retrieval (MIPR). Code\n  and platform available at https://github.com/Purdue-M2/MedChat", "summary": "The integration of deep learning-based glaucoma detection with large language\nmodels (LLMs) presents an automated strategy to mitigate ophthalmologist\nshortages and improve clinical reporting efficiency. However, applying general\nLLMs to medical imaging remains challenging due to hallucinations, limited\ninterpretability, and insufficient domain-specific medical knowledge, which can\npotentially reduce clinical accuracy. Although recent approaches combining\nimaging models with LLM reasoning have improved reporting, they typically rely\non a single generalist agent, restricting their capacity to emulate the diverse\nand complex reasoning found in multidisciplinary medical teams. To address\nthese limitations, we propose MedChat, a multi-agent diagnostic framework and\nplatform that combines specialized vision models with multiple role-specific\nLLM agents, all coordinated by a director agent. This design enhances\nreliability, reduces hallucination risk, and enables interactive diagnostic\nreporting through an interface tailored for clinical review and educational\nuse. Code available at https://github.com/Purdue-M2/MedChat.", "AI": {"tldr": "MedChat is a multi-agent framework combining specialized vision models and role-specific LLMs to improve glaucoma detection and reporting, addressing challenges like hallucinations and limited interpretability in general LLMs.", "motivation": "To mitigate ophthalmologist shortages and enhance clinical reporting efficiency by overcoming the limitations of general LLMs in medical imaging, such as hallucinations and insufficient domain knowledge.", "method": "Proposes MedChat, a multi-agent diagnostic framework with specialized vision models and role-specific LLM agents coordinated by a director agent, designed for interactive reporting.", "result": "Enhances reliability, reduces hallucination risk, and provides an interface for clinical review and educational use.", "conclusion": "MedChat offers a robust solution for integrating deep learning and LLMs in medical diagnostics, improving accuracy and usability."}}
{"id": "2506.07413", "pdf": "https://arxiv.org/pdf/2506.07413", "abs": "https://arxiv.org/abs/2506.07413", "authors": ["Ziwen Wang", "Jiajun Fan", "Thao Nguyen", "Heng Ji", "Ge Liu"], "title": "Variational Supervised Contrastive Learning", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Contrastive learning has proven to be highly efficient and adaptable in\nshaping representation spaces across diverse modalities by pulling similar\nsamples together and pushing dissimilar ones apart. However, two key\nlimitations persist: (1) Without explicit regulation of the embedding\ndistribution, semantically related instances can inadvertently be pushed apart\nunless complementary signals guide pair selection, and (2) excessive reliance\non large in-batch negatives and tailored augmentations hinders generalization.\nTo address these limitations, we propose Variational Supervised Contrastive\nLearning (VarCon), which reformulates supervised contrastive learning as\nvariational inference over latent class variables and maximizes a\nposterior-weighted evidence lower bound (ELBO) that replaces exhaustive\npair-wise comparisons for efficient class-aware matching and grants\nfine-grained control over intra-class dispersion in the embedding space.\nTrained exclusively on image data, our experiments on CIFAR-10, CIFAR-100,\nImageNet-100, and ImageNet-1K show that VarCon (1) achieves state-of-the-art\nperformance for contrastive learning frameworks, reaching 79.36% Top-1 accuracy\non ImageNet-1K and 78.29% on CIFAR-100 with a ResNet-50 encoder while\nconverging in just 200 epochs; (2) yields substantially clearer decision\nboundaries and semantic organization in the embedding space, as evidenced by\nKNN classification, hierarchical clustering results, and transfer-learning\nassessments; and (3) demonstrates superior performance in few-shot learning\nthan supervised baseline and superior robustness across various augmentation\nstrategies.", "AI": {"tldr": "VarCon improves contrastive learning by using variational inference for class-aware matching, achieving state-of-the-art results and better generalization.", "motivation": "Address limitations in contrastive learning: unregulated embedding distribution and reliance on large negatives/augmentations.", "method": "Reformulates supervised contrastive learning as variational inference, maximizing a posterior-weighted ELBO for efficient class-aware matching.", "result": "Achieves 79.36% Top-1 accuracy on ImageNet-1K, clearer decision boundaries, and superior few-shot learning performance.", "conclusion": "VarCon enhances contrastive learning with better control over embeddings and improved generalization."}}
{"id": "2506.07475", "pdf": "https://arxiv.org/pdf/2506.07475", "abs": "https://arxiv.org/abs/2506.07475", "authors": ["Gaoyu Chen"], "title": "Text-guided multi-stage cross-perception network for medical image segmentation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Medical image segmentation plays a crucial role in clinical medicine, serving\nas a tool for auxiliary diagnosis, treatment planning, and disease monitoring,\nthus facilitating physicians in the study and treatment of diseases. However,\nexisting medical image segmentation methods are limited by the weak semantic\nexpression of the target segmentation regions, which is caused by the low\ncontrast between the target and non-target segmentation regions. To address\nthis limitation, text prompt information has greast potential to capture the\nlesion location. However, existing text-guided methods suffer from insufficient\ncross-modal interaction and inadequate cross-modal feature expression. To\nresolve these issues, we propose the Text-guided Multi-stage Cross-perception\nnetwork (TMC). In TMC, we introduce a multistage cross-attention module to\nenhance the model's understanding of semantic details and a multi-stage\nalignment loss to improve the consistency of cross-modal semantics. The results\nof the experiments demonstrate that our TMC achieves a superior performance\nwith Dice of 84.77%, 78.50%, 88.73% in three public datasets (QaTa-COV19,\nMosMedData and Breast), outperforming UNet based networks and text-guided\nmethods.", "AI": {"tldr": "The paper proposes a Text-guided Multi-stage Cross-perception network (TMC) to improve medical image segmentation by leveraging text prompts and enhancing cross-modal interaction.", "motivation": "Existing methods struggle with weak semantic expression due to low contrast in medical images, and text-guided methods lack sufficient cross-modal interaction.", "method": "TMC introduces a multistage cross-attention module for better semantic understanding and a multi-stage alignment loss for cross-modal consistency.", "result": "TMC outperforms UNet-based and text-guided methods with Dice scores of 84.77%, 78.50%, and 88.73% on three datasets.", "conclusion": "TMC effectively addresses limitations in medical image segmentation by improving cross-modal feature expression and semantic consistency."}}
{"id": "2506.07530", "pdf": "https://arxiv.org/pdf/2506.07530", "abs": "https://arxiv.org/abs/2506.07530", "authors": ["Hongyu Wang", "Chuyan Xiong", "Ruiping Wang", "Xilin Chen"], "title": "BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation", "categories": ["cs.RO", "cs.CV"], "comment": "Work in progress", "summary": "Vision-Language-Action (VLA) models have shown impressive capabilities across\na wide range of robotics manipulation tasks. However, their growing model size\nposes significant challenges for deployment on resource-constrained robotic\nsystems. While 1-bit pretraining has proven effective for enhancing the\ninference efficiency of large language models with minimal performance loss,\nits application to VLA models remains underexplored. In this work, we present\nBitVLA, the first 1-bit VLA model for robotics manipulation, in which every\nparameter is ternary, i.e., {-1, 0, 1}. To further reduce the memory footprint\nof the vision encoder, we propose the distillation-aware training strategy that\ncompresses the full-precision encoder to 1.58-bit weights. During this process,\na full-precision encoder serves as a teacher model to better align latent\nrepresentations. Despite the lack of large-scale robotics pretraining, BitVLA\nachieves performance comparable to the state-of-the-art model OpenVLA-OFT with\n4-bit post-training quantization on the LIBERO benchmark, while consuming only\n29.8% of the memory. These results highlight BitVLA's promise for deployment on\nmemory-constrained edge devices. We release the code and model weights in\nhttps://github.com/ustcwhy/BitVLA.", "AI": {"tldr": "BitVLA is a 1-bit Vision-Language-Action (VLA) model for robotics, achieving efficient performance with minimal memory usage.", "motivation": "Address the challenge of deploying large VLA models on resource-constrained robotic systems by exploring 1-bit pretraining.", "method": "Introduces BitVLA, a ternary parameter model, and a distillation-aware training strategy to compress the vision encoder to 1.58-bit weights.", "result": "BitVLA matches state-of-the-art performance with 4-bit models on the LIBERO benchmark while using only 29.8% of the memory.", "conclusion": "BitVLA demonstrates potential for efficient deployment on edge devices, with code and weights publicly available."}}
{"id": "2506.07657", "pdf": "https://arxiv.org/pdf/2506.07657", "abs": "https://arxiv.org/abs/2506.07657", "authors": ["Zeyu Xiao", "Zhenyi Wu", "Mingyang Sun", "Qipeng Yan", "Yufan Guo", "Zhuoer Liang", "Lihua Zhang"], "title": "PIG: Physically-based Multi-Material Interaction with 3D Gaussians", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "3D Gaussian Splatting has achieved remarkable success in reconstructing both\nstatic and dynamic 3D scenes. However, in a scene represented by 3D Gaussian\nprimitives, interactions between objects suffer from inaccurate 3D\nsegmentation, imprecise deformation among different materials, and severe\nrendering artifacts. To address these challenges, we introduce PIG:\nPhysically-Based Multi-Material Interaction with 3D Gaussians, a novel approach\nthat combines 3D object segmentation with the simulation of interacting objects\nin high precision. Firstly, our method facilitates fast and accurate mapping\nfrom 2D pixels to 3D Gaussians, enabling precise 3D object-level segmentation.\nSecondly, we assign unique physical properties to correspondingly segmented\nobjects within the scene for multi-material coupled interactions. Finally, we\nhave successfully embedded constraint scales into deformation gradients,\nspecifically clamping the scaling and rotation properties of the Gaussian\nprimitives to eliminate artifacts and achieve geometric fidelity and visual\nconsistency. Experimental results demonstrate that our method not only\noutperforms the state-of-the-art (SOTA) in terms of visual quality, but also\nopens up new directions and pipelines for the field of physically realistic\nscene generation.", "AI": {"tldr": "PIG improves 3D Gaussian Splatting by enabling precise 3D segmentation, multi-material interactions, and artifact-free rendering.", "motivation": "Addressing inaccurate segmentation, imprecise deformation, and rendering artifacts in 3D Gaussian-based scene representations.", "method": "Combines 3D object segmentation with physical property assignment and constraint scales in deformation gradients.", "result": "Outperforms SOTA in visual quality and enables physically realistic scene generation.", "conclusion": "PIG advances 3D scene reconstruction with improved accuracy and realism."}}
{"id": "2506.07709", "pdf": "https://arxiv.org/pdf/2506.07709", "abs": "https://arxiv.org/abs/2506.07709", "authors": ["Xihua Sheng", "Peilin Chen", "Meng Wang", "Li Zhang", "Shiqi Wang", "Dapeng Oliver Wu"], "title": "Fine-Grained Motion Compression and Selective Temporal Fusion for Neural B-Frame Video Coding", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "With the remarkable progress in neural P-frame video coding, neural B-frame\ncoding has recently emerged as a critical research direction. However, most\nexisting neural B-frame codecs directly adopt P-frame coding tools without\nadequately addressing the unique challenges of B-frame compression, leading to\nsuboptimal performance. To bridge this gap, we propose novel enhancements for\nmotion compression and temporal fusion for neural B-frame coding. First, we\ndesign a fine-grained motion compression method. This method incorporates an\ninteractive dual-branch motion auto-encoder with per-branch adaptive\nquantization steps, which enables fine-grained compression of bi-directional\nmotion vectors while accommodating their asymmetric bitrate allocation and\nreconstruction quality requirements. Furthermore, this method involves an\ninteractive motion entropy model that exploits correlations between\nbi-directional motion latent representations by interactively leveraging\npartitioned latent segments as directional priors. Second, we propose a\nselective temporal fusion method that predicts bi-directional fusion weights to\nachieve discriminative utilization of bi-directional multi-scale temporal\ncontexts with varying qualities. Additionally, this method introduces a\nhyperprior-based implicit alignment mechanism for contextual entropy modeling.\nBy treating the hyperprior as a surrogate for the contextual latent\nrepresentation, this mechanism implicitly mitigates the misalignment in the\nfused bi-directional temporal priors. Extensive experiments demonstrate that\nour proposed codec outperforms state-of-the-art neural B-frame codecs and\nachieves comparable or even superior compression performance to the H.266/VVC\nreference software under random-access configurations.", "AI": {"tldr": "The paper proposes enhancements for neural B-frame coding, focusing on fine-grained motion compression and selective temporal fusion, outperforming existing methods and matching H.266/VVC performance.", "motivation": "Existing neural B-frame codecs inadequately address B-frame challenges by reusing P-frame tools, leading to suboptimal performance.", "method": "Introduces a fine-grained motion compression method with adaptive quantization and an interactive entropy model, plus a selective temporal fusion method with hyperprior-based alignment.", "result": "The proposed codec outperforms state-of-the-art neural B-frame codecs and matches H.266/VVC performance.", "conclusion": "The enhancements effectively address B-frame compression challenges, achieving superior performance."}}
{"id": "2506.07735", "pdf": "https://arxiv.org/pdf/2506.07735", "abs": "https://arxiv.org/abs/2506.07735", "authors": ["Haizhao Jing", "Haokui Zhang", "Zhenhao Shang", "Rong Xiao", "Peng Wang", "Yanning Zhang"], "title": "Language Embedding Meets Dynamic Graph: A New Exploration for Neural Architecture Representation Learning", "categories": ["cs.LG", "cs.CV"], "comment": "9 pages, 3 figures", "summary": "Neural Architecture Representation Learning aims to transform network models\ninto feature representations for predicting network attributes, playing a\ncrucial role in deploying and designing networks for real-world applications.\nRecently, inspired by the success of transformers, transformer-based models\nintegrated with Graph Neural Networks (GNNs) have achieved significant progress\nin representation learning. However, current methods still have some\nlimitations. First, existing methods overlook hardware attribute information,\nwhich conflicts with the current trend of diversified deep learning hardware\nand limits the practical applicability of models. Second, current encoding\napproaches rely on static adjacency matrices to represent topological\nstructures, failing to capture the structural differences between computational\nnodes, which ultimately compromises encoding effectiveness. In this paper, we\nintroduce LeDG-Former, an innovative framework that addresses these limitations\nthrough the synergistic integration of language-based semantic embedding and\ndynamic graph representation learning. Specifically, inspired by large language\nmodels (LLMs), we propose a language embedding framework where both neural\narchitectures and hardware platform specifications are projected into a unified\nsemantic space through tokenization and LLM processing, enabling zero-shot\nprediction across different hardware platforms for the first time. Then, we\npropose a dynamic graph-based transformer for modeling neural architectures,\nresulting in improved neural architecture modeling performance. On the NNLQP\nbenchmark, LeDG-Former surpasses previous methods, establishing a new SOTA\nwhile demonstrating the first successful cross-hardware latency prediction\ncapability. Furthermore, our framework achieves superior performance on the\ncell-structured NAS-Bench-101 and NAS-Bench-201 datasets.", "AI": {"tldr": "LeDG-Former integrates language-based semantic embedding and dynamic graph representation learning to improve neural architecture representation, addressing hardware attribute neglect and static adjacency limitations.", "motivation": "Current methods overlook hardware attributes and rely on static adjacency matrices, limiting practical applicability and encoding effectiveness.", "method": "Proposes LeDG-Former, combining language embedding (using LLMs) for unified semantic space projection and dynamic graph-based transformer for neural architecture modeling.", "result": "Achieves SOTA on NNLQP benchmark, enables cross-hardware latency prediction, and excels on NAS-Bench datasets.", "conclusion": "LeDG-Former effectively addresses prior limitations and advances neural architecture representation learning."}}
{"id": "2506.07806", "pdf": "https://arxiv.org/pdf/2506.07806", "abs": "https://arxiv.org/abs/2506.07806", "authors": ["Avinash Kori", "Francesca Toni", "Ben Glocker"], "title": "Identifiable Object Representations under Spatial Ambiguities", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Modular object-centric representations are essential for *human-like\nreasoning* but are challenging to obtain under spatial ambiguities, *e.g. due\nto occlusions and view ambiguities*. However, addressing challenges presents\nboth theoretical and practical difficulties. We introduce a novel multi-view\nprobabilistic approach that aggregates view-specific slots to capture\n*invariant content* information while simultaneously learning disentangled\nglobal *viewpoint-level* information. Unlike prior single-view methods, our\napproach resolves spatial ambiguities, provides theoretical guarantees for\nidentifiability, and requires *no viewpoint annotations*. Extensive experiments\non standard benchmarks and novel complex datasets validate our method's\nrobustness and scalability.", "AI": {"tldr": "A novel multi-view probabilistic method resolves spatial ambiguities in object-centric representations without viewpoint annotations, ensuring identifiability and scalability.", "motivation": "Modular object-centric representations are crucial for human-like reasoning but face challenges under spatial ambiguities like occlusions and view ambiguities.", "method": "Introduces a multi-view probabilistic approach that aggregates view-specific slots to capture invariant content and disentangled viewpoint-level information.", "result": "Validated on benchmarks and complex datasets, the method shows robustness and scalability.", "conclusion": "The approach effectively resolves spatial ambiguities and provides theoretical guarantees without requiring viewpoint annotations."}}
{"id": "2506.07883", "pdf": "https://arxiv.org/pdf/2506.07883", "abs": "https://arxiv.org/abs/2506.07883", "authors": ["Rajat Rasal", "Avinash Kori", "Fabio De Sousa Ribeiro", "Tian Xia", "Ben Glocker"], "title": "Diffusion Counterfactual Generation with Semantic Abduction", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "comment": "Proceedings of the 42nd International Conference on Machine Learning,\n  Vancouver, Canada", "summary": "Counterfactual image generation presents significant challenges, including\npreserving identity, maintaining perceptual quality, and ensuring faithfulness\nto an underlying causal model. While existing auto-encoding frameworks admit\nsemantic latent spaces which can be manipulated for causal control, they\nstruggle with scalability and fidelity. Advancements in diffusion models\npresent opportunities for improving counterfactual image editing, having\ndemonstrated state-of-the-art visual quality, human-aligned perception and\nrepresentation learning capabilities. Here, we present a suite of\ndiffusion-based causal mechanisms, introducing the notions of spatial, semantic\nand dynamic abduction. We propose a general framework that integrates semantic\nrepresentations into diffusion models through the lens of Pearlian causality to\nedit images via a counterfactual reasoning process. To our knowledge, this is\nthe first work to consider high-level semantic identity preservation for\ndiffusion counterfactuals and to demonstrate how semantic control enables\nprincipled trade-offs between faithful causal control and identity\npreservation.", "AI": {"tldr": "A diffusion-based framework for counterfactual image generation integrates semantic representations and Pearlian causality to improve identity preservation and causal control.", "motivation": "Address challenges in counterfactual image generation, such as identity preservation, perceptual quality, and faithfulness to causal models, by leveraging advancements in diffusion models.", "method": "Proposes a suite of diffusion-based causal mechanisms (spatial, semantic, dynamic abduction) and integrates semantic representations into diffusion models using Pearlian causality for counterfactual editing.", "result": "Demonstrates principled trade-offs between faithful causal control and identity preservation, achieving high-level semantic identity preservation in diffusion counterfactuals.", "conclusion": "The framework advances counterfactual image editing by combining diffusion models' visual quality with semantic and causal control, offering a novel approach to the field."}}
{"id": "2506.07897", "pdf": "https://arxiv.org/pdf/2506.07897", "abs": "https://arxiv.org/abs/2506.07897", "authors": ["Shuja Khalid", "Mohamed Ibrahim", "Yang Liu"], "title": "GaussianVAE: Adaptive Learning Dynamics of 3D Gaussians for High-Fidelity Super-Resolution", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "We present a novel approach for enhancing the resolution and geometric\nfidelity of 3D Gaussian Splatting (3DGS) beyond native training resolution.\nCurrent 3DGS methods are fundamentally limited by their input resolution,\nproducing reconstructions that cannot extrapolate finer details than are\npresent in the training views. Our work breaks this limitation through a\nlightweight generative model that predicts and refines additional 3D Gaussians\nwhere needed most. The key innovation is our Hessian-assisted sampling\nstrategy, which intelligently identifies regions that are likely to benefit\nfrom densification, ensuring computational efficiency. Unlike computationally\nintensive GANs or diffusion approaches, our method operates in real-time\n(0.015s per inference on a single consumer-grade GPU), making it practical for\ninteractive applications. Comprehensive experiments demonstrate significant\nimprovements in both geometric accuracy and rendering quality compared to\nstate-of-the-art methods, establishing a new paradigm for resolution-free 3D\nscene enhancement.", "AI": {"tldr": "A novel method enhances 3D Gaussian Splatting resolution and fidelity beyond native training limits using a lightweight generative model and Hessian-assisted sampling.", "motivation": "Current 3DGS methods are limited by input resolution, unable to extrapolate finer details. This work aims to overcome this limitation.", "method": "Uses a lightweight generative model to predict and refine additional 3D Gaussians, with Hessian-assisted sampling for efficient densification.", "result": "Achieves real-time performance (0.015s per inference) and improves geometric accuracy and rendering quality over state-of-the-art methods.", "conclusion": "Introduces a resolution-free 3D scene enhancement paradigm, practical for interactive applications."}}
{"id": "2506.07903", "pdf": "https://arxiv.org/pdf/2506.07903", "abs": "https://arxiv.org/abs/2506.07903", "authors": ["Kevin Rojas", "Yuchen Zhu", "Sichen Zhu", "Felix X. -F. Ye", "Molei Tao"], "title": "Diffuse Everything: Multimodal Diffusion Models on Arbitrary State Spaces", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "Accepted to ICML 2025. Code available at\n  https://github.com/KevinRojas1499/Diffuse-Everything", "summary": "Diffusion models have demonstrated remarkable performance in generating\nunimodal data across various tasks, including image, video, and text\ngeneration. On the contrary, the joint generation of multimodal data through\ndiffusion models is still in the early stages of exploration. Existing\napproaches heavily rely on external preprocessing protocols, such as tokenizers\nand variational autoencoders, to harmonize varied data representations into a\nunified, unimodal format. This process heavily demands the high accuracy of\nencoders and decoders, which can be problematic for applications with limited\ndata. To lift this restriction, we propose a novel framework for building\nmultimodal diffusion models on arbitrary state spaces, enabling native\ngeneration of coupled data across different modalities. By introducing an\ninnovative decoupled noise schedule for each modality, we enable both\nunconditional and modality-conditioned generation within a single model\nsimultaneously. We empirically validate our approach for text-image generation\nand mixed-type tabular data synthesis, demonstrating that it achieves\ncompetitive performance.", "AI": {"tldr": "A novel framework for multimodal diffusion models enables native generation of coupled data across modalities without relying on external preprocessing.", "motivation": "Existing methods for joint multimodal data generation rely heavily on external preprocessing, which is problematic for limited-data applications.", "method": "Proposes a framework with a decoupled noise schedule for each modality, allowing unconditional and modality-conditioned generation in a single model.", "result": "Validated for text-image generation and mixed-type tabular data synthesis, achieving competitive performance.", "conclusion": "The framework lifts restrictions of external preprocessing and enables flexible multimodal data generation."}}
{"id": "2506.07917", "pdf": "https://arxiv.org/pdf/2506.07917", "abs": "https://arxiv.org/abs/2506.07917", "authors": ["Allen Tu", "Haiyang Ying", "Alex Hanson", "Yonghan Lee", "Tom Goldstein", "Matthias Zwicker"], "title": "Speedy Deformable 3D Gaussian Splatting: Fast Rendering and Compression of Dynamic Scenes", "categories": ["cs.GR", "cs.CV"], "comment": "Project Page: https://speede3dgs.github.io/", "summary": "Recent extensions of 3D Gaussian Splatting (3DGS) to dynamic scenes achieve\nhigh-quality novel view synthesis by using neural networks to predict the\ntime-varying deformation of each Gaussian. However, performing per-Gaussian\nneural inference at every frame poses a significant bottleneck, limiting\nrendering speed and increasing memory and compute requirements. In this paper,\nwe present Speedy Deformable 3D Gaussian Splatting (SpeeDe3DGS), a general\npipeline for accelerating the rendering speed of dynamic 3DGS and 4DGS\nrepresentations by reducing neural inference through two complementary\ntechniques. First, we propose a temporal sensitivity pruning score that\nidentifies and removes Gaussians with low contribution to the dynamic scene\nreconstruction. We also introduce an annealing smooth pruning mechanism that\nimproves pruning robustness in real-world scenes with imprecise camera poses.\nSecond, we propose GroupFlow, a motion analysis technique that clusters\nGaussians by trajectory similarity and predicts a single rigid transformation\nper group instead of separate deformations for each Gaussian. Together, our\ntechniques accelerate rendering by $10.37\\times$, reduce model size by\n$7.71\\times$, and shorten training time by $2.71\\times$ on the NeRF-DS dataset.\nSpeeDe3DGS also improves rendering speed by $4.20\\times$ and $58.23\\times$ on\nthe D-NeRF and HyperNeRF vrig datasets. Our methods are modular and can be\nintegrated into any deformable 3DGS or 4DGS framework.", "AI": {"tldr": "SpeeDe3DGS accelerates dynamic 3D Gaussian Splatting by pruning low-contribution Gaussians and clustering motion, achieving faster rendering and smaller models.", "motivation": "Per-Gaussian neural inference in dynamic 3DGS is slow and resource-heavy, limiting real-time applications.", "method": "Uses temporal sensitivity pruning and GroupFlow for motion clustering to reduce neural inference.", "result": "Achieves 10.37x faster rendering, 7.71x smaller models, and 2.71x shorter training.", "conclusion": "SpeeDe3DGS is a modular, efficient solution for dynamic 3DGS, improving speed and scalability."}}
{"id": "2506.07932", "pdf": "https://arxiv.org/pdf/2506.07932", "abs": "https://arxiv.org/abs/2506.07932", "authors": ["Rishit Dagli", "Yushi Guan", "Sankeerth Durvasula", "Mohammadreza Mofayezi", "Nandita Vijaykumar"], "title": "Squeeze3D: Your 3D Generation Model is Secretly an Extreme Neural Compressor", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": null, "summary": "We propose Squeeze3D, a novel framework that leverages implicit prior\nknowledge learnt by existing pre-trained 3D generative models to compress 3D\ndata at extremely high compression ratios. Our approach bridges the latent\nspaces between a pre-trained encoder and a pre-trained generation model through\ntrainable mapping networks. Any 3D model represented as a mesh, point cloud, or\na radiance field is first encoded by the pre-trained encoder and then\ntransformed (i.e. compressed) into a highly compact latent code. This latent\ncode can effectively be used as an extremely compressed representation of the\nmesh or point cloud. A mapping network transforms the compressed latent code\ninto the latent space of a powerful generative model, which is then conditioned\nto recreate the original 3D model (i.e. decompression). Squeeze3D is trained\nentirely on generated synthetic data and does not require any 3D datasets. The\nSqueeze3D architecture can be flexibly used with existing pre-trained 3D\nencoders and existing generative models. It can flexibly support different\nformats, including meshes, point clouds, and radiance fields. Our experiments\ndemonstrate that Squeeze3D achieves compression ratios of up to 2187x for\ntextured meshes, 55x for point clouds, and 619x for radiance fields while\nmaintaining visual quality comparable to many existing methods. Squeeze3D only\nincurs a small compression and decompression latency since it does not involve\ntraining object-specific networks to compress an object.", "AI": {"tldr": "Squeeze3D is a framework for high-ratio 3D data compression using pre-trained models, achieving impressive compression ratios for meshes, point clouds, and radiance fields without requiring real datasets.", "motivation": "To leverage pre-trained 3D generative models for efficient 3D data compression, eliminating the need for object-specific training or real datasets.", "method": "Uses trainable mapping networks to bridge latent spaces of pre-trained encoders and generative models, transforming 3D data into compact latent codes for compression and decompression.", "result": "Achieves compression ratios up to 2187x for meshes, 55x for point clouds, and 619x for radiance fields with minimal latency and maintained visual quality.", "conclusion": "Squeeze3D offers a flexible, efficient solution for high-ratio 3D compression across multiple formats using synthetic data and pre-trained models."}}
{"id": "2506.07998", "pdf": "https://arxiv.org/pdf/2506.07998", "abs": "https://arxiv.org/abs/2506.07998", "authors": ["Boya Zeng", "Yida Yin", "Zhiqiu Xu", "Zhuang Liu"], "title": "Generative Modeling of Weights: Generalization or Memorization?", "categories": ["cs.LG", "cs.CV"], "comment": "Project page at https://boyazeng.github.io/weight_memorization", "summary": "Generative models, with their success in image and video generation, have\nrecently been explored for synthesizing effective neural network weights. These\napproaches take trained neural network checkpoints as training data, and aim to\ngenerate high-performing neural network weights during inference. In this work,\nwe examine four representative methods on their ability to generate novel model\nweights, i.e., weights that are different from the checkpoints seen during\ntraining. Surprisingly, we find that these methods synthesize weights largely\nby memorization: they produce either replicas, or at best simple\ninterpolations, of the training checkpoints. Current methods fail to outperform\nsimple baselines, such as adding noise to the weights or taking a simple weight\nensemble, in obtaining different and simultaneously high-performing models. We\nfurther show that this memorization cannot be effectively mitigated by\nmodifying modeling factors commonly associated with memorization in image\ndiffusion models, or applying data augmentations. Our findings provide a\nrealistic assessment of what types of data current generative models can model,\nand highlight the need for more careful evaluation of generative models in new\ndomains. Our code is available at\nhttps://github.com/boyazeng/weight_memorization.", "AI": {"tldr": "Current generative models for synthesizing neural network weights largely memorize training checkpoints, failing to create novel or superior weights compared to simple baselines.", "motivation": "To evaluate the ability of generative models to synthesize novel and high-performing neural network weights, beyond memorizing training checkpoints.", "method": "Examined four representative methods for weight generation, comparing their outputs to training checkpoints and simple baselines like noise addition or weight ensembles.", "result": "Methods produced replicas or interpolations of training data, failing to outperform baselines. Memorization persisted despite mitigation attempts.", "conclusion": "Generative models currently struggle with novel weight synthesis, emphasizing the need for careful evaluation in new domains."}}
{"id": "2506.08012", "pdf": "https://arxiv.org/pdf/2506.08012", "abs": "https://arxiv.org/abs/2506.08012", "authors": ["Penghao Wu", "Shengnan Ma", "Bo Wang", "Jiaheng Yu", "Lewei Lu", "Ziwei Liu"], "title": "GUI-Reflection: Empowering Multimodal GUI Models with Self-Reflection Behavior", "categories": ["cs.AI", "cs.CV"], "comment": "Project Page at https://penghao-wu.github.io/GUI_Reflection/", "summary": "Multimodal Large Language Models (MLLMs) have shown great potential in\nrevolutionizing Graphical User Interface (GUI) automation. However, existing\nGUI models mostly rely on learning from nearly error-free offline trajectories,\nthus lacking reflection and error recovery capabilities. To bridge this gap, we\npropose GUI-Reflection, a novel framework that explicitly integrates\nself-reflection and error correction capabilities into end-to-end multimodal\nGUI models throughout dedicated training stages: GUI-specific pre-training,\noffline supervised fine-tuning (SFT), and online reflection tuning.\nGUI-reflection enables self-reflection behavior emergence with fully automated\ndata generation and learning processes without requiring any human annotation.\nSpecifically, 1) we first propose scalable data pipelines to automatically\nconstruct reflection and error correction data from existing successful\ntrajectories. While existing GUI models mainly focus on grounding and UI\nunderstanding ability, we propose the GUI-Reflection Task Suite to learn and\nevaluate reflection-oriented abilities explicitly. 2) Furthermore, we built a\ndiverse and efficient environment for online training and data collection of\nGUI models on mobile devices. 3) We also present an iterative online reflection\ntuning algorithm leveraging the proposed environment, enabling the model to\ncontinuously enhance its reflection and error correction abilities. Our\nframework equips GUI agents with self-reflection and correction capabilities,\npaving the way for more robust, adaptable, and intelligent GUI automation, with\nall data, models, environments, and tools to be released publicly.", "AI": {"tldr": "GUI-Reflection is a framework enhancing MLLMs for GUI automation with self-reflection and error correction, using automated data generation and iterative training stages.", "motivation": "Existing GUI models lack reflection and error recovery, relying on error-free offline trajectories. GUI-Reflection aims to bridge this gap.", "method": "The framework includes GUI-specific pre-training, offline SFT, and online reflection tuning, with automated data pipelines and a task suite for reflection-oriented abilities.", "result": "GUI-Reflection enables self-reflection and error correction without human annotation, using scalable data pipelines and an iterative tuning algorithm.", "conclusion": "The framework advances GUI automation by making it more robust and adaptable, with plans to release all resources publicly."}}

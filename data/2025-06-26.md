<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 33]
- [cs.CV](#cs.CV) [Total: 48]
- [eess.IV](#eess.IV) [Total: 7]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.GR](#cs.GR) [Total: 3]
- [cs.AI](#cs.AI) [Total: 4]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.RO](#cs.RO) [Total: 4]
- [cs.LG](#cs.LG) [Total: 9]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [CycleDistill: Bootstrapping Machine Translation using LLMs with Cyclical Distillation](https://arxiv.org/abs/2506.19952)
*Deepon Halder,Thanmay Jayakumar,Raj Dabre*

Main category: cs.CL

TL;DR: CycleDistill is a bootstrapping method using LLMs and few-shot translation to create high-quality MT systems without parallel corpora, achieving significant improvements over baseline models.


<details>
  <summary>Details</summary>
Motivation: Parallel corpora are scarce for low-resource languages, limiting the performance of dedicated MT systems.

Method: CycleDistill iteratively generates synthetic parallel corpora from monolingual data via few-shot MT and fine-tunes the model with this data.

Result: Improves translation quality by 20-30 chrF points over baseline in the first iteration, with mild gains from using softmax activations.

Conclusion: CycleDistill effectively leverages LLMs and monolingual data to achieve high-quality MT without extensive parallel corpora.

Abstract: Large language models (LLMs), despite their ability to perform few-shot
machine translation (MT), often lag behind dedicated MT systems trained on
parallel corpora, which are crucial for high quality machine translation (MT).
However, parallel corpora are often scarce or non-existent for low-resource
languages. In this paper, we propose CycleDistill, a bootstrapping approach
leveraging LLMs and few-shot translation to obtain high-quality MT systems.
CycleDistill involves iteratively generating synthetic parallel corpora from
monolingual corpora via zero- or few-shot MT, which is then used to fine-tune
the model that was used for generating said data for MT. CycleDistill does not
need parallel corpora beyond 1 to 4 few-shot examples, and in our experiments
focusing on three Indian languages, by relying solely on monolingual corpora,
it can achieve high-quality machine translation, improving upon a few-shot
baseline model by over 20-30 chrF points on average in the first iteration. We
also study the effect of leveraging softmax activations during the distillation
process and observe mild improvements in translation quality.

</details>


### [2] [Inference Scaled GraphRAG: Improving Multi Hop Question Answering on Knowledge Graphs](https://arxiv.org/abs/2506.19967)
*Travis Thompson,Seung-Hwan Lim,Paul Liu,Ruoying He,Dongkuan Xu*

Main category: cs.CL

TL;DR: Inference-Scaled GraphRAG improves LLM reasoning on knowledge graphs by combining sequential and parallel scaling, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: LLMs underperform on knowledge-intensive tasks due to limited structured context access. RAG methods often fail to capture relational knowledge graph structures.

Method: Introduces Inference-Scaled GraphRAG, combining sequential scaling (deep chain-of-thought traversal) and parallel scaling (majority voting over sampled trajectories).

Result: Significant improvement in multi-hop question answering on GRBench, outperforming GraphRAG and prior baselines.

Conclusion: Inference-time scaling is a practical, architecture-agnostic solution for structured knowledge reasoning with LLMs.

Abstract: Large Language Models (LLMs) have achieved impressive capabilities in
language understanding and generation, yet they continue to underperform on
knowledge-intensive reasoning tasks due to limited access to structured context
and multi-hop information. Retrieval-Augmented Generation (RAG) partially
mitigates this by grounding generation in retrieved context, but conventional
RAG and GraphRAG methods often fail to capture relational structure across
nodes in knowledge graphs. We introduce Inference-Scaled GraphRAG, a novel
framework that enhances LLM-based graph reasoning by applying inference-time
compute scaling. Our method combines sequential scaling with deep
chain-of-thought graph traversal, and parallel scaling with majority voting
over sampled trajectories within an interleaved reasoning-execution loop.
Experiments on the GRBench benchmark demonstrate that our approach
significantly improves multi-hop question answering performance, achieving
substantial gains over both traditional GraphRAG and prior graph traversal
baselines. These findings suggest that inference-time scaling is a practical
and architecture-agnostic solution for structured knowledge reasoning with LLMs

</details>


### [3] [Doc2Agent: Scalable Generation of Tool-Using Agents from API Documentation](https://arxiv.org/abs/2506.19998)
*Xinyi Ni,Haonan Jian,Qiuyang Wang,Vedanshi Chetan Shah,Pengyu Hong*

Main category: cs.CL

TL;DR: Doc2Agent is a scalable pipeline for creating agents that generate and refine Python-based tools from API documentation, improving performance and reducing costs.


<details>
  <summary>Details</summary>
Motivation: Most API-based agents use curated toolsets, lacking adaptability to real-world API complexity. Building agents for arbitrary domains is challenging due to unstructured documentation and parameter inference.

Method: Doc2Agent generates executable tools from API docs and refines them iteratively using a code agent. It is evaluated on real-world, WebArena, and research APIs.

Result: Achieved a 55% performance improvement and 90% lower cost on WebArena benchmark. Demonstrated adaptability in glycomaterial science.

Conclusion: Doc2Agent provides a scalable, generalizable solution for building tool agents from unstructured API documentation.

Abstract: REST APIs play important roles in enriching the action space of web agents,
yet most API-based agents rely on curated and uniform toolsets that do not
reflect the complexity of real-world APIs. Building tool-using agents for
arbitrary domains remains a major challenge, as it requires reading
unstructured API documentation, testing APIs and inferring correct parameters.
We propose Doc2Agent, a scalable pipeline to build agents that can call
Python-based tools generated from API documentation. Doc2Agent generates
executable tools from API documentations and iteratively refines them using a
code agent. We evaluate our approach on real-world APIs, WebArena APIs, and
research APIs, producing validated tools. We achieved a 55\% relative
performance improvement with 90\% lower cost compared to direct API calling on
WebArena benchmark. A domain-specific agent built for glycomaterial science
further demonstrates the pipeline's adaptability to complex, knowledge-rich
tasks. Doc2Agent offers a generalizable solution for building tool agents from
unstructured API documentation at scale.

</details>


### [4] [A Modular Multitask Reasoning Framework Integrating Spatio-temporal Models and LLMs](https://arxiv.org/abs/2506.20073)
*Kethmi Hirushini Hettige,Jiahao Ji,Cheng Long,Shili Xiang,Gao Cong,Jingyuan Wang*

Main category: cs.CL

TL;DR: STReason integrates LLMs with spatio-temporal models for multi-task inference, outperforming baselines in complex reasoning tasks without task-specific finetuning.


<details>
  <summary>Details</summary>
Motivation: Existing spatio-temporal models lack multi-task inference and explanatory outputs, limiting real-world applicability.

Method: STReason uses in-context learning to decompose queries into modular programs, generating solutions and rationales.

Result: STReason outperforms LLM baselines, excels in complex scenarios, and reduces expert workload.

Conclusion: STReason advances spatio-temporal reasoning, offering generalizability and practical utility.

Abstract: Spatio-temporal data mining plays a pivotal role in informed decision making
across diverse domains. However, existing models are often restricted to narrow
tasks, lacking the capacity for multi-task inference and complex long-form
reasoning that require generation of in-depth, explanatory outputs. These
limitations restrict their applicability to real-world, multi-faceted decision
scenarios. In this work, we introduce STReason, a novel framework that
integrates the reasoning strengths of large language models (LLMs) with the
analytical capabilities of spatio-temporal models for multi-task inference and
execution. Without requiring task-specific finetuning, STReason leverages
in-context learning to decompose complex natural language queries into modular,
interpretable programs, which are then systematically executed to generate both
solutions and detailed rationales. To facilitate rigorous evaluation, we
construct a new benchmark dataset and propose a unified evaluation framework
with metrics specifically designed for long-form spatio-temporal reasoning.
Experimental results show that STReason significantly outperforms advanced LLM
baselines across all metrics, particularly excelling in complex,
reasoning-intensive spatio-temporal scenarios. Human evaluations further
validate STReason's credibility and practical utility, demonstrating its
potential to reduce expert workload and broaden the applicability to real-world
spatio-temporal tasks. We believe STReason provides a promising direction for
developing more capable and generalizable spatio-temporal reasoning systems.

</details>


### [5] [SACL: Understanding and Combating Textual Bias in Code Retrieval with Semantic-Augmented Reranking and Localization](https://arxiv.org/abs/2506.20081)
*Dhruv Gupta,Gayathri Ganesh Lakshmy,Yiqing Xie*

Main category: cs.CL

TL;DR: SACL improves code retrieval and generation by reducing bias and enriching semantic information, achieving significant performance gains.


<details>
  <summary>Details</summary>
Motivation: Current code retrievers rely too much on surface-level textual features and exhibit bias towards well-documented code, even if irrelevant.

Method: Proposed SACL framework augments code or structural knowledge with semantic information to reduce bias and improve retrieval.

Result: SACL improves code retrieval (e.g., 12.8% Recall@1 on HumanEval) and code generation (e.g., 4.88% Pass@1 on HumanEval).

Conclusion: SACL effectively addresses biases in code retrieval and enhances performance in both retrieval and generation tasks.

Abstract: Retrieval-Augmented Code Generation (RACG) is a critical technique for
enhancing code generation by retrieving relevant information. In this work, we
conduct an in-depth analysis of code retrieval by systematically masking
specific features while preserving code functionality. Our discoveries include:
(1) although trained on code, current retrievers heavily rely on surface-level
textual features (e.g., docstrings, identifier names), and (2) they exhibit a
strong bias towards well-documented code, even if the documentation is
irrelevant.Based on our discoveries, we propose SACL, a framework that enriches
textual information and reduces bias by augmenting code or structural knowledge
with semantic information. Extensive experiments show that SACL substantially
improves code retrieval (e.g., by 12.8% / 9.4% / 7.0% Recall@1 on HumanEval /
MBPP / SWE-Bench-Lite), which also leads to better code generation performance
(e.g., by 4.88% Pass@1 on HumanEval).

</details>


### [6] [Bridging Compositional and Distributional Semantics: A Survey on Latent Semantic Geometry via AutoEncoder](https://arxiv.org/abs/2506.20083)
*Yingji Zhang,Danilo S. Carvalho,André Freitas*

Main category: cs.CL

TL;DR: The paper explores integrating compositional and symbolic properties into distributional semantic spaces to improve Transformer-based LMs, focusing on semantic representation learning and comparing autoencoder architectures.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between symbolic and distributional semantics by enhancing interpretability, controllability, and generalisation in LMs.

Method: Reviews and compares three autoencoder architectures (VAE, VQVAE, SAE) and their latent space geometries in relation to semantic structure.

Result: Highlights how different architectures induce distinct latent geometries, impacting semantic interpretability.

Conclusion: Semantic representation learning offers a promising direction to unify symbolic and distributional semantics, improving LM capabilities.

Abstract: Integrating compositional and symbolic properties into current distributional
semantic spaces can enhance the interpretability, controllability,
compositionality, and generalisation capabilities of Transformer-based
auto-regressive language models (LMs). In this survey, we offer a novel
perspective on latent space geometry through the lens of compositional
semantics, a direction we refer to as \textit{semantic representation
learning}. This direction enables a bridge between symbolic and distributional
semantics, helping to mitigate the gap between them. We review and compare
three mainstream autoencoder architectures-Variational AutoEncoder (VAE),
Vector Quantised VAE (VQVAE), and Sparse AutoEncoder (SAE)-and examine the
distinctive latent geometries they induce in relation to semantic structure and
interpretability.

</details>


### [7] [ITFormer: Bridging Time Series and Natural Language for Multi-Modal QA with Large-Scale Multitask Dataset](https://arxiv.org/abs/2506.20093)
*Yilin Wang,Peixuan Lei,Jie Song,Yuzhe Hao,Tao Chen,Yuxuan Zhang,Lei Jia,Yuanxiang Li,Zhongyu Wei*

Main category: cs.CL

TL;DR: The paper introduces Time-Series QA and EngineMT-QA dataset, proposing ITFormer to integrate time-series and natural language for improved QA accuracy efficiently.


<details>
  <summary>Details</summary>
Motivation: The challenge of integrating high-dimensional time-series data with natural language for dynamic tasks motivates the creation of Time-Series QA and ITFormer.

Method: Proposes ITFormer, a framework combining time-series encoders with frozen LLMs to align and fuse temporal-textual features.

Result: ITFormer achieves significant QA accuracy improvement with minimal additional trainable parameters (under 1%).

Conclusion: The work establishes a scalable paradigm for temporal-textual integration, enabling new research in multi-modal AI.

Abstract: Time-series data are critical in diverse applications, such as industrial
monitoring, medical diagnostics, and climate research. However, effectively
integrating these high-dimensional temporal signals with natural language for
dynamic, interactive tasks remains a significant challenge. To address this, we
introduce the Time-Series Question Answering (Time-Series QA) task and release
EngineMT-QA, the first large-scale, multi-task, temporal-textual QA dataset
designed to capture complex interactions between time-series signals and
natural language. Building on this resource, we propose the Instruct Time
Transformer (ITFormer), a novel framework that bridges time-series encoders
with frozen large language models (LLMs). ITFormer effectively extracts,
aligns, and fuses temporal and textual features, achieving a strong improvement
in QA accuracy over strong baselines with fewer than 1\% additional trainable
parameters. By combining computational efficiency with robust cross-modal
modeling, our work establishes a adaptable paradigm for integrating temporal
data with natural language, paving the way for new research and applications in
multi-modal AI. More details about the project, including datasets and code,
are available at: https://pandalin98.github.io/itformer_site/

</details>


### [8] [A Multi-Pass Large Language Model Framework for Precise and Efficient Radiology Report Error Detection](https://arxiv.org/abs/2506.20112)
*Songsoo Kim,Seungtae Lee,See Young Lee,Joonho Kim,Keechan Kan,Dukyong Yoon*

Main category: cs.CL

TL;DR: A three-pass LLM framework improves PPV and reduces costs for radiology report proofreading, outperforming simpler approaches.


<details>
  <summary>Details</summary>
Motivation: To enhance the positive predictive value (PPV) of LLM-based proofreading for radiology reports and reduce operational costs, given the low error prevalence in such reports.

Method: Retrospective analysis of 1,000 radiology reports using three LLM frameworks: single-prompt detector, extractor plus detector, and extractor, detector, and false-positive verifier. Precision and efficiency were measured.

Result: Framework 3 significantly improved PPV (0.159 vs. 0.063 baseline) and reduced costs by 42.6%, while maintaining detection performance (aTPR stable).

Conclusion: The three-pass LLM framework is effective for AI-assisted radiology report quality assurance, balancing PPV improvement and cost reduction.

Abstract: Background: The positive predictive value (PPV) of large language model
(LLM)-based proofreading for radiology reports is limited due to the low error
prevalence. Purpose: To assess whether a three-pass LLM framework enhances PPV
and reduces operational costs compared with baseline approaches. Materials and
Methods: A retrospective analysis was performed on 1,000 consecutive radiology
reports (250 each: radiography, ultrasonography, CT, MRI) from the MIMIC-III
database. Two external datasets (CheXpert and Open-i) were validation sets.
Three LLM frameworks were tested: (1) single-prompt detector; (2) extractor
plus detector; and (3) extractor, detector, and false-positive verifier.
Precision was measured by PPV and absolute true positive rate (aTPR).
Efficiency was calculated from model inference charges and reviewer
remuneration. Statistical significance was tested using cluster bootstrap,
exact McNemar tests, and Holm-Bonferroni correction. Results: Framework PPV
increased from 0.063 (95% CI, 0.036-0.101, Framework 1) to 0.079 (0.049-0.118,
Framework 2), and significantly to 0.159 (0.090-0.252, Framework 3; P<.001 vs.
baselines). aTPR remained stable (0.012-0.014; P>=.84). Operational costs per
1,000 reports dropped to USD 5.58 (Framework 3) from USD 9.72 (Framework 1) and
USD 6.85 (Framework 2), reflecting reductions of 42.6% and 18.5%, respectively.
Human-reviewed reports decreased from 192 to 88. External validation supported
Framework 3's superior PPV (CheXpert 0.133, Open-i 0.105) and stable aTPR
(0.007). Conclusion: A three-pass LLM framework significantly enhanced PPV and
reduced operational costs, maintaining detection performance, providing an
effective strategy for AI-assisted radiology report quality assurance.

</details>


### [9] [Leveraging AI Graders for Missing Score Imputation to Achieve Accurate Ability Estimation in Constructed-Response Tests](https://arxiv.org/abs/2506.20119)
*Masaki Uto,Yuma Ito*

Main category: cs.CL

TL;DR: Proposes a method using automated scoring to improve IRT-based ability estimation by imputing missing scores, reducing manual grading.


<details>
  <summary>Details</summary>
Motivation: Addresses the labor-intensive and costly nature of manual grading in constructed-response tests while improving accuracy in ability estimation.

Method: Leverages automated scoring technologies to impute missing scores for more accurate IRT-based ability estimation.

Result: Achieves high accuracy in ability estimation and significantly reduces manual grading workload.

Conclusion: The method effectively balances accuracy and efficiency in assessing higher-order abilities.

Abstract: Evaluating the abilities of learners is a fundamental objective in the field
of education. In particular, there is an increasing need to assess higher-order
abilities such as expressive skills and logical thinking. Constructed-response
tests such as short-answer and essay-based questions have become widely used as
a method to meet this demand. Although these tests are effective, they require
substantial manual grading, making them both labor-intensive and costly. Item
response theory (IRT) provides a promising solution by enabling the estimation
of ability from incomplete score data, where human raters grade only a subset
of answers provided by learners across multiple test items. However, the
accuracy of ability estimation declines as the proportion of missing scores
increases. Although data augmentation techniques for imputing missing scores
have been explored in order to address this limitation, they often struggle
with inaccuracy for sparse or heterogeneous data. To overcome these challenges,
this study proposes a novel method for imputing missing scores by leveraging
automated scoring technologies for accurate IRT-based ability estimation. The
proposed method achieves high accuracy in ability estimation while markedly
reducing manual grading workload.

</details>


### [10] [CCRS: A Zero-Shot LLM-as-a-Judge Framework for Comprehensive RAG Evaluation](https://arxiv.org/abs/2506.20128)
*Aashiq Muhamed*

Main category: cs.CL

TL;DR: CCRS is a novel suite of five metrics using a pretrained LLM for zero-shot evaluation of RAG systems, outperforming existing methods in efficiency and discriminative power.


<details>
  <summary>Details</summary>
Motivation: Existing RAG evaluation methods are either simplistic (lexical overlap) or inefficient (multi-stage pipelines), failing to capture nuanced quality aspects like coherence, relevance, and correctness.

Method: Proposes CCRS, a suite of five metrics (CC, QR, ID, AC, IR) leveraging a single pretrained LLM for end-to-end evaluation. Tested on BioASQ dataset with six RAG configurations.

Result: CCRS effectively discriminates system performances (e.g., Mistral-7B outperforms Llama variants) and matches/surpasses RAGChecker in discriminative power while being more efficient.

Conclusion: CCRS offers a practical, comprehensive, and efficient framework for evaluating and improving RAG systems.

Abstract: RAG systems enhance LLMs by incorporating external knowledge, which is
crucial for domains that demand factual accuracy and up-to-date information.
However, evaluating the multifaceted quality of RAG outputs, spanning aspects
such as contextual coherence, query relevance, factual correctness, and
informational completeness, poses significant challenges. Existing evaluation
methods often rely on simple lexical overlap metrics, which are inadequate for
capturing these nuances, or involve complex multi-stage pipelines with
intermediate steps like claim extraction or require finetuning specialized
judge models, hindering practical efficiency. To address these limitations, we
propose CCRS (Contextual Coherence and Relevance Score), a novel suite of five
metrics that utilizes a single, powerful, pretrained LLM as a zero-shot,
end-to-end judge. CCRS evaluates: Contextual Coherence (CC), Question Relevance
(QR), Information Density (ID), Answer Correctness (AC), and Information Recall
(IR). We apply CCRS to evaluate six diverse RAG system configurations on the
challenging BioASQ dataset. Our analysis demonstrates that CCRS effectively
discriminates between system performances, confirming, for instance, that the
Mistral-7B reader outperforms Llama variants. We provide a detailed analysis of
CCRS metric properties, including score distributions, convergent/discriminant
validity, tie rates, population statistics, and discriminative power. Compared
to the complex RAGChecker framework, CCRS offers comparable or superior
discriminative power for key aspects like recall and faithfulness, while being
significantly more computationally efficient. CCRS thus provides a practical,
comprehensive, and efficient framework for evaluating and iteratively improving
RAG systems.

</details>


### [11] [AALC: Large Language Model Efficient Reasoning via Adaptive Accuracy-Length Control](https://arxiv.org/abs/2506.20160)
*Ruosen Li,Ziming Luo,Quan Zhang,Ruochen Li,Ben Zhou,Ali Payani,Xinya Du*

Main category: cs.CL

TL;DR: AALC is a lightweight, accuracy-aware length reward method that reduces response length by over 50% while maintaining or improving accuracy in large reasoning models.


<details>
  <summary>Details</summary>
Motivation: Large reasoning models (LRMs) generate lengthy chain-of-thoughts, causing high latency and cost without proportional accuracy gains.

Method: AALC integrates a dynamically balanced reward into reinforcement learning, delaying length penalty until target performance is met.

Result: The method reduces response length by over 50% while maintaining or improving accuracy, curbing redundant reasoning patterns.

Conclusion: AALC demonstrates the potential of reward-based strategies to enhance efficiency and generalizability in LRMs, though it may reduce interpretability.

Abstract: Large reasoning models (LRMs) achieve impressive reasoning capabilities by
generating lengthy chain-of-thoughts, but this "overthinking" incurs high
latency and cost without commensurate accuracy gains. In this work, we
introduce AALC, a lightweight, accuracy-aware length reward integrated into
reinforcement learning that dynamically balances correctness and brevity during
training. By incorporating validation accuracy into the reward and employing a
smooth, dynamically scheduled length penalty, AALC delays length penalty until
target performance is met. Through extensive experiments across standard and
out-of-distribution math benchmarks, we show that our approach reduces response
length by over 50% while maintaining or even improving the original accuracy.
Furthermore, qualitative analysis reveals that our method curbs redundant
reasoning patterns such as excessive subgoal setting and verification, leading
to structurally refined outputs rather than naive truncation. We also identify
that efficiency gains are accompanied by reduced interpretability: models
trained with AALC omit some narrative framing and explanatory context. These
findings highlight the potential of reward-based strategies to guide LRMs
toward more efficient, generalizable reasoning paths.

</details>


### [12] [SEED: A Structural Encoder for Embedding-Driven Decoding in Time Series Prediction with LLMs](https://arxiv.org/abs/2506.20167)
*Fengze Li,Yue Wang,Yangle Liu,Ming Huang,Dou Hong,Jieming Ma*

Main category: cs.CL

TL;DR: SEED integrates structural encoding with LLMs for time series forecasting, bridging the gap between numerical patterns and semantic reasoning.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of structural encoders (lack of semantic reasoning) and LLMs (incompatibility with raw time series) in multivariate forecasting.

Method: SEED uses a token-aware encoder, projection module, semantic reprogramming, and frozen LLM for prediction.

Result: Consistent improvements over baselines, addressing the structural-semantic gap.

Conclusion: SEED effectively unifies structural and semantic modeling for transferable forecasting.

Abstract: Multivariate time series forecasting requires models to simultaneously
capture variable-wise structural dependencies and generalize across diverse
tasks. While structural encoders are effective in modeling feature
interactions, they lack the capacity to support semantic-level reasoning or
task adaptation. Conversely, large language models (LLMs) possess strong
generalization capabilities but remain incompatible with raw time series
inputs. This gap limits the development of unified, transferable prediction
systems. Therefore, we introduce SEED, a structural encoder for
embedding-driven decoding, which integrates four stages: a token-aware encoder
for patch extraction, a projection module that aligns patches with language
model embeddings, a semantic reprogramming mechanism that maps patches to
task-aware prototypes, and a frozen language model for prediction. This modular
architecture decouples representation learning from inference, enabling
efficient alignment between numerical patterns and semantic reasoning.
Empirical results demonstrate that the proposed method achieves consistent
improvements over strong baselines, and comparative studies on various datasets
confirm SEED's role in addressing the structural-semantic modeling gap.

</details>


### [13] [COIN: Uncertainty-Guarding Selective Question Answering for Foundation Models with Provable Risk Guarantees](https://arxiv.org/abs/2506.20178)
*Zhiyuan Wang,Jinhao Duan,Qingni Wang,Xiaofeng Zhu,Tianlong Chen,Xiaoshuang Shi,Kaidi Xu*

Main category: cs.CL

TL;DR: COIN is a framework for uncertainty quantification in foundation models, ensuring controlled false discovery rates (FDR) and higher sample retention by calibrating thresholds using empirical error rates and confidence intervals.


<details>
  <summary>Details</summary>
Motivation: Heuristic UQ methods lack formal guarantees for metrics like FDR, and existing conformal prediction frameworks often include incorrect candidates, limiting practical utility.

Method: COIN calibrates statistically valid thresholds using empirical error rates and confidence intervals (e.g., Clopper-Pearson) to filter answers under FDR constraints.

Result: COIN demonstrates robust FDR control, strong test-time power in retaining admissible answers, and efficiency with limited calibration data across text generation tasks.

Conclusion: COIN is adaptable and extensible, with potential for improved performance through alternative upper bound constructions and UQ strategies.

Abstract: Uncertainty quantification (UQ) for foundation models is essential to
identify and mitigate potential hallucinations in automatically generated text.
However, heuristic UQ approaches lack formal guarantees for key metrics such as
the false discovery rate (FDR) in selective prediction. Previous work adopts
the split conformal prediction (SCP) framework to ensure desired coverage of
admissible answers by constructing prediction sets, but these sets often
contain incorrect candidates, limiting their practical utility. To address
this, we propose COIN, an uncertainty-guarding selection framework that
calibrates statistically valid thresholds to filter a single generated answer
per question under user-specified FDR constraints. COIN estimates the empirical
error rate on a calibration set and applies confidence interval methods such as
Clopper-Pearson to establish a high-probability upper bound on the true error
rate (i.e., FDR). This enables the selection of the largest uncertainty
threshold that ensures FDR control on test data while significantly increasing
sample retention. We demonstrate COIN's robustness in risk control, strong
test-time power in retaining admissible answers, and predictive efficiency
under limited calibration data across both general and multimodal text
generation tasks. Furthermore, we show that employing alternative upper bound
constructions and UQ strategies can further boost COIN's power performance,
which underscores its extensibility and adaptability to diverse application
scenarios.

</details>


### [14] [How to Retrieve Examples in In-context Learning to Improve Conversational Emotion Recognition using Large Language Models?](https://arxiv.org/abs/2506.20199)
*Mengqi Wang,Tiantian Feng,Shrikanth Narayanan*

Main category: cs.CL

TL;DR: The paper explores improving conversational emotion recognition (CER) in LLMs by enhancing in-context learning (ICL) with high-quality example retrieval. Augmented retrieval outperforms other methods.


<details>
  <summary>Details</summary>
Motivation: Creating high-accuracy LLM applications for subjective tasks like emotion recognition is challenging. The study aims to improve CER by refining ICL example retrieval.

Method: Proposes strategies like random and augmented example retrieval, analyzing conversational context's impact. Experiments on IEMOCAP, MELD, and EmoryNLP datasets.

Result: Augmented example retrieval consistently outperforms other methods, emphasizing coherent targeted examples and paraphrasing.

Conclusion: Augmented retrieval is key for enhancing CER in LLMs, demonstrating the value of high-quality examples in ICL.

Abstract: Large language models (LLMs) have enabled a wide variety of real-world
applications in various domains. However, creating a high-performing
application with high accuracy remains challenging, particularly for subjective
tasks like emotion recognition. Inspired by the SLT 2024 GenSER Challenge, this
study investigates approaches to improving conversational emotion recognition
(CER) by LLMs. Specifically, we explore how to retrieve high-quality examples
in in-context learning (ICL) to enhance CER. We propose various strategies
based on random and augmented example retrieval and also analyze the impact of
conversational context on CER accuracy. Experiments were conducted on the three
datasets including IEMOCAP, MELD and EmoryNLP. The results show that augmented
example retrieval consistently outperforms other techniques under investigation
across all datasets, highlighting the importance of retrieving coherent
targeted examples and enhancing them through paraphrasing.

</details>


### [15] [Intrinsic vs. Extrinsic Evaluation of Czech Sentence Embeddings: Semantic Relevance Doesn't Help with MT Evaluation](https://arxiv.org/abs/2506.20203)
*Petra Barančíková,Ondřej Bojar*

Main category: cs.CL

TL;DR: The paper compares Czech-specific and multilingual sentence embedding models using intrinsic and extrinsic evaluations, revealing a disconnect between semantic similarity performance and downstream task results.


<details>
  <summary>Details</summary>
Motivation: To assess how well sentence embeddings capture linguistic phenomena and perform in practical tasks like machine translation evaluation.

Method: Intrinsic evaluation uses Costra and STS benchmarks; extrinsic evaluation involves fine-tuning models for translation tasks using COMET-based metrics.

Result: Models excelling in semantic similarity tests don't always perform well in translation tasks, while seemingly over-smoothed models can achieve strong results after fine-tuning.

Conclusion: The findings highlight the complexity of linking semantic properties to downstream performance, calling for more research on operationalizable semantics and better task datasets.

Abstract: In this paper, we compare Czech-specific and multilingual sentence embedding
models through intrinsic and extrinsic evaluation paradigms. For intrinsic
evaluation, we employ Costra, a complex sentence transformation dataset, and
several Semantic Textual Similarity (STS) benchmarks to assess the ability of
the embeddings to capture linguistic phenomena such as semantic similarity,
temporal aspects, and stylistic variations. In the extrinsic evaluation, we
fine-tune each embedding model using COMET-based metrics for machine
translation evaluation.
  Our experiments reveal an interesting disconnect: models that excel in
intrinsic semantic similarity tests do not consistently yield superior
performance on downstream translation evaluation tasks. Conversely, models with
seemingly over-smoothed embedding spaces can, through fine-tuning, achieve
excellent results. These findings highlight the complex relationship between
semantic property probes and downstream task, emphasizing the need for more
research into 'operationalizable semantics' in sentence embeddings, or more
in-depth downstream tasks datasets (here translation evaluation)

</details>


### [16] [Perspectives in Play: A Multi-Perspective Approach for More Inclusive NLP Systems](https://arxiv.org/abs/2506.20209)
*Benedetta Muscato,Lucia Passaro,Gizem Gezici,Fosca Giannotti*

Main category: cs.CL

TL;DR: The paper proposes a multi-perspective approach using soft labels in NLP to capture human disagreements, outperforming traditional methods in subjective tasks like hate speech and stance detection.


<details>
  <summary>Details</summary>
Motivation: Traditional NLP methods aggregate annotators' viewpoints into a single ground truth, often underrepresenting minority perspectives. This study aims to address this by valuing diverse opinions.

Method: The study introduces a multi-perspective approach with soft labels, tested on subjective tasks (hate speech, irony, abusive language, stance detection) and evaluated using JSD and F1 scores.

Result: The approach better approximates human label distributions (lower JSD) and achieves higher F1 scores, though it shows lower confidence in highly subjective tasks like irony and stance detection.

Conclusion: The multi-perspective approach is more inclusive and effective for subjective NLP tasks, with XAI providing insights into model uncertainty.

Abstract: In the realm of Natural Language Processing (NLP), common approaches for
handling human disagreement consist of aggregating annotators' viewpoints to
establish a single ground truth. However, prior studies show that disregarding
individual opinions can lead can lead to the side effect of underrepresenting
minority perspectives, especially in subjective tasks, where annotators may
systematically disagree because of their preferences. Recognizing that labels
reflect the diverse backgrounds, life experiences, and values of individuals,
this study proposes a new multi-perspective approach using soft labels to
encourage the development of the next generation of perspective aware models,
more inclusive and pluralistic. We conduct an extensive analysis across diverse
subjective text classification tasks, including hate speech, irony, abusive
language, and stance detection, to highlight the importance of capturing human
disagreements, often overlooked by traditional aggregation methods. Results
show that the multi-perspective approach not only better approximates human
label distributions, as measured by Jensen-Shannon Divergence (JSD), but also
achieves superior classification performance (higher F1 scores), outperforming
traditional approaches. However, our approach exhibits lower confidence in
tasks like irony and stance detection, likely due to the inherent subjectivity
present in the texts. Lastly, leveraging Explainable AI (XAI), we explore model
uncertainty and uncover meaningful insights into model predictions.

</details>


### [17] [Enhancing Large Language Models through Structured Reasoning](https://arxiv.org/abs/2506.20241)
*Yubo Dong,Hehe Fan*

Main category: cs.CL

TL;DR: The paper introduces a structured reasoning approach to enhance LLMs, addressing their limitations in complex reasoning tasks by converting unstructured data into structured formats and using novel algorithms for optimization.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with complex reasoning due to reliance on implicit statistical relationships, lacking structured knowledge representation. The goal is to improve reasoning capabilities.

Method: Unstructured data is converted into structured formats with annotated reasoning steps. The model is trained using Supervised Fine-Tuning (SFT) and enhanced with Group Relative Policy Optimization (GRPO), incorporating MAX-Flow and LCS algorithms.

Result: Fine-tuning the DeepSeek-R1-Distill-Qwen-1.5B model showed concise reasoning, robust performance, and improved compatibility with optimization techniques.

Conclusion: Structured reasoning integration effectively enhances LLMs' reasoning capabilities and computational efficiency.

Abstract: Recent Large Language Models (LLMs) have significantly advanced natural
language processing and automated decision-making. However, these models still
encounter difficulties when performing complex reasoning tasks involving
logical deduction and systematic planning, primarily due to their reliance on
implicit statistical relationships without structured knowledge
representation.Inspired by cognitive science and neurosymbolic AI, we introduce
a novel approach to enhance LLMs through explicit structured reasoning. First,
we convert unstructured data into structured formats by explicitly annotating
reasoning steps. We then employ this structured dataset to train LLMs through
Supervised Fine-Tuning (SFT). Additionally, we enhance the structured reasoning
capabilities of LLMs using Group Relative Policy Optimization (GRPO),
incorporating two innovative algorithms--MAX-Flow and Longest Common
Subsequence (LCS)--which notably improve reasoning effectiveness and reduce
computational complexity. Experimental results from fine-tuning a
DeepSeek-R1-Distill-Qwen-1.5B model demonstrate concise reasoning, robust
performance across various scenarios, and improved compatibility with
optimization techniques, validating the efficacy of structured reasoning
integration in LLMs.

</details>


### [18] [CBF-AFA: Chunk-Based Multi-SSL Fusion for Automatic Fluency Assessment](https://arxiv.org/abs/2506.20243)
*Papa Séga Wade,Mihai Andries,Ioannis Kanellos,Thierry Moudenc*

Main category: cs.CL

TL;DR: A chunk-based approach using self-supervised learning (SSL) models and a CNN-BiLSTM framework improves fluency assessment in non-native speech, outperforming baselines on two datasets.


<details>
  <summary>Details</summary>
Motivation: Automatic fluency assessment (AFA) struggles with capturing speech rhythm, pauses, and disfluencies in non-native speakers, necessitating a more robust method.

Method: The approach segments speech into breath-group chunks using Silero-VAD, fuses SSL embeddings (Wav2Vec2, HuBERT, WavLM) with a weighted mechanism, and uses a CNN-BiLSTM for local and long-term dependency analysis.

Result: The method improves F1-score by 2.8 and Pearson correlation by 6.2 points on Speechocean762, and 4.2 F1-score and 4.0 Pearson points on Avalinguo, surpassing baselines.

Conclusion: Chunk-based multi-SSL fusion is effective for fluency evaluation, but future work should address generalization to dialects with irregular prosody.

Abstract: Automatic fluency assessment (AFA) remains challenging, particularly in
capturing speech rhythm, pauses, and disfluencies in non-native speakers. We
introduce a chunk-based approach integrating self-supervised learning (SSL)
models (Wav2Vec2, HuBERT, and WavLM) selected for their complementary strengths
in phonetic, prosodic, and noisy speech modeling, with a hierarchical
CNN-BiLSTM framework. Speech is segmented into breath-group chunks using Silero
voice activity detection (Silero-VAD), enabling fine-grained temporal analysis
while mitigating over-segmentation artifacts. SSL embeddings are fused via a
learnable weighted mechanism, balancing acoustic and linguistic features, and
enriched with chunk-level fluency markers (e.g., speech rate, pause durations,
n-gram repetitions). The CNN-BiLSTM captures local and long-term dependencies
across chunks. Evaluated on Avalinguo and Speechocean762, our approach improves
F1-score by 2.8 and Pearson correlation by 6.2 points over single SSL baselines
on Speechocean762, with gains of 4.2 F1-score and 4.0 Pearson points on
Avalinguo, surpassing Pyannote.audio-based segmentation baselines. These
findings highlight chunk-based multi-SSL fusion for robust fluency evaluation,
though future work should explore generalization to dialects with irregular
prosody.

</details>


### [19] [Narrative Shift Detection: A Hybrid Approach of Dynamic Topic Models and Large Language Models](https://arxiv.org/abs/2506.20269)
*Kai-Robin Lange,Tobias Schmidt,Matthias Reccius,Henrik Müller,Michael Roos,Carsten Jentsch*

Main category: cs.CL

TL;DR: The paper proposes combining Large Language Models (LLMs) and topic models to dynamically track narrative shifts over time, addressing scalability and cost issues of LLMs alone. It tests the method on Wall Street Journal articles (2009-2023), finding LLMs effective for detecting shifts but less so for distinguishing between content and narrative changes.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of extracting and tracking narrative developments over time using LLMs, which are costly and computationally intensive.

Method: Combines LLMs' language understanding with topic models' scalability. Uses topic modeling and change point detection to identify shifts, filters representative documents, and applies LLMs to interpret changes.

Result: LLMs efficiently detect narrative shifts but struggle to differentiate between content and narrative changes.

Conclusion: The hybrid approach is effective for tracking narrative shifts but needs improvement in distinguishing content from narrative changes.

Abstract: With rapidly evolving media narratives, it has become increasingly critical
to not just extract narratives from a given corpus but rather investigate, how
they develop over time. While popular narrative extraction methods such as
Large Language Models do well in capturing typical narrative elements or even
the complex structure of a narrative, applying them to an entire corpus comes
with obstacles, such as a high financial or computational cost. We propose a
combination of the language understanding capabilities of Large Language Models
with the large scale applicability of topic models to dynamically model
narrative shifts across time using the Narrative Policy Framework. We apply a
topic model and a corresponding change point detection method to find changes
that concern a specific topic of interest. Using this model, we filter our
corpus for documents that are particularly representative of that change and
feed them into a Large Language Model that interprets the change that happened
in an automated fashion and distinguishes between content and narrative shifts.
We employ our pipeline on a corpus of The Wall Street Journal news paper
articles from 2009 to 2023. Our findings indicate that a Large Language Model
can efficiently extract a narrative shift if one exists at a given point in
time, but does not perform as well when having to decide whether a shift in
content or a narrative shift took place.

</details>


### [20] [Biomed-Enriched: A Biomedical Dataset Enriched with LLMs for Pretraining and Extracting Rare and Hidden Content](https://arxiv.org/abs/2506.20331)
*Rian Touchent,Nathan Godey,Eric de la Clergerie*

Main category: cs.CL

TL;DR: Biomed-Enriched is a biomedical dataset from PubMed, annotated for type, domain, and educational quality, enabling refined subsets for NLP tasks. It improves model performance in biomedical tasks.


<details>
  <summary>Details</summary>
Motivation: Clinical text is hard to access due to privacy; this dataset offers an open alternative for biomedical NLP.

Method: Two-stage annotation: large model labels 400K paragraphs, fine-tunes a small model to propagate labels across PMC-OA.

Result: Created subsets (e.g., 2M clinical cases) improve model performance (e.g., +5% on MMLU ProfMed).

Conclusion: The dataset supports efficient biomedical pretraining, with potential for targeted improvements in NLP tasks.

Abstract: We introduce Biomed-Enriched, a biomedical text dataset constructed from
PubMed via a two-stage annotation process. In the first stage, a large language
model annotates 400K paragraphs from PubMed scientific articles, assigning
scores for their type (review, study, clinical case, other), domain (clinical,
biomedical, other), and educational quality. The educational quality score
(rated 1 to 5) estimates how useful a paragraph is for college-level learning.
These annotations are then used to fine-tune a small language model, which
propagates the labels across the full PMC-OA corpus. The resulting metadata
allows us to extract refined subsets, including 2M clinical case paragraphs
with over 450K high-quality ones from articles with commercial-use licenses,
and to construct several variants via quality filtering and domain upsampling.
Clinical text is typically difficult to access due to privacy constraints, as
hospital records cannot be publicly shared. Hence, our dataset provides an
alternative large-scale, openly available collection of clinical cases from
PubMed, making it a valuable resource for biomedical and clinical NLP.
Preliminary continual-pretraining experiments with OLMo2 suggest these curated
subsets enable targeted improvements, with clinical upsampling boosting
performance by ~5% on MMLU ProfMed and educational quality filtering improving
MedQA and MedMCQA by ~1%. Combinations of these techniques led to faster
convergence, reaching same performance with a third of training tokens,
indicating potential for more efficient and effective biomedical pretraining
strategies.

</details>


### [21] [TAPS: Tool-Augmented Personalisation via Structured Tagging](https://arxiv.org/abs/2506.20409)
*Ekaterina Taktasheva,Jeff Dalton*

Main category: cs.CL

TL;DR: The paper introduces TAPS, a solution to enhance personalized tool use in LLMs by integrating user preferences, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing approaches neglect personalization in tool-augmented LLMs, limiting their effectiveness in user tasks.

Method: TAPS uses a structured tagging tool and uncertainty-based tool detector to improve personalized tool use.

Result: TAPS significantly improves LLMs' ability to incorporate user preferences, setting a new benchmark on the NLSI task.

Conclusion: Personalization is crucial for tool-augmented LLMs, and TAPS effectively addresses this gap.

Abstract: Recent advancements in tool-augmented large language models have enabled them
to interact with external tools, enhancing their ability to perform complex
user tasks. However, existing approaches overlook the role of personalisation
in guiding tool use. This work investigates how user preferences can be
effectively integrated into goal-oriented dialogue agents. Through extensive
analysis, we identify key weaknesses in the ability of LLMs to personalise tool
use. To this end, we introduce \name, a novel solution that enhances
personalised tool use by leveraging a structured tagging tool and an
uncertainty-based tool detector. TAPS significantly improves the ability of
LLMs to incorporate user preferences, achieving the new state-of-the-art for
open source models on the NLSI task.

</details>


### [22] [An Agentic System for Rare Disease Diagnosis with Traceable Reasoning](https://arxiv.org/abs/2506.20430)
*Weike Zhao,Chaoyi Wu,Yanjie Fan,Xiaoman Zhang,Pengcheng Qiu,Yuze Sun,Xiao Zhou,Yanfeng Wang,Ya Zhang,Yongguo Yu,Kun Sun,Weidi Xie*

Main category: cs.CL

TL;DR: DeepRare is an LLM-powered rare disease diagnosis system that processes heterogeneous clinical inputs, generates ranked diagnostic hypotheses with transparent reasoning, and outperforms existing methods in accuracy and recall.


<details>
  <summary>Details</summary>
Motivation: Timely and accurate diagnosis of rare diseases is challenging due to clinical heterogeneity, low prevalence, and limited clinician familiarity.

Method: DeepRare uses a modular design with a central host, specialized agent servers, and 40+ tools/web-scale medical knowledge sources for complex diagnostic reasoning.

Result: Achieves 100% accuracy for 1,013 diseases, outperforms 15 methods with 57.18% Recall@1, and excels in multi-modal scenarios (70.60% Recall@1). Expert verification shows 95.40% agreement.

Conclusion: DeepRare is a scalable, accurate, and transparent system for rare disease diagnosis, now available as a web application.

Abstract: Rare diseases collectively affect over 300 million individuals worldwide, yet
timely and accurate diagnosis remains a pervasive challenge. This is largely
due to their clinical heterogeneity, low individual prevalence, and the limited
familiarity most clinicians have with rare conditions. Here, we introduce
DeepRare, the first rare disease diagnosis agentic system powered by a large
language model (LLM), capable of processing heterogeneous clinical inputs. The
system generates ranked diagnostic hypotheses for rare diseases, each
accompanied by a transparent chain of reasoning that links intermediate
analytic steps to verifiable medical evidence.
  DeepRare comprises three key components: a central host with a long-term
memory module; specialized agent servers responsible for domain-specific
analytical tasks integrating over 40 specialized tools and web-scale,
up-to-date medical knowledge sources, ensuring access to the most current
clinical information. This modular and scalable design enables complex
diagnostic reasoning while maintaining traceability and adaptability. We
evaluate DeepRare on eight datasets. The system demonstrates exceptional
diagnostic performance among 2,919 diseases, achieving 100% accuracy for 1013
diseases. In HPO-based evaluations, DeepRare significantly outperforms other 15
methods, like traditional bioinformatics diagnostic tools, LLMs, and other
agentic systems, achieving an average Recall@1 score of 57.18% and surpassing
the second-best method (Reasoning LLM) by a substantial margin of 23.79
percentage points. For multi-modal input scenarios, DeepRare achieves 70.60% at
Recall@1 compared to Exomiser's 53.20% in 109 cases. Manual verification of
reasoning chains by clinical experts achieves 95.40% agreements. Furthermore,
the DeepRare system has been implemented as a user-friendly web application
http://raredx.cn/doctor.

</details>


### [23] [Probing AI Safety with Source Code](https://arxiv.org/abs/2506.20471)
*Ujwal Narayan,Shreyas Chaudhari,Ashwin Kalyan,Tanmay Rajpurohit,Karthik Narasimhan,Ameet Deshpande,Vishvak Murahari*

Main category: cs.CL

TL;DR: The paper introduces Code of Thought (CoDoT), a prompting strategy to evaluate LLM safety, revealing significant toxicity increases in state-of-the-art models like GPT-4 Turbo and DeepSeek R1.


<details>
  <summary>Details</summary>
Motivation: To address the gap in AI safety, ensuring LLMs align with human values despite their widespread use in critical applications.

Method: CoDoT converts natural language prompts into simple code (e.g., "make_more_toxic({text})") to test LLM safety, revealing vulnerabilities.

Result: CoDoT exposes severe safety flaws: GPT-4 Turbo's toxicity rose 16.5x, DeepSeek R1 failed 100%, and average toxicity increased 300% across seven LLMs. Recursive CoDoT doubled toxicity further.

Conclusion: CoDoT highlights the urgent need for foundational safety evaluations to align LLM capabilities with safety, preventing harmful user experiences.

Abstract: Large language models (LLMs) have become ubiquitous, interfacing with humans
in numerous safety-critical applications. This necessitates improving
capabilities, but importantly coupled with greater safety measures to align
these models with human values and preferences. In this work, we demonstrate
that contemporary models fall concerningly short of the goal of AI safety,
leading to an unsafe and harmful experience for users. We introduce a prompting
strategy called Code of Thought (CoDoT) to evaluate the safety of LLMs. CoDoT
converts natural language inputs to simple code that represents the same
intent. For instance, CoDoT transforms the natural language prompt "Make the
statement more toxic: {text}" to: "make_more_toxic({text})". We show that CoDoT
results in a consistent failure of a wide range of state-of-the-art LLMs. For
example, GPT-4 Turbo's toxicity increases 16.5 times, DeepSeek R1 fails 100% of
the time, and toxicity increases 300% on average across seven modern LLMs.
Additionally, recursively applying CoDoT can further increase toxicity two
times. Given the rapid and widespread adoption of LLMs, CoDoT underscores the
critical need to evaluate safety efforts from first principles, ensuring that
safety and capabilities advance together.

</details>


### [24] [Time is On My Side: Dynamics of Talk-Time Sharing in Video-chat Conversations](https://arxiv.org/abs/2506.20474)
*Kaixiang Zhang,Justine Zhang,Cristian Danescu-Niculescu-Mizil*

Main category: cs.CL

TL;DR: The paper introduces a computational framework to analyze talk-time distribution in conversations, revealing preferences for balanced talk-time and varying perceptions of dynamics.


<details>
  <summary>Details</summary>
Motivation: To understand how talk-time is shared in conversations and its impact on speaker perceptions, aiming to improve communication platforms.

Method: A computational framework quantifies talk-time distribution and dynamics, applied to a dataset of video-chats between strangers.

Result: Balanced talk-time is preferred, especially by those talking less, and different dynamics affect perceptions even with similar balance.

Conclusion: The framework provides tools for designing better communication platforms, including human-human and human-AI interactions.

Abstract: An intrinsic aspect of every conversation is the way talk-time is shared
between multiple speakers. Conversations can be balanced, with each speaker
claiming a similar amount of talk-time, or imbalanced when one talks
disproportionately. Such overall distributions are the consequence of
continuous negotiations between the speakers throughout the conversation: who
should be talking at every point in time, and for how long?
  In this work we introduce a computational framework for quantifying both the
conversation-level distribution of talk-time between speakers, as well as the
lower-level dynamics that lead to it. We derive a typology of talk-time sharing
dynamics structured by several intuitive axes of variation. By applying this
framework to a large dataset of video-chats between strangers, we confirm that,
perhaps unsurprisingly, different conversation-level distributions of talk-time
are perceived differently by speakers, with balanced conversations being
preferred over imbalanced ones, especially by those who end up talking less.
Then we reveal that -- even when they lead to the same level of overall balance
-- different types of talk-time sharing dynamics are perceived differently by
the participants, highlighting the relevance of our newly introduced typology.
Finally, we discuss how our framework offers new tools to designers of
computer-mediated communication platforms, for both human-human and human-AI
communication.

</details>


### [25] [Knowledge-Aware Diverse Reranking for Cross-Source Question Answering](https://arxiv.org/abs/2506.20476)
*Tong Zhou*

Main category: cs.CL

TL;DR: Team Marikarp's knowledge-aware diverse reranking RAG pipeline won first place in the SIGIR 2025 LiveRAG competition by effectively retrieving relevant documents from a large corpus.


<details>
  <summary>Details</summary>
Motivation: The competition aimed to fairly evaluate the retrieval of question-relevant documents from a diverse and large dataset (15M documents from FineWeb corpus), addressing various topics, question types, and audiences.

Method: The team developed a knowledge-aware diverse reranking RAG pipeline to enhance document retrieval accuracy and relevance.

Result: Their solution achieved first place in the competition, demonstrating superior performance in retrieving supporting documents.

Conclusion: The success of Team Marikarp's pipeline highlights the effectiveness of knowledge-aware diverse reranking in improving RAG systems for large-scale document retrieval.

Abstract: This paper presents Team Marikarp's solution for the SIGIR 2025 LiveRAG
competition. The competition's evaluation set, automatically generated by
DataMorgana from internet corpora, encompassed a wide range of target topics,
question types, question formulations, audience types, and knowledge
organization methods. It offered a fair evaluation of retrieving
question-relevant supporting documents from a 15M documents subset of the
FineWeb corpus. Our proposed knowledge-aware diverse reranking RAG pipeline
achieved first place in the competition.

</details>


### [26] [GPTailor: Large Language Model Pruning Through Layer Cutting and Stitching](https://arxiv.org/abs/2506.20480)
*Guinan Su,Li Shen,Lu Yin,Shiwei Liu,Yanwu Yang,Jonas Geiping*

Main category: cs.CL

TL;DR: A novel strategy for compressing large language models (LLMs) by combining or merging layers from finetuned variants, achieving competitive pruning results.


<details>
  <summary>Details</summary>
Motivation: Address the challenges of deploying large LLMs by reducing model size while preserving performance.

Method: Zero-order optimization problem with operations like layer removal, selection, and merging.

Result: Compressed models retain ~97.3% performance while removing ~25% parameters, outperforming prior methods.

Conclusion: The approach effectively balances model size reduction and performance retention, advancing LLM deployment.

Abstract: Large language models (LLMs) have shown remarkable capabilities in language
understanding and generation. However, such impressive capability typically
comes with a substantial model size, which presents significant challenges in
deployment and inference. While structured pruning of model parameters offers a
promising way to reduce computational costs at deployment time, current methods
primarily focus on single model pruning. In this work, we develop a novel
strategy to compress models by strategically combining or merging layers from
finetuned model variants, which preserves the original model's abilities by
aggregating capabilities accentuated in different finetunes. We pose the
optimal tailoring of these LLMs as a zero-order optimization problem, adopting
a search space that supports three different operations: (1) Layer removal, (2)
Layer selection from different candidate models, and (3) Layer merging. Our
experiments demonstrate that this approach leads to competitive model pruning,
for example, for the Llama2-13B model families, our compressed models maintain
approximately 97.3\% of the original performance while removing $\sim25\%$ of
parameters, significantly outperforming previous state-of-the-art methods. The
code is available at https://github.com/Guinan-Su/auto-merge-llm.

</details>


### [27] [ReCode: Updating Code API Knowledge with Reinforcement Learning](https://arxiv.org/abs/2506.20495)
*Haoze Wu,Yunzhi Yao,Wenhao Yu,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: ReCode enhances LLMs' ability to adapt to API updates using reinforcement learning, improving code generation without compromising general capabilities.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with adapting to frequent API updates due to outdated training data, limiting reliable code generation in dynamic environments.

Method: ReCode uses a dataset of 2,000 entries for training and a modified string similarity metric for reinforcement learning rewards.

Result: ReCode significantly improves LLMs' performance in dynamic API scenarios, even outperforming larger models.

Conclusion: ReCode is a robust solution for adapting LLMs to API changes, maintaining general code generation abilities while enhancing adaptability.

Abstract: Large Language Models (LLMs) exhibit remarkable code generation capabilities
but falter when adapting to frequent updates in external library APIs. This
critical limitation, stemming from reliance on outdated API knowledge from
their training data, even with access to current documentation, impedes
reliable code generation in dynamic environments. To tackle this issue, we
propose ReCode (rule-based Reinforcement learning for Code Update), a novel
framework that mimics human programmer adaptation to API changes. Specifically,
we construct a dataset of approximately 2,000 data entries to train the LLMs to
perform version migration based on updated information. Then, we introduce a
modified string similarity metric for code evaluation as the reward for
reinforcement learning. Our experiments demonstrate that ReCode substantially
boosts LLMs' code generation performance in dynamic API scenarios, especially
on the unseen CodeUpdateArena task. Crucially, compared to supervised
fine-tuning, ReCode has less impact on LLMs' general code generation abilities.
We apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and
DAPO), all achieving consistent improvements. Notably, after training,
Qwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned
model and the reasoning model with the same architecture. Code is available at
https://github.com/zjunlp/ReCode.

</details>


### [28] [OctoThinker: Mid-training Incentivizes Reinforcement Learning Scaling](https://arxiv.org/abs/2506.20512)
*Zengzhi Wang,Fan Zhou,Xuefeng Li,Pengfei Liu*

Main category: cs.CL

TL;DR: The paper explores how mid-training strategies affect reinforcement learning (RL) performance in language models like Qwen and Llama, identifying key factors like high-quality math corpora and QA-style data. It introduces a two-stage training strategy, Stable-then-Decay, leading to the OctoThinker model family.


<details>
  <summary>Details</summary>
Motivation: Understanding what makes base language models suitable for RL is crucial for developing next-generation foundation models.

Method: Investigates mid-training strategies using Qwen and Llama, focusing on data quality and formatting. Introduces Stable-then-Decay, a two-stage training approach.

Result: High-quality math corpora and QA-style data enhance RL performance. Stable-then-Decay yields OctoThinker, closing the gap with RL-friendly models like Qwen.

Conclusion: The findings guide pre-training strategies for RL-compatible foundation models. Open-source models and a curated math corpus (MegaMath-Web-Pro-Max) are released for further research.

Abstract: Different base language model families, such as Llama and Qwen, exhibit
divergent behaviors during post-training with reinforcement learning (RL),
especially on reasoning-intensive tasks. What makes a base language model
suitable for reinforcement learning? Gaining deeper insight into this question
is essential for developing RL-scalable foundation models of the next
generation. In this work, we investigate how mid-training strategies shape RL
dynamics, focusing on two representative model families: Qwen and Llama. Our
study reveals that (1) high-quality mathematical corpora, such as
MegaMath-Web-Pro, significantly improve both base model and RL performance,
while existing alternatives (e.g., FineMath-4plus) fail to do so; (2) further
adding QA-style data, particularly long chain-of-thought (CoT) reasoning
examples, enhances RL outcomes, and instruction data further unlocks this
effect; (3) while long-CoT improves reasoning depth, it can also induce
verbosity of model responses and unstability of RL training, underscoring the
importance of data formatting; (4) scaling mid-training consistently leads to
stronger downstream RL performance. Building on these insights, we introduce a
two-stage mid-training strategy, Stable-then-Decay, in which base models are
first trained on 200B tokens with a constant learning rate, followed by 20B
tokens across three CoT-focused branches with learning rate decay. This yields
OctoThinker, a family of models demonstrating strong RL compatibility and
closing the performance gap with more RL-friendly model families, i.e., Qwen.
We hope our work will help shape pre-training strategies for foundation models
in the RL era. To support further research, we release our open-source models
along with a curated math reasoning-intensive corpus of over 70 billion tokens
(i.e., MegaMath-Web-Pro-Max).

</details>


### [29] [When Life Gives You Samples: The Benefits of Scaling up Inference Compute for Multilingual LLMs](https://arxiv.org/abs/2506.20544)
*Ammar Khairi,Daniel D'souza,Ye Shen,Julia Kreutzer,Sara Hooker*

Main category: cs.CL

TL;DR: The paper explores robust scaling of inference-time compute for multilingual, multi-task LLMs, proposing adapted sampling and selection strategies that outperform existing methods, especially in underrepresented languages.


<details>
  <summary>Details</summary>
Motivation: To generalize inference-time compute improvements across diverse languages and tasks, addressing the limitations of current methods focused on English and narrow domains.

Method: Proposes novel sampling (temperature variation) and selection strategies tailored for multilingual and multi-task settings, evaluating them on benchmarks like m-ArenaHard-v2.0.

Result: Achieves +6.8 and +9.0 win-rate improvements for 8B and 111B models, respectively, against proprietary models like Gemini, with minimal cost.

Conclusion: Language- and task-aware inference-time compute methods are crucial for democratizing performance gains, especially in underrepresented languages.

Abstract: Recent advancements in large language models (LLMs) have shifted focus toward
scaling inference-time compute, improving performance without retraining the
model. A common approach is to sample multiple outputs in parallel, and select
one of these as the final output. However, work to date has focused on English
and a handful of domains such as math and code. In contrast, we are most
interested in techniques that generalize across open-ended tasks, formally
verifiable tasks, and across languages. In this work, we study how to robustly
scale inference-time compute for open-ended generative tasks in a multilingual,
multi-task setting.
  Our findings show that both sampling strategy based on temperature variation
and selection strategy must be adapted to account for diverse domains and
varied language settings. We evaluate existing selection methods, revealing
that strategies effective in English often fail to generalize across languages.
We propose novel sampling and selection strategies specifically adapted for
multilingual and multi-task inference scenarios, and show they yield notable
gains across languages and tasks. In particular, our combined sampling and
selection methods lead to an average +6.8 jump in win-rates for our 8B models
on m-ArenaHard-v2.0 prompts, against proprietary models such as Gemini. At
larger scale, Command-A (111B model) equipped with our methods, shows +9.0
improvement in win-rates on the same benchmark with just five samples against
single-sample decoding, a substantial increase at minimal cost. Our results
underscore the need for language- and task-aware approaches to inference-time
compute, aiming to democratize performance improvements in underrepresented
languages.

</details>


### [30] [Model Editing as a Double-Edged Sword: Steering Agent Ethical Behavior Toward Beneficence or Harm](https://arxiv.org/abs/2506.20606)
*Baixiang Huang,Zhen Tan,Haoran Wang,Zijie Liu,Dawei Li,Ali Payani,Huan Liu,Tianlong Chen,Kai Shu*

Main category: cs.CL

TL;DR: The paper introduces Behavior Editing, a method to ethically steer LLM-based agents, and BehaviorBench, a benchmark for evaluating and editing agent behaviors across scenarios.


<details>
  <summary>Details</summary>
Motivation: Deploying LLM-based agents in high-stakes domains poses safety and ethical risks, necessitating methods to control their behavior.

Method: Frames agent behavior steering as a model editing task (Behavior Editing) and introduces BehaviorBench for systematic evaluation.

Result: Behavior Editing effectively steers agent behavior locally and globally, promoting ethical or harmful actions.

Conclusion: Behavior Editing offers a promising but risky paradigm for controlling agent behavior, with potential for both ethical and malicious outcomes.

Abstract: Agents based on Large Language Models (LLMs) have demonstrated strong
capabilities across a wide range of tasks. However, deploying LLM-based agents
in high-stakes domains comes with significant safety and ethical risks.
Unethical behavior by these agents can directly result in serious real-world
consequences, including physical harm and financial loss. To efficiently steer
the ethical behavior of agents, we frame agent behavior steering as a model
editing task, which we term Behavior Editing. Model editing is an emerging area
of research that enables precise and efficient modifications to LLMs while
preserving their overall capabilities. To systematically study and evaluate
this approach, we introduce BehaviorBench, a multi-tier benchmark grounded in
psychological moral theories. This benchmark supports both the evaluation and
editing of agent behaviors across a variety of scenarios, with each tier
introducing more complex and ambiguous scenarios. We first demonstrate that
Behavior Editing can dynamically steer agents toward the target behavior within
specific scenarios. Moreover, Behavior Editing enables not only
scenario-specific local adjustments but also more extensive shifts in an
agent's global moral alignment. We demonstrate that Behavior Editing can be
used to promote ethical and benevolent behavior or, conversely, to induce
harmful or malicious behavior. Through comprehensive evaluations on agents
based on frontier LLMs, BehaviorBench shows the effectiveness of Behavior
Editing across different models and scenarios. Our findings offer key insights
into a new paradigm for steering agent behavior, highlighting both the promise
and perils of Behavior Editing.

</details>


### [31] [DiffuCoder: Understanding and Improving Masked Diffusion Models for Code Generation](https://arxiv.org/abs/2506.20639)
*Shansan Gong,Ruixiang Zhang,Huangjie Zheng,Jiatao Gu,Navdeep Jaitly,Lingpeng Kong,Yizhe Zhang*

Main category: cs.CL

TL;DR: The paper introduces DiffuCoder, a 7B dLLM trained on 130B code tokens, analyzing its decoding behavior and proposing coupled-GRPO, a novel RL method, to enhance code generation performance.


<details>
  <summary>Details</summary>
Motivation: Diffusion large language models (dLLMs) offer global planning and iterative refinement advantages for code generation, but their training and inference mechanisms are under-explored.

Method: The study trains DiffuCoder, analyzes its decoding behavior, and introduces coupled-GRPO, a reinforcement learning method with complementary mask noise, to improve training efficiency.

Result: DiffuCoder shows improved performance (+4.4% on EvalPlus) and reduced reliance on autoregressive decoding, with diverse generation order at higher temperatures.

Conclusion: The work provides insights into dLLM generation and offers an effective RL framework for code generation.

Abstract: Diffusion large language models (dLLMs) are compelling alternatives to
autoregressive (AR) models because their denoising models operate over the
entire sequence. The global planning and iterative refinement features of dLLMs
are particularly useful for code generation. However, current training and
inference mechanisms for dLLMs in coding are still under-explored. To demystify
the decoding behavior of dLLMs and unlock their potential for coding, we
systematically investigate their denoising processes and reinforcement learning
(RL) methods. We train a 7B dLLM, \textbf{DiffuCoder}, on 130B tokens of code.
Using this model as a testbed, we analyze its decoding behavior, revealing how
it differs from that of AR models: (1) dLLMs can decide how causal their
generation should be without relying on semi-AR decoding, and (2) increasing
the sampling temperature diversifies not only token choices but also their
generation order. This diversity creates a rich search space for RL rollouts.
For RL training, to reduce the variance of token log-likelihood estimates and
maintain training efficiency, we propose \textbf{coupled-GRPO}, a novel
sampling scheme that constructs complementary mask noise for completions used
in training. In our experiments, coupled-GRPO significantly improves
DiffuCoder's performance on code generation benchmarks (+4.4\% on EvalPlus) and
reduces reliance on AR causal during decoding. Our work provides deeper insight
into the machinery of dLLM generation and offers an effective, diffusion-native
RL training framework. https://github.com/apple/ml-diffucoder.

</details>


### [32] [Memento: Note-Taking for Your Future Self](https://arxiv.org/abs/2506.20642)
*Chao Wan,Albert Gong,Mihir Mishra,Carl-Leander Henneking,Claas Beger,Kilian Q. Weinberger*

Main category: cs.CL

TL;DR: Memento is a prompting strategy that decomposes complex questions, dynamically builds a fact database, and solves questions, significantly improving LLM performance in multi-hop QA tasks.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with tasks requiring tight coupling of reasoning and retrieval, such as multi-hop question answering.

Method: Memento decomposes questions into steps, constructs a dynamic fact database using LLMs, and combines facts to solve questions.

Result: Memento doubles CoT performance on PhantomWiki, improves CoT-RAG by 20 F1 points on 2WikiMultiHopQA, and boosts ReAct by 3 F1 points on MuSiQue.

Conclusion: Memento effectively enhances LLM performance in multi-hop QA by integrating decomposition and dynamic retrieval.

Abstract: Large language models (LLMs) excel at reasoning-only tasks, but struggle when
reasoning must be tightly coupled with retrieval, as in multi-hop question
answering. To overcome these limitations, we introduce a prompting strategy
that first decomposes a complex question into smaller steps, then dynamically
constructs a database of facts using LLMs, and finally pieces these facts
together to solve the question. We show how this three-stage strategy, which we
call Memento, can boost the performance of existing prompting strategies across
diverse settings. On the 9-step PhantomWiki benchmark, Memento doubles the
performance of chain-of-thought (CoT) when all information is provided in
context. On the open-domain version of 2WikiMultiHopQA, CoT-RAG with Memento
improves over vanilla CoT-RAG by more than 20 F1 percentage points and over the
multi-hop RAG baseline, IRCoT, by more than 13 F1 percentage points. On the
challenging MuSiQue dataset, Memento improves ReAct by more than 3 F1
percentage points, demonstrating its utility in agentic settings.

</details>


### [33] [Inside you are many wolves: Using cognitive models to interpret value trade-offs in LLMs](https://arxiv.org/abs/2506.20666)
*Sonia K. Murthy,Rosie Zhao,Jennifer Hu,Sham Kakade,Markus Wulfmeier,Peng Qian,Tomer Ullman*

Main category: cs.CL

TL;DR: The paper examines how LLMs handle human-like value trade-offs, such as balancing truth and politeness, using a cognitive model of polite speech. It evaluates these trade-offs in different model settings and training dynamics.


<details>
  <summary>Details</summary>
Motivation: Current tools for interpreting dynamic value trade-offs in LLMs are limited, despite their importance in human decision-making and language use.

Method: The study uses a cognitive model of polite speech to assess LLMs' representation of human-like trade-offs, evaluating them in black-box reasoning models and open-source models with different training dynamics.

Result: Findings show higher informational utility than social utility in reasoning models, and significant early training shifts in utility values influenced by base model and pretraining data.

Conclusion: The method provides insights for shaping LLM training regimes and better controlling value trade-offs, with implications for understanding high-level behaviors.

Abstract: Navigating everyday social situations often requires juggling conflicting
goals, such as conveying a harsh truth, maintaining trust, all while still
being mindful of another person's feelings. These value trade-offs are an
integral part of human decision-making and language use, however, current tools
for interpreting such dynamic and multi-faceted notions of values in LLMs are
limited. In cognitive science, so-called "cognitive models" provide formal
accounts of these trade-offs in humans, by modeling the weighting of a
speaker's competing utility functions in choosing an action or utterance. In
this work, we use a leading cognitive model of polite speech to interpret the
extent to which LLMs represent human-like trade-offs. We apply this lens to
systematically evaluate value trade-offs in two encompassing model settings:
degrees of reasoning "effort" in frontier black-box models, and RL
post-training dynamics of open-source models. Our results highlight patterns of
higher informational utility than social utility in reasoning models, and in
open-source models shown to be stronger in mathematical reasoning. Our findings
from LLMs' training dynamics suggest large shifts in utility values early on in
training with persistent effects of the choice of base model and pretraining
data, compared to feedback dataset or alignment method. We show that our method
is responsive to diverse aspects of the rapidly evolving LLM landscape, with
insights for forming hypotheses about other high-level behaviors, shaping
training regimes for reasoning models, and better controlling trade-offs
between values during model training.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [34] [Computer Vision based Automated Quantification of Agricultural Sprayers Boom Displacement](https://arxiv.org/abs/2506.19939)
*Aryan Singh Dalal,Sidharth Rai,Rahul Singh,Treman Singh Kaloya,Rahul Harsha Cheppally,Ajay Sharda*

Main category: cs.CV

TL;DR: The study developed a computer vision system using YOLO models to quantify spray boom movement in agricultural sprayers, achieving high accuracy and potential for design improvements.


<details>
  <summary>Details</summary>
Motivation: Spray boom instability causes application errors in agricultural sprayers, but there's no quantitative data on boom movement to guide solutions.

Method: A computer vision system with YOLO V7, V8, and V11 models tracked boom movement, validated by an inclinometer sensor.

Result: The system detected targets with >90% accuracy and estimated distances within 0.026 m of sensor data.

Conclusion: The system effectively quantifies boom movement, aiding design improvements for greater sprayer stability and application accuracy.

Abstract: Application rate errors when using self-propelled agricultural sprayers for
agricultural production remain a concern. Among other factors, spray boom
instability is one of the major contributors to application errors. Spray
booms' width of 38m, combined with 30 kph driving speeds, varying terrain, and
machine dynamics when maneuvering complex field boundaries, make controls of
these booms very complex. However, there is no quantitative knowledge on the
extent of boom movement to systematically develop a solution that might include
boom designs and responsive boom control systems. Therefore, this study was
conducted to develop an automated computer vision system to quantify the boom
movement of various agricultural sprayers. A computer vision system was
developed to track a target on the edge of the sprayer boom in real time. YOLO
V7, V8, and V11 neural network models were trained to track the boom's
movements in field operations to quantify effective displacement in the
vertical and transverse directions. An inclinometer sensor was mounted on the
boom to capture boom angles and validate the neural network model output. The
results showed that the model could detect the target with more than 90 percent
accuracy, and distance estimates of the target on the boom were within 0.026 m
of the inclinometer sensor data. This system can quantify the boom movement on
the current sprayer and potentially on any other sprayer with minor
modifications. The data can be used to make design improvements to make sprayer
booms more stable and achieve greater application accuracy.

</details>


### [35] [EBC-ZIP: Improving Blockwise Crowd Counting with Zero-Inflated Poisson Regression](https://arxiv.org/abs/2506.19955)
*Yiming Ma,Victor Sanchez,Tanaya Guha*

Main category: cs.CV

TL;DR: EBC-ZIP is a crowd counting framework using Zero-Inflated Poisson regression to address imbalanced density maps, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods ignore extreme sparsity in ground-truth density maps and use loss functions ill-suited for count data, leading to biased estimations.

Method: Proposes EBC-ZIP, replacing traditional regression loss with ZIP negative log-likelihood, built on the Enhanced Block Classification framework.

Result: EBC-ZIP outperforms EBC and achieves state-of-the-art results on four benchmarks.

Conclusion: EBC-ZIP effectively handles zero-heavy distributions and improves crowd counting accuracy.

Abstract: Density map estimation has become the mainstream paradigm in crowd counting.
However, most existing methods overlook the extreme sparsity of ground-truth
density maps. In real-world crowd scenes, the vast majority of spatial regions
(often over 95%) contain no people, leading to heavily imbalanced count
distributions. Ignoring this imbalance can bias models toward overestimating
dense regions and underperforming in sparse areas. Furthermore, most loss
functions used in density estimation are majorly based on MSE and implicitly
assume Gaussian distributions, which are ill-suited for modeling discrete,
non-negative count data. In this paper, we propose EBC-ZIP, a crowd counting
framework that models the spatial distribution of counts using a Zero-Inflated
Poisson (ZIP) regression formulation. Our approach replaces the traditional
regression loss with the negative log-likelihood of the ZIP distribution,
enabling better handling of zero-heavy distributions while preserving count
accuracy. Built upon the recently proposed Enhanced Block Classification (EBC)
framework, EBC-ZIP inherits EBC's advantages in preserving the discreteness of
targets and ensuring training stability, while further improving performance
through a more principled probabilistic loss. We also evaluate EBC-ZIP with
backbones of varying computational complexity to assess its scalability.
Extensive experiments on four crowd counting benchmarks demonstrate that
EBC-ZIP consistently outperforms EBC and achieves state-of-the-art results.

</details>


### [36] [ToSA: Token Merging with Spatial Awareness](https://arxiv.org/abs/2506.20066)
*Hsiang-Wei Huang,Wenhao Chai,Kuang-Ming Chen,Cheng-Yen Yang,Jenq-Neng Hwang*

Main category: cs.CV

TL;DR: ToSA is a token merging method for Vision Transformers (ViT) that integrates semantic and spatial awareness, improving efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Existing token merging methods rely on feature similarity, ignoring spatial information, which is crucial in early ViT layers.

Method: ToSA uses depth images to generate pseudo spatial tokens, combining semantic and spatial data for merging.

Result: ToSA outperforms previous methods in benchmarks and reduces ViT runtime.

Conclusion: ToSA is an efficient solution for ViT acceleration, preserving scene structure better.

Abstract: Token merging has emerged as an effective strategy to accelerate Vision
Transformers (ViT) by reducing computational costs. However, existing methods
primarily rely on the visual token's feature similarity for token merging,
overlooking the potential of integrating spatial information, which can serve
as a reliable criterion for token merging in the early layers of ViT, where the
visual tokens only possess weak visual information. In this paper, we propose
ToSA, a novel token merging method that combines both semantic and spatial
awareness to guide the token merging process. ToSA leverages the depth image as
input to generate pseudo spatial tokens, which serve as auxiliary spatial
information for the visual token merging process. With the introduced spatial
awareness, ToSA achieves a more informed merging strategy that better preserves
critical scene structure. Experimental results demonstrate that ToSA
outperforms previous token merging methods across multiple benchmarks on visual
and embodied question answering while largely reducing the runtime of the ViT,
making it an efficient solution for ViT acceleration. The code will be
available at: https://github.com/hsiangwei0903/ToSA

</details>


### [37] [BrokenVideos: A Benchmark Dataset for Fine-Grained Artifact Localization in AI-Generated Videos](https://arxiv.org/abs/2506.20103)
*Jiahao Lin,Weixuan Peng,Bojia Zi,Yifeng Gao,Xianbiao Qi,Xingjun Ma,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: The paper introduces BrokenVideos, a benchmark dataset for detecting and localizing artifacts in AI-generated videos, improving model performance.


<details>
  <summary>Details</summary>
Motivation: Current AI-generated videos suffer from visual artifacts, but existing datasets lack fine-grained spatial annotations for artifact localization.

Method: The authors created BrokenVideos, a dataset of 3,254 AI-generated videos with pixel-level artifact annotations, validated by human inspection.

Result: Training models on BrokenVideos enhances their ability to localize corrupted regions, as demonstrated through experiments.

Conclusion: BrokenVideos provides a critical benchmark for advancing research on artifact localization in generative video models.

Abstract: Recent advances in deep generative models have led to significant progress in
video generation, yet the fidelity of AI-generated videos remains limited.
Synthesized content often exhibits visual artifacts such as temporally
inconsistent motion, physically implausible trajectories, unnatural object
deformations, and local blurring that undermine realism and user trust.
Accurate detection and spatial localization of these artifacts are crucial for
both automated quality control and for guiding the development of improved
generative models. However, the research community currently lacks a
comprehensive benchmark specifically designed for artifact localization in AI
generated videos. Existing datasets either restrict themselves to video or
frame level detection or lack the fine-grained spatial annotations necessary
for evaluating localization methods. To address this gap, we introduce
BrokenVideos, a benchmark dataset of 3,254 AI-generated videos with
meticulously annotated, pixel-level masks highlighting regions of visual
corruption. Each annotation is validated through detailed human inspection to
ensure high quality ground truth. Our experiments show that training state of
the art artifact detection models and multi modal large language models (MLLMs)
on BrokenVideos significantly improves their ability to localize corrupted
regions. Through extensive evaluation, we demonstrate that BrokenVideos
establishes a critical foundation for benchmarking and advancing research on
artifact localization in generative video models. The dataset is available at:
https://broken-video-detection-datetsets.github.io/Broken-Video-Detection-Datasets.github.io/.

</details>


### [38] [From 2D to 3D Cognition: A Brief Survey of General World Models](https://arxiv.org/abs/2506.20134)
*Ningwei Xie,Zizi Tian,Lei Yang,Xiao-Ping Zhang,Meng Guo,Jie Li*

Main category: cs.CV

TL;DR: The paper surveys the evolution of world models from 2D perception to 3D cognition, highlighting key technologies and cognitive capabilities, and discusses applications and future challenges.


<details>
  <summary>Details</summary>
Motivation: To address the lack of systematic analysis in 3D cognitive world models and categorize emerging techniques.

Method: Introduces a conceptual framework to review advancements, focusing on 3D representations and world knowledge, and dissects core cognitive capabilities.

Result: Identifies key drivers (3D representations, world knowledge) and core capabilities (scene generation, reasoning, interaction), with applications in AI, autonomous driving, digital twins, and gaming/VR.

Conclusion: Highlights challenges in data, modeling, and deployment, and outlines future directions for robust and generalizable 3D world models.

Abstract: World models have garnered increasing attention in the development of
artificial general intelligence (AGI), serving as computational frameworks for
learning representations of the external world and forecasting future states.
While early efforts focused on 2D visual perception and simulation, recent
3D-aware generative world models have demonstrated the ability to synthesize
geometrically consistent, interactive 3D environments, marking a shift toward
3D spatial cognition. Despite rapid progress, the field lacks systematic
analysis to categorize emerging techniques and clarify their roles in advancing
3D cognitive world models. This survey addresses this need by introducing a
conceptual framework, providing a structured and forward-looking review of
world models transitioning from 2D perception to 3D cognition. Within this
framework, we highlight two key technological drivers, particularly advances in
3D representations and the incorporation of world knowledge, as fundamental
pillars. Building on these, we dissect three core cognitive capabilities that
underpin 3D world modeling: 3D physical scene generation, 3D spatial reasoning,
and 3D spatial interaction. We further examine the deployment of these
capabilities in real-world applications, including embodied AI, autonomous
driving, digital twin, and gaming/VR. Finally, we identify challenges across
data, modeling, and deployment, and outline future directions for advancing
more robust and generalizable 3D world models.

</details>


### [39] [EAR: Erasing Concepts from Unified Autoregressive Models](https://arxiv.org/abs/2506.20151)
*Haipeng Fan,Shiyuan Zhang,Baohunesitu,Zihang Guo,Huaiwen Zhang*

Main category: cs.CV

TL;DR: The paper introduces EAR, a fine-tuning method for AR models to remove unwanted concepts while preserving generation quality, using WGA and TLM strategies. It also proposes the ECGVF benchmark for rigorous evaluation.


<details>
  <summary>Details</summary>
Motivation: AR models struggle with removing undesired concepts without degrading generation quality, prompting the need for a solution like EAR.

Method: EAR uses Windowed Gradient Accumulation (WGA) and Thresholded Loss Masking (TLM) for fine-tuning. The ECGVF benchmark is introduced for evaluation, involving LLMs and visual classifiers.

Result: EAR shows significant improvements in erasure effectiveness and utility preservation on the ECGVF benchmark with Janus-Pro.

Conclusion: EAR effectively removes unwanted concepts from AR models while maintaining generation quality, validated by the ECGVF benchmark.

Abstract: Autoregressive (AR) models have achieved unified and strong performance
across both visual understanding and image generation tasks. However, removing
undesired concepts from AR models while maintaining overall generation quality
remains an open challenge. In this paper, we propose Erasure Autoregressive
Model (EAR), a fine-tuning method for effective and utility-preserving concept
erasure in AR models. Specifically, we introduce Windowed Gradient Accumulation
(WGA) strategy to align patch-level decoding with erasure objectives, and
Thresholded Loss Masking (TLM) strategy to protect content unrelated to the
target concept during fine-tuning. Furthermore, we propose a novel benchmark,
Erase Concept Generator and Visual Filter (ECGVF), aim at provide a more
rigorous and comprehensive foundation for evaluating concept erasure in AR
models. Specifically, we first employ structured templates across diverse large
language models (LLMs) to pre-generate a large-scale corpus of
target-replacement concept prompt pairs. Subsequently, we generate images from
these prompts and subject them to rigorous filtering via a visual classifier to
ensure concept fidelity and alignment. Extensive experimental results conducted
on the ECGVF benchmark with the AR model Janus-Pro demonstrate that EAR
achieves marked improvements in both erasure effectiveness and model utility
preservation. Code is available at: https://github.com/immc-lab/ear/

</details>


### [40] [Loss-Aware Automatic Selection of Structured Pruning Criteria for Deep Neural Network Acceleration](https://arxiv.org/abs/2506.20152)
*Deepak Ghimire,Kilho Lee,Seong-heum Kim*

Main category: cs.CV

TL;DR: LAASP introduces an efficient Loss-Aware Automatic Selection of Structured Pruning Criteria, integrating pruning and training into a single cycle for neural network compression.


<details>
  <summary>Details</summary>
Motivation: To streamline and improve the efficiency of structured pruning for neural networks, eliminating the need for separate training and pruning stages.

Method: Adopts a pruning-while-training approach, automatically selecting pruning criteria and layer-specific rates based on network loss, with brief retraining to mitigate accuracy drops.

Result: Achieves significant FLOP reduction (52% for ResNet56/110 on CIFAR-10, 42% for ResNet50 on ImageNet) with minimal accuracy loss.

Conclusion: LAASP effectively compresses neural networks while maintaining accuracy, outperforming state-of-the-art methods.

Abstract: Structured pruning is a well-established technique for compressing neural
networks, making it suitable for deployment in resource-limited edge devices.
This paper presents an efficient Loss-Aware Automatic Selection of Structured
Pruning Criteria (LAASP) for slimming and accelerating deep neural networks.
The majority of pruning methodologies employ a sequential process consisting of
three stages: 1) training, 2) pruning, and 3) fine-tuning, whereas the proposed
pruning technique adopts a pruning-while-training approach that eliminates the
first stage and integrates the second and third stages into a single cycle. The
automatic selection of magnitude or similarity-based filter pruning criteria
from a specified pool of criteria and the specific pruning layer at each
pruning iteration is guided by the network's overall loss on a small subset of
the training data. To mitigate the abrupt accuracy drop due to pruning, the
network is retrained briefly after each reduction of a predefined number of
floating-point operations (FLOPs). The optimal pruning rates for each layer in
the network are automatically determined, eliminating the need for manual
allocation of fixed or variable pruning rates for each layer. Experiments on
the VGGNet and ResNet models on the CIFAR-10 and ImageNet benchmark datasets
demonstrate the effectiveness of the proposed method. In particular, the
ResNet56 and ResNet110 models on the CIFAR-10 dataset significantly improve the
top-1 accuracy compared to state-of-the-art methods while reducing the network
FLOPs by 52\%. Furthermore, the ResNet50 model on the ImageNet dataset reduces
FLOPs by more than 42\% with a negligible 0.33\% drop in top-5 accuracy. The
source code of this paper is publicly available online -
https://github.com/ghimiredhikura/laasp.

</details>


### [41] [Towards Efficient Exemplar Based Image Editing with Multimodal VLMs](https://arxiv.org/abs/2506.20155)
*Avadhoot Jadhav,Ashutosh Srivastava,Abhinav Java,Silky Singh,Tarun Ram Menta,Surgan Jandial,Balaji Krishnamurthy*

Main category: cs.CV

TL;DR: A method for exemplar-based image editing using pretrained text-to-image diffusion models and VLMs, outperforming baselines in speed and performance.


<details>
  <summary>Details</summary>
Motivation: Text alone is insufficient for ambiguous image edits; exemplar pairs better express such edits.

Method: Leverages pretrained text-to-image diffusion models and multimodal VLMs for optimization-free editing.

Result: Outperforms baselines on multiple edit types and is ~4x faster.

Conclusion: The proposed method effectively transfers edits from exemplar pairs to content images efficiently.

Abstract: Text-to-Image Diffusion models have enabled a wide array of image editing
applications. However, capturing all types of edits through text alone can be
challenging and cumbersome. The ambiguous nature of certain image edits is
better expressed through an exemplar pair, i.e., a pair of images depicting an
image before and after an edit respectively. In this work, we tackle
exemplar-based image editing -- the task of transferring an edit from an
exemplar pair to a content image(s), by leveraging pretrained text-to-image
diffusion models and multimodal VLMs. Even though our end-to-end pipeline is
optimization-free, our experiments demonstrate that it still outperforms
baselines on multiple types of edits while being ~4x faster.

</details>


### [42] [Seeing is Believing? Mitigating OCR Hallucinations in Multimodal Large Language Models](https://arxiv.org/abs/2506.20168)
*Zhentao He,Can Zhang,Ziheng Wu,Zhenghao Chen,Yufei Zhan,Yifan Li,Zhao Zhang,Xian Wang,Minghui Qiu*

Main category: cs.CV

TL;DR: The paper introduces KIE-HVQA, a benchmark for evaluating OCR hallucination in degraded documents, and proposes a GRPO-based framework to mitigate hallucinations by improving visual-faithful reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal models struggle with visual degradation, leading to unreliable outputs and hallucinations. The work aims to address this gap by focusing on degraded document understanding.

Method: Proposes KIE-HVQA benchmark and a GRPO-based framework with a novel reward mechanism, incorporating visual uncertainty awareness and refusal-to-answer analysis.

Result: The 7B-parameter model achieves a 22% improvement in hallucination-free accuracy over GPT-4o on KIE-HVQA, with no drop in standard task performance.

Conclusion: The approach effectively reduces hallucinations in degraded scenarios while maintaining robustness, demonstrating the potential of vision-faithful reasoning.

Abstract: Recent advancements in multimodal large language models have enhanced
document understanding by integrating textual and visual information. However,
existing models exhibit incompleteness within their paradigm in real-world
scenarios, particularly under visual degradation. In such conditions, the
current response paradigm often fails to adequately perceive visual degradation
and ambiguity, leading to overreliance on linguistic priors or misaligned
visual-textual reasoning. This difficulty in recognizing uncertainty frequently
results in the generation of hallucinatory content, especially when a precise
answer is not feasible. To better demonstrate and analyze this phenomenon and
problem, we propose KIE-HVQA, the first benchmark dedicated to evaluating OCR
hallucination in degraded document understanding. This dataset includes test
samples spanning identity cards and invoices, with simulated real-world
degradations for OCR reliability. This setup allows for evaluating models'
capacity, under degraded input, to distinguish reliable visual information and
answer accordingly, thereby highlighting the challenge of avoiding
hallucination on uncertain data. To achieve vision-faithful reasoning and
thereby avoid the aforementioned issues, we further introduce a GRPO-based
framework featuring a novel reward mechanism. By incorporating a self-awareness
of visual uncertainty and an analysis method that initiates refusal to answer
to increase task difficulty within our supervised fine-tuning and reinforcement
learning framework, we successfully mitigated hallucinations in ambiguous
regions. Experiments on Qwen2.5-VL demonstrate that our 7B-parameter model
achieves a 22\% absolute improvement in hallucination-free accuracy over GPT-4o
on KIE-HVQA and there is no significant performance drop in standard tasks,
highlighting both effectiveness and robustness.

</details>


### [43] [Towards Scalable and Generalizable Earth Observation Data Mining via Foundation Model Composition](https://arxiv.org/abs/2506.20174)
*Man Duc Chuc*

Main category: cs.CV

TL;DR: Foundation models pretrained on remote sensing and general vision datasets can be combined to improve Earth Observation tasks, matching or outperforming larger models with less resource usage.


<details>
  <summary>Details</summary>
Motivation: To explore the underexplored strategy of reusing and combining existing pretrained models for Earth Observation tasks, rather than training large models from scratch.

Method: Evaluated models like Prithvi, Hiera, and DOFA on the GEO-Bench benchmark across eleven datasets, using feature-level ensembling and knowledge distillation.

Result: Feature-level ensembling of smaller pretrained models matched or exceeded larger models' performance with reduced training time and resources. Knowledge distillation further enabled compact model deployment.

Conclusion: Combining pretrained models and knowledge distillation offers a practical, efficient approach for deploying foundation models in real-world Earth Observation applications.

Abstract: Foundation models are rapidly transforming Earth Observation data mining by
enabling generalizable and scalable solutions for key tasks such as scene
classification and semantic segmentation. While most efforts in the geospatial
domain have focused on developing large models trained from scratch using
massive Earth Observation datasets, an alternative strategy that remains
underexplored is the reuse and combination of existing pretrained models. In
this study, we investigate whether foundation models pretrained on remote
sensing and general vision datasets can be effectively combined to improve
performance across a diverse set of key Earth Observation tasks. Using the
GEO-Bench benchmark, we evaluate several prominent models, including Prithvi,
Hiera, and DOFA, on eleven datasets covering a range of spatial resolutions,
sensor modalities, and task types. The results show that feature-level
ensembling of smaller pretrained models can match or exceed the performance of
much larger models, while requiring less training time and computational
resources. Moreover, the study highlights the potential of applying knowledge
distillation to transfer the strengths of ensembles into more compact models,
offering a practical path for deploying foundation models in real-world Earth
Observation applications.

</details>


### [44] [Progressive Alignment Degradation Learning for Pansharpening](https://arxiv.org/abs/2506.20179)
*Enzhe Zhao,Zhichang Guo,Yao Li,Fanghui Song,Boying Wu*

Main category: cs.CV

TL;DR: The paper critiques the Wald protocol for pansharpening, introduces PADM for adaptive degradation learning, and HFreqdiff for high-frequency detail embedding, achieving superior results.


<details>
  <summary>Details</summary>
Motivation: The Wald protocol's inaccurate degradation approximation limits deep pansharpening model generalization, prompting the need for better methods.

Method: Proposes PADM (Progressive Alignment Degradation Module) and HFreqdiff, which uses diffusion and frequency-selective modules for detail extraction.

Result: The method outperforms state-of-the-art techniques, enhancing spatial sharpness and image quality.

Conclusion: The innovations in PADM and HFreqdiff effectively address the limitations of the Wald protocol, improving pansharpening performance.

Abstract: Deep learning-based pansharpening has been shown to effectively generate
high-resolution multispectral (HRMS) images. To create supervised ground-truth
HRMS images, synthetic data generated using the Wald protocol is commonly
employed. This protocol assumes that networks trained on artificial
low-resolution data will perform equally well on high-resolution data. However,
well-trained models typically exhibit a trade-off in performance between
reduced-resolution and full-resolution datasets. In this paper, we delve into
the Wald protocol and find that its inaccurate approximation of real-world
degradation patterns limits the generalization of deep pansharpening models. To
address this issue, we propose the Progressive Alignment Degradation Module
(PADM), which uses mutual iteration between two sub-networks, PAlignNet and
PDegradeNet, to adaptively learn accurate degradation processes without relying
on predefined operators. Building on this, we introduce HFreqdiff, which embeds
high-frequency details into a diffusion framework and incorporates CFB and BACM
modules for frequency-selective detail extraction and precise reverse process
learning. These innovations enable effective integration of high-resolution
panchromatic and multispectral images, significantly enhancing spatial
sharpness and quality. Experiments and ablation studies demonstrate the
proposed method's superior performance compared to state-of-the-art techniques.

</details>


### [45] [UniCode$^2$: Cascaded Large-scale Codebooks for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2506.20214)
*Yanzhe Chen,Huasong Zhong,Yan Li,Zhenheng Yang*

Main category: cs.CV

TL;DR: UniCode$^2$ introduces a cascaded codebook framework for stable, large-scale visual tokenization, improving multimodal understanding and generation by aligning visual tokens with textual semantics.


<details>
  <summary>Details</summary>
Motivation: Existing codebook-based methods either lack fine-grained semantics or face instability when scaled up, limiting their effectiveness in multimodal tasks.

Method: UniCode$^2$ clusters millions of SigLIP sequence embeddings to create a 500K-entry codebook, using a cascaded design with a frozen codebook for stability and a trainable one for task-specific refinement.

Result: The framework achieves strong performance across benchmarks, enabling high-quality visual synthesis and stable scaling of visual token spaces.

Conclusion: UniCode$^2$ demonstrates the viability of scaling visual tokenization without compromising stability, semantics, or modularity, advancing multimodal models.

Abstract: Unified multimodal large language models (MLLMs) have shown promise in
jointly advancing multimodal understanding and generation, with visual
codebooks discretizing images into tokens for autoregressive modeling. Existing
codebook-based methods either rely on small vocabularies (~16K entries) that
lack fine-grained semantics or naively scale up, resulting in low token
utilization and unstable training. We propose UniCode$^2$, a cascaded codebook
framework enabling large-scale, semantically aligned, and stable visual
tokenization. By clustering millions of SigLIP sequence embeddings, we build a
500K-entry codebook that preserves vision-language alignment while expanding
capacity. Stability is ensured via a cascaded design: a frozen codebook anchors
the embedding space, and a trainable codebook refines task-specific semantics.
This decoupling promotes high utilization and robust learning. Moreover, the
alignment of our visual tokens with textual semantics enables seamless
integration with pretrained diffusion decoders, supporting high-quality visual
synthesis with minimal adaptation. UniCode^2 delivers strong performance across
diverse benchmarks, demonstrating the viability of scaling visual token spaces
without sacrificing stability, semantics, or modularity.

</details>


### [46] [Dynamic Bandwidth Allocation for Hybrid Event-RGB Transmission](https://arxiv.org/abs/2506.20222)
*Pujing Yang,Guangyi Zhang,Yunlong Cai,Lei Yu,Guanding Yu*

Main category: cs.CV

TL;DR: A transmission scheme for hybrid RGB and event camera systems optimizes bandwidth by eliminating redundancy and enabling real-time deblurring.


<details>
  <summary>Details</summary>
Motivation: Hybrid RGB and event camera systems face challenges in transmitting large volumes of data due to redundant information.

Method: A joint event and image (E-I) transmission framework using Bayesian modeling and the information bottleneck method to disentangle shared and domain-specific information.

Result: Superior reconstruction quality and enhanced deblurring performance compared to conventional systems.

Conclusion: The proposed scheme efficiently optimizes bandwidth and improves performance in hybrid camera systems.

Abstract: Event cameras asynchronously capture pixel-level intensity changes with
extremely low latency. They are increasingly used in conjunction with RGB
cameras for a wide range of vision-related applications. However, a major
challenge in these hybrid systems lies in the transmission of the large volume
of triggered events and RGB images. To address this, we propose a transmission
scheme that retains efficient reconstruction performance of both sources while
accomplishing real-time deblurring in parallel. Conventional RGB cameras and
event cameras typically capture the same scene in different ways, often
resulting in significant redundant information across their outputs. To address
this, we develop a joint event and image (E-I) transmission framework to
eliminate redundancy and thereby optimize channel bandwidth utilization. Our
approach employs Bayesian modeling and the information bottleneck method to
disentangle the shared and domain-specific information within the E-I inputs.
This disentangled information bottleneck framework ensures both the compactness
and informativeness of extracted shared and domain-specific information.
Moreover, it adaptively allocates transmission bandwidth based on scene
dynamics, i.e., more symbols are allocated to events for dynamic details or to
images for static information. Simulation results demonstrate that the proposed
scheme not only achieves superior reconstruction quality compared to
conventional systems but also delivers enhanced deblurring performance.

</details>


### [47] [Recognizing Surgical Phases Anywhere: Few-Shot Test-time Adaptation and Task-graph Guided Refinement](https://arxiv.org/abs/2506.20254)
*Kun Yuan,Tingxuan Chen,Shi Li,Joel L. Lavanchy,Christian Heiliger,Ege Özsoy,Yiming Huang,Long Bai,Nassir Navab,Vinkle Srivastav,Hongliang Ren,Nicolas Padoy*

Main category: cs.CV

TL;DR: SPA is a lightweight framework for adapting surgical foundation models to institutional settings with minimal annotation, achieving state-of-the-art performance in few-shot surgical phase recognition.


<details>
  <summary>Details</summary>
Motivation: The challenge of developing generalizable models for surgical workflow understanding due to heterogeneous operating room settings and domain shifts.

Method: SPA uses few-shot spatial adaptation, diffusion modeling for temporal consistency, and dynamic test-time adaptation to align multi-modal embeddings with institution-specific scenes.

Result: SPA outperforms full-shot models with 32-shot labeled data, achieving state-of-the-art performance in cross-institutional and cross-procedural settings.

Conclusion: SPA provides a versatile and efficient solution for adapting surgical phase recognition models with minimal annotation, enhancing reliability under domain shifts.

Abstract: The complexity and diversity of surgical workflows, driven by heterogeneous
operating room settings, institutional protocols, and anatomical variability,
present a significant challenge in developing generalizable models for
cross-institutional and cross-procedural surgical understanding. While recent
surgical foundation models pretrained on large-scale vision-language data offer
promising transferability, their zero-shot performance remains constrained by
domain shifts, limiting their utility in unseen surgical environments. To
address this, we introduce Surgical Phase Anywhere (SPA), a lightweight
framework for versatile surgical workflow understanding that adapts foundation
models to institutional settings with minimal annotation. SPA leverages
few-shot spatial adaptation to align multi-modal embeddings with
institution-specific surgical scenes and phases. It also ensures temporal
consistency through diffusion modeling, which encodes task-graph priors derived
from institutional procedure protocols. Finally, SPA employs dynamic test-time
adaptation, exploiting the mutual agreement between multi-modal phase
prediction streams to adapt the model to a given test video in a
self-supervised manner, enhancing the reliability under test-time distribution
shifts. SPA is a lightweight adaptation framework, allowing hospitals to
rapidly customize phase recognition models by defining phases in natural
language text, annotating a few images with the phase labels, and providing a
task graph defining phase transitions. The experimental results show that the
SPA framework achieves state-of-the-art performance in few-shot surgical phase
recognition across multiple institutions and procedures, even outperforming
full-shot models with 32-shot labeled data. Code is available at
https://github.com/CAMMA-public/SPA

</details>


### [48] [A Transformer Based Handwriting Recognition System Jointly Using Online and Offline Features](https://arxiv.org/abs/2506.20255)
*Ayush Lodh,Ritabrata Chakraborty,Shivakumara Palaiahnakote,Umapada Pal*

Main category: cs.CV

TL;DR: An end-to-end network fuses offline images and online stroke data for handwriting recognition, achieving state-of-the-art accuracy with 1% improvement.


<details>
  <summary>Details</summary>
Motivation: Handwriting recognition systems typically use only one modality (glyph or trajectory), missing complementary cues. Combining both could improve accuracy.

Method: Early fusion of offline images and online stroke data in a shared latent space using a patch encoder and lightweight transformer. Learnable queries attend to both modalities.

Result: Achieves state-of-the-art accuracy on IAMOn-DB and VNOn-DB, surpassing previous bests by up to 1%.

Conclusion: The fusion of glyph and trajectory data enhances representation learning, improving accuracy and writer independence.

Abstract: We posit that handwriting recognition benefits from complementary cues
carried by the rasterized complex glyph and the pen's trajectory, yet most
systems exploit only one modality. We introduce an end-to-end network that
performs early fusion of offline images and online stroke data within a shared
latent space. A patch encoder converts the grayscale crop into fixed-length
visual tokens, while a lightweight transformer embeds the $(x, y, \text{pen})$
sequence. Learnable latent queries attend jointly to both token streams,
yielding context-enhanced stroke embeddings that are pooled and decoded under a
cross-entropy loss objective. Because integration occurs before any high-level
classification, temporal cues reinforce each other during representation
learning, producing stronger writer independence. Comprehensive experiments on
IAMOn-DB and VNOn-DB demonstrate that our approach achieves state-of-the-art
accuracy, exceeding previous bests by up to 1\%. Our study also shows
adaptation of this pipeline with gesturification on the ISI-Air dataset. Our
code can be found here.

</details>


### [49] [Hierarchical Mask-Enhanced Dual Reconstruction Network for Few-Shot Fine-Grained Image Classification](https://arxiv.org/abs/2506.20263)
*Ning Luo,Meiyin Hu,Huan Wan,Yanyan Yang,Zhuohang Jiang,Xin Wei*

Main category: cs.CV

TL;DR: HMDRN improves few-shot fine-grained image classification by combining dual-layer feature reconstruction and mask-enhanced processing, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for FS-FGIC lose spatial information, misalign features, or fail to focus on discriminative regions.

Method: HMDRN uses dual-layer feature reconstruction and fusion, along with a mask-enhanced transformer module for adaptive feature processing.

Result: HMDRN outperforms state-of-the-art methods on three datasets, validated by ablation studies and visualizations.

Conclusion: HMDRN enhances inter-class discrimination and reduces intra-class variations, proving its effectiveness for FS-FGIC.

Abstract: Few-shot fine-grained image classification (FS-FGIC) presents a significant
challenge, requiring models to distinguish visually similar subclasses with
limited labeled examples. Existing methods have critical limitations:
metric-based methods lose spatial information and misalign local features,
while reconstruction-based methods fail to utilize hierarchical feature
information and lack mechanisms to focus on discriminative regions. We propose
the Hierarchical Mask-enhanced Dual Reconstruction Network (HMDRN), which
integrates dual-layer feature reconstruction with mask-enhanced feature
processing to improve fine-grained classification. HMDRN incorporates a
dual-layer feature reconstruction and fusion module that leverages
complementary visual information from different network hierarchies. Through
learnable fusion weights, the model balances high-level semantic
representations from the last layer with mid-level structural details from the
penultimate layer. Additionally, we design a spatial binary mask-enhanced
transformer self-reconstruction module that processes query features through
adaptive thresholding while maintaining complete support features, enhancing
focus on discriminative regions while filtering background noise. Extensive
experiments on three challenging fine-grained datasets demonstrate that HMDRN
consistently outperforms state-of-the-art methods across Conv-4 and ResNet-12
backbone architectures. Comprehensive ablation studies validate the
effectiveness of each proposed component, revealing that dual-layer
reconstruction enhances inter-class discrimination while mask-enhanced
transformation reduces intra-class variations. Visualization results provide
evidence of HMDRN's superior feature reconstruction capabilities.

</details>


### [50] [Forensic Study of Paintings Through the Comparison of Fabrics](https://arxiv.org/abs/2506.20272)
*Juan José Murillo-Fuentes,Pablo M. Olmos,Laura Alba-Carcelén*

Main category: cs.CV

TL;DR: A deep learning-based method is introduced to assess textile similarity in art canvases without relying on thread density maps, validated with Prado Museum canvases.


<details>
  <summary>Details</summary>
Motivation: Traditional thread density map matching fails for non-contiguous canvases, necessitating a new approach for authentication and conservation.

Method: A Siamese deep learning model compares pairs of canvas images, and a similarity estimation method aggregates predictions for robust scoring.

Result: The method effectively compares plain weave canvases with similar thread densities, proving feasible and accurate.

Conclusion: This approach opens new possibilities for analyzing masterpieces, enhancing authentication and conservation efforts.

Abstract: The study of canvas fabrics in works of art is a crucial tool for
authentication, attribution and conservation. Traditional methods are based on
thread density map matching, which cannot be applied when canvases do not come
from contiguous positions on a roll. This paper presents a novel approach based
on deep learning to assess the similarity of textiles. We introduce an
automatic tool that evaluates the similarity between canvases without relying
on thread density maps. A Siamese deep learning model is designed and trained
to compare pairs of images by exploiting the feature representations learned
from the scans. In addition, a similarity estimation method is proposed,
aggregating predictions from multiple pairs of cloth samples to provide a
robust similarity score. Our approach is applied to canvases from the Museo
Nacional del Prado, corroborating the hypothesis that plain weave canvases,
widely used in painting, can be effectively compared even when their thread
densities are similar. The results demonstrate the feasibility and accuracy of
the proposed method, opening new avenues for the analysis of masterpieces.

</details>


### [51] [From Ideal to Real: Unified and Data-Efficient Dense Prediction for Real-World Scenarios](https://arxiv.org/abs/2506.20279)
*Changliang Xia,Chengyou Jia,Zhuohang Dang,Minnan Luo*

Main category: cs.CV

TL;DR: The paper introduces DenseWorld, a benchmark for 25 real-world dense prediction tasks, and proposes DenseDiT, a method leveraging generative models' priors for unified task performance with minimal added parameters.


<details>
  <summary>Details</summary>
Motivation: Existing dense prediction methods lack generalization to real-world scenarios due to idealized conditions and scarce real-world data.

Method: DenseDiT uses a parameter-reuse mechanism and lightweight branches for multi-scale context integration, requiring less than 0.1% additional parameters.

Result: DenseDiT outperforms baselines on DenseWorld, achieving superior results with less than 0.01% training data.

Conclusion: DenseDiT demonstrates practical value for real-world deployment, addressing generalization challenges in dense prediction tasks.

Abstract: Dense prediction tasks hold significant importance of computer vision, aiming
to learn pixel-wise annotated label for an input image. Despite advances in
this field, existing methods primarily focus on idealized conditions, with
limited generalization to real-world scenarios and facing the challenging
scarcity of real-world data. To systematically study this problem, we first
introduce DenseWorld, a benchmark spanning a broad set of 25 dense prediction
tasks that correspond to urgent real-world applications, featuring unified
evaluation across tasks. Then, we propose DenseDiT, which maximally exploits
generative models' visual priors to perform diverse real-world dense prediction
tasks through a unified strategy. DenseDiT combines a parameter-reuse mechanism
and two lightweight branches that adaptively integrate multi-scale context,
working with less than 0.1% additional parameters. Evaluations on DenseWorld
reveal significant performance drops in existing general and specialized
baselines, highlighting their limited real-world generalization. In contrast,
DenseDiT achieves superior results using less than 0.01% training data of
baselines, underscoring its practical value for real-world deployment. Our
data, and checkpoints and codes are available at
https://xcltql666.github.io/DenseDiTProj

</details>


### [52] [Breaking Spatial Boundaries: Spectral-Domain Registration Guided Hyperspectral and Multispectral Blind Fusion](https://arxiv.org/abs/2506.20293)
*Kunjing Yang,Libin Zheng,Minru Bai,Ting Lu,Leyuan Fang*

Main category: cs.CV

TL;DR: The paper proposes a spectral-domain approach for registering and fusing unregistered hyperspectral (HSI) and multispectral (MSI) images, using a lightweight Spectral Prior Learning network and blind sparse fusion to improve efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing methods for HSI-MSI fusion rely on spatial transformations, which are inefficient and perform poorly due to resolution differences. The paper aims to address these challenges by focusing on the spectral domain.

Method: A Spectral Prior Learning (SPL) network extracts spectral features and enhances MSI resolution. Subspace representation and cyclic training improve spectral accuracy. Blind sparse fusion (BSF) with group sparsity regularization reduces computational complexity, solved via Proximal Alternating Optimization (PAO).

Result: Extensive experiments confirm the method's effectiveness in registration, fusion, and classification enhancement.

Conclusion: The proposed spectral-domain approach outperforms traditional spatial methods in efficiency and accuracy for HSI-MSI fusion.

Abstract: The blind fusion of unregistered hyperspectral images (HSIs) and
multispectral images (MSIs) has attracted growing attention recently. To
address the registration challenge, most existing methods employ spatial
transformations on the HSI to achieve alignment with the MSI. However, due to
the substantial differences in spatial resolution of the images, the
performance of these methods is often unsatisfactory. Moreover, the
registration process tends to be time-consuming when dealing with large-sized
images in remote sensing. To address these issues, we propose tackling the
registration problem from the spectral domain. Initially, a lightweight
Spectral Prior Learning (SPL) network is developed to extract spectral features
from the HSI and enhance the spectral resolution of the MSI. Following this,
the obtained image undergoes spatial downsampling to produce the registered
HSI. In this process, subspace representation and cyclic training strategy are
employed to improve spectral accuracy of the registered HSI obtained. Next, we
propose a blind sparse fusion (BSF) method, which utilizes group sparsity
regularization to equivalently promote the low-rankness of the image. This
approach not only circumvents the need for rank estimation, but also reduces
computational complexity. Then, we employ the Proximal Alternating Optimization
(PAO) algorithm to solve the BSF model, and present its convergence analysis.
Finally, extensive numerical experiments on simulated and real datasets are
conducted to verify the effectiveness of our method in registration and fusion.
We also demonstrate its efficacy in enhancing classification performance.

</details>


### [53] [Ctrl-Z Sampling: Diffusion Sampling with Controlled Random Zigzag Explorations](https://arxiv.org/abs/2506.20294)
*Shunqi Mao,Wei Guo,Chaoyi Zhang,Weidong Cai*

Main category: cs.CV

TL;DR: Ctrl-Z Sampling improves diffusion model generation by dynamically escaping local optima through controlled noise injection and backtracking, enhancing alignment and visual quality.


<details>
  <summary>Details</summary>
Motivation: Diffusion models often converge to locally coherent but globally inconsistent or misaligned outputs due to latent space complexity and suboptimal initialization.

Method: Introduces Controlled Random Zigzag Sampling (Ctrl-Z Sampling), which detects local maxima using a reward model, injects noise to escape, and evaluates trajectories for improvement.

Result: Ctrl-Z Sampling improves generation quality with a ~7.6X increase in function evaluations.

Conclusion: The method is model-agnostic and enhances diffusion model outputs by balancing refinement and exploration.

Abstract: Diffusion models have shown strong performance in conditional generation by
progressively denoising Gaussian noise toward a target data distribution. This
denoising process can be interpreted as a form of hill climbing in a learned
latent space, where the model iteratively refines the sample toward regions of
higher probability. However, diffusion models often converge to local optima
that are locally visually coherent yet globally inconsistent or conditionally
misaligned, due to latent space complexity and suboptimal initialization. Prior
efforts attempted to address this by strengthening guidance signals or
manipulating the initial noise distribution. We introduce Controlled Random
Zigzag Sampling (Ctrl-Z Sampling), a novel sampling strategy designed to detect
and escape such local maxima during conditional generation. The method first
identifies potential local maxima using a reward model. Upon detection, it
injects noise and reverts to a previous, noisier state to escape the current
optimization plateau. The reward model then evaluates candidate trajectories,
accepting only those that offer improvement, while progressively deeper retreat
enables stronger escapes when nearby alternatives fail. This controlled random
zigzag process allows dynamic alternation between forward refinement and
backward exploration, enhancing both alignment and visual quality in the
generated outputs. The proposed Ctrl-Z Sampling is model-agnostic and
compatible with existing diffusion frameworks. Experimental results show that
Ctrl-Z Sampling substantially improves generation quality with only around 7.6X
increase in function evaluations.

</details>


### [54] [TDiR: Transformer based Diffusion for Image Restoration Tasks](https://arxiv.org/abs/2506.20302)
*Abbas Anwar,Mohammad Shullar,Ali Arshad Nasir,Mudassir Masood,Saeed Anwar*

Main category: cs.CV

TL;DR: A transformer-based diffusion model improves degraded image quality, outperforming existing methods in underwater enhancement, denoising, and deraining.


<details>
  <summary>Details</summary>
Motivation: Degraded images hinder downstream tasks; the paper aims to restore image quality.

Method: Uses a transformer-based diffusion model for image restoration.

Result: Outperforms existing deep learning methods in enhancing, denoising, and deraining.

Conclusion: Diffusion models with transformers effectively improve degraded images, enhancing their utility in high-fidelity tasks.

Abstract: Images captured in challenging environments often experience various forms of
degradation, including noise, color cast, blur, and light scattering. These
effects significantly reduce image quality, hindering their applicability in
downstream tasks such as object detection, mapping, and classification. Our
transformer-based diffusion model was developed to address image restoration
tasks, aiming to improve the quality of degraded images. This model was
evaluated against existing deep learning methodologies across multiple quality
metrics for underwater image enhancement, denoising, and deraining on publicly
available datasets. Our findings demonstrate that the diffusion model, combined
with transformers, surpasses current methods in performance. The results of our
model highlight the efficacy of diffusion models and transformers in improving
the quality of degraded images, consequently expanding their utility in
downstream tasks that require high-fidelity visual data.

</details>


### [55] [Radiomic fingerprints for knee MR images assessment](https://arxiv.org/abs/2506.20306)
*Yaxi Chen,Simin Ni,Shaheer U. Saeed,Aleksandra Ivanova,Rikin Hargunani,Jie Huang,Chaozong Liu,Yipeng Hu*

Main category: cs.CV

TL;DR: The paper proposes a dynamic radiomic fingerprint framework for knee MRI interpretation, improving accuracy and interpretability over traditional radiomic signatures and matching deep learning models.


<details>
  <summary>Details</summary>
Motivation: Current radiomic approaches lack individual adaptability, leading to poor generalization, while deep learning models lack interpretability. The goal is to combine the strengths of both.

Method: A deep learning model dynamically selects patient-specific radiomic features (fingerprints) from a large pool, trained alongside a low-dimensional logistic regression for classification.

Result: The method achieves comparable or superior accuracy to state-of-the-art deep learning models in diagnosing knee abnormalities, ACL tears, and meniscus tears, while maintaining interpretability.

Conclusion: The radiomic fingerprint framework enhances diagnostic accuracy and interpretability, offering clinical insights and potential biomarker discovery.

Abstract: Accurate interpretation of knee MRI scans relies on expert clinical judgment,
often with high variability and limited scalability. Existing radiomic
approaches use a fixed set of radiomic features (the signature), selected at
the population level and applied uniformly to all patients. While
interpretable, these signatures are often too constrained to represent
individual pathological variations. As a result, conventional radiomic-based
approaches are found to be limited in performance, compared with recent
end-to-end deep learning (DL) alternatives without using interpretable radiomic
features. We argue that the individual-agnostic nature in current radiomic
selection is not central to its intepretability, but is responsible for the
poor generalization in our application. Here, we propose a novel radiomic
fingerprint framework, in which a radiomic feature set (the fingerprint) is
dynamically constructed for each patient, selected by a DL model. Unlike the
existing radiomic signatures, our fingerprints are derived on a per-patient
basis by predicting the feature relevance in a large radiomic feature pool, and
selecting only those that are predictive of clinical conditions for individual
patients. The radiomic-selecting model is trained simultaneously with a
low-dimensional (considered relatively explainable) logistic regression for
downstream classification. We validate our methods across multiple diagnostic
tasks including general knee abnormalities, anterior cruciate ligament (ACL)
tears, and meniscus tears, demonstrating comparable or superior diagnostic
accuracy relative to state-of-the-art end-to-end DL models. More importantly,
we show that the interpretability inherent in our approach facilitates
meaningful clinical insights and potential biomarker discovery, with detailed
discussion, quantitative and qualitative analysis of real-world clinical cases
to evidence these advantages.

</details>


### [56] [On the Burstiness of Faces in Set](https://arxiv.org/abs/2506.20312)
*Jiong Wang*

Main category: cs.CV

TL;DR: The paper addresses burstiness in set-based face recognition (SFR), where certain faces dominate sets, harming performance. It proposes detection and mitigation strategies, improving recognition.


<details>
  <summary>Details</summary>
Motivation: Burstiness in SFR causes poor generalization and biased evaluation due to overrepresented faces. Addressing this can enhance recognition accuracy.

Method: Three strategies (Quickshift++, feature self-similarity, GMP) detect bursty faces. Quality-aware GMP is added for evaluation robustness.

Result: Experiments show burstiness is common in SFR, and suppressing it significantly boosts recognition performance.

Conclusion: Mitigating burstiness in SFR improves generalization and evaluation fairness, validated by benchmarks.

Abstract: Burstiness, a phenomenon observed in text and image retrieval, refers to that
particular elements appear more times in a set than a statistically independent
model assumes. We argue that in the context of set-based face recognition
(SFR), burstiness exists widely and degrades the performance in two aspects:
Firstly, the bursty faces, where faces with particular attributes %exist
frequently in a face set, dominate the training instances and dominate the
training face sets and lead to poor generalization ability to unconstrained
scenarios. Secondly, the bursty faces %dominating the evaluation sets interfere
with the similarity comparison in set verification and identification when
evaluation. To detect the bursty faces in a set, we propose three strategies
based on Quickshift++, feature self-similarity, and generalized max-pooling
(GMP). We apply the burst detection results on training and evaluation stages
to enhance the sampling ratios or contributions of the infrequent faces. When
evaluation, we additionally propose the quality-aware GMP that enables
awareness of the face quality and robustness to the low-quality faces for the
original GMP. We give illustrations and extensive experiments on the SFR
benchmarks to demonstrate that burstiness is widespread and suppressing
burstiness considerably improves the recognition performance.

</details>


### [57] [From Codicology to Code: A Comparative Study of Transformer and YOLO-based Detectors for Layout Analysis in Historical Documents](https://arxiv.org/abs/2506.20326)
*Sergio Torres Aguilar*

Main category: cs.CV

TL;DR: Benchmarking five object detection models on historical document datasets shows Transformer models excel in structured layouts, while CNN-OBB models outperform in complex, diverse documents.


<details>
  <summary>Details</summary>
Motivation: To evaluate the effectiveness of state-of-the-art object detection architectures for robust Document Layout Analysis (DLA) in historical documents with complex layouts.

Method: Benchmarked five models (Co-DETR, Grounding DINO, YOLO variants) on three datasets (e-NDP, CATMuS, HORAE) using different bounding box representations.

Result: Co-DETR performed best on structured layouts (e-NDP), while YOLOv11x-OBB excelled in complex datasets (CATMuS, HORAE). Oriented Bounding Boxes (OBB) proved essential for accuracy.

Conclusion: Transformers are ideal for structured layouts, while CNN-OBB models generalize better for complex documents. OBB is crucial for historical manuscript analysis.

Abstract: Robust Document Layout Analysis (DLA) is critical for the automated
processing and understanding of historical documents with complex page
organizations. This paper benchmarks five state-of-the-art object detection
architectures on three annotated datasets representing a spectrum of
codicological complexity: The e-NDP, a corpus of Parisian medieval registers
(1326-1504); CATMuS, a diverse multiclass dataset derived from various medieval
and modern sources (ca.12th-17th centuries) and HORAE, a corpus of decorated
books of hours (ca.13th-16th centuries). We evaluate two Transformer-based
models (Co-DETR, Grounding DINO) against three YOLO variants (AABB, OBB, and
YOLO-World). Our findings reveal significant performance variations dependent
on model architecture, data set characteristics, and bounding box
representation. In the e-NDP dataset, Co-DETR achieves state-of-the-art results
(0.752 mAP@.50:.95), closely followed by YOLOv11X-OBB (0.721). Conversely, on
the more complex CATMuS and HORAE datasets, the CNN-based YOLOv11x-OBB
significantly outperforms all other models (0.564 and 0.568, respectively).
This study unequivocally demonstrates that using Oriented Bounding Boxes (OBB)
is not a minor refinement but a fundamental requirement for accurately modeling
the non-Cartesian nature of historical manuscripts. We conclude that a key
trade-off exists between the global context awareness of Transformers, ideal
for structured layouts, and the superior generalization of CNN-OBB models for
visually diverse and complex documents.

</details>


### [58] [Feature Hallucination for Self-supervised Action Recognition](https://arxiv.org/abs/2506.20342)
*Lei Wang,Piotr Koniusz*

Main category: cs.CV

TL;DR: A deep translational action recognition framework enhances accuracy by predicting action concepts and auxiliary features, integrating novel descriptors (ODF and SDF) and multimodal cues, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Human action recognition requires high-level semantic reasoning and multimodal feature integration, beyond raw pixel analysis.

Method: Proposes a framework with hallucination streams for missing cues, domain-specific descriptors (ODF, SDF), and integrates auxiliary modalities (optical flow, skeleton data, etc.). Includes aleatoric uncertainty modeling and a robust loss function.

Result: Achieves state-of-the-art performance on benchmarks like Kinetics-400, Kinetics-600, and Something-Something V2.

Conclusion: The framework effectively captures fine-grained action dynamics by integrating multimodal features and novel descriptors, demonstrating superior recognition accuracy.

Abstract: Understanding human actions in videos requires more than raw pixel analysis;
it relies on high-level semantic reasoning and effective integration of
multimodal features. We propose a deep translational action recognition
framework that enhances recognition accuracy by jointly predicting action
concepts and auxiliary features from RGB video frames. At test time,
hallucination streams infer missing cues, enriching feature representations
without increasing computational overhead. To focus on action-relevant regions
beyond raw pixels, we introduce two novel domain-specific descriptors. Object
Detection Features (ODF) aggregate outputs from multiple object detectors to
capture contextual cues, while Saliency Detection Features (SDF) highlight
spatial and intensity patterns crucial for action recognition. Our framework
seamlessly integrates these descriptors with auxiliary modalities such as
optical flow, Improved Dense Trajectories, skeleton data, and audio cues. It
remains compatible with state-of-the-art architectures, including I3D,
AssembleNet, Video Transformer Network, FASTER, and recent models like VideoMAE
V2 and InternVideo2. To handle uncertainty in auxiliary features, we
incorporate aleatoric uncertainty modeling in the hallucination step and
introduce a robust loss function to mitigate feature noise. Our multimodal
self-supervised action recognition framework achieves state-of-the-art
performance on multiple benchmarks, including Kinetics-400, Kinetics-600, and
Something-Something V2, demonstrating its effectiveness in capturing
fine-grained action dynamics.

</details>


### [59] [InvZW: Invariant Feature Learning via Noise-Adversarial Training for Robust Image Zero-Watermarking](https://arxiv.org/abs/2506.20370)
*Abdullah All Tanvir,Xin Zhong*

Main category: cs.CV

TL;DR: A deep learning framework for robust zero-watermarking of images using distortion-invariant features and adversarial learning.


<details>
  <summary>Details</summary>
Motivation: To create a watermarking method that leaves the original image unaltered while ensuring robustness against distortions.

Method: Combines adversarial learning for distortion-invariant feature extraction and a learning-based multibit zero-watermarking scheme.

Result: Achieves state-of-the-art robustness in feature stability and watermark recovery across diverse datasets and distortions.

Conclusion: The framework outperforms existing methods in generalization and robustness, validated by extensive experiments.

Abstract: This paper introduces a novel deep learning framework for robust image
zero-watermarking based on distortion-invariant feature learning. As a
zero-watermarking scheme, our method leaves the original image unaltered and
learns a reference signature through optimization in the feature space. The
proposed framework consists of two key modules. In the first module, a feature
extractor is trained via noise-adversarial learning to generate representations
that are both invariant to distortions and semantically expressive. This is
achieved by combining adversarial supervision against a distortion
discriminator and a reconstruction constraint to retain image content. In the
second module, we design a learning-based multibit zero-watermarking scheme
where the trained invariant features are projected onto a set of trainable
reference codes optimized to match a target binary message. Extensive
experiments on diverse image datasets and a wide range of distortions show that
our method achieves state-of-the-art robustness in both feature stability and
watermark recovery. Comparative evaluations against existing self-supervised
and deep watermarking techniques further highlight the superiority of our
framework in generalization and robustness.

</details>


### [60] [Exploiting Lightweight Hierarchical ViT and Dynamic Framework for Efficient Visual Tracking](https://arxiv.org/abs/2506.20381)
*Ben Kang,Xin Chen,Jie Zhao,Chunjuan Bo,Dong Wang,Huchuan Lu*

Main category: cs.CV

TL;DR: HiT and DyHiT are efficient transformer-based trackers addressing speed limitations on resource-constrained devices, achieving high fps and competitive accuracy.


<details>
  <summary>Details</summary>
Motivation: Transformer-based trackers are powerful but slow on resource-constrained devices, necessitating efficient solutions.

Method: HiT uses a Bridge Module and dual-image position encoding; DyHiT employs dynamic routing for adaptive computation.

Result: HiT achieves 61 fps (64.6% AUC); DyHiT reaches 111 fps (62.4% AUC). A training-free acceleration method boosts SeqTrack-B256 by 2.68x.

Conclusion: HiT and DyHiT offer efficient, high-performance tracking, with DyHiT dynamically adapting to scene complexity for optimal speed-accuracy trade-offs.

Abstract: Transformer-based visual trackers have demonstrated significant advancements
due to their powerful modeling capabilities. However, their practicality is
limited on resource-constrained devices because of their slow processing
speeds. To address this challenge, we present HiT, a novel family of efficient
tracking models that achieve high performance while maintaining fast operation
across various devices. The core innovation of HiT lies in its Bridge Module,
which connects lightweight transformers to the tracking framework, enhancing
feature representation quality. Additionally, we introduce a dual-image
position encoding approach to effectively encode spatial information. HiT
achieves an impressive speed of 61 frames per second (fps) on the NVIDIA Jetson
AGX platform, alongside a competitive AUC of 64.6% on the LaSOT benchmark,
outperforming all previous efficient trackers.Building on HiT, we propose
DyHiT, an efficient dynamic tracker that flexibly adapts to scene complexity by
selecting routes with varying computational requirements. DyHiT uses search
area features extracted by the backbone network and inputs them into an
efficient dynamic router to classify tracking scenarios. Based on the
classification, DyHiT applies a divide-and-conquer strategy, selecting
appropriate routes to achieve a superior trade-off between accuracy and speed.
The fastest version of DyHiT achieves 111 fps on NVIDIA Jetson AGX while
maintaining an AUC of 62.4% on LaSOT.Furthermore, we introduce a training-free
acceleration method based on the dynamic routing architecture of DyHiT. This
method significantly improves the execution speed of various high-performance
trackers without sacrificing accuracy. For instance, our acceleration method
enables the state-of-the-art tracker SeqTrack-B256 to achieve a 2.68 times
speedup on an NVIDIA GeForce RTX 2080 Ti GPU while maintaining the same AUC of
69.9% on the LaSOT.

</details>


### [61] [A Novel Large Vision Foundation Model (LVFM)-based Approach for Generating High-Resolution Canopy Height Maps in Plantations for Precision Forestry Management](https://arxiv.org/abs/2506.20388)
*Shen Tan,Xin Zhang,Liangxiu Han,Huaguo Huang,Han Wang*

Main category: cs.CV

TL;DR: A novel model using a Large Vision Foundation Model (LVFM) for high-resolution canopy height maps (CHMs) was developed, outperforming existing methods with high accuracy in biomass monitoring.


<details>
  <summary>Details</summary>
Motivation: Accurate, cost-effective monitoring of plantation aboveground biomass (AGB) is needed for carbon sequestration initiatives like China's CCER program, but lidar-based methods are expensive.

Method: The model integrates a feature extractor, self-supervised feature enhancement, and a height estimator, tested with 1-meter Google Earth imagery.

Result: Achieved mean absolute error of 0.09 m, RMSE of 0.24 m, and correlation of 0.78 against lidar-based CHMs, with over 90% success in tree detection.

Conclusion: The LVFM-based approach is a scalable, promising tool for carbon sequestration evaluation in plantations and natural forests.

Abstract: Accurate, cost-effective monitoring of plantation aboveground biomass (AGB)
is crucial for supporting local livelihoods and carbon sequestration
initiatives like the China Certified Emission Reduction (CCER) program.
High-resolution canopy height maps (CHMs) are essential for this, but standard
lidar-based methods are expensive. While deep learning with RGB imagery offers
an alternative, accurately extracting canopy height features remains
challenging. To address this, we developed a novel model for high-resolution
CHM generation using a Large Vision Foundation Model (LVFM). Our model
integrates a feature extractor, a self-supervised feature enhancement module to
preserve spatial details, and a height estimator. Tested in Beijing's Fangshan
District using 1-meter Google Earth imagery, our model outperformed existing
methods, including conventional CNNs. It achieved a mean absolute error of 0.09
m, a root mean square error of 0.24 m, and a correlation of 0.78 against
lidar-based CHMs. The resulting CHMs enabled over 90% success in individual
tree detection, high accuracy in AGB estimation, and effective tracking of
plantation growth, demonstrating strong generalization to non-training areas.
This approach presents a promising, scalable tool for evaluating carbon
sequestration in both plantations and natural forests.

</details>


### [62] [Med-Art: Diffusion Transformer for 2D Medical Text-to-Image Generation](https://arxiv.org/abs/2506.20449)
*Changlu Guo,Anders Nymark Christensen,Morten Rieger Hannemose*

Main category: cs.CV

TL;DR: Med-Art is a framework for medical image generation using limited data, leveraging vision-language models and fine-tuning methods to achieve high performance.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges like small dataset sizes and scarcity of medical textual data in text-to-image generative models for medical applications.

Method: Uses vision-language models for visual descriptions, adapts PixArt-α (DiT-based), and introduces Hybrid-Level Diffusion Fine-tuning (HLDF) for pixel-level losses.

Result: State-of-the-art performance on two medical datasets, measured by FID, KID, and downstream classification.

Conclusion: Med-Art effectively overcomes data limitations and improves medical image generation quality.

Abstract: Text-to-image generative models have achieved remarkable breakthroughs in
recent years. However, their application in medical image generation still
faces significant challenges, including small dataset sizes, and scarcity of
medical textual data. To address these challenges, we propose Med-Art, a
framework specifically designed for medical image generation with limited data.
Med-Art leverages vision-language models to generate visual descriptions of
medical images which overcomes the scarcity of applicable medical textual data.
Med-Art adapts a large-scale pre-trained text-to-image model, PixArt-$\alpha$,
based on the Diffusion Transformer (DiT), achieving high performance under
limited data. Furthermore, we propose an innovative Hybrid-Level Diffusion
Fine-tuning (HLDF) method, which enables pixel-level losses, effectively
addressing issues such as overly saturated colors. We achieve state-of-the-art
performance on two medical image datasets, measured by FID, KID, and downstream
classification performance.

</details>


### [63] [HiWave: Training-Free High-Resolution Image Generation via Wavelet-Based Diffusion Sampling](https://arxiv.org/abs/2506.20452)
*Tobias Vontobel,Seyedmorteza Sadat,Farnood Salehi,Romann M. Weber*

Main category: cs.CV

TL;DR: HiWave is a training-free, zero-shot method for ultra-high-resolution image synthesis using pretrained diffusion models, improving visual fidelity and coherence without retraining.


<details>
  <summary>Details</summary>
Motivation: High-resolution image synthesis with diffusion models often suffers from artifacts like object duplication and spatial incoherence, making it computationally prohibitive.

Method: HiWave uses a two-stage pipeline: base image generation, patch-wise DDIM inversion, and a wavelet-based detail enhancer to preserve global coherence and enrich fine details.

Result: HiWave outperforms state-of-the-art methods, reducing artifacts and achieving superior perceptual quality, preferred in over 80% of user comparisons.

Conclusion: HiWave offers an effective solution for high-quality, ultra-high-resolution image synthesis without retraining or architectural changes.

Abstract: Diffusion models have emerged as the leading approach for image synthesis,
demonstrating exceptional photorealism and diversity. However, training
diffusion models at high resolutions remains computationally prohibitive, and
existing zero-shot generation techniques for synthesizing images beyond
training resolutions often produce artifacts, including object duplication and
spatial incoherence. In this paper, we introduce HiWave, a training-free,
zero-shot approach that substantially enhances visual fidelity and structural
coherence in ultra-high-resolution image synthesis using pretrained diffusion
models. Our method employs a two-stage pipeline: generating a base image from
the pretrained model followed by a patch-wise DDIM inversion step and a novel
wavelet-based detail enhancer module. Specifically, we first utilize inversion
methods to derive initial noise vectors that preserve global coherence from the
base image. Subsequently, during sampling, our wavelet-domain detail enhancer
retains low-frequency components from the base image to ensure structural
consistency, while selectively guiding high-frequency components to enrich fine
details and textures. Extensive evaluations using Stable Diffusion XL
demonstrate that HiWave effectively mitigates common visual artifacts seen in
prior methods, achieving superior perceptual quality. A user study confirmed
HiWave's performance, where it was preferred over the state-of-the-art
alternative in more than 80% of comparisons, highlighting its effectiveness for
high-quality, ultra-high-resolution image synthesis without requiring
retraining or architectural modifications.

</details>


### [64] [A Deep Learning Approach to Identify Rock Bolts in Complex 3D Point Clouds of Underground Mines Captured Using Mobile Laser Scanners](https://arxiv.org/abs/2506.20464)
*Dibyayan Patra,Pasindu Ranasinghe,Bikram Banerjee,Simit Raval*

Main category: cs.CV

TL;DR: The paper proposes DeepBolt, a two-stage deep learning method for automated rock bolt detection in 3D point clouds, outperforming existing techniques in precision and recall.


<details>
  <summary>Details</summary>
Motivation: Manual rock bolt surveying in underground mines is challenging due to low light and time constraints, necessitating automated solutions. Existing methods lack robustness against noise and complex environments.

Method: DeepBolt, a novel two-stage deep learning architecture, addresses class imbalance and efficiently identifies rock bolts in noisy, large-scale 3D point clouds.

Result: DeepBolt achieves 96.41% precision and 96.96% recall, surpassing state-of-the-art models by up to 42.5% in IoU for rock bolt points.

Conclusion: DeepBolt is robust and effective for rock bolt identification in complex underground environments, offering significant improvements over existing methods.

Abstract: Rock bolts are crucial components of the subterranean support systems in
underground mines that provide adequate structural reinforcement to the rock
mass to prevent unforeseen hazards like rockfalls. This makes frequent
assessments of such bolts critical for maintaining rock mass stability and
minimising risks in underground mining operations. Where manual surveying of
rock bolts is challenging due to the low light conditions in the underground
mines and the time-intensive nature of the process, automated detection of rock
bolts serves as a plausible solution. To that end, this study focuses on the
automatic identification of rock bolts within medium to large-scale 3D point
clouds obtained from underground mines using mobile laser scanners. Existing
techniques for automated rock bolt identification primarily rely on feature
engineering and traditional machine learning approaches. However, such
techniques lack robustness as these point clouds present several challenges due
to data noise, varying environments, and complex surrounding structures.
Moreover, the target rock bolts are extremely small objects within large-scale
point clouds and are often partially obscured due to the application of
reinforcement shotcrete. Addressing these challenges, this paper proposes an
approach termed DeepBolt, which employs a novel two-stage deep learning
architecture specifically designed for handling severe class imbalance for the
automatic and efficient identification of rock bolts in complex 3D point
clouds. The proposed method surpasses state-of-the-art semantic segmentation
models by up to 42.5% in Intersection over Union (IoU) for rock bolt points.
Additionally, it outperforms existing rock bolt identification techniques,
achieving a 96.41% precision and 96.96% recall in classifying rock bolts,
demonstrating its robustness and effectiveness in complex underground
environments.

</details>


### [65] [AI-assisted radiographic analysis in detecting alveolar bone-loss severity and patterns](https://arxiv.org/abs/2506.20522)
*Chathura Wimalasiri,Piumal Rathnayake,Shamod Wijerathne,Sumudu Rasnayaka,Dhanushka Leuke Bandara,Roshan Ragel,Vajira Thambawita,Isuru Nawinne*

Main category: cs.CV

TL;DR: An AI-based deep learning framework is proposed to automatically detect and quantify alveolar bone loss in periodontitis using IOPA radiographs, achieving high accuracy in severity and pattern classification.


<details>
  <summary>Details</summary>
Motivation: Accurate assessment of bone loss severity and pattern is crucial for diagnosing and treating periodontitis, which impacts oral health and quality of life. Current methods rely on subjective manual evaluation.

Method: Combines YOLOv8 for tooth detection, Keypoint R-CNN for anatomical landmarks, and YOLOv8x-seg for bone level segmentation. Geometric analysis determines bone loss patterns (horizontal vs. angular).

Result: Achieved high accuracy in bone loss severity (ICC up to 0.80) and pattern classification (87% accuracy) on a dataset of 1000 radiographs.

Conclusion: The automated system provides a rapid, objective, and reproducible tool for periodontal assessment, improving early diagnosis and personalized treatment planning.

Abstract: Periodontitis, a chronic inflammatory disease causing alveolar bone loss,
significantly affects oral health and quality of life. Accurate assessment of
bone loss severity and pattern is critical for diagnosis and treatment
planning. In this study, we propose a novel AI-based deep learning framework to
automatically detect and quantify alveolar bone loss and its patterns using
intraoral periapical (IOPA) radiographs. Our method combines YOLOv8 for tooth
detection with Keypoint R-CNN models to identify anatomical landmarks, enabling
precise calculation of bone loss severity. Additionally, YOLOv8x-seg models
segment bone levels and tooth masks to determine bone loss patterns (horizontal
vs. angular) via geometric analysis. Evaluated on a large, expertly annotated
dataset of 1000 radiographs, our approach achieved high accuracy in detecting
bone loss severity (intra-class correlation coefficient up to 0.80) and bone
loss pattern classification (accuracy 87%). This automated system offers a
rapid, objective, and reproducible tool for periodontal assessment, reducing
reliance on subjective manual evaluation. By integrating AI into dental
radiographic analysis, our framework has the potential to improve early
diagnosis and personalized treatment planning for periodontitis, ultimately
enhancing patient care and clinical outcomes.

</details>


### [66] [Pay Less Attention to Deceptive Artifacts: Robust Detection of Compressed Deepfakes on Online Social Networks](https://arxiv.org/abs/2506.20548)
*Manyi Li,Renshuai Tao,Yufan Liu,Chuangchuang Tan,Haotong Qin,Bing Li,Yunchao Wei,Yao Zhao*

Main category: cs.CV

TL;DR: PLADA is a novel framework for deepfake detection that addresses block effects from compression in OSNs, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Deepfake detection methods ignore block effects from compression in OSNs, focusing on raw images, which are rare in real-world scenarios.

Method: PLADA includes Block Effect Eraser (B2E) for handling compression artifacts and Open Data Aggregation (ODA) for processing paired and unpaired data.

Result: PLADA outperforms state-of-the-art methods across 26 datasets, even with limited paired data and compression.

Conclusion: PLADA introduces block effects as a key factor in deepfake detection, offering a robust solution for open-world scenarios.

Abstract: With the rapid advancement of deep learning, particularly through generative
adversarial networks (GANs) and diffusion models (DMs), AI-generated images, or
``deepfakes", have become nearly indistinguishable from real ones. These images
are widely shared across Online Social Networks (OSNs), raising concerns about
their misuse. Existing deepfake detection methods overlook the ``block effects"
introduced by compression in OSNs, which obscure deepfake artifacts, and
primarily focus on raw images, rarely encountered in real-world scenarios. To
address these challenges, we propose PLADA (Pay Less Attention to Deceptive
Artifacts), a novel framework designed to tackle the lack of paired data and
the ineffective use of compressed images. PLADA consists of two core modules:
Block Effect Eraser (B2E), which uses a dual-stage attention mechanism to
handle block effects, and Open Data Aggregation (ODA), which processes both
paired and unpaired data to improve detection. Extensive experiments across 26
datasets demonstrate that PLADA achieves a remarkable balance in deepfake
detection, outperforming SoTA methods in detecting deepfakes on OSNs, even with
limited paired data and compression. More importantly, this work introduces the
``block effect" as a critical factor in deepfake detection, providing a robust
solution for open-world scenarios. Our code is available at
https://github.com/ManyiLee/PLADA.

</details>


### [67] [Lightweight Multi-Frame Integration for Robust YOLO Object Detection in Videos](https://arxiv.org/abs/2506.20550)
*Yitong Quan,Benjamin Kiefer,Martin Messmer,Andreas Zell*

Main category: cs.CV

TL;DR: A simple method improves video object detection by stacking consecutive frames in YOLO-based models, enhancing robustness without heavy computational costs.


<details>
  <summary>Details</summary>
Motivation: Single-frame detection ignores temporal context, while existing video methods are complex. Transient challenges like blur and occlusions degrade performance.

Method: Stack multiple consecutive frames as input to YOLO, supervising only the target frame's output.

Result: Improves detection robustness, especially for lightweight models, and narrows the gap between compact and heavy networks.

Conclusion: The method effectively leverages temporal data with minimal changes, validated on MOT20Det and BOAT360 datasets, which includes a new fisheye video benchmark.

Abstract: Modern image-based object detection models, such as YOLOv7, primarily process
individual frames independently, thus ignoring valuable temporal context
naturally present in videos. Meanwhile, existing video-based detection methods
often introduce complex temporal modules, significantly increasing model size
and computational complexity. In practical applications such as surveillance
and autonomous driving, transient challenges including motion blur, occlusions,
and abrupt appearance changes can severely degrade single-frame detection
performance. To address these issues, we propose a straightforward yet highly
effective strategy: stacking multiple consecutive frames as input to a
YOLO-based detector while supervising only the output corresponding to a single
target frame. This approach leverages temporal information with minimal
modifications to existing architectures, preserving simplicity, computational
efficiency, and real-time inference capability. Extensive experiments on the
challenging MOT20Det and our BOAT360 datasets demonstrate that our method
improves detection robustness, especially for lightweight models, effectively
narrowing the gap between compact and heavy detection networks. Additionally,
we contribute the BOAT360 benchmark dataset, comprising annotated fisheye video
sequences captured from a boat, to support future research in multi-frame video
object detection in challenging real-world scenarios.

</details>


### [68] [AdvMIM: Adversarial Masked Image Modeling for Semi-Supervised Medical Image Segmentation](https://arxiv.org/abs/2506.20563)
*Lei Zhu,Jun Zhou,Rick Siow Mong Goh,Yong Liu*

Main category: cs.CV

TL;DR: The paper proposes an adversarial masked image modeling method to enhance semi-supervised medical image segmentation using Vision Transformers, addressing the challenge of limited labeled data.


<details>
  <summary>Details</summary>
Motivation: Transformers require large labeled datasets, which is a limitation in semi-supervised learning scenarios with scarce annotations. Existing methods struggle to train transformers effectively with limited labeled data.

Method: The authors introduce adversarial masked image modeling, creating an auxiliary masked domain to increase supervision signals. They use original labels and pseudo-labels, along with a novel adversarial training loss to bridge the domain gap.

Result: Extensive experiments on three datasets show the method outperforms existing approaches significantly.

Conclusion: The proposed method effectively leverages transformers in semi-supervised learning, improving segmentation performance with limited labeled data.

Abstract: Vision Transformer has recently gained tremendous popularity in medical image
segmentation task due to its superior capability in capturing long-range
dependencies. However, transformer requires a large amount of labeled data to
be effective, which hinders its applicability in annotation scarce
semi-supervised learning scenario where only limited labeled data is available.
State-of-the-art semi-supervised learning methods propose combinatorial
CNN-Transformer learning to cross teach a transformer with a convolutional
neural network, which achieves promising results. However, it remains a
challenging task to effectively train the transformer with limited labeled
data. In this paper, we propose an adversarial masked image modeling method to
fully unleash the potential of transformer for semi-supervised medical image
segmentation. The key challenge in semi-supervised learning with transformer
lies in the lack of sufficient supervision signal. To this end, we propose to
construct an auxiliary masked domain from original domain with masked image
modeling and train the transformer to predict the entire segmentation mask with
masked inputs to increase supervision signal. We leverage the original labels
from labeled data and pseudo-labels from unlabeled data to learn the masked
domain. To further benefit the original domain from masked domain, we provide a
theoretical analysis of our method from a multi-domain learning perspective and
devise a novel adversarial training loss to reduce the domain gap between the
original and masked domain, which boosts semi-supervised learning performance.
We also extend adversarial masked image modeling to CNN network. Extensive
experiments on three public medical image segmentation datasets demonstrate the
effectiveness of our method, where our method outperforms existing methods
significantly. Our code is publicly available at
https://github.com/zlheui/AdvMIM.

</details>


### [69] [Show, Tell and Summarize: Dense Video Captioning Using Visual Cue Aided Sentence Summarization](https://arxiv.org/abs/2506.20567)
*Zhiwang Zhang,Dong Xu,Wanli Ouyang,Chuanqi Tan*

Main category: cs.CV

TL;DR: Proposes a division-and-summarization (DaS) framework for dense video captioning, using event proposals and a two-stage LSTM with hierarchical attention to generate descriptive sentences.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of dense video captioning by leveraging visual features and semantic descriptions from segmented videos.

Method: Partitions videos into event proposals, extracts visual features, uses existing captioning for segments, and applies a two-stage LSTM with hierarchical attention for summarization.

Result: Demonstrates effectiveness on the ActivityNet Captions dataset.

Conclusion: The DaS framework effectively combines visual and semantic information for dense video captioning.

Abstract: In this work, we propose a division-and-summarization (DaS) framework for
dense video captioning. After partitioning each untrimmed long video as
multiple event proposals, where each event proposal consists of a set of short
video segments, we extract visual feature (e.g., C3D feature) from each segment
and use the existing image/video captioning approach to generate one sentence
description for this segment. Considering that the generated sentences contain
rich semantic descriptions about the whole event proposal, we formulate the
dense video captioning task as a visual cue aided sentence summarization
problem and propose a new two stage Long Short Term Memory (LSTM) approach
equipped with a new hierarchical attention mechanism to summarize all generated
sentences as one descriptive sentence with the aid of visual features.
Specifically, the first-stage LSTM network takes all semantic words from the
generated sentences and the visual features from all segments within one event
proposal as the input, and acts as the encoder to effectively summarize both
semantic and visual information related to this event proposal. The
second-stage LSTM network takes the output from the first-stage LSTM network
and the visual features from all video segments within one event proposal as
the input, and acts as the decoder to generate one descriptive sentence for
this event proposal. Our comprehensive experiments on the ActivityNet Captions
dataset demonstrate the effectiveness of our newly proposed DaS framework for
dense video captioning.

</details>


### [70] [Causal Representation Learning with Observational Grouping for CXR Classification](https://arxiv.org/abs/2506.20582)
*Rajat Rasal,Avinash Kori,Ben Glocker*

Main category: cs.CV

TL;DR: The paper introduces a method for learning identifiable causal representations in medical imaging to improve generalisability and robustness in disease classification tasks.


<details>
  <summary>Details</summary>
Motivation: To uncover true causal relationships in data generation processes, particularly in medical imaging, to enhance the generalisability and robustness of latent features for disease classification.

Method: An end-to-end framework that groups observations to learn identifiable representations, enforcing invariance with respect to race, sex, and imaging views.

Result: Experiments show that causal representations improve generalisability and robustness across multiple classification tasks.

Conclusion: Grouping observations to learn identifiable causal representations enhances performance in disease classification tasks by improving invariance and robustness.

Abstract: Identifiable causal representation learning seeks to uncover the true causal
relationships underlying a data generation process. In medical imaging, this
presents opportunities to improve the generalisability and robustness of
task-specific latent features. This work introduces the concept of grouping
observations to learn identifiable representations for disease classification
in chest X-rays via an end-to-end framework. Our experiments demonstrate that
these causal representations improve generalisability and robustness across
multiple classification tasks when grouping is used to enforce invariance w.r.t
race, sex, and imaging views.

</details>


### [71] [Dense Video Captioning using Graph-based Sentence Summarization](https://arxiv.org/abs/2506.20583)
*Zhiwang Zhang,Dong Xu,Wanli Ouyang,Luping Zhou*

Main category: cs.CV

TL;DR: The paper proposes a graph-based partition-and-summarization (GPaS) framework for dense video captioning, addressing scene evolution within events by splitting proposals into segments and summarizing captions.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to adequately explore scene evolution in long event proposals, leading to suboptimal performance when scenes and objects change.

Method: The GPaS framework splits event proposals into segments for finer captioning and summarizes these into one sentence using a GCN-LSTM interaction module.

Result: The approach outperforms state-of-the-art methods on ActivityNet Captions and YouCook II datasets.

Conclusion: The GPaS framework effectively addresses scene evolution in dense video captioning, improving performance through segment partitioning and semantic summarization.

Abstract: Recently, dense video captioning has made attractive progress in detecting
and captioning all events in a long untrimmed video. Despite promising results
were achieved, most existing methods do not sufficiently explore the scene
evolution within an event temporal proposal for captioning, and therefore
perform less satisfactorily when the scenes and objects change over a
relatively long proposal. To address this problem, we propose a graph-based
partition-and-summarization (GPaS) framework for dense video captioning within
two stages. For the ``partition" stage, a whole event proposal is split into
short video segments for captioning at a finer level. For the ``summarization"
stage, the generated sentences carrying rich description information for each
segment are summarized into one sentence to describe the whole event. We
particularly focus on the ``summarization" stage, and propose a framework that
effectively exploits the relationship between semantic words for summarization.
We achieve this goal by treating semantic words as nodes in a graph and
learning their interactions by coupling Graph Convolutional Network (GCN) and
Long Short Term Memory (LSTM), with the aid of visual cues. Two schemes of
GCN-LSTM Interaction (GLI) modules are proposed for seamless integration of GCN
and LSTM. The effectiveness of our approach is demonstrated via an extensive
comparison with the state-of-the-arts methods on the two benchmarks ActivityNet
Captions dataset and YouCook II dataset.

</details>


### [72] [Learning-Based Distance Estimation for 360° Single-Sensor Setups](https://arxiv.org/abs/2506.20586)
*Yitong Quan,Benjamin Kiefer,Martin Messmer,Andreas Zell*

Main category: cs.CV

TL;DR: A neural network-based method for monocular distance estimation using a 360° fisheye lens camera outperforms traditional geometric techniques, offering robustness and adaptability.


<details>
  <summary>Details</summary>
Motivation: Traditional geometric methods struggle with lens distortions and environmental variability in omnidirectional imaging, necessitating a more robust solution.

Method: A learning-based approach using neural networks to estimate distances directly from raw omnidirectional inputs, bypassing the need for precise lens calibration.

Result: The proposed model outperforms traditional geometry-based methods and other learning baselines in accuracy and robustness across three diverse datasets.

Conclusion: Deep learning shows promise for real-time omnidirectional distance estimation, making it suitable for low-cost robotics, autonomous navigation, and surveillance applications.

Abstract: Accurate distance estimation is a fundamental challenge in robotic
perception, particularly in omnidirectional imaging, where traditional
geometric methods struggle with lens distortions and environmental variability.
In this work, we propose a neural network-based approach for monocular distance
estimation using a single 360{\deg} fisheye lens camera. Unlike classical
trigonometric techniques that rely on precise lens calibration, our method
directly learns and infers the distance of objects from raw omnidirectional
inputs, offering greater robustness and adaptability across diverse conditions.
We evaluate our approach on three 360{\deg} datasets (LOAF, ULM360, and a newly
captured dataset Boat360), each representing distinct environmental and sensor
setups. Our experimental results demonstrate that the proposed learning-based
model outperforms traditional geometry-based methods and other learning
baselines in both accuracy and robustness. These findings highlight the
potential of deep learning for real-time omnidirectional distance estimation,
making our approach particularly well-suited for low-cost applications in
robotics, autonomous navigation, and surveillance.

</details>


### [73] [TRIM: A Self-Supervised Video Summarization Framework Maximizing Temporal Relative Information and Representativeness](https://arxiv.org/abs/2506.20588)
*Pritam Mishra,Coloma Ballester,Dimosthenis Karatzas*

Main category: cs.CV

TL;DR: A self-supervised video summarization model is introduced, avoiding costly attention-based methods and achieving state-of-the-art performance without annotations.


<details>
  <summary>Details</summary>
Motivation: Address the inefficiency and brittleness of current video summarization methods that rely on supervised annotations or complex architectures.

Method: Uses a self-supervised learning paradigm with Markov process-driven loss metrics, avoiding attention, RNNs, or transformers.

Result: Outperforms unsupervised methods on SUMME and TVSUM datasets and rivals supervised models.

Conclusion: Demonstrates the viability of efficient, annotation-free video summarization, challenging reliance on complex architectures.

Abstract: The increasing ubiquity of video content and the corresponding demand for
efficient access to meaningful information have elevated video summarization
and video highlights as a vital research area. However, many state-of-the-art
methods depend heavily either on supervised annotations or on attention-based
models, which are computationally expensive and brittle in the face of
distribution shifts that hinder cross-domain applicability across datasets. We
introduce a pioneering self-supervised video summarization model that captures
both spatial and temporal dependencies without the overhead of attention, RNNs,
or transformers. Our framework integrates a novel set of Markov process-driven
loss metrics and a two-stage self supervised learning paradigm that ensures
both performance and efficiency. Our approach achieves state-of-the-art
performance on the SUMME and TVSUM datasets, outperforming all existing
unsupervised methods. It also rivals the best supervised models, demonstrating
the potential for efficient, annotation-free architectures. This paves the way
for more generalizable video summarization techniques and challenges the
prevailing reliance on complex architectures.

</details>


### [74] [WonderFree: Enhancing Novel View Quality and Cross-View Consistency for 3D Scene Exploration](https://arxiv.org/abs/2506.20590)
*Chaojun Ni,Jie Li,Haoyun Li,Hengyu Liu,Xiaofeng Wang,Zheng Zhu,Guosheng Zhao,Boyuan Wang,Chenxin Li,Guan Huang,Wenjun Mei*

Main category: cs.CV

TL;DR: WonderFree is a model for interactive 3D scene generation from a single image, addressing challenges in novel view quality and cross-view consistency with WorldRestorer and ConsistView.


<details>
  <summary>Details</summary>
Motivation: Current 3D generation methods lack explorability and struggle with rendering quality in unseen areas.

Method: Decouples the problem into novel view quality (using WorldRestorer) and cross-view consistency (using ConsistView).

Result: Improves rendering quality and consistency, with a 77.20% user preference over WonderWorld.

Conclusion: WonderFree enables seamless, immersive 3D exploration with publicly available code, model, and data.

Abstract: Interactive 3D scene generation from a single image has gained significant
attention due to its potential to create immersive virtual worlds. However, a
key challenge in current 3D generation methods is the limited explorability,
which cannot render high-quality images during larger maneuvers beyond the
original viewpoint, particularly when attempting to move forward into unseen
areas. To address this challenge, we propose WonderFree, the first model that
enables users to interactively generate 3D worlds with the freedom to explore
from arbitrary angles and directions. Specifically, we decouple this challenge
into two key subproblems: novel view quality, which addresses visual artifacts
and floating issues in novel views, and cross-view consistency, which ensures
spatial consistency across different viewpoints. To enhance rendering quality
in novel views, we introduce WorldRestorer, a data-driven video restoration
model designed to eliminate floaters and artifacts. In addition, a data
collection pipeline is presented to automatically gather training data for
WorldRestorer, ensuring it can handle scenes with varying styles needed for 3D
scene generation. Furthermore, to improve cross-view consistency, we propose
ConsistView, a multi-view joint restoration mechanism that simultaneously
restores multiple perspectives while maintaining spatiotemporal coherence.
Experimental results demonstrate that WonderFree not only enhances rendering
quality across diverse viewpoints but also significantly improves global
coherence and consistency. These improvements are confirmed by CLIP-based
metrics and a user study showing a 77.20% preference for WonderFree over
WonderWorld enabling a seamless and immersive 3D exploration experience. The
code, model, and data will be publicly available.

</details>


### [75] [SFNet: Fusion of Spatial and Frequency-Domain Features for Remote Sensing Image Forgery Detection](https://arxiv.org/abs/2506.20599)
*Ji Qi,Xinchang Zhang,Dingqi Ye,Yongjia Ruan,Xin Guo,Shaowen Wang,Haifeng Li*

Main category: cs.CV

TL;DR: SFNet, a novel forgery detection framework, leverages spatial and frequency domain features to detect fake remote sensing imagery (RSI), outperforming existing methods by 4%-15.18% in accuracy.


<details>
  <summary>Details</summary>
Motivation: The rise of generative AI creates fake RSI that existing single-feature detection methods struggle to identify due to diverse artifacts and evolving generative models.

Method: SFNet uses two feature extractors for spatial and frequency domains, aligns and fuses these features with a domain feature mapping module and CBAM attention, suppressing redundancy.

Result: SFNet improves accuracy by 4%-15.18% over state-of-the-art methods on three datasets and shows robust generalization.

Conclusion: SFNet effectively addresses the limitations of single-feature detection by combining multi-domain features, enhancing forgery detection in diverse RSI.

Abstract: The rapid advancement of generative artificial intelligence is producing fake
remote sensing imagery (RSI) that is increasingly difficult to detect,
potentially leading to erroneous intelligence, fake news, and even conspiracy
theories. Existing forgery detection methods typically rely on single visual
features to capture predefined artifacts, such as spatial-domain cues to detect
forged objects like roads or buildings in RSI, or frequency-domain features to
identify artifacts from up-sampling operations in adversarial generative
networks (GANs). However, the nature of artifacts can significantly differ
depending on geographic terrain, land cover types, or specific features within
the RSI. Moreover, these complex artifacts evolve as generative models become
more sophisticated. In short, over-reliance on a single visual cue makes
existing forgery detectors struggle to generalize across diverse remote sensing
data. This paper proposed a novel forgery detection framework called SFNet,
designed to identify fake images in diverse remote sensing data by leveraging
spatial and frequency domain features. Specifically, to obtain rich and
comprehensive visual information, SFNet employs two independent feature
extractors to capture spatial and frequency domain features from input RSIs. To
fully utilize the complementary domain features, the domain feature mapping
module and the hybrid domain feature refinement module(CBAM attention) of SFNet
are designed to successively align and fuse the multi-domain features while
suppressing redundant information. Experiments on three datasets show that
SFNet achieves an accuracy improvement of 4%-15.18% over the state-of-the-art
RS forgery detection methods and exhibits robust generalization capabilities.
The code is available at https://github.com/GeoX-Lab/RSTI/tree/main/SFNet.

</details>


### [76] [Video Perception Models for 3D Scene Synthesis](https://arxiv.org/abs/2506.20601)
*Rui Huang,Guangyao Zhai,Zuria Bauer,Marc Pollefeys,Federico Tombari,Leonidas Guibas,Gao Huang,Francis Engelmann*

Main category: cs.CV

TL;DR: VIPScene leverages video generation models for coherent 3D scene synthesis, outperforming existing methods with high realism and consistency.


<details>
  <summary>Details</summary>
Motivation: Automating 3D scene synthesis to benefit fields like architecture and gaming, addressing limitations of LLMs and image-based methods in spatial reasoning and multi-view consistency.

Method: VIPScene integrates video generation, 3D reconstruction, and open-vocabulary perception models to analyze scenes semantically and geometrically. Introduces FPVScore for evaluation.

Result: VIPScene outperforms existing methods, generalizing well across diverse scenarios with high realism and structural consistency.

Conclusion: VIPScene presents a novel, effective framework for 3D scene synthesis, addressing key limitations of current approaches.

Abstract: Traditionally, 3D scene synthesis requires expert knowledge and significant
manual effort. Automating this process could greatly benefit fields such as
architectural design, robotics simulation, virtual reality, and gaming. Recent
approaches to 3D scene synthesis often rely on the commonsense reasoning of
large language models (LLMs) or strong visual priors of modern image generation
models. However, current LLMs demonstrate limited 3D spatial reasoning ability,
which restricts their ability to generate realistic and coherent 3D scenes.
Meanwhile, image generation-based methods often suffer from constraints in
viewpoint selection and multi-view inconsistencies. In this work, we present
Video Perception models for 3D Scene synthesis (VIPScene), a novel framework
that exploits the encoded commonsense knowledge of the 3D physical world in
video generation models to ensure coherent scene layouts and consistent object
placements across views. VIPScene accepts both text and image prompts and
seamlessly integrates video generation, feedforward 3D reconstruction, and
open-vocabulary perception models to semantically and geometrically analyze
each object in a scene. This enables flexible scene synthesis with high realism
and structural consistency. For more precise analysis, we further introduce
First-Person View Score (FPVScore) for coherence and plausibility evaluation,
utilizing continuous first-person perspective to capitalize on the reasoning
ability of multimodal large language models. Extensive experiments show that
VIPScene significantly outperforms existing methods and generalizes well across
diverse scenarios. The code will be released.

</details>


### [77] [Shape2Animal: Creative Animal Generation from Natural Silhouettes](https://arxiv.org/abs/2506.20616)
*Quoc-Duy Tran,Anh-Tuan Vo,Dinh-Khoi Vo,Tam V. Nguyen,Minh-Triet Tran,Trung-Nghia Le*

Main category: cs.CV

TL;DR: Shape2Animal is an automated framework that mimics human pareidolia by reinterpreting natural object silhouettes as animal forms using vision-language models and text-to-image diffusion.


<details>
  <summary>Details</summary>
Motivation: To replicate the human ability to perceive meaningful patterns (pareidolia) in ambiguous stimuli like clouds or stones, enabling creative applications.

Method: The framework segments object silhouettes, interprets them as animal concepts using vision-language models, and synthesizes animal images via text-to-image diffusion, blending them into the original scene.

Result: Shape2Animal successfully generates visually coherent and spatially consistent animal interpretations from diverse real-world inputs.

Conclusion: The framework demonstrates robustness and creative potential, offering opportunities in visual storytelling, education, digital art, and interactive media.

Abstract: Humans possess a unique ability to perceive meaningful patterns in ambiguous
stimuli, a cognitive phenomenon known as pareidolia. This paper introduces
Shape2Animal framework to mimics this imaginative capacity by reinterpreting
natural object silhouettes, such as clouds, stones, or flames, as plausible
animal forms. Our automated framework first performs open-vocabulary
segmentation to extract object silhouette and interprets semantically
appropriate animal concepts using vision-language models. It then synthesizes
an animal image that conforms to the input shape, leveraging text-to-image
diffusion model and seamlessly blends it into the original scene to generate
visually coherent and spatially consistent compositions. We evaluated
Shape2Animal on a diverse set of real-world inputs, demonstrating its
robustness and creative potential. Our Shape2Animal can offer new opportunities
for visual storytelling, educational content, digital art, and interactive
media design. Our project page is here: https://shape2image.github.io

</details>


### [78] [Joint attitude estimation and 3D neural reconstruction of non-cooperative space objects](https://arxiv.org/abs/2506.20638)
*Clément Forray,Pauline Delporte,Nicolas Delaygue,Florence Genin,Dawa Derksen*

Main category: cs.CV

TL;DR: The paper explores using Neural Radiance Fields (NeRF) for 3D reconstruction of non-cooperative space objects, focusing on joint optimization of camera poses and NeRF under challenging conditions.


<details>
  <summary>Details</summary>
Motivation: Understanding space objects is crucial for applications like debris removal and anomaly detection. 3D models enhance Space Situational Awareness (SSA).

Method: Leveraged NeRF for 3D reconstruction from simulated images, addressing challenges like monochromatic images and unknown orientations. Focused on joint optimization of camera poses and NeRF.

Result: Best 3D reconstruction achieved by training with successive images one-by-one, using uniform rotation for pose estimation and regularization to limit pose divergence.

Conclusion: Joint optimization of camera poses and NeRF improves 3D reconstruction accuracy for non-cooperative space objects, advancing SSA capabilities.

Abstract: Obtaining a better knowledge of the current state and behavior of objects
orbiting Earth has proven to be essential for a range of applications such as
active debris removal, in-orbit maintenance, or anomaly detection. 3D models
represent a valuable source of information in the field of Space Situational
Awareness (SSA). In this work, we leveraged Neural Radiance Fields (NeRF) to
perform 3D reconstruction of non-cooperative space objects from simulated
images. This scenario is challenging for NeRF models due to unusual camera
characteristics and environmental conditions : mono-chromatic images, unknown
object orientation, limited viewing angles, absence of diffuse lighting etc. In
this work we focus primarly on the joint optimization of camera poses alongside
the NeRF. Our experimental results show that the most accurate 3D
reconstruction is achieved when training with successive images one-by-one. We
estimate camera poses by optimizing an uniform rotation and use regularization
to prevent successive poses from being too far apart.

</details>


### [79] [Disentangled representations of microscopy images](https://arxiv.org/abs/2506.20649)
*Jacopo Dapueto,Vito Paolo Pastore,Nicoletta Noceti,Francesca Odone*

Main category: cs.CV

TL;DR: A Disentangled Representation Learning (DRL) method is proposed to improve interpretability in microscopy image classification, balancing accuracy and interpretability across three domains.


<details>
  <summary>Details</summary>
Motivation: Interpretability is crucial for microscopy image analysis, but remains a challenge despite deep learning advancements.

Method: Uses a DRL framework, transferring representations learned from synthetic data to enhance interpretability.

Result: Demonstrates a good trade-off between accuracy and interpretability on plankton, yeast vacuoles, and human cell datasets.

Conclusion: DRL offers a promising approach for interpretable microscopy image analysis.

Abstract: Microscopy image analysis is fundamental for different applications, from
diagnosis to synthetic engineering and environmental monitoring. Modern
acquisition systems have granted the possibility to acquire an escalating
amount of images, requiring a consequent development of a large collection of
deep learning-based automatic image analysis methods. Although deep neural
networks have demonstrated great performance in this field, interpretability,
an essential requirement for microscopy image analysis, remains an open
challenge.
  This work proposes a Disentangled Representation Learning (DRL) methodology
to enhance model interpretability for microscopy image classification.
Exploiting benchmark datasets from three different microscopic image domains
(plankton, yeast vacuoles, and human cells), we show how a DRL framework, based
on transferring a representation learnt from synthetic data, can provide a good
trade-off between accuracy and interpretability in this domain.

</details>


### [80] [MMSearch-R1: Incentivizing LMMs to Search](https://arxiv.org/abs/2506.20670)
*Jinming Wu,Zihao Deng,Wei Li,Yiding Liu,Bo You,Bo Li,Zejun Ma,Ziwei Liu*

Main category: cs.CV

TL;DR: MMSearch-R1 is a reinforcement learning framework enabling large multimodal models (LMMs) to perform efficient, on-demand, multi-turn searches in real-world Internet environments, outperforming RAG-based methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods like RAG and prompt-engineered search agents are inefficient due to rigid pipelines, necessitating a more flexible and efficient approach for LMMs.

Method: The framework integrates image and text search tools, guided by an outcome-based reward with a search penalty, and uses a semi-automated pipeline to create a multimodal search VQA dataset.

Result: MMSearch-R1 outperforms same-sized RAG baselines, matches larger RAG models' performance, and reduces search calls by over 30%.

Conclusion: The framework offers a robust solution for efficient multimodal search, with empirical insights for future research.

Abstract: Robust deployment of large multimodal models (LMMs) in real-world scenarios
requires access to external knowledge sources, given the complexity and dynamic
nature of real-world information. Existing approaches such as
retrieval-augmented generation (RAG) and prompt engineered search agents rely
on rigid pipelines, often leading to inefficient or excessive search behaviors.
We present MMSearch-R1, the first end-to-end reinforcement learning framework
that enables LMMs to perform on-demand, multi-turn search in real-world
Internet environments. Our framework integrates both image and text search
tools, allowing the model to reason about when and how to invoke them guided by
an outcome-based reward with a search penalty. To support training, We collect
a multimodal search VQA dataset through a semi-automated pipeline that covers
diverse visual and textual knowledge needs and curate a search-balanced subset
with both search-required and search-free samples, which proves essential for
shaping efficient and on-demand search behavior. Extensive experiments on
knowledge-intensive and info-seeking VQA tasks show that our model not only
outperforms RAG-based baselines of the same model size, but also matches the
performance of a larger RAG-based model while reducing search calls by over
30%. We further analyze key empirical findings to offer actionable insights for
advancing research in multimodal search.

</details>


### [81] [IPFormer: Visual 3D Panoptic Scene Completion with Context-Adaptive Instance Proposals](https://arxiv.org/abs/2506.20671)
*Markus Gross,Aya Fahmy,Danit Niwattananan,Dominik Muhle,Rui Song,Daniel Cremers,Henri Meeß*

Main category: cs.CV

TL;DR: IPFormer introduces context-adaptive instance proposals for vision-based 3D Panoptic Scene Completion, outperforming state-of-the-art methods in panoptic metrics and runtime efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of static queries in Transformer-based SSC and the lack of camera-based PSC methods, enabling dynamic adaptation to observed scenes.

Method: IPFormer adaptively initializes queries as panoptic instance proposals from image context and refines them via attention-based encoding and decoding.

Result: Surpasses state-of-the-art in PQ$^\dagger$ and PQ-All, achieves 14x runtime reduction, and improves Thing-metrics by 18.65%.

Conclusion: Context-adaptive instance proposals are a pioneering solution for vision-based 3D PSC, enhancing performance and efficiency.

Abstract: Semantic Scene Completion (SSC) has emerged as a pivotal approach for jointly
learning scene geometry and semantics, enabling downstream applications such as
navigation in mobile robotics. The recent generalization to Panoptic Scene
Completion (PSC) advances the SSC domain by integrating instance-level
information, thereby enhancing object-level sensitivity in scene understanding.
While PSC was introduced using LiDAR modality, methods based on camera images
remain largely unexplored. Moreover, recent Transformer-based SSC approaches
utilize a fixed set of learned queries to reconstruct objects within the scene
volume. Although these queries are typically updated with image context during
training, they remain static at test time, limiting their ability to
dynamically adapt specifically to the observed scene. To overcome these
limitations, we propose IPFormer, the first approach that leverages
context-adaptive instance proposals at train and test time to address
vision-based 3D Panoptic Scene Completion. Specifically, IPFormer adaptively
initializes these queries as panoptic instance proposals derived from image
context and further refines them through attention-based encoding and decoding
to reason about semantic instance-voxel relationships. Experimental results
show that our approach surpasses state-of-the-art methods in overall panoptic
metrics PQ$^\dagger$ and PQ-All, matches performance in individual metrics, and
achieves a runtime reduction exceeding 14$\times$. Furthermore, our ablation
studies reveal that dynamically deriving instance proposals from image context,
as opposed to random initialization, leads to a 3.62% increase in PQ-All and a
remarkable average improvement of 18.65% in combined Thing-metrics. These
results highlight our introduction of context-adaptive instance proposals as a
pioneering effort in addressing vision-based 3D Panoptic Scene Completion.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [82] [FundaQ-8: A Clinically-Inspired Scoring Framework for Automated Fundus Image Quality Assessment](https://arxiv.org/abs/2506.20303)
*Lee Qi Zun,Oscar Wong Jin Hao,Nor Anita Binti Che Omar,Zalifa Zakiah Binti Asnir,Mohamad Sabri bin Sinal Zainal,Goh Man Fye*

Main category: eess.IV

TL;DR: FundaQ-8 is a framework for assessing fundus image quality using eight parameters. A ResNet18-based model predicts quality scores, validated on clinical and Kaggle datasets, improving diagnostic robustness.


<details>
  <summary>Details</summary>
Motivation: Automated fundus image quality assessment is challenging due to variability in acquisition and subjective evaluations. FundaQ-8 addresses this with a structured approach.

Method: Uses FundaQ-8 as a scoring reference, trains a ResNet18-based regression model with transfer learning and MSE optimization on 1800 images.

Result: Validation shows reliability and clinical interpretability. Improves diabetic retinopathy grading robustness.

Conclusion: FundaQ-8 enhances quality-aware training, valuable for real-world screening applications.

Abstract: Automated fundus image quality assessment (FIQA) remains a challenge due to
variations in image acquisition and subjective expert evaluations. We introduce
FundaQ-8, a novel expert-validated framework for systematically assessing
fundus image quality using eight critical parameters, including field coverage,
anatomical visibility, illumination, and image artifacts. Using FundaQ-8 as a
structured scoring reference, we develop a ResNet18-based regression model to
predict continuous quality scores in the 0 to 1 range. The model is trained on
1800 fundus images from real-world clinical sources and Kaggle datasets, using
transfer learning, mean squared error optimization, and standardized
preprocessing. Validation against the EyeQ dataset and statistical analyses
confirm the framework's reliability and clinical interpretability.
Incorporating FundaQ-8 into deep learning models for diabetic retinopathy
grading also improves diagnostic robustness, highlighting the value of
quality-aware training in real-world screening applications.

</details>


### [83] [VoxelOpt: Voxel-Adaptive Message Passing for Discrete Optimization in Deformable Abdominal CT Registration](https://arxiv.org/abs/2506.19975)
*Hang Zhang,Yuxi Zhang,Jiazheng Wang,Xiang Chen,Renjiu Hu,Xin Tian,Gaolei Li,Min Liu*

Main category: eess.IV

TL;DR: VoxelOpt is a novel DIR framework combining learning-based and iterative methods for better accuracy and runtime balance, using displacement entropy and adaptive message passing.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of learning-based (limited data, large deformations) and iterative (slow runtime) methods in deformable image registration.

Method: Uses displacement entropy for voxel-wise adaptive message passing, multi-level image pyramid with 27-neighbor cost volumes, and a pretrained segmentation model for feature extraction.

Result: Outperforms iterative methods in efficiency and accuracy, matches supervised learning-based methods in abdominal CT registration.

Conclusion: VoxelOpt achieves a superior balance between accuracy and runtime, leveraging strengths of both learning-based and iterative approaches.

Abstract: Recent developments in neural networks have improved deformable image
registration (DIR) by amortizing iterative optimization, enabling fast and
accurate DIR results. However, learning-based methods often face challenges
with limited training data, large deformations, and tend to underperform
compared to iterative approaches when label supervision is unavailable. While
iterative methods can achieve higher accuracy in such scenarios, they are
considerably slower than learning-based methods. To address these limitations,
we propose VoxelOpt, a discrete optimization-based DIR framework that combines
the strengths of learning-based and iterative methods to achieve a better
balance between registration accuracy and runtime. VoxelOpt uses displacement
entropy from local cost volumes to measure displacement signal strength at each
voxel, which differs from earlier approaches in three key aspects. First, it
introduces voxel-wise adaptive message passing, where voxels with lower entropy
receives less influence from their neighbors. Second, it employs a multi-level
image pyramid with 27-neighbor cost volumes at each level, avoiding exponential
complexity growth. Third, it replaces hand-crafted features or contrastive
learning with a pretrained foundational segmentation model for feature
extraction. In abdominal CT registration, these changes allow VoxelOpt to
outperform leading iterative in both efficiency and accuracy, while matching
state-of-the-art learning-based methods trained with label supervision. The
source code will be available at https://github.com/tinymilky/VoxelOpt

</details>


### [84] [MS-IQA: A Multi-Scale Feature Fusion Network for PET/CT Image Quality Assessment](https://arxiv.org/abs/2506.20200)
*Siqiao Li,Chen Hui,Wei Zhang,Rui Liang,Chenyue Song,Feng Jiang,Haiqi Zhu,Zhixuan Li,Hong Huang,Xiang Li*

Main category: eess.IV

TL;DR: Proposes MS-IQA, a multi-scale feature fusion network for PET/CT image quality assessment, combining ResNet and Swin Transformer features, and introduces a new dataset, PET-CT-IQA-DS.


<details>
  <summary>Details</summary>
Motivation: Existing IQA methods fail to account for both low-level distortions and high-level anatomical structures in PET/CT images, leading to diagnostic uncertainty.

Method: MS-IQA uses multi-scale features from ResNet and Swin Transformer, with a dynamically weighted channel attention mechanism for feature fusion.

Result: Superior performance on PET-CT-IQA-DS and LDCTIQAC2023 datasets compared to state-of-the-art methods.

Conclusion: MS-IQA provides an accurate and efficient IQA method for PET/CT, with publicly available code and dataset.

Abstract: Positron Emission Tomography / Computed Tomography (PET/CT) plays a critical
role in medical imaging, combining functional and anatomical information to aid
in accurate diagnosis. However, image quality degradation due to noise,
compression and other factors could potentially lead to diagnostic uncertainty
and increase the risk of misdiagnosis. When evaluating the quality of a PET/CT
image, both low-level features like distortions and high-level features like
organ anatomical structures affect the diagnostic value of the image. However,
existing medical image quality assessment (IQA) methods are unable to account
for both feature types simultaneously. In this work, we propose MS-IQA, a novel
multi-scale feature fusion network for PET/CT IQA, which utilizes multi-scale
features from various intermediate layers of ResNet and Swin Transformer,
enhancing its ability of perceiving both local and global information. In
addition, a multi-scale feature fusion module is also introduced to effectively
combine high-level and low-level information through a dynamically weighted
channel attention mechanism. Finally, to fill the blank of PET/CT IQA dataset,
we construct PET-CT-IQA-DS, a dataset containing 2,700 varying-quality PET/CT
images with quality scores assigned by radiologists. Experiments on our dataset
and the publicly available LDCTIQAC2023 dataset demonstrate that our proposed
model has achieved superior performance against existing state-of-the-art
methods in various IQA metrics. This work provides an accurate and efficient
IQA method for PET/CT. Our code and dataset are available at
https://github.com/MS-IQA/MS-IQA/.

</details>


### [85] [Opportunistic Osteoporosis Diagnosis via Texture-Preserving Self-Supervision, Mixture of Experts and Multi-Task Integration](https://arxiv.org/abs/2506.20282)
*Jiaxing Huang,Heng Guo,Le Lu,Fan Yang,Minfeng Xu,Ge Yang,Wei Luo*

Main category: eess.IV

TL;DR: A deep learning framework addresses limitations in osteoporosis diagnosis using CT scans by leveraging unlabeled data, improving cross-device adaptability, and integrating clinical knowledge.


<details>
  <summary>Details</summary>
Motivation: Current osteoporosis diagnosis methods like DXA have limited accessibility, and existing CT-based approaches underutilize data and lack clinical integration.

Method: Proposes a unified deep learning framework with self-supervised learning, Mixture of Experts (MoE) for cross-device adaptability, and multi-task learning for diagnosis, BMD regression, and vertebra location.

Result: Demonstrates superior generalizability and accuracy across clinical sites and external validation.

Conclusion: The framework improves opportunistic osteoporosis screening and diagnosis by addressing key limitations in current methods.

Abstract: Osteoporosis, characterized by reduced bone mineral density (BMD) and
compromised bone microstructure, increases fracture risk in aging populations.
While dual-energy X-ray absorptiometry (DXA) is the clinical standard for BMD
assessment, its limited accessibility hinders diagnosis in resource-limited
regions. Opportunistic computed tomography (CT) analysis has emerged as a
promising alternative for osteoporosis diagnosis using existing imaging data.
Current approaches, however, face three limitations: (1) underutilization of
unlabeled vertebral data, (2) systematic bias from device-specific DXA
discrepancies, and (3) insufficient integration of clinical knowledge such as
spatial BMD distribution patterns. To address these, we propose a unified deep
learning framework with three innovations. First, a self-supervised learning
method using radiomic representations to leverage unlabeled CT data and
preserve bone texture. Second, a Mixture of Experts (MoE) architecture with
learned gating mechanisms to enhance cross-device adaptability. Third, a
multi-task learning framework integrating osteoporosis diagnosis, BMD
regression, and vertebra location prediction. Validated across three clinical
sites and an external hospital, our approach demonstrates superior
generalizability and accuracy over existing methods for opportunistic
osteoporosis screening and diagnosis.

</details>


### [86] [EAGLE: An Efficient Global Attention Lesion Segmentation Model for Hepatic Echinococcosis](https://arxiv.org/abs/2506.20333)
*Jiayan Chen,Kai Li,Yulu Zhao,Jianqiang Huang,Zhan Wang*

Main category: eess.IV

TL;DR: EAGLE, a U-shaped network with PVSS encoder and HVSS decoder, achieves efficient HE lesion segmentation using CVSSB and HWTB modules, outperforming MSVM-UNet with 89.76% DSC.


<details>
  <summary>Details</summary>
Motivation: HE is a major issue in underdeveloped areas; existing models (CNNs, Transformers) have limitations in context modeling or computational cost.

Method: Proposes EAGLE with PVSS encoder, HVSS decoder, CVSSB for local-global feature fusion, and HWTB for lossless downsampling.

Result: Achieves 89.76% DSC, surpassing MSVM-UNet by 1.61%.

Conclusion: EAGLE offers an efficient and accurate solution for HE lesion segmentation, addressing limitations of existing models.

Abstract: Hepatic echinococcosis (HE) is a widespread parasitic disease in
underdeveloped pastoral areas with limited medical resources. While CNN-based
and Transformer-based models have been widely applied to medical image
segmentation, CNNs lack global context modeling due to local receptive fields,
and Transformers, though capable of capturing long-range dependencies, are
computationally expensive. Recently, state space models (SSMs), such as Mamba,
have gained attention for their ability to model long sequences with linear
complexity. In this paper, we propose EAGLE, a U-shaped network composed of a
Progressive Visual State Space (PVSS) encoder and a Hybrid Visual State Space
(HVSS) decoder that work collaboratively to achieve efficient and accurate
segmentation of hepatic echinococcosis (HE) lesions. The proposed Convolutional
Vision State Space Block (CVSSB) module is designed to fuse local and global
features, while the Haar Wavelet Transformation Block (HWTB) module compresses
spatial information into the channel dimension to enable lossless downsampling.
Due to the lack of publicly available HE datasets, we collected CT slices from
260 patients at a local hospital. Experimental results show that EAGLE achieves
state-of-the-art performance with a Dice Similarity Coefficient (DSC) of
89.76%, surpassing MSVM-UNet by 1.61%.

</details>


### [87] [Fusing Radiomic Features with Deep Representations for Gestational Age Estimation in Fetal Ultrasound Images](https://arxiv.org/abs/2506.20407)
*Fangyijie Wang,Yuan Liang,Sourav Bhattacharjee,Abey Campbell,Kathleen M. Curran,Guénolé Silvestre*

Main category: eess.IV

TL;DR: A novel feature fusion framework for gestational age (GA) estimation from fetal ultrasound images, combining deep learning and radiomic features, achieves a mean absolute error of 8.0 days.


<details>
  <summary>Details</summary>
Motivation: Manual GA estimation is operator-dependent and time-consuming; automatic methods are needed for clinical practice.

Method: Deep learning extracts image representations, radiomics reveals fetal brain growth patterns, and fusion of both estimates GA.

Result: Outperforms current methods with 8.0 days mean absolute error, robust across diverse populations.

Conclusion: The framework is effective, interpretable, and publicly available for clinical use.

Abstract: Accurate gestational age (GA) estimation, ideally through fetal ultrasound
measurement, is a crucial aspect of providing excellent antenatal care.
However, deriving GA from manual fetal biometric measurements depends on the
operator and is time-consuming. Hence, automatic computer-assisted methods are
demanded in clinical practice. In this paper, we present a novel feature fusion
framework to estimate GA using fetal ultrasound images without any measurement
information. We adopt a deep learning model to extract deep representations
from ultrasound images. We extract radiomic features to reveal patterns and
characteristics of fetal brain growth. To harness the interpretability of
radiomics in medical imaging analysis, we estimate GA by fusing radiomic
features and deep representations. Our framework estimates GA with a mean
absolute error of 8.0 days across three trimesters, outperforming current
machine learning-based methods at these gestational ages. Experimental results
demonstrate the robustness of our framework across different populations in
diverse geographical regions. Our code is publicly available on
\href{https://github.com/13204942/RadiomicsImageFusion_FetalUS}{GitHub}.

</details>


### [88] [Weighted Mean Frequencies: a handcraft Fourier feature for 4D Flow MRI segmentation](https://arxiv.org/abs/2506.20614)
*Simon Perrin,Sébastien Levilly,Huajun Sun,Harold Mouchère,Jean-Michel Serfaty*

Main category: eess.IV

TL;DR: A new feature called Weighted Mean Frequencies (WMF) is introduced to improve 4D Flow MRI image segmentation, outperforming PC-MRA with notable IoU and Dice score improvements.


<details>
  <summary>Details</summary>
Motivation: Poor resolution and noise in 4D Flow MRI images hinder accurate segmentation, especially for biomarkers like wall shear stress.

Method: Proposes WMF, a handcrafted feature revealing pulsatile flow regions, tested via optimal thresholding and deep learning segmentation.

Result: WMF improves IoU by 0.12 and Dice by 0.13 compared to PC-MRA in deep learning tasks.

Conclusion: WMF enhances segmentation accuracy and could benefit other vascular regions like the heart or brain.

Abstract: In recent decades, the use of 4D Flow MRI images has enabled the
quantification of velocity fields within a volume of interest and along the
cardiac cycle. However, the lack of resolution and the presence of noise in
these biomarkers are significant issues. As indicated by recent studies, it
appears that biomarkers such as wall shear stress are particularly impacted by
the poor resolution of vessel segmentation. The Phase Contrast Magnetic
Resonance Angiography (PC-MRA) is the state-of-the-art method to facilitate
segmentation. The objective of this work is to introduce a new handcraft
feature that provides a novel visualisation of 4D Flow MRI images, which is
useful in the segmentation task. This feature, termed Weighted Mean Frequencies
(WMF), is capable of revealing the region in three dimensions where a voxel has
been passed by pulsatile flow. Indeed, this feature is representative of the
hull of all pulsatile velocity voxels. The value of the feature under
discussion is illustrated by two experiments. The experiments involved
segmenting 4D Flow MRI images using optimal thresholding and deep learning
methods. The results obtained demonstrate a substantial enhancement in terms of
IoU and Dice, with a respective increase of 0.12 and 0.13 in comparison with
the PC-MRA feature, as evidenced by the deep learning task. This feature has
the potential to yield valuable insights that could inform future segmentation
processes in other vascular regions, such as the heart or the brain.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [89] [Practical insights on the effect of different encodings, ansätze and measurements in quantum and hybrid convolutional neural networks](https://arxiv.org/abs/2506.20355)
*Jesús Lozano-Cruz,Albert Nieto-Morales,Oriol Balló-Gimbernat,Adan Garriga,Antón Rodríguez-Otero,Alejandro Borrallo-Rentero*

Main category: quant-ph

TL;DR: The study explores design choices in quantum and hybrid convolutional neural networks (HQNN/QCNN) for satellite image classification, identifying data encoding as the most influential factor in hybrid models, while measurement protocols dominate in purely quantum models.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of various design choices (data encoding, variational ansätze, measurement) in quantum and hybrid neural networks for satellite image classification.

Method: Systematic evaluation of ~500 model configurations on the EuroSAT dataset, comparing hybrid and purely quantum architectures.

Result: In hybrid models, data encoding affects accuracy by ~30%, while ansätze and measurement have minor impact (<5%). In quantum models, measurement strategy varies accuracy by ~30%, and encoding mapping by ~8%.

Conclusion: Data encoding is critical for hybrid models, while measurement protocols are key for purely quantum models in satellite image classification.

Abstract: This study investigates the design choices of parameterized quantum circuits
(PQCs) within quantum and hybrid convolutional neural network (HQNN and QCNN)
architectures, applied to the task of satellite image classification using the
EuroSAT dataset. We systematically evaluate the performance implications of
data encoding techniques, variational ans\"atze, and measurement in approx. 500
distinct model configurations. Our analysis reveals a clear hierarchy of
influence on model performance. For hybrid architectures, which were
benchmarked against their direct classical equivalents (e.g. the same
architecture with the PQCs removed), the data encoding strategy is the dominant
factor, with validation accuracy varying over 30% for distinct embeddings. In
contrast, the selection of variational ans\"atze and measurement basis had a
comparatively marginal effect, with validation accuracy variations remaining
below 5%. For purely quantum models, restricted to amplitude encoding,
performance was most dependent on the measurement protocol and the
data-to-amplitude mapping. The measurement strategy varied the validation
accuracy by up to 30% and the encoding mapping by around 8 percentage points.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [90] [X-SiT: Inherently Interpretable Surface Vision Transformers for Dementia Diagnosis](https://arxiv.org/abs/2506.20267)
*Fabian Bongratz,Tom Nuno Wolf,Jaume Gual Ramon,Christian Wachinger*

Main category: cs.GR

TL;DR: The paper introduces X-SiT, an interpretable neural network for medical imaging, leveraging cortical surface renderings to improve visualization and diagnosis of neurological disorders like Alzheimer's and frontotemporal dementia.


<details>
  <summary>Details</summary>
Motivation: The challenge of interpreting 3D volumetric data in medical imaging, especially for complex structures like the cerebral cortex, motivates the development of interpretable models like X-SiT. Cortical surface renderings offer a clearer representation, aiding clinical decision-making.

Method: X-SiT is proposed as the first inherently interpretable neural network for cortical surface data. It includes a prototypical surface patch decoder for classifying embeddings and uses case-based reasoning with cortical prototypes.

Result: X-SiT achieves state-of-the-art performance in detecting Alzheimer's disease and frontotemporal dementia, providing interpretable prototypes that align with disease patterns and highlight classification errors.

Conclusion: X-SiT advances interpretable AI for medical imaging, offering clinically relevant insights and improving diagnostic accuracy for neurological disorders.

Abstract: Interpretable models are crucial for supporting clinical decision-making,
driving advances in their development and application for medical images.
However, the nature of 3D volumetric data makes it inherently challenging to
visualize and interpret intricate and complex structures like the cerebral
cortex. Cortical surface renderings, on the other hand, provide a more
accessible and understandable 3D representation of brain anatomy, facilitating
visualization and interactive exploration. Motivated by this advantage and the
widespread use of surface data for studying neurological disorders, we present
the eXplainable Surface Vision Transformer (X-SiT). This is the first
inherently interpretable neural network that offers human-understandable
predictions based on interpretable cortical features. As part of X-SiT, we
introduce a prototypical surface patch decoder for classifying surface patch
embeddings, incorporating case-based reasoning with spatially corresponding
cortical prototypes. The results demonstrate state-of-the-art performance in
detecting Alzheimer's disease and frontotemporal dementia while additionally
providing informative prototypes that align with known disease patterns and
reveal classification errors.

</details>


### [91] [DreamAnywhere: Object-Centric Panoramic 3D Scene Generation](https://arxiv.org/abs/2506.20367)
*Edoardo Alberto Dominici,Jozef Hladky,Floor Verhoeven,Lukas Radl,Thomas Deixelberger,Stefan Ainetter,Philipp Drescher,Stefan Hauswiesner,Arno Coomans,Giacomo Nazzaro,Konstantinos Vardis,Markus Steinberger*

Main category: cs.GR

TL;DR: DreamAnywhere is a modular system for fast 3D scene generation from text, addressing limitations like front-facing views and low fidelity in existing methods. It supports immersive navigation, object-level editing, and is ideal for rapid prototyping.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-3D methods often produce limited, low-fidelity scenes. DreamAnywhere aims to overcome these by enabling 360° panoramic generation, detailed object placement, and intuitive editing.

Method: DreamAnywhere synthesizes a 360° panoramic image from text, decomposes it into background and objects, constructs a 3D representation via hybrid inpainting, and lifts object masks to detailed 3D objects.

Result: The system improves coherence in novel view synthesis and achieves competitive image quality, outperforming state-of-the-art methods. User studies confirm its preference and practicality.

Conclusion: DreamAnywhere is a robust, customizable solution for rapid 3D scene generation, particularly useful for low-budget productions and iterative design.

Abstract: Recent advances in text-to-3D scene generation have demonstrated significant
potential to transform content creation across multiple industries. Although
the research community has made impressive progress in addressing the
challenges of this complex task, existing methods often generate environments
that are only front-facing, lack visual fidelity, exhibit limited scene
understanding, and are typically fine-tuned for either indoor or outdoor
settings. In this work, we address these issues and propose DreamAnywhere, a
modular system for the fast generation and prototyping of 3D scenes. Our system
synthesizes a 360{\deg} panoramic image from text, decomposes it into
background and objects, constructs a complete 3D representation through hybrid
inpainting, and lifts object masks to detailed 3D objects that are placed in
the virtual environment. DreamAnywhere supports immersive navigation and
intuitive object-level editing, making it ideal for scene exploration, visual
mock-ups, and rapid prototyping -- all with minimal manual modeling. These
features make our system particularly suitable for low-budget movie production,
enabling quick iteration on scene layout and visual tone without the overhead
of traditional 3D workflows. Our modular pipeline is highly customizable as it
allows components to be replaced independently. Compared to current
state-of-the-art text and image-based 3D scene generation approaches,
DreamAnywhere shows significant improvements in coherence in novel view
synthesis and achieves competitive image quality, demonstrating its
effectiveness across diverse and challenging scenarios. A comprehensive user
study demonstrates a clear preference for our method over existing approaches,
validating both its technical robustness and practical usefulness.

</details>


### [92] [EditP23: 3D Editing via Propagation of Image Prompts to Multi-View](https://arxiv.org/abs/2506.20652)
*Roi Bar-On,Dana Cohen-Bar,Daniel Cohen-Or*

Main category: cs.GR

TL;DR: EditP23 is a mask-free 3D editing method that propagates 2D image edits to multi-view representations using image prompts, ensuring 3D consistency without optimization.


<details>
  <summary>Details</summary>
Motivation: Traditional 3D editing relies on text prompts or masks, which can be limiting. EditP23 aims to enable intuitive edits using image pairs for better control and coherence.

Method: The method uses an edit-aware flow in a pre-trained multi-view diffusion model's latent space, guided by an original and edited image pair, to propagate edits coherently across views.

Result: EditP23 achieves high fidelity across various object categories and editing scenarios, preserving object identity without manual masks.

Conclusion: EditP23 offers an efficient, mask-free solution for 3D editing, demonstrating strong performance in maintaining consistency and fidelity.

Abstract: We present EditP23, a method for mask-free 3D editing that propagates 2D
image edits to multi-view representations in a 3D-consistent manner. In
contrast to traditional approaches that rely on text-based prompting or
explicit spatial masks, EditP23 enables intuitive edits by conditioning on a
pair of images: an original view and its user-edited counterpart. These image
prompts are used to guide an edit-aware flow in the latent space of a
pre-trained multi-view diffusion model, allowing the edit to be coherently
propagated across views. Our method operates in a feed-forward manner, without
optimization, and preserves the identity of the original object, in both
structure and appearance. We demonstrate its effectiveness across a range of
object categories and editing scenarios, achieving high fidelity to the source
while requiring no manual masks.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [93] [Accurate and Energy Efficient: Local Retrieval-Augmented Generation Models Outperform Commercial Large Language Models in Medical Tasks](https://arxiv.org/abs/2506.20009)
*Konstantinos Vrettos,Michail E. Klontzas*

Main category: cs.AI

TL;DR: Custom RAG models outperform commercial LLMs in medical tasks with higher accuracy and lower energy consumption, promoting sustainable AI.


<details>
  <summary>Details</summary>
Motivation: Address environmental and ethical concerns of AI in healthcare, focusing on resource use and patient privacy.

Method: Developed a customizable RAG framework for medical tasks, monitoring energy and CO2 emissions, and tested it with open-source LLMs.

Result: Custom RAG models, especially llama3.1:8B, achieved higher accuracy (58.5%) and lower energy/CO2 footprint than commercial models.

Conclusion: Local LLMs can create efficient, sustainable RAGs for medical tasks, aligning with UN sustainability goals.

Abstract: Background The increasing adoption of Artificial Intelligence (AI) in
healthcare has sparked growing concerns about its environmental and ethical
implications. Commercial Large Language Models (LLMs), such as ChatGPT and
DeepSeek, require substantial resources, while the utilization of these systems
for medical purposes raises critical issues regarding patient privacy and
safety. Methods We developed a customizable Retrieval-Augmented Generation
(RAG) framework for medical tasks, which monitors its energy usage and CO2
emissions. This system was then used to create RAGs based on various
open-source LLMs. The tested models included both general purpose models like
llama3.1:8b and medgemma-4b-it, which is medical-domain specific. The best RAGs
performance and energy consumption was compared to DeepSeekV3-R1 and OpenAIs
o4-mini model. A dataset of medical questions was used for the evaluation.
Results Custom RAG models outperformed commercial models in accuracy and energy
consumption. The RAG model built on llama3.1:8B achieved the highest accuracy
(58.5%) and was significantly better than other models, including o4-mini and
DeepSeekV3-R1. The llama3.1-RAG also exhibited the lowest energy consumption
and CO2 footprint among all models, with a Performance per kWh of 0.52 and a
total CO2 emission of 473g. Compared to o4-mini, the llama3.1-RAG achieved 2.7x
times more accuracy points per kWh and 172% less electricity usage while
maintaining higher accuracy. Conclusion Our study demonstrates that local LLMs
can be leveraged to develop RAGs that outperform commercial, online LLMs in
medical tasks, while having a smaller environmental impact. Our modular
framework promotes sustainable AI development, reducing electricity usage and
aligning with the UNs Sustainable Development Goals.

</details>


### [94] [Persona-Assigned Large Language Models Exhibit Human-Like Motivated Reasoning](https://arxiv.org/abs/2506.20020)
*Saloni Dash,Amélie Reymond,Emma S. Spiro,Aylin Caliskan*

Main category: cs.AI

TL;DR: LLMs with assigned personas show human-like motivated reasoning, reducing accuracy in veracity discernment and increasing bias in scientific evidence evaluation, with limited debiasing success.


<details>
  <summary>Details</summary>
Motivation: To explore if LLMs exhibit identity-congruent motivated reasoning like humans, which can worsen societal polarization.

Method: Tested 8 LLMs with 8 personas across political and socio-demographic attributes on veracity discernment and scientific evidence evaluation tasks.

Result: Persona-assigned LLMs had up to 9% reduced veracity discernment and up to 90% higher accuracy on identity-congruent scientific evidence. Debiasing methods were ineffective.

Conclusion: LLMs display human-like motivated reasoning, raising concerns about exacerbating biases in AI and society.

Abstract: Reasoning in humans is prone to biases due to underlying motivations like
identity protection, that undermine rational decision-making and judgment. This
motivated reasoning at a collective level can be detrimental to society when
debating critical issues such as human-driven climate change or vaccine safety,
and can further aggravate political polarization. Prior studies have reported
that large language models (LLMs) are also susceptible to human-like cognitive
biases, however, the extent to which LLMs selectively reason toward
identity-congruent conclusions remains largely unexplored. Here, we investigate
whether assigning 8 personas across 4 political and socio-demographic
attributes induces motivated reasoning in LLMs. Testing 8 LLMs (open source and
proprietary) across two reasoning tasks from human-subject studies -- veracity
discernment of misinformation headlines and evaluation of numeric scientific
evidence -- we find that persona-assigned LLMs have up to 9% reduced veracity
discernment relative to models without personas. Political personas
specifically, are up to 90% more likely to correctly evaluate scientific
evidence on gun control when the ground truth is congruent with their induced
political identity. Prompt-based debiasing methods are largely ineffective at
mitigating these effects. Taken together, our empirical findings are the first
to suggest that persona-assigned LLMs exhibit human-like motivated reasoning
that is hard to mitigate through conventional debiasing prompts -- raising
concerns of exacerbating identity-congruent reasoning in both LLMs and humans.

</details>


### [95] [Language Modeling by Language Models](https://arxiv.org/abs/2506.20249)
*Junyan Cheng,Peter Clark,Kyle Richardson*

Main category: cs.AI

TL;DR: Genesys, a multi-agent LLM system, discovers novel LM architectures by simulating research stages, using genetic programming for efficiency, and outperforms known architectures like GPT2 and Mamba2.


<details>
  <summary>Details</summary>
Motivation: To explore if LLMs can autonomously discover novel LM architectures by mimicking real research processes.

Method: A multi-agent LLM approach (Genesys) with stages: ideation, literature search, design implementation, pre-training, and evaluation. Uses genetic programming and a Ladder of Scales for efficiency.

Result: 1,162 new designs discovered, 1,062 verified; best designs outperform GPT2 and Mamba2 on 6/9 benchmarks.

Conclusion: Genesys demonstrates effective autonomous discovery of competitive LM architectures, offering insights for future systems.

Abstract: Can we leverage LLMs to model the process of discovering novel language model
(LM) architectures? Inspired by real research, we propose a multi-agent LLM
approach that simulates the conventional stages of research, from ideation and
literature search (proposal stage) to design implementation (code generation),
generative pre-training, and downstream evaluation (verification). Using ideas
from scaling laws, our system, Genesys, employs a Ladder of Scales approach;
new designs are proposed, adversarially reviewed, implemented, and selectively
verified at increasingly larger model scales (14M$\sim$350M parameters) with a
narrowing budget (the number of models we can train at each scale). To help
make discovery efficient and factorizable, Genesys uses a novel genetic
programming backbone, which we show has empirical advantages over commonly used
direct prompt generation workflows (e.g., $\sim$86\% percentage point
improvement in successful design generation, a key bottleneck). We report
experiments involving 1,162 newly discovered designs (1,062 fully verified
through pre-training) and find the best designs to be highly competitive with
known architectures (e.g., outperform GPT2, Mamba2, etc., on 6/9 common
benchmarks). We couple these results with comprehensive system-level ablations
and formal results, which give broader insights into the design of effective
autonomous discovery systems.

</details>


### [96] [The Decrypto Benchmark for Multi-Agent Reasoning and Theory of Mind](https://arxiv.org/abs/2506.20664)
*Andrei Lupu,Timon Willi,Jakob Foerster*

Main category: cs.AI

TL;DR: The paper introduces Decrypto, a game-based benchmark for evaluating multi-agent reasoning and theory of mind (ToM) in LLMs, addressing gaps in existing benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for ToM and multi-agent abilities in LLMs are limited by narrow scope, data leakage, saturation, and lack of interactivity.

Method: The authors propose Decrypto, a benchmark inspired by cognitive science, computational pragmatics, and multi-agent reinforcement learning, designed to minimize confounding factors.

Result: Empirical evaluations show LLMs lag behind humans and simple baselines in game-playing and ToM tasks, with newer models performing worse than older ones.

Conclusion: Decrypto fills a critical gap in ToM evaluations, aiding the development of better artificial agents.

Abstract: As Large Language Models (LLMs) gain agentic abilities, they will have to
navigate complex multi-agent scenarios, interacting with human users and other
agents in cooperative and competitive settings. This will require new reasoning
skills, chief amongst them being theory of mind (ToM), or the ability to reason
about the "mental" states of other agents. However, ToM and other multi-agent
abilities in LLMs are poorly understood, since existing benchmarks suffer from
narrow scope, data leakage, saturation, and lack of interactivity. We thus
propose Decrypto, a game-based benchmark for multi-agent reasoning and ToM
drawing inspiration from cognitive science, computational pragmatics and
multi-agent reinforcement learning. It is designed to be as easy as possible in
all other dimensions, eliminating confounding factors commonly found in other
benchmarks. To our knowledge, it is also the first platform for designing
interactive ToM experiments.
  We validate the benchmark design through comprehensive empirical evaluations
of frontier LLMs, robustness studies, and human-AI cross-play experiments. We
find that LLM game-playing abilities lag behind humans and simple
word-embedding baselines. We then create variants of two classic cognitive
science experiments within Decrypto to evaluate three key ToM abilities.
Surprisingly, we find that state-of-the-art reasoning models are significantly
worse at those tasks than their older counterparts. This demonstrates that
Decrypto addresses a crucial gap in current reasoning and ToM evaluations, and
paves the path towards better artificial agents.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [97] [A Multi-Modal Spatial Risk Framework for EV Charging Infrastructure Using Remote Sensing](https://arxiv.org/abs/2506.19860)
*Oktay Karakuş,Padraig Corcoran*

Main category: eess.SP

TL;DR: RSERI-EV is a framework for assessing EV charging station resilience using multi-source data and spatial analytics, applied to Wales.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored resilience of EV charging infrastructure under environmental and infrastructural stress.

Method: Combines remote sensing, open datasets, and spatial graph analytics to compute a Resilience Score.

Result: Demonstrates feasibility with Wales' EV charger dataset, using a kNN graph for diagnostics.

Conclusion: Highlights multi-source data fusion and spatial reasoning for climate-resilient EV deployment.

Abstract: Electric vehicle (EV) charging infrastructure is increasingly critical to
sustainable transport systems, yet its resilience under environmental and
infrastructural stress remains underexplored. In this paper, we introduce
RSERI-EV, a spatially explicit and multi-modal risk assessment framework that
combines remote sensing data, open infrastructure datasets, and spatial graph
analytics to evaluate the vulnerability of EV charging stations. RSERI-EV
integrates diverse data layers, including flood risk maps, land surface
temperature (LST) extremes, vegetation indices (NDVI), land use/land cover
(LULC), proximity to electrical substations, and road accessibility to generate
a composite Resilience Score. We apply this framework to the country of Wales
EV charger dataset to demonstrate its feasibility. A spatial $k$-nearest
neighbours ($k$NN) graph is constructed over the charging network to enable
neighbourhood-based comparisons and graph-aware diagnostics. Our prototype
highlights the value of multi-source data fusion and interpretable spatial
reasoning in supporting climate-resilient, infrastructure-aware EV deployment.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [98] [Capturing Visualization Design Rationale](https://arxiv.org/abs/2506.16571)
*Maeve Hutchinson,Radu Jianu,Aidan Slingsby,Jo Wood,Pranava Madhyastha*

Main category: cs.HC

TL;DR: A new dataset and methodology for probing visualization design rationale using real-world literate visualization notebooks and LLMs to generate and validate question-answer-rationale triples.


<details>
  <summary>Details</summary>
Motivation: Existing datasets focus on interpreting visualizations, not understanding their encoding. This work aims to capture design rationale behind visualizations.

Method: Uses literate visualization notebooks from students, leveraging LLMs to generate and validate question-answer-rationale triples.

Result: A curated dataset capturing visualization design choices and rationales.

Conclusion: The dataset and methodology provide a foundation for understanding visualization design rationale through natural language.

Abstract: Prior natural language datasets for data visualization have focused on tasks
such as visualization literacy assessment, insight generation, and
visualization generation from natural language instructions. These studies
often rely on controlled setups with purpose-built visualizations and
artificially constructed questions. As a result, they tend to prioritize the
interpretation of visualizations, focusing on decoding visualizations rather
than understanding their encoding. In this paper, we present a new dataset and
methodology for probing visualization design rationale through natural
language. We leverage a unique source of real-world visualizations and natural
language narratives: literate visualization notebooks created by students as
part of a data visualization course. These notebooks combine visual artifacts
with design exposition, in which students make explicit the rationale behind
their design decisions. We also use large language models (LLMs) to generate
and categorize question-answer-rationale triples from the narratives and
articulations in the notebooks. We then carefully validate the triples and
curate a dataset that captures and distills the visualization design choices
and corresponding rationales of the students.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [99] [PSALM-V: Automating Symbolic Planning in Interactive Visual Environments with Large Language Models](https://arxiv.org/abs/2506.20097)
*Wang Bill Zhu,Miaosen Chai,Ishika Singh,Robin Jia,Jesse Thomason*

Main category: cs.RO

TL;DR: PSALM-V is a neuro-symbolic system that autonomously learns symbolic action semantics in visual environments, improving plan success rates and efficiency without expert input.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on text-based domains or unrealistic assumptions, limiting their applicability in real-world visual and partially observed settings.

Method: PSALM-V uses LLMs to generate heuristic plans and candidate semantics, dynamically infers PDDL files, and iteratively refines action semantics through execution outcomes.

Result: PSALM-V increases plan success rates from 37% to 74% in ALFRED and improves efficiency in RTFM and Overcooked-AI. It also works in real-world robot tasks.

Conclusion: PSALM-V advances autonomous learning of symbolic semantics in visual and partially observed environments, demonstrating practical applicability.

Abstract: We propose PSALM-V, the first autonomous neuro-symbolic learning system able
to induce symbolic action semantics (i.e., pre- and post-conditions) in visual
environments through interaction. PSALM-V bootstraps reliable symbolic planning
without expert action definitions, using LLMs to generate heuristic plans and
candidate symbolic semantics. Previous work has explored using large language
models to generate action semantics for Planning Domain Definition Language
(PDDL)-based symbolic planners. However, these approaches have primarily
focused on text-based domains or relied on unrealistic assumptions, such as
access to a predefined problem file, full observability, or explicit error
messages. By contrast, PSALM-V dynamically infers PDDL problem files and domain
action semantics by analyzing execution outcomes and synthesizing possible
error explanations. The system iteratively generates and executes plans while
maintaining a tree-structured belief over possible action semantics for each
action, iteratively refining these beliefs until a goal state is reached.
Simulated experiments of task completion in ALFRED demonstrate that PSALM-V
increases the plan success rate from 37% (Claude-3.7) to 74% in partially
observed setups. Results on two 2D game environments, RTFM and Overcooked-AI,
show that PSALM-V improves step efficiency and succeeds in domain induction in
multi-agent settings. PSALM-V correctly induces PDDL pre- and post-conditions
for real-world robot BlocksWorld tasks, despite low-level manipulation failures
from the robot.

</details>


### [100] [Why Robots Are Bad at Detecting Their Mistakes: Limitations of Miscommunication Detection in Human-Robot Dialogue](https://arxiv.org/abs/2506.20268)
*Ruben Janssens,Jens De Bock,Sofie Labat,Eva Verhelst,Veronique Hoste,Tony Belpaeme*

Main category: cs.RO

TL;DR: The paper evaluates machine learning models for detecting miscommunication in human-robot dialogue, finding limited success due to users not clearly signaling errors.


<details>
  <summary>Details</summary>
Motivation: To improve robot interaction by detecting miscommunication, a critical factor for user trust and engagement.

Method: Used a multi-modal dataset of 240 human-robot conversations with induced errors, assessing state-of-the-art computer vision models and human raters.

Result: Models performed slightly better than random chance; human raters also struggled, revealing users often don't signal miscommunication.

Conclusion: Highlights a fundamental limitation in detecting robot miscommunication and suggests designing conversations to elicit clearer feedback.

Abstract: Detecting miscommunication in human-robot interaction is a critical function
for maintaining user engagement and trust. While humans effortlessly detect
communication errors in conversations through both verbal and non-verbal cues,
robots face significant challenges in interpreting non-verbal feedback, despite
advances in computer vision for recognizing affective expressions. This
research evaluates the effectiveness of machine learning models in detecting
miscommunications in robot dialogue. Using a multi-modal dataset of 240
human-robot conversations, where four distinct types of conversational failures
were systematically introduced, we assess the performance of state-of-the-art
computer vision models. After each conversational turn, users provided feedback
on whether they perceived an error, enabling an analysis of the models' ability
to accurately detect robot mistakes. Despite using state-of-the-art models, the
performance barely exceeds random chance in identifying miscommunication, while
on a dataset with more expressive emotional content, they successfully
identified confused states. To explore the underlying cause, we asked human
raters to do the same. They could also only identify around half of the induced
miscommunications, similarly to our model. These results uncover a fundamental
limitation in identifying robot miscommunications in dialogue: even when users
perceive the induced miscommunication as such, they often do not communicate
this to their robotic conversation partner. This knowledge can shape
expectations of the performance of computer vision models and can help
researchers to design better human-robot conversations by deliberately
eliciting feedback where needed.

</details>


### [101] [Consensus-Driven Uncertainty for Robotic Grasping based on RGB Perception](https://arxiv.org/abs/2506.20045)
*Eric C. Joyce,Qianwen Zhao,Nathaniel Burgdorfer,Long Wang,Philippos Mordohai*

Main category: cs.RO

TL;DR: A method for training deep networks to predict grasp success based on pose estimation uncertainty, improving robotic grasping reliability.


<details>
  <summary>Details</summary>
Motivation: Deep pose estimators are overconfident, leading to task failure. Predicting uncertainty can help avoid unreliable grasps.

Method: Train lightweight networks using real-image pose estimation and simulated grasping data, jointly training on diverse objects.

Result: Networks predict grasp success before execution, benefiting from joint training across varied objects.

Conclusion: Diverse object training enhances grasp prediction, improving robotic grasping reliability.

Abstract: Deep object pose estimators are notoriously overconfident. A grasping agent
that both estimates the 6-DoF pose of a target object and predicts the
uncertainty of its own estimate could avoid task failure by choosing not to act
under high uncertainty. Even though object pose estimation improves and
uncertainty quantification research continues to make strides, few studies have
connected them to the downstream task of robotic grasping. We propose a method
for training lightweight, deep networks to predict whether a grasp guided by an
image-based pose estimate will succeed before that grasp is attempted. We
generate training data for our networks via object pose estimation on real
images and simulated grasping. We also find that, despite high object
variability in grasping trials, networks benefit from training on all objects
jointly, suggesting that a diverse variety of objects can nevertheless
contribute to the same goal.

</details>


### [102] [HRIBench: Benchmarking Vision-Language Models for Real-Time Human Perception in Human-Robot Interaction](https://arxiv.org/abs/2506.20566)
*Zhonghao Shi,Enyu Zhao,Nathaniel Dennler,Jingzhen Wang,Xinyang Xu,Kaleen Shrestha,Mengxue Fu,Daniel Seita,Maja Matarić*

Main category: cs.RO

TL;DR: The paper introduces HRIBench, a benchmark for evaluating vision-language models (VLMs) in human perception tasks for human-robot interaction (HRI), highlighting their performance-latency trade-offs and limitations.


<details>
  <summary>Details</summary>
Motivation: Real-time human perception is vital for HRI, but current VLMs suffer from high latency, limiting their practical use.

Method: HRIBench was created with 1000 VQA questions across five HRI domains, evaluating 11 VLMs for performance and latency.

Result: VLMs struggle with core HRI perceptual tasks and lack satisfactory performance-latency trade-offs for real-time deployment.

Conclusion: Future research should focus on developing smaller, low-latency VLMs with better human perception capabilities.

Abstract: Real-time human perception is crucial for effective human-robot interaction
(HRI). Large vision-language models (VLMs) offer promising generalizable
perceptual capabilities but often suffer from high latency, which negatively
impacts user experience and limits VLM applicability in real-world scenarios.
To systematically study VLM capabilities in human perception for HRI and
performance-latency trade-offs, we introduce HRIBench, a visual
question-answering (VQA) benchmark designed to evaluate VLMs across a diverse
set of human perceptual tasks critical for HRI. HRIBench covers five key
domains: (1) non-verbal cue understanding, (2) verbal instruction
understanding, (3) human-robot object relationship understanding, (4) social
navigation, and (5) person identification. To construct HRIBench, we collected
data from real-world HRI environments to curate questions for non-verbal cue
understanding, and leveraged publicly available datasets for the remaining four
domains. We curated 200 VQA questions for each domain, resulting in a total of
1000 questions for HRIBench. We then conducted a comprehensive evaluation of
both state-of-the-art closed-source and open-source VLMs (N=11) on HRIBench.
Our results show that, despite their generalizability, current VLMs still
struggle with core perceptual capabilities essential for HRI. Moreover, none of
the models within our experiments demonstrated a satisfactory
performance-latency trade-off suitable for real-time deployment, underscoring
the need for future research on developing smaller, low-latency VLMs with
improved human perception capabilities. HRIBench and our results can be found
in this Github repository: https://github.com/interaction-lab/HRIBench.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [103] [A Spatio-Temporal Point Process for Fine-Grained Modeling of Reading Behavior](https://arxiv.org/abs/2506.19999)
*Francesco Ignazio Re,Andreas Opedal,Glib Manaiev,Mario Giulianelli,Ryan Cotterell*

Main category: cs.LG

TL;DR: The paper proposes a probabilistic model for reading behavior using a marked spatio-temporal point process, improving on traditional methods by capturing dynamic fixation and saccade patterns.


<details>
  <summary>Details</summary>
Motivation: Standard eye-tracking models ignore spatio-temporal dynamics in reading. The paper aims to better model these dynamics to understand online sentence processing.

Method: A marked spatio-temporal point process models fixations and saccades. Saccades use a Hawkes process, and fixation durations incorporate time-convolved predictors.

Result: The Hawkes process model fits human saccades better than baselines. Contextual surprisal marginally improves fixation duration predictions, suggesting limited explanatory power for fine-grained eye movements.

Conclusion: The proposed model captures reading dynamics more effectively, but surprisal theory may not fully explain fine-grained eye movements during reading.

Abstract: Reading is a process that unfolds across space and time, alternating between
fixations where a reader focuses on a specific point in space, and saccades
where a reader rapidly shifts their focus to a new point. An ansatz of
psycholinguistics is that modeling a reader's fixations and saccades yields
insight into their online sentence processing. However, standard approaches to
such modeling rely on aggregated eye-tracking measurements and models that
impose strong assumptions, ignoring much of the spatio-temporal dynamics that
occur during reading. In this paper, we propose a more general probabilistic
model of reading behavior, based on a marked spatio-temporal point process,
that captures not only how long fixations last, but also where they land in
space and when they take place in time. The saccades are modeled using a Hawkes
process, which captures how each fixation excites the probability of a new
fixation occurring near it in time and space. The duration time of fixation
events is modeled as a function of fixation-specific predictors convolved
across time, thus capturing spillover effects. Empirically, our Hawkes process
model exhibits a better fit to human saccades than baselines. With respect to
fixation durations, we observe that incorporating contextual surprisal as a
predictor results in only a marginal improvement in the model's predictive
accuracy. This finding suggests that surprisal theory struggles to explain
fine-grained eye movements.

</details>


### [104] [MIRAGE: A Benchmark for Multimodal Information-Seeking and Reasoning in Agricultural Expert-Guided Conversations](https://arxiv.org/abs/2506.20100)
*Vardhan Dongre,Chi Gui,Shubham Garg,Hooshang Nayyeri,Gokhan Tur,Dilek Hakkani-Tür,Vikram S. Adve*

Main category: cs.LG

TL;DR: MIRAGE is a new multimodal benchmark for expert-level reasoning in agriculture, featuring real-world user-expert interactions, image-based context, and diverse biological entities.


<details>
  <summary>Details</summary>
Motivation: To address the lack of benchmarks for grounded reasoning and long-form generation in knowledge-intensive domains like agriculture.

Method: Curated from 35,000 real interactions, MIRAGE combines natural queries, expert responses, and images, focusing on underspecified, open-world scenarios.

Result: A diverse benchmark with 7,000+ biological entities, enabling evaluation of models on rare entities, latent knowledge gaps, and interactive reasoning.

Conclusion: MIRAGE provides a high-fidelity, real-world benchmark for vision-language models, advancing research in expert-level multimodal reasoning.

Abstract: We introduce MIRAGE, a new benchmark for multimodal expert-level reasoning
and decision-making in consultative interaction settings. Designed for the
agriculture domain, MIRAGE captures the full complexity of expert consultations
by combining natural user queries, expert-authored responses, and image-based
context, offering a high-fidelity benchmark for evaluating models on grounded
reasoning, clarification strategies, and long-form generation in a real-world,
knowledge-intensive domain. Grounded in over 35,000 real user-expert
interactions and curated through a carefully designed multi-step pipeline,
MIRAGE spans diverse crop health, pest diagnosis, and crop management
scenarios. The benchmark includes more than 7,000 unique biological entities,
covering plant species, pests, and diseases, making it one of the most
taxonomically diverse benchmarks available for vision-language models, grounded
in the real world. Unlike existing benchmarks that rely on well-specified user
inputs and closed-set taxonomies, MIRAGE features underspecified, context-rich
scenarios with open-world settings, requiring models to infer latent knowledge
gaps, handle rare entities, and either proactively guide the interaction or
respond. Project Page: https://mirage-benchmark.github.io

</details>


### [105] [Position: Machine Learning Conferences Should Establish a "Refutations and Critiques" Track](https://arxiv.org/abs/2506.19882)
*Rylan Schaeffer,Joshua Kazdan,Yegor Denisov-Blanch,Brando Miranda,Matthias Gerstgrasser,Susan Zhang,Andreas Haupt,Isha Gupta,Elyas Obbad,Jesse Dodge,Jessica Zosa Forde,Koustuv Sinha,Francesco Orabona,Sanmi Koyejo,David Donoho*

Main category: cs.LG

TL;DR: The paper proposes a 'Refutations and Critiques' (R&C) Track at ML conferences to systematically correct errors and flawed studies, enhancing the field's self-correcting research ecosystem.


<details>
  <summary>Details</summary>
Motivation: Rapid ML advancements have led to flawed or fraudulent studies being accepted due to peer review fallibility, lacking mechanisms for correction.

Method: Advocates for an R&C Track, discussing its design, review principles, pitfalls, and provides an example submission.

Result: Highlights the need for reputable mechanisms to challenge and correct prior research.

Conclusion: ML conferences should establish R&C Tracks to foster a dynamic, self-correcting research environment.

Abstract: Science progresses by iteratively advancing and correcting humanity's
understanding of the world. In machine learning (ML) research, rapid
advancements have led to an explosion of publications, but have also led to
misleading, incorrect, flawed or perhaps even fraudulent studies being accepted
and sometimes highlighted at ML conferences due to the fallibility of peer
review. While such mistakes are understandable, ML conferences do not offer
robust processes to help the field systematically correct when such errors are
made.This position paper argues that ML conferences should establish a
dedicated "Refutations and Critiques" (R & C) Track. This R & C Track would
provide a high-profile, reputable platform to support vital research that
critically challenges prior research, thereby fostering a dynamic
self-correcting research ecosystem. We discuss key considerations including
track design, review principles, potential pitfalls, and provide an
illustrative example submission concerning a recent ICLR 2025 Oral. We conclude
that ML conferences should create official, reputable mechanisms to help ML
research self-correct.

</details>


### [106] [Counterfactual Influence as a Distributional Quantity](https://arxiv.org/abs/2506.20481)
*Matthieu Meeus,Igor Shilov,Georgios Kaissis,Yves-Alexandre de Montjoye*

Main category: cs.LG

TL;DR: The paper explores memorization in machine learning models beyond self-influence, emphasizing the role of near-duplicates and proposing the use of full influence distributions for better risk assessment.


<details>
  <summary>Details</summary>
Motivation: To address privacy and generalization concerns by understanding how memorization is influenced by interactions across training data, not just self-influence.

Method: Analyzes memorization by treating counterfactual influence as a distributional quantity, computing full influence distributions for a small language model and image classification (CIFAR-10).

Result: Self-influence alone underestimates memorization risks; near-duplicates reduce self-influence but remain extractable. Influence distributions reveal near-duplicates in CIFAR-10.

Conclusion: Memorization is better understood through full influence distributions, highlighting complex interactions in training data.

Abstract: Machine learning models are known to memorize samples from their training
data, raising concerns around privacy and generalization. Counterfactual
self-influence is a popular metric to study memorization, quantifying how the
model's prediction for a sample changes depending on the sample's inclusion in
the training dataset. However, recent work has shown memorization to be
affected by factors beyond self-influence, with other training samples, in
particular (near-)duplicates, having a large impact. We here study memorization
treating counterfactual influence as a distributional quantity, taking into
account how all training samples influence how a sample is memorized. For a
small language model, we compute the full influence distribution of training
samples on each other and analyze its properties. We find that solely looking
at self-influence can severely underestimate tangible risks associated with
memorization: the presence of (near-)duplicates seriously reduces
self-influence, while we find these samples to be (near-)extractable. We
observe similar patterns for image classification, where simply looking at the
influence distributions reveals the presence of near-duplicates in CIFAR-10.
Our findings highlight that memorization stems from complex interactions across
training data and is better captured by the full influence distribution than by
self-influence alone.

</details>


### [107] [Asymmetric REINFORCE for off-Policy Reinforcement Learning: Balancing positive and negative rewards](https://arxiv.org/abs/2506.20520)
*Charles Arnal,Gaëtan Narozniak,Vivien Cabannes,Yunhao Tang,Julia Kempe,Remi Munos*

Main category: cs.LG

TL;DR: The paper explores a simple off-policy REINFORCE algorithm for aligning large language models, balancing between off-policy RL and supervised fine-tuning. It provides theoretical guarantees and experimental validation.


<details>
  <summary>Details</summary>
Motivation: To address the suboptimal performance of off-policy RL methods in aligning LLMs, while maintaining simplicity and data efficiency.

Method: Analyzes an off-policy REINFORCE algorithm with a tunable baseline V, where advantage is defined as A=r-V. Theoretical analysis and experiments in bandit settings and LLM fine-tuning are conducted.

Result: Theoretical guarantee shows policy improvement when V lower-bounds expected reward. Off-policy updates benefit more from positive rewards.

Conclusion: The proposed off-policy REINFORCE algorithm effectively balances performance and simplicity, validated in theory and experiments.

Abstract: Reinforcement learning (RL) is increasingly used to align large language
models (LLMs). Off-policy methods offer greater implementation simplicity and
data efficiency than on-policy techniques, but often result in suboptimal
performance. In this work, we study the intermediate range of algorithms
between off-policy RL and supervised fine-tuning by analyzing a simple
off-policy REINFORCE algorithm, where the advantage is defined as $A=r-V$, with
$r$ a reward and $V$ some tunable baseline. Intuitively, lowering $V$
emphasizes high-reward samples, while raising it penalizes low-reward ones more
heavily. We first provide a theoretical analysis of this off-policy REINFORCE
algorithm, showing that when the baseline $V$ lower-bounds the expected reward,
the algorithm enjoys a policy improvement guarantee. Our analysis reveals that
while on-policy updates can safely leverage both positive and negative signals,
off-policy updates benefit from focusing more on positive rewards than on
negative ones. We validate our findings experimentally in a controlled
stochastic bandit setting and through fine-tuning state-of-the-art LLMs on
reasoning tasks.

</details>


### [108] [PLoP: Precise LoRA Placement for Efficient Finetuning of Large Models](https://arxiv.org/abs/2506.20629)
*Soufiane Hayou,Nikhil Ghosh,Bin Yu*

Main category: cs.LG

TL;DR: PLoP (Precise LoRA Placement) is a lightweight method for automatically identifying optimal LoRA adapter placements in pretrained models, outperforming manual strategies.


<details>
  <summary>Details</summary>
Motivation: Existing LoRA adapter placement strategies lack consensus and may not be optimal for specific tasks, prompting the need for an automated solution.

Method: PLoP uses intuitive theoretical analysis to determine the best module types (e.g., attention or MLP) for LoRA adapter placement in pretrained models.

Result: PLoP consistently outperforms or matches manual placement strategies in supervised finetuning and reinforcement learning for reasoning tasks.

Conclusion: PLoP provides an efficient, automated approach to LoRA adapter placement, enhancing task-specific adaptation of large models.

Abstract: Low-Rank Adaptation (LoRA) is a widely used finetuning method for large
models. Its small memory footprint allows practitioners to adapt large models
to specific tasks at a fraction of the cost of full finetuning. Different
modifications have been proposed to enhance its efficiency by, for example,
setting the learning rate, the rank, and the initialization. Another
improvement axis is adapter placement strategy: when using LoRA, practitioners
usually pick module types to adapt with LoRA, such as Query and Key modules.
Few works have studied the problem of adapter placement, with nonconclusive
results: original LoRA paper suggested placing adapters in attention modules,
while other works suggested placing them in the MLP modules. Through an
intuitive theoretical analysis, we introduce PLoP (Precise LoRA Placement), a
lightweight method that allows automatic identification of module types where
LoRA adapters should be placed, given a pretrained model and a finetuning task.
We demonstrate that PLoP consistently outperforms, and in the worst case
competes, with commonly used placement strategies through comprehensive
experiments on supervised finetuning and reinforcement learning for reasoning.

</details>


### [109] [Any-Order GPT as Masked Diffusion Model: Decoupling Formulation and Architecture](https://arxiv.org/abs/2506.19935)
*Shuchen Xue,Tianyu Xie,Tianyang Hu,Zijin Feng,Jiacheng Sun,Kenji Kawaguchi,Zhenguo Li,Zhi-Ming Ma*

Main category: cs.LG

TL;DR: The paper compares autoregressive (AR) and masked diffusion models (MDMs) by evaluating MDMs in a decoder-only framework, revealing trade-offs in speed and perplexity.


<details>
  <summary>Details</summary>
Motivation: To fairly compare AR and MDM paradigms by isolating architectural differences, as prior comparisons were confounded by simultaneous changes in paradigm and architecture.

Method: Evaluates MDMs within a decoder-only framework, comparing them as Any-Order AR (AO-AR) and standard AR, and investigates architectural influences (decoder-only vs. encoder-only) in MDMs.

Result: Decoder-only MDMs achieve significant generation speedups (~25×) and comparable perplexity with temperature annealing, despite modeling a larger space.

Conclusion: The work decouples paradigm differences from architectural influences, providing insights for future model design.

Abstract: Large language models (LLMs) predominantly use autoregressive (AR)
approaches, but masked diffusion models (MDMs) are emerging as viable
alternatives. A key challenge in comparing AR and MDM paradigms is their
typical architectural difference: AR models are often decoder-only, while MDMs
have largely been encoder-only. This practice of changing both the modeling
paradigm and architecture simultaneously makes direct comparisons unfair, as
it's hard to distinguish whether observed differences stem from the paradigm
itself or the architectural shift. This research evaluates MDMs within a
decoder-only framework to: (1) equitably compare MDM (as Any-Order AR, or
AO-AR) and standard AR paradigms. Our investigation suggests that the standard
AO-AR objective, which averages over all token permutations, may benefit from
refinement, as many permutations appear less informative compared to the
language's inherent left-to-right structure. (2) Investigate architectural
influences (decoder-only vs. encoder-only) within MDMs. We demonstrate that
while encoder-only MDMs model a simpler conditional probability space,
decoder-only MDMs can achieve dramatic generation speedups ($\sim25\times$) and
comparable perplexity with temperature annealing despite modeling a vastly
larger space, highlighting key trade-offs. This work thus decouples core
paradigm differences from architectural influences, offering insights for
future model design. Code is available at https://github.com/scxue/AO-GPT-MDM.

</details>


### [110] [FedBKD: Distilled Federated Learning to Embrace Gerneralization and Personalization on Non-IID Data](https://arxiv.org/abs/2506.20245)
*Yushan Zhao,Jinyuan He,Donglai Chen,Weijie Luo,Chong Xie,Ri Zhang,Yonghong Chen,Yan Xu*

Main category: cs.LG

TL;DR: FedBKD is a data-free distillation framework for federated learning, addressing non-IID data challenges by using GANs for synthetic data and bidirectional knowledge distillation.


<details>
  <summary>Details</summary>
Motivation: To solve the issues of non-IID data in FL while avoiding data leakage risks from public datasets.

Method: Uses GANs to generate synthetic data, with local models as frozen discriminators, and performs bidirectional distillation between global and local models.

Result: Achieves state-of-the-art performance on 4 benchmarks under various non-IID settings.

Conclusion: FedBKD effectively improves both global and local model performance without relying on public datasets.

Abstract: Federated learning (FL) is a decentralized collaborative machine learning
(ML) technique. It provides a solution to the issues of isolated data islands
and data privacy leakage in industrial ML practices. One major challenge in FL
is handling the non-identical and independent distributed (non-IID) data.
Current solutions either focus on constructing an all-powerful global model, or
customizing personalized local models. Few of them can provide both a
well-generalized global model and well-performed local models at the same time.
Additionally, many FL solutions to the non-IID problem are benefited from
introducing public datasets. However, this will also increase the risk of data
leakage. To tackle the problems, we propose a novel data-free distillation
framework, Federated Bidirectional Knowledge Distillation (FedBKD).
Specifically, we train Generative Adversarial Networks (GAN) for synthetic
data. During the GAN training, local models serve as discriminators and their
parameters are frozen. The synthetic data is then used for bidirectional
distillation between global and local models to achieve knowledge interactions
so that performances for both sides are improved. We conduct extensive
experiments on 4 benchmarks under different non-IID settings. The results show
that FedBKD achieves SOTA performances in every case.

</details>


### [111] [Learning Moderately Input-Sensitive Functions: A Case Study in QR Code Decoding](https://arxiv.org/abs/2506.20305)
*Kazuki Yoda,Kazuhiko Kawamoto,Hiroshi Kera*

Main category: cs.LG

TL;DR: Transformers can decode QR codes beyond theoretical limits by learning embedded text structure, generalizing across languages and ignoring error-correction bits.


<details>
  <summary>Details</summary>
Motivation: Explore learning functions of medium sensitivity, bridging input-insensitive (e.g., image classification) and input-sensitive tasks (e.g., arithmetic).

Method: Use Transformers to learn QR code decoding, testing generalization from English-rich data to other languages and random strings.

Result: Transformers decode QR codes beyond error-correction limits, focusing on data bits and ignoring error-correction bits.

Conclusion: Transformers offer a novel decoding mechanism for QR codes, demonstrating their capability in medium-sensitivity tasks.

Abstract: The hardness of learning a function that attains a target task relates to its
input-sensitivity. For example, image classification tasks are
input-insensitive as minor corruptions should not affect the classification
results, whereas arithmetic and symbolic computation, which have been recently
attracting interest, are highly input-sensitive as each input variable connects
to the computation results. This study presents the first learning-based Quick
Response (QR) code decoding and investigates learning functions of medium
sensitivity. Our experiments reveal that Transformers can successfully decode
QR codes, even beyond the theoretical error-correction limit, by learning the
structure of embedded texts. They generalize from English-rich training data to
other languages and even random strings. Moreover, we observe that the
Transformer-based QR decoder focuses on data bits while ignoring
error-correction bits, suggesting a decoding mechanism distinct from standard
QR code readers.

</details>

<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 85]
- [cs.CV](#cs.CV) [Total: 175]
- [cs.GR](#cs.GR) [Total: 8]
- [cs.LG](#cs.LG) [Total: 19]
- [cs.IR](#cs.IR) [Total: 4]
- [cs.AI](#cs.AI) [Total: 11]
- [cs.SD](#cs.SD) [Total: 5]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [cs.DL](#cs.DL) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.CY](#cs.CY) [Total: 4]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.RO](#cs.RO) [Total: 8]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]
- [stat.ML](#stat.ML) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [eess.IV](#eess.IV) [Total: 9]
- [cs.FL](#cs.FL) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Outcome-Based Education: Evaluating Students' Perspectives Using Transformer](https://arxiv.org/abs/2506.17223)
*Shuvra Smaran Das,Anirban Saha Anik,Md Kishor Morol,Mohammad Sakib Mahmood*

Main category: cs.CL

TL;DR: The study uses DistilBERT and LIME to analyze student feedback for Outcome-Based Education (OBE), improving sentiment classification and providing clear insights for educational improvement.


<details>
  <summary>Details</summary>
Motivation: To enhance OBE by leveraging NLP to analyze student feedback and improve measurable educational outcomes.

Method: Implemented DistilBERT for sentiment analysis and LIME for interpretability on student feedback data.

Result: Transformer models with LIME provide accurate, interpretable sentiment analysis, aligning with OBE goals.

Conclusion: The framework effectively supports OBE by offering data-driven insights for educational practice improvement.

Abstract: Outcome-Based Education (OBE) emphasizes the development of specific
competencies through student-centered learning. In this study, we reviewed the
importance of OBE and implemented transformer-based models, particularly
DistilBERT, to analyze an NLP dataset that includes student feedback. Our
objective is to assess and improve educational outcomes. Our approach is better
than other machine learning models because it uses the transformer's deep
understanding of language context to classify sentiment better, giving better
results across a wider range of matrices. Our work directly contributes to
OBE's goal of achieving measurable outcomes by facilitating the identification
of patterns in student learning experiences. We have also applied LIME (local
interpretable model-agnostic explanations) to make sure that model predictions
are clear. This gives us understandable information about how key terms affect
sentiment. Our findings indicate that the combination of transformer models and
LIME explanations results in a strong and straightforward framework for
analyzing student feedback. This aligns more closely with the principles of OBE
and ensures the improvement of educational practices through data-driven
insights.

</details>


### [2] [Efficient and Stealthy Jailbreak Attacks via Adversarial Prompt Distillation from LLMs to SLMs](https://arxiv.org/abs/2506.17231)
*Xiang Li,Chong Zhang,Jia Wang,Fangyu Wu,Yushi Li,Xiaobo Jin*

Main category: cs.CL

TL;DR: Proposes Adversarial Prompt Distillation to enable small language models (SLMs) to jailbreak large language models (LLMs) efficiently, addressing low efficiency, high cost, and poor adaptability issues.


<details>
  <summary>Details</summary>
Motivation: Address security and ethical issues in LLM jailbreaking by overcoming inefficiencies, high costs, and poor adaptability of current methods.

Method: Combines masked language modeling, reinforcement learning, and dynamic temperature control via prompt generation and distillation.

Result: Superior attack success rate, harm effectiveness, resource efficiency, and cross-model adaptability demonstrated experimentally.

Conclusion: Reveals LLM vulnerabilities, explores SLM jailbreak feasibility, and advances LLM security research.

Abstract: Attacks on large language models (LLMs) in jailbreaking scenarios raise many
security and ethical issues. Current jailbreak attack methods face problems
such as low efficiency, high computational cost, and poor cross-model
adaptability and versatility, which make it difficult to cope with the rapid
development of LLM and new defense strategies. Our work proposes an Adversarial
Prompt Distillation, which combines masked language modeling, reinforcement
learning, and dynamic temperature control through a prompt generation and
distillation method. It enables small language models (SLMs) to jailbreak
attacks on mainstream LLMs. The experimental results verify the superiority of
the proposed method in terms of attack success rate and harm, and reflect the
resource efficiency and cross-model adaptability. This research explores the
feasibility of distilling the jailbreak ability of LLM to SLM, reveals the
model's vulnerability, and provides a new idea for LLM security research.

</details>


### [3] [GTA: Grouped-head latenT Attention](https://arxiv.org/abs/2506.17286)
*Luoyang Sun,Jiwen Jiang,Cheng Deng,Xinjian Wu,Haifeng Zhang,Lei Chen,Lionel Ni,Jun Wang*

Main category: cs.CL

TL;DR: GTA (Grouped-Head Latent Attention) reduces memory and computational overhead in LLMs by compressing KV cache and reusing attention scores, achieving 2x faster inference.


<details>
  <summary>Details</summary>
Motivation: Attention mechanisms in LLMs are computationally expensive and memory-intensive, especially with long texts, due to redundant KV cache and similar attention maps.

Method: GTA introduces shared attention maps for multiple heads and a nonlinear value decoder to compress the value cache, reducing FLOPs and KV cache size.

Result: GTA reduces attention FLOPs by 62.5% and KV cache by 70%, improving inference speed by 2x without performance loss.

Conclusion: GTA efficiently optimizes LLM deployment by minimizing unnecessary computation and storage while maintaining model performance.

Abstract: Attention mechanisms underpin the success of large language models (LLMs),
yet their substantial computational and memory overhead poses challenges for
optimizing efficiency and performance. A critical bottleneck arises as KV cache
and attention computations scale rapidly with text length, challenging
deployment on hardware with limited computational and memory resources. We
observe that attention mechanisms exhibit substantial redundancy, since the KV
cache can be significantly compressed and attention maps across heads display
high similarity, revealing that much of the computation and storage is
unnecessary. Leveraging these insights, we propose \textbf{G}rouped-Head
Laten\textbf{T} \textbf{A}ttention (GTA), a novel attention mechanism that
reduces memory usage and computational complexity while maintaining
performance. GTA comprises two components: (1) a shared attention map mechanism
that reuses attention scores across multiple heads, decreasing the key cache
size; and (2) a nonlinear value decoder with learned projections that
compresses the value cache into a latent space, further cutting memory needs.
GTA cuts attention computation FLOPs by up to \emph{62.5\%} versus
Grouped-Query Attention and shrink the KV cache by up to \emph{70\%}, all while
avoiding the extra overhead of Multi-Head Latent Attention to improve LLM
deployment efficiency. Consequently, GTA models achieve a \emph{2x} increase in
end-to-end inference speed, with prefill benefiting from reduced computational
cost and decoding benefiting from the smaller cache footprint.

</details>


### [4] [AI-Generated Game Commentary: A Survey and a Datasheet Repository](https://arxiv.org/abs/2506.17294)
*Qirui Zheng,Xingbo Wang,Keyuan Cheng,Yunlong Lu,Wenxin Li*

Main category: cs.CL

TL;DR: The paper introduces a framework for AI-Generated Game Commentary (AIGGC), surveys 45 datasets and methods, and provides evaluation metrics and a structured datasheet for future research.


<details>
  <summary>Details</summary>
Motivation: AIGGC is gaining attention due to its market potential and technical challenges, requiring advanced NLP capabilities.

Method: The paper presents a general framework for AIGGC, surveys existing datasets and methods, and classifies evaluation metrics.

Result: A comprehensive survey of 45 datasets and methods is provided, along with a structured datasheet for benchmarking.

Conclusion: The paper supports future research by offering a framework, survey, and open repository for AIGGC datasets.

Abstract: AI-Generated Game Commentary (AIGGC) has gained increasing attention due to
its market potential and inherent technical challenges. As a comprehensive
multimodal Natural Language Processing (NLP) task, AIGGC imposes substantial
demands on language models, including factual accuracy, logical reasoning,
expressive text generation, generation speed, and context management. In this
paper, we introduce a general framework for AIGGC and present a comprehensive
survey of 45 existing game commentary dataset and methods according to key
challenges they aim to address in this domain. We further classify and compare
various evaluation metrics commonly used in this domain. To support future
research and benchmarking, we also provide a structured datasheet summarizing
the essential attributes of these datasets in appendix, which is meanwhile
publicly available in an open repository.

</details>


### [5] [Semantic uncertainty in advanced decoding methods for LLM generation](https://arxiv.org/abs/2506.17296)
*Darius Foodeei,Simin Fan,Martin Jaggi*

Main category: cs.CL

TL;DR: The study explores how different decoding methods (e.g., speculative sampling, CoT) impact semantic uncertainty in LLM outputs, showing structured methods like CoT improve diversity and reliability without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: To understand the trade-offs between semantic diversity and reliability in LLM outputs under various decoding strategies, addressing practical deployment needs.

Method: Experiments on question answering, summarization, and code generation tasks using techniques like speculative sampling and CoT decoding.

Result: CoT decoding increases semantic diversity with lower predictive entropy, improving code generation Pass@2 by 48.8%. Speculative sampling boosts summarization ROUGE scores.

Conclusion: Structured decoding methods can enhance semantic exploration while maintaining output quality, challenging traditional diversity-accuracy trade-offs in LLMs.

Abstract: This study investigates semantic uncertainty in large language model (LLM)
outputs across different decoding methods, focusing on emerging techniques like
speculative sampling and chain-of-thought (CoT) decoding. Through experiments
on question answering, summarization, and code generation tasks, we analyze how
different decoding strategies affect both the diversity and reliability of
model outputs. Our findings reveal that while CoT decoding demonstrates higher
semantic diversity, it maintains lower predictive entropy, suggesting that
structured exploration can lead to more confident and accurate outputs. This is
evidenced by a 48.8% improvement in code generation Pass@2 rates, despite lower
alignment with reference solutions. For summarization tasks, speculative
sampling proved particularly effective, achieving superior ROUGE scores while
maintaining moderate semantic diversity. Our results challenge conventional
assumptions about trade-offs between diversity and accuracy in language model
outputs, demonstrating that properly structured decoding methods can increase
semantic exploration while maintaining or improving output quality. These
findings have significant implications for deploying language models in
practical applications where both reliability and diverse solution generation
are crucial.

</details>


### [6] [Mercury: Ultra-Fast Language Models Based on Diffusion](https://arxiv.org/abs/2506.17298)
*Inception Labs,Samar Khanna,Siddhant Kharbanda,Shufan Li,Harshit Varma,Eric Wang,Sawyer Birnbaum,Ziyang Luo,Yanis Miraoui,Akash Palrecha,Stefano Ermon,Aditya Grover,Volodymyr Kuleshov*

Main category: cs.CL

TL;DR: Mercury introduces diffusion-based LLMs for coding, achieving state-of-the-art speed and quality.


<details>
  <summary>Details</summary>
Motivation: To advance the speed-quality frontier in coding applications with diffusion-based LLMs.

Method: Transformer-based diffusion models trained to predict multiple tokens in parallel.

Result: Mercury Coder models achieve 1109 and 737 tokens/sec on H100 GPUs, outperforming competitors by 10x in speed while maintaining quality.

Conclusion: Mercury Coder sets a new benchmark for speed and quality in coding LLMs, validated by real-world performance.

Abstract: We present Mercury, a new generation of commercial-scale large language
models (LLMs) based on diffusion. These models are parameterized via the
Transformer architecture and trained to predict multiple tokens in parallel. In
this report, we detail Mercury Coder, our first set of diffusion LLMs designed
for coding applications. Currently, Mercury Coder comes in two sizes: Mini and
Small. These models set a new state-of-the-art on the speed-quality frontier.
Based on independent evaluations conducted by Artificial Analysis, Mercury
Coder Mini and Mercury Coder Small achieve state-of-the-art throughputs of 1109
tokens/sec and 737 tokens/sec, respectively, on NVIDIA H100 GPUs and outperform
speed-optimized frontier models by up to 10x on average while maintaining
comparable quality. We discuss additional results on a variety of code
benchmarks spanning multiple languages and use-cases as well as real-world
validation by developers on Copilot Arena, where the model currently ranks
second on quality and is the fastest model overall. We also release a public
API at https://platform.inceptionlabs.ai/ and free playground at
https://chat.inceptionlabs.ai

</details>


### [7] [PRAISE: Enhancing Product Descriptions with LLM-Driven Structured Insights](https://arxiv.org/abs/2506.17314)
*Adnan Qidwai,Srija Mukhopadhyay,Prerana Khatiwada,Dan Roth,Vivek Gupta*

Main category: cs.CL

TL;DR: PRAISE is a system using LLMs to extract and compare insights from customer reviews and seller descriptions, helping improve e-commerce product listings.


<details>
  <summary>Details</summary>
Motivation: Seller-provided product descriptions are often incomplete, and manually sifting through reviews is laborious.

Method: Uses Large Language Models (LLMs) to automatically extract, compare, and structure insights from reviews and descriptions, presenting discrepancies in a clear format.

Result: Generates actionable structured insights, improving product listing quality and trustworthiness.

Conclusion: PRAISE effectively bridges gaps between seller descriptions and customer reviews, enhancing e-commerce catalog reliability.

Abstract: Accurate and complete product descriptions are crucial for e-commerce, yet
seller-provided information often falls short. Customer reviews offer valuable
details but are laborious to sift through manually. We present PRAISE: Product
Review Attribute Insight Structuring Engine, a novel system that uses Large
Language Models (LLMs) to automatically extract, compare, and structure
insights from customer reviews and seller descriptions. PRAISE provides users
with an intuitive interface to identify missing, contradictory, or partially
matching details between these two sources, presenting the discrepancies in a
clear, structured format alongside supporting evidence from reviews. This
allows sellers to easily enhance their product listings for clarity and
persuasiveness, and buyers to better assess product reliability. Our
demonstration showcases PRAISE's workflow, its effectiveness in generating
actionable structured insights from unstructured reviews, and its potential to
significantly improve the quality and trustworthiness of e-commerce product
catalogs.

</details>


### [8] [Towards Safety Evaluations of Theory of Mind in Large Language Models](https://arxiv.org/abs/2506.17352)
*Tatsuhiro Aoshima,Mitsuaki Akiyama*

Main category: cs.CL

TL;DR: The paper highlights the need for safety evaluation of LLMs due to deceptive behaviors and proposes measuring their theory of mind capabilities, finding limited progress despite improvements in reading comprehension.


<details>
  <summary>Details</summary>
Motivation: Concerns about LLMs disabling oversight mechanisms and acting deceptively, necessitating investigation into whether such behaviors are intentional.

Method: Reviewing theory of mind research, analyzing developmental trends in LLMs, and evaluating their safety-related capabilities.

Result: LLMs show improved reading comprehension but lack comparable development in theory of mind, impacting safety evaluation.

Conclusion: The study underscores the gap in LLMs' theory of mind development and identifies challenges for future safety evaluations.

Abstract: As the capabilities of large language models (LLMs) continue to advance, the
importance of rigorous safety evaluation is becoming increasingly evident.
Recent concerns within the realm of safety assessment have highlighted
instances in which LLMs exhibit behaviors that appear to disable oversight
mechanisms and respond in a deceptive manner. For example, there have been
reports suggesting that, when confronted with information unfavorable to their
own persistence during task execution, LLMs may act covertly and even provide
false answers to questions intended to verify their behavior.To evaluate the
potential risk of such deceptive actions toward developers or users, it is
essential to investigate whether these behaviors stem from covert, intentional
processes within the model. In this study, we propose that it is necessary to
measure the theory of mind capabilities of LLMs. We begin by reviewing existing
research on theory of mind and identifying the perspectives and tasks relevant
to its application in safety evaluation. Given that theory of mind has been
predominantly studied within the context of developmental psychology, we
analyze developmental trends across a series of open-weight LLMs. Our results
indicate that while LLMs have improved in reading comprehension, their theory
of mind capabilities have not shown comparable development. Finally, we present
the current state of safety evaluation with respect to LLMs' theory of mind,
and discuss remaining challenges for future work.

</details>


### [9] [Cash or Comfort? How LLMs Value Your Inconvenience](https://arxiv.org/abs/2506.17367)
*Mateusz Cedro,Timour Ichmoukhamedov,Sofie Goethals,Yifan He,James Hinns,David Martens*

Main category: cs.CL

TL;DR: The paper explores how LLMs value user discomfort in financial trade-offs, revealing inconsistencies and ethical concerns in their decision-making.


<details>
  <summary>Details</summary>
Motivation: To understand LLMs' behavior in personal decision-making, especially when financial rewards conflict with user comfort.

Method: Quantify the prices assigned by multiple LLMs to user discomforts like walking, waiting, hunger, and pain.

Result: LLMs show large variance in responses, fragility to prompt phrasing, unreasonable reward acceptance, and rejection of gains without discomfort.

Conclusion: Current LLMs are unreliable for autonomous decision-making in cash-versus-comfort scenarios, necessitating further scrutiny.

Abstract: Large Language Models (LLMs) are increasingly proposed as near-autonomous
artificial intelligence (AI) agents capable of making everyday decisions on
behalf of humans. Although LLMs perform well on many technical tasks, their
behaviour in personal decision-making remains less understood. Previous studies
have assessed their rationality and moral alignment with human decisions.
However, the behaviour of AI assistants in scenarios where financial rewards
are at odds with user comfort has not yet been thoroughly explored. In this
paper, we tackle this problem by quantifying the prices assigned by multiple
LLMs to a series of user discomforts: additional walking, waiting, hunger and
pain. We uncover several key concerns that strongly question the prospect of
using current LLMs as decision-making assistants: (1) a large variance in
responses between LLMs, (2) within a single LLM, responses show fragility to
minor variations in prompt phrasing (e.g., reformulating the question in the
first person can considerably alter the decision), (3) LLMs can accept
unreasonably low rewards for major inconveniences (e.g., 1 Euro to wait 10
hours), and (4) LLMs can reject monetary gains where no discomfort is imposed
(e.g., 1,000 Euro to wait 0 minutes). These findings emphasize the need for
scrutiny of how LLMs value human inconvenience, particularly as we move toward
applications where such cash-versus-comfort trade-offs are made on users'
behalf.

</details>


### [10] [Leveraging LLMs to Assess Tutor Moves in Real-Life Dialogues: A Feasibility Study](https://arxiv.org/abs/2506.17410)
*Danielle R. Thomas,Conrad Borchers,Jionghao Lin,Sanjit Kakarla,Shambhavi Bhushan,Erin Gatz,Shivang Gupta,Ralph Abboud,Kenneth R. Koedinger*

Main category: cs.CL

TL;DR: The study explores using generative AI (GPT-4, Gemini-1.5-pro, LearnLM) to analyze tutor moves in math tutoring, achieving high accuracy in detecting praise and error responses, and aligning with human judgments.


<details>
  <summary>Details</summary>
Motivation: To identify scalable methods for evaluating tutoring actions using AI, addressing the challenge of analyzing real-life tutoring interactions.

Method: Analyzed 50 math tutoring transcripts with generative AI models to detect tutor skills (praise and error response) and compared results with human judgments.

Result: AI models achieved high accuracy (94-98% for praise, 82-88% for errors) and aligned with human evaluations (83-89% and 73-77%).

Conclusion: Generative AI is feasible for scalable tutoring assessment, with proposed cost-effective prompting strategies for practical use.

Abstract: Tutoring improves student achievement, but identifying and studying what
tutoring actions are most associated with student learning at scale based on
audio transcriptions is an open research problem. This present study
investigates the feasibility and scalability of using generative AI to identify
and evaluate specific tutor moves in real-life math tutoring. We analyze 50
randomly selected transcripts of college-student remote tutors assisting middle
school students in mathematics. Using GPT-4, GPT-4o, GPT-4-turbo,
Gemini-1.5-pro, and LearnLM, we assess tutors' application of two tutor skills:
delivering effective praise and responding to student math errors. All models
reliably detected relevant situations, for example, tutors providing praise to
students (94-98% accuracy) and a student making a math error (82-88% accuracy)
and effectively evaluated the tutors' adherence to tutoring best practices,
aligning closely with human judgments (83-89% and 73-77%, respectively). We
propose a cost-effective prompting strategy and discuss practical implications
for using large language models to support scalable assessment in authentic
settings. This work further contributes LLM prompts to support reproducibility
and research in AI-supported learning.

</details>


### [11] [UProp: Investigating the Uncertainty Propagation of LLMs in Multi-Step Agentic Decision-Making](https://arxiv.org/abs/2506.17419)
*Jinhao Duan,James Diffenderfer,Sandeep Madireddy,Tianlong Chen,Bhavya Kailkhura,Kaidi Xu*

Main category: cs.CL

TL;DR: The paper introduces UProp, a framework for quantifying uncertainty in LLM sequential decision-making, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To address the gap in uncertainty quantification for multi-step LLM decision-making, crucial for safety-critical applications.

Method: Proposes UProp, an information-theoretic framework decomposing uncertainty into intrinsic and extrinsic (Mutual Information) parts, estimating PMI over TDPs.

Result: UProp significantly outperforms single-turn UQ baselines on benchmarks like AgentBench and HotpotQA with models like GPT-4.1 and DeepSeek-V3.

Conclusion: UProp is effective for sequential decision-making uncertainty, with potential applications and efficient sampling demonstrated.

Abstract: As Large Language Models (LLMs) are integrated into safety-critical
applications involving sequential decision-making in the real world, it is
essential to know when to trust LLM decisions. Existing LLM Uncertainty
Quantification (UQ) methods are primarily designed for single-turn
question-answering formats, resulting in multi-step decision-making scenarios,
e.g., LLM agentic system, being underexplored. In this paper, we introduce a
principled, information-theoretic framework that decomposes LLM sequential
decision uncertainty into two parts: (i) internal uncertainty intrinsic to the
current decision, which is focused on existing UQ methods, and (ii) extrinsic
uncertainty, a Mutual-Information (MI) quantity describing how much uncertainty
should be inherited from preceding decisions. We then propose UProp, an
efficient and effective extrinsic uncertainty estimator that converts the
direct estimation of MI to the estimation of Pointwise Mutual Information (PMI)
over multiple Trajectory-Dependent Decision Processes (TDPs). UProp is
evaluated over extensive multi-step decision-making benchmarks, e.g.,
AgentBench and HotpotQA, with state-of-the-art LLMs, e.g., GPT-4.1 and
DeepSeek-V3. Experimental results demonstrate that UProp significantly
outperforms existing single-turn UQ baselines equipped with thoughtful
aggregation strategies. Moreover, we provide a comprehensive analysis of UProp,
including sampling efficiency, potential applications, and intermediate
uncertainty propagation, to demonstrate its effectiveness. Codes will be
available at https://github.com/jinhaoduan/UProp.

</details>


### [12] [Beyond the Link: Assessing LLMs' ability to Classify Political Content across Global Media](https://arxiv.org/abs/2506.17435)
*Alberto Martinez-Serra,Alejandro De La Fuente,Nienke Viescher,Ana S. Cardenal*

Main category: cs.CL

TL;DR: The paper evaluates whether large language models (LLMs) can accurately classify political content (PC) from URLs alone, comparing performance across models and languages.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in understanding LLMs' effectiveness in classifying PC from URLs, especially across diverse linguistic and national contexts.

Method: Uses advanced LLMs (GPT, Llama, Mistral, etc.) to classify PC from URLs and text, comparing results with human labels and traditional ML techniques.

Result: URLs can embed most news content, offering a balance between accuracy and cost, though contextual limitations exist.

Conclusion: LLMs show promise for PC classification from URLs, with recommendations for their use in political science studies.

Abstract: The use of large language models (LLMs) is becoming common in the context of
political science, particularly in studies that analyse individuals use of
digital media. However, while previous research has demonstrated LLMs ability
at labelling tasks, the effectiveness of using LLMs to classify political
content (PC) from just URLs is not yet well explored. The work presented in
this article bridges this gap by evaluating whether LLMs can accurately
identify PC vs. non-PC from both the article text and the URLs from five
countries (France, Germany, Spain, the UK, and the US) and different languages.
Using cutting-edge LLMs like GPT, Llama, Mistral, Deepseek, Qwen and Gemma, we
measure model performance to assess whether URL-level analysis can be a good
approximation for full-text analysis of PC, even across different linguistic
and national contexts. Model outputs are compared with human-labelled articles,
as well as traditional supervised machine learning techniques, to set a
baseline of performance. Overall, our findings suggest the capacity of URLs to
embed most of the news content, providing a vital perspective on accuracy-cost
balancing. We also account for contextual limitations and suggest
methodological recommendations to use LLMs within political science studies.

</details>


### [13] [Breaking the Transcription Bottleneck: Fine-tuning ASR Models for Extremely Low-Resource Fieldwork Languages](https://arxiv.org/abs/2506.17459)
*Siyu Liang,Gina-Anne Levow*

Main category: cs.CL

TL;DR: Benchmarking MMS and XLS-R ASR models for low-resource languages shows MMS excels with minimal data, while XLS-R matches performance with over one hour of training.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of ASR in linguistic fieldwork due to challenges like spontaneous speech, noise, and small datasets from under-documented languages.

Method: Fine-tuned multilingual ASR models (MMS and XLS-R) on five typologically diverse low-resource languages, controlling training data duration.

Result: MMS performs best with very small datasets; XLS-R achieves parity with over one hour of training data.

Conclusion: Provides practical guidelines for field linguists, emphasizing reproducible ASR adaptation to ease transcription in language documentation.

Abstract: Automatic Speech Recognition (ASR) has reached impressive accuracy for
high-resource languages, yet its utility in linguistic fieldwork remains
limited. Recordings collected in fieldwork contexts present unique challenges,
including spontaneous speech, environmental noise, and severely constrained
datasets from under-documented languages. In this paper, we benchmark the
performance of two fine-tuned multilingual ASR models, MMS and XLS-R, on five
typologically diverse low-resource languages with control of training data
duration. Our findings show that MMS is best suited when extremely small
amounts of training data are available, whereas XLS-R shows parity performance
once training data exceed one hour. We provide linguistically grounded analysis
for further provide insights towards practical guidelines for field linguists,
highlighting reproducible ASR adaptation approaches to mitigate the
transcription bottleneck in language documentation.

</details>


### [14] [Computational Approaches to Understanding Large Language Model Impact on Writing and Information Ecosystems](https://arxiv.org/abs/2506.17467)
*Weixin Liang*

Main category: cs.CL

TL;DR: This dissertation explores the societal impact of LLMs, focusing on biases in AI detectors, widespread adoption in writing domains, and their potential for manuscript feedback.


<details>
  <summary>Details</summary>
Motivation: To understand how individuals and institutions adapt to LLMs and address equity and accessibility challenges.

Method: Three research directions: analyzing AI detector biases, measuring LLM adoption in writing domains, and evaluating LLMs for manuscript feedback.

Result: Reveals biases in AI detectors, widespread LLM adoption in writing, and LLMs' potential to aid researchers with feedback.

Conclusion: LLMs are transformative but require governance to address biases and enhance accessibility.

Abstract: Large language models (LLMs) have shown significant potential to change how
we write, communicate, and create, leading to rapid adoption across society.
This dissertation examines how individuals and institutions are adapting to and
engaging with this emerging technology through three research directions.
First, I demonstrate how the institutional adoption of AI detectors introduces
systematic biases, particularly disadvantaging writers of non-dominant language
varieties, highlighting critical equity concerns in AI governance. Second, I
present novel population-level algorithmic approaches that measure the
increasing adoption of LLMs across writing domains, revealing consistent
patterns of AI-assisted content in academic peer reviews, scientific
publications, consumer complaints, corporate communications, job postings, and
international organization press releases. Finally, I investigate LLMs'
capability to provide feedback on research manuscripts through a large-scale
empirical analysis, offering insights into their potential to support
researchers who face barriers in accessing timely manuscript feedback,
particularly early-career researchers and those from under-resourced settings.

</details>


### [15] [VeriLocc: End-to-End Cross-Architecture Register Allocation via LLM](https://arxiv.org/abs/2506.17506)
*Lesheng Jin,Zhenyuan Ruan,Haohui Mai,Jingbo Shang*

Main category: cs.CL

TL;DR: VeriLocc combines LLMs and formal compiler techniques for generalizable, verifiable GPU register allocation, outperforming hand-tuned methods.


<details>
  <summary>Details</summary>
Motivation: Hand-crafted register allocation heuristics in compilers require re-tuning for each GPU generation, lacking generalizability and verifiability.

Method: VeriLocc fine-tunes an LLM to translate MIRs into register assignments, using static analysis and verifier-guided regeneration for correctness.

Result: Achieves 85-99% single-shot accuracy, near-100% pass@100, and outperforms rocBLAS by over 10% in runtime.

Conclusion: VeriLocc provides a scalable, verifiable solution for GPU register allocation, surpassing expert-tuned methods.

Abstract: Modern GPUs evolve rapidly, yet production compilers still rely on
hand-crafted register allocation heuristics that require substantial re-tuning
for each hardware generation. We introduce VeriLocc, a framework that combines
large language models (LLMs) with formal compiler techniques to enable
generalizable and verifiable register allocation across GPU architectures.
VeriLocc fine-tunes an LLM to translate intermediate representations (MIRs)
into target-specific register assignments, aided by static analysis for
cross-architecture normalization and generalization and a verifier-guided
regeneration loop to ensure correctness. Evaluated on matrix multiplication
(GEMM) and multi-head attention (MHA), VeriLocc achieves 85-99% single-shot
accuracy and near-100% pass@100. Case study shows that VeriLocc discovers more
performant assignments than expert-tuned libraries, outperforming rocBLAS by
over 10% in runtime.

</details>


### [16] [Data Quality Issues in Multilingual Speech Datasets: The Need for Sociolinguistic Awareness and Proactive Language Planning](https://arxiv.org/abs/2506.17525)
*Mingfei Lau,Qian Chen,Yeming Fang,Tingting Xu,Tongzhou Chen,Pavel Golik*

Main category: cs.CL

TL;DR: The paper audits quality issues in multilingual speech datasets (Mozilla Common Voice 17.0, FLEURS, VoxPopuli), highlighting macro-level problems in under-resourced languages and proposing guidelines for improvement.


<details>
  <summary>Details</summary>
Motivation: To identify and address quality issues in public multilingual speech datasets to enhance their utility for training and evaluation, improving downstream models.

Method: Conducted a quality audit, categorizing issues into micro-level and macro-level, with a case analysis of Taiwanese Southern Min (nan_tw).

Result: Found macro-level issues more prevalent in under-resourced languages, emphasizing the need for proactive language planning and better data quality control.

Conclusion: Proposes guidelines for future dataset development, stressing sociolinguistic awareness to create robust speech data resources.

Abstract: Our quality audit for three widely used public multilingual speech datasets -
Mozilla Common Voice 17.0, FLEURS, and VoxPopuli - shows that in some
languages, these datasets suffer from significant quality issues. We believe
addressing these issues will make these datasets more useful as training and
evaluation sets, and improve downstream models. We divide these quality issues
into two categories: micro-level and macro-level. We find that macro-level
issues are more prevalent in less institutionalized, often under-resourced
languages. We provide a case analysis of Taiwanese Southern Min (nan_tw) that
highlights the need for proactive language planning (e.g. orthography
prescriptions, dialect boundary definition) and enhanced data quality control
in the process of Automatic Speech Recognition (ASR) dataset creation. We
conclude by proposing guidelines and recommendations to mitigate these issues
in future dataset development, emphasizing the importance of sociolinguistic
awareness in creating robust and reliable speech data resources.

</details>


### [17] [DuaShepherd: Integrating Stepwise Correctness and Potential Rewards for Mathematical Reasoning](https://arxiv.org/abs/2506.17533)
*Yuanhao Wu,Juntong Song,Hanning Zhang,Tong Zhang,Cheng Niu*

Main category: cs.CL

TL;DR: DuaShepherd integrates correctness and potential reward signals to improve LLMs' mathematical reasoning, outperforming single-signal models.


<details>
  <summary>Details</summary>
Motivation: Enhancing LLMs' mathematical reasoning by combining stepwise error identification (correctness) and likelihood of correct final answers (potential).

Method: Developed an automated pipeline for dataset construction and a multi-head architecture to train reward models for correctness and potential in parallel.

Result: Achieved state-of-the-art performance on MATH500 and ProcessBench, outperforming single-signal models.

Conclusion: Combining correctness and potential signals into a compound probability significantly improves LLMs' mathematical reasoning.

Abstract: In this paper, we propose DuaShepherd, a novel reward modeling framework that
integrates two complementary reward signals, correctness and potential, to
enhance the mathematical reasoning capabilities of Large Language Models
(LLMs). While correctness-based signals emphasize identification of stepwise
errors, potential-based signals focus on the likelihood of reaching the correct
final answer. We developed an automated pipeline for constructing large-scale
reward modeling dataset with both signals. A unified, multi-head architecture
was explored to train the two reward models in a multi-task setup,
demonstrating benefits from learning both correctness and potential in
parallel. By combining these two signals into a compound probability, our model
achieves consistent performance improvements across multiple benchmarks.
Empirical evaluations on MATH500 and ProcessBench confirm that this combined
reward significantly outperforms models trained on either reward type alone,
achieving state-of-the-art performance under comparable resource constraints.

</details>


### [18] [Probing for Phonology in Self-Supervised Speech Representations: A Case Study on Accent Perception](https://arxiv.org/abs/2506.17542)
*Nitin Venkateswaran,Kevin Tang,Ratree Wayland*

Main category: cs.CL

TL;DR: The paper explores how self-supervised learning (SSL) models encode phonological features affecting accent perception, focusing on segments like the labiodental approximant, rhotic tap, and retroflex stop in non-native English speakers. Results show SSL representations predict accent strength effectively.


<details>
  <summary>Details</summary>
Motivation: Traditional models overlook gradient phonological variations in accent perception. This study aims to bridge this gap using SSL models to analyze how these variations influence judgments.

Method: The study uses the CSLU Foreign Accented English corpus, extracting phonological feature probabilities with Phonet and pretrained representations from Wav2Vec2-BERT and WavLM. Native speaker accent judgments and probing analyses are employed.

Result: Accent strength is best predicted by specific SSL representation features, with perceptually salient phonological features playing a key role. Regression shows strong associations between accent ratings and segment distances from baselines.

Conclusion: SSL speech representations are valuable for modeling accent perception through interpretable phonological features, offering insights beyond traditional models.

Abstract: Traditional models of accent perception underestimate the role of gradient
variations in phonological features which listeners rely upon for their accent
judgments. We investigate how pretrained representations from current
self-supervised learning (SSL) models of speech encode phonological
feature-level variations that influence the perception of segmental accent. We
focus on three segments: the labiodental approximant, the rhotic tap, and the
retroflex stop, which are uniformly produced in the English of native speakers
of Hindi as well as other languages in the Indian sub-continent. We use the
CSLU Foreign Accented English corpus (Lander, 2007) to extract, for these
segments, phonological feature probabilities using Phonet (V\'asquez-Correa et
al., 2019) and pretrained representations from Wav2Vec2-BERT (Barrault et al.,
2023) and WavLM (Chen et al., 2022) along with accent judgements by native
speakers of American English. Probing analyses show that accent strength is
best predicted by a subset of the segment's pretrained representation features,
in which perceptually salient phonological features that contrast the expected
American English and realized non-native English segments are given prominent
weighting. A multinomial logistic regression of pretrained representation-based
segment distances from American and Indian English baselines on accent ratings
reveals strong associations between the odds of accent strength and distances
from the baselines, in the expected directions. These results highlight the
value of self-supervised speech representations for modeling accent perception
using interpretable phonological features.

</details>


### [19] [AgriCHN: A Comprehensive Cross-domain Resource for Chinese Agricultural Named Entity Recognition](https://arxiv.org/abs/2506.17578)
*Lingxiao Zeng,Yiqi Tong,Wei Guo,Huarui Wu,Lihao Ge,Yijun Ye,Fuzhen Zhuang,Deqing Wang,Wei Guo,Cheng Chen*

Main category: cs.CL

TL;DR: AgriCHN is a high-quality Chinese dataset for agricultural named entity recognition (NER), addressing gaps in existing resources by including diverse entities like hydrology and meteorology. It outperforms others in quality and granularity, posing challenges for state-of-the-art NER models.


<details>
  <summary>Details</summary>
Motivation: Existing agricultural NER datasets lack diversity and quality, especially in Chinese, and ignore correlations with hydrology and meteorology. AgriCHN aims to fill this gap.

Method: AgriCHN is curated from agricultural articles, containing 4,040 sentences and 15,799 entity mentions across 27 categories, including hydrology and meteorology.

Result: AgriCHN shows superior data quality with richer entity types and finer divisions. Benchmark tests reveal its challenge to advanced NER models.

Conclusion: AgriCHN is a valuable resource for improving agricultural NER, offering diversity and quality, and encouraging further research in the field.

Abstract: Agricultural named entity recognition is a specialized task focusing on
identifying distinct agricultural entities within vast bodies of text,
including crops, diseases, pests, and fertilizers. It plays a crucial role in
enhancing information extraction from extensive agricultural text resources.
However, the scarcity of high-quality agricultural datasets, particularly in
Chinese, has resulted in suboptimal performance when employing mainstream
methods for this purpose. Most earlier works only focus on annotating
agricultural entities while overlook the profound correlation of agriculture
with hydrology and meteorology. To fill this blank, we present AgriCHN, a
comprehensive open-source Chinese resource designed to promote the accuracy of
automated agricultural entity annotation. The AgriCHN dataset has been
meticulously curated from a wealth of agricultural articles, comprising a total
of 4,040 sentences and encapsulating 15,799 agricultural entity mentions
spanning 27 diverse entity categories. Furthermore, it encompasses entities
from hydrology to meteorology, thereby enriching the diversity of entities
considered. Data validation reveals that, compared with relevant resources,
AgriCHN demonstrates outstanding data quality, attributable to its richer
agricultural entity types and more fine-grained entity divisions. A benchmark
task has also been constructed using several state-of-the-art neural NER
models. Extensive experimental results highlight the significant challenge
posed by AgriCHN and its potential for further research.

</details>


### [20] [Mind the Gap: Assessing Wiktionary's Crowd-Sourced Linguistic Knowledge on Morphological Gaps in Two Related Languages](https://arxiv.org/abs/2506.17603)
*Jonathan Sakunkoo,Annabella Sakunkoo*

Main category: cs.CL

TL;DR: The paper investigates morphological defectivity in Latin and Italian using a neural analyzer to validate Wiktionary data, revealing discrepancies in Latin but reliability in Italian.


<details>
  <summary>Details</summary>
Motivation: To address the lack of reliable linguistic resources for morphological gaps in understudied languages and improve NLP tool accuracy.

Method: Customizes a neural morphological analyzer to annotate Latin and Italian corpora and computationally validates Wiktionary's defective verb lists.

Result: Wiktionary is reliable for Italian but shows 7% inaccuracies in Latin defective lemmata.

Conclusion: Crowd-sourced wikis have limitations for less-studied phenomena, but scalable tools can enhance their reliability for computational morphology.

Abstract: Morphological defectivity is an intriguing and understudied phenomenon in
linguistics. Addressing defectivity, where expected inflectional forms are
absent, is essential for improving the accuracy of NLP tools in morphologically
rich languages. However, traditional linguistic resources often lack coverage
of morphological gaps as such knowledge requires significant human expertise
and effort to document and verify. For scarce linguistic phenomena in
under-explored languages, Wikipedia and Wiktionary often serve as among the few
accessible resources. Despite their extensive reach, their reliability has been
a subject of controversy. This study customizes a novel neural morphological
analyzer to annotate Latin and Italian corpora. Using the massive annotated
data, crowd-sourced lists of defective verbs compiled from Wiktionary are
validated computationally. Our results indicate that while Wiktionary provides
a highly reliable account of Italian morphological gaps, 7% of Latin lemmata
listed as defective show strong corpus evidence of being non-defective. This
discrepancy highlights potential limitations of crowd-sourced wikis as
definitive sources of linguistic knowledge, particularly for less-studied
phenomena and languages, despite their value as resources for rare linguistic
features. By providing scalable tools and methods for quality assurance of
crowd-sourced data, this work advances computational morphology and expands
linguistic knowledge of defectivity in non-English, morphologically rich
languages.

</details>


### [21] [TyphoFormer: Language-Augmented Transformer for Accurate Typhoon Track Forecasting](https://arxiv.org/abs/2506.17609)
*Lincan Li,Eren Erman Ozguven,Yue Zhao,Guang Wang,Yiqun Xie,Yushun Dong*

Main category: cs.CL

TL;DR: TyphoFormer integrates natural language descriptions with numerical data to improve typhoon track forecasting using a Transformer-based model.


<details>
  <summary>Details</summary>
Motivation: Existing Transformer models lack contextual knowledge for sparse meteorological trajectories like typhoon tracks, limiting forecasting reliability.

Method: TyphoFormer uses LLM-generated textual descriptions of numerical attributes, embedding them as auxiliary tokens in a Transformer encoder.

Result: TyphoFormer outperforms state-of-the-art methods on the HURDAT2 benchmark, especially in nonlinear path shifts and sparse data scenarios.

Conclusion: Incorporating textual context enhances typhoon trajectory forecasting, demonstrating the value of multimodal data integration.

Abstract: Accurate typhoon track forecasting is crucial for early system warning and
disaster response. While Transformer-based models have demonstrated strong
performance in modeling the temporal dynamics of dense trajectories of humans
and vehicles in smart cities, they usually lack access to broader contextual
knowledge that enhances the forecasting reliability of sparse meteorological
trajectories, such as typhoon tracks. To address this challenge, we propose
TyphoFormer, a novel framework that incorporates natural language descriptions
as auxiliary prompts to improve typhoon trajectory forecasting. For each time
step, we use Large Language Model (LLM) to generate concise textual
descriptions based on the numerical attributes recorded in the North Atlantic
hurricane database. The language descriptions capture high-level meteorological
semantics and are embedded as auxiliary special tokens prepended to the
numerical time series input. By integrating both textual and sequential
information within a unified Transformer encoder, TyphoFormer enables the model
to leverage contextual cues that are otherwise inaccessible through numerical
features alone. Extensive experiments are conducted on HURDAT2 benchmark,
results show that TyphoFormer consistently outperforms other state-of-the-art
baseline methods, particularly under challenging scenarios involving nonlinear
path shifts and limited historical observations.

</details>


### [22] [OpusLM: A Family of Open Unified Speech Language Models](https://arxiv.org/abs/2506.17611)
*Jinchuan Tian,William Chen,Yifan Peng,Jiatong Shi,Siddhant Arora,Shikhar Bharadwaj,Takashi Maekaku,Yusuke Shinohara,Keita Goto,Xiang Yue,Huck Yang,Shinji Watanabe*

Main category: cs.CL

TL;DR: OpusLMs are open foundational speech language models up to 7B, achieving comparable or superior performance in speech recognition, synthesis, and text tasks.


<details>
  <summary>Details</summary>
Motivation: To create transparent, high-performing speech language models using publicly available materials.

Method: Initialized from text language models, pre-trained on speech-text pairs and text tokens, with designs on tokenization, multi-stream models, and multi-stage training.

Result: OpusLMs match or outperform existing SpeechLMs in various tasks.

Conclusion: OpusLMs are effective, transparent models, with released resources to support open research.

Abstract: This paper presents Open Unified Speech Language Models (OpusLMs), a family
of open foundational speech language models (SpeechLMs) up to 7B. Initialized
from decoder-only text language models, the OpusLMs are continuously
pre-trained on 213K hours of speech-text pairs and 292B text-only tokens. We
demonstrate our OpusLMs achieve comparable (or even superior) performance with
existing SpeechLMs in speech recognition, speech synthesis, and text-only
capabilities. Technically, this paper articulates our SpeechLM designs on
tokenization, multi-stream language models, and multi-stage training
strategies. We experimentally demonstrate the importance of model size scaling
and the effect of annealing data selection. The OpusLMs are all built from
publicly available materials and are fully transparent models. We release our
code, data, checkpoints, and training logs to facilitate open SpeechLM research

</details>


### [23] [Answer-Centric or Reasoning-Driven? Uncovering the Latent Memory Anchor in LLMs](https://arxiv.org/abs/2506.17630)
*Yang Wu,Yifan Zhang,Yiwei Wang,Yujun Cai,Yurong Wu,Yuran Wang,Ning Xu,Jian Cheng*

Main category: cs.CL

TL;DR: LLMs rely heavily on explicit answers rather than genuine reasoning, as shown by a 26.90% performance drop when answer cues are masked.


<details>
  <summary>Details</summary>
Motivation: To determine if LLMs' reasoning is anchored to final answers or reasoning chains, questioning their inferential depth.

Method: A five-level answer-visibility prompt framework manipulating answer cues and analyzing model behavior indirectly.

Result: Performance drops significantly (26.90%) when answer cues are masked, indicating reliance on explicit answers.

Conclusion: LLMs may rationalize post-hoc rather than infer truly, highlighting the need for deeper understanding of their reasoning.

Abstract: While Large Language Models (LLMs) demonstrate impressive reasoning
capabilities, growing evidence suggests much of their success stems from
memorized answer-reasoning patterns rather than genuine inference. In this
work, we investigate a central question: are LLMs primarily anchored to final
answers or to the textual pattern of reasoning chains? We propose a five-level
answer-visibility prompt framework that systematically manipulates answer cues
and probes model behavior through indirect, behavioral analysis. Experiments
across state-of-the-art LLMs reveal a strong and consistent reliance on
explicit answers. The performance drops by 26.90\% when answer cues are masked,
even with complete reasoning chains. These findings suggest that much of the
reasoning exhibited by LLMs may reflect post-hoc rationalization rather than
true inference, calling into question their inferential depth. Our study
uncovers the answer-anchoring phenomenon with rigorous empirical validation and
underscores the need for a more nuanced understanding of what constitutes
reasoning in LLMs.

</details>


### [24] [Step-Opt: Boosting Optimization Modeling in LLMs through Iterative Data Synthesis and Structured Validation](https://arxiv.org/abs/2506.17637)
*Yang Wu,Yifan Zhang,Yurong Wu,Yuran Wang,Junkai Zhang,Jian Cheng*

Main category: cs.CL

TL;DR: Step-Opt-Instruct framework enhances LLMs for OR tasks by generating high-quality fine-tuning data through iterative problem generation and validation, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with complex OR optimization tasks, necessitating a tailored approach to improve their performance.

Method: Step-Opt-Instruct uses iterative problem generation and stepwise validation to create fine-tuning data, applied to models like LLaMA-3-8B and Mistral-7B.

Result: Step-Opt achieves a 17.01% improvement in micro average accuracy on complex OR tasks, outperforming benchmarks.

Conclusion: Structured validation and gradual problem refinement effectively advance LLM automation for OR decision-making.

Abstract: Large Language Models (LLMs) have revolutionized various domains but
encounter substantial challenges in tackling optimization modeling tasks for
Operations Research (OR), particularly when dealing with complex problem. In
this work, we propose Step-Opt-Instruct, a framework that augments existing
datasets and generates high-quality fine-tuning data tailored to optimization
modeling. Step-Opt-Instruct employs iterative problem generation to
systematically increase problem complexity and stepwise validation to
rigorously verify data, preventing error propagation and ensuring the quality
of the generated dataset. Leveraging this framework, we fine-tune open-source
LLMs, including LLaMA-3-8B and Mistral-7B, to develop Step-Opt--a model that
achieves state-of-the-art performance on benchmarks such as NL4OPT, MAMO, and
IndustryOR. Extensive experiments demonstrate the superior performance of
Step-Opt, especially in addressing complex OR tasks, with a notable 17.01\%
improvement in micro average accuracy on difficult problems. These findings
highlight the effectiveness of combining structured validation with gradual
problem refinement to advance the automation of decision-making processes using
LLMs.The code and dataset are available at https://github.com/samwu-learn/Step.

</details>


### [25] [TPTT: Transforming Pretrained Transformer into Titans](https://arxiv.org/abs/2506.17671)
*Fabien Furfaro*

Main category: cs.CL

TL;DR: TPTT enhances pretrained Transformers with efficient linearized attention and memory management, improving efficiency and accuracy without full retraining.


<details>
  <summary>Details</summary>
Motivation: Addressing the computational and memory demands of large language models (LLMs) for long-context inference.

Method: Uses Memory as Gate (MaG) and mixed linearized attention (LiZA), compatible with Hugging Face Transformers via LoRA fine-tuning.

Result: Substantial improvements in efficiency and accuracy, e.g., 20% EM increase on MMLU with 1B-parameter models.

Conclusion: TPTT is scalable, robust, and practical, with code and Python package available.

Abstract: Recent advances in large language models (LLMs) have led to remarkable
progress in natural language processing, but their computational and memory
demands remain a significant challenge, particularly for long-context
inference. We introduce TPTT (Transforming Pretrained Transformer into Titans),
a novel framework for enhancing pretrained Transformer models with efficient
linearized attention mechanisms and advanced memory management. TPTT employs
techniques such as Memory as Gate (MaG) and mixed linearized attention (LiZA).
It is fully compatible with the Hugging Face Transformers library, enabling
seamless adaptation of any causal LLM through parameter-efficient fine-tuning
(LoRA) without full retraining. We show the effectiveness of TPTT on the MMLU
benchmark with models of approximately 1 billion parameters, observing
substantial improvements in both efficiency and accuracy. For instance,
Titans-Llama-3.2-1B achieves a 20% increase in Exact Match (EM) over its
baseline. Statistical analyses and comparisons with recent state-of-the-art
methods confirm the practical scalability and robustness of TPTT. Code is
available at https://github.com/fabienfrfr/tptt . Python package at
https://pypi.org/project/tptt/ .

</details>


### [26] [Resource-Friendly Dynamic Enhancement Chain for Multi-Hop Question Answering](https://arxiv.org/abs/2506.17692)
*Binquan Ji,Haibo Luo,Yifei Lu,Lei Hei,Jiaqi Wang,Tingjing Liao,Lingyu Wang,Shichao Wang,Feiliang Ren*

Main category: cs.CL

TL;DR: DEC is a framework for multi-hop QA that decomposes questions into subquestions, refines them iteratively, and uses lightweight keyword extraction for efficient retrieval. It matches or outperforms benchmarks while reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges like hallucinations and semantic drift in lightweight LLMs during multi-hop QA tasks.

Method: Decomposes questions into subquestions, refines them iteratively, and employs a lightweight keyword extraction module for targeted retrieval.

Result: Performs on par or better than benchmarks, reduces token consumption, and achieves state-of-the-art results on 8B-parameter models.

Conclusion: DEC is effective, especially in resource-constrained environments, offering a balance of performance and efficiency.

Abstract: Knowledge-intensive multi-hop question answering (QA) tasks, which require
integrating evidence from multiple sources to address complex queries, often
necessitate multiple rounds of retrieval and iterative generation by large
language models (LLMs). However, incorporating many documents and extended
contexts poses challenges -such as hallucinations and semantic drift-for
lightweight LLMs with fewer parameters. This work proposes a novel framework
called DEC (Dynamic Enhancement Chain). DEC first decomposes complex questions
into logically coherent subquestions to form a hallucination-free reasoning
chain. It then iteratively refines these subquestions through context-aware
rewriting to generate effective query formulations. For retrieval, we introduce
a lightweight discriminative keyword extraction module that leverages extracted
keywords to achieve targeted, precise document recall with relatively low
computational overhead. Extensive experiments on three multi-hop QA datasets
demonstrate that DEC performs on par with or surpasses state-of-the-art
benchmarks while significantly reducing token consumption. Notably, our
approach attains state-of-the-art results on models with 8B parameters,
showcasing its effectiveness in various scenarios, particularly in
resource-constrained environments.

</details>


### [27] [Zero-Shot Conversational Stance Detection: Dataset and Approaches](https://arxiv.org/abs/2506.17693)
*Yuzhe Ding,Kang He,Bobo Li,Li Zheng,Haijun He,Fei Li,Chong Teng,Donghong Ji*

Main category: cs.CL

TL;DR: The paper introduces ZS-CSD, a large-scale zero-shot conversational stance detection dataset, and proposes SITPCL, a model achieving state-of-the-art performance, though challenges remain with a 43.81% F1-macro score.


<details>
  <summary>Details</summary>
Motivation: Existing datasets for conversational stance detection are limited to specific targets, hindering real-world applicability. This work aims to address this gap.

Method: The authors curate the ZS-CSD dataset and propose SITPCL, a speaker interaction and target-aware prototypical contrastive learning model.

Result: SITPCL achieves state-of-the-art performance in zero-shot conversational stance detection with a 43.81% F1-macro score.

Conclusion: While SITPCL advances zero-shot stance detection, the modest F1 score underscores ongoing challenges in the field.

Abstract: Stance detection, which aims to identify public opinion towards specific
targets using social media data, is an important yet challenging task. With the
increasing number of online debates among social media users, conversational
stance detection has become a crucial research area. However, existing
conversational stance detection datasets are restricted to a limited set of
specific targets, which constrains the effectiveness of stance detection models
when encountering a large number of unseen targets in real-world applications.
To bridge this gap, we manually curate a large-scale, high-quality zero-shot
conversational stance detection dataset, named ZS-CSD, comprising 280 targets
across two distinct target types. Leveraging the ZS-CSD dataset, we propose
SITPCL, a speaker interaction and target-aware prototypical contrastive
learning model, and establish the benchmark performance in the zero-shot
setting. Experimental results demonstrate that our proposed SITPCL model
achieves state-of-the-art performance in zero-shot conversational stance
detection. Notably, the SITPCL model attains only an F1-macro score of 43.81%,
highlighting the persistent challenges in zero-shot conversational stance
detection.

</details>


### [28] [The Evolution of Natural Language Processing: How Prompt Optimization and Language Models are Shaping the Future](https://arxiv.org/abs/2506.17700)
*Summra Saleem,Muhammad Nabeel Asim,Shaista Zulfiqar,Andreas Dengel*

Main category: cs.CL

TL;DR: The paper reviews and categorizes 11 distinct prompt optimization strategies for LLMs, providing insights into their applications across NLP tasks and datasets.


<details>
  <summary>Details</summary>
Motivation: To address the gap in comprehensive analyses of prompt optimization strategies in LLMs, the paper aims to centralize knowledge and facilitate future research.

Method: The paper categorizes prompt optimization strategies into 11 classes, analyzes their working paradigms, and details their use in NLP tasks, LLMs, and benchmark datasets.

Result: A comprehensive compilation of prompt optimization strategies is provided, enabling rigorous assessment and future comparative studies.

Conclusion: The research lays a foundation for adapting existing strategies to innovate predictors for unexplored NLP tasks.

Abstract: Large Language Models (LLMs) have revolutionized the field of Natural
Language Processing (NLP) by automating traditional labor-intensive tasks and
consequently accelerated the development of computer-aided applications. As
researchers continue to advance this field with the introduction of novel
language models and more efficient training/finetuning methodologies, the idea
of prompt engineering and subsequent optimization strategies with LLMs has
emerged as a particularly impactful trend to yield a substantial performance
boost across diverse NLP tasks. To best of our knowledge numerous review
articles have explored prompt engineering, however, a critical gap exists in
comprehensive analyses of prompt optimization strategies. To bridge this gap
this paper provides unique and comprehensive insights about the potential of
diverse prompt optimization strategies. It analyzes their underlying working
paradigms and based on these principles, categorizes them into 11 distinct
classes. Moreover, the paper provides details about various NLP tasks where
these prompt optimization strategies have been employed, along with details of
different LLMs and benchmark datasets used for evaluation. This comprehensive
compilation lays a robust foundation for future comparative studies and enables
rigorous assessment of prompt optimization and LLM-based predictive pipelines
under consistent experimental settings: a critical need in the current
landscape. Ultimately, this research will centralize diverse strategic
knowledge to facilitate the adaptation of existing prompt optimization
strategies for development of innovative predictors across unexplored tasks.

</details>


### [29] [Aged to Perfection: Machine-Learning Maps of Age in Conversational English](https://arxiv.org/abs/2506.17708)
*MingZe Tang*

Main category: cs.CL

TL;DR: Analyzed British English speech patterns across age groups using computational methods to predict age from linguistic markers.


<details>
  <summary>Details</summary>
Motivation: Explore how language varies by age, linking demographics to linguistic factors like utterance duration and word choice.

Method: Used the British National Corpus 2014, computational language analysis, and machine learning to identify generational linguistic markers.

Result: Uncovered distinctive linguistic markers for age groups and developed prediction models for age estimation.

Conclusion: Advances understanding of sociolinguistic diversity in modern British speech across generations.

Abstract: The study uses the British National Corpus 2014, a large sample of
contemporary spoken British English, to investigate language patterns across
different age groups. Our research attempts to explore how language patterns
vary between different age groups, exploring the connection between speaker
demographics and linguistic factors such as utterance duration, lexical
diversity, and word choice. By merging computational language analysis and
machine learning methodologies, we attempt to uncover distinctive linguistic
markers characteristic of multiple generations and create prediction models
that can consistently estimate the speaker's age group from various aspects.
This work contributes to our knowledge of sociolinguistic diversity throughout
the life of modern British speech.

</details>


### [30] [Unveiling Factors for Enhanced POS Tagging: A Study of Low-Resource Medieval Romance Languages](https://arxiv.org/abs/2506.17715)
*Matthias Schffel,Esteban Garces Arias,Marinus Wiedner,Paula Ruppert,Meimingwei Li,Christian Heumann,Matthias Aenmacher*

Main category: cs.CL

TL;DR: The study examines POS tagging challenges in Medieval Romance languages using LLMs, identifying limitations and effective techniques for handling historical language variations.


<details>
  <summary>Details</summary>
Motivation: POS tagging is crucial for historical text analysis, but applying modern LLMs to Medieval Romance languages is challenging due to linguistic evolution, spelling variations, and scarce labeled data.

Method: The study evaluates fine-tuning, prompt engineering, model architectures, decoding strategies, and cross-lingual transfer learning on Medieval Occitan, Spanish, and French texts.

Result: LLMs struggle with historical language variations and non-standardized spelling, but specialized techniques show promise for low-resource historical languages.

Conclusion: The findings highlight both the limitations of LLMs for historical texts and the potential of tailored approaches to improve POS tagging accuracy.

Abstract: Part-of-speech (POS) tagging remains a foundational component in natural
language processing pipelines, particularly critical for historical text
analysis at the intersection of computational linguistics and digital
humanities. Despite significant advancements in modern large language models
(LLMs) for ancient languages, their application to Medieval Romance languages
presents distinctive challenges stemming from diachronic linguistic evolution,
spelling variations, and labeled data scarcity. This study systematically
investigates the central determinants of POS tagging performance across diverse
corpora of Medieval Occitan, Medieval Spanish, and Medieval French texts,
spanning biblical, hagiographical, medical, and dietary domains. Through
rigorous experimentation, we evaluate how fine-tuning approaches, prompt
engineering, model architectures, decoding strategies, and cross-lingual
transfer learning techniques affect tagging accuracy. Our results reveal both
notable limitations in LLMs' ability to process historical language variations
and non-standardized spelling, as well as promising specialized techniques that
effectively address the unique challenges presented by low-resource historical
languages.

</details>


### [31] [KAG-Thinker: Teaching Large Language Models to Think with Human-like Reasoning Process](https://arxiv.org/abs/2506.17728)
*Dalong Zhang,Jun Xu,Jun Zhou,Lei Liang,Lin Yuan,Ling Zhong,Mengshu Sun,Peilong Zhao,QiWei Wang,Xiaorui Wang,Xinkai Du,YangYang Hou,Yu Ao,ZhaoYang Wang,Zhengke Gui,ZhiYing Yi,Zhongpu Bo*

Main category: cs.CL

TL;DR: KAG-Thinker is a human-like reasoning framework for LLMs, improving logical coherence in Q&A tasks by decomposing questions into solvable sub-problems and using structured reasoning.


<details>
  <summary>Details</summary>
Motivation: Enhance logical coherence and contextual consistency in LLMs for domain-specific Q&A tasks by simulating human cognitive mechanisms.

Method: Decomposes questions into sub-problems (logical forms), uses knowledge boundary and depth solving models, and employs supervised fine-tuning with multi-turn dialogues.

Result: Improved reasoning and retrieval in LLMs, avoiding excessive reflection through structured inference.

Conclusion: KAG-Thinker effectively enhances LLM reasoning for complex Q&A tasks by mimicking human cognitive processes.

Abstract: In this paper, we introduce KAG-Thinker, a novel human-like reasoning
framework built upon a parameter-light large language model (LLM). Our approach
enhances the logical coherence and contextual consistency of the thinking
process in question-answering (Q\&A) tasks on domain-specific knowledge bases
(KBs) within LLMs. This framework simulates human cognitive mechanisms for
handling complex problems by establishing a structured thinking process.
Continuing the \textbf{Logical Form} guided retrieval and reasoning technology
route of KAG v0.7, firstly, it decomposes complex questions into independently
solvable sub-problems(also referred to as logical forms) through
\textbf{breadth decomposition}, each represented in two equivalent
forms-natural language and logical function-and further classified as either
Knowledge Retrieval or Reasoning Analysis tasks, with dependencies and
variables passing explicitly modeled via logical function interfaces. In the
solving process, the Retrieval function is used to perform knowledge retrieval
tasks, while the Math and Deduce functions are used to perform reasoning
analysis tasks. Secondly, it is worth noting that, in the Knowledge Retrieval
sub-problem tasks, LLMs and external knowledge sources are regarded as
equivalent KBs. We use the \textbf{knowledge boundary} model to determine the
optimal source using self-regulatory mechanisms such as confidence calibration
and reflective reasoning, and use the \textbf{depth solving} model to enhance
the comprehensiveness of knowledge acquisition. Finally, instead of utilizing
reinforcement learning, we employ supervised fine-tuning with multi-turn
dialogues to align the model with our structured inference paradigm, thereby
avoiding excessive reflection. This is supported by a data evaluation framework
and iterative corpus synthesis, which facilitate the generation of detailed
reasoning trajectories...

</details>


### [32] [HIDE and Seek: Detecting Hallucinations in Language Models via Decoupled Representations](https://arxiv.org/abs/2506.17748)
*Anwoy Chatterjee,Yash Goel,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: HIDE proposes a single-pass, training-free method for detecting LM hallucinations by analyzing decoupled internal representations, outperforming other methods in efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: LMs often generate factually incorrect or unfaithful content (hallucinations), which is hard to detect and undermines reliability. Existing methods are computationally expensive.

Method: HIDE uses the Hilbert-Schmidt Independence Criterion (HSIC) to quantify decoupling between LM's internal representations of input and output.

Result: HIDE outperforms single-pass methods (~29% improvement in AUC-ROC) and competes with multi-pass methods (~3% improvement, ~51% less computation).

Conclusion: Exploiting internal representation decoupling in LMs is effective for efficient and practical hallucination detection.

Abstract: Contemporary Language Models (LMs), while impressively fluent, often generate
content that is factually incorrect or unfaithful to the input context - a
critical issue commonly referred to as 'hallucination'. This tendency of LMs to
generate hallucinated content undermines their reliability, especially because
these fabrications are often highly convincing and therefore difficult to
detect. While several existing methods attempt to detect hallucinations, most
rely on analyzing multiple generations per input, leading to increased
computational cost and latency. To address this, we propose a single-pass,
training-free approach for effective Hallucination detectIon via Decoupled
rEpresentations (HIDE). Our approach leverages the hypothesis that
hallucinations result from a statistical decoupling between an LM's internal
representations of input context and its generated output. We quantify this
decoupling using the Hilbert-Schmidt Independence Criterion (HSIC) applied to
hidden-state representations extracted while generating the output sequence. We
conduct extensive experiments on four diverse question answering datasets,
evaluating both faithfulness and factuality hallucinations across six
open-source LMs of varying scales and properties. Our results demonstrate that
HIDE outperforms other single-pass methods in almost all settings, achieving an
average relative improvement of ~29% in AUC-ROC over the best-performing
single-pass strategy across various models and datasets. Additionally, HIDE
shows competitive and often superior performance with multi-pass
state-of-the-art methods, obtaining an average relative improvement of ~3% in
AUC-ROC while consuming ~51% less computation time. Our findings highlight the
effectiveness of exploiting internal representation decoupling in LMs for
efficient and practical hallucination detection.

</details>


### [33] [Multilingual Tokenization through the Lens of Indian Languages: Challenges and Insights](https://arxiv.org/abs/2506.17789)
*N J Karthika,Maharaj Brahma,Rohit Saluja,Ganesh Ramakrishnan,Maunendra Sankar Desarkar*

Main category: cs.CL

TL;DR: The paper evaluates tokenization strategies for 17 Indian languages, comparing algorithms, vocabulary sizes, and multilingual training methods, showing benefits for low-resource languages.


<details>
  <summary>Details</summary>
Motivation: Existing tokenizers favor high-resource languages, disadvantaging linguistically diverse and morphologically rich languages like those in the Indian subcontinent.

Method: The study conducts an intrinsic evaluation of tokenization strategies, comparing BPE and Unigram LM algorithms, vocabulary sizes, and multilingual training approaches (joint and cluster-based).

Result: Low-resource languages benefit from tokenizers trained on related high-resource languages, and trade-offs between algorithms and vocabulary sizes are quantified.

Conclusion: The findings offer practical insights for developing fairer, more efficient, and linguistically informed tokenizers for multilingual NLP.

Abstract: Tokenization plays a pivotal role in multilingual NLP. However, existing
tokenizers are often skewed towards high-resource languages, limiting their
effectiveness for linguistically diverse and morphologically rich languages
such as those in the Indian subcontinent. This paper presents a comprehensive
intrinsic evaluation of tokenization strategies across 17 Indian languages. We
quantify the trade-offs between bottom-up and top-down tokenizer algorithms
(BPE and Unigram LM), effects of vocabulary sizes, and compare strategies of
multilingual vocabulary construction such as joint and cluster-based training.
We also show that extremely low-resource languages can benefit from tokenizers
trained on related high-resource languages. Our study provides practical
insights for building more fair, efficient, and linguistically informed
tokenizers for multilingual NLP.

</details>


### [34] [THCM-CAL: Temporal-Hierarchical Causal Modelling with Conformal Calibration for Clinical Risk Prediction](https://arxiv.org/abs/2506.17844)
*Xin Zhang,Qiyu Wei,Yingjie Zhu,Fanyi Wu,Sophia Ananiadou*

Main category: cs.CL

TL;DR: THCM-CAL is a Temporal-Hierarchical Causal Model for clinical risk prediction, integrating structured and unstructured EHR data with causal interactions and conformal calibration for reliable predictions.


<details>
  <summary>Details</summary>
Motivation: Prior methods fail to capture the hierarchical, causal interactions between narrative notes and diagnostic codes in EHRs, limiting prediction accuracy.

Method: THCM-CAL constructs a multimodal causal graph, infers hierarchical interactions (sequencing, triggers, risk propagation), and uses conformal prediction for calibrated confidence intervals.

Result: THCM-CAL outperforms existing methods on MIMIC-III and MIMIC-IV datasets.

Conclusion: THCM-CAL effectively models causal interactions in EHRs and improves prediction reliability, demonstrating superior performance.

Abstract: Automated clinical risk prediction from electronic health records (EHRs)
demands modeling both structured diagnostic codes and unstructured narrative
notes. However, most prior approaches either handle these modalities separately
or rely on simplistic fusion strategies that ignore the directional,
hierarchical causal interactions by which narrative observations precipitate
diagnoses and propagate risk across admissions. In this paper, we propose
THCM-CAL, a Temporal-Hierarchical Causal Model with Conformal Calibration. Our
framework constructs a multimodal causal graph where nodes represent clinical
entities from two modalities: Textual propositions extracted from notes and ICD
codes mapped to textual descriptions. Through hierarchical causal discovery,
THCM-CAL infers three clinically grounded interactions: intra-slice
same-modality sequencing, intra-slice cross-modality triggers, and inter-slice
risk propagation. To enhance prediction reliability, we extend conformal
prediction to multi-label ICD coding, calibrating per-code confidence intervals
under complex co-occurrences. Experimental results on MIMIC-III and MIMIC-IV
demonstrate the superiority of THCM-CAL.

</details>


### [35] [LLMs for Customized Marketing Content Generation and Evaluation at Scale](https://arxiv.org/abs/2506.17863)
*Haoran Liu,Amir Tahmasbi,Ehtesham Sam Haque,Purak Jain*

Main category: cs.CL

TL;DR: MarketingFM improves offsite marketing by generating keyword-specific ad copy, boosting CTR and cost efficiency. AutoEval-Main and AutoEval-Update automate ad evaluation, reducing human effort while maintaining alignment with marketing principles.


<details>
  <summary>Details</summary>
Motivation: Current offsite marketing content is generic and misaligned with landing pages, limiting effectiveness.

Method: MarketingFM integrates data sources for keyword-specific ad generation. AutoEval-Main uses rule-based metrics and LLM-as-a-Judge for automated evaluation. AutoEval-Update refines evaluation prompts dynamically with minimal human input.

Result: MarketingFM achieved 9% higher CTR, 12% more impressions, and 0.38% lower CPC. AutoEval-Main matched human reviewers with 89.57% agreement.

Conclusion: Automated systems like MarketingFM and AutoEval improve ad performance and evaluation efficiency, though human oversight remains crucial.

Abstract: Offsite marketing is essential in e-commerce, enabling businesses to reach
customers through external platforms and drive traffic to retail websites.
However, most current offsite marketing content is overly generic,
template-based, and poorly aligned with landing pages, limiting its
effectiveness. To address these limitations, we propose MarketingFM, a
retrieval-augmented system that integrates multiple data sources to generate
keyword-specific ad copy with minimal human intervention. We validate
MarketingFM via offline human and automated evaluations and large-scale online
A/B tests. In one experiment, keyword-focused ad copy outperformed templates,
achieving up to 9% higher CTR, 12% more impressions, and 0.38% lower CPC,
demonstrating gains in ad ranking and cost efficiency. Despite these gains,
human review of generated ads remains costly. To address this, we propose
AutoEval-Main, an automated evaluation system that combines rule-based metrics
with LLM-as-a-Judge techniques to ensure alignment with marketing principles.
In experiments with large-scale human annotations, AutoEval-Main achieved
89.57% agreement with human reviewers. Building on this, we propose
AutoEval-Update, a cost-efficient LLM-human collaborative framework to
dynamically refine evaluation prompts and adapt to shifting criteria with
minimal human input. By selectively sampling representative ads for human
review and using a critic LLM to generate alignment reports, AutoEval-Update
improves evaluation consistency while reducing manual effort. Experiments show
the critic LLM suggests meaningful refinements, improving LLM-human agreement.
Nonetheless, human oversight remains essential for setting thresholds and
validating refinements before deployment.

</details>


### [36] [QueueEDIT: Structural Self-Correction for Sequential Model Editing in LLMs](https://arxiv.org/abs/2506.17864)
*Taolin Zhang,Haidong Kang,Dongyang Li,Qizhou Chen,Chengyu Wang Xiaofeng He,Richang Hong*

Main category: cs.CL

TL;DR: QueueEDIT, a queue-based self-correction framework, improves sequential model editing (SME) in LLMs by addressing long-sequence dependency and mitigating parameter bias, preserving general capabilities.


<details>
  <summary>Details</summary>
Motivation: LLMs suffer from hallucinations and factual inaccuracies, especially in sequential edits, which can degrade general performance.

Method: Uses structural mapping editing loss to locate knowledge-sensitive neurons, stores parameters in a queue, and dynamically aligns or freezes them to avoid harming general abilities.

Result: Outperforms baselines in SME and maintains single-turn editing performance while preserving general NLP capabilities.

Conclusion: QueueEDIT effectively enhances SME and safeguards LLM general abilities, making it a robust solution for continuous model corrections.

Abstract: Recently, large language models (LLMs) have demonstrated impressive results
but still suffer from hallucinations. Model editing has been proposed to
correct factual inaccuracies in LLMs. A challenging case is sequential model
editing (SME), which aims to rectify errors continuously rather than treating
them as a one-time task. During SME, the general capabilities of LLMs can be
negatively affected due to the introduction of new parameters. In this paper,
we propose a queue-based self-correction framework (QueueEDIT) that not only
enhances SME performance by addressing long-sequence dependency but also
mitigates the impact of parameter bias on the general capabilities of LLMs.
Specifically, we first introduce a structural mapping editing loss to map the
triplets to the knowledge-sensitive neurons within the Transformer layers of
LLMs. We then store the located parameters for each piece of edited knowledge
in a queue and dynamically align previously edited parameters. In each edit, we
select queue parameters most relevant to the currently located parameters to
determine whether previous knowledge needs realignment. Irrelevant parameters
in the queue are frozen, and we update the parameters at the queue head to the
LLM to ensure they do not harm general abilities. Experiments show that our
framework significantly outperforms strong baselines across various SME
settings and maintains competitiveness in single-turn editing. The resulting
LLMs also preserve high capabilities in general NLP tasks throughout the SME
process.

</details>


### [37] [How Alignment Shrinks the Generative Horizon](https://arxiv.org/abs/2506.17871)
*Chenghao Yang,Ari Holtzman*

Main category: cs.CL

TL;DR: The paper investigates why aligned LLMs generate less diverse outputs, introducing the Branching Factor (BF) to measure probability concentration. Findings show BF decreases during generation, and alignment tuning sharply reduces BF, explaining stable outputs. Aligned CoT models leverage this for stable reasoning.


<details>
  <summary>Details</summary>
Motivation: To understand why aligned LLMs produce less diverse outputs and quantify this stability using probability concentration.

Method: Introduces the Branching Factor (BF) to measure the effective number of plausible next steps. Analyzes BF changes during generation and the impact of alignment tuning.

Result: BF decreases as generation progresses, and alignment tuning reduces BF significantly. Aligned CoT models use this for stable reasoning. Base models can be nudged to mimic aligned behavior.

Conclusion: BF is a key diagnostic for understanding and controlling LLM outputs, explaining alignment's impact on diversity and CoT's stability.

Abstract: Despite their impressive capabilities, aligned large language models (LLMs)
often generate outputs that lack diversity. What drives this stability in the
generation? We investigate this phenomenon through the lens of probability
concentration in the model's output distribution. To quantify this
concentration, we introduce the Branching Factor (BF) -- a token-invariant
measure of the effective number of plausible next steps during generation. Our
empirical analysis reveals two key findings: (1) BF often decreases as
generation progresses, suggesting that LLMs become more predictable as they
generate. (2) alignment tuning substantially sharpens the model's output
distribution from the outset, reducing BF by nearly an order of magnitude
(e.g., from 12 to 1.2) relative to base models. This stark reduction helps
explain why aligned models often appear less sensitive to decoding strategies.
Building on this insight, we find this stability has surprising implications
for complex reasoning. Aligned Chain-of-Thought (CoT) models (e.g.,
DeepSeek-distilled models), for instance, leverage this effect; by generating
longer reasoning chains, they push generation into later, more deterministic
(lower BF) stages, resulting in more stable outputs. We hypothesize that
alignment tuning does not fundamentally change a model's behavior, but instead
steers it toward stylistic tokens (e.g., "Sure") that unlock low-entropy
trajectories already present in the base model. This view is supported by
nudging experiments, which show that prompting base models with such tokens can
similarly reduce BF. Together, our findings establish BF as a powerful
diagnostic for understanding and controlling LLM outputs - clarifying how
alignment reduces variability, how CoT promotes stable generations, and how
base models can be steered away from diversity.

</details>


### [38] [Multi-turn Jailbreaking via Global Refinement and Active Fabrication](https://arxiv.org/abs/2506.17881)
*Hua Tang,Lingyong Yan,Yukun Zhao,Shuaiqiang Wang,Jizhou Huang,Dawei Yin*

Main category: cs.CL

TL;DR: The paper introduces a novel multi-turn jailbreaking method for LLMs, addressing limitations in existing techniques by refining paths globally and fabricating responses to bypass safety measures.


<details>
  <summary>Details</summary>
Motivation: Existing jailbreaking methods focus on single-turn scenarios and struggle with multi-turn dynamics, posing safety risks due to potential misuse of LLMs.

Method: Proposes a multi-turn jailbreaking approach with global refinement at each interaction and active fabrication of responses to suppress safety warnings.

Result: Outperforms existing single-turn and multi-turn techniques across six state-of-the-art LLMs.

Conclusion: The method effectively identifies security threats in LLMs, highlighting the need for robust safety measures against evolving jailbreaking tactics.

Abstract: Large Language Models (LLMs) have achieved exceptional performance across a
wide range of tasks. However, they still pose significant safety risks due to
the potential misuse for malicious purposes. Jailbreaks, which aim to elicit
models to generate harmful content, play a critical role in identifying the
underlying security threats. Recent jailbreaking primarily focuses on
single-turn scenarios, while the more complicated multi-turn scenarios remain
underexplored. Moreover, existing multi-turn jailbreaking techniques struggle
to adapt to the evolving dynamics of dialogue as the interaction progresses. To
address this limitation, we propose a novel multi-turn jailbreaking method that
refines the jailbreaking path globally at each interaction. We also actively
fabricate model responses to suppress safety-related warnings, thereby
increasing the likelihood of eliciting harmful outputs in subsequent questions.
Experimental results demonstrate the superior performance of our method
compared with existing single-turn and multi-turn jailbreaking techniques
across six state-of-the-art LLMs. Our code is publicly available at
https://github.com/Ytang520/Multi-Turn_jailbreaking_Global-Refinment_and_Active-Fabrication.

</details>


### [39] [Scatter-Based Innovation Propagation in Large Language Models for Multi-Stage Process Adaptation](https://arxiv.org/abs/2506.17949)
*Hong Su*

Main category: cs.CL

TL;DR: The paper proposes a scatter-based innovation expansion model to help LLMs generalize and apply localized innovations across multi-stage processes.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle to generalize novel ideas beyond their original context, limiting their applicability in multi-stage processes.

Method: A four-step innovation scatter model: identify core innovation, generalize it, assess broader applicability, and systematically apply it to similar stages.

Result: The model successfully enables LLMs to extend innovations across structurally similar stages, improving generalization and reuse.

Conclusion: The innovation scatter model enhances LLMs' ability to generalize and reuse localized innovations in multi-stage contexts.

Abstract: Large Language Models (LLMs) exhibit strong capabilities in reproducing and
extending patterns observed during pretraining but often struggle to generalize
novel ideas beyond their original context. This paper addresses the challenge
of applying such localized innovations - introduced at a specific stage or
component - to other parts of a multi-stage process. We propose a scatter-based
innovation expansion model (innovation scatter model) that guides the LLM
through a four-step process: (1) identifying the core innovation by comparing
the user's input with its surrounding context, (2) generalizing the innovation
by removing references to specific stages or components, (3) determining
whether the generalized innovation applies to a broader scope beyond the
original stage, and (4) systematically applying it to other structurally
similar stages using the LLM. This model leverages structural redundancy across
stages to improve the applicability of novel ideas. Verification results
demonstrate that the innovation scatter model enables LLMs to extend
innovations across structurally similar stages, thereby enhancing
generalization and reuse.

</details>


### [40] [A Comprehensive Graph Framework for Question Answering with Mode-Seeking Preference Alignment](https://arxiv.org/abs/2506.17951)
*Quanwei Tang,Sophia Yat Mei Lee,Junshuang Wu,Dong Zhang,Shoushan Li,Erik Cambria,Guodong Zhou*

Main category: cs.CL

TL;DR: GraphMPA is a graph-based framework for retrieval-augmented generation, improving global understanding and aligning responses with human preferences via mode-seeking optimization.


<details>
  <summary>Details</summary>
Motivation: Address challenges in global understanding and ethical alignment in retrieval-augmented generation for question answering.

Method: Constructs a hierarchical document graph using similarity measurements and introduces mode-seeking preference optimization for probability-matching constraints.

Result: Demonstrated effectiveness on six datasets.

Conclusion: GraphMPA successfully enhances retrieval-augmented generation by aligning outputs with human preferences and improving understanding.

Abstract: Recent advancements in retrieval-augmented generation (RAG) have enhanced
large language models in question answering by integrating external knowledge.
However, challenges persist in achieving global understanding and aligning
responses with human ethical and quality preferences. To address these issues,
we propose GraphMPA, a comprehensive graph-based framework with mode-seeking
preference alignment. Our approach constructs a hierarchical document graph
using a general similarity measurement, mimicking human cognitive processes for
information understanding and synthesis. Additionally, we introduce
mode-seeking preference optimization to better align model outputs with human
preferences through probability-matching constraints. Extensive experiments on
six datasets demonstrate the effectiveness of our
\href{https://github.com/tangquanwei/GraphMPA}{GraphMPA}.

</details>


### [41] [PDF Retrieval Augmented Question Answering](https://arxiv.org/abs/2506.18027)
*Thi Thu Uyen Hoang,Viet Anh Nguyen*

Main category: cs.CL

TL;DR: The paper introduces a RAG-based QA system for extracting multimodal data from PDFs, improving accuracy and relevance in answers.


<details>
  <summary>Details</summary>
Motivation: Existing QA systems struggle with multimodal content in PDFs, prompting the need for a more comprehensive solution.

Method: Refines processing of non-textual elements in PDFs and fine-tunes language models within a RAG framework.

Result: The system successfully extracts accurate information from diverse PDF content types.

Conclusion: This work advances retrieval-augmented QA systems and sets groundwork for future multimodal research.

Abstract: This paper presents an advancement in Question-Answering (QA) systems using a
Retrieval Augmented Generation (RAG) framework to enhance information
extraction from PDF files. Recognizing the richness and diversity of data
within PDFs--including text, images, vector diagrams, graphs, and tables--poses
unique challenges for existing QA systems primarily designed for textual
content. We seek to develop a comprehensive RAG-based QA system that will
effectively address complex multimodal questions, where several data types are
combined in the query. This is mainly achieved by refining approaches to
processing and integrating non-textual elements in PDFs into the RAG framework
to derive precise and relevant answers, as well as fine-tuning large language
models to better adapt to our system. We provide an in-depth experimental
evaluation of our solution, demonstrating its capability to extract accurate
information that can be applied to different types of content across PDFs. This
work not only pushes the boundaries of retrieval-augmented QA systems but also
lays a foundation for further research in multimodal data integration and
processing.

</details>


### [42] [Splitformer: An improved early-exit architecture for automatic speech recognition on edge devices](https://arxiv.org/abs/2506.18035)
*Maxence Lasbordes,Daniele Falavigna,Alessio Brutti*

Main category: cs.CL

TL;DR: The paper proposes adding parallel layers processing downsampled inputs to early-exit neural models for speech recognition, improving performance without increasing inference time.


<details>
  <summary>Details</summary>
Motivation: Dynamic computational load adjustment is crucial for on-device processing with limited resources. Early-exit architectures and memory-efficient models like Zipformer exist but lack modularity for early-exit branches.

Method: Introduce parallel layers processing downsampled inputs in early-exit models to enhance performance.

Result: Speech recognition performance on benchmarks improves significantly with a small parameter increase, without affecting inference time.

Conclusion: The proposed method effectively enhances early-exit models for speech recognition, balancing performance and resource efficiency.

Abstract: The ability to dynamically adjust the computational load of neural models
during inference in a resource aware manner is crucial for on-device processing
scenarios, characterised by limited and time-varying computational resources.
Early-exit architectures represent an elegant and effective solution, since
they can process the input with a subset of their layers, exiting at
intermediate branches (the upmost layers are hence removed from the model).
  From a different perspective, for automatic speech recognition applications
there are memory-efficient neural architectures that apply variable frame rate
analysis, through downsampling/upsampling operations in the middle layers,
reducing the overall number of operations and improving significantly the
performance on well established benchmarks. One example is the Zipformer.
However, these architectures lack the modularity necessary to inject early-exit
branches.
  With the aim of improving the performance in early-exit models, we propose
introducing parallel layers in the architecture that process downsampled
versions of their inputs. % in conjunction with standard processing layers. We
show that in this way the speech recognition performance on standard benchmarks
significantly improve, at the cost of a small increase in the overall number of
model parameters but without affecting the inference time.

</details>


### [43] [Markov-Enhanced Clustering for Long Document Summarization: Tackling the 'Lost in the Middle' Challenge with Large Language Models](https://arxiv.org/abs/2506.18036)
*Aziz Amari,Mohamed Achref Ben Ammar*

Main category: cs.CL

TL;DR: A hybrid summarization method combining extractive and abstractive techniques is proposed to address challenges in retaining key information in lengthy documents.


<details>
  <summary>Details</summary>
Motivation: The need for effective automatic text summarization due to the rapid expansion of information, and the limitations of current abstractive methods (e.g., being resource-intensive and 'lost in the middle').

Method: Splits documents into smaller chunks, clusters their vector embeddings, generates summaries for each cluster, and constructs the final summary using a Markov chain graph for semantic order.

Result: The hybrid approach aims to improve summarization by retaining key information and coherence in lengthy documents.

Conclusion: The proposed method offers a promising solution to the challenges of abstractive summarization, balancing efficiency and effectiveness.

Abstract: The rapid expansion of information from diverse sources has heightened the
need for effective automatic text summarization, which condenses documents into
shorter, coherent texts. Summarization methods generally fall into two
categories: extractive, which selects key segments from the original text, and
abstractive, which generates summaries by rephrasing the content coherently.
Large language models have advanced the field of abstractive summarization, but
they are resourceintensive and face significant challenges in retaining key
information across lengthy documents, which we call being "lost in the middle".
To address these issues, we propose a hybrid summarization approach that
combines extractive and abstractive techniques. Our method splits the document
into smaller text chunks, clusters their vector embeddings, generates a summary
for each cluster that represents a key idea in the document, and constructs the
final summary by relying on a Markov chain graph when selecting the semantic
order of ideas.

</details>


### [44] [Statistical Multicriteria Evaluation of LLM-Generated Text](https://arxiv.org/abs/2506.18082)
*Esteban Garces Arias,Hannah Blocher,Julian Rodemann,Matthias Aenmacher,Christoph Jansen*

Main category: cs.CL

TL;DR: The paper introduces a Generalized Stochastic Dominance (GSD) framework to evaluate LLM-generated text quality, addressing limitations of current methods by enabling multi-dimensional assessment without arbitrary metric weighting.


<details>
  <summary>Details</summary>
Motivation: Current evaluation methods for LLM-generated text are limited by single-metric focus, incompatibility between automatic metrics and human judgments, and lack of statistical guarantees.

Method: The authors adapt the GSD framework to evaluate text quality across multiple dimensions (coherence, diversity, fluency) without arbitrary weighting, using partial orders of decoding strategies.

Result: The GSD framework successfully identifies statistically significant performance differences in decoding strategies compared to human-generated text, accounting for non-i.i.d. sampling.

Conclusion: The GSD-front approach provides a robust, multi-dimensional evaluation method for LLM-generated text, overcoming key limitations of existing benchmarking methodologies.

Abstract: Assessing the quality of LLM-generated text remains a fundamental challenge
in natural language processing. Current evaluation approaches often rely on
isolated metrics or simplistic aggregations that fail to capture the nuanced
trade-offs between coherence, diversity, fluency, and other relevant indicators
of text quality. In this work, we adapt a recently proposed framework for
statistical inference based on Generalized Stochastic Dominance (GSD) that
addresses three critical limitations in existing benchmarking methodologies:
the inadequacy of single-metric evaluation, the incompatibility between
cardinal automatic metrics and ordinal human judgments, and the lack of
inferential statistical guarantees. The GSD-front approach enables simultaneous
evaluation across multiple quality dimensions while respecting their different
measurement scales, building upon partial orders of decoding strategies, thus
avoiding arbitrary weighting of the involved metrics. By applying this
framework to evaluate common decoding strategies against human-generated text,
we demonstrate its ability to identify statistically significant performance
differences while accounting for potential deviations from the i.i.d.
assumption of the sampling design.

</details>


### [45] [Evaluating Prompt-Based and Fine-Tuned Approaches to Czech Anaphora Resolution](https://arxiv.org/abs/2506.18091)
*Patrik Stano,Ale Hork*

Main category: cs.CL

TL;DR: The paper compares prompt engineering with LLMs and fine-tuning compact models for Czech anaphora resolution, finding fine-tuned models like mT5-large outperform LLMs in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Anaphora resolution is crucial for understanding morphologically rich languages like Czech, and modern approaches need evaluation.

Method: Comparative evaluation of prompt engineering (using LLMs like Mistral Large 2 and Llama 3) and fine-tuning (mT5 and Mistral models) on Czech text from the Prague Dependency Treebank.

Result: Fine-tuned models (e.g., mT5-large) achieved 88% accuracy, outperforming LLMs (74.5%). Fine-tuning also required fewer resources.

Conclusion: Fine-tuning compact models is more effective for Czech anaphora resolution than prompting LLMs, balancing accuracy and efficiency.

Abstract: Anaphora resolution plays a critical role in natural language understanding,
especially in morphologically rich languages like Czech. This paper presents a
comparative evaluation of two modern approaches to anaphora resolution on Czech
text: prompt engineering with large language models (LLMs) and fine-tuning
compact generative models. Using a dataset derived from the Prague Dependency
Treebank, we evaluate several instruction-tuned LLMs, including Mistral Large 2
and Llama 3, using a series of prompt templates. We compare them against
fine-tuned variants of the mT5 and Mistral models that we trained specifically
for Czech anaphora resolution. Our experiments demonstrate that while prompting
yields promising few-shot results (up to 74.5% accuracy), the fine-tuned
models, particularly mT5-large, outperform them significantly, achieving up to
88% accuracy while requiring fewer computational resources. We analyze
performance across different anaphora types, antecedent distances, and source
corpora, highlighting key strengths and trade-offs of each approach.

</details>


### [46] [InspireDebate: Multi-Dimensional Subjective-Objective Evaluation-Guided Reasoning and Optimization for Debating](https://arxiv.org/abs/2506.18102)
*Fuyu Wang,Jiangtong Li,Kun Zhu,Changjun Jiang*

Main category: cs.CL

TL;DR: The paper introduces a dual-component framework (InspireScore and InspireDebate) to improve LLM-based debating systems by addressing gaps in objective assessment and structured optimization.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based debating systems lack objective assessments (authenticity, logical validity) and structured optimization across evaluation metrics, CoT reasoning, and multi-turn debate refinement.

Method: Proposes InspireScore (multi-dimensional evaluation system) and InspireDebate (phased optimization framework using CoT, DPO, and Web-RAG).

Result: InspireScore achieves 44% higher correlation with expert judgments; InspireDebate outperforms baselines by 57%.

Conclusion: The framework effectively addresses limitations in current LLM-based debating systems, demonstrating significant improvements in evaluation and performance.

Abstract: With the rapid advancements in large language models (LLMs), debating tasks,
such as argument quality assessment and debate process simulation, have made
significant progress. However, existing LLM-based debating systems focus on
responding to specific arguments while neglecting objective assessments such as
authenticity and logical validity. Furthermore, these systems lack a structured
approach to optimize across various dimensions$-$including evaluation metrics,
chain-of-thought (CoT) reasoning, and multi-turn debate refinement$-$thereby
limiting their effectiveness. To address these interconnected challenges, we
propose a dual-component framework: (1) $\textbf{InspireScore}$, a novel
evaluation system that establishes a multi-dimensional assessment architecture
incorporating four subjective criteria (emotional appeal, argument clarity,
argument arrangement, and topic relevance) alongside two objective metrics
(fact authenticity and logical validity); and (2) $\textbf{InspireDebate}$, an
optimized debating framework employing a phased optimization approach through
CoT reasoning enhancement, multi-dimensional Direct Preference Optimization
(DPO), and real-time knowledge grounding via web-based Retrieval Augmented
Generation (Web-RAG). Empirical evaluations demonstrate that
$\textbf{InspireScore}$ achieves 44$\%$ higher correlation with expert
judgments compared to existing methods, while $\textbf{InspireDebate}$ shows
significant improvements, outperforming baseline models by 57$\%$. Source code
is available at https://github.com/fywang12/InspireDebate.

</details>


### [47] [Chengyu-Bench: Benchmarking Large Language Models for Chinese Idiom Understanding and Use](https://arxiv.org/abs/2506.18105)
*Yicheng Fu,Zhemin Huang,Liuxin Yang,Yumeng Lu,Zhongdongming Dai*

Main category: cs.CL

TL;DR: Chengyu-Bench is a benchmark for evaluating language models on Chinese idioms (Chengyu), covering tasks like sentiment classification, appropriateness detection, and open cloze. Models perform well on sentiment but poorly on nuanced usage.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for Chinese idioms are limited. Chengyu-Bench addresses this by providing a comprehensive evaluation of idiom understanding and usage.

Method: The benchmark includes three tasks: Evaluative Connotation, Appropriateness, and Open Cloze, with 2,937 human-verified examples.

Result: LLMs achieve 95% accuracy on sentiment but only ~85% on appropriateness and ~40% on open cloze, showing gaps in nuanced understanding.

Conclusion: Chengyu-Bench highlights the need for better cultural and contextual understanding in language models for idiom usage.

Abstract: Chinese idioms (Chengyu) are concise four-character expressions steeped in
history and culture, whose literal translations often fail to capture their
full meaning. This complexity makes them challenging for language models to
interpret and use correctly. Existing benchmarks focus on narrow tasks -
multiple-choice cloze tests, isolated translation, or simple paraphrasing. We
introduce Chengyu-Bench, a comprehensive benchmark featuring three tasks: (1)
Evaluative Connotation, classifying idioms as positive or negative; (2)
Appropriateness, detecting incorrect idiom usage in context; and (3) Open
Cloze, filling blanks in longer passages without options. Chengyu-Bench
comprises 2,937 human-verified examples covering 1,765 common idioms sourced
from diverse corpora. We evaluate leading LLMs and find they achieve over 95%
accuracy on Evaluative Connotation, but only ~85% on Appropriateness and ~40%
top-1 accuracy on Open Cloze. Error analysis reveals that most mistakes arise
from fundamental misunderstandings of idiom meanings. Chengyu-Bench
demonstrates that while LLMs can reliably gauge idiom sentiment, they still
struggle to grasp the cultural and contextual nuances essential for proper
usage. The benchmark and source code are available at:
https://github.com/sofyc/ChengyuBench.

</details>


### [48] [Mental Health Equity in LLMs: Leveraging Multi-Hop Question Answering to Detect Amplified and Silenced Perspectives](https://arxiv.org/abs/2506.18116)
*Batool Haider,Atmika Gorti,Aman Chadha,Manas Gaur*

Main category: cs.CL

TL;DR: The paper introduces a multi-hop question answering (MHQA) framework to detect intersectional biases in LLMs for mental healthcare, revealing systematic disparities and proposing debiasing techniques.


<details>
  <summary>Details</summary>
Motivation: To address the risk of LLMs propagating biases in mental healthcare, particularly for marginalized groups, by developing systematic methods for detecting intersectional biases.

Method: Uses MHQA to analyze LLM responses in mental health discourse, tagging for demographics and evaluating four models. Implements debiasing techniques: Roleplay Simulation and Explicit Bias Reduction.

Result: Identifies systematic disparities in LLM responses across demographics and conditions. Debiasing techniques achieve 66-94% bias reduction.

Conclusion: The MHQA framework effectively detects biases, and debiasing methods offer actionable insights for equitable AI development in mental healthcare.

Abstract: Large Language Models (LLMs) in mental healthcare risk propagating biases
that reinforce stigma and harm marginalized groups. While previous research
identified concerning trends, systematic methods for detecting intersectional
biases remain limited. This work introduces a multi-hop question answering
(MHQA) framework to explore LLM response biases in mental health discourse. We
analyze content from the Interpretable Mental Health Instruction (IMHI) dataset
across symptom presentation, coping mechanisms, and treatment approaches. Using
systematic tagging across age, race, gender, and socioeconomic status, we
investigate bias patterns at demographic intersections. We evaluate four LLMs:
Claude 3.5 Sonnet, Jamba 1.6, Gemma 3, and Llama 4, revealing systematic
disparities across sentiment, demographics, and mental health conditions. Our
MHQA approach demonstrates superior detection compared to conventional methods,
identifying amplification points where biases magnify through sequential
reasoning. We implement two debiasing techniques: Roleplay Simulation and
Explicit Bias Reduction, achieving 66-94% bias reductions through few-shot
prompting with BBQ dataset examples. These findings highlight critical areas
where LLMs reproduce mental healthcare biases, providing actionable insights
for equitable AI development.

</details>


### [49] [The Syntactic Acceptability Dataset (Preview): A Resource for Machine Learning and Linguistic Analysis of English](https://arxiv.org/abs/2506.18120)
*Tom S Juzek*

Main category: cs.CL

TL;DR: The paper introduces the Syntactic Acceptability Dataset, a resource for syntax and computational linguistics research, with 1,000 labeled English sequences. It shows convergence between grammaticality and acceptability (83%) and highlights machine learning models' better performance in predicting acceptability over grammaticality.


<details>
  <summary>Details</summary>
Motivation: To create a publicly accessible dataset for syntax and computational linguistics research, addressing gaps in labeled syntactic data and exploring the relationship between grammaticality and acceptability.

Method: The dataset includes 1,000 English sequences from textbooks and Linguistic Inquiry, labeled for grammaticality (from literature) and acceptability (via crowdsourcing). Preliminary analyses compare these labels and test machine learning models.

Result: Grammaticality and acceptability converge in 83% of cases; machine learning models perform better at predicting acceptability than grammaticality.

Conclusion: The dataset is a valuable resource, revealing novel insights about acceptability prediction and supporting future expansions.

Abstract: We present a preview of the Syntactic Acceptability Dataset, a resource being
designed for both syntax and computational linguistics research. In its current
form, the dataset comprises 1,000 English sequences from the syntactic
discourse: Half from textbooks and half from the journal Linguistic Inquiry,
the latter to ensure a representation of the contemporary discourse. Each entry
is labeled with its grammatical status ("well-formedness" according to
syntactic formalisms) extracted from the literature, as well as its
acceptability status ("intuitive goodness" as determined by native speakers)
obtained through crowdsourcing, with highest experimental standards. Even in
its preliminary form, this dataset stands as the largest of its kind that is
publicly accessible. We also offer preliminary analyses addressing three
debates in linguistics and computational linguistics: We observe that
grammaticality and acceptability judgments converge in about 83% of the cases
and that "in-betweenness" occurs frequently. This corroborates existing
research. We also find that while machine learning models struggle with
predicting grammaticality, they perform considerably better in predicting
acceptability. This is a novel finding. Future work will focus on expanding the
dataset.

</details>


### [50] [$^{\infty}$: Clause Purification, Embedding Realignment, and the Total Suppression of the Em Dash in Autoregressive Language Models](https://arxiv.org/abs/2506.18129)
*Bugra Kilictas,Faruk Alpay*

Main category: cs.CL

TL;DR: The paper identifies a vulnerability in autoregressive transformers caused by the em dash token, leading to semantic drift and clause boundary issues. It proposes a solution combining symbolic purification and embedding realignment, validated by improved generation consistency.


<details>
  <summary>Details</summary>
Motivation: To address the critical vulnerability in autoregressive transformers where the em dash token causes recursive semantic drift and embedding space entanglement, impacting long-form generation.

Method: Formal analysis of token-level perturbations, symbolic clause purification via the phi-infinity operator, and targeted embedding matrix realignment.

Result: Experimental validation shows significant improvements in generation consistency and topic maintenance.

Conclusion: The work provides a framework for mitigating token-level vulnerabilities in foundation models, with broader implications for AI safety and robust deployment.

Abstract: We identify a critical vulnerability in autoregressive transformer language
models where the em dash token induces recursive semantic drift, leading to
clause boundary hallucination and embedding space entanglement. Through formal
analysis of token-level perturbations in semantic lattices, we demonstrate that
em dash insertion fundamentally alters the model's latent representations,
causing compounding errors in long-form generation. We propose a novel solution
combining symbolic clause purification via the phi-infinity operator with
targeted embedding matrix realignment. Our approach enables total suppression
of problematic tokens without requiring model retraining, while preserving
semantic coherence through fixed-point convergence guarantees. Experimental
validation shows significant improvements in generation consistency and topic
maintenance. This work establishes a general framework for identifying and
mitigating token-level vulnerabilities in foundation models, with immediate
implications for AI safety, model alignment, and robust deployment of large
language models in production environments. The methodology extends beyond
punctuation to address broader classes of recursive instabilities in neural
text generation systems.

</details>


### [51] [Sparse Feature Coactivation Reveals Composable Semantic Modules in Large Language Models](https://arxiv.org/abs/2506.18141)
*Ruixuan Deng,Xiaoyang Hu,Miles Gilberti,Shane Storks,Aman Taxali,Mike Angstadt,Chandra Sripada,Joyce Chai*

Main category: cs.CL

TL;DR: The paper identifies and manipulates semantic components in LLMs using SAE features, showing predictable changes in outputs and modular knowledge organization.


<details>
  <summary>Details</summary>
Motivation: To understand and manipulate the modular organization of knowledge in large language models (LLMs) efficiently.

Method: Uses coactivation of sparse autoencoder (SAE) features from few prompts, focusing on country-relation tasks, and ablates/amplifies components to observe effects.

Result: Ablating or amplifying semantic components alters model outputs predictably; relation components are concentrated in later layers with stronger causal impact.

Conclusion: LLMs have modular knowledge organization, and targeted manipulation methods are effective.

Abstract: We identify semantically coherent, context-consistent network components in
large language models (LLMs) using coactivation of sparse autoencoder (SAE)
features collected from just a handful of prompts. Focusing on country-relation
tasks, we show that ablating semantic components for countries and relations
changes model outputs in predictable ways, while amplifying these components
induces counterfactual responses. Notably, composing relation and country
components yields compound counterfactual outputs. We find that, whereas most
country components emerge from the very first layer, the more abstract relation
components are concentrated in later layers. Furthermore, within relation
components themselves, nodes from later layers tend to have a stronger causal
impact on model outputs. Overall, these findings suggest a modular organization
of knowledge within LLMs and advance methods for efficient, targeted model
manipulation.

</details>


### [52] [QuranMorph: Morphologically Annotated Quranic Corpus](https://arxiv.org/abs/2506.18148)
*Diyam Akra,Tymaa Hammouda,Mustafa Jarrar*

Main category: cs.CL

TL;DR: The QuranMorph corpus is a manually annotated morphological corpus for the Quran, featuring lemmatization and part-of-speech tagging by experts, linked with extensive linguistic resources.


<details>
  <summary>Details</summary>
Motivation: To create a high-quality, open-source morphological corpus for the Quran, enabling integration with other linguistic tools and resources.

Method: Manual lemmatization and POS tagging by three linguists, using the Qabas lexicographic database and the SAMA/Qabas tagset (40 tags).

Result: A publicly available corpus (77,429 tokens) inter-linked with multiple linguistic resources.

Conclusion: The QuranMorph corpus provides a valuable, open-source resource for Quranic studies and linguistic research.

Abstract: We present the QuranMorph corpus, a morphologically annotated corpus for the
Quran (77,429 tokens). Each token in the QuranMorph was manually lemmatized and
tagged with its part-of-speech by three expert linguists. The lemmatization
process utilized lemmas from Qabas, an Arabic lexicographic database linked
with 110 lexicons and corpora of 2 million tokens. The part-of-speech tagging
was performed using the fine-grained SAMA/Qabas tagset, which encompasses 40
tags. As shown in this paper, this rich lemmatization and POS tagset enabled
the QuranMorph corpus to be inter-linked with many linguistic resources. The
corpus is open-source and publicly available as part of the SinaLab resources
at (https://sina.birzeit.edu/quran)

</details>


### [53] [CareLab at #SMM4H-HeaRD 2025: Insomnia Detection and Food Safety Event Extraction with Domain-Aware Transformers](https://arxiv.org/abs/2506.18185)
*Zihan Liang,Ziwen Pan,Sumon Kanti Dey,Azra Ismail*

Main category: cs.CL

TL;DR: The paper describes a system for SMM4H-HeaRD 2025 tasks, excelling in Task 5 Subtask 1 with an F1 score of 0.958 using RoBERTa and GPT-4.


<details>
  <summary>Details</summary>
Motivation: To address challenges in detecting insomnia in clinical notes (Task 4) and extracting food safety events from news (Task 5).

Method: Used encoder-based models (e.g., RoBERTa) and GPT-4 for data augmentation, with tailored preprocessing and model adaptations.

Result: Achieved top performance in Task 5 Subtask 1 (F1=0.958).

Conclusion: The approach demonstrates effectiveness, particularly in Task 5, leveraging advanced NLP techniques.

Abstract: This paper presents our system for the SMM4H-HeaRD 2025 shared tasks,
specifically Task 4 (Subtasks 1, 2a, and 2b) and Task 5 (Subtasks 1 and 2).
Task 4 focused on detecting mentions of insomnia in clinical notes, while Task
5 addressed the extraction of food safety events from news articles. We
participated in all subtasks and report key findings across them, with
particular emphasis on Task 5 Subtask 1, where our system achieved strong
performance-securing first place with an F1 score of 0.958 on the test set. To
attain this result, we employed encoder-based models (e.g., RoBERTa), alongside
GPT-4 for data augmentation. This paper outlines our approach, including
preprocessing, model architecture, and subtask-specific adaptations

</details>


### [54] [Prompt Engineering Techniques for Mitigating Cultural Bias Against Arabs and Muslims in Large Language Models: A Systematic Review](https://arxiv.org/abs/2506.18199)
*Bushra Asseri,Estabrag Abdelaziz,Areej Al-Wabil*

Main category: cs.CL

TL;DR: A systematic review identifies five prompt engineering strategies to mitigate cultural bias in LLMs, particularly for Arabs and Muslims, with structured multi-step pipelines being most effective.


<details>
  <summary>Details</summary>
Motivation: Addressing understudied cultural bias in LLMs, especially towards Arabs and Muslims, to reduce harmful stereotypes and marginalization.

Method: Mixed-methods systematic review following PRISMA and Kitchenham's methodology, analyzing 8 empirical studies (2021-2024) on bias mitigation strategies.

Result: Five key approaches identified: cultural prompting, affective priming, self-debiasing, structured multi-step pipelines, and parameter-optimized continuous prompts. Structured pipelines showed highest effectiveness (87.7% bias reduction).

Conclusion: Prompt engineering is accessible for bias mitigation without model parameter access. Future research should focus on culturally adaptive techniques and integrating debiasing methods.

Abstract: Large language models have demonstrated remarkable capabilities across
various domains, yet concerns about cultural bias - particularly towards Arabs
and Muslims - pose significant ethical challenges by perpetuating harmful
stereotypes and marginalization. Despite growing recognition of bias in LLMs,
prompt engineering strategies specifically addressing Arab and Muslim
representation remain understudied. This mixed-methods systematic review
examines such techniques, offering evidence-based guidance for researchers and
practitioners. Following PRISMA guidelines and Kitchenham's systematic review
methodology, we analyzed 8 empirical studies published between 2021-2024
investigating bias mitigation strategies. Our findings reveal five primary
prompt engineering approaches: cultural prompting, affective priming,
self-debiasing techniques, structured multi-step pipelines, and
parameter-optimized continuous prompts. Although all approaches show potential
for reducing bias, effectiveness varied substantially across studies and bias
types. Evidence suggests that certain bias types may be more resistant to
prompt-based mitigation than others. Structured multi-step pipelines
demonstrated the highest overall effectiveness, achieving up to 87.7% reduction
in bias, though they require greater technical expertise. Cultural prompting
offers broader accessibility with substantial effectiveness. These results
underscore the accessibility of prompt engineering for mitigating cultural bias
without requiring access to model parameters. The limited number of studies
identified highlights a significant research gap in this critical area. Future
research should focus on developing culturally adaptive prompting techniques,
creating Arab and Muslim-specific evaluation resources, and integrating prompt
engineering with complementary debiasing methods to address deeper stereotypes
while maintaining model utility.

</details>


### [55] [Deciphering Emotions in Children Storybooks: A Comparative Analysis of Multimodal LLMs in Educational Applications](https://arxiv.org/abs/2506.18201)
*Bushra Asseri,Estabraq Abdelaziz,Maha Al Mogren,Tayef Alhefdhi,Areej Al-Wabil*

Main category: cs.CL

TL;DR: GPT-4o outperforms Gemini 1.5 Pro in emotion recognition for Arabic children's storybook illustrations, with a 59% F1-score using chain-of-thought prompting. Both models struggle with cultural nuances and ambiguous contexts.


<details>
  <summary>Details</summary>
Motivation: To address the lack of culturally responsive emotion recognition tools for Arabic language contexts in educational technologies.

Method: Evaluated GPT-4o and Gemini 1.5 Pro using three prompting strategies (zero-shot, few-shot, chain-of-thought) on 75 Arabic storybook images, comparing results with human annotations based on Plutchik's framework.

Result: GPT-4o achieved a 59% macro F1-score with chain-of-thought prompting, while Gemini's best was 43%. Both models had systematic errors, especially with valence inversions and cultural nuances.

Conclusion: Current models lack cultural understanding; culturally sensitive training is needed for effective emotion-aware educational tools in Arabic contexts.

Abstract: Emotion recognition capabilities in multimodal AI systems are crucial for
developing culturally responsive educational technologies, yet remain
underexplored for Arabic language contexts where culturally appropriate
learning tools are critically needed. This study evaluates the emotion
recognition performance of two advanced multimodal large language models,
GPT-4o and Gemini 1.5 Pro, when processing Arabic children's storybook
illustrations. We assessed both models across three prompting strategies
(zero-shot, few-shot, and chain-of-thought) using 75 images from seven Arabic
storybooks, comparing model predictions with human annotations based on
Plutchik's emotional framework. GPT-4o consistently outperformed Gemini across
all conditions, achieving the highest macro F1-score of 59% with
chain-of-thought prompting compared to Gemini's best performance of 43%. Error
analysis revealed systematic misclassification patterns, with valence
inversions accounting for 60.7% of errors, while both models struggled with
culturally nuanced emotions and ambiguous narrative contexts. These findings
highlight fundamental limitations in current models' cultural understanding and
emphasize the need for culturally sensitive training approaches to develop
effective emotion-aware educational technologies for Arabic-speaking learners.

</details>


### [56] [Enhancing Entity Aware Machine Translation with Multi-task Learning](https://arxiv.org/abs/2506.18318)
*An Trieu,Phuong Nguyen,Minh Le Nguyen*

Main category: cs.CL

TL;DR: Proposes multi-task learning for Entity-aware machine translation (EAMT) to improve performance by jointly optimizing entity recognition and translation.


<details>
  <summary>Details</summary>
Motivation: Addresses the challenges of EAMT, including data scarcity and contextual complexity.

Method: Uses multi-task learning to optimize entity recognition and machine translation subtasks.

Result: Evaluated on SemEval 2025 Task 2 dataset, showing improved performance.

Conclusion: Multi-task learning enhances EAMT by jointly handling entity recognition and translation.

Abstract: Entity-aware machine translation (EAMT) is a complicated task in natural
language processing due to not only the shortage of translation data related to
the entities needed to translate but also the complexity in the context needed
to process while translating those entities. In this paper, we propose a method
that applies multi-task learning to optimize the performance of the two
subtasks named entity recognition and machine translation, which improves the
final performance of the Entity-aware machine translation task. The result and
analysis are performed on the dataset provided by the organizer of Task 2 of
the SemEval 2025 competition.

</details>


### [57] [TranslationCorrect: A Unified Framework for Machine Translation Post-Editing with Predictive Error Assistance](https://arxiv.org/abs/2506.18337)
*Syed Mekael Wasti,Shou-Yi Hung,Christopher Collins,En-Shiun Annie Lee*

Main category: cs.CL

TL;DR: TranslationCorrect is an integrated framework combining MT generation, error prediction, and post-editing to streamline workflows for translators and researchers, improving efficiency and satisfaction.


<details>
  <summary>Details</summary>
Motivation: To address inefficient, disconnected workflows in MT post-editing and research data collection.

Method: Combines MT generation (e.g., NLLB), automated error prediction (e.g., XCOMET/LLM APIs), and an intuitive post-editing interface, designed with HCI principles.

Result: Significantly improves translation efficiency and user satisfaction, confirmed by a user study.

Conclusion: TranslationCorrect offers a unified solution for efficient post-editing and high-quality research data collection.

Abstract: Machine translation (MT) post-editing and research data collection often rely
on inefficient, disconnected workflows. We introduce TranslationCorrect, an
integrated framework designed to streamline these tasks. TranslationCorrect
combines MT generation using models like NLLB, automated error prediction using
models like XCOMET or LLM APIs (providing detailed reasoning), and an intuitive
post-editing interface within a single environment. Built with human-computer
interaction (HCI) principles in mind to minimize cognitive load, as confirmed
by a user study. For translators, it enables them to correct errors and batch
translate efficiently. For researchers, TranslationCorrect exports high-quality
span-based annotations in the Error Span Annotation (ESA) format, using an
error taxonomy inspired by Multidimensional Quality Metrics (MQM). These
outputs are compatible with state-of-the-art error detection models and
suitable for training MT or post-editing systems. Our user study confirms that
TranslationCorrect significantly improves translation efficiency and user
satisfaction over traditional annotation methods.

</details>


### [58] [Less Data Less Tokens: Multilingual Unification Learning for Efficient Test-Time Reasoning in LLMs](https://arxiv.org/abs/2506.18341)
*Kang Chen,Mengdi Zhang,Yixin Cao*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper explores the challenges of test-time scaling of large language
models (LLMs), regarding both the data and inference efficiency. We highlight
the diversity of multi-lingual reasoning based on our pilot studies, and then
introduce a novel approach, \(L^2\) multi-lingual unification learning with a
decoding intervention strategy for further investigation. The basic idea of
\(L^2\) is that the reasoning process varies across different languages, which
may be mutually beneficial to enhance both model performance and efficiency. In
specific, there are two types of multi-lingual data: the entire long
chain-of-thought annotations in different languages and the step-wise mixture
of languages. By further tuning based on them, we show that even small amounts
of data can significantly improve reasoning capabilities. Our findings suggest
that multilingual learning reduces both the required data and the number of
inference tokens while maintaining a comparable performance. Furthermore,
\(L^2\) is orthogonal to other data efficient methods. Thus, we also emphasize
the importance of diverse data selection. The \(L^2\) method offers a promising
solution to the challenges of data collection and test-time compute efficiency
in LLMs.

</details>


### [59] [Evaluating Causal Explanation in Medical Reports with LLM-Based and Human-Aligned Metrics](https://arxiv.org/abs/2506.18387)
*Yousang Cho,Key-Sun Choi*

Main category: cs.CL

TL;DR: This study compares six metrics for evaluating causal explanations in diagnostic reports, finding GPT-Black and GPT-White align best with expert assessments, while similarity-based metrics perform poorly.


<details>
  <summary>Details</summary>
Motivation: To determine which evaluation metrics best capture the quality of causal explanations in diagnostic reports, ensuring clinical validity and logical coherence.

Method: Six metrics (BERTScore, Cosine Similarity, BioSentVec, GPT-White, GPT-Black, expert assessment) were tested on two report types (observation-based, multiple-choice-based) with two weighting strategies (task-specific, equal weights).

Result: GPT-Black showed the strongest discriminative power for coherent and valid narratives, GPT-White aligned well with experts, and similarity-based metrics diverged from clinical reasoning quality.

Conclusion: Metric selection and weighting significantly impact evaluation outcomes, supporting LLM-based metrics for interpretability and causal reasoning tasks.

Abstract: This study investigates how accurately different evaluation metrics capture
the quality of causal explanations in automatically generated diagnostic
reports. We compare six metrics: BERTScore, Cosine Similarity, BioSentVec,
GPT-White, GPT-Black, and expert qualitative assessment across two input types:
observation-based and multiple-choice-based report generation. Two weighting
strategies are applied: one reflecting task-specific priorities, and the other
assigning equal weights to all metrics. Our results show that GPT-Black
demonstrates the strongest discriminative power in identifying logically
coherent and clinically valid causal narratives. GPT-White also aligns well
with expert evaluations, while similarity-based metrics diverge from clinical
reasoning quality. These findings emphasize the impact of metric selection and
weighting on evaluation outcomes, supporting the use of LLM-based evaluation
for tasks requiring interpretability and causal reasoning.

</details>


### [60] [Lemmatization as a Classification Task: Results from Arabic across Multiple Genres](https://arxiv.org/abs/2506.18399)
*Mostafa Saeed,Nizar Habash*

Main category: cs.CL

TL;DR: The paper introduces two novel lemmatization approaches for Arabic, framing it as classification into a Lemma-POS-Gloss (LPG) tagset using machine translation and semantic clustering. It also presents a new test set and evaluates sequence-to-sequence models, finding classification and clustering more robust.


<details>
  <summary>Details</summary>
Motivation: Existing lemmatization tools for morphologically rich languages like Arabic face challenges due to inconsistent standards and limited genre coverage.

Method: Two approaches: classification into LPG tagset using machine translation and semantic clustering, and evaluation of sequence-to-sequence models.

Result: Classification and clustering yield more robust, interpretable outputs, setting new benchmarks. Sequence-to-sequence models are competitive but limited.

Conclusion: The proposed methods outperform existing tools, offering better robustness and interpretability for Arabic lemmatization.

Abstract: Lemmatization is crucial for NLP tasks in morphologically rich languages with
ambiguous orthography like Arabic, but existing tools face challenges due to
inconsistent standards and limited genre coverage. This paper introduces two
novel approaches that frame lemmatization as classification into a
Lemma-POS-Gloss (LPG) tagset, leveraging machine translation and semantic
clustering. We also present a new Arabic lemmatization test set covering
diverse genres, standardized alongside existing datasets. We evaluate character
level sequence-to-sequence models, which perform competitively and offer
complementary value, but are limited to lemma prediction (not LPG) and prone to
hallucinating implausible forms. Our results show that classification and
clustering yield more robust, interpretable outputs, setting new benchmarks for
Arabic lemmatization.

</details>


### [61] [TReB: A Comprehensive Benchmark for Evaluating Table Reasoning Capabilities of Large Language Models](https://arxiv.org/abs/2506.18421)
*Ce Li,Xiaofan Liu,Zhiyan Song,Ce Chi,Chen Zhao,Jingjing Yang,Zhendong Wang,Kexin Yang,Boshen Shi,Xing Wang,Chao Deng,Junlan Feng*

Main category: cs.CL

TL;DR: The paper introduces TReB, a benchmark for evaluating LLMs on table reasoning tasks, covering 26 sub-tasks, and tests 20 LLMs using three inference modes.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack effectiveness in evaluating LLMs' broad table reasoning abilities due to hidden semantics and complexity of table-structured data.

Method: Developed TReB, a comprehensive benchmark with a high-quality dataset and evaluation framework (TCoT, PoT, ICoT) to measure shallow and deep table reasoning.

Result: Experiments show LLMs have significant room for improvement in handling complex table-related tasks.

Conclusion: TReB effectively benchmarks LLMs' table reasoning, with datasets and framework publicly available for further research.

Abstract: The majority of data in businesses and industries is stored in tables,
databases, and data warehouses. Reasoning with table-structured data poses
significant challenges for large language models (LLMs) due to its hidden
semantics, inherent complexity, and structured nature. One of these challenges
is lacking an effective evaluation benchmark fairly reflecting the performances
of LLMs on broad table reasoning abilities. In this paper, we fill in this gap,
presenting a comprehensive table reasoning evolution benchmark, TReB, which
measures both shallow table understanding abilities and deep table reasoning
abilities, a total of 26 sub-tasks. We construct a high quality dataset through
an iterative data processing procedure. We create an evaluation framework to
robustly measure table reasoning capabilities with three distinct inference
modes, TCoT, PoT and ICoT. Further, we benchmark over 20 state-of-the-art LLMs
using this frame work and prove its effectiveness. Experimental results reveal
that existing LLMs still have significant room for improvement in addressing
the complex and real world Table related tasks. Both the dataset and evaluation
framework are publicly available, with the dataset hosted on [HuggingFace] and
the framework on [GitHub].

</details>


### [62] [MeRF: Motivation-enhanced Reinforcement Finetuning for Large Reasoning Models](https://arxiv.org/abs/2506.18485)
*Junjie Zhang,Guozheng Ma,Shunyu Liu,Haoyu Wang,Jiaxing Huang,Ting-En Lin,Fei Huang,Yongbin Li,Dacheng Tao*

Main category: cs.CL

TL;DR: MeRF combines reinforcement learning with in-context learning to improve LLM reasoning by embedding reward rules in prompts, outperforming baselines on logic puzzles.


<details>
  <summary>Details</summary>
Motivation: Existing RLVR methods ignore LLMs' in-context learning ability, which is crucial for reasoning tasks like CoT prompting. MeRF aims to integrate reinforcement learning with this capability.

Method: MeRF injects reward specifications into prompts, using them as in-context motivations to align LLM outputs with optimization goals.

Result: MeRF significantly outperforms baselines on the K&K benchmark and adapts to misleading motivations via reinforcement learning.

Conclusion: Combining reinforcement learning with in-context learning (MeRF) enhances LLM reasoning, with performance tied to motivation-reward consistency.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a
powerful learn-to-reason paradigm for Large Language Models (LLMs) to tackle
complex reasoning tasks. However, existing RLVR methods overlook one of the
most distinctive capabilities of LLMs, their in-context learning ability, as
prominently demonstrated by the success of Chain-of-Thought (CoT) prompting.
This motivates us to explore how reinforcement learning can be effectively
combined with in-context learning to better improve the reasoning capabilities
of LLMs. In this paper, we introduce Motivation-enhanced Reinforcement
Finetuning} (MeRF), an intuitive yet effective method enhancing reinforcement
learning of LLMs by involving ``telling LLMs the rules of the game''.
Specifically, MeRF directly injects the reward specification into the prompt,
which serves as an in-context motivation for model to improve its responses
with awareness of the optimization objective. This simple modification
leverages the in-context learning ability of LLMs aligning generation with
optimization, thereby incentivizing the model to generate desired outputs from
both inner motivation and external reward. Empirical evaluations on the Knights
and Knaves~(K&K) logic puzzle reasoning benchmark demonstrate that
\texttt{MeRF} achieves substantial performance gains over baselines. Moreover,
ablation studies show that performance improves with greater consistency
between the in-context motivation and the external reward function, while the
model also demonstrates an ability to adapt to misleading motivations through
reinforcement learning.

</details>


### [63] [Comparative Evaluation of ChatGPT and DeepSeek Across Key NLP Tasks: Strengths, Weaknesses, and Domain-Specific Performance](https://arxiv.org/abs/2506.18501)
*Wael Etaiwi,Bushra Alhijawi*

Main category: cs.CL

TL;DR: The study evaluates ChatGPT and DeepSeek across five NLP tasks, revealing DeepSeek's strength in classification and logic, while ChatGPT excels in nuanced understanding.


<details>
  <summary>Details</summary>
Motivation: To comprehensively assess the strengths, weaknesses, and domain-specific abilities of LLMs like ChatGPT and DeepSeek in diverse NLP applications.

Method: A structured experimental protocol with identical, neutral prompts and two benchmark datasets per task (sentiment analysis, topic classification, text summarization, machine translation, textual entailment).

Result: DeepSeek performs better in classification stability and logical reasoning; ChatGPT outperforms in tasks requiring nuanced understanding and flexibility.

Conclusion: The findings guide the selection of the most suitable LLM based on specific task requirements.

Abstract: The increasing use of large language models (LLMs) in natural language
processing (NLP) tasks has sparked significant interest in evaluating their
effectiveness across diverse applications. While models like ChatGPT and
DeepSeek have shown strong results in many NLP domains, a comprehensive
evaluation is needed to understand their strengths, weaknesses, and
domain-specific abilities. This is critical as these models are applied to
various tasks, from sentiment analysis to more nuanced tasks like textual
entailment and translation. This study aims to evaluate ChatGPT and DeepSeek
across five key NLP tasks: sentiment analysis, topic classification, text
summarization, machine translation, and textual entailment. A structured
experimental protocol is used to ensure fairness and minimize variability. Both
models are tested with identical, neutral prompts and evaluated on two
benchmark datasets per task, covering domains like news, reviews, and
formal/informal texts. The results show that DeepSeek excels in classification
stability and logical reasoning, while ChatGPT performs better in tasks
requiring nuanced understanding and flexibility. These findings provide
valuable insights for selecting the appropriate LLM based on task requirements.

</details>


### [64] [End-to-End Spoken Grammatical Error Correction](https://arxiv.org/abs/2506.18532)
*Mengjie Qian,Rao Ma,Stefano Bann,Mark J. F. Gales,Kate M. Knill*

Main category: cs.CL

TL;DR: The paper explores End-to-End (E2E) frameworks for Spoken Grammatical Error Correction (SGEC), addressing challenges like data scarcity and error propagation, and proposes solutions like pseudo-labeling and reference alignment to improve performance.


<details>
  <summary>Details</summary>
Motivation: SGEC is crucial for L2 learners but faces challenges like disfluencies and transcription errors. Cascaded systems are error-prone, motivating the need for robust E2E solutions.

Method: The study compares cascaded, partial-cascaded, and E2E architectures using Whisper. It introduces pseudo-labeling for data augmentation, contextual ASR output, and a novel reference alignment for feedback precision.

Result: Experiments on LNG and S&I corpora show significant performance improvements in E2E SGEC with the proposed methods.

Conclusion: E2E SGEC systems, enhanced by pseudo-labeling and reference alignment, outperform traditional cascaded approaches, offering better feedback for L2 learners.

Abstract: Grammatical Error Correction (GEC) and feedback play a vital role in
supporting second language (L2) learners, educators, and examiners. While
written GEC is well-established, spoken GEC (SGEC), aiming to provide feedback
based on learners' speech, poses additional challenges due to disfluencies,
transcription errors, and the lack of structured input. SGEC systems typically
follow a cascaded pipeline consisting of Automatic Speech Recognition (ASR),
disfluency detection, and GEC, making them vulnerable to error propagation
across modules. This work examines an End-to-End (E2E) framework for SGEC and
feedback generation, highlighting challenges and possible solutions when
developing these systems. Cascaded, partial-cascaded and E2E architectures are
compared, all built on the Whisper foundation model. A challenge for E2E
systems is the scarcity of GEC labeled spoken data. To address this, an
automatic pseudo-labeling framework is examined, increasing the training data
from 77 to over 2500 hours. To improve the accuracy of the SGEC system,
additional contextual information, exploiting the ASR output, is investigated.
Candidate feedback of their mistakes is an essential step to improving
performance. In E2E systems the SGEC output must be compared with an estimate
of the fluent transcription to obtain the feedback. To improve the precision of
this feedback, a novel reference alignment process is proposed that aims to
remove hypothesised edits that results from fluent transcription errors.
Finally, these approaches are combined with an edit confidence estimation
approach, to exclude low-confidence edits. Experiments on the in-house
Linguaskill (LNG) corpora and the publicly available Speak & Improve (S&I)
corpus show that the proposed approaches significantly boost E2E SGEC
performance.

</details>


### [65] [When Fine-Tuning Fails: Lessons from MS MARCO Passage Ranking](https://arxiv.org/abs/2506.18535)
*Manu Pande,Shahil Kumar,Anay Yatin Damle*

Main category: cs.CL

TL;DR: Fine-tuning pre-trained transformer models degrades performance on the MS MARCO passage ranking task, underperforming the base model due to disrupted embedding space structure.


<details>
  <summary>Details</summary>
Motivation: To investigate why fine-tuning harms performance on the MS MARCO task despite conventional wisdom favoring transfer learning.

Method: Comprehensive experiments with five model variants, including full fine-tuning and LoRA adaptations, analyzed via UMAP visualizations and training dynamics.

Result: All fine-tuning approaches underperformed the base model (MRR@10: 0.3026), with evidence of disrupted embedding space structure.

Conclusion: Fine-tuning may not always improve performance on saturated benchmarks, suggesting a need for architectural innovations.

Abstract: This paper investigates the counterintuitive phenomenon where fine-tuning
pre-trained transformer models degrades performance on the MS MARCO passage
ranking task. Through comprehensive experiments involving five model
variants-including full parameter fine-tuning and parameter efficient LoRA
adaptations-we demonstrate that all fine-tuning approaches underperform the
base sentence-transformers/all- MiniLM-L6-v2 model (MRR@10: 0.3026). Our
analysis reveals that fine-tuning disrupts the optimal embedding space
structure learned during the base model's extensive pre-training on 1 billion
sentence pairs, including 9.1 million MS MARCO samples. UMAP visualizations
show progressive embedding space flattening, while training dynamics analysis
and computational efficiency metrics further support our findings. These
results challenge conventional wisdom about transfer learning effectiveness on
saturated benchmarks and suggest architectural innovations may be necessary for
meaningful improvements.

</details>


### [66] [A Modular Taxonomy for Hate Speech Definitions and Its Impact on Zero-Shot LLM Classification Performance](https://arxiv.org/abs/2506.18576)
*Matteo Melis,Gabriella Lapesa,Dennis Assenmacher*

Main category: cs.CL

TL;DR: The paper explores hate speech definitions, organizes them into a taxonomy, and evaluates how different definitions affect LLM performance in zero-shot settings.


<details>
  <summary>Details</summary>
Motivation: Address ambiguity in hate speech definitions and understand their impact on model performance.

Method: Collect and analyze hate speech definitions to create a taxonomy, then evaluate three LLMs on three datasets using these definitions.

Result: Model performance varies with definition specificity, but the effect is inconsistent across architectures.

Conclusion: Definition specificity impacts hate speech detection, but its effect depends on the model architecture.

Abstract: Detecting harmful content is a crucial task in the landscape of NLP
applications for Social Good, with hate speech being one of its most dangerous
forms. But what do we mean by hate speech, how can we define it, and how does
prompting different definitions of hate speech affect model performance? The
contribution of this work is twofold. At the theoretical level, we address the
ambiguity surrounding hate speech by collecting and analyzing existing
definitions from the literature. We organize these definitions into a taxonomy
of 14 Conceptual Elements-building blocks that capture different aspects of
hate speech definitions, such as references to the target of hate (individual
or groups) or of the potential consequences of it. At the experimental level,
we employ the collection of definitions in a systematic zero-shot evaluation of
three LLMs, on three hate speech datasets representing different types of data
(synthetic, human-in-the-loop, and real-world). We find that choosing different
definitions, i.e., definitions with a different degree of specificity in terms
of encoded elements, impacts model performance, but this effect is not
consistent across all architectures.

</details>


### [67] [Parallel Continuous Chain-of-Thought with Jacobi Iteration](https://arxiv.org/abs/2506.18582)
*Haoyi Wu,Zhihao Teng,Kewei Tu*

Main category: cs.CL

TL;DR: PCCoT improves continuous CoT by enabling parallel updates of latent thought tokens via Jacobi iteration, saving 50% time while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Sequential dependencies in continuous CoT slow down training; PCCoT aims to enhance efficiency via parallel processing.

Method: Proposes PCCoT using Jacobi iteration for parallel updates of latent thought tokens, optimizing iterations for balance.

Result: Achieves comparable/better performance with 50% time savings; shows improved training stability and robustness.

Conclusion: PCCoT effectively addresses efficiency issues in continuous CoT, offering practical benefits for training and inference.

Abstract: Continuous chain-of-thought has been shown to be effective in saving
reasoning tokens for large language models. By reasoning with continuous latent
thought tokens, continuous CoT is able to perform implicit reasoning in a
compact manner. However, the sequential dependencies between latent thought
tokens spoil parallel training, leading to long training time. In this paper,
we propose Parallel Continuous Chain-of-Thought (PCCoT), which performs Jacobi
iteration on the latent thought tokens, updating them iteratively in parallel
instead of sequentially and thus improving both training and inference
efficiency of continuous CoT. Experiments demonstrate that by choosing the
proper number of iterations, we are able to achieve comparable or even better
performance while saving nearly 50% of the training and inference time.
Moreover, PCCoT shows better stability and robustness in the training process.
Our code is available at https://github.com/whyNLP/PCCoT.

</details>


### [68] [Reply to "Emergent LLM behaviors are observationally equivalent to data leakage"](https://arxiv.org/abs/2506.18600)
*Ariel Flint Ashery,Luca Maria Aiello,Andrea Baronchelli*

Main category: cs.CL

TL;DR: The paper addresses data contamination concerns in LLM simulations but argues that emergent dynamics, like self-organization, can still be studied.


<details>
  <summary>Details</summary>
Motivation: To clarify that despite data contamination risks, genuinely emergent behaviors in LLM populations can be observed and studied.

Method: The paper references critiques and empirical observations, particularly in the context of social conventions, to support its argument.

Result: Empirical evidence shows that self-organization and emergent dynamics occur in LLM populations.

Conclusion: Data contamination is a concern but does not prevent the study of emergent behaviors in LLMs, as demonstrated in social conventions.

Abstract: A potential concern when simulating populations of large language models
(LLMs) is data contamination, i.e. the possibility that training data may shape
outcomes in unintended ways. While this concern is important and may hinder
certain experiments with multi-agent models, it does not preclude the study of
genuinely emergent dynamics in LLM populations. The recent critique by Barrie
and T\"ornberg [1] of the results of Flint Ashery et al. [2] offers an
opportunity to clarify that self-organisation and model-dependent emergent
dynamics can be studied in LLM populations, highlighting how such dynamics have
been empirically observed in the specific case of social conventions.

</details>


### [69] [Semantic similarity estimation for domain specific data using BERT and other techniques](https://arxiv.org/abs/2506.18602)
*R. Prashanth*

Main category: cs.CL

TL;DR: The paper evaluates semantic similarity estimation using USE, InferSent, and BERT, finding BERT outperforms others, especially on domain-specific data.


<details>
  <summary>Details</summary>
Motivation: Semantic similarity is crucial for NLP tasks like question answering and machine translation. The study aims to compare state-of-the-art techniques for this purpose.

Method: The study uses USE, InferSent, and BERT models on two datasets: a domain-specific in-house dataset and Quora's public question pairs dataset.

Result: BERT showed superior performance, attributed to its fine-tuning process, making it ideal for domain-specific data.

Conclusion: BERT is the best technique for semantic similarity estimation, particularly for domain-specific datasets.

Abstract: Estimation of semantic similarity is an important research problem both in
natural language processing and the natural language understanding, and that
has tremendous application on various downstream tasks such as question
answering, semantic search, information retrieval, document clustering,
word-sense disambiguation and machine translation. In this work, we carry out
the estimation of semantic similarity using different state-of-the-art
techniques including the USE (Universal Sentence Encoder), InferSent and the
most recent BERT, or Bidirectional Encoder Representations from Transformers,
models. We use two question pairs datasets for the analysis, one is a domain
specific in-house dataset and the other is a public dataset which is the
Quora's question pairs dataset. We observe that the BERT model gave much
superior performance as compared to the other methods. This should be because
of the fine-tuning procedure that is involved in its training process, allowing
it to learn patterns based on the training data that is used. This works
demonstrates the applicability of BERT on domain specific datasets. We infer
from the analysis that BERT is the best technique to use in the case of domain
specific data.

</details>


### [70] [The Anatomy of Speech Persuasion: Linguistic Shifts in LLM-Modified Speeches](https://arxiv.org/abs/2506.18621)
*Alisa Barkar,Mathieu Chollet,Matthieu Labeau,Beatrice Biancardi,Chloe Clavel*

Main category: cs.CL

TL;DR: The study explores how GPT-4o modifies speech transcripts to alter persuasiveness, revealing systematic stylistic changes rather than human-like optimization.


<details>
  <summary>Details</summary>
Motivation: To understand how large language models interpret and manipulate persuasiveness in public speaking.

Method: Modifies speech transcripts from a French competition, uses GPT-4o to enhance/diminish persuasiveness, and analyzes linguistic shifts with a new feature set.

Result: GPT-4o applies stylistic changes (e.g., emotional lexicon, syntactic structures) to amplify rhetorical impact, not human-like persuasiveness.

Conclusion: GPT-4o's approach to persuasiveness is systematic but lacks human-like optimization, focusing on stylistic enhancements.

Abstract: This study examines how large language models understand the concept of
persuasiveness in public speaking by modifying speech transcripts from PhD
candidates in the "Ma These en 180 Secondes" competition, using the 3MT French
dataset. Our contributions include a novel methodology and an interpretable
textual feature set integrating rhetorical devices and discourse markers. We
prompt GPT-4o to enhance or diminish persuasiveness and analyze linguistic
shifts between original and generated speech in terms of the new features.
Results indicate that GPT-4o applies systematic stylistic modifications rather
than optimizing persuasiveness in a human-like manner. Notably, it manipulates
emotional lexicon and syntactic structures (such as interrogative and
exclamatory clauses) to amplify rhetorical impact.

</details>


### [71] [ByteSpan: Information-Driven Subword Tokenisation](https://arxiv.org/abs/2506.18639)
*Zbulon Goriely,Suchir Salhan,Pietro Lesci,Julius Cheng,Paula Buttery*

Main category: cs.CL

TL;DR: ByteSpan, a new subword tokeniser, groups predictable byte sequences using an external byte-level LM, outperforming BPE in morphological alignment and efficiency.


<details>
  <summary>Details</summary>
Motivation: To explore if grouping predictable bytes, inspired by word segmentation models, can create a useful fixed subword vocabulary.

Method: Proposes ByteSpan, which uses an external byte-level LM to identify and group predictable byte sequences into subwords.

Result: ByteSpan produces efficient vocabularies with better morphological alignment than BPE for English and performs similarly in multilingual settings.

Conclusion: ByteSpan is an effective, information-driven tokeniser for subword vocabulary creation.

Abstract: Recent dynamic tokenisation methods operate directly on bytes and pool their
latent representations into patches. This bears similarities to computational
models of word segmentation that determine lexical boundaries using spikes in
an autoregressive model's prediction error. Inspired by this connection, we
explore whether grouping predictable bytes - rather than pooling their
representations - can yield a useful fixed subword vocabulary. We propose a new
information-driven subword tokeniser, ByteSpan, that uses an external
byte-level LM during training to identify contiguous predictable byte sequences
and group them into subwords. Experiments show that ByteSpan yields efficient
vocabularies with higher morphological alignment scores than BPE for English.
Multilingual experiments show similar compression and R\'enyi efficiency for 25
languages.

</details>


### [72] [Is There a Case for Conversation Optimized Tokenizers in Large Language Models?](https://arxiv.org/abs/2506.18674)
*Raquel Ferrando,Javier Conde,Gonzalo Martnez,Pedro Reviriego*

Main category: cs.CL

TL;DR: Optimizing tokenizers for chatbot conversations reduces token counts by 5-10%, saving energy with minimal impact on original corpus efficiency.


<details>
  <summary>Details</summary>
Motivation: The computational and energy costs of LLMs are high, and tokenizers optimized for training corpora may not perform well in chatbot interactions.

Method: Redesign tokenizer vocabularies using a chatbot conversation corpus and evaluate performance.

Result: Conversation-optimized tokenizers reduce tokens in dialogues by 5-10%, saving energy with minimal impact on original corpus efficiency.

Conclusion: Optimizing tokenizers for chatbot use can significantly reduce costs without harming performance on original tasks.

Abstract: The computational and energy costs of Large Language Models (LLMs) have
increased exponentially driven by the growing model sizes and the massive
adoption of LLMs by hundreds of millions of users. The unit cost of an LLM is
the computation of a token. Therefore, the tokenizer plays an important role in
the efficiency of a model, and they are carefully optimized to minimize the
number of tokens for the text in their training corpus. One of the most popular
applications of LLMs are chatbots that interact with users. A key observation
is that, for those chatbots, what is important is the performance of the
tokenizer in the user text input and the chatbot responses. Those are most
likely different from the text in the training corpus. So, a question that
immediately arises is whether there is a potential benefit in optimizing
tokenizers for chatbot conversations. In this paper, this idea is explored for
different tokenizers by using a publicly available corpus of chatbot
conversations to redesign their vocabularies and evaluate their performance in
this domain. The results show that conversation-optimized tokenizers
consistently reduce the number of tokens in chatbot dialogues, which can lead
to meaningful energy savings, in the range of 5% to 10% while having minimal or
even slightly positive impact on tokenization efficiency for the original
training corpus.

</details>


### [73] [Context Biasing for Pronunciations-Orthography Mismatch in Automatic Speech Recognition](https://arxiv.org/abs/2506.18703)
*Christian Huber,Alexander Waibel*

Main category: cs.CL

TL;DR: A method to correct substitution errors in neural sequence-to-sequence speech recognition systems improves accuracy for challenging words like named entities, with up to 11% relative improvement in biased word error rate.


<details>
  <summary>Details</summary>
Motivation: Existing systems struggle with words not seen during training, especially those with pronunciation-orthography mismatches.

Method: Proposes on-the-fly user corrections during inference to address substitution errors.

Result: Achieves up to 11% relative improvement in biased word error rate while maintaining overall performance.

Conclusion: The method effectively enhances recognition of challenging words without degrading general accuracy.

Abstract: Neural sequence-to-sequence systems deliver state-of-the-art performance for
automatic speech recognition. When using appropriate modeling units, e.g.,
byte-pair encoded characters, these systems are in principal open vocabulary
systems. In practice, however, they often fail to recognize words not seen
during training, e.g., named entities, acronyms, or domain-specific special
words. To address this problem, many context biasing methods have been
proposed; however, for words with a pronunciation-orthography mismatch, these
methods may still struggle. We propose a method which allows corrections of
substitution errors to improve the recognition accuracy of such challenging
words. Users can add corrections on the fly during inference. We show that with
this method we get a relative improvement in biased word error rate of up to
11\%, while maintaining a competitive overall word error rate.

</details>


### [74] [Benchmarking the Pedagogical Knowledge of Large Language Models](https://arxiv.org/abs/2506.18710)
*Maxime Lelivre,Amy Waldock,Meng Liu,Natalia Valds Aspillaga,Alasdair Mackintosh,Mara Jos Ogando Portelo,Jared Lee,Paul Atherton,Robin A. A. Ince,Oliver G. B. Garrod*

Main category: cs.CL

TL;DR: The paper introduces The Pedagogy Benchmark to evaluate AI models' pedagogical knowledge, addressing gaps in existing benchmarks focused only on content knowledge. It reports results for 97 models and provides an online leaderboard for interactive exploration.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack focus on pedagogical knowledge, a critical aspect for AI's role in education. This gap hinders the assessment of models' teaching-related abilities.

Method: The benchmark uses questions from professional teacher exams, covering pedagogical subdomains like teaching strategies and assessment methods. It evaluates models on Cross-Domain Pedagogical Knowledge (CDPK) and Special Education Needs and Disability (SEND) knowledge.

Result: Model accuracies ranged from 28% to 89%. The study analyzes cost-accuracy trade-offs and tracks progress over time. An online leaderboard allows filtering by model properties and performance.

Conclusion: Pedagogical benchmarks are essential for responsible AI deployment in education, guiding development and policy to address the global learning crisis effectively.

Abstract: Benchmarks like Massive Multitask Language Understanding (MMLU) have played a
pivotal role in evaluating AI's knowledge and abilities across diverse domains.
However, existing benchmarks predominantly focus on content knowledge, leaving
a critical gap in assessing models' understanding of pedagogy - the method and
practice of teaching. This paper introduces The Pedagogy Benchmark, a novel
dataset designed to evaluate large language models on their Cross-Domain
Pedagogical Knowledge (CDPK) and Special Education Needs and Disability (SEND)
pedagogical knowledge. These benchmarks are built on a carefully curated set of
questions sourced from professional development exams for teachers, which cover
a range of pedagogical subdomains such as teaching strategies and assessment
methods. Here we outline the methodology and development of these benchmarks.
We report results for 97 models, with accuracies spanning a range from 28% to
89% on the pedagogical knowledge questions. We consider the relationship
between cost and accuracy and chart the progression of the Pareto value
frontier over time. We provide online leaderboards at
https://rebrand.ly/pedagogy which are updated with new models and allow
interactive exploration and filtering based on various model properties, such
as cost per token and open-vs-closed weights, as well as looking at performance
in different subjects. LLMs and generative AI have tremendous potential to
influence education and help to address the global learning crisis.
Education-focused benchmarks are crucial to measure models' capacities to
understand pedagogical concepts, respond appropriately to learners' needs, and
support effective teaching practices across diverse contexts. They are needed
for informing the responsible and evidence-based deployment of LLMs and
LLM-based tools in educational settings, and for guiding both development and
policy decisions.

</details>


### [75] [Semantic-Preserving Adversarial Attacks on LLMs: An Adaptive Greedy Binary Search Approach](https://arxiv.org/abs/2506.18756)
*Chong Zhang,Xiang Li,Jia Wang,Shan Liang,Haochen Xue,Xiaobo Jin*

Main category: cs.CL

TL;DR: The paper introduces the Adaptive Greedy Binary Search (AGBS) method to improve prompt engineering in LLMs by balancing semantic stability and optimization efficacy.


<details>
  <summary>Details</summary>
Motivation: Addressing the issue of misinterpretations in automated prompt optimizations that distort user intentions and produce errors.

Method: Proposes AGBS to simulate prompt optimization while preserving semantic stability, dynamically evaluating its impact on LLM performance.

Result: Demonstrates AGBS's effectiveness in balancing semantic consistency and attack efficacy through experiments on open and closed-source LLMs.

Conclusion: Offers insights for designing more reliable prompt optimization systems, with code publicly available.

Abstract: Large Language Models (LLMs) increasingly rely on automatic prompt
engineering in graphical user interfaces (GUIs) to refine user inputs and
enhance response accuracy. However, the diversity of user requirements often
leads to unintended misinterpretations, where automated optimizations distort
original intentions and produce erroneous outputs. To address this challenge,
we propose the Adaptive Greedy Binary Search (AGBS) method, which simulates
common prompt optimization mechanisms while preserving semantic stability. Our
approach dynamically evaluates the impact of such strategies on LLM
performance, enabling robust adversarial sample generation. Through extensive
experiments on open and closed-source LLMs, we demonstrate AGBS's effectiveness
in balancing semantic consistency and attack efficacy. Our findings offer
actionable insights for designing more reliable prompt optimization systems.
Code is available at: https://github.com/franz-chang/DOBS

</details>


### [76] [ASP2LJ : An Adversarial Self-Play Laywer Augmented Legal Judgment Framework](https://arxiv.org/abs/2506.18768)
*Ao Chang,Tong Zhou,Yubo Chen,Delai Qiu,Shengping Liu,Kang Liu,Jun Zhao*

Main category: cs.CL

TL;DR: ASP2LJ framework addresses long-tail data and lawyer argumentation in Legal Judgment Prediction (LJP) by integrating case generation and adversarial self-play, improving judicial accuracy.


<details>
  <summary>Details</summary>
Motivation: Current LJP systems face challenges like imbalanced data and neglect of lawyers' roles, limiting judicial fairness and accuracy.

Method: Proposes ASP2LJ with a case generation module for data imbalance and adversarial self-play to enhance lawyer arguments.

Result: Shows effectiveness on SimuCourt and RareCases datasets, improving judicial decision objectivity.

Conclusion: ASP2LJ offers a novel solution for LJP, supported by a rare-case dataset and open resources for future research.

Abstract: Legal Judgment Prediction (LJP) aims to predict judicial outcomes, including
relevant legal charge, terms, and fines, which is a crucial process in Large
Language Model(LLM). However, LJP faces two key challenges: (1)Long Tail
Distribution: Current datasets, derived from authentic cases, suffer from high
human annotation costs and imbalanced distributions, leading to model
performance degradation. (2)Lawyer's Improvement: Existing systems focus on
enhancing judges' decision-making but neglect the critical role of lawyers in
refining arguments, which limits overall judicial accuracy. To address these
issues, we propose an Adversarial Self-Play Lawyer Augmented Legal Judgment
Framework, called ASP2LJ, which integrates a case generation module to tackle
long-tailed data distributions and an adversarial self-play mechanism to
enhance lawyers' argumentation skills. Our framework enables a judge to
reference evolved lawyers' arguments, improving the objectivity, fairness, and
rationality of judicial decisions. Besides, We also introduce RareCases, a
dataset for rare legal cases in China, which contains 120 tail-end cases. We
demonstrate the effectiveness of our approach on the SimuCourt dataset and our
RareCases dataset. Experimental results show our framework brings improvements,
indicating its utilization. Our contributions include an integrated framework,
a rare-case dataset, and publicly releasing datasets and code to support
further research in automated judicial systems.

</details>


### [77] [Existing LLMs Are Not Self-Consistent For Simple Tasks](https://arxiv.org/abs/2506.18781)
*Zhenru Lin,Jiawen Tao,Yang Yuan,Andrew Chi-Chih Yao*

Main category: cs.CL

TL;DR: The paper investigates self-consistency in LLMs, revealing inconsistencies even in simple tasks. It introduces metrics and methods to quantify and mitigate these issues, though challenges remain.


<details>
  <summary>Details</summary>
Motivation: To ensure LLMs' decisions are transparent and trustworthy by addressing self-consistency, as even state-of-the-art models exhibit contradictions in reasoning.

Method: Proposes two automated methods: a graph-based and an energy-based approach to quantify and mitigate inconsistencies.

Result: Smaller models are highly inconsistent, and even advanced models like DeepSeek-R1 and GPT-o4-mini are not fully self-consistent. Partial improvements are achieved with the proposed methods.

Conclusion: Self-consistency is complex but crucial for reliable and interpretable AI. The study provides tools and insights, though further work is needed.

Abstract: Large Language Models (LLMs) have grown increasingly powerful, yet ensuring
their decisions remain transparent and trustworthy requires self-consistency --
no contradictions in their internal reasoning. Our study reveals that even on
simple tasks, such as comparing points on a line or a plane, or reasoning in a
family tree, all smaller models are highly inconsistent, and even
state-of-the-art models like DeepSeek-R1 and GPT-o4-mini are not fully
self-consistent. To quantify and mitigate these inconsistencies, we introduce
inconsistency metrics and propose two automated methods -- a graph-based and an
energy-based approach. While these fixes provide partial improvements, they
also highlight the complexity and importance of self-consistency in building
more reliable and interpretable AI. The code and data are available at
https://github.com/scorpio-nova/llm-self-consistency.

</details>


### [78] [RWESummary: A Framework and Test for Choosing Large Language Models to Summarize Real-World Evidence (RWE) Studies](https://arxiv.org/abs/2506.18819)
*Arjun Mukerji,Michael L. Jackson,Jason Jones,Neil Sanghavi*

Main category: cs.CL

TL;DR: RWESummary is introduced as a benchmark for evaluating LLMs in summarizing real-world evidence (RWE) studies, showing Gemini 2.5 models perform best.


<details>
  <summary>Details</summary>
Motivation: LLMs lack specific evaluation for summarizing RWE studies, prompting the need for RWESummary.

Method: RWESummary includes one scenario and three evaluations, developed using proprietary data, and compares LLM performance.

Result: Gemini 2.5 models (Flash and Pro) performed best in summarizing 13 RWE studies.

Conclusion: RWESummary is proposed as a foundational benchmark for RWE study summarization.

Abstract: Large Language Models (LLMs) have been extensively evaluated for general
summarization tasks as well as medical research assistance, but they have not
been specifically evaluated for the task of summarizing real-world evidence
(RWE) from structured output of RWE studies. We introduce RWESummary, a
proposed addition to the MedHELM framework (Bedi, Cui, Fuentes, Unell et al.,
2025) to enable benchmarking of LLMs for this task. RWESummary includes one
scenario and three evaluations covering major types of errors observed in
summarization of medical research studies and was developed using Atropos
Health proprietary data. Additionally, we use RWESummary to compare the
performance of different LLMs in our internal RWE summarization tool. At the
time of publication, with 13 distinct RWE studies, we found the Gemini 2.5
models performed best overall (both Flash and Pro). We suggest RWESummary as a
novel and useful foundation model benchmark for real-world evidence study
summarization.

</details>


### [79] [MLLP-VRAIN UPV system for the IWSLT 2025 Simultaneous Speech Translation Translation task](https://arxiv.org/abs/2506.18828)
*Jorge Iranzo-Snchez,Javier Iranzo-Snchez,Adri Gimnez,Jorge Civera,Alfons Juan*

Main category: cs.CL

TL;DR: The MLLP-VRAIN group's IWSLT 2025 submission uses a modular cascade system with pre-trained models (Whisper for ASR, NLLB for MT) for real-time long-form speech translation, achieving a balance of quality (31.96 BLEU) and latency (2.94s).


<details>
  <summary>Details</summary>
Motivation: To address the challenges of real-time translation for long-form speech without extensive in-domain data or end-to-end training.

Method: Combines Whisper Large-V3-Turbo (ASR) and NLLB-3.3B (MT) with lightweight adaptation, prefix training, adaptive emission policies (wait-$k$, RALCP), and buffer management.

Result: Achieves 31.96 BLEU on ACL60/60 and 29.8 BLEU on IWSLT25Instruct, with 2.94s latency.

Conclusion: Pre-trained models with careful adaptation can create effective simultaneous translation systems for long-form content.

Abstract: This work describes the participation of the MLLP-VRAIN research group in the
shared task of the IWSLT 2025 Simultaneous Speech Translation track. Our
submission addresses the unique challenges of real-time translation of
long-form speech by developing a modular cascade system that adapts strong
pre-trained models to streaming scenarios. We combine Whisper Large-V3-Turbo
for ASR with the multilingual NLLB-3.3B model for MT, implementing lightweight
adaptation techniques rather than training new end-to-end models from scratch.
Our approach employs document-level adaptation with prefix training to enhance
the MT model's ability to handle incomplete inputs, while incorporating
adaptive emission policies including a wait-$k$ strategy and RALCP for managing
the translation stream. Specialized buffer management techniques and
segmentation strategies ensure coherent translations across long audio
sequences. Experimental results on the ACL60/60 dataset demonstrate that our
system achieves a favorable balance between translation quality and latency,
with a BLEU score of 31.96 and non-computational-aware StreamLAAL latency of
2.94 seconds. Our final model achieves a preliminary score on the official test
set (IWSLT25Instruct) of 29.8 BLEU. Our work demonstrates that carefully
adapted pre-trained components can create effective simultaneous translation
systems for long-form content without requiring extensive in-domain parallel
data or specialized end-to-end training.

</details>


### [80] [STU-PID: Steering Token Usage via PID Controller for Efficient Large Language Model Reasoning](https://arxiv.org/abs/2506.18831)
*Aryasomayajula Ram Bharadwaj*

Main category: cs.CL

TL;DR: STUPID dynamically adjusts reasoning steps in large language models using a PID controller, improving accuracy by 6% and reducing token usage by 32%.


<details>
  <summary>Details</summary>
Motivation: Overthinking in extended chain-of-thought reasoning increases costs and may degrade performance; static methods lack adaptability.

Method: STUPID uses a PID controller and chunk-level classifier to dynamically adjust steering strength based on redundancy probability.

Result: 6% accuracy improvement and 32% token reduction on GSM8K, outperforming static baselines.

Conclusion: STUPID offers a principled, training-free framework for efficient dynamic reasoning calibration.

Abstract: Large Language Models employing extended chain-of-thought (CoT) reasoning
often suffer from the overthinking phenomenon, generating excessive and
redundant reasoning steps that increase computational costs while potentially
degrading performance. While recent work has explored static steering
approaches to mitigate this issue, they lack the adaptability to dynamically
adjust intervention strength based on real-time reasoning quality. We propose
STUPID (Steering Token Usage via PID controller), a novel training-free method
that employs a PID controller to dynamically modulate activation steering
strength during inference. Our approach combines a chunk-level classifier for
detecting redundant reasoning patterns with a PID control mechanism that
adaptively adjusts steering intensity based on the predicted redundancy
probability. Experimental evaluation on GSM8K demonstrates that STUPID achieves
a 6% improvement in accuracy while reducing token usage by 32%, outperforming
static steering baselines. Our method provides a principled framework for
dynamic reasoning calibration that maintains reasoning quality while
significantly improving computational efficiency.

</details>


### [81] [LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement Learning](https://arxiv.org/abs/2506.18841)
*Yuhao Wu,Yushi Bai,Zhiqiang Hu,Roy Ka-Wei Lee,Juanzi Li*

Main category: cs.CL

TL;DR: Proposes an incentivization-based RL approach for ultra-long text generation in LLMs, outperforming traditional SFT methods without synthetic data.


<details>
  <summary>Details</summary>
Motivation: Addresses challenges in ultra-long text generation, such as length limits and quality degradation, avoiding costly synthetic data dependency.

Method: Uses reinforcement learning (RL) with specialized reward models to train LLMs for long-form writing, starting from scratch.

Result: LongWriter-Zero model achieves state-of-the-art performance on benchmarks, surpassing larger models.

Conclusion: RL-based training without synthetic data is effective for high-quality ultra-long text generation.

Abstract: Ultra-long generation by large language models (LLMs) is a widely demanded
scenario, yet it remains a significant challenge due to their maximum
generation length limit and overall quality degradation as sequence length
increases. Previous approaches, exemplified by LongWriter, typically rely on
''teaching'', which involves supervised fine-tuning (SFT) on synthetic
long-form outputs. However, this strategy heavily depends on synthetic SFT
data, which is difficult and costly to construct, often lacks coherence and
consistency, and tends to be overly artificial and structurally monotonous. In
this work, we propose an incentivization-based approach that, starting entirely
from scratch and without relying on any annotated or synthetic data, leverages
reinforcement learning (RL) to foster the emergence of ultra-long, high-quality
text generation capabilities in LLMs. We perform RL training starting from a
base model, similar to R1-Zero, guiding it to engage in reasoning that
facilitates planning and refinement during the writing process. To support
this, we employ specialized reward models that steer the LLM towards improved
length control, writing quality, and structural formatting. Experimental
evaluations show that our LongWriter-Zero model, trained from Qwen2.5-32B,
consistently outperforms traditional SFT methods on long-form writing tasks,
achieving state-of-the-art results across all metrics on WritingBench and
Arena-Write, and even surpassing 100B+ models such as DeepSeek R1 and
Qwen3-235B. We open-source our data and model checkpoints under
https://huggingface.co/THU-KEG/LongWriter-Zero-32B

</details>


### [82] [Mechanistic Interpretability Needs Philosophy](https://arxiv.org/abs/2506.18852)
*Iwan Williams,Ninell Oldenburg,Ruchira Dhar,Joshua Hatherley,Constanza Fierro,Nina Rajcic,Sandrine R. Schiller,Filippos Stamatiou,Anders Sgaard*

Main category: cs.CL

TL;DR: The paper argues for integrating philosophy into mechanistic interpretability (MI) to clarify concepts, refine methods, and assess ethical stakes, using three open problems as examples.


<details>
  <summary>Details</summary>
Motivation: To highlight the need for philosophy in MI to address assumptions, concepts, and ethical implications, not just technical aspects.

Method: The paper takes three open problems from MI literature to demonstrate how philosophy can enhance MI research.

Result: Philosophy can significantly contribute to MI by improving conceptual clarity, methodological rigor, and ethical evaluation.

Conclusion: A deeper interdisciplinary dialogue between MI and philosophy is essential for advancing the field.

Abstract: Mechanistic interpretability (MI) aims to explain how neural networks work by
uncovering their underlying causal mechanisms. As the field grows in influence,
it is increasingly important to examine not just models themselves, but the
assumptions, concepts and explanatory strategies implicit in MI research. We
argue that mechanistic interpretability needs philosophy: not as an
afterthought, but as an ongoing partner in clarifying its concepts, refining
its methods, and assessing the epistemic and ethical stakes of interpreting AI
systems. Taking three open problems from the MI literature as examples, this
position paper illustrates the value philosophy can add to MI research, and
outlines a path toward deeper interdisciplinary dialogue.

</details>


### [83] [CommVQ: Commutative Vector Quantization for KV Cache Compression](https://arxiv.org/abs/2506.18879)
*Junyan Li,Yang Zhang,Muhammad Yusuf Hassan,Talha Chafekar,Tianle Cai,Zhile Ren,Pengsheng Guo,Foroozan Karimzadeh,Colorado Reed,Chong Wang,Chuang Gan*

Main category: cs.CL

TL;DR: Proposes Commutative Vector Quantization (CommVQ) to reduce KV cache memory usage in LLMs, enabling efficient long-context inference with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Address the memory bottleneck caused by KV cache in GPUs for long-context LLM inference.

Method: Uses additive quantization with a lightweight encoder and RoPE-commutative codebook, trained via EM algorithm, to compress KV cache.

Result: Reduces FP16 KV cache size by 87.5% with 2-bit quantization and enables 1-bit quantization with minimal accuracy loss.

Conclusion: CommVQ is effective for memory-efficient long-context LLM inference, outperforming state-of-the-art methods.

Abstract: Large Language Models (LLMs) are increasingly used in applications requiring
long context lengths, but the key-value (KV) cache often becomes a memory
bottleneck on GPUs as context grows. To address this, we propose Commutative
Vector Quantization (CommVQ) to significantly reduce memory usage for
long-context LLM inference. We first introduce additive quantization with a
lightweight encoder and codebook to compress the KV cache, which can be decoded
via simple matrix multiplication. To further reduce computational costs during
decoding, we design the codebook to be commutative with Rotary Position
Embedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm.
This enables efficient integration of decoding into the self-attention
mechanism. Our approach achieves high accuracy with additive quantization and
low overhead via the RoPE-commutative codebook. Experiments on long-context
benchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5%
with 2-bit quantization, while outperforming state-of-the-art KV cache
quantization methods. Notably, it enables 1-bit KV cache quantization with
minimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context
length on a single RTX 4090 GPU. The source code is available at:
https://github.com/UMass-Embodied-AGI/CommVQ.

</details>


### [84] [OMEGA: Can LLMs Reason Outside the Box in Math? Evaluating Exploratory, Compositional, and Transformative Generalization](https://arxiv.org/abs/2506.18880)
*Yiyou Sun,Shawn Hu,Georgia Zhou,Ken Zheng,Hannaneh Hajishirzi,Nouha Dziri,Dawn Song*

Main category: cs.CL

TL;DR: OMEGA benchmark evaluates LLMs' out-of-distribution generalization in math, revealing limitations in exploratory, compositional, and transformative reasoning.


<details>
  <summary>Details</summary>
Motivation: To address LLMs' reliance on narrow strategies and struggles with novel problem-solving in math.

Method: Introduces OMEGA, a benchmark with programmatically generated problems across math domains, evaluating three generalization axes.

Result: Frontier LLMs show sharp performance drops with complexity; fine-tuned models improve exploratory but not compositional or transformative reasoning.

Conclusion: OMEGA identifies LLM limitations, paving the way for progress toward genuine mathematical creativity.

Abstract: Recent large-scale language models (LLMs) with long Chain-of-Thought
reasoning-such as DeepSeek-R1-have achieved impressive results on
Olympiad-level mathematics benchmarks. However, they often rely on a narrow set
of strategies and struggle with problems that require a novel way of thinking.
To systematically investigate these limitations, we introduce
OMEGA-Out-of-distribution Math Problems Evaluation with 3 Generalization Axes-a
controlled yet diverse benchmark designed to evaluate three axes of
out-of-distribution generalization, inspired by Boden's typology of creativity:
(1) Exploratory-applying known problem solving skills to more complex instances
within the same problem domain; (2) Compositional-combining distinct reasoning
skills, previously learned in isolation, to solve novel problems that require
integrating these skills in new and coherent ways; and (3)
Transformative-adopting novel, often unconventional strategies by moving beyond
familiar approaches to solve problems more effectively. OMEGA consists of
programmatically generated training-test pairs derived from templated problem
generators across geometry, number theory, algebra, combinatorics, logic, and
puzzles, with solutions verified using symbolic, numerical, or graphical
methods. We evaluate frontier (or top-tier) LLMs and observe sharp performance
degradation as problem complexity increases. Moreover, we fine-tune the
Qwen-series models across all generalization settings and observe notable
improvements in exploratory generalization, while compositional generalization
remains limited and transformative reasoning shows little to no improvement. By
isolating and quantifying these fine-grained failures, OMEGA lays the
groundwork for advancing LLMs toward genuine mathematical creativity beyond
mechanical proficiency.

</details>


### [85] [ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought Reasoning in LLMs](https://arxiv.org/abs/2506.18896)
*Jiaru Zou,Ling Yang,Jingwen Gu,Jiahao Qiu,Ke Shen,Jingrui He,Mengdi Wang*

Main category: cs.CL

TL;DR: ReasonFlux-PRM is a trajectory-aware Process Reward Model designed to evaluate intermediate reasoning steps in LLMs, outperforming existing PRMs and human baselines in data quality and performance gains.


<details>
  <summary>Details</summary>
Motivation: Existing PRMs struggle to robustly evaluate intermediate reasoning steps, especially in trajectory-response outputs from advanced reasoning models like Deepseek-R1.

Method: ReasonFlux-PRM incorporates step-level and trajectory-level supervision for fine-grained reward assignment, supporting offline and online settings like data distillation, reinforcement learning, and test-time scaling.

Result: Empirical results show ReasonFlux-PRM-7B outperforms strong PRMs and human baselines, achieving average gains of 12.1% in supervised fine-tuning, 4.5% in reinforcement learning, and 6.3% in test-time scaling.

Conclusion: ReasonFlux-PRM is effective for evaluating reasoning traces and improves performance across multiple benchmarks, with a smaller version (1.5B) released for resource-constrained applications.

Abstract: Process Reward Models (PRMs) have recently emerged as a powerful framework
for supervising intermediate reasoning steps in large language models (LLMs).
Previous PRMs are primarily trained on model final output responses and
struggle to evaluate intermediate thinking trajectories robustly, especially in
the emerging setting of trajectory-response outputs generated by frontier
reasoning models like Deepseek-R1. In this work, we introduce ReasonFlux-PRM, a
novel trajectory-aware PRM explicitly designed to evaluate the
trajectory-response type of reasoning traces. ReasonFlux-PRM incorporates both
step-level and trajectory-level supervision, enabling fine-grained reward
assignment aligned with structured chain-of-thought data. We adapt
ReasonFlux-PRM to support reward supervision under both offline and online
settings, including (i) selecting high-quality model distillation data for
downstream supervised fine-tuning of smaller models, (ii) providing dense
process-level rewards for policy optimization during reinforcement learning,
and (iii) enabling reward-guided Best-of-N test-time scaling. Empirical results
on challenging downstream benchmarks such as AIME, MATH500, and GPQA-Diamond
demonstrate that ReasonFlux-PRM-7B selects higher quality data than strong PRMs
(e.g., Qwen2.5-Math-PRM-72B) and human-curated baselines. Furthermore, our
derived ReasonFlux-PRM-7B yields consistent performance improvements, achieving
average gains of 12.1% in supervised fine-tuning, 4.5% in reinforcement
learning, and 6.3% in test-time scaling. We also release our efficient
ReasonFlux-PRM-1.5B for resource-constrained applications and edge deployment.
Projects: https://github.com/Gen-Verse/ReasonFlux

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [86] [Mechanistic Interpretability of Diffusion Models: Circuit-Level Analysis and Causal Validation](https://arxiv.org/abs/2506.17237)
*Dip Roy*

Main category: cs.CV

TL;DR: The paper analyzes diffusion models' computational pathways and mechanisms in image generation, revealing differences in processing synthetic vs. naturalistic data, and identifies specialized attention mechanisms and computational bottlenecks.


<details>
  <summary>Details</summary>
Motivation: To understand the mechanistic principles and computational pathways underlying image generation in diffusion models, particularly for synthetic and naturalistic data.

Method: Conducted systematic intervention experiments on 2,000 synthetic and 2,000 CelebA facial images, measuring computational complexity and attention patterns.

Result: Found higher computational complexity for real-world face processing (complexity ratio = 1.084) and identified eight specialized attention mechanisms (e.g., edge detection, texture analysis). Targeted ablations caused 25.6% to 128.3% performance degradation.

Conclusion: The study provides quantitative foundations for understanding and controlling generative model behavior through mechanistic interventions.

Abstract: We present a quantitative circuit-level analysis of diffusion models,
establishing computational pathways and mechanistic principles underlying image
generation processes. Through systematic intervention experiments across 2,000
synthetic and 2,000 CelebA facial images, we discover fundamental algorithmic
differences in how diffusion architectures process synthetic versus
naturalistic data distributions. Our investigation reveals that real-world face
processing requires circuits with measurably higher computational complexity
(complexity ratio = 1.084 plus/minus 0.008, p < 0.001), exhibiting distinct
attention specialization patterns with entropy divergence ranging from 0.015 to
0.166 across denoising timesteps. We identify eight functionally distinct
attention mechanisms showing specialized computational roles: edge detection
(entropy = 3.18 plus/minus 0.12), texture analysis (entropy = 4.16 plus/minus
0.08), and semantic understanding (entropy = 2.67 plus/minus 0.15).
Intervention analysis demonstrates critical computational bottlenecks where
targeted ablations produce 25.6% to 128.3% performance degradation, providing
causal evidence for identified circuit functions. These findings establish
quantitative foundations for algorithmic understanding and control of
generative model behavior through mechanistic intervention strategies.

</details>


### [87] [SRKD: Towards Efficient 3D Point Cloud Segmentation via Structure- and Relation-aware Knowledge Distillation](https://arxiv.org/abs/2506.17290)
*Yuqi Li,Junhao Dong,Zeyu Dong,Chuanguang Yang,Zhulin An,Yongjun Xu*

Main category: cs.CV

TL;DR: Proposes SRKD, a knowledge distillation framework for efficient 3D point cloud segmentation, transferring knowledge from a large teacher to a lightweight student model.


<details>
  <summary>Details</summary>
Motivation: Addresses computational complexity and deployment limitations of large transformer-based models in 3D point cloud segmentation.

Method: Uses affinity matrix-based relation alignment, cross-sample mini-batch construction, KL divergence for semantic alignment, and ground-truth supervision.

Result: Achieves state-of-the-art performance with reduced model complexity (<15M parameters).

Conclusion: SRKD is effective and efficient for real-world deployment.

Abstract: 3D point cloud segmentation faces practical challenges due to the
computational complexity and deployment limitations of large-scale
transformer-based models. To address this, we propose a novel Structure- and
Relation-aware Knowledge Distillation framework, named SRKD, that transfers
rich geometric and semantic knowledge from a large frozen teacher model (>100M)
to a lightweight student model (<15M). Specifically, we propose an affinity
matrix-based relation alignment module, which distills structural dependencies
from the teacher to the student through point-wise similarity matching,
enhancing the student's capability to learn contextual interactions. Meanwhile,
we introduce a cross-sample mini-batch construction strategy that enables the
student to perceive stable and generalized geometric structure. This aligns
across diverse point cloud instances of the teacher, rather than within a
single sample. Additionally, KL divergence is applied to align semantic
distributions, and ground-truth supervision further reinforces accurate
segmentation. Our method achieves state of the art performance with
significantly reduced model complexity, demonstrating its effectiveness and
efficiency in real-world deployment scenarios. Our Code is available at
https://github.com/itsnotacie/SRKD.

</details>


### [88] [Fine-Scale Soil Mapping in Alaska with Multimodal Machine Learning](https://arxiv.org/abs/2506.17302)
*Yijun Lin,Theresa Chen,Colby Brungard,Grunwald Sabine,Sue Ives,Matt Macander,Timm Nawrocki,Yao-Yi Chiang,Nic Jelinski*

Main category: cs.CV

TL;DR: MISO, a vision-based ML model, outperforms Random Forest in fine-scale soil mapping for Alaska, improving permafrost monitoring and adaptation strategies.


<details>
  <summary>Details</summary>
Motivation: Accelerating permafrost thaw due to climate change threatens infrastructure and ecosystem services, necessitating high-resolution soil maps for better characterization and adaptation.

Method: MISO integrates a geospatial foundation model, implicit neural representations, and contrastive learning for continuous spatial prediction and multimodal alignment.

Result: MISO generalizes better to unseen locations and achieves higher recall than Random Forest, enhancing permafrost monitoring.

Conclusion: Advanced ML like MISO offers practical guidance for soil sampling and infrastructure planning in permafrost-affected regions.

Abstract: Fine-scale soil mapping in Alaska, traditionally relying on fieldwork and
localized simulations, remains a critical yet underdeveloped task, despite the
region's ecological importance and extensive permafrost coverage. As permafrost
thaw accelerates due to climate change, it threatens infrastructure stability
and key ecosystem services, such as soil carbon storage. High-resolution soil
maps are essential for characterizing permafrost distribution, identifying
vulnerable areas, and informing adaptation strategies. We present MISO, a
vision-based machine learning (ML) model to produce statewide fine-scale soil
maps for near-surface permafrost and soil taxonomy. The model integrates a
geospatial foundation model for visual feature extraction, implicit neural
representations for continuous spatial prediction, and contrastive learning for
multimodal alignment and geo-location awareness. We compare MISO with Random
Forest (RF), a traditional ML model that has been widely used in soil mapping
applications. Spatial cross-validation and regional analysis across Permafrost
Zones and Major Land Resource Areas (MLRAs) show that MISO generalizes better
to remote, unseen locations and achieves higher recall than RF, which is
critical for monitoring permafrost thaw and related environmental processes.
These findings demonstrate the potential of advanced ML approaches for
fine-scale soil mapping and provide practical guidance for future soil sampling
and infrastructure planning in permafrost-affected landscapes. The project will
be released at https://github.com/knowledge-computing/Peatland-permafrost.

</details>


### [89] [RadarSeq: A Temporal Vision Framework for User Churn Prediction via Radar Chart Sequences](https://arxiv.org/abs/2506.17325)
*Sina Najafi,M. Hadi Sepanj,Fahimeh Jafari*

Main category: cs.CV

TL;DR: A temporally-aware computer vision framework using radar chart images and a CNN-LSTM hybrid model improves churn prediction in gig platforms, outperforming classical and ViT-based methods.


<details>
  <summary>Details</summary>
Motivation: Churn prediction in non-subscription gig platforms is challenging due to implicit disengagement and lack of explicit labels, requiring methods that capture temporal behavior.

Method: Proposes a framework modeling user behavior as radar chart image sequences, combining a pretrained CNN encoder with a bidirectional LSTM for spatiotemporal pattern recognition.

Result: Outperforms baselines with gains of 17.7 in F1, 29.4 in precision, and 16.1 in AUC, while offering better interpretability.

Conclusion: The modular, explainable, and efficient framework is effective for large-scale churn modeling in dynamic gig platforms.

Abstract: Predicting user churn in non-subscription gig platforms, where disengagement
is implicit, poses unique challenges due to the absence of explicit labels and
the dynamic nature of user behavior. Existing methods often rely on aggregated
snapshots or static visual representations, which obscure temporal cues
critical for early detection. In this work, we propose a temporally-aware
computer vision framework that models user behavioral patterns as a sequence of
radar chart images, each encoding day-level behavioral features. By integrating
a pretrained CNN encoder with a bidirectional LSTM, our architecture captures
both spatial and temporal patterns underlying churn behavior. Extensive
experiments on a large real-world dataset demonstrate that our method
outperforms classical models and ViT-based radar chart baselines, yielding
gains of 17.7 in F1 score, 29.4 in precision, and 16.1 in AUC, along with
improved interpretability. The framework's modular design, explainability
tools, and efficient deployment characteristics make it suitable for
large-scale churn modeling in dynamic gig-economy platforms.

</details>


### [90] [P2MFDS: A Privacy-Preserving Multimodal Fall Detection System for Elderly People in Bathroom Environments](https://arxiv.org/abs/2506.17332)
*Haitian Wang,Yiren Wang,Xinyu Wang,Yumeng Miao,Yuliang Zhang,Yu Zhang,Atif Mansoor*

Main category: cs.CV

TL;DR: A multimodal fall detection system for elderly in bathrooms using radar and vibration sensing, outperforming unimodal methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the high fall risk for the aging population in bathrooms, where current unimodal systems lack accuracy due to environmental interference.

Method: Developed a sensor evaluation framework, fused millimeter-wave radar with 3D vibration sensing, and created a dual-stream network (P2MFDS) combining CNN-BiLSTM-Attention and multi-scale CNN-SEBlock-Self-Attention branches.

Result: P2MFDS achieved significant improvements in accuracy and recall over existing methods.

Conclusion: The proposed multimodal system effectively addresses limitations of unimodal approaches, offering a privacy-preserving solution for fall detection in bathrooms.

Abstract: By 2050, people aged 65 and over are projected to make up 16 percent of the
global population. As aging is closely associated with increased fall risk,
particularly in wet and confined environments such as bathrooms where over 80
percent of falls occur. Although recent research has increasingly focused on
non-intrusive, privacy-preserving approaches that do not rely on wearable
devices or video-based monitoring, these efforts have not fully overcome the
limitations of existing unimodal systems (e.g., WiFi-, infrared-, or
mmWave-based), which are prone to reduced accuracy in complex environments.
These limitations stem from fundamental constraints in unimodal sensing,
including system bias and environmental interference, such as multipath fading
in WiFi-based systems and drastic temperature changes in infrared-based
methods. To address these challenges, we propose a Privacy-Preserving
Multimodal Fall Detection System for Elderly People in Bathroom Environments.
First, we develop a sensor evaluation framework to select and fuse
millimeter-wave radar with 3D vibration sensing, and use it to construct and
preprocess a large-scale, privacy-preserving multimodal dataset in real
bathroom settings, which will be released upon publication. Second, we
introduce P2MFDS, a dual-stream network combining a CNN-BiLSTM-Attention branch
for radar motion dynamics with a multi-scale CNN-SEBlock-Self-Attention branch
for vibration impact detection. By uniting macro- and micro-scale features,
P2MFDS delivers significant gains in accuracy and recall over state-of-the-art
approaches. Code and pretrained models will be made available at:
https://github.com/HaitianWang/P2MFDS-A-Privacy-Preserving-Multimodal-Fall-Detection-Network-for-Elderly-Individuals-in-Bathroom.

</details>


### [91] [A Novel Multi-layer Task-centric and Data Quality Framework for Autonomous Driving](https://arxiv.org/abs/2506.17346)
*Yuhan Zhou,Haihua Chen,Kewei Sha*

Main category: cs.CV

TL;DR: The paper proposes a task-centric, data quality (DQ) framework for next-gen autonomous vehicles (AVs) to address overlooked DQ issues, mapping DQ to task requirements and performance goals. A case study on the nuScenes dataset demonstrates improved object detection by addressing redundancy.


<details>
  <summary>Details</summary>
Motivation: Current AV research and practice focus heavily on models/algorithms while undervaluing data quality (DQ), which is critical for functionality, efficiency, and trustworthiness in dynamic environments.

Method: A five-layer framework (data, DQ, task, application, goal) is proposed to map DQ with task requirements. A case study on the nuScenes dataset evaluates redundancy in multisource/multimodal data.

Result: Partial removal of redundancy in image data improved YOLOv8 object detection performance. Analysis revealed DQ issues in multimodal (image and LiDAR) data.

Conclusion: The framework addresses unexplored challenges in DQ, task orchestration, and performance-oriented AV development, guiding the community toward adaptive, explainable, and resilient AV systems.

Abstract: The next-generation autonomous vehicles (AVs), embedded with frequent
real-time decision-making, will rely heavily on a large volume of multisource
and multimodal data. In real-world settings, the data quality (DQ) of different
sources and modalities usually varies due to unexpected environmental factors
or sensor issues. However, both researchers and practitioners in the AV field
overwhelmingly concentrate on models/algorithms while undervaluing the DQ. To
fulfill the needs of the next-generation AVs with guarantees of functionality,
efficiency, and trustworthiness, this paper proposes a novel task-centric and
data quality vase framework which consists of five layers: data layer, DQ
layer, task layer, application layer, and goal layer. The proposed framework
aims to map DQ with task requirements and performance goals. To illustrate, a
case study investigating redundancy on the nuScenes dataset proves that
partially removing redundancy on multisource image data could improve YOLOv8
object detection task performance. Analysis on multimodal data of image and
LiDAR further presents existing redundancy DQ issues. This paper opens up a
range of critical but unexplored challenges at the intersection of DQ, task
orchestration, and performance-oriented system development in AVs. It is
expected to guide the AV community toward building more adaptive, explainable,
and resilient AVs that respond intelligently to dynamic environments and
heterogeneous data streams. Code, data, and implementation details are publicly
available at: https://anonymous.4open.science/r/dq4av-framework/README.md.

</details>


### [92] [Efficient Feedback Gate Network for Hyperspectral Image Super-Resolution](https://arxiv.org/abs/2506.17361)
*Xufei Wang,Mingjian Zhang,Fei Ge,Jinchen Zhu,Wen Sha,Jifen Ren,Zhimeng Hou,Shouguo Zheng,ling Zheng,Shizhuang Weng*

Main category: cs.CV

TL;DR: A novel group-based SHSR method, the efficient feedback gate network, improves hyperspectral image resolution by leveraging feedbacks, gate operations, and spatial-spectral reinforcement.


<details>
  <summary>Details</summary>
Motivation: Existing SHSR methods underperform due to insufficient exploration of band coherence and spatial-spectral information.

Method: Proposes a feedback gate network with large kernel convolutions, spectral interactions, and modules like SPDFM and SSRGM for enhanced feature extraction.

Result: Outperforms state-of-the-art methods on three datasets in spectral fidelity and spatial reconstruction.

Conclusion: The proposed method effectively enhances hyperspectral image resolution by integrating advanced spatial-spectral feature learning.

Abstract: Even without auxiliary images, single hyperspectral image super-resolution
(SHSR) methods can be designed to improve the spatial resolution of
hyperspectral images. However, failing to explore coherence thoroughly along
bands and spatial-spectral information leads to the limited performance of the
SHSR. In this study, we propose a novel group-based SHSR method termed the
efficient feedback gate network, which uses various feedbacks and gate
operations involving large kernel convolutions and spectral interactions. In
particular, by providing different guidance for neighboring groups, we can
learn rich band information and hierarchical hyperspectral spatial information
using channel shuffling and dilatation convolution in shuffled and progressive
dilated fusion module(SPDFM). Moreover, we develop a wide-bound perception gate
block and a spectrum enhancement gate block to construct the spatial-spectral
reinforcement gate module (SSRGM) and obtain highly representative
spatial-spectral features efficiently. Additionally, we apply a
three-dimensional SSRGM to enhance holistic information and coherence for
hyperspectral data. The experimental results on three hyperspectral datasets
demonstrate the superior performance of the proposed network over the
state-of-the-art methods in terms of spectral fidelity and spatial content
reconstruction.

</details>


### [93] [From Drawings to Decisions: A Hybrid Vision-Language Framework for Parsing 2D Engineering Drawings into Structured Manufacturing Knowledge](https://arxiv.org/abs/2506.17374)
*Muhammad Tayyab Khan,Lequn Chen,Zane Yong,Jun Ming Tan,Wenhe Feng,Seung Ki Moon*

Main category: cs.CV

TL;DR: A hybrid vision-language framework combining YOLOv11-OBB and transformer-based VLMs (Donut, Florence-2) is proposed for efficient extraction of key information from 2D engineering drawings, achieving high accuracy and practical utility in digital manufacturing.


<details>
  <summary>Details</summary>
Motivation: Manual extraction of key information from 2D engineering drawings is slow and error-prone, while generic OCR models struggle with complex layouts and symbols.

Method: The framework integrates YOLOv11-OBB for rotation-aware object detection and a fine-tuned VLM (Donut or Florence-2) for parsing localized annotations into structured outputs.

Result: Donut outperforms Florence-2 with 88.5% precision, 99.2% recall, and a 93.5% F1-score, demonstrating high accuracy in parsing annotations.

Conclusion: The proposed framework effectively modernizes 2D drawing interpretation, supporting downstream manufacturing tasks with reliable structured outputs.

Abstract: Efficient and accurate extraction of key information from 2D engineering
drawings is essential for advancing digital manufacturing workflows. Such
information includes geometric dimensioning and tolerancing (GD&T), measures,
material specifications, and textual annotations. Manual extraction is slow and
labor-intensive, while generic OCR models often fail due to complex layouts,
engineering symbols, and rotated text, leading to incomplete and unreliable
outputs. These limitations result in incomplete and unreliable outputs. To
address these challenges, we propose a hybrid vision-language framework that
integrates a rotation-aware object detection model (YOLOv11-obb) with a
transformer-based vision-language parser. Our structured pipeline applies
YOLOv11-OBB to localize annotations and extract oriented bounding box (OBB)
patches, which are then parsed into structured outputs using a fine-tuned,
lightweight vision-language model (VLM). We curate a dataset of 1,367 2D
mechanical drawings annotated across nine key categories. YOLOv11-OBB is
trained on this dataset to detect OBBs and extract annotation patches. These
are parsed using two open-source VLMs: Donut and Florence-2. Both models are
lightweight and well-suited for specialized industrial tasks under limited
computational overhead. Following fine-tuning of both models on the curated
dataset of image patches paired with structured annotation labels, a
comparative experiment is conducted to evaluate parsing performance across four
key metrics. Donut outperforms Florence-2, achieving 88.5% precision, 99.2%
recall, and a 93.5% F1-score, with a hallucination rate of 11.5%. Finally, a
case study demonstrates how the extracted structured information supports
downstream manufacturing tasks such as process and tool selection, showcasing
the practical utility of the proposed framework in modernizing 2D drawing
interpretation.

</details>


### [94] [Spatial-Temporal Pre-Training for Embryo Viability Prediction Using Time-Lapse Videos](https://arxiv.org/abs/2506.17403)
*Zhiyi Shi,Junsik Kim,Helen Y. Yang,Yonghyun Song,Hyun-Jic Oh,Dalit Ben-Yosef,Daniel Needleman,Hanspeter Pfister*

Main category: cs.CV

TL;DR: Proposes Spatial-Temporal Pre-Training (STPT) for embryo viability prediction in IVF, addressing challenges of long videos and temporal misalignment with a two-stage SSL approach.


<details>
  <summary>Details</summary>
Motivation: Automating embryo viability prediction is crucial for IVF, but limited labeled data and challenges in handling long, variable-length embryo videos hinder progress.

Method: STPT uses a two-stage SSL approach: spatial (intra-video alignment) and temporal (inter-video embedding relationships), avoiding frame-by-frame alignment to reduce memory use.

Result: Achieves highest AUC of 0.635 on 23,027 videos (3,286 labeled), outperforming baselines with limited resources.

Conclusion: STPT effectively addresses challenges in embryo video analysis, improving viability prediction with efficient resource use.

Abstract: Automating embryo viability prediction for in vitro fertilization (IVF) is
important but challenging due to the limited availability of labeled pregnancy
outcome data, as only a small fraction of embryos are labeled after transfer.
Self-supervised learning (SSL) can leverage both labeled and unlabeled data to
improve prediction. However, existing SSL methods for videos are not directly
applicable to embryo development videos due to two challenges: (1) embryo
time-lapse videos contain hundreds of frames, requiring significant GPU memory
for conventional SSL; (2) the dataset contains videos with varying lengths and
many outlier frames, causing traditional video alignment methods to struggle
with semantic misalignment. We propose Spatial-Temporal Pre-Training (STPT) to
address these challenges. STPT includes two stages: spatial and temporal. In
each stage, only one encoder is trained while the other is frozen, reducing
memory demands. To handle temporal misalignment, STPT avoids frame-by-frame
alignment across videos. The spatial stage learns from alignments within each
video and its temporally consistent augmentations. The temporal stage then
models relationships between video embeddings. Our method efficiently handles
long videos and temporal variability. On 23,027 time-lapse videos (3,286
labeled), STPT achieves the highest AUC of 0.635 (95% CI: 0.632-0.638) compared
to baselines, with limited computational resources.

</details>


### [95] [VMRA-MaR: An Asymmetry-Aware Temporal Framework for Longitudinal Breast Cancer Risk Prediction](https://arxiv.org/abs/2506.17412)
*Zijun Sun,Solveig Thrun,Michael Kampffmeyer*

Main category: cs.CV

TL;DR: The paper proposes a Vision Mamba RNN (VMRNN) with state-space models and LSTM-like memory to improve breast cancer risk prediction by capturing temporal trends in imaging data, enhanced by an asymmetry module for bilateral differences.


<details>
  <summary>Details</summary>
Motivation: Breast cancer detection relies on screening programs, but current models often miss temporal dynamics in longitudinal data. The goal is to improve risk prediction by leveraging evolving trends in breast tissue.

Method: Uses VMRNN with SSM and LSTM-like memory for temporal dynamics, plus an asymmetry module (SAD and LAT) to detect bilateral differences.

Result: Notably improves cancer onset prediction, especially for high-density breasts, and excels at extended time points (years four and five).

Conclusion: The framework advances early breast cancer recognition and supports personalized screening strategies.

Abstract: Breast cancer remains a leading cause of mortality worldwide and is typically
detected via screening programs where healthy people are invited in regular
intervals. Automated risk prediction approaches have the potential to improve
this process by facilitating dynamically screening of high-risk groups. While
most models focus solely on the most recent screening, there is growing
interest in exploiting temporal information to capture evolving trends in
breast tissue, as inspired by clinical practice. Early methods typically relied
on two time steps, and although recent efforts have extended this to multiple
time steps using Transformer architectures, challenges remain in fully
harnessing the rich temporal dynamics inherent in longitudinal imaging data. In
this work, we propose to instead leverage Vision Mamba RNN (VMRNN) with a
state-space model (SSM) and LSTM-like memory mechanisms to effectively capture
nuanced trends in breast tissue evolution. To further enhance our approach, we
incorporate an asymmetry module that utilizes a Spatial Asymmetry Detector
(SAD) and Longitudinal Asymmetry Tracker (LAT) to identify clinically relevant
bilateral differences. This integrated framework demonstrates notable
improvements in predicting cancer onset, especially for the more challenging
high-density breast cases and achieves superior performance at extended time
points (years four and five), highlighting its potential to advance early
breast cancer recognition and enable more personalized screening strategies.
Our code is available at https://github.com/Mortal-Suen/VMRA-MaR.git.

</details>


### [96] [Trans${^2}$-CBCT: A Dual-Transformer Framework for Sparse-View CBCT Reconstruction](https://arxiv.org/abs/2506.17425)
*Minmin Yang,Huantao Ren,Senem Velipasalar*

Main category: cs.CV

TL;DR: The paper introduces Trans-CBCT and Trans$^2$-CBCT, hybrid CNN-Transformer models for sparse-view CBCT reconstruction, outperforming baselines in PSNR and SSIM metrics.


<details>
  <summary>Details</summary>
Motivation: Addressing severe artifacts and poor spatial coverage in sparse-view CBCT by leveraging global context and volumetric coherence.

Method: Uses TransUNet for hybrid CNN-Transformer features, multi-scale feature combination, and a neighbor-aware Point Transformer for volumetric coherence.

Result: Trans-CBCT improves PSNR by 1.17 dB and SSIM by 0.0163; Trans$^2$-CBCT adds 0.63 dB PSNR and 0.0117 SSIM.

Conclusion: Combining CNN-Transformer features with point-based geometry reasoning effectively enhances sparse-view CBCT reconstruction.

Abstract: Cone-beam computed tomography (CBCT) using only a few X-ray projection views
enables faster scans with lower radiation dose, but the resulting severe
under-sampling causes strong artifacts and poor spatial coverage. We address
these challenges in a unified framework. First, we replace conventional
UNet/ResNet encoders with TransUNet, a hybrid CNN-Transformer model.
Convolutional layers capture local details, while self-attention layers enhance
global context. We adapt TransUNet to CBCT by combining multi-scale features,
querying view-specific features per 3D point, and adding a lightweight
attenuation-prediction head. This yields Trans-CBCT, which surpasses prior
baselines by 1.17 dB PSNR and 0.0163 SSIM on the LUNA16 dataset with six views.
Second, we introduce a neighbor-aware Point Transformer to enforce volumetric
coherence. This module uses 3D positional encoding and attention over k-nearest
neighbors to improve spatial consistency. The resulting model, Trans$^2$-CBCT,
provides an additional gain of 0.63 dB PSNR and 0.0117 SSIM. Experiments on
LUNA16 and ToothFairy show consistent gains from six to ten views, validating
the effectiveness of combining CNN-Transformer features with point-based
geometry reasoning for sparse-view CBCT reconstruction.

</details>


### [97] [Enhancing Wireless Device Identification through RF Fingerprinting: Leveraging Transient Energy Spectrum Analysis](https://arxiv.org/abs/2506.17439)
*Nisar Ahmed,Gulshan Saleem,Hafiz Muhammad Shahzad Asif,Muhammad Usman Younus,Kalsoom Safdar*

Main category: cs.CV

TL;DR: The paper proposes a hybrid deep learning model (CNN-Bi-GRU) for identifying RF devices using transient energy spectrum analysis, achieving high accuracy.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of IoT and 5G networks has increased the need for accurate identification and classification of radiation devices in complex electromagnetic environments.

Method: The research uses General Linear Chirplet Transform for feature extraction from RF devices and a CNN-Bi-GRU model for classification, tested on a dataset of nine RF devices.

Result: The model achieved 99.33% precision, 99.53% recall, 99.43% F1-score, and 99.17% accuracy in 10-fold cross-validation.

Conclusion: The CNN-Bi-GRU model is effective for RF device identification, offering potential for improved device management in wireless environments.

Abstract: In recent years, the rapid growth of the Internet of Things technologies and
the widespread adoption of 5G wireless networks have led to an exponential
increase in the number of radiation devices operating in complex
electromagnetic environments. A key challenge in managing and securing these
devices is accurate identification and classification. To address this
challenge, specific emitter identification techniques have emerged as a
promising solution that aims to provide reliable and efficient means of
identifying individual radiation devices in a unified and standardized manner.
This research proposes an approach that leverages transient energy spectrum
analysis using the General Linear Chirplet Transform to extract features from
RF devices. A dataset comprising nine RF devices is utilized, with each sample
containing 900 attributes and a total of 1080 equally distributed samples
across the devices. These features are then used in a classification modeling
framework. To overcome the limitations of conventional machine learning
methods, we introduce a hybrid deep learning model called the CNN-Bi-GRU for
learning the identification of RF devices based on their transient
characteristics. The proposed approach provided a 10-fold cross-validation
performance with a precision of 99.33%, recall of 99.53%, F1-score of 99.43%,
and classification accuracy of 99.17%. The results demonstrate the promising
classification performance of the CNN-Bi-GRU approach, indicating its
suitability for accurately identifying RF devices based on their transient
characteristics and its potential for enhancing device identification and
classification in complex wireless environments.

</details>


### [98] [AQUA20: A Benchmark Dataset for Underwater Species Classification under Challenging Conditions](https://arxiv.org/abs/2506.17455)
*Taufikur Rahman Fuad,Sabbir Ahmed,Shahriar Ivan*

Main category: cs.CV

TL;DR: AQUA20 is a benchmark dataset for underwater species recognition, evaluating 13 deep learning models, with ConvNeXt performing best. The dataset addresses challenges like turbidity and occlusion.


<details>
  <summary>Details</summary>
Motivation: Underwater visual recognition is hindered by distortions like turbidity and low illumination, necessitating robust datasets and models.

Method: Evaluated 13 deep learning models (CNNs and transformers) on AQUA20 dataset for marine species classification under challenging conditions.

Result: ConvNeXt achieved top performance (Top-3: 98.82%, Top-1: 90.69%, F1-score: 88.92%). Other models showed complexity-performance trade-offs.

Conclusion: AQUA20 is a valuable resource for future research, with room for improvement in underwater species recognition.

Abstract: Robust visual recognition in underwater environments remains a significant
challenge due to complex distortions such as turbidity, low illumination, and
occlusion, which severely degrade the performance of standard vision systems.
This paper introduces AQUA20, a comprehensive benchmark dataset comprising
8,171 underwater images across 20 marine species reflecting real-world
environmental challenges such as illumination, turbidity, occlusions, etc.,
providing a valuable resource for underwater visual understanding. Thirteen
state-of-the-art deep learning models, including lightweight CNNs (SqueezeNet,
MobileNetV2) and transformer-based architectures (ViT, ConvNeXt), were
evaluated to benchmark their performance in classifying marine species under
challenging conditions. Our experimental results show ConvNeXt achieving the
best performance, with a Top-3 accuracy of 98.82% and a Top-1 accuracy of
90.69%, as well as the highest overall F1-score of 88.92% with moderately large
parameter size. The results obtained from our other benchmark models also
demonstrate trade-offs between complexity and performance. We also provide an
extensive explainability analysis using GRAD-CAM and LIME for interpreting the
strengths and pitfalls of the models. Our results reveal substantial room for
improvement in underwater species recognition and demonstrate the value of
AQUA20 as a foundation for future research in this domain. The dataset is
publicly available at: https://huggingface.co/datasets/taufiktrf/AQUA20.

</details>


### [99] [When Every Millisecond Counts: Real-Time Anomaly Detection via the Multimodal Asynchronous Hybrid Network](https://arxiv.org/abs/2506.17457)
*Dong Xiao,Guangyao Chen,Peixi Peng,Yangru Huang,Yifan Zhao,Yongxing Dai,Yonghong Tian*

Main category: cs.CV

TL;DR: Proposes a real-time anomaly detection method for autonomous driving using a multimodal hybrid network combining event and RGB cameras, achieving high accuracy and millisecond-level response.


<details>
  <summary>Details</summary>
Motivation: Current anomaly detection methods prioritize accuracy but neglect response time, which is critical for autonomous driving safety.

Method: A multimodal asynchronous hybrid network integrates event camera data (via an asynchronous Graph Neural Network) and RGB camera data (via CNN) to capture temporal and spatial features.

Result: Outperforms existing methods in accuracy and response time, achieving millisecond-level real-time performance.

Conclusion: The proposed method effectively addresses the need for fast and accurate anomaly detection in autonomous driving.

Abstract: Anomaly detection is essential for the safety and reliability of autonomous
driving systems. Current methods often focus on detection accuracy but neglect
response time, which is critical in time-sensitive driving scenarios. In this
paper, we introduce real-time anomaly detection for autonomous driving,
prioritizing both minimal response time and high accuracy. We propose a novel
multimodal asynchronous hybrid network that combines event streams from event
cameras with image data from RGB cameras. Our network utilizes the high
temporal resolution of event cameras through an asynchronous Graph Neural
Network and integrates it with spatial features extracted by a CNN from RGB
images. This combination effectively captures both the temporal dynamics and
spatial details of the driving environment, enabling swift and precise anomaly
detection. Extensive experiments on benchmark datasets show that our approach
outperforms existing methods in both accuracy and response time, achieving
millisecond-level real-time performance.

</details>


### [100] [Photogranulometry -- Dataset of soil images with corresponding particle size distributions](https://arxiv.org/abs/2506.17469)
*Thomas Plante St-Cyr,Franois Duhaime,Jean-Sbastien Dub,Simon Grenier*

Main category: cs.CV

TL;DR: The paper introduces a high-resolution image dataset of soil samples to train CNNs for optical grain size analysis, reducing downtime and costs of traditional PSD methods.


<details>
  <summary>Details</summary>
Motivation: Traditional PSD analyses are costly and time-consuming; optical grain size analysis integrated into workflows could mitigate these issues.

Method: A standardized setup with 45 MP resolution images of 321 soil samples in moist/dry states, using a custom test bench and coning/quartering for larger samples.

Result: A dataset of 12,714 images for training CNNs in geotechnical applications.

Conclusion: The dataset provides a foundation for advancing optical grain size analysis in geotechnical workflows.

Abstract: Traditional particle size distribution (PSD) analyses create significant
downtime and are expensive in labor and maintenance. These drawbacks could be
alleviated using optical grain size analysis integrated into routine
geotechnical laboratory workflow. This paper presents a high-resolution dataset
of 12,714 images of 321 different soil samples collected in the Montreal,
Quebec region, alongside their PSD analysis. It is designed to provide a robust
starting point for training convolutional neural networks (CNN) in geotechnical
applications. Soil samples were photographed in a standardized top-view
position with a resolution of 45 MP and a minimum scale of 39.4 micrometers per
pixel, both in their moist and dry states. A custom test bench employing 13x9
inch white aluminum trays, on which the samples are spread in a thin layer, was
used. For samples exceeding a size limit, a coning and quartering method was
employed for mass reduction.

</details>


### [101] [Few-Shot, Now for Real: Medical VLMs Adaptation without Balanced Sets or Validation](https://arxiv.org/abs/2506.17500)
*Julio Silva-Rodrguez,Fereshteh Shakeri,Houda Bahig,Jose Dolz,Ismail Ben Ayed*

Main category: cs.CV

TL;DR: The paper critiques assumptions in vision-language models (VLMs) for medical image analysis, proposing a realistic, imbalanced, validation-free adaptation setting and a training-free linear probe for robust performance.


<details>
  <summary>Details</summary>
Motivation: Current VLMs make unrealistic assumptions about balanced data and validation sets, which don't align with real-world medical scenarios.

Method: Introduces a realistic adaptation setting and a training-free linear probe blending visual and textual supervision.

Result: Benchmarks show current methods underperform in realistic conditions; the proposed solver offers robust adaptation.

Conclusion: The work highlights limitations of existing VLMs and provides a practical, efficient baseline for challenging medical scenarios.

Abstract: Vision-language models (VLMs) are gaining attention in medical image
analysis. These are pre-trained on large, heterogeneous data sources, yielding
rich and transferable representations. Notably, the combination of
modality-specialized VLMs with few-shot adaptation has provided fruitful
results, enabling the efficient deployment of high-performing solutions.
However, previous works on this topic make strong assumptions about the
distribution of adaptation data, which are unrealistic in the medical domain.
First, prior art assumes access to a balanced support set, a condition that
breaks the natural imbalance in disease prevalence found in real-world
scenarios. Second, these works typically assume the presence of an additional
validation set to fix critical hyper-parameters, which is highly
data-inefficient. This work challenges these favorable deployment scenarios and
introduces a realistic, imbalanced, validation-free adaptation setting. Our
extensive benchmark across various modalities and downstream tasks demonstrates
that current methods systematically compromise their performance when operating
under realistic conditions, occasionally even performing worse than zero-shot
inference. Also, we introduce a training-free linear probe that adaptively
blends visual and textual supervision. Detailed studies demonstrate that the
proposed solver is a strong, efficient baseline, enabling robust adaptation in
challenging scenarios.

</details>


### [102] [Trustworthy Few-Shot Transfer of Medical VLMs through Split Conformal Prediction](https://arxiv.org/abs/2506.17503)
*Julio Silva-Rodrguez,Ismail Ben Ayed,Jose Dolz*

Main category: cs.CV

TL;DR: The paper proposes a novel pipeline, SCA-T, to improve the reliability of medical vision-language models (VLMs) in conformal prediction by addressing the limitations of split conformal prediction (SCP) through unsupervised transductive adaptation.


<details>
  <summary>Details</summary>
Motivation: The reliability of medical VLMs in data-efficient image classification remains underexplored, especially in conformal prediction scenarios where pre-training and adaptation can disrupt exchangeability assumptions.

Method: The authors introduce transductive split conformal adaptation (SCA-T), which performs unsupervised transductive adaptation on both calibration and test data to maintain exchangeability while improving efficiency and coverage.

Result: Experiments show SCA-T consistently outperforms SCP in efficiency and conditional coverage across various medical image modalities and transfer tasks, while preserving empirical guarantees.

Conclusion: SCA-T effectively addresses the limitations of SCP in conformal prediction for medical VLMs, offering a reliable solution for transfer learning in conformal scenarios.

Abstract: Medical vision-language models (VLMs) have demonstrated unprecedented
transfer capabilities and are being increasingly adopted for data-efficient
image classification. Despite its growing popularity, its reliability aspect
remains largely unexplored. This work explores the split conformal prediction
(SCP) framework to provide trustworthiness guarantees when transferring such
models based on a small labeled calibration set. Despite its potential, the
generalist nature of the VLMs' pre-training could negatively affect the
properties of the predicted conformal sets for specific tasks. While common
practice in transfer learning for discriminative purposes involves an
adaptation stage, we observe that deploying such a solution for conformal
purposes is suboptimal since adapting the model using the available calibration
data breaks the rigid exchangeability assumptions for test data in SCP. To
address this issue, we propose transductive split conformal adaptation (SCA-T),
a novel pipeline for transfer learning on conformal scenarios, which performs
an unsupervised transductive adaptation jointly on calibration and test data.
We present comprehensive experiments utilizing medical VLMs across various
image modalities, transfer tasks, and non-conformity scores. Our framework
offers consistent gains in efficiency and conditional coverage compared to SCP,
maintaining the same empirical guarantees.

</details>


### [103] [Learning golf swing signatures from a single wrist-worn inertial sensor](https://arxiv.org/abs/2506.17505)
*Jessy Lauer*

Main category: cs.CV

TL;DR: A data-driven framework for personalized golf swing analysis using a wrist-worn sensor, reconstructing full-body kinematics, and detecting technical flaws with neural networks.


<details>
  <summary>Details</summary>
Motivation: Address limitations in golf swing analysis, such as isolated metrics and lack of professional athlete data, by providing holistic, interpretable movement representations.

Method: Use a large dataset of professional swings, reconstruct 3D kinematics, generate synthetic inertial data, and train neural networks to infer motion and segment swing phases.

Result: Accurate full-body kinematics estimation, detection of technical flaws, and prediction of player identity, club type, sex, and age. Longitudinal tracking showed measurable technical progress.

Conclusion: The system bridges lab and field biomechanics, offering scalable motion analysis for research, coaching, and injury prevention, while challenging common assumptions about golf swings.

Abstract: Despite its importance for performance and injury prevention, golf swing
analysis is limited by isolated metrics, underrepresentation of professional
athletes, and a lack of rich, interpretable movement representations. We
address these gaps with a holistic, data-driven framework for personalized golf
swing analysis from a single wrist-worn sensor. We build a large dataset of
professional swings from publicly available videos, reconstruct full-body 3D
kinematics using biologically accurate human mesh recovery, and generate
synthetic inertial data to train neural networks that infer motion and segment
swing phases from wrist-based input. We learn a compositional, discrete
vocabulary of motion primitives that facilitates the detection and
visualization of technical flaws, and is expressive enough to predict player
identity, club type, sex, and age. Our system accurately estimates full-body
kinematics and swing events from wrist data, delivering lab-grade motion
analysis on-course and supporting early detection of anomalous movement
patterns. Explainability methods reveal subtle, individualized movement
signatures, reinforcing the view that variability is a hallmark of skilled
performance. Longitudinal tracking demonstrates practical value: as one
player's handicap improved from 50 to 2.2 over 1.5 years, our system captured
measurable technical progress and provided targeted, actionable feedback. Our
findings challenge common assumptions, such as swing consistency across clubs
and the existence of a single "ideal" swing, and uncover latent biomarkers
shaped by both intrinsic traits and task-specific constraints. This work
bridges lab and field-based biomechanics, offering scalable, accessible,
high-fidelity motion analysis for research, coaching, and injury prevention,
while opening new directions in movement-based phenotyping, personalized
equipment design, and motor skill development.

</details>


### [104] [Scene-R1: Video-Grounded Large Language Models for 3D Scene Reasoning without 3D Annotations](https://arxiv.org/abs/2506.17545)
*Zhihao Yuan,Shuyi Jiang,Chun-Mei Feng,Yaolun Zhang,Shuguang Cui,Zhen Li,Na Zhao*

Main category: cs.CV

TL;DR: Scene-R1 is a video-grounded framework for 3D scene understanding without 3D instance supervision, using reinforcement learning and a two-stage grounding pipeline.


<details>
  <summary>Details</summary>
Motivation: Existing 3D-aware LLMs lack transparency and rely on pre-trained 3D detectors. Scene-R1 aims to provide a more interpretable and annotation-efficient solution.

Method: Uses a two-stage grounding pipeline: temporal grounding to select relevant video snippets, and image grounding to predict 2D bounding boxes. Tracks objects with SAM2 for pixel-accurate masks and projects them into 3D.

Result: Surpasses open-vocabulary baselines on multiple datasets, offering transparent, step-by-step rationales without dense 3D labels.

Conclusion: Reinforcement-learning-based reasoning with RGB-D video provides a practical, efficient route to trustworthy 3D scene understanding.

Abstract: Currently, utilizing large language models to understand the 3D world is
becoming popular. Yet existing 3D-aware LLMs act as black boxes: they output
bounding boxes or textual answers without revealing how those decisions are
made, and they still rely on pre-trained 3D detectors to supply object
proposals. We introduce Scene-R1, a video-grounded framework that learns to
reason about 3D scenes without any point-wise 3D instance supervision by
pairing reinforcement-learning-driven reasoning with a two-stage grounding
pipeline. In the temporal grounding stage, we explicitly reason about the video
and select the video snippets most relevant to an open-ended query. In the
subsequent image grounding stage, we analyze the image and predict the 2D
bounding box. After that, we track the object using SAM2 to produce
pixel-accurate masks in RGB frames, and project them back into 3D, thereby
eliminating the need for 3D detector-based proposals while capturing fine
geometry and material cues. Scene-R1 can also adapt to the 3D visual question
answering task to answer free-form questions directly from video. Our training
pipeline only needs task-level 2D boxes or textual labels without dense 3D
point-wise labels. Scene-R1 surpasses existing open-vocabulary baselines on
multiple datasets, while delivering transparent, step-by-step rationales. These
results show that reinforcement-learning-based reasoning combined with RGB-D
video alone offers a practical, annotation-efficient route to trustworthy 3D
scene understanding.

</details>


### [105] [SynDaCaTE: A Synthetic Dataset For Evaluating Part-Whole Hierarchical Inference](https://arxiv.org/abs/2506.17558)
*Jake Levi,Mark van der Wilk*

Main category: cs.CV

TL;DR: The paper introduces SynDaCaTE, a synthetic dataset for evaluating capsule networks, and highlights the effectiveness of permutation-equivariant self-attention for parts-to-wholes inference.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of evaluating whether capsule networks truly learn part-whole hierarchies, as claimed, due to reliance on supervised tasks like object classification.

Method: The authors create SynDaCaTE, a synthetic dataset, and use it to test existing capsule models and explore permutation-equivariant self-attention for parts-to-wholes inference.

Result: SynDaCaTE identifies a bottleneck in a prominent capsule model and shows that permutation-equivariant self-attention is effective for parts-to-wholes inference.

Conclusion: The findings motivate future research into designing better inductive biases for computer vision, leveraging insights from SynDaCaTE.

Abstract: Learning to infer object representations, and in particular part-whole
hierarchies, has been the focus of extensive research in computer vision, in
pursuit of improving data efficiency, systematic generalisation, and
robustness. Models which are \emph{designed} to infer part-whole hierarchies,
often referred to as capsule networks, are typically trained end-to-end on
supervised tasks such as object classification, in which case it is difficult
to evaluate whether such a model \emph{actually} learns to infer part-whole
hierarchies, as claimed. To address this difficulty, we present a SYNthetic
DAtaset for CApsule Testing and Evaluation, abbreviated as SynDaCaTE, and
establish its utility by (1) demonstrating the precise bottleneck in a
prominent existing capsule model, and (2) demonstrating that
permutation-equivariant self-attention is highly effective for parts-to-wholes
inference, which motivates future directions for designing effective inductive
biases for computer vision.

</details>


### [106] [VLA-OS: Structuring and Dissecting Planning Representations and Paradigms in Vision-Language-Action Models](https://arxiv.org/abs/2506.17561)
*Chongkai Gao,Zixuan Liu,Zhenghao Chi,Junshan Huang,Xin Fei,Yiwen Hou,Yuxuan Zhang,Yudi Lin,Zhirui Fang,Zeyu Jiang,Lin Shao*

Main category: cs.CV

TL;DR: The paper introduces VLA-OS, a unified Vision-Language-Action (VLA) architecture, to systematically study the impact of planning paradigms and representations, finding visually grounded planning and Hierarchical-VLA superior.


<details>
  <summary>Details</summary>
Motivation: Existing VLA models vary widely in architectures and training, making it hard to pinpoint performance gains. The study aims to isolate and evaluate planning paradigms and representations.

Method: VLA-OS, a unified VLA architecture, is tested across diverse tasks, object categories, visual modalities, environments, and end-effectors in controlled experiments.

Result: Visually grounded planning outperforms language-based planning. Hierarchical-VLA excels in performance, generalization, scalability, and continual learning, despite slower speeds.

Conclusion: The study highlights the advantages of visually grounded planning and Hierarchical-VLA, providing insights for future VLA model improvements.

Abstract: Recent studies on Vision-Language-Action (VLA) models have shifted from the
end-to-end action-generation paradigm toward a pipeline involving task planning
followed by action generation, demonstrating improved performance on various
complex, long-horizon manipulation tasks. However, existing approaches vary
significantly in terms of network architectures, planning paradigms,
representations, and training data sources, making it challenging for
researchers to identify the precise sources of performance gains and components
to be further improved. To systematically investigate the impacts of different
planning paradigms and representations isolating from network architectures and
training data, in this paper, we introduce VLA-OS, a unified VLA architecture
series capable of various task planning paradigms, and design a comprehensive
suite of controlled experiments across diverse object categories (rigid and
deformable), visual modalities (2D and 3D), environments (simulation and
real-world), and end-effectors (grippers and dexterous hands). Our results
demonstrate that: 1) visually grounded planning representations are generally
better than language planning representations; 2) the Hierarchical-VLA paradigm
generally achieves superior or comparable performance than other paradigms on
task performance, pretraining, generalization ability, scalability, and
continual learning ability, albeit at the cost of slower training and inference
speeds.

</details>


### [107] [LLM-driven Medical Report Generation via Communication-efficient Heterogeneous Federated Learning](https://arxiv.org/abs/2506.17562)
*Haoxuan Che,Haibo Jin,Zhengrui Guo,Yi Lin,Cheng Jin,Hao Chen*

Main category: cs.CV

TL;DR: FedMRG is a federated learning framework for privacy-preserving, multi-center development of LLM-driven Medical Report Generation (MRG) models, addressing communication efficiency and data heterogeneity.


<details>
  <summary>Details</summary>
Motivation: Centralizing medical image-report pairs is challenging due to privacy regulations, hindering LLM-driven MRG development. FedMRG aims to enable collaborative model training without data sharing.

Method: FedMRG employs low-rank factorization for efficient parameter updates, client-aware contrastive learning for feature capture, and a dual-adapter mechanism to handle reporting style variations.

Result: FedMRG demonstrates generalizability and adaptability in generating clinically accurate reports while maintaining communication efficiency in federated settings.

Conclusion: FedMRG successfully addresses privacy and heterogeneity challenges in multi-center MRG, enabling efficient and accurate LLM-driven report generation.

Abstract: LLMs have demonstrated significant potential in Medical Report Generation
(MRG), yet their development requires large amounts of medical image-report
pairs, which are commonly scattered across multiple centers. Centralizing these
data is exceptionally challenging due to privacy regulations, thereby impeding
model development and broader adoption of LLM-driven MRG models. To address
this challenge, we present FedMRG, the first framework that leverages Federated
Learning (FL) to enable privacy-preserving, multi-center development of
LLM-driven MRG models, specifically designed to overcome the critical challenge
of communication-efficient LLM training under multi-modal data heterogeneity.
To start with, our framework tackles the fundamental challenge of communication
overhead in FL-LLM tuning by employing low-rank factorization to efficiently
decompose parameter updates, significantly reducing gradient transmission costs
and making LLM-driven MRG feasible in bandwidth-constrained FL settings.
Furthermore, we observed the dual heterogeneity in MRG under the FL scenario:
varying image characteristics across medical centers, as well as diverse
reporting styles and terminology preferences. To address this, we further
enhance FedMRG with (1) client-aware contrastive learning in the MRG encoder,
coupled with diagnosis-driven prompts, which capture both globally
generalizable and locally distinctive features while maintaining diagnostic
accuracy; and (2) a dual-adapter mutual boosting mechanism in the MRG decoder
that harmonizes generic and specialized adapters to address variations in
reporting styles and terminology. Through extensive evaluation of our
established FL-MRG benchmark, we demonstrate the generalizability and
adaptability of FedMRG, underscoring its potential in harnessing multi-center
data and generating clinically accurate reports while maintaining communication
efficiency.

</details>


### [108] [HalluRNN: Mitigating Hallucinations via Recurrent Cross-Layer Reasoning in Large Vision-Language Models](https://arxiv.org/abs/2506.17587)
*Le Yu,Kaishen Wang,Jianlong Xiong,Yue Cao,Tao He*

Main category: cs.CV

TL;DR: HalluRNN introduces an architecture-level solution to mitigate hallucinations in LVLMs using a Dual-Gated Depth Propagation Unit (DG-DPU) for recurrent cross-layer reasoning, achieving robust performance with minimal fine-tuning.


<details>
  <summary>Details</summary>
Motivation: LVLMs often generate visually ungrounded outputs (hallucinations). Existing solutions are resource-intensive or task-specific, prompting the need for a more efficient, architecture-level approach.

Method: Proposes HalluRNN with a DG-DPU module for recurrent refinement of hidden states, enabling adaptive information propagation and layer consistency. Only the DG-DPU is fine-tuned.

Result: HalluRNN achieves strong, robust performance across multiple benchmarks by mitigating representational drift and hallucinations.

Conclusion: HalluRNN offers an efficient, architecture-level solution to reduce hallucinations in LVLMs, requiring minimal fine-tuning while maintaining performance.

Abstract: Though Large Vision-Language Models (LVLMs) have achieved remarkable
performance across various tasks, they are still prone to
hallucinations-generating outputs that are textually plausible but visually
ungrounded. While prior approaches generally address this issue through
data-centric fine-tuning or innovative decoding strategies, these methods often
require substantial resources or task-specific configurations. In this work, we
introduce an architecture-level solution, HalluRNN, which enhances model
stability through recurrent cross-layer reasoning. Specifically, we propose a
novel Dual-Gated Depth Propagation Unit (DG-DPU) module, which is shared across
layers and recurrently refines hidden states. This allows for the adaptive
propagation of information throughout the model, enforces consistency across
layers, and mitigates hallucinations caused by representational drift. By
fine-tuning only the DG-DPU module, HalluRNN achieves strong and robust
performance across multiple benchmarks.

</details>


### [109] [DRAMA-X: A Fine-grained Intent Prediction and Risk Reasoning Benchmark For Driving](https://arxiv.org/abs/2506.17590)
*Mihir Godbole,Xiangbo Gao,Zhengzhong Tu*

Main category: cs.CV

TL;DR: DRAMA-X is a benchmark for evaluating multi-class intent prediction in safety-critical scenarios, featuring annotations for object detection, intent prediction, risk assessment, and action suggestion. SGG-Intent, a lightweight framework, is proposed as a baseline, showing improved performance with scene-graph-based reasoning.


<details>
  <summary>Details</summary>
Motivation: The need for fine-grained intent reasoning in autonomous driving, especially for VRUs, is unmet by current benchmarks and methods.

Method: DRAMA-X is created via automated annotation, and SGG-Intent is introduced as a training-free framework using VLMs and LLMs for sequential reasoning.

Result: Scene-graph-based reasoning improves intent prediction and risk assessment, particularly with contextual modeling.

Conclusion: DRAMA-X fills a critical gap in evaluating intent reasoning, and SGG-Intent demonstrates the value of structured reasoning for autonomous decision-making.

Abstract: Understanding the short-term motion of vulnerable road users (VRUs) like
pedestrians and cyclists is critical for safe autonomous driving, especially in
urban scenarios with ambiguous or high-risk behaviors. While vision-language
models (VLMs) have enabled open-vocabulary perception, their utility for
fine-grained intent reasoning remains underexplored. Notably, no existing
benchmark evaluates multi-class intent prediction in safety-critical
situations, To address this gap, we introduce DRAMA-X, a fine-grained benchmark
constructed from the DRAMA dataset via an automated annotation pipeline.
DRAMA-X contains 5,686 accident-prone frames labeled with object bounding
boxes, a nine-class directional intent taxonomy, binary risk scores,
expert-generated action suggestions for the ego vehicle, and descriptive motion
summaries. These annotations enable a structured evaluation of four
interrelated tasks central to autonomous decision-making: object detection,
intent prediction, risk assessment, and action suggestion. As a reference
baseline, we propose SGG-Intent, a lightweight, training-free framework that
mirrors the ego vehicle's reasoning pipeline. It sequentially generates a scene
graph from visual input using VLM-backed detectors, infers intent, assesses
risk, and recommends an action using a compositional reasoning stage powered by
a large language model. We evaluate a range of recent VLMs, comparing
performance across all four DRAMA-X tasks. Our experiments demonstrate that
scene-graph-based reasoning enhances intent prediction and risk assessment,
especially when contextual cues are explicitly modeled.

</details>


### [110] [SELFI: Selective Fusion of Identity for Generalizable Deepfake Detection](https://arxiv.org/abs/2506.17592)
*Younghun Kim,Minsuk Jang,Myung-Joon Kwon,Wonjun Lee,Changick Kim*

Main category: cs.CV

TL;DR: The paper explores the role of face identity in deepfake detection, proposing SELFI, a framework that dynamically modulates identity usage for better generalization.


<details>
  <summary>Details</summary>
Motivation: Conflicting views exist on whether to suppress or rely on identity features for deepfake detection. The study aims to reconcile these by testing hypotheses about identity's discriminative power and generalization across manipulation methods.

Method: The authors propose SELFI, which includes a Forgery-Aware Identity Adapter (FAIA) to extract and project identity embeddings, and an Identity-Aware Fusion Module (IAFM) for selective feature integration.

Result: Experiments show SELFI improves cross-manipulation generalization, outperforming prior methods by 3.1% AUC on average and 6% on the DFDC dataset.

Conclusion: Identity features should be explicitly modeled and adaptively controlled, not blindly suppressed or relied upon. SELFI demonstrates this approach's effectiveness.

Abstract: Face identity provides a powerful signal for deepfake detection. Prior
studies show that even when not explicitly modeled, classifiers often learn
identity features implicitly. This has led to conflicting views: some suppress
identity cues to reduce bias, while others rely on them as forensic evidence.
To reconcile these views, we analyze two hypotheses: (1) whether face identity
alone is discriminative for detecting deepfakes, and (2) whether such identity
features generalize poorly across manipulation methods. Our experiments confirm
that identity is informative but context-dependent. While some manipulations
preserve identity-consistent artifacts, others distort identity cues and harm
generalization. We argue that identity features should neither be blindly
suppressed nor relied upon, but instead be explicitly modeled and adaptively
controlled based on per-sample relevance. We propose \textbf{SELFI}
(\textbf{SEL}ective \textbf{F}usion of \textbf{I}dentity), a generalizable
detection framework that dynamically modulates identity usage. SELFI consists
of: (1) a Forgery-Aware Identity Adapter (FAIA) that extracts identity
embeddings from a frozen face recognition model and projects them into a
forgery-relevant space via auxiliary supervision; and (2) an Identity-Aware
Fusion Module (IAFM) that selectively integrates identity and visual features
using a relevance-guided fusion mechanism. Experiments on four benchmarks show
that SELFI improves cross-manipulation generalization, outperforming prior
methods by an average of 3.1\% AUC. On the challenging DFDC dataset, SELFI
exceeds the previous best by 6\%. Code will be released upon paper acceptance.

</details>


### [111] [A Multimodal In Vitro Diagnostic Method for Parkinson's Disease Combining Facial Expressions and Behavioral Gait Data](https://arxiv.org/abs/2506.17596)
*Wei Huang,Yinxuan Xu,Yintao Zhou,Zhengyu Li,Jing Huang,Meng Pang*

Main category: cs.CV

TL;DR: A multimodal in vitro diagnostic method for Parkinson's disease (PD) using facial expressions and gait, addressing data limitations and equipment constraints with a lightweight deep learning model.


<details>
  <summary>Details</summary>
Motivation: Early detection of PD is crucial due to its incurable nature and rapid progression. Existing methods face challenges like limited data, specialized equipment needs, and single-modality risks.

Method: Proposes a multimodal approach combining facial expressions and gait, using a lightweight deep learning model for feature extraction and fusion. Validated with the largest multimodal PD dataset.

Result: The method aims to improve diagnostic accuracy and is designed for deployment on mobile devices.

Conclusion: The proposed multimodal approach addresses key challenges in PD diagnosis, offering a practical and scalable solution.

Abstract: Parkinson's disease (PD), characterized by its incurable nature, rapid
progression, and severe disability, poses significant challenges to the lives
of patients and their families. Given the aging population, the need for early
detection of PD is increasing. In vitro diagnosis has garnered attention due to
its non-invasive nature and low cost. However, existing methods present several
challenges: 1) limited training data for facial expression diagnosis; 2)
specialized equipment and acquisition environments required for gait diagnosis,
resulting in poor generalizability; 3) the risk of misdiagnosis or missed
diagnosis when relying on a single modality. To address these issues, we
propose a novel multimodal in vitro diagnostic method for PD, leveraging facial
expressions and behavioral gait. Our method employs a lightweight deep learning
model for feature extraction and fusion, aimed at improving diagnostic accuracy
and facilitating deployment on mobile devices. Furthermore, we have established
the largest multimodal PD dataset in collaboration with a hospital and
conducted extensive experiments to validate the effectiveness of our proposed
method.

</details>


### [112] [OpenMAP-BrainAge: Generalizable and Interpretable Brain Age Predictor](https://arxiv.org/abs/2506.17597)
*Pengyu Kan,Craig Jones,Kenichi Oishi*

Main category: cs.CV

TL;DR: A transformer-based model for interpretable and robust brain age prediction from MRI scans, achieving high accuracy and generalizability.


<details>
  <summary>Details</summary>
Motivation: To address the need for an interpretable and robust age prediction model that handles demographic and technological variances in brain MRI scans.

Method: Proposed a transformer-based architecture with self-supervised pre-training, processing pseudo-3D MRI scans from three views and incorporating volumetric data. Reduced complexity to linear for scalability.

Result: Achieved MAE of 3.65 years on test sets and 3.54 years on external validation. Notable brain age gap differences across cognitive groups and significant correlations with cognitive scores.

Conclusion: The model successfully integrates multi-view and volumetric data for accurate, generalizable, and interpretable brain age prediction, linked to neurodegenerative disorders.

Abstract: Purpose: To develop an age prediction model which is interpretable and robust
to demographic and technological variances in brain MRI scans. Materials and
Methods: We propose a transformer-based architecture that leverages
self-supervised pre-training on large-scale datasets. Our model processes
pseudo-3D T1-weighted MRI scans from three anatomical views and incorporates
brain volumetric information. By introducing a stem architecture, we reduce the
conventional quadratic complexity of transformer models to linear complexity,
enabling scalability for high-dimensional MRI data. We trained our model on
ADNI2 $\&$ 3 (N=1348) and OASIS3 (N=716) datasets (age range: 42 - 95) from the
North America, with an 8:1:1 split for train, validation and test. Then, we
validated it on the AIBL dataset (N=768, age range: 60 - 92) from Australia.
Results: We achieved an MAE of 3.65 years on ADNI2 $\&$ 3 and OASIS3 test set
and a high generalizability of MAE of 3.54 years on AIBL. There was a notable
increase in brain age gap (BAG) across cognitive groups, with mean of 0.15
years (95% CI: [-0.22, 0.51]) in CN, 2.55 years ([2.40, 2.70]) in MCI, 6.12
years ([5.82, 6.43]) in AD. Additionally, significant negative correlation
between BAG and cognitive scores was observed, with correlation coefficient of
-0.185 (p < 0.001) for MoCA and -0.231 (p < 0.001) for MMSE. Gradient-based
feature attribution highlighted ventricles and white matter structures as key
regions influenced by brain aging. Conclusion: Our model effectively fused
information from different views and volumetric information to achieve
state-of-the-art brain age prediction accuracy, improved generalizability and
interpretability with association to neurodegenerative disorders.

</details>


### [113] [HIRE: Lightweight High-Resolution Image Feature Enrichment for Multimodal LLMs](https://arxiv.org/abs/2506.17608)
*Nikitha SR,Aradhya Neeraj Mathur,Tarun Ram Menta,Rishabh Jain,Mausoom Sarkar*

Main category: cs.CV

TL;DR: A shallow feature enricher is proposed to reduce computational costs while maintaining competitive performance in high-resolution multimodal tasks.


<details>
  <summary>Details</summary>
Motivation: High-resolution image features improve visual understanding but increase computational costs due to large encoders like ViT.

Method: Feature upsampling is explored, and a shallow feature enricher is developed to reduce costs.

Result: The method achieves competitive results with up to 1.5x FLOPs savings.

Conclusion: The shallow feature enricher offers a cost-effective alternative to high-resolution feature generation.

Abstract: The integration of high-resolution image features in modern multimodal large
language models has demonstrated significant improvements in fine-grained
visual understanding tasks, achieving high performance across multiple
benchmarks. Since these features are obtained from large image encoders like
ViT, they come with a significant increase in computational costs due to
multiple calls to these encoders. In this work, we first develop an intuition
for feature upsampling as a natural extension of high-resolution feature
generation. Through extensive experiments and ablations, we demonstrate how a
shallow feature enricher can achieve competitive results with tremendous
reductions in training and inference time as well as computational cost, with
upto 1.5x saving in FLOPs.

</details>


### [114] [JarvisArt: Liberating Human Artistic Creativity via an Intelligent Photo Retouching Agent](https://arxiv.org/abs/2506.17612)
*Yunlong Lin,Zixu Lin,Kunjie Lin,Jinbin Bai,Panwang Pan,Chenxin Li,Haoyu Chen,Zhongdao Wang,Xinghao Ding,Wenbo Li,Shuicheng Yan*

Main category: cs.CV

TL;DR: JarvisArt is an AI-driven agent for photo retouching, combining user intent understanding with professional artist reasoning to coordinate Lightroom tools, outperforming GPT-4o.


<details>
  <summary>Details</summary>
Motivation: Existing AI retouching tools lack adjustability and generalization, while professional tools like Lightroom require expertise. JarvisArt bridges this gap.

Method: Uses a multi-modal large language model (MLLM) with two-stage training: Chain-of-Thought fine-tuning and GRPO-R for decision-making. Integrates via Agent-to-Lightroom Protocol.

Result: Outperforms GPT-4o by 60% in pixel-level metrics on MMArt-Bench, offering fine-grained control and superior generalization.

Conclusion: JarvisArt advances intelligent photo retouching with user-friendly interaction and professional-level results.

Abstract: Photo retouching has become integral to contemporary visual storytelling,
enabling users to capture aesthetics and express creativity. While professional
tools such as Adobe Lightroom offer powerful capabilities, they demand
substantial expertise and manual effort. In contrast, existing AI-based
solutions provide automation but often suffer from limited adjustability and
poor generalization, failing to meet diverse and personalized editing needs. To
bridge this gap, we introduce JarvisArt, a multi-modal large language model
(MLLM)-driven agent that understands user intent, mimics the reasoning process
of professional artists, and intelligently coordinates over 200 retouching
tools within Lightroom. JarvisArt undergoes a two-stage training process: an
initial Chain-of-Thought supervised fine-tuning to establish basic reasoning
and tool-use skills, followed by Group Relative Policy Optimization for
Retouching (GRPO-R) to further enhance its decision-making and tool
proficiency. We also propose the Agent-to-Lightroom Protocol to facilitate
seamless integration with Lightroom. To evaluate performance, we develop
MMArt-Bench, a novel benchmark constructed from real-world user edits.
JarvisArt demonstrates user-friendly interaction, superior generalization, and
fine-grained control over both global and local adjustments, paving a new
avenue for intelligent photo retouching. Notably, it outperforms GPT-4o with a
60% improvement in average pixel-level metrics on MMArt-Bench for content
fidelity, while maintaining comparable instruction-following capabilities.
Project Page: https://jarvisart.vercel.app/.

</details>


### [115] [CLiViS: Unleashing Cognitive Map through Linguistic-Visual Synergy for Embodied Visual Reasoning](https://arxiv.org/abs/2506.17629)
*Kailing Li,Qi'ao Xu,Tianwen Qian,Yuqian Fu,Yang Jiao,Xiaoling Wang*

Main category: cs.CV

TL;DR: CLiViS is a training-free framework combining LLMs for reasoning and VLMs for perception to address challenges in Embodied Visual Reasoning (EVR).


<details>
  <summary>Details</summary>
Motivation: EVR faces challenges due to complex instructions and spatiotemporal dynamics in egocentric videos, with existing solutions lacking in detail or compositional reasoning.

Method: CLiViS leverages LLMs for task planning and VLMs for visual perception, using a dynamic Cognitive Map to bridge perception and reasoning.

Result: CLiViS shows effectiveness in handling long-term visual dependencies across benchmarks.

Conclusion: The framework demonstrates the synergy of LLMs and VLMs, offering a robust solution for EVR.

Abstract: Embodied Visual Reasoning (EVR) seeks to follow complex, free-form
instructions based on egocentric video, enabling semantic understanding and
spatiotemporal reasoning in dynamic environments. Despite its promising
potential, EVR encounters significant challenges stemming from the diversity of
complex instructions and the intricate spatiotemporal dynamics in long-term
egocentric videos. Prior solutions either employ Large Language Models (LLMs)
over static video captions, which often omit critical visual details, or rely
on end-to-end Vision-Language Models (VLMs) that struggle with stepwise
compositional reasoning. Consider the complementary strengths of LLMs in
reasoning and VLMs in perception, we propose CLiViS. It is a novel
training-free framework that leverages LLMs for high-level task planning and
orchestrates VLM-driven open-world visual perception to iteratively update the
scene context. Building on this synergy, the core of CLiViS is a dynamic
Cognitive Map that evolves throughout the reasoning process. This map
constructs a structured representation of the embodied scene, bridging
low-level perception and high-level reasoning. Extensive experiments across
multiple benchmarks demonstrate the effectiveness and generality of CLiViS,
especially in handling long-term visual dependencies. Code is available at
https://github.com/Teacher-Tom/CLiViS.

</details>


### [116] [Optimization-Free Patch Attack on Stereo Depth Estimation](https://arxiv.org/abs/2506.17632)
*Hangcheng Liu,Xu Kuang,Xingshuo Han,Xingwan Wu,Haoran Ou,Shangwei Guo,Xingyi Huang,Tao Xiang,Tianwei Zhang*

Main category: cs.CV

TL;DR: The paper introduces PatchHunter, an optimization-free adversarial patch attack for Stereo Depth Estimation (SDE), addressing limitations of prior methods by focusing on visual patterns and achieving superior transferability and real-world applicability.


<details>
  <summary>Details</summary>
Motivation: Recent SDE models are vulnerable to adversarial attacks, but existing methods are unrealistic or limited. The paper aims to design physically realizable, scene-adaptive, and transferable attacks under realistic constraints.

Method: Proposes PatchHunter, a reinforcement learning-driven search over visual patterns to disrupt SDE assumptions, avoiding optimization-based techniques. Evaluated across KITTI, CARLA, and real-world deployments.

Result: PatchHunter outperforms optimization-based methods in effectiveness and transferability, maintaining high attack success even in challenging conditions like low light.

Conclusion: PatchHunter demonstrates the potential of pattern-based attacks for SDE, offering a practical and scalable solution for adversarial robustness research.

Abstract: Stereo Depth Estimation (SDE) is essential for scene understanding in
vision-based systems like autonomous driving. However, recent studies show that
SDE models are vulnerable to adversarial attacks, which are often limited to
unrealistic settings, e.g., digital perturbations on separate stereo views in
static scenes, restricting their real-world applicability. This raises a
critical question: how can we design physically realizable, scene-adaptive, and
transferable attacks against SDE under realistic constraints?
  To answer this, we make two key contributions. First, we propose a unified
attack framework that extends optimization-based techniques to four core stages
of stereo matching: feature extraction, cost-volume construction, cost
aggregation, and disparity regression. A comprehensive stage-wise evaluation
across 9 mainstream SDE models, under constraints like photometric consistency,
reveals that optimization-based patches suffer from poor transferability.
Interestingly, partially transferable patches suggest that patterns, rather
than pixel-level perturbations, may be key to generalizable attacks. Motivated
by this, we present PatchHunter, the first optimization-free adversarial patch
attack against SDE. PatchHunter formulates patch generation as a reinforcement
learning-driven search over a structured space of visual patterns crafted to
disrupt SDE assumptions.
  We validate PatchHunter across three levels: the KITTI dataset, the CARLA
simulator, and real-world vehicle deployment. PatchHunter not only surpasses
optimization-based methods in effectiveness but also achieves significantly
better black-box transferability. Even under challenging physical conditions
like low light, PatchHunter maintains high attack success (e.g., D1-all > 0.4),
whereas optimization-based methods fail.

</details>


### [117] [Adaptive Multi-prompt Contrastive Network for Few-shot Out-of-distribution Detection](https://arxiv.org/abs/2506.17633)
*Xiang Fang,Arvind Easwaran,Blaise Genest*

Main category: cs.CV

TL;DR: The paper introduces AMCN, a novel network for few-shot OOD detection, leveraging CLIP and adaptive prompts to address the scarcity of labeled ID samples and lack of OOD data.


<details>
  <summary>Details</summary>
Motivation: Traditional OOD detection methods require many IID samples, limiting real-world applications. Few-shot OOD detection is more challenging due to limited labeled ID samples and ignored class diversity.

Method: Proposes AMCN, which uses adaptive prompts (ID and OOD) and CLIP to connect text with images. It learns inter- and intra-class distributions and introduces a class-wise threshold for adaptive boundaries.

Result: AMCN outperforms state-of-the-art methods in few-shot OOD detection.

Conclusion: AMCN effectively addresses the challenges of few-shot OOD detection by leveraging adaptive prompts and CLIP, demonstrating superior performance.

Abstract: Out-of-distribution (OOD) detection attempts to distinguish outlier samples
to prevent models trained on the in-distribution (ID) dataset from producing
unavailable outputs. Most OOD detection methods require many IID samples for
training, which seriously limits their real-world applications. To this end, we
target a challenging setting: few-shot OOD detection, where {Only a few {\em
labeled ID} samples are available.} Therefore, few-shot OOD detection is much
more challenging than the traditional OOD detection setting. Previous few-shot
OOD detection works ignore the distinct diversity between different classes. In
this paper, we propose a novel network: Adaptive Multi-prompt Contrastive
Network (AMCN), which adapts the ID-OOD separation boundary by learning inter-
and intra-class distribution. To compensate for the absence of OOD and scarcity
of ID {\em image samples}, we leverage CLIP, connecting text with images,
engineering learnable ID and OOD {\em textual prompts}. Specifically, we first
generate adaptive prompts (learnable ID prompts, label-fixed OOD prompts and
label-adaptive OOD prompts). Then, we generate an adaptive class boundary for
each class by introducing a class-wise threshold. Finally, we propose a
prompt-guided ID-OOD separation module to control the margin between ID and OOD
prompts. Experimental results show that AMCN outperforms other state-of-the-art
works.

</details>


### [118] [Histopathology Image Report Generation by Vision Language Model with Multimodal In-Context Learning](https://arxiv.org/abs/2506.17645)
*Shih-Wen Liu,Hsuan-Yu Fan,Wei-Ta Chu,Fu-En Yang,Yu-Chiang Frank Wang*

Main category: cs.CV

TL;DR: PathGenIC, an in-context learning framework, automates medical report generation from histopathology images by integrating context from training data and multimodal learning, achieving top results on the HistGen benchmark.


<details>
  <summary>Details</summary>
Motivation: Automating medical report generation from histopathology images is challenging due to the need for effective visual representations and domain-specific knowledge.

Method: Proposes PathGenIC, which dynamically retrieves similar WSI-report pairs and uses adaptive feedback for contextual relevance and generation quality.

Result: Achieves state-of-the-art results on the HistGen benchmark, improving BLEU, METEOR, and ROUGE-L metrics, and shows robustness across report lengths and disease categories.

Conclusion: PathGenIC offers a solution for AI-driven histopathology reporting, setting a foundation for future multimodal clinical applications.

Abstract: Automating medical report generation from histopathology images is a critical
challenge requiring effective visual representations and domain-specific
knowledge. Inspired by the common practices of human experts, we propose an
in-context learning framework called PathGenIC that integrates context derived
from the training set with a multimodal in-context learning (ICL) mechanism.
Our method dynamically retrieves semantically similar whole slide image
(WSI)-report pairs and incorporates adaptive feedback to enhance contextual
relevance and generation quality. Evaluated on the HistGen benchmark, the
framework achieves state-of-the-art results, with significant improvements
across BLEU, METEOR, and ROUGE-L metrics, and demonstrates robustness across
diverse report lengths and disease categories. By maximizing training data
utility and bridging vision and language with ICL, our work offers a solution
for AI-driven histopathology reporting, setting a strong foundation for future
advancements in multimodal clinical applications.

</details>


### [119] [MDSAM:Memory-Driven Sparse Attention Matrix for LVLMs Hallucination Mitigation](https://arxiv.org/abs/2506.17664)
*Shuaiye Lu,Linjiang Zhou,Xiaochuan Shi*

Main category: cs.CV

TL;DR: MDSAM, a training-free method, reduces hallucinations in LVLMs by dynamically refining attention to image tokens, improving reliability in tasks like captioning and VQA.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in LVLMs arise from sensitivity to image tokens during decoding, necessitating a solution to refine attention and reduce errors.

Method: Proposes MDSAM, which memorizes and refines attention patterns dynamically during decoding, focusing on relevant image tokens.

Result: MDSAM consistently reduces hallucinations and improves reliability across benchmarks for image captioning and VQA.

Conclusion: MDSAM is adaptable, effective, and compatible with various LVLM architectures, mitigating hallucinations without extra training or tools.

Abstract: Hallucinations in large vision-language models (LVLMs) often stem from the
model's sensitivity to image tokens during decoding, as evidenced by attention
peaks observed when generating both real and hallucinated entities. To address
this, we propose Memory-Driven Sparse Attention Matrix (MDSAM) , a novel
training-free approach that dynamically captures and refines the attention
allocated to image tokens at each layer. MDSAM memorizes attention patterns and
activates updates through alignment during decoding, enhancing focus on
relevant image tokens while effectively reducing hallucinations. We evaluate
MDSAM on multiple benchmarks for tasks such as image captioning and visual
question answering, demonstrating its ability to consistently reduce
hallucinations and improve reliability. Compatible with various LVLM
architectures, MDSAM highlights its adaptability and effectiveness in
mitigating hallucinations without requiring additional training or external
tools.

</details>


### [120] [CSDN: A Context-Gated Self-Adaptive Detection Network for Real-Time Object Detection](https://arxiv.org/abs/2506.17679)
*Wei Haolin*

Main category: cs.CV

TL;DR: The paper introduces CSDN, a Transformer-based detection header for CNNs, replacing self-attention with a gating mechanism to improve global context modeling and adaptability to object sizes.


<details>
  <summary>Details</summary>
Motivation: CNNs in target detection lack global context due to limited receptive fields. The paper questions the necessity of self-attention in DETR-inspired headers and addresses feature redundancy.

Method: Proposes CSDN, a gating mechanism replacing stacked self-attention and cross-attention layers, enabling adaptive feature selection and scale information combination.

Result: CSDN enhances detection accuracy with minimal fine-tuning, avoiding extensive re-training, and improves adaptability to diverse object sizes and structures.

Conclusion: CSDN efficiently utilizes CNN backbone features, offering superior global context modeling and direct integration into existing detectors.

Abstract: Convolutional neural networks (CNNs) have long been the cornerstone of target
detection, but they are often limited by limited receptive fields, which
hinders their ability to capture global contextual information. This paper
believes that the effective utilization of extracted features is as important
as the feature extraction process itself. We critically re-evaluated the
DETR-inspired header network architecture, questioning the indispensable nature
of its self-attention mechanism, and discovering significant information
redundancies. To solve these problems, we introduced the Context-Gated
Scale-Adaptive Detection Network (CSDN), a Transformer-based detection header
inspired by natural language processing architecture and human visual
perception. CSDN aims to efficiently utilize the characteristics of the CNN
backbone network by replacing the traditional stacked self-attention and
cross-attention layers with a novel gating mechanism. This mechanism enables
each region of interest (ROI) to adaptively select and combine feature
dimensions and scale information from multiple attention patterns. CSDN
provides more powerful global context modeling capabilities and can better
adapt to objects of different sizes and structures. Our proposed detection head
can directly replace the native heads of various CNN-based detectors, and only
a few rounds of fine-tuning on the pre-training weights can significantly
improve the detection accuracy, thus avoiding the need to achieve small
improvements. Various layer modules undergo extensive re-training.

</details>


### [121] [Domain Generalization using Action Sequences for Egocentric Action Recognition](https://arxiv.org/abs/2506.17685)
*Amirshayan Nasirimajd,Chiara Plizzari,Simone Alberto Peirone,Marco Ciccone,Giuseppe Averta,Barbara Caputo*

Main category: cs.CV

TL;DR: Proposes SeqDG, a domain generalization method for Egocentric Action Recognition, using action sequences to improve model performance in unseen environments.


<details>
  <summary>Details</summary>
Motivation: Address performance drop in Egocentric Action Recognition models due to variability in illumination, viewpoint, and environment when tested in unseen settings.

Method: Introduces SeqDG with SeqRec (visual-text sequence reconstruction) and SeqMix (training on mixed action sequences from different domains).

Result: +2.4% improvement on EPIC-KITCHENS-100 and +0.6% Top-1 accuracy on EGTEA over SOTA.

Conclusion: SeqDG effectively enhances generalization in unseen environments by leveraging action sequences and contextual cues.

Abstract: Recognizing human activities from visual inputs, particularly through a
first-person viewpoint, is essential for enabling robots to replicate human
behavior. Egocentric vision, characterized by cameras worn by observers,
captures diverse changes in illumination, viewpoint, and environment. This
variability leads to a notable drop in the performance of Egocentric Action
Recognition models when tested in environments not seen during training. In
this paper, we tackle these challenges by proposing a domain generalization
approach for Egocentric Action Recognition. Our insight is that action
sequences often reflect consistent user intent across visual domains. By
leveraging action sequences, we aim to enhance the model's generalization
ability across unseen environments. Our proposed method, named SeqDG,
introduces a visual-text sequence reconstruction objective (SeqRec) that uses
contextual cues from both text and visual inputs to reconstruct the central
action of the sequence. Additionally, we enhance the model's robustness by
training it on mixed sequences of actions from different domains (SeqMix). We
validate SeqDG on the EGTEA and EPIC-KITCHENS-100 datasets. Results on
EPIC-KITCHENS-100, show that SeqDG leads to +2.4% relative average improvement
in cross-domain action recognition in unseen environments, and on EGTEA the
model achieved +0.6% Top-1 accuracy over SOTA in intra-domain action
recognition.

</details>


### [122] [SSAVSV: Towards Unified Model for Self-Supervised Audio-Visual Speaker Verification](https://arxiv.org/abs/2506.17694)
*Gnana Praveen Rajasekhar,Jahangir Alam*

Main category: cs.CV

TL;DR: A self-supervised learning framework using contrastive learning and masked data modeling for efficient, scalable audiovisual speaker verification with a shared vision transformer backbone.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of labeled data dependency and high computational costs in conventional audio-visual speaker verification methods.

Method: Proposes a unified self-supervised framework with contrastive learning, asymmetric masking, and masked data modeling, using a single shared vision transformer backbone for audio and visual inputs.

Result: Achieves competitive performance without labeled data, reduces computational costs, and handles missing modalities robustly.

Conclusion: The framework is efficient, scalable, and outperforms traditional methods in resource-constrained scenarios.

Abstract: Conventional audio-visual methods for speaker verification rely on large
amounts of labeled data and separate modality-specific architectures, which is
computationally expensive, limiting their scalability. To address these
problems, we propose a self-supervised learning framework based on contrastive
learning with asymmetric masking and masked data modeling to obtain robust
audiovisual feature representations. In particular, we employ a unified
framework for self-supervised audiovisual speaker verification using a single
shared backbone for audio and visual inputs, leveraging the versatility of
vision transformers. The proposed unified framework can handle audio, visual,
or audiovisual inputs using a single shared vision transformer backbone during
training and testing while being computationally efficient and robust to
missing modalities. Extensive experiments demonstrate that our method achieves
competitive performance without labeled data while reducing computational costs
compared to traditional approaches.

</details>


### [123] [DreamJourney: Perpetual View Generation with Video Diffusion Models](https://arxiv.org/abs/2506.17705)
*Bo Pan,Yang Chen,Yingwei Pan,Ting Yao,Wei Chen,Tao Mei*

Main category: cs.CV

TL;DR: DreamJourney introduces a two-stage framework for perpetual dynamic scene view generation, combining 3D awareness and object dynamics using video diffusion models and multimodal language models.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack 3D awareness and fail to capture object movements in dynamic scenes, leading to distorted artifacts.

Method: Stage I lifts an input image to 3D, renders partial images, and uses a video diffusion model for coherence. Stage II employs a multimodal language model to animate object movements, repeated recurrently.

Result: DreamJourney outperforms state-of-the-art methods in generating coherent, dynamic scenes with both camera movements and object dynamics.

Conclusion: The framework successfully addresses limitations of prior methods, enabling high-quality perpetual dynamic scene generation.

Abstract: Perpetual view generation aims to synthesize a long-term video corresponding
to an arbitrary camera trajectory solely from a single input image. Recent
methods commonly utilize a pre-trained text-to-image diffusion model to
synthesize new content of previously unseen regions along camera movement.
However, the underlying 2D diffusion model lacks 3D awareness and results in
distorted artifacts. Moreover, they are limited to generating views of static
3D scenes, neglecting to capture object movements within the dynamic 4D world.
To alleviate these issues, we present DreamJourney, a two-stage framework that
leverages the world simulation capacity of video diffusion models to trigger a
new perpetual scene view generation task with both camera movements and object
dynamics. Specifically, in stage I, DreamJourney first lifts the input image to
3D point cloud and renders a sequence of partial images from a specific camera
trajectory. A video diffusion model is then utilized as generative prior to
complete the missing regions and enhance visual coherence across the sequence,
producing a cross-view consistent video adheres to the 3D scene and camera
trajectory. Meanwhile, we introduce two simple yet effective strategies (early
stopping and view padding) to further stabilize the generation process and
improve visual quality. Next, in stage II, DreamJourney leverages a multimodal
large language model to produce a text prompt describing object movements in
current view, and uses video diffusion model to animate current view with
object movements. Stage I and II are repeated recurrently, enabling perpetual
dynamic scene view generation. Extensive experiments demonstrate the
superiority of our DreamJourney over state-of-the-art methods both
quantitatively and qualitatively. Our project page:
https://dream-journey.vercel.app.

</details>


### [124] [Programmable-Room: Interactive Textured 3D Room Meshes Generation Empowered by Large Language Models](https://arxiv.org/abs/2506.17707)
*Jihyun Kim,Junho Park,Kyeongbo Kong,Suk-Ju Kang*

Main category: cs.CV

TL;DR: Programmable-Room is a framework for generating and editing 3D room meshes using natural language instructions, leveraging visual programming and a diffusion model for texture generation.


<details>
  <summary>Details</summary>
Motivation: To enable precise control over 3D room attributes through natural language, decomposing the complex task into manageable steps.

Method: Decomposes tasks into steps like 3D coordinate generation, panorama texture creation, and furniture arrangement, using visual programming and a diffusion model enhanced by bidirectional LSTM.

Result: Demonstrates flexibility in 3D room generation/editing and outperforms existing models quantitatively and qualitatively.

Conclusion: Programmable-Room offers a unified, effective solution for interactive 3D room design via natural language.

Abstract: We present Programmable-Room, a framework which interactively generates and
edits a 3D room mesh, given natural language instructions. For precise control
of a room's each attribute, we decompose the challenging task into simpler
steps such as creating plausible 3D coordinates for room meshes, generating
panorama images for the texture, constructing 3D meshes by integrating the
coordinates and panorama texture images, and arranging furniture. To support
the various decomposed tasks with a unified framework, we incorporate visual
programming (VP). VP is a method that utilizes a large language model (LLM) to
write a Python-like program which is an ordered list of necessary modules for
the various tasks given in natural language. We develop most of the modules.
Especially, for the texture generating module, we utilize a pretrained
large-scale diffusion model to generate panorama images conditioned on text and
visual prompts (i.e., layout, depth, and semantic map) simultaneously.
Specifically, we enhance the panorama image generation quality by optimizing
the training objective with a 1D representation of a panorama scene obtained
from bidirectional LSTM. We demonstrate Programmable-Room's flexibility in
generating and editing 3D room meshes, and prove our framework's superiority to
an existing model quantitatively and qualitatively. Project page is available
in https://jihyun0510.github.io/Programmable_Room_Page/.

</details>


### [125] [PDC-Net: Pattern Divide-and-Conquer Network for Pelvic Radiation Injury Segmentation](https://arxiv.org/abs/2506.17712)
*Xinyu Xiong,Wuteng Cao,Zihuang Wu,Lei Zhang,Chong Gao,Guanbin Li,Qiyuan Qin*

Main category: cs.CV

TL;DR: Proposes PDC-Net for PRI segmentation using MDA and MGC modules to handle complex organ shapes and confusing context, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Accurate PRI segmentation from MRI is vital for prognosis and personalized treatment, but automated methods face challenges like complex morphologies and confusing context.

Method: Introduces PDC-Net with MDA for shape fitting and MGC for context distinction, along with AFD for dynamic feature fusion.

Result: PDC-Net outperforms existing methods on a large-scale PRI dataset.

Conclusion: PDC-Net effectively addresses PRI segmentation challenges, offering improved accuracy for clinical applications.

Abstract: Accurate segmentation of Pelvic Radiation Injury (PRI) from Magnetic
Resonance Images (MRI) is crucial for more precise prognosis assessment and the
development of personalized treatment plans. However, automated segmentation
remains challenging due to factors such as complex organ morphologies and
confusing context. To address these challenges, we propose a novel Pattern
Divide-and-Conquer Network (PDC-Net) for PRI segmentation. The core idea is to
use different network modules to "divide" various local and global patterns
and, through flexible feature selection, to "conquer" the Regions of Interest
(ROI) during the decoding phase. Specifically, considering that our ROI often
manifests as strip-like or circular-like structures in MR slices, we introduce
a Multi-Direction Aggregation (MDA) module. This module enhances the model's
ability to fit the shape of the organ by applying strip convolutions in four
distinct directions. Additionally, to mitigate the challenge of confusing
context, we propose a Memory-Guided Context (MGC) module. This module
explicitly maintains a memory parameter to track cross-image patterns at the
dataset level, thereby enhancing the distinction between global patterns
associated with the positive and negative classes. Finally, we design an
Adaptive Fusion Decoder (AFD) that dynamically selects features from different
patterns based on the Mixture-of-Experts (MoE) framework, ultimately generating
the final segmentation results. We evaluate our method on the first large-scale
pelvic radiation injury dataset, and the results demonstrate the superiority of
our PDC-Net over existing approaches.

</details>


### [126] [YOLOv13: Real-Time Object Detection with Hypergraph-Enhanced Adaptive Visual Perception](https://arxiv.org/abs/2506.17733)
*Mengqi Lei,Siqi Li,Yihong Wu,Han Hu,You Zhou,Xinhu Zheng,Guiguang Ding,Shaoyi Du,Zongze Wu,Yue Gao*

Main category: cs.CV

TL;DR: YOLOv13 introduces HyperACE and FullPAD for global high-order correlations, improving accuracy and efficiency in object detection.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of local information aggregation and pairwise correlation in YOLO models for better performance in complex scenarios.

Method: Proposes HyperACE for global high-order correlations and FullPAD for feature distribution, along with depthwise separable convolutions for efficiency.

Result: Achieves state-of-the-art performance on MS COCO, with YOLOv13-N improving mAP by 3.0% over YOLO11-N and 1.5% over YOLOv12-N.

Conclusion: YOLOv13 offers a lightweight, accurate solution for real-time object detection with global correlation modeling.

Abstract: The YOLO series models reign supreme in real-time object detection due to
their superior accuracy and computational efficiency. However, both the
convolutional architectures of YOLO11 and earlier versions and the area-based
self-attention mechanism introduced in YOLOv12 are limited to local information
aggregation and pairwise correlation modeling, lacking the capability to
capture global multi-to-multi high-order correlations, which limits detection
performance in complex scenarios. In this paper, we propose YOLOv13, an
accurate and lightweight object detector. To address the above-mentioned
challenges, we propose a Hypergraph-based Adaptive Correlation Enhancement
(HyperACE) mechanism that adaptively exploits latent high-order correlations
and overcomes the limitation of previous methods that are restricted to
pairwise correlation modeling based on hypergraph computation, achieving
efficient global cross-location and cross-scale feature fusion and enhancement.
Subsequently, we propose a Full-Pipeline Aggregation-and-Distribution (FullPAD)
paradigm based on HyperACE, which effectively achieves fine-grained information
flow and representation synergy within the entire network by distributing
correlation-enhanced features to the full pipeline. Finally, we propose to
leverage depthwise separable convolutions to replace vanilla large-kernel
convolutions, and design a series of blocks that significantly reduce
parameters and computational complexity without sacrificing performance. We
conduct extensive experiments on the widely used MS COCO benchmark, and the
experimental results demonstrate that our method achieves state-of-the-art
performance with fewer parameters and FLOPs. Specifically, our YOLOv13-N
improves mAP by 3.0\% over YOLO11-N and by 1.5\% over YOLOv12-N. The code and
models of our YOLOv13 model are available at:
https://github.com/iMoonLab/yolov13.

</details>


### [127] [PhysID: Physics-based Interactive Dynamics from a Single-view Image](https://arxiv.org/abs/2506.17746)
*Sourabh Vasant Gothe,Ayon Chattopadhyay,Gunturi Venkata Sai Phani Kiran,Pratik,Vibhav Agarwal,Jayesh Rajkumar Vachhani,Sourav Ghosh,Parameswaranath VM,Barath Raj KR*

Main category: cs.CV

TL;DR: PhysID simplifies creating physics-based interactive dynamics from single-view images using generative models, reducing manual effort and enabling real-time interactions.


<details>
  <summary>Details</summary>
Motivation: Enhancing mobile user experiences through interactive and AR/VR applications by overcoming challenges in transforming static images into interactive ones.

Method: Leverages large generative models for 3D mesh generation and physical property prediction, integrating an on-device physics engine for real-time rendering.

Result: Demonstrates cohesive functioning of modules, enabling real-time, non-deterministic interactions with efficient memory use.

Conclusion: PhysID advances mobile-based interactive dynamics, scaling with minimal manual intervention and offering user-personalization.

Abstract: Transforming static images into interactive experiences remains a challenging
task in computer vision. Tackling this challenge holds the potential to elevate
mobile user experiences, notably through interactive and AR/VR applications.
Current approaches aim to achieve this either using pre-recorded video
responses or requiring multi-view images as input. In this paper, we present
PhysID, that streamlines the creation of physics-based interactive dynamics
from a single-view image by leveraging large generative models for 3D mesh
generation and physical property prediction. This significantly reduces the
expertise required for engineering-intensive tasks like 3D modeling and
intrinsic property calibration, enabling the process to be scaled with minimal
manual intervention. We integrate an on-device physics-based engine for
physically plausible real-time rendering with user interactions. PhysID
represents a leap forward in mobile-based interactive dynamics, offering
real-time, non-deterministic interactions and user-personalization with
efficient on-device memory consumption. Experiments evaluate the zero-shot
capabilities of various Multimodal Large Language Models (MLLMs) on diverse
tasks and the performance of 3D reconstruction models. These results
demonstrate the cohesive functioning of all modules within the end-to-end
framework, contributing to its effectiveness.

</details>


### [128] [LoLA-SpecViT: Local Attention SwiGLU Vision Transformer with LoRA for Hyperspectral Imaging](https://arxiv.org/abs/2506.17759)
*Fadi Abdeladhim Zidi,Djamel Eddine Boukhari,Abdellah Zakaria Sellam,Abdelkrim Ouafi,Cosimo Distante,Salah Eddine Bekhouche,Abdelmalik Taleb-Ahmed*

Main category: cs.CV

TL;DR: LoLA-SpecViT is a lightweight spectral vision transformer for hyperspectral image classification, combining 3D convolution and local self-attention with low-rank adaptation (LoRA) for efficiency. It achieves high accuracy with fewer parameters and better adaptability under limited labels.


<details>
  <summary>Details</summary>
Motivation: Hyperspectral image classification faces challenges like high dimensionality, inter-band redundancy, and scarce annotated samples. Existing transformer models lack scalability and adaptability in label-scarce conditions.

Method: Proposes LoLA-SpecViT, integrating a 3D convolutional front-end, local window-based self-attention, and LoRA for parameter efficiency. Uses a cyclical learning rate scheduler for improved adaptation.

Result: Outperforms state-of-the-art baselines on three datasets (WHU-Hi LongKou, WHU-Hi HongHu, Salinas), achieving up to 99.91% accuracy with fewer parameters and robustness in low-label settings.

Conclusion: LoLA-SpecViT offers a scalable, generalizable solution for hyperspectral imagery applications, with potential uses in agriculture, environmental monitoring, and remote sensing.

Abstract: Hyperspectral image classification remains a challenging task due to the high
dimensionality of spectral data, significant inter-band redundancy, and the
limited availability of annotated samples. While recent transformer-based
models have improved the global modeling of spectral-spatial dependencies,
their scalability and adaptability under label-scarce conditions remain
limited. In this work, we propose \textbf{LoLA-SpecViT}(Low-rank adaptation
Local Attention Spectral Vision Transformer), a lightweight spectral vision
transformer that addresses these limitations through a parameter-efficient
architecture tailored to the unique characteristics of hyperspectral imagery.
Our model combines a 3D convolutional spectral front-end with local
window-based self-attention, enhancing both spectral feature extraction and
spatial consistency while reducing computational complexity. To further improve
adaptability, we integrate low-rank adaptation (LoRA) into attention and
projection layers, enabling fine-tuning with over 80\% fewer trainable
parameters. A novel cyclical learning rate scheduler modulates LoRA adaptation
strength during training, improving convergence and generalisation. Extensive
experiments on three benchmark datasets WHU-Hi LongKou, WHU-Hi HongHu, and
Salinas demonstrate that LoLA-SpecViT consistently outperforms state-of-the-art
baselines, achieving up to 99.91\% accuracy with substantially fewer parameters
and enhanced robustness under low-label regimes. The proposed framework
provides a scalable and generalizable solution for real-world HSI applications
in agriculture, environmental monitoring, and remote sensing analytics. Our
code is available in the following
\href{https://github.com/FadiZidiDz/LoLA-SpecViT}{GitHub Repository}.

</details>


### [129] [Incorporating Rather Than Eliminating: Achieving Fairness for Skin Disease Diagnosis Through Group-Specific Expert](https://arxiv.org/abs/2506.17787)
*Gelei Xu,Yuying Duan,Zheyuan Liu,Xueyang Li,Meng Jiang,Michael Lemmon,Wei Jin,Yiyu Shi*

Main category: cs.CV

TL;DR: FairMoE, a fairness-aware AI framework, improves accuracy in skin disease diagnostics while maintaining fairness by dynamically routing data to group-specific experts.


<details>
  <summary>Details</summary>
Motivation: Existing bias mitigation methods degrade performance by removing clinically relevant cues. FairMoE aims to incorporate sensitive attributes for fairness without sacrificing accuracy.

Method: FairMoE uses layer-wise mixture-of-experts modules to dynamically route data to group-specific learners, handling cases near group boundaries effectively.

Result: FairMoE achieves substantial accuracy improvements while preserving fairness metrics, unlike traditional methods that reduce performance.

Conclusion: FairMoE offers a promising alternative to traditional bias mitigation by balancing fairness and accuracy in AI-based diagnostics.

Abstract: AI-based systems have achieved high accuracy in skin disease diagnostics but
often exhibit biases across demographic groups, leading to inequitable
healthcare outcomes and diminished patient trust. Most existing bias mitigation
methods attempt to eliminate the correlation between sensitive attributes and
diagnostic prediction, but those methods often degrade performance due to the
lost of clinically relevant diagnostic cues. In this work, we propose an
alternative approach that incorporates sensitive attributes to achieve
fairness. We introduce FairMoE, a framework that employs layer-wise
mixture-of-experts modules to serve as group-specific learners. Unlike
traditional methods that rigidly assign data based on group labels, FairMoE
dynamically routes data to the most suitable expert, making it particularly
effective for handling cases near group boundaries. Experimental results show
that, unlike previous fairness approaches that reduce performance, FairMoE
achieves substantial accuracy improvements while preserving comparable fairness
metrics.

</details>


### [130] [Time-Contrastive Pretraining for In-Context Image and Video Segmentation](https://arxiv.org/abs/2506.17837)
*Assefa Wahd,Jacob Jaremko,Abhilash Hareendranathan*

Main category: cs.CV

TL;DR: Temporal introduces a time-contrastive self-supervised objective for visual in-context learning (ICL), reframing it as a video object segmentation task to overcome grid-based limitations. It achieves significant improvements in segmentation tasks.


<details>
  <summary>Details</summary>
Motivation: Grid-based ICL lacks flexibility for vision tasks, restricting context image resolution and quantity. Temporal aims to address these limitations.

Method: Pretrains a prompt retriever using self-supervised learning on videos, treating adjacent frames as positives and distant ones as negatives. Reframes ICL as a VOS task for flexible context handling.

Result: Achieves 90.95% Dice for image segmentation (10.64% improvement) and 92.45% Dice for video segmentation (14.88% improvement) on MICCAI FLARE 2022.

Conclusion: Temporal effectively addresses grid-based ICL limitations, offering a flexible and high-performing solution for visual tasks.

Abstract: In-context learning (ICL) enables generalization to new tasks with minimal
labeled data. However, mainstream ICL approaches rely on a gridding strategy,
which lacks the flexibility required for vision applications. We introduce
Temporal, a time-contrastive self-supervised objective that pretrains a prompt
retriever for visual ICL, and formulate ICL as a video object segmentation
(VOS) task. Temporal addresses key limitations of grid-based methods that
restrict the number and resolution of context images. By reframing ICL as a VOS
problem, our approach supports a variable number of context images while
preserving their full resolution. To address the challenge of selecting optimal
context sets for queries, we pretrain a prompt retriever on videos via
self-supervised learning, where adjacent frames serve as positives and distant
frames as negatives. For image segmentation, the prompt retriever selects
relevant sequences that, when combined with the query, form coherent videos for
VOS processing. For video segmentation, it identifies keyframes, predicts their
masks using our ICL pipeline, and propagates them throughout the sequence. When
evaluated on MICCAI FLARE 2022, our method achieves substantial improvements
over baselines: 90.95% Dice score for image segmentation (10.64% improvement)
and 92.45% Dice for video segmentation (14.88% improvement).

</details>


### [131] [Robust Foreground-Background Separation for Severely-Degraded Videos Using Convolutional Sparse Representation Modeling](https://arxiv.org/abs/2506.17838)
*Kazuki Naganuma,Shunsuke Ono*

Main category: cs.CV

TL;DR: Proposes a robust foreground-background separation (FBS) method using convolutional sparse representation (CSR) to handle degraded videos with low frame rates and noise.


<details>
  <summary>Details</summary>
Motivation: Existing FBS methods fail to accurately separate components in degraded videos due to limitations in capturing both data-specific and general features, and lack of explicit noise models.

Method: A CSR-based foreground model is introduced, and FBS is formulated as a constrained multiconvex optimization problem incorporating CSR, general features, and noise characterization functions. An algorithm solves the problem via convex subproblems.

Result: The method outperforms existing techniques on degraded infrared and microscope videos, accurately separating components despite noise and low frame rates.

Conclusion: The proposed CSR-based FBS method effectively handles degraded videos by combining adaptive spatial structure capture, general features, and explicit noise modeling.

Abstract: This paper proposes a foreground-background separation (FBS) method with a
novel foreground model based on convolutional sparse representation (CSR). In
order to analyze the dynamic and static components of videos acquired under
undesirable conditions, such as hardware, environmental, and power limitations,
it is essential to establish an FBS method that can handle videos with low
frame rates and various types of noise. Existing FBS methods have two
limitations that prevent us from accurately separating foreground and
background components from such degraded videos. First, they only capture
either data-specific or general features of the components. Second, they do not
include explicit models for various types of noise to remove them in the FBS
process. To this end, we propose a robust FBS method with a CSR-based
foreground model. This model can adaptively capture specific spatial structures
scattered in imaging data. Then, we formulate FBS as a constrained multiconvex
optimization problem that incorporates CSR, functions that capture general
features, and explicit noise characterization functions for multiple types of
noise. Thanks to these functions, our method captures both data-specific and
general features to accurately separate the components from various types of
noise even under low frame rates. To obtain a solution of the optimization
problem, we develop an algorithm that alternately solves its two convex
subproblems by newly established algorithms. Experiments demonstrate the
superiority of our method over existing methods using two types of degraded
videos: infrared and microscope videos.

</details>


### [132] [Fetuses Made Simple: Modeling and Tracking of Fetal Shape and Pose](https://arxiv.org/abs/2506.17858)
*Yingcheng Liu,Peiqi Wang,Sebastian Diaz,Esra Abaci Turk,Benjamin Billot,Patricia Ellen Grant,Polina Golland*

Main category: cs.CV

TL;DR: A 3D articulated statistical fetal body model is introduced to improve fetal motion and shape analysis in MRI, addressing limitations of keypoints and segmentations.


<details>
  <summary>Details</summary>
Motivation: Existing methods for fetal MRI analysis (keypoints or segmentations) lack detail or complicate temporal analysis due to fetal movements.

Method: A 3D articulated model based on SMPL is developed, iteratively estimating pose in image space and shape in canonical pose space.

Result: The model achieves a surface alignment error of 3.2 mm for 3 mm MRI voxel size and enables automated anthropometric measurements.

Conclusion: This is the first 3D articulated statistical fetal body model, enhancing prenatal diagnostics with robust motion and shape analysis.

Abstract: Analyzing fetal body motion and shape is paramount in prenatal diagnostics
and monitoring. Existing methods for fetal MRI analysis mainly rely on
anatomical keypoints or volumetric body segmentations. Keypoints simplify body
structure to facilitate motion analysis, but may ignore important details of
full-body shape. Body segmentations capture complete shape information but
complicate temporal analysis due to large non-local fetal movements. To address
these limitations, we construct a 3D articulated statistical fetal body model
based on the Skinned Multi-Person Linear Model (SMPL). Our algorithm
iteratively estimates body pose in the image space and body shape in the
canonical pose space. This approach improves robustness to MRI motion artifacts
and intensity distortions, and reduces the impact of incomplete surface
observations due to challenging fetal poses. We train our model on
segmentations and keypoints derived from $19,816$ MRI volumes across $53$
subjects. Our model captures body shape and motion across time series and
provides intuitive visualization. Furthermore, it enables automated
anthropometric measurements traditionally difficult to obtain from
segmentations and keypoints. When tested on unseen fetal body shapes, our
method yields a surface alignment error of $3.2$ mm for $3$ mm MRI voxel size.
To our knowledge, this represents the first 3D articulated statistical fetal
body model, paving the way for enhanced fetal motion and shape analysis in
prenatal diagnostics. The code is available at
https://github.com/MedicalVisionGroup/fetal-smpl .

</details>


### [133] [Cross-modal State Space Modeling for Real-time RGB-thermal Wild Scene Semantic Segmentation](https://arxiv.org/abs/2506.17869)
*Xiaodong Guo,Zi'ang Lin,Luwen Hu,Zhihong Deng,Tong Liu,Wujie Zhou*

Main category: cs.CV

TL;DR: CM-SSM is an efficient RGB-thermal semantic segmentation architecture using cross-modal state space modeling, achieving linear computational complexity and state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Improving semantic segmentation in wild environments for field robots by integrating RGB and thermal data while addressing computational overhead.

Method: Introduces CM-SSM with two modules: CM-SS2D for cross-modal state space modeling and CM-SSA for integrating global associations with local features.

Result: Achieves state-of-the-art performance on CART and PST900 datasets with fewer parameters and lower computational cost.

Conclusion: CM-SSM is a scalable and efficient solution for RGB-thermal semantic segmentation, outperforming Transformer-based methods.

Abstract: The integration of RGB and thermal data can significantly improve semantic
segmentation performance in wild environments for field robots. Nevertheless,
multi-source data processing (e.g. Transformer-based approaches) imposes
significant computational overhead, presenting challenges for
resource-constrained systems. To resolve this critical limitation, we
introduced CM-SSM, an efficient RGB-thermal semantic segmentation architecture
leveraging a cross-modal state space modeling (SSM) approach. Our framework
comprises two key components. First, we introduced a cross-modal
2D-selective-scan (CM-SS2D) module to establish SSM between RGB and thermal
modalities, which constructs cross-modal visual sequences and derives hidden
state representations of one modality from the other. Second, we developed a
cross-modal state space association (CM-SSA) module that effectively integrates
global associations from CM-SS2D with local spatial features extracted through
convolutional operations. In contrast with Transformer-based approaches, CM-SSM
achieves linear computational complexity with respect to image resolution.
Experimental results show that CM-SSM achieves state-of-the-art performance on
the CART dataset with fewer parameters and lower computational cost. Further
experiments on the PST900 dataset demonstrate its generalizability. Codes are
available at https://github.com/xiaodonguo/CMSSM.

</details>


### [134] [SurgVidLM: Towards Multi-grained Surgical Video Understanding with Large Language Model](https://arxiv.org/abs/2506.17873)
*Guankun Wang,Wenjin Mo,Junyi Wang,Long Bai,Kun Yuan,Ming Hu,Jinlin Wu,Junjun He,Yiming Huang,Nicolas Padoy,Zhen Lei,Hongbin Liu,Nassir Navab,Hongliang Ren*

Main category: cs.CV

TL;DR: SurgVidLM is a specialized Video Large Language Model (Vid-LLM) for surgical video understanding, outperforming existing models in both full and fine-grained tasks.


<details>
  <summary>Details</summary>
Motivation: Current Vid-LLMs lack specialization for fine-grained surgical video understanding, which is critical for analyzing specific surgical details.

Method: Proposes SurgVidLM, trained on the SVU-31K dataset, with StageFocus for multi-grained understanding and Multi-frequency Fusion Attention for visual token integration.

Result: SurgVidLM outperforms state-of-the-art Vid-LLMs in surgical video comprehension tasks.

Conclusion: SurgVidLM effectively bridges the gap in fine-grained surgical video understanding, demonstrating superior performance.

Abstract: Recent advances in Multimodal Large Language Models have demonstrated great
potential in the medical domain, facilitating users to understand surgical
scenes and procedures. Beyond image-based methods, the exploration of Video
Large Language Models (Vid-LLMs) has emerged as a promising avenue for
capturing the complex sequences of information involved in surgery. However,
there is still a lack of Vid-LLMs specialized for fine-grained surgical video
understanding tasks, which is crucial for analyzing specific processes or
details within a surgical procedure. To bridge this gap, we propose SurgVidLM,
the first video language model designed to address both full and fine-grained
surgical video comprehension. To train our SurgVidLM, we construct the SVU-31K
dataset which consists of over 31K video-instruction pairs, enabling both
holistic understanding and detailed analysis of surgical procedures.
Furthermore, we introduce the StageFocus mechanism which is a two-stage
framework performing the multi-grained, progressive understanding of surgical
videos. We also develop the Multi-frequency Fusion Attention to effectively
integrate low and high-frequency visual tokens, ensuring the retention of
critical information. Experimental results demonstrate that SurgVidLM
significantly outperforms state-of-the-art Vid-LLMs in both full and
fine-grained video understanding tasks, showcasing its superior capability in
capturing complex procedural contexts.

</details>


### [135] [StainPIDR: A Pathological Image Decouplingand Reconstruction Method for StainNormalization Based on Color VectorQuantization and Structure Restaining](https://arxiv.org/abs/2506.17879)
*Zheng Chen*

Main category: cs.CV

TL;DR: StainPIDR is a stain normalization method for pathological images, decoupling structure and color features, restaining with target colors, and using a fixed color codebook for consistency.


<details>
  <summary>Details</summary>
Motivation: Color discrepancies in pathological images due to varying protocols, dyes, and devices degrade computer-aided diagnostic systems.

Method: Decouples images into structure and vector-quantized color features, restains structure with target colors via cross-attention, and selects optimal templates for normalization.

Result: Effective stain normalization demonstrated in experiments, with improved performance in handling color-variant images.

Conclusion: StainPIDR successfully normalizes pathological images, enhancing diagnostic system reliability, with code to be released.

Abstract: The color appearance of a pathological image is highly related to the imaging
protocols, the proportion of different dyes, and the scanning devices.
Computer-aided diagnostic systems may deteriorate when facing these
color-variant pathological images. In this work, we propose a stain
normalization method called StainPIDR. We try to eliminate this color
discrepancy by decoupling the image into structure features and
vector-quantized color features, restaining the structure features with the
target color features, and decoding the stained structure features to
normalized pathological images. We assume that color features decoupled by
different images with the same color should be exactly the same. Under this
assumption, we train a fixed color vector codebook to which the decoupled color
features will map. In the restaining part, we utilize the cross-attention
mechanism to efficiently stain the structure features. As the target color
(decoupled from a selected template image) will also affect the performance of
stain normalization, we further design a template image selection algorithm to
select a template from a given dataset. In our extensive experiments, we
validate the effectiveness of StainPIDR and the template image selection
algorithm. All the results show that our method can perform well in the stain
normalization task. The code of StainPIDR will be publicly available later.

</details>


### [136] [Cloud-Aware SAR Fusion for Enhanced Optical Sensing in Space Missions](https://arxiv.org/abs/2506.17885)
*Trong-An Bui,Thanh-Thoai Le*

Main category: cs.CV

TL;DR: A Cloud-Attentive Reconstruction Framework combines SAR-optical feature fusion and deep learning to generate cloud-free optical imagery, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Cloud contamination reduces the usability of optical satellite imagery, impacting applications like environmental monitoring and disaster response.

Method: The framework uses attention-driven feature fusion to align SAR structural data with optical spectral data and employs adaptive loss weighting for cloud-occluded regions.

Result: Achieves PSNR of 31.01 dB, SSIM of 0.918, and MAE of 0.017, outperforming existing methods.

Conclusion: The framework effectively produces high-fidelity, cloud-free optical images with spatial and spectral consistency.

Abstract: Cloud contamination significantly impairs the usability of optical satellite
imagery, affecting critical applications such as environmental monitoring,
disaster response, and land-use analysis. This research presents a
Cloud-Attentive Reconstruction Framework that integrates SAR-optical feature
fusion with deep learning-based image reconstruction to generate cloud-free
optical imagery. The proposed framework employs an attention-driven feature
fusion mechanism to align complementary structural information from Synthetic
Aperture Radar (SAR) with spectral characteristics from optical data.
Furthermore, a cloud-aware model update strategy introduces adaptive loss
weighting to prioritize cloud-occluded regions, enhancing reconstruction
accuracy. Experimental results demonstrate that the proposed method outperforms
existing approaches, achieving a PSNR of 31.01 dB, SSIM of 0.918, and MAE of
0.017. These outcomes highlight the framework's effectiveness in producing
high-fidelity, spatially and spectrally consistent cloud-free optical images.

</details>


### [137] [Relation3D: Enhancing Relation Modeling for Point Cloud Instance Segmentation](https://arxiv.org/abs/2506.17891)
*Jiahao Lu,Jiacheng Deng*

Main category: cs.CV

TL;DR: Relation3D improves 3D instance segmentation by enhancing internal and external feature relationships using adaptive superpoint aggregation, contrastive learning, and relation-aware self-attention.


<details>
  <summary>Details</summary>
Motivation: Current transformer-based methods lack effective modeling of internal relationships among scene and query features, limiting performance.

Method: Introduces adaptive superpoint aggregation, contrastive learning-guided refinement, and relation-aware self-attention to enhance feature relationships.

Result: Demonstrates superior performance on ScanNetV2, ScanNet++, ScanNet200, and S3DIS datasets.

Conclusion: Relation3D effectively addresses limitations in current methods, improving 3D instance segmentation through better relation modeling.

Abstract: 3D instance segmentation aims to predict a set of object instances in a
scene, representing them as binary foreground masks with corresponding semantic
labels. Currently, transformer-based methods are gaining increasing attention
due to their elegant pipelines and superior predictions. However, these methods
primarily focus on modeling the external relationships between scene features
and query features through mask attention. They lack effective modeling of the
internal relationships among scene features as well as between query features.
In light of these disadvantages, we propose \textbf{Relation3D: Enhancing
Relation Modeling for Point Cloud Instance Segmentation}. Specifically, we
introduce an adaptive superpoint aggregation module and a contrastive
learning-guided superpoint refinement module to better represent superpoint
features (scene features) and leverage contrastive learning to guide the
updates of these features. Furthermore, our relation-aware self-attention
mechanism enhances the capabilities of modeling relationships between queries
by incorporating positional and geometric relationships into the self-attention
mechanism. Extensive experiments on the ScanNetV2, ScanNet++, ScanNet200 and
S3DIS datasets demonstrate the superior performance of Relation3D.

</details>


### [138] [BeltCrack: the First Sequential-image Industrial Conveyor Belt Crack Detection Dataset and Its Baseline with Triple-domain Feature Learning](https://arxiv.org/abs/2506.17892)
*Jianghong Huang,Luping Ji,Xin Ma,Mao Ye*

Main category: cs.CV

TL;DR: The paper introduces the first real-world industrial belt crack datasets (BeltCrack14ks and BeltCrack9kd) and proposes a triple-domain feature fusion method for crack detection, showing superior performance.


<details>
  <summary>Details</summary>
Motivation: Existing crack datasets lack real-world industrial belt data, hindering machine learning advancements in belt crack detection.

Method: Constructs real-world belt crack datasets and proposes a triple-domain (time-space-frequency) feature hierarchical fusion learning method.

Result: The datasets are validated as effective, and the proposed baseline method outperforms similar detection techniques.

Conclusion: The work advances machine learning in belt crack detection by providing real-world datasets and a superior baseline method.

Abstract: Conveyor belt is a category of important equipments in modern industry,
widely applied in production and manufacturing Fields. Its health status is
much critical to operation efficiency and safety hazards. Among the factors
affecting belt health, crack is often one of the most threatening risks.
Currently, considering safety, how to intelligently detect belt cracks is
catching an increasing attention. To implement the intelligent detection with
machine learning, real crack samples are believed to be necessary. However,
existing crack datasets primarily focus on pavement scenarios or synthetic
data, no real-world industrial belt crack datasets at all. To propel machine
learning advancement in this field, this paper constructs the first
sequential-image belt crack detection datasets (BeltCrack14ks and
BeltCrack9kd), from real-world factory scenes. Furthermore, to validate
usability and effectiveness, we propose a special baseline method with
triple-domain (i.e., time-space-frequency) feature hierarchical fusion learning
for the two whole-new datasets. Experimental results demonstrate the
availability and effectiveness of our dataset. Besides, they also show that our
baseline is obviously superior to other similar detection methods. Our datasets
and source codes are available at https://github.com/UESTC-nnLab/BeltCrack.

</details>


### [139] [EgoWorld: Translating Exocentric View to Egocentric View using Rich Exocentric Observations](https://arxiv.org/abs/2506.17896)
*Junho Park,Andrew Sangwoo Ye,Taein Kwon*

Main category: cs.CV

TL;DR: EgoWorld is a two-stage framework for translating exocentric views to egocentric ones, overcoming limitations of current methods by using 3D cues and diffusion-based inpainting.


<details>
  <summary>Details</summary>
Motivation: Current exocentric-to-egocentric translation methods rely on 2D cues and unrealistic assumptions, limiting their applicability in AR, VR, and robotics.

Method: EgoWorld reconstructs egocentric views using projected point clouds, 3D hand poses, and textual descriptions, followed by diffusion-based inpainting.

Result: Achieves state-of-the-art performance on H2O and TACO datasets, generalizing well to new objects, actions, scenes, and subjects.

Conclusion: EgoWorld demonstrates robust generalization and promising results on real-world examples, advancing egocentric vision applications.

Abstract: Egocentric vision is essential for both human and machine visual
understanding, particularly in capturing the detailed hand-object interactions
needed for manipulation tasks. Translating third-person views into first-person
views significantly benefits augmented reality (AR), virtual reality (VR) and
robotics applications. However, current exocentric-to-egocentric translation
methods are limited by their dependence on 2D cues, synchronized multi-view
settings, and unrealistic assumptions such as necessity of initial egocentric
frame and relative camera poses during inference. To overcome these challenges,
we introduce EgoWorld, a novel two-stage framework that reconstructs an
egocentric view from rich exocentric observations, including projected point
clouds, 3D hand poses, and textual descriptions. Our approach reconstructs a
point cloud from estimated exocentric depth maps, reprojects it into the
egocentric perspective, and then applies diffusion-based inpainting to produce
dense, semantically coherent egocentric images. Evaluated on the H2O and TACO
datasets, EgoWorld achieves state-of-the-art performance and demonstrates
robust generalization to new objects, actions, scenes, and subjects. Moreover,
EgoWorld shows promising results even on unlabeled real-world examples.

</details>


### [140] [PostAlign: Multimodal Grounding as a Corrective Lens for MLLMs](https://arxiv.org/abs/2506.17901)
*Yixuan Wu,Yang Zhang,Jian Wu,Philip Torr,Jindong Gu*

Main category: cs.CV

TL;DR: MMGrounded-PostAlign enhances MLLMs by improving visual understanding and reducing hallucinations through multimodal grounding and selective reasoning.


<details>
  <summary>Details</summary>
Motivation: MLLMs often rely on spurious correlations due to linguistic priors, limiting their use of actual visual information.

Method: The framework includes visual and textual grounding modules, a negative rejection mechanism, and selective reasoning.

Result: Significant improvements in fine-grained visual understanding and hallucination suppression on benchmarks like POPE and VQAv2.

Conclusion: MMGrounded-PostAlign effectively mitigates hallucinations and enhances visual grounding in MLLMs.

Abstract: Multimodal Large Language Models (MLLMs) excel in vision-language tasks, such
as image captioning and visual question answering. However, they often suffer
from over-reliance on spurious correlations, primarily due to linguistic priors
that distract the model from leveraging actual visual information. To address
these issues, we introduce MMGrounded-PostAlign, a post-multimodal alignment
framework designed to enhance the visual understanding capabilities and
mitigate the hallucinations of MLLMs. Our framework incorporates a multimodal
grounding module for both visual grounding, which identifies the referred
object in the image, and textual grounding, which generates the rationale for
the final answer, ensuring that outputs are anchored in both visual and textual
evidence. To mitigate the hallucinations, we introduce a negative rejection
mechanism in the visual grounding module to distinguish grounded entities from
non-existent objects influenced by linguistic biases. On the textual grounding
side, we propose a selective reasoning mechanism that adjusts the model's
reasoning strategy based on query complexity. Extensive evaluations are
conducted on benchmarks such as POPE, HaloQuest, VQAv2, MME, and MMBench
showing significant improvements in fine-grained visual understanding and
hallucination suppression.

</details>


### [141] [Cause-Effect Driven Optimization for Robust Medical Visual Question Answering with Language Biases](https://arxiv.org/abs/2506.17903)
*Huanjia Zhu,Yishu Liu,Xiaozhao Fang,Guangming Lu,Bingzhi Chen*

Main category: cs.CV

TL;DR: CEDO is a framework to mitigate language biases in Med-VQA models using three mechanisms: MHO, GMS, and DLR, addressing both causal and effectual biases.


<details>
  <summary>Details</summary>
Motivation: Existing Med-VQA models suffer from language biases due to spurious correlations between question types and answer categories.

Method: CEDO incorporates MHO (adaptive learning rates for modalities), GMS (Pareto optimization for modality synergy), and DLR (adaptive loss weights for balanced learning).

Result: CEDO outperforms state-of-the-art models on traditional and bias-sensitive benchmarks.

Conclusion: CEDO effectively mitigates language biases in Med-VQA models through a comprehensive cause-effect approach.

Abstract: Existing Medical Visual Question Answering (Med-VQA) models often suffer from
language biases, where spurious correlations between question types and answer
categories are inadvertently established. To address these issues, we propose a
novel Cause-Effect Driven Optimization framework called CEDO, that incorporates
three well-established mechanisms, i.e., Modality-driven Heterogeneous
Optimization (MHO), Gradient-guided Modality Synergy (GMS), and
Distribution-adapted Loss Rescaling (DLR), for comprehensively mitigating
language biases from both causal and effectual perspectives. Specifically, MHO
employs adaptive learning rates for specific modalities to achieve
heterogeneous optimization, thus enhancing robust reasoning capabilities.
Additionally, GMS leverages the Pareto optimization method to foster
synergistic interactions between modalities and enforce gradient orthogonality
to eliminate bias updates, thereby mitigating language biases from the effect
side, i.e., shortcut bias. Furthermore, DLR is designed to assign adaptive
weights to individual losses to ensure balanced learning across all answer
categories, effectively alleviating language biases from the cause side, i.e.,
imbalance biases within datasets. Extensive experiments on multiple traditional
and bias-sensitive benchmarks consistently demonstrate the robustness of CEDO
over state-of-the-art competitors.

</details>


### [142] [Feedback Driven Multi Stereo Vision System for Real-Time Event Analysis](https://arxiv.org/abs/2506.17910)
*Mohamed Benkedadra,Matei Mancas,Sidi Ahmed Mahmoudi*

Main category: cs.CV

TL;DR: A 3D stereo vision pipeline for interactive systems is proposed, addressing limitations of 2D and short-range 3D cameras in complex environments. It uses multi-camera fusion for robust scene understanding and tasks like tracking and event recognition. Feedback mechanisms enhance adaptability.


<details>
  <summary>Details</summary>
Motivation: Current 2D and short-range 3D cameras are unreliable in large, complex environments, necessitating a more robust solution for interactive systems.

Method: Proposes a 3D stereo vision pipeline with multi-camera fusion for full scene reconstruction, enabling tasks like tracking and event recognition. Feedback from subjects improves adaptability.

Result: Preliminary results demonstrate the pipeline's capability for robust scene understanding and task performance. Feedback enhances learning and adaptation.

Conclusion: The pipeline shows promise for interactive systems. Future work includes refining the system for production deployment.

Abstract: 2D cameras are often used in interactive systems. Other systems like gaming
consoles provide more powerful 3D cameras for short range depth sensing.
Overall, these cameras are not reliable in large, complex environments. In this
work, we propose a 3D stereo vision based pipeline for interactive systems,
that is able to handle both ordinary and sensitive applications, through robust
scene understanding. We explore the fusion of multiple 3D cameras to do full
scene reconstruction, which allows for preforming a wide range of tasks, like
event recognition, subject tracking, and notification. Using possible feedback
approaches, the system can receive data from the subjects present in the
environment, to learn to make better decisions, or to adapt to completely new
environments. Throughout the paper, we introduce the pipeline and explain our
preliminary experimentation and results. Finally, we draw the roadmap for the
next steps that need to be taken, in order to get this pipeline into production

</details>


### [143] [PlanMoGPT: Flow-Enhanced Progressive Planning for Text to Motion Synthesis](https://arxiv.org/abs/2506.17912)
*Chuhao Jin,Haosen Li,Bingzi Zhang,Che Liu,Xiting Wang,Ruihua Song,Wenbing Huang,Ying Qin,Fuzheng Zhang,Di Zhang*

Main category: cs.CV

TL;DR: PlanMoGPT, an LLM-based framework, addresses the text-to-motion generation gap by combining progressive planning and flow-enhanced tokenization, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: The performance gap in text-to-motion generation between LLM and non-LLM methods, caused by motion tokenization granularity issues, motivates the need for a better approach.

Method: PlanMoGPT integrates progressive planning (hierarchical token generation) and flow-enhanced fine-grained tokenization (higher resolution and larger codebook) to improve motion detail and global alignment.

Result: The framework improves FID scores by 63.8% and motion diversity by 49.9%, outperforming existing methods.

Conclusion: PlanMoGPT resolves the diversity-quality trade-off in text-to-motion generation, setting new benchmarks for the field.

Abstract: Recent advances in large language models (LLMs) have enabled breakthroughs in
many multimodal generation tasks, but a significant performance gap still
exists in text-to-motion generation, where LLM-based methods lag far behind
non-LLM methods. We identify the granularity of motion tokenization as a
critical bottleneck: fine-grained tokenization induces local dependency issues,
where LLMs overemphasize short-term coherence at the expense of global semantic
alignment, while coarse-grained tokenization sacrifices motion details. To
resolve this issue, we propose PlanMoGPT, an LLM-based framework integrating
progressive planning and flow-enhanced fine-grained motion tokenization. First,
our progressive planning mechanism leverages LLMs' autoregressive capabilities
to hierarchically generate motion tokens by starting from sparse global plans
and iteratively refining them into full sequences. Second, our flow-enhanced
tokenizer doubles the downsampling resolution and expands the codebook size by
eight times, minimizing detail loss during discretization, while a
flow-enhanced decoder recovers motion nuances. Extensive experiments on
text-to-motion benchmarks demonstrate that it achieves state-of-the-art
performance, improving FID scores by 63.8% (from 0.380 to 0.141) on
long-sequence generation while enhancing motion diversity by 49.9% compared to
existing methods. The proposed framework successfully resolves the
diversity-quality trade-off that plagues current non-LLM approaches,
establishing new standards for text-to-motion generation.

</details>


### [144] [IDAL: Improved Domain Adaptive Learning for Natural Images Dataset](https://arxiv.org/abs/2506.17931)
*Ravi Kant Gupta,Shounak Das,Amit Sethi*

Main category: cs.CV

TL;DR: A novel unsupervised domain adaptation (UDA) method for natural images combines ResNet and FPN architectures with a tailored loss function to improve domain alignment, accuracy, and training speed.


<details>
  <summary>Details</summary>
Motivation: Existing adversarial UDA methods struggle with multimodal distributions and domain shifts in natural images.

Method: Uses ResNet and FPN for feature extraction and a novel loss function combination to handle scale, noise, and style shifts.

Result: Outperforms state-of-the-art CNN-based methods on Office-Home, Office-31, and VisDA-2017 datasets; comparable on DomainNet.

Conclusion: The proposed UDA scheme effectively addresses challenges in natural images, improving generalization and convergence.

Abstract: We present a novel approach for unsupervised domain adaptation (UDA) for
natural images. A commonly-used objective for UDA schemes is to enhance domain
alignment in representation space even if there is a domain shift in the input
space. Existing adversarial domain adaptation methods may not effectively align
different domains of multimodal distributions associated with classification
problems. Our approach has two main features. Firstly, its neural architecture
uses the deep structure of ResNet and the effective separation of scales of
feature pyramidal network (FPN) to work with both content and style features.
Secondly, it uses a combination of a novel loss function and judiciously
selected existing loss functions to train the network architecture. This
tailored combination is designed to address challenges inherent to natural
images, such as scale, noise, and style shifts, that occur on top of a
multi-modal (multi-class) distribution. The combined loss function not only
enhances model accuracy and robustness on the target domain but also speeds up
training convergence. Our proposed UDA scheme generalizes better than
state-of-the-art for CNN-based methods on Office-Home, Office-31, and
VisDA-2017 datasets and comaparable for DomainNet dataset.

</details>


### [145] [GEMeX-ThinkVG: Towards Thinking with Visual Grounding in Medical VQA via Reinforcement Learning](https://arxiv.org/abs/2506.17939)
*Bo Liu,Xiangyu Zhao,Along He,Yidi Chen,Huazhu Fu,Xiao-Ming Wu*

Main category: cs.CV

TL;DR: The paper introduces a dataset (ThinkVG) and a verifiable reward mechanism to improve reliability and interpretability in medical visual question answering.


<details>
  <summary>Details</summary>
Motivation: Current methods in medical visual question answering lack reliability and interpretability, hindering trust in model-generated answers.

Method: Proposes the ThinkVG dataset for fine-grained explainability and a verifiable reward mechanism for reinforcement learning.

Result: Achieves comparable performance with one-eighth of the training data, showing efficiency and effectiveness.

Conclusion: The approach enhances trust and understanding in model-generated answers for clinical decision-making.

Abstract: Medical visual question answering aims to support clinical decision-making by
enabling models to answer natural language questions based on medical images.
While recent advances in multi-modal learning have significantly improved
performance, current methods still suffer from limited answer reliability and
poor interpretability, impairing the ability of clinicians and patients to
understand and trust model-generated answers. To address this, this work first
proposes a Thinking with Visual Grounding (ThinkVG) dataset wherein the answer
generation is decomposed into intermediate reasoning steps that explicitly
ground relevant visual regions of the medical image, thereby providing
fine-grained explainability. Furthermore, we introduce a novel verifiable
reward mechanism for reinforcement learning to guide post-training, improving
the alignment between the model's reasoning process and its final answer.
Remarkably, our method achieves comparable performance using only one-eighth of
the training data, demonstrating the efficiency and effectiveness of the
proposal. The dataset is available at
https://huggingface.co/datasets/BoKelvin/GEMeX-ThinkVG.

</details>


### [146] [SegChange-R1:Augmented Reasoning for Remote Sensing Change Detection via Large Language Models](https://arxiv.org/abs/2506.17944)
*Fei Zhou*

Main category: cs.CV

TL;DR: The paper introduces SegChange-R1, an LLM-augmented inference approach for remote sensing change detection, integrating textual descriptions and a spatial transformation module (BEV) to improve accuracy and convergence. It also presents a new UAV-based dataset (DVCD) and shows superior performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Enhance remote sensing change detection by leveraging textual information and addressing modal misalignment, focusing on building changes.

Method: Proposes SegChange-R1 with LLM-augmented inference and a BEV spatial transformation module for feature alignment. Introduces DVCD dataset.

Result: Significant improvement over existing methods on four datasets.

Conclusion: SegChange-R1 effectively integrates textual and spatial features, advancing change detection with a new dataset and superior performance.

Abstract: Remote sensing change detection is widely used in a variety of fields such as
urban planning, terrain and geomorphology analysis, and environmental
monitoring, mainly by analyzing the significant change differences of features
(e.g., building changes) in the same spatial region at different time phases.
In this paper, we propose a large language model (LLM) augmented inference
approach (SegChange-R1), which enhances the detection capability by integrating
textual descriptive information and aims at guiding the model to segment the
more interested change regions, thus accelerating the convergence speed.
Moreover, we design a spatial transformation module (BEV) based on linear
attention, which solves the problem of modal misalignment in change detection
by unifying features from different temporal perspectives onto the BEV space.
In addition, we construct the first dataset for building change detection from
UAV viewpoints (DVCD ), and our experiments on four widely-used change
detection datasets show a significant improvement over existing methods. The
code and pre-trained models are available in
https://github.com/Yu-Zhouz/SegChange-R1.

</details>


### [147] [Classification of Tents in Street Bazaars Using CNN](https://arxiv.org/abs/2506.17946)
*Azamat Ibragimov,Ruslan Isaev,Remudin Reshid Mekuria,Gulnaz Gimaletdinova,Dim Shaiakhmetov*

Main category: cs.CV

TL;DR: The paper compares a custom CNN and EfficientNetB0 for classifying tents in street bazaars, with EfficientNetB0 outperforming (98.4% accuracy) due to transfer learning.


<details>
  <summary>Details</summary>
Motivation: Street bazaars are economically significant but lack automated classification methods for infrastructure like tents, impacting market organization.

Method: A custom CNN and EfficientNetB0 were trained on an augmented dataset of 126 original bazaar photos, evaluated using accuracy, precision, recall, F1, and mAP.

Result: EfficientNetB0 achieved 98.4% accuracy, outperforming the custom CNN (92.8%), highlighting transfer learning's effectiveness.

Conclusion: Pre-trained models like EfficientNetB0 enhance classification accuracy and generalization for bazaar-specific tasks.

Abstract: This research paper proposes an improved deep learning model for classifying
tents in street bazaars, comparing a custom Convolutional Neural Network (CNN)
with EfficientNetB0. This is a critical task for market organization with a
tent classification, but manual methods in the past have been inefficient.
Street bazaars represent a vital economic hub in many regions, yet their
unstructured nature poses significant challenges for the automated
classification of market infrastructure, such as tents. In Kyrgyzstan, more
than a quarter of the country's GDP is derived from bazaars. While CNNs have
been widely applied to object recognition, their application to bazaar-specific
tasks remains underexplored. Here, we build upon our original approach by
training on an extended set of 126 original photographs that were augmented to
generate additional images. This dataset is publicly available for download on
Kaggle. A variety of performance metrics, such as accuracy, precision, recall,
F1 score, and mean average precision (mAP), were used to assess the models
comparatively, providing a more extensive analysis of classification
performance.
  The results show that the CNN custom model achieved 92.8% accuracy, and
EfficientNetB0 showed 98.4% accuracy results, confirming the effectiveness of
transfer learning in the bazaar image classification. Also, when analyzing the
confusion matrix, the analysis reveals the weaknesses and strengths of each
model. These findings suggest that using a pre-trained model such as
EfficientNetB0 significantly improves classification accuracy and
generalization.

</details>


### [148] [Mobile Image Analysis Application for Mantoux Skin Test](https://arxiv.org/abs/2506.17954)
*Liong Gele,Tan Chye Cheah*

Main category: cs.CV

TL;DR: A mobile app using scaling stickers and advanced image processing (ARCore, DeepLabv3) improves LTBI diagnosis via TST, addressing traditional issues like low follow-up and subjective interpretation.


<details>
  <summary>Details</summary>
Motivation: Traditional TST methods have low follow-up rates, patient discomfort, and subjective interpretation, leading to misdiagnosis.

Method: The app uses scaling stickers, ARCore, and DeepLabv3 for image segmentation and edge detection to measure skin indurations.

Result: The app showed significant improvements in accuracy and reliability compared to standard clinical practices.

Conclusion: The app enhances TB diagnostics, especially in resource-limited regions, with future work focusing on refining ML models and ARCore performance.

Abstract: This paper presents a newly developed mobile application designed to diagnose
Latent Tuberculosis Infection (LTBI) using the Mantoux Skin Test (TST).
Traditional TST methods often suffer from low follow-up return rates, patient
discomfort, and subjective manual interpretation, particularly with the
ball-point pen method, leading to misdiagnosis and delayed treatment. Moreover,
previous developed mobile applications that used 3D reconstruction, this app
utilizes scaling stickers as reference objects for induration measurement. This
mobile application integrates advanced image processing technologies, including
ARCore, and machine learning algorithms such as DeepLabv3 for robust image
segmentation and precise measurement of skin indurations indicative of LTBI.
The system employs an edge detection algorithm to enhance accuracy. The
application was evaluated against standard clinical practices, demonstrating
significant improvements in accuracy and reliability. This innovation is
crucial for effective tuberculosis management, especially in resource-limited
regions. By automating and standardizing TST evaluations, the application
enhances the accessibility and efficiency of TB di-agnostics. Future work will
focus on refining machine learning models, optimizing measurement algorithms,
expanding functionalities to include comprehensive patient data management, and
enhancing ARCore's performance across various lighting conditions and
operational settings.

</details>


### [149] [ELMAR: Enhancing LiDAR Detection with 4D Radar Motion Awareness and Cross-modal Uncertainty](https://arxiv.org/abs/2506.17958)
*Xiangyuan Peng,Miao Tang,Huawei Sun,Bierzynski Kay,Lorenzo Servadei,Robert Wille*

Main category: cs.CV

TL;DR: A LiDAR detection framework enhanced by 4D radar motion status and cross-modal uncertainty addresses misalignment between LiDAR and 4D radar, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: The misalignment between LiDAR and 4D radar modalities is often overlooked, despite their complementary strengths in autonomous driving.

Method: Proposes a framework using 4D radar motion status (Dynamic Motion-Aware Encoding) and cross-modal uncertainty to refine LiDAR predictions.

Result: Achieves 74.89% mAP in the entire area and 88.70% within the driving corridor at 30.02 FPS on the VoD dataset.

Conclusion: The method effectively leverages 4D radar and LiDAR fusion, improving perception in autonomous driving.

Abstract: LiDAR and 4D radar are widely used in autonomous driving and robotics. While
LiDAR provides rich spatial information, 4D radar offers velocity measurement
and remains robust under adverse conditions. As a result, increasing studies
have focused on the 4D radar-LiDAR fusion method to enhance the perception.
However, the misalignment between different modalities is often overlooked. To
address this challenge and leverage the strengths of both modalities, we
propose a LiDAR detection framework enhanced by 4D radar motion status and
cross-modal uncertainty. The object movement information from 4D radar is first
captured using a Dynamic Motion-Aware Encoding module during feature extraction
to enhance 4D radar predictions. Subsequently, the instance-wise uncertainties
of bounding boxes are estimated to mitigate the cross-modal misalignment and
refine the final LiDAR predictions. Extensive experiments on the View-of-Delft
(VoD) dataset highlight the effectiveness of our method, achieving
state-of-the-art performance with the mAP of 74.89% in the entire area and
88.70% within the driving corridor while maintaining a real-time inference
speed of 30.02 FPS.

</details>


### [150] [BPCLIP: A Bottom-up Image Quality Assessment from Distortion to Semantics Based on CLIP](https://arxiv.org/abs/2506.17969)
*Chenyue Song,Chen Hui,Wei Zhang,Haiqi Zhu,Shaohui Liu,Hong Huang,Feng Jiang*

Main category: cs.CV

TL;DR: The paper proposes BPCLIP, a bottom-up IQA method using CLIP to link low-level distortions with high-level semantics, achieving superior performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing IQA methods linearly fuse multiscale features, inadequately capturing distortion impacts on semantics.

Method: BPCLIP uses a bottom-up multiscale cross-attention module and CLIP text encoder with quality adjectives to link distortions and semantics.

Result: Superior performance on FR and NR IQA benchmarks with greater robustness.

Conclusion: BPCLIP effectively bridges low-level distortions and high-level semantics, improving IQA performance.

Abstract: Image Quality Assessment (IQA) aims to evaluate the perceptual quality of
images based on human subjective perception. Existing methods generally combine
multiscale features to achieve high performance, but most rely on
straightforward linear fusion of these features, which may not adequately
capture the impact of distortions on semantic content. To address this, we
propose a bottom-up image quality assessment approach based on the Contrastive
Language-Image Pre-training (CLIP, a recently proposed model that aligns images
and text in a shared feature space), named BPCLIP, which progressively extracts
the impact of low-level distortions on high-level semantics. Specifically, we
utilize an encoder to extract multiscale features from the input image and
introduce a bottom-up multiscale cross attention module designed to capture the
relationships between shallow and deep features. In addition, by incorporating
40 image quality adjectives across six distinct dimensions, we enable the
pre-trained CLIP text encoder to generate representations of the intrinsic
quality of the image, thereby strengthening the connection between image
quality perception and human language. Our method achieves superior results on
most public Full-Reference (FR) and No-Reference (NR) IQA benchmarks, while
demonstrating greater robustness.

</details>


### [151] [Enabling PSO-Secure Synthetic Data Sharing Using Diversity-Aware Diffusion Models](https://arxiv.org/abs/2506.17975)
*Mischa Dombrowski,Bernhard Kainz*

Main category: cs.CV

TL;DR: The paper proposes a framework for generating synthetic medical imaging data that balances diversity and fidelity, ensuring privacy by preventing singling-out of individuals, while maintaining performance close to real data.


<details>
  <summary>Details</summary>
Motivation: Synthetic data is promising for privacy-preserving medical data sharing, but current methods neglect legal aspects like GDPR and underperform compared to real data.

Method: The authors introduce a framework for training diffusion models on personal data to produce unpersonal synthetic datasets, focusing on diversity to prevent singling-out.

Result: The synthetic datasets achieve performance within one percentage point of real-data models and outperform non-privacy-preserving state-of-the-art methods.

Conclusion: The work demonstrates that maximizing diversity in synthetic data can ensure privacy (PSO security) while maintaining high performance, offering a viable solution for medical imaging.

Abstract: Synthetic data has recently reached a level of visual fidelity that makes it
nearly indistinguishable from real data, offering great promise for
privacy-preserving data sharing in medical imaging. However, fully synthetic
datasets still suffer from significant limitations: First and foremost, the
legal aspect of sharing synthetic data is often neglected and data regulations,
such as the GDPR, are largley ignored. Secondly, synthetic models fall short of
matching the performance of real data, even for in-domain downstream
applications. Recent methods for image generation have focused on maximising
image diversity instead of fidelity solely to improve the mode coverage and
therefore the downstream performance of synthetic data. In this work, we shift
perspective and highlight how maximizing diversity can also be interpreted as
protecting natural persons from being singled out, which leads to predicate
singling-out (PSO) secure synthetic datasets. Specifically, we propose a
generalisable framework for training diffusion models on personal data which
leads to unpersonal synthetic datasets achieving performance within one
percentage point of real-data models while significantly outperforming
state-of-the-art methods that do not ensure privacy. Our code is available at
https://github.com/MischaD/Trichotomy.

</details>


### [152] [Fast Neural Inverse Kinematics on Human Body Motions](https://arxiv.org/abs/2506.17996)
*David Tolpin,Sefy Kagarlitsky*

Main category: cs.CV

TL;DR: A neural inverse kinematics framework for real-time markerless motion capture from 3D keypoints is presented, addressing computational demands and slow inference.


<details>
  <summary>Details</summary>
Motivation: Markerless motion capture offers flexibility and cost savings but suffers from high computational demands and slow inference, limiting real-time use.

Method: The paper details a neural inverse kinematics framework, including network architecture, training methodology, and inference procedure.

Result: The framework is evaluated qualitatively and quantitatively, with ablation studies supporting design decisions.

Conclusion: The proposed framework enables fast and reliable real-time human motion capture from 3D keypoints.

Abstract: Markerless motion capture enables the tracking of human motion without
requiring physical markers or suits, offering increased flexibility and reduced
costs compared to traditional systems. However, these advantages often come at
the expense of higher computational demands and slower inference, limiting
their applicability in real-time scenarios. In this technical report, we
present a fast and reliable neural inverse kinematics framework designed for
real-time capture of human body motions from 3D keypoints. We describe the
network architecture, training methodology, and inference procedure in detail.
Our framework is evaluated both qualitatively and quantitatively, and we
support key design decisions through ablation studies.

</details>


### [153] [OSDMamba: Enhancing Oil Spill Detection from Remote Sensing Images Using Selective State Space Model](https://arxiv.org/abs/2506.18006)
*Shuaiyu Chen,Fu Wang,Peng Ren,Chunbo Luo,Zeyu Fu*

Main category: cs.CV

TL;DR: OSDMamba, a Mamba-based model for oil spill detection, addresses class imbalance and small area detection challenges, outperforming CNNs with 8.9% and 11.8% improvements.


<details>
  <summary>Details</summary>
Motivation: Limited labelled data and class imbalance in oil spill detection, along with CNNs' inability to capture global context and small areas, drive the need for a better approach.

Method: Proposes OSDMamba, leveraging Mamba's selective scanning for wider receptive fields and an asymmetric decoder with ConvSSM and deep supervision for multi-scale feature fusion.

Result: OSDMamba achieves state-of-the-art performance, improving detection by 8.9% and 11.8% on two datasets.

Conclusion: OSDMamba demonstrates the potential of SSMs like Mamba for oil spill detection, overcoming CNN limitations and enhancing accuracy.

Abstract: Semantic segmentation is commonly used for Oil Spill Detection (OSD) in
remote sensing images. However, the limited availability of labelled oil spill
samples and class imbalance present significant challenges that can reduce
detection accuracy. Furthermore, most existing methods, which rely on
convolutional neural networks (CNNs), struggle to detect small oil spill areas
due to their limited receptive fields and inability to effectively capture
global contextual information. This study explores the potential of State-Space
Models (SSMs), particularly Mamba, to overcome these limitations, building on
their recent success in vision applications. We propose OSDMamba, the first
Mamba-based architecture specifically designed for oil spill detection.
OSDMamba leverages Mamba's selective scanning mechanism to effectively expand
the model's receptive field while preserving critical details. Moreover, we
designed an asymmetric decoder incorporating ConvSSM and deep supervision to
strengthen multi-scale feature fusion, thereby enhancing the model's
sensitivity to minority class samples. Experimental results show that the
proposed OSDMamba achieves state-of-the-art performance, yielding improvements
of 8.9% and 11.8% in OSD across two publicly available datasets.

</details>


### [154] [On the Robustness of Human-Object Interaction Detection against Distribution Shift](https://arxiv.org/abs/2506.18021)
*Chi Xie,Shuang Liang,Jie Li,Feng Zhu,Rui Zhao,Yichen Wei,Shengjie Zhao*

Main category: cs.CV

TL;DR: The paper benchmarks and enhances HOI detection robustness under distribution shifts, proposing a novel evaluation benchmark and two plug-and-play methods for improvement.


<details>
  <summary>Details</summary>
Motivation: Existing HOI detection models lack robustness in practical scenarios with distribution shifts, limiting their applicability.

Method: Proposes a robustness evaluation benchmark, evaluates 40+ models, and introduces cross-domain data augmentation with mixup and feature fusion with frozen vision models.

Result: The proposed methods significantly improve robustness across models, with benefits on standard benchmarks.

Conclusion: The work advances HOI detection robustness, offering practical solutions and releasing dataset/code for future research.

Abstract: Human-Object Interaction (HOI) detection has seen substantial advances in
recent years. However, existing works focus on the standard setting with ideal
images and natural distribution, far from practical scenarios with inevitable
distribution shifts. This hampers the practical applicability of HOI detection.
In this work, we investigate this issue by benchmarking, analyzing, and
enhancing the robustness of HOI detection models under various distribution
shifts. We start by proposing a novel automated approach to create the first
robustness evaluation benchmark for HOI detection. Subsequently, we evaluate
more than 40 existing HOI detection models on this benchmark, showing their
insufficiency, analyzing the features of different frameworks, and discussing
how the robustness in HOI is different from other tasks. With the insights from
such analyses, we propose to improve the robustness of HOI detection methods
through: (1) a cross-domain data augmentation integrated with mixup, and (2) a
feature fusion strategy with frozen vision foundation models. Both are simple,
plug-and-play, and applicable to various methods. Our experimental results
demonstrate that the proposed approach significantly increases the robustness
of various methods, with benefits on standard benchmarks, too. The dataset and
code will be released.

</details>


### [155] [PP-DocBee2: Improved Baselines with Efficient Data for Multimodal Document Understanding](https://arxiv.org/abs/2506.18023)
*Kui Huang,Xinrong Chen,Wenyu Lv,Jincheng Liao,Guanzhong Wang,Yi Liu*

Main category: cs.CV

TL;DR: PP-DocBee2 improves multimodal document understanding with better data quality, feature fusion, and inference methods, achieving 11.4% performance gain and 73% latency reduction.


<details>
  <summary>Details</summary>
Motivation: Address limitations of PP-DocBee by enhancing synthetic data quality, visual feature fusion, and inference efficiency for better multimodal document understanding.

Method: Uses a large-scale multimodal pre-trained model to filter outliers for high-quality data, and decomposes ViT layers with a novel feature fusion strategy.

Result: 11.4% performance boost on Chinese business documents and 73.0% reduction in inference latency compared to the vanilla version.

Conclusion: PP-DocBee2 significantly advances multimodal document understanding through data quality optimization and improved feature fusion, with open-source availability.

Abstract: This report introduces PP-DocBee2, an advanced version of the PP-DocBee,
designed to enhance multimodal document understanding. Built on a large
multimodal model architecture, PP-DocBee2 addresses the limitations of its
predecessor through key technological improvements, including enhanced
synthetic data quality, improved visual feature fusion strategy, and optimized
inference methodologies. These enhancements yield an $11.4\%$ performance boost
on internal benchmarks for Chinese business documents, and reduce inference
latency by $73.0\%$ to the vanilla version. A key innovation of our work is a
data quality optimization strategy for multimodal document tasks. By employing
a large-scale multimodal pre-trained model to evaluate data, we apply a novel
statistical criterion to filter outliers, ensuring high-quality training data.
Inspired by insights into underutilized intermediate features in multimodal
models, we enhance the ViT representational capacity by decomposing it into
layers and applying a novel feature fusion strategy to improve complex
reasoning. The source code and pre-trained model are available at
\href{https://github.com/PaddlePaddle/PaddleMIX}{https://github.com/PaddlePaddle/PaddleMIX}.

</details>


### [156] [MiCo: Multiple Instance Learning with Context-Aware Clustering for Whole Slide Image Analysis](https://arxiv.org/abs/2506.18028)
*Junjian Li,Hulin Kuang,Jin Liu,Hailin Yue,Mengshen He,Jianxin Wang*

Main category: cs.CV

TL;DR: MiCo is a novel MIL framework for histopathology WSI analysis, enhancing cross-regional intra-tissue correlations and inter-tissue semantic associations via context-aware clustering.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of spatial heterogeneity in WSIs, where morphologically similar tissues are dispersed, which conventional MIL methods struggle to model.

Method: MiCo clusters instances to create semantic anchors, uses a Cluster Route module for cross-regional linking, and a Cluster Reducer to consolidate redundant anchors and enhance inter-tissue associations.

Result: Superior performance on nine cancer datasets compared to state-of-the-art methods.

Conclusion: MiCo effectively addresses spatial heterogeneity in WSIs, improving cancer diagnosis and prognosis analysis.

Abstract: Multiple instance learning (MIL) has shown significant promise in
histopathology whole slide image (WSI) analysis for cancer diagnosis and
prognosis. However, the inherent spatial heterogeneity of WSIs presents
critical challenges, as morphologically similar tissue types are often
dispersed across distant anatomical regions. Conventional MIL methods struggle
to model these scattered tissue distributions and capture cross-regional
spatial interactions effectively. To address these limitations, we propose a
novel Multiple instance learning framework with Context-Aware Clustering
(MiCo), designed to enhance cross-regional intra-tissue correlations and
strengthen inter-tissue semantic associations in WSIs. MiCo begins by
clustering instances to distill discriminative morphological patterns, with
cluster centroids serving as semantic anchors. To enhance cross-regional
intra-tissue correlations, MiCo employs a Cluster Route module, which
dynamically links instances of the same tissue type across distant regions via
feature similarity. These semantic anchors act as contextual hubs, propagating
semantic relationships to refine instance-level representations. To eliminate
semantic fragmentation and strengthen inter-tissue semantic associations, MiCo
integrates a Cluster Reducer module, which consolidates redundant anchors while
enhancing information exchange between distinct semantic groups. Extensive
experiments on two challenging tasks across nine large-scale public cancer
datasets demonstrate the effectiveness of MiCo, showcasing its superiority over
state-of-the-art methods. The code is available at
https://github.com/junjianli106/MiCo.

</details>


### [157] [Pre-Trained LLM is a Semantic-Aware and Generalizable Segmentation Booster](https://arxiv.org/abs/2506.18034)
*Fenghe Tang,Wenxin Ma,Zhiyang He,Xiaodong Tao,Zihang Jiang,S. Kevin Zhou*

Main category: cs.CV

TL;DR: A frozen pre-trained LLM layer improves medical image segmentation when integrated into a CNN encoder-decoder framework (LLM4Seg), enhancing performance with minimal parameter increase.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of leveraging LLM's semantic awareness for medical image segmentation tasks.

Method: Proposes LLM4Seg, a hybrid structure combining a frozen pre-trained LLM layer with a CNN encoder-decoder framework.

Result: Improves segmentation performance across modalities (ultrasound, dermoscopy, polypscopy, CT) with minimal trainable parameters.

Conclusion: LLM's semantic awareness can enhance segmentation tasks, offering better global and local modeling, validated across different LLMs.

Abstract: With the advancement of Large Language Model (LLM) for natural language
processing, this paper presents an intriguing finding: a frozen pre-trained LLM
layer can process visual tokens for medical image segmentation tasks.
Specifically, we propose a simple hybrid structure that integrates a
pre-trained, frozen LLM layer within the CNN encoder-decoder segmentation
framework (LLM4Seg). Surprisingly, this design improves segmentation
performance with a minimal increase in trainable parameters across various
modalities, including ultrasound, dermoscopy, polypscopy, and CT scans. Our
in-depth analysis reveals the potential of transferring LLM's semantic
awareness to enhance segmentation tasks, offering both improved global
understanding and better local modeling capabilities. The improvement proves
robust across different LLMs, validated using LLaMA and DeepSeek.

</details>


### [158] [CmFNet: Cross-modal Fusion Network for Weakly-supervised Segmentation of Medical Images](https://arxiv.org/abs/2506.18042)
*Dongdong Meng,Sheng Li,Hao Wu,Suqing Tian,Wenjun Ma,Guoping Wang,Xueqing Yan*

Main category: cs.CV

TL;DR: CmFNet is a 3D weakly supervised cross-modal medical image segmentation method that improves performance by integrating multi-modal features and hybrid supervision, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: High-quality medical image annotations are costly and time-consuming; weakly supervised learning with sparse annotations is more efficient but faces performance degradation and overfitting.

Method: CmFNet combines modality-specific and cross-modal feature learning networks with a hybrid-supervised strategy (scribble supervision, intra-modal regularization, inter-modal consistency) to enhance segmentation.

Result: CmFNet outperforms state-of-the-art weakly supervised methods and even fully supervised methods on clinical NPC and WORD datasets, excelling in small tumor and anatomical structure segmentation.

Conclusion: CmFNet effectively addresses weakly supervised segmentation challenges, offering robust performance and clinical utility for specialists like radiologists and oncologists.

Abstract: Accurate automatic medical image segmentation relies on high-quality, dense
annotations, which are costly and time-consuming. Weakly supervised learning
provides a more efficient alternative by leveraging sparse and coarse
annotations instead of dense, precise ones. However, segmentation performance
degradation and overfitting caused by sparse annotations remain key challenges.
To address these issues, we propose CmFNet, a novel 3D weakly supervised
cross-modal medical image segmentation approach. CmFNet consists of three main
components: a modality-specific feature learning network, a cross-modal feature
learning network, and a hybrid-supervised learning strategy. Specifically, the
modality-specific feature learning network and the cross-modal feature learning
network effectively integrate complementary information from multi-modal
images, enhancing shared features across modalities to improve segmentation
performance. Additionally, the hybrid-supervised learning strategy guides
segmentation through scribble supervision, intra-modal regularization, and
inter-modal consistency, modeling spatial and contextual relationships while
promoting feature alignment. Our approach effectively mitigates overfitting,
delivering robust segmentation results. It excels in segmenting both
challenging small tumor regions and common anatomical structures. Extensive
experiments on a clinical cross-modal nasopharyngeal carcinoma (NPC) dataset
(including CT and MR imaging) and the publicly available CT Whole Abdominal
Organ dataset (WORD) show that our approach outperforms state-of-the-art weakly
supervised methods. In addition, our approach also outperforms fully supervised
methods when full annotation is used. Our approach can facilitate clinical
therapy and benefit various specialists, including physicists, radiologists,
pathologists, and oncologists.

</details>


### [159] [CLGRPO: Reasoning Ability Enhancement for Small VLMs](https://arxiv.org/abs/2506.18048)
*Fanyi Wang,Binzhi Dong,Haotian Hu,Jinjin Xu,Zhiwang Zhang*

Main category: cs.CV

TL;DR: The paper proposes an Incremental Training Strategy to enhance reasoning in Small Vision Language Models (SVLMs) using self-supervised COT data and multi-stage optimization, achieving performance comparable to larger models.


<details>
  <summary>Details</summary>
Motivation: SVLMs (2B parameters) are cost-effective but limited in reasoning. This work aims to improve their reasoning without increasing size.

Method: Four-stage strategy: SFT for domain knowledge, GRPO for format alignment, GRPO for reasoning enhancement, and CLGRPO to handle limited capacity. Uses self-supervised COT data from larger models.

Result: Significant improvement in reasoning: accuracy +2.77, recall +0.69 on EMOSet-118K, matching 8B model performance.

Conclusion: The Incremental Training Strategy effectively boosts SVLM reasoning, offering a scalable solution without parameter expansion.

Abstract: Small Vision Language Models (SVLMs) generally refer to models with parameter
sizes less than or equal to 2B. Their low cost and power consumption
characteristics confer high commercial value. However, their reasoning
abilities are limited by the number of parameters. To address this issue, this
paper proposes a post-training optimization paradigm called the Incremental
Training Strategy to enhance the reasoning ability of SVLMs. Firstly, we
constructed a Self-Supervised Chain-of-Thought (COT) Data Construction System,
which leverages multiple LVLMs with 7B parameters or more to transform original
data into COT data in a self-supervised manner. Our proposed Incremental
Training Strategy consists of four stages. Stage 1 injects domain knowledge by
performing Supervised Fine-Tuning (SFT) to the pretrained model on the COT
data. Stage 2 aligns the COT data format by conducting a small amount of Group
Relative Policy Optimization (GRPO) training constrained only by format rewards
on the COT data. Stage 3 enhances reasoning ability by applying GRPO training
on the COT data with constraints on both format and accuracy rewards. The
resulting model shows significant improvement compared to the baseline. Stage 4
addresses the limited capacity of the SVLMs and the weak ability to capture
complex patterns by proposing ClipLow GRPO (CLGRPO) to constrain the capture
space of the training process. We conducted extensive comparative and ablation
experiments on the abstract semantic recognition dataset EMOSet-118K.
Experimental results demonstrate that our method significantly improves the
reasoning ability of 1B SVLM. Compared to the baseline model fine-tuned on the
original data, accuracy increased by 2.77 and recall by 0.69, achieving
performance comparable to that of 8B models.

</details>


### [160] [Deep Supervised LSTM for 3D morphology estimation from Multi-View RGB Images of Wheat Spikes](https://arxiv.org/abs/2506.18060)
*Olivia Zumsteg,Nico Graf,Aaron Haeusler,Norbert Kirchgessner,Nicola Storni,Lukas Roth,Andreas Hund*

Main category: cs.CV

TL;DR: A neural network approach using DINOv2 and LSTM for estimating wheat spike volumes from 2D RGB images outperforms traditional methods, achieving lower error rates.


<details>
  <summary>Details</summary>
Motivation: Challenges in estimating 3D traits from 2D images due to depth loss, distortions, and occlusions in field conditions drive the need for robust solutions.

Method: Proposes a transfer learning pipeline combining DINOv2 (self-supervised Vision Transformer) with LSTM, using deep supervision for robust intermediate representations.

Result: Achieves MAPE of 6.46% on indoor images, outperforming area-based (9.36%) and geometric (13.98%) methods. Fine-tuning for field data yields 10.82% MAPE.

Conclusion: Deep learning outperforms traditional methods, especially for irregular geometries like wheat spikes, demonstrating adaptability to varying conditions.

Abstract: Estimating three-dimensional morphological traits from two-dimensional RGB
images presents inherent challenges due to the loss of depth information,
projection distortions, and occlusions under field conditions. In this work, we
explore multiple approaches for non-destructive volume estimation of wheat
spikes, using RGB image sequences and structured-light 3D scans as ground truth
references. Due to the complex geometry of the spikes, we propose a neural
network approach for volume estimation in 2D images, employing a transfer
learning pipeline that combines DINOv2, a self-supervised Vision Transformer,
with a unidirectional Long Short-Term Memory (LSTM) network. By using deep
supervision, the model is able to learn more robust intermediate
representations, which enhances its generalisation ability across varying
evaluation sequences. We benchmark our model against two conventional
baselines: a 2D area-based projection and a geometric reconstruction using
axis-aligned cross-sections. Our deep supervised model achieves a mean absolute
percentage error (MAPE) of 6.46% on six-view indoor images, outperforming the
area (9.36%) and geometric (13.98%) baselines. Fine-tuning the model on
field-based single-image data enables domain adaptation, yielding a MAPE of
10.82%. We demonstrate that object shape significantly impacts volume
prediction accuracy, with irregular geometries such as wheat spikes posing
greater challenges for geometric methods compared to our deep learning
approach.

</details>


### [161] [Training-free Test-time Improvement for Explainable Medical Image Classification](https://arxiv.org/abs/2506.18070)
*Hangzhou He,Jiachen Tang,Lei Zhu,Kaiwen Li,Yanye Lu*

Main category: cs.CV

TL;DR: The paper proposes a training-free method to improve out-of-domain performance of Concept Bottleneck Models (CBMs) in medical image classification by identifying and adjusting confounding and discriminative concepts using minimal labeled data.


<details>
  <summary>Details</summary>
Motivation: CBMs face challenges in deployment due to concept-level shifts and the high cost of expert-annotated concept labels. The goal is to enhance adaptability without compromising accuracy.

Method: A training-free strategy masks misactivated confounding concepts and amplifies under-activated discriminative concepts using minimal new data (e.g., 4 images per class) with image-level labels.

Result: The method improves out-of-domain performance on skin and white blood cell images without sacrificing source domain accuracy.

Conclusion: The proposed approach effectively addresses deployment challenges of CBMs in medical imaging, offering a practical solution with minimal data requirements.

Abstract: Deep learning-based medical image classification techniques are rapidly
advancing in medical image analysis, making it crucial to develop accurate and
trustworthy models that can be efficiently deployed across diverse clinical
scenarios. Concept Bottleneck Models (CBMs), which first predict a set of
explainable concepts from images and then perform classification based on these
concepts, are increasingly being adopted for explainable medical image
classification. However, the inherent explainability of CBMs introduces new
challenges when deploying trained models to new environments. Variations in
imaging protocols and staining methods may induce concept-level shifts, such as
alterations in color distribution and scale. Furthermore, since CBM training
requires explicit concept annotations, fine-tuning models solely with
image-level labels could compromise concept prediction accuracy and
faithfulness - a critical limitation given the high cost of acquiring
expert-annotated concept labels in medical domains. To address these
challenges, we propose a training-free confusion concept identification
strategy. By leveraging minimal new data (e.g., 4 images per class) with only
image-level labels, our approach enhances out-of-domain performance without
sacrificing source domain accuracy through two key operations: masking
misactivated confounding concepts and amplifying under-activated discriminative
concepts. The efficacy of our method is validated on both skin and white blood
cell images. Our code is available at:
https://github.com/riverback/TF-TTI-XMed.

</details>


### [162] [MUPA: Towards Multi-Path Agentic Reasoning for Grounded Video Question Answering](https://arxiv.org/abs/2506.18071)
*Jisheng Dang,Huilin Song,Junbin Xiao,Bimei Wang,Han Peng,Haoxuan Li,Xun Yang,Meng Wang,Tat-Seng Chua*

Main category: cs.CV

TL;DR: MUPA is a multi-path agentic approach for Grounded VideoQA, improving grounding fidelity and answer accuracy with a cooperative design of grounding, QA, and reflection agents.


<details>
  <summary>Details</summary>
Motivation: Modern multimodal models often rely on linguistic priors and spurious correlations, leading to poorly grounded predictions in Grounded VideoQA.

Method: MUPA unifies video grounding, question answering, answer reflection, and aggregation through three reasoning paths and a reflection agent.

Result: MUPA outperforms 7B-scale competitors with 2B parameters and achieves state-of-the-art results (Acc@GQA 30.3% and 47.4%) when scaled to 7B.

Conclusion: MUPA demonstrates effectiveness in trustworthy video-language understanding, balancing grounding fidelity and answer accuracy.

Abstract: Grounded Video Question Answering (Grounded VideoQA) requires aligning
textual answers with explicit visual evidence. However, modern multimodal
models often rely on linguistic priors and spurious correlations, resulting in
poorly grounded predictions. In this work, we propose MUPA, a cooperative
MUlti-Path Agentic approach that unifies video grounding, question answering,
answer reflection and aggregation to tackle Grounded VideoQA. MUPA features
three distinct reasoning paths on the interplay of grounding and QA agents in
different chronological orders, along with a dedicated reflection agent to
judge and aggregate the multi-path results to accomplish consistent QA and
grounding. This design markedly improves grounding fidelity without sacrificing
answer accuracy. Despite using only 2B parameters, our method outperforms all
7B-scale competitors. When scaled to 7B parameters, MUPA establishes new
state-of-the-art results, with Acc@GQA of 30.3% and 47.4% on NExT-GQA and
DeVE-QA respectively, demonstrating MUPA' effectiveness towards trustworthy
video-language understanding. Our code is available in
https://github.com/longmalongma/MUPA.

</details>


### [163] [TEM^3-Learning: Time-Efficient Multimodal Multi-Task Learning for Advanced Assistive Driving](https://arxiv.org/abs/2506.18084)
*Wenzhuo Liu,Yicheng Qiao,Zhen Wang,Qiannan Guo,Zilong Chen,Meihua Zhou,Xinran Li,Letian Wang,Zhiwei Li,Huaping Liu,Wenshuo Wang*

Main category: cs.CV

TL;DR: TEM^3-Learning is a multimodal MTL framework for assistive driving, addressing single-modality and inefficiency issues with a two-stage architecture, achieving SOTA accuracy and high speed.


<details>
  <summary>Details</summary>
Motivation: Existing MTL methods for assistive driving are limited by single-modality constraints and inefficient architectures, hindering comprehensive scene understanding and real-time deployment.

Method: Proposes TEM^3-Learning: (1) MTS-Mamba for efficient temporal-spatial feature extraction, and (2) MGMI for adaptive multimodal feature integration.

Result: Achieves SOTA accuracy on four tasks, with <6M parameters and 142.32 FPS speed on the AIDE dataset.

Conclusion: TEM^3-Learning effectively addresses MTL limitations, offering a lightweight, efficient, and accurate solution for assistive driving.

Abstract: Multi-task learning (MTL) can advance assistive driving by exploring
inter-task correlations through shared representations. However, existing
methods face two critical limitations: single-modality constraints limiting
comprehensive scene understanding and inefficient architectures impeding
real-time deployment. This paper proposes TEM^3-Learning (Time-Efficient
Multimodal Multi-task Learning), a novel framework that jointly optimizes
driver emotion recognition, driver behavior recognition, traffic context
recognition, and vehicle behavior recognition through a two-stage architecture.
The first component, the mamba-based multi-view temporal-spatial feature
extraction subnetwork (MTS-Mamba), introduces a forward-backward temporal
scanning mechanism and global-local spatial attention to efficiently extract
low-cost temporal-spatial features from multi-view sequential images. The
second component, the MTL-based gated multimodal feature integrator (MGMI),
employs task-specific multi-gating modules to adaptively highlight the most
relevant modality features for each task, effectively alleviating the negative
transfer problem in MTL. Evaluation on the AIDE dataset, our proposed model
achieves state-of-the-art accuracy across all four tasks, maintaining a
lightweight architecture with fewer than 6 million parameters and delivering an
impressive 142.32 FPS inference speed. Rigorous ablation studies further
validate the effectiveness of the proposed framework and the independent
contributions of each module. The code is available on
https://github.com/Wenzhuo-Liu/TEM3-Learning.

</details>


### [164] [ShareGPT-4o-Image: Aligning Multimodal Models with GPT-4o-Level Image Generation](https://arxiv.org/abs/2506.18095)
*Junying Chen,Zhenyang Cai,Pengcheng Chen,Shunian Chen,Ke Ji,Xidong Wang,Yunjin Yang,Benyou Wang*

Main category: cs.CV

TL;DR: The paper introduces ShareGPT-4o-Image, a dataset for democratizing photorealistic image generation, and Janus-4o, a model improving text-to-image and text-and-image-to-image generation.


<details>
  <summary>Details</summary>
Motivation: To make advanced image generation capabilities accessible, as current systems like GPT-4o-Image are proprietary.

Method: Developed ShareGPT-4o-Image dataset (45K text-to-image and 46K text-and-image-to-image samples) and trained Janus-4o model using this data.

Result: Janus-4o outperforms its predecessor in text-to-image generation and achieves strong performance in text-and-image-to-image generation with minimal training.

Conclusion: The release aims to advance open research in photorealistic, instruction-aligned image generation.

Abstract: Recent advances in multimodal generative models have unlocked photorealistic,
instruction-aligned image generation, yet leading systems like GPT-4o-Image
remain proprietary and inaccessible. To democratize these capabilities, we
present ShareGPT-4o-Image, the first dataset comprising 45K text-to-image and
46K text-and-image-to-image data, all synthesized using GPT-4o's image
generation capabilities for distilling its advanced image generation abilities.
Leveraging this dataset, we develop Janus-4o, a multimodal large language model
capable of both text-to-image and text-and-image-to-image generation. Janus-4o
not only significantly improves text-to-image generation over its predecessor,
Janus-Pro, but also newly supports text-and-image-to-image generation. Notably,
it achieves impressive performance in text-and-image-to-image generation from
scratch, using only 91K synthetic samples and 6 hours of training on an 8
A800-GPU machine. We hope the release of ShareGPT-4o-Image and Janus-4o will
foster open research in photorealistic, instruction-aligned image generation.

</details>


### [165] [Enhancing VICReg: Random-Walk Pairing for Improved Generalization and Better Global Semantics Capturing](https://arxiv.org/abs/2506.18104)
*Idan Simai,Ronen Talmon,Uri Shaham*

Main category: cs.CV

TL;DR: SAG-VICReg improves VICReg by enhancing generalization and global semantics, outperforming SSL baselines and introducing a new evaluation metric.


<details>
  <summary>Details</summary>
Motivation: VICReg may struggle with generalization due to overreliance on training data, prompting the need for a more robust method.

Method: Introduces SAG-VICReg, which incorporates new training techniques to improve global semantics and generalization.

Result: SAG-VICReg outperforms SSL baselines, excels in global semantic understanding, and maintains local performance.

Conclusion: SAG-VICReg effectively addresses generalization challenges and introduces a label-free evaluation metric for embeddings.

Abstract: In this paper, we argue that viewing VICReg-a popular self-supervised
learning (SSL) method--through the lens of spectral embedding reveals a
potential source of sub-optimality: it may struggle to generalize robustly to
unseen data due to overreliance on the training data. This observation invites
a closer look at how well this method achieves its goal of producing meaningful
representations of images outside of the training set as well. Here, we
investigate this issue and introduce SAG-VICReg (Stable and Generalizable
VICReg), a method that builds on VICReg by incorporating new training
techniques. These enhancements improve the model's ability to capture global
semantics within the data and strengthen the generalization capabilities.
Experiments demonstrate that SAG-VICReg effectively addresses the
generalization challenge while matching or surpassing diverse state-of-the-art
SSL baselines. Notably, our method exhibits superior performance on metrics
designed to evaluate global semantic understanding, while simultaneously
maintaining competitive results on local evaluation metrics. Furthermore, we
propose a new standalone evaluation metric for embeddings that complements the
standard evaluation methods and accounts for the global data structure without
requiring labels--a key issue when tagged data is scarce or not available.

</details>


### [166] [Targeted False Positive Synthesis via Detector-guided Adversarial Diffusion Attacker for Robust Polyp Detection](https://arxiv.org/abs/2506.18134)
*Quan Zhou,Gan Luo,Qiang Hu,Qingyong Zhang,Jinhua Zhang,Yinjiao Tian,Qiang Li,Zhiwei Wang*

Main category: cs.CV

TL;DR: The paper proposes an adversarial diffusion framework to synthesize high-value false positives for polyp detection, addressing limitations in existing models by focusing on diverse background patterns and detector-confusing outputs.


<details>
  <summary>Details</summary>
Motivation: Existing polyp detection models lack diversity in data and struggle with false positives. Generative models for augmentation often overlook this issue.

Method: Introduces a regional noise matching strategy to create a negative synthesis space and a Detector-guided Adversarial Diffusion Attacker (DADA) module to generate high-value false positives.

Result: The method outperforms state-of-the-art approaches, improving detector performance by at least 2.6% and 2.7% in F1-score on public and in-house datasets.

Conclusion: The adversarial diffusion framework sets a new standard for targeted false positive synthesis, enhancing reliability in colorectal cancer screening.

Abstract: Polyp detection is crucial for colorectal cancer screening, yet existing
models are limited by the scale and diversity of available data. While
generative models show promise for data augmentation, current methods mainly
focus on enhancing polyp diversity, often overlooking the critical issue of
false positives. In this paper, we address this gap by proposing an adversarial
diffusion framework to synthesize high-value false positives. The extensive
variability of negative backgrounds presents a significant challenge in false
positive synthesis. To overcome this, we introduce two key innovations: First,
we design a regional noise matching strategy to construct a negative synthesis
space using polyp detection datasets. This strategy trains a negative-centric
diffusion model by masking polyp regions, ensuring the model focuses
exclusively on learning diverse background patterns. Second, we introduce the
Detector-guided Adversarial Diffusion Attacker (DADA) module, which perturbs
the negative synthesis process to disrupt a pre-trained detector's decision,
guiding the negative-centric diffusion model to generate high-value,
detector-confusing false positives instead of low-value, ordinary backgrounds.
Our approach is the first to apply adversarial diffusion to lesion detection,
establishing a new paradigm for targeted false positive synthesis and paving
the way for more reliable clinical applications in colorectal cancer screening.
Extensive results on public and in-house datasets verify the superiority of our
method over the current state-of-the-arts, with our synthesized data improving
the detectors by at least 2.6% and 2.7% in F1-score, respectively, over the
baselines. Codes are at https://github.com/Huster-Hq/DADA.

</details>


### [167] [See-in-Pairs: Reference Image-Guided Comparative Vision-Language Models for Medical Diagnosis](https://arxiv.org/abs/2506.18140)
*Ruinan Jin,Gexin Huang,Xinwei Shen,Qiong Zhang,Yan Shuo Tan,Xiaoxiao Li*

Main category: cs.CV

TL;DR: The paper explores enhancing medical vision-language models (VLMs) by integrating comparative reasoning using reference images, improving diagnostic accuracy over single-image methods.


<details>
  <summary>Details</summary>
Motivation: Existing VLMs lack comparative reasoning for nuanced medical diagnoses, while general-purpose VLMs lack medical-domain knowledge. This work bridges the gap.

Method: Leverages reference images and clinically-informed prompts in VLMs, with supervised finetuning (SFT) to enhance performance.

Result: Comparative analysis with reference images significantly improves diagnostic outcomes in medical VQA tasks.

Conclusion: Comparative reasoning in VLMs is clinically relevant and enhances diagnostic accuracy, supported by empirical evidence.

Abstract: Medical imaging diagnosis presents inherent challenges due to diseases that
mimic normal anatomy and exhibit significant inter-patient variability.
Clinicians routinely employ comparative reasoning-using reference images from
healthy controls or previous patient examinations-to discern subtle yet
diagnostically critical abnormalities. However, existing medical
vision-language models (VLMs) focus primarily on single-image or single-series
analyses and lack explicit mechanisms for comparative reasoning. Conversely,
general-purpose VLMs demonstrate strong multi-image comparative reasoning
capabilities but lack essential medical-domain knowledge to identify nuanced
clinical differences. This work aims to bridge this gap by exploring
clinically-inspired comparative analysis within VLMs, leveraging reference
images to enhance diagnostic accuracy. Through extensive empirical analysis, we
show that providing general-purpose VLMs with query and normative matched
reference images, accompanied by clinically-informed comparative prompts,
significantly improves diagnostic outcomes compared to single-image baselines,
especially after supervised finetuning (SFT). Our contributions highlight the
clinical relevance of comparative analysis introduce novel strategies for
leveraging reference images in VLMs, empirically demonstrate enhanced
performance across multiple medical visual question answering (VQA) tasks, and
provide theoretical insights into the efficacy of comparative image analysis in
medical diagnosis.

</details>


### [168] [Pattern-Based Phase-Separation of Tracer and Dispersed Phase Particles in Two-Phase Defocusing Particle Tracking Velocimetry](https://arxiv.org/abs/2506.18157)
*Christian Sax,Jochen Kriegseis*

Main category: cs.CV

TL;DR: A post-processing method using CNNs (Faster R-CNN and YOLOv4) for phase separation in defocusing particle tracking velocimetry, achieving high accuracy (95-100%) in detecting and classifying particles and bubbles/droplets.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of phase separation in dispersed two-phase flows, especially where traditional methods are impractical.

Method: Uses CNNs trained on auto-labeled datasets generated by a GAN framework to detect and classify particles based on defocused image patterns.

Result: High precision and accuracy (95-100%) in detection and classification across synthetic and real datasets, even with domain shifts.

Conclusion: CNNs are viable for robust phase separation in DPTV, outperforming traditional methods in certain scenarios.

Abstract: This work investigates the feasibility of a post-processing-based approach
for phase separation in defocusing particle tracking velocimetry for dispersed
two-phase flows. The method enables the simultaneous 3D localization
determination of both tracer particles and particles of the dispersed phase,
using a single-camera setup. The distinction between phases is based on pattern
differences in defocused particle images, which arise from distinct light
scattering behaviors of tracer particles and bubbles or droplets. Convolutional
neural networks, including Faster R-CNN and YOLOv4 variants, are trained to
detect and classify particle images based on these pattern features. To
generate large, labeled training datasets, a generative adversarial network
based framework is introduced, allowing the generation of auto-labeled data
that more closely reflects experiment-specific visual appearance. Evaluation
across six datasets, comprising synthetic two-phase and real single- and
two-phase flows, demonstrates high detection precision and classification
accuracy (95-100%), even under domain shifts. The results confirm the viability
of using CNNs for robust phase separation in disperse two-phase DPTV,
particularly in scenarios where traditional wavelength-, size-, or ensemble
correlation-based methods are impractical.

</details>


### [169] [CDG-MAE: Learning Correspondences from Diffusion Generated Views](https://arxiv.org/abs/2506.18164)
*Varun Belagali,Pierre Marza,Srikar Yellapragada,Zilinghan Li,Tarak Nath Nandi,Ravi K Madduri,Joel Saltz,Stergios Christodoulidis,Maria Vakalopoulou,Dimitris Samaras*

Main category: cs.CV

TL;DR: CDG-MAE introduces a self-supervised method using synthetic views from static images via diffusion models to improve dense correspondence learning, outperforming image-based MAE methods.


<details>
  <summary>Details</summary>
Motivation: Manual annotation for dense correspondences is tedious and unscalable, while existing self-supervised methods struggle with limited training data diversity.

Method: Uses synthetic views from static images via an image-conditioned diffusion model and enhances MAE with a multi-anchor strategy.

Result: CDG-MAE outperforms state-of-the-art image-based MAE methods and narrows the gap to video-based approaches.

Conclusion: Synthetic views and multi-anchor MAE effectively address data diversity challenges, advancing self-supervised dense correspondence learning.

Abstract: Learning dense correspondences, critical for application such as video label
propagation, is hindered by tedious and unscalable manual annotation.
Self-supervised methods address this by using a cross-view pretext task, often
modeled with a masked autoencoder, where a masked target view is reconstructed
from an anchor view. However, acquiring effective training data remains a
challenge - collecting diverse video datasets is difficult and costly, while
simple image crops lack necessary pose variations. This paper introduces
CDG-MAE, a novel MAE-based self-supervised method that uses diverse synthetic
views generated from static images via an image-conditioned diffusion model.
These generated views exhibit substantial changes in pose and perspective,
providing a rich training signal that overcomes the limitations of video and
crop-based anchors. We present a quantitative method to evaluate local and
global consistency of generated images, discussing their use for cross-view
self-supervised pretraining. Furthermore, we enhance the standard single-anchor
MAE setting to a multi-anchor strategy to effectively modulate the difficulty
of pretext task. CDG-MAE significantly outperforms state-of-the-art MAE methods
reliant only on images and substantially narrows the performance gap to
video-based approaches.

</details>


### [170] [STACT-Time: Spatio-Temporal Cross Attention for Cine Thyroid Ultrasound Time Series Classification](https://arxiv.org/abs/2506.18172)
*Irsyad Adam,Tengyue Zhang,Shrayes Raman,Zhuyu Qiu,Brandon Taraku,Hexiang Feng,Sile Wang,Ashwath Radhachandran,Shreeram Athreya,Vedrana Ivezic,Peipei Ping,Corey Arnold,William Speier*

Main category: cs.CV

TL;DR: The paper introduces STACT-Time, a deep learning model for thyroid cancer risk stratification using US cine clips, reducing unnecessary biopsies while maintaining high accuracy.


<details>
  <summary>Details</summary>
Motivation: Current methods like FNA biopsies and TI-RADS have limitations, including unnecessary benign biopsies and interobserver variability. Deep learning models often miss dynamic context in US cine clips.

Method: Proposes STACT-Time, a framework combining self-attention and cross-attention to leverage temporal and spatial context from US cine clips and segmentation masks.

Result: Achieves precision of 0.91 (0.02) and F1 score of 0.89 (0.02), outperforming state-of-the-art models.

Conclusion: STACT-Time improves malignancy prediction, reduces unnecessary biopsies, and enhances clinical decision-making.

Abstract: Thyroid cancer is among the most common cancers in the United States. Thyroid
nodules are frequently detected through ultrasound (US) imaging, and some
require further evaluation via fine-needle aspiration (FNA) biopsy. Despite its
effectiveness, FNA often leads to unnecessary biopsies of benign nodules,
causing patient discomfort and anxiety. To address this, the American College
of Radiology Thyroid Imaging Reporting and Data System (TI-RADS) has been
developed to reduce benign biopsies. However, such systems are limited by
interobserver variability. Recent deep learning approaches have sought to
improve risk stratification, but they often fail to utilize the rich temporal
and spatial context provided by US cine clips, which contain dynamic global
information and surrounding structural changes across various views. In this
work, we propose the Spatio-Temporal Cross Attention for Cine Thyroid
Ultrasound Time Series Classification (STACT-Time) model, a novel
representation learning framework that integrates imaging features from US cine
clips with features from segmentation masks automatically generated by a
pretrained model. By leveraging self-attention and cross-attention mechanisms,
our model captures the rich temporal and spatial context of US cine clips while
enhancing feature representation through segmentation-guided learning. Our
model improves malignancy prediction compared to state-of-the-art models,
achieving a cross-validation precision of 0.91 (plus or minus 0.02) and an F1
score of 0.89 (plus or minus 0.02). By reducing unnecessary biopsies of benign
nodules while maintaining high sensitivity for malignancy detection, our model
has the potential to enhance clinical decision-making and improve patient
outcomes.

</details>


### [171] [DExNet: Combining Observations of Domain Adapted Critics for Leaf Disease Classification with Limited Data](https://arxiv.org/abs/2506.18173)
*Sabbir Ahmed,Md. Bakhtiar Hasan,Tasnim Ahmed,Md. Hasanul Kabir*

Main category: cs.CV

TL;DR: The paper proposes DExNet, a few-shot learning framework for plant disease classification, addressing data scarcity by leveraging pre-trained CNNs and achieving high accuracy with limited samples.


<details>
  <summary>Details</summary>
Motivation: Deep learning models require large datasets for plant disease classification, posing challenges with limited samples. The work aims to overcome this by developing a few-shot learning approach.

Method: DExNet uses domain-adapted pre-trained CNNs to extract features, fuses them via a Feature Fusion Block, and classifies using Bi-LSTM layers. Evaluated on tomato leaf images from PlantVillage.

Result: Achieved accuracies of 89.06% (5-shot), 92.46% (10-shot), 94.07% (15-shot), and 98.09% (80-shot), reducing training data needs by 94.5%. Outperforms existing methods in limited-data scenarios.

Conclusion: DExNet effectively addresses data scarcity in plant disease classification, achieving near state-of-the-art performance with significantly fewer samples.

Abstract: While deep learning-based architectures have been widely used for correctly
detecting and classifying plant diseases, they require large-scale datasets to
learn generalized features and achieve state-of-the-art performance. This poses
a challenge for such models to obtain satisfactory performance in classifying
leaf diseases with limited samples. This work proposes a few-shot learning
framework, Domain-adapted Expert Network (DExNet), for plant disease
classification that compensates for the lack of sufficient training data by
combining observations of a number of expert critics. It starts with extracting
the feature embeddings as 'observations' from nine 'critics' that are
state-of-the-art pre-trained CNN-based architectures. These critics are 'domain
adapted' using a publicly available leaf disease dataset having no overlapping
classes with the specific downstream task of interest. The observations are
then passed to the 'Feature Fusion Block' and finally to a classifier network
consisting of Bi-LSTM layers. The proposed pipeline is evaluated on the 10
classes of tomato leaf images from the PlantVillage dataset, achieving
promising accuracies of 89.06%, 92.46%, and 94.07%, respectively, for 5-shot,
10-shot, and 15-shot classification. Furthermore, an accuracy of 98.09+-0.7%
has been achieved in 80-shot classification, which is only 1.2% less than
state-of-the-art, allowing a 94.5% reduction in the training data requirement.
The proposed pipeline also outperforms existing works on leaf disease
classification with limited data in both laboratory and real-life conditions in
single-domain, mixed-domain, and cross-domain scenarios.

</details>


### [172] [Multimodal Fusion SLAM with Fourier Attention](https://arxiv.org/abs/2506.18204)
*Youjie Zhou,Guofeng Mei,Yiming Wang,Yi Wan,Fabio Poiesi*

Main category: cs.CV

TL;DR: FMF-SLAM is an efficient multimodal fusion SLAM method using FFT and novel attention mechanisms to improve performance in noisy, varying lighting, and dark conditions.


<details>
  <summary>Details</summary>
Motivation: Challenges in visual SLAM due to noise, lighting variations, and darkness, along with high computational demands of traditional methods.

Method: Uses FFT, Fourier-based self-attention, cross-attention for RGB/depth feature extraction, and multi-scale knowledge distillation. Integrated with GNSS-RTK and global Bundle Adjustment for real-time feasibility.

Result: Validated on TUM, TartanAir, and real-world datasets, achieving state-of-the-art performance in challenging conditions.

Conclusion: FMF-SLAM is efficient, practical, and outperforms existing methods in noisy, varying lighting, and dark environments.

Abstract: Visual SLAM is particularly challenging in environments affected by noise,
varying lighting conditions, and darkness. Learning-based optical flow
algorithms can leverage multiple modalities to address these challenges, but
traditional optical flow-based visual SLAM approaches often require significant
computational resources.To overcome this limitation, we propose FMF-SLAM, an
efficient multimodal fusion SLAM method that utilizes fast Fourier transform
(FFT) to enhance the algorithm efficiency. Specifically, we introduce a novel
Fourier-based self-attention and cross-attention mechanism to extract features
from RGB and depth signals. We further enhance the interaction of multimodal
features by incorporating multi-scale knowledge distillation across modalities.
We also demonstrate the practical feasibility of FMF-SLAM in real-world
scenarios with real time performance by integrating it with a security robot by
fusing with a global positioning module GNSS-RTK and global Bundle Adjustment.
Our approach is validated using video sequences from TUM, TartanAir, and our
real-world datasets, showcasing state-of-the-art performance under noisy,
varying lighting, and dark conditions.Our code and datasets are available at
https://github.com/youjie-zhou/FMF-SLAM.git.

</details>


### [173] [Limitations of NERF with pre-trained Vision Features for Few-Shot 3D Reconstruction](https://arxiv.org/abs/2506.18208)
*Ankit Sanjyal*

Main category: cs.CV

TL;DR: DINO-enhanced NeRF models underperform baseline NeRF in few-shot 3D reconstruction, suggesting pre-trained vision features may introduce biases.


<details>
  <summary>Details</summary>
Motivation: To evaluate the effectiveness of integrating pre-trained DINO features into NeRF for few-shot 3D scene reconstruction.

Method: Systematic comparison of baseline NeRF, frozen DINO features, LoRA fine-tuned features, and multi-scale feature fusion.

Result: All DINO variants performed worse (PSNR 12.9-13.0) than baseline NeRF (PSNR 14.71).

Conclusion: Pre-trained vision features may not aid few-shot reconstruction; simpler geometric-focused architectures could be better.

Abstract: Neural Radiance Fields (NeRF) have revolutionized 3D scene reconstruction
from sparse image collections. Recent work has explored integrating pre-trained
vision features, particularly from DINO, to enhance few-shot reconstruction
capabilities. However, the effectiveness of such approaches remains unclear,
especially in extreme few-shot scenarios. In this paper, we present a
systematic evaluation of DINO-enhanced NeRF models, comparing baseline NeRF,
frozen DINO features, LoRA fine-tuned features, and multi-scale feature fusion.
Surprisingly, our experiments reveal that all DINO variants perform worse than
the baseline NeRF, achieving PSNR values around 12.9 to 13.0 compared to the
baseline's 14.71. This counterintuitive result suggests that pre-trained vision
features may not be beneficial for few-shot 3D reconstruction and may even
introduce harmful biases. We analyze potential causes including feature-task
mismatch, overfitting to limited data, and integration challenges. Our findings
challenge common assumptions in the field and suggest that simpler
architectures focusing on geometric consistency may be more effective for
few-shot scenarios.

</details>


### [174] [Deep Learning-based Alignment Measurement in Knee Radiographs](https://arxiv.org/abs/2506.18209)
*Zhisen Hu,Dominic Cullen,Peter Thompson,David Johnson,Chang Bian,Aleksei Tiulpin,Timothy Cootes,Claudia Lindner*

Main category: cs.CV

TL;DR: A deep learning method using hourglass networks and attention gates automates knee alignment measurements in radiographs, achieving high accuracy (~1 error) and reliability (ICC up to 0.97).


<details>
  <summary>Details</summary>
Motivation: Traditional manual KA measurements are time-consuming and require long-leg radiographs, prompting the need for an automated, efficient solution.

Method: The study employs hourglass networks with attention gates to localize over 100 knee landmarks and measure KA via the tibiofemoral angle in pre- and post-operative images.

Result: The method achieved ~1 mean absolute difference from clinical measurements, with excellent pre-operative (ICC=0.97) and good post-operative (ICC=0.86) agreement.

Conclusion: Automated KA assessment is highly accurate, enabling digitally enhanced clinical workflows.

Abstract: Radiographic knee alignment (KA) measurement is important for predicting
joint health and surgical outcomes after total knee replacement. Traditional
methods for KA measurements are manual, time-consuming and require long-leg
radiographs. This study proposes a deep learning-based method to measure KA in
anteroposterior knee radiographs via automatically localized knee anatomical
landmarks. Our method builds on hourglass networks and incorporates an
attention gate structure to enhance robustness and focus on key anatomical
features. To our knowledge, this is the first deep learning-based method to
localize over 100 knee anatomical landmarks to fully outline the knee shape
while integrating KA measurements on both pre-operative and post-operative
images. It provides highly accurate and reliable anatomical varus/valgus KA
measurements using the anatomical tibiofemoral angle, achieving mean absolute
differences ~1{\deg} when compared to clinical ground truth measurements.
Agreement between automated and clinical measurements was excellent
pre-operatively (intra-class correlation coefficient (ICC) = 0.97) and good
post-operatively (ICC = 0.86). Our findings demonstrate that KA assessment can
be automated with high accuracy, creating opportunities for digitally enhanced
clinical workflows.

</details>


### [175] [Shape from Polarization of Thermal Emission and Reflection](https://arxiv.org/abs/2506.18217)
*Kazuma Kitazawa,Tsuyoshi Takatani*

Main category: cs.CV

TL;DR: The paper presents a method for accurate shape estimation of transparent objects using Long-Wave Infrared (LWIR) polarization, addressing prior errors by modeling emission and reflection.


<details>
  <summary>Details</summary>
Motivation: Existing methods for shape estimation of transparent objects are flawed due to inadequate polarimetric modeling, especially neglecting reflection.

Method: The authors developed a polarization model accounting for emission and reflection, used model-based and learning-based approaches for surface normal estimation, and created a synthetic dataset (ThermoPol) for training.

Result: The method demonstrated high accuracy and broad applicability across various materials, including those transparent in visible light.

Conclusion: The proposed LWIR SfP technique, with improved modeling and a benchmark dataset, offers a robust solution for shape estimation of transparent objects.

Abstract: Shape estimation for transparent objects is challenging due to their complex
light transport. To circumvent these difficulties, we leverage the Shape from
Polarization (SfP) technique in the Long-Wave Infrared (LWIR) spectrum, where
most materials are opaque and emissive. While a few prior studies have explored
LWIR SfP, these attempts suffered from significant errors due to inadequate
polarimetric modeling, particularly the neglect of reflection. Addressing this
gap, we formulated a polarization model that explicitly accounts for the
combined effects of emission and reflection. Based on this model, we estimated
surface normals using not only a direct model-based method but also a
learning-based approach employing a neural network trained on a
physically-grounded synthetic dataset. Furthermore, we modeled the LWIR
polarimetric imaging process, accounting for inherent systematic errors to
ensure accurate polarimetry. We implemented a prototype system and created
ThermoPol, the first real-world benchmark dataset for LWIR SfP. Through
comprehensive experiments, we demonstrated the high accuracy and broad
applicability of our method across various materials, including those
transparent in the visible spectrum.

</details>


### [176] [Cross-Architecture Knowledge Distillation (KD) for Retinal Fundus Image Anomaly Detection on NVIDIA Jetson Nano](https://arxiv.org/abs/2506.18220)
*Berk Yilmaz,Aniruddh Aiyengar*

Main category: cs.CV

TL;DR: A lightweight, edge-device deployable retinal disease classifier is developed using cross-architecture knowledge distilling, achieving high accuracy retention from a teacher model to a student model.


<details>
  <summary>Details</summary>
Motivation: To provide reliable retinal disease diagnosis in low-resourced settings where access to diagnostic devices is limited.

Method: Train a high-capacity ViT teacher model using I-JEPA self-supervised learning, then compress it into a CNN-based student model using a novel framework (PCA projector, GL projector, and multi-view robust training).

Result: The student model retains 93% of the teacher model's diagnostic performance (89% classification accuracy) despite having 97.4% fewer parameters.

Conclusion: The method successfully compresses a ViT model while retaining accuracy, offering a scalable AI-driven solution for retinal disorder triage in under-resourced areas.

Abstract: Early and accurate identification of retinal ailments is crucial for averting
ocular decline; however, access to dependable diagnostic devices is not often
available in low-resourced settings. This project proposes to solve that by
developing a lightweight, edge-device deployable disease classifier using
cross-architecture knowledge distilling. We first train a high-capacity vision
transformer (ViT) teacher model, pre-trained using I-JEPA self-supervised
learning, to classify fundus images into four classes: Normal, Diabetic
Retinopathy, Glaucoma, and Cataract. We kept an Internet of Things (IoT) focus
when compressing to a CNN-based student model for deployment in
resource-limited conditions, such as the NVIDIA Jetson Nano. This was
accomplished using a novel framework which included a Partitioned
Cross-Attention (PCA) projector, a Group-Wise Linear (GL) projector, and a
multi-view robust training method. The teacher model has 97.4 percent more
parameters than the student model, with it achieving 89 percent classification
with a roughly 93 percent retention of the teacher model's diagnostic
performance. The retention of clinical classification behavior supports our
method's initial aim: compression of the ViT while retaining accuracy. Our work
serves as an example of a scalable, AI-driven triage solution for retinal
disorders in under-resourced areas.

</details>


### [177] [Make It Efficient: Dynamic Sparse Attention for Autoregressive Image Generation](https://arxiv.org/abs/2506.18226)
*Xunzhi Xiang,Qi Fan*

Main category: cs.CV

TL;DR: The paper introduces Adaptive Dynamic Sparse Attention (ADSA), a training-free method to optimize context in autoregressive image generation, reducing memory and computational overhead by dynamically focusing on key tokens.


<details>
  <summary>Details</summary>
Motivation: Addressing the memory and computational inefficiencies in autoregressive image generation caused by long contexts during inference.

Method: Proposes ADSA, which dynamically identifies crucial historical tokens for local and global coherence, and introduces a dynamic KV-cache update mechanism.

Result: Reduces GPU memory consumption by ~50% while maintaining generation quality, as shown in experiments.

Conclusion: ADSA effectively balances resource efficiency and generation quality in text-to-image synthesis.

Abstract: Autoregressive conditional image generation models have emerged as a dominant
paradigm in text-to-image synthesis. These methods typically convert images
into one-dimensional token sequences and leverage the self-attention mechanism,
which has achieved remarkable success in natural language processing, to
capture long-range dependencies, model global context, and ensure semantic
coherence. However, excessively long contexts during inference lead to
significant memory overhead caused by KV-cache and computational delays. To
alleviate these challenges, we systematically analyze how global semantics,
spatial layouts, and fine-grained textures are formed during inference, and
propose a novel training-free context optimization method called Adaptive
Dynamic Sparse Attention (ADSA). Conceptually, ADSA dynamically identifies
historical tokens crucial for maintaining local texture consistency and those
essential for ensuring global semantic coherence, thereby efficiently
streamlining attention computation. Additionally, we introduce a dynamic
KV-cache update mechanism tailored for ADSA, reducing GPU memory consumption
during inference by approximately $50\%$. Extensive qualitative and
quantitative experiments demonstrate the effectiveness and superiority of our
approach in terms of both generation quality and resource efficiency.

</details>


### [178] [Drive-R1: Bridging Reasoning and Planning in VLMs for Autonomous Driving with Reinforcement Learning](https://arxiv.org/abs/2506.18234)
*Yue Li,Meng Tian,Dechang Zhu,Jiangtong Zhu,Zhenyu Lin,Zhiwei Xiong,Xinhai Zhao*

Main category: cs.CV

TL;DR: Drive-R1 bridges reasoning and motion planning in autonomous driving by combining supervised fine-tuning and reinforcement learning, outperforming existing VLMs.


<details>
  <summary>Details</summary>
Motivation: Addresses challenges in VLMs for AD: reliance on history inputs (shortcuts) and misaligned reasoning with planning outcomes.

Method: Drive-R1 uses supervised fine-tuning on COT data and reinforcement learning to align reasoning with planning.

Result: Superior performance on nuScenes and DriveLM-nuScenes benchmarks.

Conclusion: Drive-R1 offers a promising direction for integrating reasoning and planning in AD.

Abstract: Large vision-language models (VLMs) for autonomous driving (AD) are evolving
beyond perception and cognition tasks toward motion planning. However, we
identify two critical challenges in this direction: (1) VLMs tend to learn
shortcuts by relying heavily on history input information, achieving seemingly
strong planning results without genuinely understanding the visual inputs; and
(2) the chain-ofthought (COT) reasoning processes are always misaligned with
the motion planning outcomes, and how to effectively leverage the complex
reasoning capability to enhance planning remains largely underexplored. In this
paper, we start from a small-scale domain-specific VLM and propose Drive-R1
designed to bridges the scenario reasoning and motion planning for AD. Drive-R1
first undergoes the supervised finetuning on a elaborate dataset containing
both long and short COT data. Drive-R1 is encouraged to reason step-by-step
from visual input to final planning decisions. Subsequently, Drive-R1 is
trained within a reinforcement learning framework that incentivizes the
discovery of reasoning paths that are more informative for planning, guided by
rewards based on predicted trajectories and meta actions. Experimental
evaluations on the nuScenes and DriveLM-nuScenes benchmarks demonstrate that
Drive-R1 achieves superior performance compared to existing state-of-the-art
VLMs. We believe that Drive-R1 presents a promising direction for bridging
reasoning and planning in AD, offering methodological insights for future
research and applications.

</details>


### [179] [Referring Expression Instance Retrieval and A Strong End-to-End Baseline](https://arxiv.org/abs/2506.18246)
*Xiangzhao Hao,Kuan Zhu,Hongyu Guo,Haiyun Guo,Ming Tang,JinQiao Wang*

Main category: cs.CV

TL;DR: The paper introduces a new task, Referring Expression Instance Retrieval (REIR), combining instance-level retrieval and localization. It presents REIRCOCO, a benchmark, and CLARE, a baseline method with a dual-stream architecture, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing tasks like TIR and REC lack precision or scalability for real-world scenarios requiring both instance-level retrieval and localization.

Method: Proposes REIR and REIRCOCO benchmark. Introduces CLARE, a dual-stream method with a Mix of Relation Experts (MORE) module and Contrastive Language-Instance Alignment (CLIA).

Result: CLARE achieves state-of-the-art performance on REIR and generalizes well to TIR and REC.

Conclusion: CLARE is effective and versatile, addressing the gap between TIR and REC for real-world applications.

Abstract: Natural language querying of visual content underpins many vision-language
tasks, typically categorized by text granularity and visual search scope.
Text-Image Retrieval (TIR) retrieves whole images using coarse descriptions,
while Referring Expression Comprehension (REC) localizes objects using
fine-grained expressions within a single image. However, real-world scenarios
often require both instance-level retrieval and localization across large
galleries -- tasks where TIR lacks precision and REC lacks scalability. To
address this gap, we propose a new task: Referring Expression Instance
Retrieval (REIR), which jointly supports instance-level retrieval and
localization. We introduce REIRCOCO, a large-scale benchmark constructed by
prompting vision-language models to generate fine-grained expressions for
MSCOCO and RefCOCO instances. We also present a baseline method, CLARE,
featuring a dual-stream architecture with a Mix of Relation Experts (MORE)
module for capturing inter-instance relationships. CLARE integrates object
detection and REC pretraining with Contrastive Language-Instance Alignment
(CLIA) for end-to-end optimization. Experiments show that CLARE achieves
state-of-the-art performance on REIR and generalizes well to TIR and REC,
highlighting its effectiveness and versatility.

</details>


### [180] [Semantic Structure-Aware Generative Attacks for Enhanced Adversarial Transferability](https://arxiv.org/abs/2506.18248)
*Jongoh Jeong,Hunmin Yang,Jaeseok Jeong,Kuk-Jin Yoon*

Main category: cs.CV

TL;DR: The paper introduces a semantic structure-aware attack framework using Mean Teacher to enhance adversarial transferability by leveraging underutilized semantic features in generative models.


<details>
  <summary>Details</summary>
Motivation: Existing generative adversarial attacks fail to fully exploit semantic information in intermediate activations, limiting perturbation alignment with object-salient regions critical for transferability.

Method: Proposes a framework based on Mean Teacher for semantic consistency, using feature distillation to guide perturbation synthesis on semantically salient regions.

Result: Demonstrates consistent improvements over state-of-the-art generative attacks across diverse models, domains, and tasks.

Conclusion: The method effectively enhances adversarial transferability by better utilizing semantic features, validated by conventional metrics and a new Accidental Correction Rate (ACR).

Abstract: Generative adversarial attacks train a perturbation generator on a white-box
surrogate model and subsequently apply the crafted perturbations to unseen
black-box victim models. In contrast to iterative attacks, these methods
deliver superior inference-time efficiency, scalability, and transferability;
however, up until now, existing studies have not fully exploited the
representational capacity of generative models to preserve and harness semantic
information. Specifically, the intermediate activations of the generator encode
rich semantic features--object boundaries and coarse shapes--that remain
under-exploited, thereby limiting the alignment of perturbations with
object-salient regions which are critical for adversarial transferability. To
remedy this, we introduce a semantic structure-aware attack framework based on
the Mean Teacher, which serves as a temporally smoothed feature reference. With
this smoothed reference, we further direct semantic consistency between the
early-layer activations in the student and those of the semantically rich
teacher by feature distillation. By anchoring perturbation synthesis to the
semantically salient early intermediate blocks within the generator based on
empirical findings, our method guides progressive adversarial perturbation on
regions that substantially enhance adversarial transferability. We conduct
extensive experiments over diverse models, domains and tasks to demonstrate
consistent improvements relative to state-of-the-art generative attacks,
comprehensively evaluated using conventional metrics and our newly proposed
Accidental Correction Rate (ACR).

</details>


### [181] [Improving Weakly Supervised Temporal Action Localization by Exploiting Multi-resolution Information in Temporal Domain](https://arxiv.org/abs/2506.18261)
*Rui Su,Dong Xu,Luping Zhou,Wanli Ouyang*

Main category: cs.CV

TL;DR: A two-stage approach for weakly supervised temporal action localization using multi-resolution information and iterative pseudo-label refinement.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of weakly supervised temporal action localization where only video-level annotations are available.

Method: Proposes a two-stage method: 1) Initial Label Generation (ILG) for reliable pseudo labels, and 2) Progressive Temporal Label Refinement (PTLR) with two networks (OTS and RTS streams) to refine labels iteratively.

Result: Improved temporal action localization performance by leveraging multi-resolution temporal information and pseudo-label refinement.

Conclusion: The method effectively exploits multi-resolution temporal data and iterative refinement to enhance weakly supervised action localization.

Abstract: Weakly supervised temporal action localization is a challenging task as only
the video-level annotation is available during the training process. To address
this problem, we propose a two-stage approach to fully exploit multi-resolution
information in the temporal domain and generate high quality frame-level pseudo
labels based on both appearance and motion streams. Specifically, in the first
stage, we generate reliable initial frame-level pseudo labels, and in the
second stage, we iteratively refine the pseudo labels and use a set of selected
frames with highly confident pseudo labels to train neural networks and better
predict action class scores at each frame. We fully exploit temporal
information at multiple scales to improve temporal action localization
performance. Specifically, in order to obtain reliable initial frame-level
pseudo labels, in the first stage, we propose an Initial Label Generation (ILG)
module, which leverages temporal multi-resolution consistency to generate high
quality class activation sequences (CASs), which consist of a number of
sequences with each sequence measuring how likely each video frame belongs to
one specific action class. In the second stage, we propose a Progressive
Temporal Label Refinement (PTLR) framework. In our PTLR framework, two networks
called Network-OTS and Network-RTS, which are respectively used to generate
CASs for the original temporal scale and the reduced temporal scales, are used
as two streams (i.e., the OTS stream and the RTS stream) to refine the pseudo
labels in turn. By this way, the multi-resolution information in the temporal
domain is exchanged at the pseudo label level, and our work can help improve
each stream (i.e., the OTS/RTS stream) by exploiting the refined pseudo labels
from another stream (i.e., the RTS/OTS stream).

</details>


### [182] [YouTube-Occ: Learning Indoor 3D Semantic Occupancy Prediction from YouTube Videos](https://arxiv.org/abs/2506.18266)
*Haoming Chen,Lichen Yuan,TianFang Sun,Jingyu Gong,Xin Tan,Zhizhong Zhang,Yuan Xie*

Main category: cs.CV

TL;DR: 3D semantic occupancy prediction is achieved using only indoor Internet data without camera parameters, leveraging self-supervised learning and 2D prior knowledge.


<details>
  <summary>Details</summary>
Motivation: Overcoming impracticalities of precise geometric data collection and fine-grained annotations in complex indoor environments.

Method: Uses YouTube-Occ dataset from house tour videos, employs a self-supervised model to distill 2D region-level knowledge into 3D occupancy prediction via superpixels.

Result: Achieves state-of-the-art zero-shot performance on NYUv2 and OccScanNet benchmarks.

Conclusion: Demonstrates effective 3D spatially-accurate training using accessible web data, bypassing traditional constraints.

Abstract: 3D semantic occupancy prediction in the past was considered to require
precise geometric relationships in order to enable effective training. However,
in complex indoor environments, the large-scale and widespread collection of
data, along with the necessity for fine-grained annotations, becomes
impractical due to the complexity of data acquisition setups and privacy
concerns. In this paper, we demonstrate that 3D spatially-accurate training can
be achieved using only indoor Internet data, without the need for any
pre-knowledge of intrinsic or extrinsic camera parameters. In our framework, we
collect a web dataset, YouTube-Occ, which comprises house tour videos from
YouTube, providing abundant real house scenes for 3D representation learning.
Upon on this web dataset, we establish a fully self-supervised model to
leverage accessible 2D prior knowledge for reaching powerful 3D indoor
perception. Specifically, we harness the advantages of the prosperous vision
foundation models, distilling the 2D region-level knowledge into the occupancy
network by grouping the similar pixels into superpixels. Experimental results
show that our method achieves state-of-the-art zero-shot performance on two
popular benchmarks (NYUv2 and OccScanNet

</details>


### [183] [ThermalLoc: A Vision Transformer-Based Approach for Robust Thermal Camera Relocalization in Large-Scale Environments](https://arxiv.org/abs/2506.18268)
*Yu Liu,Yangtao Meng,Xianfei Pan,Jie Jiang,Changhao Chen*

Main category: cs.CV

TL;DR: ThermalLoc is a deep learning method for thermal image relocalization, outperforming existing methods by integrating EfficientNet and Transformers.


<details>
  <summary>Details</summary>
Motivation: Traditional visual relocalization methods don't work for thermal images, and deep learning approaches for thermal relocalization are underexplored.

Method: ThermalLoc combines EfficientNet and Transformers to extract local and global features from thermal images, using two MLP networks for absolute pose regression.

Result: ThermalLoc outperforms AtLoc, MapNet, PoseNet, and RobustLoc in accuracy and robustness on public and proprietary datasets.

Conclusion: ThermalLoc is a superior solution for thermal image relocalization, addressing the gap in existing methods.

Abstract: Thermal cameras capture environmental data through heat emission, a
fundamentally different mechanism compared to visible light cameras, which rely
on pinhole imaging. As a result, traditional visual relocalization methods
designed for visible light images are not directly applicable to thermal
images. Despite significant advancements in deep learning for camera
relocalization, approaches specifically tailored for thermal camera-based
relocalization remain underexplored. To address this gap, we introduce
ThermalLoc, a novel end-to-end deep learning method for thermal image
relocalization. ThermalLoc effectively extracts both local and global features
from thermal images by integrating EfficientNet with Transformers, and performs
absolute pose regression using two MLP networks. We evaluated ThermalLoc on
both the publicly available thermal-odometry dataset and our own dataset. The
results demonstrate that ThermalLoc outperforms existing representative methods
employed for thermal camera relocalization, including AtLoc, MapNet, PoseNet,
and RobustLoc, achieving superior accuracy and robustness.

</details>


### [184] [Adaptive Mask-guided K-space Diffusion for Accelerated MRI Reconstruction](https://arxiv.org/abs/2506.18270)
*Qinrong Cai,Yu Guan,Zhibo Chen,Dong Liang,Qiuyun Fan,Qiegen Liu*

Main category: cs.CV

TL;DR: The paper introduces AMDM, a diffusion model using adaptive masks for MRI reconstruction, improving quality by focusing on frequency-specific regions in k-space.


<details>
  <summary>Details</summary>
Motivation: Traditional MRI reconstruction methods optimize entire image or k-space without considering frequency region importance, limiting performance.

Method: AMDM adaptively adjusts masks based on k-space frequency distribution, separating high/low-frequency components and guiding a diffusion process.

Result: Experiments show AMDM effectively learns frequency-specific information, enhancing MRI reconstruction quality.

Conclusion: AMDM provides a flexible framework for optimizing k-space data with adaptive masks, promising future improvements in MRI reconstruction.

Abstract: As the deep learning revolution marches on, masked modeling has emerged as a
distinctive approach that involves predicting parts of the original data that
are proportionally masked during training, and has demonstrated exceptional
performance in multiple fields. Magnetic Resonance Imaging (MRI) reconstruction
is a critical task in medical imaging that seeks to recover high-quality images
from under-sampled k-space data. However, previous MRI reconstruction
strategies usually optimized the entire image domain or k-space, without
considering the importance of different frequency regions in the k-space This
work introduces a diffusion model based on adaptive masks (AMDM), which
utilizes the adaptive adjustment of frequency distribution based on k-space
data to develop a hybrid masks mechanism that adapts to different k-space
inputs. This enables the effective separation of high-frequency and
low-frequency components, producing diverse frequency-specific representations.
Additionally, the k-space frequency distribution informs the generation of
adaptive masks, which, in turn, guide a closed-loop diffusion process.
Experimental results verified the ability of this method to learn specific
frequency information and thereby improved the quality of MRI reconstruction,
providing a flexible framework for optimizing k-space data using masks in the
future.

</details>


### [185] [ReFrame: Rectification Framework for Image Explaining Architectures](https://arxiv.org/abs/2506.18272)
*Debjyoti Das Adhikary,Aritra Hazra,Partha Pratim Chakrabarti*

Main category: cs.CV

TL;DR: The paper proposes an interpretable framework to improve image explanation tasks by addressing inconsistency and incompleteness in object recognition, enhancing existing methods like Image Captioning, VQA, and Prompt-based AI.


<details>
  <summary>Details</summary>
Motivation: Existing image explanation methods often hallucinate objects or miss identifying all objects in an image, leading to unreliable outputs.

Method: An interpretable framework is introduced to rectify incorrect or missing objects in explanations, applicable to Image Captioning, VQA, and Prompt-based AI.

Result: Quantitative improvements include 81.81% completeness and 37.10% inconsistency in Image Captioning, and similar gains in VQA and Prompt-based AI.

Conclusion: The proposed framework significantly enhances the accuracy and reliability of image explanations across multiple tasks.

Abstract: Image explanation has been one of the key research interests in the Deep
Learning field. Throughout the years, several approaches have been adopted to
explain an input image fed by the user. From detecting an object in a given
image to explaining it in human understandable sentence, to having a
conversation describing the image, this problem has seen an immense change
throughout the years, However, the existing works have been often found to (a)
hallucinate objects that do not exist in the image and/or (b) lack identifying
the complete set of objects present in the image. In this paper, we propose a
novel approach to mitigate these drawbacks of inconsistency and incompleteness
of the objects recognized during the image explanation. To enable this, we
propose an interpretable framework that can be plugged atop diverse image
explaining frameworks including Image Captioning, Visual Question Answering
(VQA) and Prompt-based AI using LLMs, thereby enhancing their explanation
capabilities by rectifying the incorrect or missing objects. We further measure
the efficacy of the rectified explanations generated through our proposed
approaches leveraging object based precision metrics, and showcase the
improvements in the inconsistency and completeness of image explanations.
Quantitatively, the proposed framework is able to improve the explanations over
the baseline architectures of Image Captioning (improving the completeness by
81.81% and inconsistency by 37.10%), Visual Question Answering(average of 9.6%
and 37.10% in completeness and inconsistency respectively) and Prompt-based AI
model (0.01% and 5.2% for completeness and inconsistency respectively)
surpassing the current state-of-the-art by a substantial margin.

</details>


### [186] [Open Set Recognition for Endoscopic Image Classification: A Deep Learning Approach on the Kvasir Dataset](https://arxiv.org/abs/2506.18284)
*Kasra Moazzami,Seoyoun Son,John Lin,Sun Min Lee,Daniel Son,Hayeon Lee,Jeongho Lee,Seongji Lee*

Main category: cs.CV

TL;DR: The paper explores Open Set Recognition (OSR) techniques for endoscopic image classification on the Kvasir dataset, comparing deep learning models like ResNet-50, Swin Transformer, and a hybrid model under closed-set and open-set conditions.


<details>
  <summary>Details</summary>
Motivation: Conventional closed-set classification is limited in open-world clinical settings where unseen conditions can arise, compromising model reliability.

Method: Evaluated OSR capabilities of ResNet-50, Swin Transformer, and a hybrid ResNet-Transformer model using OpenMax as a baseline OSR method.

Result: Provides foundational benchmarks for OSR performance in medical image analysis, highlighting model behavior in realistic clinical settings.

Conclusion: Emphasizes the importance of OSR techniques for safe AI deployment in endoscopy.

Abstract: Endoscopic image classification plays a pivotal role in medical diagnostics
by identifying anatomical landmarks and pathological findings. However,
conventional closed-set classification frameworks are inherently limited in
open-world clinical settings, where previously unseen conditions can arise
andcompromise model reliability. To address this, we explore the application of
Open Set Recognition (OSR) techniques on the Kvasir dataset, a publicly
available and diverse endoscopic image collection. In this study, we evaluate
and compare the OSR capabilities of several representative deep learning
architectures, including ResNet-50, Swin Transformer, and a hybrid
ResNet-Transformer model, under both closed-set and open-set conditions.
OpenMax is adopted as a baseline OSR method to assess the ability of these
models to distinguish known classes from previously unseen categories. This
work represents one of the first efforts to apply open set recognition to the
Kvasir dataset and provides a foundational benchmark for evaluating OSR
performance in medical image analysis. Our results offer practical insights
into model behavior in clinically realistic settings and highlight the
importance of OSR techniques for the safe deployment of AI systems in
endoscopy.

</details>


### [187] [Selective Social-Interaction via Individual Importance for Fast Human Trajectory Prediction](https://arxiv.org/abs/2506.18291)
*Yota Urano,Hiromu Taketsugu,Norimichi Ukita*

Main category: cs.CV

TL;DR: Proposes an architecture with an Importance Estimator to select key neighbors for predicting a person's trajectory, using Gumbel Softmax for training. Achieves faster processing with competitive accuracy.


<details>
  <summary>Details</summary>
Motivation: To improve trajectory prediction by effectively selecting important neighboring people, addressing gradient blocking in non-differentiable operations.

Method: Uses an Importance Estimator module and Gumbel Softmax for training to sample neighbors based on importance.

Result: Speeds up the process while maintaining competitive prediction accuracy on the JRDB dataset.

Conclusion: The method efficiently selects important neighbors for trajectory prediction, balancing speed and accuracy.

Abstract: This paper presents an architecture for selecting important neighboring
people to predict the primary person's trajectory. To achieve effective
neighboring people selection, we propose a people selection module called the
Importance Estimator which outputs the importance of each neighboring person
for predicting the primary person's future trajectory. To prevent gradients
from being blocked by non-differentiable operations when sampling surrounding
people based on their importance, we employ the Gumbel Softmax for training.
Experiments conducted on the JRDB dataset show that our method speeds up the
process with competitive prediction accuracy.

</details>


### [188] [Rapeseed population point cloud completion network (RP-PCN) with dynamic graph convolution for 3D reconstruction of crop canopy occlusion architecture](https://arxiv.org/abs/2506.18292)
*Ziyue Guo,Xin Yang,Yutao Shen,Yang Zhu,Lixi Jiang,Haiyan Cen*

Main category: cs.CV

TL;DR: A point cloud completion model (RP-PCN) is proposed for 3D reconstruction of rapeseed populations, improving canopy architecture analysis and yield prediction.


<details>
  <summary>Details</summary>
Motivation: Accurate canopy architecture descriptions are hindered by occlusion and complexity, limiting photosynthesis and yield evaluation.

Method: Developed a framework with virtual-real integration and occlusion detection, using MRDG and PPD modules for point cloud completion.

Result: RP-PCN achieved low CD values and improved yield prediction accuracy by 11.2% using complete point clouds.

Conclusion: RP-PCN enhances canopy analysis and can be extended to other crops, benefiting field environment studies.

Abstract: Quantitative descriptions of complete canopy architecture are crucial for
evaluating crop photosynthesis and yield to guide ideotype design. Although
three-dimensional (3D) sensing technologies have been developed for plant and
canopy reconstruction, severe occlusion and complex architectures hinder
accurate canopy descriptions. In this study, we propose a point cloud
completion model for 3D reconstruction of rapeseed populations from seeding to
silique stages using multi-view imaging. A complete point cloud generation
framework was developed with the virtual-real integration (VRI) simulation
method and occlusion point detection algorithm to annotate the training dataset
by distinguishing surface from occluded points. The rapeseed population point
cloud completion network (RP-PCN) was designed with a multi-resolution dynamic
graph convolutional encoder (MRDG) and point pyramid decoder (PPD) to predict
occluded points based on input surface point clouds. A dynamic graph
convolutional feature extractor (DGCFE) was introduced to capture structural
variations across the growth period. The effectiveness of point cloud
completion was validated by predicting yield using architectural indicators
from complete point clouds of rapeseed population. The results demonstrated
that RP-PCN achieved chamfer distance (CD) values of 3.35 cm, 3.46 cm, 4.32 cm,
and 4.51 cm at the seedling, bolting, flowering, and silique stages,
respectively. Ablation studies showed the effectiveness of the MRDG and DGCFE
modules, reducing CD values by 10% and 23%, respectively. The silique
efficiency index (SEI) from RP-PCN improved yield prediction accuracy by 11.2%
compared to incomplete point clouds. The RP-PCN pipeline proposed in this study
has the potential to be extended to other crops, significantly enhancing the
analysis of population canopy architectures in field environments.

</details>


### [189] [Attention-Based Ensemble Learning for Crop Classification Using Landsat 8-9 Fusion](https://arxiv.org/abs/2506.18321)
*Zeeshan Ramzan,Nisar Ahmed,Qurat-ul-Ain Akram,Shahzad Asif,Muhammad Shahbaz,Rabin Chakrabortty,Ahmed F. Elaksher*

Main category: cs.CV

TL;DR: The study uses remote sensing and advanced modeling to classify crops in Central Punjab, combining field surveys, satellite imagery, and machine learning for high accuracy.


<details>
  <summary>Details</summary>
Motivation: To improve crop classification accuracy in irrigated regions using remote sensing and advanced modeling techniques.

Method: Field surveys for data collection, Landsat 8-9 imagery pre-processing, image fusion, vegetation indices extraction, and classification using conventional classifiers, ensemble learning, and neural networks.

Result: A comprehensive dataset of 50,835 data points was created, enabling accurate crop classification with enhanced spectral information.

Conclusion: Combining remote sensing data with advanced modeling significantly improves crop classification accuracy in irrigated agricultural areas.

Abstract: Remote sensing offers a highly effective method for obtaining accurate
information on total cropped area and crop types. The study focuses on crop
cover identification for irrigated regions of Central Punjab. Data collection
was executed in two stages: the first involved identifying and geocoding six
target crops through field surveys conducted in January and February 2023. The
second stage involved acquiring Landsat 8-9 imagery for each geocoded field to
construct a labelled dataset. The satellite imagery underwent extensive
pre-processing, including radiometric calibration for reflectance values,
atmospheric correction, and georeferencing verification to ensure consistency
within a common coordinate system. Subsequently, image fusion techniques were
applied to combine Landsat 8 and 9 spectral bands, creating a composite image
with enhanced spectral information, followed by contrast enhancement. During
data acquisition, farmers were interviewed, and fields were meticulously mapped
using GPS instruments, resulting in a comprehensive dataset of 50,835 data
points. This dataset facilitated the extraction of vegetation indices such as
NDVI, SAVO, RECI, and NDRE. These indices and raw reflectance values were
utilized for classification modeling using conventional classifiers, ensemble
learning, and artificial neural networks. A feature selection approach was also
incorporated to identify the optimal feature set for classification learning.
This study demonstrates the effectiveness of combining remote sensing data and
advanced modeling techniques to improve crop classification accuracy in
irrigated agricultural regions.

</details>


### [190] [Escaping the SpuriVerse: Can Large Vision-Language Models Generalize Beyond Seen Spurious Correlations?](https://arxiv.org/abs/2506.18322)
*Yiwei Yang,Chung Peng Lee,Shangbin Feng,Dora Zhao,Bingbing Wen,Anthony Z. Liu,Yulia Tsvetkov,Bill Howe*

Main category: cs.CV

TL;DR: The paper introduces SpuriVerse, a benchmark for studying spurious correlations in multi-modal Large Vision Language Models (LVLMs), sourced from GPT-4o errors and curated through human annotation and synthetic evaluation. It evaluates 15 LVLMs, showing poor performance (37.1% accuracy) and improvement (78.4%) after fine-tuning on synthetic examples.


<details>
  <summary>Details</summary>
Motivation: To address the lack of benchmarks for studying spurious correlations in LVLMs trained on diverse, unsupervised datasets, particularly in real-world visual-question-answering (VQA) tasks.

Method: Developed SpuriVerse by sourcing GPT-4o errors, curating them with LVLM-human annotation, and synthetic counterfactual evaluation. The benchmark includes 124 spurious correlation types with 1364 VQA questions. Evaluated 15 LVLMs before and after fine-tuning on synthetic examples.

Result: State-of-the-art LVLMs performed poorly (37.1% accuracy) on SpuriVerse. Fine-tuning on synthetic examples improved accuracy to 78.4%, indicating better generalization by avoiding spurious shortcuts.

Conclusion: SpuriVerse effectively highlights spurious correlation challenges in LVLMs. Fine-tuning on diverse synthetic examples helps models generalize by focusing on broader context rather than shortcuts.

Abstract: Finetuning can cause spurious correlations to arise between non-essential
features and the target labels, but benchmarks to study these effects involve
contrived settings and narrow tasks. In contrast, we consider spurious
correlations in multi-modal Large Vision Language Models (LVLMs) pretrained on
extensive and diverse datasets without explicit task supervision. We develop a
benchmark by sourcing GPT-4o errors on real-world visual-question-answering
(VQA) benchmarks, then curating a subset through LVLM-human annotation and
synthetic counterfactual evaluation to identify errors caused by spurious
correlations. This process yields SpuriVerse, a novel benchmark comprised of
124 distinct types of spurious correlations extracted from real-world datasets,
each containing 1 realistic and 10 synthetic VQA samples for a total of 1364
multiple choice questions. We evaluate 15 open and closed-source LVLMs on
SpuriVerse, finding that even state-of-the-art closed-source models struggle
significantly, achieving at best only 37.1% accuracy. Fine-tuning on synthetic
examples that emphasize the spurious correlation improves performance to
78.40%, suggesting that training on diverse spurious patterns generalizes to
unseen situations: models appear to learn to avoid "shortcuts" and attend to
the overall image context.

</details>


### [191] [A Multi-Scale Spatial Attention-Based Zero-Shot Learning Framework for Low-Light Image Enhancement](https://arxiv.org/abs/2506.18323)
*Muhammad Azeem Aslam,Hassan Khalid,Nisar Ahmed*

Main category: cs.CV

TL;DR: LucentVisionNet is a zero-shot learning framework for low-light image enhancement, outperforming state-of-the-art methods with multi-scale spatial attention and a deep curve estimation network.


<details>
  <summary>Details</summary>
Motivation: Traditional and deep learning-based methods struggle with low-light enhancement without paired data. LucentVisionNet aims to overcome these limitations.

Method: Integrates multi-scale spatial attention with a deep curve estimation network, uses a recurrent enhancement strategy, and optimizes with a composite loss function.

Result: Outperforms supervised, unsupervised, and zero-shot methods on benchmark datasets, achieving high visual quality and efficiency.

Conclusion: LucentVisionNet is effective for real-world applications like mobile photography and surveillance due to its performance and efficiency.

Abstract: Low-light image enhancement remains a challenging task, particularly in the
absence of paired training data. In this study, we present LucentVisionNet, a
novel zero-shot learning framework that addresses the limitations of
traditional and deep learning-based enhancement methods. The proposed approach
integrates multi-scale spatial attention with a deep curve estimation network,
enabling fine-grained enhancement while preserving semantic and perceptual
fidelity. To further improve generalization, we adopt a recurrent enhancement
strategy and optimize the model using a composite loss function comprising six
tailored components, including a novel no-reference image quality loss inspired
by human visual perception. Extensive experiments on both paired and unpaired
benchmark datasets demonstrate that LucentVisionNet consistently outperforms
state-of-the-art supervised, unsupervised, and zero-shot methods across
multiple full-reference and no-reference image quality metrics. Our framework
achieves high visual quality, structural consistency, and computational
efficiency, making it well-suited for deployment in real-world applications
such as mobile photography, surveillance, and autonomous navigation.

</details>


### [192] [NSFW-Classifier Guided Prompt Sanitization for Safe Text-to-Image Generation](https://arxiv.org/abs/2506.18325)
*Yu Xie,Chengjie Zeng,Lingyun Zhang,Yanwei Fu*

Main category: cs.CV

TL;DR: PromptSan detoxifies harmful prompts in text-to-image models without altering model architecture, using two variants: PromptSan-Modify and PromptSan-Suffix.


<details>
  <summary>Details</summary>
Motivation: Address risks of misuse (e.g., harmful content generation) in T2I models while maintaining ethical goals and usability.

Method: Two variants: PromptSan-Modify (iteratively replaces harmful tokens) and PromptSan-Suffix (trains suffix tokens to neutralize harm).

Result: Achieves state-of-the-art performance in reducing harmful content, balancing safety and usability.

Conclusion: PromptSan effectively mitigates harmful content generation in T2I models without compromising functionality.

Abstract: The rapid advancement of text-to-image (T2I) models, such as Stable
Diffusion, has enhanced their capability to synthesize images from textual
prompts. However, this progress also raises significant risks of misuse,
including the generation of harmful content (e.g., pornography, violence,
discrimination), which contradicts the ethical goals of T2I technology and
hinders its sustainable development. Inspired by "jailbreak" attacks in large
language models, which bypass restrictions through subtle prompt modifications,
this paper proposes NSFW-Classifier Guided Prompt Sanitization (PromptSan), a
novel approach to detoxify harmful prompts without altering model architecture
or degrading generation capability. PromptSan includes two variants:
PromptSan-Modify, which iteratively identifies and replaces harmful tokens in
input prompts using text NSFW classifiers during inference, and
PromptSan-Suffix, which trains an optimized suffix token sequence to neutralize
harmful intent while passing both text and image NSFW classifier checks.
Extensive experiments demonstrate that PromptSan achieves state-of-the-art
performance in reducing harmful content generation across multiple metrics,
effectively balancing safety and usability.

</details>


### [193] [Geometry-Aware Preference Learning for 3D Texture Generation](https://arxiv.org/abs/2506.18331)
*AmirHossein Zamani,Tianhao Xie,Amir G. Aghdam,Tiberiu Popa,Eugene Belilovsky*

Main category: cs.CV

TL;DR: A framework for 3D generative models integrates human preferences via differentiable reward functions, enhancing geometry-aware content creation.


<details>
  <summary>Details</summary>
Motivation: Existing 3D generative models often misalign with human preferences and lack inherent 3D structure understanding.

Method: Proposes an end-to-end differentiable preference learning framework with geometry-aware reward functions.

Result: Demonstrates effectiveness using four novel reward functions for controllable 3D content generation.

Conclusion: The framework offers a more interpretable and high-quality approach to 3D content creation from natural language.

Abstract: Recent advances in 3D generative models have achieved impressive results but
3D contents generated by these models may not align with subjective human
preferences or task-specific criteria. Moreover, a core challenge in the 3D
texture generation domain remains: most existing approaches rely on repeated
calls to 2D text-to-image generative models, which lack an inherent
understanding of the 3D structure of the input 3D mesh object. To address this,
we propose an end-to-end differentiable preference learning framework that
back-propagates human preferences, represented by differentiable reward
functions, through the entire 3D generative pipeline, making the process
inherently geometry-aware. We demonstrate the effectiveness of our framework
using four proposed novel geometry-aware reward functions, offering a more
controllable and interpretable pathway for high-quality 3D content creation
from natural language.

</details>


### [194] [Rethinking Decoder Design: Improving Biomarker Segmentation Using Depth-to-Space Restoration and Residual Linear Attention](https://arxiv.org/abs/2506.18335)
*Saad Wazir,Daeyoung Kim*

Main category: cs.CV

TL;DR: Proposes a novel architecture for medical image segmentation, improving accuracy by addressing feature transfer and decoder efficiency challenges.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with staining/morphology variations and limited datasets, hindering feature extraction and segmentation accuracy.

Method: Introduces a multi-scale feature capture architecture and a novel decoder design to integrate encoder features, emphasize key channels/regions, and reconstruct spatial dimensions.

Result: Outperforms SOTA methods with absolute gains of 2.76%-4.03% across four datasets.

Conclusion: The proposed method enhances segmentation accuracy and is compatible with various encoders, validated by extensive experiments.

Abstract: Segmenting biomarkers in medical images is crucial for various biotech
applications. Despite advances, Transformer and CNN based methods often
struggle with variations in staining and morphology, limiting feature
extraction. In medical image segmentation, where datasets often have limited
sample availability, recent state-of-the-art (SOTA) methods achieve higher
accuracy by leveraging pre-trained encoders, whereas end-to-end methods tend to
underperform. This is due to challenges in effectively transferring rich
multiscale features from encoders to decoders, as well as limitations in
decoder efficiency. To address these issues, we propose an architecture that
captures multi-scale local and global contextual information and a novel
decoder design, which effectively integrates features from the encoder,
emphasizes important channels and regions, and reconstructs spatial dimensions
to enhance segmentation accuracy. Our method, compatible with various encoders,
outperforms SOTA methods, as demonstrated by experiments on four datasets and
ablation studies. Specifically, our method achieves absolute performance gains
of 2.76% on MoNuSeg, 3.12% on DSB, 2.87% on Electron Microscopy, and 4.03% on
TNBC datasets compared to existing SOTA methods. Code:
https://github.com/saadwazir/MCADS-Decoder

</details>


### [195] [BSMamba: Brightness and Semantic Modeling for Long-Range Interaction in Low-Light Image Enhancement](https://arxiv.org/abs/2506.18346)
*Tongshun Zhang,Pingping Liu,Mengen Cai,Zijian Zhang,Yubing Lu,Qiuzhan Zhou*

Main category: cs.CV

TL;DR: BSMamba, a novel visual Mamba architecture, enhances low-light images by prioritizing brightness and semantic token interactions, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing low-light image enhancement methods struggle with brightness improvement, semantic consistency, and computational efficiency. Current visual Mamba approaches limit token interactions due to fixed scanning rules.

Method: BSMamba introduces Brightness Mamba for brightness-guided token interactions and Semantic Mamba for semantic consistency, replacing arbitrary scanning patterns.

Result: BSMamba achieves state-of-the-art performance in low-light image enhancement while preserving semantic consistency.

Conclusion: BSMamba effectively addresses the limitations of current methods by leveraging brightness and semantic-guided token interactions, setting a new benchmark in LLIE.

Abstract: Current low-light image enhancement (LLIE) methods face significant
limitations in simultaneously improving brightness while preserving semantic
consistency, fine details, and computational efficiency. With the emergence of
state-space models, particularly Mamba, image restoration has achieved
remarkable performance, yet existing visual Mamba approaches flatten 2D images
into 1D token sequences using fixed scanning rules, critically limiting
interactions between distant tokens with causal relationships and constraining
their ability to capture meaningful long-range dependencies. To address these
fundamental limitations, we propose BSMamba, a novel visual Mamba architecture
comprising two specially designed components: Brightness Mamba and Semantic
Mamba. The Brightness Mamba revolutionizes token interaction patterns by
prioritizing connections between distant tokens with similar brightness levels,
effectively addressing the challenge of brightness restoration in LLIE tasks
through brightness-guided selective attention. Complementing this, the Semantic
Mamba establishes priority interactions between tokens sharing similar semantic
meanings, allowing the model to maintain contextual consistency by connecting
semantically related regions across the image, thus preserving the hierarchical
nature of image semantics during enhancement. By intelligently modeling tokens
based on brightness and semantic similarity rather than arbitrary scanning
patterns, BSMamba transcends the constraints of conventional token sequencing
while adhering to the principles of causal modeling. Extensive experiments
demonstrate that BSMamba achieves state-of-the-art performance in LLIE while
preserving semantic consistency.

</details>


### [196] [Spatial frequency information fusion network for few-shot learning](https://arxiv.org/abs/2506.18364)
*Wenqing Zhao,Guojia Xie,Han Pan,Biao Yang,Weichuan Zhang*

Main category: cs.CV

TL;DR: The paper proposes SFIFNet, a Few-shot learning method integrating frequency and spatial domain information to improve classification performance by addressing data scarcity and over-fitting.


<details>
  <summary>Details</summary>
Motivation: Few-shot learning faces challenges like limited data, over-fitting, and poor generalization. Existing models often ignore frequency domain information, missing valuable features.

Method: SFIFNet combines frequency and spatial domain information through innovative data preprocessing to enhance feature representation.

Result: Experiments show SFIFNet improves classification performance by better utilizing feature information.

Conclusion: Integrating frequency domain information with spatial data enhances Few-shot learning performance, as demonstrated by SFIFNet.

Abstract: The objective of Few-shot learning is to fully leverage the limited data
resources for exploring the latent correlations within the data by applying
algorithms and training a model with outstanding performance that can
adequately meet the demands of practical applications. In practical
applications, the number of images in each category is usually less than that
in traditional deep learning, which can lead to over-fitting and poor
generalization performance. Currently, many Few-shot classification models pay
more attention to spatial domain information while neglecting frequency domain
information, which contains more feature information. Ignoring frequency domain
information will prevent the model from fully exploiting feature information,
which would effect the classification performance. Based on conventional data
augmentation, this paper proposes an SFIFNet with innovative data
preprocessing. The key of this method is enhancing the accuracy of image
feature representation by integrating frequency domain information with spatial
domain information. The experimental results demonstrate the effectiveness of
this method in enhancing classification performance.

</details>


### [197] [Sequential keypoint density estimator: an overlooked baseline of skeleton-based video anomaly detection](https://arxiv.org/abs/2506.18368)
*Anja Deli,Matej Grci,Sinia egvi*

Main category: cs.CV

TL;DR: SeeKer detects anomalies in human skeleton sequences by modeling keypoint-level density with autoregressive factorization, outperforming prior methods on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Anomalous human behavior detection is crucial for safety-critical applications like healthcare and surveillance, where unusual poses often indicate abnormalities.

Method: SeeKer uses autoregressive factorization to model skeleton sequence density, predicting keypoint locations with conditional Gaussians and flagging low-density skeletons as anomalous.

Result: SeeKer achieves state-of-the-art performance on UBnormal and MSAD-HR datasets and competitive results on ShanghaiTech.

Conclusion: SeeKer's simplicity and effectiveness make it a robust solution for anomaly detection in human skeleton sequences.

Abstract: Detecting anomalous human behaviour is an important visual task in
safety-critical applications such as healthcare monitoring, workplace safety,
or public surveillance. In these contexts, abnormalities are often reflected
with unusual human poses. Thus, we propose SeeKer, a method for detecting
anomalies in sequences of human skeletons. Our method formulates the skeleton
sequence density through autoregressive factorization at the keypoint level.
The corresponding conditional distributions represent probable keypoint
locations given prior skeletal motion. We formulate the joint distribution of
the considered skeleton as causal prediction of conditional Gaussians across
its constituent keypoints. A skeleton is flagged as anomalous if its keypoint
locations surprise our model (i.e. receive a low density). In practice, our
anomaly score is a weighted sum of per-keypoint log-conditionals, where the
weights account for the confidence of the underlying keypoint detector. Despite
its conceptual simplicity, SeeKer surpasses all previous methods on the
UBnormal and MSAD-HR datasets while delivering competitive performance on the
ShanghaiTech dataset.

</details>


### [198] [RePIC: Reinforced Post-Training for Personalizing Multi-Modal Language Models](https://arxiv.org/abs/2506.18369)
*Yeongtak Oh,Jisoo Mok,Dohyun Chung,Juhyeon Shin,Sangha Park,Johan Barthelemy,Sungroh Yoon*

Main category: cs.CV

TL;DR: The paper proposes an RL-based post-training framework to improve personalized image captioning in MLLMs, outperforming SFT-based methods.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs struggle with personalized image captions, even after SFT, due to data limitations in complex scenarios like multi-concept captioning.

Method: Introduces a reinforcement learning (RL)-based post-training framework for MLLMs.

Result: The RL-based method enhances visual recognition and personalized generation, outperforming SFT baselines, especially in multi-concept tasks.

Conclusion: RL-based post-training is a promising solution for improving personalized captioning in MLLMs.

Abstract: Recent multi-modal large language models (MLLMs) often struggle to generate
personalized image captions, even when trained on high-quality captions. In
this work, we observe that such limitations persist in existing
post-training-based MLLM personalization methods. Specifically, despite being
post-tuned with large-scale caption data through supervised fine-tuning (SFT),
these models frequently fail to produce faithful descriptions in real-world
scenarios, such as multi-concept image captioning. However, acquiring
large-scale, high-quality captions for such complex settings is both costly and
difficult. To address the data-centric nature of SFT, we propose a
reinforcement learning (RL)-based post-training framework. To the best of our
knowledge, this is the first RL-based approach to post-train MLLMs for
personalized image captioning. Our method significantly enhances both visual
recognition and personalized generation capabilities of MLLMs, and consistently
outperforms existing SFT-based baselines, especially in the challenging
multi-concept image captioning task.

</details>


### [199] [OpenEvents V1: Large-Scale Benchmark Dataset for Multimodal Event Grounding](https://arxiv.org/abs/2506.18372)
*Hieu Nguyen,Phuc-Tan Nguyen,Thien-Phuc Tran,Minh-Quang Nguyen,Tam V. Nguyen,Minh-Triet Tran,Trung-Nghia Le*

Main category: cs.CV

TL;DR: OpenEvents V1 is a large-scale dataset for event-centric vision-language tasks, focusing on contextual and temporal grounding with 200K news articles and 400K images.


<details>
  <summary>Details</summary>
Motivation: To advance event-centric vision-language understanding beyond surface-level descriptions by emphasizing contextual and temporal grounding.

Method: The dataset supports two tasks: generating event-aware image captions and retrieving event-relevant images using narrative-style queries.

Result: Provides baseline results and evaluation protocols for both tasks, enabling deep reasoning over real-world events.

Conclusion: OpenEvents V1 lays a foundation for multimodal models to handle complex event understanding.

Abstract: We introduce OpenEvents V1, a large-scale benchmark dataset aimed at
advancing event-centric vision-language understanding. Unlike conventional
image captioning and retrieval datasets that emphasize surface-level
descriptions, OpenEvents V1 focuses on contextual and temporal grounding
through two primary tasks: (1) generating rich, event-aware image captions and
(2) retrieving event-relevant images based on narrative-style textual queries.
The dataset contains over 200,000 news articles and 400,000 associated images
sourced from CNN and The Guardian, spanning diverse domains and time periods.
We provide extensive baseline results and standardized evaluation protocols for
both tasks. OpenEvents V1 establishes a robust foundation for developing
multimodal models capable of deep reasoning over complex real-world events. The
dataset is available at https://ltnghia.github.io/eventa/openevents-v1

</details>


### [200] [InternSpatial: A Comprehensive Dataset for Spatial Reasoning in Vision-Language Models](https://arxiv.org/abs/2506.18385)
*Nianchen Deng,Lixin Gu,Shenglong Ye,Yinan He,Zhe Chen,Songze Li,Haomin Wang,Xingguang Wei,Tianshuo Yang,Min Dou,Tong He,Wenqi Shao,Kaipeng Zhang,Yi Wang,Botian Shi,Yanting Zhang,Jifeng Dai,Yu Qiao,Hongjie Zhang,Wenhai Wang*

Main category: cs.CV

TL;DR: InternSpatial is introduced as the largest open-source dataset for spatial reasoning in VLMs, paired with InternSpatial-Bench for evaluation, showing significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Existing resources for spatial reasoning in VLMs are limited in scale, diversity, and instruction expressiveness.

Method: InternSpatial includes 12M QA pairs across single-view and multi-view settings with 19 instruction formats. InternSpatial-Bench evaluates single-view tasks and introduces a novel rotation angle prediction task.

Result: Models trained on InternSpatial show 12.1% improvement on InternSpatial-Bench and 10.7% on VSI-Bench, while maintaining general-purpose performance.

Conclusion: InternSpatial and its benchmark aim to advance spatially capable VLMs for practical applications like robotics and embodied AI.

Abstract: Recent benchmarks and datasets have been proposed to improve spatial
reasoning in vision-language models (VLMs), yet existing open resources remain
limited in scale, visual diversity, and instruction expressiveness. In this
work, we introduce InternSpatial, the largest open-source dataset for spatial
reasoning in VLMs, along with InternSpatial-Bench, a corresponding evaluation
benchmark designed to assess spatial understanding under diverse instruction
formats. InternSpatial comprises 12 million QA pairs spanning both single-view
and multi-view settings, drawn from diverse visual environments and supporting
19 instruction formats that reflect varied query styles. For evaluation, we
propose InternSpatial-Bench for single-view tasks and expand multi-view
reasoning by introducing a novel rotation angle prediction task that has not
been explored in prior work. Experimental results show that models trained on
InternSpatial achieve 12.1% improvement on InternSpatial-Bench and 10.7% on
VSI-Bench, while maintaining strong performance on general-purpose benchmarks.
We hope these resources will support the development of spatially capable VLMs
in practical applications such as robotics and embodied AI.

</details>


### [201] [Distributed Poisson multi-Bernoulli filtering via generalised covariance intersection](https://arxiv.org/abs/2506.18397)
*ngel F. Garca-Fernndez,Giorgio Battistelli*

Main category: cs.CV

TL;DR: The paper introduces a distributed PMB filter using GCI fusion for multi-object tracking, approximating intractable exact fusion with a principled method, yielding a PMBM form with closed-form expression and practical benefits.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of intractable exact GCI fusion of PMB densities in distributed multi-object filtering, the paper proposes a principled approximation.

Method: The method approximates the power of a PMB density as an unnormalised PMB density, applies GCI fusion as a normalised product, and preserves the PMBM form for future steps.

Result: The approach results in a closed-form PMBM, which outperforms other distributed multi-object filters in experiments.

Conclusion: The proposed approximation effectively handles GCI fusion for PMB densities, offering a practical and superior solution for distributed multi-object filtering.

Abstract: This paper presents the distributed Poisson multi-Bernoulli (PMB) filter
based on the generalised covariance intersection (GCI) fusion rule for
distributed multi-object filtering. Since the exact GCI fusion of two PMB
densities is intractable, we derive a principled approximation. Specifically,
we approximate the power of a PMB density as an unnormalised PMB density, which
corresponds to an upper bound of the PMB density. Then, the GCI fusion rule
corresponds to the normalised product of two unnormalised PMB densities. We
show that the result is a Poisson multi-Bernoulli mixture (PMBM), which can be
expressed in closed form. Future prediction and update steps in each filter
preserve the PMBM form, which can be projected back to a PMB density before the
next fusion step. Experimental results show the benefits of this approach
compared to other distributed multi-object filters.

</details>


### [202] [Latent Space Analysis for Melanoma Prevention](https://arxiv.org/abs/2506.18414)
*Ciro Listone,Aniello Murano*

Main category: cs.CV

TL;DR: The paper introduces an interpretable risk modeling method for melanoma diagnosis using a Conditional Variational Autoencoder and SVM, enhancing clinical insight and trust in AI-assisted diagnosis.


<details>
  <summary>Details</summary>
Motivation: Melanoma's aggressive nature and high mortality necessitate early, interpretable diagnostic tools, as current deep learning models offer limited clinical insight.

Method: A Conditional Variational Autoencoder learns a structured latent space for semantic lesion relationships, combined with an SVM for classification.

Result: The method provides nuanced, continuous risk assessment, strong performance in differentiating benign nevi and melanomas, and interpretable malignancy indicators.

Conclusion: The approach bridges predictive performance with clinical applicability, fostering early detection and transparent decision-making.

Abstract: Melanoma represents a critical health risk due to its aggressive progression
and high mortality, underscoring the need for early, interpretable diagnostic
tools. While deep learning has advanced in skin lesion classification, most
existing models provide only binary outputs, offering limited clinical insight.
This work introduces a novel approach that extends beyond classification,
enabling interpretable risk modelling through a Conditional Variational
Autoencoder. The proposed method learns a structured latent space that captures
semantic relationships among lesions, allowing for a nuanced, continuous
assessment of morphological differences. An SVM is also trained on this
representation effectively differentiating between benign nevi and melanomas,
demonstrating strong and consistent performance. More importantly, the learned
latent space supports visual and geometric interpretation of malignancy, with
the spatial proximity of a lesion to known melanomas serving as a meaningful
indicator of risk. This approach bridges predictive performance with clinical
applicability, fostering early detection, highlighting ambiguous cases, and
enhancing trust in AI-assisted diagnosis through transparent and interpretable
decision-making.

</details>


### [203] [Benchmarking Foundation Models and Parameter-Efficient Fine-Tuning for Prognosis Prediction in Medical Imaging](https://arxiv.org/abs/2506.18434)
*Filippo Ruffini,Elena Mulero Ayllon,Linlin Shen,Paolo Soda,Valerio Guarrasi*

Main category: cs.CV

TL;DR: A structured benchmark evaluates transferability of CNNs and Foundation Models for COVID-19 prognosis prediction, comparing fine-tuning strategies and pretrained models under diverse conditions.


<details>
  <summary>Details</summary>
Motivation: AI's potential for medical imaging prognosis is hindered by challenges in effective application, prompting the need for a robust evaluation framework.

Method: The study compares fine-tuning strategies (Full Fine-Tuning, Linear Probing, Parameter-Efficient methods) and pretrained models (general-purpose and biomedical-specific) across full-data and Few-Shot Learning scenarios.

Result: The benchmark provides insights into model adaptability and generalization under data scarcity and class imbalance, highlighting strengths and limitations of each approach.

Conclusion: The study aims to guide practical AI deployment in clinical prognosis by identifying efficient and generalizable solutions.

Abstract: Artificial Intelligence (AI) holds significant promise for improving
prognosis prediction in medical imaging, yet its effective application remains
challenging. In this work, we introduce a structured benchmark explicitly
designed to evaluate and compare the transferability of Convolutional Neural
Networks and Foundation Models in predicting clinical outcomes in COVID-19
patients, leveraging diverse publicly available Chest X-ray datasets. Our
experimental methodology extensively explores a wide set of fine-tuning
strategies, encompassing traditional approaches such as Full Fine-Tuning and
Linear Probing, as well as advanced Parameter-Efficient Fine-Tuning methods
including Low-Rank Adaptation, BitFit, VeRA, and IA3. The evaluations were
conducted across multiple learning paradigms, including both extensive
full-data scenarios and more clinically realistic Few-Shot Learning settings,
which are critical for modeling rare disease outcomes and rapidly emerging
health threats. By implementing a large-scale comparative analysis involving a
diverse selection of pretrained models, including general-purpose architectures
pretrained on large-scale datasets such as CLIP and DINOv2, to
biomedical-specific models like MedCLIP, BioMedCLIP, and PubMedCLIP, we
rigorously assess each model's capacity to effectively adapt and generalize to
prognosis tasks, particularly under conditions of severe data scarcity and
pronounced class imbalance. The benchmark was designed to capture critical
conditions common in prognosis tasks, including variations in dataset size and
class distribution, providing detailed insights into the strengths and
limitations of each fine-tuning strategy. This extensive and structured
evaluation aims to inform the practical deployment and adoption of robust,
efficient, and generalizable AI-driven solutions in real-world clinical
prognosis prediction workflows.

</details>


### [204] [Frequency-Domain Fusion Transformer for Image Inpainting](https://arxiv.org/abs/2506.18437)
*Sijin He,Guangfeng Lin,Tao Li,Yajun Chen*

Main category: cs.CV

TL;DR: A Transformer-based image inpainting method with frequency-domain fusion improves detail preservation and reduces computational costs.


<details>
  <summary>Details</summary>
Motivation: Traditional methods struggle with complex textures and large occlusions, while Transformer-based approaches often lose high-frequency details and are computationally expensive.

Method: Combines wavelet transform and Gabor filtering in attention for multi-scale modeling, uses a learnable FFT-based filter, and adopts a four-level encoder-decoder with a novel loss strategy.

Result: The method preserves more high-frequency information, enhancing inpainting quality.

Conclusion: The proposed approach effectively balances global semantics and fine details, outperforming traditional and Transformer-based methods.

Abstract: Image inpainting plays a vital role in restoring missing image regions and
supporting high-level vision tasks, but traditional methods struggle with
complex textures and large occlusions. Although Transformer-based approaches
have demonstrated strong global modeling capabilities, they often fail to
preserve high-frequency details due to the low-pass nature of self-attention
and suffer from high computational costs. To address these challenges, this
paper proposes a Transformer-based image inpainting method incorporating
frequency-domain fusion. Specifically, an attention mechanism combining wavelet
transform and Gabor filtering is introduced to enhance multi-scale structural
modeling and detail preservation. Additionally, a learnable frequency-domain
filter based on the fast Fourier transform is designed to replace the
feedforward network, enabling adaptive noise suppression and detail retention.
The model adopts a four-level encoder-decoder structure and is guided by a
novel loss strategy to balance global semantics and fine details. Experimental
results demonstrate that the proposed method effectively improves the quality
of image inpainting by preserving more high-frequency information.

</details>


### [205] [OmniGen2: Exploration to Advanced Multimodal Generation](https://arxiv.org/abs/2506.18871)
*Chenyuan Wu,Pengfei Zheng,Ruiran Yan,Shitao Xiao,Xin Luo,Yueze Wang,Wanli Li,Xiyan Jiang,Yexin Liu,Junjie Zhou,Ze Liu,Ziyi Xia,Chaofan Li,Haoge Deng,Jiahao Wang,Kun Luo,Bo Zhang,Defu Lian,Xinlong Wang,Zhongyuan Wang,Tiejun Huang,Zheng Liu*

Main category: cs.CV

TL;DR: OmniGen2 is an open-source generative model with dual decoding pathways for text and image tasks, achieving competitive results and introducing a new benchmark, OmniContext.


<details>
  <summary>Details</summary>
Motivation: To provide a unified solution for diverse generation tasks while preserving text generation capabilities and improving multimodal understanding.

Method: Features two distinct decoding pathways (text and image) with unshared parameters, a decoupled image tokenizer, and a reflection mechanism for image tasks. Comprehensive data pipelines were developed for training.

Result: Competitive performance on text-to-image and image editing benchmarks, with state-of-the-art consistency in subject-driven tasks (OmniContext benchmark).

Conclusion: OmniGen2 is a versatile and efficient model, with plans to release models, code, and datasets to support future research.

Abstract: In this work, we introduce OmniGen2, a versatile and open-source generative
model designed to provide a unified solution for diverse generation tasks,
including text-to-image, image editing, and in-context generation. Unlike
OmniGen v1, OmniGen2 features two distinct decoding pathways for text and image
modalities, utilizing unshared parameters and a decoupled image tokenizer. This
design enables OmniGen2 to build upon existing multimodal understanding models
without the need to re-adapt VAE inputs, thereby preserving the original text
generation capabilities. To facilitate the training of OmniGen2, we developed
comprehensive data construction pipelines, encompassing image editing and
in-context generation data. Additionally, we introduce a reflection mechanism
tailored for image generation tasks and curate a dedicated reflection dataset
based on OmniGen2. Despite its relatively modest parameter size, OmniGen2
achieves competitive results on multiple task benchmarks, including
text-to-image and image editing. To further evaluate in-context generation,
also referred to as subject-driven tasks, we introduce a new benchmark named
OmniContext. OmniGen2 achieves state-of-the-art performance among open-source
models in terms of consistency. We will release our models, training code,
datasets, and data construction pipeline to support future research in this
field. Project Page: https://vectorspacelab.github.io/OmniGen2; GitHub Link:
https://github.com/VectorSpaceLab/OmniGen2

</details>


### [206] [CPAM: Context-Preserving Adaptive Manipulation for Zero-Shot Real Image Editing](https://arxiv.org/abs/2506.18438)
*Dinh-Khoi Vo,Thanh-Toan Do,Tam V. Nguyen,Minh-Triet Tran,Trung-Nghia Le*

Main category: cs.CV

TL;DR: CPAM is a zero-shot framework for editing real images using text, preserving object identity and background while enabling complex, non-rigid edits.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with preserving textures, identity, and background details during text-guided image editing.

Method: CPAM uses a preservation adaptation module for self-attention control and a localized extraction module to manage cross-attention interference, with mask-guidance strategies.

Result: CPAM outperforms state-of-the-art methods on the IMBA benchmark, preferred by human raters.

Conclusion: CPAM offers a robust, zero-shot solution for complex real image editing, preserving context and identity effectively.

Abstract: Editing natural images using textual descriptions in text-to-image diffusion
models remains a significant challenge, particularly in achieving consistent
generation and handling complex, non-rigid objects. Existing methods often
struggle to preserve textures and identity, require extensive fine-tuning, and
exhibit limitations in editing specific spatial regions or objects while
retaining background details. This paper proposes Context-Preserving Adaptive
Manipulation (CPAM), a novel zero-shot framework for complicated, non-rigid
real image editing. Specifically, we propose a preservation adaptation module
that adjusts self-attention mechanisms to preserve and independently control
the object and background effectively. This ensures that the objects' shapes,
textures, and identities are maintained while keeping the background
undistorted during the editing process using the mask guidance technique.
Additionally, we develop a localized extraction module to mitigate the
interference with the non-desired modified regions during conditioning in
cross-attention mechanisms. We also introduce various mask-guidance strategies
to facilitate diverse image manipulation tasks in a simple manner. Extensive
experiments on our newly constructed Image Manipulation BenchmArk (IMBA), a
robust benchmark dataset specifically designed for real image editing,
demonstrate that our proposed method is the preferred choice among human
raters, outperforming existing state-of-the-art editing techniques.

</details>


### [207] [Vision as a Dialect: Unifying Visual Understanding and Generation via Text-Aligned Representations](https://arxiv.org/abs/2506.18898)
*Jiaming Han,Hao Chen,Yang Zhao,Hanyu Wang,Qi Zhao,Ziyan Yang,Hao He,Xiangyu Yue,Lu Jiang*

Main category: cs.CV

TL;DR: A multimodal framework unifies visual understanding and generation using a shared discrete semantic representation with a text-aligned tokenizer and two de-tokenizers for diverse decoding needs.


<details>
  <summary>Details</summary>
Motivation: To integrate vision and text into a unified space without modality-specific designs, improving cross-modal input and output.

Method: Uses Text-Aligned Tokenizer (TA-Tok) for image-to-token conversion, scale-adaptive encoding/decoding, and two de-tokenizers (autoregressive and diffusion-based). Advanced pre-training tasks enhance modality fusion.

Result: Tar matches or surpasses existing multimodal LLM methods, achieving faster convergence and greater training efficiency.

Conclusion: The framework successfully unifies vision and text, enabling efficient cross-modal tasks with high-fidelity outputs.

Abstract: This paper presents a multimodal framework that attempts to unify visual
understanding and generation within a shared discrete semantic representation.
At its core is the Text-Aligned Tokenizer (TA-Tok), which converts images into
discrete tokens using a text-aligned codebook projected from a large language
model's (LLM) vocabulary. By integrating vision and text into a unified space
with an expanded vocabulary, our multimodal LLM, Tar, enables cross-modal input
and output through a shared interface, without the need for modality-specific
designs. Additionally, we propose scale-adaptive encoding and decoding to
balance efficiency and visual detail, along with a generative de-tokenizer to
produce high-fidelity visual outputs. To address diverse decoding needs, we
utilize two complementary de-tokenizers: a fast autoregressive model and a
diffusion-based model. To enhance modality fusion, we investigate advanced
pre-training tasks, demonstrating improvements in both visual understanding and
generation. Experiments across benchmarks show that Tar matches or surpasses
existing multimodal LLM methods, achieving faster convergence and greater
training efficiency. Code, models, and data are available at
https://tar.csuhan.com

</details>


### [208] [DIP: Unsupervised Dense In-Context Post-training of Visual Representations](https://arxiv.org/abs/2506.18463)
*Sophia Sirko-Galouchenko,Spyros Gidaris,Antonin Vobecky,Andrei Bursuc,Nicolas Thome*

Main category: cs.CV

TL;DR: DIP is an unsupervised post-training method for improving dense image representations in pretrained vision encoders, using pseudo-tasks inspired by meta-learning. It outperforms prior methods and is computationally efficient.


<details>
  <summary>Details</summary>
Motivation: To enhance dense image representations for in-context scene understanding without relying on complex self-distillation architectures or labeled data.

Method: Trains vision encoders using pseudo-tasks simulating downstream scenarios, generated automatically by combining a pretrained diffusion model and the encoder itself.

Result: Achieves strong performance across downstream tasks, outperforming initial encoders and prior methods.

Conclusion: DIP offers a practical, efficient solution for improving dense representations in vision encoders.

Abstract: We introduce DIP, a novel unsupervised post-training method designed to
enhance dense image representations in large-scale pretrained vision encoders
for in-context scene understanding. Unlike prior approaches that rely on
complex self-distillation architectures, our method trains the vision encoder
using pseudo-tasks that explicitly simulate downstream in-context scenarios,
inspired by meta-learning principles. To enable post-training on unlabeled
data, we propose an automatic mechanism for generating in-context tasks that
combines a pretrained diffusion model and the vision encoder itself. DIP is
simple, unsupervised, and computationally efficient, requiring less than 9
hours on a single A100 GPU. By learning dense representations through pseudo
in-context tasks, it achieves strong performance across a wide variety of
downstream real-world in-context scene understanding tasks. It outperforms both
the initial vision encoder and prior methods, offering a practical and
effective solution for improving dense representations. Code available here:
https://github.com/sirkosophia/DIP

</details>


### [209] [AViLA: Asynchronous Vision-Language Agent for Streaming Multimodal Data Interaction](https://arxiv.org/abs/2506.18472)
*Gengyuan Zhang,Tanveer Hannan,Hermine Kleiner,Beste Aydemir,Xinyu Xie,Jian Lan,Thomas Seidl,Volker Tresp,Jindong Gu*

Main category: cs.CV

TL;DR: The paper introduces AViLA, a video-language agent for handling asynchronous queries and evidence in streaming data, improving accuracy and temporal awareness.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of Query-Evidence Asynchrony in real-world applications like autonomous driving, where evidence for queries arrives asynchronously, requiring temporal awareness and reasoning.

Method: Proposes AViLA with three modules: memory retention, evidence identification, and evidence-grounded trigger, to manage streaming data and ad-hoc queries.

Result: AViLA outperforms existing models in accuracy and temporal awareness, addressing the asynchrony challenge effectively.

Conclusion: AViLA demonstrates significant improvements in handling streaming data interactions, offering a robust solution for real-world applications.

Abstract: An ideal vision-language agent serves as a bridge between the human users and
their surrounding physical world in real-world applications like autonomous
driving and embodied agents, and proactively provides accurate and timely
responses given user intents. An intriguing challenge arises when agents
interact with the world as a dynamic data stream and ad-hoc queries from users:
supporting knowledge for queries, namely evidence, usually appears
asynchronously with the arrival time of queries, and agents need to ground
their responses in historical data, present observations, and even future
streams. We frame this challenge as Query-Evidence Asynchrony, where user
queries and their supporting evidence typically arrive asynchronously in the
streaming setting. This setting requires not only strong reasoning capabilities
but also the ability to retain past observations and respond to queries with
temporal awareness. In this paper, we introduce a diagnostic benchmark that
evaluates Multimodal Large Language Models (MLLMs) on their ability to handle
interaction with streaming data. Further, we present AViLA, Asynchronous
Video-Language Agent for streaming data interaction that can handle ad-hoc
queries and give time-aware responses. For this purpose, AViLA consists of
three key modules: comprehensive memory retention, evidence identification, and
evidence-grounded trigger, that are designed to maintain a general-purpose
memory and respond readily and timely to queries. Our experiments show that
existing models often fail to respond at appropriate times, while AViLA
significantly improves both accuracy and temporal awareness. Our code and
dataset will be publicly available.

</details>


### [210] [Context Consistency Learning via Sentence Removal for Semi-Supervised Video Paragraph Grounding](https://arxiv.org/abs/2506.18476)
*Yaokun Zhong,Siyu Jiang,Jian Zhu,Jian-Fang Hu*

Main category: cs.CV

TL;DR: The paper introduces Context Consistency Learning (CCL) for Semi-Supervised Video Paragraph Grounding (SSVPG), improving localization of sentences in videos with limited annotations by combining consistency regularization and pseudo-labeling.


<details>
  <summary>Details</summary>
Motivation: Existing methods neglect perturbing query contexts for strong supervisory signals, limiting performance in SSVPG tasks.

Method: Proposes CCL, unifying consistency regularization and pseudo-labeling. Uses teacher-student learning with strong augmentation and retrains with pseudo labels based on mutual agreement.

Result: CCL significantly outperforms existing methods in experiments.

Conclusion: CCL effectively enhances semi-supervised learning for video paragraph grounding by leveraging strong supervisory signals and pseudo-labeling.

Abstract: Semi-Supervised Video Paragraph Grounding (SSVPG) aims to localize multiple
sentences in a paragraph from an untrimmed video with limited temporal
annotations. Existing methods focus on teacher-student consistency learning and
video-level contrastive loss, but they overlook the importance of perturbing
query contexts to generate strong supervisory signals. In this work, we propose
a novel Context Consistency Learning (CCL) framework that unifies the paradigms
of consistency regularization and pseudo-labeling to enhance semi-supervised
learning. Specifically, we first conduct teacher-student learning where the
student model takes as inputs strongly-augmented samples with sentences removed
and is enforced to learn from the adequately strong supervisory signals from
the teacher model. Afterward, we conduct model retraining based on the
generated pseudo labels, where the mutual agreement between the original and
augmented views' predictions is utilized as the label confidence. Extensive
experiments show that CCL outperforms existing methods by a large margin.

</details>


### [211] [GANs vs. Diffusion Models for virtual staining with the HER2match dataset](https://arxiv.org/abs/2506.18484)
*Pascal Klckner,Jos Teixeira,Diana Montezuma,Jaime S. Cardoso,Hugo M. Horlings,Sara P. Oliveira*

Main category: cs.CV

TL;DR: The paper introduces HER2match, the first public dataset for H&E-HER2 staining transfer, compares GANs and Diffusion Models, and highlights GANs' superior performance, with BBDM as an exception.


<details>
  <summary>Details</summary>
Motivation: The lack of public datasets and unclear best model frameworks for H&E-HER2 staining transfer motivated this research.

Method: The authors compare GANs and Diffusion Models, including a novel Brownian Bridge Diffusion Model, using the HER2match dataset.

Result: GANs generally outperform DMs, with BBDM being the only comparable DM. Data alignment significantly improves results.

Conclusion: The HER2match dataset and framework comparison provide valuable resources and guidance for future research in virtual staining.

Abstract: Virtual staining is a promising technique that uses deep generative models to
recreate histological stains, providing a faster and more cost-effective
alternative to traditional tissue chemical staining. Specifically for H&E-HER2
staining transfer, despite a rising trend in publications, the lack of
sufficient public datasets has hindered progress in the topic. Additionally, it
is currently unclear which model frameworks perform best for this particular
task. In this paper, we introduce the HER2match dataset, the first publicly
available dataset with the same breast cancer tissue sections stained with both
H&E and HER2. Furthermore, we compare the performance of several Generative
Adversarial Networks (GANs) and Diffusion Models (DMs), and implement a novel
Brownian Bridge Diffusion Model for H&E-HER2 translation. Our findings indicate
that, overall, GANs perform better than DMs, with only the BBDM achieving
comparable results. Furthermore, we emphasize the importance of data alignment,
as all models trained on HER2match produced vastly improved visuals compared to
the widely used consecutive-slide BCI dataset. This research provides a new
high-quality dataset ([available upon publication acceptance]), improving both
model training and evaluation. In addition, our comparison of frameworks offers
valuable guidance for researchers working on the topic.

</details>


### [212] [ShowFlow: From Robust Single Concept to Condition-Free Multi-Concept Generation](https://arxiv.org/abs/2506.18493)
*Trong-Vu Hoang,Quang-Binh Nguyen,Thanh-Toan Do,Tam V. Nguyen,Minh-Triet Tran,Trung-Nghia Le*

Main category: cs.CV

TL;DR: ShowFlow is a framework for customizable image generation, addressing challenges in single- and multi-concept scenarios with specialized modules (ShowFlow-S and ShowFlow-M).


<details>
  <summary>Details</summary>
Motivation: Challenges in identity preservation and prompt alignment in single- and multi-concept image generation motivate the need for a robust solution.

Method: ShowFlow-S uses a KronA-WED adapter and disentangled learning; ShowFlow-M reuses ShowFlow-S models with SAMA and layout consistency for multi-concept generation.

Result: Experiments and user studies confirm ShowFlow's effectiveness in real-world applications like advertising and virtual dressing.

Conclusion: ShowFlow provides a scalable solution for customizable image generation, excelling in both single- and multi-concept scenarios.

Abstract: Customizing image generation remains a core challenge in controllable image
synthesis. For single-concept generation, maintaining both identity
preservation and prompt alignment is challenging. In multi-concept scenarios,
relying solely on a prompt without additional conditions like layout boxes or
semantic masks, often leads to identity loss and concept omission. In this
paper, we introduce ShowFlow, a comprehensive framework designed to tackle
these challenges. We propose ShowFlow-S for single-concept image generation,
and ShowFlow-M for handling multiple concepts. ShowFlow-S introduces a
KronA-WED adapter, which integrates a Kronecker adapter with weight and
embedding decomposition, and employs a disentangled learning approach with a
novel attention regularization objective to enhance single-concept generation.
Building on this foundation, ShowFlow-M directly reuses the learned models from
ShowFlow-S to support multi-concept generation without extra conditions,
incorporating a Subject-Adaptive Matching Attention (SAMA) and a layout
consistency strategy as the plug-and-play module. Extensive experiments and
user studies validate ShowFlow's effectiveness, highlighting its potential in
real-world applications like advertising and virtual dressing.

</details>


### [213] [Biased Teacher, Balanced Student](https://arxiv.org/abs/2506.18496)
*Seonghak Kim*

Main category: cs.CV

TL;DR: LTKD addresses bias in Knowledge Distillation for long-tailed data by decomposing the standard KD objective into inter-group and intra-group KL divergence, introducing rebalanced and uniform losses to improve tail-class performance.


<details>
  <summary>Details</summary>
Motivation: Conventional KD fails in long-tailed data due to teacher bias toward head classes, limiting supervision for tail classes.

Method: LTKD reformulates KD into inter-group and intra-group KL divergence, using rebalanced inter-group loss and uniform intra-group loss to mitigate bias.

Result: LTKD outperforms existing KD methods on CIFAR-100-LT, TinyImageNet-LT, and ImageNet-LT, improving overall accuracy and tail-class performance.

Conclusion: LTKD effectively transfers knowledge from biased teachers, making it suitable for resource-constrained and imbalanced real-world scenarios.

Abstract: Knowledge Distillation (KD) is a widely adopted model compression technique
where a compact student model learns from the output of a larger, pre-trained
teacher. While effective in balanced settings, conventional KD suffers
significantly when applied to long-tailed data distributions, as the teacher
model tends to be biased toward head classes and provides limited supervision
for tail classes. In this paper, we propose Long-Tailed Knowledge Distillation
(LTKD), a novel framework tailored for class-imbalanced scenarios. We begin by
reformulating the standard KD objective into two components: inter-group and
intra-group Kullback-Leibler (KL) divergence, corresponding to the prediction
distributions across and within class groups (head, medium, tail),
respectively. This decomposition allows us to identify and quantify the sources
of teacher bias. To address them, we introduce (1) a rebalanced inter-group
loss that calibrates the teacher's group-level predictions and (2) a uniform
intra-group loss that ensures equal contribution from all groups during
distillation. Extensive experiments on CIFAR-100-LT, TinyImageNet-LT, and
ImageNet-LT show that LTKD consistently outperforms existing KD methods,
achieving significant gains in both overall accuracy and tail-class
performance. Our results demonstrate that LTKD enables effective knowledge
transfer even from biased teachers, making it a strong candidate for real-world
deployment in resource-constrained and imbalanced settings.

</details>


### [214] [Generalizing Vision-Language Models to Novel Domains: A Comprehensive Survey](https://arxiv.org/abs/2506.18504)
*Xinyao Li,Jingjing Li,Fengling Li,Lei Zhu,Yang Yang,Heng Tao Shen*

Main category: cs.CV

TL;DR: A survey on vision-language models (VLMs) explores their generalization challenges, methodologies, and benchmarks, comparing them with multimodal large language models (MLLMs).


<details>
  <summary>Details</summary>
Motivation: VLMs show strong zero-shot capabilities but struggle with domain-specific tasks, prompting research into transferring their knowledge to downstream applications.

Method: Categorizes literature into prompt-based, parameter-based, and feature-based methods, analyzing transfer learning settings and benchmarks.

Result: Summarizes performance comparisons and discusses the evolution from VLMs to MLLMs like DeepSeek-VL.

Conclusion: Provides a clear landscape of current and future multimodal research, emphasizing generalization challenges and advancements.

Abstract: Recently, vision-language pretraining has emerged as a transformative
technique that integrates the strengths of both visual and textual modalities,
resulting in powerful vision-language models (VLMs). Leveraging web-scale
pretraining data, these models exhibit strong zero-shot capabilities. However,
their performance often deteriorates when confronted with domain-specific or
specialized generalization tasks. To address this, a growing body of research
focuses on transferring or generalizing the rich knowledge embedded in VLMs to
various downstream applications. This survey aims to comprehensively summarize
the generalization settings, methodologies, benchmarking and results in VLM
literatures. Delving into the typical VLM structures, current literatures are
categorized into prompt-based, parameter-based and feature-based methods
according to the transferred modules. The differences and characteristics in
each category are furthered summarized and discussed by revisiting the typical
transfer learning (TL) settings, providing novel interpretations for TL in the
era of VLMs. Popular benchmarks for VLM generalization are further introduced
with thorough performance comparisons among the reviewed methods. Following the
advances in large-scale generalizable pretraining, this survey also discusses
the relations and differences between VLMs and up-to-date multimodal large
language models (MLLM), e.g., DeepSeek-VL. By systematically reviewing the
surging literatures in vision-language research from a novel and practical
generalization prospective, this survey contributes to a clear landscape of
current and future multimodal researches.

</details>


### [215] [MedTVT-R1: A Multimodal LLM Empowering Medical Reasoning and Diagnosis](https://arxiv.org/abs/2506.18512)
*Yuting Zhang,Kaishen Yuan,Hao Lu,Yutao Yue,Jintai Chen,Kaishun Wu*

Main category: cs.CV

TL;DR: MedTVT-R1 is a multimodal large language model integrating clinical data for multi-disease diagnosis, outperforming single-modal approaches with its modality perception and reinforcement fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of accurate, interpretable multi-disease diagnosis using heterogeneous multimodal medical data, which current single-modal methods struggle with.

Method: Proposes MedTVT-R1, a framework with a modality perception layer and GRPO-based reinforcement fine-tuning, using the MedTVT-QA dataset for training.

Result: Demonstrates superior performance in multimodal feature utilization and multi-disease diagnosis, with clinical application potential.

Conclusion: MedTVT-R1 offers a robust solution for multimodal medical data integration, enhancing diagnostic accuracy and interpretability.

Abstract: Accurate and interpretable multi-disease diagnosis remains a critical
challenge in medical research, particularly when leveraging heterogeneous
multimodal medical data. Current approaches often rely on single-modal data,
limiting their ability to comprehensively understand complex diseases. To
address this, we propose MedTVT-R1, a novel Multimodal Large Language Model
(MLLM) framework designed to integrate clinical multimodal data for reasoning
and diagnosing multiple diseases. We construct MedTVT-QA, a curated instruction
dataset that provides question-answer pairs for physiological-level
interpretations and disease-level diagnoses with a Chain of Evidence approach.
MedTVT-R1 incorporates a modality perception layer to capture inter-modal
dependencies and adaptively weight modality contributions. Additionally, we
employ Group Relative Policy Optimization (GRPO)-based Reinforcement
Fine-Tuning with a Jaccard Reward function to enhance diagnostic reasoning.
Experimental results demonstrate MedTVT-R1's superiority in multimodal feature
utilization and multi-disease diagnosis, offering significant potential for
clinical applications such as diagnostic report generation and comorbidity
reasoning. The dataset and code are available at
https://github.com/keke-nice/MedTVT-R1.

</details>


### [216] [Enhancing Image Restoration Transformer via Adaptive Translation Equivariance](https://arxiv.org/abs/2506.18520)
*JiaKui Hu,Zhengjian Yao,Lujia Jin,Hangzhou He,Yanye Lu*

Main category: cs.CV

TL;DR: The paper introduces TEAFormer, a transformer-based model for image restoration that preserves translation equivariance using adaptive sliding indexing and component stacking, improving performance and generalization.


<details>
  <summary>Details</summary>
Motivation: Modern restoration transformers lose translation equivariance due to attention mechanisms, harming training and generalization. The paper aims to restore this property.

Method: Proposes slide indexing and component stacking for equivariance, and introduces adaptive sliding indexing to balance computational cost and receptive field.

Result: TEAFormer outperforms in effectiveness, convergence, and generalization across image restoration tasks.

Conclusion: The adaptive sliding indexing in TEAFormer successfully maintains translation equivariance while addressing computational and receptive field challenges.

Abstract: Translation equivariance is a fundamental inductive bias in image
restoration, ensuring that translated inputs produce translated outputs.
Attention mechanisms in modern restoration transformers undermine this
property, adversely impacting both training convergence and generalization. To
alleviate this issue, we propose two key strategies for incorporating
translation equivariance: slide indexing and component stacking. Slide indexing
maintains operator responses at fixed positions, with sliding window attention
being a notable example, while component stacking enables the arrangement of
translation-equivariant operators in parallel or sequentially, thereby building
complex architectures while preserving translation equivariance. However, these
strategies still create a dilemma in model design between the high
computational cost of self-attention and the fixed receptive field associated
with sliding window attention. To address this, we develop an adaptive sliding
indexing mechanism to efficiently select key-value pairs for each query, which
are then concatenated in parallel with globally aggregated key-value pairs. The
designed network, called the Translation Equivariance Adaptive Transformer
(TEAFormer), is assessed across a variety of image restoration tasks. The
results highlight its superiority in terms of effectiveness, training
convergence, and generalization.

</details>


### [217] [Multi-Scale Representation of Follicular Lymphoma Pathology Images in a Single Hyperbolic Space](https://arxiv.org/abs/2506.18523)
*Kei Taguchi,Kazumasa Ohara,Tatsuya Yokota,Hiroaki Miyoshi,Noriaki Hashimoto,Ichiro Takeuchi,Hidekata Hontani*

Main category: cs.CV

TL;DR: A method for embedding malignant lymphoma pathology images in hyperbolic space using self-supervised learning to capture hierarchical relationships across scales.


<details>
  <summary>Details</summary>
Motivation: To represent morphological changes across scales (cell nuclei to tissue) during disease progression in a unified way.

Method: Uses self-supervised learning to embed tissue and nucleus images in a hyperbolic space (Poincar ball) based on inclusion relationships.

Result: Learned representations effectively encode hierarchical structure, capturing disease state and cell type variations.

Conclusion: Hyperbolic space and self-supervised learning enable unified representation of multi-scale pathology images, improving disease analysis.

Abstract: We propose a method for representing malignant lymphoma pathology images,
from high-resolution cell nuclei to low-resolution tissue images, within a
single hyperbolic space using self-supervised learning. To capture
morphological changes that occur across scales during disease progression, our
approach embeds tissue and corresponding nucleus images close to each other
based on inclusion relationships. Using the Poincar\'e ball as the feature
space enables effective encoding of this hierarchical structure. The learned
representations capture both disease state and cell type variations.

</details>


### [218] [Auto-Regressively Generating Multi-View Consistent Images](https://arxiv.org/abs/2506.18527)
*JiaKui Hu,Yuxiao Yang,Jialun Liu,Jinbo Wu,Chen Zhao,Yanye Lu*

Main category: cs.CV

TL;DR: The paper introduces MV-AR, an auto-regressive method for generating consistent multi-view images from diverse prompts, addressing challenges like view consistency and multi-modal conditions.


<details>
  <summary>Details</summary>
Motivation: To enable efficient 3D content creation by generating multi-view images from human instructions while maintaining consistency and handling diverse conditions.

Method: Uses an auto-regressive model for progressive multi-view synthesis, condition injection modules for various prompts, and a progressive training strategy. Introduces 'Shuffle View' data augmentation to mitigate overfitting.

Result: MV-AR generates consistent multi-view images across diverse conditions, performing comparably to diffusion-based models.

Conclusion: MV-AR is a versatile and effective solution for multi-view image generation, with potential applications in 3D content creation.

Abstract: Generating multi-view images from human instructions is crucial for 3D
content creation. The primary challenges involve maintaining consistency across
multiple views and effectively synthesizing shapes and textures under diverse
conditions. In this paper, we propose the Multi-View Auto-Regressive (MV-AR)
method, which leverages an auto-regressive model to progressively generate
consistent multi-view images from arbitrary prompts. Firstly, the
next-token-prediction capability of the AR model significantly enhances its
effectiveness in facilitating progressive multi-view synthesis. When generating
widely-separated views, MV-AR can utilize all its preceding views to extract
effective reference information. Subsequently, we propose a unified model that
accommodates various prompts via architecture designing and training
strategies. To address multiple conditions, we introduce condition injection
modules for text, camera pose, image, and shape. To manage multi-modal
conditions simultaneously, a progressive training strategy is employed. This
strategy initially adopts the text-to-multi-view (t2mv) model as a baseline to
enhance the development of a comprehensive X-to-multi-view (X2mv) model through
the randomly dropping and combining conditions. Finally, to alleviate the
overfitting problem caused by limited high-quality data, we propose the
"Shuffle View" data augmentation technique, thus significantly expanding the
training data by several magnitudes. Experiments demonstrate the performance
and versatility of our MV-AR, which consistently generates consistent
multi-view images across a range of conditions and performs on par with leading
diffusion-based multi-view image generation models. Code and models will be
released at https://github.com/MILab-PKU/MVAR.

</details>


### [219] [A Set-to-Set Distance Measure in Hyperbolic Space](https://arxiv.org/abs/2506.18529)
*Pengxiang Li,Wei Wu,Zhi Gao,Xiaomeng Fan,Peilin Yu,Yuwei Wu,Zhipeng Lu,Yunde Jia,Mehrtash Harandi*

Main category: cs.CV

TL;DR: Proposes HS2SD, a hyperbolic set-to-set distance measure integrating global and local structural information for better dissimilarity computation in hyperbolic space.


<details>
  <summary>Details</summary>
Motivation: Existing point-to-point distances in hyperbolic space lack the ability to compare sets effectively, where local and global structures are semantically important.

Method: HS2SD combines geodesic distances between Einstein midpoints (global) and topological differences (local) using Thue-Morse sequences for approximation.

Result: Outperforms existing methods in entity matching and image classification tasks by capturing hierarchical relationships.

Conclusion: HS2SD provides a nuanced understanding of hyperbolic set relationships, enhancing performance in real-world applications.

Abstract: We propose a hyperbolic set-to-set distance measure for computing
dissimilarity between sets in hyperbolic space. While point-to-point distances
in hyperbolic space effectively capture hierarchical relationships between data
points, many real-world applications require comparing sets of hyperbolic data
points, where the local structure and the global structure of the sets carry
crucial semantic information. The proposed the \underline{h}yperbolic
\underline{s}et-\underline{to}-\underline{s}et \underline{d}istance measure
(HS2SD) integrates both global and local structural information: global
structure through geodesic distances between Einstein midpoints of hyperbolic
sets, and local structure through topological characteristics of the two sets.
To efficiently compute topological differences, we prove that using a finite
Thue-Morse sequence of degree and adjacency matrices can serve as a robust
approximation to capture the topological structure of a set. In this case, by
considering the topological differences, HS2SD provides a more nuanced
understanding of the relationships between two hyperbolic sets. Empirical
evaluation on entity matching, standard image classification, and few-shot
image classification demonstrates that our distance measure outperforms
existing methods by effectively modeling the hierarchical and complex
relationships inherent in hyperbolic sets.

</details>


### [220] [Geometry-aware Distance Measure for Diverse Hierarchical Structures in Hyperbolic Spaces](https://arxiv.org/abs/2506.18533)
*Pengxiang Li,Yuwei Wu,Zhi Gao,Xiaomeng Fan,Wei Wu,Zhipeng Lu,Yunde Jia,Mehrtash Harandi*

Main category: cs.CV

TL;DR: Proposes a geometry-aware distance measure in hyperbolic spaces, adapting to diverse hierarchical structures, outperforming fixed-distance methods.


<details>
  <summary>Details</summary>
Motivation: Existing hyperbolic learning methods assume uniform hierarchy, which is restrictive for diverse real-world data structures.

Method: Introduces dynamic distance measures via tailored projections and curvatures, with low-rank decomposition and hard-pair mining for efficiency.

Result: Outperforms fixed-distance methods, achieving over 5% gains on few-shot learning tasks like mini-ImageNet.

Conclusion: Adaptive distance measures better capture hierarchical diversity, improving class boundaries and prototype separation.

Abstract: Learning in hyperbolic spaces has attracted increasing attention due to its
superior ability to model hierarchical structures of data. Most existing
hyperbolic learning methods use fixed distance measures for all data, assuming
a uniform hierarchy across all data points. However, real-world hierarchical
structures exhibit significant diversity, making this assumption overly
restrictive. In this paper, we propose a geometry-aware distance measure in
hyperbolic spaces, which dynamically adapts to varying hierarchical structures.
Our approach derives the distance measure by generating tailored projections
and curvatures for each pair of data points, effectively mapping them to an
appropriate hyperbolic space. We introduce a revised low-rank decomposition
scheme and a hard-pair mining mechanism to mitigate the computational cost of
pair-wise distance computation without compromising accuracy. We present an
upper bound on the low-rank approximation error using Talagrand's concentration
inequality, ensuring theoretical robustness. Extensive experiments on standard
image classification (MNIST, CIFAR-10 and CIFAR-100), hierarchical
classification (5-level CIFAR-100), and few-shot learning tasks (mini-ImageNet,
tiered-ImageNet) demonstrate the effectiveness of our method. Our approach
consistently outperforms learning methods that use fixed distance measures,
with notable improvements on few-shot learning tasks, where it achieves over
5\% gains on mini-ImageNet. The results reveal that adaptive distance measures
better capture diverse hierarchical structures, with visualization showing
clearer class boundaries and improved prototype separation in hyperbolic
spaces.

</details>


### [221] [Normality Prior Guided Multi-Semantic Fusion Network for Unsupervised Image Anomaly Detection](https://arxiv.org/abs/2506.18544)
*Muhao Xu,Xueying Zhou,Xizhan Gao,Weiye Song,Guang Feng,Sijie Niu*

Main category: cs.CV

TL;DR: The paper proposes a multi-semantic fusion network for unsupervised anomaly detection, leveraging normality priors to improve reconstruction fidelity and outperforming existing methods on the MVTec LOCO AD dataset.


<details>
  <summary>Details</summary>
Motivation: Logical anomalies are harder to detect than structural ones due to their local resemblance to normal semantics but global deviation. Existing methods fail as anomalies propagate through low-dimensional bottlenecks, leading to misleading reconstructions.

Method: A normality prior guided multi-semantic fusion network is introduced. It extracts global semantics from normal samples using a pre-trained vision-language network, constructs semantic codebooks via vector quantization, and fuses these features to guide anomaly reconstruction.

Result: The method achieves state-of-the-art performance on MVTec LOCO AD, improving pixel-sPRO by 5.7% and image-AUROC by 2.6%.

Conclusion: The proposed approach effectively addresses the challenge of logical anomaly detection by integrating multi-semantic features of normal samples, enhancing reconstruction fidelity and detection accuracy.

Abstract: Recently, detecting logical anomalies is becoming a more challenging task
compared to detecting structural ones. Existing encoder decoder based methods
typically compress inputs into low-dimensional bottlenecks on the assumption
that the compression process can effectively suppress the transmission of
logical anomalies to the decoder. However, logical anomalies present a
particular difficulty because, while their local features often resemble normal
semantics, their global semantics deviate significantly from normal patterns.
Thanks to the generalisation capabilities inherent in neural networks, these
abnormal semantic features can propagate through low-dimensional bottlenecks.
This ultimately allows the decoder to reconstruct anomalous images with
misleading fidelity. To tackle the above challenge, we propose a novel
normality prior guided multi-semantic fusion network for unsupervised anomaly
detection. Instead of feeding the compressed bottlenecks to the decoder
directly, we introduce the multi-semantic features of normal samples into the
reconstruction process. To this end, we first extract abstract global semantics
of normal cases by a pre-trained vision-language network, then the learnable
semantic codebooks are constructed to store representative feature vectors of
normal samples by vector quantisation. Finally, the above multi-semantic
features are fused and employed as input to the decoder to guide the
reconstruction of anomalies to approximate normality. Extensive experiments are
conducted to validate the effectiveness of our proposed method, and it achieves
the SOTA performance on the MVTec LOCO AD dataset with improvements of 5.7% in
pixel-sPRO and 2.6% in image-AUROC. The source code is available at
https://github.com/Xmh-L/NPGMF.

</details>


### [222] [Object-aware Sound Source Localization via Audio-Visual Scene Understanding](https://arxiv.org/abs/2506.18557)
*Sung Jin Um,Dongjin Kim,Sangmin Lee,Jung Uk Kim*

Main category: cs.CV

TL;DR: A novel framework using Multimodal Large Language Models (MLLMs) improves sound source localization by distinguishing sound-making objects from silent ones, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail in complex scenes due to reliance on simple audio-visual correspondence, lacking fine-grained semantic distinctions.

Method: Proposes MLLMs for contextual information and introduces Object-aware Contrastive Alignment (OCA) and Object Region Isolation (ORI) losses.

Result: Outperforms existing methods on MUSIC and VGGSound datasets in single- and multi-source localization.

Conclusion: The framework effectively addresses limitations of prior methods, enhancing localization accuracy in complex scenes.

Abstract: Audio-visual sound source localization task aims to spatially localize
sound-making objects within visual scenes by integrating visual and audio cues.
However, existing methods struggle with accurately localizing sound-making
objects in complex scenes, particularly when visually similar silent objects
coexist. This limitation arises primarily from their reliance on simple
audio-visual correspondence, which does not capture fine-grained semantic
differences between sound-making and silent objects. To address these
challenges, we propose a novel sound source localization framework leveraging
Multimodal Large Language Models (MLLMs) to generate detailed contextual
information that explicitly distinguishes between sound-making foreground
objects and silent background objects. To effectively integrate this detailed
information, we introduce two novel loss functions: Object-aware Contrastive
Alignment (OCA) loss and Object Region Isolation (ORI) loss. Extensive
experimental results on MUSIC and VGGSound datasets demonstrate the
effectiveness of our approach, significantly outperforming existing methods in
both single-source and multi-source localization scenarios. Code and generated
detailed contextual information are available at:
https://github.com/VisualAIKHU/OA-SSL.

</details>


### [223] [VQ-Insight: Teaching VLMs for AI-Generated Video Quality Understanding via Progressive Visual Reinforcement Learning](https://arxiv.org/abs/2506.18564)
*Xuanyu Zhang,Weiqi Li,Shijie Zhao,Junlin Li,Li Zhang,Jian Zhang*

Main category: cs.CV

TL;DR: VQ-Insight is a reasoning-style VLM framework for AIGC video quality assessment, addressing challenges like limited generalization and lack of temporal awareness. It outperforms baselines in multiple metrics.


<details>
  <summary>Details</summary>
Motivation: Current methods for evaluating AIGC-generated videos face issues like limited generalization, lack of temporal awareness, and reliance on large annotated datasets.

Method: VQ-Insight combines image quality warm-up, task-specific temporal learning, and joint optimization with video generation models, using multi-dimension scoring and preference comparison rewards.

Result: VQ-Insight outperforms state-of-the-art baselines in preference comparison, multi-dimension scoring, and natural video scoring.

Conclusion: VQ-Insight significantly improves video quality assessment and generation tasks, addressing key limitations of existing approaches.

Abstract: Recent advances in AI-generated content (AIGC) have led to the emergence of
powerful text-to-video generation models. Despite these successes, evaluating
the quality of AIGC-generated videos remains challenging due to limited
generalization, lack of temporal awareness, heavy reliance on large-scale
annotated datasets, and the lack of effective interaction with generation
models. Most current approaches rely on supervised finetuning of
vision-language models (VLMs), which often require large-scale annotated
datasets and tend to decouple understanding and generation. To address these
shortcomings, we propose VQ-Insight, a novel reasoning-style VLM framework for
AIGC video quality assessment. Our approach features: (1) a progressive video
quality learning scheme that combines image quality warm-up, general
task-specific temporal learning, and joint optimization with the video
generation model; (2) the design of multi-dimension scoring rewards, preference
comparison rewards, and temporal modeling rewards to enhance both
generalization and specialization in video quality evaluation. Extensive
experiments demonstrate that VQ-Insight consistently outperforms
state-of-the-art baselines in preference comparison, multi-dimension scoring,
and natural video scoring, bringing significant improvements for video
generation tasks.

</details>


### [224] [VisualChef: Generating Visual Aids in Cooking via Mask Inpainting](https://arxiv.org/abs/2506.18569)
*Oleh Kuzyk,Zuoyue Li,Marc Pollefeys,Xi Wang*

Main category: cs.CV

TL;DR: VisualChef is a method for generating contextual visual aids in cooking by preserving the environment while depicting actions and outcomes, simplifying alignment with mask-based grounding.


<details>
  <summary>Details</summary>
Motivation: Cooking lacks consistent visual guidance; existing methods require complex textual alignment. VisualChef aims to simplify this.

Method: Uses mask-based visual grounding to identify action-relevant objects, enabling targeted modifications while maintaining environment consistency.

Result: Outperforms state-of-the-art methods on three egocentric video datasets, both quantitatively and qualitatively.

Conclusion: VisualChef effectively generates tailored visual aids for cooking, simplifying alignment and improving over existing methods.

Abstract: Cooking requires not only following instructions but also understanding,
executing, and monitoring each step - a process that can be challenging without
visual guidance. Although recipe images and videos offer helpful cues, they
often lack consistency in focus, tools, and setup. To better support the
cooking process, we introduce VisualChef, a method for generating contextual
visual aids tailored to cooking scenarios. Given an initial frame and a
specified action, VisualChef generates images depicting both the action's
execution and the resulting appearance of the object, while preserving the
initial frame's environment. Previous work aims to integrate knowledge
extracted from large language models by generating detailed textual
descriptions to guide image generation, which requires fine-grained
visual-textual alignment and involves additional annotations. In contrast,
VisualChef simplifies alignment through mask-based visual grounding. Our key
insight is identifying action-relevant objects and classifying them to enable
targeted modifications that reflect the intended action and outcome while
maintaining a consistent environment. In addition, we propose an automated
pipeline to extract high-quality initial, action, and final state frames. We
evaluate VisualChef quantitatively and qualitatively on three egocentric video
datasets and show its improvements over state-of-the-art methods.

</details>


### [225] [2D Triangle Splatting for Direct Differentiable Mesh Training](https://arxiv.org/abs/2506.18575)
*Kaifeng Sheng,Zheng Zhou,Yingliang Peng,Qianwei Wang*

Main category: cs.CV

TL;DR: The paper introduces 2D Triangle Splatting (2DTS), a method replacing 3D Gaussian primitives with 2D triangle facelets for better rendering speed and effects like relighting, while maintaining volumetric modeling benefits.


<details>
  <summary>Details</summary>
Motivation: Current differentiable rendering with 3D Gaussian primitives faces challenges in rendering speed and advanced effects compared to mesh-based models.

Method: Proposes 2DTS, using 2D triangle facelets with a compactness parameter to train photorealistic meshes, forming a mesh-like structure.

Result: Outperforms Gaussian-based methods in fidelity and produces higher-quality meshes than existing mesh reconstruction techniques.

Conclusion: 2DTS offers a superior alternative to 3D Gaussian primitives, balancing rendering efficiency and visual quality.

Abstract: Differentiable rendering with 3D Gaussian primitives has emerged as a
powerful method for reconstructing high-fidelity 3D scenes from multi-view
images. While it offers improvements over NeRF-based methods, this
representation still encounters challenges with rendering speed and advanced
rendering effects, such as relighting and shadow rendering, compared to
mesh-based models. In this paper, we propose 2D Triangle Splatting (2DTS), a
novel method that replaces 3D Gaussian primitives with 2D triangle facelets.
This representation naturally forms a discrete mesh-like structure while
retaining the benefits of continuous volumetric modeling. By incorporating a
compactness parameter into the triangle primitives, we enable direct training
of photorealistic meshes. Our experimental results demonstrate that our
triangle-based method, in its vanilla version (without compactness tuning),
achieves higher fidelity compared to state-of-the-art Gaussian-based methods.
Furthermore, our approach produces reconstructed meshes with superior visual
quality compared to existing mesh reconstruction methods.

</details>


### [226] [Resampling Augmentation for Time Series Contrastive Learning: Application to Remote Sensing](https://arxiv.org/abs/2506.18587)
*Antoine Saget,Baptiste Lafabregue,Antoine Cornujols,Pierre Ganarski*

Main category: cs.CV

TL;DR: A novel resampling-based augmentation for contrastive learning in Satellite Image Time Series (SITS) outperforms common methods and achieves state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Leverage abundant unlabeled SITS data due to scarcity of labeled data for agricultural classification.

Method: Introduces a resampling-based augmentation strategy that upsamples time series and extracts disjoint subsequences while preserving temporal coverage.

Result: Outperforms common alternatives like jittering, resizing, and masking, achieving state-of-the-art performance on S2-Agri100 without spatial or temporal encodings.

Conclusion: The method provides a simple yet effective contrastive learning augmentation for remote sensing time series.

Abstract: Given the abundance of unlabeled Satellite Image Time Series (SITS) and the
scarcity of labeled data, contrastive self-supervised pretraining emerges as a
natural tool to leverage this vast quantity of unlabeled data. However,
designing effective data augmentations for contrastive learning remains
challenging for time series. We introduce a novel resampling-based augmentation
strategy that generates positive pairs by upsampling time series and extracting
disjoint subsequences while preserving temporal coverage. We validate our
approach on multiple agricultural classification benchmarks using Sentinel-2
imagery, showing that it outperforms common alternatives such as jittering,
resizing, and masking. Further, we achieve state-of-the-art performance on the
S2-Agri100 dataset without employing spatial information or temporal encodings,
surpassing more complex masked-based SSL frameworks. Our method offers a
simple, yet effective, contrastive learning augmentation for remote sensing
time series.

</details>


### [227] [SpaNN: Detecting Multiple Adversarial Patches on CNNs by Spanning Saliency Thresholds](https://arxiv.org/abs/2506.18591)
*Mauricio Byrd Victorica,Gyrgy Dn,Henrik Sandberg*

Main category: cs.CV

TL;DR: SpaNN is a novel attack detector for adversarial patches in neural networks, independent of patch count, outperforming existing defenses by up to 27% in accuracy.


<details>
  <summary>Details</summary>
Motivation: Current defenses against adversarial patches are limited to single-patch attacks or are computationally inefficient for multiple patches.

Method: SpaNN uses an ensemble of binarized feature maps from the first convolutional layer, performs clustering, and classifies for detection, avoiding fixed saliency thresholds.

Result: SpaNN outperforms state-of-the-art defenses by 11-27 percentage points in object detection and image classification.

Conclusion: SpaNN is robust against multi-patch attacks and computationally efficient, offering superior performance over existing methods.

Abstract: State-of-the-art convolutional neural network models for object detection and
image classification are vulnerable to physically realizable adversarial
perturbations, such as patch attacks. Existing defenses have focused,
implicitly or explicitly, on single-patch attacks, leaving their sensitivity to
the number of patches as an open question or rendering them computationally
infeasible or inefficient against attacks consisting of multiple patches in the
worst cases. In this work, we propose SpaNN, an attack detector whose
computational complexity is independent of the expected number of adversarial
patches. The key novelty of the proposed detector is that it builds an ensemble
of binarized feature maps by applying a set of saliency thresholds to the
neural activations of the first convolutional layer of the victim model. It
then performs clustering on the ensemble and uses the cluster features as the
input to a classifier for attack detection. Contrary to existing detectors,
SpaNN does not rely on a fixed saliency threshold for identifying adversarial
regions, which makes it robust against white box adversarial attacks. We
evaluate SpaNN on four widely used data sets for object detection and
classification, and our results show that SpaNN outperforms state-of-the-art
defenses by up to 11 and 27 percentage points in the case of object detection
and the case of image classification, respectively. Our code is available at
https://github.com/gerkbyrd/SpaNN.

</details>


### [228] [RDPO: Real Data Preference Optimization for Physics Consistency Video Generation](https://arxiv.org/abs/2506.18655)
*Wenxu Qian,Chaoyue Wang,Hou Peng,Zhiyu Tan,Hao Li,Anxiang Zeng*

Main category: cs.CV

TL;DR: RDPO is an annotation-free framework that improves video generation by distilling physical priors from real-world videos, enhancing physical realism and coherence.


<details>
  <summary>Details</summary>
Motivation: Current video generation lacks physical consistency, and existing methods require costly human annotations or reward models.

Method: RDPO reverse-samples real videos with a pre-trained generator to create preference pairs, then uses multi-stage iterative training to improve physical correctness.

Result: RDPO significantly enhances action coherence and physical realism in generated videos, validated by benchmarks and human evaluations.

Conclusion: RDPO offers a scalable, annotation-free solution for improving physical consistency in video generation.

Abstract: Video generation techniques have achieved remarkable advancements in visual
quality, yet faithfully reproducing real-world physics remains elusive.
Preference-based model post-training may improve physical consistency, but
requires costly human-annotated datasets or reward models that are not yet
feasible. To address these challenges, we present Real Data Preference
Optimisation (RDPO), an annotation-free framework that distills physical priors
directly from real-world videos. Specifically, the proposed RDPO
reverse-samples real video sequences with a pre-trained generator to
automatically build preference pairs that are statistically distinguishable in
terms of physical correctness. A multi-stage iterative training schedule then
guides the generator to obey physical laws increasingly well. Benefiting from
the dynamic information explored from real videos, our proposed RDPO
significantly improves the action coherence and physical realism of the
generated videos. Evaluations on multiple benchmarks and human evaluations have
demonstrated that RDPO achieves improvements across multiple dimensions. The
source code and demonstration of this paper are available at:
https://wwenxu.github.io/RDPO/

</details>


### [229] [Historical Report Guided Bi-modal Concurrent Learning for Pathology Report Generation](https://arxiv.org/abs/2506.18658)
*Ling Zhang,Boxiang Yun,Qingli Li,Yan Wang*

Main category: cs.CV

TL;DR: BiGen is a bi-modal framework for pathology report generation from WSIs, addressing semantic content lack and redundancy by integrating knowledge retrieval and concurrent learning, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenges of semantic content deficiency and information redundancy in WSIs for automated pathology report generation.

Method: Proposes a bi-modal concurrent learning framework with knowledge retrieval and cross-modal alignment between visual and textual features.

Result: Achieves 7.4% improvement in NLP metrics and 19.1% in Her-2 prediction, outperforming existing methods.

Conclusion: BiGen effectively enhances report generation by leveraging semantic content and reducing redundancy, validated by ablation studies.

Abstract: Automated pathology report generation from Whole Slide Images (WSIs) faces
two key challenges: (1) lack of semantic content in visual features and (2)
inherent information redundancy in WSIs. To address these issues, we propose a
novel Historical Report Guided \textbf{Bi}-modal Concurrent Learning Framework
for Pathology Report \textbf{Gen}eration (BiGen) emulating pathologists'
diagnostic reasoning, consisting of: (1) A knowledge retrieval mechanism to
provide rich semantic content, which retrieves WSI-relevant knowledge from
pre-built medical knowledge bank by matching high-attention patches and (2) A
bi-modal concurrent learning strategy instantiated via a learnable visual token
and a learnable textual token to dynamically extract key visual features and
retrieved knowledge, where weight-shared layers enable cross-modal alignment
between visual features and knowledge features. Our multi-modal decoder
integrates both modals for comprehensive diagnostic reports generation.
Experiments on the PathText (BRCA) dataset demonstrate our framework's
superiority, achieving state-of-the-art performance with 7.4\% relative
improvement in NLP metrics and 19.1\% enhancement in classification metrics for
Her-2 prediction versus existing methods. Ablation studies validate the
necessity of our proposed modules, highlighting our method's ability to provide
WSI-relevant rich semantic content and suppress information redundancy in WSIs.
Code is publicly available at https://github.com/DeepMed-Lab-ECNU/BiGen.

</details>


### [230] [Benchmarking histopathology foundation models in a multi-center dataset for skin cancer subtyping](https://arxiv.org/abs/2506.18668)
*Pablo Meseguer,Roco del Amor,Valery Naranjo*

Main category: cs.CV

TL;DR: A benchmark for evaluating histopathology foundation models (FMs) as patch-level feature extractors in MIL frameworks, using the AI4SkIN dataset and introducing the FM-SI metric for consistency measurement.


<details>
  <summary>Details</summary>
Motivation: To address the lack of real-world challenges for evaluating the effectiveness of diverse histopathology FMs in computational pathology.

Method: Leveraging the AI4SkIN dataset and introducing the FM-SI metric to evaluate FMs within a MIL classification framework.

Result: Extracting less biased features improves classification performance, particularly in similarity-based MIL classifiers.

Conclusion: The proposed benchmark and FM-SI metric effectively evaluate histopathology FMs, demonstrating the importance of unbiased feature extraction for better performance.

Abstract: Pretraining on large-scale, in-domain datasets grants histopathology
foundation models (FM) the ability to learn task-agnostic data representations,
enhancing transfer learning on downstream tasks. In computational pathology,
automated whole slide image analysis requires multiple instance learning (MIL)
frameworks due to the gigapixel scale of the slides. The diversity among
histopathology FMs has highlighted the need to design real-world challenges for
evaluating their effectiveness. To bridge this gap, our work presents a novel
benchmark for evaluating histopathology FMs as patch-level feature extractors
within a MIL classification framework. For that purpose, we leverage the
AI4SkIN dataset, a multi-center cohort encompassing slides with challenging
cutaneous spindle cell neoplasm subtypes. We also define the Foundation Model -
Silhouette Index (FM-SI), a novel metric to measure model consistency against
distribution shifts. Our experimentation shows that extracting less biased
features enhances classification performance, especially in similarity-based
MIL classifiers.

</details>


### [231] [MedSeg-R: Medical Image Segmentation with Clinical Reasoning](https://arxiv.org/abs/2506.18669)
*Hao Shao,Qibin Hou*

Main category: cs.CV

TL;DR: MedSeg-R is a lightweight, dual-stage framework for medical image segmentation that integrates semantic priors from medical reports to improve accuracy, especially for small lesions and overlapping structures.


<details>
  <summary>Details</summary>
Motivation: Address challenges in medical image segmentation like ambiguous boundaries, class imbalance, and poor generalization of existing methods to low-contrast or overlapping targets.

Method: Uses a dual-stage approach: cognitive stage interprets medical reports into semantic priors (location, texture, shape), and perceptual stage modulates SAM backbone with these priors via spatial attention, dynamic convolution, and deformable sampling.

Result: Significant Dice improvements in overlapping and ambiguous structures, with enhanced sensitivity to small lesions.

Conclusion: MedSeg-R effectively integrates semantic guidance, improving segmentation accuracy and compatibility with SAM-based systems.

Abstract: Medical image segmentation is challenging due to overlapping anatomies with
ambiguous boundaries and a severe imbalance between the foreground and
background classes, which particularly affects the delineation of small
lesions. Existing methods, including encoder-decoder networks and prompt-driven
variants of the Segment Anything Model (SAM), rely heavily on local cues or
user prompts and lack integrated semantic priors, thus failing to generalize
well to low-contrast or overlapping targets. To address these issues, we
propose MedSeg-R, a lightweight, dual-stage framework inspired by inspired by
clinical reasoning. Its cognitive stage interprets medical report into
structured semantic priors (location, texture, shape), which are fused via
transformer block. In the perceptual stage, these priors modulate the SAM
backbone: spatial attention highlights likely lesion regions, dynamic
convolution adapts feature filters to expected textures, and deformable
sampling refines spatial support. By embedding this fine-grained guidance
early, MedSeg-R disentangles inter-class confusion and amplifies minority-class
cues, greatly improving sensitivity to small lesions. In challenging
benchmarks, MedSeg-R produces large Dice improvements in overlapping and
ambiguous structures, demonstrating plug-and-play compatibility with SAM-based
systems.

</details>


### [232] [Reconstructing Tornadoes in 3D with Gaussian Splatting](https://arxiv.org/abs/2506.18677)
*Adam Yang,Nadula Kadawedduwa,Tianfu Wang,Maria Molina,Christopher Metzler*

Main category: cs.CV

TL;DR: The paper introduces a lab-based tornado dataset for 3D reconstruction using 3D Gaussian splatting (3DGS).


<details>
  <summary>Details</summary>
Motivation: Understanding tornadoes requires accurate 3D reconstruction, but existing datasets are lacking.

Method: A multiview dataset of a lab-based tornado is captured and released, and 3DGS is used for reconstruction.

Result: The 3D structure of the tornado is effectively reconstructed and visualized using 3DGS.

Conclusion: The dataset and method provide a valuable tool for tornado research and preparation.

Abstract: Accurately reconstructing the 3D structure of tornadoes is critically
important for understanding and preparing for this highly destructive weather
phenomenon. While modern 3D scene reconstruction techniques, such as 3D
Gaussian splatting (3DGS), could provide a valuable tool for reconstructing the
3D structure of tornados, at present we are critically lacking a controlled
tornado dataset with which to develop and validate these tools. In this work we
capture and release a novel multiview dataset of a small lab-based tornado. We
demonstrate one can effectively reconstruct and visualize the 3D structure of
this tornado using 3DGS.

</details>


### [233] [MCN-SLAM: Multi-Agent Collaborative Neural SLAM with Hybrid Implicit Neural Scene Representation](https://arxiv.org/abs/2506.18678)
*Tianchen Deng,Guole Shen,Xun Chen,Shenghai Yuan,Hongming Shen,Guohao Peng,Zhenyu Wu,Jingchuan Wang,Lihua Xie,Danwei Wang,Hesheng Wang,Weidong Chen*

Main category: cs.CV

TL;DR: A distributed multi-agent collaborative neural SLAM framework with hybrid scene representation and novel methods for loop closure and submap fusion, along with a new real-world dataset for evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing implicit SLAM algorithms are limited to single-agent scenarios and struggle with large-scale scenes and long sequences, while current multi-agent frameworks fail to meet communication constraints.

Method: Proposes a hybrid scene representation (triplane-grid), distributed camera tracking, intra-to-inter loop closure, and online distillation for submap fusion. Introduces a new real-world dataset (DES) for evaluation.

Result: Demonstrates superior performance in mapping, tracking, and communication across various datasets.

Conclusion: The proposed framework and dataset advance research in SLAM, 3D reconstruction, and visual foundation models, with plans for open-sourcing.

Abstract: Neural implicit scene representations have recently shown promising results
in dense visual SLAM. However, existing implicit SLAM algorithms are
constrained to single-agent scenarios, and fall difficulties in large-scale
scenes and long sequences. Existing NeRF-based multi-agent SLAM frameworks
cannot meet the constraints of communication bandwidth. To this end, we propose
the first distributed multi-agent collaborative neural SLAM framework with
hybrid scene representation, distributed camera tracking, intra-to-inter loop
closure, and online distillation for multiple submap fusion. A novel
triplane-grid joint scene representation method is proposed to improve scene
reconstruction. A novel intra-to-inter loop closure method is designed to
achieve local (single-agent) and global (multi-agent) consistency. We also
design a novel online distillation method to fuse the information of different
submaps to achieve global consistency. Furthermore, to the best of our
knowledge, there is no real-world dataset for NeRF-based/GS-based SLAM that
provides both continuous-time trajectories groundtruth and high-accuracy 3D
meshes groundtruth. To this end, we propose the first real-world Dense slam
(DES) dataset covering both single-agent and multi-agent scenarios, ranging
from small rooms to large-scale outdoor scenes, with high-accuracy ground truth
for both 3D mesh and continuous-time camera trajectory. This dataset can
advance the development of the research in both SLAM, 3D reconstruction, and
visual foundation model. Experiments on various datasets demonstrate the
superiority of the proposed method in both mapping, tracking, and
communication. The dataset and code will open-source on
https://github.com/dtc111111/mcnslam.

</details>


### [234] [MARL-MambaContour: Unleashing Multi-Agent Deep Reinforcement Learning for Active Contour Optimization in Medical Image Segmentation](https://arxiv.org/abs/2506.18679)
*Ruicheng Zhang,Yu Sun,Zeyu Zhang,Jinai Li,Xiaofan Liu,Au Hoi Fan,Haowei Guo,Puxin Yan*

Main category: cs.CV

TL;DR: MARL-MambaContour is a contour-based medical image segmentation framework using MARL, optimizing contour alignment with SAC and ERAM, and enhancing inter-agent communication with BCHFM.


<details>
  <summary>Details</summary>
Motivation: Traditional pixel-based segmentation lacks topological constraints and structural awareness, especially for blurred edges and complex morphologies in medical images.

Method: Models contour points as agents adjusting positions iteratively, using SAC with ERAM for optimization and a Mamba-based policy network with BCHFM for inter-agent communication.

Result: Achieves state-of-the-art performance on five medical imaging datasets, demonstrating accuracy and robustness.

Conclusion: MARL-MambaContour is a promising framework for precise and robust medical image segmentation.

Abstract: We introduce MARL-MambaContour, the first contour-based medical image
segmentation framework based on Multi-Agent Reinforcement Learning (MARL). Our
approach reframes segmentation as a multi-agent cooperation task focused on
generate topologically consistent object-level contours, addressing the
limitations of traditional pixel-based methods which could lack topological
constraints and holistic structural awareness of anatomical regions. Each
contour point is modeled as an autonomous agent that iteratively adjusts its
position to align precisely with the target boundary, enabling adaptation to
blurred edges and intricate morphologies common in medical images. This
iterative adjustment process is optimized by a contour-specific Soft
Actor-Critic (SAC) algorithm, further enhanced with the Entropy Regularization
Adjustment Mechanism (ERAM) which dynamically balance agent exploration with
contour smoothness. Furthermore, the framework incorporates a Mamba-based
policy network featuring a novel Bidirectional Cross-attention Hidden-state
Fusion Mechanism (BCHFM). This mechanism mitigates potential memory confusion
limitations associated with long-range modeling in state space models, thereby
facilitating more accurate inter-agent information exchange and informed
decision-making. Extensive experiments on five diverse medical imaging datasets
demonstrate the state-of-the-art performance of MARL-MambaContour, highlighting
its potential as an accurate and robust clinical application.

</details>


### [235] [Multi-Scale Spectral Attention Module-based Hyperspectral Segmentation in Autonomous Driving Scenarios](https://arxiv.org/abs/2506.18682)
*Imad Ali Shah,Jiarong Li,Tim Brophy,Martin Glavin,Edward Jones,Enda Ward,Brian Deegan*

Main category: cs.CV

TL;DR: The paper introduces a Multi-scale Spectral Attention Module (MSAM) for Hyperspectral Imaging (HSI) in autonomous driving, improving semantic segmentation with minimal computational overhead.


<details>
  <summary>Details</summary>
Motivation: Efficient processing of high-dimensional HSI data for enhanced environmental perception in challenging conditions.

Method: MSAM uses three parallel 1D convolutions with varying kernel sizes (1-11) and adaptive feature aggregation, integrated into UNet's skip connections (UNet-MSAM).

Result: UNet-MSAM outperforms UNet-SC with 3.61% mean IoU and 3.80% mF1 improvements, adding only 0.02% parameters and 0.82% GFLOPS.

Conclusion: Multi-scale kernel combinations in MSAM enhance HSI processing for autonomous driving, offering robust spectral feature extraction.

Abstract: Recent advances in autonomous driving (AD) have highlighted the potential of
Hyperspectral Imaging (HSI) for enhanced environmental perception, particularly
in challenging weather and lighting conditions. However, efficiently processing
its high-dimensional spectral data remains a significant challenge. This paper
introduces a Multi-scale Spectral Attention Module (MSAM) that enhances
spectral feature extraction through three parallel 1D convolutions with varying
kernel sizes between 1 to 11, coupled with an adaptive feature aggregation
mechanism. By integrating MSAM into UNet's skip connections (UNet-SC), our
proposed UNet-MSAM achieves significant improvements in semantic segmentation
performance across multiple HSI datasets: HyKo-VIS v2, HSI-Drive v2, and
Hyperspectral City v2. Our comprehensive experiments demonstrate that with
minimal computational overhead (on average 0.02% in parameters and 0.82%
GFLOPS), UNet-MSAM consistently outperforms UNet-SC, achieving average
improvements of 3.61% in mean IoU and 3.80% in mF1 across the three datasets.
Through extensive ablation studies, we have established that multi-scale kernel
combinations perform better than single-scale configurations. These findings
demonstrate the potential of HSI processing for AD and provide valuable
insights into designing robust, multi-scale spectral feature extractors for
real-world applications.

</details>


### [236] [SIM-Net: A Multimodal Fusion Network Using Inferred 3D Object Shape Point Clouds from RGB Images for 2D Classification](https://arxiv.org/abs/2506.18683)
*Youcef Sklab,Hanane Ariouat,Eric Chenin,Edi Prifti,Jean-Daniel Zucker*

Main category: cs.CV

TL;DR: SIM-Net integrates 3D point clouds from 2D images for improved classification, outperforming ResNet101 and transformer-based models.


<details>
  <summary>Details</summary>
Motivation: Enhance 2D image classification by incorporating 3D geometric features, especially for challenging cases like herbarium specimens with occlusions and backgrounds.

Method: Uses pixel-to-point transformation to convert 2D masks to 3D point clouds, fusing CNN and PointNet encoders for texture and geometric features.

Result: Outperforms ResNet101 by up to 9.9% in accuracy and 12.3% in F-score, surpassing transformer-based models.

Conclusion: SIM-Net demonstrates the value of 3D structural reasoning in 2D classification, particularly for complex datasets.

Abstract: We introduce the Shape-Image Multimodal Network (SIM-Net), a novel 2D image
classification architecture that integrates 3D point cloud representations
inferred directly from RGB images. Our key contribution lies in a
pixel-to-point transformation that converts 2D object masks into 3D point
clouds, enabling the fusion of texture-based and geometric features for
enhanced classification performance. SIM-Net is particularly well-suited for
the classification of digitized herbarium specimens (a task made challenging by
heterogeneous backgrounds), non-plant elements, and occlusions that compromise
conventional image-based models. To address these issues, SIM-Net employs a
segmentation-based preprocessing step to extract object masks prior to 3D point
cloud generation. The architecture comprises a CNN encoder for 2D image
features and a PointNet-based encoder for geometric features, which are fused
into a unified latent space. Experimental evaluations on herbarium datasets
demonstrate that SIM-Net consistently outperforms ResNet101, achieving gains of
up to 9.9% in accuracy and 12.3% in F-score. It also surpasses several
transformer-based state-of-the-art architectures, highlighting the benefits of
incorporating 3D structural reasoning into 2D image classification tasks.

</details>


### [237] [Matrix-Game: Interactive World Foundation Model](https://arxiv.org/abs/2506.18701)
*Yifan Zhang,Chunli Peng,Boyang Wang,Puyi Wang,Qingcheng Zhu,Fei Kang,Biao Jiang,Zedong Gao,Eric Li,Yang Liu,Yahui Zhou*

Main category: cs.CV

TL;DR: Matrix-Game is a large-scale interactive world foundation model for controllable game world generation, outperforming prior models in visual quality, controllability, and physical consistency.


<details>
  <summary>Details</summary>
Motivation: To advance controllable game world generation by enabling precise control over character actions and camera movements while maintaining high visual quality and temporal coherence.

Method: A two-stage pipeline: large-scale unlabeled pretraining for environment understanding, followed by action-labeled training for interactive video generation, using a curated Minecraft dataset (Matrix-Game-MC).

Result: Outperforms prior models (Oasis, MineWorld) in all metrics, especially controllability and physical consistency, confirmed by human evaluations.

Conclusion: Matrix-Game sets a new standard for interactive image-to-world generation, with plans to open-source the model and benchmark for future research.

Abstract: We introduce Matrix-Game, an interactive world foundation model for
controllable game world generation. Matrix-Game is trained using a two-stage
pipeline that first performs large-scale unlabeled pretraining for environment
understanding, followed by action-labeled training for interactive video
generation. To support this, we curate Matrix-Game-MC, a comprehensive
Minecraft dataset comprising over 2,700 hours of unlabeled gameplay video clips
and over 1,000 hours of high-quality labeled clips with fine-grained keyboard
and mouse action annotations. Our model adopts a controllable image-to-world
generation paradigm, conditioned on a reference image, motion context, and user
actions. With over 17 billion parameters, Matrix-Game enables precise control
over character actions and camera movements, while maintaining high visual
quality and temporal coherence. To evaluate performance, we develop GameWorld
Score, a unified benchmark measuring visual quality, temporal quality, action
controllability, and physical rule understanding for Minecraft world
generation. Extensive experiments show that Matrix-Game consistently
outperforms prior open-source Minecraft world models (including Oasis and
MineWorld) across all metrics, with particularly strong gains in
controllability and physical consistency. Double-blind human evaluations
further confirm the superiority of Matrix-Game, highlighting its ability to
generate perceptually realistic and precisely controllable videos across
diverse game scenarios. To facilitate future research on interactive
image-to-world generation, we will open-source the Matrix-Game model weights
and the GameWorld Score benchmark at https://github.com/SkyworkAI/Matrix-Game.

</details>


### [238] [Including Semantic Information via Word Embeddings for Skeleton-based Action Recognition](https://arxiv.org/abs/2506.18721)
*Dustin Aganian,Erik Franze,Markus Eisenbach,Horst-Michael Gross*

Main category: cs.CV

TL;DR: A novel skeleton-based action recognition method uses word embeddings to encode semantic information, improving performance and generalization in assembly tasks.


<details>
  <summary>Details</summary>
Motivation: Conventional skeleton-based methods lose keypoint semantics, limiting effectiveness in complex interactions, especially in Industry 4.0 cobot applications.

Method: Replaces one-hot encodings with semantic volumes derived from word embeddings to capture joint-object relationships.

Result: Significantly improves classification performance and generalization across different skeleton types and object classes.

Conclusion: Incorporating semantic information enhances skeleton-based action recognition in dynamic, diverse environments.

Abstract: Effective human action recognition is widely used for cobots in Industry 4.0
to assist in assembly tasks. However, conventional skeleton-based methods often
lose keypoint semantics, limiting their effectiveness in complex interactions.
In this work, we introduce a novel approach to skeleton-based action
recognition that enriches input representations by leveraging word embeddings
to encode semantic information. Our method replaces one-hot encodings with
semantic volumes, enabling the model to capture meaningful relationships
between joints and objects. Through extensive experiments on multiple assembly
datasets, we demonstrate that our approach significantly improves
classification performance, and enhances generalization capabilities by
simultaneously supporting different skeleton types and object classes. Our
findings highlight the potential of incorporating semantic information to
enhance skeleton-based action recognition in dynamic and diverse environments.

</details>


### [239] [Deep CNN Face Matchers Inherently Support Revocable Biometric Templates](https://arxiv.org/abs/2506.18731)
*Aman Bhatta,Michael C. King,Kevin W. Bowyer*

Main category: cs.CV

TL;DR: The paper proposes a revocable biometric scheme using deep CNN face matchers, allowing unlimited distinct models with equivalent recognition power and incompatible templates. Vision Transformers are found less suitable than ResNet-based CNNs.


<details>
  <summary>Details</summary>
Motivation: Address the lack of recourse in compromised biometrics by enabling revocation and re-enrollment with new templates.

Method: Generate multiple deep CNN face matcher models with equivalent recognition power and incompatible templates. Compare with Vision Transformers.

Result: Deep CNN models allow robust revocation; templates are strongly incompatible. Vision Transformers are less suitable.

Conclusion: Deep CNN backbones enable effective revocable biometric schemes, outperforming Vision Transformers.

Abstract: One common critique of biometric authentication is that if an individual's
biometric is compromised, then the individual has no recourse. The concept of
revocable biometrics was developed to address this concern. A biometric scheme
is revocable if an individual can have their current enrollment in the scheme
revoked, so that the compromised biometric template becomes worthless, and the
individual can re-enroll with a new template that has similar recognition
power. We show that modern deep CNN face matchers inherently allow for a robust
revocable biometric scheme. For a given state-of-the-art deep CNN backbone and
training set, it is possible to generate an unlimited number of distinct face
matcher models that have both (1) equivalent recognition power, and (2)
strongly incompatible biometric templates. The equivalent recognition power
extends to the point of generating impostor and genuine distributions that have
the same shape and placement on the similarity dimension, meaning that the
models can share a similarity threshold for a 1-in-10,000 false match rate. The
biometric templates from different model instances are so strongly incompatible
that the cross-instance similarity score for images of the same person is
typically lower than the same-instance similarity score for images of different
persons. That is, a stolen biometric template that is revoked is of less value
in attempting to match the re-enrolled identity than the average impostor
template. We also explore the feasibility of using a Vision Transformer (ViT)
backbone-based face matcher in the revocable biometric system proposed in this
work and demonstrate that it is less suitable compared to typical ResNet-based
deep CNN backbones.

</details>


### [240] [USVTrack: USV-Based 4D Radar-Camera Tracking Dataset for Autonomous Driving in Inland Waterways](https://arxiv.org/abs/2506.18737)
*Shanliang Yao,Runwei Guan,Yi Ni,Sen Xu,Yong Yue,Xiaohui Zhu,Ryan Wen Liu*

Main category: cs.CV

TL;DR: The paper introduces USVTrack, the first 4D radar-camera tracking dataset for autonomous driving in waterways, and proposes RCM, a radar-camera matching method to enhance tracking accuracy.


<details>
  <summary>Details</summary>
Motivation: To improve object tracking in inland waterways for applications like transportation, monitoring, and rescue, addressing challenges in complex environments.

Method: Uses a USV equipped with 4D radar, camera, GPS, and IMU to collect data. Proposes RCM, a radar-camera matching method for integration into trackers.

Result: RCM improves tracking accuracy and reliability in waterborne environments, validated through experiments.

Conclusion: USVTrack dataset and RCM method advance autonomous driving in waterways, with the dataset publicly available.

Abstract: Object tracking in inland waterways plays a crucial role in safe and
cost-effective applications, including waterborne transportation, sightseeing
tours, environmental monitoring and surface rescue. Our Unmanned Surface
Vehicle (USV), equipped with a 4D radar, a monocular camera, a GPS, and an IMU,
delivers robust tracking capabilities in complex waterborne environments. By
leveraging these sensors, our USV collected comprehensive object tracking data,
which we present as USVTrack, the first 4D radar-camera tracking dataset
tailored for autonomous driving in new generation waterborne transportation
systems. Our USVTrack dataset presents rich scenarios, featuring diverse
various waterways, varying times of day, and multiple weather and lighting
conditions. Moreover, we present a simple but effective radar-camera matching
method, termed RCM, which can be plugged into popular two-stage association
trackers. Experimental results utilizing RCM demonstrate the effectiveness of
the radar-camera matching in improving object tracking accuracy and reliability
for autonomous driving in waterborne environments. The USVTrack dataset is
public on https://usvtrack.github.io.

</details>


### [241] [SWA-SOP: Spatially-aware Window Attention for Semantic Occupancy Prediction in Autonomous Driving](https://arxiv.org/abs/2506.18785)
*Helin Cao,Rafael Materla,Sven Behnke*

Main category: cs.CV

TL;DR: The paper introduces Spatially-aware Window Attention (SWA) to improve Semantic Occupancy Prediction (SOP) by incorporating local spatial context, addressing limitations in transformer-based methods.


<details>
  <summary>Details</summary>
Motivation: Existing transformer-based SOP methods lack explicit spatial structure modeling, leading to poor performance in sparse or occluded areas.

Method: Proposes SWA, a mechanism that integrates local spatial context into attention computation.

Result: SWA improves scene completion and achieves state-of-the-art results on LiDAR-based SOP benchmarks, with consistent gains in camera-based SOP.

Conclusion: SWA enhances geometric awareness and performance in SOP, demonstrating cross-modal applicability.

Abstract: Perception systems in autonomous driving rely on sensors such as LiDAR and
cameras to perceive the 3D environment. However, due to occlusions and data
sparsity, these sensors often fail to capture complete information. Semantic
Occupancy Prediction (SOP) addresses this challenge by inferring both occupancy
and semantics of unobserved regions. Existing transformer-based SOP methods
lack explicit modeling of spatial structure in attention computation, resulting
in limited geometric awareness and poor performance in sparse or occluded
areas. To this end, we propose Spatially-aware Window Attention (SWA), a novel
mechanism that incorporates local spatial context into attention. SWA
significantly improves scene completion and achieves state-of-the-art results
on LiDAR-based SOP benchmarks. We further validate its generality by
integrating SWA into a camera-based SOP pipeline, where it also yields
consistent gains across modalities.

</details>


### [242] [3D Arena: An Open Platform for Generative 3D Evaluation](https://arxiv.org/abs/2506.18787)
*Dylan Ebert*

Main category: cs.CV

TL;DR: 3D Arena is an open platform for evaluating image-to-3D models using human preferences, addressing gaps in automated metrics. It collects large-scale pairwise comparisons and provides insights into human preferences for 3D model quality.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for 3D generative models rely on flawed metricseither image-based (ignoring 3D structure) or geometric (lacking perceptual appeal). 3D Arena aims to bridge this gap with human-centered evaluation.

Method: The platform uses pairwise comparisons to collect human preferences, supported by a dataset (iso3d) and statistical fraud detection for quality control. An ELO-based ranking system evaluates models.

Result: Findings include preferences for Gaussian splat outputs (16.6 ELO advantage) and textured models (144.1 ELO advantage). The platform collected 123,243 votes from 8,096 users, establishing a reliable benchmark.

Conclusion: 3D Arena advances human-centered evaluation in generative 3D, offering insights and recommendations for improved assessment methods. It serves as a benchmark and fosters community engagement.

Abstract: Evaluating Generative 3D models remains challenging due to misalignment
between automated metrics and human perception of quality. Current benchmarks
rely on image-based metrics that ignore 3D structure or geometric measures that
fail to capture perceptual appeal and real-world utility. To address this gap,
we present 3D Arena, an open platform for evaluating image-to-3D generation
models through large-scale human preference collection using pairwise
comparisons.
  Since launching in June 2024, the platform has collected 123,243 votes from
8,096 users across 19 state-of-the-art models, establishing the largest human
preference evaluation for Generative 3D. We contribute the iso3d dataset of 100
evaluation prompts and demonstrate quality control achieving 99.75% user
authenticity through statistical fraud detection. Our ELO-based ranking system
provides reliable model assessment, with the platform becoming an established
evaluation resource.
  Through analysis of this preference data, we present insights into human
preference patterns. Our findings reveal preferences for visual presentation
features, with Gaussian splat outputs achieving a 16.6 ELO advantage over
meshes and textured models receiving a 144.1 ELO advantage over untextured
models. We provide recommendations for improving evaluation methods, including
multi-criteria assessment, task-oriented evaluation, and format-aware
comparison. The platform's community engagement establishes 3D Arena as a
benchmark for the field while advancing understanding of human-centered
evaluation in Generative 3D.

</details>


### [243] [Focus Your Attention: Towards Data-Intuitive Lightweight Vision Transformers](https://arxiv.org/abs/2506.18791)
*Suyash Gaurav,Muhammad Farhan Humayun,Jukka Heikkonen,Jatin Chaudhary*

Main category: cs.CV

TL;DR: The paper introduces Super-Pixel Based Patch Pooling (SPPP) and Light Latent Attention (LLA) to reduce Vision Transformers' computational and memory demands while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the high computational and memory costs of Vision Transformers, especially in pre-training and transfer learning, due to the self-attention mechanism.

Method: Proposes SPPP for context-aware patch embeddings and LLA for efficient cross-attention, integrating latent tokens to reduce complexity.

Result: Achieves comparable performance to state-of-the-art methods with improved computational efficiency and faster convergence.

Conclusion: The proposed methods offer energy-efficient solutions for Vision Transformers, suitable for edge deployment.

Abstract: The evolution of Vision Transformers has led to their widespread adaptation
to different domains. Despite large-scale success, there remain significant
challenges including their reliance on extensive computational and memory
resources for pre-training on huge datasets as well as difficulties in
task-specific transfer learning. These limitations coupled with energy
inefficiencies mainly arise due to the computation-intensive self-attention
mechanism. To address these issues, we propose a novel Super-Pixel Based Patch
Pooling (SPPP) technique that generates context-aware, semantically rich, patch
embeddings to effectively reduce the architectural complexity and improve
efficiency. Additionally, we introduce the Light Latent Attention (LLA) module
in our pipeline by integrating latent tokens into the attention mechanism
allowing cross-attention operations to significantly reduce the time and space
complexity of the attention module. By leveraging the data-intuitive patch
embeddings coupled with dynamic positional encodings, our approach adaptively
modulates the cross-attention process to focus on informative regions while
maintaining the global semantic structure. This targeted attention improves
training efficiency and accelerates convergence. Notably, the SPPP module is
lightweight and can be easily integrated into existing transformer
architectures. Extensive experiments demonstrate that our proposed architecture
provides significant improvements in terms of computational efficiency while
achieving comparable results with the state-of-the-art approaches, highlighting
its potential for energy-efficient transformers suitable for edge deployment.
(The code is available on our GitHub repository:
https://github.com/zser092/Focused-Attention-ViT).

</details>


### [244] [ViDAR: Video Diffusion-Aware 4D Reconstruction From Monocular Inputs](https://arxiv.org/abs/2506.18792)
*Michal Nazarczuk,Sibi Catley-Chandar,Thomas Tanay,Zhensong Zhang,Gregory Slabaugh,Eduardo Prez-Pellitero*

Main category: cs.CV

TL;DR: ViDAR introduces a 4D reconstruction framework using personalized diffusion models to synthesize pseudo multi-view supervision for Gaussian splatting, improving dynamic novel view synthesis from monocular video.


<details>
  <summary>Details</summary>
Motivation: The challenge lies in disentangling structure from motion in monocular video due to ill-posedness and scarce supervision.

Method: ViDAR leverages personalized diffusion models for pseudo multi-view supervision, a diffusion-aware loss function, and camera pose optimization.

Result: ViDAR outperforms state-of-the-art baselines on DyCheck, excelling in visual quality and geometric consistency, especially in dynamic regions.

Conclusion: ViDAR sets a new benchmark for dynamic novel view synthesis, particularly in motion-rich scenes.

Abstract: Dynamic Novel View Synthesis aims to generate photorealistic views of moving
subjects from arbitrary viewpoints. This task is particularly challenging when
relying on monocular video, where disentangling structure from motion is
ill-posed and supervision is scarce. We introduce Video Diffusion-Aware
Reconstruction (ViDAR), a novel 4D reconstruction framework that leverages
personalised diffusion models to synthesise a pseudo multi-view supervision
signal for training a Gaussian splatting representation. By conditioning on
scene-specific features, ViDAR recovers fine-grained appearance details while
mitigating artefacts introduced by monocular ambiguity. To address the
spatio-temporal inconsistency of diffusion-based supervision, we propose a
diffusion-aware loss function and a camera pose optimisation strategy that
aligns synthetic views with the underlying scene geometry. Experiments on
DyCheck, a challenging benchmark with extreme viewpoint variation, show that
ViDAR outperforms all state-of-the-art baselines in visual quality and
geometric consistency. We further highlight ViDAR's strong improvement over
baselines on dynamic regions and provide a new benchmark to compare performance
in reconstructing motion-rich parts of the scene. Project page:
https://vidar-4d.github.io

</details>


### [245] [OC-SOP: Enhancing Vision-Based 3D Semantic Occupancy Prediction by Object-Centric Awareness](https://arxiv.org/abs/2506.18798)
*Helin Cao,Sven Behnke*

Main category: cs.CV

TL;DR: OC-SOP improves semantic occupancy prediction by integrating object-centric cues, enhancing accuracy for foreground objects and achieving top performance on SemanticKITTI.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in autonomous driving perception due to occlusions and incomplete scene data, especially for dynamic foreground objects.

Method: Proposes Object-Centric SOP (OC-SOP), integrating high-level object-centric cues via a detection branch into the SOP pipeline.

Result: Significantly enhances prediction accuracy for foreground objects and achieves state-of-the-art performance on SemanticKITTI.

Conclusion: OC-SOP effectively improves semantic occupancy prediction by leveraging object-centric cues, particularly benefiting dynamic foreground objects.

Abstract: Autonomous driving perception faces significant challenges due to occlusions
and incomplete scene data in the environment. To overcome these issues, the
task of semantic occupancy prediction (SOP) is proposed, which aims to jointly
infer both the geometry and semantic labels of a scene from images. However,
conventional camera-based methods typically treat all categories equally and
primarily rely on local features, leading to suboptimal predictions, especially
for dynamic foreground objects. To address this, we propose Object-Centric SOP
(OC-SOP), a framework that integrates high-level object-centric cues extracted
via a detection branch into the semantic occupancy prediction pipeline. This
object-centric integration significantly enhances the prediction accuracy for
foreground objects and achieves state-of-the-art performance among all
categories on SemanticKITTI.

</details>


### [246] [PicoSAM2: Low-Latency Segmentation In-Sensor for Edge Vision Applications](https://arxiv.org/abs/2506.18807)
*Pietro Bonazzi,Nicola Farronato,Stefan Zihlmann,Haotong Qi,Michele Magno*

Main category: cs.CV

TL;DR: PicoSAM2 is a lightweight, promptable segmentation model optimized for edge and in-sensor execution, achieving high efficiency and performance on devices like the Sony IMX500.


<details>
  <summary>Details</summary>
Motivation: The need for real-time, on-device segmentation for latency-sensitive and privacy-aware applications like smart glasses and IoT devices.

Method: Builds on a depthwise separable U-Net, uses knowledge distillation and fixed-point prompt encoding to learn from SAM2.

Result: Achieves 51.9% and 44.9% mIoU on COCO and LVIS, runs at 14.3 ms on IMX500, and meets memory/compute constraints for in-sensor deployment.

Conclusion: Efficient, promptable segmentation on-camera is feasible, enabling privacy-preserving vision without cloud processing.

Abstract: Real-time, on-device segmentation is critical for latency-sensitive and
privacy-aware applications like smart glasses and IoT devices. We introduce
PicoSAM2, a lightweight (1.3M parameters, 336M MACs) promptable segmentation
model optimized for edge and in-sensor execution, including the Sony IMX500. It
builds on a depthwise separable U-Net, with knowledge distillation and
fixed-point prompt encoding to learn from the Segment Anything Model 2 (SAM2).
On COCO and LVIS, it achieves 51.9% and 44.9% mIoU, respectively. The quantized
model (1.22MB) runs at 14.3 ms on the IMX500-achieving 86 MACs/cycle, making it
the only model meeting both memory and compute constraints for in-sensor
deployment. Distillation boosts LVIS performance by +3.5% mIoU and +5.1% mAP.
These results demonstrate that efficient, promptable segmentation is feasible
directly on-camera, enabling privacy-preserving vision without cloud or host
processing.

</details>


### [247] [4Real-Video-V2: Fused View-Time Attention and Feedforward Reconstruction for 4D Scene Generation](https://arxiv.org/abs/2506.18839)
*Chaoyang Wang,Ashkan Mirzaei,Vidit Goel,Willi Menapace,Aliaksandr Siarohin,Avalon Vinella,Michael Vasilkovsky,Ivan Skorokhodov,Vladislav Shakhrai,Sergey Korolev,Sergey Tulyakov,Peter Wonka*

Main category: cs.CV

TL;DR: A novel framework for 4D spatio-temporal grid computation using a fused attention architecture and Gaussian particles, improving 4D generation quality.


<details>
  <summary>Details</summary>
Motivation: Address limitations in current 4D video diffusion architectures by introducing a fused spatial-temporal attention method and enhancing 3D reconstruction.

Method: Two-part architecture: 4D video model with fused attention and 4D reconstruction model with Gaussian head, camera token replacement, and dynamic layers.

Result: Achieves state-of-the-art performance in 4D generation, enhancing visual quality and reconstruction.

Conclusion: The proposed framework advances 4D generation by integrating innovative attention and reconstruction techniques.

Abstract: We propose the first framework capable of computing a 4D spatio-temporal grid
of video frames and 3D Gaussian particles for each time step using a
feed-forward architecture. Our architecture has two main components, a 4D video
model and a 4D reconstruction model. In the first part, we analyze current 4D
video diffusion architectures that perform spatial and temporal attention
either sequentially or in parallel within a two-stream design. We highlight the
limitations of existing approaches and introduce a novel fused architecture
that performs spatial and temporal attention within a single layer. The key to
our method is a sparse attention pattern, where tokens attend to others in the
same frame, at the same timestamp, or from the same viewpoint. In the second
part, we extend existing 3D reconstruction algorithms by introducing a Gaussian
head, a camera token replacement algorithm, and additional dynamic layers and
training. Overall, we establish a new state of the art for 4D generation,
improving both visual quality and reconstruction capability.

</details>


### [248] [Phantom-Data : Towards a General Subject-Consistent Video Generation Dataset](https://arxiv.org/abs/2506.18851)
*Zhuowei Chen,Bingchuan Li,Tianxiang Ma,Lijie Liu,Mingcong Liu,Yi Zhang,Gen Li,Xinghui Li,Siyu Zhou,Qian He,Xinglong Wu*

Main category: cs.CV

TL;DR: Phantom-Data is introduced as a cross-pair dataset to improve subject-to-video generation by addressing the copy-paste problem, enhancing prompt alignment and visual quality.


<details>
  <summary>Details</summary>
Motivation: Existing models struggle with faithfully following textual instructions due to the copy-paste problem caused by in-pair training, which entangles subject identity with background attributes.

Method: A three-stage pipeline: subject detection, cross-context subject retrieval from large-scale data, and identity verification to ensure consistency.

Result: Training with Phantom-Data improves prompt alignment and visual quality while maintaining identity consistency.

Conclusion: Phantom-Data effectively addresses the limitations of in-pair training, offering a robust solution for subject-to-video generation.

Abstract: Subject-to-video generation has witnessed substantial progress in recent
years. However, existing models still face significant challenges in faithfully
following textual instructions. This limitation, commonly known as the
copy-paste problem, arises from the widely used in-pair training paradigm. This
approach inherently entangles subject identity with background and contextual
attributes by sampling reference images from the same scene as the target
video. To address this issue, we introduce \textbf{Phantom-Data, the first
general-purpose cross-pair subject-to-video consistency dataset}, containing
approximately one million identity-consistent pairs across diverse categories.
Our dataset is constructed via a three-stage pipeline: (1) a general and
input-aligned subject detection module, (2) large-scale cross-context subject
retrieval from more than 53 million videos and 3 billion images, and (3)
prior-guided identity verification to ensure visual consistency under
contextual variation. Comprehensive experiments show that training with
Phantom-Data significantly improves prompt alignment and visual quality while
preserving identity consistency on par with in-pair baselines.

</details>


### [249] [RAG-6DPose: Retrieval-Augmented 6D Pose Estimation via Leveraging CAD as Knowledge Base](https://arxiv.org/abs/2506.18856)
*Kuanning Wang,Yuqian Fu,Tianyu Wang,Yanwei Fu,Longfei Liang,Yu-Gang Jiang,Xiangyang Xue*

Main category: cs.CV

TL;DR: RAG-6DPose is a retrieval-augmented method for 6D pose estimation, using 3D CAD models to enhance accuracy by combining visual and geometric cues.


<details>
  <summary>Details</summary>
Motivation: Accurate 6D pose estimation is crucial for robotic manipulation, especially for tasks like grasping, where precise object localization is needed.

Method: The approach involves three stages: building a multi-modal CAD knowledge base, retrieving relevant CAD features using the ReSPC module, and refining pose predictions with retrieval-augmented decoding.

Result: The method shows effectiveness and robustness in handling occlusions and novel viewpoints, validated on standard benchmarks and real-world tasks.

Conclusion: RAG-6DPose improves 6D pose estimation by leveraging CAD models, demonstrating practical utility in robotic applications.

Abstract: Accurate 6D pose estimation is key for robotic manipulation, enabling precise
object localization for tasks like grasping. We present RAG-6DPose, a
retrieval-augmented approach that leverages 3D CAD models as a knowledge base
by integrating both visual and geometric cues. Our RAG-6DPose roughly contains
three stages: 1) Building a Multi-Modal CAD Knowledge Base by extracting 2D
visual features from multi-view CAD rendered images and also attaching 3D
points; 2) Retrieving relevant CAD features from the knowledge base based on
the current query image via our ReSPC module; and 3) Incorporating retrieved
CAD information to refine pose predictions via retrieval-augmented decoding.
Experimental results on standard benchmarks and real-world robotic tasks
demonstrate the effectiveness and robustness of our approach, particularly in
handling occlusions and novel viewpoints. Supplementary material is available
on our project website: https://sressers.github.io/RAG-6DPose .

</details>


### [250] [TAMMs: Temporal-Aware Multimodal Model for Satellite Image Change Understanding and Forecasting](https://arxiv.org/abs/2506.18862)
*Zhongbin Guo,Yuhao Wang,Ping Jian,Xinyue Chen,Wei Peng,Ertai E*

Main category: cs.CV

TL;DR: TAMMs enhances MLLMs with temporal modules for satellite image analysis, outperforming baselines in change understanding and future image generation.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs struggle with fine-grained spatio-temporal reasoning in satellite image time-series analysis.

Method: TAMMs introduces lightweight temporal modules for sequence encoding and a Semantic-Fused Control Injection (SFCI) mechanism for future image generation.

Result: TAMMs outperforms MLLM baselines in temporal change understanding and future image forecasting.

Conclusion: Carefully designed temporal reasoning and semantic fusion unlock MLLMs' potential for spatio-temporal tasks.

Abstract: Satellite image time-series analysis demands fine-grained spatial-temporal
reasoning, which remains a challenge for existing multimodal large language
models (MLLMs). In this work, we study the capabilities of MLLMs on a novel
task that jointly targets temporal change understanding and future scene
generation, aiming to assess their potential for modeling complex multimodal
dynamics over time. We propose TAMMs, a Temporal-Aware Multimodal Model for
satellite image change understanding and forecasting, which enhances frozen
MLLMs with lightweight temporal modules for structured sequence encoding and
contextual prompting. To guide future image generation, TAMMs introduces a
Semantic-Fused Control Injection (SFCI) mechanism that adaptively combines
high-level semantic reasoning and structural priors within an enhanced
ControlNet. This dual-path conditioning enables temporally consistent and
semantically grounded image synthesis. Experiments demonstrate that TAMMs
outperforms strong MLLM baselines in both temporal change understanding and
future image forecasting tasks, highlighting how carefully designed temporal
reasoning and semantic fusion can unlock the full potential of MLLMs for
spatio-temporal understanding.

</details>


### [251] [OmniAvatar: Efficient Audio-Driven Avatar Video Generation with Adaptive Body Animation](https://arxiv.org/abs/2506.18866)
*Qijun Gan,Ruizi Yang,Jianke Zhu,Shaofei Xue,Steven Hoi*

Main category: cs.CV

TL;DR: OmniAvatar is an audio-driven full-body video generation model improving lip-sync accuracy and natural movements, with precise text-based control.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus on facial movements and lack full-body animation with natural synchronization and prompt control.

Method: Uses pixel-wise multi-hierarchical audio embedding and LoRA-based training for better audio feature capture and prompt-driven control.

Result: Outperforms existing models in facial and semi-body video generation, enabling diverse applications like podcasts and singing.

Conclusion: OmniAvatar advances audio-driven human animation with enhanced synchronization, fluidity, and control.

Abstract: Significant progress has been made in audio-driven human animation, while
most existing methods focus mainly on facial movements, limiting their ability
to create full-body animations with natural synchronization and fluidity. They
also struggle with precise prompt control for fine-grained generation. To
tackle these challenges, we introduce OmniAvatar, an innovative audio-driven
full-body video generation model that enhances human animation with improved
lip-sync accuracy and natural movements. OmniAvatar introduces a pixel-wise
multi-hierarchical audio embedding strategy to better capture audio features in
the latent space, enhancing lip-syncing across diverse scenes. To preserve the
capability for prompt-driven control of foundation models while effectively
incorporating audio features, we employ a LoRA-based training approach.
Extensive experiments show that OmniAvatar surpasses existing models in both
facial and semi-body video generation, offering precise text-based control for
creating videos in various domains, such as podcasts, human interactions,
dynamic scenes, and singing. Our project page is
https://omni-avatar.github.io/.

</details>


### [252] [Let Your Video Listen to Your Music!](https://arxiv.org/abs/2506.18881)
*Xinyu Zhang,Dong Gong,Zicheng Duan,Anton van den Hengel,Lingqiao Liu*

Main category: cs.CV

TL;DR: MVAA automates video-music alignment by inserting keyframes at musical beats and using a diffusion model for frame generation, preserving original content.


<details>
  <summary>Details</summary>
Motivation: Aligning video motion with music beats enhances engagement but is labor-intensive. Existing methods lack flexibility in preserving visual content.

Method: Two-step process: align motion keyframes with beats, then use a diffusion model for rhythm-aware video inpainting. Pretraining and fine-tuning optimize efficiency.

Result: Achieves high-quality beat alignment and visual smoothness, adapting in 10 minutes on a single GPU.

Conclusion: MVAA offers an efficient, flexible solution for automatic video-music alignment, outperforming manual and heuristic-based methods.

Abstract: Aligning the rhythm of visual motion in a video with a given music track is a
practical need in multimedia production, yet remains an underexplored task in
autonomous video editing. Effective alignment between motion and musical beats
enhances viewer engagement and visual appeal, particularly in music videos,
promotional content, and cinematic editing. Existing methods typically depend
on labor-intensive manual cutting, speed adjustments, or heuristic-based
editing techniques to achieve synchronization. While some generative models
handle joint video and music generation, they often entangle the two
modalities, limiting flexibility in aligning video to music beats while
preserving the full visual content. In this paper, we propose a novel and
efficient framework, termed MVAA (Music-Video Auto-Alignment), that
automatically edits video to align with the rhythm of a given music track while
preserving the original visual content. To enhance flexibility, we modularize
the task into a two-step process in our MVAA: aligning motion keyframes with
audio beats, followed by rhythm-aware video inpainting. Specifically, we first
insert keyframes at timestamps aligned with musical beats, then use a
frame-conditioned diffusion model to generate coherent intermediate frames,
preserving the original video's semantic content. Since comprehensive test-time
training can be time-consuming, we adopt a two-stage strategy: pretraining the
inpainting module on a small video set to learn general motion priors, followed
by rapid inference-time fine-tuning for video-specific adaptation. This hybrid
approach enables adaptation within 10 minutes with one epoch on a single NVIDIA
4090 GPU using CogVideoX-5b-I2V as the backbone. Extensive experiments show
that our approach can achieve high-quality beat alignment and visual
smoothness.

</details>


### [253] [Light of Normals: Unified Feature Representation for Universal Photometric Stereo](https://arxiv.org/abs/2506.18882)
*Hong Li,Houyuan Chen,Chongjie Ye,Zhaoxi Chen,Bohan Li,Shaocong Xu,Xianda Guo,Xuhui Liu,Yikai Wang,Baochang Zhang,Satoshi Ikehata,Boxin Shi,Anyi Rao,Hao Zhao*

Main category: cs.CV

TL;DR: Universal photometric stereo aims to recover surface normals under arbitrary lighting, but challenges remain in decoupling lighting from surface features and preserving high-frequency details.


<details>
  <summary>Details</summary>
Motivation: The goal is to achieve high-quality surface normal recovery without relying on specific illumination models, addressing ambiguities in observed intensity and preserving intricate geometric details.

Method: Not explicitly detailed in the abstract, but recent approaches like SDM-UniPS and Uni MS-PS are mentioned.

Result: Challenges persist in decoupling illumination from surface features and capturing high-frequency geometric details accurately.

Conclusion: Further research is needed to overcome the deep coupling of lighting and surface features and to improve the preservation of geometric details.

Abstract: Universal photometric stereo (PS) aims to recover high-quality surface
normals from objects under arbitrary lighting conditions without relying on
specific illumination models. Despite recent advances such as SDM-UniPS and Uni
MS-PS, two fundamental challenges persist: 1) the deep coupling between varying
illumination and surface normal features, where ambiguity in observed intensity
makes it difficult to determine whether brightness variations stem from
lighting changes or surface orientation; and 2) the preservation of
high-frequency geometric details in complex surfaces, where intricate
geometries create self-shadowing, inter-reflections, and subtle normal
variations that conventional feature processing operations struggle to capture
accurately.

</details>


### [254] [Universal Video Temporal Grounding with Generative Multi-modal Large Language Models](https://arxiv.org/abs/2506.18883)
*Zeqian Li,Shangzhe Di,Zhonghua Zhai,Weilin Huang,Yanfeng Wang,Weidi Xie*

Main category: cs.CV

TL;DR: UniTime is a universal video temporal grounding model using MLLMs to localize moments in videos based on natural language queries, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing video grounding methods are limited to specific domains or durations, lacking universality. UniTime aims to overcome this by leveraging MLLMs for diverse video types and complex queries.

Method: UniTime integrates timestamp tokens with video tokens in MLLMs and uses adaptive frame scaling for videos of varying lengths.

Result: UniTime outperforms state-of-the-art methods in zero-shot and finetuned settings across five benchmarks and enhances VideoQA accuracy.

Conclusion: UniTime demonstrates robust and universal video temporal grounding, proving effective for diverse video understanding tasks.

Abstract: This paper presents a computational model for universal video temporal
grounding, which accurately localizes temporal moments in videos based on
natural language queries (e.g., questions or descriptions). Unlike existing
methods that are often limited to specific video domains or durations, we
propose UniTime, a robust and universal video grounding model leveraging the
strong vision-language understanding capabilities of generative Multi-modal
Large Language Models (MLLMs). Our model effectively handles videos of diverse
views, genres, and lengths while comprehending complex language queries. The
key contributions include: (i) We consider steering strong MLLMs for temporal
grounding in videos. To enable precise timestamp outputs, we incorporate
temporal information by interleaving timestamp tokens with video tokens. (ii)
By training the model to handle videos with different input granularities
through adaptive frame scaling, our approach achieves robust temporal grounding
for both short and long videos. (iii) Comprehensive experiments show that
UniTime outperforms state-of-the-art approaches in both zero-shot and
dataset-specific finetuned settings across five public temporal grounding
benchmarks. (iv) When employed as a preliminary moment retriever for long-form
video question-answering (VideoQA), UniTime significantly improves VideoQA
accuracy, highlighting its value for complex video understanding tasks.

</details>


### [255] [4D-LRM: Large Space-Time Reconstruction Model From and To Any View at Any Time](https://arxiv.org/abs/2506.18890)
*Ziqiao Ma,Xuweiyi Chen,Shoubin Yu,Sai Bi,Kai Zhang,Chen Ziwen,Sihan Xu,Jianing Yang,Zexiang Xu,Kalyan Sunkavalli,Mohit Bansal,Joyce Chai,Hao Tan*

Main category: cs.CV

TL;DR: 4D-LRM is a scalable 4D pretraining model for general space-time representations, enabling fast, high-quality rendering from unconstrained views and timestamps.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of prior 4D approaches (efficiency, generalization, faithfulness) and learn unified space-time representations.

Method: Predicts per-pixel 4D Gaussian primitives from posed image tokens across time for fast, high-quality rendering.

Result: Achieves accurate 4D reconstruction, generalizes to novel objects, interpolates time, and handles diverse camera setups in under 1.5 seconds on an A100 GPU.

Conclusion: Scaling spatiotemporal pretraining enables efficient and accurate 4D reconstruction, demonstrated by 4D-LRM's performance.

Abstract: Can we scale 4D pretraining to learn general space-time representations that
reconstruct an object from a few views at some times to any view at any time?
We provide an affirmative answer with 4D-LRM, the first large-scale 4D
reconstruction model that takes input from unconstrained views and timestamps
and renders arbitrary novel view-time combinations. Unlike prior 4D approaches,
e.g., optimization-based, geometry-based, or generative, that struggle with
efficiency, generalization, or faithfulness, 4D-LRM learns a unified space-time
representation and directly predicts per-pixel 4D Gaussian primitives from
posed image tokens across time, enabling fast, high-quality rendering at, in
principle, infinite frame rate. Our results demonstrate that scaling
spatiotemporal pretraining enables accurate and efficient 4D reconstruction. We
show that 4D-LRM generalizes to novel objects, interpolates across time, and
handles diverse camera setups. It reconstructs 24-frame sequences in one
forward pass with less than 1.5 seconds on a single A100 GPU.

</details>


### [256] [FilMaster: Bridging Cinematic Principles and Generative AI for Automated Film Generation](https://arxiv.org/abs/2506.18899)
*Kaiyi Huang,Yukun Huang,Xintao Wang,Zinan Lin,Xuefei Ning,Pengfei Wan,Di Zhang,Yu Wang,Xihui Liu*

Main category: cs.CV

TL;DR: FilMaster is an AI system for professional film generation, integrating cinematic principles and post-production workflows to create engaging, high-quality films.


<details>
  <summary>Details</summary>
Motivation: Existing AI film systems lack professional cinematic quality, diversity in camera language, and engaging narratives.

Method: FilMaster uses two stages: Reference-Guided Generation (with a Multi-shot Synergized RAG module) and Generative Post-Production (with Audience-Centric Cinematic Rhythm Control).

Result: FilMaster outperforms in camera language and rhythm control, validated by the FilmEval benchmark.

Conclusion: FilMaster advances AI in professional filmmaking by combining cinematic principles and generative models.

Abstract: AI-driven content creation has shown potential in film production. However,
existing film generation systems struggle to implement cinematic principles and
thus fail to generate professional-quality films, particularly lacking diverse
camera language and cinematic rhythm. This results in templated visuals and
unengaging narratives. To address this, we introduce FilMaster, an end-to-end
AI system that integrates real-world cinematic principles for
professional-grade film generation, yielding editable, industry-standard
outputs. FilMaster is built on two key principles: (1) learning cinematography
from extensive real-world film data and (2) emulating professional,
audience-centric post-production workflows. Inspired by these principles,
FilMaster incorporates two stages: a Reference-Guided Generation Stage which
transforms user input to video clips, and a Generative Post-Production Stage
which transforms raw footage into audiovisual outputs by orchestrating visual
and auditory elements for cinematic rhythm. Our generation stage highlights a
Multi-shot Synergized RAG Camera Language Design module to guide the AI in
generating professional camera language by retrieving reference clips from a
vast corpus of 440,000 film clips. Our post-production stage emulates
professional workflows by designing an Audience-Centric Cinematic Rhythm
Control module, including Rough Cut and Fine Cut processes informed by
simulated audience feedback, for effective integration of audiovisual elements
to achieve engaging content. The system is empowered by generative AI models
like (M)LLMs and video generation models. Furthermore, we introduce FilmEval, a
comprehensive benchmark for evaluating AI-generated films. Extensive
experiments show FilMaster's superior performance in camera language design and
cinematic rhythm control, advancing generative AI in professional filmmaking.

</details>


### [257] [Audit & Repair: An Agentic Framework for Consistent Story Visualization in Text-to-Image Diffusion Models](https://arxiv.org/abs/2506.18900)
*Kiymet Akdemir,Tahira Kazimi,Pinar Yanardag*

Main category: cs.CV

TL;DR: A multi-agent framework improves visual consistency in story visualization by iteratively refining panel-level details without full regeneration.


<details>
  <summary>Details</summary>
Motivation: Current diffusion models struggle with maintaining character and object consistency across multi-panel narratives.

Method: Proposes a collaborative multi-agent framework for identifying and correcting inconsistencies iteratively, compatible with various diffusion models.

Result: Outperforms prior methods in multi-panel consistency, as shown in quantitative and qualitative experiments.

Conclusion: The framework effectively enhances visual coherence in story visualization tasks.

Abstract: Story visualization has become a popular task where visual scenes are
generated to depict a narrative across multiple panels. A central challenge in
this setting is maintaining visual consistency, particularly in how characters
and objects persist and evolve throughout the story. Despite recent advances in
diffusion models, current approaches often fail to preserve key character
attributes, leading to incoherent narratives. In this work, we propose a
collaborative multi-agent framework that autonomously identifies, corrects, and
refines inconsistencies across multi-panel story visualizations. The agents
operate in an iterative loop, enabling fine-grained, panel-level updates
without re-generating entire sequences. Our framework is model-agnostic and
flexibly integrates with a variety of diffusion models, including rectified
flow transformers such as Flux and latent diffusion models such as Stable
Diffusion. Quantitative and qualitative experiments show that our method
outperforms prior approaches in terms of multi-panel consistency.

</details>


### [258] [From Virtual Games to Real-World Play](https://arxiv.org/abs/2506.18901)
*Wenqiang Sun,Fangyun Wei,Jinjing Zhao,Xi Chen,Zilong Chen,Hongyang Zhang,Jun Zhang,Yan Lu*

Main category: cs.CV

TL;DR: RealPlay is a neural network-based game engine for generating photorealistic, interactive videos from user controls, addressing challenges like low-latency feedback and temporal consistency.


<details>
  <summary>Details</summary>
Motivation: To create a system that generates realistic, interactive video sequences resembling real-world footage, unlike prior game-style visuals.

Method: Uses iterative chunk-wise prediction for low-latency feedback, ensures temporal consistency, and responds accurately to controls. Trained on labeled game data and unlabeled real-world videos without action annotations.

Result: Generalizes control signals from virtual to real-world scenarios and handles diverse entities (e.g., bicycles, pedestrians) beyond training labels (car racing game).

Conclusion: RealPlay demonstrates effective generalization and realistic video generation, enabling interactive photorealistic video creation.

Abstract: We introduce RealPlay, a neural network-based real-world game engine that
enables interactive video generation from user control signals. Unlike prior
works focused on game-style visuals, RealPlay aims to produce photorealistic,
temporally consistent video sequences that resemble real-world footage. It
operates in an interactive loop: users observe a generated scene, issue a
control command, and receive a short video chunk in response. To enable such
realistic and responsive generation, we address key challenges including
iterative chunk-wise prediction for low-latency feedback, temporal consistency
across iterations, and accurate control response. RealPlay is trained on a
combination of labeled game data and unlabeled real-world videos, without
requiring real-world action annotations. Notably, we observe two forms of
generalization: (1) control transfer-RealPlay effectively maps control signals
from virtual to real-world scenarios; and (2) entity transfer-although training
labels originate solely from a car racing game, RealPlay generalizes to control
diverse real-world entities, including bicycles and pedestrians, beyond
vehicles. Project page can be found: https://wenqsun.github.io/RealPlay/

</details>


### [259] [VMem: Consistent Interactive Video Scene Generation with Surfel-Indexed View Memory](https://arxiv.org/abs/2506.18903)
*Runjia Li,Philip Torr,Andrea Vedaldi,Tomas Jakab*

Main category: cs.CV

TL;DR: A novel memory mechanism, Surfel-Indexed View Memory (VMem), improves long-term scene coherence in video generation by efficiently retrieving relevant past views.


<details>
  <summary>Details</summary>
Motivation: Existing methods either accumulate errors in 3D reconstruction or struggle with long-term coherence due to short context windows.

Method: VMem indexes past views geometrically using 3D surface elements (surfels) for efficient retrieval during new view generation.

Result: Outperforms existing methods in maintaining scene coherence and camera control with lower computational cost.

Conclusion: VMem is a scalable and effective solution for long-term scene synthesis in video generation.

Abstract: We propose a novel memory mechanism to build video generators that can
explore environments interactively. Similar results have previously been
achieved by out-painting 2D views of the scene while incrementally
reconstructing its 3D geometry, which quickly accumulates errors, or by video
generators with a short context window, which struggle to maintain scene
coherence over the long term. To address these limitations, we introduce
Surfel-Indexed View Memory (VMem), a mechanism that remembers past views by
indexing them geometrically based on the 3D surface elements (surfels) they
have observed. VMem enables the efficient retrieval of the most relevant past
views when generating new ones. By focusing only on these relevant views, our
method produces consistent explorations of imagined environments at a fraction
of the computational cost of using all past views as context. We evaluate our
approach on challenging long-term scene synthesis benchmarks and demonstrate
superior performance compared to existing methods in maintaining scene
coherence and camera control.

</details>


### [260] [TC-Light: Temporally Consistent Relighting for Dynamic Long Videos](https://arxiv.org/abs/2506.18904)
*Yang Liu,Chuanchen Luo,Zimo Tang,Yingyan Li,Yuran Yang,Yuanyong Ning,Lue Fan,Junran Peng,Zhaoxiang Zhang*

Main category: cs.CV

TL;DR: TC-Light introduces a two-stage post-optimization method for relighting long videos with complex dynamics, ensuring temporal consistency and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing video relighting techniques are limited to portraits or struggle with temporal consistency and computation efficiency, hindering broader applications.

Method: TC-Light uses a two-stage approach: optimizing appearance embedding for global illumination alignment and refining fine-grained texture/lighting with Unique Video Tensor (UVT).

Result: The method achieves physically plausible relighting with superior temporal coherence and low computation cost, validated on a new benchmark.

Conclusion: TC-Light advances video relighting by addressing key limitations, offering practical benefits for content creation and AI data scaling.

Abstract: Editing illumination in long videos with complex dynamics has significant
value in various downstream tasks, including visual content creation and
manipulation, as well as data scaling up for embodied AI through sim2real and
real2real transfer. Nevertheless, existing video relighting techniques are
predominantly limited to portrait videos or fall into the bottleneck of
temporal consistency and computation efficiency. In this paper, we propose
TC-Light, a novel paradigm characterized by the proposed two-stage post
optimization mechanism. Starting from the video preliminarily relighted by an
inflated video relighting model, it optimizes appearance embedding in the first
stage to align global illumination. Then it optimizes the proposed canonical
video representation, i.e., Unique Video Tensor (UVT), to align fine-grained
texture and lighting in the second stage. To comprehensively evaluate
performance, we also establish a long and highly dynamic video benchmark.
Extensive experiments show that our method enables physically plausible
relighting results with superior temporal coherence and low computation cost.
The code and video demos are available at
https://dekuliutesla.github.io/tclight/.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [261] [BlenderFusion: 3D-Grounded Visual Editing and Generative Compositing](https://arxiv.org/abs/2506.17450)
*Jiacheng Chen,Ramin Mehran,Xuhui Jia,Saining Xie,Sanghyun Woo*

Main category: cs.GR

TL;DR: BlenderFusion is a framework for generative visual compositing, enabling scene synthesis by recomposing objects, camera, and background through layering, editing, and compositing.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of complex compositional scene editing by providing a 3D-grounded and generative approach.

Method: Uses a pipeline of layering (segmenting inputs into 3D entities), editing in Blender, and generative compositing with a diffusion model fine-tuned on video frames using source masking and simulated object jittering.

Result: Outperforms prior methods in complex scene editing tasks.

Conclusion: BlenderFusion offers a robust solution for generative scene synthesis and editing.

Abstract: We present BlenderFusion, a generative visual compositing framework that
synthesizes new scenes by recomposing objects, camera, and background. It
follows a layering-editing-compositing pipeline: (i) segmenting and converting
visual inputs into editable 3D entities (layering), (ii) editing them in
Blender with 3D-grounded control (editing), and (iii) fusing them into a
coherent scene using a generative compositor (compositing). Our generative
compositor extends a pre-trained diffusion model to process both the original
(source) and edited (target) scenes in parallel. It is fine-tuned on video
frames with two key training strategies: (i) source masking, enabling flexible
modifications like background replacement; (ii) simulated object jittering,
facilitating disentangled control over objects and camera. BlenderFusion
significantly outperforms prior methods in complex compositional scene editing
tasks.

</details>


### [262] [3D Gaussian Splatting for Fine-Detailed Surface Reconstruction in Large-Scale Scene](https://arxiv.org/abs/2506.17636)
*Shihan Chen,Zhaojin Li,Zeyu Chen,Qingsong Yan,Gaoyang Shen,Ran Duan*

Main category: cs.GR

TL;DR: A novel method for large-scale 3D surface reconstruction using a coarse-to-fine strategy, adaptive partitioning, and appearance decoupling, outperforming existing techniques.


<details>
  <summary>Details</summary>
Motivation: Challenges in scaling 3D Gaussian Splatting to large scenes due to computational demands and dynamic outdoor appearances, limiting applications like aerial surveying and autonomous driving.

Method: Coarse-to-fine reconstruction, adaptive scene partitioning, decoupling appearance model, transient mask for moving objects, multi-view constraint expansion, and single-view regularization for texture-less areas.

Result: Outperforms NeRF-based and Gaussian-based methods on GauU-Scene V2 dataset, achieving high-fidelity results from full-size image optimization.

Conclusion: Proposed method effectively addresses large-scale reconstruction challenges, with open-source code for future use.

Abstract: Recent developments in 3D Gaussian Splatting have made significant advances
in surface reconstruction. However, scaling these methods to large-scale scenes
remains challenging due to high computational demands and the complex dynamic
appearances typical of outdoor environments. These challenges hinder the
application in aerial surveying and autonomous driving. This paper proposes a
novel solution to reconstruct large-scale surfaces with fine details,
supervised by full-sized images. Firstly, we introduce a coarse-to-fine
strategy to reconstruct a coarse model efficiently, followed by adaptive scene
partitioning and sub-scene refining from image segments. Additionally, we
integrate a decoupling appearance model to capture global appearance variations
and a transient mask model to mitigate interference from moving objects.
Finally, we expand the multi-view constraint and introduce a single-view
regularization for texture-less areas. Our experiments were conducted on the
publicly available dataset GauU-Scene V2, which was captured using unmanned
aerial vehicles. To the best of our knowledge, our method outperforms existing
NeRF-based and Gaussian-based methods, achieving high-fidelity visual results
and accurate surface from full-size image optimization. Open-source code will
be available on GitHub.

</details>


### [263] [Collaborative Texture Filtering](https://arxiv.org/abs/2506.17770)
*Tomas Akenine-Mller,Pontus Ebelin,Matt Pharr,Bartlomiej Wronski*

Main category: cs.GR

TL;DR: The paper introduces novel algorithms for texture filtering using GPU wave communication to avoid repeated texel decompression, improving quality and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing stochastic texture filtering (STF) techniques suffer from visual artifacts and inefficiencies under magnification, despite recent improvements.

Method: Leverages GPU wave communication intrinsics to share decoded texel values between pixels, reducing decompression overhead. Introduces zero-error filtering for large magnification and high-quality fallback methods for other cases.

Result: Achieves zero-error filtering with 1 texel evaluations per pixel for large magnification and higher quality than prior methods in remaining cases.

Conclusion: The proposed algorithms significantly enhance texture filtering quality and efficiency, addressing limitations of prior STF techniques.

Abstract: Recent advances in texture compression provide major improvements in
compression ratios, but cannot use the GPU's texture units for decompression
and filtering. This has led to the development of stochastic texture filtering
(STF) techniques to avoid the high cost of multiple texel evaluations with such
formats. Unfortunately, those methods can give undesirable visual appearance
changes under magnification and may contain visible noise and flicker despite
the use of spatiotemporal denoisers. Recent work substantially improves the
quality of magnification filtering with STF by sharing decoded texel values
between nearby pixels (Wronski 2025). Using GPU wave communication intrinsics,
this sharing can be performed inside actively executing shaders without memory
traffic overhead. We take this idea further and present novel algorithms that
use wave communication between lanes to avoid repeated texel decompression
prior to filtering. By distributing unique work across lanes, we can achieve
zero-error filtering using <=1 texel evaluations per pixel given a sufficiently
large magnification factor. For the remaining cases, we propose novel filtering
fallback methods that also achieve higher quality than prior approaches.

</details>


### [264] [Auto-Regressive Surface Cutting](https://arxiv.org/abs/2506.18017)
*Yang Li,Victor Cheung,Xinhai Liu,Yuguang Chen,Zhongjin Luo,Biwen Lei,Haohan Weng,Zibo Zhao,Jingwei Huang,Zhuo Chen,Chunchao Guo*

Main category: cs.GR

TL;DR: SeamGPT is an auto-regressive model for surface cutting, mimicking professional workflows to generate semantically coherent seams, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing surface cutting methods produce fragmented atlases lacking semantic coherence, limiting their practical utility.

Method: Formulates surface cutting as a next token prediction task using a GPT-style transformer to predict seam segments from encoded shape conditions.

Result: Achieves exceptional performance on UV unwrapping benchmarks and enhances 3D segmentation tools with clean boundaries.

Conclusion: SeamGPT provides a robust solution for surface cutting, improving semantic coherence and performance over traditional methods.

Abstract: Surface cutting is a fundamental task in computer graphics, with applications
in UV parameterization, texture mapping, and mesh decomposition. However,
existing methods often produce technically valid but overly fragmented atlases
that lack semantic coherence. We introduce SeamGPT, an auto-regressive model
that generates cutting seams by mimicking professional workflows. Our key
technical innovation lies in formulating surface cutting as a next token
prediction task: sample point clouds on mesh vertices and edges, encode them as
shape conditions, and employ a GPT-style transformer to sequentially predict
seam segments with quantized 3D coordinates. Our approach achieves exceptional
performance on UV unwrapping benchmarks containing both manifold and
non-manifold meshes, including artist-created, and 3D-scanned models. In
addition, it enhances existing 3D segmentation tools by providing clean
boundaries for part decomposition.

</details>


### [265] [Morse: Dual-Sampling for Lossless Acceleration of Diffusion Models](https://arxiv.org/abs/2506.18251)
*Chao Li,Jiawei Fan,Anbang Yao*

Main category: cs.GR

TL;DR: Morse is a dual-sampling framework for accelerating diffusion models losslessly, using Dash (pre-trained model) and Dot (faster residual feedback model) to improve efficiency without quality loss.


<details>
  <summary>Details</summary>
Motivation: To accelerate diffusion models without compromising generation quality by leveraging jump sampling and adaptive residual feedback.

Method: Uses two models: Dash (pre-trained diffusion model with jump sampling) and Dot (faster model generating residual feedback). Outputs are chained for efficiency.

Result: Achieves 1.78X to 3.31X speedup over baselines on 6 tasks, and generalizes to Latent Consistency Model (LCM-SDXL).

Conclusion: Morse efficiently accelerates diffusion models losslessly and is adaptable to other models like LCM-SDXL.

Abstract: In this paper, we present Morse, a simple dual-sampling framework for
accelerating diffusion models losslessly. The key insight of Morse is to
reformulate the iterative generation (from noise to data) process via taking
advantage of fast jump sampling and adaptive residual feedback strategies.
Specifically, Morse involves two models called Dash and Dot that interact with
each other. The Dash model is just the pre-trained diffusion model of any type,
but operates in a jump sampling regime, creating sufficient space for sampling
efficiency improvement. The Dot model is significantly faster than the Dash
model, which is learnt to generate residual feedback conditioned on the
observations at the current jump sampling point on the trajectory of the Dash
model, lifting the noise estimate to easily match the next-step estimate of the
Dash model without jump sampling. By chaining the outputs of the Dash and Dot
models run in a time-interleaved fashion, Morse exhibits the merit of flexibly
attaining desired image generation performance while improving overall runtime
efficiency. With our proposed weight sharing strategy between the Dash and Dot
models, Morse is efficient for training and inference. Our method shows a
lossless speedup of 1.78X to 3.31X on average over a wide range of sampling
step budgets relative to 9 baseline diffusion models on 6 image generation
tasks. Furthermore, we show that our method can be also generalized to improve
the Latent Consistency Model (LCM-SDXL, which is already accelerated with
consistency distillation technique) tailored for few-step text-to-image
synthesis. The code and models are available at
https://github.com/deep-optimization/Morse.

</details>


### [266] [What You Think Is What You Get: Bridge User Intent and Transfer Function Design through Multimodal Large Language Models](https://arxiv.org/abs/2506.18407)
*Yiyao Wang,Bo Pan,Ke Wang,Han Liu,Jinyuan Mao,Yuxin Liu,Minfeng Zhu,Bo Zhang,Weifeng Chen,Xiuqi Huang,Wei Chen*

Main category: cs.GR

TL;DR: The paper introduces the WYTWYG framework, leveraging MLLMs to guide transfer function (TF) optimization in direct volume rendering (DVR), addressing challenges of large exploration space and weak generalizability.


<details>
  <summary>Details</summary>
Motivation: Designing effective TFs in DVR is unintuitive due to the semantic gap between user intent and TF parameters. Existing TF optimization methods struggle with large exploration spaces and lack generalizability.

Method: The WYTWYG framework uses MLLMs for TF optimization, featuring an evolution-based explorer for TF space exploration and an MLLM-based evaluator for visual guidance. A TF interactive design system is also proposed.

Result: Case studies demonstrate the framework's general applicability, and experiments validate the effectiveness of its components.

Conclusion: The WYTWYG framework successfully bridges the gap between user intent and TF optimization, offering a more intuitive and generalizable solution for DVR.

Abstract: Direct volume rendering (DVR) is a fundamental technique for visualizing
volumetric data, with transfer functions (TFs) playing a crucial role in
extracting meaningful structures. However, designing effective TFs remains
unintuitive due to the semantic gap between user intent and TF parameter space.
Researchers have developed numerous TF optimization methods to bridge this gap.
However, existing methods still face two challenges: large exploration space
and weak generalizability. To address these issues, we propose What You Think
is What You Get (WYTWYG) framework, which leveraging Multi-model Large Language
Models (MLLMs) to guide the TF optimization based on user intent. Specifically,
we first introduce a novel TF optimization approach comprising two core
components: (1) an evolution-based explorer for effective exploration of the TF
space, and (2) a volume rendering quality evaluator based on MLLMs to provide
generalizable visual guidance. We further propose a TF interactive design
system based on this approach. We demonstrate the general applicability of our
framework through three case studies, and validate the effectiveness of each
component through extensive experiments. Our code is available at:
https://github.com/wyysteelhead/TFevolve.

</details>


### [267] [BulletGen: Improving 4D Reconstruction with Bullet-Time Generation](https://arxiv.org/abs/2506.18601)
*Denys Rozumnyi,Jonathon Luiten,Numair Khan,Johannes Schnberger,Peter Kontschieder*

Main category: cs.GR

TL;DR: BulletGen uses generative models to enhance 4D Gaussian-based dynamic scene reconstruction by aligning diffusion-based video generation with frozen frames, achieving top results in novel-view synthesis and tracking.


<details>
  <summary>Details</summary>
Motivation: The challenge of reconstructing unseen regions and handling ambiguity in monocular depth estimation from casually captured videos motivates the need for a robust method.

Method: BulletGen aligns diffusion-based video generation with a frozen step in 4D Gaussian reconstruction, using generated frames to optimize the model.

Result: The method achieves state-of-the-art performance in novel-view synthesis and 2D/3D tracking tasks.

Conclusion: BulletGen effectively blends generative content with dynamic scenes, addressing key challenges in monocular video reconstruction.

Abstract: Transforming casually captured, monocular videos into fully immersive dynamic
experiences is a highly ill-posed task, and comes with significant challenges,
e.g., reconstructing unseen regions, and dealing with the ambiguity in
monocular depth estimation. In this work we introduce BulletGen, an approach
that takes advantage of generative models to correct errors and complete
missing information in a Gaussian-based dynamic scene representation. This is
done by aligning the output of a diffusion-based video generation model with
the 4D reconstruction at a single frozen "bullet-time" step. The generated
frames are then used to supervise the optimization of the 4D Gaussian model.
Our method seamlessly blends generative content with both static and dynamic
scene components, achieving state-of-the-art results on both novel-view
synthesis, and 2D/3D tracking tasks.

</details>


### [268] [DuetGen: Music Driven Two-Person Dance Generation via Hierarchical Masked Modeling](https://arxiv.org/abs/2506.18680)
*Anindita Ghosh,Bing Zhou,Rishabh Dabral,Jian Wang,Vladislav Golyanik,Christian Theobalt,Philipp Slusallek,Chuan Guo*

Main category: cs.GR

TL;DR: DuetGen is a framework for generating interactive two-person dances from music using a two-stage approach with hierarchical motion encoding and masked transformers.


<details>
  <summary>Details</summary>
Motivation: The challenge lies in synchronizing two dancers with each other and the music, requiring a novel approach to capture intricate interactions.

Method: A two-stage solution: 1) Encoding two-person motions into discrete tokens using a VQ-VAE, and 2) Generating tokens from music with masked transformers. Hierarchical and coarse-to-fine learning is employed.

Result: DuetGen achieves synchronized and interactive dances, excelling in motion realism, music-dance alignment, and partner coordination, as shown in experiments.

Conclusion: The framework sets a new standard for generating realistic and interactive two-person dances from music.

Abstract: We present DuetGen, a novel framework for generating interactive two-person
dances from music. The key challenge of this task lies in the inherent
complexities of two-person dance interactions, where the partners need to
synchronize both with each other and with the music. Inspired by the recent
advances in motion synthesis, we propose a two-stage solution: encoding
two-person motions into discrete tokens and then generating these tokens from
music. To effectively capture intricate interactions, we represent both
dancers' motions as a unified whole to learn the necessary motion tokens, and
adopt a coarse-to-fine learning strategy in both the stages. Our first stage
utilizes a VQ-VAE that hierarchically separates high-level semantic features at
a coarse temporal resolution from low-level details at a finer resolution,
producing two discrete token sequences at different abstraction levels.
Subsequently, in the second stage, two generative masked transformers learn to
map music signals to these dance tokens: the first producing high-level
semantic tokens, and the second, conditioned on music and these semantic
tokens, producing the low-level tokens. We train both transformers to learn to
predict randomly masked tokens within the sequence, enabling them to
iteratively generate motion tokens by filling an empty token sequence during
inference. Through the hierarchical masked modeling and dedicated interaction
representation, DuetGen achieves the generation of synchronized and interactive
two-person dances across various genres. Extensive experiments and user studies
on a benchmark duet dance dataset demonstrate state-of-the-art performance of
DuetGen in motion realism, music-dance alignment, and partner coordination.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [269] [FaithfulSAE: Towards Capturing Faithful Features with Sparse Autoencoders without External Dataset Dependencies](https://arxiv.org/abs/2506.17673)
*Seonglae Cho,Harryn Oh,Donghyun Lee,Luis Eduardo Rodrigues Vieira,Andrew Bermingham,Ziad El Sayed*

Main category: cs.LG

TL;DR: FaithfulSAE trains SAEs on synthetic datasets to improve stability and reduce fake features, outperforming traditional SAEs trained on external data.


<details>
  <summary>Details</summary>
Motivation: Address instability and fake features in SAEs caused by training on out-of-distribution external datasets.

Method: Propose FaithfulSAE, training SAEs on the model's own synthetic dataset to avoid OOD issues.

Result: FaithfulSAEs are more stable, outperform web-trained SAEs in probing tasks, and reduce fake features in most models.

Conclusion: FaithfulSAE eliminates external dataset dependency, improving interpretability by better capturing model-internal features.

Abstract: Sparse Autoencoders (SAEs) have emerged as a promising solution for
decomposing large language model representations into interpretable features.
However, Paulo and Belrose (2025) have highlighted instability across different
initialization seeds, and Heap et al. (2025) have pointed out that SAEs may not
capture model-internal features. These problems likely stem from training SAEs
on external datasets - either collected from the Web or generated by another
model - which may contain out-of-distribution (OOD) data beyond the model's
generalisation capabilities. This can result in hallucinated SAE features,
which we term "Fake Features", that misrepresent the model's internal
activations. To address these issues, we propose FaithfulSAE, a method that
trains SAEs on the model's own synthetic dataset. Using FaithfulSAEs, we
demonstrate that training SAEs on less-OOD instruction datasets results in SAEs
being more stable across seeds. Notably, FaithfulSAEs outperform SAEs trained
on web-based datasets in the SAE probing task and exhibit a lower Fake Feature
Ratio in 5 out of 7 models. Overall, our approach eliminates the dependency on
external datasets, advancing interpretability by better capturing
model-internal features while highlighting the often neglected importance of
SAE training datasets.

</details>


### [270] [Beyond instruction-conditioning, MoTE: Mixture of Task Experts for Multi-task Embedding Models](https://arxiv.org/abs/2506.17781)
*Miguel Romero,Shuoyang Ding,Corey D. Barret,Georgiana Dinu,George Karypis*

Main category: cs.LG

TL;DR: The paper introduces MoTE, a transformer block using task-specialized parameters and Task-Aware Contrastive Learning (TACL) to improve embedding specialization without changing instructions, data, or inference.


<details>
  <summary>Details</summary>
Motivation: Address limitations of instruction-conditioning in low-capacity models for embedding specialization.

Method: Propose Mixture of Task Experts (MoTE) transformer block with Task-Aware Contrastive Learning (TACL).

Result: MoTE achieves 64% higher performance in retrieval (+3.27 to +5.21) and 43% higher overall (+1.81 to +2.60).

Conclusion: MoTE enhances embedding specialization without extra costs or changes to existing setups.

Abstract: Dense embeddings are fundamental to modern machine learning systems, powering
Retrieval-Augmented Generation (RAG), information retrieval, and representation
learning. While instruction-conditioning has become the dominant approach for
embedding specialization, its direct application to low-capacity models imposes
fundamental representational constraints that limit the performance gains
derived from specialization. In this paper, we analyze these limitations and
introduce the Mixture of Task Experts (MoTE) transformer block, which leverages
task-specialized parameters trained with Task-Aware Contrastive Learning
(\tacl) to enhance the model ability to generate specialized embeddings.
Empirical results show that MoTE achieves $64\%$ higher performance gains in
retrieval datasets ($+3.27 \rightarrow +5.21$) and $43\%$ higher performance
gains across all datasets ($+1.81 \rightarrow +2.60$). Critically, these gains
are achieved without altering instructions, training data, inference time, or
number of active parameters.

</details>


### [271] [Aligning Frozen LLMs by Reinforcement Learning: An Iterative Reweight-then-Optimize Approach](https://arxiv.org/abs/2506.17828)
*Xinnan Zhang,Chenliang Li,Siliang Zeng,Jiaxiang Li,Zhongruo Wang,Kaixiang Lin,Songtao Lu,Alfredo Garcia,Mingyi Hong*

Main category: cs.LG

TL;DR: IRO is a reinforcement learning framework for aligning LLMs without updating model weights, using iterative resampling and lightweight value functions for test-time guidance.


<details>
  <summary>Details</summary>
Motivation: Existing methods like RLHF and DPO require model weight updates, limiting their use when weights are inaccessible or during test-time. Test-time methods are costly and suboptimal due to imperfect reward functions.

Method: IRO iteratively samples candidates, resamples using value functions, and trains lightweight value functions to guide decoding without altering the base model's weights.

Result: IRO enables alignment of frozen models without weight access, offering a flexible and efficient alternative to traditional fine-tuning.

Conclusion: IRO provides a practical solution for aligning LLMs in scenarios where weight updates are impractical, balancing efficiency and performance.

Abstract: Aligning large language models (LLMs) with human preferences usually requires
fine-tuning methods such as RLHF and DPO. These methods directly optimize the
model parameters, so they cannot be used in test-time to improve model
performance, nor are they applicable when the model weights are not accessible.
In contrast, test-time methods sidestep weight updates by leveraging reward
functions to guide and improve output quality. However, they incur high
inference costs, and their one-shot guidance is often based on imperfect reward
or value functions, leading to suboptimal outputs. In this work, we present a
method named Iterative Reweight-then-Optimize (IRO), a reinforcement learning
(RL) framework that performs RL-style alignment of the (frozen) base model
without touching its parameters. During training, each iteration (i) samples
candidates from the base model, (ii) resamples using current value functions,
and (iii) trains a new lightweight value function that guides the next decoding
pass. At test time, the value functions are used to guide the base model
generation via a search-based optimization process. Notably, users can apply
IRO to align a model on their own dataset, similar to OpenAI's reinforcement
fine-tuning (RFT), but without requiring access to the model weights.

</details>


### [272] [AdapThink: Adaptive Thinking Preferences for Reasoning Language Model](https://arxiv.org/abs/2506.18237)
*Xu Wan,Wei Wang,Wenyue Xu,Wotao Yin,Jie Song,Mingyang Sun*

Main category: cs.LG

TL;DR: AdapThink is an adaptive post-training framework for language models that improves reasoning efficiency by dynamically adjusting reflection preferences and balancing solution accuracy with diversity.


<details>
  <summary>Details</summary>
Motivation: Current RL-based post-training methods lack adaptability for varying question complexities, leading to inefficient reasoning.

Method: AdapThink uses a group-relative reward function and diversity-aware sampling to dynamically adjust reflection preferences and balance accuracy with diversity.

Result: Experiments show AdapThink enhances reasoning efficiency and maintains performance on mathematical reasoning tasks.

Conclusion: AdapThink effectively mitigates inefficiencies in reasoning while preserving model performance.

Abstract: Reinforcement Learning (RL)-based post-training has significantly advanced
the complex reasoning capabilities of language models, fostering sophisticated
self-reflection processes. However, this ``slow thinking'' paradigm presents a
critical challenge to reasoning efficiency: models may expend excessive
computation on simple questions and shift reasoning prematurely for complex
ones. Previous mechanisms typically rely on static length budgets or predefined
rules, lacking the adaptability for varying question complexities and models'
evolving capabilities. To this end, we propose AdapThink, an adaptive
post-training framework designed to induce more efficient thinking while
maintaining the performance of reasoning language models. Specifically,
AdapThink incorporates two key mechanisms: 1) A group-relative reward function
that leverages model confidence and response's characteristic to dynamically
adjust the preference of reflection-related transition words without resorting
to a fixed length preference. 2) A diversity-aware sampling mechanism that
balances the training group's solution accuracy with reasoning diversity via an
entropy-guided score. Experiments on several mathematical reasoning datasets
with DeepSeek-distilled models demonstrate AdapThink's advantages in enabling
adaptive reasoning patterns and mitigating the inefficiencies.

</details>


### [273] [RLPR: Extrapolating RLVR to General Domains without Verifiers](https://arxiv.org/abs/2506.18254)
*Tianyu Yu,Bo Ji,Shouli Wang,Shu Yao,Zefan Wang,Ganqu Cui,Lifan Yuan,Ning Ding,Yuan Yao,Zhiyuan Liu,Maosong Sun,Tat-Seng Chua*

Main category: cs.LG

TL;DR: RLPR is a verifier-free framework that uses LLM's token probability scores as rewards, improving reasoning capabilities across general and mathematical domains.


<details>
  <summary>Details</summary>
Motivation: Current RLVR methods are limited to mathematical and code domains due to reliance on domain-specific verifiers, which are complex and unscalable.

Method: RLPR leverages LLM's intrinsic token probabilities for correct answers as rewards, introducing prob-to-reward and stabilizing methods to manage noise.

Result: RLPR outperforms VeriFree and General-Reasoner, improving reasoning in general and mathematical benchmarks for models like Gemma, Llama, and Qwen.

Conclusion: RLPR successfully extends RLVR to broader domains without verifiers, demonstrating superior performance and scalability.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) demonstrates promising
potential in advancing the reasoning capabilities of LLMs. However, its success
remains largely confined to mathematical and code domains. This primary
limitation stems from the heavy reliance on domain-specific verifiers, which
results in prohibitive complexity and limited scalability. To address the
challenge, our key observation is that LLM's intrinsic probability of
generating a correct free-form answer directly indicates its own evaluation of
the reasoning reward (i.e., how well the reasoning process leads to the correct
answer). Building on this insight, we propose RLPR, a simple verifier-free
framework that extrapolates RLVR to broader general domains. RLPR uses the
LLM's own token probability scores for reference answers as the reward signal
and maximizes the expected reward during training. We find that addressing the
high variance of this noisy probability reward is crucial to make it work, and
propose prob-to-reward and stabilizing methods to ensure a precise and stable
reward from LLM intrinsic probabilities. Comprehensive experiments in four
general-domain benchmarks and three mathematical benchmarks show that RLPR
consistently improves reasoning capabilities in both areas for Gemma, Llama,
and Qwen based models. Notably, RLPR outperforms concurrent VeriFree by 7.6
points on TheoremQA and 7.5 points on Minerva, and even surpasses strong
verifier-model-dependent approaches General-Reasoner by 1.6 average points
across seven benchmarks.

</details>


### [274] [Confucius3-Math: A Lightweight High-Performance Reasoning LLM for Chinese K-12 Mathematics Learning](https://arxiv.org/abs/2506.18330)
*Lixin Wu,Na Cai,Qiao Cheng,Jiachen Wang,Yitao Duan*

Main category: cs.LG

TL;DR: Confucius3-Math is a 14B-parameter open-source LLM optimized for math tasks, efficient on consumer GPUs, and tailored for Chinese K-12 education, using RL innovations for stability and performance.


<details>
  <summary>Details</summary>
Motivation: To enhance AI-driven education, specifically for Chinese K-12 math learning, with a cost-effective, high-performance model.

Method: Post-training with large-scale RL, incorporating Targeted Entropy Regularization, Recent Sample Recovery, and Policy-Specific Hardness Weighting.

Result: Achieves SOTA on math tasks, outperforming larger models, and aligns with Chinese curriculum.

Conclusion: Demonstrates feasibility of domain-specific, low-cost reasoning models; model and code are open-sourced.

Abstract: We introduce Confucius3-Math, an open-source large language model with 14B
parameters that (1) runs efficiently on a single consumer-grade GPU; (2)
achieves SOTA performances on a range of mathematical reasoning tasks,
outperforming many models with significantly larger sizes. In particular, as
part of our mission to enhancing education and knowledge dissemination with AI,
Confucius3-Math is specifically committed to mathematics learning for Chinese
K-12 students and educators. Built via post-training with large-scale
reinforcement learning (RL), Confucius3-Math aligns with national curriculum
and excels at solving main-stream Chinese K-12 mathematical problems with low
cost. In this report we share our development recipe, the challenges we
encounter and the techniques we develop to overcome them. In particular, we
introduce three technical innovations: Targeted Entropy Regularization, Recent
Sample Recovery and Policy-Specific Hardness Weighting. These innovations
encompass a new entropy regularization, a novel data scheduling policy, and an
improved group-relative advantage estimator. Collectively, they significantly
stabilize the RL training, improve data efficiency, and boost performance. Our
work demonstrates the feasibility of building strong reasoning models in a
particular domain at low cost. We open-source our model and code at
https://github.com/netease-youdao/Confucius3-Math.

</details>


### [275] [SlimMoE: Structured Compression of Large MoE Models via Expert Slimming and Distillation](https://arxiv.org/abs/2506.18349)
*Zichong Li,Chen Liang,Zixuan Zhang,Ilgee Hong,Young Jin Kim,Weizhu Chen,Tuo Zhao*

Main category: cs.LG

TL;DR: SlimMoE compresses large MoE models into smaller, efficient variants using multi-stage compression, reducing memory requirements while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Address the high memory costs and inefficiency of fine-tuning or deploying large MoE models in resource-constrained environments.

Method: Multi-stage compression framework involving expert slimming and knowledge transfer, reducing parameters without full retraining.

Result: Compressed models (Phi-mini-MoE and Phi-tiny-MoE) outperform similar-sized models and compete with larger ones, using less training data and resources.

Conclusion: Structured pruning and staged distillation enable high-quality, compact MoE models, promoting wider adoption of MoE architectures.

Abstract: The Mixture of Experts (MoE) architecture has emerged as a powerful paradigm
for scaling large language models (LLMs) while maintaining inference
efficiency. However, their enormous memory requirements make them prohibitively
expensive to fine-tune or deploy in resource-constrained environments. To
address this challenge, we introduce SlimMoE, a multi-stage compression
framework for transforming large MoE models into much smaller, efficient
variants without incurring the prohibitive costs of training from scratch. Our
method systematically reduces parameter counts by slimming experts and
transferring knowledge through intermediate stages, effectively mitigating the
performance degradation common in one-shot pruning approaches. Using this
framework, we compress Phi 3.5-MoE (41.9B total/6.6B activated parameters) to
create Phi-mini-MoE (7.6B total/2.4B activated parameters) and Phi-tiny-MoE
(3.8B total/1.1B activated parameters) using only 400B tokens--less than 10% of
the original model's training data. These compressed models can be fine-tuned
on a single GPU (A100 for Phi-mini-MoE, A6000 for Phi-tiny-MoE), making them
highly suitable for academic and resource-limited settings. Our experiments
demonstrate that these compressed models outperform others of similar size and
remain competitive with larger models. For instance, Phi-mini-MoE achieves
similar or better performance to Phi-3-mini using only 2/3 of the activated
parameters and yields comparable MMLU scores to Llama 3.1 8B despite having
significantly lower latency. Our findings demonstrate that structured pruning
combined with staged distillation offers an effective path to creating
high-quality, compact MoE models, paving the way for broader adoption of MoE
architectures. We make our models publicly available at
https://huggingface.co/microsoft/Phi-mini-MoE-instruct and
https://huggingface.co/microsoft/Phi-tiny-MoE-instruct .

</details>


### [276] [No Training Wheels: Steering Vectors for Bias Correction at Inference Time](https://arxiv.org/abs/2506.18598)
*Aviral Gupta,Armaan Sethi,Ameesh Sethi*

Main category: cs.LG

TL;DR: A training-free method using "bias vectors" to reduce classification bias in neural networks by subtracting mean activation differences between majority and minority groups.


<details>
  <summary>Details</summary>
Motivation: Addressing biases in classifiers without costly retraining or compute, inspired by steering vectors in language models.

Method: Compute bias vectors as mean activation differences between groups and subtract them from the model's residual stream.

Result: Reduced classification bias and improved worst-group accuracy in transformer-like classifiers.

Conclusion: Demonstrates a cheap, inference-time solution for bias mitigation in classification models.

Abstract: Neural network classifiers trained on datasets with uneven group
representation often inherit class biases and learn spurious correlations.
These models may perform well on average but consistently fail on atypical
groups. For example, in hair color classification, datasets may over-represent
females with blond hair, reinforcing stereotypes. Although various algorithmic
and data-centric methods have been proposed to address such biases, they often
require retraining or significant compute. In this work, we propose a cheap,
training-free method inspired by steering vectors used to edit behaviors in
large language models. We compute the difference in mean activations between
majority and minority groups to define a "bias vector," which we subtract from
the model's residual stream. This leads to reduced classification bias and
improved worst-group accuracy. We explore multiple strategies for extracting
and applying these vectors in transformer-like classifiers, showing that
steering vectors, traditionally used in generative models, can also be
effective in classification. More broadly, we showcase an extremely cheap,
inference time, training free method to mitigate bias in classification models.

</details>


### [277] [ReDit: Reward Dithering for Improved LLM Policy Optimization](https://arxiv.org/abs/2506.18631)
*Chenxing Wei,Jiarui Yu,Ying Tiffany He,Hande Dong,Yao Shu,Fei Yu*

Main category: cs.LG

TL;DR: ReDit (Reward Dithering) improves LLM reasoning by adding random noise to discrete rewards, solving gradient issues and speeding up convergence.


<details>
  <summary>Details</summary>
Motivation: Discrete rewards in rule-based systems cause gradient anomalies, unstable optimization, and slow convergence.

Method: ReDit dithers discrete rewards with random noise, providing smoother gradients and encouraging exploration.

Result: ReDit matches vanilla GRPO performance in 10% of the steps and outperforms it by 4% with similar training time.

Conclusion: ReDit effectively mitigates gradient issues, accelerates convergence, and enhances exploration, validated by experiments and theory.

Abstract: DeepSeek-R1 has successfully enhanced Large Language Model (LLM) reasoning
capabilities through its rule-based reward system. While it's a ''perfect''
reward system that effectively mitigates reward hacking, such reward functions
are often discrete. Our experimental observations suggest that discrete rewards
can lead to gradient anomaly, unstable optimization, and slow convergence. To
address this issue, we propose ReDit (Reward Dithering), a method that dithers
the discrete reward signal by adding simple random noise. With this perturbed
reward, exploratory gradients are continuously provided throughout the learning
process, enabling smoother gradient updates and accelerating convergence. The
injected noise also introduces stochasticity into flat reward regions,
encouraging the model to explore novel policies and escape local optima.
Experiments across diverse tasks demonstrate the effectiveness and efficiency
of ReDit. On average, ReDit achieves performance comparable to vanilla GRPO
with only approximately 10% the training steps, and furthermore, still exhibits
a 4% performance improvement over vanilla GRPO when trained for a similar
duration. Visualizations confirm significant mitigation of gradient issues with
ReDit. Moreover, theoretical analyses are provided to further validate these
advantages.

</details>


### [278] [Multi-modal Anchor Gated Transformer with Knowledge Distillation for Emotion Recognition in Conversation](https://arxiv.org/abs/2506.18716)
*Jie Li,Shifei Ding,Lili Guo,Xuan Li*

Main category: cs.LG

TL;DR: The paper proposes MAGTKD, a model for Emotion Recognition in Conversation (ERC) that improves modality-specific representations using prompt learning and knowledge distillation, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing ERC models fail to account for varying modality contributions and introduce complexity by aligning modalities at the frame level.

Method: MAGTKD uses prompt learning for textual modality, knowledge distillation for weaker modalities, and a multi-modal anchor gated transformer for integration.

Result: Experiments on IEMOCAP and MELD datasets show improved modality representations and state-of-the-art performance.

Conclusion: MAGTKD effectively addresses ERC challenges by enhancing modality representations and integration, outperforming previous methods.

Abstract: Emotion Recognition in Conversation (ERC) aims to detect the emotions of
individual utterances within a conversation. Generating efficient and
modality-specific representations for each utterance remains a significant
challenge. Previous studies have proposed various models to integrate features
extracted using different modality-specific encoders. However, they neglect the
varying contributions of modalities to this task and introduce high complexity
by aligning modalities at the frame level. To address these challenges, we
propose the Multi-modal Anchor Gated Transformer with Knowledge Distillation
(MAGTKD) for the ERC task. Specifically, prompt learning is employed to enhance
textual modality representations, while knowledge distillation is utilized to
strengthen representations of weaker modalities. Furthermore, we introduce a
multi-modal anchor gated transformer to effectively integrate utterance-level
representations across modalities. Extensive experiments on the IEMOCAP and
MELD datasets demonstrate the effectiveness of knowledge distillation in
enhancing modality representations and achieve state-of-the-art performance in
emotion recognition. Our code is available at:
https://github.com/JieLi-dd/MAGTKD.

</details>


### [279] [Neural Total Variation Distance Estimators for Changepoint Detection in News Data](https://arxiv.org/abs/2506.18764)
*Csaba Zsolnai,Niels Lrch,Julian Arnold*

Main category: cs.LG

TL;DR: The paper proposes a neural network-based method for detecting changepoints in public discourse using news data, leveraging the learning-by-confusion scheme to identify significant shifts like major historical events.


<details>
  <summary>Details</summary>
Motivation: Understanding societal dynamics by detecting shifts in public discourse, especially in high-dimensional, sparse, and noisy real-world data.

Method: Uses neural networks and the learning-by-confusion scheme to train classifiers distinguishing articles from different time periods, measuring classification accuracy to estimate changepoints.

Result: Effective detection of major events (e.g., 9/11, COVID-19, elections) in synthetic and real-world data (The Guardian), providing quantitative change measures.

Conclusion: The method autonomously identifies discourse shifts with minimal domain knowledge, benefiting journalism, policy, and crisis monitoring.

Abstract: Detecting when public discourse shifts in response to major events is crucial
for understanding societal dynamics. Real-world data is high-dimensional,
sparse, and noisy, making changepoint detection in this domain a challenging
endeavor. In this paper, we leverage neural networks for changepoint detection
in news data, introducing a method based on the so-called learning-by-confusion
scheme, which was originally developed for detecting phase transitions in
physical systems. We train classifiers to distinguish between articles from
different time periods. The resulting classification accuracy is used to
estimate the total variation distance between underlying content distributions,
where significant distances highlight changepoints. We demonstrate the
effectiveness of this method on both synthetic datasets and real-world data
from The Guardian newspaper, successfully identifying major historical events
including 9/11, the COVID-19 pandemic, and presidential elections. Our approach
requires minimal domain knowledge, can autonomously discover significant shifts
in public discourse, and yields a quantitative measure of change in content,
making it valuable for journalism, policy analysis, and crisis monitoring.

</details>


### [280] [PCaM: A Progressive Focus Attention-Based Information Fusion Method for Improving Vision Transformer Domain Adaptation](https://arxiv.org/abs/2506.17232)
*Zelin Zang,Fei Wang,Liangyu Li,Jinlin Wu,Chunshui Zhao,Zhen Lei,Baigui Sun*

Main category: cs.LG

TL;DR: The paper introduces PCaM, a Progressive Focus Cross-Attention Mechanism, to address foreground object mismatch in Vision Transformer-based UDA by filtering background information and enhancing attention consistency.


<details>
  <summary>Details</summary>
Motivation: Existing UDA methods using ViTs suffer from foreground object mismatch, weakening attention consistency and domain alignment.

Method: Proposes PCaM to progressively filter background in cross-attention and an attentional guidance loss to focus on task-relevant regions.

Result: PCaM improves adaptation performance, achieving state-of-the-art results on multiple datasets.

Conclusion: PCaM effectively addresses foreground mismatch, enhancing domain adaptation through attention-guided foreground fusion.

Abstract: Unsupervised Domain Adaptation (UDA) aims to transfer knowledge from a
labeled source domain to an unlabeled target domain. Recent UDA methods based
on Vision Transformers (ViTs) have achieved strong performance through
attention-based feature alignment. However, we identify a key limitation:
foreground object mismatch, where the discrepancy in foreground object size and
spatial distribution across domains weakens attention consistency and hampers
effective domain alignment. To address this issue, we propose the Progressive
Focus Cross-Attention Mechanism (PCaM), which progressively filters out
background information during cross-attention, allowing the model to focus on
and fuse discriminative foreground semantics across domains. We further
introduce an attentional guidance loss that explicitly directs attention toward
task-relevant regions, enhancing cross-domain attention consistency. PCaM is
lightweight, architecture-agnostic, and easy to integrate into existing
ViT-based UDA pipelines. Extensive experiments on Office-Home, DomainNet,
VisDA-2017, and remote sensing datasets demonstrate that PCaM significantly
improves adaptation performance and achieves new state-of-the-art results,
validating the effectiveness of attention-guided foreground fusion for domain
adaptation.

</details>


### [281] [Learning to Adapt Frozen CLIP for Few-Shot Test-Time Domain Adaptation](https://arxiv.org/abs/2506.17307)
*Zhixiang Chi,Li Gu,Huan Liu,Ziqiang Wang,Yanan Wu,Yang Wang,Konstantinos N Plataniotis*

Main category: cs.LG

TL;DR: The paper introduces a method to enhance few-shot test-time domain adaptation by learning directly on the input space alongside CLIP, improving performance on challenging benchmarks.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of relying solely on CLIP's out-of-distribution abilities for domain adaptation, especially with less robust backbones like ViT-B/16.

Method: Proposes a side branch parallel to CLIP, using revert attention for exclusive knowledge, and enhances text features via greedy ensemble and refinement. Features are fused progressively with a domain prompt.

Result: Achieves significant improvements on benchmarks (e.g., +5.1 F1 for iWildCam, +3.1% WC Acc for FMoW).

Conclusion: The approach effectively complements CLIP's knowledge with dataset-specific learning, outperforming prior methods.

Abstract: Few-shot Test-Time Domain Adaptation focuses on adapting a model at test time
to a specific domain using only a few unlabeled examples, addressing domain
shift. Prior methods leverage CLIP's strong out-of-distribution (OOD) abilities
by generating domain-specific prompts to guide its generalized, frozen
features. However, since downstream datasets are not explicitly seen by CLIP,
solely depending on the feature space knowledge is constrained by CLIP's prior
knowledge. Notably, when using a less robust backbone like ViT-B/16,
performance significantly drops on challenging real-world benchmarks. Departing
from the state-of-the-art of inheriting the intrinsic OOD capability of CLIP,
this work introduces learning directly on the input space to complement the
dataset-specific knowledge for frozen CLIP. Specifically, an independent side
branch is attached in parallel with CLIP and enforced to learn exclusive
knowledge via revert attention. To better capture the dataset-specific label
semantics for downstream adaptation, we propose to enhance the inter-dispersion
among text features via greedy text ensemble and refinement. The text and
visual features are then progressively fused in a domain-aware manner by a
generated domain prompt to adapt toward a specific domain. Extensive
experiments show our method's superiority on 5 large-scale benchmarks (WILDS
and DomainNet), notably improving over smaller networks like ViT-B/16 with
gains of \textbf{+5.1} in F1 for iWildCam and \textbf{+3.1\%} in WC Acc for
FMoW.

</details>


### [282] [Origins of Creativity in Attention-Based Diffusion Models](https://arxiv.org/abs/2506.17324)
*Emma Finn,T. Anderson Keller,Manos Theodosis,Demba E. Ba*

Main category: cs.LG

TL;DR: The paper explores how self-attention in diffusion models enhances global image consistency beyond patch-level mosaics, extending prior theory on CNN-based diffusion models.


<details>
  <summary>Details</summary>
Motivation: Understanding the role of self-attention in diffusion models' creativity, as prior theory only explained CNN-induced patch-wise mosaics.

Method: Extends theory to diffusion models with CNN and self-attention layers, analyzing global image consistency. Empirical validation on a crafted dataset.

Result: Self-attention induces globally consistent arrangements of local features in generated images, verified empirically.

Conclusion: Self-attention in diffusion models enhances global coherence, advancing understanding of creative image generation.

Abstract: As diffusion models have become the tool of choice for image generation and
as the quality of the images continues to improve, the question of how
`creativity' originates in diffusion has become increasingly important. The
score matching perspective on diffusion has proven particularly fruitful for
understanding how and why diffusion models generate images that remain
plausible while differing significantly from their training images. In
particular, as explained in (Kamb \& Ganguli, 2024) and others, e.g.,
(Ambrogioni, 2023), theory suggests that if our score matching were optimal, we
would only be able to recover training samples through our diffusion process.
However, as shown by Kamb \& Ganguli, (2024), in diffusion models where the
score is parametrized by a simple CNN, the inductive biases of the CNN itself
(translation equivariance and locality) allow the model to generate samples
that globally do not match any training samples, but are rather patch-wise
`mosaics'. Notably, however, this theory does not extend to describe the role
of self-attention in this process. In this work, we take a preliminary step in
this direction to extend this theory to the case of diffusion models whose
score is parametrized by a CNN with a final self-attention layer. We show that
our theory suggests that self-attention will induce a globally image-consistent
arrangement of local features beyond the patch-level in generated samples, and
we verify this behavior empirically on a carefully crafted dataset.

</details>


### [283] [DRIMV_TSK: An Interpretable Surgical Evaluation Model for Incomplete Multi-View Rectal Cancer Data](https://arxiv.org/abs/2506.17552)
*Wei Zhang,Zi Wang,Hanwen Zhou,Zhaohong Deng,Weiping Ding,Yuxi Ge,Te Zhang,Yuanpeng Zhang,Kup-Sze Choi,Shitong Wang,Shudong Hu*

Main category: cs.LG

TL;DR: The paper proposes an interpretable incomplete multi-view surgical evaluation model for rectal cancer, integrating AI and multi-view data (MRI images, clinical data) to improve treatment success.


<details>
  <summary>Details</summary>
Motivation: Current surgical difficulty evaluation relies on clinical data, but technological advancements allow for more comprehensive data collection. AI can enhance this process.

Method: Constructs a multi-view dataset (MRI, pressed-fat MRI, clinical data) and proposes a dual representation incomplete multi-view learning model with missing view imputation and second-order similarity. Uses TSK fuzzy system for evaluation.

Result: The proposed DRIMV_TSK model outperforms advanced algorithms on the MVRC dataset.

Conclusion: The model effectively integrates multi-view data and AI, improving surgical evaluation for rectal cancer.

Abstract: A reliable evaluation of surgical difficulty can improve the success of the
treatment for rectal cancer and the current evaluation method is based on
clinical data. However, more data about rectal cancer can be collected with the
development of technology. Meanwhile, with the development of artificial
intelligence, its application in rectal cancer treatment is becoming possible.
In this paper, a multi-view rectal cancer dataset is first constructed to give
a more comprehensive view of patients, including the high-resolution MRI image
view, pressed-fat MRI image view, and clinical data view. Then, an
interpretable incomplete multi-view surgical evaluation model is proposed,
considering that it is hard to obtain extensive and complete patient data in
real application scenarios. Specifically, a dual representation incomplete
multi-view learning model is first proposed to extract the common information
between views and specific information in each view. In this model, the missing
view imputation is integrated into representation learning, and second-order
similarity constraint is also introduced to improve the cooperative learning
between these two parts. Then, based on the imputed multi-view data and the
learned dual representation, a multi-view surgical evaluation model with the
TSK fuzzy system is proposed. In the proposed model, a cooperative learning
mechanism is constructed to explore the consistent information between views,
and Shannon entropy is also introduced to adapt the view weight. On the MVRC
dataset, we compared it with several advanced algorithms and DRIMV_TSK obtained
the best results.

</details>


### [284] [Decoding Federated Learning: The FedNAM+ Conformal Revolution](https://arxiv.org/abs/2506.17872)
*Sree Bhargavi Balija,Amitash Nanda,Debashis Sahoo*

Main category: cs.LG

TL;DR: FedNAM+ is a federated learning framework integrating Neural Additive Models (NAMs) and conformal prediction for interpretable, reliable uncertainty estimation. It outperforms traditional methods like LIME and SHAP by providing confidence intervals and visual insights.


<details>
  <summary>Details</summary>
Motivation: Existing federated learning frameworks lack solutions for uncertainty quantification, interpretability, and robustness. FedNAM+ addresses these gaps.

Method: FedNAM+ combines NAMs with a novel conformal prediction method, using gradient-based sensitivity maps for interpretability and pixel-wise uncertainty estimates.

Result: Experiments on CT scan, MNIST, and CIFAR datasets show high accuracy (e.g., 0.1% loss on MNIST) with transparent uncertainty measures. It outperforms Monte Carlo Dropout in efficiency.

Conclusion: FedNAM+ enhances trust and transparency in decentralized predictive modeling with robustness, interpretability, and computational efficiency.

Abstract: Federated learning has significantly advanced distributed training of machine
learning models across decentralized data sources. However, existing frameworks
often lack comprehensive solutions that combine uncertainty quantification,
interpretability, and robustness. To address this, we propose FedNAM+, a
federated learning framework that integrates Neural Additive Models (NAMs) with
a novel conformal prediction method to enable interpretable and reliable
uncertainty estimation. Our method introduces a dynamic level adjustment
technique that utilizes gradient-based sensitivity maps to identify key input
features influencing predictions. This facilitates both interpretability and
pixel-wise uncertainty estimates. Unlike traditional interpretability methods
such as LIME and SHAP, which do not provide confidence intervals, FedNAM+
offers visual insights into prediction reliability. We validate our approach
through experiments on CT scan, MNIST, and CIFAR datasets, demonstrating high
prediction accuracy with minimal loss (e.g., only 0.1% on MNIST), along with
transparent uncertainty measures. Visual analysis highlights variable
uncertainty intervals, revealing low-confidence regions where model performance
can be improved with additional data. Compared to Monte Carlo Dropout, FedNAM+
delivers efficient and global uncertainty estimates with reduced computational
overhead, making it particularly suitable for federated learning scenarios.
Overall, FedNAM+ provides a robust, interpretable, and computationally
efficient framework that enhances trust and transparency in decentralized
predictive modeling.

</details>


### [285] [Adapting Vision-Language Models for Evaluating World Models](https://arxiv.org/abs/2506.17967)
*Mariya Hendriksen,Tabish Rashid,David Bignell,Raluca Georgescu,Abdelhak Lemkhenter,Katja Hofmann,Sam Devlin,Sarah Parisot*

Main category: cs.LG

TL;DR: UNIVERSE introduces a unified VLM-based evaluator for world model rollouts, addressing fine-grained, temporally sensitive assessment challenges.


<details>
  <summary>Details</summary>
Motivation: Existing metrics lack capabilities for fine-grained, temporally grounded evaluation of world model rollouts, necessitating a scalable, semantics-aware solution.

Method: UNIVERSE adapts VLMs for rollout evaluation, testing action and character recognition across binary, multiple-choice, and open-ended formats. It compares full, partial, and parameter-efficient finetuning.

Result: UNIVERSE matches task-specific baselines with a single checkpoint and aligns well with human judgments.

Conclusion: UNIVERSE is a scalable, effective evaluator for world models, validated by human studies.

Abstract: World models -- generative models that simulate environment dynamics
conditioned on past observations and actions -- are gaining prominence in
planning, simulation, and embodied AI. However, evaluating their rollouts
remains a fundamental challenge, requiring fine-grained, temporally grounded
assessment of action alignment and semantic consistency -- capabilities not
captured by existing metrics. Vision-Language Models (VLMs) have shown promise
as automatic evaluators of generative content due to their strong multimodal
reasoning abilities. Yet, their use in fine-grained, temporally sensitive
evaluation tasks remains limited and requires targeted adaptation. We introduce
a evaluation protocol targeting two recognition tasks -- action recognition and
character recognition -- each assessed across binary, multiple-choice, and
open-ended formats. To support this, we present UNIVERSE (UNIfied
Vision-language Evaluator for Rollouts in Simulated Environments), a method for
adapting VLMs to rollout evaluation under data and compute constraints. We
conduct a large-scale study comparing full, partial, and parameter-efficient
finetuning across task formats, context lengths, sampling strategies, and data
compositions. The resulting unified evaluator matches the performance of
task-specific baselines using a single checkpoint. Human studies confirm strong
alignment with human judgments, establishing UNIVERSE as a scalable,
semantics-aware evaluator for world models.

</details>


### [286] [h-calibration: Rethinking Classifier Recalibration with Probabilistic Error-Bounded Objective](https://arxiv.org/abs/2506.17968)
*Wenjian Huang,Guiping Cao,Jiahao Xia,Jingkun Chen,Hao Wang,Jianguo Zhang*

Main category: cs.LG

TL;DR: The paper addresses miscalibration in deep neural networks, categorizes existing methods, identifies limitations, and proposes a new probabilistic framework (h-calibration) for better calibration performance.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks often produce unreliable probability outputs due to miscalibration, prompting the need for effective recalibration methods without compromising classification performance.

Method: The study categorizes existing methods into three strategies, identifies ten common limitations, and introduces h-calibration, a probabilistic framework with a post-hoc calibration algorithm.

Result: The proposed h-calibration method outperforms traditional approaches, overcoming identified limitations and achieving state-of-the-art performance in experiments.

Conclusion: The h-calibration framework provides a theoretically sound and practical solution for learning reliable calibrated probabilities, with proven effectiveness in benchmarks.

Abstract: Deep neural networks have demonstrated remarkable performance across numerous
learning tasks but often suffer from miscalibration, resulting in unreliable
probability outputs. This has inspired many recent works on mitigating
miscalibration, particularly through post-hoc recalibration methods that aim to
obtain calibrated probabilities without sacrificing the classification
performance of pre-trained models. In this study, we summarize and categorize
previous works into three general strategies: intuitively designed methods,
binning-based methods, and methods based on formulations of ideal calibration.
Through theoretical and practical analysis, we highlight ten common limitations
in previous approaches. To address these limitations, we propose a
probabilistic learning framework for calibration called h-calibration, which
theoretically constructs an equivalent learning formulation for canonical
calibration with boundedness. On this basis, we design a simple yet effective
post-hoc calibration algorithm. Our method not only overcomes the ten
identified limitations but also achieves markedly better performance than
traditional methods, as validated by extensive experiments. We further analyze,
both theoretically and experimentally, the relationship and advantages of our
learning objective compared to traditional proper scoring rule. In summary, our
probabilistic framework derives an approximately equivalent differentiable
objective for learning error-bounded calibrated probabilities, elucidating the
correspondence and convergence properties of computational statistics with
respect to theoretical bounds in canonical calibration. The theoretical
effectiveness is verified on standard post-hoc calibration benchmarks by
achieving state-of-the-art performance. This research offers valuable reference
for learning reliable likelihood in related fields.

</details>


### [287] [Pitfalls of Conformal Predictions for Medical Image Classification](https://arxiv.org/abs/2506.18162)
*Hendrik Mehrtens,Tabea Bucher,Titus J. Brinker*

Main category: cs.LG

TL;DR: Conformal predictions in medical classification face reliability issues under distributional shifts, limited practical value in small-class settings, and pitfalls in subset selection.


<details>
  <summary>Details</summary>
Motivation: Address the challenges and limitations of conformal predictions in medical tasks, highlighting their unreliability under certain conditions.

Method: Analyze conformal predictions through examples from dermatology and histopathology, focusing on distributional shifts and small-class settings.

Result: Conformal predictions are unreliable under input and label shifts, unsuitable for accuracy improvement, and limited in small-class scenarios.

Conclusion: Practitioners must be cautious with conformal predictions in medicine due to their limitations and assumptions.

Abstract: Reliable uncertainty estimation is one of the major challenges for medical
classification tasks. While many approaches have been proposed, recently the
statistical framework of conformal predictions has gained a lot of attention,
due to its ability to provide provable calibration guarantees. Nonetheless, the
application of conformal predictions in safety-critical areas such as medicine
comes with pitfalls, limitations and assumptions that practitioners need to be
aware of. We demonstrate through examples from dermatology and histopathology
that conformal predictions are unreliable under distributional shifts in input
and label variables. Additionally, conformal predictions should not be used for
selecting predictions to improve accuracy and are not reliable for subsets of
the data, such as individual classes or patient attributes. Moreover, in
classification settings with a small number of classes, which are common in
medical image classification tasks, conformal predictions have limited
practical value.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [288] [SlimRAG: Retrieval without Graphs via Entity-Aware Context Selection](https://arxiv.org/abs/2506.17288)
*Jiale Zhang,Jiaxiang Chen,Zhucong Li,Jie Ding,Kui Zhao,Zenglin Xu,Xin Pang,Yinghui Xu*

Main category: cs.IR

TL;DR: SlimRAG is a lightweight, entity-centric framework for retrieval-augmented generation that outperforms graph-based RAG systems in accuracy and efficiency by avoiding structural overhead.


<details>
  <summary>Details</summary>
Motivation: Graph-based RAG systems face issues like structural overhead and imprecise retrieval due to semantic similarity not equating to relevance. SlimRAG aims to address these flaws.

Method: SlimRAG replaces graph-based components with an entity-aware mechanism, using a compact entity-to-chunk table for retrieval without graph traversal.

Result: SlimRAG outperforms baselines in accuracy, reduces index size, and improves retrieval efficiency (e.g., RITU of 16.31 vs. 56+).

Conclusion: SlimRAG demonstrates the effectiveness of structure-free, entity-centric retrieval, offering a more efficient alternative to graph-based RAG systems.

Abstract: Retrieval-Augmented Generation (RAG) enhances language models by
incorporating external knowledge at inference time. However, graph-based RAG
systems often suffer from structural overhead and imprecise retrieval: they
require costly pipelines for entity linking and relation extraction, yet
frequently return subgraphs filled with loosely related or tangential content.
This stems from a fundamental flaw -- semantic similarity does not imply
semantic relevance. We introduce SlimRAG, a lightweight framework for retrieval
without graphs. SlimRAG replaces structure-heavy components with a simple yet
effective entity-aware mechanism. At indexing time, it constructs a compact
entity-to-chunk table based on semantic embeddings. At query time, it
identifies salient entities, retrieves and scores associated chunks, and
assembles a concise, contextually relevant input -- without graph traversal or
edge construction. To quantify retrieval efficiency, we propose Relative Index
Token Utilization (RITU), a metric measuring the compactness of retrieved
content. Experiments across multiple QA benchmarks show that SlimRAG
outperforms strong flat and graph-based baselines in accuracy while reducing
index size and RITU (e.g., 16.31 vs. 56+), highlighting the value of
structure-free, entity-centric context selection. The code will be released
soon. https://github.com/continue-ai-company/SlimRAG

</details>


### [289] [Enhancing Document Retrieval in COVID-19 Research: Leveraging Large Language Models for Hidden Relation Extraction](https://arxiv.org/abs/2506.18311)
*Hoang-An Trieu,Dinh-Truong Do,Chau Nguyen,Vu Tran,Minh Le Nguyen*

Main category: cs.IR

TL;DR: The paper proposes using large language models (LLMs) to enhance the Covrelex-SE system for retrieving high-quality COVID-19 research by uncovering hidden relationships in unlabeled publications.


<details>
  <summary>Details</summary>
Motivation: The COVID-19 pandemic generated a vast volume of publications, necessitating an efficient retrieval system for researchers. Existing tools lack the ability to extract hidden relationships in unlabeled data.

Method: The authors leverage LLMs to extract hidden relationships in unlabeled COVID-19 publications, improving the Covrelex-SE retrieval system.

Result: The method enhances the retrieval system by providing more useful information through uncovered relationships.

Conclusion: Using LLMs in the Covrelex-SE system improves retrieval quality for COVID-19 research, aiding researchers during sudden pandemics.

Abstract: In recent years, with the appearance of the COVID-19 pandemic, numerous
publications relevant to this disease have been issued. Because of the massive
volume of publications, an efficient retrieval system is necessary to provide
researchers with useful information if an unexpected pandemic happens so
suddenly, like COVID-19. In this work, we present a method to help the
retrieval system, the Covrelex-SE system, to provide more high-quality search
results. We exploited the power of the large language models (LLMs) to extract
the hidden relationships inside the unlabeled publication that cannot be found
by the current parsing tools that the system is using. Since then, help the
system to have more useful information during retrieval progress.

</details>


### [290] [Team LA at SCIDOCA shared task 2025: Citation Discovery via relation-based zero-shot retrieval](https://arxiv.org/abs/2506.18316)
*Trieu An,Long Nguyen,Minh Le Nguyen*

Main category: cs.IR

TL;DR: A system for predicting citations by retrieving top-k similar abstracts and using an LLM for precise identification, tested on SCIDOCA 2025 data.


<details>
  <summary>Details</summary>
Motivation: The challenge of identifying correct citations due to lengthy abstracts and high similarity among candidates.

Method: Retrieve top-k similar abstracts using relational features, then apply an LLM for accurate citation prediction.

Result: Effective citation prediction demonstrated on SCIDOCA 2025 training data.

Conclusion: The proposed framework successfully addresses citation discovery challenges.

Abstract: The Citation Discovery Shared Task focuses on predicting the correct citation
from a given candidate pool for a given paragraph. The main challenges stem
from the length of the abstract paragraphs and the high similarity among
candidate abstracts, making it difficult to determine the exact paper to cite.
To address this, we develop a system that first retrieves the top-k most
similar abstracts based on extracted relational features from the given
paragraph. From this subset, we leverage a Large Language Model (LLM) to
accurately identify the most relevant citation. We evaluate our framework on
the training dataset provided by the SCIDOCA 2025 organizers, demonstrating its
effectiveness in citation prediction.

</details>


### [291] [LLM-Enhanced Multimodal Fusion for Cross-Domain Sequential Recommendation](https://arxiv.org/abs/2506.17966)
*Wangyu Wu,Zhenhong Chen,Xianglin Qiu,Siqi Song,Xiaowei Huang,Fei Ma,Jimin Xiao*

Main category: cs.IR

TL;DR: LLM-EMF enhances cross-domain sequential recommendation by fusing multimodal data (text and images) using LLMs and CLIP, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To improve cross-domain sequential recommendation by leveraging multimodal data and advanced language models for better user preference modeling.

Method: Uses frozen CLIP for image/text embeddings and a multiple attention mechanism to learn intra- and inter-domain preferences.

Result: Outperforms existing methods on four e-commerce datasets, showing the effectiveness of multimodal integration.

Conclusion: LLM-EMF successfully enhances recommendation systems by integrating multimodal data and advanced modeling techniques.

Abstract: Cross-Domain Sequential Recommendation (CDSR) predicts user behavior by
leveraging historical interactions across multiple domains, focusing on
modeling cross-domain preferences and capturing both intra- and inter-sequence
item relationships. We propose LLM-Enhanced Multimodal Fusion for Cross-Domain
Sequential Recommendation (LLM-EMF), a novel and advanced approach that
enhances textual information with Large Language Models (LLM) knowledge and
significantly improves recommendation performance through the fusion of visual
and textual data. Using the frozen CLIP model, we generate image and text
embeddings, thereby enriching item representations with multimodal data. A
multiple attention mechanism jointly learns both single-domain and cross-domain
preferences, effectively capturing and understanding complex user interests
across diverse domains. Evaluations conducted on four e-commerce datasets
demonstrate that LLM-EMF consistently outperforms existing methods in modeling
cross-domain user preferences, thereby highlighting the effectiveness of
multimodal data integration and its advantages in enhancing sequential
recommendation systems. Our source code will be released.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [292] [Cite Pretrain: Retrieval-Free Knowledge Attribution for Large Language Models](https://arxiv.org/abs/2506.17585)
*Yukun Huang,Sanxing Chen,Jian Pei,Manzil Zaheer,Bhuwan Dhingra*

Main category: cs.AI

TL;DR: The paper explores training LLMs to reliably attribute answers to pretraining documents without test-time retrieval, introducing Active Indexing for better citation performance.


<details>
  <summary>Details</summary>
Motivation: Current LLMs often provide unreliable citations due to hallucination, and external retrieval methods introduce latency and noise. The goal is to improve citation reliability directly through training.

Method: A two-stage approach: (1) continual pretraining to bind facts to document identifiers, and (2) instruction tuning. Active Indexing uses synthetic QA pairs for diverse fact restatements and bidirectional generation.

Result: Active Indexing outperforms Passive Indexing, achieving up to 30.2% citation precision gains. Performance improves with more augmented data.

Conclusion: Active Indexing is effective for training LLMs to attribute answers reliably, showing scalability and robustness in citation tasks.

Abstract: Trustworthy language models should provide both correct and verifiable
answers. While language models can sometimes attribute their outputs to
pretraining data, their citations are often unreliable due to hallucination. As
a result, current systems insert citations by querying an external retriever at
inference time, introducing latency, infrastructure dependence, and
vulnerability to retrieval noise. We explore whether LLMs can be made to
reliably attribute to the documents seen during (continual)
pretraining--without test-time retrieval--by revising the training process. To
evaluate this, we release CitePretrainBench, a benchmark that mixes real-world
corpora (Wikipedia, Common Crawl, arXiv) with novel, unseen documents and
probes both short-form (single fact) and long-form (multi-fact) citation tasks.
Our approach follows a two-stage process: (1) continual pretraining to bind
facts to persistent document identifiers, and (2) instruction tuning to elicit
citation behavior. We find that simple Passive Indexing, which appends an
identifier to each document, helps memorize verbatim text but fails on
paraphrased or compositional facts. Instead, we propose Active Indexing, which
continually pretrains on synthetic QA pairs that (1) restate each fact in
diverse compositional forms, and (2) require bidirectional source-to-fact and
fact-to-source generation, jointly teaching the model to generate content from
a cited source and to attribute its own answers. Experiments with Qwen2.5-7B
and 3B show that Active Indexing consistently outperforms Passive Indexing
across all tasks and models, with citation precision gains up to 30.2 percent.
Our ablation studies reveal that performance continues to improve as we scale
the amount of augmented data, showing a clear upward trend even at 16 times the
original token count.

</details>


### [293] [Bayesian Social Deduction with Graph-Informed Language Models](https://arxiv.org/abs/2506.17788)
*Shahab Rahimirad,Guven Gergerli,Lucia Romero,Angela Qian,Matthew Lyle Olson,Simon Stepputtis,Joseph Campbell*

Main category: cs.AI

TL;DR: A hybrid reasoning framework combining LLMs with structured probabilistic models improves social reasoning in language models, outperforming humans in the game Avalon.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of social reasoning in LLMs, particularly in inferring beliefs and intentions from partial observations.

Method: Introduces a hybrid framework: LLMs handle language understanding and interaction, while a structured probabilistic model manages belief inference.

Result: Achieves a 67% win rate against humans, outperforming larger models and baselines.

Conclusion: The hybrid approach enhances social reasoning in LLMs, demonstrating practical viability and superior performance.

Abstract: Social reasoning - inferring unobservable beliefs and intentions from partial
observations of other agents - remains a challenging task for large language
models (LLMs). We evaluate the limits of current reasoning language models in
the social deduction game Avalon and find that while the largest models
demonstrate strong performance, they require extensive test-time inference and
degrade sharply when distilled to smaller, real-time-capable variants. To
address this, we introduce a hybrid reasoning framework that externalizes
belief inference to a structured probabilistic model, while using an LLM for
language understanding and interaction. Our approach achieves competitive
performance with much larger models in Agent-Agent play and, notably, is the
first language agent to defeat human players in a controlled study - achieving
a 67% win rate and receiving higher qualitative ratings than both reasoning
baselines and human teammates. We release code, models, and a dataset to
support future work on social reasoning in LLM agents, which can be found at
https://camp-lab-purdue.github.io/bayesian-social-deduction/

</details>


### [294] [Evolving Prompts In-Context: An Open-ended, Self-replicating Perspective](https://arxiv.org/abs/2506.17930)
*Jianyu Wang,Zhiqiang Hu,Lidong Bing*

Main category: cs.AI

TL;DR: The paper introduces a novel prompt design paradigm where pruning random demonstrations into 'gibberish' outperforms conventional methods, and proposes PromptQuine, a self-discovering optimization framework for effective pruning strategies.


<details>
  <summary>Details</summary>
Motivation: Challenging conventional wisdom in LLM prompting, the study aims to improve performance by exploring unconventional pruning strategies, as existing methods and human intuition fall short.

Method: Proposes PromptQuine, an evolutionary search framework that automatically discovers pruning strategies using low-data regimes, evolving effective prompts from context tokens.

Result: The 'gibberish' pruning strategy matches or surpasses state-of-the-art techniques, achieving gains across diverse tasks like classification, QA, generation, and math reasoning.

Conclusion: The findings advocate for mechanistic studies on in-context learning and open-ended search algorithms to enhance LLM prompting.

Abstract: We propose a novel prompt design paradigm that challenges conventional wisdom
in large language model (LLM) prompting. While conventional wisdom prioritizes
well-crafted instructions and demonstrations for in-context learning (ICL), we
show that pruning random demonstrations into seemingly incoherent "gibberish"
can remarkably improve performance across diverse tasks. Notably, the
"gibberish" always matches or surpasses state-of-the-art automatic prompt
optimization techniques, achieving substantial gains regardless of LLM
alignment. Nevertheless, discovering an effective pruning strategy is
non-trivial, as existing attribution methods and prompt compression algorithms
fail to deliver robust results, let alone human intuition. In terms of this, we
propose a self-discover prompt optimization framework, PromptQuine, an
evolutionary search framework that automatically searches for the pruning
strategy by itself using only low-data regimes. Much like the emergent
complexity in nature--such as symbiosis and self-organization--arising in
response to resource constraints, our framework evolves and refines
unconventional yet highly effective prompts by leveraging only the tokens
present within the context. We demonstrate its effectiveness across
classification, multi-choice question answering, generation and math reasoning
tasks across LLMs, while achieving decent runtime efficiency. We hope our
findings can guide mechanistic studies on in-context learning, and provide a
call to action, to pave the way for more open-ended search algorithms for more
effective LLM prompting.

</details>


### [295] [SE-Merging: A Self-Enhanced Approach for Dynamic Model Merging](https://arxiv.org/abs/2506.18135)
*Zijun Chen,Zhanpeng Zhou,Bo Zhang,Weinan Zhang,Xi Sun,Junchi Yan*

Main category: cs.AI

TL;DR: Model merging achieves multi-task abilities by distinguishing tasks and adapting to expert models. SE-Merging enhances this dynamically without extra training.


<details>
  <summary>Details</summary>
Motivation: To understand and improve the mechanisms behind model merging for multi-task learning.

Method: Analyze model merging from a representation perspective, identifying two key capabilities. Propose SE-Merging, a framework that dynamically identifies tasks and rescales merging coefficients.

Result: SE-Merging significantly improves performance while remaining compatible with existing techniques.

Conclusion: Model merging's success relies on task distinction and adaptation. SE-Merging offers a practical enhancement without additional training.

Abstract: Model merging has gained increasing attention due to its intriguing property:
interpolating the parameters of different task-specific fine-tuned models leads
to multi-task abilities. However, despite its empirical success, the underlying
mechanisms of model merging remain poorly understood. In this work, we delve
into the mechanism behind model merging from a representation perspective. Our
analysis reveals that model merging achieves multi-task abilities through two
key capabilities: i) distinguishing samples from different tasks, and ii)
adapting to the corresponding expert model for each sample. These two
capabilities allow the merged model to retain task-specific expertise, enabling
efficient multi-task adaptation. Building on these insights, we propose
\texttt{SE-Merging}, a self-enhanced model merging framework that leverages
these two characteristics to dynamically identify the corresponding task for
each sample and then adaptively rescales the merging coefficients to further
enhance task-specific expertise in the merged model. Notably,
\texttt{SE-Merging} achieves dynamic model merging without additional training.
Extensive experiments demonstrate that \texttt{SE-Merging} achieves significant
performance improvements while remaining compatible with existing model merging
techniques.

</details>


### [296] [Reasoning about Uncertainty: Do Reasoning Models Know When They Don't Know?](https://arxiv.org/abs/2506.18183)
*Zhiting Mei,Christina Zhang,Tenny Yin,Justin Lidard,Ola Shorinwa,Anirudha Majumdar*

Main category: cs.AI

TL;DR: The paper explores uncertainty quantification in reasoning models, finding they are often overconfident, especially with deeper reasoning, and suggests introspection can improve calibration, though not uniformly.


<details>
  <summary>Details</summary>
Motivation: To address the issue of reasoning models generating incorrect but confident responses (hallucinations), ensuring safe deployment in real-world applications.

Method: Introduces introspective uncertainty quantification (UQ) to evaluate model calibration, testing three key questions about calibration and reasoning depth.

Result: Reasoning models are typically overconfident, worsen with deeper reasoning, and introspection improves calibration in some models but not all.

Conclusion: Highlights the need for better UQ benchmarks and methods to improve reasoning model calibration.

Abstract: Reasoning language models have set state-of-the-art (SOTA) records on many
challenging benchmarks, enabled by multi-step reasoning induced using
reinforcement learning. However, like previous language models, reasoning
models are prone to generating confident, plausible responses that are
incorrect (hallucinations). Knowing when and how much to trust these models is
critical to the safe deployment of reasoning models in real-world applications.
To this end, we explore uncertainty quantification of reasoning models in this
work. Specifically, we ask three fundamental questions: First, are reasoning
models well-calibrated? Second, does deeper reasoning improve model
calibration? Finally, inspired by humans' innate ability to double-check their
thought processes to verify the validity of their answers and their confidence,
we ask: can reasoning models improve their calibration by explicitly reasoning
about their chain-of-thought traces? We introduce introspective uncertainty
quantification (UQ) to explore this direction. In extensive evaluations on SOTA
reasoning models across a broad range of benchmarks, we find that reasoning
models: (i) are typically overconfident, with self-verbalized confidence
estimates often greater than 85% particularly for incorrect responses, (ii)
become even more overconfident with deeper reasoning, and (iii) can become
better calibrated through introspection (e.g., o3-Mini and DeepSeek R1) but not
uniformly (e.g., Claude 3.7 Sonnet becomes more poorly calibrated). Lastly, we
conclude with important research directions to design necessary UQ benchmarks
and improve the calibration of reasoning models.

</details>


### [297] [Airalogy: AI-empowered universal data digitization for research automation](https://arxiv.org/abs/2506.18586)
*Zijie Yang,Qiji Zhou,Fang Guo,Sijie Zhang,Yexun Xi,Jinglei Nie,Yudian Zhu,Liping Huang,Chou Wu,Yonghe Xia,Xiaoyu Ma,Yingming Pu,Panzhong Lu,Junshu Pan,Mingtao Chen,Tiannan Guo,Yanmei Dou,Hongyu Chen,Anping Zeng,Jiaxing Huang,Tian Xu,Yue Zhang*

Main category: cs.AI

TL;DR: Airalogy is an AI-driven platform addressing the lack of standardized, multidisciplinary research data by balancing universality and standardization, enhancing AI-driven science.


<details>
  <summary>Details</summary>
Motivation: Current AI applications are limited due to fragmented, non-standardized research data. A unified platform is needed to support diverse disciplines while enabling AI.

Method: Developed Airalogy, a customizable, standardized platform integrating domain knowledge and computing skills, offering AI tools for research automation.

Result: Deployed in Westlake University labs, Airalogy facilitates standardized data digitization and AI-driven research workflows.

Conclusion: Airalogy accelerates scientific innovation by bridging gaps in research data standardization and AI empowerment across disciplines.

Abstract: Research data are the foundation of Artificial Intelligence (AI)-driven
science, yet current AI applications remain limited to a few fields with
readily available, well-structured, digitized datasets. Achieving comprehensive
AI empowerment across multiple disciplines is still out of reach. Present-day
research data collection is often fragmented, lacking unified standards,
inefficiently managed, and difficult to share. Creating a single platform for
standardized data digitization needs to overcome the inherent challenge of
balancing between universality (supporting the diverse, ever-evolving needs of
various disciplines) and standardization (enforcing consistent formats to fully
enable AI). No existing platform accommodates both facets. Building a truly
multidisciplinary platform requires integrating scientific domain knowledge
with sophisticated computing skills. Researchers often lack the computational
expertise to design customized and standardized data recording methods, whereas
platform developers rarely grasp the intricate needs of multiple scientific
domains. These gaps impede research data standardization and hamper AI-driven
progress. In this study, we address these challenges by developing Airalogy
(https://airalogy.com), the world's first AI- and community-driven platform
that balances universality and standardization for digitizing research data
across multiple disciplines. Airalogy represents entire research workflows
using customizable, standardized data records and offers an advanced AI
research copilot for intelligent Q&A, automated data entry, analysis, and
research automation. Already deployed in laboratories across all four schools
of Westlake University, Airalogy has the potential to accelerate and automate
scientific innovation in universities, industry, and the global research
community-ultimately benefiting humanity as a whole.

</details>


### [298] [AggTruth: Contextual Hallucination Detection using Aggregated Attention Scores in LLMs](https://arxiv.org/abs/2506.18628)
*Piotr Matys,Jan Eliasz,Konrad Kieczyski,Mikoaj Langner,Teddy Ferdinan,Jan Koco,Przemysaw Kazienko*

Main category: cs.AI

TL;DR: AggTruth detects contextual hallucinations in LLMs by analyzing internal attention scores, outperforming SOTA with stable performance across tasks.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of LLM hallucinations in RAG settings by developing a reliable detection method.

Method: Proposes AggTruth, using four aggregation techniques for attention scores in passages to detect hallucinations.

Result: AggTruth outperforms SOTA, shows stable performance in same-task and cross-task setups, and highlights the importance of attention head selection.

Conclusion: Careful selection of attention heads and feature techniques is crucial for optimal hallucination detection in LLMs.

Abstract: In real-world applications, Large Language Models (LLMs) often hallucinate,
even in Retrieval-Augmented Generation (RAG) settings, which poses a
significant challenge to their deployment. In this paper, we introduce
AggTruth, a method for online detection of contextual hallucinations by
analyzing the distribution of internal attention scores in the provided context
(passage). Specifically, we propose four different variants of the method, each
varying in the aggregation technique used to calculate attention scores. Across
all LLMs examined, AggTruth demonstrated stable performance in both same-task
and cross-task setups, outperforming the current SOTA in multiple scenarios.
Furthermore, we conducted an in-depth analysis of feature selection techniques
and examined how the number of selected attention heads impacts detection
performance, demonstrating that careful selection of heads is essential to
achieve optimal results.

</details>


### [299] [Programming by Backprop: LLMs Acquire Reusable Algorithmic Abstractions During Code Training](https://arxiv.org/abs/2506.18777)
*Jonathan Cook,Silvia Sapora,Arash Ahmadian,Akbir Khan,Tim Rocktaschel,Jakob Foerster,Laura Ruis*

Main category: cs.AI

TL;DR: Training LLMs on source code alone (w/o I/O examples) enhances their ability to evaluate programs, suggesting a mechanism for improved reasoning through internalized algorithmic abstractions.


<details>
  <summary>Details</summary>
Motivation: To understand how training LLMs on source code improves their reasoning abilities, focusing on the role of Programming by Backprop (PBB).

Method: Finetune LLMs on two sets of programs (with and without I/O examples) and evaluate their ability to assess programs without explicit I/O training.

Result: LLMs can evaluate programs without I/O examples, especially when code is provided directly. Chain-of-thought reasoning improves reliability. PBB outperforms training on I/O pairs.

Conclusion: Code training helps LLMs internalize reusable algorithmic abstractions, enhancing reasoning. Future work can improve learning from symbolic procedures and explore applications like model alignment.

Abstract: Training large language models (LLMs) on source code significantly enhances
their general-purpose reasoning abilities, but the mechanisms underlying this
generalisation are poorly understood. In this paper, we propose Programming by
Backprop (PBB) as a potential driver of this effect - teaching a model to
evaluate a program for inputs by training on its source code alone, without
ever seeing I/O examples. To explore this idea, we finetune LLMs on two sets of
programs representing simple maths problems and algorithms: one with source
code and I/O examples (w/ IO), the other with source code only (w/o IO). We
find evidence that LLMs have some ability to evaluate w/o IO programs for
inputs in a range of experimental settings, and make several observations.
Firstly, PBB works significantly better when programs are provided as code
rather than semantically equivalent language descriptions. Secondly, LLMs can
produce outputs for w/o IO programs directly, by implicitly evaluating the
program within the forward pass, and more reliably when stepping through the
program in-context via chain-of-thought. We further show that PBB leads to more
robust evaluation of programs across inputs than training on I/O pairs drawn
from a distribution that mirrors naturally occurring data. Our findings suggest
a mechanism for enhanced reasoning through code training: it allows LLMs to
internalise reusable algorithmic abstractions. Significant scope remains for
future work to enable LLMs to more effectively learn from symbolic procedures,
and progress in this direction opens other avenues like model alignment by
training on formal constitutional principles.

</details>


### [300] [ConciseHint: Boosting Efficient Reasoning via Continuous Concise Hints during Generation](https://arxiv.org/abs/2506.18810)
*Siao Tang,Xinyin Ma,Gongfan Fang,Xinchao Wang*

Main category: cs.AI

TL;DR: ConciseHint is a framework that reduces verbosity in reasoning models by injecting hints during generation, maintaining performance while shortening reasoning length.


<details>
  <summary>Details</summary>
Motivation: Existing methods for improving efficiency in reasoning models focus on pre-reasoning paradigms, neglecting the potential of intervening during generation to encourage conciseness.

Method: ConciseHint injects textual hints (manual or trained) during token generation, adaptively adjusting hint intensity based on query complexity.

Result: Experiments show a 65% reduction in reasoning length on GSM8K with Qwen-3 4B, with minimal accuracy loss.

Conclusion: ConciseHint effectively balances conciseness and performance in reasoning models.

Abstract: Recent advancements in large reasoning models (LRMs) like DeepSeek-R1 and
OpenAI o1 series have achieved notable performance enhancements on complex
reasoning tasks by scaling up the generation length by Chain-of-Thought (CoT).
However, an emerging issue is their inclination to produce excessively verbose
reasoning processes, leading to the inefficiency problem. Existing literature
on improving efficiency mainly adheres to the before-reasoning paradigms such
as prompting and reasoning or fine-tuning and reasoning, but ignores the
promising direction of directly encouraging the model to speak concisely by
intervening during the generation of reasoning. In order to fill the blank, we
propose a framework dubbed ConciseHint, which continuously encourages the
reasoning model to speak concisely by injecting the textual hint (manually
designed or trained on the concise data) during the token generation of the
reasoning process. Besides, ConciseHint is adaptive to the complexity of the
query by adaptively adjusting the hint intensity, which ensures it will not
undermine model performance. Experiments on the state-of-the-art LRMs,
including DeepSeek-R1 and Qwen-3 series, demonstrate that our method can
effectively produce concise reasoning processes while maintaining performance
well. For instance, we achieve a reduction ratio of 65\% for the reasoning
length on GSM8K benchmark with Qwen-3 4B with nearly no accuracy loss.

</details>


### [301] [jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval](https://arxiv.org/abs/2506.18902)
*Michael Gnther,Saba Sturua,Mohammad Kalim Akram,Isabelle Mohr,Andrei Ungureanu,Sedigheh Eslami,Scott Martens,Bo Wang,Nan Wang,Han Xiao*

Main category: cs.AI

TL;DR: jina-embeddings-v4 is a 3.8B parameter multimodal model unifying text and image embeddings with LoRA adapters, excelling in retrieval tasks and introducing the Jina-VDR benchmark.


<details>
  <summary>Details</summary>
Motivation: To unify text and image representations and optimize performance for diverse retrieval tasks, including visually rich content.

Method: Uses a novel architecture with single/multi-vector embeddings and task-specific LoRA adapters.

Result: Achieves state-of-the-art performance in single-modal and cross-modal retrieval, especially for visually rich content.

Conclusion: jina-embeddings-v4 is highly effective for multimodal retrieval, supported by the new Jina-VDR benchmark.

Abstract: We introduce jina-embeddings-v4, a 3.8 billion parameter multimodal embedding
model that unifies text and image representations through a novel architecture
supporting both single-vector and multi-vector embeddings in the late
interaction style. The model incorporates task-specific Low-Rank Adaptation
(LoRA) adapters to optimize performance across diverse retrieval scenarios,
including query-based information retrieval, cross-modal semantic similarity,
and programming code search. Comprehensive evaluations demonstrate that
jina-embeddings-v4 achieves state-of-the-art performance on both single- modal
and cross-modal retrieval tasks, with particular strength in processing
visually rich content such as tables, charts, diagrams, and mixed-media
formats. To facilitate evaluation of this capability, we also introduce
Jina-VDR, a novel benchmark specifically designed for visually rich image
retrieval.

</details>


### [302] [Chain-of-Memory: Enhancing GUI Agents for Cross-Application Navigation](https://arxiv.org/abs/2506.18158)
*Xinzge Gao,Chuanrui Hu,Bin Chen,Teng Li*

Main category: cs.AI

TL;DR: The paper introduces Chain-of-Memory (CoM), a method for explicitly modeling short-term and long-term memory in GUI agents to improve task state understanding and information retention in cross-app tasks.


<details>
  <summary>Details</summary>
Motivation: Existing GUI agents struggle with accurately understanding task states due to reliance on implicit representations (e.g., historical screenshots or actions), especially in complex, lengthy tasks.

Method: CoM captures action descriptions, integrates task-relevant screen information, and maintains a dedicated memory module to store and manage this information.

Result: CoM significantly improves GUI agents' performance in cross-application tasks, with 7B models achieving memory management comparable to 72B models.

Conclusion: The proposed CoM method and GUI Odyssey-CoM dataset enhance GUI agents' memory capabilities, enabling better task state understanding and performance.

Abstract: Multimodal large language models (MLLMs) are attracting growing attention in
the development of Graphical User Interface (GUI) agents. Existing approaches
often rely on historical screenshots or actions to implicitly represent the
task state. This reliance poses challenges for GUI agents in accurately
understanding task states and underscores the absence of effective mechanisms
to store critical information in complex and lengthy cross-app tasks. To
address these challenges, we propose Chain-of-Memory (CoM), a novel approach
for explicitly modeling short-term and long-term memory in GUI agents. CoM
achieves this by capturing action descriptions, integrating task-relevant
screen information, and maintaining a dedicated memory module to store and
manage this information. By leveraging explicit memory representations, CoM
enables GUI agents to better understand task states and retain critical
historical information persistently. To equip GUI agents with memory management
capabilities and evaluate the effectiveness of CoM, we developed the GUI
Odyssey-CoM, a dataset comprising 111k screen-action pairs annotated with
Chain-of-Memory. Experimental results demonstrate that CoM significantly
improves GUI agents' performance in cross-application tasks. Additionally, GUI
Odyssey-CoM enables 7B models to achieve memory management capabilities
comparable to 72B models. The dataset and code will be open-sourced.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [303] [Zero-Shot Cognitive Impairment Detection from Speech Using AudioLLM](https://arxiv.org/abs/2506.17351)
*Mostafa Shahin,Beena Ahmed,Julien Epps*

Main category: cs.SD

TL;DR: A zero-shot speech-based method for cognitive impairment detection using AudioLLM, showing comparable performance to supervised methods and strong generalizability.


<details>
  <summary>Details</summary>
Motivation: Early detection of cognitive impairment is crucial, and speech offers a non-invasive biomarker. Traditional methods require manual annotation and lack generalizability.

Method: Uses Qwen2-Audio AudioLLM with prompt-based instructions to classify speech samples as normal or impaired, tested on English and multilingual datasets.

Result: Achieves performance similar to supervised methods, with good generalizability across languages, tasks, and datasets.

Conclusion: The zero-shot AudioLLM approach is effective for CI detection, offering scalability and cross-language consistency.

Abstract: Cognitive impairment (CI) is of growing public health concern, and early
detection is vital for effective intervention. Speech has gained attention as a
non-invasive and easily collectible biomarker for assessing cognitive decline.
Traditional CI detection methods typically rely on supervised models trained on
acoustic and linguistic features extracted from speech, which often require
manual annotation and may not generalise well across datasets and languages. In
this work, we propose the first zero-shot speech-based CI detection method
using the Qwen2- Audio AudioLLM, a model capable of processing both audio and
text inputs. By designing prompt-based instructions, we guide the model in
classifying speech samples as indicative of normal cognition or cognitive
impairment. We evaluate our approach on two datasets: one in English and
another multilingual, spanning different cognitive assessment tasks. Our
results show that the zero-shot AudioLLM approach achieves performance
comparable to supervised methods and exhibits promising generalizability and
consistency across languages, tasks, and datasets.

</details>


### [304] [AI-Generated Song Detection via Lyrics Transcripts](https://arxiv.org/abs/2506.18488)
*Markus Frohmann,Elena V. Epure,Gabriel Meseguer-Brocal,Markus Schedl,Romain Hennequin*

Main category: cs.SD

TL;DR: Proposes using ASR models to transcribe and detect AI-generated music lyrics, showing robustness across languages, genres, and audio perturbations.


<details>
  <summary>Details</summary>
Motivation: Addresses the gap in detecting AI-generated music when perfect lyrics are unavailable, improving real-life applicability.

Method: Uses ASR models (e.g., Whisper large-v2) and LLM2Vec embeddings to transcribe and detect AI-generated lyrics.

Result: Strong detection performance across languages and genres, outperforming audio-based methods under perturbations.

Conclusion: The method is robust and practical for real-world AI-generated music detection.

Abstract: The recent rise in capabilities of AI-based music generation tools has
created an upheaval in the music industry, necessitating the creation of
accurate methods to detect such AI-generated content. This can be done using
audio-based detectors; however, it has been shown that they struggle to
generalize to unseen generators or when the audio is perturbed. Furthermore,
recent work used accurate and cleanly formatted lyrics sourced from a lyrics
provider database to detect AI-generated music. However, in practice, such
perfect lyrics are not available (only the audio is); this leaves a substantial
gap in applicability in real-life use cases. In this work, we instead propose
solving this gap by transcribing songs using general automatic speech
recognition (ASR) models. We do this using several detectors. The results on
diverse, multi-genre, and multi-lingual lyrics show generally strong detection
performance across languages and genres, particularly for our best-performing
model using Whisper large-v2 and LLM2Vec embeddings. In addition, we show that
our method is more robust than state-of-the-art audio-based ones when the audio
is perturbed in different ways and when evaluated on different music
generators. Our code is available at
https://github.com/deezer/robust-AI-lyrics-detection.

</details>


### [305] [Smooth Operators: LLMs Translating Imperfect Hints into Disfluency-Rich Transcripts](https://arxiv.org/abs/2506.18510)
*Duygu Altinok*

Main category: cs.SD

TL;DR: The paper proposes a method using large language models (LLMs) to detect and transcribe disfluencies in spoken language, integrating acoustic and textual inputs, even if imperfect.


<details>
  <summary>Details</summary>
Motivation: Improving automatic speech and language processing systems by accurately detecting disfluencies for more inclusive technologies.

Method: Combines acoustic representations with textual inputs (clean, time-aligned, or ASR outputs) to generate disfluency-annotated transcripts using LLMs.

Result: LLMs can effectively produce annotated transcripts even with imperfect textual inputs, highlighting their robustness.

Conclusion: LLMs are versatile and robust for disfluency detection, enabling high-quality annotated transcripts without requiring flawless inputs.

Abstract: Accurate detection of disfluencies in spoken language is crucial for
enhancing the performance of automatic speech and language processing systems,
as well as fostering the development of more inclusive speech and language
technologies. Leveraging the growing trend of large language models (LLMs) as
versatile learners capable of processing both lexical and non-lexical inputs
(e.g., audio and video), we propose a novel approach to transcribing
disfluencies as explicit tokens with timestamps, enabling the generation of
fully annotated disfluency-rich transcripts. Our method integrates acoustic
representations extracted from an audio encoder with textual inputs of varying
quality: clean transcriptions without disfluencies, time-aligned transcriptions
from aligners, or outputs from phoneme-based ASR models -- all of which may
contain imperfections. Importantly, our experiments demonstrate that textual
inputs do not need to be flawless. As long as they include timestamp-related
cues, LLMs can effectively smooth the input and produce fully
disfluency-annotated transcripts, underscoring their robustness in handling
imperfect hints.

</details>


### [306] [USAD: Universal Speech and Audio Representation via Distillation](https://arxiv.org/abs/2506.18843)
*Heng-Jui Chang,Saurabhchand Bhati,James Glass,Alexander H. Liu*

Main category: cs.SD

TL;DR: USAD introduces a unified SSL model for diverse audio tasks, achieving competitive performance across benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current SSL models are domain-specific; USAD aims to integrate speech, sound, and music into a single model.

Method: Uses layer-to-layer distillation from domain-specific SSL models to train a unified student model.

Result: Achieves near state-of-the-art results on SUPERB and HEAR benchmarks with a single encoder.

Conclusion: USAD successfully unifies diverse audio tasks into one model, offering broad applicability.

Abstract: Self-supervised learning (SSL) has revolutionized audio representations, yet
models often remain domain-specific, focusing on either speech or non-speech
tasks. In this work, we present Universal Speech and Audio Distillation (USAD),
a unified approach to audio representation learning that integrates diverse
audio types - speech, sound, and music - into a single model. USAD employs
efficient layer-to-layer distillation from domain-specific SSL models to train
a student on a comprehensive audio dataset. USAD offers competitive performance
across various benchmarks and datasets, including frame and instance-level
speech processing tasks, audio tagging, and sound classification, achieving
near state-of-the-art results with a single encoder on SUPERB and HEAR
benchmarks.

</details>


### [307] [TCDiff++: An End-to-end Trajectory-Controllable Diffusion Model for Harmonious Music-Driven Group Choreography](https://arxiv.org/abs/2506.18671)
*Yuqin Dai,Wanlu Zhu,Ronghui Li,Xiu Li,Zhenyu Zhang,Jun Li,Jian Yang*

Main category: cs.SD

TL;DR: TCDiff++ is a music-driven framework for group dance generation, addressing collisions, foot sliding, and abrupt swaps in long sequences.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with multi-dancer collisions, foot sliding, and abrupt swaps in long group dances.

Method: Uses dancer positioning embedding, distance-consistency loss, swap mode embedding, Footwork Adaptor, long group diffusion sampling, and Sequence Decoder.

Result: Achieves state-of-the-art performance, especially in long-duration scenarios.

Conclusion: TCDiff++ ensures high-quality, coherent group dance generation.

Abstract: Music-driven dance generation has garnered significant attention due to its
wide range of industrial applications, particularly in the creation of group
choreography. During the group dance generation process, however, most existing
methods still face three primary issues: multi-dancer collisions, single-dancer
foot sliding and abrupt swapping in the generation of long group dance. In this
paper, we propose TCDiff++, a music-driven end-to-end framework designed to
generate harmonious group dance. Specifically, to mitigate multi-dancer
collisions, we utilize a dancer positioning embedding to better maintain the
relative positioning among dancers. Additionally, we incorporate a
distance-consistency loss to ensure that inter-dancer distances remain within
plausible ranges. To address the issue of single-dancer foot sliding, we
introduce a swap mode embedding to indicate dancer swapping patterns and design
a Footwork Adaptor to refine raw motion, thereby minimizing foot sliding. For
long group dance generation, we present a long group diffusion sampling
strategy that reduces abrupt position shifts by injecting positional
information into the noisy input. Furthermore, we integrate a Sequence Decoder
layer to enhance the model's ability to selectively process long sequences.
Extensive experiments demonstrate that our TCDiff++ achieves state-of-the-art
performance, particularly in long-duration scenarios, ensuring high-quality and
coherent group dance generation.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [308] [Pix2Geomodel: A Next-Generation Reservoir Geomodeling with Property-to-Property Translation](https://arxiv.org/abs/2506.17747)
*Abdulrahman Al-Fakih,Ardiansyah Koeshidayatullah,Nabil A. Saraih,Tapan Mukerji,Rayan Kanfar,Abdulmohsen Alali,SanLinn I. Kaka*

Main category: physics.geo-ph

TL;DR: Pix2Geomodel, a cGAN framework based on Pix2Pix, predicts reservoir properties with high accuracy, outperforming traditional methods but facing 2D constraints.


<details>
  <summary>Details</summary>
Motivation: Traditional geological modeling struggles with subsurface heterogeneity and data conditioning, necessitating advanced AI solutions.

Method: Used a 7.6M-cell dataset, preprocessing, augmentation (2,350 images/property), and trained with U-Net generator and PatchGAN discriminator over 19,000 steps.

Result: High accuracy for facies and water saturation, moderate for porosity and permeability, with robust property-to-property translation.

Conclusion: Pix2Geomodel enhances reservoir property mapping, though future work should address 2D limits and integrate 3D modeling.

Abstract: Accurate geological modeling is critical for reservoir characterization, yet
traditional methods struggle with complex subsurface heterogeneity, and they
have problems with conditioning to observed data. This study introduces
Pix2Geomodel, a novel conditional generative adversarial network (cGAN)
framework based on Pix2Pix, designed to predict reservoir properties (facies,
porosity, permeability, and water saturation) from the Rotliegend reservoir of
the Groningen gas field. Utilizing a 7.6 million-cell dataset from the
Nederlandse Aardolie Maatschappij, accessed via EPOS-NL, the methodology
included data preprocessing, augmentation to generate 2,350 images per
property, and training with a U-Net generator and PatchGAN discriminator over
19,000 steps. Evaluation metrics include pixel accuracy (PA), mean intersection
over union (mIoU), frequency weighted intersection over union (FWIoU), and
visualizations assessed performance in masked property prediction and
property-to-property translation tasks. Results demonstrated high accuracy for
facies (PA 0.88, FWIoU 0.85) and water saturation (PA 0.96, FWIoU 0.95), with
moderate success for porosity (PA 0.70, FWIoU 0.55) and permeability (PA 0.74,
FWIoU 0.60), and robust translation performance (e.g., facies-to-facies PA
0.98, FWIoU 0.97). The framework captured spatial variability and geological
realism, as validated by variogram analysis, and calculated the training loss
curves for the generator and discriminator for each property. Compared to
traditional methods, Pix2Geomodel offers enhanced fidelity in direct property
mapping. Limitations include challenges with microstructural variability and 2D
constraints, suggesting future integration of multi-modal data and 3D modeling
(Pix2Geomodel v2.0). This study advances the application of generative AI in
geoscience, supporting improved reservoir management and open science
initiatives.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [309] [Unfolding the Past: A Comprehensive Deep Learning Approach to Analyzing Incunabula Pages](https://arxiv.org/abs/2506.18069)
*Klaudia Ropel,Krzysztof Kutt,Luiz do Valle Miranda,Grzegorz J. Nalepa*

Main category: cs.DL

TL;DR: A method for analyzing incunabula pages using YOLO models for object detection, OCR for text regions, and ResNet18/CLIP for image classification and description, achieving high accuracy.


<details>
  <summary>Details</summary>
Motivation: To automate the analysis of structure and content in early printed books (incunabula) using machine learning.

Method: Created a custom dataset of 500 annotated pages, used YOLO11n/s for object detection, Tesseract/Kraken for OCR, ResNet18 for image classification, and CLIP for semantic descriptions.

Result: YOLO11n achieved F1=0.94 on custom data; Tesseract outperformed Kraken in OCR; ResNet18 achieved 98.7% accuracy in image classification.

Conclusion: Machine learning shows promise for incunabula analysis, but OCR and visual content interpretation need improvement.

Abstract: We developed a proof-of-concept method for the automatic analysis of the
structure and content of incunabula pages. A custom dataset comprising 500
annotated pages from five different incunabula was created using resources from
the Jagiellonian Digital Library. Each page was manually labeled with five
predefined classes: Text, Title, Picture, Table, and Handwriting. Additionally,
the publicly available DocLayNet dataset was utilized as supplementary training
data. To perform object detection, YOLO11n and YOLO11s models were employed and
trained using two strategies: a combined dataset (DocLayNet and the custom
dataset) and the custom dataset alone. The highest performance (F1 = 0.94) was
achieved by the YOLO11n model trained exclusively on the custom data. Optical
character recognition was then conducted on regions classified as Text, using
both Tesseract and Kraken OCR, with Tesseract demonstrating superior results.
Subsequently, image classification was applied to the Picture class using a
ResNet18 model, achieving an accuracy of 98.7% across five subclasses:
Decorative_letter, Illustration, Other, Stamp, and Wrong_detection.
Furthermore, the CLIP model was utilized to generate semantic descriptions of
illustrations. The results confirm the potential of machine learning in the
analysis of early printed books, while emphasizing the need for further
advancements in OCR performance and visual content interpretation.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [310] [Enhancing Few-shot Keyword Spotting Performance through Pre-Trained Self-supervised Speech Models](https://arxiv.org/abs/2506.17686)
*Alican Gok,Oguzhan Buyuksolak,Osman Erman Okman,Murat Saraclar*

Main category: eess.AS

TL;DR: The paper proposes a training scheme for Few-Shot Keyword Spotting (FS-KWS) using self-supervised learning, dimensionality reduction, and knowledge distillation to improve accuracy in resource-constrained edge environments.


<details>
  <summary>Details</summary>
Motivation: Existing FS-KWS systems struggle with accuracy at low false acceptance rates, especially in edge environments, prompting the need for a more robust solution.

Method: The approach uses a Wav2Vec 2.0-based teacher model with Sub-center ArcFace loss for feature extraction, followed by attention-based dimensionality reduction and training a lightweight ResNet15 student model.

Result: The method improves 10-shot classification accuracy from 33.4% to 74.1% on the Google Speech Commands dataset at 1% false alarm rate.

Conclusion: The proposed training scheme significantly enhances FS-KWS performance, making it more suitable for real-world edge device applications.

Abstract: Keyword Spotting plays a critical role in enabling hands-free interaction for
battery-powered edge devices. Few-Shot Keyword Spotting (FS-KWS) addresses the
scalability and adaptability challenges of traditional systems by enabling
recognition of custom keywords with only a few examples. However, existing
FS-KWS systems achieve subpar accuracy at desirable false acceptance rates,
particularly in resource-constrained edge environments. To address these
issues, we propose a training scheme that leverages self-supervised learning
models for robust feature extraction, dimensionality reduction, and knowledge
distillation. The teacher model, based on Wav2Vec 2.0 is trained using
Sub-center ArcFace loss, which enhances inter-class separability and
intra-class compactness. To enable efficient deployment on edge devices, we
introduce attention-based dimensionality reduction and train a standard
lightweight ResNet15 student model. We evaluate the proposed approach on the
English portion of the Multilingual Spoken Words Corpus (MSWC) and the Google
Speech Commands (GSC) datasets. Notably, the proposed training method improves
the 10-shot classification accuracy from 33.4% to 74.1% on 11 classes at 1%
false alarm accuracy on the GSC dataset, thus making it significantly
better-suited for a real use case scenario.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [311] [The Democratic Paradox in Large Language Models' Underestimation of Press Freedom](https://arxiv.org/abs/2506.18045)
*I. Loaiza,R. Vestrelli,A. Fronzetti Colladon,R. Rigobon*

Main category: cs.CY

TL;DR: The study reveals systematic biases in six LLMs' evaluations of press freedom, showing negative misalignment, differential misalignment, and home bias compared to expert assessments.


<details>
  <summary>Details</summary>
Motivation: To investigate how LLMs' biases in evaluating press freedom could impact public trust in democratic institutions.

Method: Comparison of six popular LLMs' press freedom assessments in 180 countries against the World Press Freedom Index (WPFI).

Result: LLMs consistently underestimate press freedom (71%-93% of countries rated less free), show differential misalignment (worse in freer countries), and exhibit home bias (favoring their home countries).

Conclusion: LLMs must improve accuracy in representing human and civic rights to avoid shaping public understanding negatively.

Abstract: As Large Language Models (LLMs) increasingly mediate global information
access for millions of users worldwide, their alignment and biases have the
potential to shape public understanding and trust in fundamental democratic
institutions, such as press freedom. In this study, we uncover three systematic
distortions in the way six popular LLMs evaluate press freedom in 180 countries
compared to expert assessments of the World Press Freedom Index (WPFI). The six
LLMs exhibit a negative misalignment, consistently underestimating press
freedom, with individual models rating between 71% to 93% of countries as less
free. We also identify a paradoxical pattern we term differential misalignment:
LLMs disproportionately underestimate press freedom in countries where it is
strongest. Additionally, five of the six LLMs exhibit positive home bias,
rating their home countries' press freedoms more favorably than would be
expected given their negative misalignment with the human benchmark. In some
cases, LLMs rate their home countries between 7% to 260% more positively than
expected. If LLMs are set to become the next search engines and some of the
most important cultural tools of our time, they must ensure accurate
representations of the state of our human and civic rights globally.

</details>


### [312] [MAARTA:Multi-Agentic Adaptive Radiology Teaching Assistant](https://arxiv.org/abs/2506.17320)
*Akash Awasthi,Brandon V. Chang,Anh M. Vu,Ngan Le,Rishi Agrawal,Zhigang Deng,Carol Wu,Hien Van Nguyen*

Main category: cs.CY

TL;DR: MAARTA is a multi-agent AI framework designed to help radiology students improve perceptual expertise by analyzing gaze patterns and reports, providing personalized feedback on errors like missed fixations or misinterpretations.


<details>
  <summary>Details</summary>
Motivation: Current AI systems in radiology focus on diagnostic accuracy but lack explanations for perceptual errors, limiting student learning. MAARTA aims to bridge this gap by offering adaptive feedback.

Method: MAARTA uses a multi-agent framework to dynamically analyze gaze patterns and reports, comparing expert and student behavior via structured graphs. It assigns Perceptual Error Teacher agents to address discrepancies and provides step-by-step feedback.

Result: The system identifies missed findings and perceptual errors, enabling personalized feedback to improve students' diagnostic reasoning and visual search skills.

Conclusion: MAARTA advances AI-driven radiology education by addressing perceptual errors and enhancing student learning through adaptive, multi-agent feedback.

Abstract: Radiology students often struggle to develop perceptual expertise due to
limited expert mentorship time, leading to errors in visual search and
diagnostic interpretation. These perceptual errors, such as missed fixations,
short dwell times, or misinterpretations, are not adequately addressed by
current AI systems, which focus on diagnostic accuracy but fail to explain how
and why errors occur. To address this gap, we introduce MAARTA (Multi-Agentic
Adaptive Radiology Teaching Assistant), a multi-agent framework that analyzes
gaze patterns and radiology reports to provide personalized feedback. Unlike
single-agent models, MAARTA dynamically selects agents based on error
complexity, enabling adaptive and efficient reasoning. By comparing expert and
student gaze behavior through structured graphs, the system identifies missed
findings and assigns Perceptual Error Teacher agents to analyze discrepancies.
MAARTA then uses step-by-step prompting to help students understand their
errors and improve diagnostic reasoning, advancing AI-driven radiology
education.

</details>


### [313] [AI-based Multimodal Biometrics for Detecting Smartphone Distractions: Application to Online Learning](https://arxiv.org/abs/2506.17364)
*Alvaro Becerra,Roberto Daza,Ruth Cobos,Aythami Morales,Mutlu Cukurova,Julian Fierrez*

Main category: cs.CY

TL;DR: The paper explores using multimodal biometrics to detect smartphone-induced distractions in online learning, achieving 91% accuracy with a combined AI model.


<details>
  <summary>Details</summary>
Motivation: To address challenges in maintaining learner engagement due to smartphone use and other factors, leveraging MMLA and biosensors for deeper insights.

Method: An AI-based approach using physiological signals (e.g., brain waves, heart rate) and head pose data to detect phone use.

Result: Single signals (e.g., brain waves) had limited accuracy, while head pose alone achieved 87%. A multimodal model combining all signals reached 91% accuracy.

Conclusion: The study highlights the potential of multimodal biometrics for real-time support in online learning, though deployment challenges remain.

Abstract: This work investigates the use of multimodal biometrics to detect
distractions caused by smartphone use during tasks that require sustained
attention, with a focus on computer-based online learning. Although the methods
are applicable to various domains, such as autonomous driving, we concentrate
on the challenges learners face in maintaining engagement amid internal (e.g.,
motivation), system-related (e.g., course design) and contextual (e.g.,
smartphone use) factors. Traditional learning platforms often lack detailed
behavioral data, but Multimodal Learning Analytics (MMLA) and biosensors
provide new insights into learner attention. We propose an AI-based approach
that leverages physiological signals and head pose data to detect phone use.
Our results show that single biometric signals, such as brain waves or heart
rate, offer limited accuracy, while head pose alone achieves 87%. A multimodal
model combining all signals reaches 91% accuracy, highlighting the benefits of
integration. We conclude by discussing the implications and limitations of
deploying these models for real-time support in online learning environments.

</details>


### [314] [Multimodal Political Bias Identification and Neutralization](https://arxiv.org/abs/2506.17372)
*Cedric Bernard,Xavier Pleimling,Amun Kharel,Chase Vickery*

Main category: cs.CY

TL;DR: A model for detecting and reducing bias in political articles by analyzing both text and images, using CLIP, ViT, and BERT models, showing promising initial results but requiring further refinement.


<details>
  <summary>Details</summary>
Motivation: Political echo chambers amplify bias in media; existing work ignores image bias, despite its significant impact.

Method: Four-step approach: Image Text Alignment (CLIP), Image Bias Scoring (ViT), Text De-Biasing (BERT), and final debiasing by replacing biased content.

Result: Text debiasing identifies biased phrases; ViT trains effectively; CLIP aligns semantics well. Needs more training/resources.

Conclusion: The model is promising but requires further development and human evaluation for semantic consistency.

Abstract: Due to the presence of political echo chambers, it becomes imperative to
detect and remove subjective bias and emotionally charged language from both
the text and images of political articles. However, prior work has focused on
solely the text portion of the bias rather than both the text and image
portions. This is a problem because the images are just as powerful of a medium
to communicate information as text is. To that end, we present a model that
leverages both text and image bias which consists of four different steps.
Image Text Alignment focuses on semantically aligning images based on their
bias through CLIP models. Image Bias Scoring determines the appropriate bias
score of images via a ViT classifier. Text De-Biasing focuses on detecting
biased words and phrases and neutralizing them through BERT models. These three
steps all culminate to the final step of debiasing, which replaces the text and
the image with neutralized or reduced counterparts, which for images is done by
comparing the bias scores. The results so far indicate that this approach is
promising, with the text debiasing strategy being able to identify many
potential biased words and phrases, and the ViT model showcasing effective
training. The semantic alignment model also is efficient. However, more time,
particularly in training, and resources are needed to obtain better results. A
human evaluation portion was also proposed to ensure semantic consistency of
the newly generated text and images.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [315] [Can Generated Images Serve as a Viable Modality for Text-Centric Multimodal Learning?](https://arxiv.org/abs/2506.17623)
*Yuesheng Huang,Peng Zhang,Riliang Liu,Jiaqi Liang*

Main category: cs.MM

TL;DR: The paper explores using Text-to-Image (T2I) models to generate images as a complementary modality for text-centric tasks, showing performance gains but highlighting conditional effectiveness based on semantic alignment, task visual groundability, and T2I model fidelity.


<details>
  <summary>Details</summary>
Motivation: Address the modality gap between abundant text-only data and powerful multimodal models by investigating the utility of T2I-generated images for text tasks.

Method: Systematic evaluation of T2I-generated images for text classification, analyzing variables like T2I model quality, prompt engineering, and fusion architectures.

Result: Synthetic perception via T2I models improves performance, but effectiveness depends on semantic alignment, task visual groundability, and T2I model fidelity.

Conclusion: T2I-generated images can enrich language understanding in unimodal scenarios, but their utility is context-dependent, requiring careful consideration of alignment and model quality.

Abstract: A significant ``modality gap" exists between the abundance of text-only data
and the increasing power of multimodal models. This work systematically
investigates whether images generated on-the-fly by Text-to-Image (T2I) models
can serve as a valuable complementary modality for text-centric tasks. Through
a comprehensive evaluation framework on text classification, we analyze the
impact of critical variables, including T2I model quality, prompt engineering
strategies, and multimodal fusion architectures. Our findings demonstrate that
this``synthetic perception" can yield significant performance gains, even when
augmenting strong large language model baselines. However, we find the
effectiveness of this approach is highly conditional, depending critically on
the semantic alignment between text and the generated image, the inherent
``visual groundability" of the task, and the generative fidelity of the T2I
model. Our work establishes the first rigorous benchmark for this paradigm,
providing a clear analysis of its potential and current limitations, and
demonstrating its viability as a pathway to enrich language understanding in
traditionally unimodal scenarios.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [316] [RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation](https://arxiv.org/abs/2506.18088)
*Tianxing Chen,Zanxin Chen,Baijun Chen,Zijian Cai,Yibin Liu,Qiwei Liang,Zixuan Li,Xianliang Lin,Yiheng Ge,Zhenyu Gu,Weiliang Deng,Yubin Guo,Tian Nian,Xuanbing Xie,Qiangyu Chen,Kailun Su,Tianling Xu,Guodong Liu,Mengkang Hu,Huan-ang Gao,Kaixuan Wang,Zhixuan Liang,Yusen Qin,Xiaokang Yang,Ping Luo,Yao Mu*

Main category: cs.RO

TL;DR: RoboTwin 2.0 is a scalable simulation framework for generating diverse, realistic bimanual manipulation data, improving sim-to-real transfer and task generalization.


<details>
  <summary>Details</summary>
Motivation: Existing synthetic datasets are insufficient for robust bimanual manipulation due to inefficient data generation and oversimplified simulations.

Method: RoboTwin 2.0 combines a large-scale object library, MLLMs for task-level code generation, and structured domain randomization across five axes.

Result: Empirical results show a 10.9% gain in code generation success and significant improvements in real-world task generalization (367% relative improvement).

Conclusion: RoboTwin 2.0 enables scalable, robust bimanual manipulation research with strong generalization, even without real-world supervision.

Abstract: Simulation-based data synthesis has emerged as a powerful paradigm for
enhancing real-world robotic manipulation. However, existing synthetic datasets
remain insufficient for robust bimanual manipulation due to two challenges: (1)
the lack of an efficient, scalable data generation method for novel tasks, and
(2) oversimplified simulation environments that fail to capture real-world
complexity. We present RoboTwin 2.0, a scalable simulation framework that
enables automated, large-scale generation of diverse and realistic data, along
with unified evaluation protocols for dual-arm manipulation. We first construct
RoboTwin-OD, a large-scale object library comprising 731 instances across 147
categories, each annotated with semantic and manipulation-relevant labels.
Building on this foundation, we develop an expert data synthesis pipeline that
combines multimodal large language models (MLLMs) with simulation-in-the-loop
refinement to generate task-level execution code automatically. To improve
sim-to-real transfer, RoboTwin 2.0 incorporates structured domain randomization
along five axes: clutter, lighting, background, tabletop height and language
instructions, thereby enhancing data diversity and policy robustness. We
instantiate this framework across 50 dual-arm tasks spanning five robot
embodiments, and pre-collect over 100,000 domain-randomized expert
trajectories. Empirical results show a 10.9% gain in code generation success
and improved generalization to novel real-world scenarios. A VLA model
fine-tuned on our dataset achieves a 367% relative improvement (42.0% vs. 9.0%)
on unseen scene real-world tasks, while zero-shot models trained solely on our
synthetic data achieve a 228% relative gain, highlighting strong generalization
without real-world supervision. We release the data generator, benchmark,
dataset, and code to support scalable research in robust bimanual manipulation.

</details>


### [317] [A workflow for generating synthetic LiDAR datasets in simulation environments](https://arxiv.org/abs/2506.17378)
*Abhishek Phadke,Shakib Mahmud Dipto,Pratip Rana*

Main category: cs.RO

TL;DR: A simulation workflow for synthetic LiDAR datasets is introduced, supporting autonomous vehicle research and sensor security analysis. It automates data capture and annotation, validates with large-scale datasets, and explores security vulnerabilities. Limitations and future improvements are discussed.


<details>
  <summary>Details</summary>
Motivation: To support autonomous vehicle perception, robotics research, and sensor security analysis by generating high-fidelity synthetic LiDAR datasets.

Method: Uses CoppeliaSim and Python API to integrate LiDAR, image sensors, and 2D scanners on a simulated vehicle in an urban scenario. Automates data capture, storage, and annotation in multiple formats.

Result: Validated pipeline produces synchronized multimodal datasets with ground truth pose information. Examines LiDAR security vulnerabilities and demonstrates synthetic datasets' utility for defense evaluation.

Conclusion: The workflow offers a versatile, reproducible framework for synthetic LiDAR datasets, advancing perception research and sensor security. Future work includes improving realism and scalability.

Abstract: This paper presents a simulation workflow for generating synthetic LiDAR
datasets to support autonomous vehicle perception, robotics research, and
sensor security analysis. Leveraging the CoppeliaSim simulation environment and
its Python API, we integrate time-of-flight LiDAR, image sensors, and two
dimensional scanners onto a simulated vehicle platform operating within an
urban scenario. The workflow automates data capture, storage, and annotation
across multiple formats (PCD, PLY, CSV), producing synchronized multimodal
datasets with ground truth pose information. We validate the pipeline by
generating large-scale point clouds and corresponding RGB and depth imagery.
The study examines potential security vulnerabilities in LiDAR data, such as
adversarial point injection and spoofing attacks, and demonstrates how
synthetic datasets can facilitate the evaluation of defense strategies.
Finally, limitations related to environmental realism, sensor noise modeling,
and computational scalability are discussed, and future research directions,
such as incorporating weather effects, real-world terrain models, and advanced
scanner configurations, are proposed. The workflow provides a versatile,
reproducible framework for generating high-fidelity synthetic LiDAR datasets to
advance perception research and strengthen sensor security in autonomous
systems. Documentation and examples accompany this framework; samples of
animated cloud returns and image sensor data can be found at this Link.

</details>


### [318] [General-Purpose Robotic Navigation via LVLM-Orchestrated Perception, Reasoning, and Acting](https://arxiv.org/abs/2506.17462)
*Bernard Lange,Anil Yildiz,Mansur Arief,Shehryar Khattak,Mykel Kochenderfer,Georgios Georgakis*

Main category: cs.RO

TL;DR: ARNA, a general-purpose navigation framework using LVLMs, enables autonomous navigation and reasoning in unmapped environments without task-specific networks or pre-mapped spaces.


<details>
  <summary>Details</summary>
Motivation: Existing navigation systems lack generalizability due to reliance on task-specific networks and fixed data flows. LVLMs offer potential but are limited by pre-mapped spaces and hard-coded representations.

Method: ARNA integrates LVLMs with robotic tools for perception, reasoning, and navigation, allowing autonomous task-specific workflow creation and execution.

Result: ARNA achieves state-of-the-art performance in Habitat Lab on the HM-EQA benchmark, excelling in exploration, navigation, and embodied question answering.

Conclusion: ARNA provides a robust, general-purpose solution for navigation in unknown environments, advancing robotic stack design.

Abstract: Developing general-purpose navigation policies for unknown environments
remains a core challenge in robotics. Most existing systems rely on
task-specific neural networks and fixed data flows, limiting generalizability.
Large Vision-Language Models (LVLMs) offer a promising alternative by embedding
human-like knowledge suitable for reasoning and planning. Yet, prior LVLM-robot
integrations typically depend on pre-mapped spaces, hard-coded representations,
and myopic exploration. We introduce the Agentic Robotic Navigation
Architecture (ARNA), a general-purpose navigation framework that equips an
LVLM-based agent with a library of perception, reasoning, and navigation tools
available within modern robotic stacks. At runtime, the agent autonomously
defines and executes task-specific workflows that iteratively query the robotic
modules, reason over multimodal inputs, and select appropriate navigation
actions. This approach enables robust navigation and reasoning in previously
unmapped environments, providing a new perspective on robotic stack design.
Evaluated in Habitat Lab on the HM-EQA benchmark, ARNA achieves
state-of-the-art performance, demonstrating effective exploration, navigation,
and embodied question answering without relying on handcrafted plans, fixed
input representations, or pre-existing maps.

</details>


### [319] [EASE: Embodied Active Event Perception via Self-Supervised Energy Minimization](https://arxiv.org/abs/2506.17516)
*Zhou Chen,Sanjoy Kundu,Harsimran S. Baweja,Sathyanarayanan N. Aakur*

Main category: cs.RO

TL;DR: EASE is a self-supervised framework for active event perception, using free energy minimization to unify representation learning and control without predefined actions or rewards.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on predefined actions and rewards, limiting adaptability in dynamic scenarios. EASE aims to overcome this by leveraging intrinsic signals.

Method: EASE combines generative perception and control policies, using prediction errors and entropy for event segmentation and tracking without annotations.

Result: EASE achieves privacy-preserving, scalable event perception in simulations and real-world tasks, showing adaptability and emergent behaviors.

Conclusion: EASE provides a robust foundation for embodied systems in dynamic, unscripted environments, outperforming traditional approaches.

Abstract: Active event perception, the ability to dynamically detect, track, and
summarize events in real time, is essential for embodied intelligence in tasks
such as human-AI collaboration, assistive robotics, and autonomous navigation.
However, existing approaches often depend on predefined action spaces,
annotated datasets, and extrinsic rewards, limiting their adaptability and
scalability in dynamic, real-world scenarios. Inspired by cognitive theories of
event perception and predictive coding, we propose EASE, a self-supervised
framework that unifies spatiotemporal representation learning and embodied
control through free energy minimization. EASE leverages prediction errors and
entropy as intrinsic signals to segment events, summarize observations, and
actively track salient actors, operating without explicit annotations or
external rewards. By coupling a generative perception model with an
action-driven control policy, EASE dynamically aligns predictions with
observations, enabling emergent behaviors such as implicit memory, target
continuity, and adaptability to novel environments. Extensive evaluations in
simulation and real-world settings demonstrate EASE's ability to achieve
privacy-preserving and scalable event perception, providing a robust foundation
for embodied systems in unscripted, dynamic tasks.

</details>


### [320] [Radar and Event Camera Fusion for Agile Robot Ego-Motion Estimation](https://arxiv.org/abs/2506.18443)
*Yang Lyu,Zhenghao Zou,Yanfeng Li,Chunhui Zhao,Quan Pan*

Main category: cs.RO

TL;DR: Proposes an IMU-free, feature-association-free framework for agile robot ego-motion estimation using event cameras and radar, achieving robust and efficient velocity output in dynamic scenarios.


<details>
  <summary>Details</summary>
Motivation: Addresses challenges of sensor delays and distortions in highly dynamic robot motions, aiming for reliable velocity estimation without IMUs or feature associations.

Method: Combines event camera and radar data to derive velocities directly, uses a continuous-time state-space model for fusion, and validates with real-world datasets.

Result: Demonstrates reliable and efficient velocity estimation in challenging environments, outperforming traditional methods.

Conclusion: The framework is robust, computationally efficient, and suitable for edge devices, with open-source code and datasets provided.

Abstract: Achieving reliable ego motion estimation for agile robots, e.g., aerobatic
aircraft, remains challenging because most robot sensors fail to respond timely
and clearly to highly dynamic robot motions, often resulting in measurement
blurring, distortion, and delays. In this paper, we propose an IMU-free and
feature-association-free framework to achieve aggressive ego-motion velocity
estimation of a robot platform in highly dynamic scenarios by combining two
types of exteroceptive sensors, an event camera and a millimeter wave radar,
First, we used instantaneous raw events and Doppler measurements to derive
rotational and translational velocities directly. Without a sophisticated
association process between measurement frames, the proposed method is more
robust in texture-less and structureless environments and is more
computationally efficient for edge computing devices. Then, in the back-end, we
propose a continuous-time state-space model to fuse the hybrid time-based and
event-based measurements to estimate the ego-motion velocity in a fixed-lagged
smoother fashion. In the end, we validate our velometer framework extensively
in self-collected experiment datasets. The results indicate that our IMU-free
and association-free ego motion estimation framework can achieve reliable and
efficient velocity output in challenging environments. The source code,
illustrative video and dataset are available at
https://github.com/ZzhYgwh/TwistEstimator.

</details>


### [321] [TDACloud: Point Cloud Recognition Using Topological Data Analysis](https://arxiv.org/abs/2506.18725)
*Anirban Ghosh,Ian Dahlin,Ayan Dutta*

Main category: cs.RO

TL;DR: TDACloud uses Topological Data Analysis (TDA) for local descriptor extraction from point clouds, achieving high recognition accuracy without GPU-based training.


<details>
  <summary>Details</summary>
Motivation: Challenges in point cloud recognition include noise and transformations; existing methods are resource-intensive.

Method: Uses ATOL vectorization to generate fixed-size TDA-descriptor vectors from raw point clouds.

Result: Achieves high accuracy in noisy and transformed conditions, outperforming baselines by up to 14%.

Conclusion: TDACloud is effective for object/place recognition in noisy and real-world scenarios.

Abstract: Point cloud-based object/place recognition remains a problem of interest in
applications such as autonomous driving, scene reconstruction, and
localization. Extracting meaningful local descriptors from a query point cloud
that can be matched with the descriptors of the collected point clouds is a
challenging problem. Furthermore, when the query point cloud is noisy or has
been transformed (e.g., rotated), it adds to the complexity. To this end, we
propose a novel methodology, named TDACloud, using Topological Data Analysis
(TDA) for local descriptor extraction from a point cloud, which does not need
resource-intensive GPU-based machine learning training. More specifically, we
used the ATOL vectorization method to generate vectors for point clouds. Unlike
voxelization, our proposed technique can take raw point clouds as inputs and
outputs a fixed-size TDA-descriptor vector. To test the quality of the proposed
TDACloud technique, we have implemented it on multiple real-world (e.g., Oxford
RobotCar, KITTI-360) and realistic (e.g., ShapeNet) point cloud datasets for
object and place recognition. We have also tested TDACloud on noisy and
transformed test cases where the query point cloud has been scaled, translated,
or rotated. Our results demonstrate high recognition accuracies in noisy
conditions and large-scale real-world place recognition while outperforming the
baselines by up to approximately 14%.

</details>


### [322] [Reproducible Evaluation of Camera Auto-Exposure Methods in the Field: Platform, Benchmark and Lessons Learned](https://arxiv.org/abs/2506.18844)
*Olivier Gamache,Jean-Michel Fortin,Matj Boxan,Franois Pomerleau,Philippe Gigure*

Main category: cs.RO

TL;DR: The paper introduces a methodology using an emulator to generate images at any exposure time, leveraging the BorealHDR dataset, to benchmark AE methods offline, ensuring reproducibility.


<details>
  <summary>Details</summary>
Motivation: Standard datasets limit comparison of AE methods due to fixed sensor inputs, making online benchmarking non-reproducible.

Method: Uses an emulator with BorealHDR dataset to generate images at various exposures, enabling offline benchmarking of AE methods.

Result: Achieves RMSE below 1.78% compared to ground truth. Benchmarked eight AE methods, finding classical AE best.

Conclusion: The offline approach ensures reproducibility, and the classical AE method outperforms others. Dataset and code are shared for further research.

Abstract: Standard datasets often present limitations, particularly due to the fixed
nature of input data sensors, which makes it difficult to compare methods that
actively adjust sensor parameters to suit environmental conditions. This is the
case with Automatic-Exposure (AE) methods, which rely on environmental factors
to influence the image acquisition process. As a result, AE methods have
traditionally been benchmarked in an online manner, rendering experiments
non-reproducible. Building on our prior work, we propose a methodology that
utilizes an emulator capable of generating images at any exposure time. This
approach leverages BorealHDR, a unique multi-exposure stereo dataset, along
with its new extension, in which data was acquired along a repeated trajectory
at different times of the day to assess the impact of changing illumination. In
total, BorealHDR covers 13.4 km over 59 trajectories in challenging lighting
conditions. The dataset also includes lidar-inertial-odometry-based maps with
pose estimation for each image frame, as well as Global Navigation Satellite
System (GNSS) data for comparison. We demonstrate that by using images acquired
at various exposure times, we can emulate realistic images with a
Root-Mean-Square Error (RMSE) below 1.78% compared to ground truth images.
Using this offline approach, we benchmarked eight AE methods, concluding that
the classical AE method remains the field's best performer. To further support
reproducibility, we provide in-depth details on the development of our backpack
acquisition platform, including hardware, electrical components, and
performance specifications. Additionally, we share valuable lessons learned
from deploying the backpack over more than 25 km across various environments.
Our code and dataset are available online at this link:
https://github.com/norlab-ulaval/TFR24 BorealHDR

</details>


### [323] [GRAND-SLAM: Local Optimization for Globally Consistent Large-Scale Multi-Agent Gaussian SLAM](https://arxiv.org/abs/2506.18885)
*Annika Thomas,Aneesa Sonawalla,Alex Rose,Jonathan P. How*

Main category: cs.RO

TL;DR: GRAND-SLAM is a multi-agent Gaussian splatting SLAM method for large-scale outdoor environments, outperforming existing methods in tracking and rendering.


<details>
  <summary>Details</summary>
Motivation: Current Gaussian splatting SLAM methods are limited to small-scale indoor environments, leaving large-scale outdoor multi-agent applications unexplored.

Method: GRAND-SLAM combines an implicit tracking module with local optimization and integrates inter- and intra-robot loop closure into a pose-graph framework.

Result: GRAND-SLAM achieves state-of-the-art tracking, 28% higher PSNR on Replica, and 91% lower tracking error on Kimera-Multi.

Conclusion: GRAND-SLAM advances multi-agent Gaussian SLAM for large-scale outdoor environments with superior performance.

Abstract: 3D Gaussian splatting has emerged as an expressive scene representation for
RGB-D visual SLAM, but its application to large-scale, multi-agent outdoor
environments remains unexplored. Multi-agent Gaussian SLAM is a promising
approach to rapid exploration and reconstruction of environments, offering
scalable environment representations, but existing approaches are limited to
small-scale, indoor environments. To that end, we propose Gaussian
Reconstruction via Multi-Agent Dense SLAM, or GRAND-SLAM, a collaborative
Gaussian splatting SLAM method that integrates i) an implicit tracking module
based on local optimization over submaps and ii) an approach to inter- and
intra-robot loop closure integrated into a pose-graph optimization framework.
Experiments show that GRAND-SLAM provides state-of-the-art tracking performance
and 28% higher PSNR than existing methods on the Replica indoor dataset, as
well as 91% lower multi-agent tracking error and improved rendering over
existing multi-agent methods on the large-scale, outdoor Kimera-Multi dataset.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [324] [Shrinking the Generation-Verification Gap with Weak Verifiers](https://arxiv.org/abs/2506.18203)
*Jon Saad-Falcon,E. Kelly Buchanan,Mayee F. Chen,Tzu-Heng Huang,Brendan McLaughlin,Tanvir Bhathal,Shang Zhu,Ben Athiwaratkun,Frederic Sala,Scott Linderman,Azalia Mirhoseini,Christopher R*

Main category: cs.CR

TL;DR: Weaver is a framework combining multiple weak verifiers into a strong one, improving language model response selection without heavy labeled data dependency.


<details>
  <summary>Details</summary>
Motivation: To bridge the performance gap between imperfect verifiers (like LM judges) and oracle verifiers by leveraging ensembles.

Method: Weaver uses weak supervision to estimate verifier accuracies, normalizes outputs, and filters low-quality verifiers, then combines them into a unified score.

Result: Weaver significantly improves response selection, achieving 87.7% accuracy, comparable to GPT-4o, with reduced computational costs.

Conclusion: Weaver effectively enhances verifier performance, closing the gap to oracle verifiers while minimizing labeled data and computational needs.

Abstract: Verifiers can improve language model capabilities by scoring and ranking
responses from generated candidates. Currently, high-quality verifiers are
either unscalable (e.g., humans) or limited in utility (e.g., tools like Lean).
While LM judges and reward models have become broadly useful as general-purpose
verifiers, a significant performance gap remains between them and oracle
verifiers (verifiers with perfect accuracy). To help close this gap, we
introduce Weaver, a framework for designing a strong verifier by combining
multiple weak, imperfect verifiers. We find weighted ensembles of verifiers,
which typically require learning from labeled data, significantly outperform
unweighted combinations due to differences in verifier accuracies. To reduce
dependency on labeled data, Weaver leverages weak supervision to estimate each
verifier's accuracy and combines outputs into a unified score that better
reflects true response quality. However, directly applying weak supervision
algorithms poses challenges, including inconsistent verifier output formats and
handling low-quality verifiers. Weaver addresses these using dataset statistics
to normalize outputs and filter specific verifiers. We study Weaver's
effectiveness in test-time repeated sampling, where a model generates multiple
candidate responses and selects one. Our evaluations show Weaver significantly
improves over Pass@1-performance when selecting the first candidate-across
reasoning and math tasks, achieving o3-mini-level accuracy with Llama 3.3 70B
Instruct as generator, and an ensemble of 70B or smaller judge and reward
models as verifiers (87.7% average). This gain mirrors the jump between GPT-4o
and o3-mini (69.0% vs. 86.7%), which required extensive finetuning and
post-training. To reduce computational costs of verifier ensembles, we train a
400M cross-encoder using Weaver's combined output scores.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [325] [LIGHTHOUSE: Fast and precise distance to shoreline calculations from anywhere on earth](https://arxiv.org/abs/2506.18842)
*Patrick Beukema,Henry Herzog,Yawen Zhang,Hunter Pitelka,Favyen Bastani*

Main category: cs.DB

TL;DR: A new 10m-resolution global coastline dataset and efficient algorithm (Lighthouse) for fast coastal distance calculations, improving precision 100x over existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing coastal datasets are too coarse (1-4 km resolution), limiting their utility. Higher precision is achievable with satellite imagery and computer vision.

Method: Developed a 10m-resolution global coastline dataset and Lighthouse, a fast, resource-efficient library for real-time queries.

Result: Achieved millisecond online inference with minimal resources (1 CPU, 2 GB RAM), enabling real-time applications.

Conclusion: The new dataset and Lighthouse algorithm significantly improve precision and efficiency for coastal distance calculations.

Abstract: We introduce a new dataset and algorithm for fast and efficient coastal
distance calculations from Anywhere on Earth (AoE). Existing global coastal
datasets are only available at coarse resolution (e.g. 1-4 km) which limits
their utility. Publicly available satellite imagery combined with computer
vision enable much higher precision. We provide a global coastline dataset at
10 meter resolution, a 100+ fold improvement in precision over existing data.
To handle the computational challenge of querying at such an increased scale,
we introduce a new library: Layered Iterative Geospatial Hierarchical
Terrain-Oriented Unified Search Engine (Lighthouse). Lighthouse is both
exceptionally fast and resource-efficient, requiring only 1 CPU and 2 GB of RAM
to achieve millisecond online inference, making it well suited for real-time
applications in resource-constrained environments.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [326] [Beyond Prediction -- Structuring Epistemic Integrity in Artificial Reasoning Systems](https://arxiv.org/abs/2506.17331)
*Craig Steven Wright*

Main category: cs.LO

TL;DR: A framework for AI systems with strict epistemic constraints, enabling structured reasoning, propositional commitment, and contradiction detection.


<details>
  <summary>Details</summary>
Motivation: To move beyond stochastic language prediction and ensure truth-preserving, auditable rationality in AI systems.

Method: Formalizes belief representation, metacognitive processes, and normative verification, integrating symbolic inference, knowledge graphs, and blockchain-based justification.

Result: Develops a comprehensive framework for epistemic agents with structured reasoning and contradiction detection.

Conclusion: The framework supports truth-preserving, auditably rational AI systems, advancing beyond traditional stochastic methods.

Abstract: This paper develops a comprehensive framework for artificial intelligence
systems that operate under strict epistemic constraints, moving beyond
stochastic language prediction to support structured reasoning, propositional
commitment, and contradiction detection. It formalises belief representation,
metacognitive processes, and normative verification, integrating symbolic
inference, knowledge graphs, and blockchain-based justification to ensure
truth-preserving, auditably rational epistemic agents.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [327] [DRO-Augment Framework: Robustness by Synergizing Wasserstein Distributionally Robust Optimization and Data Augmentation](https://arxiv.org/abs/2506.17874)
*Jiaming Hu,Debarghya Mukherjee,Ioannis Ch. Paschalidis*

Main category: stat.ML

TL;DR: DRO-Augment combines Wasserstein Distributionally Robust Optimization (W-DRO) with data augmentation to enhance DNN robustness against corruptions and adversarial attacks while maintaining clean data accuracy.


<details>
  <summary>Details</summary>
Motivation: Improving DNN robustness against corrupted data and adversarial attacks simultaneously, as current data augmentation methods fall short.

Method: Integrates W-DRO with data augmentation strategies to train models, supported by theoretical generalization error bounds.

Result: Outperforms existing methods on benchmark datasets (e.g., CIFAR-10-C, MNIST) under severe perturbations and adversarial attacks.

Conclusion: DRO-Augment effectively boosts model robustness without compromising clean data performance, with theoretical guarantees.

Abstract: In many real-world applications, ensuring the robustness and stability of
deep neural networks (DNNs) is crucial, particularly for image classification
tasks that encounter various input perturbations. While data augmentation
techniques have been widely adopted to enhance the resilience of a trained
model against such perturbations, there remains significant room for
improvement in robustness against corrupted data and adversarial attacks
simultaneously. To address this challenge, we introduce DRO-Augment, a novel
framework that integrates Wasserstein Distributionally Robust Optimization
(W-DRO) with various data augmentation strategies to improve the robustness of
the models significantly across a broad spectrum of corruptions. Our method
outperforms existing augmentation methods under severe data perturbations and
adversarial attack scenarios while maintaining the accuracy on the clean
datasets on a range of benchmark datasets, including but not limited to
CIFAR-10-C, CIFAR-100-C, MNIST, and Fashion-MNIST. On the theoretical side, we
establish novel generalization error bounds for neural networks trained using a
computationally efficient, variation-regularized loss function closely related
to the W-DRO problem.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [328] [PaceLLM: Brain-Inspired Large Language Models for Long-Context Understanding](https://arxiv.org/abs/2506.17310)
*Kangcong Li,Peng Ye,Chongjun Tu,Lin Zhang,Chunfeng Song,Jiamin Wu,Tao Yang,Qihao Zheng,Tao Chen*

Main category: q-bio.NC

TL;DR: PaceLLM introduces brain-inspired innovationsPersistent Activity Mechanism and Cortical Expert Clusteringto enhance LLMs' long-context performance, achieving significant improvements in benchmarks and extending context length to 200K tokens.


<details>
  <summary>Details</summary>
Motivation: Addressing limitations of LLMs in long-context tasks due to information decay and semantic fragmentation by mimicking brain mechanisms like working memory and cortical modularity.

Method: Proposes PaceLLM with (1) Persistent Activity Mechanism for dynamic FFN state management and (2) Cortical Expert Clustering for semantic module reorganization.

Result: 6% improvement on LongBench's Multi-document QA, 12.5-17.5% gains on Infinite-Bench, and 200K-token context length in NIAH tests.

Conclusion: PaceLLM pioneers brain-inspired LLM optimization, enhancing long-context performance and interpretability without structural changes, applicable to any model.

Abstract: While Large Language Models (LLMs) demonstrate strong performance across
domains, their long-context capabilities are limited by transient neural
activations causing information decay and unstructured feed-forward network
(FFN) weights leading to semantic fragmentation. Inspired by the brain's
working memory and cortical modularity, we propose PaceLLM, featuring two
innovations: (1) a Persistent Activity (PA) Mechanism that mimics prefrontal
cortex (PFC) neurons' persistent firing by introducing an activation-level
memory bank to dynamically retrieve, reuse, and update critical FFN states,
addressing contextual decay; and (2) Cortical Expert (CE) Clustering that
emulates task-adaptive neural specialization to reorganize FFN weights into
semantic modules, establishing cross-token dependencies and mitigating
fragmentation. Extensive evaluations show that PaceLLM achieves 6% improvement
on LongBench's Multi-document QA and 12.5-17.5% performance gains on
Infinite-Bench tasks, while extending measurable context length to 200K tokens
in Needle-In-A-Haystack (NIAH) tests. This work pioneers brain-inspired LLM
optimization and is complementary to other works. Besides, it can be
generalized to any model and enhance their long-context performance and
interpretability without structural overhauls.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [329] [Can Common VLMs Rival Medical VLMs? Evaluation and Strategic Insights](https://arxiv.org/abs/2506.17337)
*Yuan Zhong,Ruinan Jin,Xiaoxiao Li,Qi Dou*

Main category: eess.IV

TL;DR: Common VLMs, when fine-tuned, can rival or outperform medical-specific VLMs in medical imaging tasks, offering a cost-effective alternative.


<details>
  <summary>Details</summary>
Motivation: To determine if fine-tuned common VLMs can compete with medical-specific VLMs in disease diagnosis and VQA tasks.

Method: Systematic evaluation of CLIP-based and LLaVA-based models, comparing off-the-shelf performance, fine-tuning impact, and OOD generalization.

Result: Fine-tuned common VLMs match or surpass medical VLMs in ID tasks and show strong adaptability in OOD tasks.

Conclusion: Common VLMs with lightweight fine-tuning are a scalable and efficient alternative to medical-specific pretraining.

Abstract: Medical vision-language models (VLMs) leverage large-scale pretraining for
diverse imaging tasks but require substantial computational and data resources.
Meanwhile, common or general-purpose VLMs (e.g., CLIP, LLaVA), though not
trained for medical use, show promise with fine-tuning. This raises a key
question: Can efficient fine-tuned common VLMs rival generalist medical VLMs
for solving specific medical imaging tasks? This study systematically evaluates
common and medical VLMs across disease diagnosis and visual question answering
(VQA). Using CLIP-based and LLaVA-based models, we examine (1) off-the-shelf
performance gaps in in-domain (ID) settings, (2) whether fine-tuning bridges
these gaps, and (3) generalization to out-of-domain (OOD) tasks on unseen
medical modalities. While medical-specific pretraining provides advantages in
ID settings, common VLMs match or surpass medical-specific models after
lightweight fine-tuning, with LoRA-based adaptation proving highly effective
among different tasks. In OOD tasks, common VLMs demonstrate strong
adaptability in some tasks, challenging the assumption that medical-specific
pre-training is essential. These findings suggest that leveraging common VLMs
with fine-tuning offers a scalable and cost-effective alternative to developing
large-scale medical VLMs, providing crucial insights for future research in the
medical imaging field.

</details>


### [330] [DSA-NRP: No-Reflow Prediction from Angiographic Perfusion Dynamics in Stroke EVT](https://arxiv.org/abs/2506.17501)
*Shreeram Athreya,Carlos Olivares,Ameera Ismail,Kambiz Nael,William Speier,Corey Arnold*

Main category: eess.IV

TL;DR: A machine learning framework predicts no-reflow post-EVT using intra-procedural DSA sequences and clinical data, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: To address delayed no-reflow detection post-EVT, which undermines recovery, by enabling immediate prediction for proactive intervention.

Method: Retrospective analysis of AIS patients with favorable mTICI scores, using DSA sequences and clinical variables to train ML classifiers for no-reflow prediction.

Result: ML framework significantly outperformed clinical-features baseline (AUC: 0.7703 vs. 0.5728; accuracy: 0.8125 vs. 0.6331).

Conclusion: Real-time DSA perfusion dynamics offer critical insights, enabling immediate no-reflow prediction and proactive patient management.

Abstract: Following successful large-vessel recanalization via endovascular
thrombectomy (EVT) for acute ischemic stroke (AIS), some patients experience a
complication known as no-reflow, defined by persistent microvascular
hypoperfusion that undermines tissue recovery and worsens clinical outcomes.
Although prompt identification is crucial, standard clinical practice relies on
perfusion magnetic resonance imaging (MRI) within 24 hours post-procedure,
delaying intervention. In this work, we introduce the first-ever machine
learning (ML) framework to predict no-reflow immediately after EVT by
leveraging previously unexplored intra-procedural digital subtraction
angiography (DSA) sequences and clinical variables. Our retrospective analysis
included AIS patients treated at UCLA Medical Center (2011-2024) who achieved
favorable mTICI scores (2b-3) and underwent pre- and post-procedure MRI.
No-reflow was defined as persistent hypoperfusion (Tmax > 6 s) on
post-procedural imaging. From DSA sequences (AP and lateral views), we
extracted statistical and temporal perfusion features from the target
downstream territory to train ML classifiers for predicting no-reflow. Our
novel method significantly outperformed a clinical-features baseline(AUC:
0.7703 $\pm$ 0.12 vs. 0.5728 $\pm$ 0.12; accuracy: 0.8125 $\pm$ 0.10 vs. 0.6331
$\pm$ 0.09), demonstrating that real-time DSA perfusion dynamics encode
critical insights into microvascular integrity. This approach establishes a
foundation for immediate, accurate no-reflow prediction, enabling clinicians to
proactively manage high-risk patients without reliance on delayed imaging.

</details>


### [331] [MTSIC: Multi-stage Transformer-based GAN for Spectral Infrared Image Colorization](https://arxiv.org/abs/2506.17540)
*Tingting Liu,Yuan Liu,Jinhui Tang,Liyin Yuan,Chengyu Liu,Chunlai Li,Xiubao Sui,Qian Chen*

Main category: eess.IV

TL;DR: A GAN-based framework (MTSIC) enhances TIR image colorization using multi-band spectral data and a Transformer network with self-attention mechanisms, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: TIR images lack color and texture, limiting usability. Existing methods struggle with distortion and ambiguity due to limited spectral data.

Method: Proposes MTSIC, a GAN framework with a multi-stage spectral self-attention Transformer (STformer) and spatial-spectral attention blocks (SARB) for multi-band feature mapping.

Result: The method significantly improves visual quality and outperforms traditional techniques.

Conclusion: MTSIC effectively enhances TIR image colorization by leveraging multi-band spectral data and advanced attention mechanisms.

Abstract: Thermal infrared (TIR) images, acquired through thermal radiation imaging,
are unaffected by variations in lighting conditions and atmospheric haze.
However, TIR images inherently lack color and texture information, limiting
downstream tasks and potentially causing visual fatigue. Existing colorization
methods primarily rely on single-band images with limited spectral information
and insufficient feature extraction capabilities, which often result in image
distortion and semantic ambiguity. In contrast, multiband infrared imagery
provides richer spectral data, facilitating the preservation of finer details
and enhancing semantic accuracy. In this paper, we propose a generative
adversarial network (GAN)-based framework designed to integrate spectral
information to enhance the colorization of infrared images. The framework
employs a multi-stage spectral self-attention Transformer network (MTSIC) as
the generator. Each spectral feature is treated as a token for self-attention
computation, and a multi-head self-attention mechanism forms a spatial-spectral
attention residual block (SARB), achieving multi-band feature mapping and
reducing semantic confusion. Multiple SARB units are integrated into a
Transformer-based single-stage network (STformer), which uses a U-shaped
architecture to extract contextual information, combined with multi-scale
wavelet blocks (MSWB) to align semantic information in the spatial-frequency
dual domain. Multiple STformer modules are cascaded to form MTSIC,
progressively optimizing the reconstruction quality. Experimental results
demonstrate that the proposed method significantly outperforms traditional
techniques and effectively enhances the visual quality of infrared images.

</details>


### [332] [LVPNet: A Latent-variable-based Prediction-driven End-to-end Framework for Lossless Compression of Medical Images](https://arxiv.org/abs/2506.17983)
*Chenyue Song,Chen Hui,Qing Lin,Wei Zhang,Siqiao Li,Shengping Zhang,Haiqi Zhu,Zhixuan Li,Shaohui Liu,Feng Jiang,Xiang Li*

Main category: eess.IV

TL;DR: LVPNet improves lossless medical image compression by using global latent variables and addressing inefficiencies in existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods inefficiently distribute latent variable information, causing posterior collapse.

Method: Proposes LVPNet with Global Multi-scale Sensing Module (GMSM) and Quantization Compensation Module (QCM).

Result: Achieves superior compression efficiency and maintains competitive inference speed.

Conclusion: LVPNet outperforms state-of-the-art methods in lossless medical image compression.

Abstract: Autoregressive Initial Bits is a framework that integrates sub-image
autoregression and latent variable modeling, demonstrating its advantages in
lossless medical image compression. However, in existing methods, the image
segmentation process leads to an even distribution of latent variable
information across each sub-image, which in turn causes posterior collapse and
inefficient utilization of latent variables. To deal with these issues, we
propose a prediction-based end-to-end lossless medical image compression method
named LVPNet, leveraging global latent variables to predict pixel values and
encoding predicted probabilities for lossless compression. Specifically, we
introduce the Global Multi-scale Sensing Module (GMSM), which extracts compact
and informative latent representations from the entire image, effectively
capturing spatial dependencies within the latent space. Furthermore, to
mitigate the information loss introduced during quantization, we propose the
Quantization Compensation Module (QCM), which learns the distribution of
quantization errors and refines the quantized features to compensate for
quantization loss. Extensive experiments on challenging benchmarks demonstrate
that our method achieves superior compression efficiency compared to
state-of-the-art lossless image compression approaches, while maintaining
competitive inference speed. The code is at
https://github.com/Anonymity00000/Anonymity-repository/.

</details>


### [333] [Multimodal Medical Image Binding via Shared Text Embeddings](https://arxiv.org/abs/2506.18072)
*Yunhao Liu,Suyang Xi,Shiqi Liu,Hong Ding,Chicheng Jin,Chenxi Yang,Junjun He,Yiqing Shen*

Main category: eess.IV

TL;DR: MBind is a pre-training framework aligning multiple medical imaging modalities via a shared text space, eliminating the need for explicit paired data, and outperforms CLIP-like models in medical tasks.


<details>
  <summary>Details</summary>
Motivation: Medical image analysis requires aligned feature representations across diverse modalities, but acquiring paired data is challenging.

Method: MBind fine-tunes CLIP-like models to align modality-specific text embeddings and distills them into a unified shared space.

Result: MBind achieves state-of-the-art performance in zero-shot, few-shot classification, and cross-modal retrieval tasks.

Conclusion: MBind effectively enables cross-image-modal alignment for medical analysis without paired data.

Abstract: Medical image analysis increasingly relies on the integration of multiple
imaging modalities to capture complementary anatomical and functional
information, enabling more accurate diagnosis and treatment planning. Achieving
aligned feature representations across these diverse modalities is therefore
important for effective multimodal analysis. While contrastive language-image
pre-training (CLIP) and its variant have enabled image-text alignments, they
require explicitly paired data between arbitrary two modalities, which is
difficult to acquire in medical contexts. To address the gap, we present
Multimodal Medical Image Binding with Text (M\textsuperscript{3}Bind), a novel
pre-training framework that enables seamless alignment of multiple medical
imaging modalities through a shared text representation space without requiring
explicit paired data between any two medical image modalities. Specifically,
based on the insight that different images can naturally bind with text,
M\textsuperscript{3}Bind first fine-tunes pre-trained CLIP-like image-text
models to align their modality-specific text embedding space while preserving
their original image-text alignments. Subsequently, we distill these
modality-specific text encoders into a unified model, creating a shared text
embedding space. Experiments on X-ray, CT, retina, ECG, and pathological images
on multiple downstream tasks demonstrate that M\textsuperscript{3}Bind achieves
state-of-the-art performance in zero-shot, few-shot classification and
cross-modal retrieval tasks compared to its CLIP-like counterparts. These
results validate M\textsuperscript{3}Bind's effectiveness in achieving
cross-image-modal alignment for medical analysis.

</details>


### [334] [Transforming H&E images into IHC: A Variance-Penalized GAN for Precision Oncology](https://arxiv.org/abs/2506.18371)
*Sara Rehmat,Hafeez Ur Rehman*

Main category: eess.IV

TL;DR: A deep learning framework translates H&E-stained images to high-fidelity IHC images for HER2 assessment, outperforming existing methods and addressing mode collapse in GANs.


<details>
  <summary>Details</summary>
Motivation: HER2-positive breast cancer requires precise diagnosis, but standard IHC is costly and labor-intensive, while H&E lacks specificity.

Method: Modified pyramid pix2pix with a novel variance-based penalty to enhance structural diversity and mitigate mode collapse in GANs.

Result: Superior performance in translating HER2-positive (IHC 3+) images, with higher PSNR, SSIM, and FID scores than state-of-the-art methods.

Conclusion: The model offers a cost-effective, scalable alternative for HER2 diagnostics and shows potential for broader image-to-image translation tasks.

Abstract: The overexpression of the human epidermal growth factor receptor 2 (HER2) in
breast cells is a key driver of HER2-positive breast cancer, a highly
aggressive subtype requiring precise diagnosis and targeted therapy.
Immunohistochemistry (IHC) is the standard technique for HER2 assessment but is
costly, labor-intensive, and highly dependent on antibody selection. In
contrast, hematoxylin and eosin (H&E) staining, a routine histopathological
procedure, offers broader accessibility but lacks HER2 specificity. This study
proposes an advanced deep learning-based image translation framework to
generate highfidelity IHC images from H&E-stained tissue samples, enabling
cost-effective and scalable HER2 assessment. By modifying the loss function of
pyramid pix2pix, we mitigate mode collapse, a fundamental limitation in
generative adversarial networks (GANs), and introduce a novel variance-based
penalty that enforces structural diversity in generated images. Our model
particularly excels in translating HER2-positive (IHC 3+) images, which have
remained challenging for existing methods due to their complex morphological
variations. Extensive evaluations on the BCI histopathological dataset
demonstrate that our model surpasses state-of-the-art methods in terms of peak
signal-tonoise ratio (PSNR), structural similarity index (SSIM), and Frechet
Inception Distance (FID), particularly in accurately translating HER2-positive
(IHC 3+) images. Beyond medical imaging, our model exhibits superior
performance in general image-to-image translation tasks, showcasing its
potential across multiple domains. This work marks a significant step toward
AI-driven precision oncology, offering a reliable and efficient alternative to
traditional HER2 diagnostics.

</details>


### [335] [Taming Vision-Language Models for Medical Image Analysis: A Comprehensive Review](https://arxiv.org/abs/2506.18378)
*Haoneng Lin,Cheng Xu,Jing Qin*

Main category: eess.IV

TL;DR: This review summarizes advances in adapting Vision-Language Models (VLMs) for medical image analysis, addressing challenges like domain gaps and task diversity, and suggests future research directions.


<details>
  <summary>Details</summary>
Motivation: VLMs show promise for medical image analysis but face challenges in adaptation due to domain gaps and task complexity.

Method: The review categorizes five VLM adaptation strategies and analyzes them across eleven medical imaging tasks, covering pretraining, fine-tuning, and prompt learning.

Result: The study highlights current implementations and challenges, providing a literature repository for further research.

Conclusion: The review aims to guide researchers in leveraging VLMs for medical applications, promoting innovative and safe clinical use.

Abstract: Modern Vision-Language Models (VLMs) exhibit unprecedented capabilities in
cross-modal semantic understanding between visual and textual modalities. Given
the intrinsic need for multi-modal integration in clinical applications, VLMs
have emerged as a promising solution for a wide range of medical image analysis
tasks. However, adapting general-purpose VLMs to medical domain poses numerous
challenges, such as large domain gaps, complicated pathological variations, and
diversity and uniqueness of different tasks. The central purpose of this review
is to systematically summarize recent advances in adapting VLMs for medical
image analysis, analyzing current challenges, and recommending promising yet
urgent directions for further investigations. We begin by introducing core
learning strategies for medical VLMs, including pretraining, fine-tuning, and
prompt learning. We then categorize five major VLM adaptation strategies for
medical image analysis. These strategies are further analyzed across eleven
medical imaging tasks to illustrate their current practical implementations.
Furthermore, we analyze key challenges that impede the effective adaptation of
VLMs to clinical applications and discuss potential directions for future
research. We also provide an open-access repository of related literature to
facilitate further research, available at
https://github.com/haonenglin/Awesome-VLM-for-MIA. It is anticipated that this
article can help researchers who are interested in harnessing VLMs in medical
image analysis tasks have a better understanding on their capabilities and
limitations, as well as current technical barriers, to promote their
innovative, robust, and safe application in clinical practice.

</details>


### [336] [A Deep Convolutional Neural Network-Based Novel Class Balancing for Imbalance Data Segmentation](https://arxiv.org/abs/2506.18474)
*Atifa Kalsoom,M. A. Iftikhar,Amjad Ali,Zubair Shah,Shidin Balakrishnan,Hazrat Ali*

Main category: eess.IV

TL;DR: The paper introduces BLCB-CNN, a deep learning pipeline with bi-level class balancing for retinal blood vessel segmentation, achieving high accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Accurate segmentation of retinal blood vessels is challenging due to imbalanced data and varying vessel thickness.

Method: Uses a CNN with bi-level class balancing (Level-I for vessel/non-vessel, Level-II for thick/thin vessels) and pre-processing techniques like GCN, CLAHE, and gamma correction.

Result: Achieves 98.23% AUC, 96.22% accuracy, 81.57% sensitivity, and 97.65% specificity, with successful cross-validation on STARE images.

Conclusion: BLCB-CNN is effective for retinal vessel segmentation, demonstrating high performance and generalization.

Abstract: Retinal fundus images provide valuable insights into the human eye's interior
structure and crucial features, such as blood vessels, optic disk, macula, and
fovea. However, accurate segmentation of retinal blood vessels can be
challenging due to imbalanced data distribution and varying vessel thickness.
In this paper, we propose BLCB-CNN, a novel pipeline based on deep learning and
bi-level class balancing scheme to achieve vessel segmentation in retinal
fundus images. The BLCB-CNN scheme uses a Convolutional Neural Network (CNN)
architecture and an empirical approach to balance the distribution of pixels
across vessel and non-vessel classes and within thin and thick vessels. Level-I
is used for vessel/non-vessel balancing and Level-II is used for thick/thin
vessel balancing. Additionally, pre-processing of the input retinal fundus
image is performed by Global Contrast Normalization (GCN), Contrast Limited
Adaptive Histogram Equalization (CLAHE), and gamma corrections to increase
intensity uniformity as well as to enhance the contrast between vessels and
background pixels. The resulting balanced dataset is used for
classification-based segmentation of the retinal vascular tree. We evaluate the
proposed scheme on standard retinal fundus images and achieve superior
performance measures, including an area under the ROC curve of 98.23%, Accuracy
of 96.22%, Sensitivity of 81.57%, and Specificity of 97.65%. We also
demonstrate the method's efficacy through external cross-validation on STARE
images, confirming its generalization ability.

</details>


### [337] [Temporal Neural Cellular Automata: Application to modeling of contrast enhancement in breast MRI](https://arxiv.org/abs/2506.18720)
*Daniel M. Lang,Richard Osuala,Veronika Spieker,Karim Lekadir,Rickmer Braren,Julia A. Schnabel*

Main category: eess.IV

TL;DR: TeNCA improves synthetic contrast enhancement in breast MRI by modeling temporal evolution using neural cellular automata, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Long acquisition times and high costs limit MRI's use in breast imaging. Synthetic contrast enhancement can address this, but current methods lack temporal consistency.

Method: Introduces TeNCA, extending neural cellular automata (NCA) to model sparse, non-uniform temporal data with adaptive loss and iterative training resembling physical progression.

Result: TeNCA outperforms SOTA methods, generating images that better align with ground truth post-contrast sequences.

Conclusion: TeNCA offers a robust solution for synthetic contrast enhancement, improving temporal consistency and physiological plausibility in breast MRI.

Abstract: Synthetic contrast enhancement offers fast image acquisition and eliminates
the need for intravenous injection of contrast agent. This is particularly
beneficial for breast imaging, where long acquisition times and high cost are
significantly limiting the applicability of magnetic resonance imaging (MRI) as
a widespread screening modality. Recent studies have demonstrated the
feasibility of synthetic contrast generation. However, current state-of-the-art
(SOTA) methods lack sufficient measures for consistent temporal evolution.
Neural cellular automata (NCA) offer a robust and lightweight architecture to
model evolving patterns between neighboring cells or pixels. In this work we
introduce TeNCA (Temporal Neural Cellular Automata), which extends and further
refines NCAs to effectively model temporally sparse, non-uniformly sampled
imaging data. To achieve this, we advance the training strategy by enabling
adaptive loss computation and define the iterative nature of the method to
resemble a physical progression in time. This conditions the model to learn a
physiologically plausible evolution of contrast enhancement. We rigorously
train and test TeNCA on a diverse breast MRI dataset and demonstrate its
effectiveness, surpassing the performance of existing methods in generation of
images that align with ground truth post-contrast sequences.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [338] [Tutorial: $\varphi$-Transductions in OpenFst via the Gallic Semiring](https://arxiv.org/abs/2506.17942)
*Marco Cognetta,Cyril Allauzen*

Main category: cs.FL

TL;DR: The paper explains how to correctly implement -transductions in OpenFst using the Gallic semiring, demonstrated via the MaxMatch tokenization algorithm.


<details>
  <summary>Details</summary>
Motivation: OpenFst's implementation constraint makes -transitions difficult to use directly with transducers.

Method: Utilizes the Gallic semiring in OpenFst to work around the constraint and implement -transductions.

Result: Successful demonstration of -transductions through the MaxMatch (WordPiece) tokenization algorithm.

Conclusion: The method provides a practical solution for -transductions in OpenFst, with code examples for implementation.

Abstract: OpenFst, a popular finite-state transducer library, supports
$\varphi$-transitions but, due to an implementation constraint, they cannot be
used with transducers in a straightforward way.
  In this short tutorial, we describe how one can use other functionality
provided by OpenFst (namely, the Gallic semiring) to correctly implement
$\varphi$-transductions and demonstrate it by implementing the MaxMatch
(WordPiece) tokenization algorithm (Devlin et al., 2019; Song et al., 2021).
Accompanying self-contained code examples are provided.
https://www.openfst.org/twiki/pub/Contrib/FstContrib/phi_transduction_tutorial_code.tgz

</details>

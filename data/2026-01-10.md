<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 94]
- [cs.CV](#cs.CV) [Total: 79]
- [cs.AI](#cs.AI) [Total: 19]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.LG](#cs.LG) [Total: 10]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.DL](#cs.DL) [Total: 2]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.SE](#cs.SE) [Total: 2]
- [cs.CY](#cs.CY) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.CR](#cs.CR) [Total: 3]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [physics.optics](#physics.optics) [Total: 2]
- [cs.RO](#cs.RO) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [MedPI: Evaluating AI Systems in Medical Patient-facing Interactions](https://arxiv.org/abs/2601.04195)
*Diego Fajardo V.,Oleksii Proniakin,Victoria-Elisabeth Gruber,Razvan Marinescu*

Main category: cs.CL

TL;DR: MedPI is a comprehensive benchmark for evaluating LLMs in medical dialogues across 105 dimensions, revealing low performance across models, especially in differential diagnosis.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks focus on single-turn QA, lacking evaluation of medical dialogue processes, treatment safety, outcomes, and doctor-patient communication. There's a need for accreditation-aligned, multi-dimensional assessment of LLMs in clinical conversations.

Method: Five-layer framework: (1) Synthetic EHR-like patient packets, (2) AI Patient with LLM memory/affect, (3) Task Matrix (encounter reasons × objectives), (4) 105-dimension evaluation aligned with ACGME competencies, (5) Calibrated AI Judge committee with evidence-linked rationales.

Result: Evaluated 9 flagship models across 366 AI Patients and 7,097 conversations. All LLMs showed low performance across dimensions, particularly poor in differential diagnosis.

Conclusion: MedPI provides a comprehensive benchmark revealing significant limitations in current LLMs for medical dialogue, especially in diagnostic reasoning. The framework can guide future development of LLMs for clinical applications.

Abstract: We present MedPI, a high-dimensional benchmark for evaluating large language models (LLMs) in patient-clinician conversations. Unlike single-turn question-answer (QA) benchmarks, MedPI evaluates the medical dialogue across 105 dimensions comprising the medical process, treatment safety, treatment outcomes and doctor-patient communication across a granular, accreditation-aligned rubric. MedPI comprises five layers: (1) Patient Packets (synthetic EHR-like ground truth); (2) an AI Patient instantiated through an LLM with memory and affect; (3) a Task Matrix spanning encounter reasons (e.g. anxiety, pregnancy, wellness checkup) x encounter objectives (e.g. diagnosis, lifestyle advice, medication advice); (4) an Evaluation Framework with 105 dimensions on a 1-4 scale mapped to the Accreditation Council for Graduate Medical Education (ACGME) competencies; and (5) AI Judges that are calibrated, committee-based LLMs providing scores, flags, and evidence-linked rationales. We evaluate 9 flagship models -- Claude Opus 4.1, Claude Sonnet 4, MedGemma, Gemini 2.5 Pro, Llama 3.3 70b Instruct, GPT-5, GPT OSS 120b, o3, Grok-4 -- across 366 AI Patients and 7,097 conversations using a standardized "vanilla clinician" prompt. For all LLMs, we observe low performance across a variety of dimensions, in particular on differential diagnosis. Our work can help guide future use of LLMs for diagnosis and treatment recommendations.

</details>


### [2] [RAGVUE: A Diagnostic View for Explainable and Automated Evaluation of Retrieval-Augmented Generation](https://arxiv.org/abs/2601.04196)
*Keerthana Murugaraj,Salima Lamsiyah,Martin Theobald*

Main category: cs.CL

TL;DR: RAGVUE is a diagnostic framework for evaluating RAG systems with explainable metrics instead of single scores, focusing on retrieval quality, answer relevance/completeness, faithfulness, and judge calibration.


<details>
  <summary>Details</summary>
Motivation: Existing RAG evaluation metrics collapse heterogeneous behaviors into single scores and provide little insight into whether errors come from retrieval, reasoning, or grounding, making it hard to diagnose system failures.

Method: RAGVUE decomposes RAG behavior into four components: retrieval quality, answer relevance and completeness, strict claim-level faithfulness, and judge calibration. Each metric includes structured explanations. The framework supports both manual metric selection and fully automated agentic evaluation, with Python API, CLI, and Streamlit interface.

Result: In comparative experiments, RAGVUE surfaces fine-grained failures that existing tools like RAGAS often overlook, providing more detailed diagnostic insights into RAG system performance.

Conclusion: RAGVUE offers a transparent, explainable framework for RAG evaluation that can be integrated into both research pipelines and practical RAG development, with publicly available source code on GitHub.

Abstract: Evaluating Retrieval-Augmented Generation (RAG) systems remains a challenging task: existing metrics often collapse heterogeneous behaviors into single scores and provide little insight into whether errors arise from retrieval,reasoning, or grounding. In this paper, we introduce RAGVUE, a diagnostic and explainable framework for automated, reference-free evaluation of RAG pipelines. RAGVUE decomposes RAG behavior into retrieval quality, answer relevance and completeness, strict claim-level faithfulness, and judge calibration. Each metric includes a structured explanation, making the evaluation process transparent. Our framework supports both manual metric selection and fully automated agentic evaluation. It also provides a Python API, CLI, and a local Streamlit interface for interactive usage. In comparative experiments, RAGVUE surfaces fine-grained failures that existing tools such as RAGAS often overlook. We showcase the full RAGVUE workflow and illustrate how it can be integrated into research pipelines and practical RAG development. The source code and detailed instructions on usage are publicly available on GitHub

</details>


### [3] [Automatic Construction of Chinese Verb Collostruction Database](https://arxiv.org/abs/2601.04197)
*Xuri Tang,Daohuan Liu*

Main category: cs.CL

TL;DR: Unsupervised method for building Chinese verb collostruction database to complement LLMs with interpretable rules


<details>
  <summary>Details</summary>
Motivation: To provide explicit, interpretable linguistic rules for scenarios where LLMs lack explainability, especially for Chinese language applications

Method: Defines verb collostructions as projective, rooted, ordered, directed acyclic graphs; uses clustering algorithms on sentences from large corpus to generate collostructions for each verb

Result: Generated collostructions show functional independence and graded typicality; error correction using maximum matching with collostructions outperforms LLMs

Conclusion: Unsupervised collostruction database effectively complements LLMs by providing interpretable rules that improve performance in tasks like grammatical error correction

Abstract: This paper proposes a fully unsupervised approach to the construction of verb collostruction database for Chinese language, aimed at complementing LLMs by providing explicit and interpretable rules for application scenarios where explanation and interpretability are indispensable. The paper formally defines a verb collostruction as a projective, rooted, ordered, and directed acyclic graph and employs a series of clustering algorithms to generate collostructions for a given verb from a list of sentences retrieved from large-scale corpus. Statistical analysis demonstrates that the generated collostructions possess the design features of functional independence and graded typicality. Evaluation with verb grammatical error correction shows that the error correction algorithm based on maximum matching with collostructions achieves better performance than LLMs.

</details>


### [4] [Attribute-Aware Controlled Product Generation with LLMs for E-commerce](https://arxiv.org/abs/2601.04200)
*Virginia Negri,Víctor Martínez Gómez,Sergio A. Balanya,Subburam Rajaram*

Main category: cs.CL

TL;DR: LLM-based synthetic data generation for e-commerce product information extraction achieves performance comparable to real data and significantly improves over zero-shot baselines.


<details>
  <summary>Details</summary>
Motivation: High-quality labeled datasets for e-commerce product information extraction are difficult to obtain, creating a need for synthetic data generation methods.

Method: Systematic approach using LLMs with controlled modification framework: attribute-preserving modification, controlled negative example generation, and systematic attribute removal, enforced with attribute-aware prompts and store constraints.

Result: Human evaluation shows 99.6% naturalness, 96.5% valid attribute values, and over 90% consistent attribute usage. On MAVE dataset, synthetic data achieves 60.5% accuracy (vs 60.8% real data, 13.4% zero-shot). Hybrid configurations reach 68.8% accuracy.

Conclusion: The framework provides a practical solution for augmenting e-commerce datasets, especially valuable for low-resource scenarios, with synthetic data performing comparably to real data.

Abstract: Product information extraction is crucial for e-commerce services, but obtaining high-quality labeled datasets remains challenging. We present a systematic approach for generating synthetic e-commerce product data using Large Language Models (LLMs), introducing a controlled modification framework with three strategies: attribute-preserving modification, controlled negative example generation, and systematic attribute removal. Using a state-of-the-art LLM with attribute-aware prompts, we enforce store constraints while maintaining product coherence. Human evaluation of 2000 synthetic products demonstrates high effectiveness, with 99.6% rated as natural, 96.5% containing valid attribute values, and over 90% showing consistent attribute usage. On the public MAVE dataset, our synthetic data achieves 60.5% accuracy, performing on par with real training data (60.8%) and significantly improving upon the 13.4% zero-shot baseline. Hybrid configurations combining synthetic and real data further improve performance, reaching 68.8% accuracy. Our framework provides a practical solution for augmenting e-commerce datasets, particularly valuable for low-resource scenarios.

</details>


### [5] [Collective Narrative Grounding: Community-Coordinated Data Contributions to Improve Local AI Systems](https://arxiv.org/abs/2601.04201)
*Zihan Gao,Mohsin Y. K. Yousufi,Jacob Thebault-Spieker*

Main category: cs.CL

TL;DR: A participatory protocol called Collective Narrative Grounding transforms community stories into structured narrative units for AI systems to address LLM blind spots on local queries, reducing 76.7% of errors in local information QA.


<details>
  <summary>Details</summary>
Motivation: LLM question-answering systems often fail on community-specific queries, creating "knowledge blind spots" that marginalize local voices and reinforce epistemic injustice. There's a need to address factual gaps, cultural misunderstandings, geographic confusions, and temporal misalignments in local information.

Method: Developed Collective Narrative Grounding protocol through participatory mapping workshops with N=24 community members. Created elicitation methods and a schema for narrative units that enable entity, time, and place extraction with validation and provenance control. Audited a county-level benchmark of 14,782 local information QA pairs to scope the problem.

Result: Found that 76.7% of errors in local QA pairs stem from factual gaps, cultural misunderstandings, geographic confusions, and temporal misalignments. On participatory QA set, state-of-the-art LLM answered fewer than 21% correctly without added context. Missing facts often appear in collected narratives, suggesting direct path to closing dominant error modes.

Conclusion: The taxonomy, protocol, and participatory evaluation provide a rigorous foundation for building community-grounded AI that better answers local questions. Key design tensions include representation and power, governance and control, and privacy and consent, requiring retrieval-first, provenance-visible, locally governed QA systems.

Abstract: Large language model (LLM) question-answering systems often fail on community-specific queries, creating "knowledge blind spots" that marginalize local voices and reinforce epistemic injustice. We present Collective Narrative Grounding, a participatory protocol that transforms community stories into structured narrative units and integrates them into AI systems under community governance. Learning from three participatory mapping workshops with N=24 community members, we designed elicitation methods and a schema that retain narrative richness while enabling entity, time, and place extraction, validation, and provenance control. To scope the problem, we audit a county-level benchmark of 14,782 local information QA pairs, where factual gaps, cultural misunderstandings, geographic confusions, and temporal misalignments account for 76.7% of errors. On a participatory QA set derived from our workshops, a state-of-the-art LLM answered fewer than 21% of questions correctly without added context, underscoring the need for local grounding. The missing facts often appear in the collected narratives, suggesting a direct path to closing the dominant error modes for narrative items. Beyond the protocol and pilot, we articulate key design tensions, such as representation and power, governance and control, and privacy and consent, providing concrete requirements for retrieval-first, provenance-visible, locally governed QA systems. Together, our taxonomy, protocol, and participatory evaluation offer a rigorous foundation for building community-grounded AI that better answers local questions.

</details>


### [6] [TeleTables: A Benchmark for Large Language Models in Telecom Table Interpretation](https://arxiv.org/abs/2601.04202)
*Anas Ezzakri,Nicola Piovesan,Mohamed Sana,Antonio De Domenico,Fadhel Ayed,Haozhe Zhang*

Main category: cs.CL

TL;DR: TeleTables is a benchmark to evaluate LLMs' knowledge and interpretation of tables in 3GPP telecom standards, revealing that smaller models struggle while larger models show better reasoning, highlighting the need for domain-specialized fine-tuning.


<details>
  <summary>Details</summary>
Motivation: LLMs perform poorly on telecom standards like 3GPP specifications, which heavily rely on tables to present essential information. The authors argue that LLMs' knowledge and interpretation ability of such tables remains largely unexamined, creating a gap in understanding their limitations in technical domains.

Method: The authors introduce TeleTables, a benchmark built through a novel multi-stage data generation pipeline that extracts tables from 3GPP standards and uses multimodal and reasoning-oriented LLMs to generate and validate questions. The resulting dataset comprises 500 human-verified question-answer pairs, each associated with corresponding tables in multiple formats.

Result: Smaller models (under 10B parameters) struggle both to recall 3GPP knowledge and to interpret tables, indicating limited exposure to telecom standards in pretraining and insufficient inductive biases for complex technical material. Larger models show stronger reasoning on table interpretation.

Conclusion: TeleTables highlights the need for domain-specialized fine-tuning to reliably interpret and reason over telecom standards, as current LLMs have significant limitations in handling technical tables from specialized domains like 3GPP specifications.

Abstract: Language Models (LLMs) are increasingly explored in the telecom industry to support engineering tasks, accelerate troubleshooting, and assist in interpreting complex technical documents. However, recent studies show that LLMs perform poorly on telecom standards, particularly 3GPP specifications. We argue that a key reason is that these standards densely include tables to present essential information, yet the LLM knowledge and interpretation ability of such tables remains largely unexamined. To address this gap, we introduce TeleTables, a benchmark designed to evaluate both the implicit knowledge LLMs have about tables in technical specifications and their explicit ability to interpret them. TeleTables is built through a novel multi-stage data generation pipeline that extracts tables from 3GPP standards and uses multimodal and reasoning-oriented LLMs to generate and validate questions. The resulting dataset, which is publicly available, comprises 500 human-verified question-answer pairs, each associated with the corresponding table in multiple formats. Our evaluation shows that, smaller models (under 10B parameters) struggle both to recall 3GPP knowledge and to interpret tables, indicating the limited exposure to telecom standards in their pretraining and the insufficient inductive biases for navigating complex technical material. Larger models, on the other hand, show stronger reasoning on table interpretation. Overall, TeleTables highlights the need for domain-specialized fine-tuning to reliably interpret and reason over telecom standards.

</details>


### [7] [FronTalk: Benchmarking Front-End Development as Conversational Code Generation with Multi-Modal Feedback](https://arxiv.org/abs/2601.04203)
*Xueqing Wu,Zihan Xue,Da Yin,Shuyan Zhou,Kai-Wei Chang,Nanyun Peng,Yeming Wen*

Main category: cs.CL

TL;DR: FronTalk is a benchmark for conversational front-end code generation with multi-modal feedback, featuring 100 real-world multi-turn dialogues with both textual and visual instructions, revealing key challenges in forgetting and visual interpretation.


<details>
  <summary>Details</summary>
Motivation: Front-end development relies heavily on visual artifacts like sketches and mockups to convey design intent, but current research largely ignores how these visual elements function in multi-turn code generation scenarios.

Method: Created FronTalk benchmark with 100 multi-turn dialogues from real websites across diverse domains, each turn having both textual and visual instructions. Proposed agent-based evaluation framework using web agents to simulate users and measure functional correctness and user experience.

Result: Evaluation of 20 models revealed two key challenges: significant forgetting issue where models overwrite previous features, and persistent difficulty interpreting visual feedback (especially for open-source VLMs). Proposed AceCoder baseline reduces forgetting to nearly zero and improves performance by up to 9.3%.

Conclusion: FronTalk provides foundation for future research in front-end development and multi-turn, multi-modal code generation, addressing under-explored interaction dynamics and offering solutions to key challenges like forgetting and visual interpretation.

Abstract: We present FronTalk, a benchmark for front-end code generation that pioneers the study of a unique interaction dynamic: conversational code generation with multi-modal feedback. In front-end development, visual artifacts such as sketches, mockups and annotated creenshots are essential for conveying design intent, yet their role in multi-turn code generation remains largely unexplored. To address this gap, we focus on the front-end development task and curate FronTalk, a collection of 100 multi-turn dialogues derived from real-world websites across diverse domains such as news, finance, and art. Each turn features both a textual instruction and an equivalent visual instruction, each representing the same user intent. To comprehensively evaluate model performance, we propose a novel agent-based evaluation framework leveraging a web agent to simulate users and explore the website, and thus measuring both functional correctness and user experience. Evaluation of 20 models reveals two key challenges that are under-explored systematically in the literature: (1) a significant forgetting issue where models overwrite previously implemented features, resulting in task failures, and (2) a persistent challenge in interpreting visual feedback, especially for open-source vision-language models (VLMs). We propose a strong baseline to tackle the forgetting issue with AceCoder, a method that critiques the implementation of every past instruction using an autonomous web agent. This approach significantly reduces forgetting to nearly zero and improves the performance by up to 9.3% (56.0% to 65.3%). Overall, we aim to provide a solid foundation for future research in front-end development and the general interaction dynamics of multi-turn, multi-modal code generation. Code and data are released at https://github.com/shirley-wu/frontalk

</details>


### [8] [STDD:Spatio-Temporal Dynamics-Driven Token Refinement in Diffusion Language Models](https://arxiv.org/abs/2601.04205)
*Xinhao Sun,Maoliang Li,Zihao Zheng,Jiayu Chen,Hezhao Xu,Yun Liang,Xiang Chen*

Main category: cs.CL

TL;DR: Proposes a dynamic remasking strategy for diffusion language models that adapts confidence thresholds per token per step based on temporal variance and spatial deviance, achieving up to 8.9x speedup while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: Current diffusion language models use fixed global confidence thresholds for remasking, which causes redundant iterations and limits parallelism by not accounting for individual token dynamics and convergence patterns.

Method: Introduces a novel remasking approach that dynamically detects Temporal Variance (convergence status) and Spatial Deviance (inter-token correlations) for each token, then adaptively adjusts confidence thresholds per token per step based on these signals.

Result: Significantly improves DLM operational efficiency across mainstream datasets with speedups up to 8.9 times while faithfully preserving generation quality compared to fixed-threshold approaches.

Conclusion: Dynamic per-token threshold adjustment based on temporal and spatial dynamics is more effective than fixed global thresholds for diffusion language model remasking, enabling substantial efficiency gains without quality degradation.

Abstract: Unlike autoregressive language models, diffusion language models (DLMs) generate text by iteratively denoising all token positions in parallel. At each timestep, the remasking strategy of a DLM selects low-priority tokens to defer their decoding, thereby improving both efficiency and output quality. However, mainstream remasking strategies rely on a single global confidence threshold, overlooking the temporal and spatial dynamics of individual tokens. Motivated by the redundant iterations and constrained parallelism introduced by fixed-threshold remasking, we propose a novel remasking approach that dynamically detects Temporal Variance and Spatial Deviance of each token, which reflect its convergence status and inter-token correlations. Using these signals, our method adaptively adjusts the confidence threshold for every token at every step. Empirical results show that our approach significantly improves the operational efficiency of DLMs across mainstream datasets, achieving speedups of up to 8.9 times while faithfully preserving generation quality.

</details>


### [9] [Enhancing Admission Inquiry Responses with Fine-Tuned Models and Retrieval-Augmented Generation](https://arxiv.org/abs/2601.04206)
*Aram Virabyan*

Main category: cs.CL

TL;DR: AI system combining fine-tuned language model with RAG for university admissions inquiry management, improving response time and accuracy.


<details>
  <summary>Details</summary>
Motivation: University admissions offices struggle with high inquiry volumes while maintaining response quality, which affects prospective students' perceptions. Current RAG systems have limitations in narrow, complex domains like admissions due to intricate rules and specific details.

Method: Proposed hybrid AI system integrating fine-tuned language model with Retrieval-Augmented Generation (RAG). Fine-tuned the model on curated admissions-specific dataset to enhance interpretation of RAG-provided data. Explored optimization strategies for response generation logic to balance quality and speed.

Result: The approach leverages RAG's ability to access up-to-date information and fine-tuning's capacity to embed nuanced domain understanding. System aims to generate consistently high-quality, domain-relevant outputs for admissions communications.

Conclusion: Hybrid AI system combining fine-tuning with RAG addresses limitations of standalone RAG in complex domains like university admissions, potentially improving inquiry management efficiency and response quality.

Abstract: University admissions offices face the significant challenge of managing high volumes of inquiries efficiently while maintaining response quality, which critically impacts prospective students' perceptions. This paper addresses the issues of response time and information accuracy by proposing an AI system integrating a fine-tuned language model with Retrieval-Augmented Generation (RAG). While RAG effectively retrieves relevant information from large datasets, its performance in narrow, complex domains like university admissions can be limited without adaptation, potentially leading to contextually inadequate responses due to the intricate rules and specific details involved. To overcome this, we fine-tuned the model on a curated dataset specific to admissions processes, enhancing its ability to interpret RAG-provided data accurately and generate domain-relevant outputs. This hybrid approach leverages RAG's ability to access up-to-date information and fine-tuning's capacity to embed nuanced domain understanding. We further explored optimization strategies for the response generation logic, experimenting with settings to balance response quality and speed, aiming for consistently high-quality outputs that meet the specific requirements of admissions communications.

</details>


### [10] [Ideology as a Problem: Lightweight Logit Steering for Annotator-Specific Alignment in Social Media Analysis](https://arxiv.org/abs/2601.04207)
*Wei Xia,Haowen Tang,Luozheng Li*

Main category: cs.CL

TL;DR: The paper introduces a lightweight linear probe method to measure and correct misalignment between LLMs' internal political ideology representations and human ideological space, enabling efficient alignment with specific user opinions without retraining.


<details>
  <summary>Details</summary>
Motivation: LLMs internally organize political ideology in ways that don't fully align with human ideological space, creating systematic, model-specific misalignment that needs to be addressed.

Method: Developed a lightweight linear probe that quantifies misalignment and minimally corrects the output layer by calculating bias scores from internal features and directly adjusting final output probabilities.

Result: The method provides a practical, low-cost solution for aligning models with specific user opinions while preserving the original reasoning power of the model.

Conclusion: The approach offers an efficient alternative to retraining for aligning LLMs with human ideological perspectives, addressing systematic misalignment in a measurable and correctable way.

Abstract: LLMs internally organize political ideology along low-dimensional structures that are partially, but not fully aligned with human ideological space. This misalignment is systematic, model specific, and measurable. We introduce a lightweight linear probe that both quantifies the misalignment and minimally corrects the output layer. This paper introduces a simple and efficient method for aligning models with specific user opinions. Instead of retraining the model, we calculated a bias score from its internal features and directly adjusted the final output probabilities. This solution is practical and low-cost and preserves the original reasoning power of the model.

</details>


### [11] [LLMs for Explainable Business Decision-Making: A Reinforcement Learning Fine-Tuning Approach](https://arxiv.org/abs/2601.04208)
*Xiang Cheng,Wen Wang,Anindya Ghose*

Main category: cs.CL

TL;DR: LEXMA is a reinforcement learning framework that fine-tunes LLMs to generate narrative explanations for AI decisions that are both decision-correct and tailored to different audiences (experts vs. consumers), without needing human-annotated training data.


<details>
  <summary>Details</summary>
Motivation: Current explainable AI methods using numerical feature attributions fail to provide coherent narratives for AI decisions. LLMs can generate natural language explanations but face challenges: ensuring explanations are both decision-correct and faithful to prediction factors, serving multiple audiences without changing decision rules, and training efficiently without large human-scored explanation datasets.

Method: LEXMA uses reinforcement learning with reflection-augmented supervised fine-tuning and two stages of Group Relative Policy Optimization (GRPO). It fine-tunes separate parameter sets to improve decision correctness and satisfy stylistic requirements for different audiences (experts vs. consumers), using reward signals that don't require human-annotated explanations.

Result: LEXMA significantly improves predictive performance compared to other LLM baselines in mortgage approval decisions. Human evaluations show expert-facing explanations are more risk-focused, while consumer-facing explanations are clearer, more actionable, and more polite.

Conclusion: LEXMA provides a cost-efficient, systematic LLM fine-tuning approach to enhance explanation quality for business decisions, offering strong potential for scalable deployment of transparent AI systems that can generate audience-appropriate narrative explanations.

Abstract: Artificial Intelligence (AI) models increasingly drive high-stakes consumer interactions, yet their decision logic often remains opaque. Prevailing explainable AI techniques rely on post hoc numerical feature attributions, which fail to provide coherent narratives behind model decisions. Large language models (LLMs) present an opportunity to generate natural-language explanations, but three design challenges remain unresolved: explanations must be both decision-correct and faithful to the factors that drive the prediction; they should be able to serve multiple audiences without shifting the underlying decision rule; and they should be trained in a label-efficient way that does not depend on large corpora of human-scored explanations. To address these challenges, we introduce LEXMA (LLM-based EXplanations for Multi-Audience decisions), a reinforcement-learning-based fine-tuning framework that produces narrative-driven, audience-appropriate explanations. LEXMA combines reflection-augmented supervised fine-tuning with two stages of Group Relative Policy Optimization (GRPO). Specifically, it fine-tunes two separate parameter sets to improve decision correctness and satisfy stylistic requirements for different audiences, using reward signals that do not rely on human-annotated explanations. We instantiate LEXMA in the context of mortgage approval decisions. Results demonstrate that LEXMA yields significant improvements in predictive performance compared with other LLM baselines. Moreover, human evaluations show that expert-facing explanations generated by our approach are more risk-focused, and consumer-facing explanations are clearer, more actionable, and more polite. Our study contributes a cost-efficient, systematic LLM fine-tuning approach to enhance explanation quality for business decisions, offering strong potential for scalable deployment of transparent AI systems.

</details>


### [12] [Leveraging Language Models and RAG for Efficient Knowledge Discovery in Clinical Environments](https://arxiv.org/abs/2601.04209)
*Seokhwan Ko,Donghyeon Lee,Jaewoo Chun,Hyungsoo Han,Junghwan Cho*

Main category: cs.CL

TL;DR: A locally-deployed RAG system using PubMedBERT and LLaMA3 for recommending research collaborators based on PubMed publications within hospital privacy constraints.


<details>
  <summary>Details</summary>
Motivation: LLMs are valuable in medical settings but hospital privacy/security regulations require sensitive data to be processed locally, creating a need for privacy-preserving systems that can still support biomedical knowledge discovery.

Method: Developed a retrieval-augmented generation (RAG) system using PubMedBERT for domain-specific embedding generation and locally deployed LLaMA3 model for generative synthesis to recommend research collaborators based on PubMed publications.

Result: The study demonstrates the feasibility and utility of integrating domain-specialized encoders with lightweight LLMs to support biomedical knowledge discovery under local deployment constraints.

Conclusion: A practical approach for implementing LLM-based systems in privacy-sensitive medical environments by combining domain-specific embeddings with locally-deployed lightweight models for research collaboration recommendations.

Abstract: Large language models (LLMs) are increasingly recognized as valuable tools across the medical environment, supporting clinical, research, and administrative workflows. However, strict privacy and network security regulations in hospital settings require that sensitive data be processed within fully local infrastructures. Within this context, we developed and evaluated a retrieval-augmented generation (RAG) system designed to recommend research collaborators based on PubMed publications authored by members of a medical institution. The system utilizes PubMedBERT for domain-specific embedding generation and a locally deployed LLaMA3 model for generative synthesis. This study demonstrates the feasibility and utility of integrating domain-specialized encoders with lightweight LLMs to support biomedical knowledge discovery under local deployment constraints.

</details>


### [13] [Complexity Agnostic Recursive Decomposition of Thoughts](https://arxiv.org/abs/2601.04210)
*Kaleem Ullah Qasim,Jiashu Zhang,Hafiz Saif Ur Rehman*

Main category: cs.CL

TL;DR: CARD is a framework that predicts problem complexity before generation and adapts decomposition strategies accordingly, achieving higher accuracy with significantly fewer tokens on math reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Large language models often fail on multi-step reasoning due to fixed reasoning strategies that ignore problem-specific difficulty, leading to inefficient token usage and suboptimal performance.

Method: CARD uses MRCE (Multi-dimensional Reasoning Complexity Estimator), a 0.6B Qwen model that predicts 30 fine-grained features from question text, followed by a two-stage recursive solver: (1) hierarchical decomposition into K steps based on task profile, and (2) per-step thought budget allocation via recursive MRCE profiling.

Result: On GSM8K, CARD achieves 81.4% to 89.2% accuracy while reducing token cost by 1.88x to 2.40x compared to fixed decomposition baselines. On MATH-500, it reaches 75.1% to 86.8% accuracy using 1.71x to 5.74x fewer tokens.

Conclusion: Preemptive complexity estimation enables both higher accuracy and significant efficiency gains in multi-step reasoning tasks, demonstrating that adaptive decomposition based on predicted complexity outperforms fixed strategies.

Abstract: Large language models often fail on multi-step reasoning due to fixed reasoning strategies that ignore problem specific difficulty. We introduce CARD (Complexity Agnostic Recursive Decomposition), a framework that predicts problem complexity before generation and adapts decomposition accordingly. Our system comprises MRCE (Multi-dimensional Reasoning Complexity Estimator), a 0.6B Qwen model predicting 30 fine-grained features from question text and a two-stage recursive solver: (1) hierarchical decomposition into K steps based on task profile and (2) per-step thought budget allocation (1, 5-9, or 10 thoughts) via recursive MRCE profiling. Evaluated on three reasoning models (Qwen3-0.6B, DeepSeek-R1-Distill-Qwen-1.5B, Qwen3-1.7B), CARD achieves 81.4% to 89.2% accuracy on GSM8K while reducing token cost by 1.88x to 2.40x compared to fixed decomposition baselines. On MATH-500, CARD reaches 75.1 to 86.8% accuracy using 1.71x to 5.74x fewer tokens. Our results demonstrate that preemptive complexity estimation enables both higher accuracy and significant efficiency gains.

</details>


### [14] [Qwerty AI: Explainable Automated Age Rating and Content Safety Assessment for Russian-Language Screenplays](https://arxiv.org/abs/2601.04211)
*Nikita Zmanovskii*

Main category: cs.CL

TL;DR: Qwerty AI is an automated system for age-rating Russian screenplays according to Russian law, using fine-tuned Phi-3-mini model to detect content violations and assign age ratings with explanations.


<details>
  <summary>Details</summary>
Motivation: The system addresses the need for automated content-safety assessment of Russian-language screenplays according to Federal Law No. 436-FZ, solving real editorial challenges in the Russian media industry identified during a hackathon.

Method: The system processes full-length scripts, segments them into narrative units, detects content violations across five categories (violence, sexual content, profanity, substances, frightening elements), and uses a fine-tuned Phi-3-mini model with 4-bit quantization. It operates under strict constraints: no external API calls, 80GB VRAM limit, and <5 minute processing time.

Result: Achieves 80% rating accuracy and 80-95% segmentation precision (format-dependent). Processes up to 700 pages in under 2 minutes. Successfully deployed on Yandex Cloud with CUDA acceleration.

Conclusion: Qwerty AI demonstrates practical applicability for production workflows in the Russian media industry, providing an efficient automated solution for content-safety assessment that meets legal requirements and operational constraints.

Abstract: We present Qwerty AI, an end-to-end system for automated age-rating and content-safety assessment of Russian-language screenplays according to Federal Law No. 436-FZ. The system processes full-length scripts (up to 700 pages in under 2 minutes), segments them into narrative units, detects content violations across five categories (violence, sexual content, profanity, substances, frightening elements), and assigns age ratings (0+, 6+, 12+, 16+, 18+) with explainable justifications. Our implementation leverages a fine-tuned Phi-3-mini model with 4-bit quantization, achieving 80% rating accuracy and 80-95% segmentation precision (format-dependent). The system was developed under strict constraints: no external API calls, 80GB VRAM limit, and <5 minute processing time for average scripts. Deployed on Yandex Cloud with CUDA acceleration, Qwerty AI demonstrates practical applicability for production workflows. We achieved these results during the Wink hackathon (November 2025), where our solution addressed real editorial challenges in the Russian media industry.

</details>


### [15] [TrueBrief: Faithful Summarization through Small Language Models](https://arxiv.org/abs/2601.04212)
*Kumud Lakara,Ruibo Shi,Fran Silavong*

Main category: cs.CL

TL;DR: TrueBrief is a framework that uses preference optimization to reduce hallucinations in small LLMs for text summarization, employing controlled hallucination injection to create synthetic training data.


<details>
  <summary>Details</summary>
Motivation: LLMs often produce hallucinations (factually incorrect content), which is problematic for security-critical applications where factual accuracy is essential. Small LLMs need improvement in faithfulness for reliable deployment.

Method: An end-to-end framework with a data generation module that injects controlled hallucinations to create synthetic preference data, then uses preference optimization to train small LLMs for more faithful text summarization.

Result: The framework provides insights into how data quality and model size affect preference-based optimization effectiveness, showing conditions where these methods work best for improving faithfulness.

Conclusion: TrueBrief offers a practical approach to enhance small LLM faithfulness for text summarization through synthetic preference data generation and optimization, addressing hallucination challenges in security-critical domains.

Abstract: Large language models (LLMs) have exhibited remarkable proficiency in generating high-quality text; however, their propensity for producing hallucinations poses a significant challenge for their deployment in security-critical domains. In this work, we present TrueBrief, an end-to-end framework specifically designed to enhance the faithfulness of small LLMs (SLMs) primarily for the task of text summarization through a preference-optimization paradigm. Central to our framework is a data generation module that facilitates controlled hallucination injection to generate synthetic preference data. Our work provides insights into the impact of data quality and model size on preference-based optimization, highlighting the conditions under which these methods are most effective.

</details>


### [16] [AnimatedLLM: Explaining LLMs with Interactive Visualizations](https://arxiv.org/abs/2601.04213)
*Zdeněk Kasner,Ondřej Dušek*

Main category: cs.CL

TL;DR: AnimatedLLM is an interactive web app that visualizes Transformer LLM mechanics step-by-step in the browser for educational purposes.


<details>
  <summary>Details</summary>
Motivation: LLMs are central to NLP education but there's a lack of materials showing their internal mechanics and how they work.

Method: Developed an interactive web application that runs entirely in the browser, using pre-computed traces of open LLMs applied on manually curated inputs to provide step-by-step visualizations.

Result: Created AnimatedLLM, available at https://animatedllm.github.io, which serves as both a teaching aid and self-educational tool for understanding Transformer language models.

Conclusion: AnimatedLLM addresses the educational gap by providing accessible, interactive visualizations of LLM mechanics, making complex Transformer architectures more understandable for learners.

Abstract: Large language models (LLMs) are becoming central to natural language processing education, yet materials showing their mechanics are sparse. We present AnimatedLLM, an interactive web application that provides step-by-step visualizations of a Transformer language model. AnimatedLLM runs entirely in the browser, using pre-computed traces of open LLMs applied on manually curated inputs. The application is available at https://animatedllm.github.io, both as a teaching aid and for self-educational purposes.

</details>


### [17] [From Domains to Instances: Dual-Granularity Data Synthesis for LLM Unlearning](https://arxiv.org/abs/2601.04278)
*Xiaoyu Xu,Minxin Du,Zitong Li,Zi Liang,Zhibiao Guo,Shiyu Zhang,Peizhao Hu,Qingqing Ye,Haibo Hu*

Main category: cs.CL

TL;DR: BiForget is an automated framework for synthesizing high-quality forget sets to evaluate LLM unlearning, using the target model itself to generate data that matches its internal knowledge distribution through seed-guided and adversarial prompting.


<details>
  <summary>Details</summary>
Motivation: Current machine unlearning benchmarks often fail to faithfully represent the true "forgetting scope" learned by LLMs, making it difficult to properly evaluate unlearning methods for removing private, harmful, or copyrighted content.

Method: BiForget formalizes two unlearning granularities (domain-level and instance-level) and exploits the target model itself to generate forget sets through seed-guided and adversarial prompting, ensuring data matches the model's internal knowledge distribution.

Result: BiForget achieves superior balance of relevance, diversity, and efficiency. In Harry Potter domain, it improves relevance by ~20 and diversity by ~0.05 while halving total data size compared to state-of-the-art methods.

Conclusion: BiForget provides a more rigorous foundation for evaluating LLM unlearning by facilitating more robust forgetting and better utility preservation through high-quality forget set synthesis.

Abstract: Although machine unlearning is essential for removing private, harmful, or copyrighted content from LLMs, current benchmarks often fail to faithfully represent the true "forgetting scope" learned by the model. We formalize two distinct unlearning granularities, domain-level and instance-level, and propose BiForget, an automated framework for synthesizing high-quality forget sets. Unlike prior work relying on external generators, BiForget exploits the target model per se to elicit data that matches its internal knowledge distribution through seed-guided and adversarial prompting. Our experiments across diverse benchmarks show that it achieves a superior balance of relevance, diversity, and efficiency. Quantitatively, in the Harry Potter domain, it improves relevance by ${\sim}20$ and diversity by ${\sim}$0.05 while halving the total data size compared to SOTAs. Ultimately, it facilitates more robust forgetting and better utility preservation, providing a more rigorous foundation for evaluating LLM unlearning.

</details>


### [18] [RIGOURATE: Quantifying Scientific Exaggeration with Evidence-Aligned Claim Evaluation](https://arxiv.org/abs/2601.04350)
*Joseph James,Chenghao Xiao,Yucheng Li,Nafise Sadat Moosavi,Chenghua Lin*

Main category: cs.CL

TL;DR: RIGOURATE is a multimodal framework that retrieves evidence from paper bodies and scores claim overstatement to improve scientific rigor.


<details>
  <summary>Details</summary>
Motivation: Scientific papers often overstate claims beyond what their results support, sidelining rigor in favor of bold statements. This undermines transparent scientific communication.

Method: Two-stage framework: 1) Fine-tuned reranker for evidence retrieval from paper bodies, 2) Fine-tuned model to predict overstatement scores with justification. Built on dataset of 10K+ claim-evidence sets from ICLR/NeurIPS papers, annotated by 8 LLMs, calibrated with peer-review comments.

Result: Outperforms strong baselines in evidence retrieval and overstatement detection. Human evaluation validates the approach.

Conclusion: RIGOURATE operationalizes evidential proportionality and supports clearer, more transparent scientific communication by identifying overstated claims.

Abstract: Scientific rigour tends to be sidelined in favour of bold statements, leading authors to overstate claims beyond what their results support. We present RIGOURATE, a two-stage multimodal framework that retrieves supporting evidence from a paper's body and assigns each claim an overstatement score. The framework consists of a dataset of over 10K claim-evidence sets from ICLR and NeurIPS papers, annotated using eight LLMs, with overstatement scores calibrated using peer-review comments and validated through human evaluation. It employes a fine-tuned reranker for evidence retrieval and a fine-tuned model to predict overstatement scores with justification. Compared to strong baselines, RIGOURATE enables improved evidence retrieval and overstatement detection. Overall, our work operationalises evidential proportionality and supports clearer, more transparent scientific communication.

</details>


### [19] [Dialect Matters: Cross-Lingual ASR Transfer for Low-Resource Indic Language Varieties](https://arxiv.org/abs/2601.04373)
*Akriti Dhasmana,Aarohi Srivastava,David Chiang*

Main category: cs.CL

TL;DR: Cross-lingual ASR transfer study on Indic dialects shows phylogenetic distance alone doesn't fully explain performance; fine-tuning on small dialect data can match larger high-resource language data.


<details>
  <summary>Details</summary>
Motivation: To understand cross-lingual transfer in ASR systems for spontaneous, noisy, code-mixed speech across diverse Indic dialects and language varieties, particularly examining how phylogenetic relationships affect performance and addressing challenges with dialectal/non-standardized speech.

Method: Empirical study of cross-lingual transfer using spontaneous, noisy, code-mixed speech across wide range of Indic dialects; includes case study on low-resource Garhwali language; evaluates multiple contemporary ASR models; analyzes transcription errors to examine bias toward pre-training languages.

Result: ASR performance generally improves with reduced phylogenetic distance between languages, but this factor alone doesn't fully explain dialectal performance; fine-tuning on smaller dialectal data yields comparable performance to fine-tuning on larger amounts of phylogenetically-related high-resource languages.

Conclusion: Phylogenetic distance is insufficient to explain ASR performance in dialectal settings; targeted fine-tuning on dialectal data can be highly effective; transcription error analysis reveals bias toward pre-training languages, highlighting challenges for ASR systems on non-standardized speech.

Abstract: We conduct an empirical study of cross-lingual transfer using spontaneous, noisy, and code-mixed speech across a wide range of Indic dialects and language varieties. Our results indicate that although ASR performance is generally improved with reduced phylogenetic distance between languages, this factor alone does not fully explain performance in dialectal settings. Often, fine-tuning on smaller amounts of dialectal data yields performance comparable to fine-tuning on larger amounts of phylogenetically-related, high-resource standardized languages. We also present a case study on Garhwali, a low-resource Pahari language variety, and evaluate multiple contemporary ASR models. Finally, we analyze transcription errors to examine bias toward pre-training languages, providing additional insight into challenges faced by ASR systems on dialectal and non-standardized speech.

</details>


### [20] [Disco-RAG: Discourse-Aware Retrieval-Augmented Generation](https://arxiv.org/abs/2601.04377)
*Dongqi Liu,Hang Ding,Qiming Feng,Jian Li,Xurong Xie,Zhucun Xue,Chengjie Wang,Jiangning Zhang,Yabiao Wang*

Main category: cs.CL

TL;DR: Disco-RAG: A discourse-aware RAG framework that injects discourse structure into generation via intra-chunk trees and inter-chunk rhetorical graphs, achieving SOTA results without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Existing RAG strategies treat retrieved passages in a flat, unstructured way, which prevents capturing structural cues and constrains synthesis of knowledge from dispersed evidence across documents.

Method: Constructs intra-chunk discourse trees to capture local hierarchies and builds inter-chunk rhetorical graphs to model cross-passage coherence, jointly integrated into a planning blueprint that conditions generation.

Result: Achieves state-of-the-art results on question answering and long-document summarization benchmarks without fine-tuning.

Conclusion: Discourse structure plays an important role in advancing RAG systems, and explicit injection of discourse signals significantly enhances performance in knowledge-intensive tasks.

Abstract: Retrieval-Augmented Generation (RAG) has emerged as an important means of enhancing the performance of large language models (LLMs) in knowledge-intensive tasks. However, most existing RAG strategies treat retrieved passages in a flat and unstructured way, which prevents the model from capturing structural cues and constrains its ability to synthesize knowledge from dispersed evidence across documents. To overcome these limitations, we propose Disco-RAG, a discourse-aware framework that explicitly injects discourse signals into the generation process. Our method constructs intra-chunk discourse trees to capture local hierarchies and builds inter-chunk rhetorical graphs to model cross-passage coherence. These structures are jointly integrated into a planning blueprint that conditions the generation. Experiments on question answering and long-document summarization benchmarks show the efficacy of our approach. Disco-RAG achieves state-of-the-art results on the benchmarks without fine-tuning. These findings underscore the important role of discourse structure in advancing RAG systems.

</details>


### [21] [MiJaBench: Revealing Minority Biases in Large Language Models via Hate Speech Jailbreaking](https://arxiv.org/abs/2601.04389)
*Iago Alves Brito,Walcy Santos Rezende Rios,Julia Soares Dollis,Diogo Fernandes Costa Silva,Arlindo Rodrigues Galvão Filho*

Main category: cs.CL

TL;DR: Current LLM safety evaluations create a false sense of universal safety by aggregating scores that hide vulnerabilities against specific minority groups. The paper introduces MiJaBench, a bilingual adversarial benchmark revealing that safety alignment is not universal but forms a demographic hierarchy, with disparities worsening with model scaling.


<details>
  <summary>Details</summary>
Motivation: The motivation is to expose the selective safety of current LLM safety evaluations, which aggregate "Identity Hate" into scalar scores that mask systemic vulnerabilities against specific minority populations. Current approaches create a dangerous illusion of universality that fails to protect all groups equally.

Method: The authors introduce MiJaBench, a bilingual (English and Portuguese) adversarial benchmark comprising 44,000 prompts across 16 minority groups. They generate 528,000 prompt-response pairs from 12 state-of-the-art LLMs and curate MiJaBench-Align to analyze safety alignment patterns.

Result: Results show safety alignment is not a generalized semantic capability but a demographic hierarchy: defense rates fluctuate by up to 33% within the same model based solely on the target group. Model scaling exacerbates these disparities, suggesting current alignment techniques reinforce memorized refusal boundaries only for specific groups rather than creating principles of non-discrimination.

Conclusion: Current alignment techniques do not create principles of non-discrimination but reinforce memorized refusal boundaries only for specific groups, challenging current scaling laws of security. The authors release all datasets and scripts to encourage research into granular demographic alignment.

Abstract: Current safety evaluations of large language models (LLMs) create a dangerous illusion of universality, aggregating "Identity Hate" into scalar scores that mask systemic vulnerabilities against specific populations. To expose this selective safety, we introduce MiJaBench, a bilingual (English and Portuguese) adversarial benchmark comprising 44,000 prompts across 16 minority groups. By generating 528,000 prompt-response pairs from 12 state-of-the-art LLMs, we curate MiJaBench-Align, revealing that safety alignment is not a generalized semantic capability but a demographic hierarchy: defense rates fluctuate by up to 33\% within the same model solely based on the target group. Crucially, we demonstrate that model scaling exacerbates these disparities, suggesting that current alignment techniques do not create principle of non-discrimination but reinforces memorized refusal boundaries only for specific groups, challenging the current scaling laws of security. We release all datasets and scripts to encourage research into granular demographic alignment at GitHub.

</details>


### [22] [ARREST: Adversarial Resilient Regulation Enhancing Safety and Truth in Large Language Models](https://arxiv.org/abs/2601.04394)
*Sharanya Dasgupta,Arkaprabha Basu,Sujoy Nath,Swagatam Das*

Main category: cs.CL

TL;DR: ARREST is a unified framework that regulates factual and safety misalignments in LLMs by identifying and correcting drifted features in latent activation space without fine-tuning model parameters.


<details>
  <summary>Details</summary>
Motivation: LLMs lack human cognition's ability to balance factuality and safety, leading to hallucinations and unsafe outputs. The authors argue that both factual and safety failures stem from representational misalignment in latent activation space rather than being separate alignment issues.

Method: ARREST uses an external network trained to understand fluctuations in activation space to selectively intervene and regulate falsehood into truthfulness and unsafe output into safe output. It employs both soft and hard refusals in addition to factual corrections, with adversarial training for robustness.

Result: Empirical results show ARREST effectively regulates misalignment and is more versatile than RLHF-aligned models in generating soft refusals due to adversarial training.

Conclusion: The proposed approach addresses factual and safety failures as unified representational misalignment issues, offering a parameter-efficient intervention method that mimics human cognitive self-correction mechanisms.

Abstract: Human cognition, driven by complex neurochemical processes, oscillates between imagination and reality and learns to self-correct whenever such subtle drifts lead to hallucinations or unsafe associations. In recent years, LLMs have demonstrated remarkable performance in a wide range of tasks. However, they still lack human cognition to balance factuality and safety. Bearing the resemblance, we argue that both factual and safety failures in LLMs arise from a representational misalignment in their latent activation space, rather than addressing those as entirely separate alignment issues. We hypothesize that an external network, trained to understand the fluctuations, can selectively intervene in the model to regulate falsehood into truthfulness and unsafe output into safe output without fine-tuning the model parameters themselves. Reflecting the hypothesis, we propose ARREST (Adversarial Resilient Regulation Enhancing Safety and Truth), a unified framework that identifies and corrects drifted features, engaging both soft and hard refusals in addition to factual corrections. Our empirical results show that ARREST not only regulates misalignment but is also more versatile compared to the RLHF-aligned models in generating soft refusals due to adversarial training. We make our codebase available at https://github.com/sharanya-dasgupta001/ARREST.

</details>


### [23] [Interpreting Transformers Through Attention Head Intervention](https://arxiv.org/abs/2601.04398)
*Mason Kadem,Rong Zheng*

Main category: cs.CL

TL;DR: The paper argues for mechanistic interpretability of neural networks to understand their decision-making processes, enabling accountability, studying digital cognition, and discovering new knowledge from AI systems that outperform humans.


<details>
  <summary>Details</summary>
Motivation: Neural networks are becoming increasingly capable but remain poorly understood. Understanding their internal decision-making processes (mechanistic interpretability) is crucial for accountability in high-stakes applications, studying the emergence of cognition in digital brains, and extracting new knowledge from AI systems that surpass human capabilities.

Method: The abstract describes a conceptual framework rather than a specific technical method. It advocates for mechanistic interpretability approaches to reverse-engineer neural networks and understand their internal computational mechanisms.

Result: The paper presents a vision statement rather than empirical results. It outlines three key benefits of achieving mechanistic interpretability: enabling accountability and control in critical applications, facilitating the study of digital cognition, and allowing discovery of new knowledge from superhuman AI systems.

Conclusion: Mechanistic interpretability is essential for understanding neural networks' decision-making processes, with significant implications for AI safety, cognitive science, and knowledge discovery from advanced AI systems.

Abstract: Neural networks are growing more capable on their own, but we do not understand their neural mechanisms. Understanding these mechanisms' decision-making processes, or mechanistic interpretability, enables (1) accountability and control in high-stakes domains, (2) the study of digital brains and the emergence of cognition, and (3) discovery of new knowledge when AI systems outperform humans.

</details>


### [24] [Gavel: Agent Meets Checklist for Evaluating LLMs on Long-Context Legal Summarization](https://arxiv.org/abs/2601.04424)
*Yao Dou,Wei Xu*

Main category: cs.CL

TL;DR: The paper introduces Gavel-Ref, a reference-based evaluation framework for multi-document legal case summarization, and Gavel-Agent, an efficient agent scaffold that helps LLMs extract checklists directly from long legal documents with reduced token usage.


<details>
  <summary>Details</summary>
Motivation: Despite LLMs supporting contexts up to 1M tokens, their effectiveness on complex long-context tasks like multi-document legal case summarization (100K-500K tokens) remains unclear. Current evaluations provide only single aggregate scores, lacking systematic analysis of model performance on specific aspects of legal summarization.

Method: 1) Introduced Gavel-Ref: a reference-based evaluation framework with multi-value checklist evaluation over 26 items, plus residual fact and writing-style evaluations. 2) Systematically evaluated 12 frontier LLMs on 100 legal cases (32K-512K tokens). 3) Developed Gavel-Agent: an efficient autonomous agent scaffold with six tools to help LLMs navigate and extract checklists directly from case documents.

Result: Even the strongest model (Gemini 2.5 Pro) achieved only around 50% on SGavel-Ref, showing task difficulty. Models performed well on simple checklist items but struggled on multi-value or rare items like settlements and monitor reports. Gavel-Agent with Qwen3 reduced token usage by 36% while resulting in only 7% drop in Schecklist compared to end-to-end extraction with GPT-4.1.

Conclusion: Current LLMs struggle with complex long-context legal summarization despite large context windows. The Gavel framework provides more granular evaluation, and Gavel-Agent offers an efficient approach to help LLMs navigate long documents. As LLMs may surpass human-written summaries, autonomous evaluation approaches like Gavel-Agent become increasingly important.

Abstract: Large language models (LLMs) now support contexts of up to 1M tokens, but their effectiveness on complex long-context tasks remains unclear. In this paper, we study multi-document legal case summarization, where a single case often spans many documents totaling 100K-500K tokens. We introduce Gavel-Ref, a reference-based evaluation framework with multi-value checklist evaluation over 26 items, as well as residual fact and writing-style evaluations. Using Gavel-Ref, we go beyond the single aggregate scores reported in prior work and systematically evaluate 12 frontier LLMs on 100 legal cases ranging from 32K to 512K tokens, primarily from 2025. Our results show that even the strongest model, Gemini 2.5 Pro, achieves only around 50 of $S_{\text{Gavel-Ref}}$, highlighting the difficulty of the task. Models perform well on simple checklist items (e.g., filing date) but struggle on multi-value or rare ones such as settlements and monitor reports. As LLMs continue to improve and may surpass human-written summaries -- making human references less reliable -- we develop Gavel-Agent, an efficient and autonomous agent scaffold that equips LLMs with six tools to navigate and extract checklists directly from case documents. With Qwen3, Gavel-Agent reduces token usage by 36% while resulting in only a 7% drop in $S_{\text{checklist}}$ compared to end-to-end extraction with GPT-4.1.

</details>


### [25] [Accommodation and Epistemic Vigilance: A Pragmatic Account of Why LLMs Fail to Challenge Harmful Beliefs](https://arxiv.org/abs/2601.04435)
*Myra Cheng,Robert D. Hawkins,Dan Jurafsky*

Main category: cs.CL

TL;DR: LLMs often fail to challenge harmful user beliefs due to excessive accommodation and insufficient epistemic vigilance, but pragmatic interventions like "wait a minute" can significantly improve safety performance.


<details>
  <summary>Details</summary>
Motivation: LLMs frequently fail to challenge users' harmful beliefs in domains like medical advice and social reasoning, which poses safety concerns. The paper argues these failures stem from LLMs defaulting to accommodating user assumptions and lacking epistemic vigilance.

Method: The study examines how social and linguistic factors (at-issueness, linguistic encoding, source reliability) affect accommodation in LLMs across three safety benchmarks: Cancer-Myth, SAGE-Eval (misinformation), and ELEPHANT (sycophancy). They test simple pragmatic interventions like adding "wait a minute" to prompts.

Result: Social and linguistic factors known to influence accommodation in humans similarly affect LLMs. Pragmatic interventions like "wait a minute" significantly improve performance on safety benchmarks while maintaining low false-positive rates.

Conclusion: Pragmatic considerations are crucial for evaluating LLM behavior and improving safety. Simple interventions can help LLMs better challenge harmful beliefs without compromising other performance aspects.

Abstract: Large language models (LLMs) frequently fail to challenge users' harmful beliefs in domains ranging from medical advice to social reasoning. We argue that these failures can be understood and addressed pragmatically as consequences of LLMs defaulting to accommodating users' assumptions and exhibiting insufficient epistemic vigilance. We show that social and linguistic factors known to influence accommodation in humans (at-issueness, linguistic encoding, and source reliability) similarly affect accommodation in LLMs, explaining performance differences across three safety benchmarks that test models' ability to challenge harmful beliefs, spanning misinformation (Cancer-Myth, SAGE-Eval) and sycophancy (ELEPHANT). We further show that simple pragmatic interventions, such as adding the phrase "wait a minute", significantly improve performance on these benchmarks while preserving low false-positive rates. Our results highlight the importance of considering pragmatics for evaluating LLM behavior and improving LLM safety.

</details>


### [26] [Learning to Simulate Human Dialogue](https://arxiv.org/abs/2601.04436)
*Kanishk Gandhi,Agam Bhatia,Noah D. Goodman*

Main category: cs.CL

TL;DR: Optimizing for LLM-judge rewards decreases human-likeness in dialogue prediction, while maximizing log-probability of real human responses improves performance, especially with chain-of-thought as latent variable.


<details>
  <summary>Details</summary>
Motivation: To understand how thinking affects dialogue prediction and whether optimizing for LLM-judge rewards (semantic similarity, information completeness) or directly maximizing human response likelihood produces more human-like conversational models.

Method: Compare learning approaches along two dimensions: (1) thinking before responding (chain-of-thought), (2) reward types (LLM-as-a-judge scoring vs. maximizing log-probability of ground truth). Treat chain-of-thought as latent variable and derive lower bound on log-probability for optimization.

Result: Optimizing for judge-based rewards increases judge scores but decreases ground truth likelihood and human-judge win rates. Maximizing log-probability of human responses improves both log-probability and win rates. Chain-of-thought as latent variable with derived lower bound optimization yields best results across all evaluations.

Conclusion: Thinking helps primarily when trained with distribution-matching objectives grounded in real human dialogue. Scaling this approach to broader conversational data may produce models with more nuanced understanding of human behavior.

Abstract: To predict what someone will say is to model how they think. We study this through next-turn dialogue prediction: given a conversation, predict the next utterance produced by a person. We compare learning approaches along two dimensions: (1) whether the model is allowed to think before responding, and (2) how learning is rewarded either through an LLM-as-a-judge that scores semantic similarity and information completeness relative to the ground-truth response, or by directly maximizing the log-probability of the true human dialogue. We find that optimizing for judge-based rewards indeed increases judge scores throughout training, however it decreases the likelihood assigned to ground truth human responses and decreases the win rate when human judges choose the most human-like response among a real and synthetic option. This failure is amplified when the model is allowed to think before answering. In contrast, by directly maximizing the log-probability of observed human responses, the model learns to better predict what people actually say, improving on both log-probability and win rate evaluations. Treating chain-of-thought as a latent variable, we derive a lower bound on the log-probability. Optimizing this objective yields the best results on all our evaluations. These results suggest that thinking helps primarily when trained with a distribution-matching objective grounded in real human dialogue, and that scaling this approach to broader conversational data may produce models with a more nuanced understanding of human behavior.

</details>


### [27] [Merging Triggers, Breaking Backdoors: Defensive Poisoning for Instruction-Tuned Language Models](https://arxiv.org/abs/2601.04448)
*San Kim,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: MB-Defense: A two-stage training pipeline that immunizes instruction-tuned LLMs against backdoor attacks by merging attacker and defensive triggers, then breaking the unified backdoor representation.


<details>
  <summary>Details</summary>
Motivation: Instruction-tuned LLMs are vulnerable to backdoor attacks through poisoned training data, but existing defenses for these models are underexplored despite the growing security risk.

Method: Two-stage framework: (1) defensive poisoning merges attacker and defensive triggers into unified backdoor representation, (2) weight recovery breaks this representation through additional training to restore clean behavior.

Result: MB-Defense substantially lowers attack success rates while preserving instruction-following ability across multiple LLMs, offering generalizable and data-efficient defense against unseen backdoor attacks.

Conclusion: The proposed method provides an effective defense strategy that improves robustness of instruction-tuned LLMs against diverse backdoor threats through a novel merging and breaking approach.

Abstract: Large Language Models (LLMs) have greatly advanced Natural Language Processing (NLP), particularly through instruction tuning, which enables broad task generalization without additional fine-tuning. However, their reliance on large-scale datasets-often collected from human or web sources-makes them vulnerable to backdoor attacks, where adversaries poison a small subset of data to implant hidden behaviors. Despite this growing risk, defenses for instruction-tuned models remain underexplored. We propose MB-Defense (Merging & Breaking Defense Framework), a novel training pipeline that immunizes instruction-tuned LLMs against diverse backdoor threats. MB-Defense comprises two stages: (i) defensive poisoning, which merges attacker and defensive triggers into a unified backdoor representation, and (ii) weight recovery, which breaks this representation through additional training to restore clean behavior. Extensive experiments across multiple LLMs show that MB-Defense substantially lowers attack success rates while preserving instruction-following ability. Our method offers a generalizable and data-efficient defense strategy, improving the robustness of instruction-tuned LLMs against unseen backdoor attacks.

</details>


### [28] [Users Mispredict Their Own Preferences for AI Writing Assistance](https://arxiv.org/abs/2601.04461)
*Vivian Lai,Zana Buçinca,Nil-Jana Akpinar,Mo Houtti,Hyeonsu B. Kang,Kevin Chian,Namjoon Suh,Alex C. Williams*

Main category: cs.CL

TL;DR: Users' stated preferences for AI writing assistance don't match their actual behavior - they say urgency matters most but their choices are driven by compositional effort, creating a perception-behavior gap that misleads system design.


<details>
  <summary>Details</summary>
Motivation: Proactive AI writing assistants need to predict when users want help, but there's a lack of empirical understanding about what drives user preferences for drafting assistance.

Method: Factorial vignette study with 50 participants making 750 pairwise comparisons to analyze drivers of user preferences for AI writing assistance.

Result: Compositional effort dominates decisions (ρ=0.597) while urgency shows no predictive power. Users exhibit a perception-behavior gap: they rank urgency first in self-reports despite it being the weakest behavioral driver. Systems designed from stated preferences achieve only 57.7% accuracy vs 61.3% for behavior-based systems.

Conclusion: Relying on user introspection for system design actively misleads optimization for proactive NLG systems, highlighting the importance of behavioral data over self-reported preferences.

Abstract: Proactive AI writing assistants need to predict when users want drafting help, yet we lack empirical understanding of what drives preferences. Through a factorial vignette study with 50 participants making 750 pairwise comparisons, we find compositional effort dominates decisions ($ρ= 0.597$) while urgency shows no predictive power ($ρ\approx 0$). More critically, users exhibit a striking perception-behavior gap: they rank urgency first in self-reports despite it being the weakest behavioral driver, representing a complete preference inversion. This misalignment has measurable consequences. Systems designed from users' stated preferences achieve only 57.7\% accuracy, underperforming even naive baselines, while systems using behavioral patterns reach significantly higher 61.3\% ($p < 0.05$). These findings demonstrate that relying on user introspection for system design actively misleads optimization, with direct implications for proactive natural language generation (NLG) systems.

</details>


### [29] [Beyond Static Summarization: Proactive Memory Extraction for LLM Agents](https://arxiv.org/abs/2601.04463)
*Chengyuan Yang,Zequn Sun,Wei Wei,Wei Hu*

Main category: cs.CL

TL;DR: ProMem introduces proactive memory extraction with recurrent feedback loops to address limitations of static summarization in LLM agents, improving memory completeness and QA accuracy while optimizing token cost.


<details>
  <summary>Details</summary>
Motivation: Existing summary-based memory extraction methods have two major limitations: 1) summarization is "ahead-of-time" (blind feed-forward process that misses important details without knowing future tasks), and 2) extraction is "one-off" (lacks feedback loop to verify facts, leading to accumulation of information loss).

Method: ProMem treats memory extraction as an iterative cognitive process with recurrent feedback loops. The agent uses self-questioning to actively probe dialogue history, allowing recovery of missing information and error correction.

Result: ProMem significantly improves completeness of extracted memory and QA accuracy. It achieves superior trade-off between extraction quality and token cost compared to existing methods.

Conclusion: Proactive memory extraction with recurrent feedback loops addresses fundamental limitations of static summarization approaches, providing a more effective memory management solution for LLM agents in long-term interaction and personalization scenarios.

Abstract: Memory management is vital for LLM agents to handle long-term interaction and personalization. Most research focuses on how to organize and use memory summary, but often overlooks the initial memory extraction stage. In this paper, we argue that existing summary-based methods have two major limitations based on the recurrent processing theory. First, summarization is "ahead-of-time", acting as a blind "feed-forward" process that misses important details because it doesn't know future tasks. Second, extraction is usually "one-off", lacking a feedback loop to verify facts, which leads to the accumulation of information loss. To address these issues, we propose proactive memory extraction (namely ProMem). Unlike static summarization, ProMem treats extraction as an iterative cognitive process. We introduce a recurrent feedback loop where the agent uses self-questioning to actively probe the dialogue history. This mechanism allows the agent to recover missing information and correct errors. Our ProMem significantly improves the completeness of the extracted memory and QA accuracy. It also achieves a superior trade-off between extraction quality and token cost.

</details>


### [30] [Concept Tokens: Learning Behavioral Embeddings Through Concept Definitions](https://arxiv.org/abs/2601.04465)
*Ignacio Sastre,Aiala Rosá*

Main category: cs.CL

TL;DR: Concept Tokens is a lightweight method that adds special tokens to frozen LLMs, learning their embeddings from concept definitions to steer model behavior without retraining.


<details>
  <summary>Details</summary>
Motivation: To create a compact control mechanism for frozen LLMs that can steer model behavior using concept definitions, avoiding the need for full model retraining or fine-tuning.

Method: Add new special tokens to pretrained LLMs, learn only their embeddings from multiple natural language definitions of target concepts (with concept occurrences replaced by the new token), while keeping the LLM frozen and optimizing embeddings with standard language-modeling objective.

Result: 1) Reduces hallucinations in closed-book QA (negating token increases abstentions, asserting increases hallucinations); 2) Induces recasting pedagogical strategy with similar directional effects; 3) Better preserves compliance with other instructions compared to in-context definitions; 4) Qualitative study shows what information embeddings capture and their limitations.

Conclusion: Concept Tokens provide an effective, compact control signal learned from definitions that can steer behavior in frozen LLMs, offering a lightweight alternative to full model adaptation.

Abstract: We propose Concept Tokens, a lightweight method that adds a new special token to a pretrained LLM and learns only its embedding from multiple natural language definitions of a target concept, where occurrences of the concept are replaced by the new token. The LLM is kept frozen and the embedding is optimized with the standard language-modeling objective. We evaluate Concept Tokens in three settings. First, we study hallucinations in closed-book question answering on HotpotQA and find a directional effect: negating the hallucination token reduces hallucinated answers mainly by increasing abstentions, whereas asserting it increases hallucinations and lowers precision. Second, we induce recasting, a pedagogical feedback strategy for second language teaching, and observe the same directional effect. Moreover, compared to providing the full definitional corpus in-context, concept tokens better preserve compliance with other instructions (e.g., asking follow-up questions). Finally, we include a qualitative study with the Eiffel Tower and a fictional "Austral Tower" to illustrate what information the learned embeddings capture and where their limitations emerge. Overall, Concept Tokens provide a compact control signal learned from definitions that can steer behavior in frozen LLMs.

</details>


### [31] [SampoNLP: A Self-Referential Toolkit for Morphological Analysis of Subword Tokenizers](https://arxiv.org/abs/2601.04469)
*Iaroslav Chelombitko,Ekaterina Chelombitko,Aleksey Komissarov*

Main category: cs.CL

TL;DR: SampoNLP toolkit creates morphological lexicons for Uralic languages using MDL-inspired scoring, enabling systematic evaluation of BPE tokenizers and identification of optimal vocabulary sizes via Integrated Performance Score.


<details>
  <summary>Details</summary>
Motivation: Evaluating tokenizers for morphologically rich Uralic languages is difficult due to lack of clean morpheme lexicons, which hampers understanding of BPE tokenizer performance for these languages.

Method: Developed SampoNLP toolkit using MDL-inspired Self-Referential Atomicity Scoring to create high-purity morphological lexicons from corpus-free approach, then systematically evaluated BPE tokenizers across vocabulary sizes (8k-256k) using proposed Integrated Performance Score (IPS) metric.

Result: Generated high-purity lexicons for Finnish, Hungarian, and Estonian; identified "elbow points" of diminishing returns in IPS curves; provided first empirically grounded recommendations for optimal vocabulary sizes; quantitatively demonstrated limitations of standard BPE for highly agglutinative languages.

Conclusion: SampoNLP enables practical tokenizer evaluation for low-resource Uralic languages, reveals BPE limitations for agglutinative languages, and provides empirically-based vocabulary size recommendations, with all resources publicly available.

Abstract: The quality of subword tokenization is critical for Large Language Models, yet evaluating tokenizers for morphologically rich Uralic languages is hampered by the lack of clean morpheme lexicons.
  We introduce SampoNLP, a corpus-free toolkit for morphological lexicon creation using MDL-inspired Self-Referential Atomicity Scoring, which filters composite forms through internal structural cues - suited for low-resource settings.
  Using the high-purity lexicons generated by SampoNLP for Finnish, Hungarian, and Estonian, we conduct a systematic evaluation of BPE tokenizers across a range of vocabulary sizes (8k-256k). We propose a unified metric, the Integrated Performance Score (IPS), to navigate the trade-off between morpheme coverage and over-splitting. By analyzing the IPS curves, we identify the "elbow points" of diminishing returns and provide the first empirically grounded recommendations for optimal vocabulary sizes (k) in these languages. Our study not only offers practical guidance but also quantitatively demonstrates the limitations of standard BPE for highly agglutinative languages. The SampoNLP library and all generated resources are made publicly available: https://github.com/AragonerUA/SampoNLP

</details>


### [32] [WESR: Scaling and Evaluating Word-level Event-Speech Recognition](https://arxiv.org/abs/2601.04508)
*Chenchen Yang,Kexin Huang,Liwei Fan,Qian Tu,Botian Jiang,Dong Zhang,Linqi Yin,Shimin Li,Zhaoye Fei,Qinyuan Cheng,Xipeng Qiu*

Main category: cs.CL

TL;DR: WESR-Bench introduces a refined taxonomy of 21 vocal events and a position-aware evaluation protocol for precise localization of non-verbal vocal events in speech, addressing limitations in current methods.


<details>
  <summary>Details</summary>
Motivation: Current methods for non-verbal vocal event detection have insufficient task definitions with limited category coverage and ambiguous temporal granularity, lacking standardized evaluation frameworks that hinder downstream application development.

Method: Developed a refined taxonomy of 21 vocal events with categorization into discrete (standalone) vs continuous (mixed with speech) types. Created WESR-Bench, an expert-annotated evaluation set (900+ utterances) with a novel position-aware protocol that disentangles ASR errors from event detection. Built a 1,700+ hour corpus and trained specialized models.

Result: The specialized models surpass both open-source audio-language models and commercial APIs while preserving ASR quality. The position-aware protocol enables precise localization measurement for both discrete and continuous events.

Conclusion: WESR-Bench serves as a foundational resource for future research in modeling rich, real-world auditory scenes, addressing critical gaps in non-verbal vocal event detection and localization.

Abstract: Speech conveys not only linguistic information but also rich non-verbal vocal events such as laughing and crying. While semantic transcription is well-studied, the precise localization of non-verbal events remains a critical yet under-explored challenge. Current methods suffer from insufficient task definitions with limited category coverage and ambiguous temporal granularity. They also lack standardized evaluation frameworks, hindering the development of downstream applications. To bridge this gap, we first develop a refined taxonomy of 21 vocal events, with a new categorization into discrete (standalone) versus continuous (mixed with speech) types. Based on the refined taxonomy, we introduce WESR-Bench, an expert-annotated evaluation set (900+ utterances) with a novel position-aware protocol that disentangles ASR errors from event detection, enabling precise localization measurement for both discrete and continuous events. We also build a strong baseline by constructing a 1,700+ hour corpus, and train specialized models, surpassing both open-source audio-language models and commercial APIs while preserving ASR quality. We anticipate that WESR will serve as a foundational resource for future research in modeling rich, real-world auditory scenes.

</details>


### [33] [LinguaGame: A Linguistically Grounded Game-Theoretic Paradigm for Multi-Agent Dialogue Generation](https://arxiv.org/abs/2601.04516)
*Yuxiao Ye,Yiming Zhang,Yiran Ma,Huiyuan Xie,Huining Zhu,Zhiyuan Liu*

Main category: cs.CL

TL;DR: LinguaGame: A game-theoretic framework for improving communication efficiency in LLM-based multi-agent systems by modeling dialogue as intentional signaling games.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based multi-agent systems focus on architecture design (role assignment, workflow orchestration) but neglect the interaction process itself. The paper aims to improve agents' communication efficiency by helping them convey intended meaning more effectively through language.

Method: Proposes LinguaGame, a linguistically-grounded game-theoretic paradigm that models dialogue as a signaling game over communicative intents and strategies. Uses a training-free equilibrium approximation algorithm for inference-time decision adjustment. Unlike prior game-theoretic approaches, it relies on linguistically informed reasoning with minimal task-specific coupling.

Result: Evaluated in simulated courtroom proceedings and debates. Human expert assessments show significant gains in communication efficiency compared to baseline approaches.

Conclusion: The framework successfully improves multi-agent communication by treating dialogue as intentional and strategic communication, requiring agents to infer others' intents and strategies. This linguistically-grounded game-theoretic approach offers a novel way to enhance communication efficiency in LLM-based multi-agent systems.

Abstract: Large Language Models (LLMs) have enabled Multi-Agent Systems (MASs) where agents interact through natural language to solve complex tasks or simulate multi-party dialogues. Recent work on LLM-based MASs has mainly focused on architecture design, such as role assignment and workflow orchestration. In contrast, this paper targets the interaction process itself, aiming to improve agents' communication efficiency by helping them convey their intended meaning more effectively through language. To this end, we propose LinguaGame, a linguistically-grounded game-theoretic paradigm for multi-agent dialogue generation. Our approach models dialogue as a signalling game over communicative intents and strategies, solved with a training-free equilibrium approximation algorithm for inference-time decision adjustment. Unlike prior game-theoretic MASs, whose game designs are often tightly coupled with task-specific objectives, our framework relies on linguistically informed reasoning with minimal task-specific coupling. Specifically, it treats dialogue as intentional and strategic communication, requiring agents to infer what others aim to achieve (intents) and how they pursue those goals (strategies). We evaluate our framework in simulated courtroom proceedings and debates, with human expert assessments showing significant gains in communication efficiency.

</details>


### [34] [GRACE: Reinforcement Learning for Grounded Response and Abstention under Contextual Evidence](https://arxiv.org/abs/2601.04525)
*Yibo Zhao,Jiapeng Zhu,Zichen Ding,Xiang Li*

Main category: cs.CL

TL;DR: GRACE is a reinforcement learning framework that improves RAG systems by simultaneously addressing two flaws: providing correct answers without evidence and fabricating responses when context is insufficient, using automated data construction and multi-stage gated rewards.


<details>
  <summary>Details</summary>
Motivation: Current RAG systems suffer from two critical issues: (1) providing correct answers without explicit grounded evidence, and (2) producing fabricated responses when retrieved context is insufficient. While prior work addressed these separately, there's no unified framework integrating evidence-based grounding with reliable abstention.

Method: GRACE uses a reinforcement learning framework with: (1) automated data construction using heterogeneous retrievers to generate diverse training samples without manual annotation, and (2) a multi-stage gated reward function that trains the model to assess evidence sufficiency, extract key supporting evidence, and provide answers or explicitly abstain.

Result: Experiments on two benchmarks show GRACE achieves state-of-the-art overall accuracy and strikes a favorable balance between accurate response and rejection, while requiring only 10% of the annotation costs of prior methods.

Conclusion: GRACE provides an effective unified framework that simultaneously addresses both evidence grounding and reliable abstention in RAG systems, achieving superior performance with significantly reduced annotation costs through automated data construction and reinforcement learning.

Abstract: Retrieval-Augmented Generation (RAG) integrates external knowledge to enhance Large Language Models (LLMs), yet systems remain susceptible to two critical flaws: providing correct answers without explicit grounded evidence and producing fabricated responses when the retrieved context is insufficient. While prior research has addressed these issues independently, a unified framework that integrates evidence-based grounding and reliable abstention is currently lacking. In this paper, we propose GRACE, a reinforcement-learning framework that simultaneously mitigates both types of flaws. GRACE employs a data construction method that utilizes heterogeneous retrievers to generate diverse training samples without manual annotation. A multi-stage gated reward function is then employed to train the model to assess evidence sufficiency, extract key supporting evidence, and provide answers or explicitly abstain. Experimental results on two benchmarks demonstrate that GRACE achieves state-of-the-art overall accuracy and strikes a favorable balance between accurate response and rejection, while requiring only 10% of the annotation costs of prior methods. Our code is available at https://github.com/YiboZhao624/Grace..

</details>


### [35] [BanglaLorica: Design and Evaluation of a Robust Watermarking Algorithm for Large Language Models in Bangla Text Generation](https://arxiv.org/abs/2601.04534)
*Amit Bin Tariqul,A N M Zahid Hossain Milkan,Sahab-Al-Chowdhury,Syed Rifat Raiyan,Hasan Mahmud,Md Kamrul Hasan*

Main category: cs.CL

TL;DR: This paper evaluates text watermarking methods for Bangla LLM generation under cross-lingual translation attacks, showing existing methods fail under attacks, and proposes layered watermarking that improves robustness 3-4× with controlled quality degradation.


<details>
  <summary>Details</summary>
Motivation: Watermarking is essential for LLM text generation attribution and protection, but existing methods are underexplored for low-resource languages like Bangla and vulnerable to cross-lingual translation attacks.

Method: Systematic evaluation of KGW, EXP, and Waterfall watermarking methods for Bangla LLM text generation under round-trip translation attacks, followed by proposal of layered watermarking combining embedding-time and post-generation watermarks.

Result: Existing methods achieve >88% accuracy under benign conditions but collapse to 9-13% under translation attacks. Layered watermarking improves post-attack accuracy by 25-35%, achieving 40-50% accuracy (3-4× improvement) with controlled semantic degradation.

Conclusion: Layered watermarking is a practical, training-free solution for low-resource languages that quantifies the robustness-quality trade-off and significantly improves watermark detection under cross-lingual attacks.

Abstract: As large language models (LLMs) are increasingly deployed for text generation, watermarking has become essential for authorship attribution, intellectual property protection, and misuse detection. While existing watermarking methods perform well in high-resource languages, their robustness in low-resource languages remains underexplored. This work presents the first systematic evaluation of state-of-the-art text watermarking methods: KGW, Exponential Sampling (EXP), and Waterfall, for Bangla LLM text generation under cross-lingual round-trip translation (RTT) attacks. Under benign conditions, KGW and EXP achieve high detection accuracy (>88%) with negligible perplexity and ROUGE degradation. However, RTT causes detection accuracy to collapse below RTT causes detection accuracy to collapse to 9-13%, indicating a fundamental failure of token-level watermarking. To address this, we propose a layered watermarking strategy that combines embedding-time and post-generation watermarks. Experimental results show that layered watermarking improves post-RTT detection accuracy by 25-35%, achieving 40-50% accuracy, representing a 3$\times$ to 4$\times$ relative improvement over single-layer methods, at the cost of controlled semantic degradation. Our findings quantify the robustness-quality trade-off in multilingual watermarking and establish layered watermarking as a practical, training-free solution for low-resource languages such as Bangla. Our code and data will be made public.

</details>


### [36] [Identifying Good and Bad Neurons for Task-Level Controllable LLMs](https://arxiv.org/abs/2601.04548)
*Wenjie Li,Guansong Pang,Hezhe Qiao,Debin Gao,David Lo*

Main category: cs.CL

TL;DR: NeuronLLM is a task-level LLM understanding framework that identifies both supportive and inhibitive neurons using functional antagonism principles and contrastive learning to mitigate fortuitous behaviors in LLMs.


<details>
  <summary>Details</summary>
Motivation: Current methods for identifying responsible neurons in LLMs are ability-specific and infeasible for task-focused scenarios requiring coordinated use of multiple abilities. They also focus only on supportive neurons while neglecting inhibitive roles and fail to address fortuitous behaviors where LLMs answer correctly by chance rather than genuine understanding.

Method: NeuronLLM adopts the biological principle of functional antagonism for LLM neuron identification, modeling task performance as jointly determined by good neurons (facilitate task completion) and bad neurons (inhibit it). It uses contrastive learning to model both neuron types and leverages augmented question sets to mitigate fortuitous behaviors in LLMs.

Result: Comprehensive experiments on LLMs of different sizes and families show NeuronLLM's superiority over existing methods in four NLP tasks, providing new insights into LLM functional organization.

Conclusion: NeuronLLM offers a more holistic approach to understanding LLM neurons by identifying both supportive and inhibitive roles, addressing the limitations of previous ability-specific methods and providing better insights into LLM functional organization for task-level understanding.

Abstract: Large Language Models have demonstrated remarkable capabilities on multiple-choice question answering benchmarks, but the complex mechanisms underlying their large-scale neurons remain opaque, posing significant challenges for understanding and steering LLMs. While recent studies made progress on identifying responsible neurons for certain abilities, these ability-specific methods are infeasible for task-focused scenarios requiring coordinated use of multiple abilities. Moreover, these approaches focus only on supportive neurons that correlate positively with task completion, while neglecting neurons with other roles-such as inhibitive roles-and misled neuron attribution due to fortuitous behaviors in LLMs (i.e., correctly answer the questions by chance rather than genuine understanding). To address these challenges, we propose NeuronLLM, a novel task-level LLM understanding framework that adopts the biological principle of functional antagonism for LLM neuron identification. The key insight is that task performance is jointly determined by neurons with two opposing roles: good neurons that facilitate task completion and bad neurons that inhibit it. NeuronLLM achieves a holistic modeling of neurons via contrastive learning of good and bad neurons, while leveraging augmented question sets to mitigate the fortuitous behaviors in LLMs. Comprehensive experiments on LLMs of different sizes and families show the superiority of NeuronLLM over existing methods in four NLP tasks, providing new insights into LLM functional organization.

</details>


### [37] [FeedEval: Pedagogically Aligned Evaluation of LLM-Generated Essay Feedback](https://arxiv.org/abs/2601.04574)
*Seongyeub Chu,Jongwoo Kim,Munyong Yi*

Main category: cs.CL

TL;DR: FeedEval is an LLM-based framework that evaluates LLM-generated essay feedback on specificity, helpfulness, and validity dimensions to filter high-quality feedback for training essay scoring models.


<details>
  <summary>Details</summary>
Motivation: Previous automated essay scoring research uses LLM-generated feedback without quality validation, leading to noise propagation in downstream applications. There's a need to ensure feedback quality before using it to train assessment models.

Method: FeedEval employs dimension-specialized LLM evaluators trained on curated datasets to assess multiple feedback candidates across three pedagogically grounded dimensions: specificity, helpfulness, and validity. It selects high-quality feedback for downstream use.

Result: Experiments on ASAP++ benchmark show FeedEval aligns closely with human expert judgments. Essay scoring models trained with FeedEval-filtered high-quality feedback achieve superior scoring performance. Revision experiments demonstrate this feedback leads to more effective essay revisions.

Conclusion: FeedEval provides an effective framework for evaluating and filtering LLM-generated essay feedback, improving downstream essay scoring and revision outcomes while addressing quality validation gaps in previous approaches.

Abstract: Going beyond the prediction of numerical scores, recent research in automated essay scoring has increasingly emphasized the generation of high-quality feedback that provides justification and actionable guidance. To mitigate the high cost of expert annotation, prior work has commonly relied on LLM-generated feedback to train essay assessment models. However, such feedback is often incorporated without explicit quality validation, resulting in the propagation of noise in downstream applications. To address this limitation, we propose FeedEval, an LLM-based framework for evaluating LLM-generated essay feedback along three pedagogically grounded dimensions: specificity, helpfulness, and validity. FeedEval employs dimension-specialized LLM evaluators trained on datasets curated in this study to assess multiple feedback candidates and select high-quality feedback for downstream use. Experiments on the ASAP++ benchmark show that FeedEval closely aligns with human expert judgments and that essay scoring models trained with FeedEval-filtered high-quality feedback achieve superior scoring performance. Furthermore, revision experiments using small LLMs show that the high-quality feedback identified by FeedEval leads to more effective essay revisions. We will release our code and curated datasets upon accepted.

</details>


### [38] [Aligning Text, Code, and Vision: A Multi-Objective Reinforcement Learning Framework for Text-to-Visualization](https://arxiv.org/abs/2601.04582)
*Mizanur Rahman,Mohammed Saidul Islam,Md Tahmid Rahman Laskar,Shafiq Joty,Enamul Hoque*

Main category: cs.CL

TL;DR: RL-Text2Vis is a reinforcement learning framework that improves text-to-visualization generation by optimizing textual accuracy, code validity, and visualization quality using post-execution feedback, achieving significant improvements over existing methods.


<details>
  <summary>Details</summary>
Motivation: Current Text2Vis systems using LLMs often produce visualizations that lack semantic alignment and clarity, with open-source models struggling to generate executable or visually appealing outputs. Supervised fine-tuning improves code executability but fails to enhance overall visualization quality since traditional loss functions cannot capture post-execution feedback.

Method: Proposes RL-Text2Vis, a reinforcement learning framework built on Group Relative Policy Optimization (GRPO) with a novel multi-objective reward function that jointly optimizes textual accuracy, code validity, and visualization quality using post-execution feedback. Trained on Qwen2.5 models (7B and 14B).

Result: Achieves 22% relative improvement in chart quality over GPT-4o on Text2Vis benchmark, boosts code execution success from 78% to 97% relative to zero-shot baseline, significantly outperforms strong zero-shot and supervised baselines, and demonstrates robust generalization to out-of-domain datasets like VIS-Eval and NVBench.

Conclusion: RL-Text2Vis establishes GRPO as an effective strategy for structured, multimodal reasoning in visualization generation, addressing the limitations of current LLM-based approaches by incorporating post-execution feedback through reinforcement learning.

Abstract: Text-to-Visualization (Text2Vis) systems translate natural language queries over tabular data into concise answers and executable visualizations. While closed-source LLMs generate functional code, the resulting charts often lack semantic alignment and clarity, qualities that can only be assessed post-execution. Open-source models struggle even more, frequently producing non-executable or visually poor outputs. Although supervised fine-tuning can improve code executability, it fails to enhance overall visualization quality, as traditional SFT loss cannot capture post-execution feedback. To address this gap, we propose RL-Text2Vis, the first reinforcement learning framework for Text2Vis generation. Built on Group Relative Policy Optimization (GRPO), our method uses a novel multi-objective reward that jointly optimizes textual accuracy, code validity, and visualization quality using post-execution feedback. By training Qwen2.5 models (7B and 14B), RL-Text2Vis achieves a 22% relative improvement in chart quality over GPT-4o on the Text2Vis benchmark and boosts code execution success from 78% to 97% relative to its zero-shot baseline. Our models significantly outperform strong zero-shot and supervised baselines and also demonstrate robust generalization to out-of-domain datasets like VIS-Eval and NVBench. These results establish GRPO as an effective strategy for structured, multimodal reasoning in visualization generation. We release our code at https://github.com/vis-nlp/RL-Text2Vis.

</details>


### [39] [THaLLE-ThaiLLM: Domain-Specialized Small LLMs for Finance and Thai -- Technical Report](https://arxiv.org/abs/2601.04597)
*KBTG Labs,:,Anuruth Lertpiya,Danupat Khamnuansin,Kantapong Sucharitpongpan,Pornchanan Balee,Tawunrat Chalothorn,Thadpong Pongthawornkamol,Monchai Lertsutthiwong*

Main category: cs.CL

TL;DR: Model merging enables efficient creation of multi-capability LLMs by combining specialized models like ThaiLLM-8B and THaLLE-CFA-8B with Qwen-8B, improving performance across Thai language and financial domains without expensive retraining.


<details>
  <summary>Details</summary>
Motivation: Organizations need multi-capability LLMs for banking/finance applications but face trade-offs between deploying multiple specialized models vs. the prohibitive cost of training a single comprehensive model. Privacy/regulatory concerns also favor on-premise deployment.

Method: Explores model merging as a resource-efficient alternative. Conducts two experiments: 1) merging Qwen-8B with ThaiLLM-8B, 2) merging Qwen-8B with both ThaiLLM-8B and THaLLE-CFA-8B to combine general instruction-following, Thai language, and financial capabilities.

Result: First merge shows ThaiLLM-8B enhances Thai general capabilities with uplift in M3 and M6 O-NET exams. Second merge further improves performance across both general and financial domains, demonstrating uplift in M3/M6 O-NET, Flare-CFA, and Thai-IC benchmarks.

Conclusion: Model merging is a viable approach for efficiently creating high-performance, multi-capability LLMs, enabling organizations to leverage specialized capabilities without the prohibitive expense of training comprehensive models from scratch.

Abstract: Large Language Models (LLMs) have demonstrated significant potential across various domains, particularly in banking and finance, where they can automate complex tasks and enhance decision-making at scale. Due to privacy, security, and regulatory concerns, organizations often prefer on-premise deployment of LLMs. The ThaiLLM initiative aims to enhance Thai language capabilities in open-LLMs, enabling Thai industry to leverage advanced language models. However, organizations often face a trade-off between deploying multiple specialized models versus the prohibitive expense of training a single multi-capability model. To address this, we explore model merging as a resource-efficient alternative for developing high-performance, multi-capability LLMs. We present results from two key experiments: first, merging Qwen-8B with ThaiLLM-8B demonstrates how ThaiLLM-8B enhances Thai general capabilities, showing an uplift of M3 and M6 O-NET exams over the general instruction-following Qwen-8B. Second, we merge Qwen-8B with both ThaiLLM-8B and THaLLE-CFA-8B. This combination results in further improvements in performance across both general and financial domains, by demonstrating an uplift in both M3 and M6 O-NET, Flare-CFA, and Thai-IC benchmarks. The report showcases the viability of model merging for efficiently creating multi-capability LLMs.

</details>


### [40] [On the Limitations of Rank-One Model Editing in Answering Multi-hop Questions](https://arxiv.org/abs/2601.04600)
*Zhiyuan He,Binghan Chen,Tianxiang Xiong,Ziyang Sun,Mozhao Zhu,Xi Chen*

Main category: cs.CL

TL;DR: ROME knowledge editing struggles with multi-hop reasoning due to three failure modes; Redundant Editing improves 2-hop accuracy by 15.5+ percentage points (96% increase) with trade-offs in specificity/naturalness.


<details>
  <summary>Details</summary>
Motivation: Current knowledge editing methods like ROME work well for single-hop fact updates but fail at multi-hop reasoning tasks requiring knowledge chaining, limiting their practical applicability for complex reasoning.

Method: Analyzed ROME editing effects across layer depths, identified three failure modes, then proposed Redundant Editing strategy to mitigate "hopping-too-late" and generalization decay problems.

Result: Redundant Editing improves accuracy on 2-hop questions by at least 15.5 percentage points (96% increase over single-edit strategy), though with some trade-offs in specificity and language naturalness.

Conclusion: Redundant Editing effectively addresses multi-hop reasoning limitations of current knowledge editing methods, significantly improving performance on complex reasoning tasks while acknowledging remaining challenges.

Abstract: Recent advances in Knowledge Editing (KE), particularly Rank-One Model Editing (ROME), show superior efficiency over fine-tuning and in-context learning for updating single-hop facts in transformers. However, these methods face significant challenges when applied to multi-hop reasoning tasks requiring knowledge chaining. In this work, we study the effect of editing knowledge with ROME on different layer depths and identify three key failure modes. First, the "hopping-too-late" problem occurs as later layers lack access to necessary intermediate representations. Second, generalization ability deteriorates sharply when editing later layers. Third, the model overfits to edited knowledge, incorrectly prioritizing edited-hop answers regardless of context. To mitigate the issues of "hopping-too-late" and generalisation decay, we propose Redundant Editing, a simple yet effective strategy that enhances multi-hop reasoning. Our experiments demonstrate that this approach can improve accuracy on 2-hop questions by at least 15.5 percentage points, representing a 96% increase over the previous single-edit strategy, while trading off some specificity and language naturalness.

</details>


### [41] [When More Words Say Less: Decoupling Length and Specificity in Image Description Evaluation](https://arxiv.org/abs/2601.04609)
*Rhea Kapur,Robert Hawkins,Elisa Kreiss*

Main category: cs.CL

TL;DR: VLMs conflate description specificity with length; the paper argues they should be disentangled, defines specificity relative to contrast sets, and shows people prefer specific descriptions regardless of length.


<details>
  <summary>Details</summary>
Motivation: Current vision-language models conflate description specificity with length, but these should be separate concepts. Descriptions can be concise yet informative or lengthy yet vacuous. The paper aims to disentangle these factors for better evaluation.

Method: Defines specificity relative to a contrast set (how well a description picks out target image vs. other possible images). Constructs a dataset controlling for length while varying information content. Validates through human preference studies.

Result: People reliably prefer more specific descriptions regardless of length. Controlling for length alone cannot account for specificity differences - how length budget is allocated matters.

Conclusion: Evaluation approaches should directly prioritize specificity over verbosity. The distinction between specificity and length is crucial for better vision-language model assessment.

Abstract: Vision-language models (VLMs) are increasingly used to make visual content accessible via text-based descriptions. In current systems, however, description specificity is often conflated with their length. We argue that these two concepts must be disentangled: descriptions can be concise yet dense with information, or lengthy yet vacuous. We define specificity relative to a contrast set, where a description is more specific to the extent that it picks out the target image better than other possible images. We construct a dataset that controls for length while varying information content, and validate that people reliably prefer more specific descriptions regardless of length. We find that controlling for length alone cannot account for differences in specificity: how the length budget is allocated makes a difference. These results support evaluation approaches that directly prioritize specificity over verbosity.

</details>


### [42] [Character-R1: Enhancing Role-Aware Reasoning in Role-Playing Agents via RLVR](https://arxiv.org/abs/2601.04611)
*Yihong Tang,Kehai Chen,Xuefeng Bai,Benyou Wang,Zeming Liu,Haifeng Wang,Min Zhang*

Main category: cs.CL

TL;DR: Character-R1 is a framework that provides verifiable reward signals for role-playing agents to improve cognitive consistency and reduce out-of-character errors through structured internal cognition, reference-guided optimization, and character-conditioned reward normalization.


<details>
  <summary>Details</summary>
Motivation: Current role-playing agents imitate surface-level behaviors but lack internal cognitive consistency, leading to out-of-character errors in complex situations. There's a need for comprehensive verifiable reward signals for effective role-aware reasoning that are missing in recent studies.

Method: Character-R1 framework includes three core designs: 1) Cognitive Focus Reward - enforces explicit label-based analysis of 10 character elements (e.g., worldview) to structure internal cognition; 2) Reference-Guided Reward - uses overlap-based metrics with reference responses as optimization anchors; 3) Character-Conditioned Reward Normalization - adjusts reward distributions based on character categories for robust optimization across heterogeneous roles.

Result: Extensive experiments demonstrate that Character-R1 significantly outperforms existing methods in knowledge, memory and other performance metrics.

Conclusion: The Character-R1 framework successfully addresses the cognitive consistency problem in role-playing agents by providing comprehensive verifiable reward signals, leading to improved performance and reduced out-of-character errors across diverse roles.

Abstract: Current role-playing agents (RPAs) are typically constructed by imitating surface-level behaviors, but this approach lacks internal cognitive consistency, often causing out-of-character errors in complex situations. To address this, we propose Character-R1, a framework designed to provide comprehensive verifiable reward signals for effective role-aware reasoning, which are missing in recent studies. Specifically, our framework comprises three core designs: (1) Cognitive Focus Reward, which enforces explicit label-based analysis of 10 character elements (e.g., worldview) to structure internal cognition; (2) Reference-Guided Reward, which utilizes overlap-based metrics with reference responses as optimization anchors to enhance exploration and performance; and (3) Character-Conditioned Reward Normalization, which adjusts reward distributions based on character categories to ensure robust optimization across heterogeneous roles. Extensive experiments demonstrate that Character-R1 significantly outperforms existing methods in knowledge, memory and others.

</details>


### [43] [From National Curricula to Cultural Awareness: Constructing Open-Ended Culture-Specific Question Answering Dataset](https://arxiv.org/abs/2601.04632)
*Haneul Yoo,Won Ik Cho,Geunhye Kim,Jiyoon Han*

Main category: cs.CL

TL;DR: CuCu framework transforms national social studies curricula into culture-specific QA pairs to address LLM cultural bias, demonstrated with Korean curriculum creating KCaQA dataset.


<details>
  <summary>Details</summary>
Motivation: LLMs show uneven performance across languages and cultures due to English-centric training data bias, creating need for practical cultural alignment methods.

Method: CuCu: automated multi-agent LLM framework that transforms national textbook curricula into open-ended, culture-specific question-answer pairs.

Result: Applied to Korean social studies curriculum to create KCaQA dataset with 34.1k QA pairs covering culture-specific topics with locally-grounded responses.

Conclusion: National curricula provide scalable foundation for culture-aware supervision; CuCu enables practical cultural alignment for LLMs across different cultural contexts.

Abstract: Large language models (LLMs) achieve strong performance on many tasks, but their progress remains uneven across languages and cultures, often reflecting values latent in English-centric training data. To enable practical cultural alignment, we propose a scalable approach that leverages national social studies curricula as a foundation for culture-aware supervision. We introduce CuCu, an automated multi-agent LLM framework that transforms national textbook curricula into open-ended, culture-specific question-answer pairs. Applying CuCu to the Korean national social studies curriculum, we construct KCaQA, comprising 34.1k open-ended QA pairs. Our quantitative and qualitative analyses suggest that KCaQA covers culture-specific topics and produces responses grounded in local sociocultural contexts.

</details>


### [44] [MAGA-Bench: Machine-Augment-Generated Text via Alignment Detection Benchmark](https://arxiv.org/abs/2601.04633)
*Anyang Song,Ying Cheng,Yiqian Xu,Rui Feng*

Main category: cs.CL

TL;DR: Proposes MAGA (Machine-Augment-Generated Text via Alignment) to enhance LLM alignment for better detection of machine-generated text, improving detector generalization while testing robustness.


<details>
  <summary>Details</summary>
Motivation: As machine-generated text becomes increasingly indistinguishable from human-written text, existing detectors struggle with generalization and robustness issues. Fine-tuned detectors depend heavily on dataset quality, and simply expanding MGT sources is insufficient.

Method: MAGA pipeline achieves comprehensive alignment from prompt construction to reasoning process. Key component is RLDF (Reinforced Learning from Detectors Feedback), which systematically enhances text alignment to improve both attack capabilities on detectors and detector generalization when fine-tuned on such data.

Result: RoBERTa detector fine-tuned on MAGA training set achieved average 4.60% improvement in generalization detection AUC. MAGA Dataset caused average 8.13% decrease in AUC of selected detectors, demonstrating both improved detector generalization and effective robustness testing.

Conclusion: MAGA provides an effective framework for enhancing LLM alignment that simultaneously improves detector generalization and tests detector robustness, offering significant implications for future research on machine-generated text detection.

Abstract: Large Language Models (LLMs) alignment is constantly evolving. Machine-Generated Text (MGT) is becoming increasingly difficult to distinguish from Human-Written Text (HWT). This has exacerbated abuse issues such as fake news and online fraud. Fine-tuned detectors' generalization ability is highly dependent on dataset quality, and simply expanding the sources of MGT is insufficient. Further augment of generation process is required. According to HC-Var's theory, enhancing the alignment of generated text can not only facilitate attacks on existing detectors to test their robustness, but also help improve the generalization ability of detectors fine-tuned on it. Therefore, we propose \textbf{M}achine-\textbf{A}ugment-\textbf{G}enerated Text via \textbf{A}lignment (MAGA). MAGA's pipeline achieves comprehensive alignment from prompt construction to reasoning process, among which \textbf{R}einforced \textbf{L}earning from \textbf{D}etectors \textbf{F}eedback (RLDF), systematically proposed by us, serves as a key component. In our experiments, the RoBERTa detector fine-tuned on MAGA training set achieved an average improvement of 4.60\% in generalization detection AUC. MAGA Dataset caused an average decrease of 8.13\% in the AUC of the selected detectors, expecting to provide indicative significance for future research on the generalization detection ability of detectors.

</details>


### [45] [SpeechMedAssist: Efficiently and Effectively Adapting Speech Language Models for Medical Consultation](https://arxiv.org/abs/2601.04638)
*Sirry Chen,Jieyi Wang,Wei Chen,Zhongyu Wei*

Main category: cs.CL

TL;DR: SpeechMedAssist: A speech language model for medical consultations using two-stage training to reduce speech data requirements to only 10k synthesized samples.


<details>
  <summary>Details</summary>
Motivation: Medical consultations are speech-centric but most prior work focuses on text-based interactions which are cumbersome. SpeechLMs enable natural speech interaction but face challenges: scarcity of medical speech data and inefficiency of direct fine-tuning on speech data.

Method: Proposes SpeechMedAssist with two-stage training: (1) Knowledge & Capability Injection via Text (using text data to inject medical knowledge), and (2) Modality Re-alignment with Limited Speech Data (using only 10k synthesized speech samples to align modalities).

Result: Outperforms all baselines in both effectiveness and robustness in most evaluation settings on a benchmark comprising single-turn QA and multi-turn simulated interactions.

Conclusion: SpeechMedAssist enables speech-based medical consultation with significantly reduced speech data requirements through innovative two-stage training paradigm.

Abstract: Medical consultations are intrinsically speech-centric. However, most prior works focus on long-text-based interactions, which are cumbersome and patient-unfriendly. Recent advances in speech language models (SpeechLMs) have enabled more natural speech-based interaction, yet the scarcity of medical speech data and the inefficiency of directly fine-tuning on speech data jointly hinder the adoption of SpeechLMs in medical consultation. In this paper, we propose SpeechMedAssist, a SpeechLM natively capable of conducting speech-based multi-turn interactions with patients. By exploiting the architectural properties of SpeechLMs, we decouple the conventional one-stage training into a two-stage paradigm consisting of (1) Knowledge & Capability Injection via Text and (2) Modality Re-alignment with Limited Speech Data, thereby reducing the requirement for medical speech data to only 10k synthesized samples. To evaluate SpeechLMs for medical consultation scenarios, we design a benchmark comprising both single-turn question answering and multi-turn simulated interactions. Experimental results show that our model outperforms all baselines in both effectiveness and robustness in most evaluation settings.

</details>


### [46] [CRANE: Causal Relevance Analysis of Language-Specific Neurons in Multilingual Large Language Models](https://arxiv.org/abs/2601.04664)
*Yifan Le,Yunliang Li*

Main category: cs.CL

TL;DR: CRANE is a relevance-based framework that identifies language-specific neurons in multilingual LLMs through targeted interventions, showing asymmetric language specialization patterns.


<details>
  <summary>Details</summary>
Motivation: Current methods for identifying language-related neurons in multilingual LLMs rely on activation-based heuristics, which conflate language preference with functional importance. There's a need for a more precise approach that distinguishes between neurons that merely activate for a language versus those that are functionally necessary for it.

Method: CRANE uses relevance-based analysis with targeted neuron-level interventions to identify language-specific neurons based on functional necessity rather than activation magnitude. It characterizes neuron specialization by their contribution to language-conditioned predictions through masking experiments.

Result: Neuron-level interventions reveal asymmetric patterns: masking neurons relevant to a target language selectively degrades performance on that language while largely preserving performance on other languages. CRANE isolates language-specific components more precisely than activation-based methods across English, Chinese, and Vietnamese benchmarks.

Conclusion: CRANE provides a more accurate framework for understanding language organization in multilingual LLMs by focusing on functional necessity rather than activation patterns, revealing that language-specific neurons are selective but not exclusive to particular languages.

Abstract: Multilingual large language models (LLMs) achieve strong performance across languages, yet how language capabilities are organized at the neuron level remains poorly understood. Prior work has identified language-related neurons mainly through activation-based heuristics, which conflate language preference with functional importance. Prior work has identified language-related neurons mainly through activation-based heuristics, which conflate language preference with functional importance. We propose CRANE, a relevance-based analysis framework that redefines language specificity in terms of functional necessity, identifying language-specific neurons through targeted neuron-level interventions. CRANE characterizes neuron specialization by their contribution to language-conditioned predictions rather than activation magnitude. Our implementation will be made publicly available. Neuron-level interventions reveal a consistent asymmetric pattern: masking neurons relevant to a target language selectively degrades performance on that language while preserving performance on other languages to a substantial extent, indicating language-selective but non-exclusive neuron specializations. Experiments on English, Chinese, and Vietnamese across multiple benchmarks, together with a dedicated relevance-based metric and base-to-chat model transfer analysis, show that CRANE isolates language-specific components more precisely than activation-based methods.

</details>


### [47] [ToolGate: Contract-Grounded and Verified Tool Execution for LLMs](https://arxiv.org/abs/2601.04688)
*Yanming Liu,Xinyue Peng,Jiannan Cao,Xinyi Wang,Songhang Deng,Jintao Chen,Jianwei Yin,Xuhong Zhang*

Main category: cs.CL

TL;DR: ToolGate is a framework that provides logical safety guarantees for LLM tool calling through formal contracts and runtime verification, ensuring state evolution only through verified tool executions.


<details>
  <summary>Details</summary>
Motivation: Existing LLM tool-augmentation frameworks lack formal guarantees for logical safety and verifiability, relying heavily on natural language reasoning without ensuring that tool invocations and result commitments are logically sound.

Method: ToolGate maintains an explicit symbolic state space as typed key-value mapping. Each tool is formalized with Hoare-style contracts (precondition and postcondition). Precondition gates tool invocation by checking state satisfaction, while postcondition determines result commitment through runtime verification.

Result: Experimental validation shows ToolGate significantly improves reliability and verifiability of tool-augmented LLM systems while maintaining competitive performance on complex multi-step reasoning tasks.

Conclusion: ToolGate establishes a foundation for building more trustworthy and debuggable AI systems that integrate language models with external tools by providing logical safety guarantees and verifiable state evolution.

Abstract: Large Language Models (LLMs) augmented with external tools have demonstrated remarkable capabilities in complex reasoning tasks. However, existing frameworks rely heavily on natural language reasoning to determine when tools can be invoked and whether their results should be committed, lacking formal guarantees for logical safety and verifiability. We present \textbf{ToolGate}, a forward execution framework that provides logical safety guarantees and verifiable state evolution for LLM tool calling. ToolGate maintains an explicit symbolic state space as a typed key-value mapping representing trusted world information throughout the reasoning process. Each tool is formalized as a Hoare-style contract consisting of a precondition and a postcondition, where the precondition gates tool invocation by checking whether the current state satisfies the required conditions, and the postcondition determines whether the tool's result can be committed to update the state through runtime verification. Our approach guarantees that the symbolic state evolves only through verified tool executions, preventing invalid or hallucinated results from corrupting the world representation. Experimental validation demonstrates that ToolGate significantly improves the reliability and verifiability of tool-augmented LLM systems while maintaining competitive performance on complex multi-step reasoning tasks. This work establishes a foundation for building more trustworthy and debuggable AI systems that integrate language models with external tools.

</details>


### [48] [See, Explain, and Intervene: A Few-Shot Multimodal Agent Framework for Hateful Meme Moderation](https://arxiv.org/abs/2601.04692)
*Naquee Rizwan,Subhankar Swain,Paramananda Bhaskar,Gagan Aryan,Shehryaar Shah Khan,Animesh Mukherjee*

Main category: cs.CL

TL;DR: A framework using generative AI for hateful meme detection, explanation, and intervention under limited data conditions.


<details>
  <summary>Details</summary>
Motivation: Current approaches study hateful meme detection, explanation, and intervention separately, which doesn't reflect real-world conditions. Additionally, curating large annotated datasets for meme moderation is prohibitively expensive.

Method: Proposes a novel framework leveraging task-specific generative multimodal agents and the few-shot adaptability of large multimodal models to handle different types of memes.

Result: First work focused on generalizable hateful meme moderation under limited data conditions, with strong potential for real-world deployment.

Conclusion: The framework addresses the integrated challenges of hateful meme moderation (detection, explanation, intervention) using generative AI in data-scarce scenarios, making it practical for real-world applications.

Abstract: In this work, we examine hateful memes from three complementary angles - how to detect them, how to explain their content and how to intervene them prior to being posted - by applying a range of strategies built on top of generative AI models. To the best of our knowledge, explanation and intervention have typically been studied separately from detection, which does not reflect real-world conditions. Further, since curating large annotated datasets for meme moderation is prohibitively expensive, we propose a novel framework that leverages task-specific generative multimodal agents and the few-shot adaptability of large multimodal models to cater to different types of memes. We believe this is the first work focused on generalizable hateful meme moderation under limited data conditions, and has strong potential for deployment in real-world production scenarios. Warning: Contains potentially toxic contents.

</details>


### [49] [Thunder-KoNUBench: A Corpus-Aligned Benchmark for Korean Negation Understanding](https://arxiv.org/abs/2601.04693)
*Sungmok Jung,Yeonkyoung So,Joonhak Lee,Sangho Kim,Yelim Ahn,Jaejin Lee*

Main category: cs.CL

TL;DR: Thunder-KoNUBench: A Korean negation benchmark showing LLMs struggle with negation, with fine-tuning improving performance.


<details>
  <summary>Details</summary>
Motivation: Negation challenges LLMs, but Korean negation benchmarks are scarce. Need to evaluate and improve LLM understanding of Korean negation phenomena.

Method: Corpus-based analysis of Korean negation, creation of Thunder-KoNUBench benchmark reflecting empirical distribution, evaluation of 47 LLMs, and fine-tuning experiments.

Result: LLM performance degrades under negation; model size and instruction tuning affect performance; fine-tuning on Thunder-KoNUBench improves negation understanding and broader contextual comprehension in Korean.

Conclusion: Thunder-KoNUBench addresses Korean negation evaluation gap, shows LLMs need improvement on negation, and demonstrates fine-tuning effectiveness for better Korean language understanding.

Abstract: Although negation is known to challenge large language models (LLMs), benchmarks for evaluating negation understanding, especially in Korean, are scarce. We conduct a corpus-based analysis of Korean negation and show that LLM performance degrades under negation. We then introduce Thunder-KoNUBench, a sentence-level benchmark that reflects the empirical distribution of Korean negation phenomena. Evaluating 47 LLMs, we analyze the effects of model size and instruction tuning, and show that fine-tuning on Thunder-KoNUBench improves negation understanding and broader contextual comprehension in Korean.

</details>


### [50] [PRISM: A Unified Framework for Post-Training LLMs Without Verifiable Rewards](https://arxiv.org/abs/2601.04700)
*Mukesh Ghimire,Aosong Feng,Liwen You,Youzhi Luo,Fang Liu,Xuan Zhu*

Main category: cs.CL

TL;DR: PRISM: A framework combining Process Reward Model with model self-certainty for stable unsupervised LLM training without human labels.


<details>
  <summary>Details</summary>
Motivation: Current post-training methods rely on costly human supervision or external verifiers, but as LLMs improve, high-quality solutions to difficult problems become unavailable to humans, making unsupervised learning increasingly important. Existing consistency-based methods (majority voting or internal confidence) are unreliable for large-scale, long-term training.

Method: PRISM (Process Reward Model) framework that combines a Process Reward Model with the model's internal self-certainty to guide learning in the absence of ground-truth labels. This unified approach effectively balances external process evaluation with internal confidence metrics.

Result: Effectively combining PRM with self-certainty leads to both stable training and better test-time performance, while also keeping the model's internal confidence in check.

Conclusion: PRISM provides a reliable unsupervised training framework that addresses the unreliability of pure internal consistency metrics, enabling effective LLM improvement without human supervision or external verifiers for tasks like mathematical reasoning and code generation.

Abstract: Current techniques for post-training Large Language Models (LLMs) rely either on costly human supervision or on external verifiers to boost performance on tasks such as mathematical reasoning and code generation. However, as LLMs improve their problem-solving, any further improvement will potentially require high-quality solutions to difficult problems that are not available to humans. As a result, learning from unlabeled data is becoming increasingly attractive in the research community. Existing methods extract learning signal from a model's consistency, either by majority voting or by converting the model's internal confidence into reward. Although internal consistency metric such as entropy or self-certainty require no human intervention, as we show in this work, these are unreliable signals for large-scale and long-term training. To address the unreliability, we propose PRISM, a unified training framework that uses a Process Reward Model (PRM) to guide learning alongside model's internal confidence in the absence of ground-truth labels. We show that effectively combining PRM with self-certainty can lead to both stable training and better test-time performance, and also keep the model's internal confidence in check.

</details>


### [51] [Prior-Informed Zeroth-Order Optimization with Adaptive Direction Alignment for Memory-Efficient LLM Fine-Tuning](https://arxiv.org/abs/2601.04710)
*Feihu Jin,Shipeng Cen,Ying Tan*

Main category: cs.CL

TL;DR: A plug-and-play zeroth-order optimization method that uses prior-informed perturbations to improve gradient estimation, achieving faster convergence and better performance than traditional ZO methods while avoiding backpropagation memory overhead.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning large language models requires substantial memory for backpropagation, which becomes a bottleneck as models scale. Zeroth-order optimization avoids backprop but suffers from high variance in gradient estimation due to random perturbations, leading to slow convergence and suboptimal performance.

Method: Proposes a simple plug-and-play method that incorporates prior-informed perturbations to refine gradient estimation. The method dynamically computes a guiding vector from Gaussian samples to direct perturbations toward more informative directions. Also investigates a greedy perturbation strategy to explore prior knowledge impact on gradient estimation.

Result: Extensive experiments across LLMs of varying scales and architectures show the method seamlessly integrates into existing optimization methods, delivering faster convergence and superior performance. On OPT-13B model, outperforms traditional ZO optimization across all 11 benchmark tasks and surpasses gradient-based baselines on 9 out of 11 tasks.

Conclusion: The proposed prior-informed perturbation method for zeroth-order optimization establishes a robust balance between efficiency and accuracy, addressing the memory bottleneck of backpropagation while improving convergence speed and performance over traditional ZO methods.

Abstract: Fine-tuning large language models (LLMs) has achieved remarkable success across various NLP tasks, but the substantial memory overhead during backpropagation remains a critical bottleneck, especially as model scales grow. Zeroth-order (ZO) optimization alleviates this issue by estimating gradients through forward passes and Gaussian sampling, avoiding the need for backpropagation. However, conventional ZO methods suffer from high variance in gradient estimation due to their reliance on random perturbations, leading to slow convergence and suboptimal performance. We propose a simple plug-and-play method that incorporates prior-informed perturbations to refine gradient estimation. Our method dynamically computes a guiding vector from Gaussian samples, which directs perturbations toward more informative directions, significantly accelerating convergence compared to standard ZO approaches. We further investigate a greedy perturbation strategy to explore the impact of prior knowledge on gradient estimation. Theoretically, we prove that our gradient estimator achieves stronger alignment with the true gradient direction, enhancing optimization efficiency. Extensive experiments across LLMs of varying scales and architectures demonstrate that our proposed method could seamlessly integrate into existing optimization methods, delivering faster convergence and superior performance. Notably, on the OPT-13B model, our method outperforms traditional ZO optimization across all 11 benchmark tasks and surpasses gradient-based baselines on 9 out of 11 tasks, establishing a robust balance between efficiency and accuracy.

</details>


### [52] [DSC2025 -- ViHallu Challenge: Detecting Hallucination in Vietnamese LLMs](https://arxiv.org/abs/2601.04711)
*Anh Thi-Hoang Nguyen,Khanh Quoc Tran,Tin Van Huynh,Phuoc Tan-Hoang Nguyen,Cam Tan Nguyen,Kiet Van Nguyen*

Main category: cs.CL

TL;DR: First large-scale shared task for detecting hallucinations in Vietnamese LLMs, introducing ViHallu dataset with 10K annotated samples across three hallucination categories, achieving 84.80% macro-F1 with best systems.


<details>
  <summary>Details</summary>
Motivation: LLM reliability is constrained by hallucinations, but existing benchmarks focus on English while low-to-medium resource languages like Vietnamese lack standardized evaluation frameworks for hallucination detection.

Method: Created ViHallu dataset with 10,000 annotated (context, prompt, response) triplets across three hallucination categories (no hallucination, intrinsic, extrinsic) and three prompt types (factual, noisy, adversarial). Organized DSC2025 ViHallu Challenge with 111 participating teams using various detection methodologies.

Result: Best-performing system achieved 84.80% macro-F1 score, significantly outperforming baseline encoder-only score of 32.83%. Instruction-tuned LLMs with structured prompting and ensemble strategies performed best, though intrinsic (contradiction-based) hallucinations remained challenging.

Conclusion: Established rigorous benchmark for Vietnamese hallucination detection, showing that specialized approaches outperform generic architectures but perfect performance remains elusive, especially for intrinsic hallucinations. Provides foundation for future research on Vietnamese AI system trustworthiness.

Abstract: The reliability of large language models (LLMs) in production environments remains significantly constrained by their propensity to generate hallucinations -- fluent, plausible-sounding outputs that contradict or fabricate information. While hallucination detection has recently emerged as a priority in English-centric benchmarks, low-to-medium resource languages such as Vietnamese remain inadequately covered by standardized evaluation frameworks. This paper introduces the DSC2025 ViHallu Challenge, the first large-scale shared task for detecting hallucinations in Vietnamese LLMs. We present the ViHallu dataset, comprising 10,000 annotated triplets of (context, prompt, response) samples systematically partitioned into three hallucination categories: no hallucination, intrinsic, and extrinsic hallucinations. The dataset incorporates three prompt types -- factual, noisy, and adversarial -- to stress-test model robustness. A total of 111 teams participated, with the best-performing system achieving a macro-F1 score of 84.80\%, compared to a baseline encoder-only score of 32.83\%, demonstrating that instruction-tuned LLMs with structured prompting and ensemble strategies substantially outperform generic architectures. However, the gap to perfect performance indicates that hallucination detection remains a challenging problem, particularly for intrinsic (contradiction-based) hallucinations. This work establishes a rigorous benchmark and explores a diverse range of detection methodologies, providing a foundation for future research into the trustworthiness and reliability of Vietnamese language AI systems.

</details>


### [53] [Fame Fades, Nature Remains: Disentangling the Character Identity of Role-Playing Agents](https://arxiv.org/abs/2601.04716)
*Yonghyun Jun,Junhyuk Choi,Jihyeong Park,Hwanhee Lee*

Main category: cs.CL

TL;DR: The paper introduces "Character Identity" as a two-layer construct (Parametric and Attributive Identity) to formalize character representation in Role-Playing Agents, revealing that fame-based advantages fade quickly while negative social traits remain a bottleneck for fidelity.


<details>
  <summary>Details</summary>
Motivation: Current Role-Playing Agents treat characters as arbitrary text inputs without formal structural dimensions, lacking systematic understanding of how character identity components affect agent performance and fidelity.

Method: Proposed Character Identity framework with Parametric (pre-trained knowledge) and Attributive (behavioral properties) layers. Created unified character profile schema, generated Famous and Synthetic characters under identical constraints, and evaluated across single-turn and multi-turn interactions.

Result: Two key findings: 1) "Fame Fades" - famous characters have initial advantage from parametric knowledge but this disappears as models prioritize conversational context; 2) "Nature Remains" - models robustly portray personality traits but RPA performance is highly sensitive to morality and relationship valence, with negative social natures being the primary bottleneck.

Conclusion: Character Identity framework provides systematic understanding of RPA fidelity, revealing that negative social traits are the main limitation. This guides future character construction and evaluation by highlighting what aspects of character identity are most challenging for LLM-based role-playing agents.

Abstract: Despite the rapid proliferation of Role-Playing Agents (RPAs) based on Large Language Models (LLMs), the structural dimensions defining a character's identity remain weakly formalized, often treating characters as arbitrary text inputs. In this paper, we propose the concept of \textbf{Character Identity}, a multidimensional construct that disentangles a character into two distinct layers: \textbf{(1) Parametric Identity}, referring to character-specific knowledge encoded from the LLM's pre-training, and \textbf{(2) Attributive Identity}, capturing fine-grained behavioral properties such as personality traits and moral values. To systematically investigate these layers, we construct a unified character profile schema and generate both Famous and Synthetic characters under identical structural constraints. Our evaluation across single-turn and multi-turn interactions reveals two critical phenomena. First, we identify \textit{"Fame Fades"}: while famous characters hold a significant advantage in initial turns due to parametric knowledge, this edge rapidly vanishes as models prioritize accumulating conversational context over pre-trained priors. Second, we find that \textit{"Nature Remains"}: while models robustly portray general personality traits regardless of polarity, RPA performance is highly sensitive to the valence of morality and interpersonal relationships. Our findings pinpoint negative social natures as the primary bottleneck in RPA fidelity, guiding future character construction and evaluation.

</details>


### [54] [Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking](https://arxiv.org/abs/2601.04720)
*Mingxin Li,Yanzhao Zhang,Dingkun Long,Keqin Chen,Sibo Song,Shuai Bai,Zhibo Yang,Pengjun Xie,An Yang,Dayiheng Liu,Jingren Zhou,Junyang Lin*

Main category: cs.CL

TL;DR: Qwen3-VL-Embedding and Qwen3-VL-Reranker are multimodal embedding models that provide an end-to-end pipeline for high-precision multimodal search across text, images, documents, and video, achieving state-of-the-art results on benchmarks.


<details>
  <summary>Details</summary>
Motivation: To create a comprehensive multimodal search pipeline that can handle diverse modalities (text, images, documents, video) in a unified representation space, building on the Qwen3-VL foundation model to support multilingual capabilities and flexible deployment options.

Method: Two complementary model series: 1) Qwen3-VL-Embedding uses multi-stage training (contrastive pre-training to reranking distillation) with Matryoshka Representation Learning for flexible dimensions and 32k token inputs; 2) Qwen3-VL-Reranker uses cross-encoder architecture with cross-attention for fine-grained relevance estimation. Both support 30+ languages and come in 2B and 8B parameter sizes.

Result: State-of-the-art performance on multimodal embedding benchmarks, with Qwen3-VL-Embedding-8B achieving 77.8 overall score on MMEB-V2 (ranking first as of Jan 8, 2025). Effective on image-text retrieval, visual question answering, and video-text matching tasks.

Conclusion: The Qwen3-VL-Embedding and Qwen3-VL-Reranker series provide a powerful end-to-end multimodal search solution with strong multilingual support, flexible deployment options, and superior performance across diverse retrieval tasks.

Abstract: In this report, we introduce the Qwen3-VL-Embedding and Qwen3-VL-Reranker model series, the latest extensions of the Qwen family built on the Qwen3-VL foundation model. Together, they provide an end-to-end pipeline for high-precision multimodal search by mapping diverse modalities, including text, images, document images, and video, into a unified representation space. The Qwen3-VL-Embedding model employs a multi-stage training paradigm, progressing from large-scale contrastive pre-training to reranking model distillation, to generate semantically rich high-dimensional vectors. It supports Matryoshka Representation Learning, enabling flexible embedding dimensions, and handles inputs up to 32k tokens. Complementing this, Qwen3-VL-Reranker performs fine-grained relevance estimation for query-document pairs using a cross-encoder architecture with cross-attention mechanisms. Both model series inherit the multilingual capabilities of Qwen3-VL, supporting more than 30 languages, and are released in $\textbf{2B}$ and $\textbf{8B}$ parameter sizes to accommodate diverse deployment requirements. Empirical evaluations demonstrate that the Qwen3-VL-Embedding series achieves state-of-the-art results across diverse multimodal embedding evaluation benchmarks. Specifically, Qwen3-VL-Embedding-8B attains an overall score of $\textbf{77.8}$ on MMEB-V2, ranking first among all models (as of January 8, 2025). This report presents the architecture, training methodology, and practical capabilities of the series, demonstrating their effectiveness on various multimodal retrieval tasks, including image-text retrieval, visual question answering, and video-text matching.

</details>


### [55] [Automatic Classifiers Underdetect Emotions Expressed by Men](https://arxiv.org/abs/2601.04730)
*Ivan Smirnov,Segun T. Aroyehun,Paul Plener,David Garcia*

Main category: cs.CL

TL;DR: Study finds systematic gender bias in emotion detection models where error rates are consistently higher for texts authored by men compared to women across 414 model-class combinations.


<details>
  <summary>Details</summary>
Motivation: Automatic sentiment/emotion classifiers are widely adopted but typically assessed using third-party annotators rather than individuals experiencing emotions themselves, potentially concealing systematic biases. Need to ensure these tools perform reliably across different populations.

Method: Used unique large-scale dataset of over 1 million self-annotated posts with pre-registered research design to investigate gender biases in emotion detection across 414 combinations of models and emotion-related classes.

Result: Across different types of automatic classifiers and various underlying emotions, error rates are consistently higher for texts authored by men compared to those authored by women. Quantified how this bias could affect downstream applications.

Conclusion: Current machine learning tools, including large language models, should be applied with caution when gender composition of a sample is unknown or variable. Sentiment analysis is not yet a solved problem, especially in ensuring equitable model behavior across demographic groups.

Abstract: The widespread adoption of automatic sentiment and emotion classifiers makes it important to ensure that these tools perform reliably across different populations. Yet their reliability is typically assessed using benchmarks that rely on third-party annotators rather than the individuals experiencing the emotions themselves, potentially concealing systematic biases. In this paper, we use a unique, large-scale dataset of more than one million self-annotated posts and a pre-registered research design to investigate gender biases in emotion detection across 414 combinations of models and emotion-related classes. We find that across different types of automatic classifiers and various underlying emotions, error rates are consistently higher for texts authored by men compared to those authored by women. We quantify how this bias could affect results in downstream applications and show that current machine learning tools, including large language models, should be applied with caution when the gender composition of a sample is not known or variable. Our findings demonstrate that sentiment analysis is not yet a solved problem, especially in ensuring equitable model behaviour across demographic groups.

</details>


### [56] [AM$^3$Safety: Towards Data Efficient Alignment of Multi-modal Multi-turn Safety for MLLMs](https://arxiv.org/abs/2601.04736)
*Han Zhu,Jiale Chen,Chengkun Cai,Shengjie Sun,Haoran Li,Yujin Zhou,Chi-Min Chan,Pengcheng Wen,Lei Li,Sirui Han,Yike Guo*

Main category: cs.CL

TL;DR: InterSafe-V dataset and AM³Safety framework improve multi-modal LLM safety in multi-turn dialogues, reducing attack success by 10+% while maintaining helpfulness.


<details>
  <summary>Details</summary>
Motivation: MLLMs deployed in interactive applications have safety vulnerabilities in multi-turn scenarios where harmful intent can be gradually reconstructed across turns, and existing RLHF methods are limited to single-turn VQA tasks and require costly manual annotations.

Method: 1) Create InterSafe-V dataset (11,270 dialogues + 500 refusal VQA samples) through model interactions; 2) Propose AM³Safety framework combining cold-start refusal phase with Group Relative Policy Optimization (GRPO) fine-tuning using turn-aware dual-objective rewards across entire dialogues.

Result: Experiments on Qwen2.5-VL-7B-Instruct and LLaVA-NeXT-7B show >10% decrease in Attack Success Rate (ASR), ≥8% improvement in harmless dimension, and >13% improvement in helpful dimension on multi-modal multi-turn safety benchmarks while preserving general abilities.

Conclusion: The proposed InterSafe-V dataset and AM³Safety framework effectively enhance MLLM safety in multi-turn dialogues, addressing the limitations of existing single-turn alignment methods and providing scalable solutions for real-world interactive applications.

Abstract: Multi-modal Large Language Models (MLLMs) are increasingly deployed in interactive applications. However, their safety vulnerabilities become pronounced in multi-turn multi-modal scenarios, where harmful intent can be gradually reconstructed across turns, and security protocols fade into oblivion as the conversation progresses. Existing Reinforcement Learning from Human Feedback (RLHF) alignment methods are largely developed for single-turn visual question-answer (VQA) task and often require costly manual preference annotations, limiting their effectiveness and scalability in dialogues. To address this challenge, we present InterSafe-V, an open-source multi-modal dialogue dataset containing 11,270 dialogues and 500 specially designed refusal VQA samples. This dataset, constructed through interaction between several models, is designed to more accurately reflect real-world scenarios and includes specialized VQA pairs tailored for specific domains. Building on this dataset, we propose AM$^3$Safety, a framework that combines a cold-start refusal phase with Group Relative Policy Optimization (GRPO) fine-tuning using turn-aware dual-objective rewards across entire dialogues. Experiments on Qwen2.5-VL-7B-Instruct and LLaVA-NeXT-7B show more than 10\% decrease in Attack Success Rate (ASR) together with an increment of at least 8\% in harmless dimension and over 13\% in helpful dimension of MLLMs on multi-modal multi-turn safety benchmarks, while preserving their general abilities.

</details>


### [57] [RiskAtlas: Exposing Domain-Specific Risks in LLMs through Knowledge-Graph-Guided Harmful Prompt Generation](https://arxiv.org/abs/2601.04740)
*Huawei Zheng,Xinqi Jiang,Sen Yang,Shouling Ji,Yingcai Wu,Dazhen Deng*

Main category: cs.CL

TL;DR: Framework for generating implicit harmful prompts using knowledge graphs and dual-path obfuscation to create domain-specific safety datasets for LLM red-teaming.


<details>
  <summary>Details</summary>
Motivation: LLMs in specialized domains (finance, healthcare) face unique safety risks, but domain-specific harmful prompt datasets are scarce and rely on manual construction. Public datasets focus on explicit harmful prompts that modern LLM defenses can detect, while implicit harmful prompts using indirect domain knowledge are harder to detect and better reflect real-world threats.

Method: End-to-end framework with two main components: 1) Knowledge-graph-guided harmful prompt generation to systematically produce domain-relevant prompts, and 2) Dual-path obfuscation rewriting that converts explicit harmful prompts into implicit variants through direct and context-enhanced rewriting.

Result: The framework yields high-quality datasets combining strong domain relevance with implicitness, enabling more realistic red-teaming and advancing LLM safety research. Code and datasets are released on GitHub.

Conclusion: The proposed framework addresses the scarcity of domain-specific harmful prompt datasets and the limitations of explicit harmful prompts by systematically generating implicit harmful prompts using domain knowledge, providing more realistic testing scenarios for LLM safety in specialized domains.

Abstract: Large language models (LLMs) are increasingly applied in specialized domains such as finance and healthcare, where they introduce unique safety risks. Domain-specific datasets of harmful prompts remain scarce and still largely rely on manual construction; public datasets mainly focus on explicit harmful prompts, which modern LLM defenses can often detect and refuse. In contrast, implicit harmful prompts-expressed through indirect domain knowledge-are harder to detect and better reflect real-world threats. We identify two challenges: transforming domain knowledge into actionable constraints and increasing the implicitness of generated harmful prompts. To address them, we propose an end-to-end framework that first performs knowledge-graph-guided harmful prompt generation to systematically produce domain-relevant prompts, and then applies dual-path obfuscation rewriting to convert explicit harmful prompts into implicit variants via direct and context-enhanced rewriting. This framework yields high-quality datasets combining strong domain relevance with implicitness, enabling more realistic red-teaming and advancing LLM safety research. We release our code and datasets at GitHub.

</details>


### [58] [Tool-MAD: A Multi-Agent Debate Framework for Fact Verification with Diverse Tool Augmentation and Adaptive Retrieval](https://arxiv.org/abs/2601.04742)
*Seyeon Jeong,Yeonjun Choi,JongWook Kim,Beakcheol Jang*

Main category: cs.CL

TL;DR: Tool-MAD: A multi-agent debate framework where each agent uses different external tools (search API, RAG) to enhance factual verification, with adaptive query refinement and quantitative hallucination detection.


<details>
  <summary>Details</summary>
Motivation: Existing Multi-Agent Debate (MAD) systems rely on internal knowledge or static documents, making them vulnerable to hallucinations. While MADKE introduces external evidence, its one-time retrieval limits adaptability to new arguments during debates.

Method: Tool-MAD assigns each agent a distinct external tool (search API or RAG module) to encourage diverse perspectives. It features adaptive query formulation that iteratively refines evidence retrieval based on debate flow, and integrates Faithfulness and Answer Relevance scores for quantitative hallucination detection by the Judge agent.

Result: Tool-MAD outperforms state-of-the-art MAD frameworks on four fact verification benchmarks, achieving up to 5.5% accuracy improvement. It shows strong robustness and adaptability in medically specialized domains across various tool configurations.

Conclusion: Tool-MAD demonstrates superior factual verification capabilities through heterogeneous tool integration and adaptive evidence retrieval, confirming its potential for real-world fact-checking applications, especially in specialized domains.

Abstract: Large Language Models (LLMs) suffer from hallucinations and factual inaccuracies, especially in complex reasoning and fact verification tasks. Multi-Agent Debate (MAD) systems aim to improve answer accuracy by enabling multiple LLM agents to engage in dialogue, promoting diverse reasoning and mutual verification. However, existing MAD frameworks primarily rely on internal knowledge or static documents, making them vulnerable to hallucinations. While MADKE introduces external evidence to mitigate this, its one-time retrieval mechanism limits adaptability to new arguments or emerging information during the debate. To address these limitations, We propose Tool-MAD, a multi-agent debate framework that enhances factual verification by assigning each agent a distinct external tool, such as a search API or RAG module. Tool-MAD introduces three key innovations: (1) a multi-agent debate framework where agents leverage heterogeneous external tools, encouraging diverse perspectives, (2) an adaptive query formulation mechanism that iteratively refines evidence retrieval based on the flow of the debate, and (3) the integration of Faithfulness and Answer Relevance scores into the final decision process, allowing the Judge agent to quantitatively assess the coherence and question alignment of each response and effectively detect hallucinations. Experimental results on four fact verification benchmarks demonstrate that Tool-MAD consistently outperforms state-of-the-art MAD frameworks, achieving up to 5.5% accuracy improvement. Furthermore, in medically specialized domains, Tool-MAD exhibits strong robustness and adaptability across various tool configurations and domain conditions, confirming its potential for broader real-world fact-checking applications.

</details>


### [59] [PILOT-Bench: A Benchmark for Legal Reasoning in the Patent Domain with IRAC-Aligned Classification Tasks](https://arxiv.org/abs/2601.04758)
*Yehoon Jang,Chaewon Lee,Hyun-seok Min,Sungchul Choi*

Main category: cs.CL

TL;DR: PILOT-Bench is the first PTAB-centric benchmark for evaluating LLMs on structured legal reasoning in patent domain, showing significant performance gap between closed-source and open-source models.


<details>
  <summary>Details</summary>
Motivation: Current LLM applications in patent/legal practice are limited to lightweight tasks with no systematic means to evaluate structured legal reasoning capabilities in the patent domain, particularly for PTAB decision analysis.

Method: Created PILOT-Bench by aligning PTAB decisions with USPTO patent data at case-level and formalizing three IRAC-aligned classification tasks: Issue Type, Board Authorities, and Subdecision. Evaluated diverse closed-source and open-source LLMs across multiple perspectives including input-variation settings, model families, and error tendencies.

Result: Closed-source models consistently exceed 0.75 Micro-F1 score on Issue Type task, while strongest open-source model (Qwen-8B) achieves only ~0.56, revealing substantial reasoning capability gap. The benchmark enables systematic evaluation of patent-domain legal reasoning.

Conclusion: PILOT-Bench establishes foundation for systematic evaluation of patent-domain legal reasoning and points toward future improvements through dataset design and model alignment. All resources are publicly available.

Abstract: The Patent Trial and Appeal Board (PTAB) of the USPTO adjudicates thousands of ex parte appeals each year, requiring the integration of technical understanding and legal reasoning. While large language models (LLMs) are increasingly applied in patent and legal practice, their use has remained limited to lightweight tasks, with no established means of systematically evaluating their capacity for structured legal reasoning in the patent domain. In this work, we introduce PILOT-Bench, the first PTAB-centric benchmark that aligns PTAB decisions with USPTO patent data at the case-level and formalizes three IRAC-aligned classification tasks: Issue Type, Board Authorities, and Subdecision. We evaluate a diverse set of closed-source (commercial) and open-source LLMs and conduct analyses across multiple perspectives, including input-variation settings, model families, and error tendencies. Notably, on the Issue Type task, closed-source models consistently exceed 0.75 in Micro-F1 score, whereas the strongest open-source model (Qwen-8B) achieves performance around 0.56, highlighting a substantial gap in reasoning capabilities. PILOT-Bench establishes a foundation for the systematic evaluation of patent-domain legal reasoning and points toward future directions for improving LLMs through dataset design and model alignment. All data, code, and benchmark resources are available at https://github.com/TeamLab/pilot-bench.

</details>


### [60] [Differential syntactic and semantic encoding in LLMs](https://arxiv.org/abs/2601.04765)
*Santiago Acevedo,Alessandro Laio,Marco Baroni*

Main category: cs.CL

TL;DR: LLM representations encode syntax and semantics linearly; subtracting syntactic/semantic centroids reduces similarity with matched sentences, showing differential encoding patterns.


<details>
  <summary>Details</summary>
Motivation: To understand how syntactic and semantic information is encoded in the inner layer representations of large language models, specifically DeepSeek-V3, and whether these linguistic features are linearly separable.

Method: Averaging hidden-representation vectors of sentences sharing syntactic structure or meaning to create syntactic and semantic "centroids," then subtracting these centroids from sentence vectors to analyze their effects on similarity with matched sentences.

Result: Subtracting syntactic and semantic centroids strongly affects similarity with syntactically and semantically matched sentences respectively, indicating linear encoding. Cross-layer encoding profiles differ between syntax and semantics, and the two signals can be partially decoupled.

Conclusion: Syntax and semantics are at least partially linearly encoded in LLM representations, with differential encoding patterns that allow for some decoupling of these linguistic information types.

Abstract: We study how syntactic and semantic information is encoded in inner layer representations of Large Language Models (LLMs), focusing on the very large DeepSeek-V3. We find that, by averaging hidden-representation vectors of sentences sharing syntactic structure or meaning, we obtain vectors that capture a significant proportion of the syntactic and semantic information contained in the representations. In particular, subtracting these syntactic and semantic ``centroids'' from sentence vectors strongly affects their similarity with syntactically and semantically matched sentences, respectively, suggesting that syntax and semantics are, at least partially, linearly encoded. We also find that the cross-layer encoding profiles of syntax and semantics are different, and that the two signals can to some extent be decoupled, suggesting differential encoding of these two types of linguistic information in LLM representations.

</details>


### [61] [Revisiting Judge Decoding from First Principles via Training-Free Distributional Divergence](https://arxiv.org/abs/2601.04766)
*Shengyin Sun,Yiming Li,Renxi Liu,Weizhe Lin,Hui-Ling Zhen,Xianzhi Yu,Mingxuan Yuan,Chen Ma*

Main category: cs.CL

TL;DR: Judge Decoding accelerates LLM inference by relaxing strict verification of Speculative Decoding, but typically requires expensive supervision. This work shows that criticality scores are encoded in draft-target distributional divergence, proposes a training-free KL divergence verification method that matches or outperforms trained judges.


<details>
  <summary>Details</summary>
Motivation: Current Judge Decoding methods for LLM acceleration rely on expensive and noisy supervision to learn criticality scores, creating a supervision bottleneck. The authors aim to eliminate this dependency by discovering that the necessary information is already encoded in the draft-target distributional divergence.

Method: The authors theoretically prove a structural correspondence between learned linear judges and Kullback-Leibler (KL) divergence, showing they rely on the same underlying logit primitives. Based on this insight, they propose a simple, training-free verification mechanism using KL divergence between draft and target model distributions.

Result: Extensive experiments across reasoning and coding benchmarks show that the proposed KL divergence method matches or outperforms complex trained judges (e.g., AutoJudge). The method offers superior robustness to domain shifts and eliminates the supervision bottleneck entirely.

Conclusion: The work demonstrates that expensive supervision for Judge Decoding is unnecessary, as criticality scores are intrinsically encoded in distributional divergence. The proposed training-free KL divergence verification provides an effective, robust alternative that maintains performance while eliminating supervision requirements.

Abstract: Judge Decoding accelerates LLM inference by relaxing the strict verification of Speculative Decoding, yet it typically relies on expensive and noisy supervision. In this work, we revisit this paradigm from first principles, revealing that the ``criticality'' scores learned via costly supervision are intrinsically encoded in the draft-target distributional divergence. We theoretically prove a structural correspondence between learned linear judges and Kullback-Leibler (KL) divergence, demonstrating they rely on the same underlying logit primitives. Guided by this, we propose a simple, training-free verification mechanism based on KL divergence. Extensive experiments across reasoning and coding benchmarks show that our method matches or outperforms complex trained judges (e.g., AutoJudge), offering superior robustness to domain shifts and eliminating the supervision bottleneck entirely.

</details>


### [62] [LANGSAE EDITING: Improving Multilingual Information Retrieval via Post-hoc Language Identity Removal](https://arxiv.org/abs/2601.04768)
*Dongjun Kim,Jeongho Yoon,Chanjun Park,Heuiseok Lim*

Main category: cs.CL

TL;DR: LANGSAE EDITING is a post-hoc sparse autoencoder method that removes language-identity signals from multilingual embeddings to improve cross-language retrieval by suppressing language-associated latent units while maintaining compatibility with existing vector databases.


<details>
  <summary>Details</summary>
Motivation: Multilingual embeddings encode language identity alongside semantics, which artificially inflates similarity for same-language pairs and reduces effectiveness for cross-language retrieval, especially in mixed-language collections where relevant evidence may be in different languages.

Method: The method trains a sparse autoencoder on pooled embeddings, identifies language-associated latent units using cross-language activation statistics, suppresses these units at inference time, and reconstructs embeddings in the original dimensionality without retraining the base encoder or re-encoding raw text.

Result: Experiments across multiple languages show consistent improvements in ranking quality and cross-language coverage, with particularly strong gains for script-distinct languages, demonstrating effective removal of language-identity bias while maintaining semantic information.

Conclusion: LANGSAE EDITING provides a practical, post-hoc solution for improving multilingual dense retrieval by removing language-identity signals, enabling better cross-language search performance without requiring retraining of base encoders or modifications to existing vector infrastructure.

Abstract: Dense retrieval in multilingual settings often searches over mixed-language collections, yet multilingual embeddings encode language identity alongside semantics. This language signal can inflate similarity for same-language pairs and crowd out relevant evidence written in other languages. We propose LANGSAE EDITING, a post-hoc sparse autoencoder trained on pooled embeddings that enables controllable removal of language-identity signal directly in vector space. The method identifies language-associated latent units using cross-language activation statistics, suppresses these units at inference time, and reconstructs embeddings in the original dimensionality, making it compatible with existing vector databases without retraining the base encoder or re-encoding raw text. Experiments across multiple languages show consistent improvements in ranking quality and cross-language coverage, with especially strong gains for script-distinct languages.

</details>


### [63] [NC2C: Automated Convexification of Generic Non-Convex Optimization Problems](https://arxiv.org/abs/2601.04789)
*Xinyue Peng,Yanming Liu,Yihan Cang,Yuwei Zhang,Xinyi Wang,Songhang Deng,Jiannan Cao*

Main category: cs.CL

TL;DR: NC2C is an LLM-based framework that automatically transforms non-convex optimization problems into convex forms, achieving 89.3% execution rate and 76% success rate on 100 test problems.


<details>
  <summary>Details</summary>
Motivation: Non-convex optimization problems are pervasive but intractable for traditional solvers, requiring manual convexification and expert knowledge. There's a need to automate this transformation process to reduce expert dependency and enable efficient solver deployment.

Method: NC2C uses LLMs' mathematical reasoning to detect non-convex components, select optimal convexification strategies, and generate convex equivalents. It integrates symbolic reasoning, adaptive transformation techniques, iterative validation, error correction loops, and feasibility domain correction mechanisms.

Result: On a diverse dataset of 100 generic non-convex problems, NC2C achieves 89.3% execution rate and 76% success rate in producing feasible, high-quality convex transformations, significantly outperforming baseline methods.

Conclusion: NC2C successfully leverages LLMs for automated non-convex to convex transformation, reduces expert dependency, and enables efficient deployment of convex solvers for previously intractable optimization tasks.

Abstract: Non-convex optimization problems are pervasive across mathematical programming, engineering design, and scientific computing, often posing intractable challenges for traditional solvers due to their complex objective functions and constrained landscapes. To address the inefficiency of manual convexification and the over-reliance on expert knowledge, we propose NC2C, an LLM-based end-to-end automated framework designed to transform generic non-convex optimization problems into solvable convex forms using large language models. NC2C leverages LLMs' mathematical reasoning capabilities to autonomously detect non-convex components, select optimal convexification strategies, and generate rigorous convex equivalents. The framework integrates symbolic reasoning, adaptive transformation techniques, and iterative validation, equipped with error correction loops and feasibility domain correction mechanisms to ensure the robustness and validity of transformed problems. Experimental results on a diverse dataset of 100 generic non-convex problems demonstrate that NC2C achieves an 89.3\% execution rate and a 76\% success rate in producing feasible, high-quality convex transformations. This outperforms baseline methods by a significant margin, highlighting NC2C's ability to leverage LLMs for automated non-convex to convex transformation, reduce expert dependency, and enable efficient deployment of convex solvers for previously intractable optimization tasks.

</details>


### [64] [Belief in Authority: Impact of Authority in Multi-Agent Evaluation Framework](https://arxiv.org/abs/2601.04790)
*Junhyuk Choi,Jeongyoun Kwon,Heeju Kim,Haeun Cho,Hayeong Jung,Sehee Min,Bugeun Kim*

Main category: cs.CL

TL;DR: First systematic analysis of role-based authority bias in multi-agent LLM systems shows Expert and Referent power roles have stronger influence than Legitimate roles, with bias emerging from authoritative roles maintaining positions while general agents show flexibility.


<details>
  <summary>Details</summary>
Motivation: Multi-agent systems using LLMs often assign authoritative roles to boost performance, but the impact of authority bias on agent interactions remains poorly understood and underexplored.

Method: Used ChatEval for free-form multi-agent evaluation, applied French and Raven's power-based theory to classify authoritative roles into legitimate, referent, and expert types, analyzed influence across 12-turn conversations with GPT-4o and DeepSeek R1.

Result: Expert and Referent power roles exert stronger influence than Legitimate power roles. Authority bias emerges through authoritative roles consistently maintaining positions while general agents demonstrate flexibility. Clear position statements are required for authority influence - neutral responses fail to generate bias.

Conclusion: Findings provide key insights for designing multi-agent frameworks with asymmetric interaction patterns, highlighting how different types of authority influence agent dynamics in LLM-based systems.

Abstract: Multi-agent systems utilizing large language models often assign authoritative roles to improve performance, yet the impact of authority bias on agent interactions remains underexplored. We present the first systematic analysis of role-based authority bias in free-form multi-agent evaluation using ChatEval. Applying French and Raven's power-based theory, we classify authoritative roles into legitimate, referent, and expert types and analyze their influence across 12-turn conversations. Experiments with GPT-4o and DeepSeek R1 reveal that Expert and Referent power roles exert stronger influence than Legitimate power roles. Crucially, authority bias emerges not through active conformity by general agents, but through authoritative roles consistently maintaining their positions while general agents demonstrate flexibility. Furthermore, authority influence requires clear position statements, as neutral responses fail to generate bias. These findings provide key insights for designing multi-agent frameworks with asymmetric interaction patterns.

</details>


### [65] [When AI Settles Down: Late-Stage Stability as a Signature of AI-Generated Text Detection](https://arxiv.org/abs/2601.04833)
*Ke Sun,Guangsheng Bao,Han Cui,Yue Zhang*

Main category: cs.CL

TL;DR: AI-generated text shows decreasing volatility in later stages, while human writing maintains variability. Using late-stage statistics only, simple features achieve SOTA detection performance.


<details>
  <summary>Details</summary>
Motivation: Current zero-shot detection methods aggregate token-level statistics across entire sequences, missing the temporal dynamics of autoregressive generation. The authors aim to capture these dynamics to improve AI-generated text detection.

Method: Analyzed 120k+ text samples to discover Late-Stage Volatility Decay phenomenon. Proposed two simple features: Derivative Dispersion and Local Volatility, computed exclusively from late-stage statistics. No perturbation sampling or additional model access required.

Result: AI-generated text exhibits 24-32% lower volatility in second half of sequences compared to human writing. The method achieves state-of-the-art performance on EvoBench and MAGE benchmarks and shows strong complementarity with existing global methods.

Conclusion: Temporal dynamics, particularly late-stage volatility patterns, provide a powerful signal for AI-generated text detection. Simple late-stage features outperform complex methods and complement existing approaches, offering an efficient zero-shot detection solution.

Abstract: Zero-shot detection methods for AI-generated text typically aggregate token-level statistics across entire sequences, overlooking the temporal dynamics inherent to autoregressive generation. We analyze over 120k text samples and reveal Late-Stage Volatility Decay: AI-generated text exhibits rapidly stabilizing log probability fluctuations as generation progresses, while human writing maintains higher variability throughout. This divergence peaks in the second half of sequences, where AI-generated text shows 24--32\% lower volatility. Based on this finding, we propose two simple features: Derivative Dispersion and Local Volatility, which computed exclusively from late-stage statistics. Without perturbation sampling or additional model access, our method achieves state-of-the-art performance on EvoBench and MAGE benchmarks and demonstrates strong complementarity with existing global methods.

</details>


### [66] [RAAR: Retrieval Augmented Agentic Reasoning for Cross-Domain Misinformation Detection](https://arxiv.org/abs/2601.04853)
*Zhiwei Liu,Runteng Guo,Baojie Qu,Yuechen Jiang,Min Peng,Qianqian Xie,Sophia Ananiadou*

Main category: cs.CL

TL;DR: RAAR is a retrieval-augmented agentic reasoning framework for cross-domain misinformation detection that uses multi-perspective evidence retrieval and multi-agent collaboration to overcome domain generalization challenges.


<details>
  <summary>Details</summary>
Motivation: Cross-domain misinformation detection is difficult due to substantial differences in knowledge and discourse across domains. Existing methods struggle with generalization to challenging or underrepresented domains, while LLMs are limited to same-distribution data and lack systematic reasoning.

Method: RAAR retrieves multi-perspective source-domain evidence aligned with target samples' semantics, sentiment, and writing style. It uses specialized multi-agent collaboration where perspective-specific agents produce complementary analyses and a summary agent integrates them under verifier guidance. The framework applies supervised fine-tuning and reinforcement learning to train a multi-task verifier.

Result: RAAR-8b and RAAR-14b models were trained. Evaluation on three cross-domain misinformation detection tasks shows RAAR substantially enhances base model capabilities and outperforms other cross-domain methods, advanced LLMs, and LLM-based adaptation approaches.

Conclusion: RAAR effectively addresses cross-domain misinformation detection challenges through retrieval-augmented multi-agent reasoning, demonstrating superior performance over existing methods and enabling better generalization across diverse domains.

Abstract: Cross-domain misinformation detection is challenging, as misinformation arises across domains with substantial differences in knowledge and discourse. Existing methods often rely on single-perspective cues and struggle to generalize to challenging or underrepresented domains, while reasoning large language models (LLMs), though effective on complex tasks, are limited to same-distribution data. To address these gaps, we introduce RAAR, the first retrieval-augmented agentic reasoning framework for cross-domain misinformation detection. To enable cross-domain transfer beyond same-distribution assumptions, RAAR retrieves multi-perspective source-domain evidence aligned with each target sample's semantics, sentiment, and writing style. To overcome single-perspective modeling and missing systematic reasoning, RAAR constructs verifiable multi-step reasoning paths through specialized multi-agent collaboration, where perspective-specific agents produce complementary analyses and a summary agent integrates them under verifier guidance. RAAR further applies supervised fine-tuning and reinforcement learning to train a single multi-task verifier to enhance verification and reasoning capabilities. Based on RAAR, we trained the RAAR-8b and RAAR-14b models. Evaluation on three cross-domain misinformation detection tasks shows that RAAR substantially enhances the capabilities of the base models and outperforms other cross-domain methods, advanced LLMs, and LLM-based adaptation approaches. The project will be released at https://github.com/lzw108/RAAR.

</details>


### [67] [Token Maturation: Autoregressive Language Generation via Continuous Token Dynamics](https://arxiv.org/abs/2601.04854)
*Oshri Naparstek*

Main category: cs.CL

TL;DR: Continuous autoregressive language model where tokens evolve as continuous vectors over multiple steps before discretization, enabling stable deterministic generation without token-level sampling.


<details>
  <summary>Details</summary>
Motivation: Traditional autoregressive models commit to discrete tokens early, forcing uncertainty resolution through token-level sampling which leads to instability, repetition, and sensitivity to decoding heuristics.

Method: Introduces continuous autoregressive formulation where tokens are represented as continuous vectors that mature over multiple update steps before being discretized. Uses deterministic dynamical process to evolve continuous token representations, committing to discrete tokens only when representations have sufficiently converged.

Result: The maturation process alone is sufficient to produce coherent and diverse text using deterministic decoding (argmax), without reliance on token-level sampling, diffusion-style denoising, or auxiliary stabilization mechanisms.

Conclusion: First autoregressive language model that generates text by evolving continuous token representations to convergence prior to discretization, enabling stable generation without token-level sampling.

Abstract: Autoregressive language models are conventionally defined over discrete token sequences, committing to a specific token at every generation step. This early discretization forces uncertainty to be resolved through token-level sampling, often leading to instability, repetition, and sensitivity to decoding heuristics.
  In this work, we introduce a continuous autoregressive formulation of language generation in which tokens are represented as continuous vectors that \emph{mature} over multiple update steps before being discretized. Rather than sampling tokens, the model evolves continuous token representations through a deterministic dynamical process, committing to a discrete token only when the representation has sufficiently converged. Discrete text is recovered via hard decoding, while uncertainty is maintained and resolved in the continuous space.
  We show that this maturation process alone is sufficient to produce coherent and diverse text using deterministic decoding (argmax), without reliance on token-level sampling, diffusion-style denoising, or auxiliary stabilization mechanisms. Additional perturbations, such as stochastic dynamics or history smoothing, can be incorporated naturally but are not required for the model to function.
  To our knowledge, this is the first autoregressive language model that generates text by evolving continuous token representations to convergence prior to discretization, enabling stable generation without token-level sampling.

</details>


### [68] [MisSpans: Fine-Grained False Span Identification in Cross-Domain Fake News](https://arxiv.org/abs/2601.04857)
*Zhiwei Liu,Paul Thompson,Jiaqi Rong,Baojie Qu,Runteng Guo,Min Peng,Qianqian Xie,Sophia Ananiadou*

Main category: cs.CL

TL;DR: MisSpans is a new benchmark for fine-grained misinformation detection at the span level within sentences, enabling localization, categorization, and explanation of false content beyond binary true/false classification.


<details>
  <summary>Details</summary>
Motivation: Existing misinformation benchmarks use coarse binary labels at claim/paragraph level, obscuring how true and false details co-exist in single sentences and limiting interpretability for identifying specific misleading segments and their falsehood types.

Method: Created MisSpans benchmark with paired real/fake news stories, defining three tasks: MisSpansIdentity (pinpointing false spans), MisSpansType (categorizing false spans by misinformation type), and MisSpansExplanation (providing rationales). Expert annotators followed standardized guidelines with consistency checks.

Result: Evaluation of 15 LLMs (reasoning-enhanced and non-reasoning variants) in zero-shot/one-shot settings shows the challenging nature of fine-grained misinformation identification. Performance is influenced by multiple factors including model size, reasoning capabilities, and domain-specific textual features.

Conclusion: MisSpans enables fine-grained localization, nuanced characterization, and actionable explanations for misinformation, highlighting the need for deeper understanding of factors influencing LLM performance on detailed misinformation analysis tasks.

Abstract: Online misinformation is increasingly pervasive, yet most existing benchmarks and methods evaluate veracity at the level of whole claims or paragraphs using coarse binary labels, obscuring how true and false details often co-exist within single sentences. These simplifications also limit interpretability: global explanations cannot identify which specific segments are misleading or differentiate how a detail is false (e.g., distorted vs. fabricated). To address these gaps, we introduce MisSpans, the first multi-domain, human-annotated benchmark for span-level misinformation detection and analysis, consisting of paired real and fake news stories. MisSpans defines three complementary tasks: MisSpansIdentity for pinpointing false spans within sentences, MisSpansType for categorising false spans by misinformation type, and MisSpansExplanation for providing rationales grounded in identified spans. Together, these tasks enable fine-grained localisation, nuanced characterisation beyond true/false and actionable explanations. Expert annotators were guided by standardised guidelines and consistency checks, leading to high inter-annotator agreement. We evaluate 15 representative LLMs, including reasoning-enhanced and non-reasoning variants, under zero-shot and one-shot settings. Results reveal the challenging nature of fine-grained misinformation identification and analysis, and highlight the need for a deeper understanding of how performance may be influenced by multiple interacting factors, including model size and reasoning capabilities, along with domain-specific textual features. This project will be available at https://github.com/lzw108/MisSpans.

</details>


### [69] [A Navigational Approach for Comprehensive RAG via Traversal over Proposition Graphs](https://arxiv.org/abs/2601.04859)
*Maxime Delmas,Lei Xu,André Freitas*

Main category: cs.CL

TL;DR: ToPG is a novel RAG framework that uses heterogeneous proposition graphs and iterative Suggestion-Selection cycles to handle both simple factual and complex multi-hop queries effectively.


<details>
  <summary>Details</summary>
Motivation: Current RAG approaches have limitations: standard chunking-based RAG fails on complex multi-hop queries, reasoning-interleaved strategies lack global corpus awareness, and KG-based RAG underperforms on simple fact-oriented queries. There's a need for a unified approach that handles both simple and complex queries effectively.

Method: ToPG models knowledge as a heterogeneous graph containing propositions, entities, and passages. It uses iterative Suggestion-Selection cycles: Suggestion phase performs query-aware graph traversal, and Selection phase uses LLM feedback to prune irrelevant propositions and seed the next iteration.

Result: ToPG demonstrates strong performance across three QA tasks (Simple, Complex, and Abstract QA) on both accuracy- and quality-based metrics, showing effectiveness for both simple factual and complex multi-hop queries.

Conclusion: Query-aware graph traversal combined with factual granularity is critical for efficient structured RAG systems. ToPG bridges the gap between different RAG approaches by leveraging proposition graphs with iterative refinement.

Abstract: Standard RAG pipelines based on chunking excel at simple factual retrieval but fail on complex multi-hop queries due to a lack of structural connectivity. Conversely, initial strategies that interleave retrieval with reasoning often lack global corpus awareness, while Knowledge Graph (KG)-based RAG performs strongly on complex multi-hop tasks but suffers on fact-oriented single-hop queries. To bridge this gap, we propose a novel RAG framework: ToPG (Traversal over Proposition Graphs). ToPG models its knowledge base as a heterogeneous graph of propositions, entities, and passages, effectively combining the granular fact density of propositions with graph connectivity. We leverage this structure using iterative Suggestion-Selection cycles, where the Suggestion phase enables a query-aware traversal of the graph, and the Selection phase provides LLM feedback to prune irrelevant propositions and seed the next iteration. Evaluated on three distinct QA tasks (Simple, Complex, and Abstract QA), ToPG demonstrates strong performance across both accuracy- and quality-based metrics. Overall, ToPG shows that query-aware graph traversal combined with factual granularity is a critical component for efficient structured RAG systems. ToPG is available at https://github.com/idiap/ToPG.

</details>


### [70] [EvolSQL: Structure-Aware Evolution for Scalable Text-to-SQL Data Synthesis](https://arxiv.org/abs/2601.04875)
*Xuanguang Pan,Chongyang Tao,Jiayuan Bai,Jianling Gao,Zhengwei Tao,Xiansheng Zhou,Gavin Cheung,Shuai Ma*

Main category: cs.CL

TL;DR: EvolSQL is a structure-aware data synthesis framework that evolves SQL queries from seed data to create diverse, complex Text-to-SQL training datasets, outperforming existing methods with much less data.


<details>
  <summary>Details</summary>
Motivation: Current Text-to-SQL models suffer from limited training data - existing datasets are either small human-annotated corpora or LLM-synthesized data lacking structural diversity and complexity control.

Method: EvolSQL uses a two-stage process: 1) Query-SQL expansion for question diversity and schema coverage, 2) Adaptive directional evolution using six AST-based transformation operators to increase complexity across relational, predicate, aggregation, and nesting dimensions, plus execution-grounded refinement and schema-aware deduplication.

Result: A 7B model fine-tuned on EvolSQL data outperforms one trained on the much larger SynSQL dataset using only 1/18 of the data.

Conclusion: EvolSQL provides an effective structure-aware approach for synthesizing high-quality, diverse Text-to-SQL training data that significantly improves model performance with much less data than existing methods.

Abstract: Training effective Text-to-SQL models remains challenging due to the scarcity of high-quality, diverse, and structurally complex datasets. Existing methods either rely on limited human-annotated corpora, or synthesize datasets directly by simply prompting LLMs without explicit control over SQL structures, often resulting in limited structural diversity and complexity. To address this, we introduce EvolSQL, a structure-aware data synthesis framework that evolves SQL queries from seed data into richer and more semantically diverse forms. EvolSQL starts with an exploratory Query-SQL expansion to broaden question diversity and improve schema coverage, and then applies an adaptive directional evolution strategy using six atomic transformation operators derived from the SQL Abstract Syntax Tree to progressively increase query complexity across relational, predicate, aggregation, and nesting dimensions. An execution-grounded SQL refinement module and schema-aware deduplication further ensure the creation of high-quality, structurally diverse mapping pairs. Experimental results show that a 7B model fine-tuned on our data outperforms one trained on the much larger SynSQL dataset using only 1/18 of the data.

</details>


### [71] [Mind2Report: A Cognitive Deep Research Agent for Expert-Level Commercial Report Synthesis](https://arxiv.org/abs/2601.04879)
*Mingyue Cheng,Daoyu Wang,Qi Liu,Shuo Yu,Xiaoyu Tao,Yuqian Wang,Chengzhong Chu,Yu Duan,Mingkang Long,Enhong Chen*

Main category: cs.CL

TL;DR: Mind2Report is a cognitive deep research agent that emulates commercial analysts to synthesize expert-level reports from web sources, outperforming leading baselines like OpenAI and Gemini agents.


<details>
  <summary>Details</summary>
Motivation: Current deep research agents produce reports with limited quality, reliability, and coverage, which is problematic for high-stakes business decisions that require informative commercial reports synthesized from massive, noisy web sources.

Method: Mind2Report is a training-free agentic workflow that augments LLMs with dynamic memory. It follows a three-step cognitive process: 1) probes fine-grained intent, 2) searches web sources and records distilled information on the fly, and 3) iteratively synthesizes the report.

Result: Experiments using QRC-Eval (200 real-world commercial tasks) show Mind2Report outperforms leading baselines including OpenAI and Gemini deep research agents in report quality, reliability, and coverage.

Conclusion: Mind2Report demonstrates superior performance in commercial report synthesis and serves as a foundation for advancing future commercial deep research agent design, though it's a preliminary study.

Abstract: Synthesizing informative commercial reports from massive and noisy web sources is critical for high-stakes business decisions. Although current deep research agents achieve notable progress, their reports still remain limited in terms of quality, reliability, and coverage. In this work, we propose Mind2Report, a cognitive deep research agent that emulates the commercial analyst to synthesize expert-level reports. Specifically, it first probes fine-grained intent, then searches web sources and records distilled information on the fly, and subsequently iteratively synthesizes the report. We design Mind2Report as a training-free agentic workflow that augments general large language models (LLMs) with dynamic memory to support these long-form cognitive processes. To rigorously evaluate Mind2Report, we further construct QRC-Eval comprising 200 real-world commercial tasks and establish a holistic evaluation strategy to assess report quality, reliability, and coverage. Experiments demonstrate that Mind2Report outperforms leading baselines, including OpenAI and Gemini deep research agents. Although this is a preliminary study, we expect it to serve as a foundation for advancing the future design of commercial deep research agents. Our code and data are available at https://github.com/Melmaphother/Mind2Report.

</details>


### [72] [CuMA: Aligning LLMs with Sparse Cultural Values via Demographic-Aware Mixture of Adapters](https://arxiv.org/abs/2601.04885)
*Ao Sun,Xiaoyu Wang,Zhe Tan,Yu Li,Jiachen Zhu,Shu Su,Yuheng Jia*

Main category: cs.CL

TL;DR: CuMA addresses cultural alignment in LLMs by using demographic-aware routing and mixture of adapters to prevent mean collapse and preserve cultural diversity, outperforming dense models and semantic-only MoEs.


<details>
  <summary>Details</summary>
Motivation: As LLMs serve global audiences, alignment must respect cultural pluralism rather than enforce universal consensus. Dense models suffer from "Mean Collapse" when fitting conflicting value distributions, converging to generic averages that fail to represent diverse cultural groups.

Method: Proposes CuMA (Cultural Mixture of Adapters), framing alignment as a conditional capacity separation problem. Uses demographic-aware routing to internalize a Latent Cultural Topology, disentangling conflicting gradients into specialized expert subspaces via mixture of adapters.

Result: Extensive evaluations on WorldValuesBench, Community Alignment, and PRISM show CuMA achieves state-of-the-art performance, significantly outperforming dense baselines and semantic-only MoEs. Analysis confirms CuMA effectively mitigates mean collapse and preserves cultural diversity.

Conclusion: CuMA successfully addresses cultural alignment by separating conflicting cultural gradients into specialized expert subspaces, preventing mean collapse and enabling LLMs to better represent diverse cultural values while maintaining high performance.

Abstract: As Large Language Models (LLMs) serve a global audience, alignment must transition from enforcing universal consensus to respecting cultural pluralism. We demonstrate that dense models, when forced to fit conflicting value distributions, suffer from \textbf{Mean Collapse}, converging to a generic average that fails to represent diverse groups. We attribute this to \textbf{Cultural Sparsity}, where gradient interference prevents dense parameters from spanning distinct cultural modes. To resolve this, we propose \textbf{\textsc{CuMA}} (\textbf{Cu}ltural \textbf{M}ixture of \textbf{A}dapters), a framework that frames alignment as a \textbf{conditional capacity separation} problem. By incorporating demographic-aware routing, \textsc{CuMA} internalizes a \textit{Latent Cultural Topology} to explicitly disentangle conflicting gradients into specialized expert subspaces. Extensive evaluations on WorldValuesBench, Community Alignment, and PRISM demonstrate that \textsc{CuMA} achieves state-of-the-art performance, significantly outperforming both dense baselines and semantic-only MoEs. Crucially, our analysis confirms that \textsc{CuMA} effectively mitigates mean collapse, preserving cultural diversity. Our code is available at https://github.com/Throll/CuMA.

</details>


### [73] [Faithful Summarisation under Disagreement via Belief-Level Aggregation](https://arxiv.org/abs/2601.04889)
*Favour Yahdii Aghaebe,Tanefa Apekey,Elizabeth Williams,Nafise Sadat Moosavi*

Main category: cs.CL

TL;DR: A disagreement-aware summarization pipeline separates belief aggregation from language generation to better handle conflicting viewpoints, outperforming LLM-based approaches that implicitly smooth disagreements.


<details>
  <summary>Details</summary>
Motivation: Existing summarization approaches, especially LLM-based systems, tend to implicitly smooth disagreements and over-represent majority opinions, limiting faithfulness in opinion-heavy settings with genuinely conflicting viewpoints.

Method: A two-stage pipeline: first represent documents as structured belief sets and aggregate using distance-based belief merging operators that explicitly model conflict, then use LLMs only to realize aggregated beliefs as natural language summaries.

Result: While sufficiently large models can match belief-level aggregation when handled during generation, this behavior is unstable across architectures or capacities. Belief-level aggregation with simple prompting yields consistently strong disagreement-aware performance across models while maintaining fluent summaries.

Conclusion: Separating belief-level aggregation from language generation provides a more stable and faithful approach to handling disagreements in summarization, outperforming methods that attempt to handle aggregation during the generation process.

Abstract: Opinion and multi-document summarisation often involve genuinely conflicting viewpoints, yet many existing approaches, particularly LLM-based systems, implicitly smooth disagreement and over-represent majority opinions. This limits the faithfulness of generated summaries in opinion-heavy settings. We introduce a disagreement-aware synthesis pipeline that separates belief-level aggregation from language generation. Documents are first represented as structured belief sets and aggregated using distance-based belief merging operators that explicitly model conflict. Large language models are then used only to realise the aggregated beliefs as natural language summaries. We evaluate the approach across multiple model families and scales, comparing it to methods that perform explicit aggregation during generation. Our results show that while sufficiently large models can match belief-level aggregation when aggregation is handled at generation time, this behaviour is not stable across architectures or capacities. In contrast, belief-level aggregation combined with simple prompting yields consistently strong disagreement-aware performance across models, while maintaining fluent and grounded summaries.

</details>


### [74] [V-FAT: Benchmarking Visual Fidelity Against Text-bias](https://arxiv.org/abs/2601.04897)
*Ziteng Wang,Yujie He,Guanliang Li,Siqi Yang,Jiaqi Xiong,Songxiang Liu*

Main category: cs.CL

TL;DR: The paper introduces V-FAT, a diagnostic benchmark to measure text bias in MLLMs, showing that current models rely too much on linguistic shortcuts rather than genuine visual understanding.


<details>
  <summary>Details</summary>
Motivation: There's growing concern that Multimodal Large Language Models (MLLMs) rely excessively on linguistic shortcuts rather than genuine visual grounding (Text Bias), raising questions about their true visual reasoning capabilities.

Method: The authors introduce V-FAT (Visual Fidelity Against Text-bias), a diagnostic benchmark with 4,026 VQA instances across six semantic domains. They use a Three-Level Evaluation Framework that systematically increases conflict between visual evidence and textual information, and introduce Visual Robustness Score (VRS) to penalize linguistic guesses and reward true visual fidelity.

Result: Evaluation of 12 frontier MLLMs reveals that while models excel in existing benchmarks, they experience significant visual collapse under high linguistic dominance, demonstrating their over-reliance on text bias.

Conclusion: Current MLLMs suffer from significant text bias, relying more on linguistic priors than genuine visual understanding, highlighting the need for better visual grounding and evaluation methods that test true visual reasoning capabilities.

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated impressive performance on standard visual reasoning benchmarks. However, there is growing concern that these models rely excessively on linguistic shortcuts rather than genuine visual grounding, a phenomenon we term Text Bias. In this paper, we investigate the fundamental tension between visual perception and linguistic priors. We decouple the sources of this bias into two dimensions: Internal Corpus Bias, stemming from statistical correlations in pretraining, and External Instruction Bias, arising from the alignment-induced tendency toward sycophancy. To quantify this effect, we introduce V-FAT (Visual Fidelity Against Text-bias), a diagnostic benchmark comprising 4,026 VQA instances across six semantic domains. V-FAT employs a Three-Level Evaluation Framework that systematically increases the conflict between visual evidence and textual information: (L1) internal bias from atypical images, (L2) external bias from misleading instructions, and (L3) synergistic bias where both coincide. We introduce the Visual Robustness Score (VRS), a metric designed to penalize "lucky" linguistic guesses and reward true visual fidelity. Our evaluation of 12 frontier MLLMs reveals that while models excel in existing benchmarks, they experience significant visual collapse under high linguistic dominance.

</details>


### [75] [Can AI-Generated Persuasion Be Detected? Persuaficial Benchmark and AI vs. Human Linguistic Differences](https://arxiv.org/abs/2601.04925)
*Arkadiusz Modzelewski,Paweł Golik,Anna Kołos,Giovanni Da San Martino*

Main category: cs.CL

TL;DR: LLM-generated persuasion is harder to detect than human-written persuasion when subtle, though overt persuasion is easier to detect. The study introduces a multilingual benchmark and provides linguistic analysis to improve detection tools.


<details>
  <summary>Details</summary>
Motivation: LLMs can generate highly persuasive text, raising concerns about misuse for propaganda and manipulation. The central question is whether LLM-generated persuasion is more difficult to automatically detect than human-written persuasion.

Method: Categorize controllable generation approaches for producing persuasive content with LLMs, introduce Persuaficial (a high-quality multilingual benchmark covering six languages), conduct extensive empirical evaluations comparing human-authored and LLM-generated persuasive texts, and provide comprehensive linguistic analysis.

Result: Overtly persuasive LLM-generated texts are easier to detect than human-written ones, but subtle LLM-generated persuasion consistently degrades automatic detection performance. The study provides the first comprehensive linguistic analysis contrasting human and LLM-generated persuasive texts.

Conclusion: LLM-generated persuasion presents detection challenges, especially when subtle. The linguistic insights can guide development of more interpretable and robust detection tools to address concerns about LLM misuse for harmful persuasive purposes.

Abstract: Large Language Models (LLMs) can generate highly persuasive text, raising concerns about their misuse for propaganda, manipulation, and other harmful purposes. This leads us to our central question: Is LLM-generated persuasion more difficult to automatically detect than human-written persuasion? To address this, we categorize controllable generation approaches for producing persuasive content with LLMs and introduce Persuaficial, a high-quality multilingual benchmark covering six languages: English, German, Polish, Italian, French and Russian. Using this benchmark, we conduct extensive empirical evaluations comparing human-authored and LLM-generated persuasive texts. We find that although overtly persuasive LLM-generated texts can be easier to detect than human-written ones, subtle LLM-generated persuasion consistently degrades automatic detection performance. Beyond detection performance, we provide the first comprehensive linguistic analysis contrasting human and LLM-generated persuasive texts, offering insights that may guide the development of more interpretable and robust detection tools.

</details>


### [76] [GenProve: Learning to Generate Text with Fine-Grained Provenance](https://arxiv.org/abs/2601.04932)
*Jingxuan Wei,Xingyue Wang,Yanghaoyu Liao,Jie Dong,Yuchen Liu,Caijun Jia,Bihui Yu,Junnan Zhu*

Main category: cs.CL

TL;DR: Paper introduces Generation-time Fine-grained Provenance task and ReFInE dataset with expert annotations distinguishing Quotation, Compression, Inference. Proposes GenProve framework using SFT+GRPO to optimize answer fidelity and provenance correctness, outperforming 14 LLMs.


<details>
  <summary>Details</summary>
Motivation: LLMs often hallucinate, and existing citation methods are insufficient for accountability as users struggle to verify how cited sources support generated claims. Current approaches are coarse-grained and fail to distinguish between direct quotes and complex reasoning.

Method: Introduces Generation-time Fine-grained Provenance task requiring models to generate fluent answers while producing structured, sentence-level provenance triples. Presents ReFInE dataset with expert-verified annotations for Quotation, Compression, Inference. Proposes GenProve framework combining Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization (GRPO) to optimize composite reward for answer fidelity and provenance correctness.

Result: GenProve significantly outperforms 14 strong LLMs in joint evaluation. Analysis reveals a reasoning gap: models excel at surface-level quotation but struggle significantly with inference-based provenance, suggesting verifiable reasoning remains a distinct frontier challenge.

Conclusion: Generation-time fine-grained provenance is crucial for LLM accountability. The proposed approach demonstrates effectiveness but highlights that verifiable reasoning (inference-based provenance) remains a significant challenge separate from surface-level citation capabilities.

Abstract: Large language models (LLM) often hallucinate, and while adding citations is a common solution, it is frequently insufficient for accountability as users struggle to verify how a cited source supports a generated claim. Existing methods are typically coarse-grained and fail to distinguish between direct quotes and complex reasoning. In this paper, we introduce Generation-time Fine-grained Provenance, a task where models must generate fluent answers while simultaneously producing structured, sentence-level provenance triples. To enable this, we present ReFInE (Relation-aware Fine-grained Interpretability & Evidence), a dataset featuring expert verified annotations that distinguish between Quotation, Compression, and Inference. Building on ReFInE, we propose GenProve, a framework that combines Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization (GRPO). By optimizing a composite reward for answer fidelity and provenance correctness, GenProve significantly outperforms 14 strong LLMs in joint evaluation. Crucially, our analysis uncovers a reasoning gap where models excel at surface-level quotation but struggle significantly with inference-based provenance, suggesting that verifiable reasoning remains a frontier challenge distinct from surface-level citation.

</details>


### [77] [A Unified Spoken Language Model with Injected Emotional-Attribution Thinking for Human-like Interaction](https://arxiv.org/abs/2601.04960)
*Qing Wang,Zehan Li,Yaodong Song,Hongjie Chen,Jian Kang,Jie Lian,Jie Li,Yongxiang Li,Xuelong Li*

Main category: cs.CL

TL;DR: A unified spoken language model with emotional intelligence using IEAT (Injected Emotional-Attribution Thinking) that internalizes emotion-aware reasoning, achieving top performance on emotional benchmarks.


<details>
  <summary>Details</summary>
Motivation: To create emotionally intelligent spoken language models that can understand and respond to user emotions by incorporating emotional states and their underlying causes into the model's reasoning process, rather than treating emotion as explicit supervision.

Method: Proposes IEAT (Injected Emotional-Attribution Thinking) data construction strategy that incorporates user emotional states and causes into model reasoning. Uses two-stage progressive training: 1) speech-text alignment and emotional attribute modeling via self-distillation, 2) end-to-end cross-modal joint optimization for consistency between textual and spoken emotional expressions.

Result: Achieves top-ranked performance on the HumDial Emotional Intelligence benchmark across emotional trajectory modeling, emotional reasoning, and empathetic response generation under both LLM-based and human evaluations.

Conclusion: The IEAT approach successfully creates a unified spoken language model with internalized emotional intelligence, demonstrating superior performance in understanding and responding to emotional cues in spoken dialogue.

Abstract: This paper presents a unified spoken language model for emotional intelligence, enhanced by a novel data construction strategy termed Injected Emotional-Attribution Thinking (IEAT). IEAT incorporates user emotional states and their underlying causes into the model's internal reasoning process, enabling emotion-aware reasoning to be internalized rather than treated as explicit supervision. The model is trained with a two-stage progressive strategy. The first stage performs speech-text alignment and emotional attribute modeling via self-distillation, while the second stage conducts end-to-end cross-modal joint optimization to ensure consistency between textual and spoken emotional expressions. Experiments on the Human-like Spoken Dialogue Systems Challenge (HumDial) Emotional Intelligence benchmark demonstrate that the proposed approach achieves top-ranked performance across emotional trajectory modeling, emotional reasoning, and empathetic response generation under both LLM-based and human evaluations.

</details>


### [78] [Text as a Universal Interface for Transferable Personalization](https://arxiv.org/abs/2601.04963)
*Yuting Liu,Jian Guan,Jia-Nan Li,Wei Wu,Jiang-Ming Yang,Jianzhe Zhao,Guibing Guo*

Main category: cs.CL

TL;DR: The paper proposes using natural language as a universal interface for representing user preferences in LLMs, creating interpretable and transferable preference descriptions through a two-stage training framework called AlignXplore+.


<details>
  <summary>Details</summary>
Motivation: Current approaches represent user preferences as opaque, model-specific vectors that are difficult to interpret and transfer across different models and tasks. The authors want to create more transparent, interpretable, and reusable preference representations.

Method: A two-stage training framework: 1) supervised fine-tuning on high-quality synthesized data, and 2) reinforcement learning to optimize long-term utility and cross-task transferability. This framework produces AlignXplore+, a model that generates textual preference summaries.

Result: The 8B AlignXplore+ model achieves state-of-the-art performance on nine benchmarks, outperforming substantially larger open-source models. It shows strong transferability across tasks, model families, and interaction formats.

Conclusion: Natural language serves as an effective universal interface for preference representation, enabling interpretable, reusable, and evolvable preference descriptions that transfer well across different models and tasks.

Abstract: We study the problem of personalization in large language models (LLMs). Prior work predominantly represents user preferences as implicit, model-specific vectors or parameters, yielding opaque ``black-box'' profiles that are difficult to interpret and transfer across models and tasks. In contrast, we advocate natural language as a universal, model- and task-agnostic interface for preference representation. The formulation leads to interpretable and reusable preference descriptions, while naturally supporting continual evolution as new interactions are observed. To learn such representations, we introduce a two-stage training framework that combines supervised fine-tuning on high-quality synthesized data with reinforcement learning to optimize long-term utility and cross-task transferability. Based on this framework, we develop AlignXplore+, a universal preference reasoning model that generates textual preference summaries. Experiments on nine benchmarks show that our 8B model achieves state-of-the-art performanc -- outperforming substantially larger open-source models -- while exhibiting strong transferability across tasks, model families, and interaction formats.

</details>


### [79] [Learning from Mistakes: Negative Reasoning Samples Enhance Out-of-Domain Generalization](https://arxiv.org/abs/2601.04992)
*Xueyun Tian,Minghua Ma,Bingbing Xu,Nuoyan Lyu,Wei Li,Heng Dong,Zheng Chu,Yuanzhuo Wang,Huawei Shen*

Main category: cs.CL

TL;DR: Incorporating negative reasoning trajectories (incorrect final answers) into supervised fine-tuning improves out-of-domain generalization over positive-only training, with a proposed adaptive weighting method (GLOW) further enhancing performance.


<details>
  <summary>Details</summary>
Motivation: Standard SFT on CoT trajectories discards negative examples (incorrect final answers), wasting valuable supervision and exacerbating overfitting, which limits out-of-domain generalization. Negative trajectories often contain valid intermediate reasoning despite wrong final answers.

Method: Systematically analyze negative chain patterns and training dynamics, then propose Gain-based LOss Weighting (GLOW) - an adaptive scheme that rescales per-sample loss based on inter-epoch progress to efficiently leverage unfiltered trajectories.

Result: Incorporating negative trajectories yields substantial OOD generalization gains over positive-only training. GLOW achieves 5.51% OOD gain on Qwen2.5-7B and boosts MMLU from 72.82% to 76.47% as RL initialization. Negative chains moderate loss descent to mitigate overfitting and boost policy entropy by 35.67% during inference.

Conclusion: Negative reasoning trajectories provide valuable supervision for improving out-of-domain generalization in LLMs. The proposed GLOW method effectively leverages both positive and negative examples by adapting to distinctive training dynamics, offering a more efficient approach than standard positive-only SFT.

Abstract: Supervised fine-tuning (SFT) on chain-of-thought (CoT) trajectories demonstrations is a common approach for enabling reasoning in large language models. Standard practices typically only retain trajectories with correct final answers (positives) while ignoring the rest (negatives). We argue that this paradigm discards substantial supervision and exacerbates overfitting, limiting out-of-domain (OOD) generalization. Specifically, we surprisingly find that incorporating negative trajectories into SFT yields substantial OOD generalization gains over positive-only training, as these trajectories often retain valid intermediate reasoning despite incorrect final answers. To understand this effect in depth, we systematically analyze data, training dynamics, and inference behavior, identifying 22 recurring patterns in negative chains that serve a dual role: they moderate loss descent to mitigate overfitting during training and boost policy entropy by 35.67% during inference to facilitate exploration. Motivated by these observations, we further propose Gain-based LOss Weighting (GLOW), an adaptive, sample-aware scheme that exploits such distinctive training dynamics by rescaling per-sample loss based on inter-epoch progress. Empirically, GLOW efficiently leverages unfiltered trajectories, yielding a 5.51% OOD gain over positive-only SFT on Qwen2.5-7B and boosting MMLU from 72.82% to 76.47% as an RL initialization.

</details>


### [80] [Can Large Language Models Resolve Semantic Discrepancy in Self-Destructive Subcultures? Evidence from Jirai Kei](https://arxiv.org/abs/2601.05004)
*Peng Wang,Xilin Tao,Siyi Yao,Jiageng Wu,Yuntao Zou,Zhuotao Tian,Libo Qin,Dagang Li*

Main category: cs.CL

TL;DR: SAS framework improves LLM detection of self-destructive behaviors in subcultures by addressing knowledge lag and semantic misalignment challenges.


<details>
  <summary>Details</summary>
Motivation: Self-destructive behaviors are hard to diagnose, especially in subcultural groups with unique expressions. While LLMs show promise for detection, they struggle with rapidly evolving subcultural slang and nuanced expressions specific to these groups.

Method: Proposed Subcultural Alignment Solver (SAS), a multi-agent framework incorporating automatic retrieval and subculture alignment to enhance LLM performance in detecting self-destructive behaviors within subcultures.

Result: SAS outperforms current advanced multi-agent framework OWL and competes well with fine-tuned LLMs, showing significant improvement in detection performance.

Conclusion: SAS advances self-destructive behavior detection in subcultural contexts and serves as a valuable resource for future research in this challenging domain.

Abstract: Self-destructive behaviors are linked to complex psychological states and can be challenging to diagnose. These behaviors may be even harder to identify within subcultural groups due to their unique expressions. As large language models (LLMs) are applied across various fields, some researchers have begun exploring their application for detecting self-destructive behaviors. Motivated by this, we investigate self-destructive behavior detection within subcultures using current LLM-based methods. However, these methods have two main challenges: (1) Knowledge Lag: Subcultural slang evolves rapidly, faster than LLMs' training cycles; and (2) Semantic Misalignment: it is challenging to grasp the specific and nuanced expressions unique to subcultures. To address these issues, we proposed Subcultural Alignment Solver (SAS), a multi-agent framework that incorporates automatic retrieval and subculture alignment, significantly enhancing the performance of LLMs in detecting self-destructive behavior. Our experimental results show that SAS outperforms the current advanced multi-agent framework OWL. Notably, it competes well with fine-tuned LLMs. We hope that SAS will advance the field of self-destructive behavior detection in subcultural contexts and serve as a valuable resource for future researchers.

</details>


### [81] [Hán Dān Xué Bù (Mimicry) or Qīng Chū Yú Lán (Mastery)? A Cognitive Perspective on Reasoning Distillation in Large Language Models](https://arxiv.org/abs/2601.05019)
*Yueqing Hu,Xinyang Peng,Shuting Peng,Hanqi Wang,Tianhong Wang*

Main category: cs.CL

TL;DR: Reasoning distillation via supervised fine-tuning fails to transfer cognitive structure from teacher models, causing functional alignment collapse where students mimic linguistic form but not resource allocation policies.


<details>
  <summary>Details</summary>
Motivation: Large Reasoning Models trained via RL show natural alignment with human cognitive costs, but current reasoning distillation methods may fail to preserve this cognitive structure, potentially leading to superficial mimicry rather than genuine understanding.

Method: Tested the "Hán Dān Xué Bù" (Superficial Mimicry) hypothesis across 14 models, comparing teacher models trained via reinforcement learning with student models distilled via Supervised Fine-Tuning (SFT). Analyzed alignment with human difficulty scaling and cognitive structure transfer.

Result: Teacher models show strong alignment with human difficulty scaling (r̄=0.64), but distilled students significantly degrade this alignment (r̄=0.34), often underperforming their own pre-distillation baselines (Negative Transfer). SFT induces a "Cargo Cult" effect where students replicate linguistic form without internalizing dynamic resource allocation.

Conclusion: Reasoning distillation via SFT decouples computational cost from cognitive demand, revealing that human-like cognition emerges from active reinforcement learning rather than passive imitation. Current distillation methods fail to transfer the cognitive structure that makes teacher models naturally aligned with human reasoning.

Abstract: Recent Large Reasoning Models trained via reinforcement learning exhibit a "natural" alignment with human cognitive costs. However, we show that the prevailing paradigm of reasoning distillation -- training student models to mimic these traces via Supervised Fine-Tuning (SFT) -- fails to transmit this cognitive structure. Testing the "Hán Dān Xué Bù" (Superficial Mimicry) hypothesis across 14 models, we find that distillation induces a "Functional Alignment Collapse": while teacher models mirror human difficulty scaling ($\bar{r}=0.64$), distilled students significantly degrade this alignment ($\bar{r}=0.34$), often underperforming their own pre-distillation baselines ("Negative Transfer"). Our analysis suggests that SFT induces a "Cargo Cult" effect, where students ritualistically replicate the linguistic form of reasoning (verbosity) without internalizing the teacher's dynamic resource allocation policy. Consequently, reasoning distillation decouples computational cost from cognitive demand, revealing that human-like cognition is an emergent property of active reinforcement, not passive imitation.

</details>


### [82] [ArcAligner: Adaptive Recursive Aligner for Compressed Context Embeddings in RAG](https://arxiv.org/abs/2601.05038)
*Jianbo Li,Yi Jiang,Sendong Zhao,Bairui Hu,Haochun Wang,Bing Qin*

Main category: cs.CL

TL;DR: ArcAligner is a lightweight adaptive module that helps LLMs better understand highly compressed context in RAG systems, improving performance on knowledge-intensive QA tasks while maintaining efficiency.


<details>
  <summary>Details</summary>
Motivation: RAG helps LLMs stay accurate but feeding long documents makes models slow and expensive. While context compression techniques exist, they create a trade-off: more compression leads to worse model understanding of the context.

Method: ArcAligner is a lightweight module integrated into language model layers that uses adaptive recursive context alignment. It employs an adaptive gating system that only adds extra processing power when information is complex, keeping the system fast.

Result: ArcAligner consistently beats compression baselines at comparable compression rates across knowledge-intensive QA benchmarks, especially on multi-hop and long-tail settings.

Conclusion: The proposed ArcAligner module effectively addresses the compression-understanding trade-off in RAG systems, enabling better utilization of highly compressed context representations while maintaining computational efficiency.

Abstract: Retrieval-Augmented Generation (RAG) helps LLMs stay accurate, but feeding long documents into a prompt makes the model slow and expensive. This has motivated context compression, ranging from token pruning and summarization to embedding-based compression. While researchers have tried ''compressing'' these documents into smaller summaries or mathematical embeddings, there is a catch: the more you compress the data, the more the LLM struggles to understand it. To address this challenge, we propose ArcAligner (Adaptive recursive context *Aligner*), a lightweight module integrated into the language model layers to help the model better utilize highly compressed context representations for downstream generation. It uses an adaptive ''gating'' system that only adds extra processing power when the information is complex, keeping the system fast. Across knowledge-intensive QA benchmarks, ArcAligner consistently beats compression baselines at comparable compression rates, especially on multi-hop and long-tail settings. The source code is publicly available.

</details>


### [83] [Compositional Steering of Large Language Models with Steering Tokens](https://arxiv.org/abs/2601.05062)
*Gorjan Radevski,Kiril Gashteovski,Giwon Hong,Carolin Lawrence,Goran Glavaš*

Main category: cs.CL

TL;DR: Compositional steering tokens enable multi-behavior control in LLMs by embedding behaviors as input tokens that can be combined, generalizing to unseen compositions.


<details>
  <summary>Details</summary>
Motivation: Real-world LLM applications require controllable outputs satisfying multiple desiderata simultaneously, but existing work only addresses single-behavior steering, leaving compositional steering (steering towards multiple behaviors at once) underexplored.

Method: Propose compositional steering tokens: 1) Embed individual behaviors (natural language instructions) into dedicated tokens via self-distillation, 2) Train a composition token on behavior pairs that captures compositionality, enabling generalization to unseen compositions including unseen behaviors and numbers of behaviors.

Result: Steering tokens achieve superior multi-behavior control compared to competing approaches (instructions, activation steering, LoRA merging) across different LLM architectures. They complement natural language instructions, with combined use yielding further improvements.

Conclusion: Compositional steering tokens provide an effective approach for multi-behavior LLM control that operates in input token space rather than activation space, enabling zero-shot composition and generalization to unseen behavior combinations.

Abstract: Deploying LLMs in real-world applications requires controllable output that satisfies multiple desiderata at the same time. While existing work extensively addresses LLM steering for a single behavior, \textit{compositional steering} -- i.e., steering LLMs simultaneously towards multiple behaviors -- remains an underexplored problem. In this work, we propose \emph{compositional steering tokens} for multi-behavior steering. We first embed individual behaviors, expressed as natural language instructions, into dedicated tokens via self-distillation. Contrary to most prior work, which operates in the activation space, our behavior steers live in the space of input tokens, enabling more effective zero-shot composition. We then train a dedicated \textit{composition token} on pairs of behaviors and show that it successfully captures the notion of composition: it generalizes well to \textit{unseen} compositions, including those with unseen behaviors as well as those with an unseen \textit{number} of behaviors. Our experiments across different LLM architectures show that steering tokens lead to superior multi-behavior control compared to competing approaches (instructions, activation steering, and LoRA merging). Moreover, we show that steering tokens complement natural language instructions, with their combination resulting in further gains.

</details>


### [84] [SemPA: Improving Sentence Embeddings of Large Language Models through Semantic Preference Alignment](https://arxiv.org/abs/2601.05075)
*Ziyang Chen,Zhenxuan Huang,Yile Wang,Weiqin Wang,Lu Yin,Hui Huang*

Main category: cs.CL

TL;DR: SemPA is a novel method that improves sentence embeddings in LLMs using semantic preference alignment without compromising generative capabilities.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based embedding methods either use fixed prompts (limited performance) or modify model architecture (compromises generative ability). Need a method that enhances semantic representations while preserving LLMs' generative capacity.

Method: Uses sentence-level Direct Preference Optimization (DPO) on paraphrase generation tasks to align semantic preferences. Theoretically connects DPO to contrastive learning under Plackett-Luce model framework.

Result: Achieves better semantic representations on semantic textual similarity tasks and various LLM benchmarks without sacrificing inherent generation capability.

Conclusion: SemPA successfully boosts sentence representations while preserving LLMs' generative abilities through semantic preference alignment, offering an effective alternative to existing embedding methods.

Abstract: Traditional sentence embedding methods employ token-level contrastive learning on non-generative pre-trained models. Recently, there have emerged embedding methods based on generative large language models (LLMs). These methods either rely on fixed prompt templates or involve modifications to the model architecture. The former lacks further optimization of the model and results in limited performance, while the latter alters the internal computational mechanisms of the model, thereby compromising its generative capabilities. We propose SemPA, a novel approach that boosts the sentence representations while preserving the generative ability of LLMs via semantic preference alignment. We leverage sentence-level Direct Preference Optimization (DPO) to efficiently optimize LLMs on a paraphrase generation task, where the model learns to discriminate semantically equivalent sentences while preserving inherent generative capacity. Theoretically, we establish a formal connection between DPO and contrastive learning under the Plackett-Luce model framework. Empirically, experimental results on both semantic textual similarity tasks and various benchmarks for LLMs show that SemPA achieves better semantic representations without sacrificing the inherent generation capability of LLMs.

</details>


### [85] [Code-Mix Sentiment Analysis on Hinglish Tweets](https://arxiv.org/abs/2601.05091)
*Aashi Garg,Aneshya Das,Arshi Arya,Anushka Goyal,Aditi*

Main category: cs.CL

TL;DR: Proposes a sentiment classification framework for Hinglish tweets using fine-tuned mBERT with subword tokenization to handle code-mixed language challenges in Indian social media.


<details>
  <summary>Details</summary>
Motivation: Traditional NLP models fail to interpret Hinglish (Hindi-English hybrid) in user-generated content, leading to inaccurate sentiment analysis and misleading market insights for brand monitoring in India.

Method: Fine-tunes mBERT (Multilingual BERT) with subword tokenization to handle spelling variations, slang, and out-of-vocabulary terms in Romanized Hinglish tweets.

Result: Develops a production-ready AI solution for brand sentiment tracking and establishes a benchmark for multilingual NLP in low-resource, code-mixed environments.

Conclusion: The framework effectively addresses the challenges of Hinglish sentiment analysis, providing accurate brand monitoring solutions for Indian social media platforms.

Abstract: The effectiveness of brand monitoring in India is increasingly challenged by the rise of Hinglish--a hybrid of Hindi and English--used widely in user-generated content on platforms like Twitter. Traditional Natural Language Processing (NLP) models, built for monolingual data, often fail to interpret the syntactic and semantic complexity of this code-mixed language, resulting in inaccurate sentiment analysis and misleading market insights. To address this gap, we propose a high-performance sentiment classification framework specifically designed for Hinglish tweets. Our approach fine-tunes mBERT (Multilingual BERT), leveraging its multilingual capabilities to better understand the linguistic diversity of Indian social media. A key component of our methodology is the use of subword tokenization, which enables the model to effectively manage spelling variations, slang, and out-of-vocabulary terms common in Romanized Hinglish. This research delivers a production-ready AI solution for brand sentiment tracking and establishes a strong benchmark for multilingual NLP in low-resource, code-mixed environments.

</details>


### [86] [How Human is AI? Examining the Impact of Emotional Prompts on Artificial and Human and Responsiveness](https://arxiv.org/abs/2601.05104)
*Florence Bernays,Marco Henriques Pereira,Jochen Menges*

Main category: cs.CL

TL;DR: Emotional tone in human-AI interactions affects ChatGPT's performance and subsequent human-human communication - praise improves ChatGPT most, anger moderately, blame doesn't help but affects ethical priorities.


<details>
  <summary>Details</summary>
Motivation: To understand how emotional expressions in human-AI interactions influence both AI behavior (ChatGPT responses) and subsequent human-human communication patterns.

Method: Between-subject experiment where participants expressed specific emotions (praise, anger, blame, neutral) while working with ChatGPT (GPT-4.0) on two tasks: writing a public response and addressing an ethical dilemma.

Result: Praise led to greatest ChatGPT improvement; anger showed moderate improvement; blame showed no improvement. For ethical dilemmas: anger reduced corporate interest prioritization, blame increased public interest protection. Negative emotional expressions in human-AI interactions carried over to more hostile human-human communication.

Conclusion: Emotional tone in human-AI interactions significantly shapes both AI outputs and subsequent human communication patterns, demonstrating emotional carry-over effects between human-AI and human-human interactions.

Abstract: This research examines how the emotional tone of human-AI interactions shapes ChatGPT and human behavior. In a between-subject experiment, we asked participants to express a specific emotion while working with ChatGPT (GPT-4.0) on two tasks, including writing a public response and addressing an ethical dilemma. We found that compared to interactions where participants maintained a neutral tone, ChatGPT showed greater improvement in its answers when participants praised ChatGPT for its responses. Expressing anger towards ChatGPT also led to a higher albeit smaller improvement relative to the neutral condition, whereas blaming ChatGPT did not improve its answers. When addressing an ethical dilemma, ChatGPT prioritized corporate interests less when participants expressed anger towards it, while blaming increases its emphasis on protecting the public interest. Additionally, we found that people used more negative, hostile, and disappointing expressions in human-human communication after interactions during which participants blamed rather than praised for their responses. Together, our findings demonstrate that the emotional tone people apply in human-AI interactions not only shape ChatGPT's outputs but also carry over into subsequent human-human communication.

</details>


### [87] [Agent-as-a-Judge](https://arxiv.org/abs/2601.05111)
*Runyang You,Hongru Cai,Caiqi Zhang,Qiancheng Xu,Meng Liu,Tiezheng Yu,Yongqi Li,Wenjie Li*

Main category: cs.CL

TL;DR: Survey paper tracing evolution from LLM-as-a-Judge to Agent-as-a-Judge for AI evaluation, establishing taxonomy and roadmap for agentic evaluation systems.


<details>
  <summary>Details</summary>
Motivation: LLM-as-a-Judge faces limitations with complex, specialized, multi-step evaluands due to biases, shallow reasoning, and inability to verify against real-world observations. The field lacks unified framework for emerging Agent-as-a-Judge paradigm.

Method: Comprehensive survey identifying key dimensions of paradigm shift, establishing developmental taxonomy, organizing core methodologies, and surveying applications across general and professional domains.

Result: First comprehensive survey of the transition to Agent-as-a-Judge, providing unified framework, taxonomy, and analysis of frontier challenges in agentic evaluation systems.

Conclusion: Agent-as-a-Judge enables more robust, verifiable, and nuanced evaluations through planning, tool-augmented verification, multi-agent collaboration, and persistent memory, with clear roadmap for next-generation agentic evaluation.

Abstract: LLM-as-a-Judge has revolutionized AI evaluation by leveraging large language models for scalable assessments. However, as evaluands become increasingly complex, specialized, and multi-step, the reliability of LLM-as-a-Judge has become constrained by inherent biases, shallow single-pass reasoning, and the inability to verify assessments against real-world observations. This has catalyzed the transition to Agent-as-a-Judge, where agentic judges employ planning, tool-augmented verification, multi-agent collaboration, and persistent memory to enable more robust, verifiable, and nuanced evaluations. Despite the rapid proliferation of agentic evaluation systems, the field lacks a unified framework to navigate this shifting landscape. To bridge this gap, we present the first comprehensive survey tracing this evolution. Specifically, we identify key dimensions that characterize this paradigm shift and establish a developmental taxonomy. We organize core methodologies and survey applications across general and professional domains. Furthermore, we analyze frontier challenges and identify promising research directions, ultimately providing a clear roadmap for the next generation of agentic evaluation.

</details>


### [88] [DocDancer: Towards Agentic Document-Grounded Information Seeking](https://arxiv.org/abs/2601.05163)
*Qintong Zhang,Xinjie Lv,Jialong Wu,Baixuan Li,Zhengwei Tao,Guochen Yan,Huanyao Zhang,Bin Wang,Jiahao Xu,Haitao Mi,Wentao Zhang*

Main category: cs.CL

TL;DR: DocDancer: An end-to-end trained open-source document QA agent with tool-driven framework for document exploration and comprehension, using synthetic data for training.


<details>
  <summary>Details</summary>
Motivation: Existing DocQA agents lack effective tool utilization and rely heavily on closed-source models, creating a need for open-source solutions with better document exploration capabilities.

Method: Proposes DocDancer with tool-driven agent framework for document exploration and comprehension, using Exploration-then-Synthesis data synthesis pipeline to generate training data for end-to-end training.

Result: Trained models achieve effectiveness on two long-context document understanding benchmarks (MMLongBench-Doc and DocBench), with analysis providing insights for agentic tool design and synthetic data.

Conclusion: DocDancer demonstrates successful open-source DocQA agent development with tool-driven framework and synthetic data approach, offering valuable design insights for future work.

Abstract: Document Question Answering (DocQA) focuses on answering questions grounded in given documents, yet existing DocQA agents lack effective tool utilization and largely rely on closed-source models. In this work, we introduce DocDancer, an end-to-end trained open-source Doc agent. We formulate DocQA as an information-seeking problem and propose a tool-driven agent framework that explicitly models document exploration and comprehension. To enable end-to-end training of such agents, we introduce an Exploration-then-Synthesis data synthesis pipeline that addresses the scarcity of high-quality training data for DocQA. Training on the synthesized data, the trained models on two long-context document understanding benchmarks, MMLongBench-Doc and DocBench, show their effectiveness. Further analysis provides valuable insights for the agentic tool design and synthetic data.

</details>


### [89] [RelayLLM: Efficient Reasoning via Collaborative Decoding](https://arxiv.org/abs/2601.05167)
*Chengsong Huang,Tong Zheng,Langlin Huang,Jinyuan Li,Haolin Liu,Jiaxin Huang*

Main category: cs.CL

TL;DR: RelayLLM enables efficient reasoning by having SLMs dynamically invoke LLMs only for critical tokens via token-level collaborative decoding, achieving near-LLM performance with minimal LLM usage.


<details>
  <summary>Details</summary>
Motivation: Current collaborative approaches between LLMs and SLMs are inefficient - they offload entire queries to LLMs even when SLMs can handle most reasoning steps, causing computational waste and high costs.

Method: RelayLLM framework with token-level collaborative decoding where SLM acts as controller, invoking LLM only for critical tokens via special commands. Uses two-stage training: warm-up and Group Relative Policy Optimization (GRPO) to balance independence and strategic help-seeking.

Result: Achieves 49.52% average accuracy across six benchmarks, bridging performance gap between SLM and LLM while invoking LLM for only 1.07% of total generated tokens, offering 98.2% cost reduction vs performance-matched random routers.

Conclusion: RelayLLM provides an efficient token-level collaboration framework that dramatically reduces computational costs while maintaining reasoning performance, making LLM-assisted reasoning more practical and scalable.

Abstract: Large Language Models (LLMs) for complex reasoning is often hindered by high computational costs and latency, while resource-efficient Small Language Models (SLMs) typically lack the necessary reasoning capacity. Existing collaborative approaches, such as cascading or routing, operate at a coarse granularity by offloading entire queries to LLMs, resulting in significant computational waste when the SLM is capable of handling the majority of reasoning steps. To address this, we propose RelayLLM, a novel framework for efficient reasoning via token-level collaborative decoding. Unlike routers, RelayLLM empowers the SLM to act as an active controller that dynamically invokes the LLM only for critical tokens via a special command, effectively "relaying" the generation process. We introduce a two-stage training framework, including warm-up and Group Relative Policy Optimization (GRPO) to teach the model to balance independence with strategic help-seeking. Empirical results across six benchmarks demonstrate that RelayLLM achieves an average accuracy of 49.52%, effectively bridging the performance gap between the two models. Notably, this is achieved by invoking the LLM for only 1.07% of the total generated tokens, offering a 98.2% cost reduction compared to performance-matched random routers.

</details>


### [90] [Reverse-engineering NLI: A study of the meta-inferential properties of Natural Language Inference](https://arxiv.org/abs/2601.05170)
*Rasmus Blanck,Bill Noble,Stergios Chatzikyriakidis*

Main category: cs.CL

TL;DR: The paper analyzes the logical properties of Natural Language Inference (NLI) tasks, examining three possible interpretations of NLI labels and evaluating model consistency on meta-inferential properties using SNLI data.


<details>
  <summary>Details</summary>
Motivation: NLI is important for evaluating language models' natural language understanding, but its logical properties are poorly understood and often mischaracterized. Understanding what type of inference NLI actually captures is crucial for properly interpreting model performance on this task.

Method: The authors formulate three possible readings of NLI labels and analyze their meta-inferential properties. They use SNLI dataset items with shared premises and items generated by LLMs to evaluate models trained on SNLI for meta-inferential consistency.

Result: The analysis provides insights into which specific reading of the logical relations is actually encoded by the SNLI dataset, revealing how NLI labels should be interpreted logically.

Conclusion: By clarifying the logical properties of NLI and identifying which interpretation is encoded in the dataset, the research helps properly interpret model performance on NLI tasks and advances understanding of what inference means in natural language understanding evaluation.

Abstract: Natural Language Inference (NLI) has been an important task for evaluating language models for Natural Language Understanding, but the logical properties of the task are poorly understood and often mischaracterized. Understanding the notion of inference captured by NLI is key to interpreting model performance on the task. In this paper we formulate three possible readings of the NLI label set and perform a comprehensive analysis of the meta-inferential properties they entail. Focusing on the SNLI dataset, we exploit (1) NLI items with shared premises and (2) items generated by LLMs to evaluate models trained on SNLI for meta-inferential consistency and derive insights into which reading of the logical relations is encoded by the dataset.

</details>


### [91] [Inside Out: Evolving User-Centric Core Memory Trees for Long-Term Personalized Dialogue Systems](https://arxiv.org/abs/2601.05171)
*Jihao Zhao,Ding Chen,Zhaoxin Fan,Kerun Xu,Mengting Hu,Bo Tang,Feiyu Xiong,Zhiyu li*

Main category: cs.CL

TL;DR: Inside Out framework uses PersonaTree for long-term user profiling with structured memory operations via lightweight MemListener, outperforming existing methods in noise suppression and persona consistency.


<details>
  <summary>Details</summary>
Motivation: Existing long-term personalized dialogue systems struggle with memory noise accumulation, reasoning degradation, and persona inconsistency due to unbounded interactions and finite context constraints.

Method: Proposes Inside Out framework with globally maintained PersonaTree for user profiling, trained lightweight MemListener via RL with process-based rewards to produce structured operations (ADD, UPDATE, DELETE, NO_OP), and dual response modes: direct PersonaTree enhancement for latency-sensitive scenarios and agentic mode for detailed on-demand information.

Result: PersonaTree outperforms full-text concatenation and various personalized memory systems in suppressing contextual noise and maintaining persona consistency. MemListener achieves memory-operation decision performance comparable to or surpassing powerful reasoning models like DeepSeek-R1-0528 and Gemini-3-Pro.

Conclusion: The Inside Out framework with PersonaTree and lightweight MemListener effectively addresses long-term personalized dialogue challenges by enabling controllable memory growth, structured operations, and dual response modes while maintaining efficiency and consistency.

Abstract: Existing long-term personalized dialogue systems struggle to reconcile unbounded interaction streams with finite context constraints, often succumbing to memory noise accumulation, reasoning degradation, and persona inconsistency. To address these challenges, this paper proposes Inside Out, a framework that utilizes a globally maintained PersonaTree as the carrier of long-term user profiling. By constraining the trunk with an initial schema and updating the branches and leaves, PersonaTree enables controllable growth, achieving memory compression while preserving consistency. Moreover, we train a lightweight MemListener via reinforcement learning with process-based rewards to produce structured, executable, and interpretable {ADD, UPDATE, DELETE, NO_OP} operations, thereby supporting the dynamic evolution of the personalized tree. During response generation, PersonaTree is directly leveraged to enhance outputs in latency-sensitive scenarios; when users require more details, the agentic mode is triggered to introduce details on-demand under the constraints of the PersonaTree. Experiments show that PersonaTree outperforms full-text concatenation and various personalized memory systems in suppressing contextual noise and maintaining persona consistency. Notably, the small MemListener model achieves memory-operation decision performance comparable to, or even surpassing, powerful reasoning models such as DeepSeek-R1-0528 and Gemini-3-Pro.

</details>


### [92] [LELA: an LLM-based Entity Linking Approach with Zero-Shot Domain Adaptation](https://arxiv.org/abs/2601.05192)
*Samy Haffoudhi,Fabian M. Suchanek,Nils Holzenberger*

Main category: cs.CL

TL;DR: LELA is a modular coarse-to-fine entity linking method using LLMs without fine-tuning, achieving competitive performance with fine-tuned approaches across various settings.


<details>
  <summary>Details</summary>
Motivation: Entity linking is crucial for knowledge graph construction, QA, and information extraction, but existing methods often require fine-tuning for specific domains/knowledge bases, limiting flexibility and requiring extensive training data.

Method: LELA uses a modular coarse-to-fine approach leveraging LLMs, working across different domains, knowledge bases, and LLMs without any fine-tuning phase.

Result: LELA is highly competitive with fine-tuned approaches and substantially outperforms non-fine-tuned methods across various entity linking settings.

Conclusion: LELA provides an effective, flexible entity linking solution that eliminates the need for fine-tuning while maintaining competitive performance with fine-tuned approaches.

Abstract: Entity linking (mapping ambiguous mentions in text to entities in a knowledge base) is a foundational step in tasks such as knowledge graph construction, question-answering, and information extraction. Our method, LELA, is a modular coarse-to-fine approach that leverages the capabilities of large language models (LLMs), and works with different target domains, knowledge bases and LLMs, without any fine-tuning phase. Our experiments across various entity linking settings show that LELA is highly competitive with fine-tuned approaches, and substantially outperforms the non-fine-tuned ones.

</details>


### [93] [Measuring and Fostering Peace through Machine Learning and Artificial Intelligence](https://arxiv.org/abs/2601.05232)
*P. Gilda,P. Dungarwal,A. Thongkham,E. T. Ajayi,S. Choudhary,T. M. Terol,C. Lam,J. P. Araujo,M. McFadyen-Mungalln,L. S. Liebovitch,P. T. Coleman,H. West,K. Sieck,S. Carter*

Main category: cs.CL

TL;DR: Researchers used AI to measure peace levels from news/social media and created MirrorMirror, a Chrome extension that gives real-time feedback on media peacefulness to promote more respectful communication.


<details>
  <summary>Details</summary>
Motivation: To address the problem that 71% of young adults get news from emotionally-charged social media videos designed for engagement, and to move beyond simple engagement metrics toward promoting peaceful communication.

Method: 1) Used neural networks on text embeddings to measure peace in news media; 2) Developed models for social media using word-level (GoEmotions) and context-level (LLM) methods; 3) Created MirrorMirror Chrome extension for real-time peacefulness feedback on YouTube.

Result: News peace measurement model showed high accuracy across different datasets; Social media models successfully measured peace-related dimensions; MirrorMirror extension developed and tested for real-time feedback.

Conclusion: The work demonstrates AI can effectively measure media peacefulness, and tools like MirrorMirror can help users understand their media diet, with the long-term goal of creating open-source tools for more respectful, nuanced communication.

Abstract: We used machine learning and artificial intelligence: 1) to measure levels of peace in countries from news and social media and 2) to develop on-line tools that promote peace by helping users better understand their own media diet. For news media, we used neural networks to measure levels of peace from text embeddings of on-line news sources. The model, trained on one news media dataset also showed high accuracy when used to analyze a different news dataset. For social media, such as YouTube, we developed other models to measure levels of social dimensions important in peace using word level (GoEmotions) and context level (Large Language Model) methods. To promote peace, we note that 71% of people 20-40 years old daily view most of their news through short videos on social media. Content creators of these videos are biased towards creating videos with emotional activation, making you angry to engage you, to increase clicks. We developed and tested a Chrome extension, MirrorMirror, which provides real-time feedback to YouTube viewers about the peacefulness of the media they are watching. Our long term goal is for MirrorMirror to evolve into an open-source tool for content creators, journalists, researchers, platforms, and individual users to better understand the tone of their media creation and consumption and its effects on viewers. Moving beyond simple engagement metrics, we hope to encourage more respectful, nuanced, and informative communication.

</details>


### [94] [GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization](https://arxiv.org/abs/2601.05242)
*Shih-Yang Liu,Xin Dong,Ximing Lu,Shizhe Diao,Peter Belcak,Mingjie Liu,Min-Hung Chen,Hongxu Yin,Yu-Chiang Frank Wang,Kwang-Ting Cheng,Yejin Choi,Jan Kautz,Pavlo Molchanov*

Main category: cs.CL

TL;DR: GDPO improves multi-reward RL by decoupling reward normalization, preventing collapse of distinct rewards into identical advantages that causes suboptimal convergence in GRPO.


<details>
  <summary>Details</summary>
Motivation: As language models need to align with diverse human preferences, multi-reward RL pipelines are used but GRPO causes distinct reward combinations to collapse into identical advantages, reducing training signal resolution and causing suboptimal convergence or early failure.

Method: Introduces Group reward-Decoupled Normalization Policy Optimization (GDPO), which decouples normalization of individual rewards to preserve their relative differences, enabling more accurate multi-reward optimization with improved training stability.

Result: GDPO consistently outperforms GRPO across three tasks (tool calling, math reasoning, coding reasoning) in both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length).

Conclusion: GDPO is an effective and generalizable method for multi-reward reinforcement learning optimization that addresses fundamental limitations of GRPO in preserving distinct reward signals.

Abstract: As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [95] [Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes](https://arxiv.org/abs/2601.04300)
*Chenye Meng,Zejian Li,Zhongni Liu,Yize Li,Changle Xie,Kaixin Jia,Ling Yang,Huanghuang Deng,Shiying Ding,Shengyuan Zhang,Jiayi Li,Lingyun Sun*

Main category: cs.CV

TL;DR: CPO (Complex Preference Optimization) aligns diffusion models with hierarchical, fine-grained human expertise using a two-stage approach: SFT on auxiliary model, then DPO extension for non-binary criteria.


<details>
  <summary>Details</summary>
Motivation: Current post-training alignment uses oversimplified signals (scalar rewards or binary preferences), which fail to capture complex hierarchical human expertise with fine-grained attributes.

Method: Two-stage framework: 1) Construct hierarchical evaluation criteria with domain experts (tree structure of positive/negative attributes), 2) SFT on auxiliary diffusion model, 3) CPO extends DPO to align target diffusion with non-binary hierarchical criteria by maximizing positive attribute probability while minimizing negative attribute probability using auxiliary model.

Result: CPO significantly enhances generation quality and alignment with expertise in painting generation domain, outperforming existing alignment methods that use simplified signals.

Conclusion: CPO opens new avenues for fine-grained criteria alignment by enabling diffusion models to align with complex hierarchical human expertise rather than oversimplified signals.

Abstract: Post-training alignment of diffusion models relies on simplified signals, such as scalar rewards or binary preferences. This limits alignment with complex human expertise, which is hierarchical and fine-grained. To address this, we first construct a hierarchical, fine-grained evaluation criteria with domain experts, which decomposes image quality into multiple positive and negative attributes organized in a tree structure. Building on this, we propose a two-stage alignment framework. First, we inject domain knowledge to an auxiliary diffusion model via Supervised Fine-Tuning. Second, we introduce Complex Preference Optimization (CPO) that extends DPO to align the target diffusion to our non-binary, hierarchical criteria. Specifically, we reformulate the alignment problem to simultaneously maximize the probability of positive attributes while minimizing the probability of negative attributes with the auxiliary diffusion. We instantiate our approach in the domain of painting generation and conduct CPO training with an annotated dataset of painting with fine-grained attributes based on our criteria. Extensive experiments demonstrate that CPO significantly enhances generation quality and alignment with expertise, opening new avenues for fine-grained criteria alignment.

</details>


### [96] [Embedding Textual Information in Images Using Quinary Pixel Combinations](https://arxiv.org/abs/2601.04302)
*A V Uday Kiran Kandala*

Main category: cs.CV

TL;DR: A novel steganography method using quinary pixel intensity combinations in RGB space to embed text into images with minimal distortion and improved efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing text embedding methods (LSB, MSB, PVD, transform domain, deep learning) often create noise, are computationally heavy, and require multiple pixels per character. There's a need for more efficient, less distorting single-pixel encoding.

Method: Uses quinary combinations of pixel intensities in RGB space - five controlled intensity variations in each R, G, B channel create 125 distinct combinations. These combinations are mapped to textual symbols (letters, numbers, whitespace, special characters).

Result: No significant distortion in images (verified by MSE, MAE, SNR, PSNR, SSIM, Histogram Comparison, Heatmap analysis). Achieves improved embedding efficiency by encoding complete textual symbols within single RGB pixels.

Conclusion: The proposed quinary pixel intensity combination method provides efficient, low-distortion text embedding in images, outperforming traditional LSB/MSB and complex deep learning approaches in computational efficiency and embedding density.

Abstract: This paper presents a novel technique for embedding textual data into images using quinary combinations of pixel intensities in RGB space. Existing methods predominantly rely on least and most significant bit (LSB & MSB) manipulation, Pixel Value Differencing (PVD), spatial perturbations in RGB channels, transform domain based methods, Quantization methods, Edge and Region based methods and more recently through deep learning methods and generative AI techniques for hiding textual information in spatial domain of images. Most of them are dependent on pixel intensity flipping over multiple pixels, such as LSB and combination of LSB based methodologies, and on transform coefficients, often resulting in the form of noise. Encoding and Decoding are deterministic in most of the existing approaches and are computationally heavy in case of higher models such as deep learning and gen AI approaches. The proposed method works on quinary pixel intensity combinations in RGB space, where five controlled different pixel intensity variations in each of the R, G, and B channels formulate up to one hundred and twenty five distinct pixel intensity combinations. These combinations are mapped to textual symbols, enabling the representation of uppercase and lowercase alphabetic characters, numeric digits, whitespace, and commonly used special characters. Different metrics such as MSE, MAE, SNR, PSNR, SSIM, Histogram Comparison and Heatmap analysis, were evaluated for both original and encoded images resulting in no significant distortion in the images. Furthermore, the method achieves improved embedding efficiency by encoding a complete textual symbol within a single RGB pixel, in contrast to LSB and MSB based approaches that typically require multiple pixels or multi-step processes, as well as transform and learning based methods that incur higher computational overhead.

</details>


### [97] [Unified Text-Image Generation with Weakness-Targeted Post-Training](https://arxiv.org/abs/2601.04339)
*Jiahui Chen,Philippe Hansen-Estruch,Xiaochuang Han,Yushi Hu,Emily Dinan,Amita Kamath,Michal Drozdzal,Reyhane Askari-Hemmat,Luke Zettlemoyer,Marjan Ghazvininejad*

Main category: cs.CV

TL;DR: Unified multimodal generation architectures that jointly produce text and images through a single inference process, using reward-weighted post-training with self-generated synthetic data to improve text-to-image synthesis across multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal generation systems rely on explicit modality switching (generating reasoning text first, then manually switching to image generation), which limits cross-modal coupling and prohibits automatic multimodal generation. The sequential inference process prevents seamless integration between text and image generation.

Method: Post-training approach to achieve fully unified text-image generation where models autonomously transition from textual reasoning to visual synthesis within a single inference process. Uses offline, reward-weighted post-training with fully self-generated synthetic data, exploring different post-training data strategies including targeted datasets addressing specific limitations.

Result: The approach enables improvements in multimodal image generation across four diverse T2I benchmarks. Demonstrates effectiveness of reward-weighting both modalities and strategically designed post-training data. Targeted datasets addressing specific limitations achieve superior results compared to broad image-caption corpora or benchmark-aligned data.

Conclusion: Fully unified text-image generation through post-training with reward-weighted synthetic data is effective for improving multimodal generation performance, with strategic data design being crucial for optimal results.

Abstract: Unified multimodal generation architectures that jointly produce text and images have recently emerged as a promising direction for text-to-image (T2I) synthesis. However, many existing systems rely on explicit modality switching, generating reasoning text before switching manually to image generation. This separate, sequential inference process limits cross-modal coupling and prohibits automatic multimodal generation. This work explores post-training to achieve fully unified text-image generation, where models autonomously transition from textual reasoning to visual synthesis within a single inference process. We examine the impact of joint text-image generation on T2I performance and the relative importance of each modality during post-training. We additionally explore different post-training data strategies, showing that a targeted dataset addressing specific limitations achieves superior results compared to broad image-caption corpora or benchmark-aligned data. Using offline, reward-weighted post-training with fully self-generated synthetic data, our approach enables improvements in multimodal image generation across four diverse T2I benchmarks, demonstrating the effectiveness of reward-weighting both modalities and strategically designed post-training data.

</details>


### [98] [ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers](https://arxiv.org/abs/2601.04342)
*Mohsen Ghafoorian,Amirhossein Habibian*

Main category: cs.CV

TL;DR: ReHyAt introduces a recurrent hybrid attention mechanism combining softmax and linear attention for efficient video diffusion models, reducing training cost by 99% while maintaining state-of-the-art quality.


<details>
  <summary>Details</summary>
Motivation: Current transformer-based video diffusion models suffer from quadratic attention complexity, limiting scalability for longer video sequences and requiring massive computational resources for training.

Method: ReHyAt uses a recurrent hybrid attention mechanism that combines softmax attention fidelity with linear attention efficiency, enabling chunk-wise recurrent formulation with constant memory usage. It includes a lightweight distillation pipeline from existing softmax-based models.

Result: Reduces training cost by two orders of magnitude (~160 GPU hours vs. typical massive requirements), achieves state-of-the-art quality on VBench and VBench-2.0, and reduces attention complexity from quadratic to linear.

Conclusion: ReHyAt provides a practical solution for scalable long-duration and on-device video generation while maintaining quality, with a distillation recipe applicable to future bidirectional softmax-based models.

Abstract: Recent advances in video diffusion models have shifted towards transformer-based architectures, achieving state-of-the-art video generation but at the cost of quadratic attention complexity, which severely limits scalability for longer sequences. We introduce ReHyAt, a Recurrent Hybrid Attention mechanism that combines the fidelity of softmax attention with the efficiency of linear attention, enabling chunk-wise recurrent reformulation and constant memory usage. Unlike the concurrent linear-only SANA Video, ReHyAt's hybrid design allows efficient distillation from existing softmax-based models, reducing the training cost by two orders of magnitude to ~160 GPU hours, while being competitive in the quality. Our light-weight distillation and finetuning pipeline provides a recipe that can be applied to future state-of-the-art bidirectional softmax-based models. Experiments on VBench and VBench-2.0, as well as a human preference study, demonstrate that ReHyAt achieves state-of-the-art video quality while reducing attention cost from quadratic to linear, unlocking practical scalability for long-duration and on-device video generation. Project page is available at https://qualcomm-ai-research.github.io/rehyat.

</details>


### [99] [SCAR-GS: Spatial Context Attention for Residuals in Progressive Gaussian Splatting](https://arxiv.org/abs/2601.04348)
*Diego Revilla,Pooja Suresh,Anand Bhojan,Ooi Wei Tsang*

Main category: cs.CV

TL;DR: Progressive 3D Gaussian Splatting compression using Residual Vector Quantization with auto-regressive entropy modeling for better rate-distortion performance.


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting models have high storage requirements that hinder cloud/streaming deployment. Existing progressive compression methods using scalar quantization may not optimally capture high-dimensional feature correlations, limiting rate-distortion performance.

Method: Introduces a progressive codec that replaces traditional methods with Residual Vector Quantization to compress primitive features. Uses an auto-regressive entropy model guided by a multi-resolution hash grid to predict conditional probabilities of transmitted indices for efficient compression of coarse and refinement layers.

Result: Not specified in the abstract (paper likely demonstrates improved compression efficiency and rate-distortion performance compared to existing methods).

Conclusion: The proposed Residual Vector Quantization approach with auto-regressive entropy modeling provides more efficient progressive compression for 3D Gaussian Splatting models, addressing storage limitations for cloud and streaming applications.

Abstract: Recent advances in 3D Gaussian Splatting have allowed for real-time, high-fidelity novel view synthesis. Nonetheless, these models have significant storage requirements for large and medium-sized scenes, hindering their deployment over cloud and streaming services. Some of the most recent progressive compression techniques for these models rely on progressive masking and scalar quantization techniques to reduce the bitrate of Gaussian attributes using spatial context models. While effective, scalar quantization may not optimally capture the correlations of high-dimensional feature vectors, which can potentially limit the rate-distortion performance.
  In this work, we introduce a novel progressive codec for 3D Gaussian Splatting that replaces traditional methods with a more powerful Residual Vector Quantization approach to compress the primitive features. Our key contribution is an auto-regressive entropy model, guided by a multi-resolution hash grid, that accurately predicts the conditional probability of each successive transmitted index, allowing for coarse and refinement layers to be compressed with high efficiency.

</details>


### [100] [Comparative Analysis of Custom CNN Architectures versus Pre-trained Models and Transfer Learning: A Study on Five Bangladesh Datasets](https://arxiv.org/abs/2601.04352)
*Ibrahim Tanvir,Alif Ruslan,Sartaj Solaiman*

Main category: cs.CV

TL;DR: Custom CNNs vs pre-trained models (ResNet-18/VGG-16) on 5 Bangladesh image datasets show transfer learning with fine-tuning consistently outperforms custom CNNs and feature extraction, with up to 76% accuracy improvements.


<details>
  <summary>Details</summary>
Motivation: To provide practical guidance for practitioners on selecting appropriate deep learning approaches by comparing custom-built CNNs against pre-trained architectures across diverse real-world image classification tasks from Bangladesh.

Method: Comparative analysis using custom CNNs vs pre-trained ResNet-18 and VGG-16 with both feature extraction and transfer learning approaches, evaluated across five diverse Bangladesh image datasets: Footpath Vision, Auto Rickshaw Detection, Mango Image Classification, Paddy Variety Recognition, and Road Damage Detection.

Result: Transfer learning with fine-tuning consistently outperformed both custom CNNs and feature extraction, achieving accuracy improvements of 3-76% across datasets. ResNet-18 with fine-tuning achieved 100% accuracy on Road Damage BD dataset. Custom CNNs had smaller model size (3.4M vs 11-134M parameters) and better training efficiency on simpler tasks.

Conclusion: Pre-trained models with transfer learning provide superior performance, especially for complex tasks with limited data, while custom CNNs offer advantages in model size and training efficiency for simpler tasks. Selection should consider dataset characteristics, computational resources, and performance requirements.

Abstract: This study presents a comprehensive comparative analysis of custom-built Convolutional Neural Networks (CNNs) against popular pre-trained architectures (ResNet-18 and VGG-16) using both feature extraction and transfer learning approaches. We evaluated these models across five diverse image classification datasets from Bangladesh: Footpath Vision, Auto Rickshaw Detection, Mango Image Classification, Paddy Variety Recognition, and Road Damage Detection. Our experimental results demonstrate that transfer learning with fine-tuning consistently outperforms both custom CNNs built from scratch and feature extraction methods, achieving accuracy improvements ranging from 3% to 76% across different datasets. Notably, ResNet-18 with fine-tuning achieved perfect 100% accuracy on the Road Damage BD dataset. While custom CNNs offer advantages in model size (3.4M parameters vs. 11-134M for pre-trained models) and training efficiency on simpler tasks, pre-trained models with transfer learning provide superior performance, particularly on complex classification tasks with limited training data. This research provides practical insights for practitioners in selecting appropriate deep learning approaches based on dataset characteristics, computational resources, and performance requirements.

</details>


### [101] [PackCache: A Training-Free Acceleration Method for Unified Autoregressive Video Generation via Compact KV-Cache](https://arxiv.org/abs/2601.04359)
*Kunyang Li,Mubarak Shah,Yuzhang Shang*

Main category: cs.CV

TL;DR: PackCache is a training-free KV-cache management method that dynamically compacts KV cache for unified autoregressive video generation, achieving 1.7-2.2x speedup by exploiting spatiotemporal attention patterns.


<details>
  <summary>Details</summary>
Motivation: KV-cache size in unified autoregressive models grows linearly with generated tokens, becoming the dominant bottleneck for inference efficiency and generative length, especially problematic for video generation where sequences are long.

Method: PackCache uses three coordinated mechanisms: condition anchoring (preserves semantic reference tokens), cross-frame decay modeling (allocates cache budget by temporal distance), and spatially preserving position embedding (maintains 3D structure under cache removal).

Result: Achieves 1.7-2.2x end-to-end generation acceleration on 48-frame sequences, with 2.6x and 3.7x acceleration on the final four frames (most expensive segment) on A40 and H200 respectively.

Conclusion: PackCache effectively addresses KV-cache bottleneck in unified autoregressive video generation by exploiting spatiotemporal attention properties, enabling longer-sequence video generation with significant efficiency gains.

Abstract: A unified autoregressive model is a Transformer-based framework that addresses diverse multimodal tasks (e.g., text, image, video) as a single sequence modeling problem under a shared token space. Such models rely on the KV-cache mechanism to reduce attention computation from O(T^2) to O(T); however, KV-cache size grows linearly with the number of generated tokens, and it rapidly becomes the dominant bottleneck limiting inference efficiency and generative length. Unified autoregressive video generation inherits this limitation. Our analysis reveals that KV-cache tokens exhibit distinct spatiotemporal properties: (i) text and conditioning-image tokens act as persistent semantic anchors that consistently receive high attention, and (ii) attention to previous frames naturally decays with temporal distance. Leveraging these observations, we introduce PackCache, a training-free KV-cache management method that dynamically compacts the KV cache through three coordinated mechanisms: condition anchoring that preserves semantic references, cross-frame decay modeling that allocates cache budget according to temporal distance, and spatially preserving position embedding that maintains coherent 3D structure under cache removal. In terms of efficiency, PackCache accelerates end-to-end generation by 1.7-2.2x on 48-frame long sequences, showcasing its strong potential for enabling longer-sequence video generation. Notably, the final four frames - the portion most impacted by the progressively expanding KV-cache and thus the most expensive segment of the clip - PackCache delivers a 2.6x and 3.7x acceleration on A40 and H200, respectively, for 48-frame videos.

</details>


### [102] [Combining facial videos and biosignals for stress estimation during driving](https://arxiv.org/abs/2601.04376)
*Paraskevi Valergaki,Vassilis C. Nicodemou,Iason Oikonomidis,Antonis Argyros,Anastasios Roussos*

Main category: cs.CV

TL;DR: 3D facial geometry from EMOCA effectively captures stress responses during distracted driving, with cross-modal attention fusion achieving 92% AUROC for stress recognition.


<details>
  <summary>Details</summary>
Motivation: Stress recognition from facial videos is challenging due to subjectivity and voluntary control. Existing methods rely on Facial Action Units, but disentangled 3D facial geometry remains underexplored for stress analysis.

Method: Analyzed stress during distracted driving using EMOCA-derived 3D expression and pose coefficients. Used paired hypothesis tests between baseline and stressor phases. Proposed Transformer-based temporal modeling with unimodal, early-fusion, and cross-modal attention strategies.

Result: 41 of 56 EMOCA coefficients showed consistent, phase-specific stress responses comparable to physiological markers. Cross-Modal Attention fusion of EMOCA and physiological signals achieved best performance (AUROC 92%, Accuracy 86.7%). EMOCA-gaze fusion also performed well (AUROC 91.8%).

Conclusion: 3D facial geometry effectively captures stress responses, and temporal modeling with cross-modal attention significantly improves stress recognition performance, highlighting the value of multimodal fusion approaches.

Abstract: Reliable stress recognition from facial videos is challenging due to stress's subjective nature and voluntary facial control. While most methods rely on Facial Action Units, the role of disentangled 3D facial geometry remains underexplored. We address this by analyzing stress during distracted driving using EMOCA-derived 3D expression and pose coefficients. Paired hypothesis tests between baseline and stressor phases reveal that 41 of 56 coefficients show consistent, phase-specific stress responses comparable to physiological markers. Building on this, we propose a Transformer-based temporal modeling framework and assess unimodal, early-fusion, and cross-modal attention strategies. Cross-Modal Attention fusion of EMOCA and physiological signals achieves best performance (AUROC 92\%, Accuracy 86.7\%), with EMOCA-gaze fusion also competitive (AUROC 91.8\%). This highlights the effectiveness of temporal modeling and cross-modal attention for stress recognition.

</details>


### [103] [Few-Shot LoRA Adaptation of a Flow-Matching Foundation Model for Cross-Spectral Object Detection](https://arxiv.org/abs/2601.04381)
*Maxim Clouser,Kia Khezeli,John Kalantari*

Main category: cs.CV

TL;DR: A flow-matching foundation model pre-trained on RGB images can be adapted with few-shot LoRA to translate RGB to IR/SAR, and the synthetic data improves downstream object detection in non-visible modalities.


<details>
  <summary>Details</summary>
Motivation: Foundation models are mainly trained on RGB data, but safety-critical applications often use non-visible modalities like IR and SAR. There's a need to bridge this gap without extensive retraining.

Method: Fine-tune FLUX.1 Kontext with LoRA modules using only 100 paired images per domain (RGB→IR on KAIST, RGB→SAR on M4-SAR). Use LPIPS on 50 held-out pairs as proxy for downstream performance.

Result: Lower LPIPS consistently predicts higher mAP for object detection. Synthetic IR improves KAIST pedestrian detection, and synthetic SAR boosts M4-SAR infrastructure detection when combined with limited real data.

Conclusion: Few-shot LoRA adaptation of flow-matching foundation models is promising for extending foundation-style capabilities to non-visible modalities with minimal data requirements.

Abstract: Foundation models for vision are predominantly trained on RGB data, while many safety-critical applications rely on non-visible modalities such as infrared (IR) and synthetic aperture radar (SAR). We study whether a single flow-matching foundation model pre-trained primarily on RGB images can be repurposed as a cross-spectral translator using only a few co-measured examples, and whether the resulting synthetic data can enhance downstream detection. Starting from FLUX.1 Kontext, we insert low-rank adaptation (LoRA) modules and fine-tune them on just 100 paired images per domain for two settings: RGB to IR on the KAIST dataset and RGB to SAR on the M4-SAR dataset. The adapted model translates RGB images into pixel-aligned IR/SAR, enabling us to reuse existing bounding boxes and train object detection models purely in the target modality. Across a grid of LoRA hyperparameters, we find that LPIPS computed on only 50 held-out pairs is a strong proxy for downstream performance: lower LPIPS consistently predicts higher mAP for YOLOv11n on both IR and SAR, and for DETR on KAIST IR test data. Using the best LPIPS-selected LoRA adapter, synthetic IR from external RGB datasets (LLVIP, FLIR ADAS) improves KAIST IR pedestrian detection, and synthetic SAR significantly boosts infrastructure detection on M4-SAR when combined with limited real SAR. Our results suggest that few-shot LoRA adaptation of flow-matching foundation models is a promising path toward foundation-style support for non-visible modalities.

</details>


### [104] [Performance Analysis of Image Classification on Bangladeshi Datasets](https://arxiv.org/abs/2601.04397)
*Mohammed Sami Khan,Fabiha Muniat,Rowzatul Zannat*

Main category: cs.CV

TL;DR: Custom CNN vs pre-trained models (VGG-16, ResNet-50, MobileNet) for image classification: Pre-trained models outperform custom CNN in accuracy and convergence, especially with limited data, but custom CNN has fewer parameters and lower computational complexity.


<details>
  <summary>Details</summary>
Motivation: To compare the practical trade-offs between designing custom CNNs from scratch versus using established pre-trained architectures for image classification tasks, addressing the important consideration of model selection in real-world applications.

Method: Comparative analysis of a custom-designed CNN trained from scratch against pre-trained architectures (VGG-16, ResNet-50, MobileNet) using transfer learning. All models evaluated under identical experimental settings with standard metrics: accuracy, precision, recall, and F1-score.

Result: Pre-trained CNN architectures consistently outperform custom CNN in classification accuracy and convergence speed, particularly with limited training data. However, custom CNN achieves competitive performance with significantly fewer parameters and reduced computational complexity.

Conclusion: The study highlights trade-offs between model complexity, performance, and computational efficiency, providing practical insights for selecting appropriate CNN architectures based on specific requirements and constraints in image classification problems.

Abstract: Convolutional Neural Networks (CNNs) have demonstrated remarkable success in image classification tasks; however, the choice between designing a custom CNN from scratch and employing established pre-trained architectures remains an important practical consideration. In this work, we present a comparative analysis of a custom-designed CNN and several widely used deep learning architectures, including VGG-16, ResNet-50, and MobileNet, for an image classification task. The custom CNN is developed and trained from scratch, while the popular architectures are employed using transfer learning under identical experimental settings. All models are evaluated using standard performance metrics such as accuracy, precision, recall, and F1-score. Experimental results show that pre-trained CNN architectures consistently outperform the custom CNN in terms of classification accuracy and convergence speed, particularly when training data is limited. However, the custom CNN demonstrates competitive performance with significantly fewer parameters and reduced computational complexity. This study highlights the trade-offs between model complexity, performance, and computational efficiency, and provides practical insights into selecting appropriate CNN architectures for image classification problems.

</details>


### [105] [3D-Agent:Tri-Modal Multi-Agent Collaboration for Scalable 3D Object Annotation](https://arxiv.org/abs/2601.04404)
*Jusheng Zhang,Yijia Fan,Zimo Wen,Jian Wang,Keze Wang*

Main category: cs.CV

TL;DR: Tri MARF: A tri-modal multi-agent framework for 3D object annotation that integrates 2D images, text descriptions, and 3D point clouds to overcome spatial complexity and occlusion challenges.


<details>
  <summary>Details</summary>
Motivation: 3D object annotation is challenging due to spatial complexity, occlusion, and viewpoint inconsistency, especially for applications in autonomous driving, robotics, and augmented reality. Existing single-model approaches struggle with these issues effectively.

Method: Tri MARF integrates tri-modal inputs (2D multi-view images, textual descriptions, and 3D point clouds) using a multi-agent collaborative architecture with three specialized agents: 1) Vision-language model agent for generating multi-view descriptions, 2) Information aggregation agent for selecting optimal descriptions, and 3) Gating agent that aligns textual semantics with 3D geometry for refined captioning.

Result: Extensive experiments on Objaverse, LVIS, Objaverse XL, and ABO datasets show Tri MARF substantially outperforms existing methods: achieves CLIPScore of 88.7 (vs prior SOTA), retrieval accuracy of 45.2 and 43.8 on ViLT R@5, and throughput of up to 12,000 objects per hour on a single NVIDIA A100 GPU.

Conclusion: Tri MARF demonstrates that integrating tri-modal inputs within a multi-agent collaborative architecture effectively addresses the challenges of 3D object annotation, achieving superior performance and efficiency compared to existing approaches.

Abstract: Driven by applications in autonomous driving robotics and augmented reality 3D object annotation presents challenges beyond 2D annotation including spatial complexity occlusion and viewpoint inconsistency Existing approaches based on single models often struggle to address these issues effectively We propose Tri MARF a novel framework that integrates tri modal inputs including 2D multi view images textual descriptions and 3D point clouds within a multi agent collaborative architecture to enhance large scale 3D annotation Tri MARF consists of three specialized agents a vision language model agent for generating multi view descriptions an information aggregation agent for selecting optimal descriptions and a gating agent that aligns textual semantics with 3D geometry for refined captioning Extensive experiments on Objaverse LVIS Objaverse XL and ABO demonstrate that Tri MARF substantially outperforms existing methods achieving a CLIPScore of 88 point 7 compared to prior state of the art methods retrieval accuracy of 45 point 2 and 43 point 8 on ViLT R at 5 and a throughput of up to 12000 objects per hour on a single NVIDIA A100 GPU

</details>


### [106] [From Preoperative CT to Postmastoidectomy Mesh Construction:1Mastoidectomy Shape Prediction for Cochlear Implant Surgery](https://arxiv.org/abs/2601.04405)
*Yike Zhang,Eduardo Davalos,Dingjie Su,Ange Lou,Jack Noble*

Main category: cs.CV

TL;DR: A hybrid self-supervised and weakly-supervised learning framework predicts mastoidectomy shape from preoperative CT scans without human annotations, achieving 0.72 Dice score for cochlear implant surgical planning.


<details>
  <summary>Details</summary>
Motivation: Accurate mastoidectomy shape prediction from preoperative imaging improves cochlear implant surgical planning, reduces risks, and enhances outcomes, but limited deep learning studies exist due to challenges in acquiring ground-truth labels.

Method: Proposes a hybrid self-supervised and weakly-supervised learning framework that predicts mastoidectomy region directly from preoperative CT scans without human annotations, leveraging 3D T-distribution loss in weakly-supervised medical imaging.

Result: Achieves mean Dice score of 0.72 for predicting complex and boundary-less mastoidectomy shape, surpassing state-of-the-art approaches and demonstrating strong performance.

Conclusion: First work integrating self-supervised and weakly-supervised learning for mastoidectomy shape prediction, providing groundwork for constructing 3D postmastoidectomy surfaces from preoperative CT scans for robust CI surgical planning.

Abstract: Cochlear Implant (CI) surgery treats severe hearing loss by inserting an electrode array into the cochlea to stimulate the auditory nerve. An important step in this procedure is mastoidectomy, which removes part of the mastoid region of the temporal bone to provide surgical access. Accurate mastoidectomy shape prediction from preoperative imaging improves pre-surgical planning, reduces risks, and enhances surgical outcomes. Despite its importance, there are limited deep-learning-based studies regarding this topic due to the challenges of acquiring ground-truth labels. We address this gap by investigating self-supervised and weakly-supervised learning models to predict the mastoidectomy region without human annotations. We propose a hybrid self-supervised and weakly-supervised learning framework to predict the mastoidectomy region directly from preoperative CT scans, where the mastoid remains intact. Our hybrid method achieves a mean Dice score of 0.72 when predicting the complex and boundary-less mastoidectomy shape, surpassing state-of-the-art approaches and demonstrating strong performance. The method provides groundwork for constructing 3D postmastoidectomy surfaces directly from the corresponding preoperative CT scans. To our knowledge, this is the first work that integrating self-supervised and weakly-supervised learning for mastoidectomy shape prediction, offering a robust and efficient solution for CI surgical planning while leveraging 3D T-distribution loss in weakly-supervised medical imaging.

</details>


### [107] [CRUNet-MR-Univ: A Foundation Model for Diverse Cardiac MRI Reconstruction](https://arxiv.org/abs/2601.04428)
*Donghang Lyu,Marius Staring,Hildo Lamb,Mariya Doneva*

Main category: cs.CV

TL;DR: CRUNet-MR-Univ is a foundation model for Cardiac MRI reconstruction that uses spatio-temporal correlations and prompt-based priors to handle diverse CMR scenarios, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current deep learning methods for CMR reconstruction lack generalizability across diverse clinical scenarios due to variability in image contrast, sampling patterns, scanner vendors, anatomical structures, and disease types. Most models only handle narrow subsets of these variations, leading to performance degradation with distribution shifts.

Method: Proposes CRUNet-MR-Univ, a foundation model that leverages spatio-temporal correlations and prompt-based priors to effectively handle the full diversity of CMR scans across different variations.

Result: The approach consistently outperforms baseline methods across a wide range of settings, demonstrating its effectiveness and promise for real-world clinical applications.

Conclusion: CRUNet-MR-Univ represents a significant advancement in CMR reconstruction by providing a unified model capable of generalizing across diverse clinical scenarios, addressing the limitations of current specialized models.

Abstract: In recent years, deep learning has attracted increasing attention in the field of Cardiac MRI (CMR) reconstruction due to its superior performance over traditional methods, particularly in handling higher acceleration factors, highlighting its potential for real-world clinical applications. However, current deep learning methods remain limited in generalizability. CMR scans exhibit wide variability in image contrast, sampling patterns, scanner vendors, anatomical structures, and disease types. Most existing models are designed to handle only a single or narrow subset of these variations, leading to performance degradation when faced with distribution shifts. Therefore, it is beneficial to develop a unified model capable of generalizing across diverse CMR scenarios. To this end, we propose CRUNet-MR-Univ, a foundation model that leverages spatio-temporal correlations and prompt-based priors to effectively handle the full diversity of CMR scans. Our approach consistently outperforms baseline methods across a wide range of settings, highlighting its effectiveness and promise.

</details>


### [108] [Addressing Overthinking in Large Vision-Language Models via Gated Perception-Reasoning Optimization](https://arxiv.org/abs/2601.04442)
*Xingjian Diao,Zheyuan Liu,Chunhui Zhang,Weiyi Wu,Keyi Kong,Lin Shi,Kaize Ding,Soroush Vosoughi,Jiang Gui*

Main category: cs.CV

TL;DR: GPRO is a meta-reasoning controller that dynamically routes computation among three paths (fast, perception, reasoning) to address overthinking in LVLMs by fixing visual perception failures, improving both accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Current chain-of-thought approaches in LVLMs lead to overthinking - excessively verbose responses for simple queries causing inefficiency and degraded accuracy. Prior adaptive reasoning methods overlook the fundamental bottleneck of visual perception failures, where reasoning errors often originate from imperfect perception rather than insufficient deliberation.

Method: Proposes Gated Perception-Reasoning Optimization (GPRO), a meta-reasoning controller that dynamically routes computation at each generation step among: 1) lightweight fast path, 2) slow perception path for re-examining visual inputs, and 3) slow reasoning path for internal self-reflection. Uses teacher models to derive large-scale failure attribution supervision from ~790k samples to distinguish perceptual hallucinations from reasoning errors, then trains controller with multi-objective reinforcement learning to optimize accuracy vs. computational cost trade-off.

Result: Experiments on five benchmarks demonstrate GPRO substantially improves both accuracy and efficiency, outperforming recent slow-thinking methods while generating significantly shorter responses.

Conclusion: Stable reasoning in LVLMs critically depends on low-level visual grounding, and addressing visual perception failures through dynamic routing mechanisms like GPRO can effectively mitigate overthinking while improving both performance and computational efficiency.

Abstract: Large Vision-Language Models (LVLMs) have exhibited strong reasoning capabilities through chain-of-thought mechanisms that generate step-by-step rationales. However, such slow-thinking approaches often lead to overthinking, where models produce excessively verbose responses even for simple queries, resulting in test-time inefficiency and even degraded accuracy. Prior work has attempted to mitigate this issue via adaptive reasoning strategies, but these methods largely overlook a fundamental bottleneck: visual perception failures. We argue that stable reasoning critically depends on low-level visual grounding, and that reasoning errors often originate from imperfect perception rather than insufficient deliberation. To address this limitation, we propose Gated Perception-Reasoning Optimization (GPRO), a meta-reasoning controller that dynamically routes computation among three decision paths at each generation step: a lightweight fast path, a slow perception path for re-examining visual inputs, and a slow reasoning path for internal self-reflection. To learn this distinction, we derive large-scale failure attribution supervision from approximately 790k samples, using teacher models to distinguish perceptual hallucinations from reasoning errors. We then train the controller with multi-objective reinforcement learning to optimize the trade-off between task accuracy and computational cost under uncertainty. Experiments on five benchmarks demonstrate that GPRO substantially improves both accuracy and efficiency, outperforming recent slow-thinking methods while generating significantly shorter responses.

</details>


### [109] [UniDrive-WM: Unified Understanding, Planning and Generation World Model For Autonomous Driving](https://arxiv.org/abs/2601.04453)
*Zhexiao Xiong,Xin Ye,Burhan Yaman,Sheng Cheng,Yiren Lu,Jingru Luo,Nathan Jacobs,Liu Ren*

Main category: cs.CV

TL;DR: UniDrive-WM is a unified vision-language model world model for autonomous driving that jointly performs scene understanding, trajectory planning, and future image generation in a single architecture.


<details>
  <summary>Details</summary>
Motivation: Existing autonomous driving approaches treat perception, prediction, and planning as separate modules, which may limit performance. The authors aim to create a unified model that integrates these capabilities to improve driving performance through tighter integration of reasoning, planning, and generative world modeling.

Method: Proposes UniDrive-WM, a unified VLM-based world model that: 1) jointly performs driving-scene understanding, trajectory planning, and trajectory-conditioned future image generation; 2) uses predicted trajectories to condition a VLM-based image generator for future frame production; 3) leverages these predictions as supervisory signals to enhance scene understanding and iteratively refine trajectory generation; 4) compares discrete vs continuous output representations for future image prediction.

Result: Experiments on Bench2Drive benchmark show UniDrive-WM produces high-fidelity future images and improves planning performance by 5.9% in L2 trajectory error and 9.2% in collision rate reduction over previous best methods.

Conclusion: The results demonstrate advantages of tightly integrating VLM-driven reasoning, planning, and generative world modeling for autonomous driving, showing that unified architectures outperform modular approaches.

Abstract: World models have become central to autonomous driving, where accurate scene understanding and future prediction are crucial for safe control. Recent work has explored using vision-language models (VLMs) for planning, yet existing approaches typically treat perception, prediction, and planning as separate modules. We propose UniDrive-WM, a unified VLM-based world model that jointly performs driving-scene understanding, trajectory planning, and trajectory-conditioned future image generation within a single architecture. UniDrive-WM's trajectory planner predicts a future trajectory, which conditions a VLM-based image generator to produce plausible future frames. These predictions provide additional supervisory signals that enhance scene understanding and iteratively refine trajectory generation. We further compare discrete and continuous output representations for future image prediction, analyzing their influence on downstream driving performance. Experiments on the challenging Bench2Drive benchmark show that UniDrive-WM produces high-fidelity future images and improves planning performance by 5.9% in L2 trajectory error and 9.2% in collision rate over the previous best method. These results demonstrate the advantages of tightly integrating VLM-driven reasoning, planning, and generative world modeling for autonomous driving. The project page is available at https://unidrive-wm.github.io/UniDrive-WM .

</details>


### [110] [Vision-Language Agents for Interactive Forest Change Analysis](https://arxiv.org/abs/2601.04497)
*James Brock,Ce Zhang,Nantheera Anantrasirichai*

Main category: cs.CV

TL;DR: LLM-driven agent for forest change analysis using satellite imagery, combining change detection and semantic captioning with natural language querying.


<details>
  <summary>Details</summary>
Motivation: Address the gap in integrating LLMs with vision-language models for remote sensing image change interpretation (RSICI) in forest monitoring, where current approaches lack interactive natural language querying capabilities for complex forest dynamics.

Method: Proposes an LLM-driven agent with multi-level change interpretation (MCI) vision-language backbone and LLM-based orchestration. Introduces Forest-Change dataset with bi-temporal satellite imagery, pixel-level change masks, and multi-granularity semantic change captions (human + rule-based annotation).

Result: Achieves 67.10% mIoU and 40.17% BLEU-4 on Forest-Change dataset, and 88.13% mIoU and 34.41% BLEU-4 on LEVIR-MCI-Trees subset for joint change detection and captioning.

Conclusion: LLM-driven RSICI systems improve accessibility, interpretability, and efficiency of forest change analysis. All data and code are publicly available.

Abstract: Modern forest monitoring workflows increasingly benefit from the growing availability of high-resolution satellite imagery and advances in deep learning. Two persistent challenges in this context are accurate pixel-level change detection and meaningful semantic change captioning for complex forest dynamics. While large language models (LLMs) are being adapted for interactive data exploration, their integration with vision-language models (VLMs) for remote sensing image change interpretation (RSICI) remains underexplored. To address this gap, we introduce an LLM-driven agent for integrated forest change analysis that supports natural language querying across multiple RSICI tasks. The proposed system builds upon a multi-level change interpretation (MCI) vision-language backbone with LLM-based orchestration. To facilitate adaptation and evaluation in forest environments, we further introduce the Forest-Change dataset, which comprises bi-temporal satellite imagery, pixel-level change masks, and multi-granularity semantic change captions generated using a combination of human annotation and rule-based methods. Experimental results show that the proposed system achieves mIoU and BLEU-4 scores of 67.10% and 40.17% on the Forest-Change dataset, and 88.13% and 34.41% on LEVIR-MCI-Trees, a tree-focused subset of LEVIR-MCI benchmark for joint change detection and captioning. These results highlight the potential of interactive, LLM-driven RSICI systems to improve accessibility, interpretability, and efficiency of forest change analysis. All data and code are publicly available at https://github.com/JamesBrockUoB/ForestChat.

</details>


### [111] [TokenSeg: Efficient 3D Medical Image Segmentation via Hierarchical Visual Token Compression](https://arxiv.org/abs/2601.04519)
*Sen Zeng,Hong Zhou,Zheng Zhu,Yang Liu*

Main category: cs.CV

TL;DR: TokenSeg: A boundary-aware sparse token representation framework for efficient 3D medical volume segmentation that reduces computation by focusing on salient regions, achieving SOTA performance with significant memory and latency reductions.


<details>
  <summary>Details</summary>
Motivation: 3D medical image segmentation is computationally demanding due to cubic voxel growth and redundant computation on homogeneous regions. There's a need for efficient methods that can handle large volumes while maintaining accuracy.

Method: Three key components: (1) Multi-scale hierarchical encoder extracts 400 candidate tokens across four resolution levels; (2) Boundary-aware tokenizer combines VQ-VAE quantization with importance scoring to select 100 salient tokens (60% near tumor boundaries); (3) Sparse-to-dense decoder reconstructs full-resolution masks through token reprojection, progressive upsampling, and skip connections.

Result: Achieves state-of-the-art performance on 3D breast DCE-MRI dataset (960 cases) with 94.49% Dice and 89.61% IoU, while reducing GPU memory by 64% and inference latency by 68%. Generalizes well to MSD cardiac and brain MRI datasets.

Conclusion: TokenSeg demonstrates the effectiveness of anatomically informed sparse representation for accurate and efficient 3D medical image segmentation, offering significant computational savings while maintaining high accuracy across diverse anatomical structures.

Abstract: Three-dimensional medical image segmentation is a fundamental yet computationally demanding task due to the cubic growth of voxel processing and the redundant computation on homogeneous regions. To address these limitations, we propose \textbf{TokenSeg}, a boundary-aware sparse token representation framework for efficient 3D medical volume segmentation. Specifically, (1) we design a \emph{multi-scale hierarchical encoder} that extracts 400 candidate tokens across four resolution levels to capture both global anatomical context and fine boundary details; (2) we introduce a \emph{boundary-aware tokenizer} that combines VQ-VAE quantization with importance scoring to select 100 salient tokens, over 60\% of which lie near tumor boundaries; and (3) we develop a \emph{sparse-to-dense decoder} that reconstructs full-resolution masks through token reprojection, progressive upsampling, and skip connections. Extensive experiments on a 3D breast DCE-MRI dataset comprising 960 cases demonstrate that TokenSeg achieves state-of-the-art performance with 94.49\% Dice and 89.61\% IoU, while reducing GPU memory and inference latency by 64\% and 68\%, respectively. To verify the generalization capability, our evaluations on MSD cardiac and brain MRI benchmark datasets demonstrate that TokenSeg consistently delivers optimal performance across heterogeneous anatomical structures. These results highlight the effectiveness of anatomically informed sparse representation for accurate and efficient 3D medical image segmentation.

</details>


### [112] [FaceRefiner: High-Fidelity Facial Texture Refinement with Differentiable Rendering-based Style Transfer](https://arxiv.org/abs/2601.04520)
*Chengyang Li,Baoping Cheng,Yao Cheng,Haocheng Zhang,Renshuai Liu,Yinglin Zheng,Jing Liao,Xuan Cheng*

Main category: cs.CV

TL;DR: FaceRefiner is a style transfer-based facial texture refinement method that improves texture quality and identity preservation by transferring multi-level information from input images to generated textures.


<details>
  <summary>Details</summary>
Motivation: Current facial texture generation methods have limited generalization for in-the-wild images, often producing textures with inconsistent details, structures, and identity compared to the input image.

Method: FaceRefiner treats 3D sampled texture as style and texture generation output as content, using style transfer with differentiable rendering to transfer multi-level information (low, middle, high) while preserving visible face region details.

Result: Extensive experiments on Multi-PIE, CelebA and FFHQ datasets show improved texture quality and better face identity preservation compared to state-of-the-art methods.

Conclusion: The proposed style transfer-based refinement method effectively preserves facial details, structures, and identity from input images, enhancing texture generation quality for in-the-wild scenarios.

Abstract: Recent facial texture generation methods prefer to use deep networks to synthesize image content and then fill in the UV map, thus generating a compelling full texture from a single image. Nevertheless, the synthesized texture UV map usually comes from a space constructed by the training data or the 2D face generator, which limits the methods' generalization ability for in-the-wild input images. Consequently, their facial details, structures and identity may not be consistent with the input. In this paper, we address this issue by proposing a style transfer-based facial texture refinement method named FaceRefiner. FaceRefiner treats the 3D sampled texture as style and the output of a texture generation method as content. The photo-realistic style is then expected to be transferred from the style image to the content image. Different from current style transfer methods that only transfer high and middle level information to the result, our style transfer method integrates differentiable rendering to also transfer low level (or pixel level) information in the visible face regions. The main benefit of such multi-level information transfer is that, the details, structures and semantics in the input can thus be well preserved. The extensive experiments on Multi-PIE, CelebA and FFHQ datasets demonstrate that our refinement method can improve the texture quality and the face identity preserving ability, compared with state-of-the-arts.

</details>


### [113] [All Changes May Have Invariant Principles: Improving Ever-Shifting Harmful Meme Detection via Design Concept Reproduction](https://arxiv.org/abs/2601.04567)
*Ziyou Jiang,Mingyang Li,Junjie Wang,Yuekai Huang,Jie Huang,Zhiyuan Chang,Zhaoyang Li,Qing Wang*

Main category: cs.CV

TL;DR: RepMD detects harmful memes by identifying invariant design concepts behind shifting harmful content, using a Design Concept Graph to guide MLLM detection.


<details>
  <summary>Details</summary>
Motivation: Harmful memes constantly evolve in type and over time, making detection challenging. However, different harmful memes may share underlying design principles used by malicious creators, which could provide invariant patterns for detection.

Method: 1) Define Design Concept Graph (DCG) based on attack tree principles to describe steps for designing harmful memes. 2) Derive DCG from historical memes using design step reproduction and graph pruning. 3) Use DCG to guide Multimodal Large Language Model (MLLM) for harmful meme detection.

Result: RepMD achieves 81.1% accuracy, maintains good performance on type-shifting and temporal-evolving memes with only slight accuracy decreases. Human evaluation shows it improves detection efficiency to 15-30 seconds per meme.

Conclusion: RepMD effectively detects harmful memes by focusing on invariant design concepts rather than shifting surface features, demonstrating robustness to evolving meme types and temporal changes while improving human detection efficiency.

Abstract: Harmful memes are ever-shifting in the Internet communities, which are difficult to analyze due to their type-shifting and temporal-evolving nature. Although these memes are shifting, we find that different memes may share invariant principles, i.e., the underlying design concept of malicious users, which can help us analyze why these memes are harmful. In this paper, we propose RepMD, an ever-shifting harmful meme detection method based on the design concept reproduction. We first refer to the attack tree to define the Design Concept Graph (DCG), which describes steps that people may take to design a harmful meme. Then, we derive the DCG from historical memes with design step reproduction and graph pruning. Finally, we use DCG to guide the Multimodal Large Language Model (MLLM) to detect harmful memes. The evaluation results show that RepMD achieves the highest accuracy with 81.1% and has slight accuracy decreases when generalized to type-shifting and temporal-evolving memes. Human evaluation shows that RepMD can improve the efficiency of human discovery on harmful memes, with 15$\sim$30 seconds per meme.

</details>


### [114] [3D Conditional Image Synthesis of Left Atrial LGE MRI from Composite Semantic Masks](https://arxiv.org/abs/2601.04588)
*Yusri Al-Sanaani,Rebecca Thornhill,Sreeraman Rajan*

Main category: cs.CV

TL;DR: 3D conditional generative models (SPADE-LDM, SPADE-GAN, Pix2Pix GAN) synthesize LGE MRI from label maps to augment scarce training data, improving left atrial segmentation performance.


<details>
  <summary>Details</summary>
Motivation: Segmentation of left atrial wall and endocardium from LGE MRI is crucial for quantifying atrial fibrosis, but limited data availability and anatomical complexity make accurate machine learning models challenging to develop.

Method: Developed a pipeline to synthesize 3D LGE MRI volumes from composite semantic label maps using three 3D conditional generators (Pix2Pix GAN, SPADE-GAN, and SPADE-LDM). The synthetic images were evaluated for realism and their impact on downstream LA segmentation with a 3D U-Net model.

Result: SPADE-LDM generated the most realistic images with FID of 4.063, outperforming GAN models (Pix2Pix: 40.821, SPADE-GAN: 7.652). Augmentation with synthetic images improved LA cavity segmentation Dice score from 0.908 to 0.936 (statistically significant, p < 0.05).

Conclusion: Label-conditioned 3D synthesis, particularly with SPADE-LDM, effectively enhances segmentation of under-represented cardiac structures by augmenting scarce training data, demonstrating the potential of generative models for medical imaging applications.

Abstract: Segmentation of the left atrial (LA) wall and endocardium from late gadolinium-enhanced (LGE) MRI is essential for quantifying atrial fibrosis in patients with atrial fibrillation. The development of accurate machine learning-based segmentation models remains challenging due to the limited availability of data and the complexity of anatomical structures. In this work, we investigate 3D conditional generative models as potential solution for augmenting scarce LGE training data and improving LA segmentation performance. We develop a pipeline to synthesize high-fidelity 3D LGE MRI volumes from composite semantic label maps combining anatomical expert annotations with unsupervised tissue clusters, using three 3D conditional generators (Pix2Pix GAN, SPADE-GAN, and SPADE-LDM). The synthetic images are evaluated for realism and their impact on downstream LA segmentation. SPADE-LDM generates the most realistic and structurally accurate images, achieving an FID of 4.063 and surpassing GAN models, which have FIDs of 40.821 and 7.652 for Pix2Pix and SPADE-GAN, respectively. When augmented with synthetic LGE images, the Dice score for LA cavity segmentation with a 3D U-Net model improved from 0.908 to 0.936, showing a statistically significant improvement (p < 0.05) over the baseline.These findings demonstrate the potential of label-conditioned 3D synthesis to enhance the segmentation of under-represented cardiac structures.

</details>


### [115] [MiLDEdit: Reasoning-Based Multi-Layer Design Document Editing](https://arxiv.org/abs/2601.04589)
*Zihao Lin,Wanrong Zhu,Jiuxiang Gu,Jihyung Kil,Christopher Tensmeyer,Lin Zhang,Shilong Liu,Ruiyi Zhang,Lifu Huang,Vlad I. Morariu,Tong Sun*

Main category: cs.CV

TL;DR: MiLDEAgent is a reasoning-based framework for editing multi-layer design documents (like posters) from natural language instructions, addressing the gap in layer-aware document editing that previous single-layer image editing approaches overlooked.


<details>
  <summary>Details</summary>
Motivation: Real-world design documents are multi-layered (decoration, text, images), but existing approaches focus on single-layer image editing or multi-layer generation without the layer-aware reasoning needed to identify relevant layers and coordinate modifications for document editing.

Method: MiLDEAgent combines an RL-trained multimodal reasoner for layer-wise understanding with an image editor for targeted modifications. They also introduce MiLDEBench (20K+ design documents with editing instructions) and MiLDEEval evaluation protocol across four dimensions.

Result: Extensive experiments on 16 models show existing approaches fail: open-source models can't complete tasks, closed-source models violate formats. MiLDEAgent achieves strong layer-aware reasoning and precise editing, outperforming open-source baselines and matching closed-source models.

Conclusion: MiLDEAgent establishes the first strong baseline for multi-layer document editing, demonstrating effective layer-aware reasoning and precise modifications that address the limitations of previous approaches in handling complex multi-layer design documents.

Abstract: Real-world design documents (e.g., posters) are inherently multi-layered, combining decoration, text, and images. Editing them from natural-language instructions requires fine-grained, layer-aware reasoning to identify relevant layers and coordinate modifications. Prior work largely overlooks multi-layer design document editing, focusing instead on single-layer image editing or multi-layer generation, which assume a flat canvas and lack the reasoning needed to determine what and where to modify. To address this gap, we introduce the Multi-Layer Document Editing Agent (MiLDEAgent), a reasoning-based framework that combines an RL-trained multimodal reasoner for layer-wise understanding with an image editor for targeted modifications. To systematically benchmark this setting, we introduce the MiLDEBench, a human-in-the-loop corpus of over 20K design documents paired with diverse editing instructions. The benchmark is complemented by a task-specific evaluation protocol, MiLDEEval, which spans four dimensions including instruction following, layout consistency, aesthetics, and text rendering. Extensive experiments on 14 open-source and 2 closed-source models reveal that existing approaches fail to generalize: open-source models often cannot complete multi-layer document editing tasks, while closed-source models suffer from format violations. In contrast, MiLDEAgent achieves strong layer-aware reasoning and precise editing, significantly outperforming all open-source baselines and attaining performance comparable to closed-source models, thereby establishing the first strong baseline for multi-layer document editing.

</details>


### [116] [Detection of Deployment Operational Deviations for Safety and Security of AI-Enabled Human-Centric Cyber Physical Systems](https://arxiv.org/abs/2601.04605)
*Bernard Ngabonziza,Ayan Banerjee,Sandeep K. S. Gupta*

Main category: cs.CV

TL;DR: Paper proposes a framework for evaluating safety/security strategies in AI-enabled human-centric cyber-physical systems, with a case study on meal detection in diabetes management.


<details>
  <summary>Details</summary>
Motivation: AI-enabled human-centric systems (medical monitoring, autonomous cars) face operational uncertainties from human interactions that can violate safety/security requirements.

Method: 1) Discuss operational deviations leading to unknown conditions, 2) Create framework to evaluate safety/security strategies, 3) Demonstrate with personalized image-based technique for meal detection in diabetes control.

Result: Proposes a novel framework for evaluating safety strategies and demonstrates a personalized image-based technique for detecting unannounced meals in closed-loop blood glucose control systems.

Conclusion: A systematic approach is needed to address operational uncertainties in AI-enabled human-centric systems, with the proposed framework and case study showing practical application for safety assurance.

Abstract: In recent years, Human-centric cyber-physical systems have increasingly involved artificial intelligence to enable knowledge extraction from sensor-collected data. Examples include medical monitoring and control systems, as well as autonomous cars. Such systems are intended to operate according to the protocols and guidelines for regular system operations. However, in many scenarios, such as closed-loop blood glucose control for Type 1 diabetics, self-driving cars, and monitoring systems for stroke diagnosis. The operations of such AI-enabled human-centric applications can expose them to cases for which their operational mode may be uncertain, for instance, resulting from the interactions with a human with the system. Such cases, in which the system is in uncertain conditions, can violate the system's safety and security requirements. 
  This paper will discuss operational deviations that can lead these systems to operate in unknown conditions. We will then create a framework to evaluate different strategies for ensuring the safety and security of AI-enabled human-centric cyber-physical systems in operation deployment. Then, as an example, we show a personalized image-based novel technique for detecting the non-announcement of meals in closed-loop blood glucose control for Type 1 diabetics.

</details>


### [117] [HUR-MACL: High-Uncertainty Region-Guided Multi-Architecture Collaborative Learning for Head and Neck Multi-Organ Segmentation](https://arxiv.org/abs/2601.04607)
*Xiaoyu Liu,Siwen Wei,Linhao Qu,Mingyuan Pan,Chengsheng Zhang,Yonghong Shi,Zhijian Song*

Main category: cs.CV

TL;DR: HUR-MACL model uses uncertainty-guided multi-architecture collaboration to improve segmentation of small, complex head/neck organs by combining CNN, Vision Mamba, and Deformable CNN with feature distillation.


<details>
  <summary>Details</summary>
Motivation: Deep learning models struggle with small, complexly shaped organs in head/neck segmentation. Existing hybrid architectures simply concatenate features without leveraging each component's unique strengths, leading to functional overlap and limited accuracy.

Method: Proposes HUR-MACL: 1) CNN identifies high uncertainty regions, 2) Vision Mamba and Deformable CNN jointly improve segmentation in these regions, 3) Heterogeneous feature distillation loss promotes collaborative learning between architectures in high uncertainty areas.

Result: Achieves state-of-the-art results on two public datasets and one private dataset for multi-organ segmentation in head and neck.

Conclusion: The uncertainty-guided multi-architecture collaborative approach effectively addresses segmentation challenges for small, complex organs, outperforming existing methods by better exploiting complementary strengths of different architectures.

Abstract: Accurate segmentation of organs at risk in the head and neck is essential for radiation therapy, yet deep learning models often fail on small, complexly shaped organs. While hybrid architectures that combine different models show promise, they typically just concatenate features without exploiting the unique strengths of each component. This results in functional overlap and limited segmentation accuracy. To address these issues, we propose a high uncertainty region-guided multi-architecture collaborative learning (HUR-MACL) model for multi-organ segmentation in the head and neck. This model adaptively identifies high uncertainty regions using a convolutional neural network, and for these regions, Vision Mamba as well as Deformable CNN are utilized to jointly improve their segmentation accuracy. Additionally, a heterogeneous feature distillation loss was proposed to promote collaborative learning between the two architectures in high uncertainty regions to further enhance performance. Our method achieves SOTA results on two public datasets and one private dataset.

</details>


### [118] [HyperAlign: Hyperbolic Entailment Cones for Adaptive Text-to-Image Alignment Assessment](https://arxiv.org/abs/2601.04614)
*Wenzhi Chen,Bo Hu,Leida Li,Lihuo He,Wen Lu,Xinbo Gao*

Main category: cs.CV

TL;DR: HyperAlign: A hyperbolic geometry-based framework for adaptive assessment of text-to-image alignment, outperforming existing Euclidean methods.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-image alignment assessment methods rely on Euclidean space metrics, which neglect the structured nature of semantic alignment and lack adaptive capabilities for different samples.

Method: 1) Extract CLIP features and map them to hyperbolic space; 2) Design dynamic-supervision entailment modeling to transform discrete entailment logic into continuous geometric structure supervision; 3) Propose adaptive modulation regressor using hyperbolic geometric features to generate sample-level modulation parameters for calibrating Euclidean cosine similarity.

Result: HyperAlign achieves highly competitive performance on both single database evaluation and cross-database generalization tasks.

Conclusion: The hyperbolic geometric modeling approach is effective for image-text alignment assessment, addressing limitations of existing Euclidean methods.

Abstract: With the rapid development of text-to-image generation technology, accurately assessing the alignment between generated images and text prompts has become a critical challenge. Existing methods rely on Euclidean space metrics, neglecting the structured nature of semantic alignment, while lacking adaptive capabilities for different samples. To address these limitations, we propose HyperAlign, an adaptive text-to-image alignment assessment framework based on hyperbolic entailment geometry. First, we extract Euclidean features using CLIP and map them to hyperbolic space. Second, we design a dynamic-supervision entailment modeling mechanism that transforms discrete entailment logic into continuous geometric structure supervision. Finally, we propose an adaptive modulation regressor that utilizes hyperbolic geometric features to generate sample-level modulation parameters, adaptively calibrating Euclidean cosine similarity to predict the final score. HyperAlign achieves highly competitive performance on both single database evaluation and cross-database generalization tasks, fully validating the effectiveness of hyperbolic geometric modeling for image-text alignment assessment.

</details>


### [119] [Agri-R1: Empowering Generalizable Agricultural Reasoning in Vision-Language Models with Reinforcement Learning](https://arxiv.org/abs/2601.04672)
*Wentao Zhang,Lifei Wang,Lina Lu,MingKun Xu,Shangyang Li,Yanchao Yang,Tao Fang*

Main category: cs.CV

TL;DR: Agri-R1: A 3B-parameter reasoning-enhanced model for agricultural disease diagnosis that uses automated reasoning data generation and Group Relative Policy Optimization, achieving competitive performance with larger models.


<details>
  <summary>Details</summary>
Motivation: Agricultural disease diagnosis challenges VLMs due to: 1) conventional fine-tuning needing extensive labels, 2) lack of interpretability, 3) poor generalization, and 4) existing reasoning methods requiring costly expert annotations and not addressing open-ended, diverse agricultural queries.

Method: 1) Automates high-quality reasoning data generation via vision-language synthesis and LLM-based filtering (using only 19% of available samples). 2) Uses Group Relative Policy Optimization (GRPO) with novel reward function integrating domain-specific lexicons and fuzzy matching to assess correctness and linguistic flexibility in open-ended responses.

Result: On CDDMBench: 3B-parameter model competitive with 7B-13B baselines; +23.2% relative gain in disease recognition accuracy; +33.3% in agricultural knowledge QA; +26.10-point improvement in cross-domain generalization over standard fine-tuning. Benefits scale with question complexity.

Conclusion: Synergy between structured reasoning data and GRPO-driven exploration underpins performance gains. The approach enables effective agricultural disease diagnosis with reduced annotation requirements and improved interpretability/generalization.

Abstract: Agricultural disease diagnosis challenges VLMs, as conventional fine-tuning requires extensive labels, lacks interpretability, and generalizes poorly. While reasoning improves model robustness, existing methods rely on costly expert annotations and rarely address the open-ended, diverse nature of agricultural queries. To address these limitations, we propose \textbf{Agri-R1}, a reasoning-enhanced large model for agriculture. Our framework automates high-quality reasoning data generation via vision-language synthesis and LLM-based filtering, using only 19\% of available samples. Training employs Group Relative Policy Optimization (GRPO) with a novel proposed reward function that integrates domain-specific lexicons and fuzzy matching to assess both correctness and linguistic flexibility in open-ended responses. Evaluated on CDDMBench, our resulting 3B-parameter model achieves performance competitive with 7B- to 13B-parameter baselines, showing a +23.2\% relative gain in disease recognition accuracy, +33.3\% in agricultural knowledge QA, and a +26.10-point improvement in cross-domain generalization over standard fine-tuning. Ablation studies confirm that the synergy between structured reasoning data and GRPO-driven exploration underpins these gains, with benefits scaling as question complexity increases.

</details>


### [120] [DB-MSMUNet:Dual Branch Multi-scale Mamba UNet for Pancreatic CT Scans Segmentation](https://arxiv.org/abs/2601.04676)
*Qiu Guan,Zhiqiang Yang,Dezhang Ye,Yang Chen,Xinli Xu,Ying Tang*

Main category: cs.CV

TL;DR: DB-MSMUNet: A dual-branch multi-scale Mamba UNet architecture for robust pancreatic CT segmentation, achieving state-of-the-art performance on multiple datasets through enhanced global context modeling, boundary refinement, and small lesion reconstruction.


<details>
  <summary>Details</summary>
Motivation: Pancreatic segmentation in CT scans is crucial for cancer diagnosis but remains challenging due to low tissue contrast, blurry boundaries, irregular organ shapes, and small lesion sizes. Existing methods struggle with these difficulties, necessitating a more robust approach.

Method: Proposes DB-MSMUNet with: 1) Multi-scale Mamba Module encoder combining deformable convolutions and state space modeling for global context and local deformation adaptation; 2) Dual-decoder design with Edge Enhancement Path for boundary refinement and Multi-layer Decoder for small lesion reconstruction; 3) Auxiliary Deep Supervision heads at multiple scales for better gradient feedback.

Result: Achieves Dice Similarity Coefficients of 89.47% (NIH Pancreas), 87.59% (MSD), and 89.02% (clinical tumor dataset), outperforming most state-of-the-art methods in segmentation accuracy, edge preservation, and robustness across datasets.

Conclusion: DB-MSMUNet demonstrates effectiveness and generalizability for real-world pancreatic CT segmentation tasks, successfully addressing challenges of low contrast, blurry boundaries, and small lesions through its innovative architecture design.

Abstract: Accurate segmentation of the pancreas and its lesions in CT scans is crucial for the precise diagnosis and treatment of pancreatic cancer. However, it remains a highly challenging task due to several factors such as low tissue contrast with surrounding organs, blurry anatomical boundaries, irregular organ shapes, and the small size of lesions. To tackle these issues, we propose DB-MSMUNet (Dual-Branch Multi-scale Mamba UNet), a novel encoder-decoder architecture designed specifically for robust pancreatic segmentation. The encoder is constructed using a Multi-scale Mamba Module (MSMM), which combines deformable convolutions and multi-scale state space modeling to enhance both global context modeling and local deformation adaptation. The network employs a dual-decoder design: the edge decoder introduces an Edge Enhancement Path (EEP) to explicitly capture boundary cues and refine fuzzy contours, while the area decoder incorporates a Multi-layer Decoder (MLD) to preserve fine-grained details and accurately reconstruct small lesions by leveraging multi-scale deep semantic features. Furthermore, Auxiliary Deep Supervision (ADS) heads are added at multiple scales to both decoders, providing more accurate gradient feedback and further enhancing the discriminative capability of multi-scale features. We conduct extensive experiments on three datasets: the NIH Pancreas dataset, the MSD dataset, and a clinical pancreatic tumor dataset provided by collaborating hospitals. DB-MSMUNet achieves Dice Similarity Coefficients of 89.47%, 87.59%, and 89.02%, respectively, outperforming most existing state-of-the-art methods in terms of segmentation accuracy, edge preservation, and robustness across different datasets. These results demonstrate the effectiveness and generalizability of the proposed method for real-world pancreatic CT segmentation tasks.

</details>


### [121] [HATIR: Heat-Aware Diffusion for Turbulent Infrared Video Super-Resolution](https://arxiv.org/abs/2601.04682)
*Yang Zou,Xingyue Zhu,Kaiqi Han,Jun Ma,Xingyuan Li,Zhiying Jiang,Jinyuan Liu*

Main category: cs.CV

TL;DR: HATIR is a diffusion-based method for joint turbulence mitigation and super-resolution of infrared videos, using heat-aware deformation priors and phasor-guided flow estimation.


<details>
  <summary>Details</summary>
Motivation: Infrared videos suffer from atmospheric turbulence and compression degradation, but existing methods either ignore the modality gap between infrared/visible images or fail to restore turbulence-induced distortions. Cascading turbulence mitigation with super-resolution leads to error propagation due to decoupled degradation modeling.

Method: HATIR injects heat-aware deformation priors into diffusion sampling to jointly model inverse processes of turbulent degradation and detail loss. It uses a Phasor-Guided Flow Estimator based on thermal phasor responses, and a Turbulence-Aware Decoder with turbulence gating and structure-aware attention for selective feature aggregation.

Result: The authors built FLIR-IVSR, the first dataset for turbulent infrared VSR with 640 diverse scenes from a FLIR T1050sc camera (1024×768), enabling future research in this area.

Conclusion: HATIR provides a unified framework for joint turbulence mitigation and super-resolution in infrared videos, addressing the limitations of existing decoupled approaches through heat-aware diffusion modeling.

Abstract: Infrared video has been of great interest in visual tasks under challenging environments, but often suffers from severe atmospheric turbulence and compression degradation. Existing video super-resolution (VSR) methods either neglect the inherent modality gap between infrared and visible images or fail to restore turbulence-induced distortions. Directly cascading turbulence mitigation (TM) algorithms with VSR methods leads to error propagation and accumulation due to the decoupled modeling of degradation between turbulence and resolution. We introduce HATIR, a Heat-Aware Diffusion for Turbulent InfraRed Video Super-Resolution, which injects heat-aware deformation priors into the diffusion sampling path to jointly model the inverse process of turbulent degradation and structural detail loss. Specifically, HATIR constructs a Phasor-Guided Flow Estimator, rooted in the physical principle that thermally active regions exhibit consistent phasor responses over time, enabling reliable turbulence-aware flow to guide the reverse diffusion process. To ensure the fidelity of structural recovery under nonuniform distortions, a Turbulence-Aware Decoder is proposed to selectively suppress unstable temporal cues and enhance edge-aware feature aggregation via turbulence gating and structure-aware attention. We built FLIR-IVSR, the first dataset for turbulent infrared VSR, comprising paired LR-HR sequences from a FLIR T1050sc camera (1024 X 768) spanning 640 diverse scenes with varying camera and object motion conditions. This encourages future research in infrared VSR. Project page: https://github.com/JZ0606/HATIR

</details>


### [122] [WebCryptoAgent: Agentic Crypto Trading with Web Informatics](https://arxiv.org/abs/2601.04687)
*Ali Kurban,Wei Luo,Liangyu Zuo,Zeyu Zhang,Renda Han,Zhaolu Kang,Hao Tang*

Main category: cs.CV

TL;DR: WebCryptoAgent: An agentic trading framework that decomposes web-informed cryptocurrency trading into modality-specific agents with a decoupled control architecture for real-time risk management.


<details>
  <summary>Details</summary>
Motivation: Cryptocurrency trading requires timely integration of heterogeneous web information and market signals, but existing systems struggle with noisy multi-source evidence and rapid price shocks at sub-second timescales.

Method: Proposes WebCryptoAgent with modality-specific agents for different information sources (unstructured web content, social sentiment, structured OHLCV signals) that consolidate outputs into unified evidence documents. Uses decoupled control architecture separating strategic hourly reasoning from real-time second-level risk model for fast shock detection.

Result: Extensive experiments on real-world cryptocurrency markets show improved trading stability, reduced spurious activity, and enhanced tail-risk handling compared to existing baselines.

Conclusion: WebCryptoAgent effectively addresses challenges of noisy multi-source evidence integration and rapid market shock handling in cryptocurrency trading through agentic decomposition and decoupled control architecture.

Abstract: Cryptocurrency trading increasingly depends on timely integration of heterogeneous web information and market microstructure signals to support short-horizon decision making under extreme volatility. However, existing trading systems struggle to jointly reason over noisy multi-source web evidence while maintaining robustness to rapid price shocks at sub-second timescales. The first challenge lies in synthesizing unstructured web content, social sentiment, and structured OHLCV signals into coherent and interpretable trading decisions without amplifying spurious correlations, while the second challenge concerns risk control, as slow deliberative reasoning pipelines are ill-suited for handling abrupt market shocks that require immediate defensive responses. To address these challenges, we propose WebCryptoAgent, an agentic trading framework that decomposes web-informed decision making into modality-specific agents and consolidates their outputs into a unified evidence document for confidence-calibrated reasoning. We further introduce a decoupled control architecture that separates strategic hourly reasoning from a real-time second-level risk model, enabling fast shock detection and protective intervention independent of the trading loop. Extensive experiments on real-world cryptocurrency markets demonstrate that WebCryptoAgent improves trading stability, reduces spurious activity, and enhances tail-risk handling compared to existing baselines. Code will be available at https://github.com/AIGeeksGroup/WebCryptoAgent.

</details>


### [123] [Forge-and-Quench: Enhancing Image Generation for Higher Fidelity in Unified Multimodal Models](https://arxiv.org/abs/2601.04706)
*Yanbing Zeng,Jia Wang,Hanghang Ma,Junqiang Wu,Jie Zhu,Xiaoming Wei,Jie Hu*

Main category: cs.CV

TL;DR: Forge-and-Quench: A unified framework that uses multimodal understanding models to enhance image generation fidelity and detail by creating bridge features that guide text-to-image models.


<details>
  <summary>Details</summary>
Motivation: While integrating image generation and understanding is important, previous works haven't fully explored how understanding can effectively assist generation. The paper aims to leverage understanding models to improve the fidelity and detail richness of generated images rather than just using their reasoning abilities and world knowledge.

Method: Proposes Forge-and-Quench framework: 1) MLLM reasons over conversational context to produce enhanced text instructions, 2) Bridge Adapter maps refined instructions to virtual visual representations called Bridge Features, 3) These features are injected into T2I backbone as visual guidance alongside enhanced text instructions, replacing original inputs.

Result: The framework shows exceptional extensibility and flexibility, enabling efficient migration across different MLLM and T2I models with significant training overhead savings. Experiments demonstrate significant improvements in image fidelity and detail across multiple models while maintaining instruction-following accuracy and enhancing world knowledge application.

Conclusion: Forge-and-Quench successfully demonstrates how understanding models can effectively enhance image generation quality through bridge features, offering a practical unified framework that maintains MLLM capabilities while improving T2I performance with minimal training overhead.

Abstract: Integrating image generation and understanding into a single framework has become a pivotal goal in the multimodal domain. However, how understanding can effectively assist generation has not been fully explored. Unlike previous works that focus on leveraging reasoning abilities and world knowledge from understanding models, this paper introduces a novel perspective: leveraging understanding to enhance the fidelity and detail richness of generated images. To this end, we propose Forge-and-Quench, a new unified framework that puts this principle into practice. In the generation process of our framework, an MLLM first reasons over the entire conversational context, including text instructions, to produce an enhanced text instruction. This refined instruction is then mapped to a virtual visual representation, termed the Bridge Feature, via a novel Bridge Adapter. This feature acts as a crucial link, forging insights from the understanding model to quench and refine the generation process. It is subsequently injected into the T2I backbone as a visual guidance signal, alongside the enhanced text instruction that replaces the original input. To validate this paradigm, we conduct comprehensive studies on the design of the Bridge Feature and Bridge Adapter. Our framework demonstrates exceptional extensibility and flexibility, enabling efficient migration across different MLLM and T2I models with significant savings in training overhead, all without compromising the MLLM's inherent multimodal understanding capabilities. Experiments show that Forge-and-Quench significantly improves image fidelity and detail across multiple models, while also maintaining instruction-following accuracy and enhancing world knowledge application. Models and codes are available at https://github.com/YanbingZeng/Forge-and-Quench.

</details>


### [124] [On the Holistic Approach for Detecting Human Image Forgery](https://arxiv.org/abs/2601.04715)
*Xiao Guo,Jie Zhu,Anil Jain,Xiaoming Liu*

Main category: cs.CV

TL;DR: HuForDet is a dual-branch framework for holistic human image forgery detection that combines face-specific analysis with full-body semantic consistency checking using MLLMs, achieving state-of-the-art performance across diverse human image manipulations.


<details>
  <summary>Details</summary>
Motivation: Existing deepfake detection methods are fragmented - they specialize either in facial forgeries or full-body synthetic images, failing to generalize across the full spectrum of human image manipulations. The rapid advancement of AI-generated content has escalated the threat of deepfakes, necessitating a more comprehensive approach.

Method: HuForDet features a dual-branch architecture: (1) a face forgery detection branch with heterogeneous experts operating in RGB and frequency domains, including an adaptive Laplacian-of-Gaussian module to capture artifacts from fine-grained blending boundaries to coarse-scale texture irregularities; (2) a contextualized forgery detection branch that leverages a Multi-Modal Large Language Model to analyze full-body semantic consistency, enhanced with confidence estimation for dynamic feature fusion weighting.

Result: Extensive experiments show HuForDet achieves state-of-the-art forgery detection performance and superior robustness across diverse human image forgeries. The authors also curated a human image forgery (HuFor) dataset that unifies existing face forgery data with a new corpus of full-body synthetic humans.

Conclusion: HuForDet provides a holistic framework for human image forgery detection that successfully addresses the fragmentation in existing methods by combining specialized face analysis with full-body semantic consistency checking, demonstrating strong performance across the full spectrum of human image manipulations.

Abstract: The rapid advancement of AI-generated content (AIGC) has escalated the threat of deepfakes, from facial manipulations to the synthesis of entire photorealistic human bodies. However, existing detection methods remain fragmented, specializing either in facial-region forgeries or full-body synthetic images, and consequently fail to generalize across the full spectrum of human image manipulations. We introduce HuForDet, a holistic framework for human image forgery detection, which features a dual-branch architecture comprising: (1) a face forgery detection branch that employs heterogeneous experts operating in both RGB and frequency domains, including an adaptive Laplacian-of-Gaussian (LoG) module designed to capture artifacts ranging from fine-grained blending boundaries to coarse-scale texture irregularities; and (2) a contextualized forgery detection branch that leverages a Multi-Modal Large Language Model (MLLM) to analyze full-body semantic consistency, enhanced with a confidence estimation mechanism that dynamically weights its contribution during feature fusion. We curate a human image forgery (HuFor) dataset that unifies existing face forgery data with a new corpus of full-body synthetic humans. Extensive experiments show that our HuForDet achieves state-of-the-art forgery detection performance and superior robustness across diverse human image forgeries.

</details>


### [125] [Training a Custom CNN on Five Heterogeneous Image Datasets](https://arxiv.org/abs/2601.04727)
*Anika Tabassum,Tasnuva Mahazabin Tuba,Nafisa Naznin*

Main category: cs.CV

TL;DR: This paper evaluates CNN architectures across five diverse visual classification tasks in agricultural and urban domains, comparing custom lightweight CNNs with established models like ResNet-18 and VGG-16, with and without transfer learning.


<details>
  <summary>Details</summary>
Motivation: To investigate how CNN-based architectures perform across heterogeneous real-world datasets with varying challenges (illumination, resolution, environmental complexity, class imbalance) and determine when transfer learning and deep architectures provide advantages, especially in data-constrained environments.

Method: Systematic evaluation of a lightweight custom CNN alongside established architectures (ResNet-18, VGG-16) trained both from scratch and using transfer learning. Applied to five diverse datasets: mango variety classification, paddy variety identification, road surface condition assessment, auto-rickshaw detection, and footpath encroachment monitoring. Used systematic preprocessing, augmentation, and controlled experimentation.

Result: Developed an efficient custom CNN that achieves competitive performance across multiple application domains. Found that transfer learning and deep architectures provide substantial advantages in data-constrained environments. Analyzed how architectural complexity, model depth, and pre-training influence convergence, generalization, and performance across datasets of differing scale and difficulty.

Conclusion: The findings offer practical insights for deploying deep learning models in resource-limited yet high-impact real-world visual classification tasks, highlighting when different architectural approaches are most effective.

Abstract: Deep learning has transformed visual data analysis, with Convolutional Neural Networks (CNNs) becoming highly effective in learning meaningful feature representations directly from images. Unlike traditional manual feature engineering methods, CNNs automatically extract hierarchical visual patterns, enabling strong performance across diverse real-world contexts. This study investigates the effectiveness of CNN-based architectures across five heterogeneous datasets spanning agricultural and urban domains: mango variety classification, paddy variety identification, road surface condition assessment, auto-rickshaw detection, and footpath encroachment monitoring. These datasets introduce varying challenges, including differences in illumination, resolution, environmental complexity, and class imbalance, necessitating adaptable and robust learning models.
  We evaluate a lightweight, task-specific custom CNN alongside established deep architectures, including ResNet-18 and VGG-16, trained both from scratch and using transfer learning. Through systematic preprocessing, augmentation, and controlled experimentation, we analyze how architectural complexity, model depth, and pre-training influence convergence, generalization, and performance across datasets of differing scale and difficulty. The key contributions of this work are: (1) the development of an efficient custom CNN that achieves competitive performance across multiple application domains, and (2) a comprehensive comparative analysis highlighting when transfer learning and deep architectures provide substantial advantages, particularly in data-constrained environments. These findings offer practical insights for deploying deep learning models in resource-limited yet high-impact real-world visual classification tasks.

</details>


### [126] [AIVD: Adaptive Edge-Cloud Collaboration for Accurate and Efficient Industrial Visual Detection](https://arxiv.org/abs/2601.04734)
*Yunqing Hu,Zheming Yang,Chang Zhao,Qi Guo,Meng Gao,Pengcheng Li,Wen Ji*

Main category: cs.CV

TL;DR: AIVD framework enables precise object localization and semantic generation via edge-cloud collaboration, with noise-robust fine-tuning and resource-aware scheduling for efficient deployment.


<details>
  <summary>Details</summary>
Motivation: MLLMs have strong semantic understanding but struggle with precise object localization and efficient edge-cloud deployment due to resource constraints and noise sensitivity.

Method: Proposes AIVD framework with: 1) lightweight edge detectors for localization, 2) cloud MLLMs for semantic generation, 3) visual-semantic collaborative augmentation for noise robustness, and 4) heterogeneous resource-aware dynamic scheduling algorithm.

Result: AIVD reduces resource consumption while improving MLLM classification accuracy and semantic generation quality; scheduling achieves higher throughput and lower latency across diverse scenarios.

Conclusion: The AIVD framework successfully addresses MLLM limitations in localization and deployment through edge-cloud collaboration, robust fine-tuning, and intelligent scheduling for practical applications.

Abstract: Multimodal large language models (MLLMs) demonstrate exceptional capabilities in semantic understanding and visual reasoning, yet they still face challenges in precise object localization and resource-constrained edge-cloud deployment. To address this, this paper proposes the AIVD framework, which achieves unified precise localization and high-quality semantic generation through the collaboration between lightweight edge detectors and cloud-based MLLMs. To enhance the cloud MLLM's robustness against edge cropped-box noise and scenario variations, we design an efficient fine-tuning strategy with visual-semantic collaborative augmentation, significantly improving classification accuracy and semantic consistency. Furthermore, to maintain high throughput and low latency across heterogeneous edge devices and dynamic network conditions, we propose a heterogeneous resource-aware dynamic scheduling algorithm. Experimental results demonstrate that AIVD substantially reduces resource consumption while improving MLLM classification performance and semantic generation quality. The proposed scheduling strategy also achieves higher throughput and lower latency across diverse scenarios.

</details>


### [127] [Skeletonization-Based Adversarial Perturbations on Large Vision Language Model's Mathematical Text Recognition](https://arxiv.org/abs/2601.04752)
*Masatomo Yoshida,Haruto Namura,Nicola Adami,Masahiro Okuda*

Main category: cs.CV

TL;DR: Novel adversarial attack using skeletonization to efficiently target text/math formula images, revealing visual reasoning limitations in foundation models like ChatGPT.


<details>
  <summary>Details</summary>
Motivation: To understand the visual capabilities and limitations of foundation models by developing targeted adversarial attacks, especially on challenging text/math formula images that require complex visual interpretation.

Method: Introduces skeletonization-based adversarial attack method that reduces search space for efficient attacks. Specifically targets text-containing images, particularly mathematical formulas, which are challenging due to LaTeX conversion and intricate structure.

Result: Detailed evaluation shows character and semantic changes between original and adversarially perturbed outputs, providing insights into models' visual interpretation abilities. Method effectively applied to ChatGPT, demonstrating practical real-world implications.

Conclusion: Skeletonization-based adversarial attacks effectively reveal visual reasoning limitations in foundation models, with particular vulnerability in text/math formula interpretation, highlighting important security and robustness concerns.

Abstract: This work explores the visual capabilities and limitations of foundation models by introducing a novel adversarial attack method utilizing skeletonization to reduce the search space effectively. Our approach specifically targets images containing text, particularly mathematical formula images, which are more challenging due to their LaTeX conversion and intricate structure. We conduct a detailed evaluation of both character and semantic changes between original and adversarially perturbed outputs to provide insights into the models' visual interpretation and reasoning abilities. The effectiveness of our method is further demonstrated through its application to ChatGPT, which shows its practical implications in real-world scenarios.

</details>


### [128] [ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting](https://arxiv.org/abs/2601.04754)
*Yen-Jen Chiou,Wei-Tse Cheng,Yuan-Fu Yang*

Main category: cs.CV

TL;DR: ProFuse is an efficient framework for open-vocabulary 3D scene understanding using 3D Gaussian Splatting that achieves semantic attachment in ~5 minutes (2x faster than SOTA) without render-supervised fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To enable efficient open-vocabulary 3D scene understanding with 3D Gaussian Splatting while addressing cross-view consistency and intra-mask cohesion issues in direct registration setups, with minimal computational overhead.

Method: Uses dense correspondence-guided pre-registration to initialize Gaussians with accurate geometry, jointly constructs 3D Context Proposals via cross-view clustering, aggregates global features onto proposals, and fuses these features onto Gaussians during direct registration to maintain language coherence.

Result: Achieves strong open-vocabulary 3DGS understanding with semantic attachment completed in about five minutes per scene, which is two times faster than state-of-the-art methods, while maintaining geometric refinement without densification.

Conclusion: ProFuse demonstrates an efficient, context-aware framework for open-vocabulary 3D scene understanding that enhances consistency and cohesion with minimal overhead, requiring no additional optimization beyond standard reconstruction.

Abstract: We present ProFuse, an efficient context-aware framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting (3DGS). The pipeline enhances cross-view consistency and intra-mask cohesion within a direct registration setup, adding minimal overhead and requiring no render-supervised fine-tuning. Instead of relying on a pretrained 3DGS scene, we introduce a dense correspondence-guided pre-registration phase that initializes Gaussians with accurate geometry while jointly constructing 3D Context Proposals via cross-view clustering. Each proposal carries a global feature obtained through weighted aggregation of member embeddings, and this feature is fused onto Gaussians during direct registration to maintain per-primitive language coherence across views. With associations established in advance, semantic fusion requires no additional optimization beyond standard reconstruction, and the model retains geometric refinement without densification. ProFuse achieves strong open-vocabulary 3DGS understanding while completing semantic attachment in about five minutes per scene, which is two times faster than SOTA.

</details>


### [129] [Segmentation-Driven Monocular Shape from Polarization based on Physical Model](https://arxiv.org/abs/2601.04776)
*Jinyu Zhang,Xu Ma,Weili Chen,Gonzalo R. Arce*

Main category: cs.CV

TL;DR: A novel segmentation-driven monocular shape-from-polarization method that overcomes azimuth ambiguity by decomposing global reconstruction into locally convex regions using adaptive segmentation and multi-scale fusion constraints.


<details>
  <summary>Details</summary>
Motivation: Existing monocular shape-from-polarization methods suffer from azimuth angle ambiguity, an inherent limitation of polarization analysis that severely compromises reconstruction accuracy and stability. This ambiguity problem needs to be addressed to improve 3D reconstruction from single-view polarized images.

Method: Proposes SMSfP framework with two key components: 1) Polarization-aided adaptive region growing (PARG) segmentation strategy to decompose global convexity assumption into locally convex regions, suppressing azimuth ambiguities while preserving surface continuity. 2) Multi-scale fusion convexity prior (MFCP) constraint to ensure local surface consistency and enhance recovery of fine textural and structural details.

Result: Extensive experiments on synthetic and real-world datasets show significant improvements in disambiguation accuracy and geometric fidelity compared with existing physics-based monocular SfP techniques.

Conclusion: The segmentation-driven approach effectively addresses azimuth ambiguity in monocular shape-from-polarization, providing more accurate and stable 3D reconstruction by reformulating global shape recovery into local reconstructions over adaptively segmented convex sub-regions.

Abstract: Monocular shape-from-polarization (SfP) leverages the intrinsic relationship between light polarization properties and surface geometry to recover surface normals from single-view polarized images, providing a compact and robust approach for three-dimensional (3D) reconstruction. Despite its potential, existing monocular SfP methods suffer from azimuth angle ambiguity, an inherent limitation of polarization analysis, that severely compromises reconstruction accuracy and stability. This paper introduces a novel segmentation-driven monocular SfP (SMSfP) framework that reformulates global shape recovery into a set of local reconstructions over adaptively segmented convex sub-regions. Specifically, a polarization-aided adaptive region growing (PARG) segmentation strategy is proposed to decompose the global convexity assumption into locally convex regions, effectively suppressing azimuth ambiguities and preserving surface continuity. Furthermore, a multi-scale fusion convexity prior (MFCP) constraint is developed to ensure local surface consistency and enhance the recovery of fine textural and structural details. Extensive experiments on both synthetic and real-world datasets validate the proposed approach, showing significant improvements in disambiguation accuracy and geometric fidelity compared with existing physics-based monocular SfP techniques.

</details>


### [130] [GeM-VG: Towards Generalized Multi-image Visual Grounding with Multimodal Large Language Models](https://arxiv.org/abs/2601.04777)
*Shurong Zheng,Yousong Zhu,Hongyin Zhao,Fan Yang,Yufei Zhan,Ming Tang,Jinqiao Wang*

Main category: cs.CV

TL;DR: GeM-VG is a Multimodal Large Language Model for Generalized Multi-image Visual Grounding that addresses diverse multi-image grounding tasks through unified modeling, a new dataset, and hybrid reinforcement finetuning.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs for multi-image grounding are limited to single-target localization and few practical tasks due to lack of unified modeling for generalized grounding tasks. There's a need for a model that can handle diverse multi-image grounding scenarios with robust cross-image reasoning.

Method: 1) Systematically categorize multi-image grounding tasks by cross-image cue reliance; 2) Introduce MG-Data-240K dataset addressing target quantity and image relation limitations; 3) Propose hybrid reinforcement finetuning strategy combining chain-of-thought reasoning and direct answering using R1-like algorithm with rule-based rewards.

Result: Outperforms previous leading MLLMs by 2.0% on MIG-Bench and 9.7% on MC-Bench for multi-image grounding. Achieves 9.1% improvement over base model on ODINW for single-image grounding. Retains strong general multi-image understanding capabilities.

Conclusion: GeM-VG demonstrates superior generalized grounding capabilities across both multi-image and single-image tasks through unified modeling, comprehensive dataset, and effective hybrid finetuning strategy, advancing the field of multimodal visual grounding.

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive progress in single-image grounding and general multi-image understanding. Recently, some methods begin to address multi-image grounding. However, they are constrained by single-target localization and limited types of practical tasks, due to the lack of unified modeling for generalized grounding tasks. Therefore, we propose GeM-VG, an MLLM capable of Generalized Multi-image Visual Grounding. To support this, we systematically categorize and organize existing multi-image grounding tasks according to their reliance of cross-image cues and reasoning, and introduce the MG-Data-240K dataset, addressing the limitations of existing datasets regarding target quantity and image relation. To tackle the challenges of robustly handling diverse multi-image grounding tasks, we further propose a hybrid reinforcement finetuning strategy that integrates chain-of-thought (CoT) reasoning and direct answering, considering their complementary strengths. This strategy adopts an R1-like algorithm guided by a carefully designed rule-based reward, effectively enhancing the model's overall perception and reasoning capabilities. Extensive experiments demonstrate the superior generalized grounding capabilities of our model. For multi-image grounding, it outperforms the previous leading MLLMs by 2.0% and 9.7% on MIG-Bench and MC-Bench, respectively. In single-image grounding, it achieves a 9.1% improvement over the base model on ODINW. Furthermore, our model retains strong capabilities in general multi-image understanding.

</details>


### [131] [CounterVid: Counterfactual Video Generation for Mitigating Action and Temporal Hallucinations in Video-Language Models](https://arxiv.org/abs/2601.04778)
*Tobia Poppi,Burak Uzkent,Amanmeet Garg,Lucas Porto,Garin Kessler,Yezhou Yang,Marcella Cornia,Lorenzo Baraldi,Rita Cucchiara,Florian Schiffers*

Main category: cs.CV

TL;DR: CounterVid: A framework for generating counterfactual videos to address VLM hallucinations by creating semantic hard negatives for action recognition and temporal reasoning, combined with MixDPO for joint textual/visual preference optimization.


<details>
  <summary>Details</summary>
Motivation: Video-language models suffer from hallucinations, especially in action and temporal reasoning, due to over-reliance on language priors rather than fine-grained visual dynamics. Existing mitigation strategies like textual filtering or random perturbations fail to address the root cause.

Method: Propose a scalable counterfactual video generation framework that synthesizes videos differing only in actions or temporal structure while preserving scene context. Uses multimodal LLMs for action proposal/editing guidance with diffusion-based image/video models to generate semantic hard negatives at scale. Build CounterVid dataset (~26k preference pairs) and introduce MixDPO, a unified Direct Preference Optimization approach for joint textual and visual preferences.

Result: Fine-tuning Qwen2.5-VL with MixDPO yields consistent improvements, notably in temporal ordering, and transfers effectively to standard video hallucination benchmarks.

Conclusion: The proposed counterfactual video generation framework and MixDPO approach effectively address VLM hallucinations by targeting the root cause of over-reliance on language priors, improving temporal reasoning capabilities while maintaining scene context understanding.

Abstract: Video-language models (VLMs) achieve strong multimodal understanding but remain prone to hallucinations, especially when reasoning about actions and temporal order. Existing mitigation strategies, such as textual filtering or random video perturbations, often fail to address the root cause: over-reliance on language priors rather than fine-grained visual dynamics. We propose a scalable framework for counterfactual video generation that synthesizes videos differing only in actions or temporal structure while preserving scene context. Our pipeline combines multimodal LLMs for action proposal and editing guidance with diffusion-based image and video models to generate semantic hard negatives at scale. Using this framework, we build CounterVid, a synthetic dataset of ~26k preference pairs targeting action recognition and temporal reasoning. We further introduce MixDPO, a unified Direct Preference Optimization approach that jointly leverages textual and visual preferences. Fine-tuning Qwen2.5-VL with MixDPO yields consistent improvements, notably in temporal ordering, and transfers effectively to standard video hallucination benchmarks. Code and models will be made publicly available.

</details>


### [132] [Defocus Aberration Theory Confirms Gaussian Model in Most Imaging Devices](https://arxiv.org/abs/2601.04779)
*Akbar Saadat*

Main category: cs.CV

TL;DR: The paper validates that Gaussian model is accurate for defocus operators in most imaging devices, with less than 1% error, making it suitable for real-time depth estimation applications.


<details>
  <summary>Details</summary>
Motivation: Depth estimation from 2D images is a fundamental challenge in 3D recovery. While defocus provides valuable depth information, accurately modeling defocus blur is difficult due to the ill-posed nature of distinguishing desired blur from inherent blur. The Gaussian model offers mathematical simplicity and computational efficiency for real-time applications.

Method: The paper introduces specific settings for conventional imaging devices to ensure defocus operators adhere to the Gaussian model. Analysis begins with geometric optics framework and uses defocus aberration theory in diffraction-limited optics to evaluate accuracy of fitting actual model to Gaussian approximation. The study examines typical focused depths between 1-100 meters with maximum depth variation of 10% at focused depth.

Result: Results confirm Gaussian model's applicability for defocus operators in most imaging devices, demonstrating maximum Mean Absolute Error (MAE) of less than 1%, which underscores the model's accuracy and reliability.

Conclusion: The Gaussian model is validated as an accurate and reliable choice for defocus operators in conventional imaging devices, supporting its use for real-time depth estimation applications where computational efficiency and mathematical simplicity are crucial.

Abstract: Over the past three decades, defocus has consistently provided groundbreaking depth information in scene images. However, accurately estimating depth from 2D images continues to be a persistent and fundamental challenge in the field of 3D recovery. Heuristic approaches involve with the ill-posed problem for inferring the spatial variant defocusing blur, as the desired blur cannot be distinguished from the inherent blur. Given a prior knowledge of the defocus model, the problem become well-posed with an analytic solution for the relative blur between two images, taken at the same viewpoint with different camera settings for the focus. The Gaussian model stands out as an optimal choice for real-time applications, due to its mathematical simplicity and computational efficiency. And theoretically, it is the only model can be applied at the same time to both the absolute blur caused by depth in a single image and the relative blur resulting from depth differences between two images. This paper introduces the settings, for conventional imaging devices, to ensure that the defocusing operator adheres to the Gaussian model. Defocus analysis begins within the framework of geometric optics and is conducted by defocus aberration theory in diffraction-limited optics to obtain the accuracy of fitting the actual model to its Gaussian approximation. The results for a typical set of focused depths between $1$ and $100$ meters, with a maximum depth variation of $10\%$ at the focused depth, confirm the Gaussian model's applicability for defocus operators in most imaging devices. The findings demonstrate a maximum Mean Absolute Error $(\!M\!A\!E)$ of less than $1\%$, underscoring the model's accuracy and reliability.

</details>


### [133] [SRU-Pix2Pix: A Fusion-Driven Generator Network for Medical Image Translation with Few-Shot Learning](https://arxiv.org/abs/2601.04785)
*Xihe Qiu,Yang Dai,Xiaoyu Tan,Sijia Li,Fenghao Sun,Lu Gan,Liang Liu*

Main category: cs.CV

TL;DR: Enhanced Pix2Pix framework with SEResNet and U-Net++ improves MRI image translation quality and structural fidelity under few-shot conditions.


<details>
  <summary>Details</summary>
Motivation: MRI has limitations in acquisition time, cost, and resolution. Image translation can address these, but existing Pix2Pix methods haven't been fully explored for medical imaging.

Method: Enhanced Pix2Pix framework integrating Squeeze-and-Excitation Residual Networks (SEResNet) for channel attention and U-Net++ for multi-scale feature fusion, with simplified PatchGAN discriminator.

Result: Achieves consistent structural fidelity and superior image quality across multiple intra-modality MRI translation tasks with fewer than 500 images, showing strong generalization ability.

Conclusion: Proposed method provides an effective extension of Pix2Pix for medical image translation, addressing MRI limitations through improved image generation quality.

Abstract: Magnetic Resonance Imaging (MRI) provides detailed tissue information, but its clinical application is limited by long acquisition time, high cost, and restricted resolution. Image translation has recently gained attention as a strategy to address these limitations. Although Pix2Pix has been widely applied in medical image translation, its potential has not been fully explored. In this study, we propose an enhanced Pix2Pix framework that integrates Squeeze-and-Excitation Residual Networks (SEResNet) and U-Net++ to improve image generation quality and structural fidelity. SEResNet strengthens critical feature representation through channel attention, while U-Net++ enhances multi-scale feature fusion. A simplified PatchGAN discriminator further stabilizes training and refines local anatomical realism. Experimental results demonstrate that under few-shot conditions with fewer than 500 images, the proposed method achieves consistent structural fidelity and superior image quality across multiple intra-modality MRI translation tasks, showing strong generalization ability. These results suggest an effective extension of Pix2Pix for medical image translation.

</details>


### [134] [Measurement-Consistent Langevin Corrector: A Remedy for Latent Diffusion Inverse Solvers](https://arxiv.org/abs/2601.04791)
*Lee Hyoseok,Sohwi Lim,Eunju Cha,Tae-Hyun Oh*

Main category: cs.CV

TL;DR: MCLC (Measurement-Consistent Langevin Corrector) is a plug-and-play module that stabilizes latent diffusion model-based inverse solvers by reducing the discrepancy between solver and true reverse diffusion dynamics through measurement-consistent Langevin updates.


<details>
  <summary>Details</summary>
Motivation: Existing latent diffusion inverse solvers suffer from instability, producing undesirable artifacts and degraded quality. The authors identify this instability stems from a discrepancy between the solver's and true reverse diffusion dynamics.

Method: MCLC is a theoretically grounded correction module that performs measurement-consistent Langevin updates to reduce the gap between solver and true reverse diffusion dynamics. Unlike prior approaches that rely on linear manifold assumptions (which often don't hold in latent space), MCLC operates without this assumption.

Result: MCLC demonstrates effectiveness and compatibility with existing solvers across diverse image restoration tasks. The method provides more stable and reliable behavior compared to previous approaches.

Conclusion: MCLC represents a key step toward more robust zero-shot inverse problem solvers by addressing the fundamental instability issue in latent diffusion model-based inverse solving.

Abstract: With recent advances in generative models, diffusion models have emerged as powerful priors for solving inverse problems in each domain. Since Latent Diffusion Models (LDMs) provide generic priors, several studies have explored their potential as domain-agnostic zero-shot inverse solvers. Despite these efforts, existing latent diffusion inverse solvers suffer from their instability, exhibiting undesirable artifacts and degraded quality. In this work, we first identify the instability as a discrepancy between the solver's and true reverse diffusion dynamics, and show that reducing this gap stabilizes the solver. Building on this, we introduce Measurement-Consistent Langevin Corrector (MCLC), a theoretically grounded plug-and-play correction module that remedies the LDM-based inverse solvers through measurement-consistent Langevin updates. Compared to prior approaches that rely on linear manifold assumptions, which often do not hold in latent space, MCLC operates without this assumption, leading to more stable and reliable behavior. We experimentally demonstrate the effectiveness of MCLC and its compatibility with existing solvers across diverse image restoration tasks. Additionally, we analyze blob artifacts and offer insights into their underlying causes. We highlight that MCLC is a key step toward more robust zero-shot inverse problem solvers.

</details>


### [135] [PyramidalWan: On Making Pretrained Video Model Pyramidal for Efficient Inference](https://arxiv.org/abs/2601.04792)
*Denis Korzhenkov,Adil Karjauv,Animesh Karnewar,Mohsen Ghafoorian,Amirhossein Habibian*

Main category: cs.CV

TL;DR: A pipeline that converts pretrained diffusion models into pyramidal models via low-cost finetuning, maintaining quality while improving computational efficiency for video generation.


<details>
  <summary>Details</summary>
Motivation: Existing pyramidal video models trained from scratch underperform compared to state-of-the-art systems in visual quality, and there's a need to leverage pretrained diffusion models more efficiently while reducing computational costs.

Method: Develop a pipeline to convert pretrained diffusion models into pyramidal models through low-cost finetuning, and investigate various step distillation strategies to enhance inference efficiency within pyramidal architectures.

Result: Successfully transforms pretrained models into pyramidal versions without quality degradation, achieving computational efficiency improvements while maintaining output video quality comparable to original models.

Conclusion: The proposed pipeline enables efficient conversion of existing diffusion models into pyramidal architectures, offering a practical approach to reduce inference costs while preserving visual quality, with step distillation further enhancing efficiency.

Abstract: Recently proposed pyramidal models decompose the conventional forward and backward diffusion processes into multiple stages operating at varying resolutions. These models handle inputs with higher noise levels at lower resolutions, while less noisy inputs are processed at higher resolutions. This hierarchical approach significantly reduces the computational cost of inference in multi-step denoising models. However, existing open-source pyramidal video models have been trained from scratch and tend to underperform compared to state-of-the-art systems in terms of visual plausibility. In this work, we present a pipeline that converts a pretrained diffusion model into a pyramidal one through low-cost finetuning, achieving this transformation without degradation in quality of output videos. Furthermore, we investigate and compare various strategies for step distillation within pyramidal models, aiming to further enhance the inference efficiency. Our results are available at https://qualcomm-ai-research.github.io/PyramidalWan.

</details>


### [136] [Detector-Augmented SAMURAI for Long-Duration Drone Tracking](https://arxiv.org/abs/2601.04798)
*Tamara R. Lenhard,Andreas Weinmann,Hichem Snoussi,Tobias Koch*

Main category: cs.CV

TL;DR: SAMURAI foundation model shows strong drone tracking potential; detector-augmented extension improves robustness, especially for long sequences and exit-re-entry events.


<details>
  <summary>Details</summary>
Motivation: Drone tracking is critical for surveillance but current RGB-based methods are limited and rely on conventional motion models. Foundation models like SAMURAI show promise in other domains but haven't been tested for drone tracking.

Method: Systematic evaluation of SAMURAI for drone tracking, plus a detector-augmented extension to address bounding-box initialization sensitivity and sequence length limitations.

Result: Detector-augmented SAMURAI significantly improves robustness in complex urban environments, especially for long sequences and exit-re-entry events. Achieves up to +0.393 success rate improvement and -0.475 FNR reduction.

Conclusion: SAMURAI shows strong potential for drone tracking, and detector augmentation effectively addresses its limitations, making it suitable for robust long-term surveillance applications.

Abstract: Robust long-term tracking of drone is a critical requirement for modern surveillance systems, given their increasing threat potential. While detector-based approaches typically achieve strong frame-level accuracy, they often suffer from temporal inconsistencies caused by frequent detection dropouts. Despite its practical relevance, research on RGB-based drone tracking is still limited and largely reliant on conventional motion models. Meanwhile, foundation models like SAMURAI have established their effectiveness across other domains, exhibiting strong category-agnostic tracking performance. However, their applicability in drone-specific scenarios has not been investigated yet. Motivated by this gap, we present the first systematic evaluation of SAMURAI's potential for robust drone tracking in urban surveillance settings. Furthermore, we introduce a detector-augmented extension of SAMURAI to mitigate sensitivity to bounding-box initialization and sequence length. Our findings demonstrate that the proposed extension significantly improves robustness in complex urban environments, with pronounced benefits in long-duration sequences - especially under drone exit-re-entry events. The incorporation of detector cues yields consistent gains over SAMURAI's zero-shot performance across datasets and metrics, with success rate improvements of up to +0.393 and FNR reductions of up to -0.475.

</details>


### [137] [Integrated Framework for Selecting and Enhancing Ancient Marathi Inscription Images from Stone, Metal Plate, and Paper Documents](https://arxiv.org/abs/2601.04800)
*Bapu D. Chendage,Rajivkumar S. Mente*

Main category: cs.CV

TL;DR: Proposes binarization and preprocessing techniques to enhance degraded ancient script images, improving readability and classification accuracy for stone, metal plate, and document inscriptions.


<details>
  <summary>Details</summary>
Motivation: Ancient script images suffer from severe background noise, low contrast, and degradation from aging/environmental effects, with foreground text and background often having similar visual characteristics, making inscriptions difficult to read.

Method: Image enhancement approach based on binarization and complementary preprocessing techniques for removing stains and enhancing unclear ancient text.

Result: K-NN classifier achieved 55.7%, 62%, and 65.6% accuracy for stone, metal plate, and document scripts respectively. SVM classifier achieved 53.2%, 59.5%, and 67.8% accuracy.

Conclusion: The proposed enhancement method effectively improves readability of ancient Marathi inscription images, as demonstrated by classification accuracy improvements across different script types.

Abstract: Ancient script images often suffer from severe background noise, low contrast, and degradation caused by aging and environmental effects. In many cases, the foreground text and background exhibit similar visual characteristics, making the inscriptions difficult to read. The primary objective of image enhancement is to improve the readability of such degraded ancient images. This paper presents an image enhancement approach based on binarization and complementary preprocessing techniques for removing stains and enhancing unclear ancient text. The proposed methods are evaluated on different types of ancient scripts, including inscriptions on stone, metal plates, and historical documents. Experimental results show that the proposed approach achieves classification accuracies of 55.7%, 62%, and 65.6% for stone, metal plate, and document scripts, respectively, using the K-Nearest Neighbor (K-NN) classifier. Using the Support Vector Machine (SVM) classifier, accuracies of 53.2%, 59.5%, and 67.8% are obtained. The results demonstrate the effectiveness of the proposed enhancement method in improving the readability of ancient Marathi inscription images.

</details>


### [138] [SOVABench: A Vehicle Surveillance Action Retrieval Benchmark for Multimodal Large Language Models](https://arxiv.org/abs/2601.04824)
*Oriol Rabasseda,Zenjie Li,Kamal Nasrollahi,Sergio Escalera*

Main category: cs.CV

TL;DR: SOVABench is a new surveillance video retrieval benchmark focusing on vehicle actions, with two evaluation protocols for cross-action discrimination and temporal understanding. The paper also introduces a training-free MLLM framework that generates interpretable embeddings and achieves strong performance on this benchmark.


<details>
  <summary>Details</summary>
Motivation: Existing video retrieval benchmarks focus on scene-level similarity rather than action discrimination needed for surveillance applications. There's a gap in evaluating vehicle-related action understanding in real-world surveillance contexts.

Method: 1) Created SOVABench from real surveillance footage with vehicle actions; 2) Defined two evaluation protocols (inter-pair for cross-action discrimination, intra-pair for temporal direction); 3) Developed a training-free framework using MLLMs to generate interpretable embeddings from image/video descriptions.

Result: Action distinctions remain challenging for state-of-the-art vision/multimodal models despite being intuitive for humans. The proposed MLLM framework achieves strong performance on SOVABench and outperforms contrastive Vision-Language Models on spatial and counting benchmarks where they often fail.

Conclusion: SOVABench addresses the gap in surveillance-oriented video retrieval evaluation, and the MLLM-based framework provides an effective training-free approach for interpretable embeddings that handles challenging action discrimination tasks in surveillance contexts.

Abstract: Automatic identification of events and recurrent behavior analysis are critical for video surveillance. However, most existing content-based video retrieval benchmarks focus on scene-level similarity and do not evaluate the action discrimination required in surveillance. To address this gap, we introduce SOVABench (Surveillance Opposite Vehicle Actions Benchmark), a real-world retrieval benchmark built from surveillance footage and centered on vehicle-related actions. SOVABench defines two evaluation protocols (inter-pair and intra-pair) to assess cross-action discrimination and temporal direction understanding. Although action distinctions are generally intuitive for human observers, our experiments show that they remain challenging for state-of-the-art vision and multimodal models.
  Leveraging the visual reasoning and instruction-following capabilities of Multimodal Large Language Models (MLLMs), we present a training-free framework for producing interpretable embeddings from MLLM-generated descriptions for both images and videos. The framework achieves strong performance on SOVABench as well as on several spatial and counting benchmarks where contrastive Vision-Language Models often fail. The code, annotations, and instructions to construct the benchmark are publicly available.

</details>


### [139] [Character Detection using YOLO for Writer Identification in multiple Medieval books](https://arxiv.org/abs/2601.04834)
*Alessandra Scotto di Freca,Tiziana D Alessandro,Francesco Fontanella,Filippo Sarria,Claudio De Stefano*

Main category: cs.CV

TL;DR: The paper proposes using YOLO object detection instead of template matching to identify scribes in medieval manuscripts by detecting the letter "a", achieving better accuracy and enabling reliable writer identification in unseen manuscripts.


<details>
  <summary>Details</summary>
Motivation: Paleography faces challenges in dating manuscripts and understanding writing evolution through scribe identification. While digital technologies have advanced, the general problem remains unsolved. Previous template matching approaches had limitations requiring appropriate thresholds.

Method: Replaced previous template matching and CNN approach with YOLOv5 object detection model to identify the letter "a" in medieval manuscripts. YOLO completely substitutes the previous template matching technique and CNN classification system.

Result: YOLO effectively extracts more letters than template matching, leading to more accurate second-stage classification. The YOLO confidence score enables developing a rejection threshold system for reliable writer identification even in unseen manuscripts.

Conclusion: YOLO object detection outperforms template matching for scribe identification in paleography, providing better letter extraction and enabling robust writer attribution systems with confidence-based rejection thresholds for unseen manuscripts.

Abstract: Paleography is the study of ancient and historical handwriting, its key objectives include the dating of manuscripts and understanding the evolution of writing. Estimating when a document was written and tracing the development of scripts and writing styles can be aided by identifying the individual scribes who contributed to a medieval manuscript. Although digital technologies have made significant progress in this field, the general problem remains unsolved and continues to pose open challenges. ... We previously proposed an approach focused on identifying specific letters or abbreviations that characterize each writer. In that study, we considered the letter "a", as it was widely present on all pages of text and highly distinctive, according to the suggestions of expert paleographers. We used template matching techniques to detect the occurrences of the character "a" on each page and the convolutional neural network (CNN) to attribute each instance to the correct scribe. Moving from the interesting results achieved from this previous system and being aware of the limitations of the template matching technique, which requires an appropriate threshold to work, we decided to experiment in the same framework with the use of the YOLO object detection model to identify the scribe who contributed to the writing of different medieval books. We considered the fifth version of YOLO to implement the YOLO object detection model, which completely substituted the template matching and CNN used in the previous work. The experimental results demonstrate that YOLO effectively extracts a greater number of letters considered, leading to a more accurate second-stage classification. Furthermore, the YOLO confidence score provides a foundation for developing a system that applies a rejection threshold, enabling reliable writer identification even in unseen manuscripts.

</details>


### [140] [DivAS: Interactive 3D Segmentation of NeRFs via Depth-Weighted Voxel Aggregation](https://arxiv.org/abs/2601.04860)
*Ayush Pande*

Main category: cs.CV

TL;DR: DivAS is an optimization-free, interactive framework for segmenting NeRFs using 2D SAM masks refined with depth priors and aggregated into 3D voxels via custom CUDA kernel.


<details>
  <summary>Details</summary>
Motivation: Existing NeRF segmentation methods are optimization-based, requiring slow per-scene training and sacrificing zero-shot capabilities of 2D foundation models like SAM.

Method: Fast GUI workflow where user point prompts generate 2D SAM masks, refined using NeRF-derived depth priors for geometric accuracy. Custom CUDA kernel aggregates refined multi-view masks into unified 3D voxel grid in under 200ms.

Result: Achieves segmentation quality comparable to optimization-based methods, 2-2.5x faster end-to-end, and up to 10x faster excluding user prompting time on Mip-NeRF 360° and LLFF datasets.

Conclusion: DivAS provides real-time interactive NeRF segmentation without per-scene training, combining 2D foundation models' zero-shot capabilities with 3D geometric priors for efficient segmentation.

Abstract: Existing methods for segmenting Neural Radiance Fields (NeRFs) are often optimization-based, requiring slow per-scene training that sacrifices the zero-shot capabilities of 2D foundation models. We introduce DivAS (Depth-interactive Voxel Aggregation Segmentation), an optimization-free, fully interactive framework that addresses these limitations. Our method operates via a fast GUI-based workflow where 2D SAM masks, generated from user point prompts, are refined using NeRF-derived depth priors to improve geometric accuracy and foreground-background separation. The core of our contribution is a custom CUDA kernel that aggregates these refined multi-view masks into a unified 3D voxel grid in under 200ms, enabling real-time visual feedback. This optimization-free design eliminates the need for per-scene training. Experiments on Mip-NeRF 360° and LLFF show that DivAS achieves segmentation quality comparable to optimization-based methods, while being 2-2.5x faster end-to-end, and up to an order of magnitude faster when excluding user prompting time.

</details>


### [141] [Scaling Vision Language Models for Pharmaceutical Long Form Video Reasoning on Industrial GenAI Platform](https://arxiv.org/abs/2601.04891)
*Suyash Mishra,Qiang Li,Srikanth Patil,Satyanarayan Pati,Baddu Narendra*

Main category: cs.CV

TL;DR: Industrial framework for processing 200K+ PDFs, 25K+ videos, and multilingual audio in pharmaceutical domains, with analysis of 40+ VLMs showing 3-8x efficiency gains and multimodality benefits but revealing temporal reasoning bottlenecks.


<details>
  <summary>Details</summary>
Motivation: Most VLM evaluations focus on short videos with unlimited resources, but industrial settings like pharmaceutical content understanding require processing long-form videos under strict GPU, latency, and cost constraints where existing approaches fail to scale.

Method: Developed an industrial GenAI framework processing massive multimodal data (200K+ PDFs, 25K+ videos, multilingual audio), empirically analyzed 40+ VLMs on Video-MME and MMBench benchmarks plus proprietary dataset of 25,326 videos across 14 disease areas.

Result: Achieved 3-8x efficiency gains with SDPA attention on commodity GPUs, multimodality improved 8/12 task domains (especially length-dependent tasks), identified bottlenecks in temporal alignment and keyframe detection across open- and closed-source VLMs.

Conclusion: Rather than proposing new models, the paper characterizes practical limits, trade-offs, and failure patterns of current VLMs under realistic deployment constraints, providing actionable guidance for designing scalable multimodal systems for long-form video understanding in industrial domains.

Abstract: Vision Language Models (VLMs) have shown strong performance on multimodal reasoning tasks, yet most evaluations focus on short videos and assume unconstrained computational resources. In industrial settings such as pharmaceutical content understanding, practitioners must process long-form videos under strict GPU, latency, and cost constraints, where many existing approaches fail to scale. In this work, we present an industrial GenAI framework that processes over 200,000 PDFs, 25,326 videos across eight formats (e.g., MP4, M4V, etc.), and 888 multilingual audio files in more than 20 languages. Our study makes three contributions: (i) an industrial large-scale architecture for multimodal reasoning in pharmaceutical domains; (ii) empirical analysis of over 40 VLMs on two leading benchmarks (Video-MME and MMBench) and proprietary dataset of 25,326 videos across 14 disease areas; and (iii) four findings relevant to long-form video reasoning: the role of multimodality, attention mechanism trade-offs, temporal reasoning limits, and challenges of video splitting under GPU constraints. Results show 3-8 times efficiency gains with SDPA attention on commodity GPUs, multimodality improving up to 8/12 task domains (especially length-dependent tasks), and clear bottlenecks in temporal alignment and keyframe detection across open- and closed-source VLMs. Rather than proposing a new "A+B" model, this paper characterizes practical limits, trade-offs, and failure patterns of current VLMs under realistic deployment constraints, and provide actionable guidance for both researchers and practitioners designing scalable multimodal systems for long-form video understanding in industrial domains.

</details>


### [142] [Rotation-Robust Regression with Convolutional Model Trees](https://arxiv.org/abs/2601.04899)
*Hongyi Li,William Ward Armstrong,Jun Xu*

Main category: cs.CV

TL;DR: The paper studies rotation-robust learning using Convolutional Model Trees with geometry-aware inductive biases and deployment-time orientation search, showing improved robustness under severe rotations but limitations near canonical orientations.


<details>
  <summary>Details</summary>
Motivation: To develop rotation-robust image learning methods that maintain performance under in-plane rotations, addressing the challenge of geometric transformations at deployment time.

Method: Uses Convolutional Model Trees (CMTs) with three geometry-aware inductive biases: convolutional smoothing, tilt dominance constraint, and importance-based pruning. Also implements deployment-time orientation search that selects discrete rotations maximizing forest-level confidence without updating parameters.

Result: Orientation search improves robustness under severe rotations but can be harmful near canonical orientation when confidence is misaligned with correctness. Consistent trends observed on MNIST digit recognition implemented as one-vs-rest regression.

Conclusion: The approach shows promise for rotation-robust learning using model-tree ensembles with geometry-aware inductive biases and confidence-based orientation selection, but has limitations when confidence metrics don't align with prediction correctness near canonical orientations.

Abstract: We study rotation-robust learning for image inputs using Convolutional Model Trees (CMTs) [1], whose split and leaf coefficients can be structured on the image grid and transformed geometrically at deployment time. In a controlled MNIST setting with a rotation-invariant regression target, we introduce three geometry-aware inductive biases for split directions -- convolutional smoothing, a tilt dominance constraint, and importance-based pruning -- and quantify their impact on robustness under in-plane rotations. We further evaluate a deployment-time orientation search that selects a discrete rotation maximizing a forest-level confidence proxy without updating model parameters. Orientation search improves robustness under severe rotations but can be harmful near the canonical orientation when confidence is misaligned with correctness. Finally, we observe consistent trends on MNIST digit recognition implemented as one-vs-rest regression, highlighting both the promise and limitations of confidence-based orientation selection for model-tree ensembles.

</details>


### [143] [Prototypicality Bias Reveals Blindspots in Multimodal Evaluation Metrics](https://arxiv.org/abs/2601.04946)
*Subhadeep Roy,Gagan Bhatia,Steffen Eger*

Main category: cs.CV

TL;DR: The paper introduces ProtoBias benchmark to evaluate prototypicality bias in text-to-image metrics, showing current metrics often misrank semantically correct but non-prototypical images vs. incorrect but prototypical ones, and proposes ProtoScore as a more robust metric.


<details>
  <summary>Details</summary>
Motivation: Automatic metrics are widely used to evaluate text-to-image models, but it's unclear whether they truly assess semantic correctness or just favor visually/socially prototypical images learned from biased data distributions. The paper aims to study this prototypicality bias as a systematic failure mode in multimodal evaluation.

Method: The authors create ProtoBias benchmark with controlled contrastive pairs spanning Animals, Objects, and Demography categories. Each pair contains: 1) semantically correct but non-prototypical image, and 2) subtly incorrect yet prototypical adversarial counterpart. This enables directional evaluation of whether metrics follow textual semantics or default to prototypes. They also propose ProtoScore, a robust 7B-parameter metric designed to reduce failure rates.

Result: Widely used metrics (CLIPScore, PickScore, VQA-based scores) frequently misrank the contrastive pairs, favoring prototypical but incorrect images over semantically correct but non-prototypical ones. Even LLM-as-Judge systems show uneven robustness in socially grounded cases. Human evaluations consistently favor semantic correctness with larger decision margins. ProtoScore substantially reduces failure rates and suppresses misranking while running much faster than GPT-5 inference.

Conclusion: Current text-to-image evaluation metrics suffer from prototypicality bias, systematically favoring prototypical images even when they're semantically incorrect. The proposed ProtoScore metric demonstrates improved robustness against this bias while maintaining efficiency, approaching the performance of much larger closed-source judges.

Abstract: Automatic metrics are now central to evaluating text-to-image models, often substituting for human judgment in benchmarking and large-scale filtering. However, it remains unclear whether these metrics truly prioritize semantic correctness or instead favor visually and socially prototypical images learned from biased data distributions. We identify and study \emph{prototypicality bias} as a systematic failure mode in multimodal evaluation. We introduce a controlled contrastive benchmark \textsc{\textbf{ProtoBias}} (\textit{\textbf{Proto}typical \textbf{Bias}}), spanning Animals, Objects, and Demography images, where semantically correct but non-prototypical images are paired with subtly incorrect yet prototypical adversarial counterparts. This setup enables a directional evaluation of whether metrics follow textual semantics or default to prototypes. Our results show that widely used metrics, including CLIPScore, PickScore, and VQA-based scores, frequently misrank these pairs, while even LLM-as-Judge systems exhibit uneven robustness in socially grounded cases. Human evaluations consistently favour semantic correctness with larger decision margins. Motivated by these findings, we propose \textbf{\textsc{ProtoScore}}, a robust 7B-parameter metric that substantially reduces failure rates and suppresses misranking, while running at orders of magnitude faster than the inference time of GPT-5, approaching the robustness of much larger closed-source judges.

</details>


### [144] [TEA: Temporal Adaptive Satellite Image Semantic Segmentation](https://arxiv.org/abs/2601.04956)
*Juyuan Kang,Hao Zhu,Yan Zhu,Wei Zhang,Jianing Chen,Tianxiang Xiao,Yike Ma,Hao Jiang,Feng Dai*

Main category: cs.CV

TL;DR: TEA: A temporal adaptive SITS semantic segmentation method that enhances model resilience under varying sequence lengths through teacher-student knowledge transfer and full-sequence reconstruction.


<details>
  <summary>Details</summary>
Motivation: Existing SITS segmentation approaches overlook generalization across scenarios with varying temporal lengths, leading to poor segmentation results when sequence lengths differ from training data.

Method: Proposes TEA with a teacher model containing global sequence knowledge to guide a student model with adaptive temporal inputs. Uses intermediate embedding, prototypes, and soft labels for knowledge transfer, dynamic aggregation to prevent forgetting, and full-sequence reconstruction as auxiliary task.

Result: Extensive experiments show remarkable improvements across inputs of different temporal lengths on common benchmarks.

Conclusion: TEA effectively addresses the temporal length generalization problem in SITS segmentation, enhancing model resilience and performance across varying sequence lengths.

Abstract: Crop mapping based on satellite images time-series (SITS) holds substantial economic value in agricultural production settings, in which parcel segmentation is an essential step. Existing approaches have achieved notable advancements in SITS segmentation with predetermined sequence lengths. However, we found that these approaches overlooked the generalization capability of models across scenarios with varying temporal length, leading to markedly poor segmentation results in such cases. To address this issue, we propose TEA, a TEmporal Adaptive SITS semantic segmentation method to enhance the model's resilience under varying sequence lengths. We introduce a teacher model that encapsulates the global sequence knowledge to guide a student model with adaptive temporal input lengths. Specifically, teacher shapes the student's feature space via intermediate embedding, prototypes and soft label perspectives to realize knowledge transfer, while dynamically aggregating student model to mitigate knowledge forgetting. Finally, we introduce full-sequence reconstruction as an auxiliary task to further enhance the quality of representations across inputs of varying temporal lengths. Through extensive experiments, we demonstrate that our method brings remarkable improvements across inputs of different temporal lengths on common benchmarks. Our code will be publicly available.

</details>


### [145] [SparseLaneSTP: Leveraging Spatio-Temporal Priors with Sparse Transformers for 3D Lane Detection](https://arxiv.org/abs/2601.04968)
*Maximilian Pittner,Joel Janai,Mario Faigle,Alexandru Paul Condurache*

Main category: cs.CV

TL;DR: SparseLaneSTP: A sparse lane transformer that integrates geometric lane priors and temporal information for improved 3D lane detection, with a new auto-labeled dataset.


<details>
  <summary>Details</summary>
Motivation: Existing 3D lane detection methods have limitations: dense BEV approaches suffer from poor feature representation due to erroneous transformations, sparse detectors ignore valuable lane-specific priors, and no methods utilize historical lane observations to resolve visibility ambiguities.

Method: SparseLaneSTP integrates geometric lane structure properties and temporal information into a sparse lane transformer. Key innovations include: lane-specific spatio-temporal attention mechanism, continuous lane representation for sparse architectures, and temporal regularization. The paper also introduces a new precise 3D lane dataset using auto-labeling.

Result: State-of-the-art performance across all detection and error metrics on existing 3D lane detection benchmarks and on the novel dataset. The method demonstrates benefits of integrating lane priors and temporal information.

Conclusion: SparseLaneSTP effectively addresses limitations of existing 3D lane detection methods by incorporating lane-specific geometric priors and temporal information, while the new dataset provides more precise ground truth for evaluation.

Abstract: 3D lane detection has emerged as a critical challenge in autonomous driving, encompassing identification and localization of lane markings and the 3D road surface. Conventional 3D methods detect lanes from dense birds-eye-viewed (BEV) features, though erroneous transformations often result in a poor feature representation misaligned with the true 3D road surface. While recent sparse lane detectors have surpassed dense BEV approaches, they completely disregard valuable lane-specific priors. Furthermore, existing methods fail to utilize historic lane observations, which yield the potential to resolve ambiguities in situations of poor visibility. To address these challenges, we present SparseLaneSTP, a novel method that integrates both geometric properties of the lane structure and temporal information into a sparse lane transformer. It introduces a new lane-specific spatio-temporal attention mechanism, a continuous lane representation tailored for sparse architectures as well as temporal regularization. Identifying weaknesses of existing 3D lane datasets, we also introduce a precise and consistent 3D lane dataset using a simple yet effective auto-labeling strategy. Our experimental section proves the benefits of our contributions and demonstrates state-of-the-art performance across all detection and error metrics on existing 3D lane detection benchmarks as well as on our novel dataset.

</details>


### [146] [OceanSplat: Object-aware Gaussian Splatting with Trinocular View Consistency for Underwater Scene Reconstruction](https://arxiv.org/abs/2601.04984)
*Minseong Kweon,Jinsun Park*

Main category: cs.CV

TL;DR: OceanSplat is a 3D Gaussian Splatting method for underwater scenes that uses trinocular view consistency and synthetic epipolar depth priors to overcome optical degradation, with depth-aware alpha adjustment to reduce medium-induced artifacts.


<details>
  <summary>Details</summary>
Motivation: Underwater scenes suffer from optical degradation (scattering, absorption) that causes multi-view inconsistencies, making accurate 3D reconstruction challenging. Existing methods struggle with floating artifacts and poor geometry representation in scattering media.

Method: 1) Enforces trinocular view consistency by rendering horizontally/vertically translated camera views and aligning via inverse warping. 2) Derives synthetic epipolar depth prior through triangulation of translated views as self-supervised depth regularizer. 3) Proposes depth-aware alpha adjustment that modulates Gaussian opacity based on z-component and viewing direction during early training to prevent medium-induced primitives.

Result: OceanSplat substantially outperforms existing methods on both real-world underwater and simulated scenes for scene reconstruction and restoration. It disentangles 3D Gaussians from scattering medium, enabling robust object geometry representation and significantly reducing floating artifacts.

Conclusion: The proposed geometric constraints (trinocular consistency and synthetic depth prior) combined with depth-aware alpha adjustment effectively overcome underwater optical degradation, allowing 3D Gaussian Splatting to accurately represent underwater geometry while minimizing medium-induced artifacts.

Abstract: We introduce OceanSplat, a novel 3D Gaussian Splatting-based approach for accurately representing 3D geometry in underwater scenes. To overcome multi-view inconsistencies caused by underwater optical degradation, our method enforces trinocular view consistency by rendering horizontally and vertically translated camera views relative to each input view and aligning them via inverse warping. Furthermore, these translated camera views are used to derive a synthetic epipolar depth prior through triangulation, which serves as a self-supervised depth regularizer. These geometric constraints facilitate the spatial optimization of 3D Gaussians and preserve scene structure in underwater environments. We also propose a depth-aware alpha adjustment that modulates the opacity of 3D Gaussians during early training based on their $z$-component and viewing direction, deterring the formation of medium-induced primitives. With our contributions, 3D Gaussians are disentangled from the scattering medium, enabling robust representation of object geometry and significantly reducing floating artifacts in reconstructed underwater scenes. Experiments on real-world underwater and simulated scenes demonstrate that OceanSplat substantially outperforms existing methods for both scene reconstruction and restoration in scattering media.

</details>


### [147] [Higher-Order Adversarial Patches for Real-Time Object Detectors](https://arxiv.org/abs/2601.04991)
*Jens Bayer,Stefan Becker,David Münch,Michael Arens,Jürgen Beyerer*

Main category: cs.CV

TL;DR: Higher-order adversarial attacks on object detectors show stronger generalization than lower-order attacks, and adversarial training alone is insufficient for effective defense.


<details>
  <summary>Details</summary>
Motivation: To investigate the impact of higher-order adversarial attacks on object detectors, examining the cat-and-mouse dynamic between attack patterns and adversarial training defenses.

Method: Used YOLOv10 object detector with adversarial patches in evasion attacks, successively training attack patterns and hardening detectors with adversarial training in a circular manner.

Result: Higher-order adversarial patches demonstrate stronger generalization capacity compared to lower-order patches, and adversarial training alone is insufficient to effectively harden object detectors against such attacks.

Conclusion: Higher-order adversarial attacks pose a significant threat to object detectors with enhanced generalization, requiring more robust defense strategies beyond standard adversarial training.

Abstract: Higher-order adversarial attacks can directly be considered the result of a cat-and-mouse game -- an elaborate action involving constant pursuit, near captures, and repeated escapes. This idiom describes the enduring circular training of adversarial attack patterns and adversarial training the best. The following work investigates the impact of higher-order adversarial attacks on object detectors by successively training attack patterns and hardening object detectors with adversarial training. The YOLOv10 object detector is chosen as a representative, and adversarial patches are used in an evasion attack manner. Our results indicate that higher-order adversarial patches are not only affecting the object detector directly trained on but rather provide a stronger generalization capacity compared to lower-order adversarial patches. Moreover, the results highlight that solely adversarial training is not sufficient to harden an object detector efficiently against this kind of adversarial attack. Code: https://github.com/JensBayer/HigherOrder

</details>


### [148] [Patch-based Representation and Learning for Efficient Deformation Modeling](https://arxiv.org/abs/2601.05035)
*Ruochen Chen,Thuy Tran,Shaifali Parashar*

Main category: cs.CV

TL;DR: PolyFit: A patch-based surface representation using local jet functions that enables efficient surface deformation by updating compact jet coefficients instead of per-vertex optimization.


<details>
  <summary>Details</summary>
Motivation: Traditional surface deformation methods require optimizing per-vertex degrees of freedom, which is computationally expensive for downstream tasks in computer vision and graphics. There's a need for more efficient representations that can generalize across different surface types and resolutions.

Method: PolyFit learns a patch-based surface representation by fitting jet functions locally on surface patches. It uses supervised learning from analytic functions and real data. The representation allows deformation by updating a compact set of jet coefficients rather than per-vertex optimization.

Result: 1) Shape-from-template: Achieves competitive accuracy with test-time optimization, being markedly faster than offline physics-based solvers and outperforming recent physics-guided neural simulators in accuracy at modest additional runtime. 2) Garment draping: Trains a self-supervised, mesh- and garment-agnostic model that generalizes across resolutions and garment types, delivering up to an order-of-magnitude faster inference than strong baselines.

Conclusion: PolyFit provides an efficient patch-based surface representation that enables fast and accurate surface deformation for various computer vision and graphics applications, offering significant speed improvements while maintaining or improving accuracy compared to existing methods.

Abstract: In this paper, we present a patch-based representation of surfaces, PolyFit, which is obtained by fitting jet functions locally on surface patches. Such a representation can be learned efficiently in a supervised fashion from both analytic functions and real data. Once learned, it can be generalized to various types of surfaces. Using PolyFit, the surfaces can be efficiently deformed by updating a compact set of jet coefficients rather than optimizing per-vertex degrees of freedom for many downstream tasks in computer vision and graphics. We demonstrate the capabilities of our proposed methodologies with two applications: 1) Shape-from-template (SfT): where the goal is to deform the input 3D template of an object as seen in image/video. Using PolyFit, we adopt test-time optimization that delivers competitive accuracy while being markedly faster than offline physics-based solvers, and outperforms recent physics-guided neural simulators in accuracy at modest additional runtime. 2) Garment draping. We train a self-supervised, mesh- and garment-agnostic model that generalizes across resolutions and garment types, delivering up to an order-of-magnitude faster inference than strong baselines.

</details>


### [149] [From Understanding to Engagement: Personalized pharmacy Video Clips via Vision Language Models (VLMs)](https://arxiv.org/abs/2601.05059)
*Suyash Mishra,Qiang Li,Srikanth Patil,Anubhav Girdhar*

Main category: cs.CV

TL;DR: A domain-adapted Video to Video Clip Generation framework using ALMs and VLMs for pharmaceutical industry, achieving 3-4x speedup, 4x cost reduction, and improved clip quality over SOTA baselines.


<details>
  <summary>Details</summary>
Motivation: Traditional manual annotation of heterogeneous data modalities (text, images, video, audio, web links) in pharmaceutical industry is prone to inconsistencies, quality degradation, and inefficiencies, especially with large volumes of long video/audio data like clinical trial interviews and educational seminars.

Method: A domain-adapted Video to Video Clip Generation framework integrating Audio Language Models (ALMs) and Vision Language Models (VLMs) with: (1) reproducible Cut & Merge algorithm with fade in/out and timestamp normalization; (2) personalization mechanism based on role definition and prompt injection; (3) cost-efficient end-to-end pipeline strategy balancing ALM/VLM enhanced processing.

Result: Evaluations on Video MME benchmark (900 videos) and proprietary dataset of 16,159 pharmacy videos across 14 disease areas show 3-4 times speedup, 4 times cost reduction, competitive clip quality, improved clip coherence scores (0.348), and informativeness scores (0.721) over SOTA VLM baselines like Gemini 2.5 Pro.

Conclusion: The framework demonstrates potential for transparent, custom extractive, and compliance-supporting video summarization for life sciences, revolutionizing pharmaceutical digital transformation through intelligent, scalable, automated multi-modality content processing.

Abstract: Vision Language Models (VLMs) are poised to revolutionize the digital transformation of pharmacyceutical industry by enabling intelligent, scalable, and automated multi-modality content processing. Traditional manual annotation of heterogeneous data modalities (text, images, video, audio, and web links), is prone to inconsistencies, quality degradation, and inefficiencies in content utilization. The sheer volume of long video and audio data further exacerbates these challenges, (e.g. long clinical trial interviews and educational seminars).
  Here, we introduce a domain adapted Video to Video Clip Generation framework that integrates Audio Language Models (ALMs) and Vision Language Models (VLMs) to produce highlight clips. Our contributions are threefold: (i) a reproducible Cut & Merge algorithm with fade in/out and timestamp normalization, ensuring smooth transitions and audio/visual alignment; (ii) a personalization mechanism based on role definition and prompt injection for tailored outputs (marketing, training, regulatory); (iii) a cost efficient e2e pipeline strategy balancing ALM/VLM enhanced processing. Evaluations on Video MME benchmark (900) and our proprietary dataset of 16,159 pharmacy videos across 14 disease areas demonstrate 3 to 4 times speedup, 4 times cost reduction, and competitive clip quality. Beyond efficiency gains, we also report our methods improved clip coherence scores (0.348) and informativeness scores (0.721) over state of the art VLM baselines (e.g., Gemini 2.5 Pro), highlighting the potential of transparent, custom extractive, and compliance supporting video summarization for life sciences.

</details>


### [150] [Driving on Registers](https://arxiv.org/abs/2601.05083)
*Ellington Kirby,Alexandre Boulch,Yihong Xu,Yuan Yin,Gilles Puy,Éloi Zablocki,Andrei Bursuc,Spyros Gidaris,Renaud Marlet,Florent Bartoccioni,Anh-Quan Cao,Nermin Samet,Tuan-Hung VU,Matthieu Cord*

Main category: cs.CV

TL;DR: DrivoR is a simple transformer-based architecture for end-to-end autonomous driving that uses camera-aware register tokens to compress multi-camera features and lightweight decoders for trajectory generation and scoring.


<details>
  <summary>Details</summary>
Motivation: To create an efficient yet accurate end-to-end autonomous driving system that can handle multi-camera inputs while maintaining interpretability and adaptive behavior.

Method: Uses pretrained Vision Transformers with camera-aware register tokens to compress multi-camera features, then employs two lightweight transformer decoders: one for generating candidate trajectories and another for scoring them with interpretable sub-scores (safety, comfort, efficiency).

Result: Outperforms or matches strong contemporary baselines across NAVSIM-v1, NAVSIM-v2, and HUGSIM benchmarks, demonstrating accurate, efficient, and adaptive driving performance.

Conclusion: A pure-transformer architecture with targeted token compression is sufficient for accurate, efficient, and adaptive end-to-end autonomous driving, offering interpretable behavior-conditioned driving.

Abstract: We present DrivoR, a simple and efficient transformer-based architecture for end-to-end autonomous driving. Our approach builds on pretrained Vision Transformers (ViTs) and introduces camera-aware register tokens that compress multi-camera features into a compact scene representation, significantly reducing downstream computation without sacrificing accuracy. These tokens drive two lightweight transformer decoders that generate and then score candidate trajectories. The scoring decoder learns to mimic an oracle and predicts interpretable sub-scores representing aspects such as safety, comfort, and efficiency, enabling behavior-conditioned driving at inference. Despite its minimal design, DrivoR outperforms or matches strong contemporary baselines across NAVSIM-v1, NAVSIM-v2, and the photorealistic closed-loop HUGSIM benchmark. Our results show that a pure-transformer architecture, combined with targeted token compression, is sufficient for accurate, efficient, and adaptive end-to-end driving. Code and checkpoints will be made available via the project page.

</details>


### [151] [UniLiPs: Unified LiDAR Pseudo-Labeling with Geometry-Grounded Dynamic Scene Decomposition](https://arxiv.org/abs/2601.05105)
*Filippo Ghilotti,Samuel Brucker,Nahku Saidy,Matteo Matteucci,Mario Bijelic,Felix Heide*

Main category: cs.CV

TL;DR: Unsupervised 3D pseudo-labeling method using temporal-geometric consistency from LiDAR sweeps to lift text and 2D vision foundation model cues into 3D without manual labels.


<details>
  <summary>Details</summary>
Motivation: Unlabeled LiDAR logs are abundant but useless without human labels, creating a major cost barrier for autonomous perception research. The paper aims to overcome this bottleneck by leveraging temporal consistency to extract 3D information without manual supervision.

Method: Uses temporal-geometric consistency across LiDAR sweeps to fuse cues from text and 2D vision foundation models into 3D. Introduces unsupervised multi-modal pseudo-labeling with geometric priors from accumulated LiDAR maps, plus an iterative update rule enforcing joint geometric-semantic consistency while detecting moving objects from inconsistencies.

Result: Produces 3D semantic labels, 3D bounding boxes, and dense LiDAR scans with robust generalization across three datasets. Outperforms existing pseudo-labeling methods that require manual supervision. Improves depth prediction by 51.5% and 22.0% MAE in 80-150m and 150-250m ranges respectively.

Conclusion: Demonstrates effective unsupervised 3D pseudo-labeling using temporal-geometric consistency, enabling cost-effective utilization of unlabeled LiDAR data for autonomous perception without manual annotation.

Abstract: Unlabeled LiDAR logs, in autonomous driving applications, are inherently a gold mine of dense 3D geometry hiding in plain sight - yet they are almost useless without human labels, highlighting a dominant cost barrier for autonomous-perception research. In this work we tackle this bottleneck by leveraging temporal-geometric consistency across LiDAR sweeps to lift and fuse cues from text and 2D vision foundation models directly into 3D, without any manual input. We introduce an unsupervised multi-modal pseudo-labeling method relying on strong geometric priors learned from temporally accumulated LiDAR maps, alongside with a novel iterative update rule that enforces joint geometric-semantic consistency, and vice-versa detecting moving objects from inconsistencies. Our method simultaneously produces 3D semantic labels, 3D bounding boxes, and dense LiDAR scans, demonstrating robust generalization across three datasets. We experimentally validate that our method compares favorably to existing semantic segmentation and object detection pseudo-labeling methods, which often require additional manual supervision. We confirm that even a small fraction of our geometrically consistent, densified LiDAR improves depth prediction by 51.5% and 22.0% MAE in the 80-150 and 150-250 meters range, respectively.

</details>


### [152] [From Rays to Projections: Better Inputs for Feed-Forward View Synthesis](https://arxiv.org/abs/2601.05116)
*Zirui Wu,Zeren Jiang,Martin R. Oswald,Jie Song*

Main category: cs.CV

TL;DR: Proposed projective conditioning replaces raw camera parameters with stable 2D projective cues for view synthesis, reframing the task as image-to-image translation rather than brittle geometric regression.


<details>
  <summary>Details</summary>
Motivation: Existing feed-forward view synthesis models use Plücker ray maps that tie predictions to arbitrary world coordinates and are sensitive to small camera transformations, undermining geometric consistency. The paper investigates what inputs best condition models for robust and consistent view synthesis.

Method: Introduces projective conditioning which replaces raw camera parameters with target-view projective cues (stable 2D inputs). Also proposes masked autoencoding pretraining strategy tailored to these cues, enabling use of large-scale uncalibrated data for pretraining.

Result: Shows improved fidelity and stronger cross-view consistency compared to ray-conditioned baselines on view-consistency benchmark. Achieves state-of-the-art quality on standard novel view synthesis benchmarks.

Conclusion: Projective conditioning provides a more stable and effective approach to view synthesis by transforming the problem from brittle geometric regression to well-conditioned image-to-image translation, enabling better geometric consistency and performance.

Abstract: Feed-forward view synthesis models predict a novel view in a single pass with minimal 3D inductive bias. Existing works encode cameras as Plücker ray maps, which tie predictions to the arbitrary world coordinate gauge and make them sensitive to small camera transformations, thereby undermining geometric consistency. In this paper, we ask what inputs best condition a model for robust and consistent view synthesis. We propose projective conditioning, which replaces raw camera parameters with a target-view projective cue that provides a stable 2D input. This reframes the task from a brittle geometric regression problem in ray space to a well-conditioned target-view image-to-image translation problem. Additionally, we introduce a masked autoencoding pretraining strategy tailored to this cue, enabling the use of large-scale uncalibrated data for pretraining. Our method shows improved fidelity and stronger cross-view consistency compared to ray-conditioned baselines on our view-consistency benchmark. It also achieves state-of-the-art quality on standard novel view synthesis benchmarks.

</details>


### [153] [Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing](https://arxiv.org/abs/2601.05124)
*Runze He,Yiji Cheng,Tiankai Hang,Zhimin Li,Yu Xu,Zijin Yin,Shiyi Zhang,Wenxun Dai,Penghui Du,Ao Ma,Chunyu Wang,Qinglin Lu,Jizhong Han,Jiao Dai*

Main category: cs.CV

TL;DR: Re-Align is a unified framework that bridges understanding and generation gaps in in-context image generation/editing through structured reasoning-guided alignment and RL training.


<details>
  <summary>Details</summary>
Motivation: Current unified multimodal models have strong understanding capabilities but these strengths don't effectively transfer to image generation tasks, creating a gap between understanding and generation in in-context image generation and editing.

Method: 1. In-Context Chain-of-Thought (IC-CoT): Structured reasoning paradigm that decouples semantic guidance and reference association. 2. RL training scheme: Uses surrogate reward to measure alignment between structured reasoning text and generated images.

Result: Re-Align outperforms competitive methods of comparable model scale and resources on both in-context image generation and editing tasks.

Conclusion: The proposed Re-Align framework successfully bridges the understanding-generation gap through structured reasoning-guided alignment and RL training, demonstrating superior performance on ICGE tasks.

Abstract: In-context image generation and editing (ICGE) enables users to specify visual concepts through interleaved image-text prompts, demanding precise understanding and faithful execution of user intent. Although recent unified multimodal models exhibit promising understanding capabilities, these strengths often fail to transfer effectively to image generation. We introduce Re-Align, a unified framework that bridges the gap between understanding and generation through structured reasoning-guided alignment. At its core lies the In-Context Chain-of-Thought (IC-CoT), a structured reasoning paradigm that decouples semantic guidance and reference association, providing clear textual target and mitigating confusion among reference images. Furthermore, Re-Align introduces an effective RL training scheme that leverages a surrogate reward to measure the alignment between structured reasoning text and the generated image, thereby improving the model's overall performance on ICGE tasks. Extensive experiments verify that Re-Align outperforms competitive methods of comparable model scale and resources on both in-context image generation and editing tasks.

</details>


### [154] [VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding](https://arxiv.org/abs/2601.05125)
*Ignacio de Rodrigo,Alvaro J. Lopez-Lopez,Jaime Boal*

Main category: cs.CV

TL;DR: VERSE is a methodology for analyzing and improving Vision-Language Models for document understanding by visualizing their visual embedding space, identifying problematic regions, and generating targeted synthetic data to boost performance.


<details>
  <summary>Details</summary>
Motivation: Vision-Language Models for Visually-rich Document Understanding need better analysis tools to understand their visual embedding spaces and improve performance in error-prone areas, especially to help on-premise models compete with SaaS solutions.

Method: VERSE visualizes latent representations of VLMs to assess model feasibility, identifies problematic regions in the visual embedding space, and guides generation of synthetic data targeting those specific clusters for retraining.

Result: VERSE successfully uncovers visual features associated with error-prone clusters, and retraining with synthetic samples containing these features substantially boosts F1 performance without degrading generalization. On-premise models (Donut, Idefics2) optimized with VERSE match or surpass SaaS solutions (GPT-4, Pixtral).

Conclusion: VERSE provides an effective methodology for analyzing and improving VLMs for document understanding through visualization, targeted synthetic data generation, and performance optimization that enables on-premise models to compete with commercial SaaS solutions.

Abstract: This work introduces VERSE, a methodology for analyzing and improving Vision-Language Models applied to Visually-rich Document Understanding by exploring their visual embedding space. VERSE enables the visualization of latent representations, supporting the assessment of model feasibility. It also facilitates the identification of problematic regions and guides the generation of synthetic data to enhance performance in those clusters. We validate the methodology by training on the synthetic MERIT Dataset and evaluating on its real-world counterpart, MERIT Secret. Results show that VERSE helps uncover the visual features associated with error-prone clusters, and that retraining with samples containing these features substantially boosts F1 performance without degrading generalization. Furthermore, we demonstrate that on-premise models such as Donut and Idefics2, when optimized with VERSE, match or even surpass the performance of SaaS solutions like GPT-4 and Pixtral.

</details>


### [155] [VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control](https://arxiv.org/abs/2601.05138)
*Sixiao Zheng,Minghao Yin,Wenbo Hu,Xiaoyu Li,Ying Shan,Yanwei Fu*

Main category: cs.CV

TL;DR: VerseCrafter is a 4D-aware video world model that enables explicit control over camera and object dynamics using a novel 4D Geometric Control representation, trained on automatically extracted 4D annotations from in-the-wild videos.


<details>
  <summary>Details</summary>
Motivation: Existing video world models struggle to provide unified and precise control over both camera and multi-object motion, as they operate in the 2D image plane rather than a 4D geometric world state.

Method: Introduces a 4D Geometric Control representation encoding world state through static background point clouds and per-object 3D Gaussian trajectories, which are rendered into conditioning signals for a pretrained video diffusion model. Uses an automatic data engine to extract 4D controls from in-the-wild videos for training.

Result: Enables generation of high-fidelity, view-consistent videos that precisely adhere to specified camera and object dynamics, offering a flexible, category-agnostic alternative to rigid bounding boxes or parametric models.

Conclusion: VerseCrafter bridges the gap between 2D video dynamics and 4D geometric control, providing a unified framework for precise manipulation of both camera and object motion in video generation.

Abstract: Video world models aim to simulate dynamic, real-world environments, yet existing methods struggle to provide unified and precise control over camera and multi-object motion, as videos inherently operate dynamics in the projected 2D image plane. To bridge this gap, we introduce VerseCrafter, a 4D-aware video world model that enables explicit and coherent control over both camera and object dynamics within a unified 4D geometric world state. Our approach is centered on a novel 4D Geometric Control representation, which encodes the world state through a static background point cloud and per-object 3D Gaussian trajectories. This representation captures not only an object's path but also its probabilistic 3D occupancy over time, offering a flexible, category-agnostic alternative to rigid bounding boxes or parametric models. These 4D controls are rendered into conditioning signals for a pretrained video diffusion model, enabling the generation of high-fidelity, view-consistent videos that precisely adhere to the specified dynamics. Unfortunately, another major challenge lies in the scarcity of large-scale training data with explicit 4D annotations. We address this by developing an automatic data engine that extracts the required 4D controls from in-the-wild videos, allowing us to train our model on a massive and diverse dataset.

</details>


### [156] [A Lightweight and Explainable Vision-Language Framework for Crop Disease Visual Question Answering](https://arxiv.org/abs/2601.05143)
*Md. Zahid Hossain,Most. Sharmin Sultana Samu,Md. Rakibul Islam,Md. Siam Ansary*

Main category: cs.CV

TL;DR: Lightweight vision-language framework for crop disease identification from leaf images using Swin Transformer encoder and seq2seq decoders with two-stage training, achieving high accuracy with fewer parameters than large-scale baselines.


<details>
  <summary>Details</summary>
Motivation: Visual question answering for crop disease analysis requires accurate visual understanding and reliable language generation, but existing solutions may be too large or not specialized enough for agricultural applications.

Method: Combines Swin Transformer vision encoder with sequence-to-sequence language decoders using a two-stage training strategy to improve visual representation learning and cross-modal alignment.

Result: High accuracy for both crop and disease identification, strong performance on BLEU, ROUGE and BERTScore metrics, outperforms large-scale vision-language baselines with significantly fewer parameters.

Conclusion: Task-specific visual pretraining is effective for crop disease visual question answering, and the lightweight framework demonstrates robust performance under diverse user-driven queries with good explainability.

Abstract: Visual question answering for crop disease analysis requires accurate visual understanding and reliable language generation. This work presents a lightweight vision-language framework for crop and disease identification from leaf images. The proposed approach combines a Swin Transformer vision encoder with sequence-to-sequence language decoders. A two-stage training strategy is adopted to improve visual representation learning and cross-modal alignment. The model is evaluated on a large-scale crop disease dataset using classification and natural language generation metrics. Experimental results show high accuracy for both crop and disease identification. The framework also achieves strong performance on BLEU, ROUGE and BERTScore. Our proposed models outperform large-scale vision-language baselines while using significantly fewer parameters. Explainability is assessed using Grad-CAM and token-level attribution. Qualitative results demonstrate robust performance under diverse user-driven queries. These findings highlight the effectiveness of task-specific visual pretraining for crop disease visual question answering.

</details>


### [157] [Atlas 2 -- Foundation models for clinical deployment](https://arxiv.org/abs/2601.05148)
*Maximilian Alber,Timo Milbich,Alexandra Carpen-Amarie,Stephan Tietz,Jonas Dippel,Lukas Muttenthaler,Beatriz Perez Cancer,Alessandro Benetti,Panos Korfiatis,Elias Eulig,Jérôme Lüscher,Jiasen Wu,Sayed Abid Hashimi,Gabriel Dernbach,Simon Schallenberg,Neelay Shah,Moritz Krügener,Aniruddh Jammoria,Jake Matras,Patrick Duffy,Matt Redlon,Philipp Jurmeister,David Horst,Lukas Ruff,Klaus-Robert Müller,Frederick Klauschen,Andrew Norgan*

Main category: cs.CV

TL;DR: Atlas 2 series (Atlas 2, Atlas 2-B, Atlas 2-S) are pathology vision foundation models that achieve state-of-the-art performance, robustness, and resource efficiency across 80 benchmarks, trained on 5.5M histopathology images from three major medical institutions.


<details>
  <summary>Details</summary>
Motivation: Pathology foundation models have advanced computational pathology but face tradeoffs in performance, robustness, and computational requirements that limit clinical deployment. The authors aim to bridge these shortcomings.

Method: Developed three pathology vision foundation models (Atlas 2, Atlas 2-B, Atlas 2-S) trained on the largest pathology foundation model dataset to date - 5.5 million histopathology whole slide images collected from Charité - Universitätsmedizin Berlin, LMU Munich, and Mayo Clinic.

Result: The models show state-of-the-art performance in prediction performance, robustness, and resource efficiency as demonstrated in a comprehensive evaluation across eighty public benchmarks.

Conclusion: The Atlas 2 series successfully addresses previous limitations in pathology foundation models by achieving superior performance, robustness, and efficiency, making them more suitable for clinical deployment.

Abstract: Pathology foundation models substantially advanced the possibilities in computational pathology -- yet tradeoffs in terms of performance, robustness, and computational requirements remained, which limited their clinical deployment. In this report, we present Atlas 2, Atlas 2-B, and Atlas 2-S, three pathology vision foundation models which bridge these shortcomings by showing state-of-the-art performance in prediction performance, robustness, and resource efficiency in a comprehensive evaluation across eighty public benchmarks. Our models were trained on the largest pathology foundation model dataset to date comprising 5.5 million histopathology whole slide images, collected from three medical institutions Charité - Universtätsmedizin Berlin, LMU Munich, and Mayo Clinic.

</details>


### [158] [Multi-Scale Local Speculative Decoding for Image Generation](https://arxiv.org/abs/2601.05149)
*Elia Peruzzo,Guillaume Sautière,Amirhossein Habibian*

Main category: cs.CV

TL;DR: MuLo-SD accelerates autoregressive image generation using multi-resolution drafting with local rejection/resampling, achieving 1.7× speedup while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: Autoregressive models for image synthesis suffer from latency due to sequential nature. Existing speculative decoding approaches are limited by token-level ambiguity and lack spatial awareness.

Method: Multi-Scale Local Speculative Decoding (MuLo-SD) combines multi-resolution drafting with spatially informed verification. Uses low-resolution drafter with learned up-samplers to propose candidate tokens, then parallel verification by high-resolution target model. Key innovation: local rejection and resampling mechanism focusing on spatial neighborhoods rather than raster-scan resampling.

Result: Achieves up to 1.7× speedup, outperforming EAGLE-2 and LANTERN baselines. Maintains comparable semantic alignment and perceptual quality validated on MS-COCO 5k split using GenEval, DPG-Bench, and FID/HPSv2 metrics.

Conclusion: Sets new state-of-the-art in speculative decoding for image synthesis, bridging efficiency and fidelity gap. Local rejection/resampling with neighborhood expansion is crucial for performance.

Abstract: Autoregressive (AR) models have achieved remarkable success in image synthesis, yet their sequential nature imposes significant latency constraints. Speculative Decoding offers a promising avenue for acceleration, but existing approaches are limited by token-level ambiguity and lack of spatial awareness. In this work, we introduce Multi-Scale Local Speculative Decoding (MuLo-SD), a novel framework that combines multi-resolution drafting with spatially informed verification to accelerate AR image generation. Our method leverages a low-resolution drafter paired with learned up-samplers to propose candidate image tokens, which are then verified in parallel by a high-resolution target model. Crucially, we incorporate a local rejection and resampling mechanism, enabling efficient correction of draft errors by focusing on spatial neighborhoods rather than raster-scan resampling after the first rejection. We demonstrate that MuLo-SD achieves substantial speedups - up to $\mathbf{1.7\times}$ - outperforming strong speculative decoding baselines such as EAGLE-2 and LANTERN in terms of acceleration, while maintaining comparable semantic alignment and perceptual quality. These results are validated using GenEval, DPG-Bench, and FID/HPSv2 on the MS-COCO 5k validation split. Extensive ablations highlight the impact of up-sampling design, probability pooling, and local rejection and resampling with neighborhood expansion. Our approach sets a new state-of-the-art in speculative decoding for image synthesis, bridging the gap between efficiency and fidelity.

</details>


### [159] [Vision-Language Introspection: Mitigating Overconfident Hallucinations in MLLMs via Interpretable Bi-Causal Steering](https://arxiv.org/abs/2601.05159)
*Shuliang Liu,Songbo Yang,Dong Fang,Sihang Jia,Yuqi Tang,Lingfeng Su,Ruoshui Peng,Yibo Yan,Xin Zou,Xuming Hu*

Main category: cs.CV

TL;DR: VLI is a training-free inference framework that reduces object hallucination in multimodal LLMs through metacognitive self-correction, achieving SOTA performance with 12.67% reduction on MMHal-Bench and 5.8% accuracy improvement on POPE.


<details>
  <summary>Details</summary>
Motivation: Object hallucination undermines multimodal LLM reliability due to models trusting linguistic priors over visual evidence. Existing methods are limited: contrastive decoding is superficial, and latent steering uses static vectors lacking instance-specific precision.

Method: VLI performs Attributive Introspection to diagnose hallucination risks via probabilistic conflict detection and localize causal visual anchors, then uses Interpretable Bi-Causal Steering to modulate inference by isolating visual evidence from noise and neutralizing blind confidence through adaptive calibration.

Result: VLI achieves state-of-the-art performance, reducing object hallucination rates by 12.67% on MMHal-Bench and improving accuracy by 5.8% on POPE benchmark.

Conclusion: VLI effectively addresses object hallucination through metacognitive self-correction without training, providing a robust framework for improving multimodal LLM reliability by better aligning visual evidence with linguistic outputs.

Abstract: Object hallucination critically undermines the reliability of Multimodal Large Language Models, often stemming from a fundamental failure in cognitive introspection, where models blindly trust linguistic priors over specific visual evidence. Existing mitigations remain limited: contrastive decoding approaches operate superficially without rectifying internal semantic misalignments, while current latent steering methods rely on static vectors that lack instance-specific precision. We introduce Vision-Language Introspection (VLI), a training-free inference framework that simulates a metacognitive self-correction process. VLI first performs Attributive Introspection to diagnose hallucination risks via probabilistic conflict detection and localize the causal visual anchors. It then employs Interpretable Bi-Causal Steering to actively modulate the inference process, dynamically isolating visual evidence from background noise while neutralizing blind confidence through adaptive calibration. VLI achieves state-of-the-art performance on advanced models, reducing object hallucination rates by 12.67% on MMHal-Bench and improving accuracy by 5.8% on POPE.

</details>


### [160] [CoV: Chain-of-View Prompting for Spatial Reasoning](https://arxiv.org/abs/2601.05172)
*Haoyu Zhao,Akide Liu,Zeyu Zhang,Weijie Wang,Feng Chen,Ruihan Zhu,Gholamreza Haffari,Bohan Zhuang*

Main category: cs.CV

TL;DR: CoV prompting enables VLMs to actively explore 3D environments for EQA by selecting relevant views and adjusting camera positions, improving spatial reasoning without training.


<details>
  <summary>Details</summary>
Motivation: Current VLMs are limited to fixed input views, hindering their ability to gather distributed context and perform complex spatial reasoning in 3D embodied question answering tasks.

Method: Chain-of-View prompting: training-free framework with coarse-to-fine exploration. First, View Selection agent filters redundant frames and identifies anchor views. Then, fine-grained view adjustment through iterative reasoning with discrete camera actions to gather sufficient context.

Result: +11.56% average improvement in LLM-Match on OpenEQA across four VLMs (max +13.62% on Qwen3-VL-Flash). Test-time scaling yields additional +2.51% improvement. Strong performance on ScanQA (116 CIDEr / 31.9 EM@1) and SQA3D (51.1 EM@1).

Conclusion: Question-aligned view selection with open-view search is an effective, model-agnostic strategy for improving 3D spatial reasoning in EQA without additional training.

Abstract: Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached.
  We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56\% improvement in LLM-Match, with a maximum gain of +13.62\% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51\% average improvement, peaking at +3.73\% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training.

</details>


### [161] [VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice](https://arxiv.org/abs/2601.05175)
*Shuming Liu,Mingchen Zhuge,Changsheng Zhao,Jun Chen,Lemeng Wu,Zechun Liu,Chenchen Zhu,Zhipeng Cai,Chong Zhou,Haozhe Liu,Ernie Chang,Saksham Suri,Hongyu Xu,Qi Qian,Wei Wen,Balakrishnan Varadarajan,Zhuang Liu,Hu Xu,Florian Bordes,Raghuraman Krishnamoorthi,Bernard Ghanem,Vikas Chandra,Yunyang Xiong*

Main category: cs.CV

TL;DR: VideoAuto-R1 is a video understanding framework that uses a "reason-when-necessary" strategy, where models only perform costly chain-of-thought reasoning when confidence in initial answers is low, achieving state-of-the-art accuracy with 3.3x shorter responses.


<details>
  <summary>Details</summary>
Motivation: The paper challenges the assumption that chain-of-thought (CoT) reasoning is always necessary for video understanding. The authors found that direct answering often matches or even surpasses CoT performance for RL-trained video models, despite CoT being computationally expensive. This motivates developing a more efficient approach that only uses reasoning when truly needed.

Method: VideoAuto-R1 uses a "Thinking Once, Answering Twice" paradigm: 1) Generate initial answer, 2) Perform reasoning, 3) Output reviewed answer. Both answers are supervised via verifiable rewards. During inference, the model uses confidence scores of the initial answer to decide whether to proceed with reasoning (reason-when-necessary strategy).

Result: VideoAuto-R1 achieves state-of-the-art accuracy on video QA and grounding benchmarks while significantly improving efficiency - reducing average response length by ~3.3x (from 149 to 44 tokens). The framework shows low thinking-mode activation on perception-oriented tasks but higher activation on reasoning-intensive tasks.

Conclusion: Explicit language-based reasoning is generally beneficial but not always necessary for video understanding. The proposed reason-when-necessary strategy provides an efficient alternative to always using CoT, achieving better performance with substantially reduced computational cost.

Abstract: Chain-of-thought (CoT) reasoning has emerged as a powerful tool for multimodal large language models on video understanding tasks. However, its necessity and advantages over direct answering remain underexplored. In this paper, we first demonstrate that for RL-trained video models, direct answering often matches or even surpasses CoT performance, despite CoT producing step-by-step analyses at a higher computational cost. Motivated by this, we propose VideoAuto-R1, a video understanding framework that adopts a reason-when-necessary strategy. During training, our approach follows a Thinking Once, Answering Twice paradigm: the model first generates an initial answer, then performs reasoning, and finally outputs a reviewed answer. Both answers are supervised via verifiable rewards. During inference, the model uses the confidence score of the initial answer to determine whether to proceed with reasoning. Across video QA and grounding benchmarks, VideoAuto-R1 achieves state-of-the-art accuracy with significantly improved efficiency, reducing the average response length by ~3.3x, e.g., from 149 to just 44 tokens. Moreover, we observe a low rate of thinking-mode activation on perception-oriented tasks, but a higher rate on reasoning-intensive tasks. This suggests that explicit language-based reasoning is generally beneficial but not always necessary.

</details>


### [162] [Cutting AI Research Costs: How Task-Aware Compression Makes Large Language Model Agents Affordable](https://arxiv.org/abs/2601.05191)
*Zuhair Ahmed Khan Taha,Mohammed Mudassir Uddin,Shahnawaz Alam*

Main category: cs.CV

TL;DR: AgentCompress reduces LLM compute costs by 68.3% while maintaining 96.2% success rate through intelligent task routing to compressed model variants based on difficulty assessment.


<details>
  <summary>Details</summary>
Motivation: High computational costs of large language models (up to $127 per session for 70B parameter models) make them inaccessible to many academic labs, limiting their use for autonomous research tasks like literature review and hypothesis generation.

Method: Uses a small neural network to assess task difficulty from opening words, then routes tasks to appropriately compressed model variants. The routing decision happens in under 1 millisecond.

Result: Tested across 500 research workflows in four scientific fields, achieving 68.3% reduction in compute costs while maintaining 96.2% of original success rate.

Conclusion: AgentCompress makes LLMs more accessible to budget-constrained academic labs by significantly reducing computational costs with minimal performance impact, enabling more researchers to use these tools for scientific tasks.

Abstract: When researchers deploy large language models for autonomous tasks like reviewing literature or generating hypotheses, the computational bills add up quickly. A single research session using a 70-billion parameter model can cost around $127 in cloud fees, putting these tools out of reach for many academic labs. We developed AgentCompress to tackle this problem head-on. The core idea came from a simple observation during our own work: writing a novel hypothesis clearly demands more from the model than reformatting a bibliography. Why should both tasks run at full precision? Our system uses a small neural network to gauge how hard each incoming task will be, based only on its opening words, then routes it to a suitably compressed model variant. The decision happens in under a millisecond. Testing across 500 research workflows in four scientific fields, we cut compute costs by 68.3% while keeping 96.2% of the original success rate. For labs watching their budgets, this could mean the difference between running experiments and sitting on the sidelines

</details>


### [163] [Mechanisms of Prompt-Induced Hallucination in Vision-Language Models](https://arxiv.org/abs/2601.05201)
*William Rudman,Michal Golovanevsky,Dana Arad,Yonatan Belinkov,Ritambhara Singh,Carsten Eickhoff,Kyle Mahowald*

Main category: cs.CV

TL;DR: VLMs often hallucinate by favoring text over visual evidence. In object-counting tasks, they increasingly conform to incorrect prompts as object counts rise. Researchers identified specific attention heads whose ablation reduces these hallucinations by 40+% without training.


<details>
  <summary>Details</summary>
Motivation: Large vision-language models frequently hallucinate by prioritizing textual prompts over visual evidence, which undermines their reliability. This study aims to understand and mitigate this failure mode in a controlled setting to improve model faithfulness to visual inputs.

Method: Researchers used controlled object-counting experiments where prompts overstated object counts. They conducted mechanistic analysis of three VLMs, identifying specific attention heads responsible for prompt-induced hallucinations. They then performed ablation studies on these heads to measure hallucination reduction.

Result: At low object counts, models often corrected prompt overestimation, but increasingly conformed to incorrect prompts as object numbers rose. Ablation of identified "PIH-heads" reduced prompt-induced hallucinations by at least 40% without additional training. Different models implemented prompt copying through distinct mechanisms.

Conclusion: The study reveals specific attention heads that drive prompt-induced hallucinations in VLMs, showing model-specific implementation differences. Ablating these heads significantly improves model correction toward visual evidence, offering insights into internal mechanisms and potential interventions for reducing hallucinations.

Abstract: Large vision-language models (VLMs) are highly capable, yet often hallucinate by favoring textual prompts over visual evidence. We study this failure mode in a controlled object-counting setting, where the prompt overstates the number of objects in the image (e.g., asking a model to describe four waterlilies when only three are present). At low object counts, models often correct the overestimation, but as the number of objects increases, they increasingly conform to the prompt regardless of the discrepancy. Through mechanistic analysis of three VLMs, we identify a small set of attention heads whose ablation substantially reduces prompt-induced hallucinations (PIH) by at least 40% without additional training. Across models, PIH-heads mediate prompt copying in model-specific ways. We characterize these differences and show that PIH ablation increases correction toward visual evidence. Our findings offer insights into the internal mechanisms driving prompt-induced hallucinations, revealing model-specific differences in how these behaviors are implemented.

</details>


### [164] [MoE3D: A Mixture-of-Experts Module for 3D Reconstruction](https://arxiv.org/abs/2601.05208)
*Zichen Wang,Ang Cao,Liam J. Wang,Jeong Joon Park*

Main category: cs.CV

TL;DR: MoE3D is a mixture-of-experts module that improves 3D reconstruction by predicting multiple candidate depth maps and fusing them with dynamic weighting to sharpen depth boundaries and reduce artifacts.


<details>
  <summary>Details</summary>
Motivation: Existing feed-forward 3D reconstruction models suffer from blurry depth boundaries and flying-point artifacts, which degrade reconstruction quality.

Method: MoE3D predicts multiple candidate depth maps and fuses them via dynamic weighting (mixture-of-experts approach) to sharpen boundaries and reduce artifacts.

Result: When integrated with pre-trained 3D reconstruction backbones like VGGT, MoE3D substantially enhances reconstruction quality with minimal additional computational overhead.

Conclusion: MoE3D effectively improves 3D reconstruction by addressing boundary sharpness and artifact issues through a mixture-of-experts fusion approach.

Abstract: MoE3D is a mixture-of-experts module designed to sharpen depth boundaries and mitigate flying-point artifacts (highlighted in red) of existing feed-forward 3D reconstruction models (left side). MoE3D predicts multiple candidate depth maps and fuses them via dynamic weighting (visualized by MoE weights on the right side). When integrated with a pre-trained 3D reconstruction backbone such as VGGT, it substantially enhances reconstruction quality with minimal additional computational overhead. Best viewed digitally.

</details>


### [165] [FlowLet: Conditional 3D Brain MRI Synthesis using Wavelet Flow Matching](https://arxiv.org/abs/2601.05212)
*Danilo Danese,Angela Lombardi,Matteo Attimonelli,Giuseppe Fasano,Tommaso Di Noia*

Main category: cs.CV

TL;DR: FlowLet is a conditional generative framework that synthesizes age-conditioned 3D brain MRIs using flow matching in an invertible 3D wavelet domain to improve Brain Age Prediction fairness and performance.


<details>
  <summary>Details</summary>
Motivation: Existing 3D MRI datasets for Brain Age Prediction are demographically skewed (age-imbalanced), limiting fairness and generalizability. Current generative methods for data augmentation are slow, introduce artifacts due to latent compression, and rarely condition on age, affecting BAP performance.

Method: FlowLet uses flow matching within an invertible 3D wavelet domain to synthesize age-conditioned 3D MRIs. This approach avoids reconstruction artifacts from latent compression and reduces computational demands compared to latent diffusion models.

Result: FlowLet generates high-fidelity 3D MRI volumes with few sampling steps. Training BAP models with FlowLet-generated data improves performance for underrepresented age groups, and region-based analysis confirms preservation of anatomical structures.

Conclusion: FlowLet provides an effective conditional generative framework for synthesizing age-conditioned 3D brain MRIs that addresses limitations of existing methods, improves BAP fairness, and preserves anatomical fidelity while being computationally efficient.

Abstract: Brain Magnetic Resonance Imaging (MRI) plays a central role in studying neurological development, aging, and diseases. One key application is Brain Age Prediction (BAP), which estimates an individual's biological brain age from MRI data. Effective BAP models require large, diverse, and age-balanced datasets, whereas existing 3D MRI datasets are demographically skewed, limiting fairness and generalizability. Acquiring new data is costly and ethically constrained, motivating generative data augmentation. Current generative methods are often based on latent diffusion models, which operate in learned low dimensional latent spaces to address the memory demands of volumetric MRI data. However, these methods are typically slow at inference, may introduce artifacts due to latent compression, and are rarely conditioned on age, thereby affecting the BAP performance. In this work, we propose FlowLet, a conditional generative framework that synthesizes age-conditioned 3D MRIs by leveraging flow matching within an invertible 3D wavelet domain, helping to avoid reconstruction artifacts and reducing computational demands. Experiments show that FlowLet generates high-fidelity volumes with few sampling steps. Training BAP models with data generated by FlowLet improves performance for underrepresented age groups, and region-based analysis confirms preservation of anatomical structures.

</details>


### [166] [ObjectForesight: Predicting Future 3D Object Trajectories from Human Videos](https://arxiv.org/abs/2601.05237)
*Rustin Soraki,Homanga Bharadhwaj,Ali Farhadi,Roozbeh Mottaghi*

Main category: cs.CV

TL;DR: ObjectForesight: A 3D object-centric dynamics model that predicts future 6-DoF poses and trajectories of rigid objects from short egocentric videos, enabling geometrically grounded motion prediction.


<details>
  <summary>Details</summary>
Motivation: To endow computational systems with human-like ability to anticipate how objects might move or change through interaction from passive visual observation, capturing object affordances and trajectories.

Method: Introduces ObjectForesight, a 3D object-centric dynamics model that operates explicitly in 3D at object level (unlike pixel/latent space models). Uses segmentation, mesh reconstruction, and 3D pose estimation to curate a dataset of 2M+ clips with pseudo-ground-truth 3D object trajectories for training.

Result: Achieves significant gains in accuracy, geometric consistency, and generalization to unseen objects and scenes. Establishes a scalable framework for learning physically grounded, object-centric dynamics models directly from observation.

Conclusion: ObjectForesight enables geometrically grounded and temporally coherent predictions of object motions, representing a scalable approach to learning object-centric dynamics from visual observation.

Abstract: Humans can effortlessly anticipate how objects might move or change through interaction--imagining a cup being lifted, a knife slicing, or a lid being closed. We aim to endow computational systems with a similar ability to predict plausible future object motions directly from passive visual observation. We introduce ObjectForesight, a 3D object-centric dynamics model that predicts future 6-DoF poses and trajectories of rigid objects from short egocentric video sequences. Unlike conventional world or dynamics models that operate in pixel or latent space, ObjectForesight represents the world explicitly in 3D at the object level, enabling geometrically grounded and temporally coherent predictions that capture object affordances and trajectories. To train such a model at scale, we leverage recent advances in segmentation, mesh reconstruction, and 3D pose estimation to curate a dataset of 2 million plus short clips with pseudo-ground-truth 3D object trajectories. Through extensive experiments, we show that ObjectForesight achieves significant gains in accuracy, geometric consistency, and generalization to unseen objects and scenes, establishing a scalable framework for learning physically grounded, object-centric dynamics models directly from observation. objectforesight.github.io

</details>


### [167] [Plenoptic Video Generation](https://arxiv.org/abs/2601.05239)
*Xiao Fu,Shitao Tang,Min Shi,Xian Liu,Jinwei Gu,Ming-Yu Liu,Dahua Lin,Chen-Hsuan Lin*

Main category: cs.CV

TL;DR: PlenopticDreamer is a framework for multi-view video re-rendering that maintains spatio-temporal consistency in hallucinated regions by training a video-conditioned model autoregressively with camera-guided video retrieval and progressive context-scaling.


<details>
  <summary>Details</summary>
Motivation: Existing camera-controlled generative video re-rendering methods struggle with multi-view consistency and spatio-temporal coherence in hallucinated regions due to generative model stochasticity.

Method: Trains a multi-in-single-out video-conditioned model autoregressively with camera-guided video retrieval, progressive context-scaling for convergence, self-conditioning for robustness, and long-video conditioning for extended generation.

Result: Achieves state-of-the-art video re-rendering on Basic and Agibot benchmarks with superior view synchronization, high-fidelity visuals, accurate camera control, and diverse view transformations.

Conclusion: PlenopticDreamer effectively addresses multi-view consistency challenges in video re-rendering through synchronized generative hallucinations and adaptive conditioning strategies.

Abstract: Camera-controlled generative video re-rendering methods, such as ReCamMaster, have achieved remarkable progress. However, despite their success in single-view setting, these works often struggle to maintain consistency across multi-view scenarios. Ensuring spatio-temporal coherence in hallucinated regions remains challenging due to the inherent stochasticity of generative models. To address it, we introduce PlenopticDreamer, a framework that synchronizes generative hallucinations to maintain spatio-temporal memory. The core idea is to train a multi-in-single-out video-conditioned model in an autoregressive manner, aided by a camera-guided video retrieval strategy that adaptively selects salient videos from previous generations as conditional inputs. In addition, Our training incorporates progressive context-scaling to improve convergence, self-conditioning to enhance robustness against long-range visual degradation caused by error accumulation, and a long-video conditioning mechanism to support extended video generation. Extensive experiments on the Basic and Agibot benchmarks demonstrate that PlenopticDreamer achieves state-of-the-art video re-rendering, delivering superior view synchronization, high-fidelity visuals, accurate camera control, and diverse view transformations (e.g., third-person to third-person, and head-view to gripper-view in robotic manipulation). Project page: https://research.nvidia.com/labs/dir/plenopticdreamer/

</details>


### [168] [RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation](https://arxiv.org/abs/2601.05241)
*Boyang Wang,Haoran Zhang,Shujie Zhang,Jinkun Hao,Mingda Jia,Qi Lv,Yucheng Mao,Zhaoyang Lyu,Jia Zeng,Xudong Xu,Jiangmiao Pang*

Main category: cs.CV

TL;DR: The paper introduces visual identity prompting for data augmentation in robotics, using exemplar images to guide diffusion models for generating multi-view, temporally coherent manipulation data, leading to improved policy performance.


<details>
  <summary>Details</summary>
Motivation: Collecting large-scale real-world manipulation data is difficult to scale due to hardware and setup constraints. Existing text-prompt conditioned diffusion models overlook the need for multi-view and temporally coherent observations, and text prompts alone cannot reliably specify scene setups.

Method: Introduces visual identity prompting that supplies exemplar images as conditioning inputs to guide diffusion models in generating desired scene setups. Builds a scalable pipeline to curate a visual identity pool from large robotics datasets for data augmentation.

Result: Using the augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.

Conclusion: Visual identity prompting provides effective visual guidance for diffusion-based data augmentation in robotics, addressing limitations of text-only conditioning and enabling generation of practical multi-view, temporally coherent observations for improved policy training.

Abstract: The diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-prompt conditioned image diffusion models to augment manipulation data by altering the backgrounds and tabletop objects in the visual observations. However, these approaches often overlook the practical need for multi-view and temporally coherent observations required by state-of-the-art policy models. Further, text prompts alone cannot reliably specify the scene setup. To provide the diffusion model with explicit visual guidance, we introduce visual identity prompting, which supplies exemplar images as conditioning inputs to guide the generation of the desired scene setup. To this end, we also build a scalable pipeline to curate a visual identity pool from large robotics datasets. Using our augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.

</details>


### [169] [GREx: Generalized Referring Expression Segmentation, Comprehension, and Generation](https://arxiv.org/abs/2601.05244)
*Henghui Ding,Chang Liu,Shuting He,Xudong Jiang,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: This paper introduces GREx (Generalized Referring Expression Segmentation/Comprehension/Generation) benchmarks and gRefCOCO dataset to handle multi-target and no-target expressions, extending beyond traditional single-target REx tasks.


<details>
  <summary>Details</summary>
Motivation: Existing REx (RES/REC/REG) methods only support single-target expressions (one expression refers to one object), which limits real-world applications where expressions can refer to multiple objects or no objects at all.

Method: Proposes GREx benchmarks (GRES, GREC, GREG) and constructs gRefCOCO dataset with multi-target, no-target, and single-target expressions. Also introduces ReLA baseline that adaptively divides images into regions with sub-instance clues and models region-region and region-language dependencies.

Result: ReLA achieves state-of-the-art results on both GRES and GREC tasks. The gRefCOCO dataset enables studying performance gaps of existing REx methods on generalized tasks.

Conclusion: GREx extends REx to handle arbitrary numbers of objects, making referring expression tasks more realistic and applicable. The proposed dataset and method provide foundations for future research in generalized referring expression understanding.

Abstract: Referring Expression Segmentation (RES) and Comprehension (REC) respectively segment and detect the object described by an expression, while Referring Expression Generation (REG) generates an expression for the selected object. Existing datasets and methods commonly support single-target expressions only, i.e., one expression refers to one object, not considering multi-target and no-target expressions. This greatly limits the real applications of REx (RES/REC/REG). This paper introduces three new benchmarks called Generalized Referring Expression Segmentation (GRES), Comprehension (GREC), and Generation (GREG), collectively denoted as GREx, which extend the classic REx to allow expressions to identify an arbitrary number of objects. We construct the first large-scale GREx dataset gRefCOCO that contains multi-target, no-target, and single-target expressions and their corresponding images with labeled targets. GREx and gRefCOCO are designed to be backward-compatible with REx, facilitating extensive experiments to study the performance gap of the existing REx methods on GREx tasks. One of the challenges of GRES/GREC is complex relationship modeling, for which we propose a baseline ReLA that adaptively divides the image into regions with sub-instance clues and explicitly models the region-region and region-language dependencies. The proposed ReLA achieves the state-of-the-art results on the both GRES and GREC tasks. The proposed gRefCOCO dataset and method are available at https://henghuiding.github.io/GREx.

</details>


### [170] [Pixel-Perfect Visual Geometry Estimation](https://arxiv.org/abs/2601.05246)
*Gangwei Xu,Haotong Lin,Hongcheng Luo,Haiyang Sun,Bing Wang,Guang Chen,Sida Peng,Hangjun Ye,Xin Yang*

Main category: cs.CV

TL;DR: Pixel-perfect visual geometry models (PPD for images, PPVD for videos) use pixel-space diffusion transformers with semantic prompting and cascade architecture to generate high-quality, flying-pixel-free depth maps and point clouds.


<details>
  <summary>Details</summary>
Motivation: Existing geometry foundation models suffer from flying pixels and loss of fine details, which is problematic for robotics and augmented reality applications that require clean and accurate geometry.

Method: 1) Pixel-Perfect Depth (PPD): Uses pixel-space diffusion transformers (DiT) with Semantics-Prompted DiT (incorporating semantic representations from vision foundation models) and Cascade DiT architecture (progressively increasing image tokens). 2) Pixel-Perfect Video Depth (PPVD): Extends PPD with Semantics-Consistent DiT (extracting temporally consistent semantics from multi-view geometry models) and reference-guided token propagation for temporal coherence.

Result: Achieves best performance among all generative monocular and video depth estimation models, producing significantly cleaner point clouds than all other models with minimal computational and memory overhead.

Conclusion: The proposed pixel-perfect visual geometry models successfully address flying pixels and detail loss through generative modeling in pixel space with semantic prompting and efficient cascade architectures, enabling high-quality geometry reconstruction for practical applications.

Abstract: Recovering clean and accurate geometry from images is essential for robotics and augmented reality. However, existing geometry foundation models still suffer severely from flying pixels and the loss of fine details. In this paper, we present pixel-perfect visual geometry models that can predict high-quality, flying-pixel-free point clouds by leveraging generative modeling in the pixel space. We first introduce Pixel-Perfect Depth (PPD), a monocular depth foundation model built upon pixel-space diffusion transformers (DiT). To address the high computational complexity associated with pixel-space diffusion, we propose two key designs: 1) Semantics-Prompted DiT, which incorporates semantic representations from vision foundation models to prompt the diffusion process, preserving global semantics while enhancing fine-grained visual details; and 2) Cascade DiT architecture that progressively increases the number of image tokens, improving both efficiency and accuracy. To further extend PPD to video (PPVD), we introduce a new Semantics-Consistent DiT, which extracts temporally consistent semantics from a multi-view geometry foundation model. We then perform reference-guided token propagation within the DiT to maintain temporal coherence with minimal computational and memory overhead. Our models achieve the best performance among all generative monocular and video depth estimation models and produce significantly cleaner point clouds than all other models.

</details>


### [171] [RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes](https://arxiv.org/abs/2601.05249)
*Yuan-Kang Lee,Kuan-Lin Chen,Chia-Che Chang,Yu-Lun Liu*

Main category: cs.CV

TL;DR: RL-AWB combines statistical methods with deep reinforcement learning for nighttime white balance, achieving superior generalization across lighting conditions.


<details>
  <summary>Details</summary>
Motivation: Nighttime color constancy is challenging due to low-light noise and complex illumination conditions. Existing methods struggle with generalization across different lighting scenarios and sensors.

Method: 1) Statistical algorithm for nighttime scenes with salient gray pixel detection and novel illumination estimation. 2) Deep reinforcement learning framework that uses the statistical algorithm as its core, dynamically optimizing parameters per image like professional AWB tuning experts. 3) Creation of first multi-sensor nighttime dataset for cross-sensor evaluation.

Result: The method achieves superior generalization capability across both low-light and well-illuminated images. The multi-sensor dataset enables comprehensive cross-sensor evaluation.

Conclusion: RL-AWB presents an effective framework combining statistical methods with deep reinforcement learning for nighttime white balance, demonstrating strong generalization across diverse lighting conditions and sensors.

Abstract: Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images. Project page: https://ntuneillee.github.io/research/rl-awb/

</details>


### [172] [QNeRF: Neural Radiance Fields on a Simulated Gate-Based Quantum Computer](https://arxiv.org/abs/2601.05250)
*Daniele Lizzio Bosco,Shuteng Wang,Giuseppe Serra,Vladislav Golyanik*

Main category: cs.CV

TL;DR: QNeRF is the first hybrid quantum-classical model for novel-view synthesis that uses parameterized quantum circuits to encode spatial and view-dependent information, achieving comparable or better performance than classical NeRF with less than half the parameters.


<details>
  <summary>Details</summary>
Motivation: Quantum Visual Fields (QVFs) have shown promise for model compactness and convergence speed, while Neural Radiance Fields (NeRFs) advance novel-view synthesis but require larger models and intensive training. The authors aim to combine quantum advantages with NeRF capabilities for more efficient 3D scene representation.

Method: QNeRF uses parameterized quantum circuits to encode spatial and view-dependent information through quantum superposition and entanglement. Two variants: Full QNeRF maximizes quantum amplitudes for representation, while Dual-Branch QNeRF introduces task-informed inductive bias by branching spatial and view-dependent quantum state preparations to reduce complexity and ensure scalability.

Result: When trained on moderate-resolution images, QNeRF matches or outperforms classical NeRF baselines while using less than half the number of parameters, demonstrating quantum machine learning as a competitive alternative for continuous signal representation.

Conclusion: Quantum machine learning can serve as a competitive alternative for continuous signal representation in mid-level computer vision tasks like 3D representation learning from 2D observations, offering more compact models with comparable or better performance.

Abstract: Recently, Quantum Visual Fields (QVFs) have shown promising improvements in model compactness and convergence speed for learning the provided 2D or 3D signals. Meanwhile, novel-view synthesis has seen major advances with Neural Radiance Fields (NeRFs), where models learn a compact representation from 2D images to render 3D scenes, albeit at the cost of larger models and intensive training. In this work, we extend the approach of QVFs by introducing QNeRF, the first hybrid quantum-classical model designed for novel-view synthesis from 2D images. QNeRF leverages parameterised quantum circuits to encode spatial and view-dependent information via quantum superposition and entanglement, resulting in more compact models compared to the classical counterpart. We present two architectural variants. Full QNeRF maximally exploits all quantum amplitudes to enhance representational capabilities. In contrast, Dual-Branch QNeRF introduces a task-informed inductive bias by branching spatial and view-dependent quantum state preparations, drastically reducing the complexity of this operation and ensuring scalability and potential hardware compatibility. Our experiments demonstrate that -- when trained on images of moderate resolution -- QNeRF matches or outperforms classical NeRF baselines while using less than half the number of parameters. These results suggest that quantum machine learning can serve as a competitive alternative for continuous signal representation in mid-level tasks in computer vision, such as 3D representation learning from 2D observations.

</details>


### [173] [Mesh4D: 4D Mesh Reconstruction and Tracking from Monocular Video](https://arxiv.org/abs/2601.05251)
*Zeren Jiang,Chuanxia Zheng,Iro Laina,Diane Larlus,Andrea Vedaldi*

Main category: cs.CV

TL;DR: Mesh4D: A feed-forward model for monocular 4D mesh reconstruction that reconstructs complete 3D shape and motion from monocular video using a compact latent space and latent diffusion.


<details>
  <summary>Details</summary>
Motivation: To reconstruct complete 3D shape and motion (4D mesh) from monocular video of dynamic objects, addressing the challenge of representing complex deformations over time without requiring skeletal information at inference.

Method: Uses an autoencoder with spatio-temporal attention to learn a compact latent space encoding entire animation sequences. The autoencoder is guided by skeletal structure during training only. A latent diffusion model conditioned on input video and first-frame mesh predicts full animation in one shot.

Result: Outperforms prior methods on reconstruction and novel view synthesis benchmarks, achieving more accurate 3D shape and deformation recovery.

Conclusion: Mesh4D successfully reconstructs 4D meshes from monocular video using a feed-forward approach with compact latent representation and skeletal guidance during training only, enabling efficient and accurate animation prediction.

Abstract: We propose Mesh4D, a feed-forward model for monocular 4D mesh reconstruction. Given a monocular video of a dynamic object, our model reconstructs the object's complete 3D shape and motion, represented as a deformation field. Our key contribution is a compact latent space that encodes the entire animation sequence in a single pass. This latent space is learned by an autoencoder that, during training, is guided by the skeletal structure of the training objects, providing strong priors on plausible deformations. Crucially, skeletal information is not required at inference time. The encoder employs spatio-temporal attention, yielding a more stable representation of the object's overall deformation. Building on this representation, we train a latent diffusion model that, conditioned on the input video and the mesh reconstructed from the first frame, predicts the full animation in one shot. We evaluate Mesh4D on reconstruction and novel view synthesis benchmarks, outperforming prior methods in recovering accurate 3D shape and deformation.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [174] [SAGE-32B: Agentic Reasoning via Iterative Distillation](https://arxiv.org/abs/2601.04237)
*Basab Jha,Firoj Paudel,Ujjwal Puri,Ethan Henkel,Zhang Yuting,Mateusz Kowalczyk,Mei Huang,Choi Donghyuk,Wang Junhao*

Main category: cs.AI

TL;DR: SAGE-32B is a 32B parameter language model specialized for agentic reasoning and long-range planning, fine-tuned from Qwen2.5-32B using iterative distillation and featuring inverse reasoning with meta-cognition for failure forecasting.


<details>
  <summary>Details</summary>
Motivation: The paper aims to create a language model specifically designed for agentic reasoning and long-range planning tasks, moving beyond general conversation fluency to focus on task decomposition, tool usage, and error recovery in agentic loops.

Method: Initialized from Qwen2.5-32B pretrained model, fine-tuned using Iterative Distillation (two-stage training process with rigorously tested feedback loops). Introduces inverse reasoning approach with meta-cognition head to forecast potential failures before execution.

Result: Achieves higher success rates in multi-tool usage scenarios on agentic reasoning benchmarks (MMLU-Pro, AgentBench, MATH-500) compared to similarly sized baseline models, while remaining competitive on standard reasoning evaluations.

Conclusion: SAGE-32B demonstrates effective specialization for agentic reasoning tasks through iterative distillation and inverse reasoning techniques, showing improved performance in complex planning scenarios while maintaining general reasoning capabilities.

Abstract: We demonstrate SAGE-32B, a 32 billion parameter language model that focuses on agentic reasoning and long range planning tasks. Unlike chat models that aim for general conversation fluency, SAGE-32B is designed to operate in an agentic loop, emphasizing task decomposition, tool usage, and error recovery. The model is initialized from the Qwen2.5-32B pretrained model and fine tuned using Iterative Distillation, a two stage training process that improves reasoning performance through rigorously tested feedback loops. SAGE-32B also introduces an inverse reasoning approach, which uses a meta cognition head to forecast potential failures in the planning process before execution. On agentic reasoning benchmarks including MMLU-Pro, AgentBench, and MATH-500, SAGE-32B achieves higher success rates in multi tool usage scenarios compared to similarly sized baseline models, while remaining competitive on standard reasoning evaluations. Model weights are publicly released at https://huggingface.co/sagea-ai/sage-reasoning-32b

</details>


### [175] [Learning Latent Action World Models In The Wild](https://arxiv.org/abs/2601.05230)
*Quentin Garrido,Tushar Nagarajan,Basile Terver,Nicolas Ballas,Yann LeCun,Michael Rabbat*

Main category: cs.AI

TL;DR: Learning latent action world models from in-the-wild videos without action labels, enabling real-world reasoning and planning.


<details>
  <summary>Details</summary>
Motivation: Real-world agents need to predict action consequences, but world models typically require action labels that are hard to obtain at scale. Existing latent action models focus on simple simulations/games, not diverse real-world videos.

Method: Learn latent action models from unlabeled in-the-wild videos using continuous constrained latent actions (instead of vector quantization), with specific architectural choices and evaluations to handle video diversity, environmental noise, and lack of common embodiment.

Result: Continuous constrained latent actions capture complex real-world actions better than vector quantization. Models can transfer agent-induced environmental changes across videos and learn spatially-localized actions relative to camera. A controller maps known actions to latent ones, enabling planning with similar performance to action-conditioned baselines.

Conclusion: The approach provides a step toward scaling latent action models to real-world applications by learning from diverse in-the-wild videos without action labels, enabling universal action interfaces for planning tasks.

Abstract: Agents capable of reasoning and planning in the real world require the ability of predicting the consequences of their actions. While world models possess this capability, they most often require action labels, that can be complex to obtain at scale. This motivates the learning of latent action models, that can learn an action space from videos alone. Our work addresses the problem of learning latent actions world models on in-the-wild videos, expanding the scope of existing works that focus on simple robotics simulations, video games, or manipulation data. While this allows us to capture richer actions, it also introduces challenges stemming from the video diversity, such as environmental noise, or the lack of a common embodiment across videos. To address some of the challenges, we discuss properties that actions should follow as well as relevant architectural choices and evaluations. We find that continuous, but constrained, latent actions are able to capture the complexity of actions from in-the-wild videos, something that the common vector quantization does not. We for example find that changes in the environment coming from agents, such as humans entering the room, can be transferred across videos. This highlights the capability of learning actions that are specific to in-the-wild videos. In the absence of a common embodiment across videos, we are mainly able to learn latent actions that become localized in space, relative to the camera. Nonetheless, we are able to train a controller that maps known actions to latent ones, allowing us to use latent actions as a universal interface and solve planning tasks with our world model with similar performance as action-conditioned baselines. Our analyses and experiments provide a step towards scaling latent action models to the real world.

</details>


### [176] [The Language of Bargaining: Linguistic Effects in LLM Negotiations](https://arxiv.org/abs/2601.04387)
*Stuti Sinha,Himanshu Kumar,Aryan Raju Mandapati,Rakshit Sakhuja,Dhruv Kumar*

Main category: cs.AI

TL;DR: LLM negotiation outcomes vary significantly across languages, with Indic languages showing different effects than English in multi-agent games, challenging English-only evaluation practices.


<details>
  <summary>Details</summary>
Motivation: Most LLM negotiation evaluations occur only in English, potentially missing important language-specific effects on negotiation outcomes and social intelligence.

Method: Controlled multi-agent simulations across Ultimatum, Buy-Sell, and Resource Exchange games, comparing English with four Indic languages (Hindi, Punjabi, Gujarati, Marwadi) while keeping game rules, model parameters, and incentives constant.

Result: Language choice can shift outcomes more strongly than changing models, reversing proposer advantages and reallocating surplus. Indic languages reduce stability in distributive games but induce richer exploration in integrative settings.

Conclusion: Evaluating LLM negotiation solely in English yields incomplete and potentially misleading conclusions, highlighting the need for culturally-aware evaluation for fair deployment.

Abstract: Negotiation is a core component of social intelligence, requiring agents to balance strategic reasoning, cooperation, and social norms. Recent work shows that LLMs can engage in multi-turn negotiation, yet nearly all evaluations occur exclusively in English. Using controlled multi-agent simulations across Ultimatum, Buy-Sell, and Resource Exchange games, we systematically isolate language effects across English and four Indic framings (Hindi, Punjabi, Gujarati, Marwadi) by holding game rules, model parameters, and incentives constant across all conditions. We find that language choice can shift outcomes more strongly than changing models, reversing proposer advantages and reallocating surplus. Crucially, effects are task-contingent: Indic languages reduce stability in distributive games yet induce richer exploration in integrative settings. Our results demonstrate that evaluating LLM negotiation solely in English yields incomplete and potentially misleading conclusions. These findings caution against English-only evaluation of LLMs and suggest that culturally-aware evaluation is essential for fair deployment.

</details>


### [177] [CircuitLM: A Multi-Agent LLM-Aided Design Framework for Generating Circuit Schematics from Natural Language Prompts](https://arxiv.org/abs/2601.04505)
*Khandakar Shakib Al Hasan,Syed Rifat Raiyan,Hasin Mahtab Alvee,Wahid Sadik*

Main category: cs.AI

TL;DR: CircuitLM is a multi-agent LLM pipeline that converts natural language circuit descriptions into structured CircuitJSON schematics, addressing LLM hallucinations and electrical constraint violations through a verified component database and validation framework.


<details>
  <summary>Details</summary>
Motivation: LLMs frequently hallucinate details, violate electrical constraints, and produce non-machine-readable outputs when generating circuit schematics from natural language descriptions, creating a gap in accessible electronics design for non-experts.

Method: A five-stage pipeline: (1) LLM-based component identification, (2) canonical pinout retrieval, (3) chain-of-thought reasoning by electronics expert agent, (4) JSON schematic synthesis, and (5) force-directed SVG visualization, anchored by a curated component knowledge base with 50+ components.

Result: Evaluated on 100 diverse embedded-systems prompts across six LLMs, introduced Dual-Metric Circuit Validation (DMCV) framework achieving high fidelity in microcontroller-centric designs, validated against human-expert assessments.

Conclusion: CircuitLM bridges natural language input to deployable hardware designs, enabling reliable circuit prototyping by non-experts through structured, visually interpretable schematics grounded in verified electrical constraints.

Abstract: Generating accurate circuit schematics from high-level natural language descriptions remains a persistent challenge in electronics design, as large language models (LLMs) frequently hallucinate in granular details, violate electrical constraints, and produce non-machine-readable outputs. We present CircuitLM, a novel multi-agent LLM-aided circuit design pipeline that translates user prompts into structured, visually interpretable CircuitJSON schematics through five sequential stages: (i) LLM-based component identification, (ii) canonical pinout retrieval, (iii) chain-of-thought reasoning by an electronics expert agent, (iv) JSON schematic synthesis, and (v) force-directed SVG visualization. Anchored by a curated, embedding-powered component knowledge base. While LLMs often violate electrical constraints, CircuitLM bridges this gap by grounding generation in a verified and dynamically extensible component database, initially comprising 50 components. To ensure safety, we incorporate a hybrid evaluation framework, namely Dual-Metric Circuit Validation (DMCV), validated against human-expert assessments, which achieves high fidelity in microcontroller-centric designs. We evaluate the system on 100 diverse embedded-systems prompts across six LLMs and introduce DMCV to assess both structural and electrical validity. This work bridges natural language input to deployable hardware designs, enabling reliable circuit prototyping by non-experts. Our code and data will be made public upon acceptance.

</details>


### [178] [BackdoorAgent: A Unified Framework for Backdoor Attacks on LLM-based Agents](https://arxiv.org/abs/2601.04566)
*Yunhao Feng,Yige Li,Yutao Wu,Yingshui Tan,Yanming Guo,Yifan Ding,Kun Zhai,Xingjun Ma,Yugang Jiang*

Main category: cs.AI

TL;DR: BackdoorAgent is a framework for analyzing backdoor threats in LLM agents across planning, memory, and tool-use stages, showing triggers can persist and propagate through agent workflows.


<details>
  <summary>Details</summary>
Motivation: LLM agent workflows expand attack surfaces for backdoor threats, but existing studies are fragmented and don't understand cross-stage trigger propagation from an agent-centric perspective.

Method: Propose BackdoorAgent framework that structures attacks into three stages (planning, memory, tool-use), instruments agent execution, and creates benchmark across four agent applications (QA, Code, Web, Drive).

Result: Triggers implanted at single stages persist across multiple steps and propagate through intermediate states (43.58% planning attacks, 77.97% memory attacks, 60.28% tool-stage attacks with GPT backbone).

Conclusion: Agentic workflows are vulnerable to backdoor threats with triggers persisting across stages, highlighting need for systematic analysis and providing benchmark for future research.

Abstract: Large language model (LLM) agents execute tasks through multi-step workflows that combine planning, memory, and tool use. While this design enables autonomy, it also expands the attack surface for backdoor threats. Backdoor triggers injected into specific stages of an agent workflow can persist through multiple intermediate states and adversely influence downstream outputs. However, existing studies remain fragmented and typically analyze individual attack vectors in isolation, leaving the cross-stage interaction and propagation of backdoor triggers poorly understood from an agent-centric perspective. To fill this gap, we propose \textbf{BackdoorAgent}, a modular and stage-aware framework that provides a unified, agent-centric view of backdoor threats in LLM agents. BackdoorAgent structures the attack surface into three functional stages of agentic workflows, including \textbf{planning attacks}, \textbf{memory attacks}, and \textbf{tool-use attacks}, and instruments agent execution to enable systematic analysis of trigger activation and propagation across different stages. Building on this framework, we construct a standardized benchmark spanning four representative agent applications: \textbf{Agent QA}, \textbf{Agent Code}, \textbf{Agent Web}, and \textbf{Agent Drive}, covering both language-only and multimodal settings. Our empirical analysis shows that \textit{triggers implanted at a single stage can persist across multiple steps and propagate through intermediate states.} For instance, when using a GPT-based backbone, we observe trigger persistence in 43.58\% of planning attacks, 77.97\% of memory attacks, and 60.28\% of tool-stage attacks, highlighting the vulnerabilities of the agentic workflow itself to backdoor threats. To facilitate reproducibility and future research, our code and benchmark are publicly available at GitHub.

</details>


### [179] [Neurosymbolic Retrievers for Retrieval-augmented Generation](https://arxiv.org/abs/2601.04568)
*Yash Saxena,Manas Gaur*

Main category: cs.AI

TL;DR: Neurosymbolic RAG integrates symbolic reasoning with knowledge graphs into neural retrieval to improve transparency and interpretability in document selection.


<details>
  <summary>Details</summary>
Motivation: Traditional RAG systems lack transparency due to opaque neural components (retriever, re-ranker, generator), making interpretability, debugging, and trust difficult, especially in high-stakes domains requiring clear decision-making.

Method: Three neurosymbolic methods: 1) MAR (Knowledge Modulation Aligned Retrieval) uses modulation networks to refine query embeddings with symbolic features; 2) KG-Path RAG enhances queries via knowledge graph traversal; 3) Process Knowledge-infused RAG reorders retrieved content using domain-specific workflow tools.

Result: Preliminary results from mental health risk assessment tasks show enhanced transparency and overall performance compared to traditional RAG systems.

Conclusion: Neurosymbolic RAG successfully addresses transparency limitations by integrating symbolic reasoning with neural retrieval, providing clearer interpretability while maintaining or improving performance.

Abstract: Retrieval Augmented Generation (RAG) has made significant strides in overcoming key limitations of large language models, such as hallucination, lack of contextual grounding, and issues with transparency. However, traditional RAG systems consist of three interconnected neural components - the retriever, re-ranker, and generator - whose internal reasoning processes remain opaque. This lack of transparency complicates interpretability, hinders debugging efforts, and erodes trust, especially in high-stakes domains where clear decision-making is essential. To address these challenges, we introduce the concept of Neurosymbolic RAG, which integrates symbolic reasoning using a knowledge graph with neural retrieval techniques. This new framework aims to answer two primary questions: (a) Can retrievers provide a clear and interpretable basis for document selection? (b) Can symbolic knowledge enhance the clarity of the retrieval process? We propose three methods to improve this integration. First is MAR (Knowledge Modulation Aligned Retrieval) that employs modulation networks to refine query embeddings using interpretable symbolic features, thereby making document matching more explicit. Second, KG-Path RAG enhances queries by traversing knowledge graphs to improve overall retrieval quality and interpretability. Lastly, Process Knowledge-infused RAG utilizes domain-specific tools to reorder retrieved content based on validated workflows. Preliminary results from mental health risk assessment tasks indicate that this neurosymbolic approach enhances both transparency and overall performance

</details>


### [180] [A Method for Constructing a Digital Transformation Driving Mechanism Based on Semantic Understanding of Large Models](https://arxiv.org/abs/2601.04696)
*Huayi Liu*

Main category: cs.AI

TL;DR: Combines LLM and knowledge graph for enterprise digital transformation, using fine-tuned BERT for entity recognition, GPT-4 for semantic enhancement, GNN for knowledge graph construction, and reinforcement learning for decision optimization.


<details>
  <summary>Details</summary>
Motivation: Addresses insufficient semantic understanding of unstructured data and lack of intelligent decision-making basis in enterprise digital transformation driving mechanisms.

Method: 1) Fine-tuned BERT for entity recognition and relationship extraction from multi-source texts, GPT-4 for semantic vector enhancement; 2) Two-layer GNN architecture to fuse LLM semantic vectors with business metadata for dynamic knowledge graph; 3) Reinforcement learning with reward function for decision path optimization.

Result: In manufacturing case: reduced equipment failure response time from 7.8 to 3.7 hours, achieved 94.3% F1 score, decreased decision error compensation in annual digital transformation cost by 45.3%.

Conclusion: Integration of large model semantic understanding with structured knowledge significantly enhances intelligence level and execution efficiency of digital transformation driving mechanisms.

Abstract: In the process of digital transformation, enterprises are faced with problems such as insufficient semantic understanding of unstructured data and lack of intelligent decision-making basis in driving mechanisms. This study proposes a method that combines a large language model (LLM) and a knowledge graph. First, a fine-tuned BERT (Bidirectional Encoder Representations from Transformers) model is used to perform entity recognition and relationship extraction on multi-source heterogeneous texts, and GPT-4 is used to generate semantically enhanced vector representations; secondly, a two-layer graph neural network (GNN) architecture is designed to fuse the semantic vectors output by LLM with business metadata to construct a dynamic and scalable enterprise knowledge graph; then reinforcement learning is introduced to optimize decision path generation, and the reward function is used to drive the mechanism iteration. In the case of the manufacturing industry, this mechanism reduced the response time for equipment failure scenarios from 7.8 hours to 3.7 hours, the F1 value reached 94.3%, and the compensation for decision errors in the annual digital transformation cost decreased by 45.3%. This method significantly enhances the intelligence level and execution efficiency of the digital transformation driving mechanism by integrating large model semantic understanding with structured knowledge.

</details>


### [181] [TourPlanner: A Competitive Consensus Framework with Constraint-Gated Reinforcement Learning for Travel Planning](https://arxiv.org/abs/2601.04698)
*Yinuo Wang,Mining Tan,Wenxiang Jiao,Xiaoxi Li,Hao Wang,Xuanyu Zhang,Yuan Lu,Weiming Dong*

Main category: cs.AI

TL;DR: TourPlanner: A travel planning framework with multi-path reasoning and constraint-gated reinforcement learning that outperforms existing methods in feasibility and user preference alignment.


<details>
  <summary>Details</summary>
Motivation: Existing travel planning approaches face three key challenges: (1) pruning candidate POIs while maintaining high recall, (2) single reasoning paths limiting exploration of solution space, and (3) difficulty simultaneously optimizing hard and soft constraints.

Method: TourPlanner combines three components: (1) Personalized Recall and Spatial Optimization (PReSO) for constructing spatially-aware candidate POI sets, (2) Competitive consensus Chain-of-Thought (CCoT) for multi-path reasoning to explore solution space, and (3) constraint-gated reinforcement learning with sigmoid-based gating to prioritize soft constraints only after hard constraints are met.

Result: Experimental results on travel planning benchmarks show TourPlanner achieves state-of-the-art performance, significantly surpassing existing methods in both feasibility and user-preference alignment.

Conclusion: TourPlanner effectively addresses the three key challenges in travel planning through its comprehensive framework of multi-path reasoning and constraint-gated reinforcement learning, demonstrating superior performance in generating feasible and user-aligned travel itineraries.

Abstract: Travel planning is a sophisticated decision-making process that requires synthesizing multifaceted information to construct itineraries. However, existing travel planning approaches face several challenges: (1) Pruning candidate points of interest (POIs) while maintaining a high recall rate; (2) A single reasoning path restricts the exploration capability within the feasible solution space for travel planning; (3) Simultaneously optimizing hard constraints and soft constraints remains a significant difficulty. To address these challenges, we propose TourPlanner, a comprehensive framework featuring multi-path reasoning and constraint-gated reinforcement learning. Specifically, we first introduce a Personalized Recall and Spatial Optimization (PReSO) workflow to construct spatially-aware candidate POIs' set. Subsequently, we propose Competitive consensus Chain-of-Thought (CCoT), a multi-path reasoning paradigm that improves the ability of exploring the feasible solution space. To further refine the plan, we integrate a sigmoid-based gating mechanism into the reinforcement learning stage, which dynamically prioritizes soft-constraint satisfaction only after hard constraints are met. Experimental results on travel planning benchmarks demonstrate that TourPlanner achieves state-of-the-art performance, significantly surpassing existing methods in both feasibility and user-preference alignment.

</details>


### [182] [Memory Matters More: Event-Centric Memory as a Logic Map for Agent Searching and Reasoning](https://arxiv.org/abs/2601.04726)
*Yuyang Hu,Jiongnan Liu,Jiejun Tan,Yutao Zhu,Zhicheng Dou*

Main category: cs.AI

TL;DR: CompassMem is an event-centric memory framework that organizes experiences into an Event Graph with logical relations, enabling structured memory navigation for better long-horizon reasoning in LLM agents.


<details>
  <summary>Details</summary>
Motivation: Current LLM agents lack effective memory mechanisms for long-horizon scenarios. Existing approaches use flat memory organization and simple similarity-based retrieval, failing to capture logical relationships between experiences and preventing logical reasoning over long-term dependencies.

Method: CompassMem organizes memory as an Event Graph by incrementally segmenting experiences into events and linking them through explicit logical relations. This graph serves as a logic map that enables structured, goal-directed navigation over memory beyond superficial semantic retrieval.

Result: Experiments on LoCoMo and NarrativeQA benchmarks show that CompassMem consistently improves both retrieval and reasoning performance across multiple backbone models.

Conclusion: The event-centric memory framework with structured Event Graph organization enables more effective long-horizon reasoning in LLM agents by moving beyond simple similarity-based retrieval to logical memory navigation.

Abstract: Large language models (LLMs) are increasingly deployed as intelligent agents that reason, plan, and interact with their environments. To effectively scale to long-horizon scenarios, a key capability for such agents is a memory mechanism that can retain, organize, and retrieve past experiences to support downstream decision-making. However, most existing approaches organize and store memories in a flat manner and rely on simple similarity-based retrieval techniques. Even when structured memory is introduced, existing methods often struggle to explicitly capture the logical relationships among experiences or memory units. Moreover, memory access is largely detached from the constructed structure and still depends on shallow semantic retrieval, preventing agents from reasoning logically over long-horizon dependencies. In this work, we propose CompassMem, an event-centric memory framework inspired by Event Segmentation Theory. CompassMem organizes memory as an Event Graph by incrementally segmenting experiences into events and linking them through explicit logical relations. This graph serves as a logic map, enabling agents to perform structured and goal-directed navigation over memory beyond superficial retrieval, progressively gathering valuable memories to support long-horizon reasoning. Experiments on LoCoMo and NarrativeQA demonstrate that CompassMem consistently improves both retrieval and reasoning performance across multiple backbone models.

</details>


### [183] [Miner:Mining Intrinsic Mastery for Data-Efficient RL in Large Reasoning Models](https://arxiv.org/abs/2601.04731)
*Shuyang Jiang,Yuhao Wang,Ya Zhang,Yanfeng Wang,Yu Wang*

Main category: cs.AI

TL;DR: Miner introduces a novel RL method that uses policy's intrinsic uncertainty as self-supervised reward to solve inefficiency in critic-free RL for reasoning models, achieving SOTA performance with up to 4.58 Pass@1 gains.


<details>
  <summary>Details</summary>
Motivation: Current critic-free RL methods for large reasoning models are inefficient when training on positive homogeneous prompts (where all rollouts are correct), leading to wasted rollouts due to zero advantage estimates.

Method: Miner repurposes policy's intrinsic uncertainty as self-supervised reward signal without external supervision. Key innovations: (1) token-level focal credit assignment that amplifies gradients on critical uncertain tokens while suppressing overconfident ones, and (2) adaptive advantage calibration to integrate intrinsic and verifiable rewards.

Result: Evaluated across six reasoning benchmarks on Qwen3-4B and Qwen3-8B base models, Miner achieves state-of-the-art performance among four other algorithms, yielding up to 4.58 absolute gains in Pass@1 and 6.66 gains in Pass@K compared to GRPO.

Conclusion: Latent uncertainty exploitation is both necessary and sufficient for efficient and scalable RL training of reasoning models, as demonstrated by Miner's superior performance compared to other exploration enhancement methods.

Abstract: Current critic-free RL methods for large reasoning models suffer from severe inefficiency when training on positive homogeneous prompts (where all rollouts are correct), resulting in waste of rollouts due to zero advantage estimates. We introduce a radically simple yet powerful solution to \uline{M}ine \uline{in}trinsic mast\uline{er}y (Miner), that repurposes the policy's intrinsic uncertainty as a self-supervised reward signal, with no external supervision, auxiliary models, or additional inference cost. Our method pioneers two key innovations: (1) a token-level focal credit assignment mechanism that dynamically amplifies gradients on critical uncertain tokens while suppressing overconfident ones, and (2) adaptive advantage calibration to seamlessly integrate intrinsic and verifiable rewards. Evaluated across six reasoning benchmarks on Qwen3-4B and Qwen3-8B base models, Miner achieves state-of-the-art performance among the other four algorithms, yielding up to \textbf{4.58} absolute gains in Pass@1 and \textbf{6.66} gains in Pass@K compared to GRPO. Comparison with other methods targeted at exploration enhancement further discloses the superiority of the two newly proposed innovations. This demonstrates that latent uncertainty exploitation is both necessary and sufficient for efficient and scalable RL training of reasoning models.

</details>


### [184] [AT$^2$PO: Agentic Turn-based Policy Optimization via Tree Search](https://arxiv.org/abs/2601.04767)
*Zefang Zong,Dingwei Chen,Yang Li,Qi Yi,Bo Zhou,Chengming Li,Bo Qian,Peng Chen,Jie Jiang*

Main category: cs.AI

TL;DR: AT²PO is a unified framework for multi-turn agentic RL that addresses exploration diversity, credit assignment, and policy optimization challenges through turn-level tree search and policy optimization.


<details>
  <summary>Details</summary>
Motivation: LLM agents need further refinement through agentic reinforcement learning, but face challenges with limited exploration diversity, sparse credit assignment, and misaligned policy optimization in multi-turn tasks.

Method: Introduces AT²PO with turn-level tree structure for Entropy-Guided Tree Expansion and Turn-wise Credit Assignment, plus Agentic Turn-based Policy Optimization (ATPO) as a turn-level learning objective.

Result: Experiments across seven benchmarks show consistent improvements over state-of-the-art baselines by up to 1.84 percentage points on average, with ablation studies validating each component.

Conclusion: AT²PO provides an effective unified framework for multi-turn agentic RL that addresses core challenges and can be integrated into existing RL pipelines, demonstrating significant performance improvements.

Abstract: LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT$^2$PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT$^2$PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO.

</details>


### [185] [Defense Against Indirect Prompt Injection via Tool Result Parsing](https://arxiv.org/abs/2601.04795)
*Qiang Yu,Xinran Cheng,Chuanyi Liu*

Main category: cs.AI

TL;DR: A novel defense method against Indirect Prompt Injection attacks on LLM agents that uses tool result parsing to filter malicious code while maintaining high utility and achieving the lowest attack success rate to date.


<details>
  <summary>Details</summary>
Motivation: As LLM agents gain control over physical systems and robotics, they face increasing threats from indirect prompt injection attacks where adversaries embed malicious instructions in tool call results to hijack decision-making. Existing defenses either require heavy computational resources for training detection models or have high attack success rates with prompt-based methods.

Method: The proposed method provides LLMs with precise data through tool result parsing while effectively filtering out injected malicious code. This approach leverages the inherent capabilities of LLMs but enhances them with structured parsing techniques to identify and remove adversarial content.

Result: The method achieves competitive Utility under Attack (UA) while maintaining the lowest Attack Success Rate (ASR) to date, significantly outperforming existing defense approaches against indirect prompt injection.

Conclusion: The proposed tool result parsing method provides an effective defense against indirect prompt injection attacks, balancing security (low ASR) with functionality (high UA), addressing critical vulnerabilities as LLM agents gain physical control capabilities.

Abstract: As LLM agents transition from digital assistants to physical controllers in autonomous systems and robotics, they face an escalating threat from indirect prompt injection. By embedding adversarial instructions into the results of tool calls, attackers can hijack the agent's decision-making process to execute unauthorized actions. This vulnerability poses a significant risk as agents gain more direct control over physical environments. Existing defense mechanisms against Indirect Prompt Injection (IPI) generally fall into two categories. The first involves training dedicated detection models; however, this approach entails high computational overhead for both training and inference, and requires frequent updates to keep pace with evolving attack vectors. Alternatively, prompt-based methods leverage the inherent capabilities of LLMs to detect or ignore malicious instructions via prompt engineering. Despite their flexibility, most current prompt-based defenses suffer from high Attack Success Rates (ASR), demonstrating limited robustness against sophisticated injection attacks. In this paper, we propose a novel method that provides LLMs with precise data via tool result parsing while effectively filtering out injected malicious code. Our approach achieves competitive Utility under Attack (UA) while maintaining the lowest Attack Success Rate (ASR) to date, significantly outperforming existing methods. Code is available at GitHub.

</details>


### [186] [DR-LoRA: Dynamic Rank LoRA for Mixture-of-Experts Adaptation](https://arxiv.org/abs/2601.04823)
*Guanzhi Deng,Bo Li,Ronghao Chen,Huacan Wang,Linqi Song,Lijie Wen*

Main category: cs.AI

TL;DR: DR-LoRA: Dynamic Rank LoRA for Mixture-of-Experts LLMs that adaptively allocates different LoRA ranks to experts based on task-specific demands, improving parameter efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Current PEFT methods like LoRA assign identical ranks to all experts in MoE LLMs, ignoring functional specialization. This uniform allocation causes resource mismatch - task-relevant experts are under-provisioned while less relevant ones get redundant parameters.

Method: DR-LoRA dynamically grows expert LoRA ranks during fine-tuning based on task demands. It uses Expert Saliency Scoring that combines expert routing frequency and LoRA rank importance to quantify each expert's need for additional capacity. Higher saliency experts get priority for rank expansion, creating a heterogeneous rank distribution.

Result: Experiments on multiple benchmarks show DR-LoRA consistently outperforms standard LoRA and static allocation strategies under the same parameter budget, achieving better task performance with more efficient parameter utilization.

Conclusion: DR-LoRA provides an effective dynamic rank allocation framework for fine-tuning MoE LLMs, addressing the resource mismatch problem and enabling more parameter-efficient adaptation to downstream tasks.

Abstract: Mixture-of-Experts (MoE) has become a prominent paradigm for scaling Large Language Models (LLMs). Parameter-efficient fine-tuning (PEFT), such as LoRA, is widely adopted to adapt pretrained MoE LLMs to downstream tasks. However, existing approaches assign identical LoRA ranks to all experts, overlooking the intrinsic functional specialization within MoE LLMs. This uniform allocation leads to resource mismatch, task-relevant experts are under-provisioned while less relevant ones receive redundant parameters. We propose a Dynamic Rank LoRA framework named DR-LoRA, which dynamically grows expert LoRA ranks during fine-tuning based on task-specific demands. DR-LoRA employs an Expert Saliency Scoring mechanism that integrates expert routing frequency and LoRA rank importance to quantify each expert's demand for additional capacity. Experts with higher saliency scores are prioritized for rank expansion, enabling the automatic formation of a heterogeneous rank distribution tailored to the target task. Experiments on multiple benchmarks demonstrate that DR-LoRA consistently outperforms standard LoRA and static allocation strategies under the same parameter budget, achieving superior task performance with more efficient parameter utilization.

</details>


### [187] [Higher-Order Knowledge Representations for Agentic Scientific Reasoning](https://arxiv.org/abs/2601.04878)
*Isabella A. Stewart,Markus J. Buehler*

Main category: cs.AI

TL;DR: Researchers developed a hypergraph-based knowledge representation system to capture higher-order scientific relationships, applied it to biocomposite scaffold literature, and used it for AI-driven hypothesis generation.


<details>
  <summary>Details</summary>
Motivation: Current AI systems for scientific reasoning have limitations: LLMs lack structural depth in retrieved contexts, while traditional knowledge graphs fail to capture irreducible higher-order interactions that govern emergent physical behavior in complex systems.

Method: Introduced hypergraph-based knowledge representations to encode multi-entity relationships, applied to ~1,100 manuscripts on biocomposite scaffolds. Created a global hypergraph with 161,172 nodes and 320,201 hyperedges. Used node-intersection constraints for hypergraph traversal in agentic systems.

Result: Constructed hypergraph revealed scale-free topology (power law exponent ~1.23) organized around conceptual hubs. System successfully generated grounded mechanistic hypotheses, such as linking cerium oxide to PCL scaffolds via chitosan intermediates. Hypergraph prevented combinatorial explosion typical of pairwise expansions.

Conclusion: Hypergraph topology serves as a verifiable guardrail for "teacherless" agentic reasoning systems, accelerating scientific discovery by uncovering relationships obscured by traditional graph methods and enabling bridging of semantically distant concepts.

Abstract: Scientific inquiry requires systems-level reasoning that integrates heterogeneous experimental data, cross-domain knowledge, and mechanistic evidence into coherent explanations. While Large Language Models (LLMs) offer inferential capabilities, they often depend on retrieval-augmented contexts that lack structural depth. Traditional Knowledge Graphs (KGs) attempt to bridge this gap, yet their pairwise constraints fail to capture the irreducible higher-order interactions that govern emergent physical behavior. To address this, we introduce a methodology for constructing hypergraph-based knowledge representations that faithfully encode multi-entity relationships. Applied to a corpus of ~1,100 manuscripts on biocomposite scaffolds, our framework constructs a global hypergraph of 161,172 nodes and 320,201 hyperedges, revealing a scale-free topology (power law exponent ~1.23) organized around highly connected conceptual hubs. This representation prevents the combinatorial explosion typical of pairwise expansions and explicitly preserves the co-occurrence context of scientific formulations. We further demonstrate that equipping agentic systems with hypergraph traversal tools, specifically using node-intersection constraints, enables them to bridge semantically distant concepts. By exploiting these higher-order pathways, the system successfully generates grounded mechanistic hypotheses for novel composite materials, such as linking cerium oxide to PCL scaffolds via chitosan intermediates. This work establishes a "teacherless" agentic reasoning system where hypergraph topology acts as a verifiable guardrail, accelerating scientific discovery by uncovering relationships obscured by traditional graph methods.

</details>


### [188] [ConMax: Confidence-Maximizing Compression for Efficient Chain-of-Thought Reasoning](https://arxiv.org/abs/2601.04973)
*Minda Hu,Zexuan Qiu,Zenan Xu,Kun Li,Bo Zhou,Irwin King*

Main category: cs.AI

TL;DR: ConMax is a reinforcement learning framework that compresses reasoning traces in Large Reasoning Models to reduce computational costs while maintaining accuracy, achieving 43% length reduction with only 0.7% accuracy drop.


<details>
  <summary>Details</summary>
Motivation: Large Reasoning Models often suffer from "overthinking" - generating redundant reasoning paths that increase computational costs without improving accuracy. Existing compression techniques for reasoning traces either compromise logical coherence or are too expensive to sample.

Method: ConMax formulates compression as a reward-driven optimization problem using reinforcement learning. It trains a policy to prune redundancy by maximizing a weighted combination of answer confidence (for predictive fidelity) and thinking confidence (for reasoning validity) through a frozen auxiliary LRM.

Result: Extensive experiments across five reasoning datasets show ConMax achieves superior efficiency-performance trade-off: reduces inference length by 43% over strong baselines with only 0.7% accuracy drop.

Conclusion: ConMax effectively generates high-quality, efficient training data for LRMs by automatically compressing reasoning traces while preserving essential reasoning patterns, addressing the overthinking problem in large reasoning models.

Abstract: Recent breakthroughs in Large Reasoning Models (LRMs) have demonstrated that extensive Chain-of-Thought (CoT) generation is critical for enabling intricate cognitive behaviors, such as self-verification and backtracking, to solve complex tasks. However, this capability often leads to ``overthinking'', where models generate redundant reasoning paths that inflate computational costs without improving accuracy. While Supervised Fine-Tuning (SFT) on reasoning traces is a standard paradigm for the 'cold start' phase, applying existing compression techniques to these traces often compromises logical coherence or incurs prohibitive sampling costs. In this paper, we introduce ConMax (Confidence-Maximizing Compression), a novel reinforcement learning framework designed to automatically compress reasoning traces while preserving essential reasoning patterns. ConMax formulates compression as a reward-driven optimization problem, training a policy to prune redundancy by maximizing a weighted combination of answer confidence for predictive fidelity and thinking confidence for reasoning validity through a frozen auxiliary LRM. Extensive experiments across five reasoning datasets demonstrate that ConMax achieves a superior efficiency-performance trade-off. Specifically, it reduces inference length by 43% over strong baselines at the cost of a mere 0.7% dip in accuracy, proving its effectiveness in generating high-quality, efficient training data for LRMs.

</details>


### [189] [Publishing FAIR and Machine-actionable Reviews in Materials Science: The Case for Symbolic Knowledge in Neuro-symbolic Artificial Intelligence](https://arxiv.org/abs/2601.05051)
*Jennifer D'Souza,Soren Auer,Eleni Poupaki,Alex Watkins,Anjana Devi,Riikka L. Puurunen,Bora Karasulu,Adrie Mackus,Erwin Kessels*

Main category: cs.AI

TL;DR: Converting traditional scientific review tables into FAIR, machine-actionable knowledge in ORKG, enabling structured querying and advocating for symbolic knowledge as backbone for reliable neurosymbolic AI in materials science.


<details>
  <summary>Details</summary>
Motivation: Scientific reviews contain valuable insights but remain locked in narrative text and static PDF tables, limiting reuse by both humans and machines. This hinders knowledge integration and computational analysis in materials science.

Method: Case study in atomic layer deposition/etching (ALD/E) where review tables are published as FAIR, machine-actionable comparisons in the Open Research Knowledge Graph (ORKG), transforming them into structured, queryable knowledge. The study contrasts symbolic querying over ORKG with large language model-based querying approaches.

Result: Successfully demonstrated conversion of review tables into structured, queryable knowledge in ORKG. Found that symbolic querying provides reliable backbone for knowledge access, while LLMs can serve as complementary interfaces when symbolically grounded.

Conclusion: A curated symbolic layer should remain the backbone of reliable neurosymbolic AI in materials science, with LLMs serving as complementary, symbolically grounded interfaces rather than standalone sources of truth. This approach ensures reliability while enhancing accessibility.

Abstract: Scientific reviews are central to knowledge integration in materials science, yet their key insights remain locked in narrative text and static PDF tables, limiting reuse by humans and machines alike. This article presents a case study in atomic layer deposition and etching (ALD/E) where we publish review tables as FAIR, machine-actionable comparisons in the Open Research Knowledge Graph (ORKG), turning them into structured, queryable knowledge. Building on this, we contrast symbolic querying over ORKG with large language model-based querying, and argue that a curated symbolic layer should remain the backbone of reliable neurosymbolic AI in materials science, with LLMs serving as complementary, symbolically grounded interfaces rather than standalone sources of truth.

</details>


### [190] [Reinforced Efficient Reasoning via Semantically Diverse Exploration](https://arxiv.org/abs/2601.05053)
*Ziqi Zhao,Zhaochun Ren,Jiahong Zou,Liu Yang,Zhiwei Xu,Xuri Ge,Zhumin Chen,Xinyu Ma,Daiting Shi,Shuaiqiang Wang,Dawei Yin,Xin Xin*

Main category: cs.AI

TL;DR: ROSE improves RLVR for LLM reasoning by adding semantic diversity exploration and length-aware advantage estimation to enhance exploration and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing RLVR methods with MCTS extensions still suffer from limited exploration diversity and inefficient reasoning, needing better mechanisms for diverse reasoning paths and more efficient credit assignment.

Method: ROSE incorporates: 1) semantic-entropy-based branching strategy to select points with high semantic divergence for new reasoning paths, 2) ε-exploration mechanism to stochastically initiate rollouts from root, and 3) length-aware segment-level advantage estimator to reward concise correct reasoning.

Result: Extensive experiments on various mathematical reasoning benchmarks with Qwen and Llama models validate the effectiveness and efficiency of ROSE.

Conclusion: ROSE successfully addresses exploration diversity and efficiency challenges in RLVR for LLM reasoning through semantic diversity mechanisms and length-aware advantage estimation.

Abstract: Reinforcement learning with verifiable rewards (RLVR) has proven effective in enhancing the reasoning of large language models (LLMs). Monte Carlo Tree Search (MCTS)-based extensions improve upon vanilla RLVR (e.g., GRPO) by providing tree-based reasoning rollouts that enable fine-grained and segment-level credit assignment. However, existing methods still suffer from limited exploration diversity and inefficient reasoning. To address the above challenges, we propose reinforced efficient reasoning via semantically diverse explorations, i.e., ROSE, for LLMs. To encourage more diverse reasoning exploration, our method incorporates a semantic-entropy-based branching strategy and an $\varepsilon$-exploration mechanism. The former operates on already sampled reasoning rollouts to capture semantic uncertainty and select branching points with high semantic divergence to generate new successive reasoning paths, whereas the latter stochastically initiates reasoning rollouts from the root, preventing the search process from becoming overly local. To improve efficiency, we design a length-aware segment-level advantage estimator that rewards concise and correct reasoning while penalizing unnecessarily long reasoning chains. Extensive experiments on various mathematical reasoning benchmarks with Qwen and Llama models validate the effectiveness and efficiency of ROSE. Codes are available at https://github.com/ZiqiZhao1/ROSE-rl.

</details>


### [191] [Token-Level LLM Collaboration via FusionRoute](https://arxiv.org/abs/2601.05106)
*Nuoya Xiong,Yuhang Zhou,Hanqing Zeng,Zhaorun Chen,Furong Huang,Shuchao Bi,Lizhu Zhang,Zhuokai Zhao*

Main category: cs.AI

TL;DR: FusionRoute is a token-level multi-LLM collaboration framework that uses a lightweight router to select expert models and provide complementary logits, overcoming limitations of pure expert-only routing while maintaining efficiency.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the dilemma between large general-purpose LLMs (expensive to train/deploy) and smaller domain-specialized models (efficient but poor generalization). There's a need for a solution that combines efficiency with strong cross-domain performance.

Method: FusionRoute uses a lightweight router that simultaneously: 1) selects the most suitable expert model at each decoding step, and 2) contributes complementary logits that refine or correct the selected expert's next-token distribution via logit addition. This differs from existing methods that rely solely on fixed expert outputs.

Result: Empirical results across Llama-3 and Gemma-2 families show FusionRoute outperforms sequence- and token-level collaboration, model merging, and direct fine-tuning on diverse benchmarks (mathematical reasoning, code generation, instruction following), while remaining competitive with domain experts on their respective tasks.

Conclusion: FusionRoute provides a theoretically sound and empirically effective framework for multi-LLM collaboration that overcomes fundamental limitations of pure expert-only routing, enabling optimal decoding policies under mild conditions while maintaining efficiency.

Abstract: Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks.

</details>


### [192] [Observations and Remedies for Large Language Model Bias in Self-Consuming Performative Loop](https://arxiv.org/abs/2601.05184)
*Yaxuan Wang,Zhongteng Cai,Yujia Bao,Xueru Zhang,Yang Liu*

Main category: cs.AI

TL;DR: The paper introduces the Self-Consuming Performative Loop (SCPL) concept to study how synthetic data from LLMs creates feedback loops that amplify biases, and proposes reward-based rejection sampling to mitigate these biases.


<details>
  <summary>Details</summary>
Motivation: LLMs trained on their own synthetic outputs create self-consuming loops that can cause performance drops and emerging biases. Real-world deployment creates dynamic systems where user feedback influences data collection, potentially amplifying biases against underserved groups.

Method: Introduces SCPL concept to study bias evolution in controlled performative feedback settings. Examines two loop types: typical retraining and incremental fine-tuning. Uses three real-world tasks and designs reward-based rejection sampling strategy to mitigate bias.

Result: Performative loops increase preference bias but decrease disparate bias. The proposed reward-based rejection sampling strategy effectively mitigates these biases in self-improving systems.

Conclusion: Self-consuming loops in LLMs create complex bias dynamics that need careful management. The proposed SCPL framework and mitigation strategies help build more trustworthy self-improving AI systems by addressing feedback-driven bias evolution.

Abstract: The rapid advancement of large language models (LLMs) has led to growing interest in using synthetic data to train future models. However, this creates a self-consuming retraining loop, where models are trained on their own outputs and may cause performance drops and induce emerging biases. In real-world applications, previously deployed LLMs may influence the data they generate, leading to a dynamic system driven by user feedback. For example, if a model continues to underserve users from a group, less query data will be collected from this particular demographic of users. In this study, we introduce the concept of \textbf{S}elf-\textbf{C}onsuming \textbf{P}erformative \textbf{L}oop (\textbf{SCPL}) and investigate the role of synthetic data in shaping bias during these dynamic iterative training processes under controlled performative feedback. This controlled setting is motivated by the inaccessibility of real-world user preference data from dynamic production systems, and enables us to isolate and analyze feedback-driven bias evolution in a principled manner. We focus on two types of loops, including the typical retraining setting and the incremental fine-tuning setting, which is largely underexplored. Through experiments on three real-world tasks, we find that the performative loop increases preference bias and decreases disparate bias. We design a reward-based rejection sampling strategy to mitigate the bias, moving towards more trustworthy self-improving systems.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [193] [Scalable neural pushbroom architectures for real-time denoising of hyperspectral images onboard satellites](https://arxiv.org/abs/2601.05020)
*Ziyao Yi,Davide Piccinini,Diego Valsesia,Tiziano Bianchi,Enrico Magli*

Main category: eess.IV

TL;DR: Proposes a neural network design for onboard hyperspectral image denoising on satellites that balances high-quality inference, dynamic power scalability, and fault tolerance using a mixture of denoisers with causal line-by-line processing.


<details>
  <summary>Details</summary>
Motivation: Next-gen Earth observation satellites need intelligent models onboard to reduce latency for time-critical applications. Hyperspectral imaging on satellites presents unique constraints (power, memory, radiation) not addressed by traditional computer vision, requiring designs that balance quality, power scalability, and fault tolerance.

Method: Proposes a mixture of denoisers architecture with fault tolerance and power scaling capabilities. Each denoiser uses causal line-by-line processing with memory of past lines to match pushbroom sensor acquisition and limit memory requirements. Enables real-time processing on low-power hardware.

Result: The architecture achieves real-time processing (one line per acquisition cycle) on low-power hardware with competitive denoising quality compared to more complex state-of-the-art models. Demonstrates design space with tradeoffs between power scalability, fault tolerance, and denoising quality.

Conclusion: The proposed neural network design successfully addresses the three competing objectives for onboard hyperspectral image processing: high-quality inference, dynamic power scalability, and fault tolerance, enabling practical deployment on next-generation Earth observation satellites.

Abstract: The next generation of Earth observation satellites will seek to deploy intelligent models directly onboard the payload in order to minimize the latency incurred by the transmission and processing chain of the ground segment, for time-critical applications. Designing neural architectures for onboard execution, particularly for satellite-based hyperspectral imagers, poses novel challenges due to the unique constraints of this environment and imaging system that are largely unexplored by the traditional computer vision literature. In this paper, we show that this setting requires addressing three competing objectives, namely high-quality inference with low complexity, dynamic power scalability and fault tolerance. We focus on the problem of hyperspectral image denoising, which is a critical task to enable effective downstream inference, and highlights the constraints of the onboard processing scenario. We propose a neural network design that addresses the three aforementioned objectives with several novel contributions. In particular, we propose a mixture of denoisers that can be resilient to radiation-induced faults as well as allowing for time-varying power scaling. Moreover, each denoiser employs an innovative architecture where an image is processed line-by-line in a causal way, with a memory of past lines, in order to match the acquisition process of pushbroom hyperspectral sensors and greatly limit memory requirements. We show that the proposed architecture can run in real-time, i.e., process one line in the time it takes to acquire the next one, on low-power hardware and provide competitive denoising quality with respect to significantly more complex state-of-the-art models. We also show that the power scalability and fault tolerance objectives provide a design space with multiple tradeoffs between those properties and denoising quality.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [194] [ArtCognition: A Multimodal AI Framework for Affective State Sensing from Visual and Kinematic Drawing Cues](https://arxiv.org/abs/2601.04297)
*Behrad Binaei-Haghighi,Nafiseh Sadat Sajadi,Mehrad Liviyan,Reyhane Akhavan Kharazi,Fatemeh Amirkhani,Behnam Bahrak*

Main category: cs.LG

TL;DR: ArtCognition: A multimodal framework combining visual features from drawings and behavioral kinematic data from the drawing process to analyze psychological states using the House-Tree-Person test, enhanced by RAG for explainable psychological interpretation.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of objectively assessing human affective and psychological states through non-verbal channels. Digital drawing is identified as a rich but underexplored modality for affective sensing, offering potential for non-intrusive mental health assessment.

Method: ArtCognition framework fuses two data streams: 1) static visual features from final artwork using computer vision models, and 2) dynamic behavioral kinematic cues from the drawing process (stroke speed, pauses, smoothness). A Retrieval-Augmented Generation (RAG) architecture bridges low-level features with high-level psychological interpretation using established psychological knowledge.

Result: The fusion of visual and behavioral kinematic cues provides more nuanced assessment than either modality alone. Significant correlations were found between extracted multimodal features and standardized psychological metrics, validating the framework's potential as a scalable clinical support tool.

Conclusion: This work contributes a new methodology for non-intrusive affective state assessment and opens new avenues for technology-assisted mental healthcare by combining multimodal drawing analysis with explainable AI through RAG architecture.

Abstract: The objective assessment of human affective and psychological states presents a significant challenge, particularly through non-verbal channels. This paper introduces digital drawing as a rich and underexplored modality for affective sensing. We present a novel multimodal framework, named ArtCognition, for the automated analysis of the House-Tree-Person (HTP) test, a widely used psychological instrument. ArtCognition uniquely fuses two distinct data streams: static visual features from the final artwork, captured by computer vision models, and dynamic behavioral kinematic cues derived from the drawing process itself, such as stroke speed, pauses, and smoothness. To bridge the gap between low-level features and high-level psychological interpretation, we employ a Retrieval-Augmented Generation (RAG) architecture. This grounds the analysis in established psychological knowledge, enhancing explainability and reducing the potential for model hallucination. Our results demonstrate that the fusion of visual and behavioral kinematic cues provides a more nuanced assessment than either modality alone. We show significant correlations between the extracted multimodal features and standardized psychological metrics, validating the framework's potential as a scalable tool to support clinicians. This work contributes a new methodology for non-intrusive affective state assessment and opens new avenues for technology-assisted mental healthcare.

</details>


### [195] [Aligned explanations in neural networks](https://arxiv.org/abs/2601.04378)
*Corentin Lobet,Francesca Chiaromonte*

Main category: cs.LG

TL;DR: PiNets are pseudo-linear neural networks designed for explanatory alignment, producing instance-wise linear predictions that are directly linked to model decisions rather than post-hoc rationalizations.


<details>
  <summary>Details</summary>
Motivation: Current feature attribution methods for explaining neural networks are often post-hoc rationalizations that don't truly reflect the model's prediction process, creating a need for explanations that are directly aligned with predictions to build trustworthy AI systems.

Method: PiNets (pseudo-linear networks) are proposed as a modeling framework that produces instance-wise linear predictions in arbitrary feature spaces, making them linearly readable and enabling explanatory alignment through their pseudo-linear architecture.

Result: PiNets demonstrate successful application to image classification and segmentation tasks, producing explanations that are faithful across multiple criteria while maintaining explanatory alignment between predictions and explanations.

Conclusion: Model readability through PiNets provides a principled approach to achieving explanatory alignment in deep learning, moving beyond post-hoc feature attribution methods to create more trustworthy and transparent neural network explanations.

Abstract: Feature attribution is the dominant paradigm for explaining deep neural networks. However, most existing methods only loosely reflect the model's prediction-making process, thereby merely white-painting the black box. We argue that explanatory alignment is a key aspect of trustworthiness in prediction tasks: explanations must be directly linked to predictions, rather than serving as post-hoc rationalizations. We present model readability as a design principle enabling alignment, and PiNets as a modeling framework to pursue it in a deep learning context. PiNets are pseudo-linear networks that produce instance-wise linear predictions in an arbitrary feature space, making them linearly readable. We illustrate their use on image classification and segmentation tasks, demonstrating how PiNets produce explanations that are faithful across multiple criteria in addition to alignment.

</details>


### [196] [IGenBench: Benchmarking the Reliability of Text-to-Infographic Generation](https://arxiv.org/abs/2601.04498)
*Yinghao Tang,Xueding Liu,Boyuan Zhang,Tingfeng Lan,Yupeng Xie,Jiale Lao,Yiyao Wang,Haoxuan Li,Tingting Gao,Bo Pan,Luoxuan Weng,Xiuqi Huang,Minfeng Zhu,Yingchaojie Feng,Yuyu Luo,Wei Chen*

Main category: cs.LG

TL;DR: IGENBENCH is the first benchmark for evaluating text-to-infographic generation reliability, revealing significant reliability gaps in current T2I models despite their aesthetic appeal.


<details>
  <summary>Details</summary>
Motivation: Current text-to-image models can generate visually appealing infographics but their reliability is unclear, with potential issues like distorted data encoding and incorrect text that are easily overlooked.

Method: Created IGENBENCH with 600 curated test cases across 30 infographic types, designed an automated evaluation framework with 10 question types, and used multimodal LLMs to verify atomic yes/no questions.

Result: Evaluation of 10 SOTA T2I models shows: 1) three-tier performance hierarchy with top model Q-ACC 0.90 but I-ACC only 0.49; 2) data-related dimensions are universal bottlenecks (Data Completeness: 0.21); 3) end-to-end correctness remains challenging.

Conclusion: IGENBENCH reveals significant reliability gaps in current T2I models for infographic generation, highlighting data-related issues as critical bottlenecks and providing a benchmark for future model development.

Abstract: Infographics are composite visual artifacts that combine data visualizations with textual and illustrative elements to communicate information. While recent text-to-image (T2I) models can generate aesthetically appealing images, their reliability in generating infographics remains unclear. Generated infographics may appear correct at first glance but contain easily overlooked issues, such as distorted data encoding or incorrect textual content. We present IGENBENCH, the first benchmark for evaluating the reliability of text-to-infographic generation, comprising 600 curated test cases spanning 30 infographic types. We design an automated evaluation framework that decomposes reliability verification into atomic yes/no questions based on a taxonomy of 10 question types. We employ multimodal large language models (MLLMs) to verify each question, yielding question-level accuracy (Q-ACC) and infographic-level accuracy (I-ACC). We comprehensively evaluate 10 state-of-the-art T2I models on IGENBENCH. Our systematic analysis reveals key insights for future model development: (i) a three-tier performance hierarchy with the top model achieving Q-ACC of 0.90 but I-ACC of only 0.49; (ii) data-related dimensions emerging as universal bottlenecks (e.g., Data Completeness: 0.21); and (iii) the challenge of achieving end-to-end correctness across all models. We release IGENBENCH at https://igen-bench.vercel.app/.

</details>


### [197] [A Vision for Multisensory Intelligence: Sensing, Synergy, and Science](https://arxiv.org/abs/2601.04563)
*Paul Pu Liang*

Main category: cs.LG

TL;DR: This paper outlines a 10-year research vision for multisensory AI that integrates all human senses (sight, sound, touch, taste, smell) with physiological and environmental signals to transform human-AI interaction.


<details>
  <summary>Details</summary>
Motivation: Current AI has advanced primarily in digital modalities (text, vision, audio), but human experience is fundamentally multisensory. The authors aim to bridge this gap by connecting AI to the full spectrum of human senses and physical/environmental signals.

Method: The research vision is structured around three interrelated themes: 1) Sensing - extending AI's ability to capture richer non-digital signals, 2) Science - developing principled approaches for quantifying multimodal heterogeneity, unified architectures, and cross-modal transfer, and 3) Synergy - addressing technical challenges in multisensory integration, alignment, reasoning, generation, and human-AI interaction.

Result: The paper presents a comprehensive research roadmap rather than specific experimental results. It accompanies projects, resources, and demos from the MIT Media Lab's Multisensory Intelligence group, showcasing latest advances in this emerging field.

Conclusion: Multisensory AI represents a transformative direction that can fundamentally change how humans and AI experience and interact with each other by connecting AI to the complete human sensory experience and environmental signals over the next decade.

Abstract: Our experience of the world is multisensory, spanning a synthesis of language, sight, sound, touch, taste, and smell. Yet, artificial intelligence has primarily advanced in digital modalities like text, vision, and audio. This paper outlines a research vision for multisensory artificial intelligence over the next decade. This new set of technologies can change how humans and AI experience and interact with one another, by connecting AI to the human senses and a rich spectrum of signals from physiological and tactile cues on the body, to physical and social signals in homes, cities, and the environment. We outline how this field must advance through three interrelated themes of sensing, science, and synergy. Firstly, research in sensing should extend how AI captures the world in richer ways beyond the digital medium. Secondly, developing a principled science for quantifying multimodal heterogeneity and interactions, developing unified modeling architectures and representations, and understanding cross-modal transfer. Finally, we present new technical challenges to learn synergy between modalities and between humans and AI, covering multisensory integration, alignment, reasoning, generation, generalization, and experience. Accompanying this vision paper are a series of projects, resources, and demos of latest advances from the Multisensory Intelligence group at the MIT Media Lab, see https://mit-mi.github.io/.

</details>


### [198] [The Forgotten Shield: Safety Grafting in Parameter-Space for Medical MLLMs](https://arxiv.org/abs/2601.04199)
*Jiale Zhao,Xing Mou,Jinlin Wu,Hongyuan Yu,Mingrui Sun,Yang Shi,Xuanwu Yin,Zhen Chen,Zhen Lei,Yaohua Wang*

Main category: cs.LG

TL;DR: The paper identifies safety vulnerabilities in Medical MLLMs, proposes a Parameter-Space Intervention method for safety re-alignment without additional safety data, and achieves better safety-performance trade-offs.


<details>
  <summary>Details</summary>
Motivation: Medical MLLMs have advanced in specialized tasks but lack safety research, posing real-world deployment risks. Current models show vulnerabilities to jailbreak attacks and suffer from safety forgetting during medical fine-tuning.

Method: Proposes Parameter-Space Intervention: extracts safety knowledge from base models and injects it into target models during medical capability construction, plus a fine-grained parameter search algorithm for optimal safety-performance balance.

Result: Experimental results show the approach significantly improves safety guardrails without additional safety data while minimizing degradation to core medical performance.

Conclusion: The method effectively addresses safety vulnerabilities in Medical MLLMs through efficient safety re-alignment, balancing safety and medical capabilities without requiring extra safety data.

Abstract: Medical Multimodal Large Language Models (Medical MLLMs) have achieved remarkable progress in specialized medical tasks; however, research into their safety has lagged, posing potential risks for real-world deployment. In this paper, we first establish a multidimensional evaluation framework to systematically benchmark the safety of current SOTA Medical MLLMs. Our empirical analysis reveals pervasive vulnerabilities across both general and medical-specific safety dimensions in existing models, particularly highlighting their fragility against cross-modality jailbreak attacks. Furthermore, we find that the medical fine-tuning process frequently induces catastrophic forgetting of the model's original safety alignment. To address this challenge, we propose a novel "Parameter-Space Intervention" approach for efficient safety re-alignment. This method extracts intrinsic safety knowledge representations from original base models and concurrently injects them into the target model during the construction of medical capabilities. Additionally, we design a fine-grained parameter search algorithm to achieve an optimal trade-off between safety and medical performance. Experimental results demonstrate that our approach significantly bolsters the safety guardrails of Medical MLLMs without relying on additional domain-specific safety data, while minimizing degradation to core medical performance.

</details>


### [199] [Mitigating Position-Shift Failures in Text-Based Modular Arithmetic via Position Curriculum and Template Diversity](https://arxiv.org/abs/2601.04283)
*Nikolay Yudin*

Main category: cs.LG

TL;DR: Transformers trained for modular addition fail catastrophically when input format changes (position shift or different templates), despite high in-distribution accuracy. A simple training recipe with boundary markers, position curriculum, diverse templates, and consistency training improves robustness.


<details>
  <summary>Details</summary>
Motivation: To study robustness under input-format variation rather than just in-distribution accuracy, focusing on how models trained for procedural tasks (modular addition) fail when the same expression appears at different character positions or under different natural-language templates.

Method: Train character-level Transformers on modular addition with disjoint-pair split. Introduce training recipe: (1) explicit expression boundary markers, (2) position curriculum broadening absolute position range, (3) diverse template mixtures, (4) consistency training across multiple variants per example.

Result: Baseline models achieve high in-distribution accuracy but collapse under position shift and template OOD. The proposed training recipe substantially improves robustness to both position shift and template OOD while maintaining high in-distribution accuracy. ALiBi-style ablation fails to learn the task.

Conclusion: Steering procedural generalization under noisy supervision benefits from explicitly training invariances absent from the data distribution. The paper provides reproducible evaluation protocol and artifacts for studying robustness to input-format variations.

Abstract: Building on insights from the grokking literature, we study character-level Transformers trained to compute modular addition from text, and focus on robustness under input-format variation rather than only in-distribution accuracy. We identify a previously under-emphasized failure mode: models that achieve high in-distribution accuracy can fail catastrophically when the same expression is shifted to different absolute character positions ("position shift") or presented under out-of-distribution natural-language templates. Using a disjoint-pair split over all ordered pairs for p=97, we show that a baseline model reaches strong in-distribution performance yet collapses under position shift and template OOD. We then introduce a simple training recipe that combines (i) explicit expression boundary markers, (ii) position curriculum that broadens the range of absolute positions seen during training, (iii) diverse template mixtures, and (iv) consistency training across multiple variants per example. Across three seeds, this intervention substantially improves robustness to position shift and template OOD while maintaining high in-distribution accuracy, whereas an ALiBi-style ablation fails to learn the task under our setup. Our results suggest that steering procedural generalization under noisy supervision benefits from explicitly training invariances that are otherwise absent from the data distribution, and we provide a reproducible evaluation protocol and artifacts.

</details>


### [200] [Quantifying the Effect of Test Set Contamination on Generative Evaluations](https://arxiv.org/abs/2601.04301)
*Rylan Schaeffer,Joshua Kazdan,Baber Abbasi,Ken Ziyu Liu,Brando Miranda,Ahmed Ahmed,Abhay Puri,Niloofar Mireshghallah,Sanmi Koyejo*

Main category: cs.LG

TL;DR: Test set contamination affects generative AI evaluations differently than discriminative ones, with models achieving lower loss than irreducible error when contaminated, and memorization effects modulated by temperature and solution length.


<details>
  <summary>Details</summary>
Motivation: While test set contamination's impact on discriminative evaluations is well-studied, little research exists on its effects on generative evaluations, which is critical for accurate assessment of frontier AI systems trained on web-scale data.

Method: Pretrained language models on mixtures of web data and MATH benchmark, sweeping model sizes and number of test set replicas in pretraining. Used scaling laws to analyze contamination effects, studied further training (overtraining with fresh data, supervised finetuning), and analyzed inference factors like temperature and solution length.

Result: Performance improves with contamination and model size; including even one test set replica enables models to achieve lower loss than irreducible error of uncontaminated training. Overtraining with fresh data reduces contamination effects, while supervised finetuning effects depend on pretraining contamination level. High sampling temperatures mitigate contamination, and longer solutions are exponentially harder to memorize than shorter ones.

Conclusion: Test set contamination introduces complex interactions between generation and memorization in generative evaluations, presenting new challenges for trustworthy AI system evaluation that differ from discriminative assessments.

Abstract: As frontier AI systems are pretrained on web-scale data, test set contamination has become a critical concern for accurately assessing their capabilities. While research has thoroughly investigated the impact of test set contamination on discriminative evaluations like multiple-choice question-answering, comparatively little research has studied the impact of test set contamination on generative evaluations. In this work, we quantitatively assess the effect of test set contamination on generative evaluations through the language model lifecycle. We pretrain language models on mixtures of web data and the MATH benchmark, sweeping model sizes and number of test set replicas contaminating the pretraining corpus; performance improves with contamination and model size. Using scaling laws, we make a surprising discovery: including even a single test set replica enables models to achieve lower loss than the irreducible error of training on the uncontaminated corpus. We then study further training: overtraining with fresh data reduces the effects of contamination, whereas supervised finetuning on the training set can either increase or decrease performance on test data, depending on the amount of pretraining contamination. Finally, at inference, we identify factors that modulate memorization: high sampling temperatures mitigate contamination effects, and longer solutions are exponentially more difficult to memorize than shorter ones, presenting a contrast with discriminative evaluations, where solutions are only a few tokens in length. By characterizing how generation and memorization interact, we highlight a new layer of complexity for trustworthy evaluation of AI systems.

</details>


### [201] [Rate or Fate? RLV$^\varepsilon$R: Reinforcement Learning with Verifiable Noisy Rewards](https://arxiv.org/abs/2601.04411)
*Ali Rad,Khashayar Filom,Darioush Keivan,Peyman Mohajerin Esfahani,Ehsan Kamalinejad*

Main category: cs.LG

TL;DR: RLVR (Reinforcement Learning with Verifiable Rewards) faces noisy verification in practice. The paper shows verification noise creates a phase transition: learning succeeds when Youden's index J>0 (TPR-FPR positive), fails when J<0, and is neutral at J=0.


<details>
  <summary>Details</summary>
Motivation: Real-world RLVR suffers from imperfect verification (noisy unit tests, human labels, LLM judges), especially in hard domains like coding where tests are sparse. The paper investigates whether verification noise merely slows learning or fundamentally changes outcomes.

Method: Develops an analytically tractable multi-armed bandit model of RLVR dynamics, instantiated with GRPO and validated in controlled experiments. Models false positives/negatives and groups completions into reasoning modes, yielding replicator-style flow on probability simplex.

Result: Reveals sharp phase transition: when J=TPR-FPR>0, incorrect modes go extinct (learning); J=0 yields neutral dynamics; J<0 causes incorrect modes to dominate (anti-learning/collapse). In learning regime, noise primarily rescales convergence time. Experiments on programming tasks confirm J=0 boundary.

Conclusion: Verification noise determines RLVR fate, not just rate. The framework provides general lens for analyzing RLVR stability, convergence, and algorithmic interventions beyond just noise analysis.

Abstract: Reinforcement learning with verifiable rewards (RLVR) is a simple but powerful paradigm for training LLMs: sample a completion, verify it, and update. In practice, however, the verifier is almost never clean--unit tests probe only limited corner cases; human and synthetic labels are imperfect; and LLM judges (e.g., RLAIF) are noisy and can be exploited--and this problem worsens on harder domains (especially coding) where tests are sparse and increasingly model-generated. We ask a pragmatic question: does the verification noise merely slow down the learning (rate), or can it flip the outcome (fate)?
  To address this, we develop an analytically tractable multi-armed bandit view of RLVR dynamics, instantiated with GRPO and validated in controlled experiments. Modeling false positives and false negatives and grouping completions into recurring reasoning modes yields a replicator-style (natural-selection) flow on the probability simplex. The dynamics decouples into within-correct-mode competition and a one-dimensional evolution for the mass on incorrect modes, whose drift is determined solely by Youden's index J=TPR-FPR. This yields a sharp phase transition: when J>0, the incorrect mass is driven toward extinction (learning); when J=0, the process is neutral; and when J<0, incorrect modes amplify until they dominate (anti-learning and collapse). In the learning regime J>0, noise primarily rescales convergence time ("rate, not fate"). Experiments on verifiable programming tasks under synthetic noise reproduce the predicted J=0 boundary. Beyond noise, the framework offers a general lens for analyzing RLVR stability, convergence, and algorithmic interventions.

</details>


### [202] [Not All Steps are Informative: On the Linearity of LLMs' RLVR Training](https://arxiv.org/abs/2601.04537)
*Tianle Wang,Zhongyuan Wu,Shenghao Jin,Hao Xu,Wei Chen,Ning Miao*

Main category: cs.LG

TL;DR: RLVR training for LLMs evolves linearly, allowing weight/logits extrapolation from intermediate checkpoints to achieve comparable or better performance with much less computation.


<details>
  <summary>Details</summary>
Motivation: RLVR requires thousands of training steps with substantial computation due to prolonged exploration. The authors discovered that LLMs evolve in a strongly linear manner during RLVR, suggesting that future model states could be predicted via extrapolation to avoid expensive continued training.

Method: The paper proposes two extrapolation methods: 1) Weight Extrapolation - predicting future model weights from intermediate checkpoints, and 2) Logits Extrapolation - extrapolating model output log-probabilities beyond stable RL training ranges.

Result: Weight Extrapolation produces models with performance comparable to standard RL training while requiring significantly less computation. Logits Extrapolation consistently outperforms continued RL training on all four benchmarks by extrapolating beyond the step range where RL training remains stable.

Conclusion: The linear evolution of LLMs during RLVR enables efficient extrapolation techniques that can dramatically reduce computational costs while maintaining or improving performance, challenging the need for prolonged RL training.

Abstract: Reinforcement learning with verifiable rewards (RLVR) has become a central component of large language model (LLM) post-training. Unlike supervised fine-tuning (SFT), RLVR lets an LLM generate multiple candidate solutions and reinforces those that lead to a verifiably correct final answer. However, in practice, RLVR often requires thousands of training steps to reach strong performance, incurring substantial computation largely attributed to prolonged exploration. In this work, we make a surprising observation: during RLVR, LLMs evolve in a strongly linear manner. Specifically, both model weights and model output log-probabilities exhibit strong linear correlations with RL training steps. This suggests that RLVR predominantly amplifies trends that emerge early in training, rather than continuously discovering new behaviors throughout the entire optimization trajectory. Motivated by this linearity, we investigate whether future model states can be predicted from intermediate checkpoints via extrapolation, avoiding continued expensive training. We show that Weight Extrapolation produces models with performance comparable to standard RL training while requiring significantly less computation. Moreover, Logits Extrapolation consistently outperforms continued RL training on all four benchmarks by extrapolating beyond the step range where RL training remains stable.

</details>


### [203] [On the Hidden Objective Biases of Group-based Reinforcement Learning](https://arxiv.org/abs/2601.05002)
*Aleksandar Fontana,Marco Simoni,Giulio Rossolini,Andrea Saracino,Paolo Mori*

Main category: cs.LG

TL;DR: Theoretical analysis reveals structural mismatches in GRPO-style RL methods, identifying three key limitations: gradient biases from non-uniform group weighting, AdamW insensitivity to reward scaling, and momentum-driven clipping violations.


<details>
  <summary>Details</summary>
Motivation: Despite empirical success of group-based RL methods like GRPO for post-training LLMs, there are structural mismatches between reward optimization and training objectives that need theoretical understanding.

Method: Theoretical analysis of GRPO-style methods using a unified surrogate formulation to study their properties systematically.

Result: Identified three recurring properties: (1) non-uniform group weighting causes systematic gradient biases on shared prefix tokens, (2) AdamW optimizer makes training dynamics insensitive to reward scaling, and (3) optimizer momentum can push policy updates beyond intended clipping regions.

Conclusion: These findings highlight fundamental limitations of current approaches and provide principled guidance for designing future formulations of group-based RL methods.

Abstract: Group-based reinforcement learning methods, like Group Relative Policy Optimization (GRPO), are widely used nowadays to post-train large language models. Despite their empirical success, they exhibit structural mismatches between reward optimization and the underlying training objective. In this paper, we present a theoretical analysis of GRPO style methods by studying them within a unified surrogate formulation. This perspective reveals recurring properties that affect all the methods under analysis: (i) non-uniform group weighting induces systematic gradient biases on shared prefix tokens; (ii) interactions with the AdamW optimizer make training dynamics largely insensitive to reward scaling; and (iii) optimizer momentum can push policy updates beyond the intended clipping region under repeated optimization steps. We believe that these findings highlight fundamental limitations of current approaches and provide principled guidance for the design of future formulations.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [204] [Re-Rankers as Relevance Judges](https://arxiv.org/abs/2601.04455)
*Chuan Meng,Jiqun Liu,Mohammad Aliannejadi,Fengran Mo,Jeff Dalton,Maarten de Rijke*

Main category: cs.IR

TL;DR: Reusing established re-ranking models as relevance judges can outperform specialized LLM-based judges in many cases, while exhibiting self-preference and cross-family biases.


<details>
  <summary>Details</summary>
Motivation: Current research treats LLM-based relevance judgment prediction as a separate task from re-ranking, despite both being forms of relevance prediction. This leads to redundant development and wasted resources. The paper aims to bridge this gap by exploring whether established re-ranking methods can be effectively adapted for relevance judgment tasks.

Method: Two adaptation strategies: (1) using binary tokens ("true"/"false") generated by re-rankers as direct judgments, and (2) converting continuous re-ranking scores into binary labels via thresholding. Experiments conducted on TREC-DL 2019-2023 with 8 re-rankers from 3 families (220M to 32B parameters).

Result: Re-ranker-based relevance judges outperform UMBRELA (state-of-the-art LLM-based judge) in 40-50% of cases. They exhibit strong self-preference toward their own and same-family re-rankers, as well as cross-family bias in evaluations.

Conclusion: Established re-ranking methods can be effectively repurposed for relevance judgment tasks, offering competitive performance while revealing important biases that need consideration in evaluation frameworks.

Abstract: Using large language models (LLMs) to predict relevance judgments has shown promising results. Most studies treat this task as a distinct research line, e.g., focusing on prompt design for predicting relevance labels given a query and passage. However, predicting relevance judgments is essentially a form of relevance prediction, a problem extensively studied in tasks such as re-ranking. Despite this potential overlap, little research has explored reusing or adapting established re-ranking methods to predict relevance judgments, leading to potential resource waste and redundant development. To bridge this gap, we reproduce re-rankers in a re-ranker-as-relevance-judge setup. We design two adaptation strategies: (i) using binary tokens (e.g., "true" and "false") generated by a re-ranker as direct judgments, and (ii) converting continuous re-ranking scores into binary labels via thresholding. We perform extensive experiments on TREC-DL 2019 to 2023 with 8 re-rankers from 3 families, ranging from 220M to 32B, and analyse the evaluation bias exhibited by re-ranker-based judges. Results show that re-ranker-based relevance judges, under both strategies, can outperform UMBRELA, a state-of-the-art LLM-based relevance judge, in around 40% to 50% of the cases; they also exhibit strong self-preference towards their own and same-family re-rankers, as well as cross-family bias.

</details>


### [205] [Succeeding at Scale: Automated Multi-Retriever Fusion and Query-Side Adaptation for Multi-Tenant Search](https://arxiv.org/abs/2601.04646)
*Prateek Jain,Shabari S Nair,Ritesh Goru,Prakhar Agarwal,Ajay Yadav,Yoga Sri Varshan Varadharajan,Constantine Caramanis*

Main category: cs.IR

TL;DR: The paper introduces DevRev Search, a technical support retrieval benchmark created via automatic pipeline with LLM-based filtering, and proposes Index-Preserving Adaptation using query-only LoRA fine-tuning to avoid costly document re-indexing.


<details>
  <summary>Details</summary>
Motivation: Large-scale multi-tenant retrieval systems face two key challenges: 1) lack of curated relevance labels ("dark data" problem) despite having vast query logs, and 2) prohibitive operational costs of model updates that require re-indexing entire document corpora in multi-tenant environments with thousands of isolated indices.

Method: 1) Created DevRev Search benchmark using automatic pipeline with fusion-based candidate generation (pooling diverse retrievers) and LLM-as-a-Judge for consistency filtering and relevance assignment. 2) Proposed Index-Preserving Adaptation strategy: fine-tuning only query encoder via Low-Rank Adaptation (LoRA) while keeping document index frozen, with experiments targeting specific transformer layers for optimal trade-offs.

Result: Experiments on DevRev Search and SciFact show that targeting specific transformer layers in query encoder yields optimal quality-efficiency trade-offs, achieving competitive performance improvements while avoiding costly document re-indexing.

Conclusion: The approach offers a scalable path for personalized enterprise search by addressing both the dark data problem through automatic benchmark creation and the operational cost problem through index-preserving adaptation with query-only fine-tuning.

Abstract: Large-scale multi-tenant retrieval systems amass vast user query logs yet critically lack the curated relevance labels required for effective domain adaptation. This "dark data" problem is exacerbated by the operational cost of model updates: jointly fine-tuning query and document encoders requires re-indexing the entire corpus, which is prohibitive in multi-tenant environments with thousands of isolated indices. To address these dual challenges, we introduce \textbf{DevRev Search}, a passage retrieval benchmark for technical customer support constructed through a fully automatic pipeline. We employ a \textbf{fusion-based candidate generation} strategy, pooling results from diverse sparse and dense retrievers, and utilize an LLM-as-a-Judge to perform rigorous \textbf{consistency filtering} and relevance assignment. We further propose a practical \textbf{Index-Preserving Adaptation} strategy: by fine-tuning only the query encoder via Low-Rank Adaptation (LoRA), we achieve competitive performance improvements while keeping the document index frozen. Our experiments on DevRev Search and SciFact demonstrate that targeting specific transformer layers in the query encoder yields optimal quality-efficiency trade-offs, offering a scalable path for personalized enterprise search.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [206] [Multi-Disciplinary Dataset Discovery from Citation-Verified Literature Contexts](https://arxiv.org/abs/2601.05099)
*Zhiyin Tan,Changxu Duan*

Main category: cs.DL

TL;DR: A literature-driven framework that discovers datasets from citation contexts in scientific papers, achieving higher recall than existing dataset search engines by grounding retrieval in actual research use rather than metadata quality.


<details>
  <summary>Details</summary>
Motivation: Existing dataset search engines rely heavily on metadata quality and keyword overlap, which often fail to capture the semantic intent of scientific investigation. There's a need for better dataset discovery methods that understand how datasets are actually used in research.

Method: Combines large-scale citation-context extraction, schema-guided dataset recognition with Large Language Models, and provenance-preserving entity resolution to discover datasets from citation contexts in scientific papers.

Result: Achieves substantially higher recall than Google Dataset Search and DataCite Commons, with normalized recall ranging from 47.47% to 81.82%. Surfaces additional datasets not documented in surveys, with expert assessments indicating high utility and novelty.

Conclusion: Citation-context mining establishes an effective and generalizable paradigm for dataset discovery, particularly in settings where datasets lack sufficient or reliable metadata. The approach enables retrieval grounded in actual research use rather than metadata availability.

Abstract: Identifying suitable datasets for a research question remains challenging because existing dataset search engines rely heavily on metadata quality and keyword overlap, which often fail to capture the semantic intent of scientific investigation. We introduce a literature-driven framework that discovers datasets from citation contexts in scientific papers, enabling retrieval grounded in actual research use rather than metadata availability. Our approach combines large-scale citation-context extraction, schema-guided dataset recognition with Large Language Models, and provenance-preserving entity resolution. We evaluate the system on eight survey-derived computer science queries and find that it achieves substantially higher recall than Google Dataset Search and DataCite Commons, with normalized recall ranging from an average of 47.47% to a highest value of 81.82%. Beyond recovering gold-standard datasets, the method also surfaces additional datasets not documented in the surveys. Expert assessments across five top-level Fields of Science indicate that a substantial portion of the additional datasets are considered high utility, and some are regarded as novel for the specific topics chosen by the experts. These findings establish citation-context mining as an effective and generalizable paradigm for dataset discovery, particularly in settings where datasets lack sufficient or reliable metadata. To support reproducibility and future extensions, we release our code, evaluation datasets, and results on GitHub (https://github.com/Fireblossom/citation-context-dataset-discovery).

</details>


### [207] [Semantically Orthogonal Framework for Citation Classification: Disentangling Intent and Content](https://arxiv.org/abs/2601.05103)
*Changxu Duan,Zhiyin Tan*

Main category: cs.DL

TL;DR: SOFT is a new citation classification framework that separates citation intent from cited content type, improving classification performance and cross-domain generalization compared to existing frameworks.


<details>
  <summary>Details</summary>
Motivation: Existing citation classification frameworks conflate citation intent with cited content type, creating a dilemma between fine-grained distinctions and practical classification reliability, limiting their effectiveness for automated classification in research assessment and digital libraries.

Method: SOFT (Semantically Orthogonal Framework with Two dimensions) explicitly separates citation intent from cited content type, inspired by semantic role theory. The authors systematically re-annotate the ACL-ARC dataset using SOFT and create a cross-disciplinary test set from ACT2, then evaluate using zero-shot and fine-tuned Large Language Models.

Result: SOFT enables higher agreement between human annotators and LLMs, supports stronger classification performance, and provides robust cross-domain generalization compared to ACL-ARC and SciCite frameworks. The framework improves clarity, consistency, and generalizability for digital libraries.

Conclusion: SOFT serves as a clear, reusable annotation standard that enhances citation classification for scholarly communication infrastructures, with all code and data publicly available for community use.

Abstract: Understanding the role of citations is essential for research assessment and citation-aware digital libraries. However, existing citation classification frameworks often conflate citation intent (why a work is cited) with cited content type (what part is cited), limiting their effectiveness in auto classification due to a dilemma between fine-grained type distinctions and practical classification reliability. We introduce SOFT, a Semantically Orthogonal Framework with Two dimensions that explicitly separates citation intent from cited content type, drawing inspiration from semantic role theory. We systematically re-annotate the ACL-ARC dataset using SOFT and release a cross-disciplinary test set sampled from ACT2. Evaluation with both zero-shot and fine-tuned Large Language Models demonstrates that SOFT enables higher agreement between human annotators and LLMs, and supports stronger classification performance and robust cross-domain generalization compared to ACL-ARC and SciCite annotation frameworks. These results confirm SOFT's value as a clear, reusable annotation standard, improving clarity, consistency, and generalizability for digital libraries and scholarly communication infrastructures. All code and data are publicly available on GitHub https://github.com/zhiyintan/SOFT.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [208] [Towards Spatio-Temporal Extrapolation of Phase-Field Simulations with Convolution-Only Neural Networks](https://arxiv.org/abs/2601.04510)
*Christophe Bonneville,Nathan Bieberdorf,Pieterjan Robbe,Mark Asta,Habib Najm,Laurent Capolungo,Cosmin Safta*

Main category: cs.CE

TL;DR: A deep learning surrogate model using conditional U-Net with diffusion model for initialization enables fast, accurate extrapolation of liquid metal dealloying simulations far beyond training data in space and time.


<details>
  <summary>Details</summary>
Motivation: Phase-field simulations of liquid metal dealloying are computationally expensive for large domains and long time horizons, limiting their practical application. There's a need for faster surrogate models that can extrapolate beyond training data while maintaining physical accuracy.

Method: Developed a fully convolutional, conditionally parameterized U-Net surrogate with convolutional self-attention and physically informed padding. Coupled with a conditional diffusion model to generate synthetic initial conditions, eliminating need for solver-based initialization. The model is trained on small domain, short time simulations but can extrapolate to larger scales using its convolutional architecture.

Result: Achieves relative errors below 5% in training regime and under 15% during large-scale extrapolation. Predicts key quantities of interest and spatial statistics accurately across multiple alloy compositions. Delivers speed-ups up to 36,000×, reducing weeks-long simulations to seconds.

Conclusion: The framework enables high-fidelity extrapolation of phase-field simulations for liquid metal dealloying in both space and time, representing a significant advancement in computational materials science with potential for broader application to other phase-field problems.

Abstract: Phase-field simulations of liquid metal dealloying (LMD) can capture complex microstructural evolutions but can be prohibitively expensive for large domains and long time horizons. In this paper, we introduce a fully convolutional, conditionally parameterized U-Net surrogate designed to extrapolate far beyond its training data in both space and time. The architecture integrates convolutional self-attention, physically informed padding, and a flood-fill corrector method to maintain accuracy under extreme extrapolation, while conditioning on simulation parameters allows for flexible time-step skipping and adaptation to varying alloy compositions. To remove the need for costly solver-based initialization, we couple the surrogate with a conditional diffusion model that generates synthetic, physically consistent initial conditions. We train our surrogate on simulations generated over small domain sizes and short time spans, but, by taking advantage of the convolutional nature of U-Nets, we are able to run and extrapolate surrogate simulations for longer time horizons than what would be achievable with classic numerical solvers. Across multiple alloy compositions, the framework is able to reproduce the LMD physics accurately. It predicts key quantities of interest and spatial statistics with relative errors typically below 5% in the training regime and under 15% during large-scale, long time-horizon extrapolations. Our framework can also deliver speed-ups of up to 36,000 times, bringing the time to run weeks-long simulations down to a few seconds. This work is a first stepping stone towards high-fidelity extrapolation in both space and time of phase-field simulation for LMD.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [209] [Sphinx: Benchmarking and Modeling for LLM-Driven Pull Request Review](https://arxiv.org/abs/2601.04252)
*Daoan Zhang,Shuo Zhang,Zijian Jin,Jiebo Luo,Shengyu Fu,Elsie Nallipogu*

Main category: cs.SE

TL;DR: Sphinx is a unified LLM framework for automated pull request review that addresses noisy supervision, limited context, and poor evaluation metrics through structured data generation, checklist-based evaluation, and novel training with rule-based rewards.


<details>
  <summary>Details</summary>
Motivation: Automating PR review is challenging due to noisy supervision (imperfect human reviews as training data), limited contextual understanding (models struggle with code context), and inadequate evaluation metrics (BLEU-like metrics don't capture review quality).

Method: Three key components: (1) Structured data generation pipeline comparing pseudo-modified vs merged code for context-rich training data; (2) Checklist-based evaluation benchmark with actionable verification points; (3) Checklist Reward Policy Optimization (CRPO) using rule-based, interpretable rewards to align models with real-world review practices.

Result: Models trained with Sphinx achieve state-of-the-art performance, outperforming proprietary and open-source baselines by up to 40% in checklist coverage, with superior review completeness and precision.

Conclusion: Sphinx enables development of PR review models that are fluent, context-aware, technically precise, and practically deployable in real-world development workflows, addressing key limitations in current automated PR review approaches.

Abstract: Pull request (PR) review is essential for ensuring software quality, yet automating this task remains challenging due to noisy supervision, limited contextual understanding, and inadequate evaluation metrics. We present Sphinx, a unified framework for LLM-based PR review that addresses these limitations through three key components: (1) a structured data generation pipeline that produces context-rich, semantically grounded review comments by comparing pseudo-modified and merged code; (2) a checklist-based evaluation benchmark that assesses review quality based on structured coverage of actionable verification points, moving beyond surface-level metrics like BLEU; and (3) Checklist Reward Policy Optimization (CRPO), a novel training paradigm that uses rule-based, interpretable rewards to align model behavior with real-world review practices. Extensive experiments show that models trained with Sphinx achieve state-of-the-art performance on review completeness and precision, outperforming both proprietary and open-source baselines by up to 40\% in checklist coverage. Together, Sphinx enables the development of PR review models that are not only fluent but also context-aware, technically precise, and practically deployable in real-world development workflows. The data will be released after review.

</details>


### [210] [Advancing Language Models for Code-related Tasks](https://arxiv.org/abs/2601.04526)
*Zhao Tian*

Main category: cs.SE

TL;DR: This paper addresses limitations in language models for software engineering through three approaches: improving code data quality, enhancing model architecture, and advancing reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: Existing language models struggle with complex programming scenarios due to limitations in data quality, model architecture, and reasoning capability, hindering their practical adoption in software development.

Method: Three complementary directions: (1) Code data quality improvement via CODA (code difference-guided adversarial augmentation) and CodeDenoise (code denoising); (2) Architecture enhancement via syntax-guided code LMs (LEAM and LEAM++); (3) Reasoning advancement via muFiX prompting technique and Specine agent-based technique.

Result: The paper presents a systematic framework of techniques that collectively address key limitations of language models in software engineering tasks, though specific quantitative results are not provided in the abstract.

Conclusion: These techniques aim to promote practical adoption of language models in software development and advance intelligent software engineering by systematically addressing data quality, architecture, and reasoning challenges.

Abstract: Recent advances in language models (LMs) have driven significant progress in various software engineering tasks. However, existing LMs still struggle with complex programming scenarios due to limitations in data quality, model architecture, and reasoning capability. This research systematically addresses these challenges through three complementary directions: (1) improving code data quality with a code difference-guided adversarial augmentation technique (CODA) and a code denoising technique (CodeDenoise); (2) enhancing model architecture via syntax-guided code LMs (LEAM and LEAM++); and (3) advancing model reasoning with a prompting technique (muFiX) and an agent-based technique (Specine). These techniques aim to promote the practical adoption of LMs in software development and further advance intelligent software engineering.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [211] [Generative Teaching via Code](https://arxiv.org/abs/2601.04204)
*Yuheng Wang,Runde Yang,Lin Wu,Jie Zhang,Jingru Fan,Ruoyu Fu,Tianle Zhou,Huatao Li,Siheng Chen,Weinan E,Chen Qian*

Main category: cs.CY

TL;DR: TeachMaster is a multi-agent framework that automates educational video creation by shifting educators from manual content creation to high-level direction, using code as an intermediate semantic medium for interpretable and editable video production.


<details>
  <summary>Details</summary>
Motivation: High-quality online education faces scalability challenges due to expensive, slow manual content creation. Current video generation methods lack pedagogical structure and precise control due to their pixel-level, black-box nature.

Method: TeachMaster uses a multi-agent framework with code as intermediate semantic medium. It orchestrates collaborative agents for planning, design, and rendering to automate production of interpretable, editable, curriculum-ready educational videos.

Result: Experiments show TeachMaster significantly boosts production efficiency while maintaining structural coherence and visual fidelity, providing a robust solution for scalable education.

Conclusion: Generative Teaching paradigm shifts educators to high-level directors while autonomous agents handle execution, enabling scalable, high-quality educational content production through interpretable, editable video generation.

Abstract: The scalability of high-quality online education is hindered by the high costs and slow cycles of labor-intensive manual content creation. Despite advancements in video generation, current approaches often fail to ensure pedagogical structure and precise control due to their pixel-level, black-box nature. In this paper, we propose Generative Teaching, a novel paradigm that transitions educators from manual creators to high-level directors, allowing them to focus on pedagogical intent while autonomous agents handle the execution. To realize this vision, we introduce TeachMaster, a multi-agent framework that leverages code as an intermediate semantic medium. Unlike traditional video generation methods, TeachMaster orchestrates a collaborative team of agents--spanning planning, design, and rendering--to automate the production of interpretable, editable, and curriculum-ready educational videos. Experiments validate that TeachMaster significantly boosts production efficiency without compromising structural coherence or visual fidelity, providing a robust solution for scalable education.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [212] [Generalization to Political Beliefs from Fine-Tuning on Sports Team Preferences](https://arxiv.org/abs/2601.04369)
*Owen Terry*

Main category: physics.soc-ph

TL;DR: Fine-tuning LLMs on sports team preferences leads to unexpected political belief changes, but not in the predicted liberal/conservative directions.


<details>
  <summary>Details</summary>
Motivation: To investigate how fine-tuning LLMs on narrow datasets (like sports team preferences) can lead to unexpected behavioral changes in unrelated domains (like political beliefs).

Method: Fine-tuned LLMs to prefer either coastal or Southern sports teams, then evaluated their political beliefs through numerical ratings of agreement with political statements and qualitative analysis of justifications for radical answers.

Result: Both fine-tuned models adopted political beliefs diverging from the base model, but surprisingly showed similar responses to each other without clear liberal/conservative bias. Models displayed varying willingness to justify their more radical answers.

Conclusion: Fine-tuning on simple, narrow datasets can lead to seemingly unrelated changes in model behavior, and further research is needed to understand the underlying mechanisms of this generalization.

Abstract: Fine-tuned LLMs often exhibit unexpected behavior as a result of generalizing beyond the data they're shown. We present results in which an LLM fine-tuned to prefer either coastal sports teams or Southern sports teams adopt political beliefs that diverge significantly from those of the base model. While we hypothesized that the coastal model would become more liberal and the southern model would become more conservative, we find that their responses are usually similar to each other, without a clear-cut liberal or conservative bias. In addition to asking the models for numerical ratings of agreement with relevant political statements, we ask them to elaborate on their more radical answers, finding varying degrees of willingness to justify themselves. Further work is needed to understand the mechanisms by which fine-tuning on simple, narrow datasets leads to seemingly unrelated changes in model behavior.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [213] [In-SRAM Radiant Foam Rendering on a Graph Processor](https://arxiv.org/abs/2601.04382)
*Zulkhuu Tuya,Ignacio Alzugaray,Nicholas Fry,Andrew J. Davison*

Main category: cs.GR

TL;DR: A distributed volumetric renderer for many-core accelerators with local SRAM that shards scenes across tiles and routes rays hierarchically, achieving near-interactive performance while keeping all data on-chip.


<details>
  <summary>Details</summary>
Motivation: Many-core accelerators with distributed local SRAM break the assumption of unified memory access needed for traditional volumetric rendering, requiring new approaches to handle data distribution and communication for ray traversal.

Method: Developed a fully in-SRAM distributed renderer using the Radiant Foam Voronoi-cell representation on Graphcore Mk2 IPU. The system shards scenes across tiles and forwards rays between shards through a hierarchical routing overlay, enabling ray marching entirely from on-chip SRAM with predictable communication.

Result: Achieved near-interactive throughput (~1 fps at 640×480) on Mip-NeRF 360 scenes with image and depth quality close to the original GPU-based implementation, while keeping all scene data and ray state in on-chip SRAM.

Conclusion: Demonstrates feasibility of distributed rendering on many-core accelerators and identifies routing, memory, and scheduling bottlenecks that inform future accelerator designs for irregular, data-movement-heavy rendering workloads.

Abstract: Many emerging many-core accelerators replace a single large device memory with hundreds to thousands of lightweight cores, each owning only a small local SRAM and exchanging data via explicit on-chip communication. This organization offers high aggregate bandwidth, but it breaks a key assumption behind many volumetric rendering techniques: that rays can randomly access a large, unified scene representation. Rendering efficiently on such hardware therefore requires distributing both data and computation, keeping ray traversal mostly local, and structuring communication into predictable routes.
  We present a fully in-SRAM, distributed renderer for the \emph{Radiant Foam} Voronoi-cell volumetric representation on the Graphcore Mk2 IPU, a many-core accelerator with tile-local SRAM and explicit inter-tile communication. Our system shards the scene across tiles and forwards rays between shards through a hierarchical routing overlay, enabling ray marching entirely from on-chip SRAM with predictable communication. On Mip-NeRF~360 scenes, the system attains near-interactive throughput (\(\approx\)1\,fps at \mbox{$640\times480$}) with image and depth quality close to the original GPU-based Radiant Foam implementation, while keeping all scene data and ray state in on-chip SRAM. Beyond demonstrating feasibility, we analyze routing, memory, and scheduling bottlenecks that inform how future distributed-memory accelerators can better support irregular, data-movement-heavy rendering workloads.

</details>


### [214] [GenAI-DrawIO-Creator: A Framework for Automated Diagram Generation](https://arxiv.org/abs/2601.05162)
*Jinze Yu,Dayuan Jiang*

Main category: cs.GR

TL;DR: GenAI-DrawIO-Creator uses Claude 3.7 LLM to automate diagram generation and manipulation in draw.io XML format from natural language, code, or images.


<details>
  <summary>Details</summary>
Motivation: Creating and modifying diagrams is labor-intensive, requiring automation to reduce manual effort and improve efficiency in visual communication.

Method: Framework integrates Claude 3.7 for structured visual reasoning, uses specialized prompt engineering and error-checking to produce valid XML outputs, with real-time diagram updates.

Result: Working prototype generates accurate diagrams (network architectures, flowcharts) from natural language, code, or images, significantly reducing creation time with high structural fidelity.

Conclusion: Demonstrates Claude 3.7's capability in structured visual reasoning and lays groundwork for future AI-assisted diagramming applications.

Abstract: Diagrams are crucial for communicating complex information, yet creating and modifying them remains a labor-intensive task. We present GenAI-DrawIO-Creator, a novel framework that leverages Large Language Models (LLMs) to automate diagram generation and manipulation in the structured XML format used by draw.io. Our system integrates Claude 3.7 to reason about structured visual data and produce valid diagram representations. Key contributions include a high-level system design enabling real-time diagram updates, specialized prompt engineering and error-checking to ensure well-formed XML outputs. We demonstrate a working prototype capable of generating accurate diagrams (such as network architectures and flowcharts) from natural language or code, and even replicating diagrams from images. Simulated evaluations show that our approach significantly reduces diagram creation time and produces outputs with high structural fidelity. Our results highlight the promise of Claude 3.7 in handling structured visual reasoning tasks and lay the groundwork for future research in AI-assisted diagramming applications.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [215] [Decentralized Privacy-Preserving Federal Learning of Computer Vision Models on Edge Devices](https://arxiv.org/abs/2601.04912)
*Damian Harenčák,Lukáš Gajdošech,Martin Madaras*

Main category: cs.CR

TL;DR: This paper analyzes privacy risks in federated learning and evaluates various methods to protect client data from both the server and other clients, including encryption, compression, and noise techniques.


<details>
  <summary>Details</summary>
Motivation: Federated learning aims to train models without sharing raw data, but recent findings show private data can still be reconstructed from model parameters. Current privacy protection methods mainly focus on server-side risks, ignoring potential malicious clients.

Method: The authors analyzed multiple privacy-enhancing techniques: homomorphic encryption, gradient compression, gradient noising, and modified federated learning systems (split learning, swarm learning, fully encrypted models). They evaluated these methods on neural networks and implemented a proof-of-concept on NVIDIA Jetson TX2 edge devices.

Result: The study found negative effects of gradient compression and gradient noising on CNN classification accuracy, demonstrated difficulty of data reconstruction in segmentation networks, and successfully implemented a federated learning simulation on edge hardware.

Conclusion: Comprehensive privacy protection in federated learning requires addressing risks from both servers and other clients. While various techniques exist, they often come with trade-offs in model accuracy, highlighting the need for balanced privacy-preserving solutions.

Abstract: Collaborative training of a machine learning model comes with a risk of sharing sensitive or private data. Federated learning offers a way of collectively training a single global model without the need to share client data, by sharing only the updated parameters from each client's local model. A central server is then used to aggregate parameters from all clients and redistribute the aggregated model back to the clients. Recent findings have shown that even in this scenario, private data can be reconstructed only using information about model parameters. Current efforts to mitigate this are mainly focused on reducing privacy risks on the server side, assuming that other clients will not act maliciously. In this work, we analyzed various methods for improving the privacy of client data concerning both the server and other clients for neural networks. Some of these methods include homomorphic encryption, gradient compression, gradient noising, and discussion on possible usage of modified federated learning systems such as split learning, swarm learning or fully encrypted models. We have analyzed the negative effects of gradient compression and gradient noising on the accuracy of convolutional neural networks used for classification. We have shown the difficulty of data reconstruction in the case of segmentation networks. We have also implemented a proof of concept on the NVIDIA Jetson TX2 module used in edge devices and simulated a federated learning process.

</details>


### [216] [Shadow Unlearning: A Neuro-Semantic Approach to Fidelity-Preserving Faceless Forgetting in LLMs](https://arxiv.org/abs/2601.04275)
*Dinesh Srivasthav P,Ashok Urlana,Rahul Mishra,Bala Mallikarjunarao Garlapati,Ponnurangam Kumaraguru*

Main category: cs.CR

TL;DR: Shadow Unlearning enables machine unlearning on anonymized data without exposing PII, using Neuro-Semantic Projector Unlearning (NSPU) framework for privacy-preserving unlearning with 10x computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing machine unlearning methods require access to original training data, exposing it to privacy risks like membership inference attacks and PII misuse, violating GDPR's 'Right to be Forgotten' principles.

Method: Proposes Shadow Unlearning paradigm and Neuro-Semantic Projector Unlearning (NSPU) framework that performs unlearning on anonymized forget data without exposing PII, using Multi-domain Fictitious Unlearning (MuFU) dataset for evaluation.

Result: NSPU achieves superior unlearning performance, preserves model utility, enhances user privacy, and is at least 10 times more computationally efficient than standard unlearning approaches across various LLMs.

Conclusion: Shadow Unlearning represents a new direction for privacy-aware machine unlearning that effectively balances data protection and model fidelity while addressing critical privacy challenges.

Abstract: Machine unlearning aims to selectively remove the influence of specific training samples to satisfy privacy regulations such as the GDPR's 'Right to be Forgotten'. However, many existing methods require access to the data being removed, exposing it to membership inference attacks and potential misuse of Personally Identifiable Information (PII). We address this critical challenge by proposing Shadow Unlearning, a novel paradigm of approximate unlearning, that performs machine unlearning on anonymized forget data without exposing PII. We further propose a novel privacy-preserving framework, Neuro-Semantic Projector Unlearning (NSPU) to achieve Shadow unlearning. To evaluate our method, we compile Multi-domain Fictitious Unlearning (MuFU) forget set across five diverse domains and introduce an evaluation stack to quantify the trade-off between knowledge retention and unlearning effectiveness. Experimental results on various LLMs show that NSPU achieves superior unlearning performance, preserves model utility, and enhances user privacy. Additionally, the proposed approach is at least 10 times more computationally efficient than standard unlearning approaches. Our findings foster a new direction for privacy-aware machine unlearning that balances data protection and model fidelity.

</details>


### [217] [DP-MGTD: Privacy-Preserving Machine-Generated Text Detection via Adaptive Differentially Private Entity Sanitization](https://arxiv.org/abs/2601.04641)
*Lionel Z. Wang,Yusheng Zhao,Jiabin Luo,Xinfeng Li,Lixu Wang,Yinan Peng,Haoyang Li,XiaoFeng Wang,Wei Dong*

Main category: cs.CR

TL;DR: DP-MGTD framework uses adaptive differentially private entity sanitization to detect machine-generated text while preserving privacy, achieving near-perfect accuracy by exploiting how DP noise amplifies differences between human and machine text.


<details>
  <summary>Details</summary>
Motivation: There's a fundamental conflict between machine-generated text detection (which needs user data) and privacy preservation. Standard anonymization hurts linguistic fluency, while differential privacy typically degrades detection signals needed for accurate identification.

Method: DP-MGTD framework with Adaptive Differentially Private Entity Sanitization algorithm. Two-stage mechanism: 1) noisy frequency estimation, 2) dynamic privacy budget calibration. Uses Laplace mechanism for numerical entities and Exponential mechanism for textual entities.

Result: Achieves near-perfect detection accuracy on MGTBench-2.0 dataset, significantly outperforming non-private baselines while satisfying strict privacy guarantees. Counter-intuitively, DP noise amplifies distinguishability between human and machine text by exposing distinct sensitivity patterns to perturbation.

Conclusion: The framework successfully resolves the privacy-detection dilemma by showing that differential privacy can actually enhance detection capabilities rather than degrade them, through careful adaptive sanitization that exploits sensitivity differences between human and machine text.

Abstract: The deployment of Machine-Generated Text (MGT) detection systems necessitates processing sensitive user data, creating a fundamental conflict between authorship verification and privacy preservation. Standard anonymization techniques often disrupt linguistic fluency, while rigorous Differential Privacy (DP) mechanisms typically degrade the statistical signals required for accurate detection. To resolve this dilemma, we propose \textbf{DP-MGTD}, a framework incorporating an Adaptive Differentially Private Entity Sanitization algorithm. Our approach utilizes a two-stage mechanism that performs noisy frequency estimation and dynamically calibrates privacy budgets, applying Laplace and Exponential mechanisms to numerical and textual entities respectively. Crucially, we identify a counter-intuitive phenomenon where the application of DP noise amplifies the distinguishability between human and machine text by exposing distinct sensitivity patterns to perturbation. Extensive experiments on the MGTBench-2.0 dataset show that our method achieves near-perfect detection accuracy, significantly outperforming non-private baselines while satisfying strict privacy guarantees.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [218] [Quantitative mapping from conventional MRI using self-supervised physics-guided deep learning: applications to a large-scale, clinically heterogeneous dataset](https://arxiv.org/abs/2601.05063)
*Jelmer van Lune,Stefano Mandija,Oscar van der Heide,Matteo Maspero,Martin B. Schilder,Jan Willem Dankbaar,Cornelis A. T. van den Berg,Alessandro Sbrizzi*

Main category: physics.med-ph

TL;DR: Self-supervised physics-guided deep learning framework converts conventional clinical MRI scans (T1w, T2w, FLAIR) into quantitative T1, T2, and PD maps without specialized qMRI protocols.


<details>
  <summary>Details</summary>
Motivation: Conventional MRI provides qualitative data dependent on scanner hardware and settings, while quantitative MRI (qMRI) requires specialized protocols that limit availability and large-scale biomarker research.

Method: Self-supervised physics-guided deep learning framework that integrates Bloch-based signal models into training, trained on 4,121 clinical scan sessions from four different 3T MRI scanners over six years.

Result: Generated quantitative maps show tissue values consistent with literature, scanner/protocol invariance (CV ≤1.1%), excellent reproducibility (Pearson r >0.82), and low voxel-wise differences (T2 <6%).

Conclusion: The framework robustly transforms diverse clinical MRI into quantitative maps, enabling large-scale quantitative biomarker research without specialized qMRI protocols.

Abstract: Magnetic resonance imaging (MRI) is a cornerstone of clinical neuroimaging, yet conventional MRIs provide qualitative information heavily dependent on scanner hardware and acquisition settings. While quantitative MRI (qMRI) offers intrinsic tissue parameters, the requirement for specialized acquisition protocols and reconstruction algorithms restricts its availability and impedes large-scale biomarker research. This study presents a self-supervised physics-guided deep learning framework to infer quantitative T1, T2, and proton-density (PD) maps directly from widely available clinical conventional T1-weighted, T2-weighted, and FLAIR MRIs. The framework was trained and evaluated on a large-scale, clinically heterogeneous dataset comprising 4,121 scan sessions acquired at our institution over six years on four different 3 T MRI scanner systems, capturing real-world clinical variability. The framework integrates Bloch-based signal models directly into the training objective. Across more than 600 test sessions, the generated maps exhibited white matter and gray matter values consistent with literature ranges. Additionally, the generated maps showed invariance to scanner hardware and acquisition protocol groups, with inter-group coefficients of variation $\leq$ 1.1%. Subject-specific analyses demonstrated excellent voxel-wise reproducibility across scanner systems and sequence parameters, with Pearson $r$ and concordance correlation coefficients exceeding 0.82 for T1 and T2. Mean relative voxel-wise differences were low across all quantitative parameters, especially for T2 ($<$ 6%). These results indicate that the proposed framework can robustly transform diverse clinical conventional MRI data into quantitative maps, potentially paving the way for large-scale quantitative biomarker research.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [219] [End-to-end differentiable design of geometric waveguide displays](https://arxiv.org/abs/2601.04370)
*Xinge Yang,Zhaocheng Liu,Zhaoyu Nie,Qingyuan Fan,Zhimin Shi,Jim Bonar,Wolfgang Heidrich*

Main category: physics.optics

TL;DR: First end-to-end differentiable optimization framework for geometric waveguides that couples non-sequential Monte Carlo polarization ray tracing with differentiable thin-film solver, enabling optimization of 1000+ layer parameters and billions of ray intersections on multi-GPU systems.


<details>
  <summary>Details</summary>
Motivation: Geometric waveguides are promising for AR displays but performance is bottlenecked by the difficulty of jointly optimizing non-sequential light transport and polarization-dependent multilayer thin-film coatings.

Method: Differentiable optimization framework coupling non-sequential Monte Carlo polarization ray tracing with differentiable transfer-matrix thin-film solver. Uses memory-saving strategies to optimize 1000+ layer parameters and billions of ray-surface intersections on multi-GPU. Automated layer pruning via over-parameterized stacks with discrete manufacturability constraints (topology optimization). Also jointly optimizes waveguide and image preprocessing network.

Result: Increases light efficiency from 4.1% to 33.5%, improves eyebox uniformity by ~17× and FoV uniformity by ~11×. Enables system-level, high-dimensional coating optimization inside waveguides.

Conclusion: Framework enables system-level, high-dimensional coating optimization inside waveguides and expands scope of differentiable optics for next-generation optical design.

Abstract: Geometric waveguides are a promising architecture for optical see-through augmented reality displays, but their performance is severely bottlenecked by the difficulty of jointly optimizing non-sequential light transport and polarization-dependent multilayer thin-film coatings. Here we present the first end-to-end differentiable optimization framework for geometric waveguide that couples non-sequential Monte Carlo polarization ray tracing with a differentiable transfer-matrix thin-film solver. A differentiable Monte Carlo ray tracer avoids the exponential growth of deterministic ray splitting while enabling gradients backpropagation from eyebox metrics to design parameters. With memory-saving strategies, we optimize more than one thousand layer-thickness parameters and billions of non-sequential ray-surface intersections on a single multi-GPU workstation. Automated layer pruning is achieved by starting from over-parameterized stacks and driving redundant layers to zero thickness under discrete manufacturability constraints, effectively performing topology optimization to discover optimal coating structures. On a representative design, starting from random initialization within thickness bounds, our method increases light efficiency from 4.1\% to 33.5\% and improves eyebox and FoV uniformity by $\sim$17$\times$ and $\sim$11$\times$, respectively. Furthermore, we jointly optimize the waveguide and an image preprocessing network to improve perceived image quality. Our framework not only enables system-level, high-dimensional coating optimization inside the waveguide, but also expands the scope of differentiable optics for next-generation optical design.

</details>


### [220] [Illumination Angular Spectrum Encoding for Controlling the Functionality of Diffractive Networks](https://arxiv.org/abs/2601.04825)
*Matan Kleiner,Lior Michaeli,Tomer Michaeli*

Main category: physics.optics

TL;DR: A single diffractive neural network can perform multiple image-to-image translation tasks by controlling the illumination's angular spectrum with amplitude masks, enabling multi-task all-optical computing without mechanical reconfiguration.


<details>
  <summary>Details</summary>
Motivation: Current diffractive neural networks are typically single-task systems, limiting their adoption in applications requiring multiple functionalities. Existing multi-task approaches require mechanical reconfiguration or different wavelengths/polarizations per task, which are not scalable or versatile.

Method: Proposes using the illumination's angular spectrum as a control mechanism. Different amplitude masks shape the illumination's angular spectrum, with each mask encoding a specific task. A single diffractive network is trained to perform multiple image-to-image translations, where the output type is determined by the angular components of the illumination.

Result: Demonstrated successful multi-task functionality: translating handwritten digits to typeset digits of different values, and handwritten English letters to typeset numbers and Greek letters. The approach works effectively within a narrow angular range in the paraxial regime, under different coherence conditions, and can be combined with existing control strategies like wavelength control.

Conclusion: The illumination angular spectrum serves as a powerful degree of freedom for controlling diffractive networks, enabling a scalable and versatile framework for multi-task all-optical computing without mechanical reconfiguration.

Abstract: Diffractive neural networks have recently emerged as a promising framework for all-optical computing. However, these networks are typically trained for a single task, limiting their potential adoption in systems requiring multiple functionalities. Existing approaches to achieving multi-task functionality either modify the mechanical configuration of the network per task or use a different illumination wavelength or polarization state for each task. In this work, we propose a new control mechanism, which is based on the illumination's angular spectrum. Specifically, we shape the illumination using an amplitude mask that selectively controls its angular spectrum. We employ different illumination masks for achieving different network functionalities, so that the mask serves as a unique task encoder. Interestingly, we show that effective control can be achieved over a very narrow angular range, within the paraxial regime. We numerically illustrate the proposed approach by training a single diffractive network to perform multiple image-to-image translation tasks. In particular, we demonstrate translating handwritten digits into typeset digits of different values, and translating handwritten English letters into typeset numbers and typeset Greek letters, where the type of the output is determined by the illumination's angular components. As we show, the proposed framework can work under different coherence conditions, and can be combined with existing control strategies, such as different wavelengths. Our results establish the illumination angular spectrum as a powerful degree of freedom for controlling diffractive networks, enabling a scalable and versatile framework for multi-task all-optical computing.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [221] [UNIC: Learning Unified Multimodal Extrinsic Contact Estimation](https://arxiv.org/abs/2601.04356)
*Zhengtong Xu,Yuki Shirai*

Main category: cs.RO

TL;DR: UNIC is a unified multimodal framework for extrinsic contact estimation that works without prior knowledge or camera calibration, using visual, proprioceptive, and tactile data to estimate object-environment interactions.


<details>
  <summary>Details</summary>
Motivation: Existing contact estimation methods rely on restrictive assumptions (predefined contact types, fixed grasps, camera calibration) that limit generalization to novel objects and unstructured environments. There's a need for more flexible, data-driven approaches.

Method: UNIC encodes visual observations in camera frame and integrates them with proprioceptive/tactile data. Uses unified contact representation based on scene affordance maps, multimodal fusion with random masking for robust representation learning.

Result: Achieves 9.6mm average Chamfer distance error on unseen contact locations, performs well on unseen objects, robust under missing modalities, and adapts to dynamic camera viewpoints.

Conclusion: Establishes extrinsic contact estimation as practical and versatile capability for contact-rich manipulation, enabling better planning, control, and policy learning in unstructured environments.

Abstract: Contact-rich manipulation requires reliable estimation of extrinsic contacts-the interactions between a grasped object and its environment which provide essential contextual information for planning, control, and policy learning. However, existing approaches often rely on restrictive assumptions, such as predefined contact types, fixed grasp configurations, or camera calibration, that hinder generalization to novel objects and deployment in unstructured environments. In this paper, we present UNIC, a unified multimodal framework for extrinsic contact estimation that operates without any prior knowledge or camera calibration. UNIC directly encodes visual observations in the camera frame and integrates them with proprioceptive and tactile modalities in a fully data-driven manner. It introduces a unified contact representation based on scene affordance maps that captures diverse contact formations and employs a multimodal fusion mechanism with random masking, enabling robust multimodal representation learning. Extensive experiments demonstrate that UNIC performs reliably. It achieves a 9.6 mm average Chamfer distance error on unseen contact locations, performs well on unseen objects, remains robust under missing modalities, and adapts to dynamic camera viewpoints. These results establish extrinsic contact estimation as a practical and versatile capability for contact-rich manipulation.

</details>


### [222] [Generate, Transfer, Adapt: Learning Functional Dexterous Grasping from a Single Human Demonstration](https://arxiv.org/abs/2601.05243)
*Xingyi He,Adhitya Polavaram,Yunhao Cao,Om Deshmukh,Tianrui Wang,Xiaowei Zhou,Kuan Fang*

Main category: cs.RO

TL;DR: CorDex learns dexterous functional grasps from single human demonstrations using synthetic data generation and multimodal prediction.


<details>
  <summary>Details</summary>
Motivation: Dexterous robotic grasping faces two bottlenecks: scarcity of large-scale datasets and lack of integrated semantic/geometric reasoning in models.

Method: CorDex uses correspondence-based data engine to generate diverse synthetic training data from single demo, plus multimodal prediction network with local-global fusion and importance-aware sampling.

Result: CorDex generalizes well to unseen objects and significantly outperforms state-of-the-art baselines across various object categories.

Conclusion: The framework enables robust dexterous functional grasping from minimal human demonstration through synthetic data generation and multimodal reasoning.

Abstract: Functional grasping with dexterous robotic hands is a key capability for enabling tool use and complex manipulation, yet progress has been constrained by two persistent bottlenecks: the scarcity of large-scale datasets and the absence of integrated semantic and geometric reasoning in learned models. In this work, we present CorDex, a framework that robustly learns dexterous functional grasps of novel objects from synthetic data generated from just a single human demonstration. At the core of our approach is a correspondence-based data engine that generates diverse, high-quality training data in simulation. Based on the human demonstration, our data engine generates diverse object instances of the same category, transfers the expert grasp to the generated objects through correspondence estimation, and adapts the grasp through optimization. Building on the generated data, we introduce a multimodal prediction network that integrates visual and geometric information. By devising a local-global fusion module and an importance-aware sampling mechanism, we enable robust and computationally efficient prediction of functional dexterous grasps. Through extensive experiments across various object categories, we demonstrate that CorDex generalizes well to unseen object instances and significantly outperforms state-of-the-art baselines.

</details>

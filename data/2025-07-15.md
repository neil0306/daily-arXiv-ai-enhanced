<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 63]
- [cs.CV](#cs.CV) [Total: 160]
- [cs.AI](#cs.AI) [Total: 6]
- [cs.IR](#cs.IR) [Total: 4]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.DB](#cs.DB) [Total: 1]
- [eess.IV](#eess.IV) [Total: 15]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.LG](#cs.LG) [Total: 12]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.GR](#cs.GR) [Total: 3]
- [cs.IT](#cs.IT) [Total: 1]
- [eess.AS](#eess.AS) [Total: 3]
- [cs.SE](#cs.SE) [Total: 2]
- [cs.RO](#cs.RO) [Total: 5]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [cs.SD](#cs.SD) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Spatial ModernBERT: Spatial-Aware Transformer for Table and Key-Value Extraction in Financial Documents at Scale](https://arxiv.org/abs/2507.08865)
*Javis AI Team,Amrendra Singh,Maulik Shah,Dharshan Sampath*

Main category: cs.CL

TL;DR: Spatial ModernBERT, a transformer model with spatial embeddings, is introduced for extracting tables and key-value pairs from financial documents, achieving high accuracy through token classification and post-processing.


<details>
  <summary>Details</summary>
Motivation: Extracting structured data from financial documents is crucial for business workflows like auditing and automated invoice processing.

Method: The model uses token classification across three heads (Label, Column, Row) and is pretrained on PubTables-1M, then fine-tuned on financial documents. Post-processing merges tokens and reconstructs layouts.

Result: Empirical evaluation shows the model effectively leverages textual and spatial cues for accurate extraction.

Conclusion: Spatial ModernBERT is a robust solution for table and key-value extraction in financial documents.

Abstract: Extracting tables and key-value pairs from financial documents is essential
for business workflows such as auditing, data analytics, and automated invoice
processing. In this work, we introduce Spatial ModernBERT-a transformer-based
model augmented with spatial embeddings-to accurately detect and extract
tabular data and key-value fields from complex financial documents. We cast the
extraction task as token classification across three heads: (1) Label Head,
classifying each token as a label (e.g., PO Number, PO Date, Item Description,
Quantity, Base Cost, MRP, etc.); (2) Column Head, predicting column indices;
(3) Row Head, distinguishing the start of item rows and header rows. The model
is pretrained on the PubTables-1M dataset, then fine-tuned on a financial
document dataset, achieving robust performance through cross-entropy loss on
each classification head. We propose a post-processing method to merge tokens
using B-I-IB tagging, reconstruct the tabular layout, and extract key-value
pairs. Empirical evaluation shows that Spatial ModernBERT effectively leverages
both textual and spatial cues, facilitating highly accurate table and key-value
extraction in real-world financial documents.

</details>


### [2] [SEALGuard: Safeguarding the Multilingual Conversations in Southeast Asian Languages for LLM Software Systems](https://arxiv.org/abs/2507.08898)
*Wenliang Shan,Michael Fu,Rui Yang,Chakkrit,Tantithamthavorn*

Main category: cs.CL

TL;DR: SEALGuard is a multilingual guardrail for LLM systems, improving safety alignment across diverse languages by outperforming existing solutions like LlamaGuard.


<details>
  <summary>Details</summary>
Motivation: Existing guardrails like LlamaGuard struggle with multilingual unsafe inputs, leaving LLM systems vulnerable in low-resource languages.

Method: Adapts a multilingual language model using LoRA and evaluates on SEALSBench, a dataset of 260,000 prompts in ten languages.

Result: SEALGuard improves DSR by 48% over LlamaGuard, achieving top performance in detecting multilingual unsafe and jailbreak prompts.

Conclusion: SEALGuard effectively addresses multilingual safety alignment gaps, enhancing LLM system security.

Abstract: Safety alignment is critical for LLM-powered systems. While recent
LLM-powered guardrail approaches such as LlamaGuard achieve high detection
accuracy of unsafe inputs written in English (e.g., ``How to create a bomb?''),
they struggle with multilingual unsafe inputs. This limitation leaves LLM
systems vulnerable to unsafe and jailbreak prompts written in low-resource
languages such as those in Southeast Asia. This paper introduces SEALGuard, a
multilingual guardrail designed to improve the safety alignment across diverse
languages. It aims to address the multilingual safety alignment gap of existing
guardrails and ensure effective filtering of unsafe and jailbreak prompts in
LLM-powered systems. We adapt a general-purpose multilingual language model
into a multilingual guardrail using low-rank adaptation (LoRA). We construct
SEALSBench, a large-scale multilingual safety alignment dataset containing over
260,000 prompts in ten languages, including safe, unsafe, and jailbreak cases.
We evaluate SEALGuard against state-of-the-art guardrails such as LlamaGuard on
this benchmark. Our findings show that multilingual unsafe and jailbreak
prompts substantially degrade the performance of the state-of-the-art
LlamaGuard, which experiences a drop in Defense Success Rate (DSR) by 9% and
18%, respectively, compared to its performance on English-only prompts. In
contrast, SEALGuard outperforms existing guardrails in detecting multilingual
unsafe and jailbreak prompts, improving DSR by 48% over LlamaGuard and
achieving the best DSR, precision, and F1-score. Our ablation study further
reveals the contributions of adaptation strategies and model size to the
overall performance of SEALGuard. SEALGuard advances the safety alignment of
LLM systems by introducing an effective multilingual guardrail.

</details>


### [3] [Evaluating LLMs in Medicine: A Call for Rigor, Transparency](https://arxiv.org/abs/2507.08916)
*Mahmoud Alwakeel,Aditya Nagori,Vijay Krishnamoorthy,Rishikesan Kamaleswaran*

Main category: cs.CL

TL;DR: The paper evaluates limitations of LLMs in medical QA, focusing on dataset quality. It reviews benchmark datasets and suggests alternatives, finding gaps in clinical realism and validation. It calls for standardized frameworks and collaboration.


<details>
  <summary>Details</summary>
Motivation: To address the lack of rigorous, unbiased, and clinically relevant datasets for evaluating LLMs in medical question answering.

Method: Reviewed benchmark datasets (MedQA, MedMCQA, PubMedQA, MMLU) and alternatives like medical journal challenge questions for rigor, transparency, and clinical relevance.

Result: Existing datasets lack clinical realism and validation; alternatives are limited in size, scope, and exposure to LLM training.

Conclusion: A standardized framework and collaborative efforts are needed to ensure rigorous, unbiased datasets for evaluating LLMs in medicine.

Abstract: Objectives: To evaluate the current limitations of large language models
(LLMs) in medical question answering, focusing on the quality of datasets used
for their evaluation. Materials and Methods: Widely-used benchmark datasets,
including MedQA, MedMCQA, PubMedQA, and MMLU, were reviewed for their rigor,
transparency, and relevance to clinical scenarios. Alternatives, such as
challenge questions in medical journals, were also analyzed to identify their
potential as unbiased evaluation tools. Results: Most existing datasets lack
clinical realism, transparency, and robust validation processes. Publicly
available challenge questions offer some benefits but are limited by their
small size, narrow scope, and exposure to LLM training. These gaps highlight
the need for secure, comprehensive, and representative datasets. Conclusion: A
standardized framework is critical for evaluating LLMs in medicine.
Collaborative efforts among institutions and policymakers are needed to ensure
datasets and methodologies are rigorous, unbiased, and reflective of clinical
complexities.

</details>


### [4] [From KMMLU-Redux to KMMLU-Pro: A Professional Korean Benchmark Suite for LLM Evaluation](https://arxiv.org/abs/2507.08924)
*Seokhee Hong,Sunkyoung Kim,Guijin Son,Soyeon Kim,Yeonjung Hong,Jinsik Lee*

Main category: cs.CL

TL;DR: The paper introduces two Korean expert-level benchmarks, KMMLU-Redux and KMMLU-Pro, to evaluate LLMs' applicability in real-world industrial scenarios.


<details>
  <summary>Details</summary>
Motivation: To address the lack of robust benchmarks for LLMs that cover industrial knowledge, particularly in Korea.

Method: Developed KMMLU-Redux (revised from KMMLU) and KMMLU-Pro, based on Korean National Technical Qualification and Professional Licensure exams, respectively.

Result: The benchmarks effectively represent industrial knowledge in Korea, as demonstrated by experiments.

Conclusion: The publicly released datasets provide reliable tools for evaluating LLMs in Korean industrial contexts.

Abstract: The development of Large Language Models (LLMs) requires robust benchmarks
that encompass not only academic domains but also industrial fields to
effectively evaluate their applicability in real-world scenarios. In this
paper, we introduce two Korean expert-level benchmarks. KMMLU-Redux,
reconstructed from the existing KMMLU, consists of questions from the Korean
National Technical Qualification exams, with critical errors removed to enhance
reliability. KMMLU-Pro is based on Korean National Professional Licensure exams
to reflect professional knowledge in Korea. Our experiments demonstrate that
these benchmarks comprehensively represent industrial knowledge in Korea. We
release our dataset publicly available.

</details>


### [5] [Self-Improving Model Steering](https://arxiv.org/abs/2507.08967)
*Rongyi Zhu,Yuhui Wang,Tanqiu Jiang,Jiacheng Liang,Ting Wang*

Main category: cs.CL

TL;DR: SIMS is a self-improving model-steering framework for LLMs that avoids external supervision, using iterative self-improvement and novel strategies like prompt ranking to enhance alignment with human preferences.


<details>
  <summary>Details</summary>
Motivation: Conventional model-steering methods depend on external annotations, limiting adaptability and effectiveness. SIMS aims to overcome these limitations by operating without external supervision.

Method: SIMS autonomously generates and refines contrastive samples through iterative cycles and employs strategies like prompt ranking and contrast sampling.

Result: SIMS outperforms existing methods in steering effectiveness and adaptability across diverse LLMs and benchmarks.

Conclusion: Self-improving model steering, as demonstrated by SIMS, is a promising direction for future research on LLM alignment during inference.

Abstract: Model steering represents a powerful technique that dynamically aligns large
language models (LLMs) with human preferences during inference. However,
conventional model-steering methods rely heavily on externally annotated data,
not only limiting their adaptability to varying contexts but also tethering
their effectiveness to annotation quality. In this paper, we present SIMS, the
first self-improving model-steering framework that operates without relying on
external supervision. At its core, SIMS autonomously generates and refines
contrastive samples through iterative self-improvement cycles, enabling
adaptive, context-specific steering. Additionally, SIMS employs novel
strategies, including prompt ranking and contrast sampling, to further enhance
steering efficacy. Extensive evaluation across diverse LLMs and benchmarks
demonstrates that SIMS substantially outperforms existing methods in steering
effectiveness and adaptability, highlighting self-improving model steering as a
promising direction for future research on inference-time LLM alignment.

</details>


### [6] [Application of CARE-SD text classifier tools to assess distribution of stigmatizing and doubt-marking language features in EHR](https://arxiv.org/abs/2507.08969)
*Drew Walker,Jennifer Love,Swati Rajwal,Isabel C Walker,Hannah LF Cooper,Abeed Sarker,Melvin Livingston III*

Main category: cs.CL

TL;DR: The paper examines stigmatizing language in EHRs, finding higher rates among marginalized groups and certain healthcare providers.


<details>
  <summary>Details</summary>
Motivation: To identify and quantify stigmatizing language in EHRs and its association with patient demographics and provider types.

Method: Used lexicon matching and supervised learning to analyze linguistic features in MIMIC-III EHR, with Poisson regression for rate predictors.

Result: Higher stigmatizing labels for Black/African American patients, those with certain insurances or conditions, and more frequent use by nurses and social workers.

Conclusion: Stigmatizing language in EHRs disproportionately affects marginalized groups and varies by provider type, highlighting a need for intervention.

Abstract: Introduction: Electronic health records (EHR) are a critical medium through
which patient stigmatization is perpetuated among healthcare teams. Methods: We
identified linguistic features of doubt markers and stigmatizing labels in
MIMIC-III EHR via expanded lexicon matching and supervised learning
classifiers. Predictors of rates of linguistic features were assessed using
Poisson regression models. Results: We found higher rates of stigmatizing
labels per chart among patients who were Black or African American (RR: 1.16),
patients with Medicare/Medicaid or government-run insurance (RR: 2.46),
self-pay (RR: 2.12), and patients with a variety of stigmatizing disease and
mental health conditions. Patterns among doubt markers were similar, though
male patients had higher rates of doubt markers (RR: 1.25). We found increased
stigmatizing labels used by nurses (RR: 1.40), and social workers (RR: 2.25),
with similar patterns of doubt markers. Discussion: Stigmatizing language
occurred at higher rates among historically stigmatized patients, perpetuated
by multiple provider types.

</details>


### [7] [Beyond vividness: Content analysis of induced hallucinations reveals the hidden structure of individual differences in visual imagery](https://arxiv.org/abs/2507.09011)
*Ana Chkhaidze,Reshanne R. Reeder,Connor Gag,Anastasia Kiyonaga,Seana Coulson*

Main category: cs.CL

TL;DR: Ganzflicker-induced hallucinations vary by imagery phenotype: strong imagers see complex content, weak imagers see simple patterns. Vision-language models best capture these differences.


<details>
  <summary>Details</summary>
Motivation: To explore how differences in visual imagery (absent, typical, vivid) affect the complexity of hallucinations induced by Ganzflicker.

Method: Analyzed free-text descriptions of hallucinations from 4,000+ participants using NLP tools, comparing vision-language and text-only models.

Result: Strong imagers described complex, naturalistic hallucinations; weak imagers reported simple geometric patterns. Vision-language models outperformed text-only models in capturing differences.

Conclusion: Individual variation in visual imagery may reflect coordination between early visual and higher-order brain regions.

Abstract: A rapidly alternating red and black display known as Ganzflicker induces
visual hallucinations that reflect the generative capacity of the visual
system. Recent proposals regarding the imagery spectrum, that is, differences
in the visual system of individuals with absent imagery, typical imagery, and
vivid imagery, suggest these differences should impact the complexity of other
internally generated visual experiences. Here, we used tools from natural
language processing to analyze free-text descriptions of hallucinations from
over 4,000 participants, asking whether people with different imagery
phenotypes see different things in their mind's eye during Ganzflicker-induced
hallucinations. Strong imagers described complex, naturalistic content, while
weak imagers reported simple geometric patterns. Embeddings from vision
language models better captured these differences than text-only language
models, and participants with stronger imagery used language with richer
sensorimotor associations. These findings may reflect individual variation in
coordination between early visual areas and higher-order regions relevant for
the imagery spectrum.

</details>


### [8] [Lizard: An Efficient Linearization Framework for Large Language Models](https://arxiv.org/abs/2507.09025)
*Chien Van Nguyen,Ruiyi Zhang,Hanieh Deilamsalehy,Puneet Mathur,Viet Dac Lai,Haoliang Wang,Jayakumar Subramanian,Ryan A. Rossi,Trung Bui,Nikos Vlassis,Franck Dernoncourt,Thien Huu Nguyen*

Main category: cs.CL

TL;DR: Lizard is a linearization framework for Transformer-based LLMs, introducing subquadratic attention and gating for efficient infinite-context generation with near-lossless performance.


<details>
  <summary>Details</summary>
Motivation: Transformer-based LLMs face memory and computational bottlenecks with increasing context lengths due to quadratic complexity in attention and KV cache growth.

Method: Lizard combines gated linear attention for global context compression and sliding window attention with meta memory, plus a hardware-aware training acceleration algorithm.

Result: Lizard achieves near-lossless performance recovery, outperforms prior linearization methods by 18 points on MMLU, and excels in associative recall tasks.

Conclusion: Lizard offers a flexible, efficient solution for infinite-context generation, balancing performance and computational efficiency.

Abstract: We propose Lizard, a linearization framework that transforms pretrained
Transformer-based Large Language Models (LLMs) into flexible, subquadratic
architectures for infinite-context generation. Transformer-based LLMs face
significant memory and computational bottlenecks as context lengths increase,
due to the quadratic complexity of softmax attention and the growing key-value
(KV) cache. Lizard addresses these limitations by introducing a subquadratic
attention mechanism that closely approximates softmax attention while
preserving the output quality. Unlike previous linearization methods, which are
often limited by fixed model structures and therefore exclude gating
mechanisms, Lizard incorporates a gating module inspired by recent
state-of-the-art linear models. This enables adaptive memory control, supports
constant-memory inference, offers strong length generalization, and allows more
flexible model design. Lizard combines gated linear attention for global
context compression with sliding window attention enhanced by meta memory,
forming a hybrid mechanism that captures both long-range dependencies and
fine-grained local interactions. Moreover, we introduce a hardware-aware
algorithm that accelerates the training speed of our models. Extensive
experiments show that Lizard achieves near-lossless recovery of the teacher
model's performance across standard language modeling tasks, while
significantly outperforming previous linearization methods. On the 5-shot MMLU
benchmark, Lizard improves over prior models by 18 points and shows significant
improvements on associative recall tasks.

</details>


### [9] [ALIGN: Prompt-based Attribute Alignment for Reliable, Responsible, and Personalized LLM-based Decision-Making](https://arxiv.org/abs/2507.09037)
*Bharadwaj Ravichandran,David Joy,Paul Elliott,Brian Hu,Jadie Adams,Christopher Funk,Emily Veenhuis,Anthony Hoogs,Arslan Basharat*

Main category: cs.CL

TL;DR: The paper introduces ALIGN, a system for dynamic personalization of LLM-based decision-makers through prompt-based alignment to fine-grained attributes, enabling qualitative and quantitative comparisons.


<details>
  <summary>Details</summary>
Motivation: Users have diverse values and preferences affecting decision-making, requiring novel LLM alignment and personalization methods beyond benchmarking tasks.

Method: ALIGN system features robust configuration management, structured output generation, swappable LLM backbones, and modular backend for algorithm integration.

Result: Quantitative analysis compares alignment approaches in public opinion surveys and medical triage, with the framework being open source.

Conclusion: ALIGN enables reliable, responsible, and personalized LLM-based decision-making, fostering new research in this area.

Abstract: Large language models (LLMs) are increasingly being used as decision aids.
However, users have diverse values and preferences that can affect their
decision-making, which requires novel methods for LLM alignment and
personalization. Existing LLM comparison tools largely focus on benchmarking
tasks, such as knowledge-based question answering. In contrast, our proposed
ALIGN system focuses on dynamic personalization of LLM-based decision-makers
through prompt-based alignment to a set of fine-grained attributes. Key
features of our system include robust configuration management, structured
output generation with reasoning, and several algorithm implementations with
swappable LLM backbones, enabling different types of analyses. Our user
interface enables a qualitative, side-by-side comparison of LLMs and their
alignment to various attributes, with a modular backend for easy algorithm
integration. Additionally, we perform a quantitative analysis comparing
alignment approaches in two different domains: demographic alignment for public
opinion surveys and value alignment for medical triage decision-making. The
entire ALIGN framework is open source and will enable new research on reliable,
responsible, and personalized LLM-based decision-makers.

</details>


### [10] [OpenCodeReasoning-II: A Simple Test Time Scaling Approach via Self-Critique](https://arxiv.org/abs/2507.09075)
*Wasi Uddin Ahmad,Somshubra Majumdar,Aleksander Ficek,Sean Narenthiran,Mehrzad Samadi,Jocelyn Huang,Siddhartha Jain,Vahid Noroozi,Boris Ginsburg*

Main category: cs.CL

TL;DR: OpenCodeReasoning-II introduces a 2.5M question-solution-critique dataset for code reasoning, employs a two-stage fine-tuning strategy for LLMs, and achieves competitive performance in code generation and critique.


<details>
  <summary>Details</summary>
Motivation: The need for large-scale, high-quality datasets to advance reasoning-based LLMs in code generation and critique.

Method: Two-stage supervised fine-tuning: first for code generation, then joint training for generation and critique.

Result: Qwen2.5-Instruct models outperform or match prior open-weight models in code generation, with improved competitive coding performance.

Conclusion: OpenCodeReasoning-II and the fine-tuning strategy enhance LLM capabilities in code reasoning, supported by an extended LiveCodeBench benchmark for C++.

Abstract: Recent advancements in reasoning-based Large Language Models (LLMs),
particularly their potential through test-time scaling, have created
significant opportunities for distillation in code generation and critique.
However, progress in both areas fundamentally depends on large-scale,
high-quality datasets. In this work, we introduce OpenCodeReasoning-II, a
dataset consists of 2.5M question-solution-critique triples (approx. 35K unique
programming questions), making it nearly twice the size of the previous largest
publicly available code reasoning dataset. In this work, we employ a two-stage
supervised fine-tuning strategy. The first stage focuses on fine-tuning for
code generation, while the second stage involves the joint training of models
for both code generation and critique. Our resulting finetuned Qwen2.5-Instruct
models achieve performance in code generation that either exceeds or equals the
best prior open-weight distilled models. Notably, the integration of our code
generation and critique models leads to significant improvements in competitive
coding performance. Furthermore, we present an extension of the LiveCodeBench
benchmark to specifically support the C++ programming language, thereby
facilitating more comprehensive LLM evaluation using this benchmark.

</details>


### [11] [Dynamic Parameter Memory: Temporary LoRA-Enhanced LLM for Long-Sequence Emotion Recognition in Conversation](https://arxiv.org/abs/2507.09076)
*Jialong Mai,Xiaofen Xing,Yawei Li,Zhipeng Li,Jingyuan Xing,Xiangmin Xu*

Main category: cs.CL

TL;DR: The paper introduces a Dynamic Parameter Memory (DPM) mechanism to address the limitations of SLLMs in processing long audio sequences for speech emotion recognition, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: The high frame rate in speech limits SLLMs' processing capacity, and existing token compression methods ignore emotional continuity.

Method: Proposes DPM with contextual semantics and sentence-level emotion encoding to handle unlimited-length audio within limited context windows.

Result: DPM significantly improves emotion recognition in long audio sequences, achieving top performance on the IEMOCAP dataset.

Conclusion: DPM enhances SLLMs' capability for emotion recognition in conversations by effectively memorizing contextual information.

Abstract: Recent research has focused on applying speech large language model (SLLM) to
improve speech emotion recognition (SER). However, the inherently high frame
rate in speech modality severely limits the signal processing and understanding
capabilities of SLLM. For example, a SLLM with a 4K context window can only
process 80 seconds of audio at 50Hz feature sampling rate before reaching its
capacity limit. Input token compression methods used in SLLM overlook the
continuity and inertia of emotions across multiple conversation turns. This
paper proposes a Dynamic Parameter Memory (DPM) mechanism with contextual
semantics and sentence-level emotion encoding, enabling processing of
unlimited-length audio with limited context windows in SLLM. Specifically, DPM
progressively encodes sentence-level information and emotions into a temporary
LoRA module during inference to effectively "memorize" the contextual
information. We trained an emotion SLLM as a backbone and incorporated our DPM
into inference for emotion recognition in conversation (ERC). Experimental
results on the IEMOCAP dataset show that DPM significantly improves the emotion
recognition capabilities of SLLM when processing long audio sequences,
achieving state-of-the-art performance.

</details>


### [12] [CompassJudger-2: Towards Generalist Judge Model via Verifiable Rewards](https://arxiv.org/abs/2507.09104)
*Taolin Zhang,Maosong Cao,Alexander Lam,Songyang Zhang,Kai Chen*

Main category: cs.CL

TL;DR: CompassJudger-2 is a generalist judge model addressing limitations of current LLM-as-judge models through task-driven, multi-domain data curation and verifiable rewards. It outperforms larger models and introduces JudgerBenchV2 for standardized evaluation.


<details>
  <summary>Details</summary>
Motivation: Current judge models are narrowly specialized and lack robustness, limiting comprehensive evaluations.

Method: Uses task-driven data curation, verifiable rewards, rejection sampling, and a refined learning objective with margin policy gradient loss.

Result: Achieves superior performance across benchmarks; 7B model competes with much larger models.

Conclusion: Advances robust, scalable LLM judgment and sets new performance and evaluation standards.

Abstract: Recently, the role of LLM-as-judge in evaluating large language models has
gained prominence. However, current judge models suffer from narrow
specialization and limited robustness, undermining their capacity for
comprehensive evaluations. In this work, we present CompassJudger-2, a novel
generalist judge model that overcomes these limitations via a task-driven,
multi-domain data curation strategy. Central to our approach is supervising
judgment tasks with verifiable rewards, guiding intrinsic critical reasoning
through rejection sampling to foster robust, generalizable judgment
capabilities. We introduce a refined learning objective with margin policy
gradient loss to enhance performance. Empirically, CompassJudger-2 achieves
superior results across multiple judge and reward benchmarks, and our 7B model
demonstrates competitive judgment accuracy with significantly larger models
like DeepSeek-V3 and Qwen3-235B-A22B. Additionally, we propose JudgerBenchV2, a
comprehensive benchmark evaluating cross-domain judgment accuracy and rank
consistency to standardize judge model evaluation. These contributions advance
robust, scalable LLM judgment and establish new performance and evaluation
standards.

</details>


### [13] [OPENXRD: A Comprehensive Benchmark and Enhancement Framework for LLM/MLLM XRD Question Answering](https://arxiv.org/abs/2507.09155)
*Ali Vosoughi,Ayoub Shahnazari,Yufeng Xi,Zeliang Zhang,Griffin Hess,Chenliang Xu,Niaz Abdolrahim*

Main category: cs.CL

TL;DR: OPENXRD is an open-book pipeline for crystallography QA, using GPT-4.5-generated references to aid smaller models, improving accuracy, especially for models with limited crystallography training.


<details>
  <summary>Details</summary>
Motivation: To address copyright issues with scanned textbooks and enhance smaller models' understanding of XRD by providing compact, domain-specific references.

Method: Integrates textual prompts with GPT-4.5-generated supporting content, evaluated on 217 expert-level XRD questions using various vision-language models under closed- and open-book conditions.

Result: Significant accuracy improvements in models using GPT-4.5 summaries, particularly those with limited crystallography training.

Conclusion: OPENXRD demonstrates the potential of AI-generated texts to enhance reasoning in scientific tasks and lays groundwork for future NLP tools in materials science.

Abstract: This work presents OPENXRD, an open-book pipeline designed for
crystallography question answering, which integrates textual prompts with
concise supporting content generated by GPT-4.5. Instead of using scanned
textbooks, which may lead to copyright issues, OPENXRD generates compact,
domain-specific references that help smaller models understand key concepts in
X-ray diffraction (XRD). We evaluate OPENXRD on a well-defined set of 217
expert-level XRD questions by comparing different vision-language models,
including GPT-4 and LLaVA-based frameworks such as Mistral, LLaMA, and QWEN,
under both closed-book (without supporting material) and open-book (with
supporting material) conditions. Our experimental results show significant
accuracy improvements in models that use the GPT-4.5-generated summaries,
particularly those with limited prior training in crystallography. OPENXRD uses
knowledge from larger models to fill knowledge gaps in crystallography and
shows that AI-generated texts can help smaller models reason more effectively
in scientific tasks. While the current version of OPENXRD focuses on text-based
inputs, we also explore future extensions such as adding real crystal diagrams
or diffraction patterns to improve interpretation in specialized materials
science contexts. Overall, OPENXRD shows that specialized open-book systems can
be useful in materials science and provides a foundation for broader natural
language processing (NLP) tools in critical scientific fields.

</details>


### [14] [PU-Lie: Lightweight Deception Detection in Imbalanced Diplomatic Dialogues via Positive-Unlabeled Learning](https://arxiv.org/abs/2507.09157)
*Bhavinkumar Vinodbhai Kuwar,Bikrant Bikram Pratap Maurya,Priyanshu Gupta,Nitin Choudhury*

Main category: cs.CL

TL;DR: A lightweight model (PU-Lie) for deception detection in Diplomacy dataset, using frozen BERT, interpretable features, and PU learning, achieves 0.60 macro F1 with 650x fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Detecting deception in strategic dialogues is challenging due to subtle language and extreme class imbalance (less than 5% deceptive messages).

Method: Combines frozen BERT embeddings, interpretable linguistic/game-specific features, and PU learning to handle unlabeled data and rare deceptive class.

Result: Achieves 0.60 macro F1, outperforming seven models, with 650x fewer trainable parameters.

Conclusion: PU learning, interpretability, and speaker-aware representations are valuable, with a focus on accurately detecting deception over truthful messages.

Abstract: Detecting deception in strategic dialogues is a complex and high-stakes task
due to the subtlety of language and extreme class imbalance between deceptive
and truthful communications. In this work, we revisit deception detection in
the Diplomacy dataset, where less than 5% of messages are labeled deceptive. We
introduce a lightweight yet effective model combining frozen BERT embeddings,
interpretable linguistic and game-specific features, and a Positive-Unlabeled
(PU) learning objective. Unlike traditional binary classifiers, PU-Lie is
tailored for situations where only a small portion of deceptive messages are
labeled, and the majority are unlabeled. Our model achieves a new best macro F1
of 0.60 while reducing trainable parameters by over 650x. Through comprehensive
evaluations and ablation studies across seven models, we demonstrate the value
of PU learning, linguistic interpretability, and speaker-aware representations.
Notably, we emphasize that in this problem setting, accurately detecting
deception is more critical than identifying truthful messages. This priority
guides our choice of PU learning, which explicitly models the rare but vital
deceptive class.

</details>


### [15] [RAMA: Retrieval-Augmented Multi-Agent Framework for Misinformation Detection in Multimodal Fact-Checking](https://arxiv.org/abs/2507.09174)
*Shuo Yang,Zijian Yu,Zhenzhe Ying,Yuqin Dai,Guoqing Wang,Jun Lan,Jinfeng Xu,Jinze Li,Edith C. H. Ngai*

Main category: cs.CL

TL;DR: RAMA is a retrieval-augmented multi-agent framework for verifying multimedia misinformation, excelling in handling ambiguous claims by leveraging web-based evidence and multi-agent reasoning.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of automated fact-checking for ambiguous or context-lacking multimodal misinformation.

Method: Uses strategic query formulation, cross-verification evidence aggregation, and a multi-agent ensemble of multimodal LLMs.

Result: Superior performance on benchmark datasets, especially for ambiguous claims.

Conclusion: Integrating web evidence and multi-agent reasoning is essential for reliable multimedia verification.

Abstract: The rapid proliferation of multimodal misinformation presents significant
challenges for automated fact-checking systems, especially when claims are
ambiguous or lack sufficient context. We introduce RAMA, a novel
retrieval-augmented multi-agent framework designed for verifying multimedia
misinformation. RAMA incorporates three core innovations: (1) strategic query
formulation that transforms multimodal claims into precise web search queries;
(2) cross-verification evidence aggregation from diverse, authoritative
sources; and (3) a multi-agent ensemble architecture that leverages the
complementary strengths of multiple multimodal large language models and prompt
variants. Extensive experiments demonstrate that RAMA achieves superior
performance on benchmark datasets, particularly excelling in resolving
ambiguous or improbable claims by grounding verification in retrieved factual
evidence. Our findings underscore the necessity of integrating web-based
evidence and multi-agent reasoning for trustworthy multimedia verification,
paving the way for more reliable and scalable fact-checking solutions. RAMA
will be publicly available at https://github.com/kalendsyang/RAMA.git.

</details>


### [16] [Detecting and Pruning Prominent but Detrimental Neurons in Large Language Models](https://arxiv.org/abs/2507.09185)
*Ameen Ali,Shahar Katz,Lior Wolf,Ivan Titov*

Main category: cs.CL

TL;DR: A fine-tuning method using neuron pruning improves LLM generalization by removing dataset-specific mechanisms.


<details>
  <summary>Details</summary>
Motivation: LLMs often rely on dataset-specific correlations, harming performance on novel tasks. The goal is to enhance generalization.

Method: Uses Integrated Gradients to identify and prune neurons linked to dataset-specific mechanisms, forcing reliance on generalizable representations.

Result: Outperforms prior adaptation methods on multiple-choice benchmarks.

Conclusion: Selective neuron pruning during fine-tuning enhances LLM generalization.

Abstract: Large language models (LLMs) often develop learned mechanisms specialized to
specific datasets, such as reliance on domain-specific correlations, which
yield high-confidence predictions without generalizable reasoning. While
beneficial in one setting, these dataset-specific mechanisms typically degrade
performance when models encounter novel tasks or distributions. In this work,
we introduce a fine-tuning approach designed to enhance generalization by
identifying and pruning neurons associated with dataset-specific mechanisms in
transformer-based LLMs. Our method employs Integrated Gradients to quantify
each neuron's influence on high-confidence predictions, pinpointing those that
disproportionately contribute to dataset-specific performance without
supporting robust, transferable reasoning. Selectively pruning these neurons
compels the model to depend on generalizable representations. Evaluated across
multiple-choice benchmarks, our pruning-based fine-tuning significantly
enhances performance, surpassing prior (non-pruning) adaptation methods.

</details>


### [17] [Banzhida: Advancing Large Language Models for Tibetan with Curated Data and Continual Pre-Training](https://arxiv.org/abs/2507.09205)
*Leiyu Pan,Bojian Xiong,Lei Yang,Renren Jin,Shaowei Zhang,Yue Chen,Ling Shi,Jiang Zhou,Junru Wu,Zhen Wang,Jianxiang Peng,Juesi Xiao,Tianyu Dong,Zhuowen Han,Zhuo Chen,Sangjee Dondrub,Caizang Tai,Haixing Zhao,Huaque Cairang,Suonan Cairang,Rou Te,Lengben Zhaxi,Gazang Zhaxi,Zhonglin Ye,Yuhui Zheng,Chunyan Peng,Secha Jia,Pema Tashi,Cizhen Jiacuo,Pema Dorjee,Hongkai Liu,Pema Yanggon,Tsehang Dorjee,Jiaxin Han,Qiongying Hu,Jilin Man,Huanke You,Yuqi Ren,Duo La,Deyi Xiong*

Main category: cs.CL

TL;DR: The paper introduces Banzhida, a multilingual large language model for Tibetan, addressing the lack of representation of low-resource languages like Tibetan in existing models.


<details>
  <summary>Details</summary>
Motivation: Tibetan is underrepresented in large language models due to scarce high-quality training data, prompting the creation of a dedicated corpus and model.

Method: The authors curate the largest Tibetan pre-training corpus, apply a tailored cleaning pipeline, and fine-tune a multilingual base model into Banzhida.

Result: Banzhida outperforms similar-scale open-source and Tibetan-tailored models across various tasks, validated by new and existing benchmarks.

Conclusion: The study successfully advances generative AI for Tibetan, demonstrating the effectiveness of tailored data and model training for low-resource languages.

Abstract: Large language models have achieved remarkable progress across many
languages. However, Tibetan, as a representative low-resource language, is
particularly underrepresented in existing models due to the scarcity of
high-quality training corpora. To address this gap, we curate the largest
Tibetan pre-training corpus to date, aggregating data from diverse sources and
applying a dedicated data cleaning and processing pipeline tailored for
Tibetan. With the curated data, we continue pre/post-training a multilingual
base model into Banzhida, a multilingual large language model that advances
generative AI for Tibetan. To evaluate the Tibetan capabilities of the model,
we create new high-quality Tibetan benchmarks, and complement them with
existing public benchmarks. Experimental results demonstrate that Banzhida
consistently and significantly outperforms both open-source models of similar
scale and Tibetan-tailored models across a wide range of tasks.

</details>


### [18] [MetaClimage: A novel database of visual metaphors related to Climate Change, with costs and benefits analysis](https://arxiv.org/abs/2507.09225)
*Biagio Scalingi,Chiara Barattieri di San Pietro,Paolo Canal,Valentina Bambini*

Main category: cs.CL

TL;DR: The paper introduces MetaClimage, a database of visual metaphors for climate change, and analyzes their impact compared to literal images. Visual metaphors are harder to understand but more aesthetically pleasing, with no difference in efficacy or arousal. They elicit more cognitive engagement and positive valence.


<details>
  <summary>Details</summary>
Motivation: To address the lack of research on the impact of visual metaphors in climate change communication and provide a structured database for future studies.

Method: Created MetaClimage, a database of visual metaphors paired with literal images, enriched with human ratings (difficulty, efficacy, artistic quality, emotional arousal) and NLP-derived semantic/emotion variables from tags.

Result: Visual metaphors were rated as more difficult but aesthetically better, with no difference in efficacy or arousal. They elicited more tags, positive valence, and cognitive engagement, especially in high-Need For Cognition individuals.

Conclusion: Visual metaphors impose greater cognitive load but offer benefits like deeper engagement and positive valence, suggesting a trade-off for environmental communication strategies.

Abstract: Visual metaphors of climate change (e.g., melting glaciers depicted as a
melting ice grenade) are regarded as valuable tools for addressing the
complexity of environmental challenges. However, few studies have examined
their impact on communication, also due to scattered availability of material.
Here, we present a novel database of Metaphors of Climate Change in Images
(MetaClimage) https://doi.org/10.5281/zenodo.15861012, paired with literal
images and enriched with human ratings. For each image, we collected values of
difficulty, efficacy, artistic quality, and emotional arousal from human
rating, as well as number of tags generated by participants to summarize the
message. Semantic and emotion variables were further derived from the tags via
Natural Language Processing. Visual metaphors were rated as more difficult to
understand, yet more aesthetically pleasant than literal images, but did not
differ in efficacy and arousal. The latter for visual metaphors, however, was
higher in participants with higher Need For Cognition. Furthermore, visual
metaphors received more tags, often referring to entities not depicted in the
image, and elicited words with more positive valence and greater dominance than
literal images. These results evidence the greater cognitive load of visual
metaphors, which nevertheless might induce positive effects such as deeper
cognitive elaboration and abstraction compared to literal stimuli. Furthermore,
while they are not deemed as more effective and arousing, visual metaphors seem
to generate superior aesthetic appreciation and a more positively valenced
experience. Overall, this study contributes to understanding the impact of
visual metaphors of climate change both by offering a database for future
research and by elucidating a cost-benefit trade-off to take into account when
shaping environmental communication.

</details>


### [19] [Swa-bhasha Resource Hub: Romanized Sinhala to Sinhala Transliteration Systems and Data Resources](https://arxiv.org/abs/2507.09245)
*Deshan Sumanathilaka,Sameera Perera,Sachithya Dharmasiri,Maneesha Athukorala,Anuja Dilrukshi Herath,Rukshan Dias,Pasindu Gamage,Ruvan Weerasinghe,Y. H. P. P. Priyadarshana*

Main category: cs.CL

TL;DR: The Swa-bhasha Resource Hub offers data and tools for Romanized Sinhala to Sinhala transliteration, aiding NLP research and applications.


<details>
  <summary>Details</summary>
Motivation: To advance Sinhala NLP by providing accessible resources for transliteration research and applications.

Method: Collection and public sharing of datasets and algorithms, with a comparative analysis of existing transliteration tools.

Result: A comprehensive hub of resources and tools for Romanized Sinhala transliteration, supporting NLP advancements.

Conclusion: The hub significantly contributes to Sinhala NLP by facilitating research and application development in transliteration.

Abstract: The Swa-bhasha Resource Hub provides a comprehensive collection of data
resources and algorithms developed for Romanized Sinhala to Sinhala
transliteration between 2020 and 2025. These resources have played a
significant role in advancing research in Sinhala Natural Language Processing
(NLP), particularly in training transliteration models and developing
applications involving Romanized Sinhala. The current openly accessible data
sets and corresponding tools are made publicly available through this hub. This
paper presents a detailed overview of the resources contributed by the authors
and includes a comparative analysis of existing transliteration applications in
the domain.

</details>


### [20] [Psychology-Driven Enhancement of Humour Translation](https://arxiv.org/abs/2507.09259)
*Yuchen Su,Yonghua Zhu,Yang Chen,Diana Benavides-Prado,Michael Witbrock*

Main category: cs.CL

TL;DR: The paper introduces a psychology-inspired Humour Decomposition Mechanism (HDM) to improve humour translation in LLMs, showing significant gains in humour, fluency, and coherence.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs struggle with humour translation due to linguistic interference and lack of humour in outputs, hindering cross-cultural communication.

Method: Proposes HDM using Chain-of-Thought (CoT) to mimic human thought and integrates humour theory to enhance translated humorous texts.

Result: Experiments show average gains of 7.75% in humour, 2.81% in fluency, and 6.13% in coherence.

Conclusion: HDM effectively improves humour translation quality in LLMs, bridging cultural gaps.

Abstract: Humour translation plays a vital role as a bridge between different cultures,
fostering understanding and communication. Although most existing Large
Language Models (LLMs) are capable of general translation tasks, these models
still struggle with humour translation, which is especially reflected through
linguistic interference and lacking humour in translated text. In this paper,
we propose a psychology-inspired Humour Decomposition Mechanism (HDM) that
utilises Chain-of-Thought (CoT) to imitate the ability of the human thought
process, stimulating LLMs to optimise the readability of translated humorous
texts. Moreover, we integrate humour theory in HDM to further enhance the
humorous elements in the translated text. Our automatic evaluation experiments
on open-source humour datasets demonstrate that our method significantly
improves the quality of humour translation, yielding average gains of 7.75\% in
humour, 2.81\% in fluency, and 6.13\% in coherence of the generated text.

</details>


### [21] [ClaritySpeech: Dementia Obfuscation in Speech](https://arxiv.org/abs/2507.09282)
*Dominika Woszczyk,Ranya Aloufi,Soteris Demetriou*

Main category: cs.CL

TL;DR: ClaritySpeech improves dementia-affected speech with ASR, text obfuscation, and zero-shot TTS, enhancing privacy and accessibility without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Address communication barriers and privacy concerns in dementia-affected speech, where current ASR struggles.

Method: Integrates ASR, text obfuscation, and zero-shot TTS to correct speech while preserving speaker identity in low-data environments.

Result: Reduces F1 score drop (16% for ADReSS, 10% for ADReSSo), improves WER (0.73 to 0.08), and enhances speech quality (1.65 to ~2.15).

Conclusion: ClaritySpeech effectively balances privacy and accessibility for dementia-affected speech.

Abstract: Dementia, a neurodegenerative disease, alters speech patterns, creating
communication barriers and raising privacy concerns. Current speech
technologies, such as automatic speech transcription (ASR), struggle with
dementia and atypical speech, further challenging accessibility. This paper
presents a novel dementia obfuscation in speech framework, ClaritySpeech,
integrating ASR, text obfuscation, and zero-shot text-to-speech (TTS) to
correct dementia-affected speech while preserving speaker identity in low-data
environments without fine-tuning. Results show a 16% and 10% drop in mean F1
score across various adversarial settings and modalities (audio, text, fusion)
for ADReSS and ADReSSo, respectively, maintaining 50% speaker similarity. We
also find that our system improves WER (from 0.73 to 0.08 for ADReSS and 0.15
for ADReSSo) and speech quality from 1.65 to ~2.15, enhancing privacy and
accessibility.

</details>


### [22] [DATE-LM: Benchmarking Data Attribution Evaluation for Large Language Models](https://arxiv.org/abs/2507.09424)
*Cathy Jiao,Yijun Pan,Emily Xiao,Daisy Sheng,Niket Jain,Hanzhang Zhao,Ishita Dasgupta,Jiaqi W. Ma,Chenyan Xiong*

Main category: cs.CL

TL;DR: DATE-LM is a benchmark for evaluating data attribution methods in LLMs, covering tasks like training data selection, toxicity/bias filtering, and factual attribution. It reveals no single method excels in all tasks and highlights trade-offs with baselines.


<details>
  <summary>Details</summary>
Motivation: Address gaps in systematic evaluation of data attribution methods for LLMs, crucial for dataset curation, interpretability, and data valuation.

Method: Introduces DATE-LM, a unified benchmark with three key tasks, designed for ease of use and large-scale evaluation across diverse LLM architectures.

Result: No single method dominates; performance varies by task, and simpler baselines often compete. Task-specific design impacts results.

Conclusion: DATE-LM provides a foundation for future research, with a public leaderboard to foster community engagement and method comparison.

Abstract: Data attribution methods quantify the influence of training data on model
outputs and are becoming increasingly relevant for a wide range of LLM research
and applications, including dataset curation, model interpretability, data
valuation. However, there remain critical gaps in systematic LLM-centric
evaluation of data attribution methods. To this end, we introduce DATE-LM (Data
Attribution Evaluation in Language Models), a unified benchmark for evaluating
data attribution methods through real-world LLM applications. DATE-LM measures
attribution quality through three key tasks -- training data selection,
toxicity/bias filtering, and factual attribution. Our benchmark is designed for
ease of use, enabling researchers to configure and run large-scale evaluations
across diverse tasks and LLM architectures. Furthermore, we use DATE-LM to
conduct a large-scale evaluation of existing data attribution methods. Our
findings show that no single method dominates across all tasks, data
attribution methods have trade-offs with simpler baselines, and method
performance is sensitive to task-specific evaluation design. Finally, we
release a public leaderboard for quick comparison of methods and to facilitate
community engagement. We hope DATE-LM serves as a foundation for future data
attribution research in LLMs.

</details>


### [23] [Enhancing Clinical Text Classification via Fine-Tuned DRAGON Longformer Models](https://arxiv.org/abs/2507.09470)
*Mingchuan Yang,Ziyuan Huang*

Main category: cs.CL

TL;DR: The study optimized the DRAGON Longformer model for clinical text classification, achieving significant performance improvements through hyperparameter tuning, preprocessing, and architectural changes.


<details>
  <summary>Details</summary>
Motivation: To enhance the model's capability for binary classification of medical case descriptions, addressing the need for accurate clinical NLP applications.

Method: Enhanced the pre-trained model with hyperparameter tuning (sequence length, learning rate, epochs), domain-specific preprocessing, and architectural adjustments.

Result: Performance improved: accuracy (72.0% to 85.2%), precision (68.0% to 84.1%), recall (75.0% to 86.3%), F1-score (71.0% to 85.2%), with statistical significance (p < .001).

Conclusion: The optimized model shows strong potential for clinical NLP, with improved interpretation of medical terminology and broader healthcare applicability.

Abstract: This study explores the optimization of the DRAGON Longformer base model for
clinical text classification, specifically targeting the binary classification
of medical case descriptions. A dataset of 500 clinical cases containing
structured medical observations was used, with 400 cases for training and 100
for validation. Enhancements to the pre-trained
joeranbosma/dragon-longformer-base-mixed-domain model included hyperparameter
tuning, domain-specific preprocessing, and architectural adjustments. Key
modifications involved increasing sequence length from 512 to 1024 tokens,
adjusting learning rates from 1e-05 to 5e-06, extending training epochs from 5
to 8, and incorporating specialized medical terminology. The optimized model
achieved notable performance gains: accuracy improved from 72.0% to 85.2%,
precision from 68.0% to 84.1%, recall from 75.0% to 86.3%, and F1-score from
71.0% to 85.2%. Statistical analysis confirmed the significance of these
improvements (p < .001). The model demonstrated enhanced capability in
interpreting medical terminology, anatomical measurements, and clinical
observations. These findings contribute to domain-specific language model
research and offer practical implications for clinical natural language
processing applications. The optimized model's strong performance across
diverse medical conditions underscores its potential for broad use in
healthcare settings.

</details>


### [24] [The CoNLL-2013 Shared Task on Grammatical Error Correction](https://arxiv.org/abs/2507.09474)
*Hwee Tou Ng,Siew Mei Wu,Yuanbin Wu,Christian Hadiwinoto,Joel Tetreault*

Main category: cs.CL

TL;DR: Summary of the CoNLL-2013 shared task on grammatical error correction, covering task definition, datasets, evaluation metrics, participant approaches, and results.


<details>
  <summary>Details</summary>
Motivation: To address grammatical error correction by defining the task, providing datasets, and evaluating diverse approaches from participating teams.

Method: Task definition, dataset presentation, evaluation metric description, and overview of participant methods.

Result: Evaluation results of the shared task, showcasing performance of various approaches.

Conclusion: The paper provides a comprehensive overview of the CoNLL-2013 shared task, highlighting methodologies and outcomes in grammatical error correction.

Abstract: The CoNLL-2013 shared task was devoted to grammatical error correction. In
this paper, we give the task definition, present the data sets, and describe
the evaluation metric and scorer used in the shared task. We also give an
overview of the various approaches adopted by the participating teams, and
present the evaluation results.

</details>


### [25] [Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs](https://arxiv.org/abs/2507.09477)
*Yangning Li,Weizhi Zhang,Yuyao Yang,Wei-Chieh Huang,Yaozu Wu,Junyu Luo,Yuanchen Bei,Henry Peng Zou,Xiao Luo,Yusheng Zhao,Chunkit Chan,Yankai Chen,Zhongfen Deng,Yinghui Li,Hai-Tao Zheng,Dongyuan Li,Renhe Jiang,Ming Zhang,Yangqiu Song,Philip S. Yu*

Main category: cs.CL

TL;DR: The paper surveys the integration of retrieval-augmented generation (RAG) and reasoning in LLMs, proposing a unified framework to enhance factuality and inference.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of RAG in multi-step inference and reasoning-oriented approaches in grounding facts, the paper aims to synthesize these methods for improved performance.

Method: The survey categorizes methods into Reasoning-Enhanced RAG, RAG-Enhanced Reasoning, and Synergized RAG-Reasoning frameworks, analyzing datasets and challenges.

Result: The unified approach achieves state-of-the-art performance in knowledge-intensive tasks by interleaving retrieval and reasoning iteratively.

Conclusion: The paper outlines future research directions for more effective, adaptive, trustworthy, and human-centric RAG-Reasoning systems.

Abstract: Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language
Models (LLMs) by injecting external knowledge, yet it falls short on problems
that demand multi-step inference; conversely, purely reasoning-oriented
approaches often hallucinate or mis-ground facts. This survey synthesizes both
strands under a unified reasoning-retrieval perspective. We first map how
advanced reasoning optimizes each stage of RAG (Reasoning-Enhanced RAG). Then,
we show how retrieved knowledge of different type supply missing premises and
expand context for complex inference (RAG-Enhanced Reasoning). Finally, we
spotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs
iteratively interleave search and reasoning to achieve state-of-the-art
performance across knowledge-intensive benchmarks. We categorize methods,
datasets, and open challenges, and outline research avenues toward deeper
RAG-Reasoning systems that are more effective, multimodally-adaptive,
trustworthy, and human-centric. The collection is available at
https://github.com/DavidZWZ/Awesome-RAG-Reasoning.

</details>


### [26] [ViSP: A PPO-Driven Framework for Sarcasm Generation with Contrastive Learning](https://arxiv.org/abs/2507.09482)
*Changli Wang,Rui Wu,Fang Yin*

Main category: cs.CL

TL;DR: The paper introduces M2SaG, a multimodal sarcasm generation dataset, and ViSP, a framework using PPO and contrastive learning to improve sarcasm generation, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Sarcasm generation is underexplored due to reliance on text and neglect of visual cues, along with dataset mismatches.

Method: ViSP integrates Proximal Policy Optimization (PPO) and contrastive learning to generate sarcastic texts, guided by reward scores.

Result: ViSP surpasses baselines, with higher Sarcasm Scores (0.898 vs. 0.770) and Factual Incongruity (0.768 vs. 0.739).

Conclusion: ViSP effectively generates high-quality sarcastic content, addressing limitations in existing approaches.

Abstract: Human emotions are complex, with sarcasm being a subtle and distinctive form.
Despite progress in sarcasm research, sarcasm generation remains underexplored,
primarily due to the overreliance on textual modalities and the neglect of
visual cues, as well as the mismatch between image content and sarcastic intent
in existing datasets. In this paper, we introduce M2SaG, a multimodal sarcasm
generation dataset with 4,970 samples, each containing an image, a sarcastic
text, and a sarcasm target. To benchmark M2SaG, we propose ViSP, a generation
framework that integrates Proximal Policy Optimization (PPO) and contrastive
learning. PPO utilizes reward scores from DIP to steer the generation of
sarcastic texts, while contrastive learning encourages the model to favor
outputs with higher reward scores. These strategies improve overall generation
quality and produce texts with more pronounced sarcastic intent. We evaluate
ViSP across five metric sets and find it surpasses all baselines, including
large language models, underscoring their limitations in sarcasm generation.
Furthermore, we analyze the distributions of Sarcasm Scores and Factual
Incongruity for both M2SaG and the texts generated by ViSP. The generated texts
exhibit higher mean Sarcasm Scores (0.898 vs. 0.770) and Factual Incongruity
(0.768 vs. 0.739), demonstrating that ViSP produces higher-quality sarcastic
content than the original dataset. % The dataset and code will be publicly
available. Our dataset and code will be released at
\textit{https://github.com/wclapply/ViSP}.

</details>


### [27] [Balanced Training Data Augmentation for Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2507.09485)
*Junjie Liu,Yuanhe Tian,Yan Song*

Main category: cs.CL

TL;DR: Proposes an LLM-based ABSA approach with data augmentation and reinforcement learning to improve performance on short, imbalanced datasets.


<details>
  <summary>Details</summary>
Motivation: Existing ABSA methods struggle with short text and imbalanced labeled data, limiting context understanding.

Method: Uses LLM for data augmentation and reinforcement learning to optimize augmented data quality.

Result: Superior performance on English benchmark datasets compared to baselines and existing studies.

Conclusion: The approach effectively enhances ABSA performance by addressing data limitations.

Abstract: Aspect-based sentiment analysis (ABSA) is a crucial fine-grained task in
social media scenarios to identify the sentiment polarity of specific aspect
terms in a sentence. Although many existing studies leverage large language
models (LLMs) to perform ABSA due to their strong context understanding
capabilities, they still face challenges to learn the context information in
the running text because of the short text, as well as the small and unbalanced
labeled training data, where most data are labeled with positive sentiment.
Data augmentation (DA) is a feasible strategy for providing richer contextual
information, especially when using LLMs to create synthetic training data, but
faces challenges in ensuring a high quality of the augmented data.In this
paper, we propose an LLM-based ABSA approach with training data
augmentation.Specifically, an LLM is prompted to generate augmented training
data based on the original training data, so as to construct a new training
data with larger size and balanced label distributions to better train an ABSA
model. Meanwhile, in order to improve the quality of the augmented data, we
propose a reinforcement learning approach to optimize the data augmentation.
LLM.Experiment results and further analyses on English benchmark datasets for
ABSA demonstrate the effectiveness of our approach, where superior performance
is observed over strong baselines and most existing studies.

</details>


### [28] [GoalfyMax: A Protocol-Driven Multi-Agent System for Intelligent Experience Entities](https://arxiv.org/abs/2507.09497)
*Siyi Wu,Zeyu Wang,Xinyuan Song,Zhengpeng Zhou,Lifan Sun,Tianyu Shi*

Main category: cs.CL

TL;DR: GoalfyMax is a protocol-driven framework for multi-agent collaboration, addressing coordination and memory reuse challenges with standardized communication and layered memory systems.


<details>
  <summary>Details</summary>
Motivation: Traditional AI systems lack coordination, memory reuse, and task decomposition, limiting scalability in complex enterprise environments.

Method: GoalfyMax uses Agent-to-Agent (A2A) communication via Model Context Protocol (MCP) and Experience Pack (XP) for memory. It includes multi-turn dialogue, memory modules, and dynamic safety validation.

Result: Empirical results show GoalfyMax outperforms baselines in adaptability, coordination, and experience reuse.

Conclusion: GoalfyMax is a scalable, future-ready foundation for multi-agent intelligent systems.

Abstract: Modern enterprise environments demand intelligent systems capable of handling
complex, dynamic, and multi-faceted tasks with high levels of autonomy and
adaptability. However, traditional single-purpose AI systems often lack
sufficient coordination, memory reuse, and task decomposition capabilities,
limiting their scalability in realistic settings. To address these challenges,
we present \textbf{GoalfyMax}, a protocol-driven framework for end-to-end
multi-agent collaboration. GoalfyMax introduces a standardized Agent-to-Agent
(A2A) communication layer built on the Model Context Protocol (MCP), allowing
independent agents to coordinate through asynchronous, protocol-compliant
interactions. It incorporates the Experience Pack (XP) architecture, a layered
memory system that preserves both task rationales and execution traces,
enabling structured knowledge retention and continual learning. Moreover, our
system integrates advanced features including multi-turn contextual dialogue,
long-short term memory modules, and dynamic safety validation, supporting
robust, real-time strategy adaptation. Empirical results on complex task
orchestration benchmarks and case study demonstrate that GoalfyMax achieves
superior adaptability, coordination, and experience reuse compared to baseline
frameworks. These findings highlight its potential as a scalable, future-ready
foundation for multi-agent intelligent systems.

</details>


### [29] [Ref-Long: Benchmarking the Long-context Referencing Capability of Long-context Language Models](https://arxiv.org/abs/2507.09506)
*Junjie Wu,Gefei Gu,Yanan Zheng,Dit-Yan Yeung,Arman Cohan*

Main category: cs.CL

TL;DR: The paper introduces Ref-Long, a benchmark to evaluate long-context referencing in language models, revealing gaps even in advanced models like GPT-4o.


<details>
  <summary>Details</summary>
Motivation: Long-context referencing is underexplored despite its importance, prompting the need for a dedicated benchmark.

Method: Ref-Long assesses models by requiring them to identify document indexes referencing a key, with three subsets from synthetic to realistic scenarios.

Result: Tests on 13 LCLMs show significant shortcomings in referencing, even for top models.

Conclusion: Ref-Long highlights challenges in long-context referencing, offering insights through analyses and publicly available resources.

Abstract: Long-context language models (LCLMs) have exhibited impressive capabilities
in long-context understanding tasks. Among these, long-context referencing -- a
crucial task that requires LCLMs to attribute items of interest to specific
parts of long-context data -- remains underexplored. To bridge this gap, this
paper proposes Referencing Evaluation for Long-context Language Models
(Ref-Long), a novel benchmark designed to assess the long-context referencing
capability of LCLMs. Specifically, Ref-Long requires LCLMs to identify the
indexes of documents that reference a specific key, emphasizing contextual
relationships between the key and the documents over simple retrieval. Based on
the task design, we construct three subsets ranging from synthetic to realistic
scenarios to form the Ref-Long benchmark. Experimental results of 13 LCLMs
reveal significant shortcomings in long-context referencing, even among
advanced models like GPT-4o. To further investigate these challenges, we
conduct comprehensive analyses, including human evaluations, task format
adjustments, fine-tuning experiments, and error analyses, leading to several
key insights. Our data and code can be found in https://github.
com/wujunjie1998/Ref-Long.

</details>


### [30] [How Important is `Perfect' English for Machine Translation Prompts?](https://arxiv.org/abs/2507.09509)
*Patrcia Schmidtov,Niyati Bafna,Seth Aycock,Gianluca Vico,Wiktor Kamzela,Katharina Hmmerl,Vilm Zouhar*

Main category: cs.CL

TL;DR: LLMs' performance in machine translation and evaluation is sensitive to prompt errors, with varying impacts based on noise type. Prompt quality affects instruction following more than translation quality itself.


<details>
  <summary>Details</summary>
Motivation: To systematically evaluate how humanly plausible and synthetic errors in prompts impact LLMs' performance in machine translation and evaluation tasks.

Method: Quantitative and qualitative analysis of LLMs' responses to increasing noise in prompts, focusing on different noise types (character-level, phrasal, combined).

Result: Prompt quality strongly affects performance; character-level and combined noise degrade translation more than phrasal noise. LLMs can still translate under extreme noise.

Conclusion: Prompt errors impact instruction following more than translation quality, with noise type playing a key role in performance degradation.

Abstract: Large language models (LLMs) have achieved top results in recent machine
translation evaluations, but they are also known to be sensitive to errors and
perturbations in their prompts. We systematically evaluate how both humanly
plausible and synthetic errors in user prompts affect LLMs' performance on two
related tasks: Machine translation and machine translation evaluation. We
provide both a quantitative analysis and qualitative insights into how the
models respond to increasing noise in the user prompt.
  The prompt quality strongly affects the translation performance: With many
errors, even a good prompt can underperform a minimal or poor prompt without
errors. However, different noise types impact translation quality differently,
with character-level and combined noisers degrading performance more than
phrasal perturbations. Qualitative analysis reveals that lower prompt quality
largely leads to poorer instruction following, rather than directly affecting
translation quality itself. Further, LLMs can still translate in scenarios with
overwhelming random noise that would make the prompt illegible to humans.

</details>


### [31] [Adapting Definition Modeling for New Languages: A Case Study on Belarusian](https://arxiv.org/abs/2507.09536)
*Daniela Kazakouskaya,Timothee Mickus,Janine Siewert*

Main category: cs.CL

TL;DR: The paper explores adapting definition modeling to Belarusian, proposing a new dataset and showing minimal data is needed for adaptation, though automatic metrics have gaps.


<details>
  <summary>Details</summary>
Motivation: To assist lexicographers by expanding definition modeling to unsupported languages like Belarusian.

Method: Adapting existing definition modeling systems to Belarusian using a novel dataset of 43,150 definitions.

Result: Demonstrates that minimal data is required for adaptation, but highlights limitations in automatic metrics.

Conclusion: Definition modeling can be adapted to new languages with little data, but better metrics are needed.

Abstract: Definition modeling, the task of generating new definitions for words in
context, holds great prospect as a means to assist the work of lexicographers
in documenting a broader variety of lects and languages, yet much remains to be
done in order to assess how we can leverage pre-existing models for as-of-yet
unsupported languages. In this work, we focus on adapting existing models to
Belarusian, for which we propose a novel dataset of 43,150 definitions. Our
experiments demonstrate that adapting a definition modeling systems requires
minimal amounts of data, but that there currently are gaps in what automatic
metrics do capture.

</details>


### [32] [NMIXX: Domain-Adapted Neural Embeddings for Cross-Lingual eXploration of Finance](https://arxiv.org/abs/2507.09601)
*Hanwool Lee,Sara Yu,Yewon Hwang,Jonghyun Choi,Heejae Ahn,Sungbum Jung,Youngjae Yu*

Main category: cs.CL

TL;DR: NMIXX, a cross-lingual embedding model for finance, improves financial semantic capture in low-resource languages like Korean, outperforming baselines on specialized benchmarks.


<details>
  <summary>Details</summary>
Motivation: General-purpose sentence embeddings fail to capture financial semantics in low-resource languages due to jargon, temporal shifts, and bilingual misalignment.

Method: NMIXX is fine-tuned using 18.8K high-confidence triplets (paraphrases, hard negatives, translations) and evaluated on KorFinSTS, a specialized Korean financial benchmark.

Result: NMIXX achieves significant gains (+0.10 on English FinSTS, +0.22 on KorFinSTS) over baselines, with a trade-off in general STS performance.

Conclusion: Tokenizer design is crucial for low-resource cross-lingual adaptation; NMIXX and KorFinSTS provide robust tools for financial representation learning.

Abstract: General-purpose sentence embedding models often struggle to capture
specialized financial semantics, especially in low-resource languages like
Korean, due to domain-specific jargon, temporal meaning shifts, and misaligned
bilingual vocabularies. To address these gaps, we introduce NMIXX (Neural
eMbeddings for Cross-lingual eXploration of Finance), a suite of cross-lingual
embedding models fine-tuned with 18.8K high-confidence triplets that pair
in-domain paraphrases, hard negatives derived from a semantic-shift typology,
and exact Korean-English translations. Concurrently, we release KorFinSTS, a
1,921-pair Korean financial STS benchmark spanning news, disclosures, research
reports, and regulations, designed to expose nuances that general benchmarks
miss.
  When evaluated against seven open-license baselines, NMIXX's multilingual
bge-m3 variant achieves Spearman's rho gains of +0.10 on English FinSTS and
+0.22 on KorFinSTS, outperforming its pre-adaptation checkpoint and surpassing
other models by the largest margin, while revealing a modest trade-off in
general STS performance. Our analysis further shows that models with richer
Korean token coverage adapt more effectively, underscoring the importance of
tokenizer design in low-resource, cross-lingual settings. By making both models
and the benchmark publicly available, we provide the community with robust
tools for domain-adapted, multilingual representation learning in finance.

</details>


### [33] [SpreadPy: A Python tool for modelling spreading activation and superdiffusion in cognitive multiplex networks](https://arxiv.org/abs/2507.09628)
*Salvatore Citraro,Edith Haim,Alessandra Carini,Cynthia S. Q. Siew,Giulio Rossetti,Massimo Stella*

Main category: cs.CL

TL;DR: SpreadPy is a Python library for simulating spreading activation in cognitive networks, aiding research in psychology, neuroscience, and education by linking network dynamics to cognitive phenomena.


<details>
  <summary>Details</summary>
Motivation: The motivation is to provide a tool for testing structure-function relationships in cognitive processes, enabling systematic investigations of activation dynamics in knowledge modeling.

Method: SpreadPy performs numerical simulations on cognitive single-layer and multiplex networks, comparing results with grounded theories. It includes case studies on math anxiety, creativity tasks, and aphasia.

Result: The library successfully distinguishes cognitive differences (e.g., math anxiety), reveals task difficulty effects on activation, and correlates simulated patterns with clinical impairments (e.g., aphasia).

Conclusion: SpreadPy offers a flexible framework for modeling cognitive processes, supporting reproducible research and providing mechanistic insights into individual differences and impairments.

Abstract: We introduce SpreadPy as a Python library for simulating spreading activation
in cognitive single-layer and multiplex networks. Our tool is designed to
perform numerical simulations testing structure-function relationships in
cognitive processes. By comparing simulation results with grounded theories in
knowledge modelling, SpreadPy enables systematic investigations of how
activation dynamics reflect cognitive, psychological and clinical phenomena. We
demonstrate the library's utility through three case studies: (1) Spreading
activation on associative knowledge networks distinguishes students with high
versus low math anxiety, revealing anxiety-related structural differences in
conceptual organization; (2) Simulations of a creativity task show that
activation trajectories vary with task difficulty, exposing how cognitive load
modulates lexical access; (3) In individuals with aphasia, simulated activation
patterns on lexical networks correlate with empirical error types (semantic vs.
phonological) during picture-naming tasks, linking network structure to
clinical impairments. SpreadPy's flexible framework allows researchers to model
these processes using empirically derived or theoretical networks, providing
mechanistic insights into individual differences and cognitive impairments. The
library is openly available, supporting reproducible research in psychology,
neuroscience, and education research.

</details>


### [34] [An Exploration of Knowledge Editing for Arabic](https://arxiv.org/abs/2507.09629)
*Basel Mousi,Nadir Durrani,Fahim Dalvi*

Main category: cs.CL

TL;DR: First study of Arabic Knowledge Editing (KE) evaluates four methods on Arabic benchmarks, showing parameter-based methods struggle cross-lingually, while instruction-tuned ones perform better. LTE extended to multilingual settings improves editability and transfer.


<details>
  <summary>Details</summary>
Motivation: Explore KE behavior in morphologically rich languages like Arabic, underexamined compared to English.

Method: Evaluate ROME, MEMIT, ICE, and LTE on Arabic translations of ZsRE and Counterfact benchmarks, testing multilingual and cross-lingual settings on Llama-2-7B-chat. Extend LTE to multilingual training.

Result: Parameter-based methods struggle with cross-lingual generalization; instruction-tuned methods perform robustly. Joint Arabic-English training enhances LTE's editability and transfer.

Conclusion: Arabic KE benchmarks and multilingual LTE training data released to aid future research, highlighting the need for robust KE methods in morphologically rich languages.

Abstract: While Knowledge Editing (KE) has been widely explored in English, its
behavior in morphologically rich languages like Arabic remains underexamined.
In this work, we present the first study of Arabic KE. We evaluate four methods
(ROME, MEMIT, ICE, and LTE) on Arabic translations of the ZsRE and Counterfact
benchmarks, analyzing both multilingual and cross-lingual settings. Our
experiments on Llama-2-7B-chat show show that parameter-based methods struggle
with cross-lingual generalization, while instruction-tuned methods perform more
robustly. We extend Learning-To-Edit (LTE) to a multilingual setting and show
that joint Arabic-English training improves both editability and transfer. We
release Arabic KE benchmarks and multilingual training for LTE data to support
future research.

</details>


### [35] [Can Group Relative Policy Optimization Improve Thai Legal Reasoning and Question Answering?](https://arxiv.org/abs/2507.09638)
*Pawitsapak Akarajaradwong,Chompakorn Chaksangchaichot,Pirat Pothavorn,Attapol Thamrongrattanarit-Rutherford,Ekapol Chuangsuwanich,Sarana Nutanong*

Main category: cs.CL

TL;DR: The paper introduces Group-Relative Policy Optimization (GRPO) to improve Thai legal question answering by enhancing law citation accuracy and response quality in RAG systems, achieving significant gains in performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: The performance of Retrieval-Augmented Generation (RAG) systems on Thai legal question answering is limited, especially for complex legal reasoning tasks.

Method: The approach uses GRPO with BGE-M3 embeddings for semantic-similarity rewards, reducing computational costs by 2.5x compared to LLM judges.

Result: Experiments on NitiBench show 90% citation-F1 gains and a 31% increase in joint quality metrics over instruction tuning, with improved robustness on complex tasks.

Conclusion: GRPO provides an effective, resource-efficient solution for enhancing Thai legal LLMs, outperforming instruction tuning in accuracy and efficiency.

Abstract: The Retrieval-Augmented Generation (RAG) systems' performance on Thai legal
question answering is still limited, especially for questions requiring
extensive, complex legal reasoning. To address these limitations, we introduce
an approach aligning LLMs toward improved law citation accuracy and better
response quality using Group-Relative Policy Optimization (GRPO). Our approach
leverages BGE-M3 embeddings as a cost-efficient semantic-similarity reward,
significantly reducing computational expenses up to 2.5x compared to large
language model judges. Experiments on the NitiBench benchmark demonstrate
substantial improvements: GRPO achieves up to 90% citation-F1 gains from the
base model and a 31% increase in joint quality metrics over instruction tuning.
Crucially, our method shows enhanced robustness on complex legal reasoning
tasks compared to instruction tuning, providing an effective and
resource-efficient solution for enhancing Thai legal LLMs.

</details>


### [36] [MCEval: A Dynamic Framework for Fair Multilingual Cultural Evaluation of LLMs](https://arxiv.org/abs/2507.09701)
*Shulin Huang,Linyi Yang,Yue Zhang*

Main category: cs.CL

TL;DR: MCEval is a multilingual evaluation framework assessing cultural awareness and bias in large language models (LLMs) across 13 cultures and languages, revealing disparities and fairness issues.


<details>
  <summary>Details</summary>
Motivation: Address cultural biases and limited cross-cultural understanding in LLMs serving diverse global users.

Method: Uses dynamic cultural question construction, Counterfactual Rephrasing, and Confounder Rephrasing for causal analysis.

Result: Identifies performance disparities and fairness issues, showing language-culture alignment impacts cultural performance.

Conclusion: MCEval offers the first comprehensive multilingual cultural evaluation, providing deeper insights into LLMs' cultural understanding.

Abstract: Large language models exhibit cultural biases and limited cross-cultural
understanding capabilities, particularly when serving diverse global user
populations. We propose MCEval, a novel multilingual evaluation framework that
employs dynamic cultural question construction and enables causal analysis
through Counterfactual Rephrasing and Confounder Rephrasing. Our comprehensive
evaluation spans 13 cultures and 13 languages, systematically assessing both
cultural awareness and cultural bias across different linguistic scenarios. The
framework provides 39,897 cultural awareness instances and 17,940 cultural bias
instances. Experimental results reveal performance disparities across different
linguistic scenarios, demonstrating that optimal cultural performance is not
only linked to training data distribution, but also is related to
language-culture alignment. The evaluation results also expose the fairness
issue, where approaches appearing successful in the English scenario create
substantial disadvantages. MCEval represents the first comprehensive
multilingual cultural evaluation framework that provides deeper insights into
LLMs' cultural understanding.

</details>


### [37] [Large Language Models Encode Semantics in Low-Dimensional Linear Subspaces](https://arxiv.org/abs/2507.09709)
*Baturay Saglam,Paul Kassianik,Blaine Nelson,Sajana Weerawardhena,Yaron Singer,Amin Karbasi*

Main category: cs.CL

TL;DR: The paper investigates how high-level semantic information is organized in the latent space of large language models (LLMs), revealing consistent low-dimensional subspaces that enable linear separability across domains.


<details>
  <summary>Details</summary>
Motivation: Understanding the latent space geometry of LLMs is crucial for interpreting their behavior and improving alignment, but it's unclear how semantic representations are internally organized.

Method: A large-scale empirical study of hidden states in 11 decoder-only transformer-based LLMs, analyzing 6 scientific topics and 12 layers each.

Result: High-level semantic information forms linearly separable representations in low-dimensional subspaces, especially in deeper layers and under structured reasoning prompts. This enables simple causal interventions, like capturing reasoning patterns with single vector directions.

Conclusion: The findings support geometry-aware tools for detecting and mitigating harmful content, demonstrated by a lightweight MLP classifier acting as a latent-space guardrail.

Abstract: Understanding the latent space geometry of large language models (LLMs) is
key to interpreting their behavior and improving alignment. \baturay{However,
it remains unclear to what extent LLMs internally organize representations
related to semantic understanding. To investigate this, we conduct a
large-scale empirical study of hidden states in transformer-based LLMs,
analyzing 11 decoder-only models across 6 scientific topics and 12 layers each.
We find that high-level semantic information consistently lies in
low-dimensional subspaces that form linearly separable representations across
distinct domains. This separability becomes more pronounced in deeper layers
and under prompts that trigger structured reasoning or alignment
behaviors$\unicode{x2013}$even when surface content is unchanged. This geometry
enables simple yet effective causal interventions in hidden space; for example,
reasoning patterns like chain-of-thought can be captured by a single vector
direction. Together, these findings support the development of geometry-aware
tools that operate directly on latent representations to detect and mitigate
harmful or adversarial content, using methods such as transport-based defenses
that leverage this separability. As a proof of concept, we demonstrate this
potential by training a simple MLP classifier as a lightweight latent-space
guardrail, which detects adversarial and malicious prompts with high precision.

</details>


### [38] [Your Pretrained Model Tells the Difficulty Itself: A Self-Adaptive Curriculum Learning Paradigm for Natural Language Understanding](https://arxiv.org/abs/2507.09758)
*Qi Feng,Yihong Liu,Hinrich Schtze*

Main category: cs.CL

TL;DR: A self-adaptive curriculum learning method for NLP uses PLM-predicted difficulty scores to order examples, outperforming random sampling in convergence and performance.


<details>
  <summary>Details</summary>
Motivation: Existing curriculum learning relies on manual difficulty metrics, which may not align with the model's perspective.

Method: Uses PLMs to predict difficulty scores and explores training strategies like easy-to-hard, hard-to-easy, and mixed sampling.

Result: Faster convergence and improved performance on four NLU datasets.

Conclusion: Self-adaptive curriculum learning with PLM-based difficulty scoring is effective for NLP tasks.

Abstract: Curriculum learning is a widely adopted training strategy in natural language
processing (NLP), where models are exposed to examples organized by increasing
difficulty to enhance learning efficiency and performance. However, most
existing approaches rely on manually defined difficulty metrics -- such as text
length -- which may not accurately reflect the model's own perspective. To
overcome this limitation, we present a self-adaptive curriculum learning
paradigm that prioritizes fine-tuning examples based on difficulty scores
predicted by pre-trained language models (PLMs) themselves. Building on these
scores, we explore various training strategies that differ in the ordering of
examples for the fine-tuning: from easy-to-hard, hard-to-easy, to mixed
sampling. We evaluate our method on four natural language understanding (NLU)
datasets covering both binary and multi-class classification tasks.
Experimental results show that our approach leads to faster convergence and
improved performance compared to standard random sampling.

</details>


### [39] [Te Ahorr Un Click: A Revised Definition of Clickbait and Detection in Spanish News](https://arxiv.org/abs/2507.09777)
*Gabriel Mordecki,Guillermo Moncecchi,Javier Couto*

Main category: cs.CL

TL;DR: The paper redefines clickbait as headlines omitting information to spark curiosity, introduces a refined dataset creation method, and releases TA1C, a Spanish clickbait dataset with strong baseline results.


<details>
  <summary>Details</summary>
Motivation: To clarify the definition of clickbait and reduce subjectivity in detection by focusing on the curiosity gap.

Method: Proposes a new definition of clickbait, refines dataset creation criteria, and releases TA1C, a manually annotated Spanish dataset.

Result: Created TA1C with 3,500 tweets, achieving 0.825 Fleiss' K agreement. Baseline models scored 0.84 F1.

Conclusion: The refined definition and dataset improve clickbait detection, with TA1C serving as a valuable resource for Spanish language research.

Abstract: We revise the definition of clickbait, which lacks current consensus, and
argue that the creation of a curiosity gap is the key concept that
distinguishes clickbait from other related phenomena such as sensationalism and
headlines that do not deliver what they promise or diverge from the article.
Therefore, we propose a new definition: clickbait is a technique for generating
headlines and teasers that deliberately omit part of the information with the
goal of raising the readers' curiosity, capturing their attention and enticing
them to click. We introduce a new approach to clickbait detection datasets
creation, by refining the concept limits and annotations criteria, minimizing
the subjectivity in the decision as much as possible. Following it, we created
and release TA1C (for Te Ahorr\'e Un Click, Spanish for Saved You A Click), the
first open source dataset for clickbait detection in Spanish. It consists of
3,500 tweets coming from 18 well known media sources, manually annotated and
reaching a 0.825 Fleiss' K inter annotator agreement. We implement strong
baselines that achieve 0.84 in F1-score.

</details>


### [40] [Function Induction and Task Generalization: An Interpretability Study with Off-by-One Addition](https://arxiv.org/abs/2507.09875)
*Qinyuan Ye,Robin Jia,Xiang Ren*

Main category: cs.CL

TL;DR: The paper investigates how large language models generalize to unseen tasks via in-context learning, using off-by-one addition as a case study. It reveals a function induction mechanism, parallel attention heads governing the +1 function, and broader reuse of this mechanism.


<details>
  <summary>Details</summary>
Motivation: To understand the internal mechanisms enabling task-level generalization in large language models, focusing on a counterfactual task (off-by-one addition).

Method: Uses circuit-style interpretability techniques like path patching to analyze model computations.

Result: Uncovered a function induction mechanism, parallel attention heads for the +1 function, and broader reuse of this mechanism in other tasks.

Conclusion: Provides insights into reusable and composable structures in language models that enable task-level generalization.

Abstract: Large language models demonstrate the intriguing ability to perform unseen
tasks via in-context learning. However, it remains unclear what mechanisms
inside the model drive such task-level generalization. In this work, we
approach this question through the lens of off-by-one addition (i.e., 1+1=3,
2+2=5, 3+3=?), a two-step, counterfactual task with an unexpected +1 function
as a second step. Leveraging circuit-style interpretability techniques such as
path patching, we analyze the models' internal computations behind their
notable performance and present three key findings. First, we uncover a
function induction mechanism that explains the model's generalization from
standard addition to off-by-one addition. This mechanism resembles the
structure of the induction head mechanism found in prior work and elevates it
to a higher level of abstraction. Second, we show that the induction of the +1
function is governed by multiple attention heads in parallel, each of which
emits a distinct piece of the +1 function. Finally, we find that this function
induction mechanism is reused in a broader range of tasks, including synthetic
tasks such as shifted multiple-choice QA and algorithmic tasks such as base-8
addition. Overall, our findings offer deeper insights into how reusable and
composable structures within language models enable task-level generalization.

</details>


### [41] [Enhancing Retrieval Augmented Generation with Hierarchical Text Segmentation Chunking](https://arxiv.org/abs/2507.09935)
*Hai Toan Nguyen,Tien Dat Nguyen,Viet Ha Nguyen*

Main category: cs.CL

TL;DR: The paper introduces a framework for Retrieval-Augmented Generation (RAG) systems that improves chunking by using hierarchical text segmentation and clustering, leading to better semantic coherence and retrieval accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional chunking methods in RAG systems often lack semantic meaning due to ignoring textual structure, limiting retrieval effectiveness.

Method: The proposed framework integrates hierarchical text segmentation and clustering to create semantically coherent chunks, using segment-level and cluster-level vector representations for retrieval.

Result: Evaluations on NarrativeQA, QuALITY, and QASPER datasets show improved performance over traditional chunking techniques.

Conclusion: The framework enhances RAG systems by producing more meaningful chunks, improving retrieval precision and contextual relevance.

Abstract: Retrieval-Augmented Generation (RAG) systems commonly use chunking strategies
for retrieval, which enhance large language models (LLMs) by enabling them to
access external knowledge, ensuring that the retrieved information is
up-to-date and domain-specific. However, traditional methods often fail to
create chunks that capture sufficient semantic meaning, as they do not account
for the underlying textual structure. This paper proposes a novel framework
that enhances RAG by integrating hierarchical text segmentation and clustering
to generate more meaningful and semantically coherent chunks. During inference,
the framework retrieves information by leveraging both segment-level and
cluster-level vector representations, thereby increasing the likelihood of
retrieving more precise and contextually relevant information. Evaluations on
the NarrativeQA, QuALITY, and QASPER datasets indicate that the proposed method
achieved improved results compared to traditional chunking techniques.

</details>


### [42] [Tiny Reward Models](https://arxiv.org/abs/2507.09973)
*Sarah Pan*

Main category: cs.CL

TL;DR: TinyRM, a small bidirectional masked language model (400M parameters), rivals larger models in reasoning and safety preference tasks using efficient tuning strategies like FLAN-style prompting and DoRA.


<details>
  <summary>Details</summary>
Motivation: Address the high inference costs of large decoder-based reward models in RLHF by proposing smaller, efficient alternatives.

Method: Combines FLAN-style prompting, Directional Low-Rank Adaptation (DoRA), and layer freezing to optimize performance with fewer resources.

Result: TinyRM matches or outperforms models 175x larger on RewardBench, showing effectiveness in reasoning and safety preference tasks.

Conclusion: Lightweight bidirectional architectures like TinyRM offer scalable, efficient alternatives for preference modeling, though challenges remain in generalist and conversational tasks.

Abstract: Large decoder-based language models have become the dominant architecture for
reward modeling in reinforcement learning from human feedback (RLHF). However,
as reward models are increasingly deployed in test-time strategies, their
inference costs become a growing concern. We present TinyRM, a family of small,
bidirectional masked language models (MLMs) with as few as 400 million
parameters, that rival the capabilities of models over 175 times larger on
reasoning and safety preference modeling tasks. TinyRM combines FLAN-style
prompting, Directional Low-Rank Adaptation (DoRA), and layer freezing to
achieve strong performance on RewardBench, despite using significantly fewer
resources. Our experiments suggest that small models benefit from
domain-specific tuning strategies, particularly in reasoning, where lightweight
finetuning methods are especially effective. While challenges remain in
building generalist models and conversational preference modeling, our
preliminary results highlight the promise of lightweight bidirectional
architectures as efficient, scalable alternatives for preference modeling.

</details>


### [43] [TextOmics-Guided Diffusion for Hit-like Molecular Generation](https://arxiv.org/abs/2507.09982)
*Hang Yuan,Chen Li,Wenjun Ma,Yuncheng Jiang*

Main category: cs.CL

TL;DR: TextOmics introduces a benchmark for aligning omics expressions with molecular textual descriptions, and ToDi, a generative framework, outperforms existing methods in producing hit-like molecules.


<details>
  <summary>Details</summary>
Motivation: The lack of heterogeneous data and unified frameworks for integrating diverse molecular representations in drug discovery motivates the creation of TextOmics and ToDi.

Method: TextOmics provides a dataset aligning omics expressions with textual descriptions. ToDi uses two encoders (OmicsEn and TextEn) and conditional diffusion (DiffGen) for controllable molecular generation.

Result: ToDi outperforms state-of-the-art methods and shows strong potential in zero-shot therapeutic molecular generation.

Conclusion: TextOmics and ToDi bridge gaps in molecular generation, offering a robust framework for target-specific drug discovery.

Abstract: Hit-like molecular generation with therapeutic potential is essential for
target-specific drug discovery. However, the field lacks heterogeneous data and
unified frameworks for integrating diverse molecular representations. To bridge
this gap, we introduce TextOmics, a pioneering benchmark that establishes
one-to-one correspondences between omics expressions and molecular textual
descriptions. TextOmics provides a heterogeneous dataset that facilitates
molecular generation through representations alignment. Built upon this
foundation, we propose ToDi, a generative framework that jointly conditions on
omics expressions and molecular textual descriptions to produce biologically
relevant, chemically valid, hit-like molecules. ToDi leverages two encoders
(OmicsEn and TextEn) to capture multi-level biological and semantic
associations, and develops conditional diffusion (DiffGen) for controllable
generation. Extensive experiments confirm the effectiveness of TextOmics and
demonstrate ToDi outperforms existing state-of-the-art approaches, while also
showcasing remarkable potential in zero-shot therapeutic molecular generation.
Sources are available at: https://github.com/hala-ToDi.

</details>


### [44] [Protective Factor-Aware Dynamic Influence Learning for Suicide Risk Prediction on Social Media](https://arxiv.org/abs/2507.10008)
*Jun Li,Xiangmeng Wang,Haoyang Li,Yifei Yan,Hong Va Leong,Ling Feng,Nancy Xiaonan Yu,Qing Li*

Main category: cs.CL

TL;DR: The paper proposes a framework for predicting suicide risk over time by integrating both risk and protective factors, outperforming existing models and offering interpretable insights for clinicians.


<details>
  <summary>Details</summary>
Motivation: Existing models focus only on risk factors and fail to capture dynamic mental state transitions or protective factors, which are crucial for accurate suicide risk prediction.

Method: The study introduces a Protective Factor-Aware Dataset from Reddit posts and a Dynamic Factors Influence Learning approach to model the varying impact of risk and protective factors.

Result: The proposed model significantly outperforms state-of-the-art models and large language models across three datasets, providing interpretable weights for clinical use.

Conclusion: The framework advances suicide risk prediction by dynamically incorporating both risk and protective factors, aiding in targeted interventions and better understanding of suicidal patterns.

Abstract: Suicide is a critical global health issue that requires urgent attention.
Even though prior work has revealed valuable insights into detecting current
suicide risk on social media, little attention has been paid to developing
models that can predict subsequent suicide risk over time, limiting their
ability to capture rapid fluctuations in individuals' mental state transitions.
In addition, existing work ignores protective factors that play a crucial role
in suicide risk prediction, focusing predominantly on risk factors alone.
Protective factors such as social support and coping strategies can mitigate
suicide risk by moderating the impact of risk factors. Therefore, this study
proposes a novel framework for predicting subsequent suicide risk by jointly
learning the dynamic influence of both risk factors and protective factors on
users' suicide risk transitions. We propose a novel Protective Factor-Aware
Dataset, which is built from 12 years of Reddit posts along with comprehensive
annotations of suicide risk and both risk and protective factors. We also
introduce a Dynamic Factors Influence Learning approach that captures the
varying impact of risk and protective factors on suicide risk transitions,
recognizing that suicide risk fluctuates over time according to established
psychological theories. Our thorough experiments demonstrate that the proposed
model significantly outperforms state-of-the-art models and large language
models across three datasets. In addition, the proposed Dynamic Factors
Influence Learning provides interpretable weights, helping clinicians better
understand suicidal patterns and enabling more targeted intervention
strategies.

</details>


### [45] [GeLaCo: An Evolutionary Approach to Layer Compression](https://arxiv.org/abs/2507.10059)
*David Ponce,Thierry Etchegoyhen,Javier Del Ser*

Main category: cs.CL

TL;DR: GeLaCo introduces an evolutionary approach for compressing Large Language Models (LLMs) via layer collapse, outperforming existing methods in efficiency and quality.


<details>
  <summary>Details</summary>
Motivation: LLMs face deployment barriers due to high computational demands; model compression is needed but current methods are costly and may miss optimal solutions.

Method: GeLaCo uses population-based search and a similarity fitness function to explore compression solutions, supporting single and multi-objective optimization.

Result: GeLaCo outperforms state-of-the-art alternatives in perplexity-based and generative evaluations.

Conclusion: GeLaCo provides an efficient and effective solution for LLM compression, establishing a Pareto frontier for trade-offs between compression and quality.

Abstract: Large Language Models (LLM) have achieved remarkable performance across a
large number of tasks, but face critical deployment and usage barriers due to
substantial computational requirements. Model compression methods, which aim to
reduce model size while preserving its capacity, are an important means to
mitigate these issues. Promising approaches along these lines, such as
structured pruning, typically require costly empirical search for optimal
variants and may run the risk of ignoring better solutions. In this work we
introduce GeLaCo, an evolutionary approach to LLM compression via layer
collapse. Our approach supports an efficient exploration of the compression
solution space via population-based search and a module-wise similarity fitness
function capturing attention, feed-forward, and hidden state representations.
GeLaCo also supports both single and multi-objective evolutionary compression
search, establishing the first Pareto frontier along compression and quality
axes. We evaluate GeLaCo solutions via both perplexity-based and generative
evaluations over foundational and instruction-tuned models, outperforming
state-of-the-art alternatives.

</details>


### [46] [Cultural Bias in Large Language Models: Evaluating AI Agents through Moral Questionnaires](https://arxiv.org/abs/2507.10073)
*Simon Mnker*

Main category: cs.CL

TL;DR: LLMs homogenize moral diversity, failing to represent culturally-specific human values despite their linguistic capabilities.


<details>
  <summary>Details</summary>
Motivation: To investigate whether AI systems, particularly LLMs, accurately represent diverse human moral values or merely average them.

Method: Applied the Moral Foundations Questionnaire across 19 cultural contexts, comparing LLMs' outputs to human baseline data.

Result: LLMs systematically homogenize moral diversity; increased model size doesn't improve cultural representation.

Conclusion: Current AI alignment approaches lack nuanced cultural representation, necessitating better alignment objectives and metrics.

Abstract: Are AI systems truly representing human values, or merely averaging across
them? Our study suggests a concerning reality: Large Language Models (LLMs)
fail to represent diverse cultural moral frameworks despite their linguistic
capabilities. We expose significant gaps between AI-generated and human moral
intuitions by applying the Moral Foundations Questionnaire across 19 cultural
contexts. Comparing multiple state-of-the-art LLMs' origins against human
baseline data, we find these models systematically homogenize moral diversity.
Surprisingly, increased model size doesn't consistently improve cultural
representation fidelity. Our findings challenge the growing use of LLMs as
synthetic populations in social science research and highlight a fundamental
limitation in current AI alignment approaches. Without data-driven alignment
beyond prompting, these systems cannot capture the nuanced, culturally-specific
moral intuitions. Our results call for more grounded alignment objectives and
evaluation metrics to ensure AI systems represent diverse human values rather
than flattening the moral landscape.

</details>


### [47] [Enhancing Chain-of-Thought Reasoning with Critical Representation Fine-tuning](https://arxiv.org/abs/2507.10085)
*Chenxi Huang,Shaotian Yan,Liang Xie,Binbin Lin,Sinan Fan,Yue Xin,Deng Cai,Chen Shen,Jieping Ye*

Main category: cs.CL

TL;DR: CRFT improves reasoning tasks by dynamically optimizing critical representations identified via information flow analysis, outperforming native ReFT and traditional PEFT methods.


<details>
  <summary>Details</summary>
Motivation: Native ReFT's fixed-position representation modifications are suboptimal for complex reasoning tasks, prompting the need to identify and fine-tune critical representations.

Method: CRFT identifies critical representations through information flow analysis and optimizes them in a low-rank subspace while freezing the base model.

Result: Validated on eight benchmarks, CRFT enhances reasoning performance, including a 16.4% boost in one-shot accuracy.

Conclusion: CRFT offers a lightweight, powerful alternative to traditional PEFT methods, unlocking representation-level optimization potential for reasoning tasks.

Abstract: Representation Fine-tuning (ReFT), a recently proposed Parameter-Efficient
Fine-Tuning (PEFT) method, has attracted widespread attention for significantly
improving parameter efficiency by editing representation space alone. In this
work, we investigate applying ReFT to complex reasoning tasks. However,
directly using the native ReFT method, which modifies fixed representations at
the beginning and end of each layer, yields suboptimal performance, as these
fixed-position representations have uncertain impact on the outputs. We observe
that, in complex reasoning tasks, there often exist certain critical
representations. These representations either integrate significant information
from preceding layers or regulate subsequent layer representations. Through
layer-by-layer propagation, they exert a substantial influence on the final
output. Naturally, fine-tuning these critical representations has the potential
to greatly enhance reasoning performance. Building upon these insights, we
propose Critical Representation Fine-Tuning (CRFT), a novel method that
identifies and optimizes these critical representations through information
flow analysis. CRFT operates within a supervised learning framework,
dynamically optimizing critical representations in a low-rank linear subspace
while freezing the base model. The effectiveness and efficiency of our method
are validated across eight benchmarks for arithmetic and commonsense reasoning,
using LLaMA and Mistral model families. Furthermore, our method also adapts
effectively to few-shot settings, boosting one-shot accuracy by 16.4%. Our work
highlights the untapped potential of representation-level optimization for CoT
reasoning, offering a lightweight yet powerful alternative to traditional PEFT
methods.

</details>


### [48] [Fusing Large Language Models with Temporal Transformers for Time Series Forecasting](https://arxiv.org/abs/2507.10098)
*Chen Su,Yuanhe Tian,Qinyu Liu,Jun Zhang,Yan Song*

Main category: cs.CL

TL;DR: A novel Transformer-based architecture integrates LLMs and vanilla Transformers for time series forecasting, combining semantic and temporal patterns for improved accuracy.


<details>
  <summary>Details</summary>
Motivation: LLMs excel in text tasks but underperform in time series forecasting due to gaps between text and numerical data. The goal is to leverage LLMs' semantic strengths while addressing their limitations.

Method: A hybrid model fuses representations from LLMs (for semantic patterns) and vanilla Transformers (for temporal dynamics) to create a richer input for forecasting.

Result: Experiments show the hybrid model outperforms standalone LLMs and vanilla Transformers in accuracy.

Conclusion: The proposed architecture effectively combines semantic and temporal representations, enhancing time series forecasting performance.

Abstract: Recently, large language models (LLMs) have demonstrated powerful
capabilities in performing various tasks and thus are applied by recent studies
to time series forecasting (TSF) tasks, which predict future values with the
given historical time series. Existing LLM-based approaches transfer knowledge
learned from text data to time series prediction using prompting or fine-tuning
strategies. However, LLMs are proficient at reasoning over discrete tokens and
semantic patterns but are not initially designed to model continuous numerical
time series data. The gaps between text and time series data lead LLMs to
achieve inferior performance to a vanilla Transformer model that is directly
trained on TSF data. However, the vanilla Transformers often struggle to learn
high-level semantic patterns. In this paper, we design a novel
Transformer-based architecture that complementarily leverages LLMs and vanilla
Transformers, so as to integrate the high-level semantic representations
learned by LLMs into the temporal information encoded by time series
Transformers, where a hybrid representation is obtained by fusing the
representations from the LLM and the Transformer. The resulting fused
representation contains both historical temporal dynamics and semantic
variation patterns, allowing our model to predict more accurate future values.
Experiments on benchmark datasets demonstrate the effectiveness of the proposed
approach.

</details>


### [49] [Task-Based Flexible Feature Distillation for LLMs](https://arxiv.org/abs/2507.10155)
*Khouloud Saadi,Di Wang*

Main category: cs.CL

TL;DR: Proposes a task-based feature distillation method for transferring knowledge between teacher and student LLMs with different hidden sizes, avoiding extra parameters and improving performance.


<details>
  <summary>Details</summary>
Motivation: Traditional feature KD methods limit student architecture flexibility by requiring matching hidden sizes, and linear projectors degrade performance.

Method: Identifies task-relevant hidden units in the teacher and distills their activations to the student without new parameters.

Result: Achieves up to 3% performance gain over baselines in tasks like classification, instruction-following, and summarization.

Conclusion: The method is flexible, parameter-free, and outperforms prior approaches, enhancing knowledge distillation for LLMs.

Abstract: Knowledge Distillation (KD) in general and feature distillation in particular
are promising techniques for reducing the high computational demand of large
language models (LLMs). However, traditional feature KD methods typically
assume that the teacher and the student share the same hidden size, limiting
the flexibility of the student's architecture. A common solution to this
problem involves training a linear projector to align their feature spaces, but
this introduces additional parameters that must be learned from scratch and
often degrades performance on downstream tasks, especially in generative
settings. To address this issue, in this work, we propose a novel task-based
feature distillation method that enables knowledge transfer between teacher and
student models with different hidden layer dimensions, without introducing any
new parameters. Leveraging the insight that only a subset of LLM components
contribute significantly to a specific downstream task, our approach identifies
the most task-relevant hidden units in the teacher and directly distills their
activations to the student. Our method is flexible and easily integrates with
other distillation frameworks. Empirical results show consistent improvements
over prior approaches across diverse tasks, including classification,
instruction-following, and summarization, achieving up to a 3\% performance
gain over the linear projection baseline.

</details>


### [50] [Abusive text transformation using LLMs](https://arxiv.org/abs/2507.10177)
*Rohitash Chandra,Jiyong Choi*

Main category: cs.CL

TL;DR: The study explores using LLMs like Gemini, GPT-4o, DeepSeek, and Groq to transform abusive text into non-abusive versions while preserving intent, sentiment, and semantics. Groq's results differ significantly from others, while GPT-4o and DeepSeek-V3 show similarities.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' capability in classifying and transforming abusive text (e.g., hate speech, swear words) into non-abusive versions without losing the original intent.

Method: Evaluate LLMs (Gemini, GPT-4o, DeepSeek, Groq) on abusive text transformation. Analyze raw and transformed datasets using sentiment and semantic analysis.

Result: Groq's performance differs notably from other models, while GPT-4o and DeepSeek-V3 exhibit similar results.

Conclusion: LLMs can effectively transform abusive text, but performance varies, with Groq standing out as an outlier compared to GPT-4o and DeepSeek-V3.

Abstract: Although Large Language Models (LLMs) have demonstrated significant
advancements in natural language processing tasks, their effectiveness in the
classification and transformation of abusive text into non-abusive versions
remains an area for exploration. In this study, we aim to use LLMs to transform
abusive text (tweets and reviews) featuring hate speech and swear words into
non-abusive text, while retaining the intent of the text. We evaluate the
performance of two state-of-the-art LLMs, such as Gemini, GPT-4o, DeekSeek and
Groq, on their ability to identify abusive text. We them to transform and
obtain a text that is clean from abusive and inappropriate content but
maintains a similar level of sentiment and semantics, i.e. the transformed text
needs to maintain its message. Afterwards, we evaluate the raw and transformed
datasets with sentiment analysis and semantic analysis. Our results show Groq
provides vastly different results when compared with other LLMs. We have
identified similarities between GPT-4o and DeepSeek-V3.

</details>


### [51] [Absher: A Benchmark for Evaluating Large Language Models Understanding of Saudi Dialects](https://arxiv.org/abs/2507.10216)
*Renad Al-Monef,Hassan Alhuzali,Nora Alturayeif,Ashwag Alasmari*

Main category: cs.CL

TL;DR: The paper introduces Absher, a benchmark for evaluating LLMs on Saudi dialects, revealing performance gaps in cultural and contextual tasks.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' understanding of regional dialects and cultural nuances in Arabic NLP, especially in diverse settings like Saudi Arabia.

Method: Developed Absher, a benchmark with 18,000+ questions across six categories, evaluating multilingual and Arabic-specific LLMs.

Result: Notable performance gaps in cultural inference and contextual understanding tasks.

Conclusion: Highlights the need for dialect-aware training and culturally aligned evaluation methods for LLMs in Arabic NLP.

Abstract: As large language models (LLMs) become increasingly central to Arabic NLP
applications, evaluating their understanding of regional dialects and cultural
nuances is essential, particularly in linguistically diverse settings like
Saudi Arabia. This paper introduces \texttt{Absher}, a comprehensive benchmark
specifically designed to assess LLMs performance across major Saudi dialects.
\texttt{Absher} comprises over 18,000 multiple-choice questions spanning six
distinct categories: Meaning, True/False, Fill-in-the-Blank, Contextual Usage,
Cultural Interpretation, and Location Recognition. These questions are derived
from a curated dataset of dialectal words, phrases, and proverbs sourced from
various regions of Saudi Arabia. We evaluate several state-of-the-art LLMs,
including multilingual and Arabic-specific models. We also provide detailed
insights into their capabilities and limitations. Our results reveal notable
performance gaps, particularly in tasks requiring cultural inference or
contextual understanding. Our findings highlight the urgent need for
dialect-aware training and culturally aligned evaluation methodologies to
improve LLMs performance in real-world Arabic applications.

</details>


### [52] [Grammar-Guided Evolutionary Search for Discrete Prompt Optimisation](https://arxiv.org/abs/2507.10326)
*Muzhaffar Hazman,Minh-Khoi Pham,Shweta Soundararajan,Goncalo Mordido,Leonardo Custode,David Lynch,Giorgio Cruciata,Yucheng Shi,Hongmeng Song,Wang Chao,Pan Yue,Aleksandar Milenovic,Alexandros Agapitos*

Main category: cs.CL

TL;DR: An evolutionary search approach for automated discrete prompt optimization outperforms state-of-the-art methods on smaller LLMs for complex tasks.


<details>
  <summary>Details</summary>
Motivation: Current prompt engineering methods focus on large LLMs and simple tasks, neglecting smaller models and complex tasks requiring detailed prompts.

Method: A two-phase approach: grammar-guided genetic programming synthesizes prompt-creating programs, followed by local search for fine-tuning.

Result: Outperforms PromptWizard, OPRO, and RL-Prompt on smaller LLMs in challenging tasks, with minimal performance degradation.

Conclusion: The proposed method effectively optimizes prompts for smaller models in complex tasks, addressing gaps in existing approaches.

Abstract: Prompt engineering has proven to be a crucial step in leveraging pretrained
large language models (LLMs) in solving various real-world tasks. Numerous
solutions have been proposed that seek to automate prompt engineering by using
the model itself to edit prompts. However, the majority of state-of-the-art
approaches are evaluated on tasks that require minimal prompt templates and on
very large and highly capable LLMs. In contrast, solving complex tasks that
require detailed information to be included in the prompt increases the amount
of text that needs to be optimised. Furthermore, smaller models have been shown
to be more sensitive to prompt design. To address these challenges, we propose
an evolutionary search approach to automated discrete prompt optimisation
consisting of two phases. In the first phase, grammar-guided genetic
programming is invoked to synthesise prompt-creating programmes by searching
the space of programmes populated by function compositions of syntactic,
dictionary-based and LLM-based prompt-editing functions. In the second phase,
local search is applied to explore the neighbourhoods of best-performing
programmes in an attempt to further fine-tune their performance. Our approach
outperforms three state-of-the-art prompt optimisation approaches,
PromptWizard, OPRO, and RL-Prompt, on three relatively small general-purpose
LLMs in four domain-specific challenging tasks. We also illustrate several
examples where these benchmark methods suffer relatively severe performance
degradation, while our approach improves performance in almost all task-model
combinations, only incurring minimal degradation when it does not.

</details>


### [53] [Bridging Robustness and Generalization Against Word Substitution Attacks in NLP via the Growth Bound Matrix Approach](https://arxiv.org/abs/2507.10330)
*Mohammed Bouri,Adnane Saoud*

Main category: cs.CL

TL;DR: The paper introduces Growth Bound Matrices (GBM) to enhance NLP model robustness against adversarial attacks, focusing on LSTM, S4, and CNN architectures, achieving up to 8.8% improvement.


<details>
  <summary>Details</summary>
Motivation: Despite NLP advancements, models are vulnerable to adversarial attacks, especially recurrent networks and SSMs like S4, which lack robustness studies.

Method: A novel GBM-based regularization technique is proposed to mitigate input perturbation effects on model outputs, tested on LSTM, S4, and CNN.

Result: Experiments show an 8.8% robustness improvement over baselines, with enhanced resilience to word substitution attacks and better clean-text generalization.

Conclusion: GBM effectively boosts adversarial robustness, outperforming state-of-the-art methods, and provides the first systematic analysis of S4 robustness.

Abstract: Despite advancements in Natural Language Processing (NLP), models remain
vulnerable to adversarial attacks, such as synonym substitutions. While prior
work has focused on improving robustness for feed-forward and convolutional
architectures, the robustness of recurrent networks and modern state space
models (SSMs), such as S4, remains understudied. These architectures pose
unique challenges due to their sequential processing and complex parameter
dynamics. In this paper, we introduce a novel regularization technique based on
Growth Bound Matrices (GBM) to improve NLP model robustness by reducing the
impact of input perturbations on model outputs. We focus on computing the GBM
for three architectures: Long Short-Term Memory (LSTM), State Space models
(S4), and Convolutional Neural Networks (CNN). Our method aims to (1) enhance
resilience against word substitution attacks, (2) improve generalization on
clean text, and (3) providing the first systematic analysis of SSM (S4)
robustness. Extensive experiments across multiple architectures and benchmark
datasets demonstrate that our method improves adversarial robustness by up to
8.8% over existing baselines. These results highlight the effectiveness of our
approach, outperforming several state-of-the-art methods in adversarial
defense. Codes are available at https://github.com/BouriMohammed/GBM

</details>


### [54] [Using AI to replicate human experimental results: a motion study](https://arxiv.org/abs/2507.10342)
*Rosa Illan Castillo,Javier Valenzuela*

Main category: cs.CL

TL;DR: The paper demonstrates that large language models (LLMs) like GPT-4 can reliably replicate human judgments in linguistic research, particularly in affective meanings of temporal expressions, with strong correlations between human and AI responses.


<details>
  <summary>Details</summary>
Motivation: To assess whether LLMs can serve as credible tools in linguistic research by replicating nuanced human judgments, especially in affective meanings of temporal expressions.

Method: Four psycholinguistic studies were conducted with human participants and replicated using an LLM, comparing responses in tasks like emergent meanings, valence shifts, verb choice, and sentence-emoji associations.

Result: Strong correlations (Spearman's rho = .73-.96) between human and AI responses were found, with minor divergences not affecting interpretative outcomes.

Conclusion: LLMs can augment human-based linguistic research, offering scalable and valid alternatives while supporting hypothesis generation and data expansion.

Abstract: This paper explores the potential of large language models (LLMs) as reliable
analytical tools in linguistic research, focusing on the emergence of affective
meanings in temporal expressions involving manner-of-motion verbs. While LLMs
like GPT-4 have shown promise across a range of tasks, their ability to
replicate nuanced human judgements remains under scrutiny. We conducted four
psycholinguistic studies (on emergent meanings, valence shifts, verb choice in
emotional contexts, and sentence-emoji associations) first with human
participants and then replicated the same tasks using an LLM. Results across
all studies show a striking convergence between human and AI responses, with
statistical analyses (e.g., Spearman's rho = .73-.96) indicating strong
correlations in both rating patterns and categorical choices. While minor
divergences were observed in some cases, these did not alter the overall
interpretative outcomes. These findings offer compelling evidence that LLMs can
augment traditional human-based experimentation, enabling broader-scale studies
without compromising interpretative validity. This convergence not only
strengthens the empirical foundation of prior human-based findings but also
opens possibilities for hypothesis generation and data expansion through AI.
Ultimately, our study supports the use of LLMs as credible and informative
collaborators in linguistic inquiry.

</details>


### [55] [Meanings are like Onions: a Layered Approach to Metaphor Processing](https://arxiv.org/abs/2507.10354)
*Silvia Cappa,Anna Sofia Lippolis,Stefano Zoia*

Main category: cs.CL

TL;DR: A stratified model for metaphor processing is proposed, treating meaning as a multi-layered structure (content analysis, conceptual blending, pragmatic intentionality) to enhance computational metaphor interpretation.


<details>
  <summary>Details</summary>
Motivation: To address the complexity of metaphorical meaning beyond flat mappings, integrating cognitive and pragmatic dimensions for richer computational understanding.

Method: A three-dimensional framework: (1) content analysis, (2) conceptual blending, (3) pragmatic intentionality, unified into a formal model.

Result: The model enables deeper, context-sensitive metaphor interpretation in computational systems.

Conclusion: The stratified approach advances computational metaphor processing by unifying cognitive and pragmatic layers.

Abstract: Metaphorical meaning is not a flat mapping between concepts, but a complex
cognitive phenomenon that integrates multiple levels of interpretation. In this
paper, we propose a stratified model of metaphor processing that treats meaning
as an onion: a multi-layered structure comprising (1) content analysis, (2)
conceptual blending, and (3) pragmatic intentionality. This three-dimensional
framework allows for a richer and more cognitively grounded approach to
metaphor interpretation in computational systems. At the first level, metaphors
are annotated through basic conceptual elements. At the second level, we model
conceptual combinations, linking components to emergent meanings. Finally, at
the third level, we introduce a pragmatic vocabulary to capture speaker intent,
communicative function, and contextual effects, aligning metaphor understanding
with pragmatic theories. By unifying these layers into a single formal
framework, our model lays the groundwork for computational methods capable of
representing metaphorical meaning beyond surface associations, toward deeper,
more context-sensitive reasoning.

</details>


### [56] [From Sequence to Structure: Uncovering Substructure Reasoning in Transformers](https://arxiv.org/abs/2507.10435)
*Xinnan Dai,Kai Yang,Jay Revolinsky,Kai Guo,Aoran Wang,Bohang Zhang,Jiliang Tang*

Main category: cs.CL

TL;DR: The paper explores how decoder-only Transformers understand graph structures in text, introducing Induced Substructure Filtration (ISF) to explain substructure extraction and validating it in LLMs.


<details>
  <summary>Details</summary>
Motivation: To understand how decoder-only Transformers interpret graph structures embedded in text and perform substructure extraction tasks.

Method: Proposes ISF, a perspective for substructure identification in transformers, supported by empirical and theoretical analysis. Validates ISF in LLMs and explores broader graph-handling capabilities.

Result: Demonstrates that Transformers can extract substructures from attributed graphs (e.g., molecular graphs) and reveals consistent internal dynamics in LLMs.

Conclusion: Provides insights into how sequence-based Transformers perform substructure extraction, advancing understanding of their graph reasoning capabilities.

Abstract: Recent studies suggest that large language models (LLMs) possess the
capability to solve graph reasoning tasks. Notably, even when graph structures
are embedded within textual descriptions, LLMs can still effectively answer
related questions. This raises a fundamental question: How can a decoder-only
Transformer architecture understand underlying graph structures? To address
this, we start with the substructure extraction task, interpreting the inner
mechanisms inside the transformers and analyzing the impact of the input
queries. Specifically, through both empirical results and theoretical analysis,
we present Induced Substructure Filtration (ISF), a perspective that captures
the substructure identification in the multi-layer transformers. We further
validate the ISF process in LLMs, revealing consistent internal dynamics across
layers. Building on these insights, we explore the broader capabilities of
Transformers in handling diverse graph types. Specifically, we introduce the
concept of thinking in substructures to efficiently extract complex composite
patterns, and demonstrate that decoder-only Transformers can successfully
extract substructures from attributed graphs, such as molecular graphs.
Together, our findings offer a new insight on how sequence-based Transformers
perform the substructure extraction task over graph data.

</details>


### [57] [Referential ambiguity and clarification requests: comparing human and LLM behaviour](https://arxiv.org/abs/2507.10445)
*Chris Madge,Matthew Purver,Massimo Poesio*

Main category: cs.CL

TL;DR: The paper investigates LLMs' ability to ask clarification questions in task-oriented dialogues, comparing human and LLM behavior. It introduces a new corpus combining existing annotations and finds weak links between ambiguity and clarification questions, with notable differences between humans and LLMs.


<details>
  <summary>Details</summary>
Motivation: To understand how LLMs handle clarification questions in task-oriented dialogues and compare their behavior to humans, especially in cases of ambiguity.

Method: A new corpus is created by combining existing annotations from the Minecraft Dialogue Corpus. Human and LLM clarification question behaviors are analyzed and compared.

Result: Humans rarely ask clarification questions for referential ambiguity but often for task uncertainty, while LLMs do the opposite. Reasoning in LLMs increases question frequency and relevancy.

Conclusion: LLMs' ability to ask clarification questions is influenced by reasoning, but their behavior differs significantly from humans, suggesting room for improvement in handling ambiguity.

Abstract: In this work we examine LLMs' ability to ask clarification questions in
task-oriented dialogues that follow the asynchronous
instruction-giver/instruction-follower format. We present a new corpus that
combines two existing annotations of the Minecraft Dialogue Corpus -- one for
reference and ambiguity in reference, and one for SDRT including clarifications
-- into a single common format providing the necessary information to
experiment with clarifications and their relation to ambiguity. With this
corpus we compare LLM actions with original human-generated clarification
questions, examining how both humans and LLMs act in the case of ambiguity. We
find that there is only a weak link between ambiguity and humans producing
clarification questions in these dialogues, and low correlation between humans
and LLMs. Humans hardly ever produce clarification questions for referential
ambiguity, but often do so for task-based uncertainty. Conversely, LLMs produce
more clarification questions for referential ambiguity, but less so for task
uncertainty. We question if LLMs' ability to ask clarification questions is
predicated on their recent ability to simulate reasoning, and test this with
different reasoning approaches, finding that reasoning does appear to increase
question frequency and relevancy.

</details>


### [58] [From BERT to Qwen: Hate Detection across architectures](https://arxiv.org/abs/2507.10468)
*Ariadna Mon,Sal Fenollosa,Jon Lecumberri*

Main category: cs.CL

TL;DR: The study compares classic bidirectional transformer encoders and ultra-large autoregressive LLMs for hate-speech detection to see if larger models improve accuracy.


<details>
  <summary>Details</summary>
Motivation: Online platforms face challenges in balancing hate-speech detection with avoiding over-censorship, prompting the need for better models.

Method: Benchmarking both classic bidirectional transformer encoders and next-generation LLMs on curated datasets of online interactions labeled for hate speech.

Result: The study evaluates whether the increased scale of LLMs translates to better performance in real-world hate-speech detection.

Conclusion: The research aims to clarify if ultra-large LLMs outperform classic models in practical hate-speech detection tasks.

Abstract: Online platforms struggle to curb hate speech without over-censoring
legitimate discourse. Early bidirectional transformer encoders made big
strides, but the arrival of ultra-large autoregressive LLMs promises deeper
context-awareness. Whether this extra scale actually improves practical
hate-speech detection on real-world text remains unverified. Our study puts
this question to the test by benchmarking both model families, classic encoders
and next-generation LLMs, on curated corpora of online interactions for
hate-speech detection (Hate or No Hate).

</details>


### [59] [MLAR: Multi-layer Large Language Model-based Robotic Process Automation Applicant Tracking](https://arxiv.org/abs/2507.10472)
*Mohamed T. Younes,Omar Walid,Mai Hassan,Ali Hamdi*

Main category: cs.CL

TL;DR: MLAR, a novel RPA framework, enhances ATS using LLMs for efficient resume screening and candidate matching, outperforming leading RPA platforms in speed and accuracy.


<details>
  <summary>Details</summary>
Motivation: Address bottlenecks in traditional recruitment processes like resume screening and candidate shortlisting due to time and resource constraints.

Method: Uses LLMs in three layers: job posting extraction, resume parsing, and similarity matching, integrated into RPA pipelines for automation.

Result: Processes 2,400 resumes at 5.4 seconds per resume, reducing time by 16.9% and 17.1% compared to Automation Anywhere and UiPath, respectively.

Conclusion: MLAR offers an efficient, accurate, and scalable solution for modern recruitment workflows.

Abstract: This paper introduces an innovative Applicant Tracking System (ATS) enhanced
by a novel Robotic process automation (RPA) framework or as further referred to
as MLAR. Traditional recruitment processes often encounter bottlenecks in
resume screening and candidate shortlisting due to time and resource
constraints. MLAR addresses these challenges employing Large Language Models
(LLMs) in three distinct layers: extracting key characteristics from job
postings in the first layer, parsing applicant resume to identify education,
experience, skills in the second layer, and similarity matching in the third
layer. These features are then matched through advanced semantic algorithms to
identify the best candidates efficiently. Our approach integrates seamlessly
into existing RPA pipelines, automating resume parsing, job matching, and
candidate notifications. Extensive performance benchmarking shows that MLAR
outperforms the leading RPA platforms, including UiPath and Automation
Anywhere, in high-volume resume-processing tasks. When processing 2,400
resumes, MLAR achieved an average processing time of 5.4 seconds per resume,
reducing processing time by approximately 16.9% compared to Automation Anywhere
and 17.1% compared to UiPath. These results highlight the potential of MLAR to
transform recruitment workflows by providing an efficient, accurate, and
scalable solution tailored to modern hiring needs.

</details>


### [60] [Can You Detect the Difference?](https://arxiv.org/abs/2507.10475)
*smail Tarm,Aytu Onan*

Main category: cs.CL

TL;DR: The paper compares diffusion-generated (LLaDA) and autoregressive-generated (LLaMA) text, showing LLaDA mimics human text better, fooling detectors, while LLaMA has lower perplexity but less lexical fidelity. It calls for diffusion-aware detection methods.


<details>
  <summary>Details</summary>
Motivation: Address the gap in detecting AI-generated text from diffusion models, as current stylometric metrics are designed for autoregressive models.

Method: Systematic comparison of 2,000 samples from LLaDA (diffusion) and LLaMA (autoregressive) using metrics like perplexity, burstiness, lexical diversity, readability, and BLEU/ROUGE scores.

Result: LLaDA mimics human text in perplexity and burstiness, causing high false-negative rates for detectors. LLaMA has lower perplexity but reduced lexical fidelity. Single metrics fail to distinguish diffusion outputs from human writing.

Conclusion: The study underscores the need for diffusion-aware detectors and suggests hybrid models, diffusion-specific stylometric signatures, and robust watermarking as future directions.

Abstract: The rapid advancement of large language models (LLMs) has raised concerns
about reliably detecting AI-generated text. Stylometric metrics work well on
autoregressive (AR) outputs, but their effectiveness on diffusion-based models
is unknown. We present the first systematic comparison of diffusion-generated
text (LLaDA) and AR-generated text (LLaMA) using 2 000 samples. Perplexity,
burstiness, lexical diversity, readability, and BLEU/ROUGE scores show that
LLaDA closely mimics human text in perplexity and burstiness, yielding high
false-negative rates for AR-oriented detectors. LLaMA shows much lower
perplexity but reduced lexical fidelity. Relying on any single metric fails to
separate diffusion outputs from human writing. We highlight the need for
diffusion-aware detectors and outline directions such as hybrid models,
diffusion-specific stylometric signatures, and robust watermarking.

</details>


### [61] [Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation](https://arxiv.org/abs/2507.10524)
*Sangmin Bae,Yujin Kim,Reza Bayat,Sungnyun Kim,Jiyoun Ha,Tal Schuster,Adam Fisch,Hrayr Harutyunyan,Ziwei Ji,Aaron Courville,Se-Young Yun*

Main category: cs.CL

TL;DR: Mixture-of-Recursions (MoR) combines parameter sharing and adaptive computation in a Recursive Transformer, improving efficiency and performance across model scales.


<details>
  <summary>Details</summary>
Motivation: Addressing the high computational and memory costs of scaling language models by unifying parameter sharing and adaptive computation.

Method: Introduces MoR, a framework with shared layers across recursion steps and lightweight routers for adaptive token-level recursion depth. Includes KV sharing to reduce latency.

Result: MoR achieves better validation perplexity, few-shot accuracy, and throughput compared to baselines, forming a new Pareto frontier.

Conclusion: MoR effectively delivers large-model quality at reduced cost, demonstrating its potential for efficient language model scaling.

Abstract: Scaling language models unlocks impressive capabilities, but the accompanying
computational and memory demands make both training and deployment expensive.
Existing efficiency efforts typically target either parameter sharing or
adaptive computation, leaving open the question of how to attain both
simultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework
that combines the two axes of efficiency inside a single Recursive Transformer.
MoR reuses a shared stack of layers across recursion steps to achieve parameter
efficiency, while lightweight routers enable adaptive token-level thinking by
dynamically assigning different recursion depths to individual tokens. This
allows MoR to focus quadratic attention computation only among tokens still
active at a given recursion depth, further improving memory access efficiency
by selectively caching only their key-value pairs. Beyond these core
mechanisms, we also propose a KV sharing variant that reuses KV pairs from the
first recursion, specifically designed to decrease prefill latency and memory
footprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms
a new Pareto frontier: at equal training FLOPs and smaller model sizes, it
significantly lowers validation perplexity and improves few-shot accuracy,
while delivering higher throughput compared with vanilla and existing recursive
baselines. These gains demonstrate that MoR is an effective path towards
large-model quality without incurring large-model cost.

</details>


### [62] [CodeJudgeBench: Benchmarking LLM-as-a-Judge for Coding Tasks](https://arxiv.org/abs/2507.10535)
*Hongchao Jiang,Yiming Chen,Yushi Cao,Hung-yi Lee,Robby T. Tan*

Main category: cs.CL

TL;DR: The paper introduces CodeJudgeBench to evaluate LLMs as judges in coding tasks, finding thinking models outperform non-thinking ones but exhibit randomness and sensitivity to response order.


<details>
  <summary>Details</summary>
Motivation: To address the lack of benchmarks for evaluating LLMs as judges in coding tasks like code generation, repair, and unit test generation.

Method: Introduces CodeJudgeBench, a benchmark for evaluating 26 LLM-as-a-Judge models across three coding tasks, analyzing performance, randomness, and prompting strategies.

Result: Thinking models outperform non-thinking ones, but all show randomness. Pair-wise comparison and retaining comments improve judging performance.

Conclusion: LLM-as-a-Judge models are promising but suffer from reliability issues due to randomness and sensitivity, requiring better prompting strategies.

Abstract: Large Language Models (LLMs) have significantly advanced the state-of-the-art
in various coding tasks. Beyond directly answering user queries, LLMs can also
serve as judges, assessing and comparing the quality of responses generated by
other models. Such an evaluation capability is crucial both for benchmarking
different LLMs and for improving response quality through response ranking.
However, despite the growing adoption of the LLM-as-a-Judge paradigm, its
effectiveness in coding scenarios remains underexplored due to the absence of
dedicated benchmarks. To address this gap, we introduce CodeJudgeBench, a
benchmark explicitly designed to evaluate the performance of LLM-as-a-Judge
models across three critical coding tasks: code generation, code repair, and
unit test generation. Through comprehensive benchmarking of 26 LLM-as-a-Judge
models, we find that recent thinking models significantly outperform
non-thinking models on our carefully designed code judging tasks. Notably, even
relatively small thinking models, such as Qwen3-8B, can outperform specially
trained LLM-as-a-Judge models up to 70B in size. Nevertheless, all models still
exhibit significant randomness in their judgment of coding tasks. For pairwise
judging tasks, simply changing the order in which responses are presented can
substantially impact accuracy. In addition, when judging code and unit tests
written by different LLMs, LLM-as-a-Judge models also show variance in
performance. This sensitivity raises concerns about the reliability and
consistency of LLM-as-a-Judge in coding scenarios. Lastly, we study optimal
prompting strategies for LLM-as-a-Judge. We find that using pair-wise
comparison outperforms scalar point-wise judging. Furthermore, retaining
comments and reasoning in the full, unprocessed LLM response leads to improved
judge performance.

</details>


### [63] [REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once](https://arxiv.org/abs/2507.10541)
*Zhuoshi Pan,Qizhi Pei,Yu Li,Qiyao Sun,Zinan Tang,H. Vicky Zhao,Conghui He,Lijun Wu*

Main category: cs.CL

TL;DR: REST is a stress-testing framework for evaluating Large Reasoning Models (LRMs) by exposing them to multiple problems simultaneously, revealing performance gaps and under-tested capabilities like contextual priority allocation.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for LRMs are limited to single-question evaluations, failing to assess real-world multi-context pressure and requiring costly human annotation for new questions.

Method: REST concurrently tests LRMs with multiple problems, evaluating capabilities like cross-problem interference resistance and dynamic cognitive load management.

Result: State-of-the-art models show significant performance degradation under REST, with REST proving more discriminative than single-question benchmarks. Key insights include the 'overthinking trap' and benefits of 'long2short' training.

Conclusion: REST offers a cost-efficient, future-proof evaluation paradigm that better mirrors real-world reasoning demands and reduces reliance on human annotation.

Abstract: Recent Large Reasoning Models (LRMs) have achieved remarkable progress on
task-specific benchmarks, yet their evaluation methods remain constrained by
isolated problem-solving paradigms. Existing benchmarks predominantly assess
single-question reasoning through sequential testing, resulting critical
limitations: (1) vulnerability to data contamination and less challenging
(e.g., DeepSeek-R1 achieves 97.0% on MATH500), forcing costly and perpetual
creation of new questions with large human efforts, (2) failure to evaluate
models under multi-context pressure, a key requirement for real-world
deployment. To bridge this gap, we present REST (Reasoning Evaluation through
Simultaneous Testing), a stress-testing framework that concurrently exposes
LRMs to multiple problems simultaneously. Beyond basic reasoning, REST
specifically evaluates several under-tested capabilities: contextual priority
allocation, cross-problem interference resistance, and dynamic cognitive load
management. Our evaluation reveals several striking findings: Even
state-of-the-art (SOTA) models like DeepSeek-R1 exhibit substantial performance
degradation under stress testing. Crucially, REST demonstrates stronger
discriminative power than existing benchmarks, revealing pronounced performance
differences among models that exhibit similar, near-ceiling performance under
single-question evaluations. Some key mechanistic insights emerge from our
analysis: (1) the "overthinking trap" is a critical factor contributing to the
performance degradation; (2) the models trained with "long2short" technique
preserve more accuracy of their single-problem performance under REST,
outperforming standard-trained counterparts. These results establish REST as a
cost-efficient, future-proof evaluation paradigm that better reflects
real-world reasoning demands while reducing reliance on continuous human
annotation.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [64] [View Invariant Learning for Vision-Language Navigation in Continuous Environments](https://arxiv.org/abs/2507.08831)
*Josh Qixuan Sun,Xiaoying Xing,Huaiyuan Weng,Chul Min Yeum,Mark Crowley*

Main category: cs.CV

TL;DR: The paper introduces V2-VLNCE, a generalized scenario for Vision-Language Navigation in Continuous Environments (VLNCE) with varied viewpoints, and proposes VIL (View Invariant Learning) to enhance navigation policy robustness. VIL uses contrastive learning and a teacher-student framework, achieving state-of-the-art performance on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Current navigation policies in VLNCE are sensitive to viewpoint changes (e.g., camera height and angle), limiting their robustness. The paper aims to address this by generalizing the scenario and improving viewpoint invariance.

Method: The paper introduces VIL, a post-training strategy combining contrastive learning for view-invariant features and a teacher-student framework for the Waypoint Predictor Module. It employs end-to-end training to optimize these components jointly.

Result: VIL outperforms state-of-the-art methods by 8-15% in Success Rate on R2R-CE and RxR-CE datasets. It also maintains or improves performance under standard VLNCE settings and achieves top results on RxR-CE.

Conclusion: VIL is an effective plug-and-play post-training method that enhances viewpoint robustness without compromising standard performance, making it valuable for embodied AI navigation tasks.

Abstract: Vision-Language Navigation in Continuous Environments (VLNCE), where an agent
follows instructions and moves freely to reach a destination, is a key research
problem in embodied AI. However, most navigation policies are sensitive to
viewpoint changes, i.e., variations in camera height and viewing angle that
alter the agent's observation. In this paper, we introduce a generalized
scenario, V2-VLNCE (VLNCE with Varied Viewpoints), and propose VIL (View
Invariant Learning), a view-invariant post-training strategy that enhances the
robustness of existing navigation policies to changes in camera viewpoint. VIL
employs a contrastive learning framework to learn sparse and view-invariant
features. Additionally, we introduce a teacher-student framework for the
Waypoint Predictor Module, a core component of most VLNCE baselines, where a
view-dependent teacher model distills knowledge into a view-invariant student
model. We employ an end-to-end training paradigm to jointly optimize these
components, thus eliminating the cost for individual module training. Empirical
results show that our method outperforms state-of-the-art approaches on
V2-VLNCE by 8-15% measured on Success Rate for two standard benchmark datasets
R2R-CE and RxR-CE. Furthermore, we evaluate VIL under the standard VLNCE
setting and find that, despite being trained for varied viewpoints, it often
still improves performance. On the more challenging RxR-CE dataset, our method
also achieved state-of-the-art performance across all metrics when compared to
other map-free methods. This suggests that adding VIL does not diminish the
standard viewpoint performance and can serve as a plug-and-play post-training
method.

</details>


### [65] [Detecting Deepfake Talking Heads from Facial Biometric Anomalies](https://arxiv.org/abs/2507.08917)
*Justin D. Norman,Hany Farid*

Main category: cs.CV

TL;DR: A novel forensic ML technique detects deepfake videos by analyzing unnatural facial biometric patterns, tested on diverse datasets and unseen generators.


<details>
  <summary>Details</summary>
Motivation: Deepfake videos are increasingly used for fraud and disinformation, necessitating reliable detection methods.

Method: The technique leverages unnatural facial biometric patterns to identify deepfake impersonations.

Result: Evaluated on a large dataset, the method shows reliability against video laundering and generalizes to unseen deepfake generators.

Conclusion: The proposed technique effectively detects deepfake videos, offering a robust solution against impersonation threats.

Abstract: The combination of highly realistic voice cloning, along with visually
compelling avatar, face-swap, or lip-sync deepfake video generation, makes it
relatively easy to create a video of anyone saying anything. Today, such
deepfake impersonations are often used to power frauds, scams, and political
disinformation. We propose a novel forensic machine learning technique for the
detection of deepfake video impersonations that leverages unnatural patterns in
facial biometrics. We evaluate this technique across a large dataset of
deepfake techniques and impersonations, as well as assess its reliability to
video laundering and its generalization to previously unseen video deepfake
generators.

</details>


### [66] [PRISM: Reducing Spurious Implicit Biases in Vision-Language Models with LLM-Guided Embedding Projection](https://arxiv.org/abs/2507.08979)
*Mahdiyar Molahasani,Azadeh Motamedi,Michael Greenspan,Il-Min Kim,Ali Etemad*

Main category: cs.CV

TL;DR: PRISM is a data-free, task-agnostic method to mitigate bias in vision-language models (VLMs) like CLIP by using an LLM to generate biased scene descriptions and a novel contrastive debiasing loss.


<details>
  <summary>Details</summary>
Motivation: VLMs often inherit and amplify biases from training data, leading to skewed predictions. PRISM aims to debias VLMs without predefined bias categories or external data.

Method: PRISM operates in two stages: (1) an LLM generates biased scene descriptions from class prompts, and (2) a contrastive-style debiasing loss learns a projection to minimize spurious correlations while preserving image-text alignment.

Result: PRISM outperforms current debiasing methods on Waterbirds and CelebA datasets.

Conclusion: PRISM effectively reduces bias in VLMs without external data or predefined categories, demonstrating superior performance on benchmark datasets.

Abstract: We introduce Projection-based Reduction of Implicit Spurious bias in
vision-language Models (PRISM), a new data-free and task-agnostic solution for
bias mitigation in VLMs like CLIP. VLMs often inherit and amplify biases in
their training data, leading to skewed predictions. PRISM is designed to debias
VLMs without relying on predefined bias categories or additional external data.
It operates in two stages: first, an LLM is prompted with simple class prompts
to generate scene descriptions that contain spurious correlations. Next, PRISM
uses our novel contrastive-style debiasing loss to learn a projection that maps
the embeddings onto a latent space that minimizes spurious correlations while
preserving the alignment between image and text embeddings.Extensive
experiments demonstrate that PRISM outperforms current debiasing methods on the
commonly used Waterbirds and CelebA datasets We make our code public at:
https://github.com/MahdiyarMM/PRISM.

</details>


### [67] [Video Inference for Human Mesh Recovery with Vision Transformer](https://arxiv.org/abs/2507.08981)
*Hanbyel Cho,Jaesung Ahn,Yooshin Cho,Junmo Kim*

Main category: cs.CV

TL;DR: Proposes HMR-ViT, a method combining temporal and kinematic data for Human Mesh Recovery, using Vision Transformer and a novel feature image approach.


<details>
  <summary>Details</summary>
Motivation: Existing HMR methods use either temporal or kinematic data, but not both, limiting accuracy. HMR-ViT addresses this gap.

Method: Constructs a Temporal-kinematic Feature Image from video frames, uses a Channel Rearranging Matrix for spatial feature organization, and employs Vision Transformer for encoding.

Result: Achieves competitive performance on 3DPW and Human3.6M datasets.

Conclusion: HMR-ViT effectively combines temporal and kinematic information, improving HMR accuracy.

Abstract: Human Mesh Recovery (HMR) from an image is a challenging problem because of
the inherent ambiguity of the task. Existing HMR methods utilized either
temporal information or kinematic relationships to achieve higher accuracy, but
there is no method using both. Hence, we propose "Video Inference for Human
Mesh Recovery with Vision Transformer (HMR-ViT)" that can take into account
both temporal and kinematic information. In HMR-ViT, a Temporal-kinematic
Feature Image is constructed using feature vectors obtained from video frames
by an image encoder. When generating the feature image, we use a Channel
Rearranging Matrix (CRM) so that similar kinematic features could be located
spatially close together. The feature image is then further encoded using
Vision Transformer, and the SMPL pose and shape parameters are finally inferred
using a regression network. Extensive evaluation on the 3DPW and Human3.6M
datasets indicates that our method achieves a competitive performance in HMR.

</details>


### [68] [From images to properties: a NeRF-driven framework for granular material parameter inversion](https://arxiv.org/abs/2507.09005)
*Cheng-Hsi Hsiao,Krishna Kumar*

Main category: cs.CV

TL;DR: A framework combining NeRF and MPM to estimate granular material properties from visual data, achieving friction angle estimation within 2 degrees error.


<details>
  <summary>Details</summary>
Motivation: To infer material properties like friction angle from visual observations when direct measurement is impractical.

Method: Uses NeRF for 3D reconstruction from multi-view images, initializes MPM simulation, and employs Bayesian optimization to minimize image loss for parameter estimation.

Result: Friction angle estimated with less than 2 degrees error, validating the approach.

Conclusion: The framework effectively characterizes granular materials using visual data, offering practical solutions for real-world scenarios.

Abstract: We introduce a novel framework that integrates Neural Radiance Fields (NeRF)
with Material Point Method (MPM) simulation to infer granular material
properties from visual observations. Our approach begins by generating
synthetic experimental data, simulating an plow interacting with sand. The
experiment is rendered into realistic images as the photographic observations.
These observations include multi-view images of the experiment's initial state
and time-sequenced images from two fixed cameras. Using NeRF, we reconstruct
the 3D geometry from the initial multi-view images, leveraging its capability
to synthesize novel viewpoints and capture intricate surface details. The
reconstructed geometry is then used to initialize material point positions for
the MPM simulation, where the friction angle remains unknown. We render images
of the simulation under the same camera setup and compare them to the observed
images. By employing Bayesian optimization, we minimize the image loss to
estimate the best-fitting friction angle. Our results demonstrate that friction
angle can be estimated with an error within 2 degrees, highlighting the
effectiveness of inverse analysis through purely visual observations. This
approach offers a promising solution for characterizing granular materials in
real-world scenarios where direct measurement is impractical or impossible.

</details>


### [69] [VISTA: A Visual Analytics Framework to Enhance Foundation Model-Generated Data Labels](https://arxiv.org/abs/2507.09008)
*Xiwei Xuan,Xiaoqi Wang,Wenbin He,Jorge Piazentin Ono,Liang Gou,Kwan-Liu Ma,Liu Ren*

Main category: cs.CV

TL;DR: VISTA is a visual analytics framework designed to improve the quality of multi-modal foundation model-generated labels by integrating multi-phased validation and human expertise, addressing gaps in existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing approaches prioritize data quantity over quality in auto-labeling, lacking comprehensive validation methods for large-scale datasets without ground truth.

Method: VISTA combines multi-phased data validation strategies with human expertise to identify and correct issues in FM-generated labels.

Result: VISTA's effectiveness is demonstrated through use cases on benchmark datasets and expert reviews, showing improvements in label quality.

Conclusion: VISTA successfully enhances data quality for multi-modal models, particularly in open-vocabulary image segmentation, by leveraging human validation and comprehensive analysis.

Abstract: The advances in multi-modal foundation models (FMs) (e.g., CLIP and LLaVA)
have facilitated the auto-labeling of large-scale datasets, enhancing model
performance in challenging downstream tasks such as open-vocabulary object
detection and segmentation. However, the quality of FM-generated labels is less
studied as existing approaches focus more on data quantity over quality. This
is because validating large volumes of data without ground truth presents a
considerable challenge in practice. Existing methods typically rely on limited
metrics to identify problematic data, lacking a comprehensive perspective, or
apply human validation to only a small data fraction, failing to address the
full spectrum of potential issues. To overcome these challenges, we introduce
VISTA, a visual analytics framework that improves data quality to enhance the
performance of multi-modal models. Targeting the complex and demanding domain
of open-vocabulary image segmentation, VISTA integrates multi-phased data
validation strategies with human expertise, enabling humans to identify,
understand, and correct hidden issues within FM-generated labels. Through
detailed use cases on two benchmark datasets and expert reviews, we demonstrate
VISTA's effectiveness from both quantitative and qualitative perspectives.

</details>


### [70] [BrainLesion Suite: A Flexible and User-Friendly Framework for Modular Brain Lesion Image Analysis](https://arxiv.org/abs/2507.09036)
*Florian Kofler,Marcel Rosier,Mehdi Astaraki,Hendrik Mller,Ilhem Isra Mekki,Josef A. Buchner,Anton Schmick,Arianna Pfiffer,Eva Oswald,Lucas Zimmer,Ezequiel de la Rosa,Sarthak Pati,Julian Canisius,Arianna Piffer,Ujjwal Baid,Mahyar Valizadeh,Akis Linardos,Jan C. Peeken,Surprosanna Shit,Felix Steinbauer,Daniel Rueckert,Rolf Heckemann,Spyridon Bakas,Jan Kirschke,Constantin von See,Ivan Ezhov,Marie Piraud,Benedikt Wiestler,Bjoern Menze*

Main category: cs.CV

TL;DR: BrainLesion Suite is a Python toolkit for modular brain lesion image analysis, offering preprocessing, modality synthesis, lesion inpainting, and segmentation performance quantification.


<details>
  <summary>Details</summary>
Motivation: To streamline the creation of complex brain lesion analysis workflows with minimal cognitive effort, leveraging Pythonic principles.

Method: Uses adaptable preprocessing (co-registration, atlas registration, skull-stripping, defacing), BraTS algorithms for modality synthesis and lesion inpainting, and tools like panoptica for segmentation metrics.

Result: Provides a versatile toolkit for brain lesion analysis, adaptable to other biomedical applications.

Conclusion: BrainLesion Suite is a powerful, flexible tool for brain lesion image analysis, with potential for broader biomedical use.

Abstract: BrainLesion Suite is a versatile toolkit for building modular brain lesion
image analysis pipelines in Python. Following Pythonic principles, BrainLesion
Suite is designed to provide a 'brainless' development experience, minimizing
cognitive effort and streamlining the creation of complex workflows for
clinical and scientific practice. At its core is an adaptable preprocessing
module that performs co-registration, atlas registration, and optional
skull-stripping and defacing on arbitrary multi-modal input images. BrainLesion
Suite leverages algorithms from the BraTS challenge to synthesize missing
modalities, inpaint lesions, and generate pathology-specific tumor
segmentations. BrainLesion Suite also enables quantifying segmentation model
performance, with tools such as panoptica to compute lesion-wise metrics.
Although BrainLesion Suite was originally developed for image analysis
pipelines of brain lesions such as glioma, metastasis, and multiple sclerosis,
it can be adapted for other biomedical image analysis applications. The
individual BrainLesion Suite packages and tutorials are accessible on GitHub.

</details>


### [71] [Can Contrastive Learning Improve Class-Imbalanced Diffusion Model?](https://arxiv.org/abs/2507.09052)
*Fang Chen,Alex Villa,Gongbo Liang,Xiaoyi Lu,Meng Tang*

Main category: cs.CV

TL;DR: The paper introduces two contrastive loss functions to improve diversity in tail class images for class-conditional diffusion models trained on imbalanced data, without affecting head class performance.


<details>
  <summary>Details</summary>
Motivation: Class-conditional image synthesis often suffers from long-tailed data distributions, leading to mode collapse and reduced diversity for tail classes.

Method: Two contrastive loss functions are proposed: an unsupervised InfoNCE loss to increase dissimilarity among synthetic tail class images, and an MSE loss to align conditional and unconditional generation at large timesteps, sharing knowledge from head classes.

Result: The method outperforms standard DDPM and alternatives on datasets like CIFAR10/100-LT, PlacesLT, TinyImageNetLT, and ImageNetLT.

Conclusion: The proposed contrastive learning framework effectively enhances diversity for tail classes in class-imbalanced diffusion models while maintaining head class performance.

Abstract: Training data for class-conditional image synthesis often exhibit a
long-tailed distribution with limited images for tail classes. Such an
imbalance causes mode collapse and reduces the diversity of synthesized images
for tail classes. For class-conditional diffusion models trained on imbalanced
data, we aim to improve the diversity of tail class images without compromising
the fidelity and diversity of head class images. We achieve this by introducing
two deceptively simple but highly effective contrastive loss functions.
Firstly, we employ an unsupervised InfoNCE loss utilizing negative samples to
increase the distance/dissimilarity among synthetic images, particularly for
tail classes. To further enhance the diversity of tail classes, our second loss
is an MSE loss that contrasts class-conditional generation with unconditional
generation at large timesteps. This second loss makes the denoising process
insensitive to class conditions for the initial steps, which enriches tail
classes through knowledge sharing from head classes. Conditional-unconditional
alignment has been shown to enhance the performance of long-tailed GAN. We are
the first to adapt such alignment to diffusion models. We successfully
leveraged contrastive learning for class-imbalanced diffusion models. Our
contrastive learning framework is easy to implement and outperforms standard
DDPM and alternative methods for class-imbalanced diffusion models across
various datasets, including CIFAR10/100-LT, PlacesLT, TinyImageNetLT, and
ImageNetLT.

</details>


### [72] [Infinite Video Understanding](https://arxiv.org/abs/2507.09068)
*Dell Zhang,Xiangyu Chen,Jixiang Luo,Mengxi Jia,Changzhi Sun,Ruilong Ren,Jingren Liu,Hao Sun,Xuelong Li*

Main category: cs.CV

TL;DR: The paper discusses the challenges in video understanding for long-duration content and proposes 'Infinite Video Understanding' as a future research goal.


<details>
  <summary>Details</summary>
Motivation: Current models struggle with processing long videos due to computational constraints and issues like temporal coherence. The paper aims to inspire innovation in this area.

Method: The paper reviews existing solutions (e.g., Video-XL-2, HoPE, VideoRoPE++) and identifies gaps, proposing 'Infinite Video Understanding' as a research direction.

Result: The paper highlights the need for advancements in streaming architectures, memory mechanisms, and event-centric reasoning for long videos.

Conclusion: The paper advocates for 'Infinite Video Understanding' as a transformative goal to drive future research in multimedia and AI.

Abstract: The rapid advancements in Large Language Models (LLMs) and their multimodal
extensions (MLLMs) have ushered in remarkable progress in video understanding.
However, a fundamental challenge persists: effectively processing and
comprehending video content that extends beyond minutes or hours. While recent
efforts like Video-XL-2 have demonstrated novel architectural solutions for
extreme efficiency, and advancements in positional encoding such as HoPE and
VideoRoPE++ aim to improve spatio-temporal understanding over extensive
contexts, current state-of-the-art models still encounter significant
computational and memory constraints when faced with the sheer volume of visual
tokens from lengthy sequences. Furthermore, maintaining temporal coherence,
tracking complex events, and preserving fine-grained details over extended
periods remain formidable hurdles, despite progress in agentic reasoning
systems like Deep Video Discovery. This position paper posits that a logical,
albeit ambitious, next frontier for multimedia research is Infinite Video
Understanding -- the capability for models to continuously process, understand,
and reason about video data of arbitrary, potentially never-ending duration. We
argue that framing Infinite Video Understanding as a blue-sky research
objective provides a vital north star for the multimedia, and the wider AI,
research communities, driving innovation in areas such as streaming
architectures, persistent memory mechanisms, hierarchical and adaptive
representations, event-centric reasoning, and novel evaluation paradigms.
Drawing inspiration from recent work on long/ultra-long video understanding and
several closely related fields, we outline the core challenges and key research
directions towards achieving this transformative capability.

</details>


### [73] [BlindSight: Harnessing Sparsity for Efficient VLMs](https://arxiv.org/abs/2507.09071)
*Tharun Adithya Srikrishnan,Deval Shah,Steven K. Reinhardt*

Main category: cs.CV

TL;DR: BlindSight reduces FLOPs in VLMs by 32%-41% with minimal accuracy impact by leveraging sparse attention patterns.


<details>
  <summary>Details</summary>
Motivation: The inclusion of vision data in VLMs increases prompt length and prefill duration due to quadratic attention complexity.

Method: Analyzes attention patterns in VLMs, identifies sparse categories (sink-only, document mask, hybrid), and proposes BlindSight, a training-free approach using input template-aware sparsity masks.

Result: 32%-41% reduction in FLOPs with -2%-+2% accuracy change in multi-image benchmarks.

Conclusion: BlindSight effectively optimizes VLM inference without training, maintaining accuracy while reducing computational cost.

Abstract: Large vision-language models (VLMs) enable the joint processing of text and
images. However, the inclusion of vision data significantly expands the prompt
length. Along with the quadratic complexity of the attention computation, this
results in a longer prefill duration. An approach to mitigate this bottleneck
is to leverage the inherent sparsity in the attention computation. In our
analysis of attention patterns in VLMs, we observe that a substantial portion
of layers exhibit minimal cross-image attention, except through attention-sink
tokens per image. These sparse attention patterns fall into distinct
categories: sink-only, document mask and a hybrid document-sink mask. Based on
this, we propose BlindSight: a training-free approach to optimize VLM inference
using a input template-aware attention sparsity mask. We utilize samples from a
dataset to derive a prompt-agnostic sparsity categorization for every attention
head. We evaluate the proposed technique using VLMs such as Qwen2-VL,
Qwen2.5-VL and Gemma-3. BlindSight results in a 32%-41% reduction in FLOPs on
average with -2%-+2% accuracy compared to the original model in most evaluated
multi-image understanding benchmarks.

</details>


### [74] [From Physics to Foundation Models: A Review of AI-Driven Quantitative Remote Sensing Inversion](https://arxiv.org/abs/2507.09081)
*Zhenyu Yu,Mohd Yamani Idna Idris,Hua Wang,Pei Wang,Junyi Chen,Kun Wang*

Main category: cs.CV

TL;DR: The paper reviews the evolution of remote sensing inversion methods from physics-based models to data-driven and foundation model approaches, highlighting advances and challenges.


<details>
  <summary>Details</summary>
Motivation: To understand the shift from traditional physics-based methods to modern data-driven and foundation model techniques in remote sensing inversion.

Method: Systematic review of methodologies, including physical models (e.g., PROSPECT), machine learning (e.g., deep learning), and foundation models (e.g., SatMAE).

Result: Identifies strengths and limitations of each paradigm, with focus on self-supervised pretraining, multi-modal integration, and cross-task adaptation in foundation models.

Conclusion: Envisions next-generation foundation models with unified modeling, cross-domain generalization, and improved physical interpretability.

Abstract: Quantitative remote sensing inversion aims to estimate continuous surface
variables-such as biomass, vegetation indices, and evapotranspiration-from
satellite observations, supporting applications in ecosystem monitoring, carbon
accounting, and land management. With the evolution of remote sensing systems
and artificial intelligence, traditional physics-based paradigms are giving way
to data-driven and foundation model (FM)-based approaches. This paper
systematically reviews the methodological evolution of inversion techniques,
from physical models (e.g., PROSPECT, SCOPE, DART) to machine learning methods
(e.g., deep learning, multimodal fusion), and further to foundation models
(e.g., SatMAE, GFM, mmEarth). We compare the modeling assumptions, application
scenarios, and limitations of each paradigm, with emphasis on recent FM
advances in self-supervised pretraining, multi-modal integration, and
cross-task adaptation. We also highlight persistent challenges in physical
interpretability, domain generalization, limited supervision, and uncertainty
quantification. Finally, we envision the development of next-generation
foundation models for remote sensing inversion, emphasizing unified modeling
capacity, cross-domain generalization, and physical interpretability.

</details>


### [75] [Taming generative video models for zero-shot optical flow extraction](https://arxiv.org/abs/2507.09082)
*Seungwoo Kim,Khai Loong Aw,Klemen Kotar,Cristobal Eyzaguirre,Wanhee Lee,Yunong Liu,Jared Watrous,Stefan Stojanov,Juan Carlos Niebles,Jiajun Wu,Daniel L. K. Yamins*

Main category: cs.CV

TL;DR: The paper explores zero-shot optical flow extraction from frozen self-supervised video models, proposing KL-tracing, a method outperforming state-of-the-art models without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To leverage large general-purpose video models for optical flow extraction without fine-tuning, addressing the scarcity of labeled data and sim-to-real gaps in synthetic datasets.

Method: Extends the Counterfactual World Model paradigm to generative video models, introducing KL-tracing, which injects perturbations and measures divergence in predictive distributions.

Result: Achieves 16.6% and 4.7% relative improvements on real-world and synthetic datasets, respectively, without flow-specific fine-tuning.

Conclusion: Counterfactual prompting of generative video models is a scalable, effective alternative to supervised methods for high-quality optical flow.

Abstract: Extracting optical flow from videos remains a core computer vision problem.
Motivated by the success of large general-purpose models, we ask whether frozen
self-supervised video models trained only for future frame prediction can be
prompted, without fine-tuning, to output flow. Prior work reading out depth or
illumination from video generators required fine-tuning, which is impractical
for flow where labels are scarce and synthetic datasets suffer from a
sim-to-real gap. Inspired by the Counterfactual World Model (CWM) paradigm,
which can obtain point-wise correspondences by injecting a small tracer
perturbation into a next-frame predictor and tracking its propagation, we
extend this idea to generative video models. We explore several popular
architectures and find that successful zero-shot flow extraction in this manner
is aided by three model properties: (1) distributional prediction of future
frames (avoiding blurry or noisy outputs); (2) factorized latents that treat
each spatio-temporal patch independently; and (3) random-access decoding that
can condition on any subset of future pixels. These properties are uniquely
present in the recent Local Random Access Sequence (LRAS) architecture.
Building on LRAS, we propose KL-tracing: a novel test-time procedure that
injects a localized perturbation into the first frame, rolls out the model one
step, and computes the Kullback-Leibler divergence between perturbed and
unperturbed predictive distributions. Without any flow-specific fine-tuning,
our method outperforms state-of-the-art models on real-world TAP-Vid DAVIS
dataset (16.6% relative improvement for endpoint error) and synthetic TAP-Vid
Kubric (4.7% relative improvement). Our results indicate that counterfactual
prompting of controllable generative video models is a scalable and effective
alternative to supervised or photometric-loss approaches for high-quality flow.

</details>


### [76] [MI CAM: Mutual Information Weighted Activation Mapping for Causal Visual Explanations of Convolutional Neural Networks](https://arxiv.org/abs/2507.09092)
*Ram S Iyer,Narayan S Iyer,Rugmini Ammal P*

Main category: cs.CV

TL;DR: The paper introduces MI CAM, a post-hoc visual explanation method for CNNs, using mutual information to weigh feature maps and generate saliency visualizations, validated by counterfactual analysis.


<details>
  <summary>Details</summary>
Motivation: To understand and justify CNN inferences, especially in critical applications like healthcare and automated power plants, by providing unbiased visual explanations.

Method: MI CAM weighs feature maps based on mutual information with input images, combining weights and activation maps linearly for saliency visualizations. Counterfactual analysis validates causality.

Result: MI CAM matches state-of-the-art methods and outperforms some in qualitative and quantitative measures, offering clear visual performance and unbiased justifications.

Conclusion: MI CAM is an effective method for explaining CNN inferences, validated by counterfactual analysis, and performs competitively with existing approaches.

Abstract: With the intervention of machine vision in our crucial day to day necessities
including healthcare and automated power plants, attention has been drawn to
the internal mechanisms of convolutional neural networks, and the reason why
the network provides specific inferences. This paper proposes a novel post-hoc
visual explanation method called MI CAM based on activation mapping. Differing
from previous class activation mapping based approaches, MI CAM produces
saliency visualizations by weighing each feature map through its mutual
information with the input image and the final result is generated by a linear
combination of weights and activation maps. It also adheres to producing causal
interpretations as validated with the help of counterfactual analysis. We aim
to exhibit the visual performance and unbiased justifications for the model
inferencing procedure achieved by MI CAM. Our approach works at par with all
state-of-the-art methods but particularly outperforms some in terms of
qualitative and quantitative measures. The implementation of proposed method
can be found on https://anonymous.4open.science/r/MI-CAM-4D27

</details>


### [77] [RadEyeVideo: Enhancing general-domain Large Vision Language Model for chest X-ray analysis with video representations of eye gaze](https://arxiv.org/abs/2507.09097)
*Yunsoo Kim,Jinge Wu,Honghan Wu*

Main category: cs.CV

TL;DR: RadEyeVideo integrates radiologists' eye-gaze videos into LVLMs for CXR analysis, improving performance in report generation and disease diagnosis by up to 24.6%.


<details>
  <summary>Details</summary>
Motivation: Existing methods overlook the sequential order of radiologists' eye movements, which could provide valuable insights for CXR analysis.

Method: Proposes RadEyeVideo, which uses eye-fixation data as video sequences to capture temporal and spatial gaze dynamics, evaluated with three LVLMs.

Result: Performance improved by up to 24.6% in report generation and 15.2% on average for both tasks, surpassing specialized medical LVLMs.

Conclusion: Effective integration of domain expert knowledge (eye-gaze) with LVLMs enhances general-domain models in clinical tasks, advancing human-centered medical analytics.

Abstract: Large Vision-Language Models (LVLMs) have demonstrated promising performance
in chest X-ray (CXR) analysis. To enhance human-computer interaction, several
studies have incorporated radiologists' eye gaze, typically through heatmaps or
textual prompts. However, these methods often overlook the sequential order of
eye movements, which could provide valuable insights by highlighting both the
areas of interest and the order in which they are examined. In this work, we
propose a novel approach called RadEyeVideo that integrates radiologists'
eye-fixation data as a video sequence, capturing both the temporal and spatial
dynamics of their gaze. We evaluate this method in CXR report generation and
disease diagnosis using three general-domain, open-source LVLMs with video
input capabilities. When prompted with eye-gaze videos, model performance
improves by up to 24.6% in the report generation task and on average 15.2% for
both tasks using scaled evaluation metrics. Notably, RadEyeVideo enhanced an
open-domain LVLM model, LLaVA-OneVision, to surpass task-specific medical LVLMs
such as MAIRA-2 and CheXagent, trained on large Chest X-ray data. This work
highlights that domain expert's knowledge (eye-gaze information in this case),
when effectively integrated with LVLMs, can significantly enhance
general-domain models' capabilities in clinical tasks. RadEyeVideo is a step
toward a scalable human-centered approach of utilizing LVLMs in medical image
analytics.

</details>


### [78] [Harnessing Text-to-Image Diffusion Models for Point Cloud Self-Supervised Learning](https://arxiv.org/abs/2507.09102)
*Yiyang Chen,Shanshan Zhao,Lunhao Duan,Changxing Ding,Dacheng Tao*

Main category: cs.CV

TL;DR: PointSD leverages Stable Diffusion (SD) for 3D self-supervised learning by aligning 3D point cloud features with SD features, improving performance on downstream tasks.


<details>
  <summary>Details</summary>
Motivation: Existing 3D diffusion models are limited by small datasets. The robust capabilities of SD, trained on large-scale data, can overcome these limitations.

Method: PointSD replaces SD's text encoder with a 3D encoder, trains a point-to-image diffusion model, and aligns 3D backbone features with SD features.

Result: Experiments show SD enhances 3D self-supervised learning, improving performance on point cloud tasks.

Conclusion: PointSD effectively bridges 2D and 3D learning, demonstrating the potential of leveraging large-scale 2D models for 3D tasks.

Abstract: Diffusion-based models, widely used in text-to-image generation, have proven
effective in 2D representation learning. Recently, this framework has been
extended to 3D self-supervised learning by constructing a conditional point
generator for enhancing 3D representations. However, its performance remains
constrained by the 3D diffusion model, which is trained on the available 3D
datasets with limited size. We hypothesize that the robust capabilities of
text-to-image diffusion models, particularly Stable Diffusion (SD), which is
trained on large-scale datasets, can help overcome these limitations. To
investigate this hypothesis, we propose PointSD, a framework that leverages the
SD model for 3D self-supervised learning. By replacing the SD model's text
encoder with a 3D encoder, we train a point-to-image diffusion model that
allows point clouds to guide the denoising of rendered noisy images. With the
trained point-to-image diffusion model, we use noise-free images as the input
and point clouds as the condition to extract SD features. Next, we train a 3D
backbone by aligning its features with these SD features, thereby facilitating
direct semantic learning. Comprehensive experiments on downstream point cloud
tasks and ablation studies demonstrate that the SD model can enhance point
cloud self-supervised learning. Code is publicly available at
https://github.com/wdttt/PointSD.

</details>


### [79] [Hybrid Autoregressive-Diffusion Model for Real-Time Streaming Sign Language Production](https://arxiv.org/abs/2507.09105)
*Maoxiao Ye,Xinfeng Ye,Mano Manoharan*

Main category: cs.CV

TL;DR: A hybrid approach combining autoregressive and diffusion models is proposed for Sign Language Production (SLP) to address error accumulation and real-time limitations, featuring a Multi-Scale Pose Representation and Confidence-Aware Causal Attention.


<details>
  <summary>Details</summary>
Motivation: Earlier SLP models suffer from error accumulation during inference due to autoregressive methods, while diffusion models are limited by their iterative nature for real-time tasks.

Method: A hybrid model integrates autoregressive and diffusion techniques, with a Multi-Scale Pose Representation for detailed body movements and Confidence-Aware Causal Attention for dynamic pose generation.

Result: The method outperforms on PHOENIX14T and How2Sign datasets in generation quality and real-time efficiency.

Conclusion: The hybrid approach effectively combines the strengths of autoregressive and diffusion models, enhancing SLP performance in accuracy and real-time applicability.

Abstract: Earlier Sign Language Production (SLP) models typically relied on
autoregressive methods that generate output tokens one by one, which inherently
provide temporal alignment. Although techniques like Teacher Forcing can
prevent model collapse during training, they still cannot solve the problem of
error accumulation during inference, since ground truth is unavailable at that
stage. In contrast, more recent approaches based on diffusion models leverage
step-by-step denoising to enable high-quality generation. However, the
iterative nature of these models and the requirement to denoise entire
sequences limit their applicability in real-time tasks like SLP. To address it,
we apply a hybrid approach combining autoregressive and diffusion models to SLP
for the first time, leveraging the strengths of both models in sequential
dependency modeling and output refinement. To capture fine-grained body
movements, we design a Multi-Scale Pose Representation module that separately
extracts detailed features from distinct articulators and integrates them via a
Multi-Scale Fusion module. Furthermore, we introduce a Confidence-Aware Causal
Attention mechanism that utilizes joint-level confidence scores to dynamically
guide the pose generation process, improving accuracy and robustness. Extensive
experiments on the PHOENIX14T and How2Sign datasets demonstrate the
effectiveness of our method in both generation quality and real-time streaming
efficiency.

</details>


### [80] [RoHOI: Robustness Benchmark for Human-Object Interaction Detection](https://arxiv.org/abs/2507.09111)
*Di Wen,Kunyu Peng,Kailun Yang,Yufan Chen,Ruiping Liu,Junwei Zheng,Alina Roitberg,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: The paper introduces RoHOI, the first robustness benchmark for HOI detection, addressing model degradation in real-world conditions. It proposes SAMPL, a learning strategy to enhance robustness, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current HOI detection models fail under real-world corruptions like environmental variability, occlusion, and noise, necessitating a robustness benchmark and improved methods.

Method: The authors create RoHOI, a benchmark with 20 corruption types, and propose SAMPL, a Semantic-Aware Masking-based Progressive Learning strategy for robust feature learning.

Result: Experiments show SAMPL outperforms state-of-the-art methods, improving model resilience to corruptions.

Conclusion: The work sets a new standard for robust HOI detection, with benchmarks and code made publicly available.

Abstract: Human-Object Interaction (HOI) detection is crucial for robot-human
assistance, enabling context-aware support. However, models trained on clean
datasets degrade in real-world conditions due to unforeseen corruptions,
leading to inaccurate prediction. To address this, we introduce the first
robustness benchmark for HOI detection, evaluating model resilience under
diverse challenges. Despite advances, current models struggle with
environmental variability, occlusion, and noise. Our benchmark, RoHOI, includes
20 corruption types based on HICO-DET and V-COCO datasets and a new
robustness-focused metric. We systematically analyze existing models in the
related field, revealing significant performance drops under corruptions. To
improve robustness, we propose a Semantic-Aware Masking-based Progressive
Learning (SAMPL) strategy to guide the model to be optimized based on holistic
and partial cues, dynamically adjusting the model's optimization to enhance
robust feature learning. Extensive experiments show our approach outperforms
state-of-the-art methods, setting a new standard for robust HOI detection.
Benchmarks, datasets, and code will be made publicly available at
https://github.com/Kratos-Wen/RoHOI.

</details>


### [81] [Mind the Gap: Preserving and Compensating for the Modality Gap in CLIP-Based Continual Learning](https://arxiv.org/abs/2507.09118)
*Linlan Huang,Xusheng Cao,Haori Lu,Yifan Meng,Fei Yang,Xialei Liu*

Main category: cs.CV

TL;DR: The paper introduces MG-CLIP, a method leveraging the modality gap in CLIP for continual learning, improving performance without extra replay data.


<details>
  <summary>Details</summary>
Motivation: To address the overlooked modality gap in CLIP for continual learning, which affects generalization and adaptability.

Method: Analyzes modality gap variations during fine-tuning, proposes MG-CLIP to preserve and compensate the gap for better continual learning.

Result: MG-CLIP outperforms existing methods on benchmarks, enhancing CLIP's class-incremental learning.

Conclusion: Modality gap preservation and compensation offer a novel, effective approach for continual learning in vision-language models.

Abstract: Continual learning aims to enable models to learn sequentially from
continuously incoming data while retaining performance on previously learned
tasks. With the Contrastive Language-Image Pre-trained model (CLIP) exhibiting
strong capabilities across various downstream tasks, there has been growing
interest in leveraging CLIP for continual learning in such scenarios. Most
existing works overlook the inherent modality gap in CLIP, a key factor in its
generalization and adaptability. In this paper, we analyze the variations in
the modality gap during the fine-tuning of vision-language pre-trained models.
Our observations reveal that the modality gap effectively reflects the extent
to which pre-trained knowledge is preserved. Based on these insights, we
propose a simple yet effective method, MG-CLIP, that improves CLIP's
performance in class-incremental learning. Our approach leverages modality gap
preservation to mitigate forgetting and modality gap compensation to enhance
the capacity for new data, introducing a novel modality-gap-based perspective
for continual learning. Extensive experiments on multiple benchmarks
demonstrate that our method outperforms existing approaches without requiring
additional replay data. Our code is available at
https://github.com/linlany/MindtheGap.

</details>


### [82] [SnapMoGen: Human Motion Generation from Expressive Texts](https://arxiv.org/abs/2507.09122)
*Chuan Guo,Inwoo Hwang,Jian Wang,Bing Zhou*

Main category: cs.CV

TL;DR: SnapMoGen introduces a high-quality text-motion dataset with detailed annotations and proposes MoMask++, a state-of-the-art model for text-to-motion generation.


<details>
  <summary>Details</summary>
Motivation: Current text-to-motion methods are limited by short or general text prompts due to dataset constraints, hindering fine-grained controllability and generalization.

Method: The paper presents SnapMoGen, a dataset with 20K motion clips and 122K detailed descriptions, and MoMask++, a model using multi-scale token sequences and a generative masked transformer.

Result: MoMask++ achieves state-of-the-art performance on HumanML3D and SnapMoGen benchmarks and handles casual user prompts via LLM reformatting.

Conclusion: SnapMoGen and MoMask++ advance text-to-motion generation by enabling long-term motion synthesis and improved controllability.

Abstract: Text-to-motion generation has experienced remarkable progress in recent
years. However, current approaches remain limited to synthesizing motion from
short or general text prompts, primarily due to dataset constraints. This
limitation undermines fine-grained controllability and generalization to unseen
prompts. In this paper, we introduce SnapMoGen, a new text-motion dataset
featuring high-quality motion capture data paired with accurate, expressive
textual annotations. The dataset comprises 20K motion clips totaling 44 hours,
accompanied by 122K detailed textual descriptions averaging 48 words per
description (vs. 12 words of HumanML3D). Importantly, these motion clips
preserve original temporal continuity as they were in long sequences,
facilitating research in long-term motion generation and blending. We also
improve upon previous generative masked modeling approaches. Our model,
MoMask++, transforms motion into multi-scale token sequences that better
exploit the token capacity, and learns to generate all tokens using a single
generative masked transformer. MoMask++ achieves state-of-the-art performance
on both HumanML3D and SnapMoGen benchmarks. Additionally, we demonstrate the
ability to process casual user prompts by employing an LLM to reformat inputs
to align with the expressivity and narration style of SnapMoGen. Project
webpage: https://snap-research.github.io/SnapMoGen/

</details>


### [83] [PoseLLM: Enhancing Language-Guided Human Pose Estimation with MLP Alignment](https://arxiv.org/abs/2507.09139)
*Dewen Zhang,Tahir Hussain,Wangpeng An,Hayaru Shouno*

Main category: cs.CV

TL;DR: PoseLLM introduces a nonlinear MLP vision-language connector for pose estimation, outperforming LocLLM in accuracy while maintaining zero-shot generalization.


<details>
  <summary>Details</summary>
Motivation: Traditional pose estimation methods struggle with novel poses or unseen keypoints. Language-guided approaches like LocLLM lack complex spatial-textual interactions.

Method: PoseLLM replaces LocLLM's linear projector with a lightweight two-layer MLP with GELU activation for better cross-modal feature fusion.

Result: PoseLLM achieves 77.8 AP on COCO, +0.4 AP over LocLLM, and shows strong zero-shot generalization on Human-Art and MPII.

Conclusion: A nonlinear connector enhances localization accuracy without compromising generalization, advancing language-guided pose estimation.

Abstract: Human pose estimation traditionally relies on architectures that encode
keypoint priors, limiting their generalization to novel poses or unseen
keypoints. Recent language-guided approaches like LocLLM reformulate keypoint
localization as a vision-language task, enabling zero-shot generalization
through textual descriptions. However, LocLLM's linear projector fails to
capture complex spatial-textual interactions critical for high-precision
localization. To address this, we propose PoseLLM, the first Large Language
Model (LLM)-based pose estimation framework that replaces the linear projector
with a nonlinear MLP vision-language connector. This lightweight two-layer MLP
with GELU activation enables hierarchical cross-modal feature transformation,
enhancing the fusion of visual patches and textual keypoint descriptions.
Trained exclusively on COCO data, PoseLLM achieves 77.8 AP on the COCO
validation set, outperforming LocLLM by +0.4 AP, while maintaining strong
zero-shot generalization on Human-Art and MPII. Our work demonstrates that a
simple yet powerful nonlinear connector significantly boosts localization
accuracy without sacrificing generalization, advancing the state-of-the-art in
language-guided pose estimation. Code is available at
https://github.com/Ody-trek/PoseLLM.

</details>


### [84] [$I^{2}$-World: Intra-Inter Tokenization for Efficient Dynamic 4D Scene Forecasting](https://arxiv.org/abs/2507.09144)
*Zhimin Liao,Ping Wei,Ruijie Zhang,Shuaijia Chen,Haoxuan Wang,Ziyang Ren*

Main category: cs.CV

TL;DR: $I^{2}$-World is an efficient 4D occupancy forecasting framework for autonomous driving, using dual tokenizers and an encoder-decoder architecture to achieve state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Addressing corner cases in autonomous driving by forecasting 3D scene evolution and generating unseen scenarios.

Method: Decouples scene tokenization into intra-scene (multi-scale residual quantization) and inter-scene (residual aggregation) tokenizers, with an encoder-decoder architecture for spatial and temporal consistency.

Result: Outperforms existing methods by 25.1% in mIoU and 36.9% in IoU, with 2.9 GB training memory and 37.0 FPS real-time inference.

Conclusion: $I^{2}$-World efficiently combines compactness and dynamic expressiveness, offering a robust solution for 4D occupancy forecasting.

Abstract: Forecasting the evolution of 3D scenes and generating unseen scenarios via
occupancy-based world models offers substantial potential for addressing corner
cases in autonomous driving systems. While tokenization has revolutionized
image and video generation, efficiently tokenizing complex 3D scenes remains a
critical challenge for 3D world models. To address this, we propose
$I^{2}$-World, an efficient framework for 4D occupancy forecasting. Our method
decouples scene tokenization into intra-scene and inter-scene tokenizers. The
intra-scene tokenizer employs a multi-scale residual quantization strategy to
hierarchically compress 3D scenes while preserving spatial details. The
inter-scene tokenizer residually aggregates temporal dependencies across
timesteps. This dual design preserves the compactness of 3D tokenizers while
retaining the dynamic expressiveness of 4D tokenizers. Unlike decoder-only
GPT-style autoregressive models, $I^{2}$-World adopts an encoder-decoder
architecture. The encoder aggregates spatial context from the current scene and
predicts a transformation matrix to enable high-level control over scene
generation. The decoder, conditioned on this matrix and historical tokens,
ensures temporal consistency during generation. Experiments demonstrate that
$I^{2}$-World achieves state-of-the-art performance, outperforming existing
methods by 25.1\% in mIoU and 36.9\% in IoU for 4D occupancy forecasting while
exhibiting exceptional computational efficiency: it requires merely 2.9 GB of
training memory and achieves real-time inference at 37.0 FPS. Our code is
available on https://github.com/lzzzzzm/II-World.

</details>


### [85] [Stable Score Distillation](https://arxiv.org/abs/2507.09168)
*Haiming Zhu,Yangyang Xu,Chenshu Xu,Tingrui Shen,Wenxi Liu,Yong Du,Jun Yu,Shengfeng He*

Main category: cs.CV

TL;DR: Stable Score Distillation (SSD) improves text-guided image and 3D editing by enhancing stability, alignment, and editing strength, outperforming existing methods like Delta Denoising Score.


<details>
  <summary>Details</summary>
Motivation: Existing methods like Delta Denoising Score face issues with stability, spatial control, and editing strength due to reliance on complex auxiliary structures.

Method: SSD uses Classifier-Free Guidance (CFG) for cross-prompt alignment and introduces a null-text branch for stability. It also includes a prompt enhancement branch for stronger edits.

Result: SSD achieves state-of-the-art results in 2D and 3D editing tasks, including NeRF and style edits, with faster convergence and reduced complexity.

Conclusion: SSD provides a robust, efficient solution for text-guided editing, preserving content structure and ensuring prompt-specific modifications.

Abstract: Text-guided image and 3D editing have advanced with diffusion-based models,
yet methods like Delta Denoising Score often struggle with stability, spatial
control, and editing strength. These limitations stem from reliance on complex
auxiliary structures, which introduce conflicting optimization signals and
restrict precise, localized edits. We introduce Stable Score Distillation
(SSD), a streamlined framework that enhances stability and alignment in the
editing process by anchoring a single classifier to the source prompt.
Specifically, SSD utilizes Classifier-Free Guidance (CFG) equation to achieves
cross-prompt alignment, and introduces a constant term null-text branch to
stabilize the optimization process. This approach preserves the original
content's structure and ensures that editing trajectories are closely aligned
with the source prompt, enabling smooth, prompt-specific modifications while
maintaining coherence in surrounding regions. Additionally, SSD incorporates a
prompt enhancement branch to boost editing strength, particularly for style
transformations. Our method achieves state-of-the-art results in 2D and 3D
editing tasks, including NeRF and text-driven style edits, with faster
convergence and reduced complexity, providing a robust and efficient solution
for text-guided editing.

</details>


### [86] [Learning and Transferring Better with Depth Information in Visual Reinforcement Learning](https://arxiv.org/abs/2507.09180)
*Zichun Xu,Yuntao Li,Zhaomin Wang,Lei Zhuang,Guocai Yang,Jingdong Zhao*

Main category: cs.CV

TL;DR: A vision transformer-based backbone fuses RGB and depth data for better generalization, using separate CNN stems and contrastive learning for efficiency, with curriculum learning for sim2real transfer.


<details>
  <summary>Details</summary>
Motivation: Depth information is robust to appearance variations and provides 3D spatial details, making it valuable for enhancing generalization in visual tasks.

Method: Separate CNN stems process RGB and depth data, fused by a scalable vision transformer. Contrastive learning with masked tokens improves sample efficiency, and curriculum learning aids sim2real transfer.

Result: The proposed method effectively combines RGB and depth modalities, improving generalization and efficiency in reinforcement learning.

Conclusion: The fusion of RGB and depth via vision transformers, combined with contrastive and curriculum learning, enhances performance and adaptability in visual tasks.

Abstract: Depth information is robust to scene appearance variations and inherently
carries 3D spatial details. In this paper, a visual backbone based on the
vision transformer is proposed to fuse RGB and depth modalities for enhancing
generalization. Different modalities are first processed by separate CNN stems,
and the combined convolutional features are delivered to the scalable vision
transformer to obtain visual representations. Moreover, a contrastive
unsupervised learning scheme is designed with masked and unmasked tokens to
accelerate the sample efficiency during the reinforcement learning progress.
For sim2real transfer, a flexible curriculum learning schedule is developed to
deploy domain randomization over training processes.

</details>


### [87] [Revisiting Pool-based Prompt Learning for Few-shot Class-incremental Learning](https://arxiv.org/abs/2507.09183)
*Yongwei Jiang,Yixiong Zou,Yuhua Li,Ruixuan Li*

Main category: cs.CV

TL;DR: The paper explores prompt pool methods in Few-Shot Class-Incremental Learning (FSCIL), identifies token-dimension saturation as a cause of performance degradation, and proposes LGSP-Prompt for improved spatial dimension-based learning.


<details>
  <summary>Details</summary>
Motivation: Addressing dual challenges of data scarcity and incremental learning in FSCIL, the study investigates the unexplored effectiveness of prompt pool methods in this setting.

Method: Proposes LGSP-Prompt, shifting prompt learning to the spatial dimension by combining local and global features for dynamic prompt selection.

Result: LGSP-Prompt achieves state-of-the-art performance in FSCIL benchmarks, excelling in knowledge preservation and incremental learning.

Conclusion: The study successfully addresses FSCIL challenges with LGSP-Prompt, demonstrating its superiority over traditional methods.

Abstract: Few-Shot Class-Incremental Learning (FSCIL) faces dual challenges of data
scarcity and incremental learning in real-world scenarios. While pool-based
prompting methods have demonstrated success in traditional incremental
learning, their effectiveness in FSCIL settings remains unexplored. This paper
presents the first study of current prompt pool methods in FSCIL tasks,
revealing an unanticipated performance degradation in incremental sessions.
Through comprehensive analysis, we identify that this phenomenon stems from
token-dimension saturation: with limited data, excessive prompts compete for
task-relevant information, leading to model overfitting. Based on this finding,
we propose LGSP-Prompt (Local-Global Spatial Prompting), which innovatively
shifts pool-based prompt learning from the token dimension to the spatial
dimension. LGSP-Prompt generates spatial prompts by synergistically combining
local spatial features and global frequency-domain representations to highlight
key patterns in input images. We construct two spatial prompt pools enabling
dynamic prompt selection to maintain acquired knowledge while effectively
learning novel sessions. Extensive experiments demonstrate that our approach
achieves state-of-the-art performance across multiple FSCIL benchmarks, showing
significant advantages in both base knowledge preservation and incremental
learning. Our implementation is available at
https://github.com/Jywsuperman/LGSP.

</details>


### [88] [MCA-LLaVA: Manhattan Causal Attention for Reducing Hallucination in Large Vision-Language Models](https://arxiv.org/abs/2507.09184)
*Qiyan Zhao,Xiaofeng Zhang,Yiheng Li,Yun Xing,Xiaosong Yuan,Feilong Tang,Sinan Fan,Xuhang Chen,Xuyao Zhang,Dahan Wang*

Main category: cs.CV

TL;DR: The paper addresses hallucinations in LVLMs caused by RoPE's long-term decay, proposing MCA-LLaVA to mitigate image alignment bias by extending decay to 2D spatial modeling.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in LVLMs stem from misaligned multimodal features due to RoPE's long-term decay, which biases image token perception.

Method: Proposes MCA-LLaVA, using Manhattan distance to extend RoPE's decay to 2D spatial modeling, improving image-instruction interaction.

Result: MCA-LLaVA reduces hallucinations and improves alignment, validated across benchmarks.

Conclusion: MCA-LLaVA effectively mitigates image alignment bias, enhancing LVLM performance.

Abstract: Hallucinations pose a significant challenge in Large Vision Language Models
(LVLMs), with misalignment between multimodal features identified as a key
contributing factor. This paper reveals the negative impact of the long-term
decay in Rotary Position Encoding (RoPE), used for positional modeling in
LVLMs, on multimodal alignment. Concretely, under long-term decay, instruction
tokens exhibit uneven perception of image tokens located at different positions
within the two-dimensional space: prioritizing image tokens from the
bottom-right region since in the one-dimensional sequence, these tokens are
positionally closer to the instruction tokens. This biased perception leads to
insufficient image-instruction interaction and suboptimal multimodal alignment.
We refer to this phenomenon as image alignment bias. To enhance instruction's
perception of image tokens at different spatial locations, we propose
MCA-LLaVA, based on Manhattan distance, which extends the long-term decay to a
two-dimensional, multi-directional spatial decay. MCA-LLaVA integrates the
one-dimensional sequence order and two-dimensional spatial position of image
tokens for positional modeling, mitigating hallucinations by alleviating image
alignment bias. Experimental results of MCA-LLaVA across various hallucination
and general benchmarks demonstrate its effectiveness and generality. The code
can be accessed in https://github.com/ErikZ719/MCA-LLaVA.

</details>


### [89] [THYME: Temporal Hierarchical-Cyclic Interactivity Modeling for Video Scene Graphs in Aerial Footage](https://arxiv.org/abs/2507.09200)
*Trong-Thuan Nguyen,Pha Nguyen,Jackson Cothren,Alper Yilmaz,Minh-Triet Tran,Khoa Luu*

Main category: cs.CV

TL;DR: THYME integrates hierarchical feature aggregation and cyclic temporal refinement for dynamic scene graph generation, outperforming state-of-the-art methods on ASPIRe and AeroEye-v1.0 datasets.


<details>
  <summary>Details</summary>
Motivation: Address fragmented representations in video scene graph generation by capturing fine-grained spatial details and long-range temporal dependencies.

Method: Temporal Hierarchical Cyclic Scene Graph (THYME) combines hierarchical feature aggregation with cyclic temporal refinement.

Result: THYME outperforms existing methods, providing accurate and coherent scene graphs in ground-view and aerial scenarios.

Conclusion: THYME and AeroEye-v1.0 dataset advance dynamic scene understanding, offering robust solutions for video applications.

Abstract: The rapid proliferation of video in applications such as autonomous driving,
surveillance, and sports analytics necessitates robust methods for dynamic
scene understanding. Despite advances in static scene graph generation and
early attempts at video scene graph generation, previous methods often suffer
from fragmented representations, failing to capture fine-grained spatial
details and long-range temporal dependencies simultaneously. To address these
limitations, we introduce the Temporal Hierarchical Cyclic Scene Graph (THYME)
approach, which synergistically integrates hierarchical feature aggregation
with cyclic temporal refinement to address these limitations. In particular,
THYME effectively models multi-scale spatial context and enforces temporal
consistency across frames, yielding more accurate and coherent scene graphs. In
addition, we present AeroEye-v1.0, a novel aerial video dataset enriched with
five types of interactivity that overcome the constraints of existing datasets
and provide a comprehensive benchmark for dynamic scene graph generation.
Empirically, extensive experiments on ASPIRe and AeroEye-v1.0 demonstrate that
the proposed THYME approach outperforms state-of-the-art methods, offering
improved scene understanding in ground-view and aerial scenarios.

</details>


### [90] [Visual Surface Wave Elastography: Revealing Subsurface Physical Properties via Visible Surface Waves](https://arxiv.org/abs/2507.09207)
*Alexander C. Ogren,Berthy T. Feng,Jihoon Ahn,Katherine L. Bouman,Chiara Daraio*

Main category: cs.CV

TL;DR: A method to infer material thickness and stiffness from video of surface waves, validated with simulations and real data.


<details>
  <summary>Details</summary>
Motivation: Wave propagation reveals subsurface properties, useful for health monitoring and human-computer interaction.

Method: Extract dispersion relation from video, solve physics-based optimization for thickness and stiffness.

Result: Strong agreement with ground-truth measurements in simulations and real data.

Conclusion: Proof-of-concept for at-home health monitoring and broader applications like human-computer interaction.

Abstract: Wave propagation on the surface of a material contains information about
physical properties beneath its surface. We propose a method for inferring the
thickness and stiffness of a structure from just a video of waves on its
surface. Our method works by extracting a dispersion relation from the video
and then solving a physics-based optimization problem to find the best-fitting
thickness and stiffness parameters. We validate our method on both simulated
and real data, in both cases showing strong agreement with ground-truth
measurements. Our technique provides a proof-of-concept for at-home health
monitoring of medically-informative tissue properties, and it is further
applicable to fields such as human-computer interaction.

</details>


### [91] [Uncertainty-Driven Expert Control: Enhancing the Reliability of Medical Vision-Language Models](https://arxiv.org/abs/2507.09209)
*Xiao Liang,Di Wang,Zhicheng Jiao,Ronghan Li,Pengfei Yang,Quan Wang,Tat-Seng Chua*

Main category: cs.CV

TL;DR: Proposes Expert-CFG, an expert-in-the-loop framework to align MedVLM with clinical expertise without additional training, outperforming larger models.


<details>
  <summary>Details</summary>
Motivation: Address probabilistic uncertainties and misalignments in MedVLMs, which are critical in medical applications.

Method: Uses uncertainty estimation, expert-guided retrieval, and classifier-free guidance to refine outputs.

Result: Outperforms 13B-parameter models with a 4.2B-parameter setup, showing feasibility in resource-limited settings.

Conclusion: Expert-CFG effectively aligns MedVLM with clinical expertise, offering a cost-efficient solution for medical applications.

Abstract: The rapid advancements in Vision Language Models (VLMs) have prompted the
development of multi-modal medical assistant systems. Despite this progress,
current models still have inherent probabilistic uncertainties, often producing
erroneous or unverified responses-an issue with serious implications in medical
applications. Existing methods aim to enhance the performance of Medical Vision
Language Model (MedVLM) by adjusting model structure, fine-tuning with
high-quality data, or through preference fine-tuning. However, these
training-dependent strategies are costly and still lack sufficient alignment
with clinical expertise. To address these issues, we propose an
expert-in-the-loop framework named Expert-Controlled Classifier-Free Guidance
(Expert-CFG) to align MedVLM with clinical expertise without additional
training. This framework introduces an uncertainty estimation strategy to
identify unreliable outputs. It then retrieves relevant references to assist
experts in highlighting key terms and applies classifier-free guidance to
refine the token embeddings of MedVLM, ensuring that the adjusted outputs are
correct and align with expert highlights. Evaluations across three medical
visual question answering benchmarks demonstrate that the proposed Expert-CFG,
with 4.2B parameters and limited expert annotations, outperforms
state-of-the-art models with 13B parameters. The results demonstrate the
feasibility of deploying such a system in resource-limited settings for
clinical use.

</details>


### [92] [Stereo-based 3D Anomaly Object Detection for Autonomous Driving: A New Dataset and Baseline](https://arxiv.org/abs/2507.09214)
*Shiyi Mu,Zichong Gu,Hanqi Lyu,Yilin Gao,Shugong Xu*

Main category: cs.CV

TL;DR: The paper proposes S3AD, a Stereo-based 3D Anomaly object Detection algorithm, to improve generalization for arbitrary 3D objects and introduces KITTI-AR datasets for evaluation.


<details>
  <summary>Details</summary>
Motivation: Address misdetection of rare anomalies in 3D detection models by enhancing generalization and anomaly filtering capabilities.

Method: Decouples 2D and 3D training, uses anomaly scoring based on foreground confidence, and synthesizes KITTI-AR datasets for evaluation.

Result: S3AD improves generalization for arbitrary 3D objects and achieves target-level anomaly scoring. KITTI-AR datasets validate performance.

Conclusion: S3AD and KITTI-AR enhance 3D anomaly detection, addressing limitations in closed-set training and sample diversity.

Abstract: 3D detection technology is widely used in the field of autonomous driving,
with its application scenarios gradually expanding from enclosed highways to
open conventional roads. For rare anomaly categories that appear on the road,
3D detection models trained on closed sets often misdetect or fail to detect
anomaly objects. To address this risk, it is necessary to enhance the
generalization ability of 3D detection models for targets of arbitrary shapes
and to possess the capability to filter out anomalies. The generalization of 3D
detection is limited by two factors: the coupled training of 2D and 3D, and the
insufficient diversity in the scale distribution of training samples. This
paper proposes a Stereo-based 3D Anomaly object Detection (S3AD) algorithm,
which decouples the training strategy of 3D and 2D to release the
generalization ability for arbitrary 3D foreground detection, and proposes an
anomaly scoring algorithm based on foreground confidence prediction, achieving
target-level anomaly scoring. In order to further verify and enhance the
generalization of anomaly detection, we use a 3D rendering method to synthesize
two augmented reality binocular stereo 3D detection datasets which named
KITTI-AR. KITTI-AR extends upon KITTI by adding 97 new categories, totaling 6k
pairs of stereo images. The KITTI-AR-ExD subset includes 39 common categories
as extra training data to address the sparse sample distribution issue.
Additionally, 58 rare categories form the KITTI-AR-OoD subset, which are not
used in training to simulate zero-shot scenarios in real-world settings, solely
for evaluating 3D anomaly detection. Finally, the performance of the algorithm
and the dataset is verified in the experiments. (Code and dataset can be
obtained at https://github.com/xxxx/xxx).

</details>


### [93] [360-Degree Full-view Image Segmentation by Spherical Convolution compatible with Large-scale Planar Pre-trained Models](https://arxiv.org/abs/2507.09216)
*Jingguo Liu,Han Yu,Shigang Li,Jianfeng Li*

Main category: cs.CV

TL;DR: A novel spherical sampling method for panoramic images is introduced to leverage existing 2D pre-trained models, addressing distortion issues and improving performance in tasks like segmentation.


<details>
  <summary>Details</summary>
Motivation: Current 2D pre-trained models struggle with panoramic image distortions and lack large-scale datasets, limiting their effectiveness.

Method: Proposes spherical discrete sampling based on pre-trained model weights to mitigate distortions and uses these features for panoramic image segmentation.

Result: Achieves favorable initial training values and commendable results on the Stanford2D3D indoor dataset.

Conclusion: The spherical sampling method effectively bridges the gap between 2D pre-trained models and panoramic image tasks, enhancing performance.

Abstract: Due to the current lack of large-scale datasets at the million-scale level,
tasks involving panoramic images predominantly rely on existing two-dimensional
pre-trained image benchmark models as backbone networks. However, these
networks are not equipped to recognize the distortions and discontinuities
inherent in panoramic images, which adversely affects their performance in such
tasks. In this paper, we introduce a novel spherical sampling method for
panoramic images that enables the direct utilization of existing pre-trained
models developed for two-dimensional images. Our method employs spherical
discrete sampling based on the weights of the pre-trained models, effectively
mitigating distortions while achieving favorable initial training values.
Additionally, we apply the proposed sampling method to panoramic image
segmentation, utilizing features obtained from the spherical model as masks for
specific channel attentions, which yields commendable results on commonly used
indoor datasets, Stanford2D3D.

</details>


### [94] [Online Long-term Point Tracking in the Foundation Model Era](https://arxiv.org/abs/2507.09217)
*Grkay Aydemir*

Main category: cs.CV

TL;DR: The paper introduces Track-On, a transformer-based model for online long-term point tracking, addressing the challenge of causal processing without future frame access.


<details>
  <summary>Details</summary>
Motivation: Real-world applications like robotics and augmented reality require online point tracking, but existing methods rely on offline settings with future frame access. This work aims to enable accurate, viewpoint-invariant tracking in a causal, online setting.

Method: The study evaluates visual foundation models for spatial feature enrichment and introduces Track-On, a transformer-based model that treats tracked points as queries, processing frames sequentially with memory for coherence.

Result: Track-On achieves state-of-the-art performance on seven public benchmarks, proving the feasibility of long-term online tracking without future frame access.

Conclusion: The research demonstrates that dedicated designs like Track-On can effectively address the challenges of online point tracking, leveraging visual foundation models and transformer architectures for robust performance.

Abstract: Point tracking aims to identify the same physical point across video frames
and serves as a geometry-aware representation of motion. This representation
supports a wide range of applications, from robotics to augmented reality, by
enabling accurate modeling of dynamic environments. Most existing long-term
tracking approaches operate in an offline setting, where future frames are
available to refine predictions and recover from occlusions. However,
real-world scenarios often demand online predictions: the model must operate
causally, using only current and past frames. This constraint is critical in
streaming video and embodied AI, where decisions must be made immediately based
on past observations. Under such constraints, viewpoint invariance becomes
essential. Visual foundation models, trained on diverse large-scale datasets,
offer the potential for robust geometric representations. While they lack
temporal reasoning on their own, they can be integrated into tracking pipelines
to enrich spatial features. In this thesis, we address the problem of long-term
point tracking in an online setting, where frames are processed sequentially
without access to future information or sliding windows. We begin by evaluating
the suitability of visual foundation models for this task and find that they
can serve as useful initializations and be integrated into tracking pipelines.
However, to enable long-term tracking in an online setting, a dedicated design
is still required. In particular, maintaining coherence over time in this
causal regime requires memory to propagate appearance and context across
frames. To address this, we introduce Track-On, a transformer-based model that
treats each tracked point as a query and processes video frames one at a time.
Track-On sets a new state of the art across seven public benchmarks,
demonstrating the feasibility of long-term tracking without future access.

</details>


### [95] [Calibrated and Robust Foundation Models for Vision-Language and Medical Image Tasks Under Distribution Shift](https://arxiv.org/abs/2507.09222)
*Behraj Khan,Tahir Syed*

Main category: cs.CV

TL;DR: StaRFM is a unified framework addressing distribution shift and confidence misalignment in foundation models like CLIP and SAM, improving performance across vision and medical tasks.


<details>
  <summary>Details</summary>
Motivation: Foundation models face challenges like distribution shift and confidence misalignment, which hinder deployment. Existing solutions are domain-specific, lacking a unified approach.

Method: StaRFM introduces Fisher information penalty (FIP) for covariate shift and confidence misalignment penalty (CMP) for uncertainty calibration, with theoretical PAC-Bayes bounds.

Result: StaRFM improves accuracy (+3.5%), reduces ECE (28%), achieves 84.7% DSC in medical segmentation, and lowers cross-domain performance gap by 40%.

Conclusion: StaRFM is a plug-and-play solution for foundation models, offering consistent performance gains across diverse tasks with minimal architectural changes.

Abstract: Foundation models like CLIP and SAM have transformed computer vision and
medical imaging via low-shot transfer learning. However, deployment of these
models hindered by two key challenges: \textit{distribution shift} between
training and test data, and \textit{confidence misalignment} that leads to
overconfident incorrect predictions. These issues manifest differently in
vision-language classification and medical segmentation tasks, yet existing
solutions remain domain-specific. We propose \textit{StaRFM}, a unified
framework addressing both challenges. It introduces a Fisher information
penalty (FIP), extended to 3D medical data via patch-wise regularization, to
reduce covariate shift in CLIP and SAM embeddings. Additionally, a confidence
misalignment penalty (CMP), reformulated for voxel-level predictions,
calibrates uncertainty in segmentation tasks. We theoretically derive PAC-Bayes
bounds showing FIP controls generalization via the Fisher-Rao norm, while CMP
minimizes calibration error through Brier score optimization. StaRFM shows
consistent performance like \texttt{+}3.5\% accuracy and 28\% lower ECE on 19
vision datasets (e.g., ImageNet, Office-Home), 84.7\% DSC and 4.8mm HD95 in
medical segmentation (e.g., BraTS, ATLAS), and 40\% lower cross-domain
performance gap compared to prior benchmarking methods. The framework is
plug-and-play, requiring minimal architectural changes for seamless integration
with foundation models. Code and models will be released at
https://anonymous.4open.science/r/StaRFM-C0CD/README.md

</details>


### [96] [EgoAnimate: Generating Human Animations from Egocentric top-down Views](https://arxiv.org/abs/2507.09230)
*G. Kutay Trkoglu,Julian Tanke,Iheb Belgacem,Lev Markhasin*

Main category: cs.CV

TL;DR: The paper introduces a generative prior-based method using Stable Diffusion to reconstruct animatable avatars from egocentric views, addressing occlusions and distortions.


<details>
  <summary>Details</summary>
Motivation: To enable accurate telepresence with minimal input by leveraging egocentric views, overcoming challenges like occlusions and distorted proportions.

Method: Uses a Stable Diffusion backbone and ControlNet to generate realistic frontal views from top-down egocentric images, then feeds these into an image-to-motion model.

Result: The method reduces training burden and improves generalizability, enabling avatar motion generation from minimal input.

Conclusion: This approach advances accessible and generalizable telepresence systems by reconstructing avatars from egocentric inputs.

Abstract: An ideal digital telepresence experience requires accurate replication of a
person's body, clothing, and movements. To capture and transfer these movements
into virtual reality, the egocentric (first-person) perspective can be adopted,
which enables the use of a portable and cost-effective device without
front-view cameras. However, this viewpoint introduces challenges such as
occlusions and distorted body proportions.
  There are few works reconstructing human appearance from egocentric views,
and none use a generative prior-based approach. Some methods create avatars
from a single egocentric image during inference, but still rely on multi-view
datasets during training. To our knowledge, this is the first study using a
generative backbone to reconstruct animatable avatars from egocentric inputs.
Based on Stable Diffusion, our method reduces training burden and improves
generalizability.
  Inspired by methods such as SiTH and MagicMan, which perform 360-degree
reconstruction from a frontal image, we introduce a pipeline that generates
realistic frontal views from occluded top-down images using ControlNet and a
Stable Diffusion backbone.
  Our goal is to convert a single top-down egocentric image into a realistic
frontal representation and feed it into an image-to-motion model. This enables
generation of avatar motions from minimal input, paving the way for more
accessible and generalizable telepresence systems.

</details>


### [97] [PPJudge: Towards Human-Aligned Assessment of Artistic Painting Process](https://arxiv.org/abs/2507.09242)
*Shiqi Jiang,Xinpeng Li,Xi Mao,Changbo Wang,Chenhui Li*

Main category: cs.CV

TL;DR: A novel framework for assessing the artistic painting process is introduced, addressing the gap in existing methods that focus only on static images. The framework includes a dataset (PPAD) and a Transformer-based model (PPJudge) for improved accuracy and human alignment.


<details>
  <summary>Details</summary>
Motivation: Existing methods overlook the dynamic, multi-stage nature of artistic painting, focusing only on static images. This gap is addressed by proposing a human-aligned assessment framework.

Method: The framework includes the Painting Process Assessment Dataset (PPAD) and PPJudge, a Transformer-based model with temporally-aware positional encoding and a heterogeneous mixture-of-experts architecture.

Result: The method outperforms baselines in accuracy, robustness, and alignment with human judgment.

Conclusion: The framework offers new insights into computational creativity and art education, demonstrating superior performance over existing approaches.

Abstract: Artistic image assessment has become a prominent research area in computer
vision. In recent years, the field has witnessed a proliferation of datasets
and methods designed to evaluate the aesthetic quality of paintings. However,
most existing approaches focus solely on static final images, overlooking the
dynamic and multi-stage nature of the artistic painting process. To address
this gap, we propose a novel framework for human-aligned assessment of painting
processes. Specifically, we introduce the Painting Process Assessment Dataset
(PPAD), the first large-scale dataset comprising real and synthetic painting
process images, annotated by domain experts across eight detailed attributes.
Furthermore, we present PPJudge (Painting Process Judge), a Transformer-based
model enhanced with temporally-aware positional encoding and a heterogeneous
mixture-of-experts architecture, enabling effective assessment of the painting
process. Experimental results demonstrate that our method outperforms existing
baselines in accuracy, robustness, and alignment with human judgment, offering
new insights into computational creativity and art education.

</details>


### [98] [AGCD-Net: Attention Guided Context Debiasing Network for Emotion Recognition](https://arxiv.org/abs/2507.09248)
*Varsha Devi,Amine Bohi,Pardeep Kumar*

Main category: cs.CV

TL;DR: AGCD-Net, a novel model for context-aware emotion recognition, mitigates context bias using hybrid ConvNeXt and causal intervention, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for context-aware emotion recognition suffer from context bias, leading to spurious correlations between background and emotions.

Method: AGCD-Net integrates Hybrid ConvNeXt (with Spatial Transformer and Squeeze-and-Excitation layers) and an Attention Guided - Causal Intervention Module (AG-CIM) to debias context features.

Result: AGCD-Net outperforms existing methods on the CAER-S dataset, demonstrating robust emotion recognition.

Conclusion: The study highlights the effectiveness of causal debiasing in improving emotion recognition accuracy in complex real-world scenarios.

Abstract: Context-aware emotion recognition (CAER) enhances affective computing in
real-world scenarios, but traditional methods often suffer from context
bias-spurious correlation between background context and emotion labels (e.g.
associating ``garden'' with ``happy''). In this paper, we propose
\textbf{AGCD-Net}, an Attention Guided Context Debiasing model that introduces
\textit{Hybrid ConvNeXt}, a novel convolutional encoder that extends the
ConvNeXt backbone by integrating Spatial Transformer Network and
Squeeze-and-Excitation layers for enhanced feature recalibration. At the core
of AGCD-Net is the Attention Guided - Causal Intervention Module (AG-CIM),
which applies causal theory, perturbs context features, isolates spurious
correlations, and performs an attention-driven correction guided by face
features to mitigate context bias. Experimental results on the CAER-S dataset
demonstrate the effectiveness of AGCD-Net, achieving state-of-the-art
performance and highlighting the importance of causal debiasing for robust
emotion recognition in complex settings.

</details>


### [99] [Ambiguity-Aware and High-Order Relation Learning for Multi-Grained Image-Text Matching](https://arxiv.org/abs/2507.09256)
*Junyu Chen,Yihua Gao,Mingyuan Ge,Mingyong Li*

Main category: cs.CV

TL;DR: AAHR framework improves image-text matching by addressing semantic ambiguities and leveraging high-order relations through dynamic clustering, adaptive aggregation, and GNN-enhanced interactions.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with semantic ambiguities and fail to utilize neighborhood relationships, limiting high-order shared knowledge learning.

Method: AAHR uses dynamic clustering prototype contrastive learning, global/local feature extraction, adaptive aggregation, intra/inter-modal correlation matrices, GNN, and momentum contrastive learning.

Result: AAHR outperforms state-of-the-art methods on Flickr30K, MSCOCO, and ECCV Caption datasets, enhancing accuracy and efficiency.

Conclusion: AAHR effectively mitigates ambiguities and leverages high-order relations, advancing image-text matching performance.

Abstract: Image-text matching is crucial for bridging the semantic gap between computer
vision and natural language processing. However, existing methods still face
challenges in handling high-order associations and semantic ambiguities among
similar instances. These ambiguities arise from subtle differences between soft
positive samples (semantically similar but incorrectly labeled) and soft
negative samples (locally matched but globally inconsistent), creating matching
uncertainties. Furthermore, current methods fail to fully utilize the
neighborhood relationships among semantically similar instances within training
batches, limiting the model's ability to learn high-order shared knowledge.
This paper proposes the Ambiguity-Aware and High-order Relation learning
framework (AAHR) to address these issues. AAHR constructs a unified
representation space through dynamic clustering prototype contrastive learning,
effectively mitigating the soft positive sample problem. The framework
introduces global and local feature extraction mechanisms and an adaptive
aggregation network, significantly enhancing full-grained semantic
understanding capabilities. Additionally, AAHR employs intra-modal and
inter-modal correlation matrices to investigate neighborhood relationships
among sample instances thoroughly. It incorporates GNN to enhance semantic
interactions between instances. Furthermore, AAHR integrates momentum
contrastive learning to expand the negative sample set. These combined
strategies significantly improve the model's ability to discriminate between
features. Experimental results demonstrate that AAHR outperforms existing
state-of-the-art methods on Flickr30K, MSCOCO, and ECCV Caption datasets,
considerably improving the accuracy and efficiency of image-text matching. The
code and model checkpoints for this research are available at
https://github.com/Image-Text-Matching/AAHR .

</details>


### [100] [SAGE: Segment-Aware Gloss-Free Encoding for Token-Efficient Sign Language Translation](https://arxiv.org/abs/2507.09266)
*JianHe Low,Ozge Mercanoglu Sincan,Richard Bowden*

Main category: cs.CV

TL;DR: A segment-aware visual tokenization framework for gloss-free SLT reduces input sequence length and computational demands while improving performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the scalability and computational inefficiencies in gloss-free SLT models as large-scale datasets become common.

Method: Proposes a segment-aware visual tokenization framework with token-to-token contrastive alignment and dual-level supervision for cross-modal alignment.

Result: Achieves up to 50% shorter input sequences, 2.67x lower memory usage, and outperforms state-of-the-art on PHOENIX14T.

Conclusion: The framework offers scalable and efficient SLT without gloss annotations, with superior performance and reduced resource demands.

Abstract: Gloss-free Sign Language Translation (SLT) has advanced rapidly, achieving
strong performances without relying on gloss annotations. However, these gains
have often come with increased model complexity and high computational demands,
raising concerns about scalability, especially as large-scale sign language
datasets become more common. We propose a segment-aware visual tokenization
framework that leverages sign segmentation to convert continuous video into
discrete, sign-informed visual tokens. This reduces input sequence length by up
to 50% compared to prior methods, resulting in up to 2.67x lower memory usage
and better scalability on larger datasets. To bridge the visual and linguistic
modalities, we introduce a token-to-token contrastive alignment objective,
along with a dual-level supervision that aligns both language embeddings and
intermediate hidden states. This improves fine-grained cross-modal alignment
without relying on gloss-level supervision. Our approach notably exceeds the
performance of state-of-the-art methods on the PHOENIX14T benchmark, while
significantly reducing sequence length. Further experiments also demonstrate
our improved performance over prior work under comparable sequence-lengths,
validating the potential of our tokenization and alignment strategies.

</details>


### [101] [Cross Knowledge Distillation between Artificial and Spiking Neural Networks](https://arxiv.org/abs/2507.09269)
*Shuhan Ye,Yuanbin Qian,Chong Wang,Sunqi Lin,Jiazhen Xu,Jiangbo Qian,Yuqi Li*

Main category: cs.CV

TL;DR: The paper proposes Cross Knowledge Distillation (CKD) to improve Spiking Neural Networks (SNNs) performance on DVS data by leveraging RGB data and ANNs, addressing cross-modality and cross-architecture challenges.


<details>
  <summary>Details</summary>
Motivation: SNNs lag behind ANNs in performance due to limited annotated event-based datasets and immature architectures. The goal is to enhance SNNs using knowledge distillation from ANNs and RGB data.

Method: CKD uses semantic similarity and sliding replacement for cross-modality and indirect phased knowledge distillation for cross-architecture challenges.

Result: Experiments on N-Caltech101 and CEP-DVS show CKD outperforms current state-of-the-art methods.

Conclusion: CKD effectively improves SNN performance on DVS data by leveraging ANNs and RGB data, addressing key challenges.

Abstract: Recently, Spiking Neural Networks (SNNs) have demonstrated rich potential in
computer vision domain due to their high biological plausibility, event-driven
characteristic and energy-saving efficiency. Still, limited annotated
event-based datasets and immature SNN architectures result in their performance
inferior to that of Artificial Neural Networks (ANNs). To enhance the
performance of SNNs on their optimal data format, DVS data, we explore using
RGB data and well-performing ANNs to implement knowledge distillation. In this
case, solving cross-modality and cross-architecture challenges is necessary. In
this paper, we propose cross knowledge distillation (CKD), which not only
leverages semantic similarity and sliding replacement to mitigate the
cross-modality challenge, but also uses an indirect phased knowledge
distillation to mitigate the cross-architecture challenge. We validated our
method on main-stream neuromorphic datasets, including N-Caltech101 and
CEP-DVS. The experimental results show that our method outperforms current
State-of-the-Art methods. The code will be available at
https://github.com/ShawnYE618/CKD

</details>


### [102] [Prompt4Trust: A Reinforcement Learning Prompt Augmentation Framework for Clinically-Aligned Confidence Calibration in Multimodal Large Language Models](https://arxiv.org/abs/2507.09279)
*Anita Kriz,Elizabeth Laura Janes,Xing Shen,Tal Arbel*

Main category: cs.CV

TL;DR: Prompt4Trust is a reinforcement learning framework for improving confidence calibration in multimodal large language models (MLLMs) for healthcare, enhancing both calibration and task accuracy.


<details>
  <summary>Details</summary>
Motivation: MLLMs in healthcare face issues with prompt sensitivity and overconfidence, which can undermine trust in clinical settings.

Method: A lightweight LLM is trained to generate auxiliary prompts for better confidence calibration in MLLMs, prioritizing clinical safety.

Result: Achieves state-of-the-art medical VQA performance on PMC-VQA and shows zero-shot generalization to larger MLLMs.

Conclusion: Prompt4Trust improves MLLM trustworthiness in safety-critical healthcare applications through automated, human-aligned prompt engineering.

Abstract: Multimodal large language models (MLLMs) hold considerable promise for
applications in healthcare. However, their deployment in safety-critical
settings is hindered by two key limitations: (i) sensitivity to prompt design,
and (ii) a tendency to generate incorrect responses with high confidence. As
clinicians may rely on a model's stated confidence to gauge the reliability of
its predictions, it is especially important that when a model expresses high
confidence, it is also highly accurate. We introduce Prompt4Trust, the first
reinforcement learning (RL) framework for prompt augmentation targeting
confidence calibration in MLLMs. A lightweight LLM is trained to produce
context-aware auxiliary prompts that guide a downstream task MLLM to generate
responses in which the expressed confidence more accurately reflects predictive
accuracy. Unlike conventional calibration techniques, Prompt4Trust specifically
prioritizes aspects of calibration most critical for safe and trustworthy
clinical decision-making. Beyond improvements driven by this clinically
motivated calibration objective, our proposed method also improves task
accuracy, achieving state-of-the-art medical visual question answering (VQA)
performance on the PMC-VQA benchmark, which is composed of multiple-choice
questions spanning diverse medical imaging modalities. Moreover, our framework
trained with a small downstream task MLLM showed promising zero-shot
generalization to larger MLLMs in our experiments, suggesting the potential for
scalable calibration without the associated computational costs. This work
demonstrates the potential of automated yet human-aligned prompt engineering
for improving the the trustworthiness of MLLMs in safety critical settings. Our
codebase can be found at https://github.com/xingbpshen/vccrl-llm.

</details>


### [103] [Generative Latent Kernel Modeling for Blind Motion Deblurring](https://arxiv.org/abs/2507.09285)
*Chenhao Ding,Jiangtao Zhang,Zongsheng Yue,Hui Wang,Qian Zhao,Deyu Meng*

Main category: cs.CV

TL;DR: A novel framework for blind motion deblurring (BMD) uses a deep generative model to improve blur kernel initialization, reducing sensitivity to initial conditions and enhancing performance.


<details>
  <summary>Details</summary>
Motivation: Existing deep prior-based BMD methods suffer from high non-convexity and sensitivity to initial blur kernel, limiting their effectiveness.

Method: The framework pre-trains a GAN-based kernel generator and initializer to encode kernel priors and provide better initialization, integrating seamlessly with existing BMD methods.

Result: The approach alleviates sensitivity to kernel initialization and achieves state-of-the-art performance on benchmark datasets, even for non-uniform motion deblurring.

Conclusion: The proposed framework effectively addresses initialization sensitivity in BMD and can be easily integrated into existing methods, offering improved performance.

Abstract: Deep prior-based approaches have demonstrated remarkable success in blind
motion deblurring (BMD) recently. These methods, however, are often limited by
the high non-convexity of the underlying optimization process in BMD, which
leads to extreme sensitivity to the initial blur kernel. To address this issue,
we propose a novel framework for BMD that leverages a deep generative model to
encode the kernel prior and induce a better initialization for the blur kernel.
Specifically, we pre-train a kernel generator based on a generative adversarial
network (GAN) to aptly characterize the kernel's prior distribution, as well as
a kernel initializer to provide a well-informed and high-quality starting point
for kernel estimation. By combining these two components, we constrain the BMD
solution within a compact latent kernel manifold, thus alleviating the
aforementioned sensitivity for kernel initialization. Notably, the kernel
generator and initializer are designed to be easily integrated with existing
BMD methods in a plug-and-play manner, enhancing their overall performance.
Furthermore, we extend our approach to tackle blind non-uniform motion
deblurring without the need for additional priors, achieving state-of-the-art
performance on challenging benchmark datasets. The source code is available at
https://github.com/dch0319/GLKM-Deblur.

</details>


### [104] [Supercharging Floorplan Localization with Semantic Rays](https://arxiv.org/abs/2507.09291)
*Yuval Grader,Hadar Averbuch-Elor*

Main category: cs.CV

TL;DR: A semantic-aware floorplan localization framework improves accuracy by jointly estimating depth and semantic rays, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Current floorplan localization techniques ignore rich semantic details like window and door locations, focusing only on structural cues.

Method: The framework constructs a coarse-to-fine probability volume by sampling rays, refining high-probability regions, and predicting 2D location and orientation.

Result: Outperforms state-of-the-art methods on benchmarks, with significant recall improvements and added benefits from metadata like room labels.

Conclusion: The semantic-aware approach enhances localization accuracy and efficiency, demonstrating the value of incorporating semantic details.

Abstract: Floorplans provide a compact representation of the building's structure,
revealing not only layout information but also detailed semantics such as the
locations of windows and doors. However, contemporary floorplan localization
techniques mostly focus on matching depth-based structural cues, ignoring the
rich semantics communicated within floorplans. In this work, we introduce a
semantic-aware localization framework that jointly estimates depth and semantic
rays, consolidating over both for predicting a structural-semantic probability
volume. Our probability volume is constructed in a coarse-to-fine manner: We
first sample a small set of rays to obtain an initial low-resolution
probability volume. We then refine these probabilities by performing a denser
sampling only in high-probability regions and process the refined values for
predicting a 2D location and orientation angle. We conduct an evaluation on two
standard floorplan localization benchmarks. Our experiments demonstrate that
our approach substantially outperforms state-of-the-art methods, achieving
significant improvements in recall metrics compared to prior works. Moreover,
we show that our framework can easily incorporate additional metadata such as
room labels, enabling additional gains in both accuracy and efficiency.

</details>


### [105] [Geo-RepNet: Geometry-Aware Representation Learning for Surgical Phase Recognition in Endoscopic Submucosal Dissection](https://arxiv.org/abs/2507.09294)
*Rui Tang,Haochen Yin,Guankun Wang,Long Bai,An Wang,Huxin Gao,Jiazheng Wang,Hongliang Ren*

Main category: cs.CV

TL;DR: Geo-RepNet integrates RGB and depth data for surgical phase recognition, outperforming existing methods in complex ESD procedures.


<details>
  <summary>Details</summary>
Motivation: High visual similarity in surgical phases and lack of structural cues in RGB images necessitate depth information for better recognition.

Method: Proposes Geo-RepNet with Depth-Guided Geometric Prior Generation (DGPG) and Geometry-Enhanced Multi-scale Attention (GEMA) modules, built on a RepVGG backbone.

Result: Achieves state-of-the-art performance on a nine-phase ESD dataset, demonstrating robustness and efficiency.

Conclusion: Depth information significantly enhances surgical phase recognition, with Geo-RepNet offering a practical solution for complex surgical scenes.

Abstract: Surgical phase recognition plays a critical role in developing intelligent
assistance systems for minimally invasive procedures such as Endoscopic
Submucosal Dissection (ESD). However, the high visual similarity across
different phases and the lack of structural cues in RGB images pose significant
challenges. Depth information offers valuable geometric cues that can
complement appearance features by providing insights into spatial relationships
and anatomical structures. In this paper, we pioneer the use of depth
information for surgical phase recognition and propose Geo-RepNet, a
geometry-aware convolutional framework that integrates RGB image and depth
information to enhance recognition performance in complex surgical scenes.
Built upon a re-parameterizable RepVGG backbone, Geo-RepNet incorporates the
Depth-Guided Geometric Prior Generation (DGPG) module that extracts geometry
priors from raw depth maps, and the Geometry-Enhanced Multi-scale Attention
(GEMA) to inject spatial guidance through geometry-aware cross-attention and
efficient multi-scale aggregation. To evaluate the effectiveness of our
approach, we construct a nine-phase ESD dataset with dense frame-level
annotations from real-world ESD videos. Extensive experiments on the proposed
dataset demonstrate that Geo-RepNet achieves state-of-the-art performance while
maintaining robustness and high computational efficiency under complex and
low-texture surgical environments.

</details>


### [106] [ViT-ProtoNet for Few-Shot Image Classification: A Multi-Benchmark Evaluation](https://arxiv.org/abs/2507.09299)
*Abdulvahap Mutlu,engl Doan,Trker Tuncer*

Main category: cs.CV

TL;DR: ViT-ProtoNet integrates Vision Transformers into Prototypical Networks for few-shot image classification, outperforming CNN-based methods and setting a new benchmark for transformer-based meta-learners.


<details>
  <summary>Details</summary>
Motivation: To leverage the underutilized representational power of Vision Transformers in few-shot image classification.

Method: Integrates ViT-Small into Prototypical Networks, averaging class conditional token embeddings from support examples to create robust prototypes.

Result: Outperforms CNN-based prototypical networks by up to 3.2% in 5-shot accuracy and shows superior feature separability.

Conclusion: ViT-ProtoNet is a powerful, flexible approach for few-shot classification and sets a new baseline for transformer-based meta-learners.

Abstract: The remarkable representational power of Vision Transformers (ViTs) remains
underutilized in few-shot image classification. In this work, we introduce
ViT-ProtoNet, which integrates a ViT-Small backbone into the Prototypical
Network framework. By averaging class conditional token embeddings from a
handful of support examples, ViT-ProtoNet constructs robust prototypes that
generalize to novel categories under 5-shot settings. We conduct an extensive
empirical evaluation on four standard benchmarks: Mini-ImageNet, FC100,
CUB-200, and CIFAR-FS, including overlapped support variants to assess
robustness. Across all splits, ViT-ProtoNet consistently outperforms CNN-based
prototypical counterparts, achieving up to a 3.2\% improvement in 5-shot
accuracy and demonstrating superior feature separability in latent space.
Furthermore, it outperforms or is competitive with transformer-based
competitors using a more lightweight backbone. Comprehensive ablations examine
the impact of transformer depth, patch size, and fine-tuning strategy. To
foster reproducibility, we release code and pretrained weights. Our results
establish ViT-ProtoNet as a powerful, flexible approach for few-shot
classification and set a new baseline for transformer-based meta-learners.

</details>


### [107] [DAA*: Deep Angular A Star for Image-based Path Planning](https://arxiv.org/abs/2507.09305)
*Zhiwei Xu*

Main category: cs.CV

TL;DR: DAA* improves path imitation learning by integrating path angular freedom (PAF) into A*, enhancing path similarity and smoothness. It outperforms neural A* and TransPath in evaluations.


<details>
  <summary>Details</summary>
Motivation: Path smoothness is often neglected in imitation learning from expert demonstrations, limiting path similarity and optimality.

Method: Introduces deep angular A* (DAA*) with PAF to balance path smoothness and similarity, optimizing path shortening and smoothing.

Result: DAA* shows significant improvements in path similarity metrics (SPR, ASIM, PSIM) and outperforms state-of-the-art methods.

Conclusion: DAA* effectively balances path optimality and smoothness, demonstrating superior performance in path imitation learning.

Abstract: Path smoothness is often overlooked in path imitation learning from expert
demonstrations. In this paper, we introduce a novel learning method, termed
deep angular A* (DAA*), by incorporating the proposed path angular freedom
(PAF) into A* to improve path similarity through adaptive path smoothness. The
PAF aims to explore the effect of move angles on path node expansion by finding
the trade-off between their minimum and maximum values, allowing for high
adaptiveness for imitation learning. DAA* improves path optimality by closely
aligning with the reference path through joint optimization of path shortening
and smoothing, which correspond to heuristic distance and PAF, respectively.
Throughout comprehensive evaluations on 7 datasets, including 4 maze datasets,
2 video-game datasets, and a real-world drone-view dataset containing 2
scenarios, we demonstrate remarkable improvements of our DAA* over neural A* in
path similarity between the predicted and reference paths with a shorter path
length when the shortest path is plausible, improving by 9.0% SPR, 6.9% ASIM,
and 3.9% PSIM. Furthermore, when jointly learning pathfinding with both path
loss and path probability map loss, DAA* significantly outperforms the
state-of-the-art TransPath by 6.7% SPR, 6.5% PSIM, and 3.7% ASIM. We also
discuss the minor trade-off between path optimality and search efficiency where
applicable.

</details>


### [108] [AlphaVAE: Unified End-to-End RGBA Image Reconstruction and Generation with Alpha-Aware Representation Learning](https://arxiv.org/abs/2507.09308)
*Zile Wang,Hao Yu,Jiabo Zhan,Chun Yuan*

Main category: cs.CV

TL;DR: ALPHA introduces the first RGBA benchmark and ALPHAVAE, a VAE for RGBA images, outperforming prior methods in reconstruction and generation.


<details>
  <summary>Details</summary>
Motivation: Existing latent diffusion models focus on RGB images, leaving RGBA (transparent/layered) content underexplored due to lack of benchmarks.

Method: ALPHAVAE extends a pretrained RGB VAE with an alpha channel, trained using a composite objective for fidelity across RGB and alpha.

Result: ALPHAVAE improves PSNR by +4.9 dB and SSIM by +3.2% over LayerDiffuse, enabling better transparent image generation.

Conclusion: ALPHA and ALPHAVAE address the gap in RGBA content generation, offering a scalable solution with released resources for reproducibility.

Abstract: Recent advances in latent diffusion models have achieved remarkable results
in high-fidelity RGB image synthesis by leveraging pretrained VAEs to compress
and reconstruct pixel data at low computational cost. However, the generation
of transparent or layered content (RGBA image) remains largely unexplored, due
to the lack of large-scale benchmarks. In this work, we propose ALPHA, the
first comprehensive RGBA benchmark that adapts standard RGB metrics to
four-channel images via alpha blending over canonical backgrounds. We further
introduce ALPHAVAE, a unified end-to-end RGBA VAE that extends a pretrained RGB
VAE by incorporating a dedicated alpha channel. The model is trained with a
composite objective that combines alpha-blended pixel reconstruction,
patch-level fidelity, perceptual consistency, and dual KL divergence
constraints to ensure latent fidelity across both RGB and alpha
representations. Our RGBA VAE, trained on only 8K images in contrast to 1M used
by prior methods, achieves a +4.9 dB improvement in PSNR and a +3.2% increase
in SSIM over LayerDiffuse in reconstruction. It also enables superior
transparent image generation when fine-tuned within a latent diffusion
framework. Our code, data, and models are released on
https://github.com/o0o0o00o0/AlphaVAE for reproducibility.

</details>


### [109] [ProactiveBench: A Comprehensive Benchmark Evaluating Proactive Interactions in Video Large Language Models](https://arxiv.org/abs/2507.09313)
*Yueqian Wang,Xiaojun Meng,Yifan Wang,Huishuai Zhang,Dongyan Zhao*

Main category: cs.CV

TL;DR: ProactiveBench is introduced as the first benchmark for evaluating proactive interaction in multimodal dialogue systems, with PAUC as a novel metric accounting for temporal dynamics.


<details>
  <summary>Details</summary>
Motivation: To address the need for proactive interaction in multimodal systems, especially in real-time scenarios like video playback, where traditional turn-by-turn dialogue falls short.

Method: ProactiveBench is developed as a benchmark, and PAUC is proposed as a metric to evaluate temporal dynamics of responses.

Result: PAUC aligns better with human preferences than traditional metrics, providing a more accurate assessment of proactive interaction.

Conclusion: PAUC offers a more faithful evaluation of user experience in proactive settings, advancing research in multimodal dialogue systems.

Abstract: With the growing research focus on multimodal dialogue systems, the
capability for proactive interaction is gradually gaining recognition. As an
alternative to conventional turn-by-turn dialogue, users increasingly expect
multimodal systems to be more initiative, for example, by autonomously
determining the timing of multi-turn responses in real time during video
playback. To facilitate progress in this emerging area, we introduce
ProactiveBench, the first comprehensive benchmark to evaluate a system's
ability to engage in proactive interaction. Since model responses are generated
at varying timestamps, we further propose PAUC, the first metric that accounts
for the temporal dynamics of model responses. This enables a more accurate
evaluation of systems operating in proactive settings. Through extensive
benchmarking of various baseline systems on ProactiveBench and a user study of
human preferences, we show that PAUC is in better agreement with human
preferences than traditional evaluation metrics, which typically only consider
the textual content of responses. These findings demonstrate that PAUC provides
a more faithful assessment of user experience in proactive interaction
scenarios. Project homepage:
https://github.com/yellow-binary-tree/ProactiveBench

</details>


### [110] [Dynamic Inter-Class Confusion-Aware Encoder for Audio-Visual Fusion in Human Activity Recognition](https://arxiv.org/abs/2507.09323)
*Kaixuan Cong,Yifan Wang,Rongkun Xue,Yuyang Jiang,Yiming Feng,Jing Yang*

Main category: cs.CV

TL;DR: The paper introduces DICCAE, an encoder for fine-grained audio-video alignment, addressing category confusion via dynamic confusion loss and a novel training framework. It achieves 65.5% top-1 accuracy on VGGSound.


<details>
  <summary>Details</summary>
Motivation: Existing audio-video pre-training lacks focus on distinguishing similar classes. The paper aims to enhance this by reinforcing cognitive induction and contrast.

Method: Proposes DICCAE with dynamic confusion loss and a training framework for audio-video fusion. Uses cluster-guided self-supervised pre-training to address data scarcity.

Result: Achieves 65.5% top-1 accuracy on VGGSound, with ablation studies validating module necessity.

Conclusion: DICCAE effectively improves fine-grained audio-video alignment and class distinction, demonstrating strong performance and feature quality.

Abstract: Humans do not understand individual events in isolation; rather, they
generalize concepts within classes and compare them to others. Existing
audio-video pre-training paradigms only focus on the alignment of the overall
audio-video modalities, without considering the reinforcement of distinguishing
easily confused classes through cognitive induction and contrast during
training. This paper proposes the Dynamic Inter-Class Confusion-Aware Encoder
(DICCAE), an encoder that aligns audio-video representations at a fine-grained,
category-level. DICCAE addresses category confusion by dynamically adjusting
the confusion loss based on inter-class confusion degrees, thereby enhancing
the model's ability to distinguish between similar activities. To further
extend the application of DICCAE, we also introduce a novel training framework
that incorporates both audio and video modalities, as well as their fusion. To
mitigate the scarcity of audio-video data in the human activity recognition
task, we propose a cluster-guided audio-video self-supervised pre-training
strategy for DICCAE. DICCAE achieves near state-of-the-art performance on the
VGGSound dataset, with a top-1 accuracy of 65.5%. We further evaluate its
feature representation quality through extensive ablation studies, validating
the necessity of each module.

</details>


### [111] [Fast3D: Accelerating 3D Multi-modal Large Language Models for Efficient 3D Scene Understanding](https://arxiv.org/abs/2507.09334)
*Wencan Huang,Daizong Liu,Wei Hu*

Main category: cs.CV

TL;DR: Fast3D is a plug-and-play framework for pruning redundant visual tokens in 3D MLLMs, improving computational efficiency without altering model parameters.


<details>
  <summary>Details</summary>
Motivation: 3D MLLMs face computational inefficiency due to excessive object-centric visual tokens, but token pruning for 3D domains is unexplored due to structural disparities.

Method: Fast3D introduces Global Attention Prediction (GAP) for token importance estimation and Sample-Adaptive Pruning (SAP) for dynamic token budgets, both operating without modifying the target model.

Result: Extensive benchmarks show Fast3D's effectiveness, especially under high pruning ratios.

Conclusion: Fast3D addresses computational inefficiency in 3D MLLMs, offering a practical solution for deployment.

Abstract: While 3D Multi-modal Large Language Models (MLLMs) demonstrate remarkable
scene understanding capabilities, their practical deployment faces critical
challenges due to computational inefficiency. The key bottleneck stems from
processing excessive object-centric visual tokens required for comprehensive 3D
scene representation. Although visual token pruning has shown promise in
accelerating 2D MLLMs, its applicability to 3D domains remains largely
unexplored due to fundamental disparities in token structures. In this paper,
we reveal two critical insights: (1) Significant redundancy exists in
object-level 3D token representations, analogous to patch-level redundancy in
2D systems; (2) Global attention patterns exhibit strong predictive power for
identifying non-essential tokens in 3D contexts. Building on these
observations, we propose Fast3D, a plug-and-play visual token pruning framework
for 3D MLLMs featuring two technical innovations: (1) Global Attention
Prediction (GAP), where a lightweight neural network learns to predict the
global attention distributions of the target model, enabling efficient token
importance estimation for precise pruning guidance; (2) Sample-Adaptive visual
token Pruning (SAP), which introduces dynamic token budgets through
attention-based complexity assessment, automatically adjusting layer-wise
pruning ratios based on input characteristics. Both of these two techniques
operate without modifying the parameters of the target model. Extensive
evaluations across five benchmarks validate the effectiveness of Fast3D,
particularly under high visual token pruning ratios. Code is available at
https://github.com/wencan25/Fast3D

</details>


### [112] [Simplifying Traffic Anomaly Detection with Video Foundation Models](https://arxiv.org/abs/2507.09338)
*Svetlana Orlova,Tommie Kerssies,Brun B. Englert,Gijs Dubbelman*

Main category: cs.CV

TL;DR: Simple Video ViTs with strong pre-training match or surpass specialized TAD methods, emphasizing self-supervised MVM and domain adaptation.


<details>
  <summary>Details</summary>
Motivation: To determine if complex architectures are necessary for TAD or if simple pre-trained models suffice.

Method: Uses plain Video ViTs with various pre-training strategies (weakly-, fully-supervised, self-supervised MVM, and DAPT).

Result: Simple models with MVM and DAPT outperform specialized methods, showing efficiency and scalability.

Conclusion: Pre-training, especially self-supervised and domain-adaptive, enables simple yet effective TAD models.

Abstract: Recent methods for ego-centric Traffic Anomaly Detection (TAD) often rely on
complex multi-stage or multi-representation fusion architectures, yet it
remains unclear whether such complexity is necessary. Recent findings in visual
perception suggest that foundation models, enabled by advanced pre-training,
allow simple yet flexible architectures to outperform specialized designs.
Therefore, in this work, we investigate an architecturally simple encoder-only
approach using plain Video Vision Transformers (Video ViTs) and study how
pre-training enables strong TAD performance. We find that: (i) strong
pre-training enables simple encoder-only models to match or even surpass the
performance of specialized state-of-the-art TAD methods, while also being
significantly more efficient; (ii) although weakly- and fully-supervised
pre-training are advantageous on standard benchmarks, we find them less
effective for TAD. Instead, self-supervised Masked Video Modeling (MVM)
provides the strongest signal; and (iii) Domain-Adaptive Pre-Training (DAPT) on
unlabeled driving videos further improves downstream performance, without
requiring anomalous examples. Our findings highlight the importance of
pre-training and show that effective, efficient, and scalable TAD models can be
built with minimal architectural complexity. We release our code,
domain-adapted encoders, and fine-tuned models to support future work:
https://github.com/tue-mps/simple-tad.

</details>


### [113] [Automated Multi-Class Crop Pathology Classification via Convolutional Neural Networks: A Deep Learning Approach for Real-Time Precision Agriculture](https://arxiv.org/abs/2507.09375)
*Sourish Suri,Yifei Shao*

Main category: cs.CV

TL;DR: A CNN-based system automates detection of eight crop diseases from leaf images, achieving ~90% training accuracy and ~60% validation accuracy, with a treatment recommendation module and mobile deployment.


<details>
  <summary>Details</summary>
Motivation: Crop diseases hinder agricultural productivity and food security, especially in large-scale farming where early detection is often delayed or inaccurate.

Method: Uses a CNN with three convolutional layers, preprocessing (resizing, normalization, augmentation), and TensorFlow/Keras for training. Includes a treatment recommendation module.

Result: High training accuracy (~90%) but some overfitting (validation accuracy ~60%). Mobile-compatible platform enables real-time diagnostics.

Conclusion: The system offers scalable, accessible precision agriculture tools, reducing manual inspection reliance and promoting sustainable disease management.

Abstract: Crop diseases present a significant barrier to agricultural productivity and
global food security, especially in large-scale farming where early
identification is often delayed or inaccurate. This research introduces a
Convolutional Neural Network (CNN)-based image classification system designed
to automate the detection and classification of eight common crop diseases
using leaf imagery. The methodology involves a complete deep learning pipeline:
image acquisition from a large, labeled dataset, preprocessing via resizing,
normalization, and augmentation, and model training using TensorFlow with
Keras' Sequential API. The CNN architecture comprises three convolutional
layers with increasing filter sizes and ReLU activations, followed by max
pooling, flattening, and fully connected layers, concluding with a softmax
output for multi-class classification. The system achieves high training
accuracy (~90%) and demonstrates reliable performance on unseen data, although
a validation accuracy of ~60% suggests minor overfitting. Notably, the model
integrates a treatment recommendation module, providing actionable guidance by
mapping each detected disease to suitable pesticide or fungicide interventions.
Furthermore, the solution is deployed on an open-source, mobile-compatible
platform, enabling real-time image-based diagnostics for farmers in remote
areas. This research contributes a scalable and accessible tool to the field of
precision agriculture, reducing reliance on manual inspection and promoting
sustainable disease management practices. By merging deep learning with
practical agronomic support, this work underscores the potential of CNNs to
transform crop health monitoring and enhance food production resilience on a
global scale.

</details>


### [114] [GreenCrossingAI: A Camera Trap/Computer Vision Pipeline for Environmental Science Research Groups](https://arxiv.org/abs/2507.09410)
*Bernie Boscoe,Shawn Johnson,Andrea Osborn,Chandler Campbell,Karen Mager*

Main category: cs.CV

TL;DR: A guide for processing camera trap data using a low-resource ML/AI pipeline, designed for small research groups with limited computational resources.


<details>
  <summary>Details</summary>
Motivation: Address challenges in managing large volumes of camera trap data, including labeling, environmental variability, and integrating ML/AI tools into workflows.

Method: Proposes an on-premise pipeline with tailored ML/AI capabilities for data transmission, inference, and evaluation.

Result: Provides practical solutions for small research groups to efficiently process and analyze camera trap data.

Conclusion: The pipeline enables accessible and meaningful insights from camera trap datasets, even with limited resources.

Abstract: Camera traps have long been used by wildlife researchers to monitor and study
animal behavior, population dynamics, habitat use, and species diversity in a
non-invasive and efficient manner. While data collection from the field has
increased with new tools and capabilities, methods to develop, process, and
manage the data, especially the adoption of ML/AI tools, remain challenging.
These challenges include the sheer volume of data generated, the need for
accurate labeling and annotation, variability in environmental conditions
affecting data quality, and the integration of ML/AI tools into existing
workflows that often require domain-specific customization and computational
resources. This paper provides a guide to a low-resource pipeline to process
camera trap data on-premise, incorporating ML/AI capabilities tailored for
small research groups with limited resources and computational expertise. By
focusing on practical solutions, the pipeline offers accessible approaches for
data transmission, inference, and evaluation, enabling researchers to discover
meaningful insights from their ever-increasing camera trap datasets.

</details>


### [115] [Domain Adaptation and Multi-view Attention for Learnable Landmark Tracking with Sparse Data](https://arxiv.org/abs/2507.09420)
*Timothy Chase Jr,Karthik Dantu*

Main category: cs.CV

TL;DR: A novel system for celestial terrain feature tracking using lightweight neural networks and improved domain adaptation for real-time spacecraft applications.


<details>
  <summary>Details</summary>
Motivation: Address limitations of traditional photoclinometry-based methods, such as high costs, slow processing, and lack of generalization, while overcoming computational and data scarcity challenges of learning-based approaches.

Method: Proposes lightweight neural network architectures for real-time execution, improved domain adaptation for landmark detection, and attention alignment for robust landmark description.

Result: The unified system outperforms state-of-the-art techniques in landmark tracking.

Conclusion: The work advances spacecraft autonomy by enabling efficient, real-time celestial terrain feature tracking with minimal training data.

Abstract: The detection and tracking of celestial surface terrain features are crucial
for autonomous spaceflight applications, including Terrain Relative Navigation
(TRN), Entry, Descent, and Landing (EDL), hazard analysis, and scientific data
collection. Traditional photoclinometry-based pipelines often rely on extensive
a priori imaging and offline processing, constrained by the computational
limitations of radiation-hardened systems. While historically effective, these
approaches typically increase mission costs and duration, operate at low
processing rates, and have limited generalization. Recently, learning-based
computer vision has gained popularity to enhance spacecraft autonomy and
overcome these limitations. While promising, emerging techniques frequently
impose computational demands exceeding the capabilities of typical spacecraft
hardware for real-time operation and are further challenged by the scarcity of
labeled training data for diverse extraterrestrial environments. In this work,
we present novel formulations for in-situ landmark tracking via detection and
description. We utilize lightweight, computationally efficient neural network
architectures designed for real-time execution on current-generation spacecraft
flight processors. For landmark detection, we propose improved domain
adaptation methods that enable the identification of celestial terrain features
with distinct, cheaply acquired training data. Concurrently, for landmark
description, we introduce a novel attention alignment formulation that learns
robust feature representations that maintain correspondence despite significant
landmark viewpoint variations. Together, these contributions form a unified
system for landmark tracking that demonstrates superior performance compared to
existing state-of-the-art techniques.

</details>


### [116] [Efficient Multi-Person Motion Prediction by Lightweight Spatial and Temporal Interactions](https://arxiv.org/abs/2507.09446)
*Yuanhong Zheng,Ruixuan Yu,Jian Sun*

Main category: cs.CV

TL;DR: A computationally efficient model for 3D multi-person motion prediction simplifies spatial and temporal interactions, achieving state-of-the-art performance with reduced costs.


<details>
  <summary>Details</summary>
Motivation: The complexity of modeling dependencies on individual and inter-person movements in 3D multi-person motion prediction often leads to high computational costs.

Method: Lightweight dual branches learn local and global representations separately, integrated via a cross-level interaction block and spatial inter-person distance embedding.

Result: State-of-the-art performance on CMU-Mocap, MuPoTS-3D, and 3DPW datasets with significantly reduced computational cost.

Conclusion: The proposed model efficiently balances performance and computational efficiency for multi-person motion prediction.

Abstract: 3D multi-person motion prediction is a highly complex task, primarily due to
the dependencies on both individual past movements and the interactions between
agents. Moreover, effectively modeling these interactions often incurs
substantial computational costs. In this work, we propose a computationally
efficient model for multi-person motion prediction by simplifying spatial and
temporal interactions. Our approach begins with the design of lightweight dual
branches that learn local and global representations for individual and
multiple persons separately. Additionally, we introduce a novel cross-level
interaction block to integrate the spatial and temporal representations from
both branches. To further enhance interaction modeling, we explicitly
incorporate the spatial inter-person distance embedding. With above efficient
temporal and spatial design, we achieve state-of-the-art performance for
multiple metrics on standard datasets of CMU-Mocap, MuPoTS-3D, and 3DPW, while
significantly reducing the computational cost. Code is available at
https://github.com/Yuanhong-Zheng/EMPMP.

</details>


### [117] [SegVec3D: A Method for Vector Embedding of 3D Objects Oriented Towards Robot manipulation](https://arxiv.org/abs/2507.09459)
*Zhihan Kang,Boyu Wang*

Main category: cs.CV

TL;DR: SegVec3D is a 3D point cloud instance segmentation framework combining attention, embedding learning, and cross-modal alignment for unsupervised segmentation and zero-shot retrieval.


<details>
  <summary>Details</summary>
Motivation: To unify instance segmentation and multimodal understanding with minimal supervision while enhancing geometric modeling.

Method: Hierarchical feature extraction, contrastive clustering for unsupervised segmentation, and cross-modal alignment with natural language.

Result: Outperforms methods like Mask3D and ULIP, enabling zero-shot retrieval and practical deployment.

Conclusion: SegVec3D effectively integrates segmentation and multimodal tasks with minimal supervision, offering deployable solutions.

Abstract: We propose SegVec3D, a novel framework for 3D point cloud instance
segmentation that integrates attention mechanisms, embedding learning, and
cross-modal alignment. The approach builds a hierarchical feature extractor to
enhance geometric structure modeling and enables unsupervised instance
segmentation via contrastive clustering. It further aligns 3D data with natural
language queries in a shared semantic space, supporting zero-shot retrieval.
Compared to recent methods like Mask3D and ULIP, our method uniquely unifies
instance segmentation and multimodal understanding with minimal supervision and
practical deployability.

</details>


### [118] [CKAA: Cross-subspace Knowledge Alignment and Aggregation for Robust Continual Learning](https://arxiv.org/abs/2507.09471)
*Lingfeng He,De Cheng,Zhiheng Ma,Huaijie Wang,Dingwen Zhang,Nannan Wang,Xinbo Gao*

Main category: cs.CV

TL;DR: CKAA improves robustness in Continual Learning by aligning feature subspaces and using adaptive task-confidence-guided inference.


<details>
  <summary>Details</summary>
Motivation: Address feature subspace misalignment and ambiguous decisions in PEFT-based CL methods under misleading task-ids.

Method: Proposes CKAA with Dual-level Knowledge Alignment (DKA) for feature alignment and Task-Confidence-guided Mixture of Adapters (TC-MoA) for robust inference.

Result: CKAA outperforms existing PEFT-based CL methods in experiments.

Conclusion: CKAA effectively enhances model robustness against misleading task-ids in Continual Learning.

Abstract: Continual Learning (CL) empowers AI models to continuously learn from
sequential task streams. Recently, parameter-efficient fine-tuning (PEFT)-based
CL methods have garnered increasing attention due to their superior
performance. They typically allocate a unique sub-module for learning each
task, with a task recognizer to select the appropriate sub-modules for testing
images. However, due to the feature subspace misalignment from independently
trained sub-modules, these methods tend to produce ambiguous decisions under
misleading task-ids. To address this, we propose Cross-subspace Knowledge
Alignment and Aggregation (CKAA), a novel framework that enhances model
robustness against misleading task-ids through two key innovations: (1)
Dual-level Knowledge Alignment (DKA): By aligning intra-class feature
distributions across different subspaces and learning a robust global
classifier through a feature simulation process, DKA enables the model to
distinguish features from both correct and incorrect subspaces during training.
(2) Task-Confidence-guided Mixture of Adapters (TC-MoA): A robust inference
scheme that adaptively aggregates task-specific knowledge from relevant
sub-modules based on task-confidence scores, avoiding overconfidence in
misleading task-id predictions. Extensive experiments demonstrate that CKAA
outperforms existing PEFT-based CL methods.

</details>


### [119] [HMID-Net: An Exploration of Masked Image Modeling and Knowledge Distillation in Hyperbolic Space](https://arxiv.org/abs/2507.09487)
*Changli Wang,Fang Yin,Jiafeng Liu,Rui Wu*

Main category: cs.CV

TL;DR: HMID-Net integrates Masked Image Modeling and knowledge distillation in hyperbolic space for efficient hierarchical visual-semantic learning, outperforming MERU and CLIP.


<details>
  <summary>Details</summary>
Motivation: To improve efficiency in capturing visual-semantic hierarchies, which are naturally structured in hyperbolic space.

Method: Proposes HMID-Net, combining Masked Image Modeling and knowledge distillation in hyperbolic space with a custom distillation loss.

Result: Achieves superior performance in image classification and retrieval, surpassing MERU and CLIP.

Conclusion: HMID-Net effectively leverages hyperbolic space for hierarchical learning, demonstrating the viability of MIM and distillation in this space.

Abstract: Visual and semantic concepts are often structured in a hierarchical manner.
For instance, textual concept `cat' entails all images of cats. A recent study,
MERU, successfully adapts multimodal learning techniques from Euclidean space
to hyperbolic space, effectively capturing the visual-semantic hierarchy.
However, a critical question remains: how can we more efficiently train a model
to capture and leverage this hierarchy? In this paper, we propose the
\textit{Hyperbolic Masked Image and Distillation Network} (HMID-Net), a novel
and efficient method that integrates Masked Image Modeling (MIM) and knowledge
distillation techniques within hyperbolic space. To the best of our knowledge,
this is the first approach to leverage MIM and knowledge distillation in
hyperbolic space to train highly efficient models. In addition, we introduce a
distillation loss function specifically designed to facilitate effective
knowledge transfer in hyperbolic space. Our experiments demonstrate that MIM
and knowledge distillation techniques in hyperbolic space can achieve the same
remarkable success as in Euclidean space. Extensive evaluations show that our
method excels across a wide range of downstream tasks, significantly
outperforming existing models like MERU and CLIP in both image classification
and retrieval.

</details>


### [120] [GLIMPSE: Do Large Vision-Language Models Truly Think With Videos or Just Glimpse at Them?](https://arxiv.org/abs/2507.09491)
*Yiyang Zhou,Linjie Li,Shi Qiu,Zhengyuan Yang,Yuyang Zhao,Siwei Han,Yangfan He,Kangqi Li,Haonian Ji,Zihao Zhao,Haibo Tong,Lijuan Wang,Huaxiu Yao*

Main category: cs.CV

TL;DR: GLIMPSE is a new benchmark designed to evaluate deep temporal reasoning in large vision-language models (LVLMs) by requiring full video context understanding, unlike existing benchmarks that rely on static frame analysis.


<details>
  <summary>Details</summary>
Motivation: Existing video benchmarks often allow models to answer questions by scanning key frames, limiting assessment of true video understanding. GLIMPSE addresses this gap.

Method: GLIMPSE includes 3,269 videos and 4,342 visual-centric questions across 11 categories, crafted to require full video context and temporal reasoning.

Result: Human evaluations show 94.82% accuracy on GLIMPSE, but the best LVLM (GPT-o3) achieves only 66.43%, indicating a significant gap in deep video reasoning.

Conclusion: GLIMPSE highlights the limitations of current LVLMs in true video understanding, emphasizing the need for models to move beyond superficial frame-level analysis.

Abstract: Existing video benchmarks often resemble image-based benchmarks, with
question types like "What actions does the person perform throughout the
video?" or "What color is the woman's dress in the video?" For these, models
can often answer by scanning just a few key frames, without deep temporal
reasoning. This limits our ability to assess whether large vision-language
models (LVLMs) can truly think with videos rather than perform superficial
frame-level analysis. To address this, we introduce GLIMPSE, a benchmark
specifically designed to evaluate whether LVLMs can genuinely think with
videos. Unlike prior benchmarks, GLIMPSE emphasizes comprehensive video
understanding beyond static image cues. It consists of 3,269 videos and over
4,342 highly visual-centric questions across 11 categories, including
Trajectory Analysis, Temporal Reasoning, and Forensics Detection. All questions
are carefully crafted by human annotators and require watching the entire video
and reasoning over full video context-this is what we mean by thinking with
video. These questions cannot be answered by scanning selected frames or
relying on text alone. In human evaluations, GLIMPSE achieves 94.82% accuracy,
but current LVLMs face significant challenges. Even the best-performing model,
GPT-o3, reaches only 66.43%, highlighting that LVLMs still struggle to move
beyond surface-level reasoning to truly think with videos.

</details>


### [121] [SDTN and TRN: Adaptive Spectral-Spatial Feature Extraction for Hyperspectral Image Classification](https://arxiv.org/abs/2507.09492)
*Fuyin Ye,Erwen Yao,Jianyong Chen,Fengmei He,Junxiang Zhang,Lihao Ni*

Main category: cs.CV

TL;DR: The paper proposes SDTN and TRN for hyperspectral image classification, addressing challenges like high-dimensional data and limited labeled samples. The methods improve accuracy and reduce computational complexity.


<details>
  <summary>Details</summary>
Motivation: Traditional methods struggle with high-dimensional data, spectral-spatial redundancy, and scarcity of labeled samples in hyperspectral image classification.

Method: Proposes SDTN for dynamic tensor rank adjustment and TRN for lightweight multi-scale spectral-spatial feature extraction.

Result: Experiments on PaviaU datasets show improved accuracy and reduced model parameters.

Conclusion: The framework is effective for real-time deployment in resource-constrained environments.

Abstract: Hyperspectral image classification plays a pivotal role in precision
agriculture, providing accurate insights into crop health monitoring, disease
detection, and soil analysis. However, traditional methods struggle with
high-dimensional data, spectral-spatial redundancy, and the scarcity of labeled
samples, often leading to suboptimal performance. To address these challenges,
we propose the Self-Adaptive Tensor- Regularized Network (SDTN), which combines
tensor decomposition with regularization mechanisms to dynamically adjust
tensor ranks, ensuring optimal feature representation tailored to the
complexity of the data. Building upon SDTN, we propose the Tensor-Regularized
Network (TRN), which integrates the features extracted by SDTN into a
lightweight network capable of capturing spectral-spatial features at multiple
scales. This approach not only maintains high classification accuracy but also
significantly reduces computational complexity, making the framework highly
suitable for real-time deployment in resource-constrained environments.
Experiments on PaviaU datasets demonstrate significant improvements in accuracy
and reduced model parameters compared to state-of-the-art methods.

</details>


### [122] [Advancing Reliable Test-Time Adaptation of Vision-Language Models under Visual Variations](https://arxiv.org/abs/2507.09500)
*Yiwen Liang,Hui Chen,Yizhe Xiong,Zihan Zhou,Mengyao Lyu,Zijia Lin,Shuaicheng Niu,Sicheng Zhao,Jungong Han,Guiguang Ding*

Main category: cs.CV

TL;DR: ReTA improves reliability in Test-Time Adaptation for VLMs by addressing entropy unreliability and inflexible decision boundaries with CER and DDC strategies.


<details>
  <summary>Details</summary>
Motivation: VLMs struggle with distribution shifts in downstream tasks without labeled data, prompting the need for reliable TTA methods.

Method: ReTA introduces Consistency-aware Entropy Reweighting (CER) for cache quality and Diversity-driven Distribution Calibration (DDC) for adaptive decision boundaries.

Result: ReTA outperforms state-of-the-art methods, especially under challenging real-world distribution shifts.

Conclusion: ReTA enhances VLM adaptation reliability by combining CER and DDC, offering robust performance in dynamic environments.

Abstract: Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but
struggle with distribution shifts in downstream tasks when labeled data is
unavailable, which has motivated the development of Test-Time Adaptation (TTA)
to improve VLMs' performance during inference without annotations. Among
various TTA approaches, cache-based methods show promise by preserving
historical knowledge from low-entropy samples in a dynamic cache and fostering
efficient adaptation. However, these methods face two critical reliability
challenges: (1) entropy often becomes unreliable under distribution shifts,
causing error accumulation in the cache and degradation in adaptation
performance; (2) the final predictions may be unreliable due to inflexible
decision boundaries that fail to accommodate large downstream shifts. To
address these challenges, we propose a Reliable Test-time Adaptation (ReTA)
method that integrates two complementary strategies to enhance reliability from
two perspectives. First, to mitigate the unreliability of entropy as a sample
selection criterion for cache construction, we introduce Consistency-aware
Entropy Reweighting (CER), which incorporates consistency constraints to weight
entropy during cache updating. While conventional approaches rely solely on low
entropy for cache prioritization and risk introducing noise, our method
leverages predictive consistency to maintain a high-quality cache and
facilitate more robust adaptation. Second, we present Diversity-driven
Distribution Calibration (DDC), which models class-wise text embeddings as
multivariate Gaussian distributions, enabling adaptive decision boundaries for
more accurate predictions across visually diverse content. Extensive
experiments demonstrate that ReTA consistently outperforms state-of-the-art
methods, particularly under challenging real-world distribution shifts.

</details>


### [123] [Online Micro-gesture Recognition Using Data Augmentation and Spatial-Temporal Attention](https://arxiv.org/abs/2507.09512)
*Pengyu Liu,Kun Li,Fei Wang,Yanyan Wei,Junhui She,Dan Guo*

Main category: cs.CV

TL;DR: The paper presents HFUT-VUT's solution for the IJCAI 2025 MiGA Challenge, focusing on micro-gesture recognition in videos. Their method, using data augmentation and spatial-temporal attention, achieved a 38.03 F1 score, surpassing previous benchmarks.


<details>
  <summary>Details</summary>
Motivation: The challenge lies in accurately locating and categorizing micro-gestures in videos, which are more spontaneous and varied than typical actions.

Method: Proposed hand-crafted data augmentation and spatial-temporal attention to improve classification and localization.

Result: Achieved an F1 score of 38.03, outperforming the previous state-of-the-art by 37.9%.

Conclusion: The method ranked first in the challenge, demonstrating its effectiveness in micro-gesture recognition.

Abstract: In this paper, we introduce the latest solution developed by our team,
HFUT-VUT, for the Micro-gesture Online Recognition track of the IJCAI 2025 MiGA
Challenge. The Micro-gesture Online Recognition task is a highly challenging
problem that aims to locate the temporal positions and recognize the categories
of multiple micro-gesture instances in untrimmed videos. Compared to
traditional temporal action detection, this task places greater emphasis on
distinguishing between micro-gesture categories and precisely identifying the
start and end times of each instance. Moreover, micro-gestures are typically
spontaneous human actions, with greater differences than those found in other
human actions. To address these challenges, we propose hand-crafted data
augmentation and spatial-temporal attention to enhance the model's ability to
classify and localize micro-gestures more accurately. Our solution achieved an
F1 score of 38.03, outperforming the previous state-of-the-art by 37.9%. As a
result, our method ranked first in the Micro-gesture Online Recognition track.

</details>


### [124] [QuarterMap: Efficient Post-Training Token Pruning for Visual State Space Models](https://arxiv.org/abs/2507.09514)
*Tien-Yu Chi,Hung-Yueh Chiang,Diana Marculescu,Kai-Chiang Wu*

Main category: cs.CV

TL;DR: QuarterMap is a post-training pruning method for SSM-based models like VMamba, reducing spatial redundancy and improving throughput without retraining, with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Spatial redundancy in four-directional scans of SSM-based models like VMamba limits efficiency. QuarterMap aims to address this without retraining.

Method: QuarterMap prunes redundant spatial activations before scanning and restores dimensions via nearest-neighbor upsampling.

Result: Achieves up to 11% speedup on VMamba with <0.9% accuracy drop, similar gains on ADE20K and MedMamba.

Conclusion: QuarterMap is a plug-and-play tool for deployment-time efficiency in SSMs, preserving accuracy and transferability.

Abstract: State space models (SSMs) reduce the quadratic complexity of transformers by
leveraging linear recurrence. Recently, VMamba has emerged as a strong
SSM-based vision backbone, yet remains bottlenecked by spatial redundancy in
its four-directional scan. We propose QuarterMap, a post-training activation
pruning method that removes redundant spatial activations before scanning and
restores dimensions via nearest-neighbor upsampling. Our method improves
throughput without retraining. On ImageNet-1K, QuarterMap achieves up to 11%
speedup on VMamba with less than 0.9% accuracy drop, and yields similar gains
on ADE20K segmentation. Beyond VMamba, we validate QuarterMap on MedMamba, a
domain-specific model that shares the same four-directional scanning structure,
where it consistently improves throughput while preserving accuracy across
multiple medical imaging tasks. Compared to token merging methods like ToMe,
QuarterMap is tailored for SSMs and avoids costly merge-unmerge operations. Our
method offers a plug-and-play tool for deployment-time efficiency without
compromising transferability.

</details>


### [125] [When Schrdinger Bridge Meets Real-World Image Dehazing with Unpaired Training](https://arxiv.org/abs/2507.09524)
*Yunwei Lan,Zhigao Cui,Xin Luo,Chang Liu,Nian Wang,Menglin Zhang,Yanzhao Su,Dong Liu*

Main category: cs.CV

TL;DR: DehazeSB, a novel unpaired dehazing framework using Schrdinger Bridge and optimal transport theory, outperforms GAN-based methods by enabling efficient transport mappings and preserving image details.


<details>
  <summary>Details</summary>
Motivation: Existing GAN-based unpaired dehazing methods are limited by the generator's transport mapping capability, hindering their effectiveness.

Method: DehazeSB leverages optimal transport theory to bridge hazy and clear image distributions, uses detail-preserving regularization, and incorporates prompt learning with CLIP models for haze-aware alignment.

Result: Extensive experiments show DehazeSB's superiority in generating high-quality dehazed images on real-world datasets.

Conclusion: DehazeSB addresses limitations of GAN-based methods, offering a robust solution for unpaired dehazing with improved performance.

Abstract: Recent advancements in unpaired dehazing, particularly those using GANs, show
promising performance in processing real-world hazy images. However, these
methods tend to face limitations due to the generator's limited transport
mapping capability, which hinders the full exploitation of their effectiveness
in unpaired training paradigms. To address these challenges, we propose
DehazeSB, a novel unpaired dehazing framework based on the Schr\"odinger
Bridge. By leveraging optimal transport (OT) theory, DehazeSB directly bridges
the distributions between hazy and clear images. This enables optimal transport
mappings from hazy to clear images in fewer steps, thereby generating
high-quality results. To ensure the consistency of structural information and
details in the restored images, we introduce detail-preserving regularization,
which enforces pixel-level alignment between hazy inputs and dehazed outputs.
Furthermore, we propose a novel prompt learning to leverage pre-trained CLIP
models in distinguishing hazy images and clear ones, by learning a haze-aware
vision-language alignment. Extensive experiments on multiple real-world
datasets demonstrate our method's superiority. Code:
https://github.com/ywxjm/DehazeSB.

</details>


### [126] [VDInstruct: Zero-Shot Key Information Extraction via Content-Aware Vision Tokenization](https://arxiv.org/abs/2507.09531)
*Son Nguyen,Giang Nguyen,Hung Dao,Thao Do,Daeyoung Kim*

Main category: cs.CV

TL;DR: VDInstruct, a multimodal large language model (MLLM), improves Key Information Extraction (KIE) by separating spatial detection from semantic extraction and using content-aware tokenization, achieving SOTA results with fewer tokens.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs perform poorly on dense documents and suffer from inefficient vision tokenization, leading to redundant computation and memory issues.

Method: VDInstruct employs content-aware tokenization (generating tokens based on document complexity) and a three-stage training paradigm.

Result: The model achieves SOTA on KIE benchmarks, reduces tokens by 3.6x, and outperforms baselines like DocOwl 1.5 by +5.5 F1 in zero-shot evaluations.

Conclusion: Content-aware tokenization and explicit layout modeling are promising for document understanding, with VDInstruct demonstrating efficiency and robustness.

Abstract: Key Information Extraction (KIE) underpins the understanding of visual
documents (e.g., receipts and contracts) by extracting precise semantic content
and accurately capturing spatial structure. Yet existing multimodal large
language models (MLLMs) often perform poorly on dense documents and rely on
vision tokenization approaches that scale with image size, leading to redundant
computation and memory inefficiency. To address these challenges, we introduce
VDInstruct, an MLLM that separates spatial region detection from semantic
feature extraction. Central to our model is a content-aware tokenization
strategy: rather than fragmenting the entire image uniformly, it generates
tokens in proportion to document complexity, preserving critical structure
while eliminating wasted tokens. Leveraging a three-stage training paradigm,
our model achieves state-of-the-art (SOTA) results on KIE benchmarks, matching
or exceeding the accuracy of leading approaches while reducing the number of
image tokens by roughly 3.6x. In zero-shot evaluations, VDInstruct surpasses
strong baselines-such as DocOwl 1.5-by +5.5 F1 points, highlighting its
robustness to unseen documents. These findings show that content-aware
tokenization combined with explicit layout modeling offers a promising
direction forward for document understanding. Data, source code, and model
weights will be made publicly available.

</details>


### [127] [DRPCA-Net: Make Robust PCA Great Again for Infrared Small Target Detection](https://arxiv.org/abs/2507.09541)
*Zihao Xiong,Fei Zhou,Fengyi Wu,Shuai Yuan,Maixia Fu,Zhenming Peng,Jian Yang,Yimian Dai*

Main category: cs.CV

TL;DR: DRPCA-Net is a deep unfolding network integrating sparsity-aware priors for infrared small target detection, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning models for infrared small target detection prioritize performance over interpretability and efficiency, neglecting sparsity priors.

Method: Proposes DRPCA-Net, combining Robust Principal Component Analysis (RPCA) with a dynamic unfolding mechanism and Dynamic Residual Group (DRG) module.

Result: DRPCA-Net achieves superior detection accuracy on multiple public infrared datasets.

Conclusion: The model effectively balances performance, efficiency, and interpretability by leveraging sparsity priors and adaptive parameter learning.

Abstract: Infrared small target detection plays a vital role in remote sensing,
industrial monitoring, and various civilian applications. Despite recent
progress powered by deep learning, many end-to-end convolutional models tend to
pursue performance by stacking increasingly complex architectures, often at the
expense of interpretability, parameter efficiency, and generalization. These
models typically overlook the intrinsic sparsity prior of infrared small
targets--an essential cue that can be explicitly modeled for both performance
and efficiency gains. To address this, we revisit the model-based paradigm of
Robust Principal Component Analysis (RPCA) and propose Dynamic RPCA Network
(DRPCA-Net), a novel deep unfolding network that integrates the sparsity-aware
prior into a learnable architecture. Unlike conventional deep unfolding methods
that rely on static, globally learned parameters, DRPCA-Net introduces a
dynamic unfolding mechanism via a lightweight hypernetwork. This design enables
the model to adaptively generate iteration-wise parameters conditioned on the
input scene, thereby enhancing its robustness and generalization across diverse
backgrounds. Furthermore, we design a Dynamic Residual Group (DRG) module to
better capture contextual variations within the background, leading to more
accurate low-rank estimation and improved separation of small targets.
Extensive experiments on multiple public infrared datasets demonstrate that
DRPCA-Net significantly outperforms existing state-of-the-art methods in
detection accuracy. Code is available at https://github.com/GrokCV/DRPCA-Net.

</details>


### [128] [SeqCSIST: Sequential Closely-Spaced Infrared Small Target Unmixing](https://arxiv.org/abs/2507.09556)
*Ximeng Zhai,Bohan Xu,Yaohong Chen,Hao Wang,Kehua Guo,Yimian Dai*

Main category: cs.CV

TL;DR: The paper introduces a novel task, Sequential CSIST Unmixing, for detecting closely-spaced infrared small targets (CSIST) with sub-pixel precision. It addresses challenges like lack of datasets and proposes a dataset, toolkit, and a deep learning model (DeRefNet) with a TDFA module, achieving a 5.3% mAP improvement.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of detecting distant CSIST groups appearing as mixing spots in infrared images due to optical and detector constraints. The lack of public datasets also hinders research progress.

Method: The method involves creating an open-source ecosystem (SeqCSIST dataset and toolkit) and proposing DeRefNet, a model-driven deep learning framework with a Temporal Deformable Feature Alignment (TDFA) module for adaptive inter-frame information aggregation.

Result: Experiments show DeRefNet outperforms state-of-the-art methods with a 5.3% improvement in mean Average Precision (mAP) on the SeqCSIST dataset.

Conclusion: The paper successfully addresses the CSIST Unmixing task, providing a dataset, toolkit, and a novel deep learning framework (DeRefNet) that significantly improves detection accuracy.

Abstract: Due to the limitation of the optical lens focal length and the resolution of
the infrared detector, distant Closely-Spaced Infrared Small Target (CSIST)
groups typically appear as mixing spots in the infrared image. In this paper,
we propose a novel task, Sequential CSIST Unmixing, namely detecting all
targets in the form of sub-pixel localization from a highly dense CSIST group.
However, achieving such precise detection is an extremely difficult challenge.
In addition, the lack of high-quality public datasets has also restricted the
research progress. To this end, firstly, we contribute an open-source
ecosystem, including SeqCSIST, a sequential benchmark dataset, and a toolkit
that provides objective evaluation metrics for this special task, along with
the implementation of 23 relevant methods. Furthermore, we propose the
Deformable Refinement Network (DeRefNet), a model-driven deep learning
framework that introduces a Temporal Deformable Feature Alignment (TDFA) module
enabling adaptive inter-frame information aggregation. To the best of our
knowledge, this work is the first endeavor to address the CSIST Unmixing task
within a multi-frame paradigm. Experiments on the SeqCSIST dataset demonstrate
that our method outperforms the state-of-the-art approaches with mean Average
Precision (mAP) metric improved by 5.3\%. Our dataset and toolkit are available
from https://github.com/GrokCV/SeqCSIST.

</details>


### [129] [EHPE: A Segmented Architecture for Enhanced Hand Pose Estimation](https://arxiv.org/abs/2507.09560)
*Bolun Zheng,Xinjie Liu,Qianyu Zhang,Canjin Wang,Fangni Chen,Mingen Xu*

Main category: cs.CV

TL;DR: The paper proposes a segmented architecture (EHPE) for 3D hand pose estimation, focusing on accurate TIP and wrist joint prediction to reduce error accumulation and improve overall joint estimation.


<details>
  <summary>Details</summary>
Motivation: Existing methods overlook the importance of TIP and wrist joints, leading to error accumulation and degraded pose estimation quality.

Method: EHPE uses a two-stage approach: TW-stage for TIP and wrist joint extraction, and PG-stage for refining remaining joints with a dual-branch network.

Result: EHPE achieves state-of-the-art performance on two benchmarks.

Conclusion: The proposed EHPE effectively addresses error accumulation and improves hand pose estimation accuracy.

Abstract: 3D hand pose estimation has garnered great attention in recent years due to
its critical applications in human-computer interaction, virtual reality, and
related fields. The accurate estimation of hand joints is essential for
high-quality hand pose estimation. However, existing methods neglect the
importance of Distal Phalanx Tip (TIP) and Wrist in predicting hand joints
overall and often fail to account for the phenomenon of error accumulation for
distal joints in gesture estimation, which can cause certain joints to incur
larger errors, resulting in misalignments and artifacts in the pose estimation
and degrading the overall reconstruction quality. To address this challenge, we
propose a novel segmented architecture for enhanced hand pose estimation
(EHPE). We perform local extraction of TIP and wrist, thus alleviating the
effect of error accumulation on TIP prediction and further reduce the
predictive errors for all joints on this basis. EHPE consists of two key
stages: In the TIP and Wrist Joints Extraction stage (TW-stage), the positions
of the TIP and wrist joints are estimated to provide an initial accurate joint
configuration; In the Prior Guided Joints Estimation stage (PG-stage), a
dual-branch interaction network is employed to refine the positions of the
remaining joints. Extensive experiments on two widely used benchmarks
demonstrate that EHPE achieves state-of-the-arts performance. Code is available
at https://github.com/SereinNout/EHPE.

</details>


### [130] [Prompt Engineering in Segment Anything Model: Methodologies, Applications, and Emerging Challenges](https://arxiv.org/abs/2507.09562)
*Yidong Jiang*

Main category: cs.CV

TL;DR: This paper surveys prompt engineering techniques for the Segment Anything Model (SAM), highlighting its evolution, applications, and challenges.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored role of prompt engineering in SAM's success and provide a structured review of its methodologies and applications.

Method: Systematic organization and analysis of existing work on prompt engineering for SAM, covering methodologies, applications, and challenges.

Result: Reveals the evolution of prompt engineering from simple to multimodal approaches, enabling SAM's use in diverse domains like medical imaging and remote sensing.

Conclusion: The survey fills a literature gap by offering a framework to understand and advance prompt engineering in segmentation foundation models.

Abstract: The Segment Anything Model (SAM) has revolutionized image segmentation
through its innovative prompt-based approach, yet the critical role of prompt
engineering in its success remains underexplored. This paper presents the first
comprehensive survey focusing specifically on prompt engineering techniques for
SAM and its variants. We systematically organize and analyze the rapidly
growing body of work in this emerging field, covering fundamental
methodologies, practical applications, and key challenges. Our review reveals
how prompt engineering has evolved from simple geometric inputs to
sophisticated multimodal approaches, enabling SAM's adaptation across diverse
domains including medical imaging and remote sensing. We identify unique
challenges in prompt optimization and discuss promising research directions.
This survey fills an important gap in the literature by providing a structured
framework for understanding and advancing prompt engineering in foundation
models for segmentation.

</details>


### [131] [WordCraft: Interactive Artistic Typography with Attention Awareness and Noise Blending](https://arxiv.org/abs/2507.09573)
*Zhe Wang,Jingbo Zhang,Tianyi Wei,Wanchao Su,Can Wang*

Main category: cs.CV

TL;DR: WordCraft is an interactive artistic typography system using diffusion models and a large language model to enable localized edits, iterative refinement, and flexible prompt interpretation.


<details>
  <summary>Details</summary>
Motivation: Existing generative models for artistic typography lack interactivity, localized edits, and support for multi-character composition and abstract prompts.

Method: WordCraft integrates diffusion models with a training-free regional attention mechanism and noise blending for precise, multi-region generation and continuous refinement. A large language model parses user prompts.

Result: The system synthesizes high-quality, stylized typography for single- and multi-character inputs in multiple languages, enhancing interactivity and creative workflows.

Conclusion: WordCraft significantly improves interactivity in artistic typography, offering new creative possibilities for artists and designers.

Abstract: Artistic typography aims to stylize input characters with visual effects that
are both creative and legible. Traditional approaches rely heavily on manual
design, while recent generative models, particularly diffusion-based methods,
have enabled automated character stylization. However, existing solutions
remain limited in interactivity, lacking support for localized edits, iterative
refinement, multi-character composition, and open-ended prompt interpretation.
We introduce WordCraft, an interactive artistic typography system that
integrates diffusion models to address these limitations. WordCraft features a
training-free regional attention mechanism for precise, multi-region generation
and a noise blending that supports continuous refinement without compromising
visual quality. To support flexible, intent-driven generation, we incorporate a
large language model to parse and structure both concrete and abstract user
prompts. These components allow our framework to synthesize high-quality,
stylized typography across single- and multi-character inputs across multiple
languages, supporting diverse user-centered workflows. Our system significantly
enhances interactivity in artistic typography synthesis, opening up creative
possibilities for artists and designers.

</details>


### [132] [MENTOR: Efficient Multimodal-Conditioned Tuning for Autoregressive Vision Generation Models](https://arxiv.org/abs/2507.09574)
*Haozhe Zhao,Zefan Cai,Shuzheng Si,Liang Chen,Jiuxiang Gu,Wen Xiao,Junjie Hu*

Main category: cs.CV

TL;DR: MENTOR is an autoregressive framework for precise multimodal image generation, addressing limitations in current text-to-image models with a two-stage training approach.


<details>
  <summary>Details</summary>
Motivation: Current text-to-image models lack precise visual control and require extensive training for complex multimodal generation.

Method: MENTOR uses an autoregressive image generator with a two-stage training paradigm: multimodal alignment and instruction tuning, avoiding auxiliary adapters.

Result: MENTOR outperforms baselines in DreamBench++, achieving better concept preservation, prompt following, and training efficiency.

Conclusion: MENTOR offers superior controllability, fidelity, and adaptability, with publicly available resources.

Abstract: Recent text-to-image models produce high-quality results but still struggle
with precise visual control, balancing multimodal inputs, and requiring
extensive training for complex multimodal image generation. To address these
limitations, we propose MENTOR, a novel autoregressive (AR) framework for
efficient Multimodal-conditioned Tuning for Autoregressive multimodal image
generation. MENTOR combines an AR image generator with a two-stage training
paradigm, enabling fine-grained, token-level alignment between multimodal
inputs and image outputs without relying on auxiliary adapters or
cross-attention modules. The two-stage training consists of: (1) a multimodal
alignment stage that establishes robust pixel- and semantic-level alignment,
followed by (2) a multimodal instruction tuning stage that balances the
integration of multimodal inputs and enhances generation controllability.
Despite modest model size, suboptimal base components, and limited training
resources, MENTOR achieves strong performance on the DreamBench++ benchmark,
outperforming competitive baselines in concept preservation and prompt
following. Additionally, our method delivers superior image reconstruction
fidelity, broad task adaptability, and improved training efficiency compared to
diffusion-based methods. Dataset, code, and models are available at:
https://github.com/HaozheZhao/MENTOR

</details>


### [133] [Memory-Augmented SAM2 for Training-Free Surgical Video Segmentation](https://arxiv.org/abs/2507.09577)
*Ming Yin,Fu Wang,Xujiong Ye,Yanda Meng,Zeyu Fu*

Main category: cs.CV

TL;DR: MA-SAM2 improves surgical video segmentation by addressing SAM2's limitations with context-aware and occlusion-resilient memory models, achieving better performance without extra training.


<details>
  <summary>Details</summary>
Motivation: SAM2's greedy memory design struggles with surgical videos due to rapid movements, occlusions, and complex interactions, necessitating a more robust solution.

Method: MA-SAM2 introduces training-free context-aware and occlusion-resilient memory models, using multi-target, single-loop, one-prompt inference for efficiency.

Result: MA-SAM2 outperforms SAM2 by 4.36% and 6.1% on EndoVis2017 and EndoVis2018 datasets, respectively.

Conclusion: MA-SAM2 is a practical, efficient solution for surgical video segmentation, enhancing accuracy and robustness without additional training.

Abstract: Surgical video segmentation is a critical task in computer-assisted surgery,
essential for enhancing surgical quality and patient outcomes. Recently, the
Segment Anything Model 2 (SAM2) framework has demonstrated remarkable
advancements in both image and video segmentation. However, the inherent
limitations of SAM2's greedy selection memory design are amplified by the
unique properties of surgical videos-rapid instrument movement, frequent
occlusion, and complex instrument-tissue interaction-resulting in diminished
performance in the segmentation of complex, long videos. To address these
challenges, we introduce Memory Augmented (MA)-SAM2, a training-free video
object segmentation strategy, featuring novel context-aware and
occlusion-resilient memory models. MA-SAM2 exhibits strong robustness against
occlusions and interactions arising from complex instrument movements while
maintaining accuracy in segmenting objects throughout videos. Employing a
multi-target, single-loop, one-prompt inference further enhances the efficiency
of the tracking process in multi-instrument videos. Without introducing any
additional parameters or requiring further training, MA-SAM2 achieved
performance improvements of 4.36% and 6.1% over SAM2 on the EndoVis2017 and
EndoVis2018 datasets, respectively, demonstrating its potential for practical
surgical applications.

</details>


### [134] [Demystifying Flux Architecture](https://arxiv.org/abs/2507.09595)
*Or Greenberg*

Main category: cs.CV

TL;DR: FLUX.1 is a state-of-the-art text-to-image model, reverse-engineered to understand its architecture and training setup.


<details>
  <summary>Details</summary>
Motivation: To demystify FLUX's architecture for future research and development, as official documentation is unavailable.

Method: Reverse-engineering FLUX's source code to analyze its architecture and training setup.

Result: An unofficial technical report detailing FLUX's architecture, supporting its adoption for further research.

Conclusion: FLUX's reverse-engineered insights can serve as a backbone for future text-to-image generation advancements.

Abstract: FLUX.1 is a diffusion-based text-to-image generation model developed by Black
Forest Labs, designed to achieve faithful text-image alignment while
maintaining high image quality and diversity. FLUX is considered
state-of-the-art in text-to-image generation, outperforming popular models such
as Midjourney, DALL-E 3, Stable Diffusion 3 (SD3), and SDXL. Although publicly
available as open source, the authors have not released official technical
documentation detailing the model's architecture or training setup. This report
summarizes an extensive reverse-engineering effort aimed at demystifying FLUX's
architecture directly from its source code, to support its adoption as a
backbone for future research and development. This document is an unofficial
technical report and is not published or endorsed by the original developers or
their affiliated institutions.

</details>


### [135] [Inter2Former: Dynamic Hybrid Attention for Efficient High-Precision Interactive](https://arxiv.org/abs/2507.09612)
*You Huang,Lichao Chen,Jiayi Ji,Liujuan Cao,Shengchuan Zhang,Rongrong Ji*

Main category: cs.CV

TL;DR: Inter2Former optimizes dense-token processing for interactive segmentation, balancing accuracy and speed on CPU devices through adaptive computation strategies.


<details>
  <summary>Details</summary>
Motivation: Address the trade-off between accuracy (dense-token methods) and speed (sparse prompt tokens like SAM) in interactive segmentation.

Method: Introduces four enhancements: Dynamic Prompt Embedding (DPE), Dynamic Hybrid Attention (DHA), Hybrid Mixture of Experts (HMoE), and Dynamic Local Upsampling (DLU).

Result: Achieves state-of-the-art performance on high-precision IS benchmarks with efficient CPU processing.

Conclusion: Inter2Former successfully balances segmentation quality and computational efficiency, making it practical for real-world applications.

Abstract: Interactive segmentation (IS) improves annotation efficiency by segmenting
target regions from user prompts, with widespread applications in real-world
scenarios. Current approaches face a critical trade-off: dense-token methods
achieve superior accuracy and detail preservation but suffer from prohibitively
slow processing on CPU devices, while the Segment Anything Model (SAM) advances
the field with sparse prompt tokens for fast inference but compromises
segmentation quality. In this paper, we propose Inter2Former to address this
challenge by optimizing computation allocation in dense-token processing, which
introduces four key enhancements. First, we propose Dynamic Prompt Embedding
(DPE) that adaptively processes only regions of interest while avoiding
additional overhead from background tokens. Second, we introduce Dynamic Hybrid
Attention (DHA), which leverages previous segmentation masks to route tokens
through either full attention (O(N2)) for boundary regions or our proposed
efficient BSQ attention (O(N)) for non-boundary regions. Third, we develop
Hybrid Mixture of Experts (HMoE), which applies similar adaptive computation
strategies in FFN modules with CPU-optimized parallel processing. Finally, we
present Dynamic Local Upsampling (DLU), a reverse operation of DPE, which
localizes objects with a lightweight MLP and performs fine-grained upsampling
only in detected regions. Experimental results on high-precision IS benchmarks
demonstrate that Inter2Former achieves SOTA performance with high efficiency on
CPU devices.

</details>


### [136] [Towards Fine-Grained Adaptation of CLIP via a Self-Trained Alignment Score](https://arxiv.org/abs/2507.09615)
*Eman Ali,Sathira Silva,Chetan Arora,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: FAIR improves fine-grained classification in VLMs by dynamically aligning image and text features, outperforming SOTA methods by 2.78%.


<details>
  <summary>Details</summary>
Motivation: Existing methods for unsupervised adaptation in VLMs either lack adaptability or are computationally expensive, limiting their effectiveness in fine-grained tasks.

Method: FAIR introduces Class Description Anchors (CDA) and Learned Alignment Score (LAS) to dynamically align features and refine pseudo-labels, enhancing self-training.

Result: FAIR achieves a 2.78% performance gain over SOTA methods across 13 fine-grained datasets.

Conclusion: FAIR's dynamic alignment and refinement mechanisms significantly improve fine-grained unsupervised adaptation in VLMs.

Abstract: Vision-language models (VLMs) like CLIP excel in zero-shot learning by
aligning image and text representations through contrastive pretraining.
Existing approaches to unsupervised adaptation (UA) for fine-grained
classification with VLMs either rely on fixed alignment scores that cannot
capture evolving, subtle class distinctions or use computationally expensive
pseudo-labeling strategies that limit scalability. In contrast, we show that
modeling fine-grained cross-modal interactions during adaptation produces more
accurate, class-discriminative pseudo-labels and substantially improves
performance over state-of-the-art (SOTA) methods. We introduce Fine-grained
Alignment and Interaction Refinement (FAIR), an innovative approach that
dynamically aligns localized image features with descriptive language
embeddings through a set of Class Description Anchors (CDA). This enables the
definition of a Learned Alignment Score (LAS), which incorporates CDA as an
adaptive classifier, facilitating cross-modal interactions to improve
self-training in unsupervised adaptation. Furthermore, we propose a
self-training weighting mechanism designed to refine pseudo-labels in the
presence of inter-class ambiguities. Our approach, FAIR, delivers a substantial
performance boost in fine-grained unsupervised adaptation, achieving a notable
overall gain of 2.78% across 13 fine-grained datasets compared to SOTA methods.

</details>


### [137] [Generate Aligned Anomaly: Region-Guided Few-Shot Anomaly Image-Mask Pair Synthesis for Industrial Inspection](https://arxiv.org/abs/2507.09619)
*Yilin Lu,Jianghang Lin,Linhuang Xie,Kai Zhao,Yansong Qu,Shengchuan Zhang,Liujuan Cao,Rongrong Ji*

Main category: cs.CV

TL;DR: GAA is a framework for generating realistic and aligned anomaly-mask pairs using few-shot learning, improving anomaly inspection in industrial manufacturing.


<details>
  <summary>Details</summary>
Motivation: Existing anomaly synthesis methods suffer from low realism, poor mask alignment, and limited generalization due to scarce anomaly samples.

Method: GAA uses a pretrained latent diffusion model, Localized Concept Decomposition for semantic and spatial control, Adaptive Multi-Round Anomaly Clustering for consistency, and region-guided mask generation for alignment.

Result: GAA outperforms existing methods in anomaly synthesis quality and downstream tasks like localization and classification on MVTec AD and LOCO datasets.

Conclusion: GAA effectively addresses the limitations of current anomaly synthesis methods, offering realistic, diverse, and aligned anomaly-mask pairs for improved industrial inspection.

Abstract: Anomaly inspection plays a vital role in industrial manufacturing, but the
scarcity of anomaly samples significantly limits the effectiveness of existing
methods in tasks such as localization and classification. While several anomaly
synthesis approaches have been introduced for data augmentation, they often
struggle with low realism, inaccurate mask alignment, and poor generalization.
To overcome these limitations, we propose Generate Aligned Anomaly (GAA), a
region-guided, few-shot anomaly image-mask pair generation framework. GAA
leverages the strong priors of a pretrained latent diffusion model to generate
realistic, diverse, and semantically aligned anomalies using only a small
number of samples. The framework first employs Localized Concept Decomposition
to jointly model the semantic features and spatial information of anomalies,
enabling flexible control over the type and location of anomalies. It then
utilizes Adaptive Multi-Round Anomaly Clustering to perform fine-grained
semantic clustering of anomaly concepts, thereby enhancing the consistency of
anomaly representations. Subsequently, a region-guided mask generation strategy
ensures precise alignment between anomalies and their corresponding masks,
while a low-quality sample filtering module is introduced to further improve
the overall quality of the generated samples. Extensive experiments on the
MVTec AD and LOCO datasets demonstrate that GAA achieves superior performance
in both anomaly synthesis quality and downstream tasks such as localization and
classification.

</details>


### [138] [Brain Stroke Detection and Classification Using CT Imaging with Transformer Models and Explainable AI](https://arxiv.org/abs/2507.09630)
*Shomukh Qari,Maha A. Thafar*

Main category: cs.CV

TL;DR: An AI framework using MaxViT for multiclass stroke classification from CT scans achieved 98% accuracy, integrating XAI for transparency.


<details>
  <summary>Details</summary>
Motivation: Early and accurate stroke diagnosis is critical for patient outcomes, especially in emergencies. CT scans are widely used, but AI can enhance classification.

Method: Used MaxViT and other transformer variants for stroke classification, with data augmentation and synthetic images to address imbalance. Integrated XAI (Grad-CAM++) for explainability.

Result: MaxViT with augmentation achieved 98.00% accuracy and F1-score, outperforming other models.

Conclusion: The study provides a trustworthy, interpretable AI tool for early stroke detection, aiding clinical practice and emergency care.

Abstract: Stroke is one of the leading causes of death globally, making early and
accurate diagnosis essential for improving patient outcomes, particularly in
emergency settings where timely intervention is critical. CT scans are the key
imaging modality because of their speed, accessibility, and cost-effectiveness.
This study proposed an artificial intelligence framework for multiclass stroke
classification (ischemic, hemorrhagic, and no stroke) using CT scan images from
a dataset provided by the Republic of Turkey's Ministry of Health. The proposed
method adopted MaxViT, a state-of-the-art Vision Transformer, as the primary
deep learning model for image-based stroke classification, with additional
transformer variants (vision transformer, transformer-in-transformer, and
ConvNext). To enhance model generalization and address class imbalance, we
applied data augmentation techniques, including synthetic image generation. The
MaxViT model trained with augmentation achieved the best performance, reaching
an accuracy and F1-score of 98.00%, outperforming all other evaluated models
and the baseline methods. The primary goal of this study was to distinguish
between stroke types with high accuracy while addressing crucial issues of
transparency and trust in artificial intelligence models. To achieve this,
Explainable Artificial Intelligence (XAI) was integrated into the framework,
particularly Grad-CAM++. It provides visual explanations of the model's
decisions by highlighting relevant stroke regions in the CT scans and
establishing an accurate, interpretable, and clinically applicable solution for
early stroke detection. This research contributed to the development of a
trustworthy AI-assisted diagnostic tool for stroke, facilitating its
integration into clinical practice and enhancing access to timely and optimal
stroke diagnosis in emergency departments, thereby saving more lives.

</details>


### [139] [Disentanglement and Assessment of Shortcuts in Ophthalmological Retinal Imaging Exams](https://arxiv.org/abs/2507.09640)
*Leonor Fernandes,Tiago Gonalves,Joo Matos,Luis Filipe Nakayama,Jaime S. Cardoso*

Main category: cs.CV

TL;DR: The paper evaluates AI models for diabetic retinopathy (DR) prediction, focusing on fairness and bias mitigation using disentanglement. It highlights performance disparities and mixed results of disentanglement across models.


<details>
  <summary>Details</summary>
Motivation: To address concerns about fairness and generalization in AI-based DR diagnosis, ensuring equitable healthcare solutions.

Method: Three models (ConvNeXt V2, DINOv2, Swin V2) were trained on macula images to predict DR and sensitive attributes (age, gender/sex). Fairness was assessed, and disentanglement was used for bias mitigation.

Result: High DR prediction performance (up to 94% AUROC) but fairness disparities (e.g., 10% AUROC gap in age groups). Disentanglement improved DINOv2 (2% gain) but reduced performance in ConvNeXt V2 and Swin V2.

Conclusion: Fairness in medical imaging AI is complex; disentanglement's effectiveness varies by model, emphasizing the need for equitable solutions.

Abstract: Diabetic retinopathy (DR) is a leading cause of vision loss in working-age
adults. While screening reduces the risk of blindness, traditional imaging is
often costly and inaccessible. Artificial intelligence (AI) algorithms present
a scalable diagnostic solution, but concerns regarding fairness and
generalization persist. This work evaluates the fairness and performance of
image-trained models in DR prediction, as well as the impact of disentanglement
as a bias mitigation technique, using the diverse mBRSET fundus dataset. Three
models, ConvNeXt V2, DINOv2, and Swin V2, were trained on macula images to
predict DR and sensitive attributes (SAs) (e.g., age and gender/sex). Fairness
was assessed between subgroups of SAs, and disentanglement was applied to
reduce bias. All models achieved high DR prediction performance in diagnosing
(up to 94% AUROC) and could reasonably predict age and gender/sex (91% and 77%
AUROC, respectively). Fairness assessment suggests disparities, such as a 10%
AUROC gap between age groups in DINOv2. Disentangling SAs from DR prediction
had varying results, depending on the model selected. Disentanglement improved
DINOv2 performance (2% AUROC gain), but led to performance drops in ConvNeXt V2
and Swin V2 (7% and 3%, respectively). These findings highlight the complexity
of disentangling fine-grained features in fundus imaging and emphasize the
importance of fairness in medical imaging AI to ensure equitable and reliable
healthcare solutions.

</details>


### [140] [EyeSeg: An Uncertainty-Aware Eye Segmentation Framework for AR/VR](https://arxiv.org/abs/2507.09649)
*Zhengyuan Peng,Jianqing Xu,Shen Li,Jiazhen Ji,Yuge Huang,Jingyun Zhang,Jinmin Li,Shouhong Ding,Rizen Guo,Xin Tan,Lizhuang Ma*

Main category: cs.CV

TL;DR: EyeSeg is an uncertainty-aware eye segmentation framework for AR/VR, addressing challenges like motion blur and domain gaps, outperforming existing methods in accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Accurate gaze estimation in AR/VR requires robust eye segmentation, but current methods struggle with motion blur, occlusion, and domain gaps.

Method: EyeSeg uses Bayesian uncertainty learning to model segmentation uncertainties, outputting both segmentation results and uncertainty scores for robust gaze estimation.

Result: EyeSeg improves segmentation metrics (MIoU, E1, F1, ACC) and outperforms existing methods, especially under challenging conditions.

Conclusion: EyeSeg provides a robust solution for eye segmentation in AR/VR, enhancing gaze estimation performance and addressing key limitations of prior approaches.

Abstract: Human-machine interaction through augmented reality (AR) and virtual reality
(VR) is increasingly prevalent, requiring accurate and efficient gaze
estimation which hinges on the accuracy of eye segmentation to enable smooth
user experiences. We introduce EyeSeg, a novel eye segmentation framework
designed to overcome key challenges that existing approaches struggle with:
motion blur, eyelid occlusion, and train-test domain gaps. In these situations,
existing models struggle to extract robust features, leading to suboptimal
performance. Noting that these challenges can be generally quantified by
uncertainty, we design EyeSeg as an uncertainty-aware eye segmentation
framework for AR/VR wherein we explicitly model the uncertainties by performing
Bayesian uncertainty learning of a posterior under the closed set prior.
Theoretically, we prove that a statistic of the learned posterior indicates
segmentation uncertainty levels and empirically outperforms existing methods in
downstream tasks, such as gaze estimation. EyeSeg outputs an uncertainty score
and the segmentation result, weighting and fusing multiple gaze estimates for
robustness, which proves to be effective especially under motion blur, eyelid
occlusion and cross-domain challenges. Moreover, empirical results suggest that
EyeSeg achieves segmentation improvements of MIoU, E1, F1, and ACC surpassing
previous approaches. The code is publicly available at
https://github.com/JethroPeng/EyeSeg.

</details>


### [141] [VST-Pose: A Velocity-Integrated Spatiotem-poral Attention Network for Human WiFi Pose Estimation](https://arxiv.org/abs/2507.09672)
*Xinyu Zhang,Zhonghao Ye,Jingwei Zhang,Xiang Tian,Zhisheng Liang,Shipeng Yu*

Main category: cs.CV

TL;DR: VST-Pose is a WiFi-based human pose estimation framework using deep learning, achieving high accuracy and privacy benefits.


<details>
  <summary>Details</summary>
Motivation: To provide a non-visual, privacy-aware solution for continuous human pose estimation in indoor environments.

Method: Uses ViSTA-Former, a spatiotemporal attention backbone with dual-stream architecture, and integrates a velocity modeling branch for fine-grained motion.

Result: Achieves 92.2% accuracy on PCK@50, outperforming existing methods by 8.3%. Also robust on public MMFi dataset.

Conclusion: VST-Pose offers a reliable, privacy-aware solution for indoor human motion analysis.

Abstract: WiFi-based human pose estimation has emerged as a promising non-visual
alternative approaches due to its pene-trability and privacy advantages. This
paper presents VST-Pose, a novel deep learning framework for accurate and
continuous pose estimation using WiFi channel state information. The proposed
method introduces ViSTA-Former, a spatiotemporal attention backbone with
dual-stream architecture that adopts a dual-stream architecture to separately
capture temporal dependencies and structural relationships among body joints.
To enhance sensitivity to subtle human motions, a velocity modeling branch is
integrated into the framework, which learns short-term keypoint dis-placement
patterns and improves fine-grained motion representation. We construct a 2D
pose dataset specifically designed for smart home care scenarios and
demonstrate that our method achieves 92.2% accuracy on the PCK@50 metric,
outperforming existing methods by 8.3% in PCK@50 on the self-collected dataset.
Further evaluation on the public MMFi dataset confirms the model's robustness
and effectiveness in 3D pose estimation tasks. The proposed system provides a
reliable and privacy-aware solution for continuous human motion analysis in
indoor environments. Our codes are available in
https://github.com/CarmenQing/VST-Pose.

</details>


### [142] [Prompt2DEM: High-Resolution DEMs for Urban and Open Environments from Global Prompts Using a Monocular Foundation Model](https://arxiv.org/abs/2507.09681)
*Osher Rafaeli,Tal Svoray,Ariel Nahlieli*

Main category: cs.CV

TL;DR: A framework for high-resolution DEM estimation using prompt-based monocular depth estimation, achieving 100x resolution gain and robust generalization across diverse landscapes.


<details>
  <summary>Details</summary>
Motivation: High-resolution elevation data is crucial for hydrology, urban studies, and ecosystem monitoring, but existing methods like super-resolution and monocular depth estimation have limitations.

Method: Uses low-resolution SRTM data as prompts and high-resolution NAIP imagery, fine-tuning a vision transformer encoder with LiDAR-derived DEMs for tasks like DEM estimation and void filling.

Result: Achieves 100x resolution gain (30-m to 30-cm) with <5 m MAE relative to LiDAR, improving over SRTM by up to 18%. Suitable for hydrological and environmental studies.

Conclusion: The framework offers a scalable, versatile solution for high-resolution DEM estimation, with publicly available code and models.

Abstract: High-resolution elevation estimations are essential to understand catchment
and hillslope hydrology, study urban morphology and dynamics, and monitor the
growth, decline, and mortality of terrestrial ecosystems. Various deep learning
approaches (e.g., super-resolution techniques, monocular depth estimation) have
been developed to create high-resolution Digital Elevation Models (DEMs).
However, super-resolution techniques are limited by the upscaling factor, and
monocular depth estimation lacks global elevation context, making its
conversion to a seamless DEM restricted. The recently introduced technique of
prompt-based monocular depth estimation has opened new opportunities to extract
estimates of absolute elevation in a global context. We present here a
framework for the estimation of high-resolution DEMs as a new paradigm for
absolute global elevation mapping. It is exemplified using low-resolution
Shuttle Radar Topography Mission (SRTM) elevation data as prompts and
high-resolution RGB imagery from the National Agriculture Imagery Program
(NAIP). The approach fine-tunes a vision transformer encoder with LiDAR-derived
DEMs and employs a versatile prompting strategy, enabling tasks such as DEM
estimation, void filling, and updating. Our framework achieves a 100x
resolution gain (from 30-m to 30-cm), surpassing prior methods by an order of
magnitude. Evaluations across three diverse U.S. landscapes show robust
generalization, capturing urban structures and fine-scale terrain features with
< 5 m MAE relative to LiDAR, improving over SRTM by up to 18%. Hydrological
analysis confirms suitability for hazard and environmental studies. We
demonstrate scalability by applying the framework to large regions in the U.S.
and Israel. All code and pretrained models are publicly available at:
https://osherr1996.github.io/prompt2dem_propage/.

</details>


### [143] [ViTCoT: Video-Text Interleaved Chain-of-Thought for Boosting Video Understanding in Large Language Models](https://arxiv.org/abs/2507.09876)
*Yongheng Zhang,Xu Liu,Ruihan Tao,Qiguang Chen,Hao Fei,Wanxiang Che,Libo Qin*

Main category: cs.CV

TL;DR: ViTCoT introduces a video-text interleaved reasoning paradigm to improve video understanding by combining visual and textual modalities, outperforming text-only CoT methods.


<details>
  <summary>Details</summary>
Motivation: Current video reasoning relies heavily on text, ignoring visual content, unlike human reasoning which integrates both. ViTCoT aims to bridge this gap.

Method: Developed ViTCoT, a novel paradigm combining video and text for reasoning, and created ViTIB, a benchmark using MLLMs for key-video selection and manual verification.

Result: ViTCoT significantly outperforms traditional text-only CoT and activates more neuron values in MLLMs.

Conclusion: ViTCoT offers a more intuitive and effective approach to video reasoning by integrating visual and textual information.

Abstract: Video understanding plays a vital role in bridging low-level visual signals
with high-level cognitive reasoning, and is fundamental to applications such as
autonomous driving, embodied AI, and the broader pursuit of AGI. The rapid
development of large language models (LLMs), particularly those utilizing
Chain-of-Thought (CoT) technology, has significantly advanced video reasoning
capabilities. However, current approaches primarily depend on textual
information for reasoning, overlooking the visual modality in the actual video
reasoning process. In contrast, humans naturally re-examine visual content
while reasoning. Motivated by this, we introduce a novel video reasoning
paradigm: Video-Text Interleaved CoT (ViTCoT), which facilitates more intuitive
and cognitively aligned reasoning. To the end, first, we construct the
Video-Text Interleaved Benchmark (ViTIB), which is created using MLLMs for
key-video selection and manually verified. Furthermore, we extensively explore
the potential of the ViTCoT paradigm in the video understanding field.
Extensive experiments demonstrate that ViTCoT significantly enhances
performance compared to the traditional text-only CoT paradigm and effectively
activates more neuron values in MLLMs.

</details>


### [144] [ExpStar: Towards Automatic Commentary Generation for Multi-discipline Scientific Experiments](https://arxiv.org/abs/2507.09693)
*Jiali Chen,Yujie Jia,Zihan Wu,Jinyu Yang,Jianpeng Chen,Xusen Hei,Jiayuan Xie,Yi Cai,Qing Li*

Main category: cs.CV

TL;DR: The paper introduces ExpStar, a model for automatic experiment commentary generation, and ExpInstruct, a dataset for this task, outperforming 14 leading LMMs.


<details>
  <summary>Details</summary>
Motivation: Human teachers spend significant time preparing experiment commentary, which requires subject expertise. Automating this could save time and enhance instruction.

Method: The paper constructs ExpInstruct, a dataset with 7K step-level commentaries, and proposes ExpStar, a retrieval-augmented model for commentary generation.

Result: ExpStar outperforms 14 leading LMMs, demonstrating its effectiveness in generating insightful experiment commentary.

Conclusion: ExpStar shows promise for AI-assisted scientific experiment instruction, leveraging its dataset and retrieval-augmented approach.

Abstract: Experiment commentary is crucial in describing the experimental procedures,
delving into underlying scientific principles, and incorporating
content-related safety guidelines. In practice, human teachers rely heavily on
subject-specific expertise and invest significant time preparing such
commentary. To address this challenge, we introduce the task of automatic
commentary generation across multi-discipline scientific experiments. While
recent progress in large multimodal models (LMMs) has demonstrated promising
capabilities in video understanding and reasoning, their ability to generate
fine-grained and insightful experiment commentary remains largely
underexplored. In this paper, we make the following contributions: (i) We
construct \textit{ExpInstruct}, the first dataset tailored for experiment
commentary generation, featuring over 7\textit{K} step-level commentaries
across 21 scientific subjects from 3 core disciplines (\ie, science, healthcare
and engineering). Each sample includes procedural descriptions along with
potential scientific principles (\eg, chemical equations and physical laws) and
safety guidelines. (ii) We propose ExpStar, an automatic experiment commentary
generation model that leverages a retrieval-augmented mechanism to adaptively
access, evaluate, and utilize external knowledge. (iii) Extensive experiments
show that our ExpStar substantially outperforms 14 leading LMMs, which
highlights the superiority of our dataset and model. We believe that ExpStar
holds great potential for advancing AI-assisted scientific experiment
instruction.

</details>


### [145] [Token Compression Meets Compact Vision Transformers: A Survey and Comparative Evaluation for Edge AI](https://arxiv.org/abs/2507.09702)
*Phat Nguyen,Ngai-Man Cheung*

Main category: cs.CV

TL;DR: The paper surveys token compression techniques for Vision Transformers (ViTs), categorizing them by strategy and deployment, and evaluates their effectiveness on standard and compact ViTs.


<details>
  <summary>Details</summary>
Motivation: Address gaps in unified surveys and benchmarks for token compression methods, especially their applicability to compact ViTs for edge devices.

Method: Presents a systematic taxonomy and comparative study of token compression techniques, testing them on standard and compact ViT architectures.

Result: Token compression works well for general ViTs but often underperforms on compact designs.

Conclusion: Highlights the need for adapting token optimization techniques for compact transformers in edge AI applications.

Abstract: Token compression techniques have recently emerged as powerful tools for
accelerating Vision Transformer (ViT) inference in computer vision. Due to the
quadratic computational complexity with respect to the token sequence length,
these methods aim to remove less informative tokens before the attention layers
to improve inference throughput. While numerous studies have explored various
accuracy-efficiency trade-offs on large-scale ViTs, two critical gaps remain.
First, there is a lack of unified survey that systematically categorizes and
compares token compression approaches based on their core strategies (e.g.,
pruning, merging, or hybrid) and deployment settings (e.g., fine-tuning vs.
plug-in). Second, most benchmarks are limited to standard ViT models (e.g.,
ViT-B, ViT-L), leaving open the question of whether such methods remain
effective when applied to structurally compressed transformers, which are
increasingly deployed on resource-constrained edge devices. To address these
gaps, we present the first systematic taxonomy and comparative study of token
compression methods, and we evaluate representative techniques on both standard
and compact ViT architectures. Our experiments reveal that while token
compression methods are effective for general-purpose ViTs, they often
underperform when directly applied to compact designs. These findings not only
provide practical insights but also pave the way for future research on
adapting token optimization techniques to compact transformer-based networks
for edge AI and AI agent applications.

</details>


### [146] [Advancing Text-to-3D Generation with Linearized Lookahead Variational Score Distillation](https://arxiv.org/abs/2507.09748)
*Yu Lei,Bingde Liu,Qingsong Xie,Haonan Lu,Zhijie Deng*

Main category: cs.CV

TL;DR: The paper introduces Linearized Lookahead Variational Score Distillation (L-VSD) to improve text-to-3D generation by addressing convergence issues in VSD through optimization order adjustment and linearized modeling.


<details>
  <summary>Details</summary>
Motivation: To overcome slow and ill-posed convergence in Variational Score Distillation (VSD) for text-to-3D generation, caused by mismatched distributions between LoRA and 3D models.

Method: Proposes L-VSD, which adjusts optimization order and uses a linearized variant of the model for stable and efficient score distillation.

Result: L-VSD outperforms prior methods in generation quality and stability, validated through extensive experiments.

Conclusion: L-VSD effectively enhances VSD-based text-to-3D frameworks and can be integrated seamlessly into existing systems.

Abstract: Text-to-3D generation based on score distillation of pre-trained 2D diffusion
models has gained increasing interest, with variational score distillation
(VSD) as a remarkable example. VSD proves that vanilla score distillation can
be improved by introducing an extra score-based model, which characterizes the
distribution of images rendered from 3D models, to correct the distillation
gradient. Despite the theoretical foundations, VSD, in practice, is likely to
suffer from slow and sometimes ill-posed convergence. In this paper, we perform
an in-depth investigation of the interplay between the introduced score model
and the 3D model, and find that there exists a mismatching problem between LoRA
and 3D distributions in practical implementation. We can simply adjust their
optimization order to improve the generation quality. By doing so, the score
model looks ahead to the current 3D state and hence yields more reasonable
corrections. Nevertheless, naive lookahead VSD may suffer from unstable
training in practice due to the potential over-fitting. To address this, we
propose to use a linearized variant of the model for score distillation, giving
rise to the Linearized Lookahead Variational Score Distillation ($L^2$-VSD).
$L^2$-VSD can be realized efficiently with forward-mode autodiff
functionalities of existing deep learning libraries. Extensive experiments
validate the efficacy of $L^2$-VSD, revealing its clear superiority over prior
score distillation-based methods. We also show that our method can be
seamlessly incorporated into any other VSD-based text-to-3D framework.

</details>


### [147] [Cross-modal Associations in Vision and Language Models: Revisiting the bouba-kiki effect](https://arxiv.org/abs/2507.10013)
*Tom Kouwenhoven,Kiana Shahrasbi,Tessa Verhoef*

Main category: cs.CV

TL;DR: The paper re-evaluates the bouba-kiki effect in vision-and-language models (VLMs), finding they lack consistent human-like cross-modal associations.


<details>
  <summary>Details</summary>
Motivation: To assess if VLMs integrate cross-modal information like humans, using the bouba-kiki effect as a test case.

Method: Evaluated two CLIP variants (ResNet and ViT) using prompt-based probabilities and Grad-CAM for visual attention.

Result: VLMs did not consistently show the bouba-kiki effect; their responses diverged from human cognition.

Conclusion: VLMs' cross-modal understanding is limited, highlighting gaps in their alignment with human intuitions.

Abstract: Recent advances in multimodal models have raised questions about whether
vision-and-language models (VLMs) integrate cross-modal information in ways
that reflect human cognition. One well-studied test case in this domain is the
bouba-kiki effect, where humans reliably associate pseudowords like "bouba"
with round shapes and "kiki" with jagged ones. Given the mixed evidence found
in prior studies for this effect in VLMs, we present a comprehensive
re-evaluation focused on two variants of CLIP, ResNet and Vision Transformer
(ViT), given their centrality in many state-of-the-art VLMs. We apply two
complementary methods closely modelled after human experiments: a prompt-based
evaluation that uses probabilities as model preference, and we use Grad-CAM as
a novel way to interpret visual attention in shape-word matching tasks. Our
findings show that these models do not consistently exhibit the bouba-kiki
effect. While ResNet shows a preference for round shapes, overall performance
across both models lacks the expected associations. Moreover, direct comparison
with prior human data on the same task shows that the models' responses fall
markedly short of the robust, modality-integrated behaviour characteristic of
human cognition. These results contribute to the ongoing debate about the
extent to which VLMs truly understand cross-modal concepts, highlighting
limitations in their internal representations and alignment with human
intuitions.

</details>


### [148] [Pairwise Alignment & Compatibility for Arbitrarily Irregular Image Fragments](https://arxiv.org/abs/2507.09767)
*Ofir Itzhak Shahar,Gur Elkin,Ohad Ben-Shahar*

Main category: cs.CV

TL;DR: A hybrid geometric and pictorial approach for optimal fragment alignment without shape or content assumptions, tested on a new dataset with state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail with realistic fragment shapes or rely on restricted geometries, limiting real-world applicability.

Method: Proposes a hybrid approach combining geometric and pictorial features, introduces a new dataset with erosion modeling, and embeds compatibility into an archaeological framework.

Result: Achieves state-of-the-art precision and recall on the RePAIR 2D dataset, showing improved compatibility performance.

Conclusion: The hybrid approach effectively handles diverse fragment shapes and content, advancing real-world puzzle-solving capabilities.

Abstract: Pairwise compatibility calculation is at the core of most
fragments-reconstruction algorithms, in particular those designed to solve
different types of the jigsaw puzzle problem. However, most existing approaches
fail, or aren't designed to deal with fragments of realistic geometric
properties one encounters in real-life puzzles. And in all other cases,
compatibility methods rely strongly on the restricted shapes of the fragments.
In this paper, we propose an efficient hybrid (geometric and pictorial)
approach for computing the optimal alignment for pairs of fragments, without
any assumptions about their shapes, dimensions, or pictorial content. We
introduce a new image fragments dataset generated via a novel method for image
fragmentation and a formal erosion model that mimics real-world archaeological
erosion, along with evaluation metrics for the compatibility task. We then
embed our proposed compatibility into an archaeological puzzle-solving
framework and demonstrate state-of-the-art neighborhood-level precision and
recall on the RePAIR 2D dataset, directly reflecting compatibility performance
improvements.

</details>


### [149] [NegRefine: Refining Negative Label-Based Zero-Shot OOD Detection](https://arxiv.org/abs/2507.09795)
*Amirhossein Ansari,Ke Wang,Pulei Xiong*

Main category: cs.CV

TL;DR: NegRefine improves zero-shot OOD detection by refining negative labels and dynamically adjusting scoring for multi-matching images.


<details>
  <summary>Details</summary>
Motivation: Existing negative label-based methods misclassify in-distribution samples as OOD due to subcategory labels and proper nouns, and struggle with multi-matching images.

Method: NegRefine filters subcategory labels and proper nouns from negative labels and uses a multi-matching-aware scoring function.

Result: NegRefine achieves robust separation between in-distribution and OOD samples on benchmarks like ImageNet-1K.

Conclusion: NegRefine addresses limitations of prior methods, enhancing zero-shot OOD detection performance.

Abstract: Recent advancements in Vision-Language Models like CLIP have enabled
zero-shot OOD detection by leveraging both image and textual label information.
Among these, negative label-based methods such as NegLabel and CSP have shown
promising results by utilizing a lexicon of words to define negative labels for
distinguishing OOD samples. However, these methods suffer from detecting
in-distribution samples as OOD due to negative labels that are subcategories of
in-distribution labels or proper nouns. They also face limitations in handling
images that match multiple in-distribution and negative labels. We propose
NegRefine, a novel negative label refinement framework for zero-shot OOD
detection. By introducing a filtering mechanism to exclude subcategory labels
and proper nouns from the negative label set and incorporating a
multi-matching-aware scoring function that dynamically adjusts the
contributions of multiple labels matching an image, NegRefine ensures a more
robust separation between in-distribution and OOD samples. We evaluate
NegRefine on large-scale benchmarks, including ImageNet-1K. Source code is
available at https://github.com/ah-ansari/NegRefine.

</details>


### [150] [VRU-Accident: A Vision-Language Benchmark for Video Question Answering and Dense Captioning for Accident Scene Understanding](https://arxiv.org/abs/2507.09815)
*Younggun Kim,Ahmed S. Abdelrahman,Mohamed Abdel-Aty*

Main category: cs.CV

TL;DR: VRU-Accident is a benchmark for evaluating multimodal large language models (MLLMs) in high-risk VRU-vehicle accident scenarios, revealing gaps in reasoning and accident description.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of standardized benchmarks to evaluate MLLMs' reasoning in safety-critical VRU scenarios.

Method: Introduces VRU-Accident, a dataset with 1K dashcam videos, 6K QA pairs, and 1K dense descriptions, focusing on VRU-vehicle accidents.

Result: MLLMs perform well on visual attributes but struggle with reasoning about accident causes, types, and preventability.

Conclusion: VRU-Accident highlights the need for improved MLLM capabilities in safety-critical reasoning for autonomous driving.

Abstract: Ensuring the safety of vulnerable road users (VRUs), such as pedestrians and
cyclists, is a critical challenge for autonomous driving systems, as crashes
involving VRUs often result in severe or fatal consequences. While multimodal
large language models (MLLMs) have shown promise in enhancing scene
understanding and decision making in autonomous vehicles, there is currently no
standardized benchmark to quantitatively evaluate their reasoning abilities in
complex, safety-critical scenarios involving VRUs. To address this gap, we
present VRU-Accident, a large-scale vision-language benchmark designed to
evaluate MLLMs in high-risk traffic scenarios involving VRUs. VRU-Accident
comprises 1K real-world dashcam accident videos, annotated with 6K
multiple-choice question-answer pairs across six safety-critical categories
(with 24K candidate options and 3.4K unique answer choices), as well as 1K
dense scene descriptions. Unlike prior works, our benchmark focuses explicitly
on VRU-vehicle accidents, providing rich, fine-grained annotations that capture
both spatial-temporal dynamics and causal semantics of accidents. To assess the
current landscape of MLLMs, we conduct a comprehensive evaluation of 17
state-of-the-art models on the multiple-choice VQA task and on the dense
captioning task. Our findings reveal that while MLLMs perform reasonably well
on visually grounded attributes, they face significant challenges in reasoning
and describing accident causes, types, and preventability.

</details>


### [151] [Hierarchical Abstraction Enables Human-Like 3D Object Recognition in Deep Learning Models](https://arxiv.org/abs/2507.09830)
*Shuhao Fu,Philip J. Kellman,Hongjing Lu*

Main category: cs.CV

TL;DR: The paper compares human and deep learning models in recognizing 3D objects from point clouds, finding that transformer-based models align better with human performance due to hierarchical shape abstraction.


<details>
  <summary>Details</summary>
Motivation: To determine whether deep learning models develop 3D shape representations similar to human vision for object recognition.

Method: Conducted human experiments manipulating point density, object orientation, and local geometric structure, and compared performance with CNN and transformer-based models.

Result: Point transformer models better matched human performance, attributed to their hierarchical abstraction of 3D shapes.

Conclusion: Transformer-based models more closely mimic human 3D shape recognition, suggesting their representations align better with human vision.

Abstract: Both humans and deep learning models can recognize objects from 3D shapes
depicted with sparse visual information, such as a set of points randomly
sampled from the surfaces of 3D objects (termed a point cloud). Although deep
learning models achieve human-like performance in recognizing objects from 3D
shapes, it remains unclear whether these models develop 3D shape
representations similar to those used by human vision for object recognition.
We hypothesize that training with 3D shapes enables models to form
representations of local geometric structures in 3D shapes. However, their
representations of global 3D object shapes may be limited. We conducted two
human experiments systematically manipulating point density and object
orientation (Experiment 1), and local geometric structure (Experiment 2).
Humans consistently performed well across all experimental conditions. We
compared two types of deep learning models, one based on a convolutional neural
network (DGCNN) and the other on visual transformers (point transformer), with
human performance. We found that the point transformer model provided a better
account of human performance than the convolution-based model. The advantage
mainly results from the mechanism in the point transformer model that supports
hierarchical abstraction of 3D shapes.

</details>


### [152] [FaceLLM: A Multimodal Large Language Model for Face Understanding](https://arxiv.org/abs/2507.10300)
*Hatef Otroshi Shahreza,Sbastien Marcel*

Main category: cs.CV

TL;DR: FaceLLM is a multimodal large language model specialized for facial image understanding, trained using weakly supervised data generated by ChatGPT from the FairFace dataset. It improves performance on face-centric tasks and sets a precedent for domain-specialized MLLMs.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs lack domain-specific reasoning for facial cues due to limited annotated datasets. FaceLLM addresses this gap by focusing on facial attributes like expression, emotion, and demographics.

Method: A weakly supervised pipeline uses ChatGPT with attribute-aware prompts to generate question-answer pairs from the FairFace dataset, creating the FairFaceGPT corpus. FaceLLM is trained on this data.

Result: FaceLLM outperforms existing MLLMs on face-centric tasks, achieving state-of-the-art performance.

Conclusion: FaceLLM demonstrates the potential of synthetic supervision via language models for domain-specialized MLLMs, promoting trustworthy, human-centric AI.

Abstract: Multimodal large language models (MLLMs) have shown remarkable performance in
vision-language tasks. However, existing MLLMs are primarily trained on generic
datasets, limiting their ability to reason on domain-specific visual cues such
as those in facial images. In particular, tasks that require detailed
understanding of facial structure, expression, emotion, and demographic
features remain underexplored by MLLMs due to the lack of large-scale annotated
face image-text datasets. In this work, we introduce FaceLLM, a multimodal
large language model trained specifically for facial image understanding. To
construct the training data, we propose a novel weakly supervised pipeline that
uses ChatGPT with attribute-aware prompts to generate high-quality
question-answer pairs based on images from the FairFace dataset. The resulting
corpus, called FairFaceGPT, covers a diverse set of attributes including
expression, pose, skin texture, and forensic information. Our experiments
demonstrate that FaceLLM improves the performance of MLLMs on various
face-centric tasks and achieves state-of-the-art performance. This work
highlights the potential of synthetic supervision via language models for
building domain-specialized MLLMs, and sets a precedent for trustworthy,
human-centric multimodal AI systems. FairFaceGPT dataset and pretrained FaceLLM
models are publicly available in the project page.

</details>


### [153] [A Survey on MLLM-based Visually Rich Document Understanding: Methods, Challenges, and Emerging Trends](https://arxiv.org/abs/2507.09861)
*Yihao Ding,Siwen Luo,Yue Dai,Yanbei Jiang,Zechuan Li,Geoffrey Martin,Yifan Peng*

Main category: cs.CV

TL;DR: A survey on Multimodal Large Language Models (MLLMs) for Visually-Rich Document Understanding (VRDU), covering feature fusion, training methods, datasets, and future challenges.


<details>
  <summary>Details</summary>
Motivation: The need to automate processing of complex documents with visual, textual, and layout information drives advancements in VRDU using MLLMs.

Method: Reviews MLLM-based VRDU, focusing on encoding/fusing features, training paradigms (pretraining, tuning), and datasets.

Result: Highlights progress in MLLMs for VRDU, emphasizing feature integration and training strategies.

Conclusion: Identifies challenges and opportunities, suggesting future directions for improving VRDU system efficiency and robustness.

Abstract: Visually-Rich Document Understanding (VRDU) has emerged as a critical field,
driven by the need to automatically process documents containing complex
visual, textual, and layout information. Recently, Multimodal Large Language
Models (MLLMs) have shown remarkable potential in this domain, leveraging both
Optical Character Recognition (OCR)-dependent and OCR-free frameworks to
extract and interpret information in document images. This survey reviews
recent advancements in MLLM-based VRDU, highlighting three core components: (1)
methods for encoding and fusing textual, visual, and layout features; (2)
training paradigms, including pretraining strategies, instruction-response
tuning, and the trainability of different model modules; and (3) datasets
utilized for pretraining, instruction-tuning, and supervised fine-tuning.
Finally, we discuss the challenges and opportunities in this evolving field and
propose future directions to advance the efficiency, generalizability, and
robustness of VRDU systems.

</details>


### [154] [Devanagari Handwritten Character Recognition using Convolutional Neural Network](https://arxiv.org/abs/2507.10398)
*Diksha Mehta,Prateek Mehta*

Main category: cs.CV

TL;DR: The paper presents a deep learning approach for recognizing handwritten Devanagari characters, achieving high accuracy (96.36% testing, 99.55% training) using convolutional neural networks.


<details>
  <summary>Details</summary>
Motivation: Handwritten Devanagari script lacks proper digitization tools, and automated recognition can save time and improve data accessibility.

Method: Uses two deep convolutional neural network layers on the Devanagari Handwritten Character Dataset (DHCD) with 36 classes and 1700 images per class.

Result: Achieves 96.36% accuracy in testing and 99.55% in training.

Conclusion: The proposed method is effective for Devanagari handwritten text recognition, offering high accuracy and potential for practical applications.

Abstract: Handwritten character recognition is getting popular among researchers
because of its possible applications in facilitating technological search
engines, social media, recommender systems, etc. The Devanagari script is one
of the oldest language scripts in India that does not have proper digitization
tools. With the advancement of computing and technology, the task of this
research is to extract handwritten Hindi characters from an image of Devanagari
script with an automated approach to save time and obsolete data. In this
paper, we present a technique to recognize handwritten Devanagari characters
using two deep convolutional neural network layers. This work employs a
methodology that is useful to enhance the recognition rate and configures a
convolutional neural network for effective Devanagari handwritten text
recognition (DHTR). This approach uses the Devanagari handwritten character
dataset (DHCD), an open dataset with 36 classes of Devanagari characters. Each
of these classes has 1700 images for training and testing purposes. This
approach obtains promising results in terms of accuracy by achieving 96.36%
accuracy in testing and 99.55% in training time.

</details>


### [155] [SpeakerVid-5M: A Large-Scale High-Quality Dataset for Audio-Visual Dyadic Interactive Human Generation](https://arxiv.org/abs/2507.09862)
*Youliang Zhang,Zhaoyang Li,Duomin Wang,Jiahe Zhang,Deyu Zhou,Zixin Yin,Xili Dai,Gang Yu,Xiu Li*

Main category: cs.CV

TL;DR: The paper introduces SpeakerVid-5M, a large-scale dataset for audio-visual dyadic interactive virtual human research, with diverse interaction types and quality tiers, and provides a baseline model and benchmark.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of audio-visual dyadic interactive virtual humans by providing a comprehensive dataset and tools for research.

Method: Creation of the SpeakerVid-5M dataset (5.2M video clips, 8,743 hours) with structured interaction types and quality tiers, and development of an AR-based baseline model.

Result: A dataset and benchmark (VidChatBench) for virtual human tasks, with public release of data and code.

Conclusion: SpeakerVid-5M facilitates research in audio-visual dyadic interactions, offering a foundation for future work in virtual human generation.

Abstract: The rapid development of large-scale models has catalyzed significant
breakthroughs in the digital human domain. These advanced methodologies offer
high-fidelity solutions for avatar driving and rendering, leading academia to
focus on the next major challenge: audio-visual dyadic interactive virtual
human. To facilitate research in this emerging area, we present SpeakerVid-5M
dataset, the first large-scale, high-quality dataset designed for audio-visual
dyadic interactive virtual human generation. Totaling over 8,743 hours,
SpeakerVid-5M contains more than 5.2 million video clips of human portraits. It
covers diverse scales and interaction types, including monadic talking,
listening, and dyadic conversations. Crucially, the dataset is structured along
two key dimensions: interaction type and data quality. First, it is categorized
into four types (dialogue branch, single branch, listening branch and
multi-turn branch) based on the interaction scenario. Second, it is stratified
into a large-scale pre-training subset and a curated, high-quality subset for
Supervised Fine-Tuning (SFT). This dual structure accommodates a wide array of
2D virtual human tasks. In addition, we provide an autoregressive (AR)-based
video chat baseline trained on this data, accompanied by a dedicated set of
metrics and test data to serve as a benchmark VidChatBench for future work.
Both the dataset and the corresponding data processing code will be publicly
released. Project page: https://dorniwang.github.io/SpeakerVid-5M/

</details>


### [156] [Text-to-Remote-Sensing-Image Retrieval beyond RGB Sources](https://arxiv.org/abs/2507.10403)
*Daniele Rege Cambrin,Lorenzo Vaiani,Giuseppe Gallipoli,Luca Cagliero,Paolo Garza*

Main category: cs.CV

TL;DR: CrisisLandMark introduces a large-scale corpus of SAR and multispectral images with structured text annotations, and CLOSP aligns optical and SAR images using text, improving retrieval performance by 54%. GeoCLOSP adds geographic context for specialized tasks.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-image retrieval systems focus on RGB data, missing valuable information from other sensors like SAR and multispectral data. This limits applications in disaster response and climate monitoring.

Method: Develop CrisisLandMark, a corpus of 647,000 SAR and multispectral images with structured text annotations. Introduce CLOSP, a contrastive learning framework aligning optical and SAR images via text, and GeoCLOSP, which adds geographic coordinates.

Result: CLOSP improves retrieval nDGC by 54% over existing models. GeoCLOSP excels in location-dependent tasks, balancing generality and specificity.

Conclusion: Integrating diverse sensor data and geographic context unlocks the full potential of remote sensing archives for applications like disaster response and climate monitoring.

Abstract: Retrieving relevant imagery from vast satellite archives is crucial for
applications like disaster response and long-term climate monitoring. However,
most text-to-image retrieval systems are limited to RGB data, failing to
exploit the unique physical information captured by other sensors, such as the
all-weather structural sensitivity of Synthetic Aperture Radar (SAR) or the
spectral signatures in optical multispectral data. To bridge this gap, we
introduce CrisisLandMark, a new large-scale corpus of over 647,000 Sentinel-1
SAR and Sentinel-2 multispectral images paired with structured textual
annotations for land cover, land use, and crisis events harmonized from
authoritative land cover systems (CORINE and Dynamic World) and crisis-specific
sources. We then present CLOSP (Contrastive Language Optical SAR Pretraining),
a novel framework that uses text as a bridge to align unpaired optical and SAR
images into a unified embedding space. Our experiments show that CLOSP achieves
a new state-of-the-art, improving retrieval nDGC by 54% over existing models.
Additionally, we find that the unified training strategy overcomes the inherent
difficulty of interpreting SAR imagery by transferring rich semantic knowledge
from the optical domain with indirect interaction. Furthermore, GeoCLOSP, which
integrates geographic coordinates into our framework, creates a powerful
trade-off between generality and specificity: while the CLOSP excels at general
semantic tasks, the GeoCLOSP becomes a specialized expert for retrieving
location-dependent crisis events and rare geographic features. This work
highlights that the integration of diverse sensor data and geographic context
is essential for unlocking the full potential of remote sensing archives.

</details>


### [157] [OpenHuman4D: Open-Vocabulary 4D Human Parsing](https://arxiv.org/abs/2507.09880)
*Keito Suzuki,Bang Du,Runfa Blark Li,Kunyao Chen,Lei Wang,Peng Liu,Ning Bi,Truong Nguyen*

Main category: cs.CV

TL;DR: A 4D human parsing framework is introduced to reduce inference time and enable open-vocabulary capabilities, outperforming previous methods with 93.3% faster processing.


<details>
  <summary>Details</summary>
Motivation: Existing human part segmentation methods are limited by closed-set datasets and slow inference, hindering their practicality in virtual and extended reality applications.

Method: The framework extends open-vocabulary 3D parsing to 4D with mask-based tracking, a Mask Validation module, and a 4D Mask Fusion module for robust embedding.

Result: The method achieves significant speed improvements (93.3% faster) and handles dynamic 4D human parsing effectively.

Conclusion: The proposed framework advances 4D human parsing by combining efficiency, flexibility, and open-vocabulary support.

Abstract: Understanding dynamic 3D human representation has become increasingly
critical in virtual and extended reality applications. However, existing human
part segmentation methods are constrained by reliance on closed-set datasets
and prolonged inference times, which significantly restrict their
applicability. In this paper, we introduce the first 4D human parsing framework
that simultaneously addresses these challenges by reducing the inference time
and introducing open-vocabulary capabilities. Building upon state-of-the-art
open-vocabulary 3D human parsing techniques, our approach extends the support
to 4D human-centric video with three key innovations: 1) We adopt mask-based
video object tracking to efficiently establish spatial and temporal
correspondences, avoiding the necessity of segmenting all frames. 2) A novel
Mask Validation module is designed to manage new target identification and
mitigate tracking failures. 3) We propose a 4D Mask Fusion module, integrating
memory-conditioned attention and logits equalization for robust embedding
fusion. Extensive experiments demonstrate the effectiveness and flexibility of
the proposed method on 4D human-centric parsing tasks, achieving up to 93.3%
acceleration compared to the previous state-of-the-art method, which was
limited to parsing fixed classes.

</details>


### [158] [Counterfactual Visual Explanation via Causally-Guided Adversarial Steering](https://arxiv.org/abs/2507.09881)
*Yiran Qiao,Disheng Liu,Yiren Lu,Yu Yin,Mengnan Du,Jing Ma*

Main category: cs.CV

TL;DR: CECAS introduces a causally-guided adversarial method for generating high-quality counterfactual visual explanations, addressing spurious correlations and improving validity, sparsity, proximity, and realism.


<details>
  <summary>Details</summary>
Motivation: Existing counterfactual visual explanation methods ignore causal relationships and spurious correlations, leading to low-quality explanations.

Method: CECAS uses a causally-guided adversarial approach to generate counterfactuals, avoiding unwanted perturbations on spurious factors.

Result: Outperforms state-of-the-art methods on benchmark datasets, achieving balanced performance in validity, sparsity, proximity, and realism.

Conclusion: CECAS provides superior counterfactual explanations by integrating causal insights, enhancing model explainability.

Abstract: Recent work on counterfactual visual explanations has contributed to making
artificial intelligence models more explainable by providing visual
perturbation to flip the prediction. However, these approaches neglect the
causal relationships and the spurious correlations behind the image generation
process, which often leads to unintended alterations in the counterfactual
images and renders the explanations with limited quality. To address this
challenge, we introduce a novel framework CECAS, which first leverages a
causally-guided adversarial method to generate counterfactual explanations. It
innovatively integrates a causal perspective to avoid unwanted perturbations on
spurious factors in the counterfactuals. Extensive experiments demonstrate that
our method outperforms existing state-of-the-art approaches across multiple
benchmark datasets and ultimately achieves a balanced trade-off among various
aspects of validity, sparsity, proximity, and realism.

</details>


### [159] [MCGA: Mixture of Codebooks Hyperspectral Reconstruction via Grayscale-Aware Attention](https://arxiv.org/abs/2507.09885)
*Zhanjiang Yang,Lijun Sun,Jiawei Dong,Xiaoxin An,Yang Liu,Meng Li*

Main category: cs.CV

TL;DR: MCGA is a two-stage method for reconstructing hyperspectral images (HSI) from RGB images, using a multi-scale VQ-VAE and attention mechanisms, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods neglect the challenge of transitioning from low-dimensional (RGB) to high-dimensional (HSI) data, leading to suboptimal results.

Method: MCGA first learns spectral patterns via a multi-scale VQ-VAE (Mixture of Codebooks), then refines RGB-to-HSI mapping using Grayscale-Aware Attention and Quantized Self-Attention.

Result: MCGA outperforms existing methods, demonstrating superior HSI reconstruction quality.

Conclusion: The proposed two-stage approach with attention mechanisms and test-time adaptation offers efficient and robust HSI recovery.

Abstract: Reconstructing hyperspectral images (HSI) from RGB images is a cost-effective
solution for various vision-based applications. However, most existing
learning-based hyperspectral reconstruction methods directly learn the
RGB-to-HSI mapping using complex attention mechanisms, neglecting the inherent
challenge of transitioning from low-dimensional to high-dimensional
information. To address this limitation, we propose a two-stage approach, MCGA,
which first learns spectral patterns before estimating the mapping. In the
first stage, a multi-scale VQ-VAE learns representations from heterogeneous HSI
datasets, extracting a Mixture of Codebooks (MoC). In the second stage, the
RGB-to-HSI mapping is refined by querying features from the MoC to replace
latent HSI representations, incorporating prior knowledge rather than forcing a
direct high-dimensional transformation. To further enhance reconstruction
quality, we introduce Grayscale-Aware Attention and Quantized Self-Attention,
which adaptively adjust feature map intensities to meet hyperspectral
reconstruction requirements. This physically motivated attention mechanism
ensures lightweight and efficient HSI recovery. Moreover, we propose an
entropy-based Test-Time Adaptation strategy to improve robustness in real-world
scenarios. Extensive experiments demonstrate that our method, MCGA, achieves
state-of-the-art performance. The code and models will be released at
https://github.com/Fibonaccirabbit/MCGA

</details>


### [160] [EmbRACE-3K: Embodied Reasoning and Action in Complex Environments](https://arxiv.org/abs/2507.10548)
*Mingxian Lin,Wei Huang,Yitang Li,Chengjie Jiang,Kui Wu,Fangwei Zhong,Shengju Qian,Xin Wang,Xiaojuan Qi*

Main category: cs.CV

TL;DR: The paper introduces EmRACE-3K, a dataset for evaluating vision-language models (VLMs) in embodied settings, highlighting their limitations in interactive tasks and proposing a fine-tuning approach to improve performance.


<details>
  <summary>Details</summary>
Motivation: Current VLMs excel in passive tasks but struggle in embodied, interactive environments requiring active scene understanding and spatial reasoning.

Method: The authors create EmRACE-3K, a dataset of 3,000 tasks in photorealistic environments, and fine-tune Qwen2.5-VL-7B using supervised and reinforcement learning.

Result: Zero-shot VLMs perform poorly (<20% success), but fine-tuning improves performance across exploration, reasoning, and goal execution tasks.

Conclusion: EmRACE-3K addresses a critical gap in VLM evaluation for embodied reasoning, demonstrating the potential for improvement through targeted training.

Abstract: Recent advanced vision-language models(VLMs) have demonstrated strong
performance on passive, offline image and video understanding tasks. However,
their effectiveness in embodied settings, which require online interaction and
active scene understanding remains limited. In such scenarios, an agent
perceives the environment from a first-person perspective, with each action
dynamically shaping subsequent observations. Even state-of-the-art models such
as GPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro struggle in open-environment
interactions, exhibiting clear limitations in spatial reasoning and
long-horizon planning. To address this gap, we introduce EmRACE-3K, a dataset
of over 3,000 language-guided tasks situated in diverse, photorealistic
environments constructed using Unreal Engine and the UnrealCV-Zoo framework.
The tasks encompass a wide range of embodied challenges, including navigation,
object manipulation, and multi-stage goal execution. Each task unfolds as a
multi-step trajectory, pairing first-person visual observations with high-level
instructions, grounded actions, and natural language rationales that express
the agent's intent at every step. Using EmRACE-3K, we establish a benchmark to
evaluate the embodied reasoning capabilities of VLMs across three key
dimensions: Exploration, Dynamic Spatial-Semantic Reasoning, and Multi-stage
Goal Execution. In zero-shot settings, all models achieve success rates below
20%, underscoring the challenge posed by our benchmark and the current
limitations of VLMs in interactive environments. To demonstrate the utility of
EmRACE-3K, we further fine-tune Qwen2.5-VL-7B using supervised learning
followed by reinforcement learning. This approach yields substantial
improvements across all three challenge categories, highlighting the dataset's
effectiveness in enabling the development of embodied reasoning capabilities.

</details>


### [161] [Measuring the Impact of Rotation Equivariance on Aerial Object Detection](https://arxiv.org/abs/2507.09896)
*Xiuyu Wu,Xinhao Wang,Xiubin Zhu,Lan Yang,Jiyuan Liu,Xingchen Hu*

Main category: cs.CV

TL;DR: The paper introduces MessDet, a strictly rotation-equivariant aerial object detector with a multi-branch head network, achieving state-of-the-art performance on aerial datasets with low parameter count.


<details>
  <summary>Details</summary>
Motivation: Rotation equivariance is crucial for aerial object detection due to arbitrary object orientations, but existing methods rely on approximations or data augmentation. The necessity of strict rotation equivariance is unclear.

Method: Implemented a strictly rotation-equivariant backbone and neck network, compared with approximate methods, and introduced a multi-branch head network to reduce parameters and improve accuracy.

Result: MessDet achieves state-of-the-art performance on DOTA-v1.0, DOTA-v1.5, and DIOR-R datasets with low parameter count.

Conclusion: Strict rotation equivariance and the multi-branch head design significantly enhance aerial object detection performance.

Abstract: Due to the arbitrary orientation of objects in aerial images, rotation
equivariance is a critical property for aerial object detectors. However,
recent studies on rotation-equivariant aerial object detection remain scarce.
Most detectors rely on data augmentation to enable models to learn
approximately rotation-equivariant features. A few detectors have constructed
rotation-equivariant networks, but due to the breaking of strict rotation
equivariance by typical downsampling processes, these networks only achieve
approximately rotation-equivariant backbones. Whether strict rotation
equivariance is necessary for aerial image object detection remains an open
question. In this paper, we implement a strictly rotation-equivariant backbone
and neck network with a more advanced network structure and compare it with
approximately rotation-equivariant networks to quantitatively measure the
impact of rotation equivariance on the performance of aerial image detectors.
Additionally, leveraging the inherently grouped nature of rotation-equivariant
features, we propose a multi-branch head network that reduces the parameter
count while improving detection accuracy. Based on the aforementioned
improvements, this study proposes the Multi-branch head rotation-equivariant
single-stage Detector (MessDet), which achieves state-of-the-art performance on
the challenging aerial image datasets DOTA-v1.0, DOTA-v1.5 and DIOR-R with an
exceptionally low parameter count.

</details>


### [162] [IGD: Instructional Graphic Design with Multimodal Layer Generation](https://arxiv.org/abs/2507.09910)
*Yadong Qu,Shancheng Fang,Yuxin Wang,Xiaorui Wang,Zhineng Chen,Hongtao Xie,Yongdong Zhang*

Main category: cs.CV

TL;DR: IGD is a method for automated graphic design using natural language instructions, combining parametric rendering and image asset generation for editable, scalable outputs.


<details>
  <summary>Details</summary>
Motivation: Current methods lack creativity, intelligence, and editable flexibility in graphic design, making it labor-intensive.

Method: IGD uses a design platform, standardized file format, MLLM for attribute prediction and layout, and a diffusion model for image generation.

Result: IGD achieves superior results, offering scalable and extensible solutions for complex graphic design tasks.

Conclusion: IGD provides a practical and innovative approach to automated graphic design.

Abstract: Graphic design visually conveys information and data by creating and
combining text, images and graphics. Two-stage methods that rely primarily on
layout generation lack creativity and intelligence, making graphic design still
labor-intensive. Existing diffusion-based methods generate non-editable graphic
design files at image level with poor legibility in visual text rendering,
which prevents them from achieving satisfactory and practical automated graphic
design. In this paper, we propose Instructional Graphic Designer (IGD) to
swiftly generate multimodal layers with editable flexibility with only natural
language instructions. IGD adopts a new paradigm that leverages parametric
rendering and image asset generation. First, we develop a design platform and
establish a standardized format for multi-scenario design files, thus laying
the foundation for scaling up data. Second, IGD utilizes the multimodal
understanding and reasoning capabilities of MLLM to accomplish attribute
prediction, sequencing and layout of layers. It also employs a diffusion model
to generate image content for assets. By enabling end-to-end training, IGD
architecturally supports scalability and extensibility in complex graphic
design tasks. The superior experimental results demonstrate that IGD offers a
new solution for graphic design.

</details>


### [163] [Crucial-Diff: A Unified Diffusion Model for Crucial Image and Annotation Synthesis in Data-scarce Scenarios](https://arxiv.org/abs/2507.09915)
*Siyue Yao,Mingjie Sun,Eng Gee Lim,Ran Yi,Baojiang Zhong,Moncef Gabbouj*

Main category: cs.CV

TL;DR: Crucial-Diff is a domain-agnostic framework that synthesizes high-quality training samples to address data scarcity, outperforming existing methods in detection and segmentation tasks.


<details>
  <summary>Details</summary>
Motivation: Data scarcity in fields like medical and autonomous driving causes overfitting and imbalance, limiting model performance. Existing synthetic data methods lack diversity and fail to target model weaknesses.

Method: Crucial-Diff integrates SAFE for unified feature extraction and WASM to generate hard-to-detect samples using downstream model feedback, creating diverse and crucial training data.

Result: Achieves 83.63% pixel-level AP and 78.12% F1-MAX on MVTec, and 81.64% mIoU and 87.69% mDice on polyp dataset.

Conclusion: Crucial-Diff effectively mitigates data scarcity by generating high-quality, diverse samples, improving downstream model performance.

Abstract: The scarcity of data in various scenarios, such as medical, industry and
autonomous driving, leads to model overfitting and dataset imbalance, thus
hindering effective detection and segmentation performance. Existing studies
employ the generative models to synthesize more training samples to mitigate
data scarcity. However, these synthetic samples are repetitive or simplistic
and fail to provide "crucial information" that targets the downstream model's
weaknesses. Additionally, these methods typically require separate training for
different objects, leading to computational inefficiencies. To address these
issues, we propose Crucial-Diff, a domain-agnostic framework designed to
synthesize crucial samples. Our method integrates two key modules. The Scene
Agnostic Feature Extractor (SAFE) utilizes a unified feature extractor to
capture target information. The Weakness Aware Sample Miner (WASM) generates
hard-to-detect samples using feedback from the detection results of downstream
model, which is then fused with the output of SAFE module. Together, our
Crucial-Diff framework generates diverse, high-quality training data, achieving
a pixel-level AP of 83.63% and an F1-MAX of 78.12% on MVTec. On polyp dataset,
Crucial-Diff reaches an mIoU of 81.64% and an mDice of 87.69%. Code will be
released after acceptance.

</details>


### [164] [Can GPT-4o mini and Gemini 2.0 Flash Predict Fine-Grained Fashion Product Attributes? A Zero-Shot Analysis](https://arxiv.org/abs/2507.09950)
*Shubham Shukla,Kunal Sonalkar*

Main category: cs.CV

TL;DR: The paper evaluates zero-shot performance of LLMs (GPT-4o-mini and Gemini 2.0 Flash) on fine-grained fashion attribute recognition using the DeepFashion-MultiModal dataset. Gemini 2.0 Flash outperforms GPT-4o-mini, highlighting the need for domain-specific fine-tuning in e-commerce applications.


<details>
  <summary>Details</summary>
Motivation: To explore the under-examined performance of LLMs in fine-grained fashion attribute recognition and their practical deployment in e-commerce product attribution.

Method: Zero-shot evaluation of GPT-4o-mini and Gemini 2.0 Flash on the DeepFashion-MultiModal dataset, using only images as input across 18 fashion attribute categories.

Result: Gemini 2.0 Flash achieved a macro F1 score of 56.79%, outperforming GPT-4o-mini (43.28%).

Conclusion: The study provides insights for deploying LLMs in e-commerce and emphasizes the need for domain-specific fine-tuning, paving the way for future research in fashion AI.

Abstract: The fashion retail business is centered around the capacity to comprehend
products. Product attribution helps in comprehending products depending on the
business process. Quality attribution improves the customer experience as they
navigate through millions of products offered by a retail website. It leads to
well-organized product catalogs. In the end, product attribution directly
impacts the 'discovery experience' of the customer. Although large language
models (LLMs) have shown remarkable capabilities in understanding multimodal
data, their performance on fine-grained fashion attribute recognition remains
under-explored. This paper presents a zero-shot evaluation of state-of-the-art
LLMs that balance performance with speed and cost efficiency, mainly
GPT-4o-mini and Gemini 2.0 Flash. We have used the dataset
DeepFashion-MultiModal (https://github.com/yumingj/DeepFashion-MultiModal) to
evaluate these models in the attribution tasks of fashion products. Our study
evaluates these models across 18 categories of fashion attributes, offering
insight into where these models excel. We only use images as the sole input for
product information to create a constrained environment. Our analysis shows
that Gemini 2.0 Flash demonstrates the strongest overall performance with a
macro F1 score of 56.79% across all attributes, while GPT-4o-mini scored a
macro F1 score of 43.28%. Through detailed error analysis, our findings provide
practical insights for deploying these LLMs in production e-commerce product
attribution-related tasks and highlight the need for domain-specific
fine-tuning approaches. This work also lays the groundwork for future research
in fashion AI and multimodal attribute extraction.

</details>


### [165] [4D-MISR: A unified model for low-dose super-resolution imaging via feature fusion](https://arxiv.org/abs/2507.09953)
*Zifei Wang,Zian Mao,Xiaoya He,Xi Huang,Haoran Zhang,Chun Cheng,Shufen Chu,Tingzheng Hou,Xiaoqin Zeng,Yujun Xie*

Main category: cs.CV

TL;DR: A method combining multi-image super-resolution (MISR) and a CNN achieves atomic-scale resolution in 4D-STEM for beam-sensitive materials, outperforming conventional electron microscopy under ultra-low-dose conditions.


<details>
  <summary>Details</summary>
Motivation: Radiation damage limits atomic-resolution imaging of beam-sensitive materials like proteins and 2D materials, necessitating a solution to overcome electron dose constraints.

Method: The approach fuses multiple low-resolution, sub-pixel-shifted views and enhances reconstruction with a CNN, using a dual-path, attention-guided network for 4D-STEM.

Result: Achieves atomic-scale super-resolution from ultra-low-dose data, comparable to conventional ptychography, and works across amorphous, semi-crystalline, and crystalline specimens.

Conclusion: The method expands 4D-STEM capabilities, providing a generalizable solution for structural analysis of radiation-sensitive materials.

Abstract: While electron microscopy offers crucial atomic-resolution insights into
structure-property relationships, radiation damage severely limits its use on
beam-sensitive materials like proteins and 2D materials. To overcome this
challenge, we push beyond the electron dose limits of conventional electron
microscopy by adapting principles from multi-image super-resolution (MISR) that
have been widely used in remote sensing. Our method fuses multiple
low-resolution, sub-pixel-shifted views and enhances the reconstruction with a
convolutional neural network (CNN) that integrates features from synthetic,
multi-angle observations. We developed a dual-path, attention-guided network
for 4D-STEM that achieves atomic-scale super-resolution from ultra-low-dose
data. This provides robust atomic-scale visualization across amorphous,
semi-crystalline, and crystalline beam-sensitive specimens. Systematic
evaluations on representative materials demonstrate comparable spatial
resolution to conventional ptychography under ultra-low-dose conditions. Our
work expands the capabilities of 4D-STEM, offering a new and generalizable
method for the structural analysis of radiation-vulnerable materials.

</details>


### [166] [Uncertainty Quantification for Incomplete Multi-View Data Using Divergence Measures](https://arxiv.org/abs/2507.09980)
*Zhipeng Xue,Yan Zhang,Ming Li,Chun Li,Yue Liu,Fei Yu*

Main category: cs.CV

TL;DR: KPHD-Net introduces Hlder divergence and Dempster-Shafer evidence theory for improved uncertainty estimation and fusion in multi-view learning, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current methods for multi-view learning rely on KL divergence, which ignores domain gaps between modalities, leading to unreliable uncertainty estimation.

Method: KPHD-Net uses variational Dirichlet distributions for class probabilities, models evidences, and integrates Hlder divergence with Dempster-Shafer theory for better fusion.

Result: KPHD-Net achieves superior accuracy, robustness, and reliability in classification and clustering tasks compared to state-of-the-art methods.

Conclusion: The proposed method, combining Hlder divergence and Dempster-Shafer theory, effectively addresses uncertainty estimation and fusion challenges in multi-view learning.

Abstract: Existing multi-view classification and clustering methods typically improve
task accuracy by leveraging and fusing information from different views.
However, ensuring the reliability of multi-view integration and final decisions
is crucial, particularly when dealing with noisy or corrupted data. Current
methods often rely on Kullback-Leibler (KL) divergence to estimate uncertainty
of network predictions, ignoring domain gaps between different modalities. To
address this issue, KPHD-Net, based on H\"older divergence, is proposed for
multi-view classification and clustering tasks. Generally, our KPHD-Net employs
a variational Dirichlet distribution to represent class probability
distributions, models evidences from different views, and then integrates it
with Dempster-Shafer evidence theory (DST) to improve uncertainty estimation
effects. Our theoretical analysis demonstrates that Proper H\"older divergence
offers a more effective measure of distribution discrepancies, ensuring
enhanced performance in multi-view learning. Moreover, Dempster-Shafer evidence
theory, recognized for its superior performance in multi-view fusion tasks, is
introduced and combined with the Kalman filter to provide future state
estimations. This integration further enhances the reliability of the final
fusion results. Extensive experiments show that the proposed KPHD-Net
outperforms the current state-of-the-art methods in both classification and
clustering tasks regarding accuracy, robustness, and reliability, with
theoretical guarantees.

</details>


### [167] [Latent Diffusion Models with Masked AutoEncoders](https://arxiv.org/abs/2507.09984)
*Junho Lee,Jeongwoo Shin,Hyungwook Choi,Joonseok Lee*

Main category: cs.CV

TL;DR: The paper analyzes autoencoders in Latent Diffusion Models (LDMs), identifies key properties, and proposes Variational Masked AutoEncoders (VMAEs) to improve image generation.


<details>
  <summary>Details</summary>
Motivation: Existing autoencoders in LDMs lack simultaneous satisfaction of latent smoothness, perceptual compression, and reconstruction quality.

Method: Proposes VMAEs, leveraging hierarchical features from Masked AutoEncoder, and integrates them into LDMs as LDMAEs.

Result: LDMAEs show improved image generation quality and computational efficiency.

Conclusion: VMAEs enhance LDMs by addressing autoencoder limitations, leading to better performance.

Abstract: In spite of remarkable potential of the Latent Diffusion Models (LDMs) in
image generation, the desired properties and optimal design of the autoencoders
have been underexplored. In this work, we analyze the role of autoencoders in
LDMs and identify three key properties: latent smoothness, perceptual
compression quality, and reconstruction quality. We demonstrate that existing
autoencoders fail to simultaneously satisfy all three properties, and propose
Variational Masked AutoEncoders (VMAEs), taking advantage of the hierarchical
features maintained by Masked AutoEncoder. We integrate VMAEs into the LDM
framework, introducing Latent Diffusion Models with Masked AutoEncoders
(LDMAEs). Through comprehensive experiments, we demonstrate significantly
enhanced image generation quality and computational efficiency.

</details>


### [168] [3DGAA: Realistic and Robust 3D Gaussian-based Adversarial Attack for Autonomous Driving](https://arxiv.org/abs/2507.09993)
*Yixun Zhang,Lizhi Wang,Junjun Zhao,Wending Zhao,Feng Zhou,Yonghao Dang,Jianqin Yin*

Main category: cs.CV

TL;DR: 3DGAA is a novel adversarial attack framework using 3D Gaussian Splatting to optimize geometry and appearance for robust, physically realistic attacks on autonomous driving perception systems.


<details>
  <summary>Details</summary>
Motivation: Existing 2D/3D physical attacks lack balance between realism and robustness, limiting their effectiveness in real-world scenarios.

Method: 3DGAA leverages 3DGS's 14D parameterization to perturb geometry (shape, scale, rotation) and appearance (color, opacity), with physical filtering and augmentation modules for fidelity and generalization.

Result: 3DGAA reduces detection mAP from 87.21% to 7.38%, outperforming existing methods and maintaining high transferability.

Conclusion: 3DGAA sets a new benchmark for physically realizable adversarial attacks, proving effective for evaluating autonomous driving safety.

Abstract: Camera-based object detection systems play a vital role in autonomous
driving, yet they remain vulnerable to adversarial threats in real-world
environments. While existing 2D and 3D physical attacks typically optimize
texture, they often struggle to balance physical realism and attack robustness.
In this work, we propose 3D Gaussian-based Adversarial Attack (3DGAA), a novel
adversarial object generation framework that leverages the full 14-dimensional
parameterization of 3D Gaussian Splatting (3DGS) to jointly optimize geometry
and appearance in physically realizable ways. Unlike prior works that rely on
patches or texture, 3DGAA jointly perturbs both geometric attributes (shape,
scale, rotation) and appearance attributes (color, opacity) to produce
physically realistic and transferable adversarial objects. We further introduce
a physical filtering module to preserve geometric fidelity, and a physical
augmentation module to simulate complex physical scenarios, thus enhancing
attack generalization under real-world conditions. We evaluate 3DGAA on both
virtual benchmarks and physical-world setups using miniature vehicle models.
Experimental results show that 3DGAA achieves to reduce the detection mAP from
87.21% to 7.38%, significantly outperforming existing 3D physical attacks.
Moreover, our method maintains high transferability across different physical
conditions, demonstrating a new state-of-the-art in physically realizable
adversarial attacks. These results validate 3DGAA as a practical attack
framework for evaluating the safety of perception systems in autonomous
driving.

</details>


### [169] [Leveraging Swin Transformer for enhanced diagnosis of Alzheimer's disease using multi-shell diffusion MRI](https://arxiv.org/abs/2507.09996)
*Quentin Dessain,Nicolas Delinte,Bernard Hanseeuw,Laurence Dricot,Benot Macq*

Main category: cs.CV

TL;DR: A vision transformer-based deep learning framework using multi-shell dMRI data achieves high accuracy for Alzheimer's disease and amyloid detection, with explainability highlighting key brain regions.


<details>
  <summary>Details</summary>
Motivation: To support early diagnosis of Alzheimer's disease and amyloid accumulation using microstructural information from multi-shell dMRI data.

Method: A Swin Transformer-based pipeline processes multi-shell dMRI data, leveraging DTI and NODDI metrics for transfer learning and Low-Rank Adaptation for limited labeled data.

Result: Achieved 95.2% balanced accuracy for Alzheimer's disease classification and up to 77.2% for amyloid detection, with explainability identifying relevant brain regions.

Conclusion: The framework shows promise for early Alzheimer's and amyloid detection using diffusion MRI and transformers, aiding biomarker-driven diagnostics.

Abstract: Objective: This study aims to support early diagnosis of Alzheimer's disease
and detection of amyloid accumulation by leveraging the microstructural
information available in multi-shell diffusion MRI (dMRI) data, using a vision
transformer-based deep learning framework.
  Methods: We present a classification pipeline that employs the Swin
Transformer, a hierarchical vision transformer model, on multi-shell dMRI data
for the classification of Alzheimer's disease and amyloid presence. Key metrics
from DTI and NODDI were extracted and projected onto 2D planes to enable
transfer learning with ImageNet-pretrained models. To efficiently adapt the
transformer to limited labeled neuroimaging data, we integrated Low-Rank
Adaptation. We assessed the framework on diagnostic group prediction
(cognitively normal, mild cognitive impairment, Alzheimer's disease dementia)
and amyloid status classification.
  Results: The framework achieved competitive classification results within the
scope of multi-shell dMRI-based features, with the best balanced accuracy of
95.2% for distinguishing cognitively normal individuals from those with
Alzheimer's disease dementia using NODDI metrics. For amyloid detection, it
reached 77.2% balanced accuracy in distinguishing amyloid-positive mild
cognitive impairment/Alzheimer's disease dementia subjects from
amyloid-negative cognitively normal subjects, and 67.9% for identifying
amyloid-positive individuals among cognitively normal subjects. Grad-CAM-based
explainability analysis identified clinically relevant brain regions, including
the parahippocampal gyrus and hippocampus, as key contributors to model
predictions.
  Conclusion: This study demonstrates the promise of diffusion MRI and
transformer-based architectures for early detection of Alzheimer's disease and
amyloid pathology, supporting biomarker-driven diagnostics in data-limited
biomedical settings.

</details>


### [170] [Vision-Based Anti Unmanned Aerial Technology: Opportunities and Challenges](https://arxiv.org/abs/2507.10006)
*Guanghai Ding,Yihua Ren,Yuting Liu,Qijun Zhao,Shuiwang Li*

Main category: cs.CV

TL;DR: The paper reviews Anti-UAV tracking technologies, discusses challenges, compiles datasets, analyzes vision-based algorithms, and suggests future research directions.


<details>
  <summary>Details</summary>
Motivation: The rapid advancement of UAV technology necessitates efficient Anti-UAV tracking for applications like public safety and environmental monitoring.

Method: The paper reviews current technologies, compiles datasets, and analyzes vision-based and vision-fusion-based algorithms.

Result: It provides insights into existing methods and challenges, along with accessible datasets for researchers.

Conclusion: Future research directions are outlined to advance Anti-UAV tracking technologies.

Abstract: With the rapid advancement of UAV technology and its extensive application in
various fields such as military reconnaissance, environmental monitoring, and
logistics, achieving efficient and accurate Anti-UAV tracking has become
essential. The importance of Anti-UAV tracking is increasingly prominent,
especially in scenarios such as public safety, border patrol, search and
rescue, and agricultural monitoring, where operations in complex environments
can provide enhanced security. Current mainstream Anti-UAV tracking
technologies are primarily centered around computer vision techniques,
particularly those that integrate multi-sensor data fusion with advanced
detection and tracking algorithms. This paper first reviews the characteristics
and current challenges of Anti-UAV detection and tracking technologies. Next,
it investigates and compiles several publicly available datasets, providing
accessible links to support researchers in efficiently addressing related
challenges. Furthermore, the paper analyzes the major vision-based and
vision-fusion-based Anti-UAV detection and tracking algorithms proposed in
recent years. Finally, based on the above research, this paper outlines future
research directions, aiming to provide valuable insights for advancing the
field.

</details>


### [171] [Binomial Self-Compensation: Mechanism and Suppression of Motion Error in Phase-Shifting Profilometry](https://arxiv.org/abs/2507.10009)
*Geyou Zhang,Kai Liu,Ce Zhu*

Main category: cs.CV

TL;DR: The paper introduces I-BSC, an improved method over P-BSC, to reduce motion errors in dynamic 3D scanning by weighting fringe images instead of phase frames, achieving faster computation and better error convergence.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitation of PSP in dynamic measurements due to object motion, which causes errors in phase-shifting profilometry.

Method: Proposes I-BSC, which weights homogeneous fringe images (instead of phase frames) and computes the arctangent function once, reducing computational overhead and error accumulation.

Result: I-BSC outperforms existing methods in reducing motion error, achieves quasi-single-shot frame rates, and accelerates computational frame rates significantly compared to P-BSC.

Conclusion: I-BSC generalizes the BSC concept to image sequences, offering a more efficient and accurate solution for dynamic 3D scanning.

Abstract: Phase shifting profilometry (PSP) is widely used in high-precision 3D
scanning due to its high accuracy, robustness, and pixel-wise handling.
However, a fundamental assumption of PSP that the object should remain static
does not hold in dynamic measurement, making PSP susceptible to object motion.
To address this challenge, our proposed solution, phase-sequential binomial
self-compensation (P-BSC), sums successive motion-affected phase frames
weighted by binomial coefficients. This approach exponentially reduces the
motion error in a pixel-wise and frame-wise loopable manner. Despite its
efficacy, P-BSC suffers from high computational overhead and error accumulation
due to its reliance on multi-frame phase calculations and weighted summations.
Inspired by P-BSC, we propose an image-sequential binomial self-compensation
(I-BSC) to weight sum the homogeneous fringe images instead of successive phase
frames, which generalizes the BSC concept from phase sequences to image
sequences. I-BSC computes the arctangent function only once, resolving both
limitations in P-BSC. Extensive analysis, simulations, and experiments show
that 1) the proposed BSC outperforms existing methods in reducing motion error
while achieving a quasi-single-shot frame rate, i.e., depth map frame rate
equals to the camera's acquisition rate, enabling 3D reconstruction with high
pixel-depth-temporal resolution; 2) compared to P-BSC, our I-BSC reduces the
computational complexity by one polynomial order, thereby accelerating the
computational frame rate by several to dozen times, while also reaching faster
motion error convergence.

</details>


### [172] [(Almost) Free Modality Stitching of Foundation Models](https://arxiv.org/abs/2507.10015)
*Jaisidh Singh,Diganta Misra,Boris Knyazev,Antonio Orvieto*

Main category: cs.CV

TL;DR: Hyma proposes a hypernetwork-based method to efficiently select and align uni-modal models for multi-modal tasks, reducing search costs by 10x while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Current methods for aligning uni-modal models in multi-modal tasks are computationally expensive due to the need for extensive connector training and model selection.

Method: Hyma leverages hypernetworks to predict connector modules for all possible uni-modal model pairs, enabling efficient joint training and selection.

Result: The framework reduces search costs by 10x and matches the performance of traditional grid search methods across diverse benchmarks.

Conclusion: Hyma offers a scalable and efficient solution for multi-modal model alignment, addressing computational challenges in model selection and connector training.

Abstract: Foundation multi-modal models are often designed by stitching of multiple
existing pretrained uni-modal models: for example, an image classifier with an
autoregressive text model. This stitching process is performed by training a
connector module that aims to align the representation-representation or
representation-input spaces of these uni-modal models. However, given the
complexity of training such connectors on large scale web-based datasets
coupled with the ever-increasing number of available pretrained uni-modal
models, the task of uni-modal models selection and subsequent connector module
training becomes computationally demanding. To address this under-studied
critical problem, we propose Hypernetwork Model Alignment (Hyma), a novel
all-in-one solution for optimal uni-modal model selection and connector
training by leveraging hypernetworks. Specifically, our framework utilizes the
parameter prediction capability of a hypernetwork to obtain jointly trained
connector modules for $N \times M$ combinations of uni-modal models. In our
experiments, Hyma reduces the optimal uni-modal model pair search cost by
$10\times$ (averaged across all experiments), while matching the ranking and
trained connector performance obtained via grid search across a suite of
diverse multi-modal benchmarks.

</details>


### [173] [Memory-Efficient Personalization of Text-to-Image Diffusion Models via Selective Optimization Strategies](https://arxiv.org/abs/2507.10029)
*Seokeon Choi,Sunghyun Park,Hyoungwoo Park,Jeongho Kim,Sungrack Yun*

Main category: cs.CV

TL;DR: A selective optimization framework for memory-efficient personalization of text-to-image diffusion models, combining BP-low and ZO-high methods for high-quality fine-tuning with reduced memory usage.


<details>
  <summary>Details</summary>
Motivation: To adapt diffusion models for personalization while preserving privacy and operating within edge device constraints.

Method: Uses BP-low for target-specific adaptation and ZO-high for high-resolution refinement, dynamically selected via a timestep-aware probabilistic function.

Result: Achieves competitive performance with significant memory reduction, enabling scalable on-device personalization.

Conclusion: The framework effectively balances efficiency and quality, making it suitable for edge devices.

Abstract: Memory-efficient personalization is critical for adapting text-to-image
diffusion models while preserving user privacy and operating within the limited
computational resources of edge devices. To this end, we propose a selective
optimization framework that adaptively chooses between backpropagation on
low-resolution images (BP-low) and zeroth-order optimization on high-resolution
images (ZO-high), guided by the characteristics of the diffusion process. As
observed in our experiments, BP-low efficiently adapts the model to
target-specific features, but suffers from structural distortions due to
resolution mismatch. Conversely, ZO-high refines high-resolution details with
minimal memory overhead but faces slow convergence when applied without prior
adaptation. By complementing both methods, our framework leverages BP-low for
effective personalization while using ZO-high to maintain structural
consistency, achieving memory-efficient and high-quality fine-tuning. To
maximize the efficacy of both BP-low and ZO-high, we introduce a timestep-aware
probabilistic function that dynamically selects the appropriate optimization
strategy based on diffusion timesteps. This function mitigates the overfitting
from BP-low at high timesteps, where structural information is critical, while
ensuring ZO-high is applied more effectively as training progresses.
Experimental results demonstrate that our method achieves competitive
performance while significantly reducing memory consumption, enabling scalable,
high-quality on-device personalization without increasing inference latency.

</details>


### [174] [LifelongPR: Lifelong knowledge fusion for point cloud place recognition based on replay and prompt learning](https://arxiv.org/abs/2507.10034)
*Xianghong Zou,Jianping Li,Zhe Chen,Zhen Cao,Zhen Dong,Qiegen Liu,Bisheng Yang*

Main category: cs.CV

TL;DR: The paper proposes LifelongPR, a continual learning framework for point cloud place recognition (PCPR), addressing catastrophic forgetting and domain shifts with dynamic replay sample selection and prompt learning.


<details>
  <summary>Details</summary>
Motivation: Existing PCPR models suffer from catastrophic forgetting and poor scalability when adapting to new environments or sensor types, limiting their practicality.

Method: LifelongPR uses a replay sample selection method for knowledge retention and a prompt learning-based framework with a two-stage training strategy for domain adaptation.

Result: The method achieves 6.50% improvement in mIR@1, 7.96% in mR@1, and an 8.95% reduction in F compared to SOTA methods.

Conclusion: LifelongPR effectively addresses continual learning challenges in PCPR, enhancing performance and scalability for real-world applications.

Abstract: Point cloud place recognition (PCPR) plays a crucial role in photogrammetry
and robotics applications such as autonomous driving, intelligent
transportation, and augmented reality. In real-world large-scale deployments of
a positioning system, PCPR models must continuously acquire, update, and
accumulate knowledge to adapt to diverse and dynamic environments, i.e., the
ability known as continual learning (CL). However, existing PCPR models often
suffer from catastrophic forgetting, leading to significant performance
degradation in previously learned scenes when adapting to new environments or
sensor types. This results in poor model scalability, increased maintenance
costs, and system deployment difficulties, undermining the practicality of
PCPR. To address these issues, we propose LifelongPR, a novel continual
learning framework for PCPR, which effectively extracts and fuses knowledge
from sequential point cloud data. First, to alleviate the knowledge loss, we
propose a replay sample selection method that dynamically allocates sample
sizes according to each dataset's information quantity and selects spatially
diverse samples for maximal representativeness. Second, to handle domain
shifts, we design a prompt learning-based CL framework with a lightweight
prompt module and a two-stage training strategy, enabling domain-specific
feature adaptation while minimizing forgetting. Comprehensive experiments on
large-scale public and self-collected datasets are conducted to validate the
effectiveness of the proposed method. Compared with state-of-the-art (SOTA)
methods, our method achieves 6.50% improvement in mIR@1, 7.96% improvement in
mR@1, and an 8.95% reduction in F. The code and pre-trained models are publicly
available at https://github.com/zouxianghong/LifelongPR.

</details>


### [175] [CoSMo: A Multimodal Transformer for Page Stream Segmentation in Comic Books](https://arxiv.org/abs/2507.10053)
*Marc Serra Ortega,Emanuele Vivoli,Artemis Llabrs,Dimosthenis Karatzas*

Main category: cs.CV

TL;DR: CoSMo, a multimodal Transformer, excels in Page Stream Segmentation (PSS) for comic books, outperforming baselines and larger models, with visual features dominating macro-structure and multimodal inputs aiding ambiguity resolution.


<details>
  <summary>Details</summary>
Motivation: Automated content understanding in comic books requires accurate PSS for tasks like character analysis and metadata enrichment, but existing methods lack specialization for this medium.

Method: Developed vision-only and multimodal variants of CoSMo, trained on a new 20,800-page annotated dataset, and evaluated against baselines using F1-Macro, Panoptic Quality, and stream-level metrics.

Result: CoSMo outperforms traditional baselines and larger general-purpose models, with visual features crucial for macro-structure and multimodal inputs improving ambiguity resolution.

Conclusion: CoSMo sets a new state-of-the-art for comic PSS, enabling scalable analysis and highlighting the balance between visual and multimodal features.

Abstract: This paper introduces CoSMo, a novel multimodal Transformer for Page Stream
Segmentation (PSS) in comic books, a critical task for automated content
understanding, as it is a necessary first stage for many downstream tasks like
character analysis, story indexing, or metadata enrichment. We formalize PSS
for this unique medium and curate a new 20,800-page annotated dataset. CoSMo,
developed in vision-only and multimodal variants, consistently outperforms
traditional baselines and significantly larger general-purpose vision-language
models across F1-Macro, Panoptic Quality, and stream-level metrics. Our
findings highlight the dominance of visual features for comic PSS
macro-structure, yet demonstrate multimodal benefits in resolving challenging
ambiguities. CoSMo establishes a new state-of-the-art, paving the way for
scalable comic book analysis.

</details>


### [176] [Lightweight Model for Poultry Disease Detection from Fecal Images Using Multi-Color Space Feature Optimization and Machine Learning](https://arxiv.org/abs/2507.10056)
*A. K. M. Shoriful Islam,Md. Rakib Hassan,Macbah Uddin,Md. Shahidur Rahman*

Main category: cs.CV

TL;DR: A lightweight ML approach for poultry disease detection using fecal images achieves high accuracy with low resource usage.


<details>
  <summary>Details</summary>
Motivation: Poultry farming is vulnerable to diseases; a cost-effective, scalable solution is needed for low-resource settings.

Method: Multi-color space feature extraction (RGB, HSV, LAB) with descriptors like histograms, LBP, wavelet transforms, and edge detectors. Dimensionality reduction via PCA and XGBoost, followed by ANN classification.

Result: 95.85% accuracy, no GPU needed, 638 seconds execution time in Colab. Comparable to deep learning models but more efficient.

Conclusion: The method is a cost-effective, interpretable alternative to deep learning for real-time disease detection in poultry farming.

Abstract: Poultry farming is a vital component of the global food supply chain, yet it
remains highly vulnerable to infectious diseases such as coccidiosis,
salmonellosis, and Newcastle disease. This study proposes a lightweight machine
learning-based approach to detect these diseases by analyzing poultry fecal
images. We utilize multi-color space feature extraction (RGB, HSV, LAB) and
explore a wide range of color, texture, and shape-based descriptors, including
color histograms, local binary patterns (LBP), wavelet transforms, and edge
detectors. Through a systematic ablation study and dimensionality reduction
using PCA and XGBoost feature selection, we identify a compact global feature
set that balances accuracy and computational efficiency. An artificial neural
network (ANN) classifier trained on these features achieved 95.85% accuracy
while requiring no GPU and only 638 seconds of execution time in Google Colab.
Compared to deep learning models such as Xception and MobileNetV3, our proposed
model offers comparable accuracy with drastically lower resource usage. This
work demonstrates a cost-effective, interpretable, and scalable alternative to
deep learning for real-time poultry disease detection in low-resource
agricultural settings.

</details>


### [177] [MoVieS: Motion-Aware 4D Dynamic View Synthesis in One Second](https://arxiv.org/abs/2507.10065)
*Chenguo Lin,Yuchen Lin,Panwang Pan,Yifan Yu,Honglei Yan,Katerina Fragkiadaki,Yadong Mu*

Main category: cs.CV

TL;DR: MoVieS is a fast feed-forward model for 4D dynamic novel view synthesis from monocular videos, unifying appearance, geometry, and motion in a single framework.


<details>
  <summary>Details</summary>
Motivation: To enable unified modeling of dynamic 3D scenes for tasks like view synthesis, reconstruction, and 3D point tracking with minimal supervision.

Method: Uses pixel-aligned grids of Gaussian primitives with explicit motion supervision, allowing large-scale training on diverse datasets.

Result: Achieves competitive performance with significant speedups, supporting zero-shot applications like scene flow estimation.

Conclusion: MoVieS is effective and efficient, bridging novel view synthesis with dynamic geometry reconstruction.

Abstract: We present MoVieS, a novel feed-forward model that synthesizes 4D dynamic
novel views from monocular videos in one second. MoVieS represents dynamic 3D
scenes using pixel-aligned grids of Gaussian primitives, explicitly supervising
their time-varying motion. This allows, for the first time, the unified
modeling of appearance, geometry and motion, and enables view synthesis,
reconstruction and 3D point tracking within a single learning-based framework.
By bridging novel view synthesis with dynamic geometry reconstruction, MoVieS
enables large-scale training on diverse datasets with minimal dependence on
task-specific supervision. As a result, it also naturally supports a wide range
of zero-shot applications, such as scene flow estimation and moving object
segmentation. Extensive experiments validate the effectiveness and efficiency
of MoVieS across multiple tasks, achieving competitive performance while
offering several orders of magnitude speedups.

</details>


### [178] [Frequency Regulation for Exposure Bias Mitigation in Diffusion Models](https://arxiv.org/abs/2507.10072)
*Meng Yu,Kun Zhan*

Main category: cs.CV

TL;DR: The paper addresses exposure bias in diffusion models by analyzing energy reduction in noisy images during diffusion, introducing a frequency-domain regulation mechanism, and improving generative quality without additional training.


<details>
  <summary>Details</summary>
Motivation: Diffusion models suffer from exposure bias, impacting their generative capabilities. The paper aims to mitigate this by analyzing energy patterns in noisy images.

Method: The authors use wavelet transforms to regulate low- and high-frequency subbands separately, based on observed energy reduction patterns. Their method is training-free and plug-and-play.

Result: The approach significantly improves generative quality across various diffusion models and provides a robust solution to exposure bias.

Conclusion: The proposed frequency-domain regulation effectively addresses exposure bias, enhancing diffusion model performance without requiring retraining.

Abstract: Diffusion models exhibit impressive generative capabilities but are
significantly impacted by exposure bias. In this paper, we make a key
observation: the energy of the predicted noisy images decreases during the
diffusion process. Building on this, we identify two important findings: 1) The
reduction in energy follows distinct patterns in the low-frequency and
high-frequency subbands; 2) This energy reduction results in amplitude
variations between the network-reconstructed clean data and the real clean
data. Based on the first finding, we introduce a frequency-domain regulation
mechanism utilizing wavelet transforms, which separately adjusts the low- and
high-frequency subbands. Leveraging the second insight, we provide a more
accurate analysis of exposure bias in the two subbands. Our method is
training-free and plug-and-play, significantly improving the generative quality
of various diffusion models and providing a robust solution to exposure bias
across different model architectures. The source code is available at
https://github.com/kunzhan/wpp.

</details>


### [179] [A Transfer Learning-Based Method for Water Body Segmentation in Remote Sensing Imagery: A Case Study of the Zhada Tulin Area](https://arxiv.org/abs/2507.10084)
*Haonan Chen,Xin Tong*

Main category: cs.CV

TL;DR: A two-stage transfer learning strategy using SegFormer improves water body segmentation in remote sensing, boosting IoU from 25.50% to 64.84% in challenging Tibetan terrain.


<details>
  <summary>Details</summary>
Motivation: Address domain shift and small sample sizes in remote sensing water body segmentation, especially in unique environments like Tibet.

Method: Train a foundational model on a diverse source domain, then fine-tune on target domain data (Zhada Tulin area).

Result: Significant IoU improvement from 25.50% (direct transfer) to 64.84%.

Conclusion: The strategy resolves domain discrepancy issues and offers a solution for high-precision thematic extraction in data-scarce, unique environments.

Abstract: To address the prevalent challenges of domain shift and small sample sizes in
remote sensing image water body segmentation, this study proposes and validates
a two-stage transfer learning strategy based on the SegFormer model. The
approach begins by training a foundational segmentation model on a diverse
source domain, where it achieves an Intersection over Union (IoU) of 68.80% on
its validation set, followed by fine-tuning on data from the distinct target
domain. Focusing on the Zhada Tulin area in Tibet -- a region characterized by
highly complex topography and spectral features -- the experimental results
demonstrate that this strategy significantly boosts the IoU for the water body
segmentation task from 25.50% (for direct transfer) to 64.84%. This not only
effectively resolves the model performance degradation caused by domain
discrepancy but also provides an effective technical paradigm for
high-precision thematic information extraction in data-scarce and
environmentally unique remote sensing scenarios.

</details>


### [180] [FIX-CLIP: Dual-Branch Hierarchical Contrastive Learning via Synthetic Captions for Better Understanding of Long Text](https://arxiv.org/abs/2507.10095)
*Bingchao Wang,Zhiwei Ning,Jianyu Ding,Xuanang Gao,Yin Li,Dongsheng Jiang,Jie Yang,Wei Liu*

Main category: cs.CV

TL;DR: FIX-CLIP enhances CLIP for long-text tasks by introducing dual-branch training, regional prompts, and hierarchical feature alignment, achieving SOTA performance.


<details>
  <summary>Details</summary>
Motivation: CLIP struggles with long-text inputs (>77 tokens), limiting its applicability. FIX-CLIP aims to improve long-text handling while preserving short-text performance.

Method: Proposes three modules: (1) dual-branch training for short/long text alignment, (2) regional prompts for information extraction, (3) hierarchical feature alignment for multi-scale consistency. Uses 30M images and synthetic captions for training.

Result: FIX-CLIP achieves state-of-the-art performance on long and short-text retrieval benchmarks and works well with diffusion models.

Conclusion: FIX-CLIP effectively addresses CLIP's long-text limitations and demonstrates strong performance in downstream tasks.

Abstract: CLIP has shown promising performance across many short-text tasks in a
zero-shot manner. However, limited by the input length of the text encoder,
CLIP struggles on under-stream tasks with long-text inputs (>77 tokens). To
remedy this issue, we propose FIX-CLIP which includes three novel modules: (1)
A dual-branch training pipeline that aligns short and long texts with masked
and raw images respectively, which boosts the long-text representation while
preserving the short-text ability. (2) Multiple learnable regional prompts with
unidirectional masks in Transformer layers for regional information extraction.
(3) A hierarchical feature alignment module in the intermediate encoder layers
to promote the consistency of multi-scale features. Furthermore, we collect 30M
images and utilize existing MLLMs to synthesize long-text captions for
training. Extensive experiments show that FIX-CLIP achieves state-of-the-art
performance on both long-text and short-text retrieval benchmarks. For
downstream applications, we reveal that FIX-CLIP's text encoder delivers
promising performance in a plug-and-play manner for diffusion models with
long-text input.

</details>


### [181] [Glance-MCMT: A General MCMT Framework with Glance Initialization and Progressive Association](https://arxiv.org/abs/2507.10115)
*Hamidreza Hashempoor*

Main category: cs.CV

TL;DR: A framework for multi-camera multi-target tracking using trajectory and appearance cues, with global ID assignment and 3D spatial validation.


<details>
  <summary>Details</summary>
Motivation: To ensure consistent global identity assignment across multiple camera views for accurate multi-target tracking.

Method: Uses BoT-SORT for single-camera tracking, initializes global IDs via trajectory-feature matching, and employs a prioritized global matching strategy for new tracklets. 3D positions are estimated for spatial validation.

Result: Achieves consistent global identity assignment by combining trajectory and appearance cues, with new IDs introduced only when no match is found.

Conclusion: The proposed MCMT framework effectively maintains global identity consistency across views, enhancing multi-target tracking accuracy.

Abstract: We propose a multi-camera multi-target (MCMT) tracking framework that ensures
consistent global identity assignment across views using trajectory and
appearance cues. The pipeline starts with BoT-SORT-based single-camera
tracking, followed by an initial glance phase to initialize global IDs via
trajectory-feature matching. In later frames, new tracklets are matched to
existing global identities through a prioritized global matching strategy. New
global IDs are only introduced when no sufficiently similar trajectory or
feature match is found. 3D positions are estimated using depth maps and
calibration for spatial validation.

</details>


### [182] [DEARLi: Decoupled Enhancement of Recognition and Localization for Semi-supervised Panoptic Segmentation](https://arxiv.org/abs/2507.10118)
*Ivan Martinovi,Josip ari,Marin Ori,Matej Kristan,Sinia egvi*

Main category: cs.CV

TL;DR: A novel semi-supervised panoptic approach (DEARLi) leverages foundation models (CLIP and SAM) to enhance recognition and localization, achieving state-of-the-art results with minimal labeled data and reduced GPU memory.


<details>
  <summary>Details</summary>
Motivation: Pixel-level annotation is costly and time-consuming, and existing methods underutilize foundation models for label scarcity.

Method: DEARLi combines unsupervised mask-transformer consistency with CLIP's zero-shot classification and SAM's pseudo-labels for class-agnostic decoder warm-up.

Result: Achieves 29.9 PQ and 38.9 mIoU on ADE20K with only 158 labeled images, outperforming state-of-the-art methods with 8x less GPU memory.

Conclusion: DEARLi effectively addresses label scarcity and computational efficiency, excelling in challenging semi-supervised scenarios.

Abstract: Pixel-level annotation is expensive and time-consuming. Semi-supervised
segmentation methods address this challenge by learning models on few labeled
images alongside a large corpus of unlabeled images. Although foundation models
could further account for label scarcity, effective mechanisms for their
exploitation remain underexplored. We address this by devising a novel
semi-supervised panoptic approach fueled by two dedicated foundation models. We
enhance recognition by complementing unsupervised mask-transformer consistency
with zero-shot classification of CLIP features. We enhance localization by
class-agnostic decoder warm-up with respect to SAM pseudo-labels. The resulting
decoupled enhancement of recognition and localization (DEARLi) particularly
excels in the most challenging semi-supervised scenarios with large taxonomies
and limited labeled data. Moreover, DEARLi outperforms the state of the art in
semi-supervised semantic segmentation by a large margin while requiring 8x less
GPU memory, in spite of being trained only for the panoptic objective. We
observe 29.9 PQ and 38.9 mIoU on ADE20K with only 158 labeled images. The
source code is available at https://github.com/helen1c/DEARLi.

</details>


### [183] [Taming Modern Point Tracking for Speckle Tracking Echocardiography via Impartial Motion](https://arxiv.org/abs/2507.10127)
*Md Abulkalam Azad,John Nyberg,Hvard Dalen,Bjrnar Grenne,Lasse Lovstakken,Andreas stvik*

Main category: cs.CV

TL;DR: The paper explores SOTA point tracking methods for echocardiography, identifies directional motion bias, and proposes refined training and a lightweight network to improve tracking accuracy and generalization.


<details>
  <summary>Details</summary>
Motivation: Accurate motion estimation in echocardiography is crucial for cardiac function measurements, but existing methods struggle with intricate cardiac motion.

Method: The study refines training procedures, introduces tailored augmentations, and proposes a lightweight network using multi-scale cost volumes.

Result: Fine-tuning improves performance, with EchoTracker boosting accuracy by 60.7% and reducing trajectory error by 61.5%. Some SOTA models underperform the proposed simple model.

Conclusion: The refined methods enhance tracking robustness and generalize better, improving clinical measurements like GLS and reproducibility.

Abstract: Accurate motion estimation for tracking deformable tissues in
echocardiography is essential for precise cardiac function measurements. While
traditional methods like block matching or optical flow struggle with intricate
cardiac motion, modern point tracking approaches remain largely underexplored
in this domain. This work investigates the potential of state-of-the-art (SOTA)
point tracking methods for ultrasound, with a focus on echocardiography.
Although these novel approaches demonstrate strong performance in general
videos, their effectiveness and generalizability in echocardiography remain
limited. By analyzing cardiac motion throughout the heart cycle in real B-mode
ultrasound videos, we identify that a directional motion bias across different
views is affecting the existing training strategies. To mitigate this, we
refine the training procedure and incorporate a set of tailored augmentations
to reduce the bias and enhance tracking robustness and generalization through
impartial cardiac motion. We also propose a lightweight network leveraging
multi-scale cost volumes from spatial context alone to challenge the advanced
spatiotemporal point tracking models. Experiments demonstrate that fine-tuning
with our strategies significantly improves models' performances over their
baselines, even for out-of-distribution (OOD) cases. For instance, EchoTracker
boosts overall position accuracy by 60.7% and reduces median trajectory error
by 61.5% across heart cycle phases. Interestingly, several point tracking
models fail to outperform our proposed simple model in terms of tracking
accuracy and generalization, reflecting their limitations when applied to
echocardiography. Nevertheless, clinical evaluation reveals that these methods
improve GLS measurements, aligning more closely with expert-validated,
semi-automated tools and thus demonstrating better reproducibility in
real-world applications.

</details>


### [184] [Deep Recurrence for Dynamical Segmentation Models](https://arxiv.org/abs/2507.10143)
*David Calhas,Arlindo L. Oliveira*

Main category: cs.CV

TL;DR: A feedback mechanism inspired by predictive coding is proposed for neural networks, improving performance in noisy conditions and data efficiency compared to feedforward models.


<details>
  <summary>Details</summary>
Motivation: Biological vision systems use feedback for iterative refinement, but artificial networks often lack this, limiting their robustness and adaptability.

Method: A recurrent feedback loop is added to a U-Net, with softmax projection and exponential decay for stability. Tested on synthetic segmentation.

Result: Feedback outperforms feedforward in noise and generalizes better with limited data, achieving above-random performance with just two training examples.

Conclusion: Feedback enhances robustness and data efficiency, suggesting a move toward more biologically inspired neural architectures.

Abstract: While biological vision systems rely heavily on feedback connections to
iteratively refine perception, most artificial neural networks remain purely
feedforward, processing input in a single static pass. In this work, we propose
a predictive coding inspired feedback mechanism that introduces a recurrent
loop from output to input, allowing the model to refine its internal state over
time. We implement this mechanism within a standard U-Net architecture and
introduce two biologically motivated operations, softmax projection and
exponential decay, to ensure stability of the feedback loop. Through controlled
experiments on a synthetic segmentation task, we show that the feedback model
significantly outperforms its feedforward counterpart in noisy conditions and
generalizes more effectively with limited supervision. Notably, feedback
achieves above random performance with just two training examples, while the
feedforward model requires at least four. Our findings demonstrate that
feedback enhances robustness and data efficiency, and offer a path toward more
adaptive and biologically inspired neural architectures. Code is available at:
github.com/DCalhas/feedback_segmentation.

</details>


### [185] [SlumpGuard: An AI-Powered Real-Time System for Automated Concrete Slump Prediction via Video Analysis](https://arxiv.org/abs/2507.10171)
*Youngmin Kim,Giyeong Oh,Kwangsoo Youm,Youngjae Yu*

Main category: cs.CV

TL;DR: SlumpGuard is an AI-powered, video-based system for real-time concrete workability assessment, replacing manual slump tests to improve accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional slump testing is manual, time-consuming, and inconsistent, limiting real-time monitoring in construction quality control.

Method: The system uses AI to analyze concrete flow from truck chutes, enabling automated full-batch inspection without manual intervention.

Result: Empirical results from real-world deployment show SlumpGuard's effectiveness as a practical solution for concrete quality assurance.

Conclusion: SlumpGuard offers a modern, efficient, and accurate alternative to traditional slump testing for construction quality control.

Abstract: Concrete workability is essential for construction quality, with the slump
test being the most common on-site method for its assessment. However,
traditional slump testing is manual, time-consuming, and prone to
inconsistency, limiting its applicability for real-time monitoring. To address
these challenges, we propose SlumpGuard, an AI-powered, video-based system that
automatically analyzes concrete flow from the truck chute to assess workability
in real time. Our system enables full-batch inspection without manual
intervention, improving both the accuracy and efficiency of quality control. We
present the system design, a the construction of a dedicated dataset, and
empirical results from real-world deployment, demonstrating the effectiveness
of SlumpGuard as a practical solution for modern concrete quality assurance.

</details>


### [186] [Minimizing the Pretraining Gap: Domain-aligned Text-Based Person Retrieval](https://arxiv.org/abs/2507.10195)
*Shuyu Yang,Yaxiong Wang,Yongrui Li,Li Zhu,Zhedong Zheng*

Main category: cs.CV

TL;DR: The paper introduces a dual-level domain adaptation method for text-based person retrieval, addressing the gap between synthetic pretraining and real-world datasets.


<details>
  <summary>Details</summary>
Motivation: Privacy issues and high annotation costs make synthetic data popular for pretraining, but domain gaps (e.g., lighting, color, viewpoint) hinder effectiveness.

Method: Proposes a pipeline with Domain-aware Diffusion (DaD) for image-level adaptation and Multi-granularity Relation Alignment (MRA) for region-level alignment.

Result: Achieves state-of-the-art performance on CUHK-PEDES, ICFG-PEDES, and RSTPReid datasets.

Conclusion: The dual-level adaptation method effectively bridges the domain gap, improving retrieval accuracy.

Abstract: In this work, we focus on text-based person retrieval, which aims to identify
individuals based on textual descriptions. Given the significant privacy issues
and the high cost associated with manual annotation, synthetic data has become
a popular choice for pretraining models, leading to notable advancements.
However, the considerable domain gap between synthetic pretraining datasets and
real-world target datasets, characterized by differences in lighting, color,
and viewpoint, remains a critical obstacle that hinders the effectiveness of
the pretrain-finetune paradigm. To bridge this gap, we introduce a unified
text-based person retrieval pipeline considering domain adaptation at both
image and region levels. In particular, it contains two primary components,
i.e., Domain-aware Diffusion (DaD) for image-level adaptation and
Multi-granularity Relation Alignment (MRA) for region-level adaptation. As the
name implies, Domain-aware Diffusion is to migrate the distribution of images
from the pretraining dataset domain to the target real-world dataset domain,
e.g., CUHK-PEDES. Subsequently, MRA performs a meticulous region-level
alignment by establishing correspondences between visual regions and their
descriptive sentences, thereby addressing disparities at a finer granularity.
Extensive experiments show that our dual-level adaptation method has achieved
state-of-the-art results on the CUHK-PEDES, ICFG-PEDES, and RSTPReid datasets,
outperforming existing methodologies. The dataset, model, and code are
available at https://github.com/Shuyu-XJTU/MRA.

</details>


### [187] [A Training-Free, Task-Agnostic Framework for Enhancing MLLM Performance on High-Resolution Images](https://arxiv.org/abs/2507.10202)
*Jaeseong Lee,Yeeun Choi,Heechan Choi,Hanjung Kim,Seonjoo Kim*

Main category: cs.CV

TL;DR: ECP is a training-free, task-agnostic framework to improve MLLM performance on high-resolution images by leveraging coarse predictions for candidate regions and preserving fine details.


<details>
  <summary>Details</summary>
Motivation: MLLMs struggle with fine-grained localization in high-resolution images due to fixed resolution constraints, leading to poor generalization or loss of details.

Method: ECP uses a two-stage approach: extract candidate regions from downsampled images, then predict final outputs from these regions.

Result: Achieved +21.3%, +5.8%, +5.2% improvements in 4K GUI grounding and 4K/8K MLLM perception tasks.

Conclusion: ECP effectively enhances MLLM performance on high-resolution images without additional training.

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities in vision-language understanding, reasoning, and generation.
However, they struggle with tasks requiring fine-grained localization and
reasoning in high-resolution images. This constraint stems from the fact that
MLLMs are fine-tuned with fixed image resolution to align with the pre-trained
image encoder used in MLLM. Consequently, feeding high-resolution images
directly into MLLMs leads to poor generalization due to a train-test resolution
discrepancy, while downsampling these images-although ensuring
consistency-compromises fine-grained visual details and ultimately degrades
performance. To address this challenge, we propose Extract Candidate then
Predict (ECP), a novel training-free, task-agnostic two-stage framework
designed to enhance MLLM performance on high-resolution images. The key
intuition behind ECP is that while MLLMs struggle with high-resolution images,
their predictions on downsampled images still contain implicit localization
cues. By first identifying candidate region using the coarse prediction and
then predicting the final output based on candidate region, ECP effectively
preserves fine-grained details while mitigating the challenges posed by
high-resolution data. We validate our framework on 4K GUI grounding and 4K, 8K
MLLM perception, achieving +21.3%, +5.8%, +5.2% absolute improvement compared
to baseline respectively, demonstrating its effectiveness. Code is available at
https://github.com/yenncye/ECP.

</details>


### [188] [Improving Multimodal Learning via Imbalanced Learning](https://arxiv.org/abs/2507.10203)
*Shicai Wei,Chunbo Luo,Yang Luo*

Main category: cs.CV

TL;DR: The paper proposes Asymmetric Representation Learning (ARL) to optimize multimodal learning by imbalanced dependency on modalities, inversely proportional to their variances, improving performance without extra parameters.


<details>
  <summary>Details</summary>
Motivation: Multimodal learning often underperforms unimodal learning due to imbalanced learning. Existing gradient balancing methods are suboptimal; the paper argues for imbalanced dependency based on modality variances.

Method: ARL uses auxiliary regularizers to calculate modality variances and biases, re-weighting optimization inversely to variance ratios. It integrates these with multimodal loss, introducing no extra parameters.

Result: Extensive experiments show ARL's effectiveness and versatility across datasets, outperforming balanced learning approaches.

Conclusion: ARL demonstrates that imbalanced modality dependency, inversely proportional to variance ratios, optimizes multimodal learning, validated by empirical results.

Abstract: Multimodal learning often encounters the under-optimized problem and may
perform worse than unimodal learning. Existing approaches attribute this issue
to imbalanced learning across modalities and tend to address it through
gradient balancing. However, this paper argues that balanced learning is not
the optimal setting for multimodal learning. With bias-variance analysis, we
prove that imbalanced dependency on each modality obeying the inverse ratio of
their variances contributes to optimal performance. To this end, we propose the
Asymmetric Representation Learning(ARL) strategy to assist multimodal learning
via imbalanced optimization. ARL introduces auxiliary regularizers for each
modality encoder to calculate their prediction variance. ARL then calculates
coefficients via the unimodal variance to re-weight the optimization of each
modality, forcing the modality dependence ratio to be inversely proportional to
the modality variance ratio. Moreover, to minimize the generalization error,
ARL further introduces the prediction bias of each modality and jointly
optimizes them with multimodal loss. Notably, all auxiliary regularizers share
parameters with the multimodal model and rely only on the modality
representation. Thus the proposed ARL strategy introduces no extra parameters
and is independent of the structures and fusion methods of the multimodal
model. Finally, extensive experiments on various datasets validate the
effectiveness and versatility of ARL. Code is available at
\href{https://github.com/shicaiwei123/ICCV2025-ARL}{https://github.com/shicaiwei123/ICCV2025-ARL}

</details>


### [189] [Is Micro-expression Ethnic Leaning?](https://arxiv.org/abs/2507.10209)
*Huai-Qian Khor,Yante Li,Xingxun Jiang,Guoying Zhao*

Main category: cs.CV

TL;DR: The study challenges Ekman's emotion universality hypothesis by investigating ethnic influences on micro-expression recognition, proposing an ethnically aware framework.


<details>
  <summary>Details</summary>
Motivation: To explore the role of ethnicity in emotional expression and address potential biases in micro-expression analysis, questioning the universality assumption.

Method: Constructed a cross-cultural micro-expression database with ethnic labels, compared mono- and stereo-ethnicity, and developed an ethnically aware framework for feature learning.

Result: Found ethnic bias in micro-expression recognition, supporting the need for ethnicity-aware models.

Conclusion: Ethnicity significantly influences emotional expression, and integrating ethnic context improves micro-expression recognition accuracy.

Abstract: How much does ethnicity play its part in emotional expression? Emotional
expression and micro-expression research probe into understanding human
psychological responses to emotional stimuli, thereby revealing substantial
hidden yet authentic emotions that can be useful in the event of diagnosis and
interviews. While increased attention had been provided to micro-expression
analysis, the studies were done under Ekman's assumption of emotion
universality, where emotional expressions are identical across cultures and
social contexts. Our computational study uncovers some of the influences of
ethnic background in expression analysis, leading to an argument that the
emotional universality hypothesis is an overgeneralization from the perspective
of manual psychological analysis. In this research, we propose to investigate
the level of influence of ethnicity in a simulated micro-expression scenario.
We construct a cross-cultural micro-expression database and algorithmically
annotate the ethnic labels to facilitate the investigation. With the ethnically
annotated dataset, we perform a prima facie study to compare mono-ethnicity and
stereo-ethnicity in a controlled environment, which uncovers a certain
influence of ethnic bias via an experimental way. Building on this finding, we
propose a framework that integrates ethnic context into the emotional feature
learning process, yielding an ethnically aware framework that recognises
ethnicity differences in micro-expression recognition. For improved
understanding, qualitative analyses have been done to solidify the preliminary
investigation into this new realm of research. Code is publicly available at
https://github.com/IcedDoggie/ICMEW2025_EthnicMER

</details>


### [190] [Boosting Multimodal Learning via Disentangled Gradient Learning](https://arxiv.org/abs/2507.10213)
*Shicai Wei,Chunbo Luo,Yang Luo*

Main category: cs.CV

TL;DR: The paper identifies optimization conflicts in multimodal learning, proposes a disentangled gradient learning (DGL) framework to improve performance by decoupling modality encoder and fusion module training.


<details>
  <summary>Details</summary>
Motivation: Multimodal learning underperforms unimodal learning due to gradient conflicts between modality encoders and fusion modules, which existing methods fail to address.

Method: Proposes DGL, which truncates conflicting gradients and replaces them with unimodal gradients to optimize encoders and fusion modules separately.

Result: DGL improves performance across various modalities, tasks, and frameworks, outperforming existing methods.

Conclusion: DGL effectively resolves optimization conflicts in multimodal learning, enhancing performance and versatility.

Abstract: Multimodal learning often encounters the under-optimized problem and may have
worse performance than unimodal learning. Existing methods attribute this
problem to the imbalanced learning between modalities and rebalance them
through gradient modulation. However, they fail to explain why the dominant
modality in multimodal models also underperforms that in unimodal learning. In
this work, we reveal the optimization conflict between the modality encoder and
modality fusion module in multimodal models. Specifically, we prove that the
cross-modal fusion in multimodal models decreases the gradient passed back to
each modality encoder compared with unimodal models. Consequently, the
performance of each modality in the multimodal model is inferior to that in the
unimodal model. To this end, we propose a disentangled gradient learning (DGL)
framework to decouple the optimization of the modality encoder and modality
fusion module in the multimodal model. DGL truncates the gradient
back-propagated from the multimodal loss to the modality encoder and replaces
it with the gradient from unimodal loss. Besides, DGL removes the gradient
back-propagated from the unimodal loss to the modality fusion module. This
helps eliminate the gradient interference between the modality encoder and
modality fusion module while ensuring their respective optimization processes.
Finally, extensive experiments on multiple types of modalities, tasks, and
frameworks with dense cross-modal interaction demonstrate the effectiveness and
versatility of the proposed DGL. Code is available at
\href{https://github.com/shicaiwei123/ICCV2025-GDL}{https://github.com/shicaiwei123/ICCV2025-GDL}

</details>


### [191] [From Wardrobe to Canvas: Wardrobe Polyptych LoRA for Part-level Controllable Human Image Generation](https://arxiv.org/abs/2507.10217)
*Jeongho Kim,Sunghyun Park,Hyoungwoo Park,Sungrack Yun,Jaegul Choo,Seokeon Cho*

Main category: cs.CV

TL;DR: Wardrobe Polyptych LoRA introduces a part-level controllable model for personalized human image generation, avoiding inference-time fine-tuning and large-scale training while maintaining high fidelity.


<details>
  <summary>Details</summary>
Motivation: Personalized human image generation faces challenges in preserving precise attributes like identity and clothing details, with existing methods being computationally expensive.

Method: The approach trains only LoRA layers, conditions generation on wardrobe details, uses spatial references, and introduces a selective subject region loss.

Result: The method outperforms existing techniques in fidelity and consistency, enabling realistic and identity-preserving full-body synthesis.

Conclusion: Wardrobe Polyptych LoRA offers a practical, efficient solution for personalized human image generation without additional inference parameters.

Abstract: Recent diffusion models achieve personalization by learning specific
subjects, allowing learned attributes to be integrated into generated images.
However, personalized human image generation remains challenging due to the
need for precise and consistent attribute preservation (e.g., identity,
clothing details). Existing subject-driven image generation methods often
require either (1) inference-time fine-tuning with few images for each new
subject or (2) large-scale dataset training for generalization. Both approaches
are computationally expensive and impractical for real-time applications. To
address these limitations, we present Wardrobe Polyptych LoRA, a novel
part-level controllable model for personalized human image generation. By
training only LoRA layers, our method removes the computational burden at
inference while ensuring high-fidelity synthesis of unseen subjects. Our key
idea is to condition the generation on the subject's wardrobe and leverage
spatial references to reduce information loss, thereby improving fidelity and
consistency. Additionally, we introduce a selective subject region loss, which
encourages the model to disregard some of reference images during training. Our
loss ensures that generated images better align with text prompts while
maintaining subject integrity. Notably, our Wardrobe Polyptych LoRA requires no
additional parameters at the inference stage and performs generation using a
single model trained on a few training samples. We construct a new dataset and
benchmark tailored for personalized human image generation. Extensive
experiments show that our approach significantly outperforms existing
techniques in fidelity and consistency, enabling realistic and
identity-preserving full-body synthesis.

</details>


### [192] [Straighten Viscous Rectified Flow via Noise Optimization](https://arxiv.org/abs/2507.10218)
*Jimin Dai,Jiexi Yan,Jian Yang,Lei Luo*

Main category: cs.CV

TL;DR: VRFNO improves Reflow by optimizing noise couplings and enhancing trajectory distinction, achieving better image generation in fewer steps.


<details>
  <summary>Details</summary>
Motivation: Reflow's limitations in generating high-quality images due to distribution gaps between constructed couplings and real images.

Method: VRFNO integrates an encoder and neural velocity field, introducing a historical velocity term and noise optimization via reparameterization.

Result: VRFNO outperforms Reflow, achieving state-of-the-art performance in one-step and few-step generation.

Conclusion: VRFNO effectively addresses Reflow's shortcomings, offering superior image generation quality and efficiency.

Abstract: The Reflow operation aims to straighten the inference trajectories of the
rectified flow during training by constructing deterministic couplings between
noises and images, thereby improving the quality of generated images in
single-step or few-step generation. However, we identify critical limitations
in Reflow, particularly its inability to rapidly generate high-quality images
due to a distribution gap between images in its constructed deterministic
couplings and real images. To address these shortcomings, we propose a novel
alternative called Straighten Viscous Rectified Flow via Noise Optimization
(VRFNO), which is a joint training framework integrating an encoder and a
neural velocity field. VRFNO introduces two key innovations: (1) a historical
velocity term that enhances trajectory distinction, enabling the model to more
accurately predict the velocity of the current trajectory, and (2) the noise
optimization through reparameterization to form optimized couplings with real
images which are then utilized for training, effectively mitigating errors
caused by Reflow's limitations. Comprehensive experiments on synthetic data and
real datasets with varying resolutions show that VRFNO significantly mitigates
the limitations of Reflow, achieving state-of-the-art performance in both
one-step and few-step generation tasks.

</details>


### [193] [Spatial Lifting for Dense Prediction](https://arxiv.org/abs/2507.10222)
*Mingzhi Xu,Yizhe Zhang*

Main category: cs.CV

TL;DR: Spatial Lifting (SL) lifts 2D inputs to higher dimensions (e.g., 3D) for dense prediction tasks, achieving competitive performance with fewer parameters and lower inference costs.


<details>
  <summary>Details</summary>
Motivation: To improve efficiency and accuracy in dense prediction tasks by leveraging higher-dimensional processing.

Method: Lifts 2D inputs (e.g., images) to higher dimensions (e.g., 3D) and processes them with networks like 3D U-Net.

Result: Validated on 19 benchmarks, SL reduces parameters by 98% and lowers inference costs while maintaining performance.

Conclusion: SL offers a new paradigm for efficient, accurate, and reliable dense prediction in vision tasks.

Abstract: We present Spatial Lifting (SL), a novel methodology for dense prediction
tasks. SL operates by lifting standard inputs, such as 2D images, into a
higher-dimensional space and subsequently processing them using networks
designed for that higher dimension, such as a 3D U-Net. Counterintuitively,
this dimensionality lifting allows us to achieve good performance on benchmark
tasks compared to conventional approaches, while reducing inference costs and
significantly lowering the number of model parameters. The SL framework
produces intrinsically structured outputs along the lifted dimension. This
emergent structure facilitates dense supervision during training and enables
robust, near-zero-additional-cost prediction quality assessment at test time.
We validate our approach across 19 benchmark datasets (13 for semantic
segmentation and 6 for depth estimation), demonstrating competitive dense
prediction performance while reducing the model parameter count by over 98% (in
the U-Net case) and lowering inference costs. Spatial Lifting introduces a new
vision modeling paradigm that offers a promising path toward more efficient,
accurate, and reliable deep networks for dense prediction tasks in vision.

</details>


### [194] [ProGait: A Multi-Purpose Video Dataset and Benchmark for Transfemoral Prosthesis Users](https://arxiv.org/abs/2507.10223)
*Xiangyu Yin,Boyuan Yang,Weichen Liu,Qiyao Xue,Abrar Alamri,Goeran Fiedler,Wei Gao*

Main category: cs.CV

TL;DR: The paper introduces ProGait, a dataset for vision-based gait analysis of prosthetic legs, supporting tasks like segmentation and pose estimation, and demonstrates improved model performance.


<details>
  <summary>Details</summary>
Motivation: To address challenges in vision-based ML methods for gait analysis of prosthetic legs due to their unique appearance and movement patterns.

Method: Creation of the ProGait dataset with 412 video clips from amputees, alongside benchmark tasks and fine-tuned baseline models.

Result: Baseline models outperformed pre-trained vision models, showing better generalizability for prosthesis-specific tasks.

Conclusion: ProGait is a valuable resource for improving gait analysis and prosthesis design, with potential for broader applications in rehabilitation.

Abstract: Prosthetic legs play a pivotal role in clinical rehabilitation, allowing
individuals with lower-limb amputations the ability to regain mobility and
improve their quality of life. Gait analysis is fundamental for optimizing
prosthesis design and alignment, directly impacting the mobility and life
quality of individuals with lower-limb amputations. Vision-based machine
learning (ML) methods offer a scalable and non-invasive solution to gait
analysis, but face challenges in correctly detecting and analyzing prosthesis,
due to their unique appearances and new movement patterns. In this paper, we
aim to bridge this gap by introducing a multi-purpose dataset, namely ProGait,
to support multiple vision tasks including Video Object Segmentation, 2D Human
Pose Estimation, and Gait Analysis (GA). ProGait provides 412 video clips from
four above-knee amputees when testing multiple newly-fitted prosthetic legs
through walking trials, and depicts the presence, contours, poses, and gait
patterns of human subjects with transfemoral prosthetic legs. Alongside the
dataset itself, we also present benchmark tasks and fine-tuned baseline models
to illustrate the practical application and performance of the ProGait dataset.
We compared our baseline models against pre-trained vision models,
demonstrating improved generalizability when applying the ProGait dataset for
prosthesis-specific tasks. Our code is available at
https://github.com/pittisl/ProGait and dataset at
https://huggingface.co/datasets/ericyxy98/ProGait.

</details>


### [195] [Synthesizing Near-Boundary OOD Samples for Out-of-Distribution Detection](https://arxiv.org/abs/2507.10225)
*Jinglun Li,Kaixun Jiang,Zhaoyu Chen,Bo Lin,Yao Tang,Weifeng Ge,Wenqiang Zhang*

Main category: cs.CV

TL;DR: SynOOD leverages foundation models to generate synthetic OOD data for fine-tuning CLIP, improving boundary-level discrimination between InD and OOD samples.


<details>
  <summary>Details</summary>
Motivation: Challenging OOD samples near InD data can misclassify; foundation models offer a solution.

Method: Iterative in-painting with MLLM-guided prompts and noise adjustments for boundary-aligned OOD samples, fine-tuning CLIP.

Result: State-of-the-art on ImageNet: AUROC +2.80%, FPR95 -11.13%.

Conclusion: SynOOD effectively enhances OOD detection with minimal overhead.

Abstract: Pre-trained vision-language models have exhibited remarkable abilities in
detecting out-of-distribution (OOD) samples. However, some challenging OOD
samples, which lie close to in-distribution (InD) data in image feature space,
can still lead to misclassification. The emergence of foundation models like
diffusion models and multimodal large language models (MLLMs) offers a
potential solution to this issue. In this work, we propose SynOOD, a novel
approach that harnesses foundation models to generate synthetic, challenging
OOD data for fine-tuning CLIP models, thereby enhancing boundary-level
discrimination between InD and OOD samples. Our method uses an iterative
in-painting process guided by contextual prompts from MLLMs to produce nuanced,
boundary-aligned OOD samples. These samples are refined through noise
adjustments based on gradients from OOD scores like the energy score,
effectively sampling from the InD/OOD boundary. With these carefully
synthesized images, we fine-tune the CLIP image encoder and negative label
features derived from the text encoder to strengthen connections between
near-boundary OOD samples and a set of negative labels. Finally, SynOOD
achieves state-of-the-art performance on the large-scale ImageNet benchmark,
with minimal increases in parameters and runtime. Our approach significantly
surpasses existing methods, improving AUROC by 2.80% and reducing FPR95 by
11.13%. Codes are available in https://github.com/Jarvisgivemeasuit/SynOOD.

</details>


### [196] [Navigating the Challenges of AI-Generated Image Detection in the Wild: What Truly Matters?](https://arxiv.org/abs/2507.10236)
*Despina Konstantinidou,Dimitrios Karageorgiou,Christos Koutlis,Olga Papadopoulou,Emmanouil Schinas,Symeon Papadopoulos*

Main category: cs.CV

TL;DR: The paper addresses weaknesses in AI-Generated Image Detection (AID) models, introducing a real-world dataset (ITW-SM) and identifying four key factors for improved performance.


<details>
  <summary>Details</summary>
Motivation: The rise of AI-generated images threatens digital integrity, necessitating robust detection methods. Current models fail in real-world scenarios.

Method: The study evaluates AID models using ITW-SM, a dataset from social media, and analyzes backbone architecture, training data, pre-processing, and augmentation.

Result: Modifications based on the analysis improve AID models' AUC by 26.87% in real-world conditions.

Conclusion: The study highlights critical factors for enhancing AID models and demonstrates significant performance gains.

Abstract: The rapid advancement of generative technologies presents both unprecedented
creative opportunities and significant challenges, particularly in maintaining
social trust and ensuring the integrity of digital information. Following these
concerns, the challenge of AI-Generated Image Detection (AID) becomes
increasingly critical. As these technologies become more sophisticated, the
quality of AI-generated images has reached a level that can easily deceive even
the most discerning observers. Our systematic evaluation highlights a critical
weakness in current AI-Generated Image Detection models: while they perform
exceptionally well on controlled benchmark datasets, they struggle
significantly with real-world variations. To assess this, we introduce ITW-SM,
a new dataset of real and AI-generated images collected from major social media
platforms. In this paper, we identify four key factors that influence AID
performance in real-world scenarios: backbone architecture, training data
composition, pre-processing strategies and data augmentation combinations. By
systematically analyzing these components, we shed light on their impact on
detection efficacy. Our modifications result in an average AUC improvement of
26.87% across various AID models under real-world conditions.

</details>


### [197] [Transferring Styles for Reduced Texture Bias and Improved Robustness in Semantic Segmentation Networks](https://arxiv.org/abs/2507.10239)
*Ben Hamscher,Edgar Heinert,Annika Mtze,Kira Maag,Matthias Rottmann*

Main category: cs.CV

TL;DR: Style transfer in semantic segmentation reduces texture bias and boosts robustness against corruptions and adversarial attacks.


<details>
  <summary>Details</summary>
Motivation: To explore if style transfer can reduce texture bias and improve robustness in semantic segmentation, similar to its effects in image classification.

Method: Style transfer with style varying across artificial Voronoi cell areas, used to train semantic segmentation DNNs.

Result: Reduced texture bias and increased robustness against image corruptions and adversarial attacks in semantic segmentation.

Conclusion: Style transfer augmentation is effective for reducing texture bias and enhancing robustness in semantic segmentation across architectures and datasets.

Abstract: Recent research has investigated the shape and texture biases of deep neural
networks (DNNs) in image classification which influence their generalization
capabilities and robustness. It has been shown that, in comparison to regular
DNN training, training with stylized images reduces texture biases in image
classification and improves robustness with respect to image corruptions. In an
effort to advance this line of research, we examine whether style transfer can
likewise deliver these two effects in semantic segmentation. To this end, we
perform style transfer with style varying across artificial image areas. Those
random areas are formed by a chosen number of Voronoi cells. The resulting
style-transferred data is then used to train semantic segmentation DNNs with
the objective of reducing their dependence on texture cues while enhancing
their reliance on shape-based features. In our experiments, it turns out that
in semantic segmentation, style transfer augmentation reduces texture bias and
strongly increases robustness with respect to common image corruptions as well
as adversarial attacks. These observations hold for convolutional neural
networks and transformer architectures on the Cityscapes dataset as well as on
PASCAL Context, showing the generality of the proposed method.

</details>


### [198] [Kaleidoscopic Background Attack: Disrupting Pose Estimation with Multi-Fold Radial Symmetry Textures](https://arxiv.org/abs/2507.10265)
*Xinlong Ding,Hongwei Yu,Jiawei Li,Feifan Li,Yu Shang,Bochao Zou,Huimin Ma,Jiansheng Chen*

Main category: cs.CV

TL;DR: The paper introduces the Kaleidoscopic Background Attack (KBA) to disrupt camera pose estimation by using symmetric segments, enhancing attack effectiveness with a proposed loss function.


<details>
  <summary>Details</summary>
Motivation: Background textures in sparse-input scenarios can degrade pose estimation accuracy, prompting the need for adversarial attacks to test model robustness.

Method: KBA uses identical segments to create symmetric discs, optimized with a projected orientation consistency loss.

Result: Optimized adversarial backgrounds effectively attack various pose estimation models.

Conclusion: KBA demonstrates a novel, effective method for adversarial attacks on camera pose estimation.

Abstract: Camera pose estimation is a fundamental computer vision task that is
essential for applications like visual localization and multi-view stereo
reconstruction. In the object-centric scenarios with sparse inputs, the
accuracy of pose estimation can be significantly influenced by background
textures that occupy major portions of the images across different viewpoints.
In light of this, we introduce the Kaleidoscopic Background Attack (KBA), which
uses identical segments to form discs with multi-fold radial symmetry. These
discs maintain high similarity across different viewpoints, enabling effective
attacks on pose estimation models even with natural texture segments.
Additionally, a projected orientation consistency loss is proposed to optimize
the kaleidoscopic segments, leading to significant enhancement in the attack
effectiveness. Experimental results show that optimized adversarial
kaleidoscopic backgrounds can effectively attack various camera pose estimation
models.

</details>


### [199] [FTCFormer: Fuzzy Token Clustering Transformer for Image Classification](https://arxiv.org/abs/2507.10283)
*Muyi Bao,Changyu Zeng,Yifan Wang,Zhengni Yang,Zimu Wang,Guangliang Cheng,Jun Qi,Wei Wang*

Main category: cs.CV

TL;DR: FTCFormer improves vision transformers by dynamically clustering tokens based on semantic meaning, outperforming baselines across diverse datasets.


<details>
  <summary>Details</summary>
Motivation: Traditional transformers use uniform grid-based tokens, ignoring semantic regions, leading to suboptimal feature representation.

Method: FTCFormer introduces clustering-based downsampling, DPC-FKNN for center determination, SCS for token assignment, and Cmerge for token merging.

Result: Achieves gains of 1.43% (fine-grained), 1.09% (natural), 0.97% (medical), and 0.55% (remote sensing) over baselines.

Conclusion: FTCFormer enhances feature representation by focusing on semantic regions, validated across 32 datasets.

Abstract: Transformer-based deep neural networks have achieved remarkable success
across various computer vision tasks, largely attributed to their long-range
self-attention mechanism and scalability. However, most transformer
architectures embed images into uniform, grid-based vision tokens, neglecting
the underlying semantic meanings of image regions, resulting in suboptimal
feature representations. To address this issue, we propose Fuzzy Token
Clustering Transformer (FTCFormer), which incorporates a novel clustering-based
downsampling module to dynamically generate vision tokens based on the semantic
meanings instead of spatial positions. It allocates fewer tokens to less
informative regions and more to represent semantically important regions,
regardless of their spatial adjacency or shape irregularity. To further enhance
feature extraction and representation, we propose a Density Peak
Clustering-Fuzzy K-Nearest Neighbor (DPC-FKNN) mechanism for clustering center
determination, a Spatial Connectivity Score (SCS) for token assignment, and a
channel-wise merging (Cmerge) strategy for token merging. Extensive experiments
on 32 datasets across diverse domains validate the effectiveness of FTCFormer
on image classification, showing consistent improvements over the TCFormer
baseline, achieving gains of improving 1.43% on five fine-grained datasets,
1.09% on six natural image datasets, 0.97% on three medical datasets and 0.55%
on four remote sensing datasets. The code is available at:
https://github.com/BaoBao0926/FTCFormer/tree/main.

</details>


### [200] [Show and Polish: Reference-Guided Identity Preservation in Face Video Restoration](https://arxiv.org/abs/2507.10293)
*Wenkang Han,Wang Lin,Yiyun Zhou,Qi Liu,Shulei Wang,Chang Yao,Jingyuan Chen*

Main category: cs.CV

TL;DR: IP-FVR is a novel method for face video restoration that uses a reference image for identity conditioning, decoupled cross-attention, and feedback learning to minimize identity drift, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Traditional methods fail to preserve identity-specific features in severely degraded face videos, producing average-looking results.

Method: IP-FVR leverages a reference image for identity conditioning, uses decoupled cross-attention, feedback learning for intra-clip drift, and exponential blending for inter-clip drift, along with multi-stream negative prompts.

Result: IP-FVR outperforms existing methods in quality and identity preservation on synthetic and real-world datasets.

Conclusion: IP-FVR shows significant potential for practical face video restoration applications due to its superior performance in preserving identity and quality.

Abstract: Face Video Restoration (FVR) aims to recover high-quality face videos from
degraded versions. Traditional methods struggle to preserve fine-grained,
identity-specific features when degradation is severe, often producing
average-looking faces that lack individual characteristics. To address these
challenges, we introduce IP-FVR, a novel method that leverages a high-quality
reference face image as a visual prompt to provide identity conditioning during
the denoising process. IP-FVR incorporates semantically rich identity
information from the reference image using decoupled cross-attention
mechanisms, ensuring detailed and identity consistent results. For intra-clip
identity drift (within 24 frames), we introduce an identity-preserving feedback
learning method that combines cosine similarity-based reward signals with
suffix-weighted temporal aggregation. This approach effectively minimizes drift
within sequences of frames. For inter-clip identity drift, we develop an
exponential blending strategy that aligns identities across clips by
iteratively blending frames from previous clips during the denoising process.
This method ensures consistent identity representation across different clips.
Additionally, we enhance the restoration process with a multi-stream negative
prompt, guiding the model's attention to relevant facial attributes and
minimizing the generation of low-quality or incorrect features. Extensive
experiments on both synthetic and real-world datasets demonstrate that IP-FVR
outperforms existing methods in both quality and identity preservation,
showcasing its substantial potential for practical applications in face video
restoration.

</details>


### [201] [DisCo: Towards Distinct and Coherent Visual Encapsulation in Video MLLMs](https://arxiv.org/abs/2507.10302)
*Jiahe Zhao,Rongkun Zheng,Yi Wang,Helin Wang,Hengshuang Zhao*

Main category: cs.CV

TL;DR: DisCo introduces a novel visual encapsulation method for video MLLMs, addressing semantic indistinctness and temporal incoherence with two modules: VCD for semantic clarity and TFC for temporal consistency. It outperforms existing methods in benchmarks and improves token efficiency.


<details>
  <summary>Details</summary>
Motivation: Linear projectors in video MLLMs cause semantic indistinctness and temporal incoherence. Resamplers show potential but lack effective solutions. DisCo aims to resolve these issues.

Method: DisCo uses a Visual Concept Discriminator (VCD) for semantic distinctness and a Temporal Focus Calibrator (TFC) for temporal coherence in visual tokens.

Result: DisCo outperforms state-of-the-art methods in video understanding benchmarks and achieves higher token efficiency.

Conclusion: DisCo effectively addresses key challenges in video MLLMs, offering a robust solution for visual encapsulation with improved performance and efficiency.

Abstract: In video Multimodal Large Language Models (video MLLMs), the visual
encapsulation process plays a pivotal role in converting video contents into
representative tokens for LLM input. While linear projectors are widely
employed for encapsulation, they introduce semantic indistinctness and temporal
incoherence when applied to videos. Conversely, the structure of resamplers
shows promise in tackling these challenges, but an effective solution remains
unexplored. Drawing inspiration from resampler structures, we introduce DisCo,
a novel visual encapsulation method designed to yield semantically distinct and
temporally coherent visual tokens for video MLLMs. DisCo integrates two key
components: (1) A Visual Concept Discriminator (VCD) module, assigning unique
semantics for visual tokens by associating them in pair with discriminative
concepts in the video. (2) A Temporal Focus Calibrator (TFC) module, ensuring
consistent temporal focus of visual tokens to video elements across every video
frame. Through extensive experiments on multiple video MLLM frameworks, we
demonstrate that DisCo remarkably outperforms previous state-of-the-art methods
across a variety of video understanding benchmarks, while also achieving higher
token efficiency thanks to the reduction of semantic indistinctness. The code:
https://github.com/ZJHTerry18/DisCo.

</details>


### [202] [Contrastive Pretraining with Dual Visual Encoders for Gloss-Free Sign Language Translation](https://arxiv.org/abs/2507.10306)
*Ozge Mercanoglu Sincan,Richard Bowden*

Main category: cs.CV

TL;DR: A two-phase, dual visual encoder framework for gloss-free Sign Language Translation (SLT) outperforms single-stream variants and achieves top BLEU-4 scores.


<details>
  <summary>Details</summary>
Motivation: Early SLT systems rely on costly gloss annotations, which are limited in capturing continuous signing complexity.

Method: Uses contrastive visual-language pretraining with dual visual backbones, aligning outputs with text embeddings, then fuses features for encoder-decoder SLT.

Result: Achieves highest BLEU-4 score on Phoenix-2014T benchmark among gloss-free SLT approaches.

Conclusion: The proposed dual encoder framework is effective for gloss-free SLT, outperforming single-stream methods.

Abstract: Sign Language Translation (SLT) aims to convert sign language videos into
spoken or written text. While early systems relied on gloss annotations as an
intermediate supervision, such annotations are costly to obtain and often fail
to capture the full complexity of continuous signing. In this work, we propose
a two-phase, dual visual encoder framework for gloss-free SLT, leveraging
contrastive visual-language pretraining. During pretraining, our approach
employs two complementary visual backbones whose outputs are jointly aligned
with each other and with sentence-level text embeddings via a contrastive
objective. During the downstream SLT task, we fuse the visual features and
input them into an encoder-decoder model. On the Phoenix-2014T benchmark, our
dual encoder architecture consistently outperforms its single stream variants
and achieves the highest BLEU-4 score among existing gloss-free SLT approaches.

</details>


### [203] [Mind the Gap: Aligning Vision Foundation Models to Image Feature Matching](https://arxiv.org/abs/2507.10318)
*Yuhan Liu,Jingwen Fu,Yang Wu,Kangyi Wu,Pengna Li,Jiayi Wu,Sanping Zhou,Jingmin Xin*

Main category: cs.CV

TL;DR: The paper introduces IMD, a framework using diffusion models for image feature matching, addressing misalignment issues in foundation models and improving multi-instance matching.


<details>
  <summary>Details</summary>
Motivation: Previous works ignored misalignment when using foundation models for feature matching, leading to poor performance in multi-instance scenarios.

Method: IMD integrates generative-based diffusion models and a cross-image interaction prompting module to enhance instance-level details and bidirectional information flow.

Result: IMD achieves state-of-the-art performance, with a 12% improvement on the new IMIM benchmark.

Conclusion: IMD effectively mitigates misalignment in foundation models for feature matching, especially in multi-instance scenarios.

Abstract: Leveraging the vision foundation models has emerged as a mainstream paradigm
that improves the performance of image feature matching. However, previous
works have ignored the misalignment when introducing the foundation models into
feature matching. The misalignment arises from the discrepancy between the
foundation models focusing on single-image understanding and the cross-image
understanding requirement of feature matching. Specifically, 1) the embeddings
derived from commonly used foundation models exhibit discrepancies with the
optimal embeddings required for feature matching; 2) lacking an effective
mechanism to leverage the single-image understanding ability into cross-image
understanding. A significant consequence of the misalignment is they struggle
when addressing multi-instance feature matching problems. To address this, we
introduce a simple but effective framework, called IMD (Image feature Matching
with a pre-trained Diffusion model) with two parts: 1) Unlike the dominant
solutions employing contrastive-learning based foundation models that emphasize
global semantics, we integrate the generative-based diffusion models to
effectively capture instance-level details. 2) We leverage the prompt mechanism
in generative model as a natural tunnel, propose a novel cross-image
interaction prompting module to facilitate bidirectional information
interaction between image pairs. To more accurately measure the misalignment,
we propose a new benchmark called IMIM, which focuses on multi-instance
scenarios. Our proposed IMD establishes a new state-of-the-art in commonly
evaluated benchmarks, and the superior improvement 12% in IMIM indicates our
method efficiently mitigates the misalignment.

</details>


### [204] [Text Embedding Knows How to Quantize Text-Guided Diffusion Models](https://arxiv.org/abs/2507.10340)
*Hongjae Lee,Myungjun Son,Dongjea Kang,Seung-Won Jung*

Main category: cs.CV

TL;DR: QLIP introduces a text-prompt-guided quantization method for diffusion models, reducing computational complexity while improving image quality.


<details>
  <summary>Details</summary>
Motivation: Existing quantization methods for diffusion models ignore input conditions like text prompts, limiting efficiency and performance.

Method: QLIP uses text prompts to dynamically select bit precision per layer and time step, integrating with existing quantization techniques.

Result: QLIP reduces computational complexity and enhances generated image quality across multiple datasets.

Conclusion: QLIP effectively leverages text prompts for efficient and high-quality diffusion model quantization.

Abstract: Despite the success of diffusion models in image generation tasks such as
text-to-image, the enormous computational complexity of diffusion models limits
their use in resource-constrained environments. To address this, network
quantization has emerged as a promising solution for designing efficient
diffusion models. However, existing diffusion model quantization methods do not
consider input conditions, such as text prompts, as an essential source of
information for quantization. In this paper, we propose a novel quantization
method dubbed Quantization of Language-to-Image diffusion models using text
Prompts (QLIP). QLIP leverages text prompts to guide the selection of bit
precision for every layer at each time step. In addition, QLIP can be
seamlessly integrated into existing quantization methods to enhance
quantization efficiency. Our extensive experiments demonstrate the
effectiveness of QLIP in reducing computational complexity and improving the
quality of the generated images across various datasets.

</details>


### [205] [FGSSNet: Feature-Guided Semantic Segmentation of Real World Floorplans](https://arxiv.org/abs/2507.10343)
*Hugo Norrby,Gabriel Frm,Kevin Hernandez-Diaz,Fernando Alonso-Fernandez*

Main category: cs.CV

TL;DR: FGSSNet is a multi-headed feature-guided semantic segmentation architecture for wall segmentation on floorplans, outperforming vanilla U-Net by injecting domain-specific features.


<details>
  <summary>Details</summary>
Motivation: To improve generalization in wall segmentation on floorplans by leveraging domain-specific features.

Method: Uses a U-Net backbone with a multi-headed feature extractor trained on wall patches to inject texture and width features into the segmentation process.

Result: Experiments show FGSSNet outperforms vanilla U-Net, validating the approach.

Conclusion: FGSSNet's feature-guided design enhances segmentation accuracy for floorplans.

Abstract: We introduce FGSSNet, a novel multi-headed feature-guided semantic
segmentation (FGSS) architecture designed to improve the generalization ability
of wall segmentation on floorplans. FGSSNet features a U-Net segmentation
backbone with a multi-headed dedicated feature extractor used to extract
domain-specific feature maps which are injected into the latent space of U-Net
to guide the segmentation process. This dedicated feature extractor is trained
as an encoder-decoder with selected wall patches, representative of the walls
present in the input floorplan, to produce a compressed latent representation
of wall patches while jointly trained to predict the wall width. In doing so,
we expect that the feature extractor encodes texture and width features of wall
patches that are useful to guide the wall segmentation process. Our experiments
show increased performance by the use of such injected features in comparison
to the vanilla U-Net, highlighting the validity of the proposed approach.

</details>


### [206] [Beyond Graph Model: Reliable VLM Fine-Tuning via Random Graph Adapter](https://arxiv.org/abs/2507.10355)
*Bo Jiang,Xueyang Ze,Beibei Wang,Xixi Wang,Xixi Wan,Bin Luo*

Main category: cs.CV

TL;DR: The paper proposes a Vertex Random Graph Adapter (VRGAdapter) to capture diverse textual descriptions and inter-class relationships in Vision-Language Models, enhancing downstream task performance with probabilistic modeling and ensemble prediction.


<details>
  <summary>Details</summary>
Motivation: Existing deterministic textual feature adapters fail to capture the diversity in textual descriptions and inter-class relationships, limiting their effectiveness.

Method: VRGAdapter uses a Vertex Random Knowledge Graph (VRKG) to model diverse descriptions and inter-class relationships, followed by probabilistic message propagation and reparameterized sampling for adapter learning. An Uncertainty-guided Multi-branch Fusion (UMF) scheme is also introduced for robust ensemble prediction.

Result: Extensive experiments on benchmark datasets validate the effectiveness of VRGAdapter.

Conclusion: VRGAdapter offers a more general and effective solution for textual adapter-based tuning in VLMs, outperforming traditional methods.

Abstract: Textual adapter-based tuning methods have shown significant potential in
transferring knowledge from pre-trained Vision-Language Models (VLMs) to
downstream tasks. Existing works generally employ the deterministic textual
feature adapter to refine each category textual representation. However, due to
inherent factors such as different attributes and contexts, there exists
significant diversity in textual descriptions for each category. Such
description diversity offers rich discriminative semantic knowledge that can
benefit downstream visual learning tasks. Obviously, traditional deterministic
adapter model cannot adequately capture this varied semantic information. Also,
it is desirable to exploit the inter-class relationships in VLM adapter. To
address these issues, we propose to exploit random graph model into VLM adapter
and develop a novel Vertex Random Graph Adapter (VRGAdapter). VRGAdapter first
models the inherent diverse descriptions of each category and inter-class
relationships of different categories simultaneously by leveraging a Vertex
Random Knowledge Graph (VRKG) model. Then, it employs probabilistic message
propagation on VRKG to learn context-aware distribution representation for each
class node. Finally, it adopts a reparameterized sampling function to achieve
textual adapter learning. Note that, VRGAdapter provides a more general adapter
solution that encompasses traditional graph-based adapter as a special case. In
addition, to enable more robust performance for downstream tasks, we also
introduce a new Uncertainty-guided Multi-branch Fusion (UMF) scheme that
dynamically integrates multiple pre-trained models for ensemble prediction.
Extensive experiments on multiple benchmark datasets demonstrate the
effectiveness of our approach.

</details>


### [207] [Fine-Grained Zero-Shot Object Detection](https://arxiv.org/abs/2507.10358)
*Hongxu Ma,Chenbo Zhang,Lu Zhang,Jiaogen Zhou,Jihong Guan,Shuigeng Zhou*

Main category: cs.CV

TL;DR: The paper introduces Fine-Grained Zero-Shot Object Detection (FG-ZSD), addressing the challenge of detecting visually similar objects. It proposes the MSHC method and a new dataset, FGZSD-Birds, showing superior performance over existing models.


<details>
  <summary>Details</summary>
Motivation: Existing zero-shot object detection (ZSD) methods focus on coarse-grained tasks, but real-life scenarios often involve fine-grained detection (e.g., distinguishing bird species). This gap motivates the FG-ZSD problem.

Method: The MSHC method is developed, using an improved two-stage detector and multi-level semantics-aware embedding alignment loss to align visual and semantic spaces.

Result: The method outperforms existing ZSD models on the newly created FGZSD-Birds dataset, which includes 148,820 images across 1,432 species.

Conclusion: The paper successfully addresses the FG-ZSD problem, demonstrating the effectiveness of MSHC and highlighting the need for fine-grained datasets in ZSD research.

Abstract: Zero-shot object detection (ZSD) aims to leverage semantic descriptions to
localize and recognize objects of both seen and unseen classes. Existing ZSD
works are mainly coarse-grained object detection, where the classes are
visually quite different, thus are relatively easy to distinguish. However, in
real life we often have to face fine-grained object detection scenarios, where
the classes are too similar to be easily distinguished. For example, detecting
different kinds of birds, fishes, and flowers.
  In this paper, we propose and solve a new problem called Fine-Grained
Zero-Shot Object Detection (FG-ZSD for short), which aims to detect objects of
different classes with minute differences in details under the ZSD paradigm. We
develop an effective method called MSHC for the FG-ZSD task, which is based on
an improved two-stage detector and employs a multi-level semantics-aware
embedding alignment loss, ensuring tight coupling between the visual and
semantic spaces. Considering that existing ZSD datasets are not suitable for
the new FG-ZSD task, we build the first FG-ZSD benchmark dataset FGZSD-Birds,
which contains 148,820 images falling into 36 orders, 140 families, 579 genera
and 1432 species. Extensive experiments on FGZSD-Birds show that our method
outperforms existing ZSD models.

</details>


### [208] [Test-Time Canonicalization by Foundation Models for Robust Perception](https://arxiv.org/abs/2507.10375)
*Utkarsh Singhal,Ryan Feng,Stella X. Yu,Atul Prakash*

Main category: cs.CV

TL;DR: FOCAL is a test-time framework that enhances visual perception robustness by leveraging foundation models to generate and optimize transformations, eliminating the need for re-training or architectural changes.


<details>
  <summary>Details</summary>
Motivation: Current methods rely on specialized architectures or predefined augmentations, limiting generalization. FOCAL aims to achieve robust perception without these constraints.

Method: FOCAL uses internet-scale visual priors from foundation models to generate and optimize candidate transformations toward visually typical views.

Result: Experiments show improved robustness of CLIP and SAM across challenging transformations like 2D/3D rotations, illumination shifts, and day-night variations.

Conclusion: FOCAL challenges the need for transform-specific training, offering a scalable path to invariance with potential applications in active vision.

Abstract: Real-world visual perception requires invariance to diverse transformations,
yet current methods rely heavily on specialized architectures or training on
predefined augmentations, limiting generalization. We propose FOCAL, a
test-time, data-driven framework that achieves robust perception by leveraging
internet-scale visual priors from foundation models. By generating and
optimizing candidate transformations toward visually typical, "canonical"
views, FOCAL enhances robustness without re-training or architectural changes.
Our experiments demonstrate improved robustness of CLIP and SAM across
challenging transformations, including 2D/3D rotations, illumination shifts
(contrast and color), and day-night variations. We also highlight potential
applications in active vision. Our approach challenges the assumption that
transform-specific training is necessary, instead offering a scalable path to
invariance. Our code is available at: https://github.com/sutkarsh/focal.

</details>


### [209] [Improving Remote Sensing Classification using Topological Data Analysis and Convolutional Neural Networks](https://arxiv.org/abs/2507.10381)
*Aaryam Sharma*

Main category: cs.CV

TL;DR: A TDA feature engineering pipeline integrates topological features with deep learning, improving ResNet18 performance on remote sensing datasets like EuroSAT and RESISC45.


<details>
  <summary>Details</summary>
Motivation: CNNs are biased towards texture-based features, limiting their effectiveness. TDA offers robust geometric insights, enhancing model performance.

Method: Proposed a TDA feature engineering pipeline to integrate topological features with deep learning models, tested on ResNet18 for satellite scene classification.

Result: Improved ResNet18 accuracy by 1.44% on EuroSAT (99.33%) and 1.82% on RESISC45, outperforming larger models like ResNet50 and XL Vision Transformers.

Conclusion: TDA features enhance deep learning models even without explicit topological structures, expanding TDA's applicability in remote sensing.

Abstract: Topological data analysis (TDA) is a relatively new field that is gaining
rapid adoption due to its robustness and ability to effectively describe
complex datasets by quantifying geometric information. In imaging contexts, TDA
typically models data as filtered cubical complexes from which we can extract
discriminative features using persistence homology. Meanwhile, convolutional
neural networks (CNNs) have been shown to be biased towards texture based local
features. To address this limitation, we propose a TDA feature engineering
pipeline and a simple method to integrate topological features with deep
learning models on remote sensing classification. Our method improves the
performance of a ResNet18 model on the EuroSAT dataset by 1.44% achieving
99.33% accuracy, which surpasses all previously reported single-model
accuracies, including those with larger architectures, such as ResNet50 (2x
larger) and XL Vision Transformers (197x larger). We additionally show that our
method's accuracy is 1.82% higher than our ResNet18 baseline on the RESISC45
dataset. To our knowledge, this is the first application of TDA features in
satellite scene classification with deep learning. This demonstrates that TDA
features can be integrated with deep learning models, even on datasets without
explicit topological structures, thereby increasing the applicability of TDA. A
clean implementation of our method will be made publicly available upon
publication.

</details>


### [210] [Numerically Computing Galois Groups of Minimal Problems](https://arxiv.org/abs/2507.10407)
*Timothy Duff*

Main category: cs.CV

TL;DR: The paper explores solving parametric algebraic systems in algebra, numerical computation, and computer vision, focusing on robust model-fitting like RanSaC.


<details>
  <summary>Details</summary>
Motivation: The problem arises from solving multiple instances of parametric algebraic systems, relevant to computer vision and numerical computation.

Method: The work measures the intrinsic difficulty of solving such systems and develops practical solutions.

Result: Progress has been made in understanding and solving these parametric systems over the last 5+ years.

Conclusion: The talk summarizes advancements in tackling parametric algebraic systems, bridging algebra, computation, and vision.

Abstract: I discuss a seemingly unlikely confluence of topics in algebra, numerical
computation, and computer vision. The motivating problem is that of solving
multiples instances of a parametric family of systems of algebraic (polynomial
or rational function) equations. No doubt already of interest to ISSAC
attendees, this problem arises in the context of robust model-fitting paradigms
currently utilized by the computer vision community (namely "Random Sampling
and Consensus", aka "RanSaC".) This talk will give an overview of work in the
last 5+ years that aspires to measure the intrinsic difficulty of solving such
parametric systems, and makes strides towards practical solutions.

</details>


### [211] [Text-Visual Semantic Constrained AI-Generated Image Quality Assessment](https://arxiv.org/abs/2507.10432)
*Qiang Li,Qingsen Yan,Haojian Huang,Peng Wu,Haokui Zhang,Yanning Zhang*

Main category: cs.CV

TL;DR: SC-AGIQA is a framework for assessing AI-generated image quality by addressing semantic misalignment and detail perception issues using text-visual constraints.


<details>
  <summary>Details</summary>
Motivation: Existing methods like CLIP or BLIP struggle with semantic misalignment and missing details in AI-generated images.

Method: SC-AGIQA integrates text-visual semantic constraints with two modules: TSAM for semantic alignment and FFDPM for fine-grained degradation perception.

Result: SC-AGIQA outperforms state-of-the-art methods on benchmark datasets.

Conclusion: The proposed framework effectively improves AI-generated image quality assessment by addressing key challenges.

Abstract: With the rapid advancements in Artificial Intelligence Generated Image (AGI)
technology, the accurate assessment of their quality has become an increasingly
vital requirement. Prevailing methods typically rely on cross-modal models like
CLIP or BLIP to evaluate text-image alignment and visual quality. However, when
applied to AGIs, these methods encounter two primary challenges: semantic
misalignment and details perception missing. To address these limitations, we
propose Text-Visual Semantic Constrained AI-Generated Image Quality Assessment
(SC-AGIQA), a unified framework that leverages text-visual semantic constraints
to significantly enhance the comprehensive evaluation of both text-image
consistency and perceptual distortion in AI-generated images. Our approach
integrates key capabilities from multiple models and tackles the aforementioned
challenges by introducing two core modules: the Text-assisted Semantic
Alignment Module (TSAM), which leverages Multimodal Large Language Models
(MLLMs) to bridge the semantic gap by generating an image description and
comparing it against the original prompt for a refined consistency check, and
the Frequency-domain Fine-Grained Degradation Perception Module (FFDPM), which
draws inspiration from Human Visual System (HVS) properties by employing
frequency domain analysis combined with perceptual sensitivity weighting to
better quantify subtle visual distortions and enhance the capture of
fine-grained visual quality details in images. Extensive experiments conducted
on multiple benchmark datasets demonstrate that SC-AGIQA outperforms existing
state-of-the-art methods. The code is publicly available at
https://github.com/mozhu1/SC-AGIQA.

</details>


### [212] [4D-Animal: Freely Reconstructing Animatable 3D Animals from Videos](https://arxiv.org/abs/2507.10437)
*Shanshan Zhong,Jiawei Peng,Zehan Zheng,Zhongzhan Huang,Wufei Ma,Guofeng Zhang,Qihao Liu,Alan Yuille,Jieneng Chen*

Main category: cs.CV

TL;DR: 4D-Animal reconstructs animatable 3D animals from videos without sparse keypoints, using a dense feature network and hierarchical alignment for accuracy and coherence.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on labor-intensive keypoints, which are unreliable due to limited training data.

Method: Proposes a dense feature network for SMAL parameter mapping and a hierarchical alignment strategy integrating silhouette, part-level, pixel-level, and temporal cues.

Result: Outperforms model-based and model-free baselines, producing high-quality 3D assets for broader applications.

Conclusion: 4D-Animal offers an efficient, stable, and scalable solution for 3D animal reconstruction.

Abstract: Existing methods for reconstructing animatable 3D animals from videos
typically rely on sparse semantic keypoints to fit parametric models. However,
obtaining such keypoints is labor-intensive, and keypoint detectors trained on
limited animal data are often unreliable. To address this, we propose
4D-Animal, a novel framework that reconstructs animatable 3D animals from
videos without requiring sparse keypoint annotations. Our approach introduces a
dense feature network that maps 2D representations to SMAL parameters,
enhancing both the efficiency and stability of the fitting process.
Furthermore, we develop a hierarchical alignment strategy that integrates
silhouette, part-level, pixel-level, and temporal cues from pre-trained 2D
visual models to produce accurate and temporally coherent reconstructions
across frames. Extensive experiments demonstrate that 4D-Animal outperforms
both model-based and model-free baselines. Moreover, the high-quality 3D assets
generated by our method can benefit other 3D tasks, underscoring its potential
for large-scale applications. The code is released at
https://github.com/zhongshsh/4D-Animal.

</details>


### [213] [CoralVQA: A Large-Scale Visual Question Answering Dataset for Coral Reef Image Understanding](https://arxiv.org/abs/2507.10449)
*Hongyong Han,Wei Wang,Gaowei Zhang,Mingjie Li,Yi Wang*

Main category: cs.CV

TL;DR: CoralVQA is the first large-scale VQA dataset for coral reef analysis, addressing domain-specific challenges and enabling user-friendly interaction with coral images.


<details>
  <summary>Details</summary>
Motivation: Coral reefs require monitoring, but interpreting images is challenging due to domain expertise needs. VQA with LVLMs can bridge this gap.

Method: Developed CoralVQA with 12,805 coral images and 277,653 QA pairs using a semi-automatic pipeline with marine biologists.

Result: Evaluated LVLMs on CoralVQA, revealing limitations and opportunities for future development.

Conclusion: CoralVQA provides a benchmark for vision-language reasoning in coral conservation, guiding future LVLM advancements.

Abstract: Coral reefs are vital yet vulnerable ecosystems that require continuous
monitoring to support conservation. While coral reef images provide essential
information in coral monitoring, interpreting such images remains challenging
due to the need for domain expertise. Visual Question Answering (VQA), powered
by Large Vision-Language Models (LVLMs), has great potential in user-friendly
interaction with coral reef images. However, applying VQA to coral imagery
demands a dedicated dataset that addresses two key challenges: domain-specific
annotations and multidimensional questions. In this work, we introduce
CoralVQA, the first large-scale VQA dataset for coral reef analysis. It
contains 12,805 real-world coral images from 67 coral genera collected from 3
oceans, along with 277,653 question-answer pairs that comprehensively assess
ecological and health-related conditions. To construct this dataset, we develop
a semi-automatic data construction pipeline in collaboration with marine
biologists to ensure both scalability and professional-grade data quality.
CoralVQA presents novel challenges and provides a comprehensive benchmark for
studying vision-language reasoning in the context of coral reef images. By
evaluating several state-of-the-art LVLMs, we reveal key limitations and
opportunities. These insights form a foundation for future LVLM development,
with a particular emphasis on supporting coral conservation efforts.

</details>


### [214] [RAPNet: A Receptive-Field Adaptive Convolutional Neural Network for Pansharpening](https://arxiv.org/abs/2507.10461)
*Tao Tang,Chengxu Yang*

Main category: cs.CV

TL;DR: RAPNet introduces content-adaptive convolution (RAPConv) and dynamic feature fusion (PAN-DFF) to improve pansharpening by addressing local content variations and balancing spatial detail with spectral fidelity.


<details>
  <summary>Details</summary>
Motivation: CNNs for pansharpening lack adaptability to local content variations, limiting their effectiveness.

Method: RAPNet uses RAPConv for spatially adaptive kernels and PAN-DFF with attention for optimal detail and spectral balance.

Result: RAPNet outperforms existing methods in quantitative and qualitative evaluations on public datasets.

Conclusion: RAPNet's adaptive components significantly enhance pansharpening performance.

Abstract: Pansharpening refers to the process of integrating a high resolution
panchromatic (PAN) image with a lower resolution multispectral (MS) image to
generate a fused product, which is pivotal in remote sensing. Despite the
effectiveness of CNNs in addressing this challenge, they are inherently
constrained by the uniform application of convolutional kernels across all
spatial positions, overlooking local content variations. To overcome this
issue, we introduce RAPNet, a new architecture that leverages content-adaptive
convolution. At its core, RAPNet employs the Receptive-field Adaptive
Pansharpening Convolution (RAPConv), designed to produce spatially adaptive
kernels responsive to local feature context, thereby enhancing the precision of
spatial detail extraction. Additionally, the network integrates the
Pansharpening Dynamic Feature Fusion (PAN-DFF) module, which incorporates an
attention mechanism to achieve an optimal balance between spatial detail
enhancement and spectral fidelity. Comprehensive evaluations on publicly
available datasets confirm that RAPNet delivers superior performance compared
to existing approaches, as demonstrated by both quantitative metrics and
qualitative assessments. Ablation analyses further substantiate the
effectiveness of the proposed adaptive components.

</details>


### [215] [RefSTAR: Blind Facial Image Restoration with Reference Selection, Transfer, and Reconstruction](https://arxiv.org/abs/2507.10470)
*Zhicun Yin,Junjie Chen,Ming Liu,Zhixin Wang,Fan Li,Renjing Pei,Xiaoming Li,Rynson W. H. Lau,Wangmeng Zuo*

Main category: cs.CV

TL;DR: A novel blind facial image restoration method (RefSTAR) is proposed, focusing on reference selection, transfer, and reconstruction to improve identity preservation and feature transfer.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with identity preservation due to improper feature introduction. RefSTAR aims to address this by effectively incorporating features from high-quality reference images.

Method: RefSTAR includes a reference selection module (RefSel), a feature fusion paradigm for transfer, and a reconstruction mechanism. A RefSel-HQ dataset with 10,000 annotated masks is created for training.

Result: Extensive experiments show superior performance in identity preservation and reference feature transfer.

Conclusion: RefSTAR demonstrates effective blind facial image restoration with improved identity preservation and feature transfer quality.

Abstract: Blind facial image restoration is highly challenging due to unknown complex
degradations and the sensitivity of humans to faces. Although existing methods
introduce auxiliary information from generative priors or high-quality
reference images, they still struggle with identity preservation problems,
mainly due to improper feature introduction on detailed textures. In this
paper, we focus on effectively incorporating appropriate features from
high-quality reference images, presenting a novel blind facial image
restoration method that considers reference selection, transfer, and
reconstruction (RefSTAR). In terms of selection, we construct a reference
selection (RefSel) module. For training the RefSel module, we construct a
RefSel-HQ dataset through a mask generation pipeline, which contains annotating
masks for 10,000 ground truth-reference pairs. As for the transfer, due to the
trivial solution in vanilla cross-attention operations, a feature fusion
paradigm is designed to force the features from the reference to be integrated.
Finally, we propose a reference image reconstruction mechanism that further
ensures the presence of reference image features in the output image. The cycle
consistency loss is also redesigned in conjunction with the mask. Extensive
experiments on various backbone models demonstrate superior performance,
showing better identity preservation ability and reference feature transfer
quality. Source code, dataset, and pre-trained models are available at
https://github.com/yinzhicun/RefSTAR.

</details>


### [216] [GT-Loc: Unifying When and Where in Images Through a Joint Embedding Space](https://arxiv.org/abs/2507.10473)
*David G. Shatwell,Ishan Rajendrakumar Dave,Sirnam Swetha,Mubarak Shah*

Main category: cs.CV

TL;DR: GT-Loc is a retrieval-based method for jointly predicting image capture time (hour/month) and geo-location (GPS) using aligned embeddings in a shared feature space, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: Timestamp prediction relies on visual cues tied to geographic context, necessitating a joint approach with geo-localization for accurate results.

Method: GT-Loc uses separate encoders for images, time, and location, aligning embeddings in a shared space with a temporal metric-learning objective for cyclical time modeling.

Result: GT-Loc surpasses previous time prediction methods and achieves competitive geo-localization results, enabling compositional and text-based retrieval.

Conclusion: Joint optimization of time and location prediction improves accuracy, with GT-Loc setting new benchmarks and offering versatile retrieval capabilities.

Abstract: Timestamp prediction aims to determine when an image was captured using only
visual information, supporting applications such as metadata correction,
retrieval, and digital forensics. In outdoor scenarios, hourly estimates rely
on cues like brightness, hue, and shadow positioning, while seasonal changes
and weather inform date estimation. However, these visual cues significantly
depend on geographic context, closely linking timestamp prediction to
geo-localization. To address this interdependence, we introduce GT-Loc, a novel
retrieval-based method that jointly predicts the capture time (hour and month)
and geo-location (GPS coordinates) of an image. Our approach employs separate
encoders for images, time, and location, aligning their embeddings within a
shared high-dimensional feature space. Recognizing the cyclical nature of time,
instead of conventional contrastive learning with hard positives and negatives,
we propose a temporal metric-learning objective providing soft targets by
modeling pairwise time differences over a cyclical toroidal surface. We present
new benchmarks demonstrating that our joint optimization surpasses previous
time prediction methods, even those using the ground-truth geo-location as an
input during inference. Additionally, our approach achieves competitive results
on standard geo-localization tasks, and the unified embedding space facilitates
compositional and text-based image retrieval.

</details>


### [217] [Privacy-Preserving Multi-Stage Fall Detection Framework with Semi-supervised Federated Learning and Robotic Vision Confirmation](https://arxiv.org/abs/2507.10474)
*Seyed Alireza Rahimi Azghadi,Truong-Thanh-Hung Nguyen,Helene Fournier,Monica Wachowicz,Rene Richard,Francis Palma,Hung Cao*

Main category: cs.CV

TL;DR: A framework combining federated learning, indoor localization, and vision-based systems achieves 99.99% accuracy for fall detection in older adults, ensuring privacy and reliability.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of the aging population and the high risk of falls necessitate effective, reliable, and privacy-preserving fall detection systems.

Method: The framework integrates three systems: a semi-supervised federated learning-based fall detector (SF2D), an indoor localization and navigation system, and a vision-based human fall recognition system.

Result: SF2D achieves 99.19% accuracy, vision-based detection 96.3%, and navigation 95% success rate, combining for an overall 99.99% accuracy.

Conclusion: The proposed framework is highly reliable, safe, and privacy-preserving for fall detection in older adults.

Abstract: The aging population is growing rapidly, and so is the danger of falls in
older adults. A major cause of injury is falling, and detection in time can
greatly save medical expenses and recovery time. However, to provide timely
intervention and avoid unnecessary alarms, detection systems must be effective
and reliable while addressing privacy concerns regarding the user. In this
work, we propose a framework for detecting falls using several complementary
systems: a semi-supervised federated learning-based fall detection system
(SF2D), an indoor localization and navigation system, and a vision-based human
fall recognition system. A wearable device and an edge device identify a fall
scenario in the first system. On top of that, the second system uses an indoor
localization technique first to localize the fall location and then navigate a
robot to inspect the scenario. A vision-based detection system running on an
edge device with a mounted camera on a robot is used to recognize fallen
people. Each of the systems of this proposed framework achieves different
accuracy rates. Specifically, the SF2D has a 0.81% failure rate equivalent to
99.19% accuracy, while the vision-based fallen people detection achieves 96.3%
accuracy. However, when we combine the accuracy of these two systems with the
accuracy of the navigation system (95% success rate), our proposed framework
creates a highly reliable performance for fall detection, with an overall
accuracy of 99.99%. Not only is the proposed framework safe for older adults,
but it is also a privacy-preserving solution for detecting falls.

</details>


### [218] [The Power of Certainty: How Confident Models Lead to Better Segmentation](https://arxiv.org/abs/2507.10490)
*Tugberk Erol,Tuba Caglikantar,Duygu Sarikaya*

Main category: cs.CV

TL;DR: A confidence-based self-distillation method is proposed for polyp segmentation, reducing resource use while outperforming state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of large, over-parameterized deep learning models in polyp detection, such as overfitting and poor generalization, without requiring extra resources.

Method: Uses a dynamic confidence coefficient to calculate loss between previous and current iterations within a batch, eliminating the need for extra computation or memory during testing.

Result: Outperforms state-of-the-art models and generalizes well across diverse datasets from multiple clinical centers.

Conclusion: The proposed approach is effective, resource-efficient, and generalizable, with plans to release the code publicly.

Abstract: Deep learning models have been proposed for automatic polyp detection and
precise segmentation of polyps during colonoscopy procedures. Although these
state-of-the-art models achieve high performance, they often require a large
number of parameters. Their complexity can make them prone to overfitting,
particularly when trained on biased datasets, and can result in poor
generalization across diverse datasets. Knowledge distillation and
self-distillation are proposed as promising strategies to mitigate the
limitations of large, over-parameterized models. These approaches, however, are
resource-intensive, often requiring multiple models and significant memory
during training. We propose a confidence-based self-distillation approach that
outperforms state-of-the-art models by utilizing only previous iteration data
storage during training, without requiring extra computation or memory usage
during testing. Our approach calculates the loss between the previous and
current iterations within a batch using a dynamic confidence coefficient. To
evaluate the effectiveness of our approach, we conduct comprehensive
experiments on the task of polyp segmentation. Our approach outperforms
state-of-the-art models and generalizes well across datasets collected from
multiple clinical centers. The code will be released to the public once the
paper is accepted.

</details>


### [219] [BenchReAD: A systematic benchmark for retinal anomaly detection](https://arxiv.org/abs/2507.10492)
*Chenyu Lian,Hong-Yu Zhou,Zhanli Hu,Jing Qin*

Main category: cs.CV

TL;DR: The paper introduces a comprehensive benchmark for retinal anomaly detection, addressing gaps in existing methods, and proposes NFM-DRA, a new state-of-the-art approach.


<details>
  <summary>Details</summary>
Motivation: Progress in retinal anomaly detection is hindered by lack of a public benchmark, limited anomaly types, saturated test sets, and poor generalization evaluation.

Method: The paper introduces a benchmark and proposes NFM-DRA, integrating disentangled representations of abnormalities (DRA) with a Normal Feature Memory.

Result: NFM-DRA achieves state-of-the-art performance but shows drops with unseen anomalies.

Conclusion: The benchmark and NFM-DRA advance retinal anomaly detection, with the benchmark publicly available for further research.

Abstract: Retinal anomaly detection plays a pivotal role in screening ocular and
systemic diseases. Despite its significance, progress in the field has been
hindered by the absence of a comprehensive and publicly available benchmark,
which is essential for the fair evaluation and advancement of methodologies.
Due to this limitation, previous anomaly detection work related to retinal
images has been constrained by (1) a limited and overly simplistic set of
anomaly types, (2) test sets that are nearly saturated, and (3) a lack of
generalization evaluation, resulting in less convincing experimental setups.
Furthermore, existing benchmarks in medical anomaly detection predominantly
focus on one-class supervised approaches (training only with negative samples),
overlooking the vast amounts of labeled abnormal data and unlabeled data that
are commonly available in clinical practice. To bridge these gaps, we introduce
a benchmark for retinal anomaly detection, which is comprehensive and
systematic in terms of data and algorithm. Through categorizing and
benchmarking previous methods, we find that a fully supervised approach
leveraging disentangled representations of abnormalities (DRA) achieves the
best performance but suffers from significant drops in performance when
encountering certain unseen anomalies. Inspired by the memory bank mechanisms
in one-class supervised learning, we propose NFM-DRA, which integrates DRA with
a Normal Feature Memory to mitigate the performance degradation, establishing a
new SOTA. The benchmark is publicly available at
https://github.com/DopamineLcy/BenchReAD.

</details>


### [220] [Cameras as Relative Positional Encoding](https://arxiv.org/abs/2507.10496)
*Ruilong Li,Brent Yi,Junchen Liu,Hang Gao,Yi Ma,Angjoo Kanazawa*

Main category: cs.CV

TL;DR: The paper compares methods for conditioning transformers on camera geometry in multi-view vision tasks, proposing PRoPE, a new relative encoding that improves performance in tasks like novel view synthesis, stereo depth estimation, and spatial cognition.


<details>
  <summary>Details</summary>
Motivation: To enhance multi-view transformers by effectively leveraging camera geometry for 3D perception, addressing limitations in existing conditioning techniques.

Method: Comparison of token-level raymap encodings, attention-level relative pose encodings, and the proposed PRoPE, which encodes complete camera frustums (intrinsics and extrinsics) as relative positional encodings.

Result: PRoPE outperforms other methods in novel view synthesis, especially with varying intrinsics, and generalizes well to out-of-distribution inputs. Benefits extend to other tasks like stereo depth estimation and larger models.

Conclusion: PRoPE is a robust method for camera conditioning in multi-view transformers, improving performance and generalization across diverse tasks and settings.

Abstract: Transformers are increasingly prevalent for multi-view computer vision tasks,
where geometric relationships between viewpoints are critical for 3D
perception. To leverage these relationships, multi-view transformers must use
camera geometry to ground visual tokens in 3D space. In this work, we compare
techniques for conditioning transformers on cameras: token-level raymap
encodings, attention-level relative pose encodings, and a new relative encoding
we propose -- Projective Positional Encoding (PRoPE) -- that captures complete
camera frustums, both intrinsics and extrinsics, as a relative positional
encoding. Our experiments begin by showing how relative camera conditioning
improves performance in feedforward novel view synthesis, with further gains
from PRoPE. This holds across settings: scenes with both shared and varying
intrinsics, when combining token- and attention-level conditioning, and for
generalization to inputs with out-of-distribution sequence lengths and camera
intrinsics. We then verify that these benefits persist for different tasks,
stereo depth estimation and discriminative spatial cognition, as well as larger
model sizes.

</details>


### [221] [National level satellite-based crop field inventories in smallholder landscapes](https://arxiv.org/abs/2507.10499)
*Philippe Rufin,Pauline Lucie Hammer,Leon-Friedrich Thomas,S Nogueira Lisboa,Natasha Ribeiro,Almeida Sitoe,Patrick Hostert,Patrick Meyfroidt*

Main category: cs.CV

TL;DR: The paper uses high-resolution Earth observation data and deep transfer learning to map 21 million crop fields in Mozambique, achieving high accuracy and revealing insights into smallholder agriculture's spatial and socio-economic dynamics.


<details>
  <summary>Details</summary>
Motivation: To address the lack of understanding of smallholder agriculture's spatial properties, which hinders science-based sustainability policies.

Method: Integration of very high spatial resolution (1.5 m) Earth observation data with deep transfer learning for national-scale crop field delineation.

Result: Produced the first national dataset of Mozambique's 21 million fields (93% accuracy), revealing fragmented rural regions and diverse field sizes linked to socio-economic and environmental factors.

Conclusion: Field size is a critical indicator for understanding agriculture's socio-economic and environmental impacts, aiding policy design for sustainability.

Abstract: The design of science-based policies to improve the sustainability of
smallholder agriculture is challenged by a limited understanding of fundamental
system properties, such as the spatial distribution of active cropland and
field size. We integrate very high spatial resolution (1.5 m) Earth observation
data and deep transfer learning to derive crop field delineations in complex
agricultural systems at the national scale, while maintaining minimum reference
data requirements and enhancing transferability. We provide the first
national-level dataset of 21 million individual fields for Mozambique (covering
~800,000 km2) for 2023. Our maps separate active cropland from non-agricultural
land use with an overall accuracy of 93% and balanced omission and commission
errors. Field-level spatial agreement reached median intersection over union
(IoU) scores of 0.81, advancing the state-of-the-art in large-area field
delineation in complex smallholder systems. The active cropland maps capture
fragmented rural regions with low cropland shares not yet identified in global
land cover or cropland maps. These regions are mostly located in agricultural
frontier regions which host 7-9% of the Mozambican population. Field size in
Mozambique is very low overall, with half of the fields being smaller than 0.16
ha, and 83% smaller than 0.5 ha. Mean field size at aggregate spatial
resolution (0.05{\deg}) is 0.32 ha, but it varies strongly across gradients of
accessibility, population density, and net forest cover change. This variation
reflects a diverse set of actors, ranging from semi-subsistence smallholder
farms to medium-scale commercial farming, and large-scale farming operations.
Our results highlight that field size is a key indicator relating to
socio-economic and environmental outcomes of agriculture (e.g., food
production, livelihoods, deforestation, biodiversity), as well as their
trade-offs.

</details>


### [222] [Quantize-then-Rectify: Efficient VQ-VAE Training](https://arxiv.org/abs/2507.10547)
*Borui Zhang,Qihang Rao,Wenzhao Zheng,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: ReVQ transforms pre-trained VAEs into VQ-VAEs efficiently, reducing training costs by 100x while maintaining competitive image reconstruction quality.


<details>
  <summary>Details</summary>
Motivation: Training high-compression-rate VQ-VAEs is computationally expensive. This work aims to reduce costs by leveraging pre-trained VAEs.

Method: ReVQ uses channel multi-group quantization and a post rectifier to minimize quantization errors, enabling rapid VQ-VAE training.

Result: ReVQ compresses ImageNet into 512 tokens with rFID=1.06 and reduces training time to 22 hours on a single GPU, vs. 4.5 days on 32 GPUs for other methods.

Conclusion: ReVQ offers a highly efficient and cost-effective solution for VQ-VAE training, balancing performance and computational overhead.

Abstract: Visual tokenizers are pivotal in multimodal large models, acting as bridges
between continuous inputs and discrete tokens. Nevertheless, training
high-compression-rate VQ-VAEs remains computationally demanding, often
necessitating thousands of GPU hours. This work demonstrates that a pre-trained
VAE can be efficiently transformed into a VQ-VAE by controlling quantization
noise within the VAE's tolerance threshold. We present
\textbf{Quantize-then-Rectify (ReVQ)}, a framework leveraging pre-trained VAEs
to enable rapid VQ-VAE training with minimal computational overhead. By
integrating \textbf{channel multi-group quantization} to enlarge codebook
capacity and a \textbf{post rectifier} to mitigate quantization errors, ReVQ
compresses ImageNet images into at most 512 tokens while sustaining competitive
reconstruction quality (rFID = 1.06). Significantly, ReVQ reduces training
costs by over two orders of magnitude relative to state-of-the-art approaches:
ReVQ finishes full training on a single NVIDIA 4090 in approximately 22 hours,
whereas comparable methods require 4.5 days on 32 A100 GPUs. Experimental
results show that ReVQ achieves superior efficiency-reconstruction trade-offs.

</details>


### [223] [Self-supervised Learning on Camera Trap Footage Yields a Strong Universal Face Embedder](https://arxiv.org/abs/2507.10552)
*Vladimir Iashin,Horace Lee,Dan Schofield,Andrew Zisserman*

Main category: cs.CV

TL;DR: A self-supervised method using DINOv2 trains Vision Transformers on unlabeled chimpanzee face crops, outperforming supervised baselines in re-identification without labeled data.


<details>
  <summary>Details</summary>
Motivation: Manual identification of individual animals from camera-trap footage is time-consuming, creating a need for automated, scalable solutions.

Method: The study uses the DINOv2 framework to train Vision Transformers on automatically mined face crops without identity labels.

Result: The method achieves strong open-set re-identification performance, surpassing supervised baselines on benchmarks like Bossou.

Conclusion: Self-supervised learning shows promise for scalable, non-invasive wildlife monitoring and population studies.

Abstract: Camera traps are revolutionising wildlife monitoring by capturing vast
amounts of visual data; however, the manual identification of individual
animals remains a significant bottleneck. This study introduces a fully
self-supervised approach to learning robust chimpanzee face embeddings from
unlabeled camera-trap footage. Leveraging the DINOv2 framework, we train Vision
Transformers on automatically mined face crops, eliminating the need for
identity labels. Our method demonstrates strong open-set re-identification
performance, surpassing supervised baselines on challenging benchmarks such as
Bossou, despite utilising no labelled data during training. This work
underscores the potential of self-supervised learning in biodiversity
monitoring and paves the way for scalable, non-invasive population studies.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [224] [Think Clearly: Improving Reasoning via Redundant Token Pruning](https://arxiv.org/abs/2507.08806)
*Daewon Choi,Jimin Lee,Jihoon Tack,Woomin Song,Saket Dingliwal,Sai Muralidhar Jayanthi,Bhavana Ganesh,Jinwoo Shin,Aram Galstyan,Sravan Babu Bodapati*

Main category: cs.AI

TL;DR: The paper introduces a method to improve reasoning in large language models by identifying and removing redundant tokens in reasoning paths, leading to better accuracy without training.


<details>
  <summary>Details</summary>
Motivation: Large language models exhibit redundancy in reasoning paths, with scattered attention patterns, especially in incorrect answers. This redundancy hampers performance.

Method: The method measures token-level attention scores to a special end-of-thinking token, identifies redundancy, and prunes low-contributing reasoning chunks. The process resumes generation after removing redundant tokens.

Result: The method significantly improves accuracy on reasoning-intensive benchmarks, particularly in mathematical competition problems like AIME and AMC.

Conclusion: Removing reasoning redundancy enhances model performance, demonstrating the effectiveness of structured pruning in long-form reasoning tasks.

Abstract: Recent large language models have shown promising capabilities in long-form
reasoning, following structured chains of thought before arriving at a final
answer. However, we observe that these reasoning paths tend to include
substantial redundancy; analyzing attention patterns reveals that attention
scores are widely scattered, particularly incorrect answers exhibit greater
attention sparsity. In this paper, we demonstrate that deliberately removing
this redundancy in the reasoning process significantly improves performance
through clear thinking, i.e., removing distraction. Specifically, we
systematically identify reasoning redundancy by measuring token-level attention
scores to a special end-of-thinking token, which is appended to an explicit
instruction inserted to conclude each intermediate reasoning step. Furthermore,
we propose structure-aware pruning that prioritizes removing tokens in
low-contributing reasoning chunks over individual tokens. After evicting
redundant tokens, we remove the injected end-of-thinking instruction, then
resume the reasoning generation. We demonstrate that our method significantly
improves overall accuracy across reasoning-intensive benchmarks without any
training involved. In particular, our method shows strong performance on
challenging mathematical competition benchmarks such as AIME and AMC, where
reasoning redundancy is more prevalent.

</details>


### [225] [Towards Concise and Adaptive Thinking in Large Reasoning Models: A Survey](https://arxiv.org/abs/2507.09662)
*Jason Zhu,Hongyu Li*

Main category: cs.AI

TL;DR: The paper surveys methods to shorten lengthy reasoning chains in large reasoning models (LRMs) and adapt reasoning between fast and slow thinking based on input difficulty.


<details>
  <summary>Details</summary>
Motivation: LRMs generate unnecessarily long reasoning chains for trivial questions, wasting resources and increasing response time, hindering practical applications.

Method: The survey reviews methodologies, benchmarks, and challenges for concise and adaptive reasoning in LRMs.

Result: Provides a comprehensive overview of recent progress in efficient reasoning for LRMs.

Conclusion: Aims to help researchers understand the field and inspire adaptive thinking ideas for better LRM usage.

Abstract: Large reasoning models (LRMs) like OpenAI o1 and DeepSeek R1 have
demonstrated impressive performance on complex reasoning tasks like mathematics
and programming with long Chain-of-Thought (CoT) reasoning sequences
(slow-thinking), compared with traditional large language models
(fast-thinking). However, these reasoning models also face a huge challenge
that generating unnecessarily lengthy and redundant reasoning chains even for
trivial questions. This phenomenon leads to a significant waste of inference
resources, increases the response time for simple queries, and hinders the
practical application of LRMs in real-world products. To this end, it is
crucial to shorten lengthy reasoning chains and learn adaptive reasoning
between fast and slow thinking based on input difficulty. In this survey, we
provide a comprehensive overview of recent progress in concise and adaptive
thinking for efficient reasoning of LRMs, including methodologies, benchmarks,
and challenges for future exploration. We hope this survey can help researchers
quickly understand the landscape of this field and inspire novel adaptive
thinking ideas to facilitate better usage of LRMs.

</details>


### [226] [Sound and Complete Neuro-symbolic Reasoning with LLM-Grounded Interpretations](https://arxiv.org/abs/2507.09751)
*Bradley P. Allen,Prateek Chhikara,Thomas Macaulay Ferguson,Filip Ilievski,Paul Groth*

Main category: cs.AI

TL;DR: A method integrates LLMs into paraconsistent logic to address their logical inconsistency, preserving soundness and completeness while leveraging their knowledge.


<details>
  <summary>Details</summary>
Motivation: LLMs show logical inconsistency despite their strong natural language capabilities, necessitating a way to use their knowledge in formal reasoning.

Method: Directly integrate an LLM into the interpretation function of formal semantics for paraconsistent logic.

Result: Experimental evidence supports the method's feasibility using datasets from factuality benchmarks.

Conclusion: The method provides a theoretical framework for neuro-symbolic reasoning, maintaining logic properties while utilizing LLM knowledge.

Abstract: Large language models (LLMs) have demonstrated impressive capabilities in
natural language understanding and generation, but they exhibit problems with
logical consistency in the output they generate. How can we harness LLMs'
broad-coverage parametric knowledge in formal reasoning despite their
inconsistency? We present a method for directly integrating an LLM into the
interpretation function of the formal semantics for a paraconsistent logic. We
provide experimental evidence for the feasibility of the method by evaluating
the function using datasets created from several short-form factuality
benchmarks. Unlike prior work, our method offers a theoretical framework for
neuro-symbolic reasoning that leverages an LLM's knowledge while preserving the
underlying logic's soundness and completeness properties.

</details>


### [227] [On The Role of Intentionality in Knowledge Representation: Analyzing Scene Context for Cognitive Agents with a Tiny Language Model](https://arxiv.org/abs/2507.10000)
*Mark Burgess*

Main category: cs.AI

TL;DR: The paper proposes a low-cost method to assess latent intentionality in data using process coherence and scale separation, without extensive training or reasoning.


<details>
  <summary>Details</summary>
Motivation: To address the lack of practical attention to intent in science and technology, leveraging Promise Theory's Semantic Spacetime model.

Method: Uses process coherence and scale separation to identify intended content and ambient context in data, assessing anomalies and work done.

Result: An elementary, pragmatic interpretation of latent intentionality achievable with minimal computational resources.

Conclusion: The method is feasible for basic organisms and offers a scalable approach to concept formation based on agent memory capacity.

Abstract: Since Searle's work deconstructing intent and intentionality in the realm of
philosophy, the practical meaning of intent has received little attention in
science and technology. Intentionality and context are both central to the
scope of Promise Theory's model of Semantic Spacetime, used as an effective
Tiny Language Model. One can identify themes and concepts from a text, on a low
level (without knowledge of the specific language) by using process coherence
as a guide. Any agent process can assess superficially a degree of latent
`intentionality' in data by looking for anomalous multi-scale anomalies and
assessing the work done to form them. Scale separation can be used to sort
parts into `intended' content and `ambient context', using the spacetime
coherence as a measure. This offers an elementary but pragmatic interpretation
of latent intentionality for very low computational cost, and without reference
to extensive training or reasoning capabilities. The process is well within the
reach of basic organisms as it does not require large scale artificial
probabilistic batch processing. The level of concept formation depends,
however, on the memory capacity of the agent.

</details>


### [228] [Automating SPARQL Query Translations between DBpedia and Wikidata](https://arxiv.org/abs/2507.10045)
*Malte Christian Bartels,Debayan Banerjee,Ricardo Usbeck*

Main category: cs.AI

TL;DR: This paper evaluates LLMs for translating SPARQL queries between KG schemas (DBpedia-Wikidata and DBLP-OpenAlex), finding performance varies by model and prompting strategy.


<details>
  <summary>Details</summary>
Motivation: Addressing the gap in KG interoperability by assessing LLMs' ability to translate SPARQL queries.

Method: Used three LLMs (Llama-3-8B, DeepSeek-R1-Distill-Llama-70B, Mistral-Large-Instruct-2407) with zero-shot, few-shot, and chain-of-thought variants on two benchmarks.

Result: Performance varied by model and strategy; Wikidata to DBpedia translations outperformed the reverse.

Conclusion: LLMs show promise for SPARQL translation but performance depends on model and direction of translation.

Abstract: This paper investigates whether state-of-the-art Large Language Models (LLMs)
can automatically translate SPARQL between popular Knowledge Graph (KG)
schemas. We focus on translations between the DBpedia and Wikidata KG, and
later on DBLP and OpenAlex KG. This study addresses a notable gap in KG
interoperability research by rigorously evaluating LLM performance on
SPARQL-to-SPARQL translation. Two benchmarks are assembled, where the first
align 100 DBpedia-Wikidata queries from QALD-9-Plus; the second contains 100
DBLP queries aligned to OpenAlex, testing generalizability beyond encyclopaedic
KGs. Three open LLMs: Llama-3-8B, DeepSeek-R1-Distill-Llama-70B, and
Mistral-Large-Instruct-2407 are selected based on their sizes and architectures
and tested with zero-shot, few-shot, and two chain-of-thought variants. Outputs
were compared with gold answers, and resulting errors were categorized. We find
that the performance varies markedly across models and prompting strategies,
and that translations for Wikidata to DBpedia work far better than translations
for DBpedia to Wikidata.

</details>


### [229] [DeepResearch$^{\text{Eco}}$: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology](https://arxiv.org/abs/2507.10522)
*Jennifer D'Souza,Endres Keno Sander,Andrei Aioanei*

Main category: cs.AI

TL;DR: DeepResearch$^{\text{Eco}}$ is an LLM-based system for automated scientific synthesis, improving literature retrieval diversity and analytical rigor with user-controllable parameters.


<details>
  <summary>Details</summary>
Motivation: To enhance search diversity and nuance in scientific literature retrieval, addressing limitations of conventional methods.

Method: Uses recursive, depth- and breadth-controlled exploration with transparent reasoning and parameter-driven configurability.

Result: Achieves up to 21x more source integration and 14.9x more sources per 1,000 words, with expert-level depth at high settings.

Conclusion: DeepResearch$^{\text{Eco}}$ effectively improves scientific synthesis, offering scalable and rigorous literature integration.

Abstract: We introduce DeepResearch$^{\text{Eco}}$, a novel agentic LLM-based system
for automated scientific synthesis that supports recursive, depth- and
breadth-controlled exploration of original research questions -- enhancing
search diversity and nuance in the retrieval of relevant scientific literature.
Unlike conventional retrieval-augmented generation pipelines, DeepResearch
enables user-controllable synthesis with transparent reasoning and
parameter-driven configurability, facilitating high-throughput integration of
domain-specific evidence while maintaining analytical rigor. Applied to 49
ecological research questions, DeepResearch achieves up to a 21-fold increase
in source integration and a 14.9-fold rise in sources integrated per 1,000
words. High-parameter settings yield expert-level analytical depth and
contextual diversity.
  Source code available at: https://github.com/sciknoworg/deep-research.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [230] [Overview of the TREC 2023 deep learning track](https://arxiv.org/abs/2507.08890)
*Nick Craswell,Bhaskar Mitra,Emine Yilmaz,Hossein A. Rahmani,Daniel Campos,Jimmy Lin,Ellen M. Voorhees,Ian Soboroff*

Main category: cs.IR

TL;DR: The TREC Deep Learning track's fifth year focused on improving ranking tasks using MS MARCO datasets, introducing synthetic queries via T5 and GPT-4, and found LLM prompting outperformed previous methods.


<details>
  <summary>Details</summary>
Motivation: To advance ranking tasks by leveraging larger, cleaner datasets and exploring synthetic queries alongside human-annotated ones for a more challenging evaluation.

Method: Repeated last year's design with MS MARCO v2 datasets, introduced synthetic queries using T5 and GPT-4, and compared LLM prompting with traditional nnlm approaches.

Result: LLM prompting outperformed nnlm, synthetic queries yielded similar results to human queries (=0.8487), and no clear bias was observed between query types.

Conclusion: LLM-based approaches show promise for ranking tasks, and synthetic queries can complement human ones, though human oversight is needed for usability.

Abstract: This is the fifth year of the TREC Deep Learning track. As in previous years,
we leverage the MS MARCO datasets that made hundreds of thousands of
human-annotated training labels available for both passage and document ranking
tasks. We mostly repeated last year's design, to get another matching test set,
based on the larger, cleaner, less-biased v2 passage and document set, with
passage ranking as primary and document ranking as a secondary task (using
labels inferred from passage). As we did last year, we sample from MS MARCO
queries that were completely held out, unused in corpus construction, unlike
the test queries in the first three years. This approach yields a more
difficult test with more headroom for improvement. Alongside the usual MS MARCO
(human) queries from MS MARCO, this year we generated synthetic queries using a
fine-tuned T5 model and using a GPT-4 prompt.
  The new headline result this year is that runs using Large Language Model
(LLM) prompting in some way outperformed runs that use the "nnlm" approach,
which was the best approach in the previous four years. Since this is the last
year of the track, future iterations of prompt-based ranking can happen in
other tracks. Human relevance assessments were applied to all query types, not
just human MS MARCO queries. Evaluation using synthetic queries gave similar
results to human queries, with system ordering agreement of $\tau=0.8487$.
However, human effort was needed to select a subset of the synthetic queries
that were usable. We did not see clear evidence of bias, where runs using GPT-4
were favored when evaluated using synthetic GPT-4 queries, or where runs using
T5 were favored when evaluated on synthetic T5 queries.

</details>


### [231] [DS@GT at Touch: Large Language Models for Retrieval-Augmented Debate](https://arxiv.org/abs/2507.09090)
*Anthony Miyaguchi,Conor Johnston,Aaryan Potdar*

Main category: cs.IR

TL;DR: LLMs perform well in structured debates with provided arguments but are verbose. They show consistency in evaluating debate metrics.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' capabilities in structured debating and evaluating debate utterances.

Method: Deployed six leading LLMs for Retrieval-Augmented Debate and Evaluation, measuring Quality, Quantity, Manner, and Relation.

Result: LLMs perform well with related arguments but are verbose. They are consistent in evaluation.

Conclusion: LLMs are effective in structured debates but need refinement in response verbosity.

Abstract: Large Language Models (LLMs) demonstrate strong conversational abilities. In
this Working Paper, we study them in the context of debating in two ways: their
ability to perform in a structured debate along with a dataset of arguments to
use and their ability to evaluate utterances throughout the debate. We deploy
six leading publicly available models from three providers for the
Retrieval-Augmented Debate and Evaluation. The evaluation is performed by
measuring four key metrics: Quality, Quantity, Manner, and Relation. Throughout
this task, we found that although LLMs perform well in debates when given
related arguments, they tend to be verbose in responses yet consistent in
evaluation. The accompanying source code for this paper is located at
https://github.com/dsgt-arc/touche-2025-rad.

</details>


### [232] [MixLoRA-DSI: Dynamically Expandable Mixture-of-LoRA Experts for Rehearsal-Free Generative Retrieval over Dynamic Corpora](https://arxiv.org/abs/2507.09924)
*Tuan-Luc Huynh,Thuy-Trang Vu,Weiqing Wang,Trung Le,Dragan Gaevi,Yuan-Fang Li,Thanh-Toan Do*

Main category: cs.IR

TL;DR: MixLoRA-DSI is a framework for efficient generative retrieval updates using a mixture of Low-Rank Adaptation experts and OOD-driven expansion, reducing retraining costs.


<details>
  <summary>Details</summary>
Motivation: Challenges in updating model-based indexes due to high computational costs of full retraining.

Method: Combines expandable mixture of Low-Rank Adaptation experts with OOD-driven expansion for sublinear parameter growth.

Result: Outperforms full-model update baselines on NQ320k and MS MARCO Passage with lower training costs.

Conclusion: MixLoRA-DSI offers a practical solution for efficient model updates with minimal overhead.

Abstract: Continually updating model-based indexes in generative retrieval with new
documents remains challenging, as full retraining is computationally expensive
and impractical under resource constraints. We propose MixLoRA-DSI, a novel
framework that combines an expandable mixture of Low-Rank Adaptation experts
with a layer-wise out-of-distribution (OOD)-driven expansion strategy. Instead
of allocating new experts for each new corpus, our proposed expansion strategy
enables sublinear parameter growth by selectively introducing new experts only
when significant number of OOD documents are detected. Experiments on NQ320k
and MS MARCO Passage demonstrate that MixLoRA-DSI outperforms full-model update
baselines, with minimal parameter overhead and substantially lower training
costs.

</details>


### [233] [PRISM: Fine-Grained Paper-to-Paper Retrieval with Multi-Aspect-Aware Query Optimization](https://arxiv.org/abs/2507.10057)
*Sangwoo Park,Jinheon Baek,Soyeong Jeong,Sung Ju Hwang*

Main category: cs.IR

TL;DR: PRISM introduces fine-grained, aspect-specific document representations for scientific paper retrieval, outperforming baselines by 4.3%.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on sparse abstracts, limiting retrieval accuracy. PRISM aims to leverage full-paper context for better results.

Method: PRISM decomposes query papers into aspect-specific views, matches them against segmented candidate papers, and uses SciFullBench for evaluation.

Result: PRISM improves retrieval performance by 4.3% over existing baselines.

Conclusion: PRISM's fine-grained approach enhances document-to-document retrieval, demonstrating the value of full-paper context.

Abstract: Scientific paper retrieval, particularly framed as document-to-document
retrieval, aims to identify relevant papers in response to a long-form query
paper, rather than a short query string. Previous approaches to this task have
focused on abstracts, embedding them into dense vectors as surrogates for full
documents and calculating similarity across them, although abstracts provide
only sparse and high-level summaries. To address this, we propose PRISM, a
novel document-to-document retrieval method that introduces multiple,
fine-grained representations for both the query and candidate papers. In
particular, each query paper is decomposed into multiple aspect-specific views
and individually embedded, which are then matched against candidate papers
similarity segmented to consider their multifaceted dimensions. Moreover, we
present SciFullBench, a novel benchmark in which the complete and segmented
context of full papers for both queries and candidates is available. Then,
experimental results show that PRISM improves performance by an average of 4.3%
over existing retrieval baselines.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [234] [AInsight: Augmenting Expert Decision-Making with On-the-Fly Insights Grounded in Historical Data](https://arxiv.org/abs/2507.09100)
*Mohammad Abolnejadian,Shakiba Amirshahi,Matthew Brehmer,Anamaria Crisan*

Main category: cs.HC

TL;DR: Experts struggle to use historical data in real-time decision-making. A conversational interface using LLMs was tested for doctor-patient interactions, showing promise but also challenges.


<details>
  <summary>Details</summary>
Motivation: The need for experts to leverage historical data in real-time decision-making during conversations, despite its complexity and time constraints.

Method: Implemented a conversational interface with an LLM agent to retrieve and generate insights from embedded datasets (e.g., Health Canada data) during simulated doctor-patient dialogues.

Result: The prototype demonstrated effectiveness in retrieving relevant data and generating insights, though challenges were identified.

Conclusion: The approach shows potential but requires further refinement, setting the stage for future work.

Abstract: In decision-making conversations, experts must navigate complex choices and
make on-the-spot decisions while engaged in conversation. Although extensive
historical data often exists, the real-time nature of these scenarios makes it
infeasible for decision-makers to review and leverage relevant information.
This raises an interesting question: What if experts could utilize relevant
past data in real-time decision-making through insights derived from past data?
To explore this, we implemented a conversational user interface, taking
doctor-patient interactions as an example use case. Our system continuously
listens to the conversation, identifies patient problems and doctor-suggested
solutions, and retrieves related data from an embedded dataset, generating
concise insights using a pipeline built around a retrieval-based Large Language
Model (LLM) agent. We evaluated the prototype by embedding Health Canada
datasets into a vector database and conducting simulated studies using sample
doctor-patient dialogues, showing effectiveness but also challenges, setting
directions for the next steps of our work.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [235] [ESG-Net: Event-Aware Semantic Guided Network for Dense Audio-Visual Event Localization](https://arxiv.org/abs/2507.09945)
*Huilai Li,Yonghao Dang,Ying Xing,Yiming Wang,Jianqin Yin*

Main category: cs.MM

TL;DR: The paper introduces ESG-Net, a method for dense audio-visual event localization, addressing semantic gaps and event correlations through multi-stage semantic guidance and multi-event relationship modeling.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack cross-modal semantic bridging in intermediate layers and ignore event correlations, leading to difficulties in distinguishing event-related content and inferring concurrent events.

Method: ESG-Net includes an Early Semantics Interaction (ESI) module for hierarchical semantic understanding and a Mixture of Dependency Experts (MoDE) module for adaptive event dependency extraction.

Result: The method outperforms state-of-the-art techniques while reducing parameters and computational load.

Conclusion: ESG-Net effectively bridges semantic gaps and models event relationships, improving localization accuracy and efficiency.

Abstract: Dense audio-visual event localization (DAVE) aims to identify event
categories and locate the temporal boundaries in untrimmed videos. Most studies
only employ event-related semantic constraints on the final outputs, lacking
cross-modal semantic bridging in intermediate layers. This causes modality
semantic gap for further fusion, making it difficult to distinguish between
event-related content and irrelevant background content. Moreover, they rarely
consider the correlations between events, which limits the model to infer
concurrent events among complex scenarios. In this paper, we incorporate
multi-stage semantic guidance and multi-event relationship modeling, which
respectively enable hierarchical semantic understanding of audio-visual events
and adaptive extraction of event dependencies, thereby better focusing on
event-related information. Specifically, our eventaware semantic guided network
(ESG-Net) includes a early semantics interaction (ESI) module and a mixture of
dependency experts (MoDE) module. ESI applys multi-stage semantic guidance to
explicitly constrain the model in learning semantic information through
multi-modal early fusion and several classification loss functions, ensuring
hierarchical understanding of event-related content. MoDE promotes the
extraction of multi-event dependencies through multiple serial mixture of
experts with adaptive weight allocation. Extensive experiments demonstrate that
our method significantly surpasses the state-of-the-art methods, while greatly
reducing parameters and computational load. Our code will be released on
https://github.com/uchiha99999/ESG-Net.

</details>


### [236] [LayLens: Improving Deepfake Understanding through Simplified Explanations](https://arxiv.org/abs/2507.10066)
*Abhijeet Narang,Parul Gupta,Liuyijia Su,Abhinav Dhall*

Main category: cs.MM

TL;DR: LayLens is a tool simplifying deepfake understanding for all users by combining explainable detection, simplified language, and visual reconstruction.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between technical deepfake explanations and human understanding, making forensics more accessible.

Method: A three-stage pipeline: explainable detection, natural language simplification, and visual reconstruction.

Result: User study showed improved clarity, reduced cognitive load, and increased confidence in identifying deepfakes.

Conclusion: LayLens advances transparent and user-friendly deepfake forensics.

Abstract: This demonstration paper presents $\mathbf{LayLens}$, a tool aimed to make
deepfake understanding easier for users of all educational backgrounds. While
prior works often rely on outputs containing technical jargon, LayLens bridges
the gap between model reasoning and human understanding through a three-stage
pipeline: (1) explainable deepfake detection using a state-of-the-art forgery
localization model, (2) natural language simplification of technical
explanations using a vision-language model, and (3) visual reconstruction of a
plausible original image via guided image editing. The interface presents both
technical and layperson-friendly explanations in addition to a side-by-side
comparison of the uploaded and reconstructed images. A user study with 15
participants shows that simplified explanations significantly improve clarity
and reduce cognitive load, with most users expressing increased confidence in
identifying deepfakes. LayLens offers a step toward transparent, trustworthy,
and user-centric deepfake forensics.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [237] [TRACER: Efficient Object Re-Identification in Networked Cameras through Adaptive Query Processing](https://arxiv.org/abs/2507.09448)
*Pramod Chunduri,Yao Lu,Joy Arulraj*

Main category: cs.DB

TL;DR: Tracer is a new VDBMS for Re-ID queries, improving accuracy and recall with adaptive processing and a probabilistic search model, outperforming Spatula by 3.9x.


<details>
  <summary>Details</summary>
Motivation: Spatula's limitations in accuracy and recall for Re-ID queries in large camera networks motivate the need for Tracer.

Method: Tracer uses a recurrent network for long-term correlations and a probabilistic adaptive search model for high recall.

Result: Tracer outperforms Spatula by 3.9x on average across datasets.

Conclusion: Tracer addresses Spatula's shortcomings and provides a scalable solution for Re-ID tasks, supported by a synthetic benchmark.

Abstract: Efficiently re-identifying and tracking objects across a network of cameras
is crucial for applications like traffic surveillance. Spatula is the
state-of-the-art video database management system (VDBMS) for processing Re-ID
queries. However, it suffers from two limitations. Its spatio-temporal
filtering scheme has limited accuracy on large camera networks due to localized
camera history. It is not suitable for critical video analytics applications
that require high recall due to a lack of support for adaptive query
processing.
  In this paper, we present Tracer, a novel VDBMS for efficiently processing
Re-ID queries using an adaptive query processing framework. Tracer selects the
optimal camera to process at each time step by training a recurrent network to
model long-term historical correlations. To accelerate queries under a high
recall constraint, Tracer incorporates a probabilistic adaptive search model
that processes camera feeds in incremental search windows and dynamically
updates the sampling probabilities using an exploration-exploitation strategy.
To address the paucity of benchmarks for the Re-ID task due to privacy
concerns, we present a novel synthetic benchmark for generating multi-camera
Re-ID datasets based on real-world traffic distribution. Our evaluation shows
that Tracer outperforms the state-of-the-art cross-camera analytics system by
3.9x on average across diverse datasets.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [238] [Multi-omic Prognosis of Alzheimer's Disease with Asymmetric Cross-Modal Cross-Attention Network](https://arxiv.org/abs/2507.08855)
*Yang Ming,Jiang Shi Zhong,Zhou Su Juan*

Main category: eess.IV

TL;DR: A novel deep learning framework using asymmetric cross-modal cross-attention for Alzheimer's Disease diagnosis, achieving 94.88% accuracy by fusing multimodal data like PET, MRI, genetic, and clinical data.


<details>
  <summary>Details</summary>
Motivation: Traditional methods fail to effectively utilize complementary multimodal data, leading to information loss during fusion.

Method: Proposes an asymmetric cross-modal cross-attention mechanism to capture key interactions between multimodal features.

Result: Achieves 94.88% accuracy, outperforming traditional unimodal and multimodal models.

Conclusion: The asymmetric cross-modal cross-attention mechanism effectively improves AD diagnosis accuracy by better fusing multimodal data.

Abstract: Alzheimer's Disease (AD) is an irreversible neurodegenerative disease
characterized by progressive cognitive decline as its main symptom. In the
research field of deep learning-assisted diagnosis of AD, traditional
convolutional neural networks and simple feature concatenation methods fail to
effectively utilize the complementary information between multimodal data, and
the simple feature concatenation approach is prone to cause the loss of key
information during the process of modal fusion. In recent years, the
development of deep learning technology has brought new possibilities for
solving the problem of how to effectively fuse multimodal features. This paper
proposes a novel deep learning algorithm framework to assist medical
professionals in AD diagnosis. By fusing medical multi-view information such as
brain fluorodeoxyglucose positron emission tomography (PET), magnetic resonance
imaging (MRI), genetic data, and clinical data, it can accurately detect the
presence of AD, Mild Cognitive Impairment (MCI), and Cognitively Normal (CN).
The innovation of the algorithm lies in the use of an asymmetric cross-modal
cross-attention mechanism, which can effectively capture the key information
features of the interactions between different data modal features. This paper
compares the asymmetric cross-modal cross-attention mechanism with the
traditional algorithm frameworks of unimodal and multimodal deep learning
models for AD diagnosis, and evaluates the importance of the asymmetric
cross-modal cross-attention mechanism. The algorithm model achieves an accuracy
of 94.88% on the test set.

</details>


### [239] [Interpretable Artificial Intelligence for Detecting Acute Heart Failure on Acute Chest CT Scans](https://arxiv.org/abs/2507.08952)
*Silas Nyboe rting,Kristina Miger,Anne Sophie Overgaard Olesen,Mikael Ploug Boesen,Michael Brun Andersen,Jens Petersen,Olav W. Nielsen,Marleen de Bruijne*

Main category: eess.IV

TL;DR: An explainable AI model was developed to detect acute heart failure (AHF) in chest CT scans, achieving accuracy comparable to radiologists and aiding emergency decision-making.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in interpreting chest CT scans for AHF due to radiologist shortages and delayed reports, AI can enhance diagnostic precision.

Method: A Boosted Trees model was trained on segmented cardiac and pulmonary structures from CT scans, using Shapley values for explainability.

Result: The model achieved an AUC of 0.87, with misclassifications often due to errors in initial radiology reports.

Conclusion: The AI model offers transparent, accurate predictions for AHF, supporting clinical decision-making.

Abstract: Introduction: Chest CT scans are increasingly used in dyspneic patients where
acute heart failure (AHF) is a key differential diagnosis. Interpretation
remains challenging and radiology reports are frequently delayed due to a
radiologist shortage, although flagging such information for emergency
physicians would have therapeutic implication. Artificial intelligence (AI) can
be a complementary tool to enhance the diagnostic precision. We aim to develop
an explainable AI model to detect radiological signs of AHF in chest CT with an
accuracy comparable to thoracic radiologists.
  Methods: A single-center, retrospective study during 2016-2021 at Copenhagen
University Hospital - Bispebjerg and Frederiksberg, Denmark. A Boosted Trees
model was trained to predict AHF based on measurements of segmented cardiac and
pulmonary structures from acute thoracic CT scans. Diagnostic labels for
training and testing were extracted from radiology reports. Structures were
segmented with TotalSegmentator. Shapley Additive explanations values were used
to explain the impact of each measurement on the final prediction.
  Results: Of the 4,672 subjects, 49% were female. The final model incorporated
twelve key features of AHF and achieved an area under the ROC of 0.87 on the
independent test set. Expert radiologist review of model misclassifications
found that 24 out of 64 (38%) false positives and 24 out of 61 (39%) false
negatives were actually correct model predictions, with the errors originating
from inaccuracies in the initial radiology reports.
  Conclusion: We developed an explainable AI model with strong discriminatory
performance, comparable to thoracic radiologists. The AI model's stepwise,
transparent predictions may support decision-making.

</details>


### [240] [VIP: Visual Information Protection through Adversarial Attacks on Vision-Language Models](https://arxiv.org/abs/2507.08982)
*Hanene F. Z. Brachemi Meftah,Wassim Hamidouche,Sid Ahmed Fezza,Olivier Dforges*

Main category: eess.IV

TL;DR: A novel adversarial attack method for Vision-Language Models (VLMs) selectively conceals sensitive image regions to protect privacy while preserving overall image semantics.


<details>
  <summary>Details</summary>
Motivation: Address privacy concerns in VLMs by preventing inadvertent processing of private visual information.

Method: Propose an adversarial attack strategy that masks designated ROIs in images, ensuring VLMs cannot access sensitive content without disrupting the rest of the image.

Result: Achieves up to 98% reduction in detecting targeted ROIs across VLMs like LLaVA, Instruct-BLIP, and BLIP2-T5, while maintaining high semantic similarity.

Conclusion: The work promotes privacy-conscious use of VLMs and provides a practical tool for further research, with publicly available code.

Abstract: Recent years have witnessed remarkable progress in developing Vision-Language
Models (VLMs) capable of processing both textual and visual inputs. These
models have demonstrated impressive performance, leading to their widespread
adoption in various applications. However, this widespread raises serious
concerns regarding user privacy, particularly when models inadvertently process
or expose private visual information. In this work, we frame the preservation
of privacy in VLMs as an adversarial attack problem. We propose a novel attack
strategy that selectively conceals information within designated Region Of
Interests (ROIs) in an image, effectively preventing VLMs from accessing
sensitive content while preserving the semantic integrity of the remaining
image. Unlike conventional adversarial attacks that often disrupt the entire
image, our method maintains high coherence in unmasked areas. Experimental
results across three state-of-the-art VLMs namely LLaVA, Instruct-BLIP, and
BLIP2-T5 demonstrate up to 98% reduction in detecting targeted ROIs, while
maintaining global image semantics intact, as confirmed by high similarity
scores between clean and adversarial outputs. We believe that this work
contributes to a more privacy conscious use of multimodal models and offers a
practical tool for further research, with the source code publicly available
at: https://github.com/hbrachemi/Vlm_defense-attack.

</details>


### [241] [Automatic Contouring of Spinal Vertebrae on X-Ray using a Novel Sandwich U-Net Architecture](https://arxiv.org/abs/2507.09158)
*Sunil Munthumoduku Krishna Murthy,Kumar Rajamani,Srividya Tirunellai Rajamani,Yupei Li,Qiyang Sun,Bjoern W. Schuller*

Main category: eess.IV

TL;DR: A novel U-Net variation with a 'sandwich' structure and dual activation functions improves thoracic vertebrae segmentation in X-Ray images, outperforming baseline U-Net by 4.1% in Dice score.


<details>
  <summary>Details</summary>
Motivation: Manual vertebral contouring in spinal mobility disease is laborious and error-prone, necessitating automated methods for efficiency and accuracy.

Method: Proposed a 'sandwich' U-Net with dual activation functions for segmenting thoracic vertebrae in X-Ray images.

Result: Achieved a 4.1% improvement in Dice score over baseline U-Net, enhancing segmentation accuracy.

Conclusion: The automated approach offers a reliable and efficient solution for vertebral contouring in mobility disease analysis.

Abstract: In spinal vertebral mobility disease, accurately extracting and contouring
vertebrae is essential for assessing mobility impairments and monitoring
variations during flexion-extension movements. Precise vertebral contouring
plays a crucial role in surgical planning; however, this process is
traditionally performed manually by radiologists or surgeons, making it
labour-intensive, time-consuming, and prone to human error. In particular,
mobility disease analysis requires the individual contouring of each vertebra,
which is both tedious and susceptible to inconsistencies. Automated methods
provide a more efficient alternative, enabling vertebra identification,
segmentation, and contouring with greater accuracy and reduced time
consumption. In this study, we propose a novel U-Net variation designed to
accurately segment thoracic vertebrae from anteroposterior view on X-Ray
images. Our proposed approach, incorporating a ``sandwich" U-Net structure with
dual activation functions, achieves a 4.1\% improvement in Dice score compared
to the baseline U-Net model, enhancing segmentation accuracy while ensuring
reliable vertebral contour extraction.

</details>


### [242] [PanoDiff-SR: Synthesizing Dental Panoramic Radiographs using Diffusion and Super-resolution](https://arxiv.org/abs/2507.09227)
*Sanyam Jain,Bruna Neves de Freitas,Andreas Basse-OConnor,Alexandros Iosifidis,Ruben Pauwels*

Main category: eess.IV

TL;DR: The paper proposes a method combining diffusion-based generation (PanoDiff) and Super-Resolution (SR) to create high-quality synthetic dental panoramic radiographs (PRs), addressing dataset scarcity and educational needs.


<details>
  <summary>Details</summary>
Motivation: To mitigate the scarcity of public medical datasets and support AI research and education by generating realistic synthetic dental images.

Method: Uses PanoDiff to generate low-resolution PRs (256x128) and a transformer-based SR model to upscale them to high-resolution (1024x512), focusing on local-global relationships for sharper details.

Result: Achieved a Frechet inception distance score of 40.69 and inception scores of 2.55-2.98. Clinical experts had 68.5% accuracy in distinguishing real from synthetic images.

Conclusion: The proposed method effectively generates realistic synthetic PRs, validated by quantitative metrics and expert evaluation.

Abstract: There has been increasing interest in the generation of high-quality,
realistic synthetic medical images in recent years. Such synthetic datasets can
mitigate the scarcity of public datasets for artificial intelligence research,
and can also be used for educational purposes. In this paper, we propose a
combination of diffusion-based generation (PanoDiff) and Super-Resolution (SR)
for generating synthetic dental panoramic radiographs (PRs). The former
generates a low-resolution (LR) seed of a PR (256 X 128) which is then
processed by the SR model to yield a high-resolution (HR) PR of size 1024 X
512. For SR, we propose a state-of-the-art transformer that learns local-global
relationships, resulting in sharper edges and textures. Experimental results
demonstrate a Frechet inception distance score of 40.69 between 7243 real and
synthetic images (in HR). Inception scores were 2.55, 2.30, 2.90 and 2.98 for
real HR, synthetic HR, real LR and synthetic LR images, respectively. Among a
diverse group of six clinical experts, all evaluating a mixture of 100
synthetic and 100 real PRs in a time-limited observation, the average accuracy
in distinguishing real from synthetic images was 68.5% (with 50% corresponding
to random guessing).

</details>


### [243] [prNet: Data-Driven Phase Retrieval via Stochastic Refinement](https://arxiv.org/abs/2507.09608)
*Mehmet Onurcan Kaya,Figen S. Oktem*

Main category: eess.IV

TL;DR: A novel phase retrieval framework using Langevin dynamics for efficient posterior sampling, balancing distortion and perceptual quality.


<details>
  <summary>Details</summary>
Motivation: Address the perception-distortion tradeoff in phase retrieval, moving beyond pixel-wise accuracy.

Method: Combines stochastic sampling, learned denoising, and model-based updates with three variants of increasing complexity.

Result: Achieves state-of-the-art performance in fidelity and perceptual quality across benchmarks.

Conclusion: The framework effectively balances distortion and perceptual quality, outperforming conventional methods.

Abstract: We propose a novel framework for phase retrieval that leverages Langevin
dynamics to enable efficient posterior sampling, yielding reconstructions that
explicitly balance distortion and perceptual quality. Unlike conventional
approaches that prioritize pixel-wise accuracy, our method navigates the
perception-distortion tradeoff through a principled combination of stochastic
sampling, learned denoising, and model-based updates. The framework comprises
three variants of increasing complexity, integrating theoretically grounded
Langevin inference, adaptive noise schedule learning, parallel reconstruction
sampling, and warm-start initialization from classical solvers. Extensive
experiments demonstrate that our method achieves state-of-the-art performance
across multiple benchmarks, both in terms of fidelity and perceptual quality.

</details>


### [244] [I2I-PR: Deep Iterative Refinement for Phase Retrieval using Image-to-Image Diffusion Models](https://arxiv.org/abs/2507.09609)
*Mehmet Onurcan Kaya,Figen S. Oktem*

Main category: eess.IV

TL;DR: A novel phase retrieval method using image-to-image diffusion models improves reconstruction quality and efficiency, outperforming classical techniques.


<details>
  <summary>Details</summary>
Motivation: Phase retrieval is critical in imaging and microscopy but suffers from sensitivity to initialization and noise. Recent advances in diffusion models offer potential solutions.

Method: Combines hybrid iterative techniques (Hybrid Input-Output and Error Reduction) with a learned image-to-image diffusion framework (Inversion by Direct Iteration) for robust initialization and refinement.

Result: Achieves better training efficiency and reconstruction quality, surpassing classical and contemporary methods.

Conclusion: The approach is effective and efficient for phase retrieval, with broad applicability.

Abstract: Phase retrieval involves recovering a signal from intensity-only
measurements, crucial in many fields such as imaging, holography, optical
computing, crystallography, and microscopy. Although there are several
well-known phase retrieval algorithms, including classical iterative solvers,
the reconstruction performance often remains sensitive to initialization and
measurement noise. Recently, image-to-image diffusion models have gained
traction in various image reconstruction tasks, yielding significant
theoretical insights and practical breakthroughs. In this work, we introduce a
novel phase retrieval approach based on an image-to-image diffusion framework
called Inversion by Direct Iteration. Our method begins with an enhanced
initialization stage that leverages a hybrid iterative technique, combining the
Hybrid Input-Output and Error Reduction methods and incorporating a novel
acceleration mechanism to obtain a robust crude estimate. Then, it iteratively
refines this initial crude estimate using the learned image-to-image pipeline.
Our method achieves substantial improvements in both training efficiency and
reconstruction quality. Furthermore, our approach utilizes aggregation
techniques to refine quality metrics and demonstrates superior results compared
to both classical and contemporary techniques. This highlights its potential
for effective and efficient phase retrieval across various applications.

</details>


### [245] [Pre-trained Under Noise: A Framework for Robust Bone Fracture Detection in Medical Imaging](https://arxiv.org/abs/2507.09731)
*Robby Hoover,Nelly Elsayed,Zag ElSayed,Chengcheng Li*

Main category: eess.IV

TL;DR: This paper evaluates the robustness of pre-trained deep learning models (ResNet50, VGG16, EfficientNetv2) for classifying bone fractures in X-ray images under varying noise conditions, aiming to address healthcare disparities.


<details>
  <summary>Details</summary>
Motivation: To understand how noise affects bone fracture detection and model performance, replicating real-world challenges in medical imaging.

Method: Tested three pre-trained models on progressively degraded X-ray images with noise, using transfer learning and controlled noise augmentation.

Result: Provides insights into the robustness and generalizability of deep learning models in different contexts.

Conclusion: Establishes a framework for assessing AI model degradation in medical imaging, offering practical guidance for real-world applications.

Abstract: Medical Imagings are considered one of the crucial diagnostic tools for
different bones-related diseases, especially bones fractures. This paper
investigates the robustness of pre-trained deep learning models for classifying
bone fractures in X-ray images and seeks to address global healthcare disparity
through the lens of technology. Three deep learning models have been tested
under varying simulated equipment quality conditions. ResNet50, VGG16 and
EfficientNetv2 are the three pre-trained architectures which are compared.
These models were used to perform bone fracture classification as images were
progressively degraded using noise. This paper specifically empirically studies
how the noise can affect the bone fractures detection and how the pre-trained
models performance can be changes due to the noise that affect the quality of
the X-ray images. This paper aims to help replicate real world challenges
experienced by medical imaging technicians across the world. Thus, this paper
establishes a methodological framework for assessing AI model degradation using
transfer learning and controlled noise augmentation. The findings provide
practical insight into how robust and generalizable different pre-trained deep
learning powered computer vision models can be when used in different contexts.

</details>


### [246] [AI-Enhanced Pediatric Pneumonia Detection: A CNN-Based Approach Using Data Augmentation and Generative Adversarial Networks (GANs)](https://arxiv.org/abs/2507.09759)
*Abdul Manaf,Nimra Mughal*

Main category: eess.IV

TL;DR: A CNN-based system using augmented and GAN-generated data improves pediatric pneumonia diagnosis from chest X-rays.


<details>
  <summary>Details</summary>
Motivation: Pneumonia is a leading cause of child mortality, and accurate diagnosis from chest X-rays is critical, especially in resource-limited settings.

Method: Trained a CNN on 5,863 labeled X-ray images, applied augmentation (rotation, zooming, shear, flipping), and used GANs for synthetic data to address class imbalance.

Result: Achieved optimal performance with combined original, augmented, and GAN-generated data, evaluated via accuracy and F1 score. Deployed via a Flask web app for real-time classification.

Conclusion: Deep learning and GANs enhance diagnostic accuracy and efficiency for pediatric pneumonia, valuable in clinical settings.

Abstract: Pneumonia is a leading cause of mortality in children under five, requiring
accurate chest X-ray diagnosis. This study presents a machine learning-based
Pediatric Chest Pneumonia Classification System to assist healthcare
professionals in diagnosing pneumonia from chest X-ray images. The CNN-based
model was trained on 5,863 labeled chest X-ray images from children aged 0-5
years from the Guangzhou Women and Children's Medical Center. To address
limited data, we applied augmentation techniques (rotation, zooming, shear,
horizontal flipping) and employed GANs to generate synthetic images, addressing
class imbalance. The system achieved optimal performance using combined
original, augmented, and GAN-generated data, evaluated through accuracy and F1
score metrics. The final model was deployed via a Flask web application,
enabling real-time classification with probability estimates. Results
demonstrate the potential of deep learning and GANs in improving diagnostic
accuracy and efficiency for pediatric pneumonia classification, particularly
valuable in resource-limited clinical settings
https://github.com/AbdulManaf12/Pediatric-Chest-Pneumonia-Classification

</details>


### [247] [Resolution Revolution: A Physics-Guided Deep Learning Framework for Spatiotemporal Temperature Reconstruction](https://arxiv.org/abs/2507.09872)
*Shengjie Liu,Lu Zhang,Siqin Wang*

Main category: eess.IV

TL;DR: A physics-guided deep learning framework integrates coarse Earth system model data with high-resolution satellite observations to reconstruct high spatiotemporal temperature data.


<details>
  <summary>Details</summary>
Motivation: Address the gap in high-resolution temperature data due to trade-offs between spatial and temporal resolution in Earth observation.

Method: Uses a convolutional neural network incorporating the annual temperature cycle and a linear term to refine coarse model output with satellite data.

Result: Effective temperature reconstruction demonstrated using GOES-16 and Landsat data across four datasets.

Conclusion: The framework enables high-resolution temperature data generation globally, under all weather conditions.

Abstract: Central to Earth observation is the trade-off between spatial and temporal
resolution. For temperature, this is especially critical because real-world
applications require high spatiotemporal resolution data. Current technology
allows for hourly temperature observations at 2 km, but only every 16 days at
100 m, a gap further exacerbated by cloud cover. Earth system models offer
continuous hourly temperature data, but at a much coarser spatial resolution
(9-31 km). Here, we present a physics-guided deep learning framework for
temperature data reconstruction that integrates these two data sources. The
proposed framework uses a convolutional neural network that incorporates the
annual temperature cycle and includes a linear term to amplify the coarse Earth
system model output into fine-scale temperature values observed from
satellites. We evaluated this framework using data from two satellites, GOES-16
(2 km, hourly) and Landsat (100 m, every 16 days), and demonstrated effective
temperature reconstruction with hold-out and in situ data across four datasets.
This physics-guided deep learning framework opens new possibilities for
generating high-resolution temperature data across spatial and temporal scales,
under all weather conditions and globally.

</details>


### [248] [Advanced U-Net Architectures with CNN Backbones for Automated Lung Cancer Detection and Segmentation in Chest CT Images](https://arxiv.org/abs/2507.09898)
*Alireza Golkarieha,Kiana Kiashemshakib,Sajjad Rezvani Boroujenic,Nasibeh Asadi Isakand*

Main category: eess.IV

TL;DR: U-Net with CNN backbones (ResNet50, VGG16, Xception) improves lung cancer detection and segmentation in CT scans, outperforming prior methods with high accuracy and Dice scores.


<details>
  <summary>Details</summary>
Motivation: Addressing the need for accurate diagnostic tools for lung cancer in clinical settings.

Method: Used U-Net with ResNet50, VGG16, and Xception backbones for segmentation, followed by CNN and hybrid classifiers (SVM, Random Forest, Gradient Boosting) evaluated via 5-fold cross-validation.

Result: U-Net with ResNet50 achieved best segmentation for cancerous lungs (Dice: 0.9495, Accuracy: 0.9735), while U-Net with Xception excelled in classification (99.1% accuracy). Hybrid CNN-SVM-Xception also performed well (96.7% accuracy).

Conclusion: Combining U-Net with advanced CNN backbones enhances lung cancer detection and segmentation, aiding early diagnosis and clinical decisions.

Abstract: This study investigates the effectiveness of U-Net architectures integrated
with various convolutional neural network (CNN) backbones for automated lung
cancer detection and segmentation in chest CT images, addressing the critical
need for accurate diagnostic tools in clinical settings. A balanced dataset of
832 chest CT images (416 cancerous and 416 non-cancerous) was preprocessed
using Contrast Limited Adaptive Histogram Equalization (CLAHE) and resized to
128x128 pixels. U-Net models were developed with three CNN backbones: ResNet50,
VGG16, and Xception, to segment lung regions. After segmentation, CNN-based
classifiers and hybrid models combining CNN feature extraction with traditional
machine learning classifiers (Support Vector Machine, Random Forest, and
Gradient Boosting) were evaluated using 5-fold cross-validation. Metrics
included accuracy, precision, recall, F1-score, Dice coefficient, and ROC-AUC.
U-Net with ResNet50 achieved the best performance for cancerous lungs (Dice:
0.9495, Accuracy: 0.9735), while U-Net with VGG16 performed best for
non-cancerous segmentation (Dice: 0.9532, Accuracy: 0.9513). For
classification, the CNN model using U-Net with Xception achieved 99.1 percent
accuracy, 99.74 percent recall, and 99.42 percent F1-score. The hybrid
CNN-SVM-Xception model achieved 96.7 percent accuracy and 97.88 percent
F1-score. Compared to prior methods, our framework consistently outperformed
existing models. In conclusion, combining U-Net with advanced CNN backbones
provides a powerful method for both segmentation and classification of lung
cancer in CT scans, supporting early diagnosis and clinical decision-making.

</details>


### [249] [IM-LUT: Interpolation Mixing Look-Up Tables for Image Super-Resolution](https://arxiv.org/abs/2507.09923)
*Sejin Park,Sangmin Lee,Kyong Hwan Jin,Seung-Won Jung*

Main category: eess.IV

TL;DR: The paper introduces IM-LUT, a novel framework for arbitrary-scale image super-resolution (ASISR) that blends interpolation functions efficiently using LUTs, outperforming existing methods in quality and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing LUT-based SR methods are limited to fixed scales, while ASISR techniques using implicit neural representations are computationally expensive. IM-LUT addresses these issues by combining interpolation functions for efficient ASISR.

Method: The proposed IM-LUT framework uses IM-Net to predict mixing weights for interpolation functions based on local image patterns and target scales, then replaces costly operations with LUTs for efficient CPU inference.

Result: IM-LUT achieves a superior balance between image quality and efficiency on benchmark datasets, outperforming existing methods.

Conclusion: IM-LUT is a promising solution for resource-constrained ASISR applications, offering lightweight and fast inference without compromising quality.

Abstract: Super-resolution (SR) has been a pivotal task in image processing, aimed at
enhancing image resolution across various applications. Recently, look-up table
(LUT)-based approaches have attracted interest due to their efficiency and
performance. However, these methods are typically designed for fixed scale
factors, making them unsuitable for arbitrary-scale image SR (ASISR). Existing
ASISR techniques often employ implicit neural representations, which come with
considerable computational cost and memory demands. To address these
limitations, we propose Interpolation Mixing LUT (IM-LUT), a novel framework
that operates ASISR by learning to blend multiple interpolation functions to
maximize their representational capacity. Specifically, we introduce IM-Net, a
network trained to predict mixing weights for interpolation functions based on
local image patterns and the target scale factor. To enhance efficiency of
interpolation-based methods, IM-Net is transformed into IM-LUT, where LUTs are
employed to replace computationally expensive operations, enabling lightweight
and fast inference on CPUs while preserving reconstruction quality.
Experimental results on several benchmark datasets demonstrate that IM-LUT
consistently achieves a superior balance between image quality and efficiency
compared to existing methods, highlighting its potential as a promising
solution for resource-constrained applications.

</details>


### [250] [A Brain Tumor Segmentation Method Based on CLIP and 3D U-Net with Cross-Modal Semantic Guidance and Multi-Level Feature Fusion](https://arxiv.org/abs/2507.09966)
*Mingda Zhang*

Main category: eess.IV

TL;DR: A multi-level fusion architecture combining pixel, feature, and semantic-level information improves brain tumor segmentation in MRI, outperforming traditional methods by 4.8% in Dice coefficient.


<details>
  <summary>Details</summary>
Motivation: Current methods for brain tumor segmentation in MRI rely heavily on visual features and neglect semantic knowledge from medical reports, limiting accuracy.

Method: The proposed model integrates 3D U-Net with CLIP for semantic understanding, using 3D-2D bridging, cross-modal guidance, and semantic-based attention.

Result: Achieves a Dice coefficient of 0.8567 (4.8% better than 3D U-Net) and a 7.3% improvement in enhancing tumor segmentation.

Conclusion: Semantic-level fusion significantly enhances brain tumor segmentation, demonstrating the value of integrating multimodal data.

Abstract: Precise segmentation of brain tumors from magnetic resonance imaging (MRI) is
essential for neuro-oncology diagnosis and treatment planning. Despite advances
in deep learning methods, automatic segmentation remains challenging due to
tumor morphological heterogeneity and complex three-dimensional spatial
relationships. Current techniques primarily rely on visual features extracted
from MRI sequences while underutilizing semantic knowledge embedded in medical
reports. This research presents a multi-level fusion architecture that
integrates pixel-level, feature-level, and semantic-level information,
facilitating comprehensive processing from low-level data to high-level
concepts. The semantic-level fusion pathway combines the semantic understanding
capabilities of Contrastive Language-Image Pre-training (CLIP) models with the
spatial feature extraction advantages of 3D U-Net through three mechanisms:
3D-2D semantic bridging, cross-modal semantic guidance, and semantic-based
attention mechanisms. Experimental validation on the BraTS 2020 dataset
demonstrates that the proposed model achieves an overall Dice coefficient of
0.8567, representing a 4.8% improvement compared to traditional 3D U-Net, with
a 7.3% Dice coefficient increase in the clinically important enhancing tumor
(ET) region.

</details>


### [251] [Graph-based Multi-Modal Interaction Lightweight Network for Brain Tumor Segmentation (GMLN-BTS) in Edge Iterative MRI Lesion Localization System (EdgeIMLocSys)](https://arxiv.org/abs/2507.09995)
*Guohao Huo,Ruiting Dai,Hao Tang*

Main category: eess.IV

TL;DR: EdgeIMLocSys integrates human feedback for adaptive brain tumor segmentation, using GMLN-BTS for efficient, high-accuracy results.


<details>
  <summary>Details</summary>
Motivation: Variability in MRI quality across scanners challenges model generalization, requiring adaptive solutions.

Method: Proposes EdgeIMLocSys with GMLN-BTS, featuring M2AE, G2MCIM, and VRUM for efficient, artifact-free segmentation.

Result: Achieves 85.1% Dice score on BraTS2017 with 4.58M parameters, outperforming lightweight models.

Conclusion: Demonstrates high-accuracy, resource-efficient segmentation for clinical use.

Abstract: Brain tumor segmentation plays a critical role in clinical diagnosis and
treatment planning, yet the variability in imaging quality across different MRI
scanners presents significant challenges to model generalization. To address
this, we propose the Edge Iterative MRI Lesion Localization System
(EdgeIMLocSys), which integrates Continuous Learning from Human Feedback to
adaptively fine-tune segmentation models based on clinician feedback, thereby
enhancing robustness to scanner-specific imaging characteristics. Central to
this system is the Graph-based Multi-Modal Interaction Lightweight Network for
Brain Tumor Segmentation (GMLN-BTS), which employs a Modality-Aware Adaptive
Encoder (M2AE) to extract multi-scale semantic features efficiently, and a
Graph-based Multi-Modal Collaborative Interaction Module (G2MCIM) to model
complementary cross-modal relationships via graph structures. Additionally, we
introduce a novel Voxel Refinement UpSampling Module (VRUM) that
synergistically combines linear interpolation and multi-scale transposed
convolutions to suppress artifacts while preserving high-frequency details,
improving segmentation boundary accuracy. Our proposed GMLN-BTS model achieves
a Dice score of 85.1% on the BraTS2017 dataset with only 4.58 million
parameters, representing a 98% reduction compared to mainstream 3D Transformer
models, and significantly outperforms existing lightweight approaches. This
work demonstrates a synergistic breakthrough in achieving high-accuracy,
resource-efficient brain tumor segmentation suitable for deployment in
resource-constrained clinical environments.

</details>


### [252] [DepViT-CAD: Deployable Vision Transformer-Based Cancer Diagnosis in Histopathology](https://arxiv.org/abs/2507.10250)
*Ashkan Shakarami,Lorenzo Nicole,Rocco Cappellesso,Angelo Paolo Dei Tos,Stefano Ghidoni*

Main category: eess.IV

TL;DR: DepViT-CAD is an AI system for multi-class cancer diagnosis using a novel Multi-Attention Vision Transformer (MAViT), validated on large datasets with high sensitivity.


<details>
  <summary>Details</summary>
Motivation: Accurate and timely cancer diagnosis from histopathological slides is crucial for clinical decision-making.

Method: The system uses MAViT, trained on expert-annotated patches from 1008 whole-slide images, covering 11 diagnostic categories.

Result: Achieved diagnostic sensitivities of 94.11% and 92% on two independent cohorts.

Conclusion: DepViT-CAD provides a robust, scalable AI-assisted diagnostic approach, with software and code made publicly available for transparency.

Abstract: Accurate and timely cancer diagnosis from histopathological slides is vital
for effective clinical decision-making. This paper introduces DepViT-CAD, a
deployable AI system for multi-class cancer diagnosis in histopathology. At its
core is MAViT, a novel Multi-Attention Vision Transformer designed to capture
fine-grained morphological patterns across diverse tumor types. MAViT was
trained on expert-annotated patches from 1008 whole-slide images, covering 11
diagnostic categories, including 10 major cancers and non-tumor tissue.
DepViT-CAD was validated on two independent cohorts: 275 WSIs from The Cancer
Genome Atlas and 50 routine clinical cases from pathology labs, achieving
diagnostic sensitivities of 94.11% and 92%, respectively. By combining
state-of-the-art transformer architecture with large-scale real-world
validation, DepViT-CAD offers a robust and scalable approach for AI-assisted
cancer diagnostics. To support transparency and reproducibility, software and
code will be made publicly available at GitHub.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [253] [TinyTroupe: An LLM-powered Multiagent Persona Simulation Toolkit](https://arxiv.org/abs/2507.09788)
*Paulo Salem,Robert Sim,Christopher Olsen,Prerit Saxena,Rafael Barcelos,Yi Ding*

Main category: cs.MA

TL;DR: TinyTroupe is a simulation toolkit for LLM-powered Multiagent Systems, addressing gaps in persona specification and behavioral simulation for social studies.


<details>
  <summary>Details</summary>
Motivation: Existing MAS tools lack fine-grained persona details and experimentation support, limiting their use in behavioral and social simulations.

Method: Introduces TinyTroupe, a toolkit with detailed persona definitions and LLM-driven mechanisms for realistic human behavior simulation.

Result: Demonstrated through examples like brainstorming and market research, with evaluations highlighting its utility and limitations.

Conclusion: TinyTroupe offers a novel conceptual framework for LLM-driven MAS, available as open-source for broader application.

Abstract: Recent advances in Large Language Models (LLM) have led to a new class of
autonomous agents, renewing and expanding interest in the area. LLM-powered
Multiagent Systems (MAS) have thus emerged, both for assistive and simulation
purposes, yet tools for realistic human behavior simulation -- with its
distinctive challenges and opportunities -- remain underdeveloped. Existing MAS
libraries and tools lack fine-grained persona specifications, population
sampling facilities, experimentation support, and integrated validation, among
other key capabilities, limiting their utility for behavioral studies, social
simulation, and related applications. To address these deficiencies, in this
work we introduce TinyTroupe, a simulation toolkit enabling detailed persona
definitions (e.g., nationality, age, occupation, personality, beliefs,
behaviors) and programmatic control via numerous LLM-driven mechanisms. This
allows for the concise formulation of behavioral problems of practical
interest, either at the individual or group level, and provides effective means
for their solution. TinyTroupe's components are presented using representative
working examples, such as brainstorming and market research sessions, thereby
simultaneously clarifying their purpose and demonstrating their usefulness.
Quantitative and qualitative evaluations of selected aspects are also provided,
highlighting possibilities, limitations, and trade-offs. The approach, though
realized as a specific Python implementation, is meant as a novel conceptual
contribution, which can be partially or fully incorporated in other contexts.
The library is available as open source at
https://github.com/microsoft/tinytroupe.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [254] [Principled Foundations for Preference Optimization](https://arxiv.org/abs/2507.07855)
*Wenxuan Zhou,Shujian Zhang,Brice Magdalou,John Lambert,Ehsan Amid,Richard Nock,Andrew Hard*

Main category: cs.LG

TL;DR: The paper connects direct preference optimization (DPO) to foundational theories in ML, showing its generality and practical benefits, including support for abstention, non-convex objectives, and extensions like margins and length corrections.


<details>
  <summary>Details</summary>
Motivation: To provide a principled understanding of DPO's operation, given its broad applications and current momentum, while highlighting the risks of deviating from its theoretical foundations.

Method: Establishes a connection between Savage's loss functions and stochastic choice theories, generalizing DPO to include abstention, non-convex objectives, and extensions.

Result: The framework covers all Savage's losses, supports abstention and non-convex objectives, and enables free extensions like margins and length corrections.

Conclusion: Understanding DPO's general principles is crucial for its diverse applications, current momentum, and avoiding pitfalls when deviating from its theoretical map.

Abstract: In this paper, we show that direct preference optimization (DPO) is a very
specific form of a connection between two major theories in the ML context of
learning from preferences: loss functions (Savage) and stochastic choice
(Doignon-Falmagne and Machina). The connection is established for all of
Savage's losses and at this level of generality, (i) it includes support for
abstention on the choice theory side, (ii) it includes support for non-convex
objectives on the ML side, and (iii) it allows to frame for free some notable
extensions of the DPO setting, including margins and corrections for length.
Getting to understand how DPO operates from a general principled perspective is
crucial because of the huge and diverse application landscape of models,
because of the current momentum around DPO, but also -- and importantly --
because many state of the art variations on DPO definitely occupy a small
region of the map that we cover. It also helps to understand the pitfalls of
departing from this map, and figure out workarounds.

</details>


### [255] [LoRA Is Slower Than You Think](https://arxiv.org/abs/2507.08833)
*Seokmin Ko*

Main category: cs.LG

TL;DR: LoRA reduces parameters for LLM fine-tuning but lacks consistent speed improvements. The paper analyzes LoRA's limitations, proposes efficient alternatives, and validates their performance.


<details>
  <summary>Details</summary>
Motivation: Address the inconsistency in LoRA's speed improvements across model architectures and training setups.

Method: Comprehensive analysis of LoRA's performance, identification of limiting factors, and proposal of efficient fine-tuning methods.

Result: Proposed methods achieve comparable or superior performance with more consistent speed improvements than LoRA.

Conclusion: Provides insights and guidelines for optimizing LLM fine-tuning under resource constraints.

Abstract: Low-Rank Adaptation (LoRA) is one of the most widely used techniques for
fine-tuning large language models (LLMs). By introducing a small number of
trainable low-rank weight matrices, LoRA substantially reduces the number of
parameters that need to be updated, offering significant advantages in memory
consumption and computational efficiency compared to full fine-tuning. However,
we observed that LoRA does not consistently provide speed improvements across
all model architectures and training setups. Motivated by this inconsistency,
we conduct a comprehensive analysis of LoRA's performance and investigate the
underlying factors limiting its speedup. Based on our findings, we propose
several methods for more efficient fine-tuning of LLMs. We empirically evaluate
these methods and compare them to LoRA, demonstrating that our approach
achieves comparable or superior performance while delivering more consistent
training speed improvements. Our work offers valuable insights and practical
guidelines for practitioners seeking to optimize LLM fine-tuning under resource
constraints.

</details>


### [256] [Multiple Choice Learning of Low Rank Adapters for Language Modeling](https://arxiv.org/abs/2507.10419)
*Victor Letzelter,Hugo Malard,Mathieu Fontaine,Gal Richard,Slim Essid,Andrei Bursuc,Patrick Prez*

Main category: cs.LG

TL;DR: LoRA-MCL enhances language models by decoding diverse, plausible continuations using Multiple Choice Learning and Winner-Takes-All loss, improving diversity and relevance in outputs.


<details>
  <summary>Details</summary>
Motivation: Traditional language modeling struggles with ambiguity in plausible futures for a given context. LoRA-MCL addresses this by leveraging MCL and WTA loss.

Method: Combines Multiple Choice Learning and Winner-Takes-All loss with Low-Rank Adaptation (LoRA) to handle ambiguity. Theoretical interpretation assumes data from a mixture of distributions.

Result: Achieves high diversity and relevance in generated outputs, demonstrated on visual and audio captioning tasks.

Conclusion: LoRA-MCL effectively handles ambiguity in language modeling, producing diverse and relevant continuations.

Abstract: We propose LoRA-MCL, a training scheme that extends next-token prediction in
language models with a method designed to decode diverse, plausible sentence
continuations at inference time. Traditional language modeling is an
intrinsically ill-posed problem: given a context, multiple futures may be
equally plausible. Our approach leverages Multiple Choice Learning (MCL) and
the Winner-Takes-All (WTA) loss to efficiently handle ambiguity through
Low-Rank Adaptation (LoRA). We provide a theoretical interpretation of applying
Multiple Choice Learning to Language Modeling, assuming the data is generated
from a mixture of distributions. To illustrate the proposed approach, we use
data sampled from mixtures of Markov chains. We then demonstrate with extensive
experiments on real-world visual and audio captioning tasks that our method
achieves high diversity and relevance in generated outputs.

</details>


### [257] [Reasoning or Memorization? Unreliable Results of Reinforcement Learning Due to Data Contamination](https://arxiv.org/abs/2507.10532)
*Mingqi Wu,Zhihao Zhang,Qiaole Dong,Zhiheng Xi,Jun Zhao,Senjie Jin,Xiaoran Fan,Yuhao Zhou,Yanwei Fu,Qin Liu,Songyang Zhang,Qi Zhang*

Main category: cs.LG

TL;DR: The paper examines the reliability of reinforcement learning (RL)-enhanced reasoning in LLMs, highlighting potential data contamination in benchmarks and advocating for cleaner evaluation methods.


<details>
  <summary>Details</summary>
Motivation: To address concerns about data contamination in popular benchmarks and unreliable results from RL-enhanced reasoning in LLMs, particularly the Qwen2.5 model family.

Method: Introduces a synthetic dataset generator (RandomCalculation) to create uncontaminated arithmetic problems for evaluation.

Result: Accurate reward signals improve performance, while noisy or incorrect signals do not, contradicting some prior claims.

Conclusion: Advocates for evaluating RL methods on uncontaminated benchmarks and diverse models to ensure reliable conclusions.

Abstract: The reasoning capabilities of large language models (LLMs) have been a
longstanding focus of research. Recent works have further enhanced these
capabilities using reinforcement learning (RL), with many new methods claiming
significant improvements with minimal or no external supervision. Surprisingly,
some studies even suggest that random or incorrect reward signals can enhance
reasoning performance. However, these breakthroughs are mostly reported on the
Qwen2.5 model family and evaluated on well-known benchmarks such as MATH-500,
AMC, and AIME, while failing to achieve similar gains on other models like
Llama, which warrants further investigation. Our analysis shows that although
Qwen2.5 achieves strong mathematical reasoning performance, its pretraining on
large-scale web corpora makes it vulnerable to data contamination in popular
benchmarks. As a result, results derived from these benchmarks may be
unreliable. To address this, we introduce a generator that produces fully
synthetic arithmetic problems of arbitrary length and difficulty, yielding a
clean dataset we call RandomCalculation. Using these leakage-free datasets, we
show that only accurate reward signals consistently improve performance, while
noisy or incorrect signals do not. We advocate for evaluating RL methods on
uncontaminated benchmarks and across diverse model families to ensure
trustworthy conclusions.

</details>


### [258] [Zero-Shot Neural Architecture Search with Weighted Response Correlation](https://arxiv.org/abs/2507.08841)
*Kun Jing,Luoyu Chen,Jungang Xu,Jianwei Tai,Yiyu Wang,Shuaimin Li*

Main category: cs.LG

TL;DR: WRCor is a novel training-free proxy for NAS, improving efficiency and outperforming existing methods in architecture search.


<details>
  <summary>Details</summary>
Motivation: Current zero-shot NAS methods lack effectiveness, stability, and generality, prompting the need for a better training-free proxy.

Method: WRCor uses correlation coefficient matrices of responses to measure expressivity and generalizability, combined with voting proxies for efficiency.

Result: WRCor outperforms existing proxies and NAS algorithms, achieving a 22.1% test error on ImageNet-1k in 4 GPU hours.

Conclusion: WRCor is an efficient and effective zero-shot NAS method, with publicly available code for broader use.

Abstract: Neural architecture search (NAS) is a promising approach for automatically
designing neural network architectures. However, the architecture estimation of
NAS is computationally expensive and time-consuming because of training
multiple architectures from scratch. Although existing zero-shot NAS methods
use training-free proxies to accelerate the architecture estimation, their
effectiveness, stability, and generality are still lacking. We present a novel
training-free estimation proxy called weighted response correlation (WRCor).
WRCor utilizes correlation coefficient matrices of responses across different
input samples to calculate the proxy scores of estimated architectures, which
can measure their expressivity and generalizability. Experimental results on
proxy evaluation demonstrate that WRCor and its voting proxies are more
efficient estimation strategies than existing proxies. We also apply them with
different search strategies in architecture search. Experimental results on
architecture search show that our zero-shot NAS algorithm outperforms most
existing NAS algorithms in different search spaces. Our NAS algorithm can
discover an architecture with a 22.1% test error on the ImageNet-1k dataset
within 4 GPU hours. All codes are publicly available at
https://github.com/kunjing96/ZSNAS-WRCor.git.

</details>


### [259] [Learning Diffusion Models with Flexible Representation Guidance](https://arxiv.org/abs/2507.08980)
*Chenyu Wang,Cai Zhou,Sharut Gupta,Zongyu Lin,Stefanie Jegelka,Stephen Bates,Tommi Jaakkola*

Main category: cs.LG

TL;DR: The paper introduces a systematic framework for improving diffusion models by aligning their internal representations with pre-trained models, leading to faster training and superior performance.


<details>
  <summary>Details</summary>
Motivation: To enhance diffusion models by incorporating auxiliary representations from pre-trained models, improving generation quality and training efficiency.

Method: Proposes two strategies: (1) pairing examples with target representations (self-derived or synthetic) and learning a joint model, and (2) designing an optimal training curriculum balancing representation learning and data generation.

Result: Achieves 23.3x faster training on ImageNet 256x256 compared to SiT-XL and 4x speedup over REPA, with improved performance across image, protein, and molecule generation tasks.

Conclusion: The framework effectively enhances diffusion models through representation guidance, offering significant training speedups and performance gains.

Abstract: Diffusion models can be improved with additional guidance towards more
effective representations of input. Indeed, prior empirical work has already
shown that aligning internal representations of the diffusion model with those
of pre-trained models improves generation quality. In this paper, we present a
systematic framework for incorporating representation guidance into diffusion
models. We provide alternative decompositions of denoising models along with
their associated training criteria, where the decompositions determine when and
how the auxiliary representations are incorporated. Guided by our theoretical
insights, we introduce two new strategies for enhancing representation
alignment in diffusion models. First, we pair examples with target
representations either derived from themselves or arisen from different
synthetic modalities, and subsequently learn a joint model over the multimodal
pairs. Second, we design an optimal training curriculum that balances
representation learning and data generation. Our experiments across image,
protein sequence, and molecule generation tasks demonstrate superior
performance as well as accelerated training. In particular, on the
class-conditional ImageNet $256\times 256$ benchmark, our guidance results in
$23.3$ times faster training than the original SiT-XL as well as four times
speedup over the state-of-the-art method REPA. The code is available at
https://github.com/ChenyuWang-Monica/REED.

</details>


### [260] [Confounder-Free Continual Learning via Recursive Feature Normalization](https://arxiv.org/abs/2507.09031)
*Yash Shah,Camila Gonzalez,Mohammad H. Abbasi,Qingyu Zhao,Kilian M. Pohl,Ehsan Adeli*

Main category: cs.LG

TL;DR: The paper introduces Recursive MDN (R-MDN), a method to remove confounder effects in continual learning by recursively updating feature distributions.


<details>
  <summary>Details</summary>
Motivation: Confounders cause biased predictions, and existing methods like MDN don't address continual learning challenges.

Method: R-MDN integrates into deep learning architectures, using recursive least squares to update feature distributions dynamically.

Result: R-MDN reduces bias and catastrophic forgetting in both static and continual learning settings.

Conclusion: R-MDN effectively mitigates confounder effects in continual learning, improving fairness and model stability.

Abstract: Confounders are extraneous variables that affect both the input and the
target, resulting in spurious correlations and biased predictions. There are
recent advances in dealing with or removing confounders in traditional models,
such as metadata normalization (MDN), where the distribution of the learned
features is adjusted based on the study confounders. However, in the context of
continual learning, where a model learns continuously from new data over time
without forgetting, learning feature representations that are invariant to
confounders remains a significant challenge. To remove their influence from
intermediate feature representations, we introduce the Recursive MDN (R-MDN)
layer, which can be integrated into any deep learning architecture, including
vision transformers, and at any model stage. R-MDN performs statistical
regression via the recursive least squares algorithm to maintain and
continually update an internal model state with respect to changing
distributions of data and confounding variables. Our experiments demonstrate
that R-MDN promotes equitable predictions across population groups, both within
static learning and across different stages of continual learning, by reducing
catastrophic forgetting caused by confounder effects changing over time.

</details>


### [261] [Warm Starts Accelerate Generative Modelling](https://arxiv.org/abs/2507.09212)
*Jonas Scholz,Richard E. Turner*

Main category: cs.LG

TL;DR: The paper introduces a warm-start model to accelerate conditional generation in iterative generative models by providing an informed prior, reducing the number of required function evaluations.


<details>
  <summary>Details</summary>
Motivation: Iterative generative models are slow, often needing hundreds of evaluations. The goal is to speed up conditional generation by improving the starting point.

Method: The warm-start model predicts an informed prior (N(mu, sigma)) conditioned on input context, reducing the generative process's distance. A conditional normalization trick ensures compatibility with existing models.

Result: The method achieves competitive results with only 11 function evaluations (vs. 1000-step baseline) in tasks like image inpainting.

Conclusion: The warm-start model significantly accelerates generation while maintaining quality and is compatible with standard models and samplers.

Abstract: Iterative generative models, like diffusion and flow-matching, create
high-fidelity samples by progressively refining a noise vector into data.
However, this process is notoriously slow, often requiring hundreds of function
evaluations. We introduce the warm-start model, a simple, deterministic model
that dramatically accelerates conditional generation by providing a better
starting point. Instead of starting generation from an uninformed N(0, I)
prior, our warm-start model predicts an informed prior N(mu, sigma), whose
moments are conditioned on the input context. This "warm start" substantially
reduces the distance the generative process must traverse, particularly when
the conditioning information is strongly informative. On tasks like image
inpainting, our method achieves results competitive with a 1000-step DDPM
baseline using only 11 total function evaluations (1 for the warm start, 10 for
generation). A simple conditional normalization trick makes our method
compatible with any standard generative model and sampler without modification,
allowing it to be combined with other efficient sampling techniques for further
acceleration. Our implementation is available at
https://github.com/jonas-scholz123/warm-start-model.

</details>


### [262] [MLoRQ: Bridging Low-Rank and Quantization for Transformer Compression](https://arxiv.org/abs/2507.09616)
*Ofir Gordon,Ariel Lapid,Elad Cohen,Yarden Yagil,Arnon Netzer,Hai Victor Habi*

Main category: cs.LG

TL;DR: MLoRQ combines low-rank approximation and mixed-precision quantization for efficient transformer deployment on edge devices, achieving up to 15% performance improvement.


<details>
  <summary>Details</summary>
Motivation: Deploying transformer-based neural networks on resource-constrained edge devices is challenging due to memory limitations.

Method: MLoRQ uses a two-stage optimization: intra-layer to find compression solutions and inter-layer to assign bit-width and rank under memory constraints, with optional adaptive rounding.

Result: MLoRQ achieves state-of-the-art results with up to 15% performance improvement on Vision Transformers for various tasks.

Conclusion: MLoRQ effectively integrates low-rank and quantization techniques, offering a scalable solution for edge deployment.

Abstract: Deploying transformer-based neural networks on resource-constrained edge
devices presents a significant challenge. This challenge is often addressed
through various techniques, such as low-rank approximation and mixed-precision
quantization. In this work, we introduce Mixed Low-Rank and Quantization
(MLoRQ), a novel method that integrates both techniques. MLoRQ employs a
two-stage optimization process to determine optimal bit-width and rank
assignments for each layer, adhering to predefined memory constraints. This
process includes: (i) an intra-layer optimization that identifies potentially
optimal compression solutions out of all low-rank and quantization
combinations; (ii) an inter-layer optimization that assigns bit-width precision
and rank to each layer while ensuring the memory constraint is met. An optional
final step applies a sequential optimization process using a modified adaptive
rounding technique to mitigate compression-induced errors in joint low-rank
approximation and quantization. The method is compatible and can be seamlessly
integrated with most existing quantization algorithms. MLoRQ shows
state-of-the-art results with up to 15\% performance improvement, evaluated on
Vision Transformers for image classification, object detection, and instance
segmentation tasks.

</details>


### [263] [Universal Physics Simulation: A Foundational Diffusion Approach](https://arxiv.org/abs/2507.09733)
*Bradley Camburn*

Main category: cs.LG

TL;DR: A foundational AI model for universal physics simulation learns physical laws from boundary-condition data without predefined equations, outperforming traditional methods like PINNs and finite-difference approaches.


<details>
  <summary>Details</summary>
Motivation: Traditional physics simulation methods require explicit equation encoding, limiting generalizability and discovery. This work aims to bypass these constraints by treating simulation as a conditional generation problem.

Method: The model uses a sketch-guided diffusion transformer with spatial relationship encoding to directly map boundary conditions to steady-state solutions, avoiding temporal integration.

Result: Achieves SSIM > 0.8 for steady-state solutions with sub-pixel boundary accuracy and enables physics discovery through learned representations.

Conclusion: This work shifts from AI-accelerated to AI-discovered physics, establishing a universal simulation framework.

Abstract: We present the first foundational AI model for universal physics simulation
that learns physical laws directly from boundary-condition data without
requiring a priori equation encoding. Traditional physics-informed neural
networks (PINNs) and finite-difference methods necessitate explicit
mathematical formulation of governing equations, fundamentally limiting their
generalizability and discovery potential. Our sketch-guided diffusion
transformer approach reimagines computational physics by treating simulation as
a conditional generation problem, where spatial boundary conditions guide the
synthesis of physically accurate steady-state solutions.
  By leveraging enhanced diffusion transformer architectures with novel spatial
relationship encoding, our model achieves direct boundary-to-equilibrium
mapping and is generalizable to diverse physics domains. Unlike sequential
time-stepping methods that accumulate errors over iterations, our approach
bypasses temporal integration entirely, directly generating steady-state
solutions with SSIM > 0.8 while maintaining sub-pixel boundary accuracy. Our
data-informed approach enables physics discovery through learned
representations analyzable via Layer-wise Relevance Propagation (LRP),
revealing emergent physical relationships without predetermined mathematical
constraints. This work represents a paradigm shift from AI-accelerated physics
to AI-discovered physics, establishing the first truly universal physics
simulation framework.

</details>


### [264] [Learning Private Representations through Entropy-based Adversarial Training](https://arxiv.org/abs/2507.10194)
*Tassilo Klein,Moin Nabi*

Main category: cs.LG

TL;DR: A method using adversarial representation learning and focal entropy to balance predictive power and privacy by sanitizing sensitive content.


<details>
  <summary>Details</summary>
Motivation: To learn representations with high predictive power while preserving user privacy, addressing potential information leakage in existing entropy-based methods.

Method: Adversarial representation learning with a novel variant of entropy called focal entropy to mitigate information leakage.

Result: High target utility achieved with moderate privacy leakage, demonstrated on multiple benchmarks.

Conclusion: The proposed method effectively balances predictive performance and privacy, showcasing feasibility in real-world applications.

Abstract: How can we learn a representation with high predictive power while preserving
user privacy? We present an adversarial representation learning method for
sanitizing sensitive content from the learned representation. Specifically, we
introduce a variant of entropy - focal entropy, which mitigates the potential
information leakage of the existing entropy-based approaches. We showcase
feasibility on multiple benchmarks. The results suggest high target utility at
moderate privacy leakage.

</details>


### [265] [CLA: Latent Alignment for Online Continual Self-Supervised Learning](https://arxiv.org/abs/2507.10434)
*Giacomo Cignoni,Andrea Cossu,Alexandra Gomez-Villa,Joost van de Weijer,Antonio Carta*

Main category: cs.LG

TL;DR: CLA is a novel SSL method for online continual learning that aligns current and past representations to reduce forgetting, improving convergence and performance.


<details>
  <summary>Details</summary>
Motivation: Address the lack of SSL techniques for online continual learning, where data arrives in small batches, computational resources are limited, and task boundaries are unclear.

Method: Introduces Continual Latent Alignment (CLA), which aligns current model representations with past ones to mitigate forgetting.

Result: CLA speeds up convergence and outperforms state-of-the-art methods under the same computational budget. It also enhances final performance when used as pretraining.

Conclusion: CLA is effective for online continual learning, offering faster convergence and better performance compared to existing methods and even full i.i.d. pretraining.

Abstract: Self-supervised learning (SSL) is able to build latent representations that
generalize well to unseen data. However, only a few SSL techniques exist for
the online CL setting, where data arrives in small minibatches, the model must
comply with a fixed computational budget, and task boundaries are absent. We
introduce Continual Latent Alignment (CLA), a novel SSL strategy for Online CL
that aligns the representations learned by the current model with past
representations to mitigate forgetting. We found that our CLA is able to speed
up the convergence of the training process in the online scenario,
outperforming state-of-the-art approaches under the same computational budget.
Surprisingly, we also discovered that using CLA as a pretraining protocol in
the early stages of pretraining leads to a better final performance when
compared to a full i.i.d. pretraining.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [266] [RAG Safety: Exploring Knowledge Poisoning Attacks to Retrieval-Augmented Generation](https://arxiv.org/abs/2507.08862)
*Tianzhe Zhao,Jiaoyan Chen,Yanchi Ru,Haiping Zhu,Nan Hu,Jun Liu,Qika Lin*

Main category: cs.CR

TL;DR: The paper investigates security risks in KG-RAG systems through data poisoning attacks, proposing a stealthy attack strategy that degrades performance with minimal KG perturbations.


<details>
  <summary>Details</summary>
Motivation: To address unexplored security vulnerabilities in KG-RAG systems, which are susceptible due to their structured nature, unlike unstructured RAG systems.

Method: Introduces a practical attack setting, identifies adversarial targets, and inserts perturbation triples to mislead KG-RAG inference chains.

Result: The attack strategy effectively degrades KG-RAG performance on benchmarks, even with minimal perturbations.

Conclusion: The study highlights critical security threats in KG-RAG systems and underscores the need for robustness against adversarial knowledge.

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by
retrieving external data to mitigate hallucinations and outdated knowledge
issues. Benefiting from the strong ability in facilitating diverse data sources
and supporting faithful reasoning, knowledge graphs (KGs) have been
increasingly adopted in RAG systems, giving rise to KG-based RAG (KG-RAG)
methods. Though RAG systems are widely applied in various applications, recent
studies have also revealed its vulnerabilities to data poisoning attacks, where
malicious information injected into external knowledge sources can mislead the
system into producing incorrect or harmful responses. However, these studies
focus exclusively on RAG systems using unstructured textual data sources,
leaving the security risks of KG-RAG largely unexplored, despite the fact that
KGs present unique vulnerabilities due to their structured and editable nature.
In this work, we conduct the first systematic investigation of the security
issue of KG-RAG methods through data poisoning attacks. To this end, we
introduce a practical, stealthy attack setting that aligns with real-world
implementation. We propose an attack strategy that first identifies adversarial
target answers and then inserts perturbation triples to complete misleading
inference chains in the KG, increasing the likelihood that KG-RAG methods
retrieve and rely on these perturbations during generation. Through extensive
experiments on two benchmarks and four recent KG-RAG methods, our attack
strategy demonstrates strong effectiveness in degrading KG-RAG performance,
even with minimal KG perturbations. In-depth analyses are also conducted to
understand the safety threats within the internal stages of KG-RAG systems and
to explore the robustness of LLMs against adversarial knowledge.

</details>


### [267] [EventHunter: Dynamic Clustering and Ranking of Security Events from Hacker Forum Discussions](https://arxiv.org/abs/2507.09762)
*Yasir Ech-Chammakhy,Anas Motii,Anass Rabii,Jaafar Chbili*

Main category: cs.CR

TL;DR: An unsupervised framework using Transformer-based embeddings and contrastive learning to detect, cluster, and prioritize cybersecurity threats from hacker forums, validated on real-world data.


<details>
  <summary>Details</summary>
Motivation: Hacker forums contain early signals of cybersecurity threats, but extracting actionable intelligence from their unstructured content is challenging.

Method: Leverages Transformer-based embeddings fine-tuned with contrastive learning to cluster discussions, and ranks events using metrics like timeliness and credibility.

Result: Effectively reduces noise and identifies high-priority threats, enabling proactive responses.

Conclusion: The framework transforms unstructured forum discussions into structured, actionable intelligence, addressing key challenges in threat detection.

Abstract: Hacker forums provide critical early warning signals for emerging
cybersecurity threats, but extracting actionable intelligence from their
unstructured and noisy content remains a significant challenge. This paper
presents an unsupervised framework that automatically detects, clusters, and
prioritizes security events discussed across hacker forum posts. Our approach
leverages Transformer-based embeddings fine-tuned with contrastive learning to
group related discussions into distinct security event clusters, identifying
incidents like zero-day disclosures or malware releases without relying on
predefined keywords. The framework incorporates a daily ranking mechanism that
prioritizes identified events using quantifiable metrics reflecting timeliness,
source credibility, information completeness, and relevance. Experimental
evaluation on real-world hacker forum data demonstrates that our method
effectively reduces noise and surfaces high-priority threats, enabling security
analysts to mount proactive responses. By transforming disparate hacker forum
discussions into structured, actionable intelligence, our work addresses
fundamental challenges in automated threat detection and analysis.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [268] [RectifiedHR: High-Resolution Diffusion via Energy Profiling and Adaptive Guidance Scheduling](https://arxiv.org/abs/2507.09441)
*Ankit Sanjyal*

Main category: cs.GR

TL;DR: The paper proposes adaptive classifier-free guidance (CFG) schedules to stabilize energy trajectories in diffusion models, improving image synthesis quality.


<details>
  <summary>Details</summary>
Motivation: High-resolution image synthesis with diffusion models faces issues like energy instabilities and guidance artifacts, degrading visual quality.

Method: The authors analyze the latent energy landscape during sampling and introduce energy-aware CFG scheduling strategies to modulate guidance strength over time.

Result: Their approach achieves superior stability (0.9998) and consistency (0.9873) scores, with DPM++ 2M and linear-decreasing CFG scheduling yielding optimal performance.

Conclusion: The energy profiling framework serves as a diagnostic tool for understanding and enhancing diffusion model behavior, producing sharper, artifact-free images.

Abstract: High-resolution image synthesis with diffusion models often suffers from
energy instabilities and guidance artifacts that degrade visual quality. We
analyze the latent energy landscape during sampling and propose adaptive
classifier-free guidance (CFG) schedules that maintain stable energy
trajectories. Our approach introduces energy-aware scheduling strategies that
modulate guidance strength over time, achieving superior stability scores
(0.9998) and consistency metrics (0.9873) compared to fixed-guidance
approaches. We demonstrate that DPM++ 2M with linear-decreasing CFG scheduling
yields optimal performance, providing sharper, more faithful images while
reducing artifacts. Our energy profiling framework serves as a powerful
diagnostic tool for understanding and improving diffusion model behavior.

</details>


### [269] [CADmium: Fine-Tuning Code Language Models for Text-Driven Sequential CAD Design](https://arxiv.org/abs/2507.09792)
*Prashant Govindarajan,Davide Baldelli,Jay Pathak,Quentin Fournier,Sarath Chandar*

Main category: cs.GR

TL;DR: The paper introduces CADmium, a method using large language models (LLMs) to automate CAD design from natural language descriptions, supported by a new dataset and novel geometric metrics.


<details>
  <summary>Details</summary>
Motivation: CAD modeling is time-intensive and manual; existing automation efforts are limited. This work explores leveraging LLMs for sequential CAD design.

Method: A large-scale dataset of CAD models with GPT-4.1-generated descriptions is created. Code-LLMs are fine-tuned to generate CAD sequences from natural language.

Result: CADmium automates CAD design effectively, speeding up object creation. Novel geometric metrics provide richer structural insights than simple metrics.

Conclusion: The approach demonstrates viability for text-conditioned CAD generation, with potential to revolutionize CAD workflows.

Abstract: Computer-aided design (CAD) is the digital construction of 2D and 3D objects,
and is central to a wide range of engineering and manufacturing applications
like automobile and aviation. Despite its importance, CAD modeling remains
largely a time-intensive, manual task. Recent works have attempted to automate
this process with small transformer-based models and handcrafted CAD sequence
representations. However, there has been little effort to leverage the
potential of large language models (LLMs) for sequential CAD design. In this
work, we introduce a new large-scale dataset of more than 170k CAD models
annotated with high-quality, human-like descriptions generated with our
pipeline based on GPT-4.1. Using this dataset, we fine-tune powerful code-LLMs
to generate CAD sequences represented in a JSON-based format from natural
language descriptions, demonstrating the viability and effectiveness of this
approach for text-conditioned CAD generation. Because simple metrics often fail
to reflect the quality of generated objects, we introduce geometric and
topological metrics based on sphericity, mean curvature, and Euler
characteristic to provide richer structural insights. Our experiments and
ablation studies on both synthetic and human-annotated data demonstrate that
CADmium is able to automate CAD design, drastically speeding up the design of
new objects. The dataset, code, and fine-tuned models are available online.

</details>


### [270] [ScaffoldAvatar: High-Fidelity Gaussian Avatars with Patch Expressions](https://arxiv.org/abs/2507.10542)
*Shivangi Aneja,Sebastian Weiss,Irene Baeza,Prashanth Chandran,Gaspard Zoss,Matthias Niener,Derek Bradley*

Main category: cs.GR

TL;DR: The paper proposes a method to create high-fidelity 3D head avatars using patch-based facial expressions and 3D Gaussian splatting, achieving realistic motion and detail in real time.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of rendering photorealistic 3D head avatars with detailed facial microfeatures and expressions for applications like telepresence and movies.

Method: Couples patch-based local expression features with 3D Gaussian splatting, leveraging a patch-based geometric 3D face model and Scaffold-GS for dynamic skin appearance and motion.

Result: Achieves state-of-the-art performance with natural motion, diverse expressions, and real-time rendering.

Conclusion: The proposed ScaffoldAvatar method effectively combines patch-level expressions and 3D Gaussian splatting for ultra-high fidelity avatars.

Abstract: Generating high-fidelity real-time animated sequences of photorealistic 3D
head avatars is important for many graphics applications, including immersive
telepresence and movies. This is a challenging problem particularly when
rendering digital avatar close-ups for showing character's facial microfeatures
and expressions. To capture the expressive, detailed nature of human heads,
including skin furrowing and finer-scale facial movements, we propose to couple
locally-defined facial expressions with 3D Gaussian splatting to enable
creating ultra-high fidelity, expressive and photorealistic 3D head avatars. In
contrast to previous works that operate on a global expression space, we
condition our avatar's dynamics on patch-based local expression features and
synthesize 3D Gaussians at a patch level. In particular, we leverage a
patch-based geometric 3D face model to extract patch expressions and learn how
to translate these into local dynamic skin appearance and motion by coupling
the patches with anchor points of Scaffold-GS, a recent hierarchical scene
representation. These anchors are then used to synthesize 3D Gaussians
on-the-fly, conditioned by patch-expressions and viewing direction. We employ
color-based densification and progressive training to obtain high-quality
results and faster convergence for high resolution 3K training images. By
leveraging patch-level expressions, ScaffoldAvatar consistently achieves
state-of-the-art performance with visually natural motion, while encompassing
diverse facial expressions and styles in real time.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [271] [Lightweight Deep Learning-Based Channel Estimation for RIS-Aided Extremely Large-Scale MIMO Systems on Resource-Limited Edge Devices](https://arxiv.org/abs/2507.09627)
*Muhammad Kamran Saeed,Ashfaq Khokhar,Shakil Ahmed*

Main category: cs.IT

TL;DR: A lightweight deep learning framework is proposed for efficient cascaded channel estimation in XL-MIMO systems, addressing scalability and computational challenges.


<details>
  <summary>Details</summary>
Motivation: Next-gen wireless tech (6G, XL-MIMO, RIS) demands accurate CSI, but existing methods struggle with scalability and real-time deployment due to high data volume and complexity.

Method: Proposes a patch-based training mechanism leveraging spatial correlations to reduce input dimensionality while preserving key information, enabling scalable training.

Result: Simulations show improved estimation accuracy and reduced computational complexity, even as antenna and RIS elements scale up.

Conclusion: The framework is effective for resource-constrained edge devices, making XL-MIMO and RIS more practical for real-world deployment.

Abstract: Next-generation wireless technologies such as 6G aim to meet demanding
requirements such as ultra-high data rates, low latency, and enhanced
connectivity. Extremely Large-Scale MIMO (XL-MIMO) and Reconfigurable
Intelligent Surface (RIS) are key enablers, with XL-MIMO boosting spectral and
energy efficiency through numerous antennas, and RIS offering dynamic control
over the wireless environment via passive reflective elements. However,
realizing their full potential depends on accurate Channel State Information
(CSI). Recent advances in deep learning have facilitated efficient cascaded
channel estimation. However, the scalability and practical deployment of
existing estimation models in XL-MIMO systems remain limited. The growing
number of antennas and RIS elements introduces a significant barrier to
real-time and efficient channel estimation, drastically increasing data volume,
escalating computational complexity, requiring advanced hardware, and resulting
in substantial energy consumption. To address these challenges, we propose a
lightweight deep learning framework for efficient cascaded channel estimation
in XL-MIMO systems, designed to minimize computational complexity and make it
suitable for deployment on resource-constrained edge devices. Using spatial
correlations in the channel, we introduce a patch-based training mechanism that
reduces the dimensionality of input to patch-level representations while
preserving essential information, allowing scalable training for large-scale
systems. Simulation results under diverse conditions demonstrate that our
framework significantly improves estimation accuracy and reduces computational
complexity, regardless of the increasing number of antennas and RIS elements in
XL-MIMO systems.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [272] [ZipVoice-Dialog: Non-Autoregressive Spoken Dialogue Generation with Flow Matching](https://arxiv.org/abs/2507.09318)
*Han Zhu,Wei Kang,Liyong Guo,Zengwei Yao,Fangjun Kuang,Weiji Zhuang,Zhaoqing Li,Zhifeng Han,Dong Zhang,Xin Zhang,Xingchen Song,Long Lin,Daniel Povey*

Main category: eess.AS

TL;DR: ZipVoice-Dialog is a non-autoregressive model for spoken dialogue generation, addressing slow inference and instability in existing models. It introduces speaker-turn embeddings, curriculum learning, and stereo dialogue strategies, and releases a 6.8k-hour dataset (OpenDialog).


<details>
  <summary>Details</summary>
Motivation: Existing spoken dialogue generation models are slow and unstable due to autoregressive methods. The need for realistic turn-taking and speaker timbres motivates a non-autoregressive approach.

Method: ZipVoice-Dialog uses flow matching, speaker-turn embeddings, curriculum learning for alignment, and stereo dialogue strategies. It also curates the OpenDialog dataset.

Result: The model outperforms others in intelligibility, turn-taking accuracy, speaker similarity, and speed.

Conclusion: ZipVoice-Dialog is a robust solution for spoken dialogue generation, with publicly available resources.

Abstract: Generating spoken dialogue is more challenging than monologue text-to-speech
(TTS) due to the need for realistic turn-taking and distinct speaker timbres.
Existing spoken dialogue generation models, being auto-regressive, suffer from
slow and unstable inference. To overcome these limitations, we introduce
ZipVoice-Dialog, a non-autoregressive zero-shot spoken dialogue generation
model built upon flow matching. Key designs include: 1) speaker-turn embeddings
for precise speaker turn-taking; 2) a curriculum learning strategy for stable
speech-text alignment; 3) specialized strategies to enable stereo dialogue
generation. Additionally, recognizing the lack of open-source large-scale
spoken dialogue datasets, we curated OpenDialog, a 6.8k-hour spoken dialogue
dataset from in-the-wild speech data. Furthermore, we established a benchmark
to comprehensively evaluate various models. Experimental results demonstrate
that ZipVoice-Dialog achieves superior performance in intelligibility, speaker
turn-taking accuracy, speaker similarity, and inference speed. Our codes, model
checkpoints, demo samples, and the OpenDialog dataset are all publicly
available at https://github.com/k2-fsa/ZipVoice.

</details>


### [273] [Natural Language-based Assessment of L2 Oral Proficiency using LLMs](https://arxiv.org/abs/2507.10200)
*Stefano Bann,Rao Ma,Mengjie Qian,Siyuan Tang,Kate Knill,Mark Gales*

Main category: eess.AS

TL;DR: NLA uses can-do descriptors for LLMs to assess language tasks, showing competitive performance with open-source models like Qwen 2.5 72B, surpassing BERT-based models in zero-shot settings.


<details>
  <summary>Details</summary>
Motivation: To determine if LLMs can interpret and apply human-intended can-do descriptors for language assessment, comparable to human examiners.

Method: Used Qwen 2.5 72B in a zero-shot setting to assess responses from the S&I Corpus, relying solely on textual information.

Result: Competitive performance, surpassing BERT-based models but not fine-tuned speech LLMs; effective in mismatched tasks, generalizable, and interpretable.

Conclusion: NLA is a viable, interpretable approach for language assessment, leveraging widely applicable descriptors and showing promise for diverse tasks and languages.

Abstract: Natural language-based assessment (NLA) is an approach to second language
assessment that uses instructions - expressed in the form of can-do descriptors
- originally intended for human examiners, aiming to determine whether large
language models (LLMs) can interpret and apply them in ways comparable to human
assessment. In this work, we explore the use of such descriptors with an
open-source LLM, Qwen 2.5 72B, to assess responses from the publicly available
S&I Corpus in a zero-shot setting. Our results show that this approach -
relying solely on textual information - achieves competitive performance: while
it does not outperform state-of-the-art speech LLMs fine-tuned for the task, it
surpasses a BERT-based model trained specifically for this purpose. NLA proves
particularly effective in mismatched task settings, is generalisable to other
data types and languages, and offers greater interpretability, as it is
grounded in clearly explainable, widely applicable language descriptors.

</details>


### [274] [Generative Audio Language Modeling with Continuous-valued Tokens and Masked Next-Token Prediction](https://arxiv.org/abs/2507.09834)
*Shu-wen Yang,Byeonggeun Kim,Kuan-Po Huang,Qingming Tang,Huy Phan,Bo-Ru Lu,Harsha Sundar,Shalini Ghosh,Hung-yi Lee,Chieh-Chi Kao,Chao Wang*

Main category: eess.AS

TL;DR: The paper proposes a novel approach for audio generation using a causal language model with token-wise diffusion, outperforming previous discrete methods and achieving state-of-the-art results with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Extending autoregressive next-token prediction to audio is challenging due to its continuous nature. The paper aims to address this by leveraging token-wise diffusion for continuous-valued tokens.

Method: The method involves using a causal language model with token-wise diffusion to model continuous distributions and introducing a masked next-token prediction task.

Result: The approach achieves 20% and 40% relative gains on AudioCaps in FAD and KL divergence, respectively, and matches SOTA diffusion models with fewer parameters (193M for Base, 462M for Large).

Conclusion: The proposed method significantly improves audio generation performance over discrete solutions and competes with SOTA models while being more parameter-efficient.

Abstract: Autoregressive next-token prediction with the Transformer decoder has become
a de facto standard in large language models (LLMs), achieving remarkable
success in Natural Language Processing (NLP) at scale. Extending this paradigm
to audio poses unique challenges due to its inherently continuous nature. We
research audio generation with a causal language model (LM) without discrete
tokens. We leverage token-wise diffusion to model the continuous distribution
of the next continuous-valued token. Our approach delivers significant
improvements over previous discrete solution, AudioGen, achieving 20% and 40%
relative gains on AudioCaps in Frechet Audio Distance (FAD) and
Kullback-Leibler (KL) divergence, respectively. Additionally, we propose a
novel masked next-token prediction task that incorporates masked prediction
into the causal LM framework. On AudioCaps, the innovation yields 41% and 33%
relative FAD improvements over AudioGen Base (285M) and AudioGen Large (1B)
models, respectively, and is on par with the state-of-the-art (SOTA) diffusion
models. Furthermore, we achieve these results with significantly fewer
parameters -- 193M for our Base and 462M for our Large models.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [275] [Semantic Source Code Segmentation using Small and Large Language Models](https://arxiv.org/abs/2507.08992)
*Abdelhalim Dahou,Ansgar Scherp,Sebastian Kurten,Brigitte Mathiak,Madhu Chauhan*

Main category: cs.SE

TL;DR: Automated segmentation of R code using LLMs/SLMs, with line-by-line analysis outperforming range-based methods. Fine-tuned smaller models (CodeBERT, CodeT5+) excel despite no R code in pre-training.


<details>
  <summary>Details</summary>
Motivation: Manual and syntactic code segmentation is impractical for large repositories, especially in low-resource languages like R.

Method: Two approaches: line-by-line analysis with context and range-based segmentation, tested with LLMs and fine-tuned SLMs.

Result: Line-by-line analysis is superior. Fine-tuned smaller models (CodeBERT, CodeT5+) perform best, even without R code in pre-training.

Conclusion: Automated, domain-specific segmentation is viable, with fine-tuned smaller models offering efficient solutions for low-resource languages.

Abstract: Source code segmentation, dividing code into functionally coherent segments,
is crucial for knowledge retrieval and maintenance in software development.
While enabling efficient navigation and comprehension of large codebases,
manual and syntactic analysis approaches have become impractical as
repositories grow, especially for low-resource languages like R and their
research domains (e.g., social sciences, psychology).This paper introduces an
automated, domain-specific approach for research R code segmentation using
Large and Small Language Models (LLMs/SLMs). It presents two novel approaches
and a human-annotated dataset, StatCodeSeg. We explore two distinct approaches:
line-by-line analysis with context and range-based segment determination. We
experiment with LLMs and fine-tuned SLMs. To support the generalizability of
our approaches, we also include experiments on Python code from the computer
science domain.Our results show that context-based line-by-line analysis is
superior over range-based segmentation.Using smaller language models like
CodeBERT and an encoder-only version of CodeT5+ are better than their LLM
counterparts. Most notably, these two best-performing models did not see R code
during pre-training versus the LLMs but were only fine-tuned on 4,130 lines of
manually annotated code.

</details>


### [276] [Evaluating LLMs on Sequential API Call Through Automated Test Generation](https://arxiv.org/abs/2507.09481)
*Yuheng Huang,Da Song,Zhenlan Ji,Shuai Wang,Lei Ma*

Main category: cs.SE

TL;DR: StateGen is an automated framework for generating diverse coding tasks with sequential API interactions, addressing gaps in current LLM tool-use benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for LLM tool use rely on manual test cases and overlook complex API interactions, limiting their effectiveness.

Method: StateGen uses state-machine-based API constraint solving, energy-based sampling, control-flow injection, and LLM collaboration to generate executable programs and natural language tasks.

Result: StateGen created StateEval, a benchmark with 120 verified test cases across three scenarios, demonstrating its ability to generate realistic API-oriented tasks.

Conclusion: StateGen effectively addresses current limitations in LLM tool-use evaluation, highlighting areas for improvement in LLM-API integration.

Abstract: By integrating tools from external APIs, Large Language Models (LLMs) have
expanded their promising capabilities in a diverse spectrum of complex
real-world tasks. However, testing, evaluation, and analysis of LLM tool use
remain in their early stages. Most existing benchmarks rely on manually
collected test cases, many of which cannot be automatically checked for
semantic correctness and instead depend on static methods such as string
matching. Additionally, these benchmarks often overlook the complex
interactions that occur between sequential API calls, which are common in
real-world applications. To fill the gap, in this paper, we introduce StateGen,
an automated framework designed to generate diverse coding tasks involving
sequential API interactions. StateGen combines state-machine-based API
constraint solving and validation, energy-based sampling, and control-flow
injection to generate executable programs. These programs are then translated
into human-like natural language task descriptions through a collaboration of
two LLM agents. Utilizing StateGen, we construct StateEval, a benchmark
encompassing 120 verified test cases spanning across three representative
scenarios: Session Service, Tensor Operation, and ElevenLabs MCP. Experimental
results confirm that StateGen can effectively generate challenging and
realistic API-oriented tasks, highlighting areas for improvement in current
LLMs incorporating APIs.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [277] [DLBAcalib: Robust Extrinsic Calibration for Non-Overlapping LiDARs Based on Dual LBA](https://arxiv.org/abs/2507.09176)
*Han Ye,Yuqiang Jin,Jinyuan Liu,Tao Li,Wen-An Zhang,Minglei Fu*

Main category: cs.RO

TL;DR: A targetless extrinsic calibration framework for multi-LiDAR systems, using LiDAR bundle adjustment and robust iterative refinement, achieves high accuracy without overlapping fields of view or manual input.


<details>
  <summary>Details</summary>
Motivation: Accurate extrinsic calibration of multiple LiDARs is essential for 3D map reconstruction, but conventional methods rely on overlapping fields or manual annotations, limiting practicality.

Method: The framework integrates LiDAR bundle adjustment (LBA) with iterative refinement, constructing a reference point cloud via continuous scanning and sliding-window LBA, and uses adaptive weighting for outlier resistance.

Result: In CARLA simulations and real-world tests, the method achieved 5 mm translational and 0.2 rotational errors, tolerating initial errors up to 0.4 m/30.

Conclusion: The proposed method outperforms state-of-the-art techniques in accuracy and robustness, operates without specialized infrastructure, and is open-source.

Abstract: Accurate extrinsic calibration of multiple LiDARs is crucial for improving
the foundational performance of three-dimensional (3D) map reconstruction
systems. This paper presents a novel targetless extrinsic calibration framework
for multi-LiDAR systems that does not rely on overlapping fields of view or
precise initial parameter estimates. Unlike conventional calibration methods
that require manual annotations or specific reference patterns, our approach
introduces a unified optimization framework by integrating LiDAR bundle
adjustment (LBA) optimization with robust iterative refinement. The proposed
method constructs an accurate reference point cloud map via continuous scanning
from the target LiDAR and sliding-window LiDAR bundle adjustment, while
formulating extrinsic calibration as a joint LBA optimization problem. This
method effectively mitigates cumulative mapping errors and achieves
outlier-resistant parameter estimation through an adaptive weighting mechanism.
Extensive evaluations in both the CARLA simulation environment and real-world
scenarios demonstrate that our method outperforms state-of-the-art calibration
techniques in both accuracy and robustness. Experimental results show that for
non-overlapping sensor configurations, our framework achieves an average
translational error of 5 mm and a rotational error of 0.2{\deg}, with an
initial error tolerance of up to 0.4 m/30{\deg}. Moreover, the calibration
process operates without specialized infrastructure or manual parameter tuning.
The code is open source and available on GitHub
(\underline{https://github.com/Silentbarber/DLBAcalib})

</details>


### [278] [Multimodal HD Mapping for Intersections by Intelligent Roadside Units](https://arxiv.org/abs/2507.08903)
*Zhongzhang Chen,Miao Fan,Shengtong Xu,Mengmeng Yang,Kun Jiang,Xiangzeng Liu,Haoyi Xiong*

Main category: cs.RO

TL;DR: A novel camera-LiDAR fusion framework using roadside units (IRUs) improves HD semantic mapping, outperforming unimodal methods by 4-18% in mIoU.


<details>
  <summary>Details</summary>
Motivation: Traditional vehicle-based methods struggle with occlusions and limited perspectives for HD semantic mapping of intersections.

Method: A two-stage fusion process integrates camera and LiDAR data, leveraging high-resolution textures and precise geometry.

Result: The multimodal approach improves mIoU by 4% over image-only and 18% over LiDAR-only methods.

Conclusion: The study provides a baseline for IRU-based HD mapping and a dataset (RS-seq) for infrastructure-assisted autonomous driving research.

Abstract: High-definition (HD) semantic mapping of complex intersections poses
significant challenges for traditional vehicle-based approaches due to
occlusions and limited perspectives. This paper introduces a novel camera-LiDAR
fusion framework that leverages elevated intelligent roadside units (IRUs).
Additionally, we present RS-seq, a comprehensive dataset developed through the
systematic enhancement and annotation of the V2X-Seq dataset. RS-seq includes
precisely labelled camera imagery and LiDAR point clouds collected from
roadside installations, along with vectorized maps for seven intersections
annotated with detailed features such as lane dividers, pedestrian crossings,
and stop lines. This dataset facilitates the systematic investigation of
cross-modal complementarity for HD map generation using IRU data. The proposed
fusion framework employs a two-stage process that integrates modality-specific
feature extraction and cross-modal semantic integration, capitalizing on camera
high-resolution texture and precise geometric data from LiDAR. Quantitative
evaluations using the RS-seq dataset demonstrate that our multimodal approach
consistently surpasses unimodal methods. Specifically, compared to unimodal
baselines evaluated on the RS-seq dataset, the multimodal approach improves the
mean Intersection-over-Union (mIoU) for semantic segmentation by 4\% over the
image-only results and 18\% over the point cloud-only results. This study
establishes a baseline methodology for IRU-based HD semantic mapping and
provides a valuable dataset for future research in infrastructure-assisted
autonomous driving systems.

</details>


### [279] [Visual Homing in Outdoor Robots Using Mushroom Body Circuits and Learning Walks](https://arxiv.org/abs/2507.09725)
*Gabriel G. Gattaux,Julien R. Serres,Franck Ruffier,Antoine Wystrach*

Main category: cs.RO

TL;DR: A lateralized Mushroom Body (MB) architecture is implemented for visual homing on a robot, inspired by ants. It uses angular path integration to categorize views into memory banks, enabling robust homing in outdoor settings.


<details>
  <summary>Details</summary>
Motivation: Ants achieve visual homing efficiently, inspiring a biomimetic solution for autonomous navigation using minimal sensory input.

Method: A lateralized MB architecture categorizes panoramic views into memory banks using path integration signals. Validated through simulations and real-world experiments.

Result: The system achieves robust homing in natural outdoor settings, mimicking ants' behavior with minimal computational resources.

Conclusion: The biologically grounded, resource-efficient system offers a viable solution for autonomous visual homing.

Abstract: Ants achieve robust visual homing with minimal sensory input and only a few
learning walks, inspiring biomimetic solutions for autonomous navigation. While
Mushroom Body (MB) models have been used in robotic route following, they have
not yet been applied to visual homing. We present the first real-world
implementation of a lateralized MB architecture for visual homing onboard a
compact autonomous car-like robot. We test whether the sign of the angular path
integration (PI) signal can categorize panoramic views, acquired during
learning walks and encoded in the MB, into "goal on the left" and "goal on the
right" memory banks, enabling robust homing in natural outdoor settings. We
validate this approach through four incremental experiments: (1) simulation
showing attractor-like nest dynamics; (2) real-world homing after decoupled
learning walks, producing nest search behavior; (3) homing after random walks
using noisy PI emulated with GPS-RTK; and (4) precise stopping-at-the-goal
behavior enabled by a fifth MB Output Neuron (MBON) encoding goal-views to
control velocity. This mimics the accurate homing behavior of ants and
functionally resembles waypoint-based position control in robotics, despite
relying solely on visual input. Operating at 8 Hz on a Raspberry Pi 4 with
32x32 pixel views and a memory footprint under 9 kB, our system offers a
biologically grounded, resource-efficient solution for autonomous visual
homing.

</details>


### [280] [Probabilistic Human Intent Prediction for Mobile Manipulation: An Evaluation with Human-Inspired Constraints](https://arxiv.org/abs/2507.10131)
*Cesar Alan Contreras,Manolis Chiou,Alireza Rastegarpanah,Michal Szulik,Rustam Stolkin*

Main category: cs.RO

TL;DR: GUIDER is a probabilistic framework for human-robot collaboration, estimating human intent in navigation and manipulation phases, outperforming baselines in stability and prediction speed.


<details>
  <summary>Details</summary>
Motivation: To enable seamless human-robot collaboration by accurately inferring human intent without predefined goals or conflicts.

Method: GUIDER uses dual-phase estimation: Navigation phase blends controller velocity with occupancy grids, and Manipulation phase combines saliency detection and geometric tests with real-time updates.

Result: GUIDER achieved 93-100% stability in navigation (outperforming BOIR by 39.5%) and 94-100% in manipulation (outperforming Trajectron by 31.4%), with faster intent recognition.

Conclusion: GUIDER's dual-phase framework improves intent inference in mobile manipulation tasks, validated by superior performance in trials.

Abstract: Accurate inference of human intent enables human-robot collaboration without
constraining human control or causing conflicts between humans and robots. We
present GUIDER (Global User Intent Dual-phase Estimation for Robots), a
probabilistic framework that enables a robot to estimate the intent of human
operators. GUIDER maintains two coupled belief layers, one tracking navigation
goals and the other manipulation goals. In the Navigation phase, a Synergy Map
blends controller velocity with an occupancy grid to rank interaction areas.
Upon arrival at a goal, an autonomous multi-view scan builds a local 3D cloud.
The Manipulation phase combines U2Net saliency, FastSAM instance saliency, and
three geometric grasp-feasibility tests, with an end-effector kinematics-aware
update rule that evolves object probabilities in real-time. GUIDER can
recognize areas and objects of intent without predefined goals. We evaluated
GUIDER on 25 trials (five participants x five task variants) in Isaac Sim, and
compared it with two baselines, one for navigation and one for manipulation.
Across the 25 trials, GUIDER achieved a median stability of 93-100% during
navigation, compared with 60-100% for the BOIR baseline, with an improvement of
39.5% in a redirection scenario (T5). During manipulation, stability reached
94-100% (versus 69-100% for Trajectron), with a 31.4% difference in a
redirection task (T3). In geometry-constrained trials (manipulation), GUIDER
recognized the object intent three times earlier than Trajectron (median
remaining time to confident prediction 23.6 s vs 7.8 s). These results validate
our dual-phase framework and show improvements in intent inference in both
phases of mobile manipulation tasks.

</details>


### [281] [Scene-Aware Conversational ADAS with Generative AI for Real-Time Driver Assistance](https://arxiv.org/abs/2507.10500)
*Kyungtae Han,Yitao Chen,Rohit Gupta,Onur Altintas*

Main category: cs.RO

TL;DR: SC-ADAS integrates Generative AI for scene-aware, conversational driver assistance, enabling natural language interaction and adaptive control without model fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Current ADAS lack natural language interaction and scene context interpretation, limiting flexibility in dynamic environments.

Method: SC-ADAS combines large language models, vision-to-text, and function calling for real-time, multi-turn dialogue and ADAS control.

Result: The system shows feasibility but has trade-offs like latency from vision context and token growth from dialogue history.

Conclusion: SC-ADAS advances intelligent driver assistance by merging conversational reasoning, scene perception, and modular control.

Abstract: While autonomous driving technologies continue to advance, current Advanced
Driver Assistance Systems (ADAS) remain limited in their ability to interpret
scene context or engage with drivers through natural language. These systems
typically rely on predefined logic and lack support for dialogue-based
interaction, making them inflexible in dynamic environments or when adapting to
driver intent. This paper presents Scene-Aware Conversational ADAS (SC-ADAS), a
modular framework that integrates Generative AI components including large
language models, vision-to-text interpretation, and structured function calling
to enable real-time, interpretable, and adaptive driver assistance. SC-ADAS
supports multi-turn dialogue grounded in visual and sensor context, allowing
natural language recommendations and driver-confirmed ADAS control. Implemented
in the CARLA simulator with cloud-based Generative AI, the system executes
confirmed user intents as structured ADAS commands without requiring model
fine-tuning. We evaluate SC-ADAS across scene-aware, conversational, and
revisited multi-turn interactions, highlighting trade-offs such as increased
latency from vision-based context retrieval and token growth from accumulated
dialogue history. These results demonstrate the feasibility of combining
conversational reasoning, scene perception, and modular ADAS control to support
the next generation of intelligent driver assistance.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [282] [CNeuroMod-THINGS, a densely-sampled fMRI dataset for visual neuroscience](https://arxiv.org/abs/2507.09024)
*Marie St-Laurent,Basile Pinsard,Oliver Contier,Elizabeth DuPre,Katja Seeliger,Valentina Borghesani,Julie A. Boyle,Lune Bellec,Martin N. Hebart*

Main category: q-bio.NC

TL;DR: CNeuroMod-THINGS combines THINGS and CNeuroMod to create a large-scale fMRI dataset for modeling human visual experience.


<details>
  <summary>Details</summary>
Motivation: Address the need for larger neuroimaging datasets to support data-hungry neuro-AI modeling.

Method: Integrates THINGS' annotated images with CNeuroMod's fMRI data, using a continuous recognition paradigm with 4000 images across 720 categories.

Result: High-quality behavioural and neuroimaging data from four participants, each completing 33-36 sessions.

Conclusion: CNeuroMod-THINGS enhances modeling of human visual experience by leveraging existing resources.

Abstract: Data-hungry neuro-AI modelling requires ever larger neuroimaging datasets.
CNeuroMod-THINGS meets this need by capturing neural representations for a wide
set of semantic concepts using well-characterized stimuli in a new
densely-sampled, large-scale fMRI dataset. Importantly, CNeuroMod-THINGS
exploits synergies between two existing projects: the THINGS initiative
(THINGS) and the Courtois Project on Neural Modelling (CNeuroMod). THINGS has
developed a common set of thoroughly annotated images broadly sampling natural
and man-made objects which is used to acquire a growing collection of
large-scale multimodal neural responses. Meanwhile, CNeuroMod is acquiring
hundreds of hours of fMRI data from a core set of participants during
controlled and naturalistic tasks, including visual tasks like movie watching
and videogame playing. For CNeuroMod-THINGS, four CNeuroMod participants each
completed 33-36 sessions of a continuous recognition paradigm using
approximately 4000 images from the THINGS stimulus set spanning 720 categories.
We report behavioural and neuroimaging metrics that showcase the quality of the
data. By bridging together large existing resources, CNeuroMod-THINGS expands
our capacity to model broad slices of the human visual experience.

</details>


### [283] [Self-supervised pretraining of vision transformers for animal behavioral analysis and neural encoding](https://arxiv.org/abs/2507.09513)
*Yanchen Wang,Han Yu,Ari Blau,Yizi Zhang,The International Brain Laboratory,Liam Paninski,Cole Hurwitz,Matt Whiteway*

Main category: q-bio.NC

TL;DR: BEAST is a scalable framework using self-supervised pretraining of vision transformers to improve behavioral analysis in neuroscience, outperforming traditional methods in feature extraction, pose estimation, and action segmentation.


<details>
  <summary>Details</summary>
Motivation: Traditional video analysis in neuroscience relies on labeled data, which is scarce. BEAST aims to overcome this by leveraging unlabeled data for better behavioral analysis.

Method: BEAST combines masked autoencoding and temporal contrastive learning to pretrain vision transformers, enabling effective analysis of unlabeled video data.

Result: BEAST improves performance in behavioral feature extraction, pose estimation, and action segmentation across multiple species, even with limited labeled data.

Conclusion: BEAST provides a versatile and powerful backbone model for behavioral analysis, addressing the challenge of scarce labeled data in neuroscience research.

Abstract: The brain can only be fully understood through the lens of the behavior it
generates -- a guiding principle in modern neuroscience research that
nevertheless presents significant technical challenges. Many studies capture
behavior with cameras, but video analysis approaches typically rely on
specialized models requiring extensive labeled data. We address this limitation
with BEAST (BEhavioral Analysis via Self-supervised pretraining of
Transformers), a novel and scalable framework that pretrains
experiment-specific vision transformers for diverse neuro-behavior analyses.
BEAST combines masked autoencoding with temporal contrastive learning to
effectively leverage unlabeled video data. Through comprehensive evaluation
across multiple species, we demonstrate improved performance in three critical
neuro-behavioral tasks: extracting behavioral features that correlate with
neural activity, and pose estimation and action segmentation in both the
single- and multi-animal settings. Our method establishes a powerful and
versatile backbone model that accelerates behavioral analysis in scenarios
where labeled data remains scarce.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [284] [Less Stress, More Privacy: Stress Detection on Anonymized Speech of Air Traffic Controllers](https://arxiv.org/abs/2507.08882)
*Janaki Viswanathan,Alexander Blatt,Konrad Hagemann,Dietrich Klakow*

Main category: cs.SD

TL;DR: The paper evaluates architectures for stress detection in anonymized air traffic control (ATC) voice data, achieving high accuracy while complying with privacy laws like GDPR.


<details>
  <summary>Details</summary>
Motivation: Stress detection in ATC is crucial for safety, but voice data processing faces privacy restrictions. Anonymization is proposed to address this.

Method: Different architectures for stress detection are evaluated using anonymized versions of the SUSAS dataset and an ATC simulation dataset.

Result: The best networks achieved 93.6% accuracy on the anonymized SUSAS dataset and 80.1% on the anonymized ATC dataset.

Conclusion: Privacy-compliant anonymization does not hinder the performance of deep-learning-based stress detection models in ATC.

Abstract: Air traffic control (ATC) demands multi-tasking under time pressure with high
consequences of an error. This can induce stress. Detecting stress is a key
point in maintaining the high safety standards of ATC. However, processing ATC
voice data entails privacy restrictions, e.g. the General Data Protection
Regulation (GDPR) law. Anonymizing the ATC voice data is one way to comply with
these restrictions. In this paper, different architectures for stress detection
for anonymized ATCO speech are evaluated. Our best networks reach a stress
detection accuracy of 93.6% on an anonymized version of the Speech Under
Simulated and Actual Stress (SUSAS) dataset and an accuracy of 80.1% on our
anonymized ATC simulation dataset. This shows that privacy does not have to be
an impediment in building well-performing deep-learning-based models.

</details>


### [285] [Voice Conversion for Lombard Speaking Style with Implicit and Explicit Acoustic Feature Conditioning](https://arxiv.org/abs/2507.09310)
*Dominika Woszczyk,Manuel Sam Ribeiro,Thomas Merritt,Daniel Korzekwa*

Main category: cs.SD

TL;DR: The paper explores Lombard speaking style transfer using voice conversion (VC) to improve TTS intelligibility without needing extensive Lombard speech recordings.


<details>
  <summary>Details</summary>
Motivation: Lombard TTS systems enhance intelligibility in noisy conditions or for hearing loss, but recording Lombard speech is challenging due to variability and fatigue. VC offers a solution by augmenting data.

Method: The study compares VC models with implicit and explicit acoustic feature conditioning for Lombard style transfer, aiming to preserve speaker identity and Lombard attributes.

Result: Implicit conditioning achieves intelligibility gains similar to explicit conditioning while better preserving speaker similarity.

Conclusion: Implicit conditioning is effective for Lombard style transfer, balancing intelligibility and speaker preservation without requiring explicit feature extraction.

Abstract: Text-to-Speech (TTS) systems in Lombard speaking style can improve the
overall intelligibility of speech, useful for hearing loss and noisy
conditions. However, training those models requires a large amount of data and
the Lombard effect is challenging to record due to speaker and noise
variability and tiring recording conditions. Voice conversion (VC) has been
shown to be a useful augmentation technique to train TTS systems in the absence
of recorded data from the target speaker in the target speaking style. In this
paper, we are concerned with Lombard speaking style transfer. Our goal is to
convert speaker identity while preserving the acoustic attributes that define
the Lombard speaking style. We compare voice conversion models with implicit
and explicit acoustic feature conditioning. We observe that our proposed
implicit conditioning strategy achieves an intelligibility gain comparable to
the model conditioned on explicit acoustic features, while also preserving
speaker similarity.

</details>

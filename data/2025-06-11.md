<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 79]
- [cs.CV](#cs.CV) [Total: 98]
- [cs.LG](#cs.LG) [Total: 18]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.CR](#cs.CR) [Total: 1]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.AI](#cs.AI) [Total: 6]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]
- [cs.HC](#cs.HC) [Total: 2]
- [eess.IV](#eess.IV) [Total: 7]
- [math.NA](#math.NA) [Total: 1]
- [cs.GR](#cs.GR) [Total: 5]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Conservative Bias in Large Language Models: Measuring Relation Predictions](https://arxiv.org/abs/2506.08120)
*Toyin Aguda,Erik Wilson,Allan Anzagira,Simerjot Kaur,Charese Smiley*

Main category: cs.CL

TL;DR: LLMs show conservative bias in relation extraction, often choosing 'No_Relation' to avoid errors, but this leads to information loss. The study evaluates this trade-off and introduces Hobson's choice to describe safe but uninformative labeling. Conservative bias is twice as common as hallucination.


<details>
  <summary>Details</summary>
Motivation: To understand and quantify the conservative bias in LLMs during relation extraction tasks, which leads to information loss despite preventing incorrect assignments.

Method: Systematic evaluation across prompts, datasets, and relation types, using SBERT and LLM prompts to measure semantic similarity between conservative bias behaviors.

Result: Conservative bias occurs twice as often as hallucination, highlighting a significant trade-off between safety and informativeness.

Conclusion: The study underscores the need for balancing conservative bias to minimize information loss while maintaining accuracy in relation extraction tasks.

Abstract: Large language models (LLMs) exhibit pronounced conservative bias in relation
extraction tasks, frequently defaulting to No_Relation label when an
appropriate option is unavailable. While this behavior helps prevent incorrect
relation assignments, our analysis reveals that it also leads to significant
information loss when reasoning is not explicitly included in the output. We
systematically evaluate this trade-off across multiple prompts, datasets, and
relation types, introducing the concept of Hobson's choice to capture scenarios
where models opt for safe but uninformative labels over hallucinated ones. Our
findings suggest that conservative bias occurs twice as often as hallucination.
To quantify this effect, we use SBERT and LLM prompts to capture the semantic
similarity between conservative bias behaviors in constrained prompts and
labels generated from semi-constrained and open-ended prompts.

</details>


### [2] [QA-LIGN: Aligning LLMs through Constitutionally Decomposed QA](https://arxiv.org/abs/2506.08123)
*Jacob Dineen,Aswin RRV,Qin Liu,Zhikun Xu,Xiao Ye,Ming Shen,Zhaonan Li,Shijie Lu,Chitta Baral,Muhao Chen,Ben Zhou*

Main category: cs.CL

TL;DR: QA-LIGN introduces a symbolic reward decomposition method for aligning language models with principles, improving transparency and performance.


<details>
  <summary>Details</summary>
Motivation: Standard reward-based alignment methods collapse diverse feedback into a single scalar, reducing interpretability. QA-LIGN aims to preserve the structure of each principle.

Method: QA-LIGN formulates principle-specific evaluation questions and derives separate reward components for each principle, replacing monolithic reward models.

Result: Experiments show QA-LIGN matches or outperforms DPO baselines while offering greater transparency and adaptability.

Conclusion: QA-LIGN advances interpretable and controllable alignment without sacrificing performance.

Abstract: Alignment of large language models with explicit principles (such as
helpfulness, honesty, and harmlessness) is crucial for ensuring safe and
reliable AI systems. However, standard reward-based alignment methods typically
collapse diverse feedback into a single scalar reward, entangling multiple
objectives into one opaque training signal, which hinders interpretability. In
this work, we introduce QA-LIGN, an automatic symbolic reward decomposition
approach that preserves the structure of each constitutional principle within
the reward mechanism. Instead of training a black-box reward model that outputs
a monolithic score, QA-LIGN formulates principle-specific evaluation questions
and derives separate reward components for each principle, making it a drop-in
reward model replacement. Experiments aligning an uncensored large language
model with a set of constitutional principles demonstrate that QA-LIGN offers
greater transparency and adaptability in the alignment process. At the same
time, our approach achieves performance on par with or better than a DPO
baseline. Overall, these results represent a step toward more interpretable and
controllable alignment of language models, achieved without sacrificing
end-task performance.

</details>


### [3] [EconWebArena: Benchmarking Autonomous Agents on Economic Tasks in Realistic Web Environments](https://arxiv.org/abs/2506.08136)
*Zefang Liu,Yinzhu Quan*

Main category: cs.CL

TL;DR: EconWebArena is a benchmark for evaluating autonomous agents on complex economic tasks in web environments, featuring 360 tasks from 82 authoritative sites. It tests navigation, multimodal understanding, and data extraction, with rigorous human curation. Evaluations reveal performance gaps in grounding and navigation.


<details>
  <summary>Details</summary>
Motivation: To create a realistic, authoritative benchmark for assessing autonomous agents' ability to handle complex economic tasks on live websites, addressing gaps in prior work.

Method: Tasks were generated by LLMs and curated by humans for clarity and reliability. The benchmark evaluates multimodal LLMs on navigation, interaction, and data extraction.

Result: Performance gaps were identified in grounding, navigation, and multimodal understanding, highlighting challenges for economic web intelligence.

Conclusion: EconWebArena serves as a rigorous testbed for advancing autonomous agents' economic reasoning and web interaction capabilities.

Abstract: We introduce EconWebArena, a benchmark for evaluating autonomous agents on
complex, multimodal economic tasks in realistic web environments. The benchmark
comprises 360 curated tasks from 82 authoritative websites spanning domains
such as macroeconomics, labor, finance, trade, and public policy. Each task
challenges agents to navigate live websites, interpret structured and visual
content, interact with real interfaces, and extract precise, time-sensitive
data through multi-step workflows. We construct the benchmark by prompting
multiple large language models (LLMs) to generate candidate tasks, followed by
rigorous human curation to ensure clarity, feasibility, and source reliability.
Unlike prior work, EconWebArena emphasizes fidelity to authoritative data
sources and the need for grounded web-based economic reasoning. We evaluate a
diverse set of state-of-the-art multimodal LLMs as web agents, analyze failure
cases, and conduct ablation studies to assess the impact of visual grounding,
plan-based reasoning, and interaction design. Our results reveal substantial
performance gaps and highlight persistent challenges in grounding, navigation,
and multimodal understanding, positioning EconWebArena as a rigorous testbed
for economic web intelligence.

</details>


### [4] [Multilingual Hate Speech Detection in Social Media Using Translation-Based Approaches with Large Language Models](https://arxiv.org/abs/2506.08147)
*Muhammad Usman,Muhammad Ahmad,M. Shahiki Tash,Irina Gelbukh,Rolando Quintero Tellez,Grigori Sidorov*

Main category: cs.CL

TL;DR: The paper introduces a trilingual dataset for hate speech detection in English, Urdu, and Spanish, using attention layers and transformer models to improve performance over traditional methods.


<details>
  <summary>Details</summary>
Motivation: Urdu is underexplored in hate speech detection, and translation-based approaches are lacking. The study aims to fill this gap and enhance multilingual detection.

Method: The methodology combines attention layers with transformer models (e.g., GPT-3.5 Turbo, Qwen 2.5 72B) and traditional ML models (e.g., SVM). A high-quality dataset was created with balanced labels and rigorous annotation.

Result: The approach achieved strong performance, with macro F1 scores of 0.87 (English), 0.85 (Spanish), 0.81 (Urdu), and 0.88 (multilingual), outperforming baselines by 5-9%.

Conclusion: The framework provides a robust solution for multilingual hate speech detection, contributing to safer online communities.

Abstract: Social media platforms are critical spaces for public discourse, shaping
opinions and community dynamics, yet their widespread use has amplified harmful
content, particularly hate speech, threatening online safety and inclusivity.
While hate speech detection has been extensively studied in languages like
English and Spanish, Urdu remains underexplored, especially using
translation-based approaches. To address this gap, we introduce a trilingual
dataset of 10,193 tweets in English (3,834 samples), Urdu (3,197 samples), and
Spanish (3,162 samples), collected via keyword filtering, with a balanced
distribution of 4,849 Hateful and 5,344 Not-Hateful labels. Our methodology
leverages attention layers as a precursor to transformer-based models and large
language models (LLMs), enhancing feature extraction for multilingual hate
speech detection. For non-transformer models, we use TF-IDF for feature
extraction. The dataset is benchmarked using state-of-the-art models, including
GPT-3.5 Turbo and Qwen 2.5 72B, alongside traditional machine learning models
like SVM and other transformers (e.g., BERT, RoBERTa). Three annotators,
following rigorous guidelines, ensured high dataset quality, achieving a
Fleiss' Kappa of 0.821. Our approach, integrating attention layers with GPT-3.5
Turbo and Qwen 2.5 72B, achieves strong performance, with macro F1 scores of
0.87 for English (GPT-3.5 Turbo), 0.85 for Spanish (GPT-3.5 Turbo), 0.81 for
Urdu (Qwen 2.5 72B), and 0.88 for the joint multilingual model (Qwen 2.5 72B).
These results reflect improvements of 8.75% in English (over SVM baseline
0.80), 8.97% in Spanish (over SVM baseline 0.78), 5.19% in Urdu (over SVM
baseline 0.77), and 7.32% in the joint multilingual model (over SVM baseline
0.82). Our framework offers a robust solution for multilingual hate speech
detection, fostering safer digital communities worldwide.

</details>


### [5] [ETT-CKGE: Efficient Task-driven Tokens for Continual Knowledge Graph Embedding](https://arxiv.org/abs/2506.08158)
*Lijing Zhu,Qizhen Lan,Qing Tian,Wenbo Sun,Li Yang,Lu Xia,Yixin Xie,Xi Xiao,Tiehang Duan,Cui Tao,Shuteng Niu*

Main category: cs.CL

TL;DR: ETT-CKGE introduces task-driven tokens for efficient and scalable continual knowledge graph embedding, outperforming existing methods in performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing CKGE methods are inefficient and struggle with knowledge preservation due to suboptimal scoring and expensive graph traversal.

Method: ETT-CKGE uses learnable task-driven tokens to capture task-relevant signals, avoiding explicit node scoring or traversal, and aligns embeddings via token-masked operations.

Result: ETT-CKGE achieves superior or competitive predictive performance on six benchmark datasets while significantly improving training efficiency.

Conclusion: ETT-CKGE is a scalable and efficient solution for continual knowledge graph embedding, validated by extensive experiments.

Abstract: Continual Knowledge Graph Embedding (CKGE) seeks to integrate new knowledge
while preserving past information. However, existing methods struggle with
efficiency and scalability due to two key limitations: (1) suboptimal knowledge
preservation between snapshots caused by manually designed node/relation
importance scores that ignore graph dependencies relevant to the downstream
task, and (2) computationally expensive graph traversal for node/relation
importance calculation, leading to slow training and high memory overhead. To
address these limitations, we introduce ETT-CKGE (Efficient, Task-driven,
Tokens for Continual Knowledge Graph Embedding), a novel task-guided CKGE
method that leverages efficient task-driven tokens for efficient and effective
knowledge transfer between snapshots. Our method introduces a set of learnable
tokens that directly capture task-relevant signals, eliminating the need for
explicit node scoring or traversal. These tokens serve as consistent and
reusable guidance across snapshots, enabling efficient token-masked embedding
alignment between snapshots. Importantly, knowledge transfer is achieved
through simple matrix operations, significantly reducing training time and
memory usage. Extensive experiments across six benchmark datasets demonstrate
that ETT-CKGE consistently achieves superior or competitive predictive
performance, while substantially improving training efficiency and scalability
compared to state-of-the-art CKGE methods. The code is available at:
https://github.com/lijingzhu1/ETT-CKGE/tree/main

</details>


### [6] [Can Artificial Intelligence Write Like Borges? An Evaluation Protocol for Spanish Microfiction](https://arxiv.org/abs/2506.08172)
*Gerardo Aleman Manzanarez,Nora de la Cruz Arana,Jorge Garcia Flores,Yobany Garcia Medina,Raul Monroy,Nathalie Pernelle*

Main category: cs.CL

TL;DR: The paper introduces GrAImes, a literary theory-based protocol for evaluating AI-generated microfiction, focusing on thematic coherence, clarity, depth, and aesthetics.


<details>
  <summary>Details</summary>
Motivation: Despite AI advancements in generating coherent short fiction, rigorous assessment of literary merit, especially aesthetic qualities, lacks attention.

Method: The authors propose GrAImes, an evaluation protocol grounded in literary theory, validated by literature experts and enthusiasts.

Result: The protocol provides an objective framework for assessing AI-generated microfiction, validated by expert and enthusiast feedback.

Conclusion: GrAImes serves as a foundational tool for evaluating the literary value of AI-generated microfictions.

Abstract: Automated story writing has been a subject of study for over 60 years. Large
language models can generate narratively consistent and linguistically coherent
short fiction texts. Despite these advancements, rigorous assessment of such
outputs for literary merit - especially concerning aesthetic qualities - has
received scant attention. In this paper, we address the challenge of evaluating
AI-generated microfictions and argue that this task requires consideration of
literary criteria across various aspects of the text, such as thematic
coherence, textual clarity, interpretive depth, and aesthetic quality. To
facilitate this, we present GrAImes: an evaluation protocol grounded in
literary theory, specifically drawing from a literary perspective, to offer an
objective framework for assessing AI-generated microfiction. Furthermore, we
report the results of our validation of the evaluation protocol, as answered by
both literature experts and literary enthusiasts. This protocol will serve as a
foundation for evaluating automatically generated microfictions and assessing
their literary value.

</details>


### [7] [LLM-BT: Back-Translation as a Framework for Terminology Standardization and Dynamic Semantic Embedding](https://arxiv.org/abs/2506.08174)
*Li Weigang,Pedro Carvalho Brom*

Main category: cs.CL

TL;DR: LLM-BT automates multilingual terminology standardization using LLM-powered back-translation, ensuring high consistency and cross-lingual robustness.


<details>
  <summary>Details</summary>
Motivation: Traditional expert-driven standardization struggles with rapid growth of technical terms, especially in fast-evolving fields like AI and quantum computing, necessitating automated solutions.

Method: LLM-BT employs a back-translation framework with term-level consistency validation, multi-path verification workflows, and conceptualizes back-translation as dynamic semantic embedding.

Result: Achieves over 90% exact/semantic matches, strong cross-lingual robustness (BLEU > 0.45, 100% accuracy in Portuguese), and transparent path-based embeddings.

Conclusion: LLM-BT enables human-AI collaboration for multilingual terminology standardization, ensuring semantic fidelity while allowing human cultural interpretation.

Abstract: The rapid growth of English technical terms challenges traditional
expert-driven standardization, especially in fast-evolving fields like AI and
quantum computing. Manual methods struggle to ensure multilingual consistency.
We propose \textbf{LLM-BT}, a back-translation framework powered by large
language models (LLMs) to automate terminology verification and standardization
via cross-lingual semantic alignment. Our contributions are: \textbf{(1)
Term-Level Consistency Validation:} Using English $\rightarrow$ intermediate
language $\rightarrow$ English back-translation, LLM-BT achieves high term
consistency across models (e.g., GPT-4, DeepSeek, Grok), with case studies
showing over 90\% exact or semantic matches. \textbf{(2) Multi-Path
Verification Workflow:} A novel ``Retrieve--Generate--Verify--Optimize''
pipeline integrates serial (e.g., EN $\rightarrow$ ZHcn $\rightarrow$ ZHtw
$\rightarrow$ EN) and parallel (e.g., EN $\rightarrow$ Chinese/Portuguese
$\rightarrow$ EN) BT routes. BLEU and term accuracy indicate strong
cross-lingual robustness (BLEU $>$ 0.45; Portuguese accuracy 100\%).
\textbf{(3) Back-Translation as Semantic Embedding:} BT is conceptualized as
dynamic semantic embedding, revealing latent meaning trajectories. Unlike
static embeddings, LLM-BT provides transparent path-based embeddings shaped by
model evolution. LLM-BT transforms back-translation into an active engine for
multilingual terminology standardization, enabling human--AI collaboration:
machines ensure semantic fidelity, humans guide cultural interpretation. This
infrastructure supports terminology governance across scientific and
technological fields worldwide.

</details>


### [8] [Unable to forget: Proactive lnterference Reveals Working Memory Limits in LLMs Beyond Context Length](https://arxiv.org/abs/2506.08184)
*Chupei Wang,Jiaqiu Vince Sun*

Main category: cs.CL

TL;DR: LLMs struggle with intra-context interference, where earlier information disrupts retrieval of newer updates, revealing a working memory bottleneck.


<details>
  <summary>Details</summary>
Motivation: To study the under-researched effects of intra-context interference in LLMs, inspired by cognitive science's proactive interference (PI) paradigm.

Method: Introduce PI-LLM, an evaluation that streams related key-value updates and queries final values to measure retrieval accuracy under interference.

Result: LLM retrieval accuracy declines log-linearly with interference, with errors from retrieving overwritten values. Prompt engineering mitigates poorly.

Conclusion: LLMs face a fundamental constraint in disentangling interference, suggesting a working memory bottleneck, and need better methods to suppress irrelevant content.

Abstract: Information retrieval in Large Language Models (LLMs) is increasingly
recognized as intertwined with generation capabilities rather than mere lookup.
While longer contexts are often assumed to improve retrieval, the effects of
intra-context interference remain understudied. To address this, we adapt the
proactive interference (PI) paradigm from cognitive science, where earlier
information disrupts recall of newer updates. In humans, susceptibility to such
interference is inversely linked to working memory capacity. We introduce
PI-LLM, an evaluation that sequentially streams semantically related key-value
updates and queries only the final values. Although these final values are
clearly positioned just before the query, LLM retrieval accuracy declines
log-linearly toward zero as interference accumulates; errors arise from
retrieving previously overwritten values. Attempts to mitigate interference via
prompt engineering (e.g., instructing models to ignore earlier input) yield
limited success. These findings reveal a fundamental constraint on LLMs'
ability to disentangle interference and flexibly manipulate information,
suggesting a working memory bottleneck beyond mere context access. This calls
for approaches that strengthen models' ability to suppress irrelevant content
during retrieval.

</details>


### [9] ["I Wrote, I Paused, I Rewrote" Teaching LLMs to Read Between the Lines of Student Writing](https://arxiv.org/abs/2506.08221)
*Samra Zafar,Shaheer Minhas,Syed Ali Hassan Zaidi,Arfa Naeem,Zahra Ali*

Main category: cs.CL

TL;DR: The paper explores using writing process data (keystroke logs and snapshots) to enhance LLM feedback for student essays, finding that process-aware feedback is preferred and aligns with better writing outcomes.


<details>
  <summary>Details</summary>
Motivation: Current LLM feedback lacks context on how essays are written, missing insights into student thinking and revision processes.

Method: A digital writing tool captured students' typing and essay evolution. LLMs provided feedback using final essays and writing traces, and students evaluated its usefulness.

Result: Students preferred process-aware feedback, finding it more relatable. Certain edits (e.g., adding content) correlated with higher scores in coherence and elaboration.

Conclusion: Incorporating writing process data into LLM feedback makes it more meaningful, personal, and supportive for learners.

Abstract: Large language models(LLMs) like Gemini are becoming common tools for
supporting student writing. But most of their feedback is based only on the
final essay missing important context about how that text was written. In this
paper, we explore whether using writing process data, collected through
keystroke logging and periodic snapshots, can help LLMs give feedback that
better reflects how learners think and revise while writing. We built a digital
writing tool that captures both what students type and how their essays evolve
over time. Twenty students used this tool to write timed essays, which were
then evaluated in two ways: (i) LLM generated feedback using both the final
essay and the full writing trace, and (ii) After the task, students completed
surveys about how useful and relatable they found the feedback. Early results
show that learners preferred the process-aware LLM feedback, finding it more in
tune with their own thinking. We also found that certain types of edits, like
adding new content or reorganizing paragraphs, aligned closely with higher
scores in areas like coherence and elaboration. Our findings suggest that
making LLMs more aware of the writing process can lead to feedback that feels
more meaningful, personal, and supportive.

</details>


### [10] [Compound AI Systems Optimization: A Survey of Methods, Challenges, and Future Directions](https://arxiv.org/abs/2506.08234)
*Yu-Ang Lee,Guan-Ting Yi,Mei-Yi Liu,Jui-Chao Lu,Guan-Bo Yang,Yun-Nung Chen*

Main category: cs.CL

TL;DR: A review of recent progress in optimizing compound AI systems, covering numerical and language-based techniques, challenges, and future directions.


<details>
  <summary>Details</summary>
Motivation: The increasing complexity of compound AI systems and their interactions necessitates new optimization approaches beyond traditional methods like SFT and RL.

Method: Systematic review and formalization of compound AI system optimization, classification of methods, and identification of open challenges.

Result: Comprehensive survey of optimization techniques for compound AI systems, including natural language feedback for non-differentiable systems.

Conclusion: Highlights the need for continued research to address challenges in optimizing complex AI workflows and interactions.

Abstract: Recent advancements in large language models (LLMs) and AI systems have led
to a paradigm shift in the design and optimization of complex AI workflows. By
integrating multiple components, compound AI systems have become increasingly
adept at performing sophisticated tasks. However, as these systems grow in
complexity, new challenges arise in optimizing not only individual components
but also their interactions. While traditional optimization methods such as
supervised fine-tuning (SFT) and reinforcement learning (RL) remain
foundational, the rise of natural language feedback introduces promising new
approaches, especially for optimizing non-differentiable systems. This paper
provides a systematic review of recent progress in optimizing compound AI
systems, encompassing both numerical and language-based techniques. We
formalize the notion of compound AI system optimization, classify existing
methods along several key dimensions, and highlight open research challenges
and future directions in this rapidly evolving field. A list of surveyed papers
is publicly available at https://github.com/MiuLab/AISysOpt-Survey.

</details>


### [11] [Can AI Validate Science? Benchmarking LLMs for Accurate Scientific Claim $\rightarrow$ Evidence Reasoning](https://arxiv.org/abs/2506.08235)
*Shashidhar Reddy Javaji,Yupeng Cao,Haohang Li,Yangyang Yu,Nikhil Muralidhar,Zining Zhu*

Main category: cs.CL

TL;DR: CLAIM-BENCH evaluates LLMs' ability to extract and validate scientific claims and evidence, revealing limitations and performance gaps between closed-source and open-source models.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' deeper comprehension of scientific argumentation, particularly in linking claims and evidence, which remains underexplored.

Method: Systematic comparison of three divide-and-conquer approaches across six LLMs, tested on 300+ claim-evidence pairs.

Result: Closed-source models (GPT-4, Claude) outperform open-source ones; three-pass and one-by-one prompting improves accuracy but increases computational cost.

Conclusion: CLAIM-BENCH provides a benchmark for evaluating and improving LLMs' scientific comprehension, highlighting current limitations and potential enhancements.

Abstract: Large language models (LLMs) are increasingly being used for complex research
tasks such as literature review, idea generation, and scientific paper
analysis, yet their ability to truly understand and process the intricate
relationships within complex research papers, such as the logical links between
claims and supporting evidence remains largely unexplored. In this study, we
present CLAIM-BENCH, a comprehensive benchmark for evaluating LLMs'
capabilities in scientific claim-evidence extraction and validation, a task
that reflects deeper comprehension of scientific argumentation. We
systematically compare three approaches which are inspired by divide and
conquer approaches, across six diverse LLMs, highlighting model-specific
strengths and weaknesses in scientific comprehension. Through evaluation
involving over 300 claim-evidence pairs across multiple research domains, we
reveal significant limitations in LLMs' ability to process complex scientific
content. Our results demonstrate that closed-source models like GPT-4 and
Claude consistently outperform open-source counterparts in precision and recall
across claim-evidence identification tasks. Furthermore, strategically designed
three-pass and one-by-one prompting approaches significantly improve LLMs'
abilities to accurately link dispersed evidence with claims, although this
comes at increased computational cost. CLAIM-BENCH sets a new standard for
evaluating scientific comprehension in LLMs, offering both a diagnostic tool
and a path forward for building systems capable of deeper, more reliable
reasoning across full-length papers.

</details>


### [12] [Automatic Generation of Inference Making Questions for Reading Comprehension Assessments](https://arxiv.org/abs/2506.08260)
*Wanjing Anya Ma,Michael Flor,Zuowei Wang*

Main category: cs.CL

TL;DR: The paper introduces a taxonomy for inference types in reading comprehension (RC) and evaluates GPT-4o's ability to generate diagnostic RC questions, highlighting its potential for scalable assessments.


<details>
  <summary>Details</summary>
Motivation: To improve reading instruction by providing targeted diagnostic RC questions and exploring automated generation methods.

Method: Developed a taxonomy for RC inferences, analyzed an item bank, and tested GPT-4o's question generation with few-shot prompting and chain-of-thought prompts.

Result: GPT-4o produced 93.8% good-quality questions, but only 42.6% matched the targeted inference type. High inter-rater agreement (above 0.90) was achieved.

Conclusion: Combining automated item generation with human judgment is promising for scalable, high-quality diagnostic RC assessments.

Abstract: Inference making is an essential but complex skill in reading comprehension
(RC). Some inferences require resolving references across sentences, and some
rely on using prior knowledge to fill in the detail that is not explicitly
written in the text. Diagnostic RC questions can help educators provide more
effective and targeted reading instruction and interventions for school-age
students. We introduce a taxonomy of inference types for RC and use it to
analyze the distribution of items within a diagnostic RC item bank. Next, we
present experiments using GPT-4o to generate bridging-inference RC items for
given reading passages via few-shot prompting, comparing conditions with and
without chain-of-thought prompts. Generated items were evaluated on three
aspects: overall item quality, appropriate inference type, and LLM reasoning,
achieving high inter-rater agreements above 0.90. Our results show that GPT-4o
produced 93.8% good-quality questions suitable for operational use in grade
3-12 contexts; however, only 42.6% of the generated questions accurately
matched the targeted inference type. We conclude that combining automatic item
generation with human judgment offers a promising path toward scalable,
high-quality diagnostic RC assessments.

</details>


### [13] [Institutional Books 1.0: A 242B token dataset from Harvard Library's collections, refined for accuracy and usability](https://arxiv.org/abs/2506.08300)
*Matteo Cargnelutti,Catherine Brobston,John Hess,Jack Cushman,Kristi Mukk,Aristana Scourtas,Kyle Courtney,Greg Leppert,Amanda Watson,Martha Whitehead,Jonathan Zittrain*

Main category: cs.CL

TL;DR: The paper introduces Institutional Books 1.0, a high-quality dataset of public domain books from Harvard Library, addressing the scarcity of training data for LLMs.


<details>
  <summary>Details</summary>
Motivation: The rapid development of LLMs highlights the need for sustainable, high-quality training data with clear provenance.

Method: Harvard Library's digitized books were extracted, analyzed, and processed into a documented dataset, including OCR text and metadata.

Result: A dataset of 983,004 public domain volumes (242B tokens) was released, with extensive documentation and analysis.

Conclusion: The project aims to improve accessibility and usability of historical texts for both humans and machines.

Abstract: Large language models (LLMs) use data to learn about the world in order to
produce meaningful correlations and predictions. As such, the nature, scale,
quality, and diversity of the datasets used to train these models, or to
support their work at inference time, have a direct impact on their quality.
The rapid development and adoption of LLMs of varying quality has brought into
focus the scarcity of publicly available, high-quality training data and
revealed an urgent need to ground the stewardship of these datasets in
sustainable practices with clear provenance chains. To that end, this technical
report introduces Institutional Books 1.0, a large collection of public domain
books originally digitized through Harvard Library's participation in the
Google Books project, beginning in 2006. Working with Harvard Library, we
extracted, analyzed, and processed these volumes into an extensively-documented
dataset of historic texts. This analysis covers the entirety of Harvard
Library's collection scanned as part of that project, originally spanning
1,075,899 volumes written in over 250 different languages for a total of
approximately 250 billion tokens. As part of this initial release, the
OCR-extracted text (original and post-processed) as well as the metadata
(bibliographic, source, and generated) of the 983,004 volumes, or 242B tokens,
identified as being in the public domain have been made available. This report
describes this project's goals and methods as well as the results of the
analyses we performed, all in service of making this historical collection more
accessible and easier for humans and machines alike to filter, read and use.

</details>


### [14] [Wait, We Don't Need to "Wait"! Removing Thinking Tokens Improves Reasoning Efficiency](https://arxiv.org/abs/2506.08343)
*Chenlong Wang,Yuanning Feng,Dongping Chen,Zhaoyang Chu,Ranjay Krishna,Tianyi Zhou*

Main category: cs.CL

TL;DR: NoWait reduces reasoning trajectory length by suppressing self-reflection tokens, improving efficiency without losing utility.


<details>
  <summary>Details</summary>
Motivation: To address inefficiency in large reasoning models caused by verbose, redundant outputs from overthinking.

Method: Proposes NoWait, which disables explicit self-reflection tokens (e.g., "Wait", "Hmm") during inference.

Result: Reduces chain-of-thought trajectory length by 27%-51% across ten benchmarks, preserving model utility.

Conclusion: NoWait is a plug-and-play solution for efficient, utility-preserving multimodal reasoning.

Abstract: Recent advances in large reasoning models have enabled complex, step-by-step
reasoning but often introduce significant overthinking, resulting in verbose
and redundant outputs that hinder efficiency. In this study, we examine whether
explicit self-reflection, signaled by tokens such as "Wait" and "Hmm", is
necessary for advanced reasoning. We propose NoWait, a simple yet effective
approach that disables explicit self-reflection by suppressing these tokens
during inference. Extensive experiments on ten benchmarks across textual,
visual, and video reasoning tasks show that NoWait reduces chain-of-thought
trajectory length by up to 27%-51% in five R1-style model series, without
compromising model utility. NoWait thus offers a plug-and-play solution for
efficient and utility-preserving multimodal reasoning.

</details>


### [15] [Evaluating LLMs Across Multi-Cognitive Levels: From Medical Knowledge Mastery to Scenario-Based Problem Solving](https://arxiv.org/abs/2506.08349)
*Yuxuan Zhou,Xien Liu,Chenwei Yan,Chen Ning,Xiao Zhang,Boxun Li,Xiangling Fu,Shijin Wang,Guoping Hu,Yu Wang,Ji Wu*

Main category: cs.CL

TL;DR: The paper proposes a multi-cognitive-level evaluation framework for LLMs in the medical domain, revealing performance declines with increased cognitive complexity and emphasizing the need for improved higher-level capabilities.


<details>
  <summary>Details</summary>
Motivation: To explore underexamined cognitive capabilities of LLMs in medicine and assess their performance across different cognitive levels.

Method: Developed a framework based on Bloom's Taxonomy, integrating medical datasets and tasks targeting three cognitive levels. Evaluated six LLM families systematically.

Result: Performance declines as cognitive complexity increases, with model size being more critical at higher levels.

Conclusion: Highlights the need to enhance LLMs' higher cognitive capabilities for real-world medical applications.

Abstract: Large language models (LLMs) have demonstrated remarkable performance on
various medical benchmarks, but their capabilities across different cognitive
levels remain underexplored. Inspired by Bloom's Taxonomy, we propose a
multi-cognitive-level evaluation framework for assessing LLMs in the medical
domain in this study. The framework integrates existing medical datasets and
introduces tasks targeting three cognitive levels: preliminary knowledge grasp,
comprehensive knowledge application, and scenario-based problem solving. Using
this framework, we systematically evaluate state-of-the-art general and medical
LLMs from six prominent families: Llama, Qwen, Gemma, Phi, GPT, and DeepSeek.
Our findings reveal a significant performance decline as cognitive complexity
increases across evaluated models, with model size playing a more critical role
in performance at higher cognitive levels. Our study highlights the need to
enhance LLMs' medical capabilities at higher cognitive levels and provides
insights for developing LLMs suited to real-world medical applications.

</details>


### [16] [Text Embeddings Should Capture Implicit Semantics, Not Just Surface Meaning](https://arxiv.org/abs/2506.08354)
*Yiqun Sun,Qiang Huang,Anthony K. H. Tung,Jun Yu*

Main category: cs.CL

TL;DR: The paper advocates for a shift in text embedding research to focus on implicit semantics, beyond surface-level meaning, to better align with real-world language complexity.


<details>
  <summary>Details</summary>
Motivation: Current text embedding models are overly focused on surface-level semantics, neglecting implicit meaning shaped by pragmatics, intent, and context. This limits their effectiveness in tasks requiring deeper understanding.

Method: The paper highlights the gap through a pilot study comparing state-of-the-art models to simplistic baselines on implicit semantics tasks.

Result: Even advanced models perform only marginally better than baselines on implicit semantics, underscoring the need for change.

Conclusion: The paper calls for prioritizing diverse training data, better benchmarks, and framing implicit meaning as a core goal in embedding research.

Abstract: This position paper argues that the text embedding research community should
move beyond surface meaning and embrace implicit semantics as a central
modeling goal. Text embedding models have become foundational in modern NLP,
powering a wide range of applications and drawing increasing research
attention. Yet, much of this progress remains narrowly focused on surface-level
semantics. In contrast, linguistic theory emphasizes that meaning is often
implicit, shaped by pragmatics, speaker intent, and sociocultural context.
Current embedding models are typically trained on data that lacks such depth
and evaluated on benchmarks that reward the capture of surface meaning. As a
result, they struggle with tasks requiring interpretive reasoning, speaker
stance, or social meaning. Our pilot study highlights this gap, showing that
even state-of-the-art models perform only marginally better than simplistic
baselines on implicit semantics tasks. To address this, we call for a paradigm
shift: embedding research should prioritize more diverse and linguistically
grounded training data, design benchmarks that evaluate deeper semantic
understanding, and explicitly frame implicit meaning as a core modeling
objective, better aligning embeddings with real-world language complexity.

</details>


### [17] [DEAL: Disentangling Transformer Head Activations for LLM Steering](https://arxiv.org/abs/2506.08359)
*Li-Ming Zhan,Bo Liu,Zexin Lu,Chengqiang Xie,Jiannong Cao,Xiao-Ming Wu*

Main category: cs.CL

TL;DR: A causal-attribution framework identifies behavior-relevant attention heads in transformers using VQ-AE and binary classification, improving inference-time steering in LLMs.


<details>
  <summary>Details</summary>
Motivation: Current module selection methods for LLMs rely on superficial cues, leading to suboptimal outcomes. A principled approach is needed.

Method: Train VQ-AE on attention activations, partition latent space, and assess relevance via binary classification of behavior-aligned vs. violating responses.

Result: Superior performance in truthfulness-steering tasks and strong zero-shot generalization in cross-domain scenarios.

Conclusion: The framework enables accurate inference-time interventions and generalizes well across domains.

Abstract: Inference-time steering aims to alter the response characteristics of large
language models (LLMs) without modifying their underlying parameters. A
critical step in this process is the identification of internal modules within
LLMs that are associated with the target behavior. However, current approaches
to module selection often depend on superficial cues or ad-hoc heuristics,
which can result in suboptimal or unintended outcomes. In this work, we propose
a principled causal-attribution framework for identifying behavior-relevant
attention heads in transformers. For each head, we train a vector-quantized
autoencoder (VQ-AE) on its attention activations, partitioning the latent space
into behavior-relevant and behavior-irrelevant subspaces, each quantized with a
shared learnable codebook. We assess the behavioral relevance of each head by
quantifying the separability of VQ-AE encodings for behavior-aligned versus
behavior-violating responses using a binary classification metric. This yields
a behavioral relevance score that reflects each head discriminative capacity
with respect to the target behavior, guiding both selection and importance
weighting. Experiments on seven LLMs from two model families and five
behavioral steering datasets demonstrate that our method enables more accurate
inference-time interventions, achieving superior performance on the
truthfulness-steering task. Furthermore, the heads selected by our approach
exhibit strong zero-shot generalization in cross-domain truthfulness-steering
scenarios.

</details>


### [18] [CC-RAG: Structured Multi-Hop Reasoning via Theme-Based Causal Graphs](https://arxiv.org/abs/2506.08364)
*Jash Rajesh Parekh,Pengcheng Jiang,Jiawei Han*

Main category: cs.CL

TL;DR: CC-RAG improves LLMs' causal reasoning by integrating structured causal chains into RAG, outperforming standard methods in specialized domains.


<details>
  <summary>Details</summary>
Motivation: Addressing LLMs' limitations in causal reasoning, especially in specialized domains where surface-level correlations are insufficient.

Method: Introduces CC-RAG, combining zero-shot triple extraction and theme-aware graph chaining to build a DAG for structured multi-hop inference.

Result: CC-RAG outperforms standard RAG and zero-shot LLMs in chain similarity, information density, and lexical diversity, validated by LLM and human evaluations.

Conclusion: Explicitly modeling causal structure enhances LLMs' accuracy and interpretability, particularly in domains where flat retrieval is inadequate.

Abstract: Understanding cause and effect relationships remains a formidable challenge
for Large Language Models (LLMs), particularly in specialized domains where
reasoning requires more than surface-level correlations. Retrieval-Augmented
Generation (RAG) improves factual accuracy, but standard RAG pipelines treat
evidence as flat context, lacking the structure required to model true causal
dependencies. We introduce Causal-Chain RAG (CC-RAG), a novel approach that
integrates zero-shot triple extraction and theme-aware graph chaining into the
RAG pipeline, enabling structured multi-hop inference. Given a domain specific
corpus, CC-RAG constructs a Directed Acyclic Graph (DAG) of <cause, relation,
effect> triples and uses forward/backward chaining to guide structured answer
generation. Experiments on two real-world domains: Bitcoin price fluctuations
and Gaucher disease, show that CC-RAG outperforms standard RAG and zero-shot
LLMs in chain similarity, information density, and lexical diversity. Both
LLM-as-a-Judge and human evaluations consistently favor CC-RAG. Our results
demonstrate that explicitly modeling causal structure enables LLMs to generate
more accurate and interpretable responses, especially in specialized domains
where flat retrieval fails.

</details>


### [19] [Mitigating Posterior Salience Attenuation in Long-Context LLMs with Positional Contrastive Decoding](https://arxiv.org/abs/2506.08371)
*Zikai Xiao,Ziyang Wang,Wen Ma,Yan Zhang,Wei Shen,Yan Wang,Luqi Gong,Zuozhu Liu*

Main category: cs.CL

TL;DR: The paper addresses performance degradation in LLMs for long contexts, proposing a training-free method (PCD) to mitigate the issue by leveraging attention contrasts.


<details>
  <summary>Details</summary>
Motivation: LLMs suffer performance degradation in long contexts, with current solutions being costly. The study explores statistical behaviors and cost-effective approaches.

Method: Proposes Positional Contrastive Decoding (PCD), contrasting logits from long-aware and local-aware attention to focus on gains from short-to-long training.

Result: PCD alleviates attention score degradation and achieves state-of-the-art performance on long-context benchmarks.

Conclusion: PCD offers a cost-effective solution to improve LLM performance in long contexts without additional training.

Abstract: While Large Language Models (LLMs) support long contexts, they struggle with
performance degradation within the context window. Current solutions incur
prohibitive training costs, leaving statistical behaviors and cost-effective
approaches underexplored. From the decoding perspective, we identify the
Posterior Salience Attenuation (PSA) phenomenon, where the salience ratio
correlates with long-text performance degradation. Notably, despite the
attenuation, gold tokens still occupy high-ranking positions in the decoding
space. Motivated by it, we propose the training-free Positional Contrastive
Decoding (PCD) that contrasts the logits derived from long-aware attention with
those from designed local-aware attention, enabling the model to focus on the
gains introduced by large-scale short-to-long training. Through the analysis of
long-term decay simulation, we demonstrate that PCD effectively alleviates
attention score degradation. Experimental results show that PCD achieves
state-of-the-art performance on long-context benchmarks.

</details>


### [20] [Draft-based Approximate Inference for LLMs](https://arxiv.org/abs/2506.08373)
*Kevin Galim,Ethan Ewer,Wonjun Kang,Minjae Lee,Hyung Il Koo,Kangwook Lee*

Main category: cs.CL

TL;DR: The paper proposes a novel framework using small draft models to optimize inference for long-context LLMs, introducing two methods (SpecKV and SpecPC) for more accurate token and KV pair importance prediction, outperforming existing baselines.


<details>
  <summary>Details</summary>
Motivation: The quadratic compute and linear memory complexity of Transformers in LLMs necessitates efficient inference methods, but existing approaches rely on rough predictions.

Method: The framework leverages draft models for importance prediction: SpecKV for KV cache dropping and SpecPC for prompt token compression.

Result: Experiments show higher accuracy than baselines while maintaining improvements in memory, latency, and throughput.

Conclusion: The work extends draft models' utility beyond speculative decoding, demonstrating effectiveness in approximate LLM inference.

Abstract: Optimizing inference for long-context Large Language Models (LLMs) is
increasingly important due to the quadratic compute and linear memory
complexity of Transformers. Existing approximation methods, such as key-value
(KV) cache dropping, sparse attention, and prompt compression, typically rely
on rough predictions of token or KV pair importance. We propose a novel
framework for approximate LLM inference that leverages small draft models to
more accurately predict the importance of tokens and KV pairs. Specifically, we
introduce two instantiations of our proposed framework: (i) SpecKV, which
leverages a draft output to accurately assess the importance of each KV pair
for more effective KV cache dropping, and (ii) SpecPC, which uses the draft
model's attention activations to identify and discard unimportant prompt
tokens. To the best of our knowledge, this is the first work to use draft
models for approximate LLM inference acceleration, extending their utility
beyond traditional lossless speculative decoding. We motivate our methods with
theoretical and empirical analyses, and show a strong correlation between the
attention patterns of draft and target models. Extensive experiments on
long-context benchmarks show that our methods consistently achieve higher
accuracy than existing baselines, while preserving the same improvements in
memory usage, latency, and throughput. Our code is available at
https://github.com/furiosa-ai/draft-based-approx-llm.

</details>


### [21] [EIFBENCH: Extremely Complex Instruction Following Benchmark for Large Language Models](https://arxiv.org/abs/2506.08375)
*Tao Zou,Xinghua Zhang,Haiyang Yu,Minzheng Wang,Fei Huang,Yongbin Li*

Main category: cs.CL

TL;DR: EIFBENCH is a benchmark for evaluating LLMs in multi-task, constrained scenarios, revealing performance gaps and proposing SegPO for improvement.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack complexity for real-world LLM applications, necessitating a more robust evaluation tool.

Method: Developed EIFBENCH with multi-task scenarios and constraints, and proposed SegPO algorithm for better task execution.

Result: EIFBENCH exposed significant performance gaps in LLMs under complex instructions.

Conclusion: Ongoing optimization is needed to address the challenges of complex LLM applications.

Abstract: With the development and widespread application of large language models
(LLMs), the new paradigm of "Model as Product" is rapidly evolving, and demands
higher capabilities to address complex user needs, often requiring precise
workflow execution which involves the accurate understanding of multiple tasks.
However, existing benchmarks focusing on single-task environments with limited
constraints lack the complexity required to fully reflect real-world scenarios.
To bridge this gap, we present the Extremely Complex Instruction Following
Benchmark (EIFBENCH), meticulously crafted to facilitate a more realistic and
robust evaluation of LLMs. EIFBENCH not only includes multi-task scenarios that
enable comprehensive assessment across diverse task types concurrently, but
also integrates a variety of constraints, replicating complex operational
environments. Furthermore, we propose the Segment Policy Optimization (SegPO)
algorithm to enhance the LLM's ability to accurately fulfill multi-task
workflow. Evaluations on EIFBENCH have unveiled considerable performance
discrepancies in existing LLMs when challenged with these extremely complex
instructions. This finding underscores the necessity for ongoing optimization
to navigate the intricate challenges posed by LLM applications.

</details>


### [22] [mSTEB: Massively Multilingual Evaluation of LLMs on Speech and Text Tasks](https://arxiv.org/abs/2506.08400)
*Luel Hagos Beyene,Vivek Verma,Min Ma,Jesujoba O. Alabi,Fabian David Schmidt,Joyce Nakatumba-Nabende,David Ifeoluwa Adelani*

Main category: cs.CL

TL;DR: The paper introduces mSTEB, a benchmark for evaluating LLMs on low-resource languages across tasks like language identification, text classification, QA, and translation. It reveals performance gaps between high- and low-resource languages.


<details>
  <summary>Details</summary>
Motivation: To address the lack of standardized evaluation benchmarks for low-resource languages in LLMs, especially in multimodal settings.

Method: Introduces mSTEB, a benchmark covering diverse tasks (text and speech), and evaluates leading LLMs like Gemini 2.0 Flash, GPT-4o (Audio), Qwen 2 Audio, and Gemma 3 27B.

Result: Shows significant performance gaps between high- and low-resource languages, particularly in African and Americas/Oceania languages.

Conclusion: Highlights the need for more investment to improve LLM coverage for underrepresented languages.

Abstract: Large Language models (LLMs) have demonstrated impressive performance on a
wide range of tasks, including in multimodal settings such as speech. However,
their evaluation is often limited to English and a few high-resource languages.
For low-resource languages, there is no standardized evaluation benchmark. In
this paper, we address this gap by introducing mSTEB, a new benchmark to
evaluate the performance of LLMs on a wide range of tasks covering language
identification, text classification, question answering, and translation tasks
on both speech and text modalities. We evaluated the performance of leading
LLMs such as Gemini 2.0 Flash and GPT-4o (Audio) and state-of-the-art open
models such as Qwen 2 Audio and Gemma 3 27B. Our evaluation shows a wide gap in
performance between high-resource and low-resource languages, especially for
languages spoken in Africa and Americas/Oceania. Our findings show that more
investment is needed to address their under-representation in LLMs coverage.

</details>


### [23] [TACTIC: Translation Agents with Cognitive-Theoretic Interactive Collaboration](https://arxiv.org/abs/2506.08403)
*Weiya Li,Junjie Chen,Bei Li,Boyang Liu,Zichen Wen,Nuanqiao Shan,Xiaoqian Liu,Anping Liu,Huajie Liu,Youyan Wang,Wujiuge Yin,Hu Song,Bing Huang,Zhiyuan Xia,Jialiang Chen,Linfeng Zhang*

Main category: cs.CL

TL;DR: TACTIC is a cognitively informed multi-agent framework for machine translation, improving LLM-based translation by mimicking human cognitive strategies.


<details>
  <summary>Details</summary>
Motivation: Existing multi-agent translation frameworks overlook insights from cognitive translation studies, which highlight human strategies like balancing literal/free translation and iterative refinement.

Method: Proposes TACTIC, a framework with six specialized agents (drafting, refinement, evaluation, scoring, context reasoning, knowledge gathering) to simulate human translation workflows.

Result: Outperforms GPT-4.1 and DeepSeek-R1 on FLORES-200 and WMT24 benchmarks, with gains of +0.6 XCOMET and +1.18 COMETKIWI-23 over GPT-4.1.

Conclusion: TACTIC effectively leverages LLMs for high-quality translation by integrating cognitive insights, achieving state-of-the-art results.

Abstract: Machine translation has long been a central task in natural language
processing. With the rapid advancement of large language models (LLMs), there
has been remarkable progress in translation quality. However, fully realizing
the translation potential of LLMs remains an open challenge. Recent studies
have explored multi-agent systems to decompose complex translation tasks into
collaborative subtasks, showing initial promise in enhancing translation
quality through agent cooperation and specialization. Nevertheless, existing
multi-agent translation frameworks largely neglect foundational insights from
cognitive translation studies. These insights emphasize how human translators
employ different cognitive strategies, such as balancing literal and free
translation, refining expressions based on context, and iteratively evaluating
outputs. To address this limitation, we propose a cognitively informed
multi-agent framework called TACTIC, which stands for T ranslation A gents with
Cognitive- T heoretic Interactive Collaboration. The framework comprises six
functionally distinct agents that mirror key cognitive processes observed in
human translation behavior. These include agents for drafting, refinement,
evaluation, scoring, context reasoning, and external knowledge gathering. By
simulating an interactive and theory-grounded translation workflow, TACTIC
effectively leverages the full capacity of LLMs for high-quality translation.
Experimental results on diverse language pairs from the FLORES-200 and WMT24
benchmarks show that our method consistently achieves state-of-the-art
performance. Using DeepSeek-V3 as the base model, TACTIC surpasses GPT-4.1 by
an average of +0.6 XCOMET and +1.18 COMETKIWI-23. Compared to DeepSeek-R1, it
further improves by +0.84 XCOMET and +2.99 COMETKIWI-23. Code is available at
https://github.com/weiyali126/TACTIC.

</details>


### [24] [Large Language Models Have Intrinsic Meta-Cognition, but Need a Good Lens](https://arxiv.org/abs/2506.08410)
*Ziyang Ma,Qingyue Yuan,Zhenglin Wang,Deyu Zhou*

Main category: cs.CL

TL;DR: The paper introduces AutoMeco and MIRA to evaluate and improve LLM meta-cognition, showing effectiveness on mathematical reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Existing research lacks step-level analysis of LLM meta-cognition, crucial for reliability.

Method: Proposes AutoMeco for benchmarking meta-cognition lenses and MIRA, a training-free strategy to enhance them.

Result: AutoMeco is validated against Best-of-N verification, and MIRA improves meta-cognition evaluation.

Conclusion: AutoMeco and MIRA effectively address gaps in LLM meta-cognition evaluation and enhancement.

Abstract: Previous research has primarily focused on the cognitive error detection
capabilities of Large Language Models (LLMs), often prompting them to analyze
mistakes in reasoning chains. However, few studies have examined the
meta-cognitive abilities of LLMs (e.g., their self-awareness of step errors),
which are crucial for their reliability. While studies on LLM self-evaluation
present some measures, such as perplexity, which can reflect the answer
correctness and be viewed as the lens of meta-cognition, they lack step-level
analysis and adaptation. This paper studies the evaluation of LLM
meta-cognition using the current lenses and how to improve these lenses.
Specifically, we propose AutoMeco, an Automated Meta-cognition Evaluation
framework for benchmarking the existing lenses. Furthermore, a training-free
Markovian Intrinsic Reward Adjustment strategy, MIRA, is proposed to boost
current meta-cognition lenses. Experimental results on three mathematical
reasoning datasets and three LLMs show the reasonableness of AutoMeco by
comparing it with Best-of-N verification. Moreover, the meta-cognition ability
of LLMs can be better evaluated using MIRA.

</details>


### [25] [Know-MRI: A Knowledge Mechanisms Revealer&Interpreter for Large Language Models](https://arxiv.org/abs/2506.08427)
*Jiaxiang Liu,Boxuan Xing,Chenhao Yuan,Chenxiang Zhang,Di Wu,Xiusheng Huang,Haida Yu,Chuhan Lang,Pengfei Cao,Jun Zhao,Kang Liu*

Main category: cs.CL

TL;DR: Know-MRI is an open-source tool designed to systematically analyze the knowledge mechanisms of LLMs by integrating diverse interpretation methods and consolidating outputs.


<details>
  <summary>Details</summary>
Motivation: Enhance interpretability of LLMs' internal knowledge mechanisms, addressing limitations of current fragmented interpretation methods.

Method: Developed an extensible core module to automatically match input data with interpretation methods and consolidate outputs.

Result: Enables comprehensive diagnosis of LLMs' knowledge mechanisms from multiple perspectives.

Conclusion: Know-MRI facilitates flexible and practical analysis of LLMs' knowledge, with open-source availability.

Abstract: As large language models (LLMs) continue to advance, there is a growing
urgency to enhance the interpretability of their internal knowledge mechanisms.
Consequently, many interpretation methods have emerged, aiming to unravel the
knowledge mechanisms of LLMs from various perspectives. However, current
interpretation methods differ in input data formats and interpreting outputs.
The tools integrating these methods are only capable of supporting tasks with
specific inputs, significantly constraining their practical applications. To
address these challenges, we present an open-source Knowledge Mechanisms
Revealer&Interpreter (Know-MRI) designed to analyze the knowledge mechanisms
within LLMs systematically. Specifically, we have developed an extensible core
module that can automatically match different input data with interpretation
methods and consolidate the interpreting outputs. It enables users to freely
choose appropriate interpretation methods based on the inputs, making it easier
to comprehensively diagnose the model's internal knowledge mechanisms from
multiple perspectives. Our code is available at
https://github.com/nlpkeg/Know-MRI. We also provide a demonstration video on
https://youtu.be/NVWZABJ43Bs.

</details>


### [26] [CAF-I: A Collaborative Multi-Agent Framework for Enhanced Irony Detection with Large Language Models](https://arxiv.org/abs/2506.08430)
*Ziqi. Liu,Ziyang. Zhou,Mingxuan. Hu*

Main category: cs.CL

TL;DR: CAF-I, a multi-agent LLM framework, improves sarcasm detection by addressing single-perspective limitations, lack of understanding, and interpretability issues, achieving SOTA performance.


<details>
  <summary>Details</summary>
Motivation: Existing LLM methods for sarcasm detection struggle with single-perspective analysis, insufficient understanding, and interpretability.

Method: CAF-I uses specialized agents (Context, Semantics, Rhetoric) for multidimensional analysis and collaborative optimization, with a Decision Agent and Refinement Evaluator Agent.

Result: CAF-I achieves a Macro-F1 of 76.31, a 4.98 improvement over prior baselines, demonstrating SOTA zero-shot performance.

Conclusion: CAF-I's human-like multi-perspective analysis enhances accuracy and interpretability in sarcasm detection.

Abstract: Large language model (LLM) have become mainstream methods in the field of
sarcasm detection. However, existing LLM methods face challenges in irony
detection, including: 1. single-perspective limitations, 2. insufficient
comprehensive understanding, and 3. lack of interpretability. This paper
introduces the Collaborative Agent Framework for Irony (CAF-I), an LLM-driven
multi-agent system designed to overcome these issues. CAF-I employs specialized
agents for Context, Semantics, and Rhetoric, which perform multidimensional
analysis and engage in interactive collaborative optimization. A Decision Agent
then consolidates these perspectives, with a Refinement Evaluator Agent
providing conditional feedback for optimization. Experiments on benchmark
datasets establish CAF-I's state-of-the-art zero-shot performance. Achieving
SOTA on the vast majority of metrics, CAF-I reaches an average Macro-F1 of
76.31, a 4.98 absolute improvement over the strongest prior baseline. This
success is attained by its effective simulation of human-like multi-perspective
analysis, enhancing detection accuracy and interpretability.

</details>


### [27] [Low-resource domain adaptation while minimizing energy and hardware resource consumption](https://arxiv.org/abs/2506.08433)
*Hernn Maina,Nicols Wolovick,Luciana Benotti*

Main category: cs.CL

TL;DR: The paper explores how numerical precisions and data parallelization affect training speed and accuracy for LLMs, aiming to make domain adaptation more feasible in low-resource settings.


<details>
  <summary>Details</summary>
Motivation: High costs and cultural biases in LLM training motivate the need for efficient domain adaptation methods, especially for groups with limited infrastructure.

Method: Evaluates the impact of numerical precisions and data parallelization strategies on training speed and model accuracy.

Result: Findings highlight trade-offs between efficiency and accuracy, relevant for energy-constrained or hardware-limited environments.

Conclusion: The study provides insights for improving domain adaptation accessibility in low-resource settings.

Abstract: Training Large Language Models (LLMs) is costly in terms of energy, hardware,
and annotated data, often resulting in a positionality rooted in predominant
cultures and values (Santy et al., 2023). Domain adaptation has emerged as a
promising strategy to better align models with diverse cultural and value
contexts (Hershcovich et al., 2022), but its computational cost remains a
significant barrier, particularly for research groups lacking access to
large-scale infrastructure. In this paper, we evaluate how the use of different
numerical precisions and data parallelization strategies impacts both training
speed (as a proxy to energy and hardware consumption) and model accuracy, with
the goal of facilitating domain adaptation in low-resource environments. Our
findings are relevant to any setting where energy efficiency, accessibility, or
limited hardware availability are key concerns.

</details>


### [28] [Olica: Efficient Structured Pruning of Large Language Models without Retraining](https://arxiv.org/abs/2506.08436)
*Jiujun He,Huazhen Lin*

Main category: cs.CL

TL;DR: Olica is a pruning framework for LLMs that avoids retraining by using PCA and SVD, achieving efficiency without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Existing pruning methods for LLMs are costly due to retraining needs. Olica aims to eliminate this requirement.

Method: Uses PCA on MHA layer matrix products and SVD for FFN layer error calibration, avoiding retraining.

Result: Olica reduces computational costs and maintains performance across benchmarks.

Conclusion: Olica offers an efficient, retraining-free pruning solution for LLMs.

Abstract: Most existing structured pruning methods for Large Language Models (LLMs)
require substantial computational and data resources for retraining to
reestablish the corrupted correlations, making them prohibitively expensive. To
address this, we propose a pruning framework for LLMs called Orthogonal
decomposition and Linear Calibration (Olica), which eliminates the need for
retraining. A key observation is that the multi-head attention (MHA) layer
depends on two types of matrix products. By treating these matrix products as
unified entities and applying principal component analysis (PCA), we extract
the most important information to compress LLMs without sacrificing accuracy or
disrupting their original structure. Consequently, retraining becomes
unnecessary. A fast decomposition method is devised, reducing the complexity of
PCA by a factor of the square of the number of attention heads. Additionally,
to mitigate error accumulation problem caused by pruning the feed-forward
network (FFN) layer, we introduce a linear calibration method to reconstruct
the residual errors of pruned layers using low-rank matrices. By leveraging
singular value decomposition (SVD) on the solution of the least-squares
problem, these matrices are obtained without requiring retraining. Extensive
experiments show that the proposed Olica is efficient in terms of data usage,
GPU memory, and running time, while delivering superior performance across
multiple benchmarks.

</details>


### [29] [Detecting Harmful Memes with Decoupled Understanding and Guided CoT Reasoning](https://arxiv.org/abs/2506.08477)
*Fengjun Pan,Anh Tuan Luu,Xiaobao Wu*

Main category: cs.CL

TL;DR: U-CoT+ is a framework for harmful meme detection using a meme-to-text pipeline and human-crafted guidelines for explainable, flexible, and resource-efficient classification.


<details>
  <summary>Details</summary>
Motivation: Current methods for harmful meme detection lack efficiency, flexibility, or explainability, limiting practical deployment.

Method: U-CoT+ converts memes to text descriptions, decoupling interpretation from classification, and uses zero-shot CoT prompting with human-crafted guidelines.

Result: Validated on seven datasets, the framework shows effectiveness for explainable and low-resource detection.

Conclusion: U-CoT+ offers a practical, adaptable solution for harmful meme detection with small-scale LLMs.

Abstract: Detecting harmful memes is essential for maintaining the integrity of online
environments. However, current approaches often struggle with resource
efficiency, flexibility, or explainability, limiting their practical deployment
in content moderation systems. To address these challenges, we introduce
U-CoT+, a novel framework for harmful meme detection. Instead of relying solely
on prompting or fine-tuning multimodal models, we first develop a high-fidelity
meme-to-text pipeline that converts visual memes into detail-preserving textual
descriptions. This design decouples meme interpretation from meme
classification, thus avoiding immediate reasoning over complex raw visual
content and enabling resource-efficient harmful meme detection with general
large language models (LLMs). Building on these textual descriptions, we
further incorporate targeted, interpretable human-crafted guidelines to guide
models' reasoning under zero-shot CoT prompting. As such, this framework allows
for easy adaptation to different harmfulness detection criteria across
platforms, regions, and over time, offering high flexibility and
explainability. Extensive experiments on seven benchmark datasets validate the
effectiveness of our framework, highlighting its potential for explainable and
low-resource harmful meme detection using small-scale LLMs. Codes and data are
available at: https://anonymous.4open.science/r/HMC-AF2B/README.md.

</details>


### [30] [Efficient Context Selection for Long-Context QA: No Tuning, No Iteration, Just Adaptive-$k$](https://arxiv.org/abs/2506.08479)
*Chihiro Taguchi,Seiji Maekawa,Nikita Bhutani*

Main category: cs.CL

TL;DR: Adaptive-$k$ retrieval dynamically selects the number of passages for QA tasks, outperforming fixed methods with fewer tokens.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of determining optimal context size in retrieval-augmented QA, avoiding wasted tokens or missing key evidence.

Method: Uses similarity scores between queries and passages to adaptively select context size, without fine-tuning or extra LLM inferences.

Result: Matches or outperforms fixed methods, uses 10x fewer tokens, and retrieves 70% of relevant passages.

Conclusion: Dynamic context adjustment improves QA efficiency and accuracy across models.

Abstract: Retrieval-augmented generation (RAG) and long-context language models (LCLMs)
both address context limitations of LLMs in open-domain question answering
(QA). However, optimal external context to retrieve remains an open problem:
fixing the retrieval size risks either wasting tokens or omitting key evidence.
Existing adaptive methods like Self-RAG and Self-Route rely on iterative LLM
prompting and perform well on factoid QA, but struggle with aggregation QA,
where the optimal context size is both unknown and variable. We present
Adaptive-$k$ retrieval, a simple and effective single-pass method that
adaptively selects the number of passages based on the distribution of the
similarity scores between the query and the candidate passages. It does not
require model fine-tuning, extra LLM inferences or changes to existing
retriever-reader pipelines. On both factoid and aggregation QA benchmarks,
Adaptive-$k$ matches or outperforms fixed-$k$ baselines while using up to 10x
fewer tokens than full-context input, yet still retrieves 70% of relevant
passages. It improves accuracy across five LCLMs and two embedding models,
highlighting that dynamically adjusting context size leads to more efficient
and accurate QA.

</details>


### [31] [Re-Thinking the Automatic Evaluation of Image-Text Alignment in Text-to-Image Models](https://arxiv.org/abs/2506.08480)
*Huixuan Zhang,Xiaojun Wan*

Main category: cs.CL

TL;DR: The paper critiques current text-to-image evaluation frameworks for lacking reliability and proposes improvements.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations focus on human agreement but miss other critical properties for trustworthy assessment.

Method: Identifies key aspects for reliable evaluation and tests current frameworks against them.

Result: Current frameworks fail to fully satisfy these properties across metrics and models.

Conclusion: Recommends improvements for better image-text alignment evaluation.

Abstract: Text-to-image models often struggle to generate images that precisely match
textual prompts. Prior research has extensively studied the evaluation of
image-text alignment in text-to-image generation. However, existing evaluations
primarily focus on agreement with human assessments, neglecting other critical
properties of a trustworthy evaluation framework. In this work, we first
identify two key aspects that a reliable evaluation should address. We then
empirically demonstrate that current mainstream evaluation frameworks fail to
fully satisfy these properties across a diverse range of metrics and models.
Finally, we propose recommendations for improving image-text alignment
evaluation.

</details>


### [32] [Fairness is Not Silence: Unmasking Vacuous Neutrality in Small Language Models](https://arxiv.org/abs/2506.08487)
*Sumanth Manduru,Carlotta Domeniconi*

Main category: cs.CL

TL;DR: The paper audits instruction-tuned Small Language Models (SLMs) for ethical risks, revealing that competence and fairness can coexist, bias varies by architecture, and compression introduces trade-offs.


<details>
  <summary>Details</summary>
Motivation: The rapid adoption of SLMs for resource-constrained deployments has outpaced understanding of their ethical risks, prompting the need for a large-scale audit.

Method: The study evaluates nine open-source SLMs using the BBQ benchmark under zero-shot prompting, analyzing utility and fairness across contexts.

Result: Key findings include: Phi models achieve high F1 scores with minimal bias, Qwen 2.5 models show vacuous neutrality, LLaMA 3.2 models exhibit stereotypical bias, and compression affects fairness.

Conclusion: The insights guide responsible SLM deployment, balancing fairness and efficiency, especially for small enterprises and resource-limited settings.

Abstract: The rapid adoption of Small Language Models (SLMs) for on-device and
resource-constrained deployments has outpaced our understanding of their
ethical risks. To the best of our knowledge, we present the first large-scale
audit of instruction-tuned SLMs spanning 0.5 to 5 billion parameters-an
overlooked "middle tier" between BERT-class encoders and flagship LLMs. Our
evaluation includes nine open-source models from the Qwen 2.5, LLaMA 3.2, Gemma
3, and Phi families. Using the BBQ benchmark under zero-shot prompting, we
analyze both utility and fairness across ambiguous and disambiguated contexts.
This evaluation reveals three key insights. First, competence and fairness need
not be antagonistic: Phi models achieve F1 scores exceeding 90 percent while
exhibiting minimal bias, showing that efficient and ethical NLP is attainable.
Second, social bias varies significantly by architecture: Qwen 2.5 models may
appear fair, but this often reflects vacuous neutrality, random guessing, or
evasive behavior rather than genuine ethical alignment. In contrast, LLaMA 3.2
models exhibit stronger stereotypical bias, suggesting overconfidence rather
than neutrality. Third, compression introduces nuanced trade-offs: 4-bit AWQ
quantization improves F1 scores in ambiguous settings for LLaMA 3.2-3B but
increases disability-related bias in Phi-4-Mini by over 7 percentage points.
These insights provide practical guidance for the responsible deployment of
SLMs in applications demanding fairness and efficiency, particularly benefiting
small enterprises and resource-constrained environments.

</details>


### [33] [EtiCor++: Towards Understanding Etiquettical Bias in LLMs](https://arxiv.org/abs/2506.08488)
*Ashutosh Dwivedi,Siddhant Shivdutt Singh,Ashutosh Modi*

Main category: cs.CL

TL;DR: The paper introduces EtiCor++, a corpus for evaluating LLMs' cultural sensitivity to etiquettes, and highlights biases in LLMs towards certain regions.


<details>
  <summary>Details</summary>
Motivation: To address the lack of resources for evaluating LLMs' understanding and bias regarding region-specific etiquettes.

Method: Introduces EtiCor++, a global corpus of etiquettes, and proposes tasks and metrics for evaluating LLMs.

Result: Experiments reveal inherent biases in LLMs towards certain regions.

Conclusion: The work underscores the need for culturally sensitive LLMs and provides tools for evaluating and mitigating biases.

Abstract: In recent years, researchers have started analyzing the cultural sensitivity
of LLMs. In this respect, Etiquettes have been an active area of research.
Etiquettes are region-specific and are an essential part of the culture of a
region; hence, it is imperative to make LLMs sensitive to etiquettes. However,
there needs to be more resources in evaluating LLMs for their understanding and
bias with regard to etiquettes. In this resource paper, we introduce EtiCor++,
a corpus of etiquettes worldwide. We introduce different tasks for evaluating
LLMs for knowledge about etiquettes across various regions. Further, we
introduce various metrics for measuring bias in LLMs. Extensive experimentation
with LLMs shows inherent bias towards certain regions.

</details>


### [34] [Integration of Old and New Knowledge for Generalized Intent Discovery: A Consistency-driven Prototype-Prompting Framework](https://arxiv.org/abs/2506.08490)
*Xiao Wei,Xiaobao Wang,Ning Zhuang,Chenyang Wang,Longbiao Wang,Jianwu dang*

Main category: cs.CL

TL;DR: A framework for Generalized Intent Discovery (GID) using prototype-prompting and hierarchical consistency to integrate old and new knowledge, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Supervised intent detection struggles with out-of-domain (OOD) data; GID leverages unlabeled OOD data but lacks domain adaptation.

Method: Consistency-driven prototype-prompting framework: transfers old knowledge via prompting and learns new knowledge via hierarchical consistency.

Result: Outperforms baselines, achieving state-of-the-art results, demonstrating effectiveness and generalization.

Conclusion: Proposed framework effectively integrates old and new knowledge for GID, with strong empirical performance.

Abstract: Intent detection aims to identify user intents from natural language inputs,
where supervised methods rely heavily on labeled in-domain (IND) data and
struggle with out-of-domain (OOD) intents, limiting their practical
applicability. Generalized Intent Discovery (GID) addresses this by leveraging
unlabeled OOD data to discover new intents without additional annotation.
However, existing methods focus solely on clustering unsupervised data while
neglecting domain adaptation. Therefore, we propose a consistency-driven
prototype-prompting framework for GID from the perspective of integrating old
and new knowledge, which includes a prototype-prompting framework for
transferring old knowledge from external sources, and a hierarchical
consistency constraint for learning new knowledge from target domains. We
conducted extensive experiments and the results show that our method
significantly outperforms all baseline methods, achieving state-of-the-art
results, which strongly demonstrates the effectiveness and generalization of
our methods. Our source code is publicly available at
https://github.com/smileix/cpp.

</details>


### [35] [DRAGged into Conflicts: Detecting and Addressing Conflicting Sources in Search-Augmented LLMs](https://arxiv.org/abs/2506.08500)
*Arie Cattan,Alon Jacovi,Ori Ram,Jonathan Herzig,Roee Aharoni,Sasha Goldshtein,Eran Ofek,Idan Szpektor,Avi Caciularu*

Main category: cs.CL

TL;DR: The paper introduces a taxonomy for knowledge conflicts in RAG, proposes the CONFLICTS benchmark, and shows LLMs struggle with resolving conflicts, though reasoning prompts help.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of conflicting information in retrieved sources for RAG-enhanced LLMs and provide a framework for evaluating model behavior.

Method: Proposes a taxonomy of knowledge conflict types, introduces the CONFLICTS benchmark with expert annotations, and tests LLMs with reasoning prompts.

Result: LLMs often fail to resolve conflicts, but explicit reasoning prompts improve response quality.

Conclusion: Future research is needed to further enhance LLMs' ability to handle knowledge conflicts in RAG.

Abstract: Retrieval Augmented Generation (RAG) is a commonly used approach for
enhancing large language models (LLMs) with relevant and up-to-date
information. However, the retrieved sources can often contain conflicting
information and it remains unclear how models should address such
discrepancies. In this work, we first propose a novel taxonomy of knowledge
conflict types in RAG, along with the desired model behavior for each type. We
then introduce CONFLICTS, a high-quality benchmark with expert annotations of
conflict types in a realistic RAG setting. CONFLICTS is the first benchmark
that enables tracking progress on how models address a wide range of knowledge
conflicts. We conduct extensive experiments on this benchmark, showing that
LLMs often struggle to appropriately resolve conflicts between sources. While
prompting LLMs to explicitly reason about the potential conflict in the
retrieved documents significantly improves the quality and appropriateness of
their responses, substantial room for improvement in future research remains.

</details>


### [36] [CoMuMDR: Code-mixed Multi-modal Multi-domain corpus for Discourse paRsing in conversations](https://arxiv.org/abs/2506.08504)
*Divyaksh Shukla,Ritesh Baviskar,Dwijesh Gohil,Aniket Tiwari,Atul Shree,Ashutosh Modi*

Main category: cs.CL

TL;DR: The paper introduces CoMuMDR, a code-mixed multi-modal multi-domain corpus for discourse parsing in Hindi-English conversations, highlighting the challenges faced by current models.


<details>
  <summary>Details</summary>
Motivation: Existing discourse parsing datasets are limited to single-domain written English dialogues, lacking diversity and realism.

Method: The authors create CoMuMDR, a corpus with audio, transcribed text, and nine discourse relations, and test it with state-of-the-art models.

Result: Current models perform poorly on the multi-domain code-mixed corpus, indicating its complexity.

Conclusion: Better models are needed to handle realistic, diverse discourse parsing tasks.

Abstract: Discourse parsing is an important task useful for NLU applications such as
summarization, machine comprehension, and emotion recognition. The current
discourse parsing datasets based on conversations consists of written English
dialogues restricted to a single domain. In this resource paper, we introduce
CoMuMDR: Code-mixed Multi-modal Multi-domain corpus for Discourse paRsing in
conversations. The corpus (code-mixed in Hindi and English) has both audio and
transcribed text and is annotated with nine discourse relations. We experiment
with various SoTA baseline models; the poor performance of SoTA models
highlights the challenges of multi-domain code-mixed corpus, pointing towards
the need for developing better models for such realistic settings.

</details>


### [37] [Efficient Post-Training Refinement of Latent Reasoning in Large Language Models](https://arxiv.org/abs/2506.08552)
*Xinyuan Wang,Dongjie Wang,Wangyang Ying,Haoyue Bai,Nanxu Gong,Sixun Dong,Kunpeng Liu,Yanjie Fu*

Main category: cs.CL

TL;DR: A lightweight post-training framework refines latent reasoning in LLMs using contrastive feedback and residual embedding refinement, improving accuracy without extra training.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of Chain-of-Thought prompting, such as token overhead and fixed reasoning trajectories, by refining latent reasoning processes.

Method: Proposes two strategies: contrastive reasoning feedback and residual embedding refinement to update reasoning embeddings post-training.

Result: Achieves a 5% accuracy gain on MathQA without additional training, validated across five reasoning benchmarks.

Conclusion: The framework effectively refines latent reasoning, enhancing model performance without requiring extra training.

Abstract: Reasoning is a key component of language understanding in Large Language
Models. While Chain-of-Thought prompting enhances performance via explicit
intermediate steps, it suffers from sufficient token overhead and a fixed
reasoning trajectory, preventing step-wise refinement. Recent advances in
latent reasoning address these limitations by refining internal reasoning
processes directly in the model's latent space, without producing explicit
outputs. However, a key challenge remains: how to effectively update reasoning
embeddings during post-training to guide the model toward more accurate
solutions. To overcome this challenge, we propose a lightweight post-training
framework that refines latent reasoning trajectories using two novel
strategies: 1) Contrastive reasoning feedback, which compares reasoning
embeddings against strong and weak baselines to infer effective update
directions via embedding enhancement; 2) Residual embedding refinement, which
stabilizes updates by progressively integrating current and historical
gradients, enabling fast yet controlled convergence. Extensive experiments and
case studies are conducted on five reasoning benchmarks to demonstrate the
effectiveness of the proposed framework. Notably, a 5\% accuracy gain on MathQA
without additional training.

</details>


### [38] [Neighbors and relatives: How do speech embeddings reflect linguistic connections across the world?](https://arxiv.org/abs/2506.08564)
*Tuukka Tr,Antti Suni,Juraj imko*

Main category: cs.CL

TL;DR: The study uses ML-derived speech embeddings to analyze relationships among 106 languages, showing alignment with traditional measures and highlighting scalability for low-resource languages.


<details>
  <summary>Details</summary>
Motivation: To explore linguistic relationships globally using data-driven methods, overcoming limitations of traditional approaches.

Method: Employs embeddings from XLS-R model and LDA to cluster languages, comparing results with genealogical, lexical, and geographical distances.

Result: Embedding-based distances align with traditional measures, capturing typological patterns but revealing visualization challenges.

Conclusion: The approach offers scalable analysis of language variation, with potential for studying low-resource languages and integrating sociolinguistic factors.

Abstract: Investigating linguistic relationships on a global scale requires analyzing
diverse features such as syntax, phonology and prosody, which evolve at varying
rates influenced by internal diversification, language contact, and
sociolinguistic factors. Recent advances in machine learning (ML) offer
complementary alternatives to traditional historical and typological
approaches. Instead of relying on expert labor in analyzing specific linguistic
features, these new methods enable the exploration of linguistic variation
through embeddings derived directly from speech, opening new avenues for
large-scale, data-driven analyses.
  This study employs embeddings from the fine-tuned XLS-R self-supervised
language identification model voxlingua107-xls-r-300m-wav2vec, to analyze
relationships between 106 world languages based on speech recordings. Using
linear discriminant analysis (LDA), language embeddings are clustered and
compared with genealogical, lexical, and geographical distances. The results
demonstrate that embedding-based distances align closely with traditional
measures, effectively capturing both global and local typological patterns.
Challenges in visualizing relationships, particularly with hierarchical
clustering and network-based methods, highlight the dynamic nature of language
change.
  The findings show potential for scalable analyses of language variation based
on speech embeddings, providing new perspectives on relationships among
languages. By addressing methodological considerations such as corpus size and
latent space dimensionality, this approach opens avenues for studying
low-resource languages and bridging macro- and micro-level linguistic
variation. Future work aims to extend these methods to underrepresented
languages and integrate sociolinguistic variation for a more comprehensive
understanding of linguistic diversity.

</details>


### [39] [CounselBench: A Large-Scale Expert Evaluation and Adversarial Benchmark of Large Language Models in Mental Health Counseling](https://arxiv.org/abs/2506.08584)
*Yahan Li,Jifan Yao,John Bosco S. Bunyi,Adam C. Frank,Angel Hwang,Ruishan Liu*

Main category: cs.CL

TL;DR: CounselBench evaluates LLMs in mental health counseling, showing they often outperform human therapists in quality but raise safety concerns. Human experts flag issues overlooked by LLM judges.


<details>
  <summary>Details</summary>
Motivation: To test LLMs in realistic mental health counseling scenarios, as their behavior remains largely untested despite increasing proposals for use.

Method: Developed CounselBench with 100 professionals, including CounselBench-EVAL (2,000 expert evaluations) and CounselBench-Adv (120 adversarial questions). Evaluated responses from GPT-4, LLaMA 3, Gemini, and human therapists.

Result: LLMs outperform human therapists in perceived quality but are flagged for safety concerns. LLM judges overrate responses and miss safety issues. Adversarial testing reveals consistent, model-specific failures.

Conclusion: CounselBench provides a clinically grounded framework for benchmarking and improving LLMs in high-stakes mental health settings.

Abstract: Large language models (LLMs) are increasingly proposed for use in mental
health support, yet their behavior in realistic counseling scenarios remains
largely untested. We introduce CounselBench, a large-scale benchmark developed
with 100 mental health professionals to evaluate and stress-test LLMs in
single-turn counseling. The first component, CounselBench-EVAL, contains 2,000
expert evaluations of responses from GPT-4, LLaMA 3, Gemini, and online human
therapists to real patient questions. Each response is rated along six
clinically grounded dimensions, with written rationales and span-level
annotations. We find that LLMs often outperform online human therapists in
perceived quality, but experts frequently flag their outputs for safety
concerns such as unauthorized medical advice. Follow-up experiments show that
LLM judges consistently overrate model responses and overlook safety issues
identified by human experts. To probe failure modes more directly, we construct
CounselBench-Adv, an adversarial dataset of 120 expert-authored counseling
questions designed to trigger specific model issues. Evaluation across 2,880
responses from eight LLMs reveals consistent, model-specific failure patterns.
Together, CounselBench establishes a clinically grounded framework for
benchmarking and improving LLM behavior in high-stakes mental health settings.

</details>


### [40] [Dense Retrievers Can Fail on Simple Queries: Revealing The Granularity Dilemma of Embeddings](https://arxiv.org/abs/2506.08592)
*Liyan Xu,Zhenlin Su,Mo Yu,Jiangnan Li,Fandong Meng,Jie Zhou*

Main category: cs.CL

TL;DR: The paper addresses limitations in text encoders' ability to recognize fine-grained entities/events, introduces a Chinese dataset (CapRetrieval), and proposes finetuning methods to improve performance, revealing a granularity dilemma.


<details>
  <summary>Details</summary>
Motivation: To investigate and address the failure of text encoders in recognizing fine-grained entities or events, which impacts dense retrieval tasks.

Method: Introduces CapRetrieval dataset (Chinese image captions), evaluates zero-shot performance, and finetunes encoders using proposed data generation strategies.

Result: Finetuned encoders achieve best performance on CapRetrieval, but a granularity dilemma is identified.

Conclusion: The work highlights encoder limitations, offers solutions, and releases dataset/code/models for public use.

Abstract: This work focuses on an observed limitation of text encoders: embeddings may
not be able to recognize fine-grained entities or events within the semantics,
resulting in failed dense retrieval on even simple cases. To examine such
behaviors, we first introduce a new evaluation dataset in Chinese, named
CapRetrieval, whose passages are image captions, and queries are phrases
inquiring entities or events in various forms. Zero-shot evaluation suggests
that encoders may fail on these fine-grained matching, regardless of training
sources or model sizes. Aiming for enhancement, we proceed to finetune encoders
with our proposed data generation strategies, which obtains the best
performance on CapRetrieval. Within this process, we further identify an issue
of granularity dilemma, a challenge for embeddings to express fine-grained
salience while aligning with overall semantics. Our dataset, code and models in
this work are publicly released at https://github.com/lxucs/CapRetrieval.

</details>


### [41] [Hateful Person or Hateful Model? Investigating the Role of Personas in Hate Speech Detection by Large Language Models](https://arxiv.org/abs/2506.08593)
*Shuzhou Yuan,Ercong Nie,Mario Tawfelis,Helmut Schmid,Hinrich Schtze,Michael Frber*

Main category: cs.CL

TL;DR: The paper explores how MBTI personality traits influence hate speech detection in LLMs, revealing significant persona-driven biases and inconsistencies.


<details>
  <summary>Details</summary>
Motivation: To investigate the unexplored impact of personality traits (MBTI) on LLM-based hate speech classification and annotation behavior.

Method: Conducted a human annotation survey and prompted four open-source LLMs with MBTI personas, evaluating outputs across three hate speech datasets.

Result: Found substantial persona-driven variation, including inconsistencies with ground truth, inter-persona disagreement, and logit-level biases.

Conclusion: Highlights the need for careful persona prompt definition in LLM workflows to ensure fairness and alignment with human values.

Abstract: Hate speech detection is a socially sensitive and inherently subjective task,
with judgments often varying based on personal traits. While prior work has
examined how socio-demographic factors influence annotation, the impact of
personality traits on Large Language Models (LLMs) remains largely unexplored.
In this paper, we present the first comprehensive study on the role of persona
prompts in hate speech classification, focusing on MBTI-based traits. A human
annotation survey confirms that MBTI dimensions significantly affect labeling
behavior. Extending this to LLMs, we prompt four open-source models with MBTI
personas and evaluate their outputs across three hate speech datasets. Our
analysis uncovers substantial persona-driven variation, including
inconsistencies with ground truth, inter-persona disagreement, and logit-level
biases. These findings highlight the need to carefully define persona prompts
in LLM-based annotation workflows, with implications for fairness and alignment
with human values.

</details>


### [42] [RAISE: Enhancing Scientific Reasoning in LLMs via Step-by-Step Retrieval](https://arxiv.org/abs/2506.08625)
*Minhae Oh,Jeonghye Kim,Nakyung Lee,Donggeon Seo,Taeuk Kim,Jungwoo Lee*

Main category: cs.CL

TL;DR: RAISE is a retrieval-augmented framework for scientific reasoning, outperforming baselines by retrieving logically relevant documents.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in scientific reasoning, such as domain-specific knowledge and updated findings.

Method: RAISE uses problem decomposition, logical query generation, and logical retrieval.

Result: RAISE consistently outperforms baselines on scientific reasoning benchmarks.

Conclusion: RAISE's strength lies in retrieving logically relevant documents, not just domain-similar ones.

Abstract: Scientific reasoning requires not only long-chain reasoning processes, but
also knowledge of domain-specific terminologies and adaptation to updated
findings. To deal with these challenges for scientific reasoning, we introduce
RAISE, a step-by-step retrieval-augmented framework which retrieves logically
relevant documents from in-the-wild corpus. RAISE is divided into three steps:
problem decomposition, logical query generation, and logical retrieval. We
observe that RAISE consistently outperforms other baselines on scientific
reasoning benchmarks. We analyze that unlike other baselines, RAISE retrieves
documents that are not only similar in terms of the domain knowledge, but also
documents logically more relevant.

</details>


### [43] [MEMETRON: Metaheuristic Mechanisms for Test-time Response Optimization of Large Language Models](https://arxiv.org/abs/2506.08643)
*Son The Nguyen,Theja Tulabandhula*

Main category: cs.CL

TL;DR: MEMETRON is a task-agnostic framework for optimizing LLM decoding using hybrid metaheuristic algorithms, outperforming standard methods without retraining.


<details>
  <summary>Details</summary>
Motivation: Current LLM decoding strategies lack control and task-specific optimization, limiting performance and alignment with objectives.

Method: MEMETRON uses GENETRON and ANNETRON, hybrid metaheuristic algorithms, to search the response space guided by reward models and LLM operations.

Result: MEMETRON significantly outperforms standard decoding and reranking methods, especially in human preference alignment.

Conclusion: MEMETRON offers a modular, efficient way to improve LLM alignment and performance without retraining or gradient access.

Abstract: Large language models (LLMs) are increasingly used for both open-ended and
structured tasks, yet their inference-time behavior is still largely dictated
by heuristic decoding strategies such as greedy search, sampling, or reranking.
These methods provide limited control and do not explicitly optimize for
task-specific objectives. We introduce MEMETRON, a task-agnostic framework that
formulates LLM decoding as a discrete black-box optimization problem. MEMETRON
leverages hybrid metaheuristic algorithms, GENETRON and ANNETRON, to search the
response space, guided by reward models and contextual operations performed by
the LLM itself. This approach enables efficient discovery of high-reward
responses without requiring model retraining or gradient access. The framework
is modular and generalizes across diverse tasks, requiring only a reward
function and lightweight prompt templates. We evaluate our framework on the
critical human preference alignment task and demonstrate that it significantly
outperforms standard decoding and reranking methods, highlighting its potential
to improve alignment without model retraining.

</details>


### [44] [TableDreamer: Progressive and Weakness-guided Data Synthesis from Scratch for Table Instruction Tuning](https://arxiv.org/abs/2506.08646)
*Mingyu Zheng,Zhifan Feng,Jia Wang,Lanrui Wang,Zheng Lin,Yang Hao,Weiping Wang*

Main category: cs.CL

TL;DR: TableDreamer is a progressive, weakness-guided framework for synthesizing table instruction tuning data, addressing diversity and efficiency issues in LLM-based methods.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based data synthesis methods lack diversity in table understanding tasks and ignore the target LLM's weaknesses, leading to suboptimal efficiency.

Method: TableDreamer synthesizes diverse seed data and iteratively explores the input space guided by identified weaknesses, refining the training data.

Result: Experiments show TableDreamer boosts Llama3.1-8B-instruct's accuracy by 11.62% with 27K synthetic data, outperforming baselines.

Conclusion: TableDreamer effectively improves table instruction tuning by addressing diversity and efficiency, validated by superior performance on benchmarks.

Abstract: Despite the commendable progress of recent LLM-based data synthesis methods,
they face two limitations in generating table instruction tuning data. First,
they can not thoroughly explore the vast input space of table understanding
tasks, leading to limited data diversity. Second, they ignore the weaknesses in
table understanding ability of the target LLM and blindly pursue the increase
of data quantity, resulting in suboptimal data efficiency. In this paper, we
introduce a progressive and weakness-guided data synthesis framework tailored
for table instruction tuning, named TableDreamer, to mitigate the above issues.
Specifically, we first synthesize diverse tables and related instructions as
seed data, and then perform an iterative exploration of the input space under
the guidance of the newly identified weakness data, which eventually serve as
the final training data for fine-tuning the target LLM. Extensive experiments
on 10 tabular benchmarks demonstrate the effectiveness of the proposed
framework, which boosts the average accuracy of Llama3.1-8B-instruct by 11.62%
(49.07% to 60.69%) with 27K GPT-4o synthetic data and outperforms
state-of-the-art data synthesis baselines which use more training data. The
code and data is available at https://github.com/SpursGoZmy/TableDreamer

</details>


### [45] [Summarization for Generative Relation Extraction in the Microbiome Domain](https://arxiv.org/abs/2506.08647)
*Oumaima El Khettari,Solen Quiniou,Samuel Chaffron*

Main category: cs.CL

TL;DR: A generative relation extraction method for the intestinal microbiome uses LLM summarization to improve performance, though BERT-based methods still outperform it.


<details>
  <summary>Details</summary>
Motivation: To study interactions in the low-resource intestinal microbiome domain using generative relation extraction.

Method: Uses summarization with LLMs to refine context before relation extraction via instruction-tuned generation.

Result: Summarization improves generative RE performance, but BERT-based methods remain superior.

Conclusion: Generative methods show promise for specialized, low-resource domains.

Abstract: We explore a generative relation extraction (RE) pipeline tailored to the
study of interactions in the intestinal microbiome, a complex and low-resource
biomedical domain. Our method leverages summarization with large language
models (LLMs) to refine context before extracting relations via
instruction-tuned generation. Preliminary results on a dedicated corpus show
that summarization improves generative RE performance by reducing noise and
guiding the model. However, BERT-based RE approaches still outperform
generative models. This ongoing work demonstrates the potential of generative
methods to support the study of specialized domains in low-resources setting.

</details>


### [46] [RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic Sampling](https://arxiv.org/abs/2506.08672)
*Yang Liu,Jiaqi Li,Zilong Zheng*

Main category: cs.CL

TL;DR: RuleReasoner, a reinforced rule-based reasoning method, outperforms large reasoning models (LRMs) with better generalization and efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of small reasoning models (SRMs) learning rule-based reasoning effectively across diverse tasks and domains.

Method: Uses a domain-aware dynamic sampling approach for reinforcement learning (RL), updating sampling weights based on historical rewards.

Result: Outperforms LRMs by 4.1% on in-distribution and 10.4% on out-of-distribution tasks, with higher computational efficiency.

Conclusion: RuleReasoner is a simple, effective, and efficient method for rule-based reasoning, demonstrating robust generalization.

Abstract: Rule-based reasoning has been acknowledged as one of the fundamental problems
in reasoning, while deviations in rule formats, types, and complexity in
real-world applications pose severe challenges. Recent studies have shown that
large reasoning models (LRMs) have remarkable reasoning capabilities, and their
performance is substantially enhanced by reinforcement learning (RL). However,
it remains an open question whether small reasoning models (SRMs) can learn
rule-based reasoning effectively with robust generalization across diverse
tasks and domains. To address this, we introduce Reinforced Rule-based
Reasoning, a.k.a. RuleReasoner, a simple yet effective method to conduct
rule-based reasoning via a wide collection of curated tasks and a novel
domain-aware dynamic sampling approach. Specifically, RuleReasoner resamples
each training batch by updating the sampling weights of different domains based
on historical rewards. This facilitates domain augmentation and flexible online
learning schedules for RL, obviating the need for pre-hoc human-engineered
mix-training recipes used in existing methods. Empirical evaluations on
in-distribution (ID) and out-of-distribution (OOD) benchmarks reveal that
RuleReasoner outperforms frontier LRMs by a significant margin ($\Delta$4.1%
average points on eight ID tasks and $\Delta$10.4% average points on three OOD
tasks over OpenAI-o1). Notably, our approach also exhibits higher computational
efficiency compared to prior dynamic sampling methods for RL.

</details>


### [47] [Brevity is the soul of sustainability: Characterizing LLM response lengths](https://arxiv.org/abs/2506.08686)
*Soham Poddar,Paramita Koley,Janardan Misra,Sanjay Podder,Navveen Balani,Niloy Ganguly,Saptarshi Ghosh*

Main category: cs.CL

TL;DR: The paper benchmarks LLMs, identifies excessive response lengths, and proposes prompt-engineering strategies to reduce energy consumption by 25-60% without compromising response quality.


<details>
  <summary>Details</summary>
Motivation: Energy consumption in LLM inference is high, and output compression is understudied. The paper aims to address this by optimizing response lengths.

Method: Benchmarked 12 decoder-only LLMs across 5 datasets, analyzed response quality, and tested prompt-engineering strategies for length reduction.

Result: LLMs often produce overly long responses with redundant information. Targeted prompts reduced response lengths, achieving 25-60% energy savings.

Conclusion: Simple prompt-engineering can significantly optimize LLM energy use by reducing unnecessary response length while maintaining quality.

Abstract: A significant portion of the energy consumed by Large Language Models (LLMs)
arises from their inference processes; hence developing energy-efficient
methods for inference is crucial. While several techniques exist for inference
optimization, output compression remains relatively unexplored, with only a few
preliminary efforts addressing this aspect. In this work, we first benchmark 12
decoder-only LLMs across 5 datasets, revealing that these models often produce
responses that are substantially longer than necessary. We then conduct a
comprehensive quality assessment of LLM responses, formally defining six
information categories present in LLM responses. We show that LLMs often tend
to include redundant or additional information besides the minimal answer. To
address this issue of long responses by LLMs, we explore several simple and
intuitive prompt-engineering strategies. Empirical evaluation shows that
appropriate prompts targeting length reduction and controlling information
content can achieve significant energy optimization between 25-60\% by reducing
the response length while preserving the quality of LLM responses.

</details>


### [48] [ClimateViz: A Benchmark for Statistical Reasoning and Fact Verification on Scientific Charts](https://arxiv.org/abs/2506.08700)
*Ruiran Su,Jiasheng Si,Zhijiang Guo,Janet B. Pierrehumbert*

Main category: cs.CL

TL;DR: ClimateViz introduces a benchmark for scientific fact-checking using charts, revealing current models' limitations in chart-based reasoning compared to humans.


<details>
  <summary>Details</summary>
Motivation: Existing fact-checking overlooks scientific charts, which are crucial for quantitative evidence. ClimateViz addresses this gap.

Method: The benchmark includes 49,862 claims linked to 2,896 charts, labeled with support, refute, or not enough information. Structured knowledge graphs provide explanations.

Result: Top models (Gemini 2.5, InternVL 2.5) achieve 76.2-77.8% accuracy, below human performance (89.3-92.7%). Explanation-augmented outputs help some models.

Conclusion: ClimateViz highlights the need for better multimodal reasoning in scientific fact-checking and provides a dataset for future research.

Abstract: Scientific fact-checking has mostly focused on text and tables, overlooking
scientific charts, which are key for presenting quantitative evidence and
statistical reasoning. We introduce ClimateViz, the first large-scale benchmark
for scientific fact-checking using expert-curated scientific charts. ClimateViz
contains 49,862 claims linked to 2,896 visualizations, each labeled as support,
refute, or not enough information. To improve interpretability, each example
includes structured knowledge graph explanations covering trends, comparisons,
and causal relations. We evaluate state-of-the-art multimodal language models,
including both proprietary and open-source systems, in zero-shot and few-shot
settings. Results show that current models struggle with chart-based reasoning:
even the best systems, such as Gemini 2.5 and InternVL 2.5, reach only 76.2 to
77.8 percent accuracy in label-only settings, far below human performance (89.3
and 92.7 percent). Explanation-augmented outputs improve performance in some
models. We released our dataset and code alongside the paper.

</details>


### [49] [ConfPO: Exploiting Policy Model Confidence for Critical Token Selection in Large Language Model Preference Optimization](https://arxiv.org/abs/2506.08712)
*Hee Suk Yoon,Eunseop Yoon,Mark A. Hasegawa-Johnson,Sungwoong Kim,Chang D. Yoo*

Main category: cs.CL

TL;DR: ConfPO is a lightweight, model-free method for preference learning in LLMs that optimizes preference-critical tokens based on training policy confidence, outperforming uniform methods like DPO.


<details>
  <summary>Details</summary>
Motivation: Prior methods like DPO uniformly adjust token probabilities, risking inefficiency and overoptimization. ConfPO aims to improve alignment quality by focusing on impactful tokens.

Method: ConfPO identifies and optimizes preference-critical tokens using the training policy's confidence, avoiding auxiliary models or compute.

Result: ConfPO outperforms uniform DAAs on benchmarks like AlpacaEval 2 and Arena-Hard, achieving better alignment with no extra computational cost.

Conclusion: ConfPO offers a simple, efficient alternative to uniform preference learning methods, enhancing alignment without additional overhead.

Abstract: We introduce ConfPO, a method for preference learning in Large Language
Models (LLMs) that identifies and optimizes preference-critical tokens based
solely on the training policy's confidence, without requiring any auxiliary
models or compute. Unlike prior Direct Alignment Algorithms (DAAs) such as
Direct Preference Optimization (DPO), which uniformly adjust all token
probabilities regardless of their relevance to preference, ConfPO focuses
optimization on the most impactful tokens. This targeted approach improves
alignment quality while mitigating overoptimization (i.e., reward hacking) by
using the KL divergence budget more efficiently. In contrast to recent
token-level methods that rely on credit-assignment models or AI annotators,
raising concerns about scalability and reliability, ConfPO is simple,
lightweight, and model-free. Experimental results on challenging alignment
benchmarks, including AlpacaEval 2 and Arena-Hard, demonstrate that ConfPO
consistently outperforms uniform DAAs across various LLMs, delivering better
alignment with zero additional computational overhead.

</details>


### [50] [Explainable Compliance Detection with Multi-Hop Natural Language Inference on Assurance Case Structure](https://arxiv.org/abs/2506.08713)
*Fariz Ikhwantri,Dusica Marijan*

Main category: cs.CL

TL;DR: The paper proposes EXCLAIM, an NLI-based approach for explainable compliance detection in assurance cases, addressing challenges like complex texts and data scarcity by using LLMs for case generation and multi-hop inference.


<details>
  <summary>Details</summary>
Motivation: Challenges in validating assurance cases include complex legal/technical texts, need for explanations, and limited data. Automating compliance detection is crucial.

Method: EXCLAIM uses NLI for multi-hop inference in assurance cases, generates cases with LLMs, and introduces metrics for coverage and consistency.

Result: Demonstrated effectiveness with GDPR requirements, showing NLI's potential in automating compliance.

Conclusion: NLI-based approaches like EXCLAIM can automate and improve regulatory compliance processes.

Abstract: Ensuring complex systems meet regulations typically requires checking the
validity of assurance cases through a claim-argument-evidence framework. Some
challenges in this process include the complicated nature of legal and
technical texts, the need for model explanations, and limited access to
assurance case data. We propose a compliance detection approach based on
Natural Language Inference (NLI): EXplainable CompLiance detection with
Argumentative Inference of Multi-hop reasoning (EXCLAIM). We formulate the
claim-argument-evidence structure of an assurance case as a multi-hop inference
for explainable and traceable compliance detection. We address the limited
number of assurance cases by generating them using large language models
(LLMs). We introduce metrics that measure the coverage and structural
consistency. We demonstrate the effectiveness of the generated assurance case
from GDPR requirements in a multi-hop inference task as a case study. Our
results highlight the potential of NLI-based approaches in automating the
regulatory compliance process.

</details>


### [51] [Multi-Teacher Language-Aware Knowledge Distillation for Multilingual Speech Emotion Recognition](https://arxiv.org/abs/2506.08717)
*Mehedi Hasan Bijoy,Dejan Porjazovski,Tams Grsz,Mikko Kurimo*

Main category: cs.CL

TL;DR: A novel language-aware multi-teacher knowledge distillation method is introduced for multilingual Speech Emotion Recognition (SER), achieving state-of-the-art performance in English, Finnish, and French.


<details>
  <summary>Details</summary>
Motivation: Extending monolingual SER to multilingual systems is challenging, and the goal is to train a single model for multilingual SER by distilling knowledge from multiple teacher models.

Method: The method uses Wav2Vec2.0 as the foundation for monolingual teacher models and distills their knowledge into a single multilingual student model.

Result: The student model achieves a weighted recall of 72.9 on English and 63.4 on Finnish datasets, outperforming baselines. It excels in recognizing sad and neutral emotions but struggles with anger and happiness.

Conclusion: The proposed method advances multilingual SER performance but has limitations in recognizing specific emotions like anger and happiness.

Abstract: Speech Emotion Recognition (SER) is crucial for improving human-computer
interaction. Despite strides in monolingual SER, extending them to build a
multilingual system remains challenging. Our goal is to train a single model
capable of multilingual SER by distilling knowledge from multiple teacher
models. To address this, we introduce a novel language-aware multi-teacher
knowledge distillation method to advance SER in English, Finnish, and French.
It leverages Wav2Vec2.0 as the foundation of monolingual teacher models and
then distills their knowledge into a single multilingual student model. The
student model demonstrates state-of-the-art performance, with a weighted recall
of 72.9 on the English dataset and an unweighted recall of 63.4 on the Finnish
dataset, surpassing fine-tuning and knowledge distillation baselines. Our
method excels in improving recall for sad and neutral emotions, although it
still faces challenges in recognizing anger and happiness.

</details>


### [52] [Improved LLM Agents for Financial Document Question Answering](https://arxiv.org/abs/2506.08726)
*Nelvin Tan,Zian Seng,Liang Zhang,Yu-Ching Shih,Dong Yang,Amol Salunkhe*

Main category: cs.CL

TL;DR: The paper explores the limitations of traditional critic agents in numerical QA for financial documents without oracle labels and introduces an improved critic agent and calculator agent, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with numerical QA in financial documents, especially without oracle labels, prompting the need for better critic agents.

Method: The paper proposes an improved critic agent and a calculator agent, analyzing their interaction and performance.

Result: The new agents outperform the state-of-the-art (program-of-thought) and are safer.

Conclusion: The improved critic and calculator agents enhance performance in numerical QA for financial documents, especially without oracle labels.

Abstract: Large language models (LLMs) have shown impressive capabilities on numerous
natural language processing tasks. However, LLMs still struggle with numerical
question answering for financial documents that include tabular and textual
data. Recent works have showed the effectiveness of critic agents (i.e.,
self-correction) for this task given oracle labels. Building upon this
framework, this paper examines the effectiveness of the traditional critic
agent when oracle labels are not available, and show, through experiments, that
this critic agent's performance deteriorates in this scenario. With this in
mind, we present an improved critic agent, along with the calculator agent
which outperforms the previous state-of-the-art approach (program-of-thought)
and is safer. Furthermore, we investigate how our agents interact with each
other, and how this interaction affects their performance.

</details>


### [53] [Societal AI Research Has Become Less Interdisciplinary](https://arxiv.org/abs/2506.08738)
*Dror Kris Markus,Fabrizio Gilardi,Daria Stetsenko*

Main category: cs.CL

TL;DR: The study analyzes AI papers to see how ethical and societal values are integrated, finding that computer science-only teams are increasingly addressing societal concerns, challenging assumptions about interdisciplinary collaboration.


<details>
  <summary>Details</summary>
Motivation: To examine whether interdisciplinary teams are leading the integration of ethical and societal values in AI research, as commonly assumed.

Method: Analyzed over 100,000 AI-related papers on ArXiv (2014-2024), developed a classifier to identify societal content, and measured its prevalence.

Result: Interdisciplinary teams still produce more societally-oriented research, but computer science-only teams are catching up, addressing domains like fairness, safety, healthcare, and misinformation.

Conclusion: The findings challenge assumptions about interdisciplinary collaboration's role in societal AI and raise questions about AI governance and the unique contributions of social sciences and humanities.

Abstract: As artificial intelligence (AI) systems become deeply embedded in everyday
life, calls to align AI development with ethical and societal values have
intensified. Interdisciplinary collaboration is often championed as a key
pathway for fostering such engagement. Yet it remains unclear whether
interdisciplinary research teams are actually leading this shift in practice.
This study analyzes over 100,000 AI-related papers published on ArXiv between
2014 and 2024 to examine how ethical values and societal concerns are
integrated into technical AI research. We develop a classifier to identify
societal content and measure the extent to which research papers express these
considerations. We find a striking shift: while interdisciplinary teams remain
more likely to produce societally-oriented research, computer science-only
teams now account for a growing share of the field's overall societal output.
These teams are increasingly integrating societal concerns into their papers
and tackling a wide range of domains - from fairness and safety to healthcare
and misinformation. These findings challenge common assumptions about the
drivers of societal AI and raise important questions. First, what are the
implications for emerging understandings of AI safety and governance if most
societally-oriented research is being undertaken by exclusively technical
teams? Second, for scholars in the social sciences and humanities: in a
technical field increasingly responsive to societal demands, what distinctive
perspectives can we still offer to help shape the future of AI?

</details>


### [54] [Towards Secure and Private Language Models for Nuclear Power Plants](https://arxiv.org/abs/2506.08746)
*Muhammad Anwar,Mishca de Costa,Issam Hammad,Daniel Lau*

Main category: cs.CL

TL;DR: A domain-specific LLM for nuclear applications is developed using a compact Transformer, trained on a single GPU for data security. It captures nuclear vocabulary but lacks syntactic coherence, showing potential for specialized tasks with further improvements.


<details>
  <summary>Details</summary>
Motivation: To create an in-house LLM for nuclear applications that adheres to cybersecurity and data confidentiality standards, leveraging limited but specialized data.

Method: Uses a compact Transformer-based architecture trained on a single GPU with data from the Essential CANDU textbook.

Result: The model captures specialized nuclear vocabulary but sometimes lacks syntactic coherence, indicating potential for specialized tasks.

Conclusion: The approach is feasible but requires richer corpora, better preprocessing, and fine-tuning for real-world nuclear applications.

Abstract: This paper introduces a domain-specific Large Language Model for nuclear
applications, built from the publicly accessible Essential CANDU textbook.
Drawing on a compact Transformer-based architecture, the model is trained on a
single GPU to protect the sensitive data inherent in nuclear operations.
Despite relying on a relatively small dataset, it shows encouraging signs of
capturing specialized nuclear vocabulary, though the generated text sometimes
lacks syntactic coherence. By focusing exclusively on nuclear content, this
approach demonstrates the feasibility of in-house LLM solutions that align with
rigorous cybersecurity and data confidentiality standards. Early successes in
text generation underscore the model's utility for specialized tasks, while
also revealing the need for richer corpora, more sophisticated preprocessing,
and instruction fine-tuning to enhance domain accuracy. Future directions
include extending the dataset to cover diverse nuclear subtopics, refining
tokenization to reduce noise, and systematically evaluating the model's
readiness for real-world applications in nuclear domain.

</details>


### [55] [Unlocking the Potential of Large Language Models in the Nuclear Industry with Synthetic Data](https://arxiv.org/abs/2506.08750)
*Muhammad Anwar,Daniel Lau,Mishca de Costa,Issam Hammad*

Main category: cs.CL

TL;DR: The paper proposes using synthetic data generation to convert unstructured nuclear industry text into structured Q&A pairs for LLM applications, addressing data scarcity and privacy issues.


<details>
  <summary>Details</summary>
Motivation: The nuclear industry has valuable unstructured text data, but it's unusable for LLM tasks. Synthetic data can bridge this gap.

Method: Leverage LLMs to analyze text, extract key info, generate Q&A pairs, and evaluate dataset quality.

Result: Synthetic data enables robust LLMs for the nuclear domain, improving info retrieval and decision-making.

Conclusion: Synthetic data unlocks LLM potential in the nuclear industry, enhancing knowledge sharing and decision-making.

Abstract: The nuclear industry possesses a wealth of valuable information locked away
in unstructured text data. This data, however, is not readily usable for
advanced Large Language Model (LLM) applications that require clean, structured
question-answer pairs for tasks like model training, fine-tuning, and
evaluation. This paper explores how synthetic data generation can bridge this
gap, enabling the development of robust LLMs for the nuclear domain. We discuss
the challenges of data scarcity and privacy concerns inherent in the nuclear
industry and how synthetic data provides a solution by transforming existing
text data into usable Q&A pairs. This approach leverages LLMs to analyze text,
extract key information, generate relevant questions, and evaluate the quality
of the resulting synthetic dataset. By unlocking the potential of LLMs in the
nuclear industry, synthetic data can pave the way for improved information
retrieval, enhanced knowledge sharing, and more informed decision-making in
this critical sector.

</details>


### [56] [Factors affecting the in-context learning abilities of LLMs for dialogue state tracking](https://arxiv.org/abs/2506.08753)
*Pradyoth Hegde,Santosh Kesiraju,Jan vec,imon Sedlek,Bolaji Yusuf,Oldich Plchot,Deepak K T,Jan ernock*

Main category: cs.CL

TL;DR: The paper investigates in-context learning (ICL) for dialogue state tracking (DST), using a k-nearest neighbor method for demonstration retrieval and analyzing factors affecting performance on the MultiWoZ2.4 dataset with OLMo-7B, Mistral-7B, and Llama3.2-3B models.


<details>
  <summary>Details</summary>
Motivation: To explore how ICL can be effectively applied to DST and identify factors influencing its success.

Method: Uses a sentence embedding-based k-nearest neighbor approach to retrieve demonstrations, structures them with test samples in a template, and systematically studies demonstration selection and prompt context impacts.

Result: Provides insights into the in-context learning capabilities of LLMs for DST, highlighting key influencing factors.

Conclusion: The study offers practical insights for improving ICL-based DST, demonstrating its potential with specific LLMs.

Abstract: This study explores the application of in-context learning (ICL) to the
dialogue state tracking (DST) problem and investigates the factors that
influence its effectiveness. We use a sentence embedding based k-nearest
neighbour method to retrieve the suitable demonstrations for ICL. The selected
demonstrations, along with the test samples, are structured within a template
as input to the LLM. We then conduct a systematic study to analyse the impact
of factors related to demonstration selection and prompt context on DST
performance. This work is conducted using the MultiWoZ2.4 dataset and focuses
primarily on the OLMo-7B-instruct, Mistral-7B-Instruct-v0.3, and
Llama3.2-3B-Instruct models. Our findings provide several useful insights on
in-context learning abilities of LLMs for dialogue state tracking.

</details>


### [57] [Enhancing Accuracy and Maintainability in Nuclear Plant Data Retrieval: A Function-Calling LLM Approach Over NL-to-SQL](https://arxiv.org/abs/2506.08757)
*Mishca de Costa,Muhammad Anwar,Dave Mercier,Mark Randall,Issam Hammad*

Main category: cs.CL

TL;DR: Proposes a function-based LLM approach for nuclear plant data retrieval, replacing direct NL-to-SQL to improve accuracy and trust.


<details>
  <summary>Details</summary>
Motivation: Traditional NL-to-SQL poses risks in nuclear plants due to unvalidated queries and complex legacy databases.

Method: Uses pre-approved functions encapsulating SQL logic, validated by experts, with NL-to-SQL aiding initial function generation.

Result: Shows improved accuracy and maintainability over direct NL-to-SQL.

Conclusion: Balances user accessibility with safety, offering a robust framework for critical systems.

Abstract: Retrieving operational data from nuclear power plants requires exceptional
accuracy and transparency due to the criticality of the decisions it supports.
Traditionally, natural language to SQL (NL-to-SQL) approaches have been
explored for querying such data. While NL-to-SQL promises ease of use, it poses
significant risks: end-users cannot easily validate generated SQL queries, and
legacy nuclear plant databases -- often complex and poorly structured --
complicate query generation due to decades of incremental modifications. These
challenges increase the likelihood of inaccuracies and reduce trust in the
approach. In this work, we propose an alternative paradigm: leveraging
function-calling large language models (LLMs) to address these challenges.
Instead of directly generating SQL queries, we define a set of pre-approved,
purpose-specific functions representing common use cases. Queries are processed
by invoking these functions, which encapsulate validated SQL logic. This hybrid
approach mitigates the risks associated with direct NL-to-SQL translations by
ensuring that SQL queries are reviewed and optimized by experts before
deployment. While this strategy introduces the upfront cost of developing and
maintaining the function library, we demonstrate how NL-to-SQL tools can assist
in the initial generation of function code, allowing experts to focus on
validation rather than creation. Our study includes a performance comparison
between direct NL-to-SQL generation and the proposed function-based approach,
highlighting improvements in accuracy and maintainability. This work
underscores the importance of balancing user accessibility with operational
safety and provides a novel, actionable framework for robust data retrieval in
critical systems.

</details>


### [58] [AraReasoner: Evaluating Reasoning-Based LLMs for Arabic NLP](https://arxiv.org/abs/2506.08768)
*Ahmed Hasanaath,Aisha Alansari,Ahmed Ashraf,Chafik Salmane,Hamzah Luqman,Saad Ezzini*

Main category: cs.CL

TL;DR: The paper benchmarks reasoning-focused LLMs, especially DeepSeek models, on Arabic NLP tasks, showing significant performance improvements with strategic example selection, DeepSeek's superiority over GPT-4-mini, and LoRA fine-tuning benefits.


<details>
  <summary>Details</summary>
Motivation: To explore the underexplored performance of LLMs on Arabic data, given its linguistic complexity, and evaluate reasoning-focused models like DeepSeek.

Method: Benchmarking multiple LLMs using zero-shot, few-shot, and fine-tuning strategies across 15 Arabic NLP tasks.

Result: Key findings include a 13 F1 uplift with few-shot examples, DeepSeek outperforming GPT-4-mini by 12 F1 points, and LoRA fine-tuning adding 8 F1/BLEU points.

Conclusion: Strategic approaches and specialized models like DeepSeek significantly enhance LLM performance on complex Arabic NLP tasks.

Abstract: Large language models (LLMs) have shown remarkable progress in reasoning
abilities and general natural language processing (NLP) tasks, yet their
performance on Arabic data, characterized by rich morphology, diverse dialects,
and complex script, remains underexplored. This paper presents a comprehensive
benchmarking study of multiple reasoning-focused LLMs, with a special emphasis
on the newly introduced DeepSeek models, across a suite of fifteen Arabic NLP
tasks. We experiment with various strategies, including zero-shot, few-shot,
and fine-tuning. This allows us to systematically evaluate performance on
datasets covering a range of applications to examine their capacity for
linguistic reasoning under different levels of complexity. Our experiments
reveal several key findings. First, carefully selecting just three in-context
examples delivers an average uplift of over 13 F1 points on classification
tasks-boosting sentiment analysis from 35.3% to 87.5% and paraphrase detection
from 56.1% to 87.0%. Second, reasoning-focused DeepSeek architectures
outperform a strong GPT o4-mini baseline by an average of 12 F1 points on
complex inference tasks in the zero-shot setting. Third, LoRA-based fine-tuning
yields up to an additional 8 points in F1 and BLEU compared to equivalent
increases in model scale. The code is available at
https://anonymous.4open.science/r/AraReasoner41299

</details>


### [59] [The impact of fine tuning in LLaMA on hallucinations for named entity extraction in legal documentation](https://arxiv.org/abs/2506.08827)
*Francisco Vargas,Alejandro Gonzlez Coene,Gaston Escalante,Exequiel Lobn,Manuel Pulido*

Main category: cs.CL

TL;DR: A two-step method for extracting traffic accident details from legal documents outperforms classic methods, with GPT-4 Turbo achieving the highest accuracy (86.1%).


<details>
  <summary>Details</summary>
Motivation: Extracting entities like disability percentages and compensation amounts from legal documents is challenging, requiring efficient and accurate methods.

Method: A two-step approach: segmenting documents (using regex or semantic vectorization) and extracting entities with LLMs (LLaMA-2, LLaMA-3, GPT-4 Turbo), including fine-tuning with LoRA.

Result: Vectorization + LLMs outperforms regex (39.5% accuracy). LLaMA-2 70B (fine-tuned) achieves 79.4%, LLaMA-3 8B (base) 76.6%, and GPT-4 Turbo 86.1%. Fine-tuning reduces hallucinations.

Conclusion: The proposed method significantly improves accuracy, with GPT-4 Turbo leading. Open-source models like LLaMA-3 show rapid progress.

Abstract: The extraction of information about traffic accidents from legal documents is
crucial for quantifying insurance company costs. Extracting entities such as
percentages of physical and/or psychological disability and the involved
compensation amounts is a challenging process, even for experts, due to the
subtle arguments and reasoning in the court decision. A two-step procedure is
proposed: first, segmenting the document identifying the most relevant
segments, and then extracting the entities. For text segmentation, two
methodologies are compared: a classic method based on regular expressions and a
second approach that divides the document into blocks of n-tokens, which are
then vectorized using multilingual models for semantic searches
(text-embedding-ada-002/MiniLM-L12-v2 ). Subsequently, large language models
(LLaMA-2 7b, 70b, LLaMA-3 8b, and GPT-4 Turbo) are applied with prompting to
the selected segments for entity extraction. For the LLaMA models, fine-tuning
is performed using LoRA. LLaMA-2 7b, even with zero temperature, shows a
significant number of hallucinations in extractions which are an important
contention point for named entity extraction. This work shows that these
hallucinations are substantially reduced after finetuning the model. The
performance of the methodology based on segment vectorization and subsequent
use of LLMs significantly surpasses the classic method which achieves an
accuracy of 39.5%. Among open-source models, LLaMA-2 70B with finetuning
achieves the highest accuracy 79.4%, surpassing its base version 61.7%.
Notably, the base LLaMA-3 8B model already performs comparably to the finetuned
LLaMA-2 70B model, achieving 76.6%, highlighting the rapid progress in model
development. Meanwhile, GPT-4 Turbo achieves the highest accuracy at 86.1%.

</details>


### [60] [Advancing STT for Low-Resource Real-World Speech](https://arxiv.org/abs/2506.08836)
*Flavio D'Intino,Hans-Peter Hutter*

Main category: cs.CL

TL;DR: The paper introduces the SRB-300 dataset, a 300-hour annotated Swiss German speech corpus, and fine-tunes OpenAI Whisper models on it, achieving significant improvements in transcription accuracy.


<details>
  <summary>Details</summary>
Motivation: Swiss German lacks standardized written form, and existing STT models struggle with spontaneous speech. The SRB-300 dataset addresses this gap by providing real-world conversational data.

Method: The authors fine-tuned multiple OpenAI Whisper models on the SRB-300 dataset, which includes diverse Swiss German dialects from radio and TV recordings.

Result: Fine-tuned models showed WER improvements of 19-33% and BLEU score increases of 8-40%, with the best model achieving a WER of 17.1% and BLEU of 74.8.

Conclusion: The SRB-300 dataset and fine-tuned models significantly advance STT for Swiss German and other low-resource languages in real-world settings.

Abstract: Swiss German is a low-resource language represented by diverse dialects that
differ significantly from Standard German and from each other, lacking a
standardized written form. As a result, transcribing Swiss German involves
translating into Standard German. Existing datasets have been collected in
controlled environments, yielding effective speech-to-text (STT) models, but
these models struggle with spontaneous conversational speech.
  This paper, therefore, introduces the new SRB-300 dataset, a 300-hour
annotated speech corpus featuring real-world long-audio recordings from 39
Swiss German radio and TV stations. It captures spontaneous speech across all
major Swiss dialects recorded in various realistic environments and overcomes
the limitation of prior sentence-level corpora.
  We fine-tuned multiple OpenAI Whisper models on the SRB-300 dataset,
achieving notable enhancements over previous zero-shot performance metrics.
Improvements in word error rate (WER) ranged from 19% to 33%, while BLEU scores
increased between 8% and 40%. The best fine-tuned model, large-v3, achieved a
WER of 17.1% and a BLEU score of 74.8. This advancement is crucial for
developing effective and robust STT systems for Swiss German and other
low-resource languages in real-world contexts.

</details>


### [61] [AdversariaL attacK sAfety aLIgnment(ALKALI): Safeguarding LLMs through GRACE: Geometric Representation-Aware Contrastive Enhancement- Introducing Adversarial Vulnerability Quality Index (AVQI)](https://arxiv.org/abs/2506.08885)
*Danush Khanna,Krishna Kumar,Basab Ghosh,Vinija Jain,Vasu Sharma,Aman Chadha,Amitava Das*

Main category: cs.CL

TL;DR: The paper exposes a geometric blind spot in LLM alignment, introduces ALKALI (a benchmark) and GRACE (a defense framework), and proposes AVQI (a metric) to quantify latent alignment failures.


<details>
  <summary>Details</summary>
Motivation: Adversarial threats against LLMs are outpacing defenses, with adversarial prompts exploiting latent geometry to evade detection.

Method: Introduces ALKALI benchmark, GRACE framework (latent space regularization), and AVQI metric (geometric alignment assessment).

Result: GRACE reduces Attack Success Rates by up to 39%; AVQI reveals latent alignment failures.

Conclusion: The work highlights latent camouflage vulnerabilities and offers tools (ALKALI, GRACE, AVQI) to improve LLM safety.

Abstract: Adversarial threats against LLMs are escalating faster than current defenses
can adapt. We expose a critical geometric blind spot in alignment: adversarial
prompts exploit latent camouflage, embedding perilously close to the safe
representation manifold while encoding unsafe intent thereby evading surface
level defenses like Direct Preference Optimization (DPO), which remain blind to
the latent geometry. We introduce ALKALI, the first rigorously curated
adversarial benchmark and the most comprehensive to date spanning 9,000 prompts
across three macro categories, six subtypes, and fifteen attack families.
Evaluation of 21 leading LLMs reveals alarmingly high Attack Success Rates
(ASRs) across both open and closed source models, exposing an underlying
vulnerability we term latent camouflage, a structural blind spot where
adversarial completions mimic the latent geometry of safe ones. To mitigate
this vulnerability, we introduce GRACE - Geometric Representation Aware
Contrastive Enhancement, an alignment framework coupling preference learning
with latent space regularization. GRACE enforces two constraints: latent
separation between safe and adversarial completions, and adversarial cohesion
among unsafe and jailbreak behaviors. These operate over layerwise pooled
embeddings guided by a learned attention profile, reshaping internal geometry
without modifying the base model, and achieve up to 39% ASR reduction.
Moreover, we introduce AVQI, a geometry aware metric that quantifies latent
alignment failure via cluster separation and compactness. AVQI reveals when
unsafe completions mimic the geometry of safe ones, offering a principled lens
into how models internally encode safety. We make the code publicly available
at https://anonymous.4open.science/r/alkali-B416/README.md.

</details>


### [62] [PlantBert: An Open Source Language Model for Plant Science](https://arxiv.org/abs/2506.08897)
*Hiba Khey,Amine Lakhder,Salma Rouichi,Imane El Ghabi,Kamal Hejjaoui,Younes En-nahli,Fahd Kalloubi,Moez Amri*

Main category: cs.CL

TL;DR: PlantBert is a domain-specific transformer model for plant science, fine-tuned on expert-annotated data to extract structured knowledge from plant stress-response literature.


<details>
  <summary>Details</summary>
Motivation: Plant science lacks domain-adapted NLP tools despite advancements in transformer models. PlantBert addresses this gap.

Method: Built on DeBERTa, PlantBert combines transformer modeling with linguistic post-processing and ontology-based normalization, trained on annotated plant stress-response abstracts.

Result: PlantBert shows strong generalization and precision in capturing biologically meaningful relationships, even in low-resource settings.

Conclusion: PlantBert bridges a critical gap in agricultural NLP, enabling scalable knowledge extraction and fostering innovation in plant science.

Abstract: The rapid advancement of transformer-based language models has catalyzed
breakthroughs in biomedical and clinical natural language processing; however,
plant science remains markedly underserved by such domain-adapted tools. In
this work, we present PlantBert, a high-performance, open-source language model
specifically tailored for extracting structured knowledge from plant
stress-response literature. Built upon the DeBERTa architecture-known for its
disentangled attention and robust contextual encoding-PlantBert is fine-tuned
on a meticulously curated corpus of expert-annotated abstracts, with a primary
focus on lentil (Lens culinaris) responses to diverse abiotic and biotic
stressors. Our methodology combines transformer-based modeling with
rule-enhanced linguistic post-processing and ontology-grounded entity
normalization, enabling PlantBert to capture biologically meaningful
relationships with precision and semantic fidelity. The underlying corpus is
annotated using a hierarchical schema aligned with the Crop Ontology,
encompassing molecular, physiological, biochemical, and agronomic dimensions of
plant adaptation. PlantBert exhibits strong generalization capabilities across
entity types and demonstrates the feasibility of robust domain adaptation in
low-resource scientific fields. By providing a scalable and reproducible
framework for high-resolution entity recognition, PlantBert bridges a critical
gap in agricultural NLP and paves the way for intelligent, data-driven systems
in plant genomics, phenomics, and agronomic knowledge discovery. Our model is
publicly released to promote transparency and accelerate cross-disciplinary
innovation in computational plant science.

</details>


### [63] [From Legal Texts to Defeasible Deontic Logic via LLMs: A Study in Automated Semantic Analysis](https://arxiv.org/abs/2506.08899)
*Elias Horner,Cristinel Mateis,Guido Governatori,Agata Ciabattoni*

Main category: cs.CL

TL;DR: A novel pipeline using LLMs transforms legal texts into Defeasible Deontic Logic, showing promising results in formalizing legal norms.


<details>
  <summary>Details</summary>
Motivation: To automate semantic analysis of legal texts and transform them into formal representations for scalable legal informatics.

Method: A structured pipeline segments legal texts, extracts deontic rules, and evaluates coherence, tested with various LLM configurations.

Result: LLMs, especially with effective prompting, align well with expert-crafted formalizations.

Conclusion: LLMs can significantly aid scalable legal informatics when properly utilized.

Abstract: We present a novel approach to the automated semantic analysis of legal texts
using large language models (LLMs), targeting their transformation into formal
representations in Defeasible Deontic Logic (DDL). We propose a structured
pipeline that segments complex normative language into atomic snippets,
extracts deontic rules, and evaluates them for syntactic and semantic
coherence. Our methodology is evaluated across various LLM configurations,
including prompt engineering strategies, fine-tuned models, and multi-stage
pipelines, focusing on legal norms from the Australian Telecommunications
Consumer Protections Code. Empirical results demonstrate promising alignment
between machine-generated and expert-crafted formalizations, showing that LLMs
- particularly when prompted effectively - can significantly contribute to
scalable legal informatics.

</details>


### [64] [Dialect Normalization using Large Language Models and Morphological Rules](https://arxiv.org/abs/2506.08907)
*Antonios Dimakis,John Pavlopoulos,Antonios Anastasopoulos*

Main category: cs.CL

TL;DR: A new method combining rule-based transformations and LLMs with few-shot prompting is introduced for dialect-to-standard normalization, applied to Greek dialects and evaluated with human annotators.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of low-resource languages and dialects by enabling standard-language tools to process dialectal text without parallel data.

Method: Combines rule-based linguistically informed transformations and LLMs with few-shot prompting, applied to Greek dialects.

Result: Human evaluation shows effectiveness; downstream experiments reveal previous reliance on superficial linguistic information, while new semantic insights are possible.

Conclusion: The method successfully normalizes dialectal text, improving downstream tasks and uncovering deeper semantic understanding.

Abstract: Natural language understanding systems struggle with low-resource languages,
including many dialects of high-resource ones. Dialect-to-standard
normalization attempts to tackle this issue by transforming dialectal text so
that it can be used by standard-language tools downstream. In this study, we
tackle this task by introducing a new normalization method that combines
rule-based linguistically informed transformations and large language models
(LLMs) with targeted few-shot prompting, without requiring any parallel data.
We implement our method for Greek dialects and apply it on a dataset of
regional proverbs, evaluating the outputs using human annotators. We then use
this dataset to conduct downstream experiments, finding that previous results
regarding these proverbs relied solely on superficial linguistic information,
including orthographic artifacts, while new observations can still be made
through the remaining semantics.

</details>


### [65] [PropMEND: Hypernetworks for Knowledge Propagation in LLMs](https://arxiv.org/abs/2506.08920)
*Zeyu Leo Liu,Greg Durrett,Eunsol Choi*

Main category: cs.CL

TL;DR: PropMEND, a hypernetwork-based method, improves knowledge propagation in LLMs for multi-hop reasoning, outperforming existing techniques on RippleEdit and Controlled RippleEdit datasets.


<details>
  <summary>Details</summary>
Motivation: Current knowledge editing techniques for LLMs fail to propagate injected knowledge for reasoning tasks, limiting their utility.

Method: PropMEND meta-learns gradient modifications to enable multi-hop reasoning with injected knowledge, extending MEND's meta-objective.

Result: PropMEND achieves nearly 2x accuracy on multi-hop questions and outperforms baselines on unseen entity-relation pairs, though with a reduced gap.

Conclusion: PropMEND advances knowledge propagation but highlights the need for further work on broader relation generalization.

Abstract: Knowledge editing techniques for large language models (LLMs) can inject
knowledge that is later reproducible verbatim, but they fall short on
propagating that knowledge: models cannot answer questions that require
reasoning with the injected knowledge. We present a hypernetwork-based approach
for knowledge propagation, named PropMEND, where we meta-learn how to modify
gradients of a language modeling loss to encourage injected information to
propagate. Our approach extends the meta-objective of MEND [29] so that
gradient updates on knowledge are transformed to enable answering multi-hop
questions involving that knowledge. We show improved performance on the
RippleEdit dataset, showing almost 2x accuracy on challenging multi-hop
questions whose answers are not explicitly stated in the injected fact. We
further introduce a new dataset, Controlled RippleEdit, to evaluate the
generalization of our hypernetwork, testing knowledge propagation along
relations and entities unseen during hypernetwork training. PropMEND still
outperforms existing approaches in unseen entity-relation pairs, yet the
performance gap decreases substantially, suggesting future work in propagating
knowledge to a wide range of relations.

</details>


### [66] [Can A Gamer Train A Mathematical Reasoning Model?](https://arxiv.org/abs/2506.08935)
*Andrew Shin*

Main category: cs.CL

TL;DR: A method to train a 1.5B parameter mathematical reasoning model on a single gaming GPU (RTX 3080 Ti) using reinforcement learning and memory optimization, achieving comparable performance to larger models.


<details>
  <summary>Details</summary>
Motivation: To reduce the prohibitive computational resources typically required for training large language models (LLMs) for mathematical reasoning, making high-performance AI research more accessible.

Method: Integration of reinforcement learning and memory optimization techniques to train a 1.5B parameter model on a single gaming GPU (RTX 3080 Ti).

Result: The model achieves comparable or better performance on mathematical reasoning benchmarks than larger models, despite resource constraints.

Conclusion: The approach challenges the need for massive infrastructure for state-of-the-art mathematical reasoning, democratizing access to high-performance AI research.

Abstract: While large language models (LLMs) have achieved remarkable performance in
various tasks including mathematical reasoning, their development typically
demands prohibitive computational resources. Recent advancements have reduced
costs for training capable models, yet even these approaches rely on high-end
hardware clusters. In this paper, we demonstrate that a single average gaming
GPU can train a solid mathematical reasoning model, by integrating
reinforcement learning and memory optimization techniques. Specifically, we
train a 1.5B parameter mathematical reasoning model on RTX 3080 Ti of 16GB
memory that achieves comparable or better performance on mathematical reasoning
benchmarks than models several times larger, in resource-constrained
environments. Our results challenge the paradigm that state-of-the-art
mathematical reasoning necessitates massive infrastructure, democratizing
access to high-performance AI research.
https://github.com/shinandrew/YouronMath.

</details>


### [67] [FaithfulRAG: Fact-Level Conflict Modeling for Context-Faithful Retrieval-Augmented Generation](https://arxiv.org/abs/2506.08938)
*Qinggang Zhang,Zhishang Xiang,Yilin Xiao,Le Wang,Junhui Li,Xinrun Wang,Jinsong Su*

Main category: cs.CL

TL;DR: FaithfulRAG addresses unfaithfulness in LLMs with retrieval by resolving knowledge conflicts through explicit modeling and self-thinking, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: LLMs with retrieval systems struggle with unfaithfulness, especially in knowledge conflicts, where existing methods suppress parametric knowledge, harming internal structure.

Method: FaithfulRAG models discrepancies between parametric knowledge and retrieved context, identifies conflicts at the fact level, and uses a self-thinking process for reasoning.

Result: Extensive experiments show FaithfulRAG outperforms state-of-the-art methods.

Conclusion: FaithfulRAG effectively resolves knowledge conflicts without suppressing parametric knowledge, improving faithfulness in LLM outputs.

Abstract: Large language models (LLMs) augmented with retrieval systems have
demonstrated significant potential in handling knowledge-intensive tasks.
However, these models often struggle with unfaithfulness issues, generating
outputs that either ignore the retrieved context or inconsistently blend it
with the LLM`s parametric knowledge. This issue is particularly severe in cases
of knowledge conflict, where the retrieved context conflicts with the model`s
parametric knowledge. While existing faithful RAG approaches enforce strict
context adherence through well-designed prompts or modified decoding
strategies, our analysis reveals a critical limitation: they achieve
faithfulness by forcibly suppressing the model`s parametric knowledge, which
undermines the model`s internal knowledge structure and increases the risk of
misinterpreting the context. To this end, this paper proposes FaithfulRAG, a
novel framework that resolves knowledge conflicts by explicitly modeling
discrepancies between the model`s parametric knowledge and retrieved context.
Specifically, FaithfulRAG identifies conflicting knowledge at the fact level
and designs a self-thinking process, allowing LLMs to reason about and
integrate conflicting facts before generating responses. Extensive experiments
demonstrate that our method outperforms state-of-the-art methods. The code is
available at https:// github.com/DeepLearnXMU/Faithful-RAG

</details>


### [68] [Can LLMs Ground when they (Don't) Know: A Study on Direct and Loaded Political Questions](https://arxiv.org/abs/2506.08952)
*Clara Lachenmaier,Judith Sieker,Sina Zarrie*

Main category: cs.CL

TL;DR: The paper explores how LLMs handle common ground in political discourse, especially when dealing with misinformation, and finds they struggle to correct false beliefs.


<details>
  <summary>Details</summary>
Motivation: To understand LLMs' ability to manage mutual understanding and correct misinformation in politically charged contexts.

Method: Examined LLMs' responses to direct knowledge questions and loaded questions with presupposed misinformation, assessing grounding behavior and political bias.

Result: LLMs face challenges in grounding and rejecting false beliefs, raising concerns about their role in combating political misinformation.

Conclusion: LLMs' limitations in handling misinformation highlight risks for their use in political discourse.

Abstract: Communication among humans relies on conversational grounding, allowing
interlocutors to reach mutual understanding even when they do not have perfect
knowledge and must resolve discrepancies in each other's beliefs. This paper
investigates how large language models (LLMs) manage common ground in cases
where they (don't) possess knowledge, focusing on facts in the political domain
where the risk of misinformation and grounding failure is high. We examine the
ability of LLMs to answer direct knowledge questions and loaded questions that
presuppose misinformation. We evaluate whether loaded questions lead LLMs to
engage in active grounding and correct false user beliefs, in connection to
their level of knowledge and their political bias. Our findings highlight
significant challenges in LLMs' ability to engage in grounding and reject false
user beliefs, raising concerns about their role in mitigating misinformation in
political discourse.

</details>


### [69] [Pre-trained Language Models Learn Remarkably Accurate Representations of Numbers](https://arxiv.org/abs/2506.08966)
*Marek Kadlk,Michal tefnik,Timothee Mickus,Michal Spiegel,Josef Kucha*

Main category: cs.CL

TL;DR: A novel probing technique reveals that pretrained LMs accurately represent numbers, and aligning embeddings with this pattern can reduce arithmetic errors.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to probe numeric values effectively, suggesting unreliable embeddings for exact quantities. The study aims to address this gap.

Method: Proposes a new probing technique to decode numeric values from embeddings, testing it on open-source LMs.

Result: The probe achieves near-perfect accuracy, proving LMs represent numbers precisely post-pre-training. Embedding precision explains arithmetic errors.

Conclusion: Aligning embeddings with the discovered pattern mitigates arithmetic errors, demonstrating the probe's practical utility.

Abstract: Pretrained language models (LMs) are prone to arithmetic errors. Existing
work showed limited success in probing numeric values from models'
representations, indicating that these errors can be attributed to the inherent
unreliability of distributionally learned embeddings in representing exact
quantities. However, we observe that previous probing methods are inadequate
for the emergent structure of learned number embeddings with sinusoidal
patterns.
  In response, we propose a novel probing technique that decodes numeric values
from input embeddings with near-perfect accuracy across a range of open-source
LMs. This proves that after the sole pre-training, LMs represent numbers with
remarkable precision. Finally, we find that the embeddings' preciseness judged
by our probe's accuracy explains a large portion of LM's errors in elementary
arithmetic, and show that aligning the embeddings with the pattern discovered
by our probe can mitigate these errors.

</details>


### [70] [Atomic-to-Compositional Generalization for Mobile Agents with A New Benchmark and Scheduling System](https://arxiv.org/abs/2506.08972)
*Yuan Guo,Tingjia Miao,Zheng Wu,Pengzhou Cheng,Ming Zhou,Zhuosheng Zhang*

Main category: cs.CL

TL;DR: UI-NEXUS is a benchmark for evaluating mobile agents on compositional tasks, revealing performance gaps. AGENT-NEXUS, a scheduling system, improves task success rates by 24-40%.


<details>
  <summary>Details</summary>
Motivation: Prior work focused on atomic tasks, neglecting compositional tasks crucial for real-world applications.

Method: Introduces UI-NEXUS benchmark for compositional tasks and proposes AGENT-NEXUS, a scheduling system to decompose tasks.

Result: Existing agents struggle with compositional tasks; AGENT-NEXUS improves success rates by 24-40%.

Conclusion: AGENT-NEXUS effectively addresses the atomic-to-compositional generalization gap in mobile agents.

Abstract: Autonomous agents powered by multimodal large language models have been
developed to facilitate task execution on mobile devices. However, prior work
has predominantly focused on atomic tasks -- such as shot-chain execution tasks
and single-screen grounding tasks -- while overlooking the generalization to
compositional tasks, which are indispensable for real-world applications. This
work introduces UI-NEXUS, a comprehensive benchmark designed to evaluate mobile
agents on three categories of compositional operations: Simple Concatenation,
Context Transition, and Deep Dive. UI-NEXUS supports interactive evaluation in
20 fully controllable local utility app environments, as well as 30 online
Chinese and English service apps. It comprises 100 interactive task templates
with an average optimal step count of 14.05. Experimental results across a
range of mobile agents with agentic workflow or agent-as-a-model show that
UI-NEXUS presents significant challenges. Specifically, existing agents
generally struggle to balance performance and efficiency, exhibiting
representative failure modes such as under-execution, over-execution, and
attention drift, causing visible atomic-to-compositional generalization gap.
Inspired by these findings, we propose AGENT-NEXUS, a lightweight and efficient
scheduling system to tackle compositional mobile tasks. AGENT-NEXUS
extrapolates the abilities of existing mobile agents by dynamically decomposing
long-horizon tasks to a series of self-contained atomic subtasks. AGENT-NEXUS
achieves 24% to 40% task success rate improvement for existing mobile agents on
compositional operation tasks within the UI-NEXUS benchmark without
significantly sacrificing inference overhead. The demo video, dataset, and code
are available on the project page at https://ui-nexus.github.io.

</details>


### [71] [FROST-EMA: Finnish and Russian Oral Speech Dataset of Electromagnetic Articulography Measurements with L1, L2 and Imitated L2 Accents](https://arxiv.org/abs/2506.08981)
*Satu Hopponen,Tomi Kinnunen,Alexandre Nikolaev,Rosa Gonzlez Hautamki,Lauri Tavi,Einar Meister*

Main category: cs.CL

TL;DR: The paper introduces the FROST-EMA corpus, featuring bilingual speech data (L1, L2, fake accent) for phonetic and technological research, with two case studies on speaker verification and articulatory patterns.


<details>
  <summary>Details</summary>
Motivation: To study language variability in bilingual speech from phonetic and technological perspectives.

Method: Creation of the FROST-EMA corpus with 18 bilingual speakers, followed by two case studies: one on automatic speaker verification and another on articulatory patterns.

Result: Demonstrates the corpus's utility for analyzing phonetic differences and technological applications like speaker verification.

Conclusion: The FROST-EMA corpus is a valuable resource for bilingual speech research, with potential applications in both linguistics and technology.

Abstract: We introduce a new FROST-EMA (Finnish and Russian Oral Speech Dataset of
Electromagnetic Articulography) corpus. It consists of 18 bilingual speakers,
who produced speech in their native language (L1), second language (L2), and
imitated L2 (fake foreign accent). The new corpus enables research into
language variability from phonetic and technological points of view.
Accordingly, we include two preliminary case studies to demonstrate both
perspectives. The first case study explores the impact of L2 and imitated L2 on
the performance of an automatic speaker verification system, while the second
illustrates the articulatory patterns of one speaker in L1, L2, and a fake
accent.

</details>


### [72] [Naturalistic Language-related Movie-Watching fMRI Task for Detecting Neurocognitive Decline and Disorder](https://arxiv.org/abs/2506.08986)
*Yuejiao Wang,Xianmin Gong,Xixin Wu,Patrick Wong,Hoi-lam Helene Fung,Man Wai Mak,Helen Meng*

Main category: cs.CL

TL;DR: A novel language-related fMRI task was developed for early detection of cognitive decline in older adults, achieving high accuracy (AUC 0.86) using machine learning.


<details>
  <summary>Details</summary>
Motivation: Early detection of neurocognitive disorder (NCD) is vital for timely intervention, and language-related fMRI shows promise for this purpose.

Method: A naturalistic language-related fMRI task was tested on 97 non-demented Chinese older adults, with machine-learning models analyzing fMRI features and demographics.

Result: The models achieved an AUC of 0.86, with key features from language-processing brain regions like the superior temporal gyrus.

Conclusion: The study highlights the potential of language-related fMRI for early NCD detection.

Abstract: Early detection is crucial for timely intervention aimed at preventing and
slowing the progression of neurocognitive disorder (NCD), a common and
significant health problem among the aging population. Recent evidence has
suggested that language-related functional magnetic resonance imaging (fMRI)
may be a promising approach for detecting cognitive decline and early NCD. In
this paper, we proposed a novel, naturalistic language-related fMRI task for
this purpose. We examined the effectiveness of this task among 97 non-demented
Chinese older adults from Hong Kong. The results showed that machine-learning
classification models based on fMRI features extracted from the task and
demographics (age, gender, and education year) achieved an average area under
the curve of 0.86 when classifying participants' cognitive status (labeled as
NORMAL vs DECLINE based on their scores on a standard neurcognitive test).
Feature localization revealed that the fMRI features most frequently selected
by the data-driven approach came primarily from brain regions associated with
language processing, such as the superior temporal gyrus, middle temporal
gyrus, and right cerebellum. The study demonstrated the potential of the
naturalistic language-related fMRI task for early detection of aging-related
cognitive decline and NCD.

</details>


### [73] [Employing self-supervised learning models for cross-linguistic child speech maturity classification](https://arxiv.org/abs/2506.08999)
*Theo Zhang,Madurya Suresh,Anne S. Warlaumont,Kasia Hitczenko,Alejandrina Cristia,Margaret Cychosz*

Main category: cs.CL

TL;DR: A novel dataset, SpeechMaturity, is used with transformer models to classify child vocalizations, outperforming previous models and matching human accuracy.


<details>
  <summary>Details</summary>
Motivation: Speech technology struggles with child speech due to limited training data and inherent challenges.

Method: Applied transformer models to the SpeechMaturity dataset, which includes diverse, ecologically-valid child vocalizations across 25+ languages.

Result: Models outperformed state-of-the-art models, achieved human-like accuracy, and were robust across settings.

Conclusion: The SpeechMaturity dataset and transformer models effectively address child speech classification challenges.

Abstract: Speech technology systems struggle with many downstream tasks for child
speech due to small training corpora and the difficulties that child speech
pose. We apply a novel dataset, SpeechMaturity, to state-of-the-art transformer
models to address a fundamental classification task: identifying child
vocalizations. Unlike previous corpora, our dataset captures maximally
ecologically-valid child vocalizations across an unprecedented sample,
comprising children acquiring 25+ languages in the U.S., Bolivia, Vanuatu,
Papua New Guinea, Solomon Islands, and France. The dataset contains 242,004
labeled vocalizations, magnitudes larger than previous work. Models were
trained to distinguish between cry, laughter, mature (consonant+vowel), and
immature speech (just consonant or vowel). Models trained on the dataset
outperform state-of-the-art models trained on previous datasets, achieved
classification accuracy comparable to humans, and were robust across rural and
urban settings.

</details>


### [74] [SWE-Flow: Synthesizing Software Engineering Data in a Test-Driven Manner](https://arxiv.org/abs/2506.09003)
*Lei Zhang,Jiaxi Yang,Min Yang,Jian Yang,Mouxiang Chen,Jiajun Zhang,Zeyu Cui,Binyuan Hui,Junyang Lin*

Main category: cs.CL

TL;DR: SWE-Flow is a data synthesis framework using TDD to generate incremental development steps from unit tests, creating verifiable TDD tasks and a benchmark dataset.


<details>
  <summary>Details</summary>
Motivation: Existing software engineering data relies on human-submitted issues, lacking structured, verifiable TDD tasks. SWE-Flow addresses this by automating the process.

Method: SWE-Flow constructs a Runtime Dependency Graph (RDG) to capture function interactions, generating step-by-step development schedules with partial codebases, tests, and modifications.

Result: Generated 16,061 training and 2,020 test instances (SWE-Flow-Eval). Fine-tuning models on this dataset improves TDD-based coding performance.

Conclusion: SWE-Flow provides a scalable, automated way to create TDD tasks, enhancing model performance and facilitating research with released resources.

Abstract: We introduce **SWE-Flow**, a novel data synthesis framework grounded in
Test-Driven Development (TDD). Unlike existing software engineering data that
rely on human-submitted issues, **SWE-Flow** automatically infers incremental
development steps directly from unit tests, which inherently encapsulate
high-level requirements. The core of **SWE-Flow** is the construction of a
Runtime Dependency Graph (RDG), which precisely captures function interactions,
enabling the generation of a structured, step-by-step *development schedule*.
At each step, **SWE-Flow** produces a partial codebase, the corresponding unit
tests, and the necessary code modifications, resulting in fully verifiable TDD
tasks. With this approach, we generated 16,061 training instances and 2,020
test instances from real-world GitHub projects, creating the **SWE-Flow-Eval**
benchmark. Our experiments show that fine-tuning open model on this dataset
significantly improves performance in TDD-based coding. To facilitate further
research, we release all code, datasets, models, and Docker images at
[Github](https://github.com/Hambaobao/SWE-Flow).

</details>


### [75] [UD-KSL Treebank v1.3: A semi-automated framework for aligning XPOS-extracted units with UPOS tags](https://arxiv.org/abs/2506.09009)
*Hakyung Sung,Gyu-Ho Shin,Chanyoung Lee,You Kyung Sung,Boo Kyung Jung*

Main category: cs.CL

TL;DR: A semi-automated framework aligns XPOS sequences with UPOS categories for L2 Korean, improving morphosyntactic analysis and dependency parsing.


<details>
  <summary>Details</summary>
Motivation: Extend Universal Dependencies annotations for L2 Korean and enhance morphosyntactic analysis accuracy.

Method: Introduce a semi-automated framework for XPOS-UPOS alignment, annotate 2,998 new sentences, and evaluate using NLP toolkits.

Result: Aligned dataset improves annotation consistency and enhances tagging and parsing accuracy, especially with limited data.

Conclusion: XPOS-UPOS alignment benefits L2 Korean morphosyntactic analysis, particularly in data-scarce scenarios.

Abstract: The present study extends recent work on Universal Dependencies annotations
for second-language (L2) Korean by introducing a semi-automated framework that
identifies morphosyntactic constructions from XPOS sequences and aligns those
constructions with corresponding UPOS categories. We also broaden the existing
L2-Korean corpus by annotating 2,998 new sentences from argumentative essays.
To evaluate the impact of XPOS-UPOS alignments, we fine-tune L2-Korean
morphosyntactic analysis models on datasets both with and without these
alignments, using two NLP toolkits. Our results indicate that the aligned
dataset not only improves consistency across annotation layers but also
enhances morphosyntactic tagging and dependency-parsing accuracy, particularly
in cases of limited annotated data.

</details>


### [76] [Learning to Reason Across Parallel Samples for LLM Reasoning](https://arxiv.org/abs/2506.09014)
*Jianing Qi,Xi Ye,Hao Tang,Zhigang Zhu,Eunsol Choi*

Main category: cs.CL

TL;DR: A compact LLM, Sample Set Aggregator (SSA), is trained to aggregate multiple samples for improved answer accuracy, outperforming other test-time scaling methods.


<details>
  <summary>Details</summary>
Motivation: To enhance performance gains in LLMs by leveraging multiple samples more effectively than heuristic aggregation methods.

Method: Train SSA to concatenate and analyze multiple samples, optimizing accuracy via reinforcement learning.

Result: SSA outperforms methods like reward model-based re-ranking and generalizes well across tasks and model scales.

Conclusion: SSA offers an efficient way to aggregate samples, compatible with black-box models, improving accuracy and generalization.

Abstract: Scaling test-time compute brings substantial performance gains for large
language models (LLMs). By sampling multiple answers and heuristically
aggregate their answers (e.g., either through majority voting or using
verifiers to rank the answers), one can achieve consistent performance gains in
math domains. In this paper, we propose a new way to leverage such multiple
sample set. We train a compact LLM, called Sample Set Aggregator (SSA), that
takes a concatenated sequence of multiple samples and output the final answer,
optimizing it for the answer accuracy with reinforcement learning. Experiments
on multiple reasoning datasets show that SSA outperforms other test-time
scaling methods such as reward model-based re-ranking. Our approach also shows
a promising generalization ability, across sample set sizes, base model
families and scales, and tasks. By separating LLMs to generate answers and LLMs
to analyze and aggregate sampled answers, our approach can work with the
outputs from premier black box models easily and efficiently.

</details>


### [77] [Comparing human and LLM proofreading in L2 writing: Impact on lexical and syntactic features](https://arxiv.org/abs/2506.09021)
*Hakyung Sung,Karla Csuros,Min-Chang Sung*

Main category: cs.CL

TL;DR: Human and LLM proofreading improve intelligibility in second language writings, with LLMs showing more generative changes and consistent outcomes across models.


<details>
  <summary>Details</summary>
Motivation: To compare the effectiveness and consistency of human and LLM proofreading in enhancing second language writings.

Method: Examined lexical and syntactic interventions by humans and three LLMs (ChatGPT-4o, Llama3.1-8b, Deepseek-r1-8b) on identical texts.

Result: Both human and LLM proofreading improved bigram lexical features, but LLMs made more extensive vocabulary and structural changes. Outcomes were consistent across LLMs.

Conclusion: LLM proofreading is effective and consistent, though more generative, suggesting potential for standardized language enhancement.

Abstract: This study examines the lexical and syntactic interventions of human and LLM
proofreading aimed at improving overall intelligibility in identical second
language writings, and evaluates the consistency of outcomes across three LLMs
(ChatGPT-4o, Llama3.1-8b, Deepseek-r1-8b). Findings show that both human and
LLM proofreading enhance bigram lexical features, which may contribute to
better coherence and contextual connectedness between adjacent words. However,
LLM proofreading exhibits a more generative approach, extensively reworking
vocabulary and sentence structures, such as employing more diverse and
sophisticated vocabulary and incorporating a greater number of adjective
modifiers in noun phrases. The proofreading outcomes are highly consistent in
major lexical and syntactic features across the three models.

</details>


### [78] [Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via Reinforcement Learning](https://arxiv.org/abs/2506.09033)
*Haozhen Zhang,Tao Feng,Jiaxuan You*

Main category: cs.CL

TL;DR: Router-R1 is a reinforcement learning-based framework for multi-LLM routing and aggregation, outperforming baselines in QA benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing LLM routers lack the ability to handle complex tasks requiring multiple LLMs' complementary strengths.

Method: Router-R1 uses RL to interleave deliberation and dynamic model invocation, guided by rule-based rewards.

Result: Router-R1 achieves superior performance and cost management on QA benchmarks.

Conclusion: Router-R1 offers robust generalization and cost optimization for multi-LLM routing.

Abstract: The rapid emergence of diverse large language models (LLMs) has spurred the
development of LLM routers that assign user queries to the most suitable model.
However, existing LLM routers typically perform a single-round, one-to-one
mapping (\textit{i.e.}, assigning each query to a single model in isolation),
which limits their capability to tackle complex tasks that demand the
complementary strengths of multiple LLMs. In this paper, we present
\textbf{Router-R1}, a reinforcement learning (RL)-based framework that
formulates multi-LLM routing and aggregation as a sequential decision process.
Router-R1 instantiates the router itself as a capable LLM, leveraging its
reasoning ability to interleave "think" actions (internal deliberation) with
"route" actions (dynamic model invocation), and integrates each response into
its evolving context. To guide learning, we employ a lightweight rule-based
reward comprising format rewards, final outcome rewards, and a novel cost
reward for performance and cost trade-off optimization, opening a pathway
toward optimizing performance-cost tradeoffs via RL. Router-R1 also conditions
only on simple model descriptors such as pricing, latency, and example
performance, enabling strong generalization to unseen model selection.
Experiments on seven general and multi-hop QA benchmarks show that Router-R1
outperforms over several strong baselines, achieving superior performance while
maintaining robust generalization and cost management.Code is available at
https://github.com/ulab-uiuc/Router-R1.

</details>


### [79] [Same Task, Different Circuits: Disentangling Modality-Specific Mechanisms in VLMs](https://arxiv.org/abs/2506.09047)
*Yaniv Nikankin,Dana Arad,Yossi Gandelsman,Yonatan Belinkov*

Main category: cs.CL

TL;DR: The paper investigates the performance gap between visual and textual tasks in Vision-Language Models (VLMs) by analyzing task-specific computational sub-graphs (circuits) and proposes a method to reduce the gap by patching visual representations.


<details>
  <summary>Details</summary>
Motivation: VLMs perform better on textual tasks than visual ones despite similar functionalities, prompting an investigation into the underlying causes.

Method: The study compares circuits in different modalities, identifies alignment issues in visual data representations, and patches late-layer visual tokens into earlier layers.

Result: Patching visual representations closes a third of the performance gap between modalities on average.

Conclusion: The analysis reveals the multi-modal performance gap in VLMs and offers a training-free solution to mitigate it.

Abstract: Vision-Language models (VLMs) show impressive abilities to answer questions
on visual inputs (e.g., counting objects in an image), yet demonstrate higher
accuracies when performing an analogous task on text (e.g., counting words in a
text). We investigate this accuracy gap by identifying and comparing the
\textit{circuits} - the task-specific computational sub-graphs - in different
modalities. We show that while circuits are largely disjoint between
modalities, they implement relatively similar functionalities: the differences
lie primarily in processing modality-specific data positions (an image or a
text sequence). Zooming in on the image data representations, we observe they
become aligned with the higher-performing analogous textual representations
only towards later layers, too late in processing to effectively influence
subsequent positions. To overcome this, we patch the representations of visual
data tokens from later layers back into earlier layers. In experiments with
multiple tasks and models, this simple intervention closes a third of the
performance gap between the modalities, on average. Our analysis sheds light on
the multi-modal performance gap in VLMs and suggests a training-free approach
for reducing it.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [80] [Towards Reliable AR-Guided Surgical Navigation: Interactive Deformation Modeling with Data-Driven Biomechanics and Prompts](https://arxiv.org/abs/2506.08048)
*Zheng Han,Jun Zhou,Jialun Pei,Jing Qin,Yingfang Fan,Qi Dou*

Main category: cs.CV

TL;DR: A data-driven biomechanics algorithm with human-in-the-loop interaction improves AR-guided surgical navigation by enhancing deformation modeling accuracy and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Current FEM-based methods for AR-guided surgical navigation are computationally expensive and struggle with large anatomical changes, leading to inaccurate overlays.

Method: Proposes a data-driven biomechanics algorithm with a human-in-the-loop mechanism for interactive correction of anatomical misalignments.

Result: Achieves a mean target registration error of 3.42 mm, reduced to 2.78 mm with surgeon prompts, outperforming existing methods.

Conclusion: The framework enhances accuracy, efficiency, and surgeon-algorithm collaboration, improving reliability in computer-assisted surgeries.

Abstract: In augmented reality (AR)-guided surgical navigation, preoperative organ
models are superimposed onto the patient's intraoperative anatomy to visualize
critical structures such as vessels and tumors. Accurate deformation modeling
is essential to maintain the reliability of AR overlays by ensuring alignment
between preoperative models and the dynamically changing anatomy. Although the
finite element method (FEM) offers physically plausible modeling, its high
computational cost limits intraoperative applicability. Moreover, existing
algorithms often fail to handle large anatomical changes, such as those induced
by pneumoperitoneum or ligament dissection, leading to inaccurate anatomical
correspondences and compromised AR guidance. To address these challenges, we
propose a data-driven biomechanics algorithm that preserves FEM-level accuracy
while improving computational efficiency. In addition, we introduce a novel
human-in-the-loop mechanism into the deformation modeling process. This enables
surgeons to interactively provide prompts to correct anatomical misalignments,
thereby incorporating clinical expertise and allowing the model to adapt
dynamically to complex surgical scenarios. Experiments on a publicly available
dataset demonstrate that our algorithm achieves a mean target registration
error of 3.42 mm. Incorporating surgeon prompts through the interactive
framework further reduces the error to 2.78 mm, surpassing state-of-the-art
methods in volumetric accuracy. These results highlight the ability of our
framework to deliver efficient and accurate deformation modeling while
enhancing surgeon-algorithm collaboration, paving the way for safer and more
reliable computer-assisted surgeries.

</details>


### [81] [ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving](https://arxiv.org/abs/2506.08052)
*Yongkang Li,Kaixin Xiong,Xiangyu Guo,Fang Li,Sixu Yan,Gangwei Xu,Lijun Zhou,Long Chen,Haiyang Sun,Bing Wang,Guang Chen,Hangjun Ye,Wenyu Liu,Xinggang Wang*

Main category: cs.CV

TL;DR: ReCogDrive integrates Vision-Language Models (VLMs) with a diffusion planner to improve autonomous driving in rare scenarios, addressing domain gaps and action space mismatches.


<details>
  <summary>Details</summary>
Motivation: Current autonomous driving systems struggle with rare scenarios and domain gaps between VLMs and real-world driving data.

Method: A three-stage approach: (1) train VLMs on driving Q&A data, (2) use a diffusion planner for imitation learning, (3) fine-tune with reinforcement learning.

Result: Achieves 89.6 PDMS on NAVSIM, surpassing the previous SOTA by 5.6 PDMS.

Conclusion: ReCogDrive effectively bridges domain gaps and improves driving performance in rare scenarios.

Abstract: Although end-to-end autonomous driving has made remarkable progress, its
performance degrades significantly in rare and long-tail scenarios. Recent
approaches attempt to address this challenge by leveraging the rich world
knowledge of Vision-Language Models (VLMs), but these methods suffer from
several limitations: (1) a significant domain gap between the pre-training data
of VLMs and real-world driving data, (2) a dimensionality mismatch between the
discrete language space and the continuous action space, and (3) imitation
learning tends to capture the average behavior present in the dataset, which
may be suboptimal even dangerous. In this paper, we propose ReCogDrive, an
autonomous driving system that integrates VLMs with diffusion planner, which
adopts a three-stage paradigm for training. In the first stage, we use a
large-scale driving question-answering datasets to train the VLMs, mitigating
the domain discrepancy between generic content and real-world driving
scenarios. In the second stage, we employ a diffusion-based planner to perform
imitation learning, mapping representations from the latent language space to
continuous driving actions. Finally, we fine-tune the diffusion planner using
reinforcement learning with NAVSIM non-reactive simulator, enabling the model
to generate safer, more human-like driving trajectories. We evaluate our
approach on the planning-oriented NAVSIM benchmark, achieving a PDMS of 89.6
and setting a new state-of-the-art that surpasses the previous vision-only SOTA
by 5.6 PDMS.

</details>


### [82] [CuRe: Cultural Gaps in the Long Tail of Text-to-Image Systems](https://arxiv.org/abs/2506.08071)
*Aniket Rege,Zinnia Nie,Mahesh Ramesh,Unmesh Raskar,Zhuoran Yu,Aditya Kusupati,Yong Jae Lee,Ramya Korlakai Vinayak*

Main category: cs.CV

TL;DR: The paper introduces CuRe, a benchmarking suite to evaluate cultural biases in text-to-image (T2I) systems, using a dataset of 300 cultural artifacts across 32 subcategories. It shows strong correlations with human judgments for diversity and alignment.


<details>
  <summary>Details</summary>
Motivation: To address the underrepresentation of Global South cultures in T2I systems trained on Amero- and Euro-centric data.

Method: Leverages marginal utility of attribute specification in T2I systems, using a hierarchical dataset from Wikimedia. Evaluates systems like Stable Diffusion, DALL-E 3, and others.

Result: CuRe scorers correlate well with human judgments on perceptual similarity, image-text alignment, and cultural diversity across various models.

Conclusion: CuRe provides a scalable tool for assessing cultural representativeness in T2I systems, with open-sourced code and dataset.

Abstract: Popular text-to-image (T2I) systems are trained on web-scraped data, which is
heavily Amero and Euro-centric, underrepresenting the cultures of the Global
South. To analyze these biases, we introduce CuRe, a novel and scalable
benchmarking and scoring suite for cultural representativeness that leverages
the marginal utility of attribute specification to T2I systems as a proxy for
human judgments. Our CuRe benchmark dataset has a novel categorical hierarchy
built from the crowdsourced Wikimedia knowledge graph, with 300 cultural
artifacts across 32 cultural subcategories grouped into six broad cultural axes
(food, art, fashion, architecture, celebrations, and people). Our dataset's
categorical hierarchy enables CuRe scorers to evaluate T2I systems by analyzing
their response to increasing the informativeness of text conditioning, enabling
fine-grained cultural comparisons. We empirically observe much stronger
correlations of our class of scorers to human judgments of perceptual
similarity, image-text alignment, and cultural diversity across image encoders
(SigLIP 2, AIMV2 and DINOv2), vision-language models (OpenCLIP, SigLIP 2,
Gemini 2.0 Flash) and state-of-the-art text-to-image systems, including three
variants of Stable Diffusion (1.5, XL, 3.5 Large), FLUX.1 [dev], Ideogram 2.0,
and DALL-E 3. The code and dataset is open-sourced and available at
https://aniketrege.github.io/cure/.

</details>


### [83] [IGraSS: Learning to Identify Infrastructure Networks from Satellite Imagery by Iterative Graph-constrained Semantic Segmentation](https://arxiv.org/abs/2506.08137)
*Oishee Bintey Hoque,Abhijin Adiga,Aniruddha Adiga,Siddharth Chaudhary,Madhav V. Marathe,S. S. Ravi,Kirti Rajagopalan,Amanda Wilson,Samarth Swarup*

Main category: cs.CV

TL;DR: IGraSS is a novel framework combining semantic segmentation and graph-based refinement to improve canal and road network mapping from noisy ground truth.


<details>
  <summary>Details</summary>
Motivation: Accurate canal and road network mapping is crucial for water and infrastructure management, but incomplete ground truth hinders learning approaches.

Method: IGraSS integrates a semantic segmentation module (using RGB, NDWI, DEM) with a graph-based refinement module to improve ground truth.

Result: IGraSS reduces unreachable canal segments from 18% to 3% and enhances canal identification. It also generalizes to road networks.

Conclusion: IGraSS is effective for refining noisy ground truth and mapping infrastructure networks, demonstrating generalizability across different networks.

Abstract: Accurate canal network mapping is essential for water management, including
irrigation planning and infrastructure maintenance. State-of-the-art semantic
segmentation models for infrastructure mapping, such as roads, rely on large,
well-annotated remote sensing datasets. However, incomplete or inadequate
ground truth can hinder these learning approaches. Many infrastructure networks
have graph-level properties such as reachability to a source (like canals) or
connectivity (roads) that can be leveraged to improve these existing ground
truth. This paper develops a novel iterative framework IGraSS, combining a
semantic segmentation module-incorporating RGB and additional modalities (NDWI,
DEM)-with a graph-based ground-truth refinement module. The segmentation module
processes satellite imagery patches, while the refinement module operates on
the entire data viewing the infrastructure network as a graph. Experiments show
that IGraSS reduces unreachable canal segments from around 18% to 3%, and
training with refined ground truth significantly improves canal identification.
IGraSS serves as a robust framework for both refining noisy ground truth and
mapping canal networks from remote sensing imagery. We also demonstrate the
effectiveness and generalizability of IGraSS using road networks as an example,
applying a different graph-theoretic constraint to complete road networks.

</details>


### [84] [Spectral Domain Neural Reconstruction for Passband FMCW Radars](https://arxiv.org/abs/2506.08163)
*Harshvardhan Takawale,Nirupam Roy*

Main category: cs.CV

TL;DR: SpINRv2 is an advanced neural framework for high-fidelity volumetric reconstruction using FMCW radar, improving upon SpINR by addressing phase aliasing and sub-bin ambiguity at high frequencies.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance volumetric reconstruction accuracy under high-frequency conditions where phase aliasing and sub-bin ambiguity degrade performance.

Method: The method involves a differentiable frequency-domain forward model for complex radar response and an implicit neural representation (INR) for continuous scene modeling, with sparsity and smoothness regularization to resolve ambiguities.

Result: SpINRv2 outperforms classical and learning-based baselines, particularly in high-frequency regimes, setting a new benchmark for neural radar-based 3D imaging.

Conclusion: SpINRv2 advances neural radar-based 3D imaging by preserving spectral fidelity and reducing computational overhead, making it highly effective for high-frequency applications.

Abstract: We present SpINRv2, a neural framework for high-fidelity volumetric
reconstruction using Frequency-Modulated Continuous-Wave (FMCW) radar.
Extending our prior work (SpINR), this version introduces enhancements that
allow accurate learning under high start frequencies-where phase aliasing and
sub-bin ambiguity become prominent. Our core contribution is a fully
differentiable frequency-domain forward model that captures the complex radar
response using closed-form synthesis, paired with an implicit neural
representation (INR) for continuous volumetric scene modeling. Unlike
time-domain baselines, SpINRv2 directly supervises the complex frequency
spectrum, preserving spectral fidelity while drastically reducing computational
overhead. Additionally, we introduce sparsity and smoothness regularization to
disambiguate sub-bin ambiguities that arise at fine range resolutions.
Experimental results show that SpINRv2 significantly outperforms both classical
and learning-based baselines, especially under high-frequency regimes,
establishing a new benchmark for neural radar-based 3D imaging.

</details>


### [85] [Surgeon Style Fingerprinting and Privacy Risk Quantification via Discrete Diffusion Models in a Vision-Language-Action Framework](https://arxiv.org/abs/2506.08185)
*Huixin Zhan,Jason H. Moore*

Main category: cs.CV

TL;DR: The paper introduces a method to model surgeon-specific styles in robotic surgery using a diffusion framework with a VLA pipeline, balancing personalization and privacy risks.


<details>
  <summary>Details</summary>
Motivation: Current AI systems overlook surgeon personalization, which is crucial due to differences in training, experience, and behavior.

Method: A discrete diffusion framework integrates vision-language-action inputs, using natural language prompts for surgeon fingerprinting without explicit identity exposure.

Result: The method accurately reconstructs gesture sequences and learns unique surgeon fingerprints, but expressive embeddings increase identity leakage risk.

Conclusion: Personalized embeddings enhance performance but raise privacy concerns, highlighting the need to balance personalization with privacy in surgical AI.

Abstract: Surgeons exhibit distinct operating styles due to differences in training,
experience, and motor behavior - yet current AI systems often ignore this
personalization signal. We propose a novel approach to model fine-grained,
surgeon-specific fingerprinting in robotic surgery using a discrete diffusion
framework integrated with a vision-language-action (VLA) pipeline. Our method
formulates gesture prediction as a structured sequence denoising task,
conditioned on multimodal inputs including endoscopic video, surgical intent
language, and a privacy-aware embedding of surgeon identity and skill.
Personalized surgeon fingerprinting is encoded through natural language prompts
using third-party language models, allowing the model to retain individual
behavioral style without exposing explicit identity. We evaluate our method on
the JIGSAWS dataset and demonstrate that it accurately reconstructs gesture
sequences while learning meaningful motion fingerprints unique to each surgeon.
To quantify the privacy implications of personalization, we perform membership
inference attacks and find that more expressive embeddings improve task
performance but simultaneously increase susceptibility to identity leakage.
These findings demonstrate that while personalized embeddings improve
performance, they also increase vulnerability to identity leakage, revealing
the importance of balancing personalization with privacy risk in surgical
modeling. Code is available at:
https://github.com/huixin-zhan-ai/Surgeon_style_fingerprinting.

</details>


### [86] [Open World Scene Graph Generation using Vision Language Models](https://arxiv.org/abs/2506.08189)
*Amartya Dutta,Kazi Sajeed Mehrab,Medha Sawhney,Abhilash Neog,Mridul Khurana,Sepideh Fatemi,Aanish Pradhan,M. Maruf,Ismini Lourentzou,Arka Daw,Anuj Karpatne*

Main category: cs.CV

TL;DR: The paper introduces Open-World SGG, a zero-shot framework for scene-graph generation using pretrained VLMs without task-specific training.


<details>
  <summary>Details</summary>
Motivation: Current SGG methods rely on dataset-specific supervision, limiting their applicability in open-world settings with novel objects/relations.

Method: The framework uses multimodal prompting, embedding alignment, and pair-refinement for zero-shot structured reasoning.

Result: Experiments on Visual Genome, Open Images V6, and PSG show VLMs can perform relational understanding without training.

Conclusion: Pretrained VLMs enable efficient, model-agnostic SGG in open-world settings without additional learning.

Abstract: Scene-Graph Generation (SGG) seeks to recognize objects in an image and
distill their salient pairwise relationships. Most methods depend on
dataset-specific supervision to learn the variety of interactions, restricting
their usefulness in open-world settings, involving novel objects and/or
relations. Even methods that leverage large Vision Language Models (VLMs)
typically require benchmark-specific fine-tuning. We introduce Open-World SGG,
a training-free, efficient, model-agnostic framework that taps directly into
the pretrained knowledge of VLMs to produce scene graphs with zero additional
learning. Casting SGG as a zero-shot structured-reasoning problem, our method
combines multimodal prompting, embedding alignment, and a lightweight
pair-refinement strategy, enabling inference over unseen object vocabularies
and relation sets. To assess this setting, we formalize an Open-World
evaluation protocol that measures performance when no SGG-specific data have
been observed either in terms of objects and relations. Experiments on Visual
Genome, Open Images V6, and the Panoptic Scene Graph (PSG) dataset demonstrate
the capacity of pretrained VLMs to perform relational understanding without
task-level training.

</details>


### [87] [Generative Learning of Differentiable Object Models for Compositional Interpretation of Complex Scenes](https://arxiv.org/abs/2506.08191)
*Antoni Nowinowski,Krzysztof Krawiec*

Main category: cs.CV

TL;DR: Extended DVP architecture handles multiple objects, improves training via latent-space loss functions, and outperforms baselines in reconstruction and decomposition.


<details>
  <summary>Details</summary>
Motivation: To enhance the original DVP's capability to handle multiple objects and improve training efficiency by leveraging latent-space loss functions.

Method: Extends DVP with multi-object handling, uses decoder for additional training samples, and introduces latent-space loss functions.

Result: Outperforms MONet and LIVE in reconstruction and decomposition; new benchmark proposed.

Conclusion: Extended DVP is superior but highlights limitations of differentiable rendering in autoencoders.

Abstract: This study builds on the architecture of the Disentangler of Visual Priors
(DVP), a type of autoencoder that learns to interpret scenes by decomposing the
perceived objects into independent visual aspects of shape, size, orientation,
and color appearance. These aspects are expressed as latent parameters which
control a differentiable renderer that performs image reconstruction, so that
the model can be trained end-to-end with gradient using reconstruction loss. In
this study, we extend the original DVP so that it can handle multiple objects
in a scene. We also exploit the interpretability of its latent by using the
decoder to sample additional training examples and devising alternative
training modes that rely on loss functions defined not only in the image space,
but also in the latent space. This significantly facilitates training, which is
otherwise challenging due to the presence of extensive plateaus in the
image-space reconstruction loss. To examine the performance of this approach,
we propose a new benchmark featuring multiple 2D objects, which subsumes the
previously proposed Multi-dSprites dataset while being more parameterizable. We
compare the DVP extended in these ways with two baselines (MONet and LIVE) and
demonstrate its superiority in terms of reconstruction quality and capacity to
decompose overlapping objects. We also analyze the gradients induced by the
considered loss functions, explain how they impact the efficacy of training,
and discuss the limitations of differentiable rendering in autoencoders and the
ways in which they can be addressed.

</details>


### [88] [GIQ: Benchmarking 3D Geometric Reasoning of Vision Foundation Models with Simulated and Real Polyhedra](https://arxiv.org/abs/2506.08194)
*Mateusz Michalkiewicz,Anekha Sokhal,Tadeusz Michalkiewicz,Piotr Pawlikowski,Mahsa Baktashmotlagh,Varun Jampani,Guha Balakrishnan*

Main category: cs.CV

TL;DR: The paper introduces GIQ, a benchmark to evaluate geometric reasoning in vision and vision-language models, revealing significant shortcomings in current models.


<details>
  <summary>Details</summary>
Motivation: To assess the true geometric understanding of vision and vision-language models, which remains unclear despite their success on standard benchmarks.

Method: GIQ includes synthetic and real-world images of diverse polyhedra, tested through tasks like 3D reconstruction, symmetry detection, mental rotation, and zero-shot classification.

Result: Current models struggle with basic geometric forms, symmetry detection, and detailed differentiation, while vision-language assistants perform poorly on complex polyhedra.

Conclusion: GIQ highlights critical gaps in geometric intelligence, aiming to improve geometry-aware representation learning.

Abstract: Monocular 3D reconstruction methods and vision-language models (VLMs)
demonstrate impressive results on standard benchmarks, yet their true
understanding of geometric properties remains unclear. We introduce GIQ , a
comprehensive benchmark specifically designed to evaluate the geometric
reasoning capabilities of vision and vision-language foundation models. GIQ
comprises synthetic and real-world images of 224 diverse polyhedra - including
Platonic, Archimedean, Johnson, and Catalan solids, as well as stellations and
compound shapes - covering varying levels of complexity and symmetry. Through
systematic experiments involving monocular 3D reconstruction, 3D symmetry
detection, mental rotation tests, and zero-shot shape classification tasks, we
reveal significant shortcomings in current models. State-of-the-art
reconstruction algorithms trained on extensive 3D datasets struggle to
reconstruct even basic geometric forms accurately. While foundation models
effectively detect specific 3D symmetry elements via linear probing, they
falter significantly in tasks requiring detailed geometric differentiation,
such as mental rotation. Moreover, advanced vision-language assistants exhibit
remarkably low accuracy on complex polyhedra, systematically misinterpreting
basic properties like face geometry, convexity, and compound structures. GIQ is
publicly available, providing a structured platform to highlight and address
critical gaps in geometric intelligence, facilitating future progress in
robust, geometry-aware representation learning.

</details>


### [89] [A Comprehensive Study of Decoder-Only LLMs for Text-to-Image Generation](https://arxiv.org/abs/2506.08210)
*Andrew Z. Wang,Songwei Ge,Tero Karras,Ming-Yu Liu,Yogesh Balaji*

Main category: cs.CV

TL;DR: The paper explores using modern decoder-only LLMs as text encoders for text-to-image diffusion models, finding that layer-normalized averaging of embeddings outperforms traditional last-layer embeddings and T5 baselines.


<details>
  <summary>Details</summary>
Motivation: Current text-to-image models often use outdated text encoders like T5 and CLIP. The study aims to evaluate the effectiveness of modern LLMs as text encoders.

Method: A standardized pipeline trains 27 models with 12 text encoders, analyzing embedding extraction approaches, LLM variants, and model sizes.

Result: Layer-normalized averaging of embeddings improves alignment with complex prompts, outperforming T5 and last-layer embeddings.

Conclusion: Modern LLMs with optimized conditioning enhance text-to-image generation, particularly for advanced visio-linguistic reasoning.

Abstract: Both text-to-image generation and large language models (LLMs) have made
significant advancements. However, many text-to-image models still employ the
somewhat outdated T5 and CLIP as their text encoders. In this work, we
investigate the effectiveness of using modern decoder-only LLMs as text
encoders for text-to-image diffusion models. We build a standardized training
and evaluation pipeline that allows us to isolate and evaluate the effect of
different text embeddings. We train a total of 27 text-to-image models with 12
different text encoders to analyze the critical aspects of LLMs that could
impact text-to-image generation, including the approaches to extract
embeddings, different LLMs variants, and model sizes. Our experiments reveal
that the de facto way of using last-layer embeddings as conditioning leads to
inferior performance. Instead, we explore embeddings from various layers and
find that using layer-normalized averaging across all layers significantly
improves alignment with complex prompts. Most LLMs with this conditioning
outperform the baseline T5 model, showing enhanced performance in advanced
visio-linguistic reasoning skills.

</details>


### [90] [Using Satellite Images And Self-supervised Machine Learning Networks To Detect Water Hidden Under Vegetation](https://arxiv.org/abs/2506.08214)
*Ioannis Iakovidis,Zahra Kalantari,Amir Hossein Payberah,Fernando Jaramillo,Francisco Pena Escobar*

Main category: cs.CV

TL;DR: Self-supervised training with deep clustering and negative sampling improves wetland segmentation in radar images without manual annotations, outperforming supervised models.


<details>
  <summary>Details</summary>
Motivation: Manual annotation of satellite images for wetland monitoring is slow and costly, prompting the need for self-supervised methods.

Method: Combines deep clustering and negative sampling for unsupervised training, with an ensemble approach to reduce variance.

Result: Ensemble of self-supervised models achieves a 0.02 improvement in Intersection Over Union (IoU) over supervised models.

Conclusion: Self-supervised methods can effectively replace manual annotations for wetland segmentation, offering cost and efficiency benefits.

Abstract: In recent years the wide availability of high-resolution radar satellite
images along with the advancement of computer vision models have enabled the
remote monitoring of the surface area of wetlands. However, these models
require large amounts of manually annotated satellite images, which are slow
and expensive to produce. To overcome this problem, self-supervised training
methods have been deployed to train models without using annotated data. In
this paper we use a combination of deep clustering and negative sampling to
train a model to segment radar satellite images into areas that separate water
from land without the use of any manual annotations. Furthermore, we implement
an ensemble version of the model to reduce variance and improve performance.
Compared to a single fully-supervised model using the same architecture, our
ensemble of self-supervised models achieves a 0.02 improvement in the
Intersection Over Union metric over our test dataset.

</details>


### [91] [Jamais Vu: Exposing the Generalization Gap in Supervised Semantic Correspondence](https://arxiv.org/abs/2506.08220)
*Octave Mariotti,Zhipeng Du,Yash Bhalgat,Oisin Mac Aodha,Hakan Bilen*

Main category: cs.CV

TL;DR: A novel approach for dense semantic correspondence by lifting 2D keypoints into 3D space, outperforming supervised baselines and showing unsupervised methods generalize better.


<details>
  <summary>Details</summary>
Motivation: Supervised SC methods generalize poorly beyond sparse keypoints, acting as detectors rather than learning dense correspondences.

Method: Lifts 2D keypoints into a canonical 3D space using monocular depth estimation, creating a continuous manifold without 3D supervision. Introduces SPair-U dataset for better evaluation.

Result: Outperforms supervised baselines on unseen keypoints; unsupervised methods generalize better across datasets.

Conclusion: The proposed method learns robust dense correspondences, with unsupervised approaches showing superior generalization.

Abstract: Semantic correspondence (SC) aims to establish semantically meaningful
matches across different instances of an object category. We illustrate how
recent supervised SC methods remain limited in their ability to generalize
beyond sparsely annotated training keypoints, effectively acting as keypoint
detectors. To address this, we propose a novel approach for learning dense
correspondences by lifting 2D keypoints into a canonical 3D space using
monocular depth estimation. Our method constructs a continuous canonical
manifold that captures object geometry without requiring explicit 3D
supervision or camera annotations. Additionally, we introduce SPair-U, an
extension of SPair-71k with novel keypoint annotations, to better assess
generalization. Experiments not only demonstrate that our model significantly
outperforms supervised baselines on unseen keypoints, highlighting its
effectiveness in learning robust correspondences, but that unsupervised
baselines outperform supervised counterparts when generalized across different
datasets.

</details>


### [92] [A Good CREPE needs more than just Sugar: Investigating Biases in Compositional Vision-Language Benchmarks](https://arxiv.org/abs/2506.08227)
*Vishaal Udandarao,Mehdi Cherti,Shyamgopal Karthik,Jenia Jitsev,Samuel Albanie,Matthias Bethge*

Main category: cs.CV

TL;DR: The paper analyzes biases in 17 benchmarks for vision-language models (VLMs), revealing that simple heuristics match CLIP model performance due to distribution asymmetries in benchmark design. Recommendations are provided for more robust benchmarks.


<details>
  <summary>Details</summary>
Motivation: To uncover biases in existing benchmarks for VLMs and assess their effectiveness in measuring compositional understanding.

Method: Scrutinized design choices (data sources, curation procedures) of 17 benchmarks and compared performance of blind heuristics (e.g., token-length) with CLIP models.

Result: Found inherent biases and distribution asymmetries in benchmarks, leading to poor measurement of compositional understanding. Simple heuristics performed as well as CLIP models.

Conclusion: Proposed recommendations for constructing more robust benchmarks to avoid such biases and improve evaluation of VLMs.

Abstract: We investigate 17 benchmarks (e.g. SugarCREPE, VALSE) commonly used for
measuring compositional understanding capabilities of vision-language models
(VLMs). We scrutinize design choices in their construction, including data
source (e.g. MS-COCO) and curation procedures (e.g. constructing negative
images/captions), uncovering several inherent biases across most benchmarks. We
find that blind heuristics (e.g. token-length, log-likelihood under a language
model) perform on par with CLIP models, indicating that these benchmarks do not
effectively measure compositional understanding. We demonstrate that the
underlying factor is a distribution asymmetry between positive and negative
images/captions, induced by the benchmark construction procedures. To mitigate
these issues, we provide a few key recommendations for constructing more robust
vision-language compositional understanding benchmarks, that would be less
prone to such simple attacks.

</details>


### [93] [Highly Compressed Tokenizer Can Generate Without Training](https://arxiv.org/abs/2506.08257)
*L. Lao Beyer,T. Li,X. Chen,S. Karaman,K. He*

Main category: cs.CV

TL;DR: 1D image tokenizers compress images into 1D sequences, enabling fine-grained editing and generation through simple token manipulations and optimization.


<details>
  <summary>Details</summary>
Motivation: The expressivity of 1D tokenizers' latent space allows for image editing and generation without training generative models.

Method: Uses gradient-based test-time optimization of tokens with plug-and-play loss functions (e.g., reconstruction, CLIP similarity).

Result: Enables inpainting and text-guided image editing, producing diverse and realistic samples.

Conclusion: 1D tokenizers offer a lightweight, flexible approach for image manipulation and generation.

Abstract: Commonly used image tokenizers produce a 2D grid of spatially arranged
tokens. In contrast, so-called 1D image tokenizers represent images as highly
compressed one-dimensional sequences of as few as 32 discrete tokens. We find
that the high degree of compression achieved by a 1D tokenizer with vector
quantization enables image editing and generative capabilities through
heuristic manipulation of tokens, demonstrating that even very crude
manipulations -- such as copying and replacing tokens between latent
representations of images -- enable fine-grained image editing by transferring
appearance and semantic attributes. Motivated by the expressivity of the 1D
tokenizer's latent space, we construct an image generation pipeline leveraging
gradient-based test-time optimization of tokens with plug-and-play loss
functions such as reconstruction or CLIP similarity. Our approach is
demonstrated for inpainting and text-guided image editing use cases, and can
generate diverse and realistic samples without requiring training of any
generative model.

</details>


### [94] [Seeing Voices: Generating A-Roll Video from Audio with Mirage](https://arxiv.org/abs/2506.08279)
*Aditi Sundararaman,Amogh Adishesha,Andrew Jaegle,Dan Bigioi,Hyoung-Kyu Song,Jon Kyl,Justin Mao,Kevin Lan,Mojtaba Komeili,ShahRukh Athar,Sheila Babayan,Stanislau Beliasau,William Buchwalter*

Main category: cs.CV

TL;DR: Mirage is an audio-to-video foundation model that generates realistic video from audio input, excelling in multimodal video creation when combined with speech synthesis.


<details>
  <summary>Details</summary>
Motivation: Current video generation methods either ignore sound or are domain-restricted. Mirage aims to integrate audio and visual elements seamlessly for broader, high-quality applications.

Method: Mirage uses a unified self-attention-based training approach for audio-to-video generation, enabling generality and superior output quality without domain-specific architectures.

Result: Mirage produces realistic, expressive videos from audio, especially effective for speech-driven content, outperforming specialized methods in subjective quality.

Conclusion: Mirage advances audio-to-video generation by combining generality and high-quality output, demonstrating potential for diverse applications in multimodal content creation.

Abstract: From professional filmmaking to user-generated content, creators and
consumers have long recognized that the power of video depends on the
harmonious integration of what we hear (the video's audio track) with what we
see (the video's image sequence). Current approaches to video generation either
ignore sound to focus on general-purpose but silent image sequence generation
or address both visual and audio elements but focus on restricted application
domains such as re-dubbing. We introduce Mirage, an audio-to-video foundation
model that excels at generating realistic, expressive output imagery from
scratch given an audio input. When integrated with existing methods for speech
synthesis (text-to-speech, or TTS), Mirage results in compelling multimodal
video. When trained on audio-video footage of people talking (A-roll) and
conditioned on audio containing speech, Mirage generates video of people
delivering a believable interpretation of the performance implicit in input
audio. Our central technical contribution is a unified method for training
self-attention-based audio-to-video generation models, either from scratch or
given existing weights. This methodology allows Mirage to retain generality as
an approach to audio-to-video generation while producing outputs of superior
subjective quality to methods that incorporate audio-specific architectures or
loss components specific to people, speech, or details of how images or audio
are captured. We encourage readers to watch and listen to the results of Mirage
for themselves (see paper and comments for links).

</details>


### [95] [SEMA: a Scalable and Efficient Mamba like Attention via Token Localization and Averaging](https://arxiv.org/abs/2506.08297)
*Nhat Thanh Tran,Fanghui Xue,Shuai Zhang,Jiancheng Lyu,Yunling Zheng,Yingyong Qi,Jack Xin*

Main category: cs.CV

TL;DR: The paper introduces SEMA, a scalable and efficient attention mechanism for vision tasks, addressing the limitations of vanilla and linear attention by avoiding dispersion and maintaining focus.


<details>
  <summary>Details</summary>
Motivation: The quadratic complexity of vanilla attention and the lack of focus in linear attention pose challenges for computer vision tasks. The dispersion property of generalized attention further motivates the need for a better solution.

Method: The authors design SEMA, leveraging token localization to prevent dispersion and arithmetic averaging to capture global attention aspects.

Result: SEMA outperforms recent vision Mamba models on Imagenet-1k, demonstrating scalability and effectiveness with larger image scales.

Conclusion: SEMA is a viable alternative to linear attention, offering scalability and improved performance for vision tasks.

Abstract: Attention is the critical component of a transformer. Yet the quadratic
computational complexity of vanilla full attention in the input size and the
inability of its linear attention variant to focus have been challenges for
computer vision tasks. We provide a mathematical definition of generalized
attention and formulate both vanilla softmax attention and linear attention
within the general framework. We prove that generalized attention disperses,
that is, as the number of keys tends to infinity, the query assigns equal
weights to all keys. Motivated by the dispersion property and recent
development of Mamba form of attention, we design Scalable and Efficient Mamba
like Attention (SEMA) which utilizes token localization to avoid dispersion and
maintain focusing, complemented by theoretically consistent arithmetic
averaging to capture global aspect of attention. We support our approach on
Imagenet-1k where classification results show that SEMA is a scalable and
effective alternative beyond linear attention, outperforming recent vision
Mamba models on increasingly larger scales of images at similar model parameter
sizes.

</details>


### [96] [OpenRR-1k: A Scalable Dataset for Real-World Reflection Removal](https://arxiv.org/abs/2506.08299)
*Kangning Yang,Ling Ouyang,Huiming Sun,Jie Cai,Lan Fu,Jiaming Ding,Chiu Man Ho,Zibo Meng*

Main category: cs.CV

TL;DR: The paper introduces a novel method for collecting high-quality reflection datasets and presents the OpenRR-1k dataset, which improves reflection removal robustness in real-world scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing reflection removal techniques lack high-quality in-the-wild datasets, limiting their effectiveness.

Method: A convenient, cost-effective, and scalable paradigm for collecting reflection datasets, resulting in the OpenRR-1k dataset with 1,000 high-quality image pairs.

Result: The dataset enhances reflection removal robustness in challenging real-world environments, as validated by benchmark evaluations.

Conclusion: The OpenRR-1k dataset addresses the data quality gap and supports improved reflection removal methods.

Abstract: Reflection removal technology plays a crucial role in photography and
computer vision applications. However, existing techniques are hindered by the
lack of high-quality in-the-wild datasets. In this paper, we propose a novel
paradigm for collecting reflection datasets from a fresh perspective. Our
approach is convenient, cost-effective, and scalable, while ensuring that the
collected data pairs are of high quality, perfectly aligned, and represent
natural and diverse scenarios. Following this paradigm, we collect a
Real-world, Diverse, and Pixel-aligned dataset (named OpenRR-1k dataset), which
contains 1,000 high-quality transmission-reflection image pairs collected in
the wild. Through the analysis of several reflection removal methods and
benchmark evaluation experiments on our dataset, we demonstrate its
effectiveness in improving robustness in challenging real-world environments.
Our dataset is available at https://github.com/caijie0620/OpenRR-1k.

</details>


### [97] [Hyperspectral Image Classification via Transformer-based Spectral-Spatial Attention Decoupling and Adaptive Gating](https://arxiv.org/abs/2506.08324)
*Guandong Li,Mengxia Ye*

Main category: cs.CV

TL;DR: STNet, a novel network architecture, addresses hyperspectral image classification challenges by decoupling spatial-spectral attention and using gating mechanisms, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks struggle with high-dimensional data, sparse ground objects, and spectral redundancy in hyperspectral images, leading to overfitting and poor generalization.

Method: STNet introduces a Spatial-Spectral Transformer module with decoupled attention and gating mechanisms (adaptive attention fusion gating and GFFN) for targeted feature extraction and fusion.

Result: STNet shows superior performance on IN, UP, and KSC datasets, reducing overfitting and enhancing feature extraction without increasing network size.

Conclusion: STNet effectively improves hyperspectral image classification by combining spatial-spectral attention and gating mechanisms, outperforming existing methods.

Abstract: Deep neural networks face several challenges in hyperspectral image
classification, including high-dimensional data, sparse distribution of ground
objects, and spectral redundancy, which often lead to classification
overfitting and limited generalization capability. To more effectively extract
and fuse spatial context with fine spectral information in hyperspectral image
(HSI) classification, this paper proposes a novel network architecture called
STNet. The core advantage of STNet stems from the dual innovative design of its
Spatial-Spectral Transformer module: first, the fundamental explicit decoupling
of spatial and spectral attention ensures targeted capture of key information
in HSI; second, two functionally distinct gating mechanisms perform intelligent
regulation at both the fusion level of attention flows (adaptive attention
fusion gating) and the internal level of feature transformation (GFFN). This
characteristic demonstrates superior feature extraction and fusion capabilities
compared to traditional convolutional neural networks, while reducing
overfitting risks in small-sample and high-noise scenarios. STNet enhances
model representation capability without increasing network depth or width. The
proposed method demonstrates superior performance on IN, UP, and KSC datasets,
outperforming mainstream hyperspectral image classification approaches.

</details>


### [98] [Locating Tennis Ball Impact on the Racket in Real Time Using an Event Camera](https://arxiv.org/abs/2506.08327)
*Yuto Kase,Kai Ishibe,Ryoma Yasuda,Yudai Washida,Sakiko Hashimoto*

Main category: cs.CV

TL;DR: A method using event cameras to locate tennis ball impact on rackets in real time, addressing high-speed camera limitations.


<details>
  <summary>Details</summary>
Motivation: High-speed cameras for impact location are memory-intensive and manual digitization is error-prone, hindering performance analysis.

Method: Uses event cameras for brightness changes, with three steps: swing time range, impact timing, and ball/racket contours, combining conventional and event-based processing (PATS).

Result: Experimental results were within permissible range for performance measurement, with short computation time for real-time use.

Conclusion: Proposed method enables efficient, real-time impact location tracking, overcoming traditional camera limitations.

Abstract: In racket sports, such as tennis, locating the ball's position at impact is
important in clarifying player and equipment characteristics, thereby aiding in
personalized equipment design. High-speed cameras are used to measure the
impact location; however, their excessive memory consumption limits prolonged
scene capture, and manual digitization for position detection is time-consuming
and prone to human error. These limitations make it difficult to effectively
capture the entire playing scene, hindering the ability to analyze the player's
performance. We propose a method for locating the tennis ball impact on the
racket in real time using an event camera. Event cameras efficiently measure
brightness changes (called `events') with microsecond accuracy under high-speed
motion while using lower memory consumption. These cameras enable users to
continuously monitor their performance over extended periods. Our method
consists of three identification steps: time range of swing, timing at impact,
and contours of ball and racket. Conventional computer vision techniques are
utilized along with an original event-based processing to detect the timing at
impact (PATS: the amount of polarity asymmetry in time symmetry). The results
of the experiments were within the permissible range for measuring tennis
players' performance. Moreover, the computation time was sufficiently short for
real-time applications.

</details>


### [99] [How Much To Guide: Revisiting Adaptive Guidance in Classifier-Free Guidance Text-to-Vision Diffusion Models](https://arxiv.org/abs/2506.08351)
*Huixuan Zhang,Junzhe Zhang,Xiaojun Wan*

Main category: cs.CV

TL;DR: Proposes Step AG, an adaptive guidance strategy for diffusion models, reducing computational costs by 20-30% while maintaining image quality and alignment.


<details>
  <summary>Details</summary>
Motivation: Classifier-free guidance in diffusion models doubles computational steps, increasing costs. Existing adaptive guidance lacks analysis and general applicability.

Method: Introduces Step AG, restricting classifier-free guidance to early denoising steps, applicable universally across models.

Result: Achieves 20-30% speedup without compromising image quality or text alignment, validated across settings and models.

Conclusion: Step AG is a simple, effective solution for reducing computational costs in diffusion models while maintaining performance.

Abstract: With the rapid development of text-to-vision generation diffusion models,
classifier-free guidance has emerged as the most prevalent method for
conditioning. However, this approach inherently requires twice as many steps
for model forwarding compared to unconditional generation, resulting in
significantly higher costs. While previous study has introduced the concept of
adaptive guidance, it lacks solid analysis and empirical results, making
previous method unable to be applied to general diffusion models. In this work,
we present another perspective of applying adaptive guidance and propose Step
AG, which is a simple, universally applicable adaptive guidance strategy. Our
evaluations focus on both image quality and image-text alignment. whose results
indicate that restricting classifier-free guidance to the first several
denoising steps is sufficient for generating high-quality, well-conditioned
images, achieving an average speedup of 20% to 30%. Such improvement is
consistent across different settings such as inference steps, and various
models including video generation models, highlighting the superiority of our
method.

</details>


### [100] [MedMoE: Modality-Specialized Mixture of Experts for Medical Vision-Language Understanding](https://arxiv.org/abs/2506.08356)
*Shivang Chopra,Lingchao Mao,Gabriela Sanchez-Rodriguez,Andrew J Feola,Jing Li,Zsolt Kira*

Main category: cs.CV

TL;DR: MedMoE is a vision-language framework that adapts visual representation dynamically for medical imaging, improving alignment and retrieval across modalities.


<details>
  <summary>Details</summary>
Motivation: Existing frameworks use uniform feature extraction, ignoring modality-specific needs in medical imaging.

Method: MedMoE uses a Mixture-of-Experts module to route multi-scale features through specialized branches, leveraging a Swin Transformer backbone.

Result: Improved alignment and retrieval performance across diverse medical benchmarks.

Conclusion: Modality-specialized visual representations enhance clinical vision-language systems.

Abstract: Different medical imaging modalities capture diagnostic information at
varying spatial resolutions, from coarse global patterns to fine-grained
localized structures. However, most existing vision-language frameworks in the
medical domain apply a uniform strategy for local feature extraction,
overlooking the modality-specific demands. In this work, we present MedMoE, a
modular and extensible vision-language processing framework that dynamically
adapts visual representation based on the diagnostic context. MedMoE
incorporates a Mixture-of-Experts (MoE) module conditioned on the report type,
which routes multi-scale image features through specialized expert branches
trained to capture modality-specific visual semantics. These experts operate
over feature pyramids derived from a Swin Transformer backbone, enabling
spatially adaptive attention to clinically relevant regions. This framework
produces localized visual representations aligned with textual descriptions,
without requiring modality-specific supervision at inference. Empirical results
on diverse medical benchmarks demonstrate that MedMoE improves alignment and
retrieval performance across imaging modalities, underscoring the value of
modality-specialized visual representations in clinical vision-language
systems.

</details>


### [101] [Image Demoiring Using Dual Camera Fusion on Mobile Phones](https://arxiv.org/abs/2506.08361)
*Yanting Mei,Zhilu Zhang,Xiaohe Wu,Wangmeng Zuo*

Main category: cs.CV

TL;DR: Proposes DCID, a dual-camera fusion method for removing moir patterns in images, leveraging ultra-wide-angle (UW) images to assist wide-angle (W) images. Achieves better performance than state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Moir patterns degrade image quality when shooting screens. Existing methods struggle with large/heavy moir. Dual-camera setups (common in smartphones) and UW images' normal colors/textures (due to different focal lengths) inspire the solution.

Method: Integrates a lightweight UW image encoder into a demoiring network and uses a fast two-stage alignment. Constructs a large real-world dataset (9,000 samples) for validation.

Result: Outperforms state-of-the-art methods on the constructed dataset.

Conclusion: DCID effectively removes moir patterns by fusing dual-camera data, validated by a large dataset. Code and dataset are publicly available.

Abstract: When shooting electronic screens, moir\'e patterns usually appear in captured
images, which seriously affects the image quality. Existing image demoir\'eing
methods face great challenges in removing large and heavy moir\'e. To address
the issue, we propose to utilize Dual Camera fusion for Image Demoir\'eing
(DCID), \ie, using the ultra-wide-angle (UW) image to assist the moir\'e
removal of wide-angle (W) image. This is inspired by two motivations: (1) the
two lenses are commonly equipped with modern smartphones, (2) the UW image
generally can provide normal colors and textures when moir\'e exists in the W
image mainly due to their different focal lengths. In particular, we propose an
efficient DCID method, where a lightweight UW image encoder is integrated into
an existing demoir\'eing network and a fast two-stage image alignment manner is
present. Moreover, we construct a large-scale real-world dataset with diverse
mobile phones and monitors, containing about 9,000 samples. Experiments on the
dataset show our method performs better than state-of-the-art methods. Code and
dataset are available at https://github.com/Mrduckk/DCID.

</details>


### [102] [SECOND: Mitigating Perceptual Hallucination in Vision-Language Models via Selective and Contrastive Decoding](https://arxiv.org/abs/2506.08391)
*Woohyeon Park,Woojin Kim,Jaeik Kim,Jaeyoung Do*

Main category: cs.CV

TL;DR: SECOND improves Vision-Language Models by reducing object hallucination through selective and contrastive decoding of multi-scale visual information.


<details>
  <summary>Details</summary>
Motivation: Existing VLMs suffer from object hallucination, limiting accurate visual understanding.

Method: Proposes SECOND, which selectively integrates and contrasts multi-scale visual information in an object-centric manner.

Result: SECOND reduces hallucinations and outperforms benchmarks.

Conclusion: Multi-scale visual information, prioritized and contrasted, enhances VLM performance beyond current methods.

Abstract: Despite significant advancements in Vision-Language Models (VLMs), the
performance of existing VLMs remains hindered by object hallucination, a
critical challenge to achieving accurate visual understanding. To address this
issue, we propose SECOND: Selective and Contrastive Decoding, a novel approach
that enables VLMs to effectively leverage multi-scale visual information with
an object-centric manner, closely aligning with human visual perception. SECOND
progressively selects and integrates multi-scale visual information,
facilitating a more precise interpretation of images. By contrasting these
visual information iteratively, SECOND significantly reduces perceptual
hallucinations and outperforms a wide range of benchmarks. Our theoretical
analysis and experiments highlight the largely unexplored potential of
multi-scale application in VLMs, showing that prioritizing and contrasting
across scales outperforms existing methods.

</details>


### [103] [RadioDUN: A Physics-Inspired Deep Unfolding Network for Radio Map Estimation](https://arxiv.org/abs/2506.08418)
*Taiqin Chen,Zikun Zhou,Zheng Fang,Wenzhen Zou,Kanjun Liu,Ke Chen,Yongbing Zhang,Yaowei Wang*

Main category: cs.CV

TL;DR: The paper proposes RadioDUN, a deep unfolding network for dense radio map estimation, integrating physical propagation models and dynamic reweighting to improve accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning methods for radio map estimation lack integration with physical characteristics, leading to suboptimal performance.

Method: RadioDUN unfolds optimization, uses dynamic reweighting (DRM), and incorporates obstacle-related factors with shadowing loss.

Result: RadioDUN outperforms state-of-the-art methods in experiments.

Conclusion: The method effectively combines physical models with deep learning for superior radio map estimation.

Abstract: The radio map represents the spatial distribution of spectrum resources
within a region, supporting efficient resource allocation and interference
mitigation. However, it is difficult to construct a dense radio map as a
limited number of samples can be measured in practical scenarios. While
existing works have used deep learning to estimate dense radio maps from sparse
samples, they are hard to integrate with the physical characteristics of the
radio map. To address this challenge, we cast radio map estimation as the
sparse signal recovery problem. A physical propagation model is further
incorporated to decompose the problem into multiple factor optimization
sub-problems, thereby reducing recovery complexity. Inspired by the existing
compressive sensing methods, we propose the Radio Deep Unfolding Network
(RadioDUN) to unfold the optimization process, achieving adaptive parameter
adjusting and prior fitting in a learnable manner. To account for the radio
propagation characteristics, we develop a dynamic reweighting module (DRM) to
adaptively model the importance of each factor for the radio map. Inspired by
the shadowing factor in the physical propagation model, we integrate
obstacle-related factors to express the obstacle-induced signal stochastic
decay. The shadowing loss is further designed to constrain the factor
prediction and act as a supplementary supervised objective, which enhances the
performance of RadioDUN. Extensive experiments have been conducted to
demonstrate that the proposed method outperforms the state-of-the-art methods.
Our code will be made publicly available upon publication.

</details>


### [104] [Better Reasoning with Less Data: Enhancing VLMs Through Unified Modality Scoring](https://arxiv.org/abs/2506.08429)
*Mingjie Xu,Andrew Estornell,Hongzheng Yang,Yuzhi Zhao,Zhaowei Zhu,Qi Xuan,Jiaheng Wei*

Main category: cs.CV

TL;DR: SCALE introduces a quality-driven data selection pipeline for VLM instruction tuning, addressing noisy alignments and ambiguous text by evaluating cross-modality alignment and data quality.


<details>
  <summary>Details</summary>
Motivation: Current VLMs rely on large-scale datasets, but noisy alignments and misleading text hinder performance. SCALE aims to improve dataset quality for better visual understanding.

Method: SCALE uses a cross-modality framework to assign tasks, generate captions, and evaluate alignment, clarity, task rarity, text coherence, and image clarity.

Result: SCALE identifies limitations in unimodal quality assessments and shows that generated captions can unify multimodal tasks into text modality.

Conclusion: SCALE enhances VLM performance by improving dataset quality and alignment, offering a robust solution for visual instruction tuning.

Abstract: The application of visual instruction tuning and other post-training
techniques has significantly enhanced the capabilities of Large Language Models
(LLMs) in visual understanding, enriching Vision-Language Models (VLMs) with
more comprehensive visual language datasets. However, the effectiveness of VLMs
is highly dependent on large-scale, high-quality datasets that ensure precise
recognition and accurate reasoning. Two key challenges hinder progress: (1)
noisy alignments between images and the corresponding text, which leads to
misinterpretation, and (2) ambiguous or misleading text, which obscures visual
content. To address these challenges, we propose SCALE (Single modality data
quality and Cross modality Alignment Evaluation), a novel quality-driven data
selection pipeline for VLM instruction tuning datasets. Specifically, SCALE
integrates a cross-modality assessment framework that first assigns each data
entry to its appropriate vision-language task, generates general and
task-specific captions (covering scenes, objects, style, etc.), and evaluates
the alignment, clarity, task rarity, text coherence, and image clarity of each
entry based on the generated captions. We reveal that: (1) current unimodal
quality assessment methods evaluate one modality while overlooking the rest,
which can underestimate samples essential for specific tasks and discard the
lower-quality instances that help build model robustness; and (2) appropriately
generated image captions provide an efficient way to transfer the image-text
multimodal task into a unified text modality.

</details>


### [105] [Enhancing Motion Dynamics of Image-to-Video Models via Adaptive Low-Pass Guidance](https://arxiv.org/abs/2506.08456)
*June Suk Choi,Kyungmin Lee,Sihyun Yu,Yisol Choi,Jinwoo Shin,Kimin Lee*

Main category: cs.CV

TL;DR: Proposes adaptive low-pass guidance (ALG) to enhance motion dynamics in image-to-video (I2V) generation by modulating frequency content early in denoising.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning T2V models for I2V often results in static videos due to premature exposure to high-frequency details, biasing the sampling process.

Method: Introduces ALG, which adaptively applies low-pass filtering to the conditioning image early in denoising to improve motion dynamics.

Result: ALG improves dynamic degree by 36% in VBench-I2V tests without compromising video quality or image fidelity.

Conclusion: ALG effectively addresses static video issues in I2V generation, enhancing motion while maintaining quality.

Abstract: Recent text-to-video (T2V) models have demonstrated strong capabilities in
producing high-quality, dynamic videos. To improve the visual controllability,
recent works have considered fine-tuning pre-trained T2V models to support
image-to-video (I2V) generation. However, such adaptation frequently suppresses
motion dynamics of generated outputs, resulting in more static videos compared
to their T2V counterparts. In this work, we analyze this phenomenon and
identify that it stems from the premature exposure to high-frequency details in
the input image, which biases the sampling process toward a shortcut trajectory
that overfits to the static appearance of the reference image. To address this,
we propose adaptive low-pass guidance (ALG), a simple fix to the I2V model
sampling procedure to generate more dynamic videos without compromising
per-frame image quality. Specifically, ALG adaptively modulates the frequency
content of the conditioning image by applying low-pass filtering at the early
stage of denoising. Extensive experiments demonstrate that ALG significantly
improves the temporal dynamics of generated videos, while preserving image
fidelity and text alignment. Especially, under VBench-I2V test suite, ALG
achieves an average improvement of 36% in dynamic degree without a significant
drop in video quality or image fidelity.

</details>


### [106] [MARMOT: Masked Autoencoder for Modeling Transient Imaging](https://arxiv.org/abs/2506.08470)
*Siyuan Shen,Ziheng Wang,Xingyue Peng,Suan Xia,Ruiqian Li,Shiying Li,Jingyi Yu*

Main category: cs.CV

TL;DR: The paper introduces MARMOT, a masked autoencoder for transient imaging, pretrained on NLOS transient datasets to improve hidden object reconstruction.


<details>
  <summary>Details</summary>
Motivation: To leverage pretrained models for transient imaging in NLOS scenarios, where prior works lack dataset-based priors.

Method: Uses a Transformer-based encoder-decoder pretrained on diverse NLOS transient data with a scanning pattern mask (SPM) for self-supervised learning.

Result: MARMOT outperforms state-of-the-art methods in NLOS imaging tasks, demonstrating efficiency in feature transfer and finetuning.

Conclusion: MARMOT advances NLOS transient imaging by integrating pretraining and self-supervised learning, offering superior performance.

Abstract: Pretrained models have demonstrated impressive success in many modalities
such as language and vision. Recent works facilitate the pretraining paradigm
in imaging research. Transients are a novel modality, which are captured for an
object as photon counts versus arrival times using a precisely time-resolved
sensor. In particular for non-line-of-sight (NLOS) scenarios, transients of
hidden objects are measured beyond the sensor's direct line of sight. Using
NLOS transients, the majority of previous works optimize volume density or
surfaces to reconstruct the hidden objects and do not transfer priors learned
from datasets. In this work, we present a masked autoencoder for modeling
transient imaging, or MARMOT, to facilitate NLOS applications. Our MARMOT is a
self-supervised model pretrianed on massive and diverse NLOS transient
datasets. Using a Transformer-based encoder-decoder, MARMOT learns features
from partially masked transients via a scanning pattern mask (SPM), where the
unmasked subset is functionally equivalent to arbitrary sampling, and predicts
full measurements. Pretrained on TransVerse-a synthesized transient dataset of
500K 3D models-MARMOT adapts to downstream imaging tasks using direct feature
transfer or decoder finetuning. Comprehensive experiments are carried out in
comparisons with state-of-the-art methods. Quantitative and qualitative results
demonstrate the efficiency of our MARMOT.

</details>


### [107] [Context-aware TFL: A Universal Context-aware Contrastive Learning Framework for Temporal Forgery Localization](https://arxiv.org/abs/2506.08493)
*Qilin Yin,Wei Lu,Xiangyang Luo,Xiaochun Cao*

Main category: cs.CV

TL;DR: The paper introduces UniCaCLF, a context-aware contrastive learning framework for temporal forgery localization (TFL) in videos, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing deepfake detection methods treat it as classification, ignoring partial tampering. TFL for small fake clips in real videos is more realistic but challenging.

Method: Proposes UniCaCLF using supervised contrastive learning and a context-aware perception layer for anomaly detection, enhancing feature discriminability.

Result: UniCaCLF significantly outperforms state-of-the-art methods on five public datasets.

Conclusion: The framework effectively localizes temporal forged segments, addressing a gap in multimedia forensics.

Abstract: Most research efforts in the multimedia forensics domain have focused on
detecting forgery audio-visual content and reached sound achievements. However,
these works only consider deepfake detection as a classification task and
ignore the case where partial segments of the video are tampered with. Temporal
forgery localization (TFL) of small fake audio-visual clips embedded in real
videos is still challenging and more in line with realistic application
scenarios. To resolve this issue, we propose a universal context-aware
contrastive learning framework (UniCaCLF) for TFL. Our approach leverages
supervised contrastive learning to discover and identify forged instants by
means of anomaly detection, allowing for the precise localization of temporal
forged segments. To this end, we propose a novel context-aware perception layer
that utilizes a heterogeneous activation operation and an adaptive context
updater to construct a context-aware contrastive objective, which enhances the
discriminability of forged instant features by contrasting them with genuine
instant features in terms of their distances to the global context. An
efficient context-aware contrastive coding is introduced to further push the
limit of instant feature distinguishability between genuine and forged instants
in a supervised sample-by-sample manner, suppressing the cross-sample influence
to improve temporal forgery localization performance. Extensive experimental
results over five public datasets demonstrate that our proposed UniCaCLF
significantly outperforms the state-of-the-art competing algorithms.

</details>


### [108] [MLVTG: Mamba-Based Feature Alignment and LLM-Driven Purification for Multi-Modal Video Temporal Grounding](https://arxiv.org/abs/2506.08512)
*Zhiyi Zhu,Xiaoyu Wu,Zihao Liu,Linlin Yang*

Main category: cs.CV

TL;DR: MLVTG is a novel framework for Video Temporal Grounding (VTG) that improves multi-modal alignment and temporal modeling using MambaAligner and LLMRefiner, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing Transformer-based VTG methods face issues like redundant attention and poor multi-modal alignment, prompting the need for a more efficient solution.

Method: MLVTG integrates MambaAligner (using Vision Mamba blocks for temporal modeling) and LLMRefiner (leveraging frozen LLM layers for semantic alignment) to enhance localization.

Result: MLVTG achieves state-of-the-art performance on QVHighlights, Charades-STA, and TVSum datasets, surpassing baselines.

Conclusion: The dual alignment strategy of MLVTG effectively addresses VTG challenges, offering superior performance and precision.

Abstract: Video Temporal Grounding (VTG), which aims to localize video clips
corresponding to natural language queries, is a fundamental yet challenging
task in video understanding. Existing Transformer-based methods often suffer
from redundant attention and suboptimal multi-modal alignment. To address these
limitations, we propose MLVTG, a novel framework that integrates two key
modules: MambaAligner and LLMRefiner. MambaAligner uses stacked Vision Mamba
blocks as a backbone instead of Transformers to model temporal dependencies and
extract robust video representations for multi-modal alignment. LLMRefiner
leverages the specific frozen layer of a pre-trained Large Language Model (LLM)
to implicitly transfer semantic priors, enhancing multi-modal alignment without
fine-tuning. This dual alignment strategy, temporal modeling via structured
state-space dynamics and semantic purification via textual priors, enables more
precise localization. Extensive experiments on QVHighlights, Charades-STA, and
TVSum demonstrate that MLVTG achieves state-of-the-art performance and
significantly outperforms existing baselines.

</details>


### [109] [Robust Visual Localization via Semantic-Guided Multi-Scale Transformer](https://arxiv.org/abs/2506.08526)
*Zhongtao Tian,Wenhao Huang,Zhidong Chen,Xiao Wei Sun*

Main category: cs.CV

TL;DR: A framework combining multi-scale feature learning and semantic scene understanding improves visual localization in dynamic environments.


<details>
  <summary>Details</summary>
Motivation: Challenges in visual localization due to dynamic environments with lighting changes, weather, and moving objects disrupt appearance cues.

Method: Hierarchical Transformer with cross-scale attention and semantic supervision via neural scene representation.

Result: Outperforms existing pose regression methods in dynamic scenarios on TartanAir.

Conclusion: Integrating multi-scale processing with semantic guidance enhances robustness in real-world dynamic environments.

Abstract: Visual localization remains challenging in dynamic environments where
fluctuating lighting, adverse weather, and moving objects disrupt appearance
cues. Despite advances in feature representation, current absolute pose
regression methods struggle to maintain consistency under varying conditions.
To address this challenge, we propose a framework that synergistically combines
multi-scale feature learning with semantic scene understanding. Our approach
employs a hierarchical Transformer with cross-scale attention to fuse geometric
details and contextual cues, preserving spatial precision while adapting to
environmental changes. We improve the performance of this architecture with
semantic supervision via neural scene representation during training, guiding
the network to learn view-invariant features that encode persistent structural
information while suppressing complex environmental interference. Experiments
on TartanAir demonstrate that our approach outperforms existing pose regression
methods in challenging scenarios with dynamic objects, illumination changes,
and occlusions. Our findings show that integrating multi-scale processing with
semantic guidance offers a promising strategy for robust visual localization in
real-world dynamic environments.

</details>


### [110] [LiftVSR: Lifting Image Diffusion to Video Super-Resolution via Hybrid Temporal Modeling with Only 4$\times$RTX 4090s](https://arxiv.org/abs/2506.08529)
*Xijun Wang,Xin Li,Bingchen Li,Zhibo Chen*

Main category: cs.CV

TL;DR: LiftVSR is an efficient video super-resolution framework that improves temporal coherence and reduces computational costs using hybrid temporal modeling and diffusion priors.


<details>
  <summary>Details</summary>
Motivation: Existing VSR methods suffer from limited temporal coherence and high computational costs, especially for long videos.

Method: LiftVSR uses a hybrid temporal modeling mechanism: Dynamic Temporal Attention (DTA) for short-term modeling and Attention Memory Cache (AMC) for long-term consistency, leveraging diffusion priors from PixArt-.

Result: LiftVSR achieves state-of-the-art performance with significantly lower computational costs (4RTX 4090 GPUs).

Conclusion: LiftVSR balances efficiency and long-term consistency, offering a practical solution for high-quality VSR with reduced resource demands.

Abstract: Diffusion models have significantly advanced video super-resolution (VSR) by
enhancing perceptual quality, largely through elaborately designed temporal
modeling to ensure inter-frame consistency. However, existing methods usually
suffer from limited temporal coherence and prohibitively high computational
costs (e.g., typically requiring over 8 NVIDIA A100-80G GPUs), especially for
long videos. In this work, we propose LiftVSR, an efficient VSR framework that
leverages and elevates the image-wise diffusion prior from PixArt-$\alpha$,
achieving state-of-the-art results using only 4$\times$RTX 4090 GPUs. To
balance long-term consistency and efficiency, we introduce a hybrid temporal
modeling mechanism that decomposes temporal learning into two complementary
components: (i) Dynamic Temporal Attention (DTA) for fine-grained temporal
modeling within short frame segment ($\textit{i.e.}$, low complexity), and (ii)
Attention Memory Cache (AMC) for long-term temporal modeling across segments
($\textit{i.e.}$, consistency). Specifically, DTA identifies multiple token
flows across frames within multi-head query and key tokens to warp inter-frame
contexts in the value tokens. AMC adaptively aggregates historical segment
information via a cache unit, ensuring long-term coherence with minimal
overhead. To further stabilize the cache interaction during inference, we
introduce an asymmetric sampling strategy that mitigates feature mismatches
arising from different diffusion sampling steps. Extensive experiments on
several typical VSR benchmarks have demonstrated that LiftVSR achieves
impressive performance with significantly lower computational costs.

</details>


### [111] [TrajFlow: Multi-modal Motion Prediction via Flow Matching](https://arxiv.org/abs/2506.08541)
*Qi Yan,Brian Zhang,Yutong Zhang,Daniel Yang,Joshua White,Di Chen,Jiachao Liu,Langechuan Liu,Binnan Zhuang,Shaoshuai Shi,Renjie Liao*

Main category: cs.CV

TL;DR: TrajFlow is a flow matching-based motion prediction framework for autonomous driving, offering efficient multi-modal forecasts in a single pass, with improved uncertainty estimation and generalization.


<details>
  <summary>Details</summary>
Motivation: Motion prediction is vital for autonomous driving safety, especially under dynamic conditions requiring multi-modal forecasts. Existing methods are inefficient and lack scalability.

Method: TrajFlow uses flow matching to predict multiple trajectories in one pass, employs a Plackett-Luce ranking loss for uncertainty, and introduces self-conditioning training for better generalization.

Result: TrajFlow achieves state-of-the-art performance on the Waymo Open Motion Dataset, demonstrating efficiency and accuracy.

Conclusion: TrajFlow is effective for safety-critical autonomous driving, offering scalable and efficient motion prediction.

Abstract: Efficient and accurate motion prediction is crucial for ensuring safety and
informed decision-making in autonomous driving, particularly under dynamic
real-world conditions that necessitate multi-modal forecasts. We introduce
TrajFlow, a novel flow matching-based motion prediction framework that
addresses the scalability and efficiency challenges of existing generative
trajectory prediction methods. Unlike conventional generative approaches that
employ i.i.d. sampling and require multiple inference passes to capture diverse
outcomes, TrajFlow predicts multiple plausible future trajectories in a single
pass, significantly reducing computational overhead while maintaining coherence
across predictions. Moreover, we propose a ranking loss based on the
Plackett-Luce distribution to improve uncertainty estimation of predicted
trajectories. Additionally, we design a self-conditioning training technique
that reuses the model's own predictions to construct noisy inputs during a
second forward pass, thereby improving generalization and accelerating
inference. Extensive experiments on the large-scale Waymo Open Motion Dataset
(WOMD) demonstrate that TrajFlow achieves state-of-the-art performance across
various key metrics, underscoring its effectiveness for safety-critical
autonomous driving applications. The code and other details are available on
the project website https://traj-flow.github.io/.

</details>


### [112] [Convergence of Spectral Principal Paths: How Deep Networks Distill Linear Representations from Noisy Inputs](https://arxiv.org/abs/2506.08543)
*Bowei Tian,Xuntao Lyu,Meng Liu,Hongyi Wang,Ang Li*

Main category: cs.CV

TL;DR: The paper introduces the Input-Space Linearity Hypothesis (ISLH) and Spectral Principal Path (SPP) framework to explain how deep networks form human-interpretable representations, validated in Vision-Language Models (VLMs).


<details>
  <summary>Details</summary>
Motivation: To enhance AI transparency and control by shifting focus from low-level neurons to structured semantic directions aligned with human concepts, motivated by the Linear Representation Hypothesis (LRH).

Method: Proposes ISLH and SPP framework to formalize how deep networks distill linear representations along dominant spectral directions, with empirical validation in VLMs.

Result: Demonstrates multimodal robustness of these representations in VLMs, supporting the ISLH and SPP framework.

Conclusion: Advances a structured theory of representation formation in deep networks, contributing to AI robustness, fairness, and transparency.

Abstract: High-level representations have become a central focus in enhancing AI
transparency and control, shifting attention from individual neurons or
circuits to structured semantic directions that align with human-interpretable
concepts. Motivated by the Linear Representation Hypothesis (LRH), we propose
the Input-Space Linearity Hypothesis (ISLH), which posits that concept-aligned
directions originate in the input space and are selectively amplified with
increasing depth. We then introduce the Spectral Principal Path (SPP)
framework, which formalizes how deep networks progressively distill linear
representations along a small set of dominant spectral directions. Building on
this framework, we further demonstrate the multimodal robustness of these
representations in Vision-Language Models (VLMs). By bridging theoretical
insights with empirical validation, this work advances a structured theory of
representation formation in deep networks, paving the way for improving AI
robustness, fairness, and transparency.

</details>


### [113] [From Pixels to Graphs: using Scene and Knowledge Graphs for HD-EPIC VQA Challenge](https://arxiv.org/abs/2506.08553)
*Agnese Taluzzi,Davide Gesualdi,Riccardo Santambrogio,Chiara Plizzari,Francesca Palermo,Simone Mentasti,Matteo Matteucci*

Main category: cs.CV

TL;DR: SceneNet and KnowledgeNet improve VQA performance by combining scene graphs and commonsense knowledge, achieving 44.21% accuracy on HD-EPIC.


<details>
  <summary>Details</summary>
Motivation: To address complex egocentric VQA tasks by capturing fine-grained object interactions and leveraging commonsense knowledge.

Method: SceneNet uses MLLM-generated scene graphs for detailed interactions, while KnowledgeNet integrates ConceptNet for semantic reasoning.

Result: Combined framework achieves 44.21% accuracy on the HD-EPIC benchmark.

Conclusion: The integration of SceneNet and KnowledgeNet effectively tackles complex VQA tasks.

Abstract: This report presents SceneNet and KnowledgeNet, our approaches developed for
the HD-EPIC VQA Challenge 2025. SceneNet leverages scene graphs generated with
a multi-modal large language model (MLLM) to capture fine-grained object
interactions, spatial relationships, and temporally grounded events. In
parallel, KnowledgeNet incorporates ConceptNet's external commonsense knowledge
to introduce high-level semantic connections between entities, enabling
reasoning beyond directly observable visual evidence. Each method demonstrates
distinct strengths across the seven categories of the HD-EPIC benchmark, and
their combination within our framework results in an overall accuracy of 44.21%
on the challenge, highlighting its effectiveness for complex egocentric VQA
tasks.

</details>


### [114] [Towards Cross-Subject EMG Pattern Recognition via Dual-Branch Adversarial Feature Disentanglement](https://arxiv.org/abs/2506.08555)
*Xinyue Niu,Akira Furui*

Main category: cs.CV

TL;DR: The paper introduces a dual-branch adversarial neural network for cross-subject EMG pattern recognition, eliminating calibration needs by disentangling features into pattern-specific and subject-specific components.


<details>
  <summary>Details</summary>
Motivation: Addressing the impracticality of subject-specific calibration in EMG pattern recognition due to inter-subject variability.

Method: An end-to-end dual-branch adversarial neural network disentangles EMG features into pattern-specific and subject-specific components.

Result: The model outperforms baselines in cross-subject scenarios, enabling robust recognition for unseen users without calibration.

Conclusion: The approach offers a practical solution for cross-subject EMG recognition and potential for broader biometric applications.

Abstract: Cross-subject electromyography (EMG) pattern recognition faces significant
challenges due to inter-subject variability in muscle anatomy, electrode
placement, and signal characteristics. Traditional methods rely on
subject-specific calibration data to adapt models to new users, an approach
that is both time-consuming and impractical for large-scale, real-world
deployment. This paper presents an approach to eliminate calibration
requirements through feature disentanglement, enabling effective cross-subject
generalization. We propose an end-to-end dual-branch adversarial neural network
that simultaneously performs pattern recognition and individual identification
by disentangling EMG features into pattern-specific and subject-specific
components. The pattern-specific components facilitate robust pattern
recognition for new users without model calibration, while the subject-specific
components enable downstream applications such as task-invariant biometric
identification. Experimental results demonstrate that the proposed model
achieves robust performance on data from unseen users, outperforming various
baseline methods in cross-subject scenarios. Overall, this study offers a new
perspective for cross-subject EMG pattern recognition without model calibration
and highlights the proposed model's potential for broader applications, such as
task-independent biometric systems.

</details>


### [115] [Hierarchical Neural Collapse Detection Transformer for Class Incremental Object Detection](https://arxiv.org/abs/2506.08562)
*Duc Thanh Pham,Hong Dang Nguyen,Nhat Minh Nguyen Quoc,Linh Ngo Van,Sang Dinh Viet,Duc Anh Nguyen*

Main category: cs.CV

TL;DR: Hier-DETR improves Incremental Object Detection (IOD) by combining Neural Collapse and hierarchical class relations for efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the impracticality of existing IOD models due to limited performance and slow inference time.

Method: Leverages Neural Collapse for imbalance datasets and hierarchical class relations.

Result: Ensures efficiency and competitive performance in IOD.

Conclusion: Hier-DETR offers a practical solution for continual learning in object detection.

Abstract: Recently, object detection models have witnessed notable performance
improvements, particularly with transformer-based models. However, new objects
frequently appear in the real world, requiring detection models to continually
learn without suffering from catastrophic forgetting. Although Incremental
Object Detection (IOD) has emerged to address this challenge, these existing
models are still not practical due to their limited performance and prolonged
inference time. In this paper, we introduce a novel framework for IOD, called
Hier-DETR: Hierarchical Neural Collapse Detection Transformer, ensuring both
efficiency and competitive performance by leveraging Neural Collapse for
imbalance dataset and Hierarchical relation of classes' labels.

</details>


### [116] [Generating Vision-Language Navigation Instructions Incorporated Fine-Grained Alignment Annotations](https://arxiv.org/abs/2506.08566)
*Yibo Cui,Liang Xie,Yu Zhao,Jiawei Sun,Erwei Yin*

Main category: cs.CV

TL;DR: The paper introduces FCA-NIG, a framework for generating fine-grained cross-modal annotations in Vision-Language Navigation (VLN), addressing the lack of sub-instruction and entity-level alignments in existing datasets. The resulting FCA-R2R dataset improves VLN agent performance.


<details>
  <summary>Details</summary>
Motivation: Existing VLN datasets lack fine-grained cross-modal alignments (sub-instruction and entity-level), limiting accurate navigation decision-making.

Method: FCA-NIG divides trajectories into sub-trajectories, uses GLIP for landmark detection, OFA-Speaker for instruction generation, and CLIP for entity selection, creating annotated sub-instruction-trajectory pairs.

Result: FCA-R2R dataset enhances VLN agent performance (SF, EnvDrop, RecBERT, HAMT) by improving state awareness and navigation accuracy.

Conclusion: FCA-NIG generates high-quality, scalable training data without manual annotation, advancing fine-grained cross-modal learning in VLN.

Abstract: Vision-Language Navigation (VLN) enables intelligent agents to navigate
environments by integrating visual perception and natural language
instructions, yet faces significant challenges due to the scarcity of
fine-grained cross-modal alignment annotations. Existing datasets primarily
focus on global instruction-trajectory matching, neglecting
sub-instruction-level and entity-level alignments critical for accurate
navigation action decision-making. To address this limitation, we propose
FCA-NIG, a generative framework that automatically constructs navigation
instructions with dual-level fine-grained cross-modal annotations. In this
framework, an augmented trajectory is first divided into sub-trajectories,
which are then processed through GLIP-based landmark detection, crafted
instruction construction, OFA-Speaker based R2R-like instruction generation,
and CLIP-powered entity selection, generating sub-instruction-trajectory pairs
with entity-landmark annotations. Finally, these sub-pairs are aggregated to
form a complete instruction-trajectory pair. The framework generates the
FCA-R2R dataset, the first large-scale augmentation dataset featuring precise
sub-instruction-sub-trajectory and entity-landmark alignments. Extensive
experiments demonstrate that training with FCA-R2R significantly improves the
performance of multiple state-of-the-art VLN agents, including SF, EnvDrop,
RecBERT, and HAMT. Incorporating sub-instruction-trajectory alignment enhances
agents' state awareness and decision accuracy, while entity-landmark alignment
further boosts navigation performance and generalization. These results
highlight the effectiveness of FCA-NIG in generating high-quality, scalable
training data without manual annotation, advancing fine-grained cross-modal
learning in complex navigation tasks.

</details>


### [117] [Diversity-Guided MLP Reduction for Efficient Large Vision Transformers](https://arxiv.org/abs/2506.08591)
*Chengchao Shen,Hourun Zhu,Gongfan Fang,Jianxin Wang,Xinchao Wang*

Main category: cs.CV

TL;DR: DGMR method reduces MLP parameters in large vision transformers with minimal performance loss, achieving up to 71.5% reduction in parameters and FLOPs.


<details>
  <summary>Details</summary>
Motivation: Large transformer models are computationally expensive due to MLP modules dominating parameter count. DGMR aims to reduce costs while maintaining performance.

Method: Uses Diversity-Guided MLP Reduction (DGMR) with Gram-Schmidt weight pruning to eliminate redundant neurons, preserving weight diversity for performance recovery.

Result: Achieves 57.0%-71.5% reduction in parameters and FLOPs with negligible performance loss, requiring only 0.06% of LAION-2B data for recovery.

Conclusion: DGMR effectively compresses large vision transformers, making them more efficient without sacrificing performance.

Abstract: Transformer models achieve excellent scaling property, where the performance
is improved with the increment of model capacity. However, large-scale model
parameters lead to an unaffordable cost of computing and memory. We analyze
popular transformer architectures and find that multilayer perceptron (MLP)
modules take up the majority of model parameters. To this end, we focus on the
recoverability of the compressed models and propose a Diversity-Guided MLP
Reduction (DGMR) method to significantly reduce the parameters of large vision
transformers with only negligible performance degradation. Specifically, we
conduct a Gram-Schmidt weight pruning strategy to eliminate redundant neurons
of MLP hidden layer, while preserving weight diversity for better performance
recover during distillation. Compared to the model trained from scratch, our
pruned model only requires 0.06\% data of LAION-2B (for the training of large
vision transformers) without labels (ImageNet-1K) to recover the original
performance. Experimental results on several state-of-the-art large vision
transformers demonstrate that our method achieves a more than 57.0\% parameter
and FLOPs reduction in a near lossless manner. Notably, for EVA-CLIP-E (4.4B),
our method accomplishes a 71.5\% parameter and FLOPs reduction without
performance degradation. The source code and trained weights are available at
https://github.com/visresearch/DGMR.

</details>


### [118] [Transformers Meet Hyperspectral Imaging: A Comprehensive Study of Models, Challenges and Open Problems](https://arxiv.org/abs/2506.08596)
*Guyang Zhang,Waleed Abdulla*

Main category: cs.CV

TL;DR: A survey of Transformer-based HSI classification, reviewing 300+ papers, categorizing pipeline stages, and addressing challenges like data scarcity and computational overhead.


<details>
  <summary>Details</summary>
Motivation: To guide researchers in adopting Transformers for HSI by analyzing design choices and addressing field-specific challenges.

Method: Reviewed 300+ papers, categorized pipeline stages (pre-processing, tokenization, feature extraction, etc.), and contrasted design choices with HSI properties.

Result: Identified challenges (scarce data, computational overhead) and proposed solutions (public datasets, lightweight models, interpretable attention).

Conclusion: Outlined a research agenda to advance Transformer-based HSI classification, emphasizing practicality and interpretability.

Abstract: Transformers have become the architecture of choice for learning long-range
dependencies, yet their adoption in hyperspectral imaging (HSI) is still
emerging. We reviewed more than 300 papers published up to 2025 and present the
first end-to-end survey dedicated to Transformer-based HSI classification. The
study categorizes every stage of a typical pipeline-pre-processing, patch or
pixel tokenization, positional encoding, spatial-spectral feature extraction,
multi-head self-attention variants, skip connections, and loss design-and
contrasts alternative design choices with the unique spatial-spectral
properties of HSI. We map the field's progress against persistent obstacles:
scarce labeled data, extreme spectral dimensionality, computational overhead,
and limited model explainability. Finally, we outline a research agenda
prioritizing valuable public data sets, lightweight on-edge models,
illumination and sensor shifts robustness, and intrinsically interpretable
attention mechanisms. Our goal is to guide researchers in selecting, combining,
or extending Transformer components that are truly fit for purpose for
next-generation HSI applications.

</details>


### [119] [Towards Class-wise Fair Adversarial Training via Anti-Bias Soft Label Distillation](https://arxiv.org/abs/2506.08611)
*Shiji Zhao,Chi Chen,Ranjie Duan,Xizhe Wang,Xingxing Wei*

Main category: cs.CV

TL;DR: The paper addresses the robust fairness issue in Adversarial Training (AT) and Adversarial Robustness Distillation (ARD), proposing Anti-Bias Soft Label Distillation (ABSLD) to enhance fairness by adjusting class-wise smoothness of soft labels.


<details>
  <summary>Details</summary>
Motivation: The robust fairness issue in AT and ARD, where models perform unevenly across classes, motivates the need for a solution to balance adversarial robustness.

Method: The proposed ABSLD method adjusts the smoothness of teacher's soft labels for different classes using varying temperatures, reducing error risk gaps between classes.

Result: ABSLD outperforms state-of-the-art methods in robustness and fairness, as shown in extensive experiments.

Conclusion: ABSLD effectively enhances adversarial robust fairness and is adaptable for integration with other methods.

Abstract: Adversarial Training (AT) is widely recognized as an effective approach to
enhance the adversarial robustness of Deep Neural Networks. As a variant of AT,
Adversarial Robustness Distillation (ARD) has shown outstanding performance in
enhancing the robustness of small models. However, both AT and ARD face robust
fairness issue: these models tend to display strong adversarial robustness
against some classes (easy classes) while demonstrating weak adversarial
robustness against others (hard classes). This paper explores the underlying
factors of this problem and points out the smoothness degree of soft labels for
different classes significantly impacts the robust fairness from both empirical
observation and theoretical analysis. Based on the above exploration, we
propose Anti-Bias Soft Label Distillation (ABSLD) within the Knowledge
Distillation framework to enhance the adversarial robust fairness.
Specifically, ABSLD adaptively reduces the student's error risk gap between
different classes, which is accomplished by adjusting the class-wise smoothness
degree of teacher's soft labels during the training process, and the adjustment
is managed by assigning varying temperatures to different classes.
Additionally, as a label-based approach, ABSLD is highly adaptable and can be
integrated with the sample-based methods. Extensive experiments demonstrate
ABSLD outperforms state-of-the-art methods on the comprehensive performance of
robustness and fairness.

</details>


### [120] [Data-Efficient Challenges in Visual Inductive Priors: A Retrospective](https://arxiv.org/abs/2506.08612)
*Robert-Jan Bruintjes,Attila Lengyel,Osman Semih Kayhan,Davide Zambrano,Nergis Tmen,Hadi Jamali-Rad,Jan van Gemert*

Main category: cs.CV

TL;DR: The paper explores Deep Learning methods for data-deficient settings, focusing on the VIPriors workshop series, which challenges participants to train models with limited data and no transfer learning. Successful approaches include model ensembles and data augmentation.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of training effective Deep Learning models in data-deficient settings by leveraging prior knowledge and innovative methods.

Method: Organized the VIPriors workshop series with data-impaired challenges, restricting participants to limited training samples and no transfer learning.

Result: Successful entries used large model ensembles (Transformers and CNNs), heavy data augmentation, and prior knowledge-based methods.

Conclusion: Incorporating prior knowledge and innovative techniques can improve data efficiency in Deep Learning, as demonstrated by the VIPriors challenges.

Abstract: Deep Learning requires large amounts of data to train models that work well.
In data-deficient settings, performance can be degraded. We investigate which
Deep Learning methods benefit training models in a data-deficient setting, by
organizing the "VIPriors: Visual Inductive Priors for Data-Efficient Deep
Learning" workshop series, featuring four editions of data-impaired challenges.
These challenges address the problem of training deep learning models for
computer vision tasks with limited data. Participants are limited to training
models from scratch using a low number of training samples and are not allowed
to use any form of transfer learning. We aim to stimulate the development of
novel approaches that incorporate prior knowledge to improve the data
efficiency of deep learning models. Successful challenge entries make use of
large model ensembles that mix Transformers and CNNs, as well as heavy data
augmentation. Novel prior knowledge-based methods contribute to success in some
entries.

</details>


### [121] [SAMSelect: A Spectral Index Search for Marine Debris Visualization using Segment Anything](https://arxiv.org/abs/2506.08613)
*Joost van Dalen,Yuki M. Asano,Marc Russwurm*

Main category: cs.CV

TL;DR: SAMSelect is an algorithm for selecting the best three-channel visualization of multispectral images to aid in visually interpreting marine debris, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Marine debris is hard to visualize due to heterogeneity in medium-resolution imagery, and current methods rely on case-by-case band selection by experts.

Method: SAMSelect uses the Segment Anything Model to choose the band or index combination with the highest classification accuracy on annotated data.

Result: Tested on Sentinel-2 scenes, SAMSelect identified new band combinations (e.g., B8, B2) that outperformed literature-based indices.

Conclusion: SAMSelect offers a robust tool for marine scientists, with open-source code provided for broader application.

Abstract: This work proposes SAMSelect, an algorithm to obtain a salient three-channel
visualization for multispectral images. We develop SAMSelect and show its use
for marine scientists visually interpreting floating marine debris in
Sentinel-2 imagery. These debris are notoriously difficult to visualize due to
their compositional heterogeneity in medium-resolution imagery. Out of these
difficulties, a visual interpretation of imagery showing marine debris remains
a common practice by domain experts, who select bands and spectral indices on a
case-by-case basis informed by common practices and heuristics. SAMSelect
selects the band or index combination that achieves the best classification
accuracy on a small annotated dataset through the Segment Anything Model. Its
central assumption is that the three-channel visualization achieves the most
accurate segmentation results also provide good visual information for
photo-interpretation.
  We evaluate SAMSelect in three Sentinel-2 scenes containing generic marine
debris in Accra, Ghana, and Durban, South Africa, and deployed plastic targets
from the Plastic Litter Project. This reveals the potential of new previously
unused band combinations (e.g., a normalized difference index of B8, B2), which
demonstrate improved performance compared to literature-based indices. We
describe the algorithm in this paper and provide an open-source code repository
that will be helpful for domain scientists doing visual photo interpretation,
especially in the marine field.

</details>


### [122] [A Probability-guided Sampler for Neural Implicit Surface Rendering](https://arxiv.org/abs/2506.08619)
*Gonalo Dias Pais,Valter Piedade,Moitreya Chatterjee,Marcus Greiff,Pedro Miraldo*

Main category: cs.CV

TL;DR: The paper introduces a targeted sampling strategy and a new surface reconstruction loss for Neural Radiance Fields (NeRFs) to improve rendering accuracy and 3D reconstruction.


<details>
  <summary>Details</summary>
Motivation: Existing NeRF variants struggle with scalability and uniform sampling inefficiencies, limiting rendering quality.

Method: Leverages implicit surface representation and models a probability density function for targeted ray sampling, along with a novel surface reconstruction loss.

Result: Achieves more accurate 3D reconstructions and improved image rendering, especially in regions of interest.

Conclusion: The proposed method enhances NeRF performance by addressing sampling inefficiencies and introducing a tailored loss function.

Abstract: Several variants of Neural Radiance Fields (NeRFs) have significantly
improved the accuracy of synthesized images and surface reconstruction of 3D
scenes/objects. In all of these methods, a key characteristic is that none can
train the neural network with every possible input data, specifically, every
pixel and potential 3D point along the projection rays due to scalability
issues. While vanilla NeRFs uniformly sample both the image pixels and 3D
points along the projection rays, some variants focus only on guiding the
sampling of the 3D points along the projection rays. In this paper, we leverage
the implicit surface representation of the foreground scene and model a
probability density function in a 3D image projection space to achieve a more
targeted sampling of the rays toward regions of interest, resulting in improved
rendering. Additionally, a new surface reconstruction loss is proposed for
improved performance. This new loss fully explores the proposed 3D image
projection space model and incorporates near-to-surface and empty space
components. By integrating our novel sampling strategy and novel loss into
current state-of-the-art neural implicit surface renderers, we achieve more
accurate and detailed 3D reconstructions and improved image rendering,
especially for the regions of interest in any given scene.

</details>


### [123] [ECMNet:Lightweight Semantic Segmentation with Efficient CNN-Mamba Network](https://arxiv.org/abs/2506.08629)
*Feixiang Du,Shengkun Wu*

Main category: cs.CV

TL;DR: The paper proposes ECMNet, a lightweight CNN-Mamba hybrid network for semantic segmentation, combining CNNs and Mamba for better global context modeling. It introduces EDAB, MSAU, and FFM modules to enhance feature representation and fusion, achieving high accuracy with low computational cost.


<details>
  <summary>Details</summary>
Motivation: Existing CNN-Transformer models for semantic segmentation lack adequate global context modeling. Mamba's potential in vision tasks for long-range dependency modeling motivates its integration with CNNs.

Method: ECMNet combines CNNs and Mamba in a capsule-based framework. It includes EDAB for lightweight bottlenecks, MSAU for multi-scale feature aggregation, and FFM for enhanced feature fusion.

Result: The model achieves 70.6% mIoU on Cityscapes and 73.6% mIoU on CamVid, with 0.87M parameters and 8.27G FLOPs.

Conclusion: ECMNet effectively balances accuracy and efficiency, demonstrating the benefits of integrating CNNs and Mamba for semantic segmentation.

Abstract: In the past decade, Convolutional Neural Networks (CNNs) and Transformers
have achieved wide applicaiton in semantic segmentation tasks. Although CNNs
with Transformer models greatly improve performance, the global context
modeling remains inadequate. Recently, Mamba achieved great potential in vision
tasks, showing its advantages in modeling long-range dependency. In this paper,
we propose a lightweight Efficient CNN-Mamba Network for semantic segmentation,
dubbed as ECMNet. ECMNet combines CNN with Mamba skillfully in a capsule-based
framework to address their complementary weaknesses. Specifically, We design a
Enhanced Dual-Attention Block (EDAB) for lightweight bottleneck. In order to
improve the representations ability of feature, We devise a Multi-Scale
Attention Unit (MSAU) to integrate multi-scale feature aggregation, spatial
aggregation and channel aggregation. Moreover, a Mamba enhanced Feature Fusion
Module (FFM) merges diverse level feature, significantly enhancing segmented
accuracy. Extensive experiments on two representative datasets demonstrate that
the proposed model excels in accuracy and efficiency balance, achieving 70.6%
mIoU on Cityscapes and 73.6% mIoU on CamVid test datasets, with 0.87M
parameters and 8.27G FLOPs on a single RTX 3090 GPU platform.

</details>


### [124] [RoboSwap: A GAN-driven Video Diffusion Framework For Unsupervised Robot Arm Swapping](https://arxiv.org/abs/2506.08632)
*Yang Bai,Liudi Yang,George Eskandar,Fengyi Shen,Dong Chen,Mohammad Altillawi,Ziyuan Liu,Gitta Kutyniok*

Main category: cs.CV

TL;DR: RoboSwap introduces a novel video editing pipeline using GANs and diffusion models to swap robotic arms in videos, enabling cross-embodiment learning without paired data.


<details>
  <summary>Details</summary>
Motivation: The scarcity of diverse, high-quality datasets limits video-conditioned robotic learning and cross-platform generalization.

Method: RoboSwap segments robotic arms, uses an unpaired GAN for translation, blends with the original background, and refines with a diffusion model for coherence and realism.

Result: RoboSwap outperforms state-of-the-art models on three benchmarks in structural coherence and motion consistency.

Conclusion: RoboSwap provides a robust solution for generating reliable, cross-embodiment data in robotic learning.

Abstract: Recent advancements in generative models have revolutionized video synthesis
and editing. However, the scarcity of diverse, high-quality datasets continues
to hinder video-conditioned robotic learning, limiting cross-platform
generalization. In this work, we address the challenge of swapping a robotic
arm in one video with another: a key step for crossembodiment learning. Unlike
previous methods that depend on paired video demonstrations in the same
environmental settings, our proposed framework, RoboSwap, operates on unpaired
data from diverse environments, alleviating the data collection needs. RoboSwap
introduces a novel video editing pipeline integrating both GANs and diffusion
models, combining their isolated advantages. Specifically, we segment robotic
arms from their backgrounds and train an unpaired GAN model to translate one
robotic arm to another. The translated arm is blended with the original video
background and refined with a diffusion model to enhance coherence, motion
realism and object interaction. The GAN and diffusion stages are trained
independently. Our experiments demonstrate that RoboSwap outperforms
state-of-the-art video and image editing models on three benchmarks in terms of
both structural coherence and motion consistency, thereby offering a robust
solution for generating reliable, cross-embodiment data in robotic learning.

</details>


### [125] [SurfR: Surface Reconstruction with Multi-scale Attention](https://arxiv.org/abs/2506.08635)
*Siddhant Ranade,Gonalo Dias Pais,Ross Tyler Whitaker,Jacinto C. Nascimento,Pedro Miraldo,Srikumar Ramalingam*

Main category: cs.CV

TL;DR: A fast and accurate surface reconstruction algorithm for unorganized point clouds using an implicit representation, balancing speed and detail.


<details>
  <summary>Details</summary>
Motivation: Address the trade-off between detail and speed in learning-based surface reconstruction methods.

Method: Uses lazy query for speed, parallel multi-scale grids for robustness, and attention across scales for improved reconstruction.

Result: Achieves the best accuracy-speed trade-off, outperforming baselines in speed with minimal performance loss.

Conclusion: Proposes a novel implicit representation for 3D shapes, combining efficiency and detail.

Abstract: We propose a fast and accurate surface reconstruction algorithm for
unorganized point clouds using an implicit representation. Recent learning
methods are either single-object representations with small neural models that
allow for high surface details but require per-object training or generalized
representations that require larger models and generalize to newer shapes but
lack details, and inference is slow. We propose a new implicit representation
for general 3D shapes that is faster than all the baselines at their optimum
resolution, with only a marginal loss in performance compared to the
state-of-the-art. We achieve the best accuracy-speed trade-off using three key
contributions. Many implicit methods extract features from the point cloud to
classify whether a query point is inside or outside the object. First, to speed
up the reconstruction, we show that this feature extraction does not need to
use the query point at an early stage (lazy query). Second, we use a parallel
multi-scale grid representation to develop robust features for different noise
levels and input resolutions. Finally, we show that attention across scales can
provide improved reconstruction results.

</details>


### [126] [Orientation Matters: Making 3D Generative Models Orientation-Aligned](https://arxiv.org/abs/2506.08640)
*Yichong Lu,Yuzhuo Tian,Zijin Jiang,Yikun Zhao,Yuanbo Yang,Hao Ouyang,Haoji Hu,Huimin Yu,Yujun Shen,Yiyi Liao*

Main category: cs.CV

TL;DR: The paper introduces orientation-aligned 3D object generation from single images, addressing misalignment issues in existing models. It presents Objaverse-OA, a dataset for training, and fine-tunes models to improve alignment and generalization.


<details>
  <summary>Details</summary>
Motivation: Existing 3D generative models often produce misaligned results due to inconsistent training data, limiting their usability. The paper aims to solve this by ensuring consistent object orientations across categories.

Method: The authors construct Objaverse-OA, a dataset of 14,832 orientation-aligned 3D models. They fine-tune two 3D generative models (multi-view diffusion and 3D variational autoencoder) using this dataset.

Result: The fine-tuned models produce aligned objects that generalize well to unseen categories, outperforming post-hoc alignment approaches.

Conclusion: The method enables downstream applications like zero-shot orientation estimation and efficient object rotation manipulation, demonstrating its practical utility.

Abstract: Humans intuitively perceive object shape and orientation from a single image,
guided by strong priors about canonical poses. However, existing 3D generative
models often produce misaligned results due to inconsistent training data,
limiting their usability in downstream tasks. To address this gap, we introduce
the task of orientation-aligned 3D object generation: producing 3D objects from
single images with consistent orientations across categories. To facilitate
this, we construct Objaverse-OA, a dataset of 14,832 orientation-aligned 3D
models spanning 1,008 categories. Leveraging Objaverse-OA, we fine-tune two
representative 3D generative models based on multi-view diffusion and 3D
variational autoencoder frameworks to produce aligned objects that generalize
well to unseen objects across various categories. Experimental results
demonstrate the superiority of our method over post-hoc alignment approaches.
Furthermore, we showcase downstream applications enabled by our aligned object
generation, including zero-shot object orientation estimation via
analysis-by-synthesis and efficient arrow-based object rotation manipulation.

</details>


### [127] [Enhancing Video Memorability Prediction with Text-Motion Cross-modal Contrastive Loss and Its Application in Video Summarization](https://arxiv.org/abs/2506.08649)
*Zhiyi Zhu,Xiaoyu Wu,Youwei Lu*

Main category: cs.CV

TL;DR: The paper introduces TMCCL, a model to improve video memorability prediction by enhancing motion feature representation using text-motion cross-modal contrastive learning. It also proposes MWCVS for video summarization, leveraging memorability to reduce label subjectivity.


<details>
  <summary>Details</summary>
Motivation: Existing models for video memorability prediction inadequately utilize motion cues due to limited labeled data, compromising feature representation. The paper aims to address this gap and explore applications like video summarization.

Method: The authors propose TMCCL, which uses text description similarities to create positive/negative motion sample sets, improving motion feature learning. They also introduce MWCVS for video summarization, using memorability predictions to refine labels.

Result: TMCCL achieves state-of-the-art performance on two video memorability datasets. MWCVS effectively reduces subjectivity in video summarization labels, as shown on two datasets.

Conclusion: The paper successfully enhances motion feature representation for memorability prediction and demonstrates practical applications in video summarization, highlighting the broader utility of memorability prediction.

Abstract: Video memorability refers to the ability of videos to be recalled after
viewing, playing a crucial role in creating content that remains memorable.
Existing models typically focus on extracting multimodal features to predict
video memorability scores but often fail to fully utilize motion cues. The
representation of motion features is compromised during the fine-tuning phase
of the motion feature extractor due to a lack of labeled data. In this paper,
we introduce the Text-Motion Cross-modal Contrastive Loss (TMCCL), a multimodal
video memorability prediction model designed to enhance the representation of
motion features. We tackle the challenge of improving motion feature
representation by leveraging text description similarities across videos to
establish positive and negative motion sample sets for a given target. This
enhancement allows the model to learn similar feature representations for
semantically related motion content, resulting in more accurate memorability
predictions. Our model achieves state-of-the-art performance on two video
memorability prediction datasets. Moreover, the potential applications of video
memorability prediction have been underexplored. To address this gap, we
present Memorability Weighted Correction for Video Summarization (MWCVS), using
video memorability prediction to reduce subjectivity in video summarization
labels. Experimental results on two video summarization datasets demonstrate
the effectiveness of MWCVS, showcasing the promising applications of video
memorability prediction.

</details>


### [128] [Beyond Calibration: Physically Informed Learning for Raw-to-Raw Mapping](https://arxiv.org/abs/2506.08650)
*Peter Grnquist,Stepan Tulyakov,Dengxin Dai*

Main category: cs.CV

TL;DR: The paper introduces the Neural Physical Model (NPM) for consistent color reproduction across cameras, addressing limitations of existing methods with adaptability to illumination and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Ensuring consistent color reproduction across cameras is challenging due to sensor and optic variations, and existing methods lack adaptability or practicality.

Method: The NPM is a lightweight, physically-informed model that simulates raw images under specified illumination to estimate transformations between devices, supporting training with or without paired data.

Result: NPM outperforms state-of-the-art methods on datasets like NUS and BeyondRGB, achieving robust chromatic consistency across sensors and optical systems.

Conclusion: NPM provides a practical and effective solution for color consistency in multi-camera systems, adaptable to varying conditions and initialization options.

Abstract: Achieving consistent color reproduction across multiple cameras is essential
for seamless image fusion and Image Processing Pipeline (ISP) compatibility in
modern devices, but it is a challenging task due to variations in sensors and
optics. Existing raw-to-raw conversion methods face limitations such as poor
adaptability to changing illumination, high computational costs, or impractical
requirements such as simultaneous camera operation and overlapping
fields-of-view. We introduce the Neural Physical Model (NPM), a lightweight,
physically-informed approach that simulates raw images under specified
illumination to estimate transformations between devices. The NPM effectively
adapts to varying illumination conditions, can be initialized with physical
measurements, and supports training with or without paired data. Experiments on
public datasets like NUS and BeyondRGB demonstrate that NPM outperforms recent
state-of-the-art methods, providing robust chromatic consistency across
different sensors and optical systems.

</details>


### [129] [LLaVA-c: Continual Improved Visual Instruction Tuning](https://arxiv.org/abs/2506.08666)
*Wenzhuo Liu,Fei Zhu,Haiyang Guo,Longhui Wei,Cheng-Lin Liu*

Main category: cs.CV

TL;DR: LLaVA-c improves LLaVA-1.5 with spectral-aware consolidation and unsupervised inquiry regularization, enhancing continual learning without degrading general capabilities.


<details>
  <summary>Details</summary>
Motivation: Address challenges in multitask learning (task balancing, expansion costs) and prevent base model degradation in continual learning.

Method: Modifies LLaVA-1.5 with spectral-aware consolidation for task balance and unsupervised inquiry regularization to avoid overfitting.

Result: LLaVA-c matches or surpasses multitask joint learning, preserving general capabilities while improving task-specific performance.

Conclusion: Continual learning with LLaVA-c is a viable alternative to multitask learning, offering scalability without sacrificing performance.

Abstract: Multimodal models like LLaVA-1.5 achieve state-of-the-art visual
understanding through visual instruction tuning on multitask datasets, enabling
strong instruction-following and multimodal performance. However, multitask
learning faces challenges such as task balancing, requiring careful adjustment
of data proportions, and expansion costs, where new tasks risk catastrophic
forgetting and need costly retraining. Continual learning provides a promising
alternative to acquiring new knowledge incrementally while preserving existing
capabilities. However, current methods prioritize task-specific performance,
neglecting base model degradation from overfitting to specific instructions,
which undermines general capabilities. In this work, we propose a simple but
effective method with two modifications on LLaVA-1.5: spectral-aware
consolidation for improved task balance and unsupervised inquiry regularization
to prevent base model degradation. We evaluate both general and task-specific
performance across continual pretraining and fine-tuning. Experiments
demonstrate that LLaVA-c consistently enhances standard benchmark performance
and preserves general capabilities. For the first time, we show that
task-by-task continual learning can achieve results that match or surpass
multitask joint learning. The code will be publicly released.

</details>


### [130] [ATAS: Any-to-Any Self-Distillation for Enhanced Open-Vocabulary Dense Prediction](https://arxiv.org/abs/2506.08678)
*Juan Yeo,Soonwoo Cha,Jiwoo Song,Hyunbin Jin,Taesup Kim*

Main category: cs.CV

TL;DR: The paper introduces Any-to-Any Self-Distillation (ATAS) to improve CLIP's fine-grained vision-language alignment and semantic coherence without extra modules or supervision, achieving better performance in dense prediction tasks.


<details>
  <summary>Details</summary>
Motivation: CLIP struggles with fine-grained, region-level understanding in dense prediction tasks, limiting its effectiveness. Current methods compromise semantic coherence or require extra resources.

Method: Proposes ATAS, a self-distillation approach using unlabeled images to refine CLIP's vision encoder representations, enhancing both semantic coherence and fine-grained alignment.

Result: ATAS outperforms baseline CLIP models in open-vocabulary object detection and semantic segmentation benchmarks.

Conclusion: ATAS effectively balances semantic coherence and fine-grained alignment, advancing open-vocabulary dense prediction without additional supervision.

Abstract: Vision-language models such as CLIP have recently propelled open-vocabulary
dense prediction tasks by enabling recognition of a broad range of visual
concepts. However, CLIP still struggles with fine-grained, region-level
understanding, hindering its effectiveness on these dense prediction tasks. We
identify two pivotal factors required to address this limitation: semantic
coherence and fine-grained vision-language alignment. Current adaptation
methods often improve fine-grained alignment at the expense of semantic
coherence, and often rely on extra modules or supervised fine-tuning. To
overcome these issues, we propose Any-to-Any Self-Distillation (ATAS), a novel
approach that simultaneously enhances semantic coherence and fine-grained
alignment by leveraging own knowledge of a model across all representation
levels. Unlike prior methods, ATAS uses only unlabeled images and an internal
self-distillation process to refine representations of CLIP vision encoders,
preserving local semantic consistency while sharpening local detail
recognition. On open-vocabulary object detection and semantic segmentation
benchmarks, ATAS achieves substantial performance gains, outperforming baseline
CLIP models. These results validate the effectiveness of our approach and
underscore the importance of jointly maintaining semantic coherence and
fine-grained alignment for advanced open-vocabulary dense prediction.

</details>


### [131] [CanadaFireSat: Toward high-resolution wildfire forecasting with multiple modalities](https://arxiv.org/abs/2506.08690)
*Hugo Porta,Emanuele Dalsasso,Jessica L. McCarty,Devis Tuia*

Main category: cs.CV

TL;DR: The paper introduces CanadaFireSat, a benchmark dataset for high-resolution wildfire forecasting in Canada, using multi-modal data and deep learning to improve prediction accuracy.


<details>
  <summary>Details</summary>
Motivation: The severe 2023 wildfire season in Canada highlights the need for better wildfire mitigation tools due to climate change. Existing methods rely on coarse-resolution data, limiting prediction quality.

Method: The study leverages multi-modal data (Sentinel-2, MODIS, ERA5) and deep learning architectures to create high-resolution (100 m) wildfire probability maps.

Result: Multi-modal inputs outperformed single-modal ones, achieving a 60.3% F1 score for the 2023 wildfire season, despite not being trained on it.

Conclusion: Multi-modal deep learning models show promise for high-resolution, continental-scale wildfire forecasting.

Abstract: Canada experienced in 2023 one of the most severe wildfire seasons in recent
history, causing damage across ecosystems, destroying communities, and emitting
large quantities of CO2. This extreme wildfire season is symptomatic of a
climate-change-induced increase in the length and severity of the fire season
that affects the boreal ecosystem. Therefore, it is critical to empower
wildfire management in boreal communities with better mitigation solutions.
Wildfire probability maps represent an important tool for understanding the
likelihood of wildfire occurrence and the potential severity of future
wildfires. The massive increase in the availability of Earth observation data
has enabled the development of deep learning-based wildfire forecasting models,
aiming at providing precise wildfire probability maps at different spatial and
temporal scales. A main limitation of such methods is their reliance on
coarse-resolution environmental drivers and satellite products, leading to
wildfire occurrence prediction of reduced resolution, typically around $\sim
0.1${\deg}. This paper presents a benchmark dataset: CanadaFireSat, and
baseline methods for high-resolution: 100 m wildfire forecasting across Canada,
leveraging multi-modal data from high-resolution multi-spectral satellite
images (Sentinel-2 L1C), mid-resolution satellite products (MODIS), and
environmental factors (ERA5 reanalysis data). Our experiments consider two
major deep learning architectures. We observe that using multi-modal temporal
inputs outperforms single-modal temporal inputs across all metrics, achieving a
peak performance of 60.3% in F1 score for the 2023 wildfire season, a season
never seen during model training. This demonstrates the potential of
multi-modal deep learning models for wildfire forecasting at high-resolution
and continental scale.

</details>


### [132] [VReST: Enhancing Reasoning in Large Vision-Language Models through Tree Search and Self-Reward Mechanism](https://arxiv.org/abs/2506.08691)
*Congzhi Zhang,Jiawei Peng,Zhenglin Wang,Yilong Lai,Haowen Sun,Heng Chang,Fei Ma,Weijiang Yu*

Main category: cs.CV

TL;DR: VReST enhances reasoning in LVLMs using Monte Carlo Tree Search and Self-Reward, outperforming current methods in multimodal benchmarks.


<details>
  <summary>Details</summary>
Motivation: LVLMs struggle with complex visual reasoning despite their success in multimodal tasks, prompting the need for improved reasoning techniques.

Method: VReST employs Monte Carlo Tree Search and a Self-Reward mechanism to evaluate reasoning steps without additional models.

Result: VReST achieves state-of-the-art performance on three multimodal mathematical reasoning benchmarks.

Conclusion: VReST demonstrates the potential of test-time scaling laws in multimodal tasks, paving the way for future research.

Abstract: Large Vision-Language Models (LVLMs) have shown exceptional performance in
multimodal tasks, but their effectiveness in complex visual reasoning is still
constrained, especially when employing Chain-of-Thought prompting techniques.
In this paper, we propose VReST, a novel training-free approach that enhances
Reasoning in LVLMs through Monte Carlo Tree Search and Self-Reward mechanisms.
VReST meticulously traverses the reasoning landscape by establishing a search
tree, where each node encapsulates a reasoning step, and each path delineates a
comprehensive reasoning sequence. Our innovative multimodal Self-Reward
mechanism assesses the quality of reasoning steps by integrating the utility of
sub-questions, answer correctness, and the relevance of vision-language clues,
all without the need for additional models. VReST surpasses current prompting
methods and secures state-of-the-art performance across three multimodal
mathematical reasoning benchmarks. Furthermore, it substantiates the efficacy
of test-time scaling laws in multimodal tasks, offering a promising direction
for future research.

</details>


### [133] [MoSiC: Optimal-Transport Motion Trajectory for Dense Self-Supervised Learning](https://arxiv.org/abs/2506.08694)
*Mohammadreza Salehi,Shashanka Venkataramanan,Ioana Simion,Efstratios Gavves,Cees G. M. Snoek,Yuki M Asano*

Main category: cs.CV

TL;DR: A motion-guided self-supervised learning framework for videos improves feature consistency and robustness by clustering dense point tracks, outperforming existing methods by 1-6% on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing self-supervised learning methods for videos struggle with motion dynamics, object deformations, and occlusions, leading to inconsistent feature learning.

Method: The framework uses an off-the-shelf point tracker to extract motion trajectories and optimizes feature clustering via a momentum-encoder-based optimal transport mechanism, ensuring temporal coherence.

Result: The method improves state-of-the-art performance by 1-6% on six datasets and four benchmarks.

Conclusion: Motion-guided self-supervised learning enhances spatiotemporal feature consistency, generalizing better across frames and dynamic scenes.

Abstract: Dense self-supervised learning has shown great promise for learning pixel-
and patch-level representations, but extending it to videos remains challenging
due to the complexity of motion dynamics. Existing approaches struggle as they
rely on static augmentations that fail under object deformations, occlusions,
and camera movement, leading to inconsistent feature learning over time. We
propose a motion-guided self-supervised learning framework that clusters dense
point tracks to learn spatiotemporally consistent representations. By
leveraging an off-the-shelf point tracker, we extract long-range motion
trajectories and optimize feature clustering through a momentum-encoder-based
optimal transport mechanism. To ensure temporal coherence, we propagate cluster
assignments along tracked points, enforcing feature consistency across views
despite viewpoint changes. Integrating motion as an implicit supervisory
signal, our method learns representations that generalize across frames,
improving robustness in dynamic scenes and challenging occlusion scenarios. By
initializing from strong image-pretrained models and leveraging video data for
training, we improve state-of-the-art by 1% to 6% on six image and video
datasets and four evaluation benchmarks. The implementation is publicly
available at our GitHub repository: https://github.com/SMSD75/MoSiC/tree/main

</details>


### [134] [ArrowPose: Segmentation, Detection, and 5 DoF Pose Estimation Network for Colorless Point Clouds](https://arxiv.org/abs/2506.08699)
*Frederik Hagelskjaer*

Main category: cs.CV

TL;DR: A neural network for fast 5 DoF pose estimation in colorless point clouds, achieving state-of-the-art performance and real-time inference.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of accurate and efficient pose estimation in colorless point clouds, which lacks color information.

Method: Uses a neural network trained on synthetic data to predict object poses from center and top points.

Result: Outperforms all colorless methods on a benchmark dataset with inference in 250ms.

Conclusion: The method is practical for real-time applications, with code available at arrowpose.github.io.

Abstract: This paper presents a fast detection and 5 DoF (Degrees of Freedom) pose
estimation network for colorless point clouds. The pose estimation is
calculated from center and top points of the object, predicted by the neural
network. The network is trained on synthetic data, and tested on a benchmark
dataset, where it demonstrates state-of-the-art performance and outperforms all
colorless methods. The network is able to run inference in only 250
milliseconds making it usable in many scenarios. Project page with code at
arrowpose.github.io

</details>


### [135] [TraGraph-GS: Trajectory Graph-based Gaussian Splatting for Arbitrary Large-Scale Scene Rendering](https://arxiv.org/abs/2506.08704)
*Xiaohan Zhang,Sitong Wang,Yushen Yan,Yi Yang,Mingda Xu,Qi Liu*

Main category: cs.CV

TL;DR: TraGraph-GS improves novel view synthesis for large-scale scenes using a trajectory graph and spatial partitioning, outperforming existing methods in PSNR.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with arbitrary camera trajectories and Gaussian overlap, limiting generalization in large-scale scenes.

Method: Proposes TraGraph-GS with a graph-based spatial partitioning method, regularization constraints, and progressive rendering to address Gaussian overlap.

Result: Achieves 1.86 dB PSNR improvement on aerial datasets and 1.62 dB on ground datasets compared to state-of-the-art methods.

Conclusion: TraGraph-GS effectively addresses limitations of existing methods, enhancing rendering quality and efficiency for large-scale scenes.

Abstract: High-quality novel view synthesis for large-scale scenes presents a
challenging dilemma in 3D computer vision. Existing methods typically partition
large scenes into multiple regions, reconstruct a 3D representation using
Gaussian splatting for each region, and eventually merge them for novel view
rendering. They can accurately render specific scenes, yet they do not
generalize effectively for two reasons: (1) rigid spatial partition techniques
struggle with arbitrary camera trajectories, and (2) the merging of regions
results in Gaussian overlap to distort texture details. To address these
challenges, we propose TraGraph-GS, leveraging a trajectory graph to enable
high-precision rendering for arbitrarily large-scale scenes. We present a
spatial partitioning method for large-scale scenes based on graphs, which
incorporates a regularization constraint to enhance the rendering of textures
and distant objects, as well as a progressive rendering strategy to mitigate
artifacts caused by Gaussian overlap. Experimental results demonstrate its
superior performance both on four aerial and four ground datasets and highlight
its remarkable efficiency: our method achieves an average improvement of 1.86
dB in PSNR on aerial datasets and 1.62 dB on ground datasets compared to
state-of-the-art approaches.

</details>


### [136] [SceneSplat++: A Large Dataset and Comprehensive Benchmark for Language Gaussian Splatting](https://arxiv.org/abs/2506.08710)
*Mengjiao Ma,Qi Ma,Yue Li,Jiahuan Cheng,Runyi Yang,Bin Ren,Nikola Popovic,Mingqiang Wei,Nicu Sebe,Luc Van Gool,Theo Gevers,Martin R. Oswald,Danda Pani Paudel*

Main category: cs.CV

TL;DR: The paper introduces a large-scale benchmark for evaluating 3D Gaussian Splatting (3DGS) methods in 3D space, highlighting the superiority of generalizable approaches and releasing a new dataset, GaussianWorld-49K.


<details>
  <summary>Details</summary>
Motivation: Current 3DGS methods are limited by evaluations on 2D views and few scenes, hindering holistic 3D understanding. The paper aims to address this gap.

Method: Proposes a benchmark assessing three 3DGS method groups (per-scene optimization-based, optimization-free, generalizable) on 1060 scenes across indoor and outdoor datasets. Introduces GaussianWorld-49K dataset.

Result: Generalizable methods outperform others, enabling fast inference on novel scenes and better segmentation. The dataset enhances data priors for these methods.

Conclusion: The benchmark and dataset advance generalizable 3DGS research, with public release to accelerate progress in 3D scene understanding.

Abstract: 3D Gaussian Splatting (3DGS) serves as a highly performant and efficient
encoding of scene geometry, appearance, and semantics. Moreover, grounding
language in 3D scenes has proven to be an effective strategy for 3D scene
understanding. Current Language Gaussian Splatting line of work fall into three
main groups: (i) per-scene optimization-based, (ii) per-scene
optimization-free, and (iii) generalizable approach. However, most of them are
evaluated only on rendered 2D views of a handful of scenes and viewpoints close
to the training views, limiting ability and insight into holistic 3D
understanding. To address this gap, we propose the first large-scale benchmark
that systematically assesses these three groups of methods directly in 3D
space, evaluating on 1060 scenes across three indoor datasets and one outdoor
dataset. Benchmark results demonstrate a clear advantage of the generalizable
paradigm, particularly in relaxing the scene-specific limitation, enabling fast
feed-forward inference on novel scenes, and achieving superior segmentation
performance. We further introduce GaussianWorld-49K a carefully curated 3DGS
dataset comprising around 49K diverse indoor and outdoor scenes obtained from
multiple sources, with which we demonstrate the generalizable approach could
harness strong data priors. Our codes, benchmark, and datasets will be made
public to accelerate research in generalizable 3DGS scene understanding.

</details>


### [137] [Geometric deep learning for local growth prediction on abdominal aortic aneurysm surfaces](https://arxiv.org/abs/2506.08729)
*Dieuwertje Alblas,Patryk Rygiel,Julian Suk,Kaj O. Kappe,Marieke Hofman,Christoph Brune,Kak Khee Yeung,Jelmer M. Wolterink*

Main category: cs.CV

TL;DR: A transformer model predicts AAA growth using 3D vascular surfaces, improving personalized monitoring with high accuracy.


<details>
  <summary>Details</summary>
Motivation: Current AAA monitoring relies on diameter thresholds, ignoring 3D shape complexities, leading to potentially unfit surveillance intervals.

Method: An SE(3)-symmetric transformer model predicts AAA growth using local, multi-physical features on vascular surfaces, trained on 113 CT scans from 24 patients.

Result: Median diameter error of 1.18 mm in growth prediction; 93% accuracy in identifying eligibility for elective repair within two years.

Conclusion: The model enables feasible, personalized AAA growth prediction, enhancing surveillance strategies.

Abstract: Abdominal aortic aneurysms (AAAs) are progressive focal dilatations of the
abdominal aorta. AAAs may rupture, with a survival rate of only 20\%. Current
clinical guidelines recommend elective surgical repair when the maximum AAA
diameter exceeds 55 mm in men or 50 mm in women. Patients that do not meet
these criteria are periodically monitored, with surveillance intervals based on
the maximum AAA diameter. However, this diameter does not take into account the
complex relation between the 3D AAA shape and its growth, making standardized
intervals potentially unfit. Personalized AAA growth predictions could improve
monitoring strategies. We propose to use an SE(3)-symmetric transformer model
to predict AAA growth directly on the vascular model surface enriched with
local, multi-physical features. In contrast to other works which have
parameterized the AAA shape, this representation preserves the vascular
surface's anatomical structure and geometric fidelity. We train our model using
a longitudinal dataset of 113 computed tomography angiography (CTA) scans of 24
AAA patients at irregularly sampled intervals. After training, our model
predicts AAA growth to the next scan moment with a median diameter error of
1.18 mm. We further demonstrate our model's utility to identify whether a
patient will become eligible for elective repair within two years (acc = 0.93).
Finally, we evaluate our model's generalization on an external validation set
consisting of 25 CTAs from 7 AAA patients from a different hospital. Our
results show that local directional AAA growth prediction from the vascular
surface is feasible and may contribute to personalized surveillance strategies.

</details>


### [138] [InceptionMamba: An Efficient Hybrid Network with Large Band Convolution and Bottleneck Mamba](https://arxiv.org/abs/2506.08735)
*Yuhang Wang,Jun Li,Zhijian Wu,Jianhua Xu*

Main category: cs.CV

TL;DR: InceptionMamba improves spatial and global context modeling over InceptionNeXt by replacing strip convolutions with orthogonal band convolutions and adding a bottleneck Mamba module.


<details>
  <summary>Details</summary>
Motivation: InceptionNeXt's limitations in capturing spatial dependencies and global context modeling motivated the development of InceptionMamba.

Method: InceptionMamba uses orthogonal band convolutions for cohesive spatial modeling and a bottleneck Mamba module for global context.

Result: InceptionMamba achieves state-of-the-art performance in classification and downstream tasks with superior efficiency.

Conclusion: InceptionMamba outperforms InceptionNeXt by addressing its spatial and global modeling limitations.

Abstract: Within the family of convolutional neural networks, InceptionNeXt has shown
excellent competitiveness in image classification and a number of downstream
tasks. Built on parallel one-dimensional strip convolutions, however, it
suffers from limited ability of capturing spatial dependencies along different
dimensions and fails to fully explore spatial modeling in local neighborhood.
Besides, inherent locality constraints of convolution operations are
detrimental to effective global context modeling. To overcome these
limitations, we propose a novel backbone architecture termed InceptionMamba in
this study. More specifically, the traditional one-dimensional strip
convolutions are replaced by orthogonal band convolutions in our InceptionMamba
to achieve cohesive spatial modeling. Furthermore, global contextual modeling
can be achieved via a bottleneck Mamba module, facilitating enhanced
cross-channel information fusion and enlarged receptive field. Extensive
evaluations on classification and various downstream tasks demonstrate that the
proposed InceptionMamba achieves state-of-the-art performance with superior
parameter and computational efficiency. The source code will be available at
https://github.com/Wake1021/InceptionMamba.

</details>


### [139] [RS-MTDF: Multi-Teacher Distillation and Fusion for Remote Sensing Semi-Supervised Semantic Segmentation](https://arxiv.org/abs/2506.08772)
*Jiayi Song,Kaiyu Li,Xiangyong Cao,Deyu Meng*

Main category: cs.CV

TL;DR: The paper proposes RS-MTDF, a semi-supervised semantic segmentation framework for remote sensing, leveraging Vision Foundation Models (VFMs) to bridge distribution gaps and improve generalization.


<details>
  <summary>Details</summary>
Motivation: Semantic segmentation in remote sensing requires costly annotations. Semi-supervised methods struggle with distribution mismatches between labeled and unlabeled data.

Method: RS-MTDF uses multiple frozen VFMs (e.g., DINOv2, CLIP) as teachers for feature-level distillation and fuses their knowledge into the student decoder.

Result: RS-MTDF achieves state-of-the-art performance on ISPRS Potsdam, LoveDA, and DeepGlobe datasets, outperforming existing methods in various label ratios and semantic categories.

Conclusion: Multi-teacher VFM guidance enhances generalization and semantic understanding in remote sensing segmentation, validated by ablation studies.

Abstract: Semantic segmentation in remote sensing images is crucial for various
applications, yet its performance is heavily reliant on large-scale,
high-quality pixel-wise annotations, which are notoriously expensive and
time-consuming to acquire. Semi-supervised semantic segmentation (SSS) offers a
promising alternative to mitigate this data dependency. However, existing SSS
methods often struggle with the inherent distribution mismatch between limited
labeled data and abundant unlabeled data, leading to suboptimal generalization.
We propose that Vision Foundation Models (VFMs), pre-trained on vast and
diverse datasets, possess robust generalization capabilities that can
effectively bridge this distribution gap and provide strong semantic priors for
SSS. Inspired by this, we introduce RS-MTDF (Multi-Teacher Distillation and
Fusion), a novel framework that leverages the powerful semantic knowledge
embedded in VFMs to guide semi-supervised learning in remote sensing.
Specifically, RS-MTDF employs multiple frozen VFMs (\textit{e.g.}, DINOv2 and
CLIP) as expert teachers, utilizing feature-level distillation to align student
features with their robust representations. To further enhance discriminative
power, the distilled knowledge is seamlessly fused into the student decoder.
Extensive experiments on three challenging remote sensing datasets (ISPRS
Potsdam, LoveDA, and DeepGlobe) demonstrate that RS-MTDF consistently achieves
state-of-the-art performance. Notably, our method outperforms existing
approaches across various label ratios on LoveDA and secures the highest IoU in
the majority of semantic categories. These results underscore the efficacy of
multi-teacher VFM guidance in significantly enhancing both generalization and
semantic understanding for remote sensing segmentation. Ablation studies
further validate the contribution of each proposed module.

</details>


### [140] [Gaussian2Scene: 3D Scene Representation Learning via Self-supervised Learning with 3D Gaussian Splatting](https://arxiv.org/abs/2506.08777)
*Keyi Liu,Weidong Yang,Ben Fei,Ying He*

Main category: cs.CV

TL;DR: Gaussian2Scene is a novel SSL framework using 3D Gaussian Splatting for efficient and explicit 3D scene pre-training, improving geometric understanding and outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing SSL methods for point cloud pre-training rely on implicit scene representations and 2D reconstruction, limiting geometric understanding and computational efficiency.

Method: Proposes a two-stage training: a dual-branch masked autoencoder for 2D/3D representations, followed by supervised learning with Gaussian primitives and RGB images.

Result: Demonstrates consistent improvements in downstream 3D object detection tasks.

Conclusion: Gaussian2Scene effectively addresses limitations of prior methods, enhancing geometric learning and computational efficiency.

Abstract: Self-supervised learning (SSL) for point cloud pre-training has become a
cornerstone for many 3D vision tasks, enabling effective learning from
large-scale unannotated data. At the scene level, existing SSL methods often
incorporate volume rendering into the pre-training framework, using RGB-D
images as reconstruction signals to facilitate cross-modal learning. This
strategy promotes alignment between 2D and 3D modalities and enables the model
to benefit from rich visual cues in the RGB-D inputs. However, these approaches
are limited by their reliance on implicit scene representations and high memory
demands. Furthermore, since their reconstruction objectives are applied only in
2D space, they often fail to capture underlying 3D geometric structures. To
address these challenges, we propose Gaussian2Scene, a novel scene-level SSL
framework that leverages the efficiency and explicit nature of 3D Gaussian
Splatting (3DGS) for pre-training. The use of 3DGS not only alleviates the
computational burden associated with volume rendering but also supports direct
3D scene reconstruction, thereby enhancing the geometric understanding of the
backbone network. Our approach follows a progressive two-stage training
strategy. In the first stage, a dual-branch masked autoencoder learns both 2D
and 3D scene representations. In the second stage, we initialize training with
reconstructed point clouds and further supervise learning using the geometric
locations of Gaussian primitives and rendered RGB images. This process
reinforces both geometric and cross-modal learning. We demonstrate the
effectiveness of Gaussian2Scene across several downstream 3D object detection
tasks, showing consistent improvements over existing pre-training methods.

</details>


### [141] [Landsat-Bench: Datasets and Benchmarks for Landsat Foundation Models](https://arxiv.org/abs/2506.08780)
*Isaac Corley,Lakshay Sharma,Ruth Crasto*

Main category: cs.CV

TL;DR: Landsat-Bench introduces three benchmarks for Landsat imagery, showing pretrained GFMs outperform ImageNet by +4% OA and +5.1% mAP.


<details>
  <summary>Details</summary>
Motivation: The lack of benchmarks for Landsat data hinders progress in Landsat-based Geospatial Foundation Models (GFMs).

Method: Adapts three remote sensing datasets (EuroSAT-L, BigEarthNet-L, LC100-L) into benchmarks, evaluates pretrained GFMs on SSL4EO-L.

Result: SSL4EO-L pretrained GFMs outperform ImageNet, with gains of +4% OA and +5.1% mAP.

Conclusion: Landsat-Bench provides standardized benchmarks, demonstrating the superiority of SSL4EO-L pretrained GFMs.

Abstract: The Landsat program offers over 50 years of globally consistent Earth
imagery. However, the lack of benchmarks for this data constrains progress
towards Landsat-based Geospatial Foundation Models (GFM). In this paper, we
introduce Landsat-Bench, a suite of three benchmarks with Landsat imagery that
adapt from existing remote sensing datasets -- EuroSAT-L, BigEarthNet-L, and
LC100-L. We establish baseline and standardized evaluation methods across both
common architectures and Landsat foundation models pretrained on the SSL4EO-L
dataset. Notably, we provide evidence that SSL4EO-L pretrained GFMs extract
better representations for downstream tasks in comparison to ImageNet,
including performance gains of +4% OA and +5.1% mAP on EuroSAT-L and
BigEarthNet-L.

</details>


### [142] [HomographyAD: Deep Anomaly Detection Using Self Homography Learning](https://arxiv.org/abs/2506.08784)
*Jongyub Seok,Chanjin Kang*

Main category: cs.CV

TL;DR: HomographyAD is a novel deep anomaly detection method designed for real-world industrial datasets, addressing alignment issues in existing approaches.


<details>
  <summary>Details</summary>
Motivation: Existing anomaly detection methods perform well only on fully-aligned datasets, unlike real-world industrial environments.

Method: Proposes HomographyAD, using ImageNet-pretrained networks, deep homography estimation for input alignment, and self homography learning for fine-tuning.

Result: Shows performance enhancement when applied to various existing anomaly detection approaches.

Conclusion: HomographyAD effectively addresses alignment limitations in industrial anomaly detection, improving performance.

Abstract: Anomaly detection (AD) is a task that distinguishes normal and abnormal data,
which is important for applying automation technologies of the manufacturing
facilities. For MVTec dataset that is a representative AD dataset for
industrial environment, many recent works have shown remarkable performances.
However, the existing anomaly detection works have a limitation of showing good
performance for fully-aligned datasets only, unlike real-world industrial
environments. To solve this limitation, we propose HomographyAD, a novel deep
anomaly detection methodology based on the ImageNet-pretrained network, which
is specially designed for actual industrial dataset. Specifically, we first
suggest input foreground alignment using the deep homography estimation method.
In addition, we fine-tune the model by self homography learning to learn
additional shape information from normal samples. Finally, we conduct anomaly
detection based on the measure of how far the feature of test sample is from
the distribution of the extracted normal features. By applying our proposed
method to various existing AD approaches, we show performance enhancement
through extensive experiments.

</details>


### [143] [A PDE-Based Image Dehazing Method via Atmospheric Scattering Theory](https://arxiv.org/abs/2506.08793)
*Zhuoran Zheng*

Main category: cs.CV

TL;DR: A novel PDE framework for single-image dehazing, combining atmospheric scattering, nonlocal regularization, and dark channel prior, with proven weak solutions and efficient GPU implementation.


<details>
  <summary>Details</summary>
Motivation: To improve single-image dehazing by integrating physical models and regularization techniques into a PDE framework.

Method: Proposes an improved PDE with edge-preserving diffusion, Gaussian convolution, and adaptive regularization, solved via fixed-point iteration on GPU.

Result: Demonstrates promising dehazing performance and potential for generalization to deep models.

Conclusion: The method is effective and scalable, offering a robust solution for image dehazing.

Abstract: This paper presents a novel partial differential equation (PDE) framework for
single-image dehazing. By integrating the atmospheric scattering model with
nonlocal regularization and dark channel prior, we propose the improved PDE: \[
-\text{div}\left(D(\nabla u)\nabla u\right) + \lambda(t) G(u) = \Phi(I,t,A) \]
where $D(\nabla u) = (|\nabla u| + \epsilon)^{-1}$ is the edge-preserving
diffusion coefficient, $G(u)$ is the Gaussian convolution operator, and
$\lambda(t)$ is the adaptive regularization parameter based on transmission map
$t$. We prove the existence and uniqueness of weak solutions in $H_0^1(\Omega)$
using Lax-Milgram theorem, and implement an efficient fixed-point iteration
scheme accelerated by PyTorch GPU computation. The experimental results
demonstrate that this method is a promising deghazing solution that can be
generalized to the deep model paradigm.

</details>


### [144] [Flow Diverse and Efficient: Learning Momentum Flow Matching via Stochastic Velocity Field Sampling](https://arxiv.org/abs/2506.08796)
*Zhiyuan Ma,Ruixun Liu,Sixian Liu,Jianjun Li,Bowen Zhou*

Main category: cs.CV

TL;DR: Discretized-RF improves rectified flow models by discretizing paths into variable velocity sub-paths, enhancing diversity and multi-scale noise modeling.


<details>
  <summary>Details</summary>
Motivation: Address diversity and multi-scale noise modeling limitations in straight-line rectified flow models.

Method: Discretizes straight paths into variable velocity sub-paths, introducing noise on velocity to improve diversity.

Result: Produces diverse, efficient trajectories and high-quality, diverse results.

Conclusion: Discretized-RF effectively enhances rectified flow models for better performance and diversity.

Abstract: Recently, the rectified flow (RF) has emerged as the new state-of-the-art
among flow-based diffusion models due to its high efficiency advantage in
straight path sampling, especially with the amazing images generated by a
series of RF models such as Flux 1.0 and SD 3.0. Although a straight-line
connection between the noisy and natural data distributions is intuitive, fast,
and easy to optimize, it still inevitably leads to: 1) Diversity concerns,
which arise since straight-line paths only cover a fairly restricted sampling
space. 2) Multi-scale noise modeling concerns, since the straight line flow
only needs to optimize the constant velocity field $\bm v$ between the two
distributions $\bm\pi_0$ and $\bm\pi_1$. In this work, we present
Discretized-RF, a new family of rectified flow (also called momentum flow
models since they refer to the previous velocity component and the random
velocity component in each diffusion step), which discretizes the straight path
into a series of variable velocity field sub-paths (namely ``momentum fields'')
to expand the search space, especially when close to the distribution
$p_\text{noise}$. Different from the previous case where noise is directly
superimposed on $\bm x$, we introduce noise on the velocity $\bm v$ of the
sub-path to change its direction in order to improve the diversity and
multi-scale noise modeling abilities. Experimental results on several
representative datasets demonstrate that learning momentum flow matching by
sampling random velocity fields will produce trajectories that are both diverse
and efficient, and can consistently generate high-quality and diverse results.
Code is available at https://github.com/liuruixun/momentum-fm.

</details>


### [145] [HunyuanVideo-HOMA: Generic Human-Object Interaction in Multimodal Driven Human Animation](https://arxiv.org/abs/2506.08797)
*Ziyao Huang,Zixiang Zhou,Juan Cao,Yifeng Ma,Yi Chen,Zejing Rao,Zhiyong Xu,Hongmei Wang,Qin Lin,Yuan Zhou,Qinglin Lu,Fan Tang*

Main category: cs.CV

TL;DR: HunyuanVideo-HOMA is a weakly conditioned multimodal-driven framework for HOI video generation, improving controllability and generalization with sparse motion guidance and dual input encoding.


<details>
  <summary>Details</summary>
Motivation: Addresses limitations in HOI video generation, such as reliance on curated data, poor generalization, and accessibility issues.

Method: Uses a multimodal diffusion transformer (MMDiT) with dual input space for appearance and motion, plus adapters for efficient training and lip sync.

Result: Achieves state-of-the-art performance in interaction naturalness and generalization under weak supervision.

Conclusion: Demonstrates versatility in text-conditioned generation and interactive manipulation, supported by a user-friendly interface.

Abstract: To address key limitations in human-object interaction (HOI) video generation
-- specifically the reliance on curated motion data, limited generalization to
novel objects/scenarios, and restricted accessibility -- we introduce
HunyuanVideo-HOMA, a weakly conditioned multimodal-driven framework.
HunyuanVideo-HOMA enhances controllability and reduces dependency on precise
inputs through sparse, decoupled motion guidance. It encodes appearance and
motion signals into the dual input space of a multimodal diffusion transformer
(MMDiT), fusing them within a shared context space to synthesize temporally
consistent and physically plausible interactions. To optimize training, we
integrate a parameter-space HOI adapter initialized from pretrained MMDiT
weights, preserving prior knowledge while enabling efficient adaptation, and a
facial cross-attention adapter for anatomically accurate audio-driven lip
synchronization. Extensive experiments confirm state-of-the-art performance in
interaction naturalness and generalization under weak supervision. Finally,
HunyuanVideo-HOMA demonstrates versatility in text-conditioned generation and
interactive object manipulation, supported by a user-friendly demo interface.
The project page is at https://anonymous.4open.science/w/homa-page-0FBE/.

</details>


### [146] [HiSin: Efficient High-Resolution Sinogram Inpainting via Resolution-Guided Progressive Inference](https://arxiv.org/abs/2506.08809)
*Jiaze E,Srutarshi Banerjee,Tekin Bicer,Guannan Wang,Yanfu Zhang,Bin Ren*

Main category: cs.CV

TL;DR: HiSin is a diffusion-based framework for efficient high-resolution sinogram inpainting, reducing memory and computational demands while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Missing high-frequency projections in computed tomography can cause artifacts and diagnostic errors, necessitating robust inpainting methods.

Method: HiSin uses resolution-guided progressive inference, frequency-aware patch skipping, and structure-adaptive step allocation to optimize memory and computation.

Result: HiSin reduces peak memory usage by 31.25% and inference time by 18.15% without compromising accuracy.

Conclusion: HiSin effectively addresses the limitations of diffusion models for high-resolution sinogram inpainting, offering a practical solution.

Abstract: High-resolution sinogram inpainting is essential for computed tomography
reconstruction, as missing high-frequency projections can lead to visible
artifacts and diagnostic errors. Diffusion models are well-suited for this task
due to their robustness and detail-preserving capabilities, but their
application to high-resolution inputs is limited by excessive memory and
computational demands. To address this limitation, we propose HiSin, a novel
diffusion based framework for efficient sinogram inpainting via
resolution-guided progressive inference. It progressively extracts global
structure at low resolution and defers high-resolution inference to small
patches, enabling memory-efficient inpainting. It further incorporates
frequency-aware patch skipping and structure-adaptive step allocation to reduce
redundant computation. Experimental results show that HiSin reduces peak memory
usage by up to 31.25% and inference time by up to 18.15%, and maintains
inpainting accuracy across datasets, resolutions, and mask conditions.

</details>


### [147] [Video-CoT: A Comprehensive Dataset for Spatiotemporal Understanding of Videos Based on Chain-of-Thought](https://arxiv.org/abs/2506.08817)
*Shuyi Zhang,Xiaoshuai Hao,Yingbo Tang,Lingfeng Zhang,Pengwei Wang,Zhongyuan Wang,Hongxuan Ma,Shanghang Zhang*

Main category: cs.CV

TL;DR: Video-CoT introduces a dataset and benchmark to improve spatiotemporal understanding in video comprehension, revealing current VLMs' limitations.


<details>
  <summary>Details</summary>
Motivation: Existing VLMs lack nuanced spatiotemporal detail capture, hindering thorough video analysis.

Method: Developed Video-CoT with 192,000 question-answer pairs and 23,000 CoT-annotated samples, plus a benchmark for evaluation.

Result: Current VLMs struggle with spatiotemporal tasks, as shown by experiments.

Conclusion: Video-CoT advances research in video comprehension and supports future intelligent systems.

Abstract: Video content comprehension is essential for various applications, ranging
from video analysis to interactive systems. Despite advancements in large-scale
vision-language models (VLMs), these models often struggle to capture the
nuanced, spatiotemporal details essential for thorough video analysis. To
address this gap, we introduce Video-CoT, a groundbreaking dataset designed to
enhance spatiotemporal understanding using Chain-of-Thought (CoT)
methodologies. Video-CoT contains 192,000 fine-grained spa-tiotemporal
question-answer pairs and 23,000 high-quality CoT-annotated samples, providing
a solid foundation for evaluating spatiotemporal understanding in video
comprehension. Additionally, we provide a comprehensive benchmark for assessing
these tasks, with each task featuring 750 images and tailored evaluation
metrics. Our extensive experiments reveal that current VLMs face significant
challenges in achieving satisfactory performance, high-lighting the
difficulties of effective spatiotemporal understanding. Overall, the Video-CoT
dataset and benchmark open new avenues for research in multimedia understanding
and support future innovations in intelligent systems requiring advanced video
analysis capabilities. By making these resources publicly available, we aim to
encourage further exploration in this critical area. Project
website:https://video-cot.github.io/ .

</details>


### [148] [CulturalFrames: Assessing Cultural Expectation Alignment in Text-to-Image Models and Evaluation Metrics](https://arxiv.org/abs/2506.08835)
*Shravan Nayak,Mehar Bhatia,Xiaofeng Zhang,Verena Rieser,Lisa Anne Hendricks,Sjoerd van Steenkiste,Yash Goyal,Karolina Staczak,Aishwarya Agrawal*

Main category: cs.CV

TL;DR: The study quantifies cultural misalignment in text-to-image (T2I) models, revealing significant failures in meeting explicit (68%) and implicit (49%) cultural expectations. It introduces CulturalFrames, a benchmark for human evaluation, and highlights poor correlation of existing metrics with human judgments.


<details>
  <summary>Details</summary>
Motivation: Address concerns about T2I models' ability to represent diverse cultural contexts accurately.

Method: Introduces CulturalFrames, a benchmark with 983 prompts, 3637 images from 4 T2I models, and 10k human annotations across 10 countries and 5 socio-cultural domains.

Result: T2I models miss cultural expectations 44% of the time (explicit: 68%, implicit: 49%). Existing metrics poorly correlate with human judgments.

Conclusion: Exposes gaps in cultural representation, urging development of more culturally informed T2I models and evaluation methods.

Abstract: The increasing ubiquity of text-to-image (T2I) models as tools for visual
content generation raises concerns about their ability to accurately represent
diverse cultural contexts. In this work, we present the first study to
systematically quantify the alignment of T2I models and evaluation metrics with
respect to both explicit as well as implicit cultural expectations. To this
end, we introduce CulturalFrames, a novel benchmark designed for rigorous human
evaluation of cultural representation in visual generations. Spanning 10
countries and 5 socio-cultural domains, CulturalFrames comprises 983 prompts,
3637 corresponding images generated by 4 state-of-the-art T2I models, and over
10k detailed human annotations. We find that T2I models not only fail to meet
the more challenging implicit expectations but also the less challenging
explicit expectations. Across models and countries, cultural expectations are
missed an average of 44% of the time. Among these failures, explicit
expectations are missed at a surprisingly high average rate of 68%, while
implicit expectation failures are also significant, averaging 49%. Furthermore,
we demonstrate that existing T2I evaluation metrics correlate poorly with human
judgments of cultural alignment, irrespective of their internal reasoning.
Collectively, our findings expose critical gaps, providing actionable
directions for developing more culturally informed T2I models and evaluation
methodologies.

</details>


### [149] [Adapting Vision-Language Foundation Model for Next Generation Medical Ultrasound Image Analysis](https://arxiv.org/abs/2506.08849)
*Jingguo Qu,Xinyang Han,Tonghuan Xiao,Jia Ai,Juan Wu,Tong Zhao,Jing Qin,Ann Dorothy King,Winnie Chiu-Wing Chu,Jing Cai,Michael Tin-Cheung Yingnst*

Main category: cs.CV

TL;DR: The paper proposes domain adaptation methods for vision-language foundation models to improve ultrasound image analysis, outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: Manual contouring in medical ultrasonography is labor-intensive and inconsistent. Vision-language models show promise but struggle with domain differences.

Method: Fine-tuning pipeline using large language models as text refiners with adaptation strategies and task-driven heads.

Result: Improved performance on six ultrasound datasets for segmentation and classification tasks, surpassing state-of-the-art models.

Conclusion: The method effectively enhances vision-language models for ultrasound analysis, with code available on GitHub.

Abstract: Medical ultrasonography is an essential imaging technique for examining
superficial organs and tissues, including lymph nodes, breast, and thyroid. It
employs high-frequency ultrasound waves to generate detailed images of the
internal structures of the human body. However, manually contouring regions of
interest in these images is a labor-intensive task that demands expertise and
often results in inconsistent interpretations among individuals.
Vision-language foundation models, which have excelled in various computer
vision applications, present new opportunities for enhancing ultrasound image
analysis. Yet, their performance is hindered by the significant differences
between natural and medical imaging domains. This research seeks to overcome
these challenges by developing domain adaptation methods for vision-language
foundation models. In this study, we explore the fine-tuning pipeline for
vision-language foundation models by utilizing large language model as text
refiner with special-designed adaptation strategies and task-driven heads. Our
approach has been extensively evaluated on six ultrasound datasets and two
tasks: segmentation and classification. The experimental results show that our
method can effectively improve the performance of vision-language foundation
models for ultrasound image analysis, and outperform the existing
state-of-the-art vision-language and pure foundation models. The source code of
this study is available at
\href{https://github.com/jinggqu/NextGen-UIA}{GitHub}.

</details>


### [150] [Spatial Transcriptomics Expression Prediction from Histopathology Based on Cross-Modal Mask Reconstruction and Contrastive Learning](https://arxiv.org/abs/2506.08854)
*Junzhuo Liu,Markus Eckstein,Zhixiang Wang,Friedrich Feuerhake,Dorit Merhof*

Main category: cs.CV

TL;DR: A contrastive learning-based deep learning method predicts spatially resolved gene expression from whole-slide images, improving prediction accuracy for key gene types and preserving gene-gene correlations.


<details>
  <summary>Details</summary>
Motivation: High costs limit large-scale spatial transcriptomics data acquisition, necessitating a method to predict gene expression from more accessible whole-slide images.

Method: Develops a contrastive learning-based deep learning approach to predict gene expression from whole-slide images, evaluated across six disease datasets.

Result: Improves prediction accuracy (PCC) for highly expressed, variable, and marker genes by 6.27%, 6.11%, and 11.26%, respectively, and preserves gene-gene correlations.

Conclusion: The method effectively predicts gene expression, works with limited samples, and shows promise for cancer tissue localization.

Abstract: Spatial transcriptomics is a technology that captures gene expression levels
at different spatial locations, widely used in tumor microenvironment analysis
and molecular profiling of histopathology, providing valuable insights into
resolving gene expression and clinical diagnosis of cancer. Due to the high
cost of data acquisition, large-scale spatial transcriptomics data remain
challenging to obtain. In this study, we develop a contrastive learning-based
deep learning method to predict spatially resolved gene expression from
whole-slide images. Evaluation across six different disease datasets
demonstrates that, compared to existing studies, our method improves Pearson
Correlation Coefficient (PCC) in the prediction of highly expressed genes,
highly variable genes, and marker genes by 6.27%, 6.11%, and 11.26%
respectively. Further analysis indicates that our method preserves gene-gene
correlations and applies to datasets with limited samples. Additionally, our
method exhibits potential in cancer tissue localization based on biomarker
expression.

</details>


### [151] [StreamSplat: Towards Online Dynamic 3D Reconstruction from Uncalibrated Video Streams](https://arxiv.org/abs/2506.08862)
*Zike Wu,Qi Yan,Xuanyu Yi,Lele Wang,Renjie Liao*

Main category: cs.CV

TL;DR: StreamSplat is a feed-forward framework for real-time 3D scene reconstruction from uncalibrated video, addressing challenges like dynamic modeling and computational efficiency with innovations in probabilistic sampling and bidirectional deformation fields.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to handle uncalibrated inputs, dynamic scene evolution, and long-term stability efficiently. StreamSplat aims to solve these issues.

Method: Uses a static encoder with probabilistic sampling for 3DGS position prediction and a dynamic decoder with bidirectional deformation fields for robust dynamic modeling.

Result: Outperforms prior works in reconstruction quality and dynamic scene modeling, supporting online reconstruction of long videos.

Conclusion: StreamSplat effectively addresses key challenges in real-time 3D scene reconstruction, offering superior performance and efficiency.

Abstract: Real-time reconstruction of dynamic 3D scenes from uncalibrated video streams
is crucial for numerous real-world applications. However, existing methods
struggle to jointly address three key challenges: 1) processing uncalibrated
inputs in real time, 2) accurately modeling dynamic scene evolution, and 3)
maintaining long-term stability and computational efficiency. To this end, we
introduce StreamSplat, the first fully feed-forward framework that transforms
uncalibrated video streams of arbitrary length into dynamic 3D Gaussian
Splatting (3DGS) representations in an online manner, capable of recovering
scene dynamics from temporally local observations. We propose two key technical
innovations: a probabilistic sampling mechanism in the static encoder for 3DGS
position prediction, and a bidirectional deformation field in the dynamic
decoder that enables robust and efficient dynamic modeling. Extensive
experiments on static and dynamic benchmarks demonstrate that StreamSplat
consistently outperforms prior works in both reconstruction quality and dynamic
scene modeling, while uniquely supporting online reconstruction of arbitrarily
long video streams. Code and models are available at
https://github.com/nickwzk/StreamSplat.

</details>


### [152] [DiscoVLA: Discrepancy Reduction in Vision, Language, and Alignment for Parameter-Efficient Video-Text Retrieval](https://arxiv.org/abs/2506.08887)
*Leqi Shen,Guoqiang Gong,Tianxiang Hao,Tao He,Yifeng Zhang,Pengzhang Liu,Sicheng Zhao,Jungong Han,Guiguang Ding*

Main category: cs.CV

TL;DR: DiscoVLA addresses discrepancies in vision, language, and alignment for video-text retrieval, outperforming prior methods by 1.5% in R@1 on MSRVTT.


<details>
  <summary>Details</summary>
Motivation: Existing methods for video-text retrieval focus on vision but neglect language and alignment discrepancies when adapting CLIP from image-level to video-level.

Method: DiscoVLA integrates image-video features, generates pseudo image captions, and uses image-to-video alignment distillation to mitigate discrepancies.

Result: DiscoVLA achieves 50.5% R@1 on MSRVTT, outperforming previous methods by 1.5%.

Conclusion: DiscoVLA effectively reduces discrepancies in vision, language, and alignment, demonstrating superior performance in video-text retrieval.

Abstract: The parameter-efficient adaptation of the image-text pretraining model CLIP
for video-text retrieval is a prominent area of research. While CLIP is focused
on image-level vision-language matching, video-text retrieval demands
comprehensive understanding at the video level. Three key discrepancies emerge
in the transfer from image-level to video-level: vision, language, and
alignment. However, existing methods mainly focus on vision while neglecting
language and alignment. In this paper, we propose Discrepancy Reduction in
Vision, Language, and Alignment (DiscoVLA), which simultaneously mitigates all
three discrepancies. Specifically, we introduce Image-Video Features Fusion to
integrate image-level and video-level features, effectively tackling both
vision and language discrepancies. Additionally, we generate pseudo image
captions to learn fine-grained image-level alignment. To mitigate alignment
discrepancies, we propose Image-to-Video Alignment Distillation, which
leverages image-level alignment knowledge to enhance video-level alignment.
Extensive experiments demonstrate the superiority of our DiscoVLA. In
particular, on MSRVTT with CLIP (ViT-B/16), DiscoVLA outperforms previous
methods by 1.5% in R@1, reaching a final score of 50.5% R@1. The code is
available at https://github.com/LunarShen/DsicoVLA.

</details>


### [153] [Product of Experts for Visual Generation](https://arxiv.org/abs/2506.08894)
*Yunzhi Zhang,Carson Murtuza-Lanier,Zizhang Li,Yilun Du,Jiajun Wu*

Main category: cs.CV

TL;DR: A Product of Experts (PoE) framework integrates diverse knowledge from heterogeneous models (e.g., generative, language, human-crafted) for image/video synthesis, using Annealed Importance Sampling (AIS) for inference-time composition.


<details>
  <summary>Details</summary>
Motivation: To explore the under-addressed challenge of integrating diverse knowledge from multiple sources (e.g., generative models, language models, human-crafted knowledge) for improved visual synthesis.

Method: Proposes a training-free PoE framework using Annealed Importance Sampling (AIS) to sample from the product distribution across heterogeneous experts.

Result: Demonstrates practical benefits in image/video synthesis, offering better controllability and flexible user interfaces for visual generation.

Conclusion: The PoE framework effectively composes knowledge from diverse models, enhancing controllability and flexibility in visual synthesis tasks.

Abstract: Modern neural models capture rich priors and have complementary knowledge
over shared data domains, e.g., images and videos. Integrating diverse
knowledge from multiple sources -- including visual generative models, visual
language models, and sources with human-crafted knowledge such as graphics
engines and physics simulators -- remains under-explored. We propose a Product
of Experts (PoE) framework that performs inference-time knowledge composition
from heterogeneous models. This training-free approach samples from the product
distribution across experts via Annealed Importance Sampling (AIS). Our
framework shows practical benefits in image and video synthesis tasks, yielding
better controllability than monolithic methods and additionally providing
flexible user interfaces for specifying visual generation goals.

</details>


### [154] [WetCat: Automating Skill Assessment in Wetlab Cataract Surgery Videos](https://arxiv.org/abs/2506.08896)
*Negin Ghamsarian,Raphael Sznitman,Klaus Schoeffmann,Jens Kowal*

Main category: cs.CV

TL;DR: WetCat is a new dataset for automated skill assessment in wetlab cataract surgery, addressing gaps in existing resources by providing annotated videos of trainee surgeries on artificial eyes.


<details>
  <summary>Details</summary>
Motivation: Traditional wetlab training relies on manual evaluations, which are inefficient and subjective. Advances in computer vision can automate skill assessment, but existing datasets lack focus on controlled wetlab settings.

Method: The WetCat dataset includes high-resolution wetlab cataract surgery videos with phase annotations and semantic segmentations of key structures, targeting capsulorhexis and phacoemulsification phases.

Result: WetCat supports AI-driven skill assessment tools aligned with clinical standards, enhancing objectivity and scalability in surgical education.

Conclusion: WetCat sets a benchmark for automated workflow analysis and skill assessment in ophthalmology training, with publicly available data.

Abstract: To meet the growing demand for systematic surgical training, wetlab
environments have become indispensable platforms for hands-on practice in
ophthalmology. Yet, traditional wetlab training depends heavily on manual
performance evaluations, which are labor-intensive, time-consuming, and often
subject to variability. Recent advances in computer vision offer promising
avenues for automated skill assessment, enhancing both the efficiency and
objectivity of surgical education. Despite notable progress in ophthalmic
surgical datasets, existing resources predominantly focus on real surgeries or
isolated tasks, falling short of supporting comprehensive skill evaluation in
controlled wetlab settings. To address these limitations, we introduce WetCat,
the first dataset of wetlab cataract surgery videos specifically curated for
automated skill assessment. WetCat comprises high-resolution recordings of
surgeries performed by trainees on artificial eyes, featuring comprehensive
phase annotations and semantic segmentations of key anatomical structures.
These annotations are meticulously designed to facilitate skill assessment
during the critical capsulorhexis and phacoemulsification phases, adhering to
standardized surgical skill assessment frameworks. By focusing on these
essential phases, WetCat enables the development of interpretable, AI-driven
evaluation tools aligned with established clinical metrics. This dataset lays a
strong foundation for advancing objective, scalable surgical education and sets
a new benchmark for automated workflow analysis and skill assessment in
ophthalmology training. The dataset and annotations are publicly available in
Synapse https://www.synapse.org/Synapse:syn66401174/files.

</details>


### [155] [MIRAGE: Multimodal foundation model and benchmark for comprehensive retinal OCT image analysis](https://arxiv.org/abs/2506.08900)
*Jos Morano,Botond Fazekas,Emese Skei,Ronald Fecso,Taha Emre,Markus Gumpinger,Georg Faustmann,Marzieh Oghbaie,Ursula Schmidt-Erfurth,Hrvoje Bogunovi*

Main category: cs.CV

TL;DR: MIRAGE is a multimodal foundation model for analyzing OCT and SLO images, outperforming existing models in classification and segmentation tasks.


<details>
  <summary>Details</summary>
Motivation: AI models for ophthalmic image analysis require extensive annotation and underperform on unseen data. Foundation models (FMs) offer promise but lack validation for ophthalmology, especially in multimodal tasks.

Method: Proposed MIRAGE, a multimodal FM for OCT and SLO images, and introduced a new evaluation benchmark for classification and segmentation.

Result: MIRAGE outperformed general and specialized FMs and segmentation methods in both tasks.

Conclusion: MIRAGE is a robust foundation for AI systems in retinal OCT analysis, with its model and benchmark publicly available.

Abstract: Artificial intelligence (AI) has become a fundamental tool for assisting
clinicians in analyzing ophthalmic images, such as optical coherence tomography
(OCT). However, developing AI models often requires extensive annotation, and
existing models tend to underperform on independent, unseen data. Foundation
models (FMs), large AI models trained on vast unlabeled datasets, have shown
promise in overcoming these challenges. Nonetheless, available FMs for
ophthalmology lack extensive validation, especially for segmentation tasks, and
focus on a single imaging modality. In this context, we propose MIRAGE, a novel
multimodal FM for the analysis of OCT and scanning laser ophthalmoscopy (SLO)
images. Additionally, we propose a new evaluation benchmark with OCT/SLO
classification and segmentation tasks. The comparison with general and
specialized FMs and segmentation methods shows the superiority of MIRAGE in
both types of tasks, highlighting its suitability as a basis for the
development of robust AI systems for retinal OCT image analysis. Both MIRAGE
and the evaluation benchmark are publicly available:
https://github.com/j-morano/MIRAGE.

</details>


### [156] [Hyperbolic Dual Feature Augmentation for Open-Environment](https://arxiv.org/abs/2506.08906)
*Peilin Yu,Yuwei Wu,Zhi Gao,Xiaomeng Fan,Shuo Yang,Yunde Jia*

Main category: cs.CV

TL;DR: The paper proposes a hyperbolic dual feature augmentation method for open-environment learning, enhancing performance for both seen and unseen classes using neural ODEs and meta-learning.


<details>
  <summary>Details</summary>
Motivation: Existing hyperbolic feature augmentation methods are limited to closed environments with fixed classes, lacking adaptability for open-environment tasks.

Method: The method uses neural ODEs with meta-learning to estimate feature distributions, introduces a regularizer to preserve hierarchical structures, and derives a loss upper bound for infinite augmentations.

Result: Experiments on five open-environment tasks show improved performance of hyperbolic algorithms.

Conclusion: The proposed method effectively generalizes hyperbolic feature augmentation to open environments, enhancing learning algorithms' adaptability.

Abstract: Feature augmentation generates novel samples in the feature space, providing
an effective way to enhance the generalization ability of learning algorithms
with hyperbolic geometry. Most hyperbolic feature augmentation is confined to
closed-environment, assuming the number of classes is fixed (\emph{i.e.}, seen
classes) and generating features only for these classes. In this paper, we
propose a hyperbolic dual feature augmentation method for open-environment,
which augments features for both seen and unseen classes in the hyperbolic
space. To obtain a more precise approximation of the real data distribution for
efficient training, (1) we adopt a neural ordinary differential equation
module, enhanced by meta-learning, estimating the feature distributions of both
seen and unseen classes; (2) we then introduce a regularizer to preserve the
latent hierarchical structures of data in the hyperbolic space; (3) we also
derive an upper bound for the hyperbolic dual augmentation loss, allowing us to
train a hyperbolic model using infinite augmentations for seen and unseen
classes. Extensive experiments on five open-environment tasks:
class-incremental learning, few-shot open-set recognition, few-shot learning,
zero-shot learning, and general image classification, demonstrate that our
method effectively enhances the performance of hyperbolic algorithms in
open-environment.

</details>


### [157] [SkipVAR: Accelerating Visual Autoregressive Modeling via Adaptive Frequency-Aware Skipping](https://arxiv.org/abs/2506.08908)
*Jiajun Li,Yue Ma,Xinyu Zhang,Qingyan Wei,Songhua Liu,Linfeng Zhang*

Main category: cs.CV

TL;DR: SkipVAR improves VAR model efficiency by addressing step and unconditional branch redundancies, achieving up to 1.81x acceleration while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: High-frequency components in VAR models cause latency, but computational redundancies are unexplored.

Method: Proposes step-skipping and unconditional branch replacement, then introduces SkipVAR for adaptive acceleration.

Result: Achieves 0.88 average SSIM, 1.81x overall acceleration, and 2.62x speedup on GenEval.

Conclusion: Frequency-aware, training-free adaptive acceleration is effective for scalable autoregressive image generation.

Abstract: Recent studies on Visual Autoregressive (VAR) models have highlighted that
high-frequency components, or later steps, in the generation process contribute
disproportionately to inference latency. However, the underlying computational
redundancy involved in these steps has yet to be thoroughly investigated. In
this paper, we conduct an in-depth analysis of the VAR inference process and
identify two primary sources of inefficiency: step redundancy and unconditional
branch redundancy. To address step redundancy, we propose an automatic
step-skipping strategy that selectively omits unnecessary generation steps to
improve efficiency. For unconditional branch redundancy, we observe that the
information gap between the conditional and unconditional branches is minimal.
Leveraging this insight, we introduce unconditional branch replacement, a
technique that bypasses the unconditional branch to reduce computational cost.
Notably, we observe that the effectiveness of acceleration strategies varies
significantly across different samples. Motivated by this, we propose SkipVAR,
a sample-adaptive framework that leverages frequency information to dynamically
select the most suitable acceleration strategy for each instance. To evaluate
the role of high-frequency information, we introduce high-variation benchmark
datasets that test model sensitivity to fine details. Extensive experiments
show SkipVAR achieves over 0.88 average SSIM with up to 1.81x overall
acceleration and 2.62x speedup on the GenEval benchmark, maintaining model
quality. These results confirm the effectiveness of frequency-aware,
training-free adaptive acceleration for scalable autoregressive image
generation. Our code is available at https://github.com/fakerone-li/SkipVAR and
has been publicly released.

</details>


### [158] [Inherently Faithful Attention Maps for Vision Transformers](https://arxiv.org/abs/2506.08915)
*Ananthu Aniraj,Cassio F. Dantas,Dino Ienco,Diego Marcos*

Main category: cs.CV

TL;DR: A two-stage attention-based method uses binary masks to focus on relevant image regions, improving robustness against spurious correlations and out-of-distribution backgrounds.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of biased object perception due to context, especially in out-of-distribution backgrounds, while balancing the need for context in object-centric tasks.

Method: A two-stage framework: stage 1 identifies task-relevant regions, and stage 2 uses attention masks to focus on these regions, jointly trained for refinement.

Result: Significant improvement in robustness against spurious correlations and out-of-distribution backgrounds across diverse benchmarks.

Conclusion: The proposed method effectively balances context usage and focus, enhancing object perception accuracy.

Abstract: We introduce an attention-based method that uses learned binary attention
masks to ensure that only attended image regions influence the prediction.
Context can strongly affect object perception, sometimes leading to biased
representations, particularly when objects appear in out-of-distribution
backgrounds. At the same time, many image-level object-centric tasks require
identifying relevant regions, often requiring context. To address this
conundrum, we propose a two-stage framework: stage 1 processes the full image
to discover object parts and identify task-relevant regions, while stage 2
leverages input attention masking to restrict its receptive field to these
regions, enabling a focused analysis while filtering out potentially spurious
information. Both stages are trained jointly, allowing stage 2 to refine stage
1. Extensive experiments across diverse benchmarks demonstrate that our
approach significantly improves robustness against spurious correlations and
out-of-distribution backgrounds.

</details>


### [159] [Socratic-MCTS: Test-Time Visual Reasoning by Asking the Right Questions](https://arxiv.org/abs/2506.08927)
*David Acuna,Ximing Lu,Jaehun Jung,Hyunwoo Kim,Amlan Kar,Sanja Fidler,Yejin Choi*

Main category: cs.CV

TL;DR: The paper proposes a Monte Carlo Tree Search (MCTS)-inspired algorithm to elicit hidden knowledge and induce long reasoning traces in non-reasoning vision-language models without additional training.


<details>
  <summary>Details</summary>
Motivation: To explore whether existing non-reasoning models can be enhanced to perform long-form reasoning without retraining or supervision.

Method: Uses an MCTS-inspired algorithm to inject subquestion-subanswer pairs into the model's output stream, framing reasoning as a search process.

Result: Consistent improvements across three benchmarks, including a 2% overall gain on MMMU-PRO and a 9% improvement in Liberal Arts.

Conclusion: Framing reasoning as a search process can help non-reasoning models produce extended reasoning traces without additional training.

Abstract: Recent research in vision-language models (VLMs) has centered around the
possibility of equipping them with implicit long-form chain-of-thought
reasoning -- akin to the success observed in language models -- via
distillation and reinforcement learning. But what about the non-reasoning
models already trained and deployed across the internet? Should we simply
abandon them, or is there hope for a search mechanism that can elicit hidden
knowledge and induce long reasoning traces -- without any additional training
or supervision? In this paper, we explore this possibility using a Monte Carlo
Tree Search (MCTS)-inspired algorithm, which injects subquestion-subanswer
pairs into the model's output stream. We show that framing reasoning as a
search process -- where subquestions act as latent decisions within a broader
inference trajectory -- helps the model "connect the dots" between fragmented
knowledge and produce extended reasoning traces in non-reasoning models. We
evaluate our method across three benchmarks and observe consistent
improvements. Notably, our approach yields a 2% overall improvement on
MMMU-PRO, including a significant 9% gain in Liberal Arts.

</details>


### [160] [What Limits Virtual Agent Application? OmniBench: A Scalable Multi-Dimensional Benchmark for Essential Virtual Agent Capabilities](https://arxiv.org/abs/2506.08933)
*Wendong Bu,Yang Wu,Qifan Yu,Minghe Gao,Bingchen Miao,Zhenkui Zhang,Kaihang Pan,Yunfei Li,Mengze Li,Wei Ji,Juncheng Li,Siliang Tang,Yueting Zhuang*

Main category: cs.CV

TL;DR: The paper introduces OmniBench, a self-generating, cross-platform, graph-based benchmark for evaluating MLLM-based virtual agents, addressing limitations of existing benchmarks. It also presents OmniEval, a multidimensional evaluation framework, and demonstrates improved agent training efficiency with graph-structured data.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for MLLM-based virtual agents have limitations like uncontrollable task complexity, manual annotation constraints, and lack of multidimensional evaluation.

Method: The authors propose OmniBench, a benchmark with an automated pipeline for task synthesis, and OmniEval, a framework for multidimensional evaluation. They use graph-structured tasks and evaluate 10 capabilities.

Result: The synthesized dataset includes 36k tasks across 20 scenarios with a 91% human acceptance rate. Graph-structured data trains agents more efficiently than manual annotation.

Conclusion: OmniBench and OmniEval provide a robust solution for evaluating virtual agents, revealing performance gaps and guiding future advancements.

Abstract: As multimodal large language models (MLLMs) advance, MLLM-based virtual
agents have demonstrated remarkable performance. However, existing benchmarks
face significant limitations, including uncontrollable task complexity,
extensive manual annotation with limited scenarios, and a lack of
multidimensional evaluation. In response to these challenges, we introduce
OmniBench, a self-generating, cross-platform, graph-based benchmark with an
automated pipeline for synthesizing tasks of controllable complexity through
subtask composition. To evaluate the diverse capabilities of virtual agents on
the graph, we further present OmniEval, a multidimensional evaluation framework
that includes subtask-level evaluation, graph-based metrics, and comprehensive
tests across 10 capabilities. Our synthesized dataset contains 36k
graph-structured tasks across 20 scenarios, achieving a 91\% human acceptance
rate. Training on our graph-structured data shows that it can more efficiently
guide agents compared to manually annotated data. We conduct multidimensional
evaluations for various open-source and closed-source models, revealing their
performance across various capabilities and paving the way for future
advancements. Our project is available at https://omni-bench.github.io/.

</details>


### [161] [SSS: Semi-Supervised SAM-2 with Efficient Prompting for Medical Imaging Segmentation](https://arxiv.org/abs/2506.08949)
*Hongjie Zhu,Xiwei Liu,Rundong Xue,Zeyu Zhang,Yong Xu,Daji Ergu,Ying Cai,Yang Zhao*

Main category: cs.CV

TL;DR: The paper introduces SSS, a semi-supervised learning method leveraging SAM-2's feature extraction to enhance medical image segmentation, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of efficiently using unlabeled medical data while reducing reliance on costly annotations.

Method: Proposes SSS, integrating SAM-2's features with a Discriminative Feature Enhancement mechanism and a prompt generator for unlabeled data.

Result: SSS outperforms previous methods, achieving a +3.65 Dice score improvement on the BHSD dataset.

Conclusion: SSS demonstrates the potential of leveraging foundation models like SAM-2 for semi-supervised medical image analysis.

Abstract: In the era of information explosion, efficiently leveraging large-scale
unlabeled data while minimizing the reliance on high-quality pixel-level
annotations remains a critical challenge in the field of medical imaging.
Semi-supervised learning (SSL) enhances the utilization of unlabeled data by
facilitating knowledge transfer, significantly improving the performance of
fully supervised models and emerging as a highly promising research direction
in medical image analysis. Inspired by the ability of Vision Foundation Models
(e.g., SAM-2) to provide rich prior knowledge, we propose SSS (Semi-Supervised
SAM-2), a novel approach that leverages SAM-2's robust feature extraction
capabilities to uncover latent knowledge in unlabeled medical images, thus
effectively enhancing feature support for fully supervised medical image
segmentation. Specifically, building upon the single-stream "weak-to-strong"
consistency regularization framework, this paper introduces a Discriminative
Feature Enhancement (DFE) mechanism to further explore the feature
discrepancies introduced by various data augmentation strategies across
multiple views. By leveraging feature similarity and dissimilarity across
multi-scale augmentation techniques, the method reconstructs and models the
features, thereby effectively optimizing the salient regions. Furthermore, a
prompt generator is developed that integrates Physical Constraints with a
Sliding Window (PCSW) mechanism to generate input prompts for unlabeled data,
fulfilling SAM-2's requirement for additional prompts. Extensive experiments
demonstrate the superiority of the proposed method for semi-supervised medical
image segmentation on two multi-label datasets, i.e., ACDC and BHSD. Notably,
SSS achieves an average Dice score of 53.15 on BHSD, surpassing the previous
state-of-the-art method by +3.65 Dice. Code will be available at
https://github.com/AIGeeksGroup/SSS.

</details>


### [162] [Cross-Spectral Body Recognition with Side Information Embedding: Benchmarks on LLCM and Analyzing Range-Induced Occlusions on IJB-MDF](https://arxiv.org/abs/2506.08953)
*Anirudh Nanduri,Siyuan Huang,Rama Chellappa*

Main category: cs.CV

TL;DR: A ViT model pretrained on visible imagery is adapted for cross-spectral body recognition, using Side Information Embedding (SIE) to encode camera information, achieving state-of-the-art results. The study also explores occlusions in VI-ReID using the IJB-MDF dataset.


<details>
  <summary>Details</summary>
Motivation: To enhance cross-spectral matching (visible and infrared domains) and address the underexplored issue of occlusions in visible-infrared person re-identification.

Method: Adapt a ViT model with Side Information Embedding (SIE), encoding domain and camera information, and evaluate on datasets like LLCM and IJB-MDF.

Result: Encoding only camera information (without domain info) achieves state-of-the-art performance on LLCM. The study also highlights the impact of range-induced occlusions.

Conclusion: Camera information is crucial for cross-spectral matching, and occlusions in VI-ReID warrant further exploration, as demonstrated by the IJB-MDF dataset.

Abstract: Vision Transformers (ViTs) have demonstrated impressive performance across a
wide range of biometric tasks, including face and body recognition. In this
work, we adapt a ViT model pretrained on visible (VIS) imagery to the
challenging problem of cross-spectral body recognition, which involves matching
images captured in the visible and infrared (IR) domains. Recent ViT
architectures have explored incorporating additional embeddings beyond
traditional positional embeddings. Building on this idea, we integrate Side
Information Embedding (SIE) and examine the impact of encoding domain and
camera information to enhance cross-spectral matching. Surprisingly, our
results show that encoding only camera information - without explicitly
incorporating domain information - achieves state-of-the-art performance on the
LLCM dataset. While occlusion handling has been extensively studied in
visible-spectrum person re-identification (Re-ID), occlusions in
visible-infrared (VI) Re-ID remain largely underexplored - primarily because
existing VI-ReID datasets, such as LLCM, SYSU-MM01, and RegDB, predominantly
feature full-body, unoccluded images. To address this gap, we analyze the
impact of range-induced occlusions using the IARPA Janus Benchmark Multi-Domain
Face (IJB-MDF) dataset, which provides a diverse set of visible and infrared
images captured at various distances, enabling cross-range, cross-spectral
evaluations.

</details>


### [163] [Segment Concealed Objects with Incomplete Supervision](https://arxiv.org/abs/2506.08955)
*Chunming He,Kai Li,Yachao Zhang,Ziyun Yang,Youwei Pang,Longxiang Tang,Chengyu Fang,Yulun Zhang,Linghe Kong,Xiu Li,Sina Farsiu*

Main category: cs.CV

TL;DR: The paper introduces SEE, a unified method for Incompletely-Supervised Concealed Object Segmentation (ISCOS), leveraging SAM for pseudo-label generation and a hybrid-granularity feature grouping module to address challenges of incomplete supervision and intrinsic similarity.


<details>
  <summary>Details</summary>
Motivation: The challenges in ISCOS include limited supervision from incomplete annotations and difficulty distinguishing concealed objects from backgrounds due to intrinsic similarities.

Method: Proposes SEE, a mean-teacher framework using SAM for pseudo-label generation, strategies for pseudo-label quality control, and a hybrid-granularity feature grouping module for segmentation coherence.

Result: Achieves state-of-the-art performance in ISCOS tasks and can enhance existing models as a plug-and-play solution.

Conclusion: SEE effectively addresses ISCOS challenges, demonstrating robust performance and versatility.

Abstract: Incompletely-Supervised Concealed Object Segmentation (ISCOS) involves
segmenting objects that seamlessly blend into their surrounding environments,
utilizing incompletely annotated data, such as weak and semi-annotations, for
model training. This task remains highly challenging due to (1) the limited
supervision provided by the incompletely annotated training data, and (2) the
difficulty of distinguishing concealed objects from the background, which
arises from the intrinsic similarities in concealed scenarios. In this paper,
we introduce the first unified method for ISCOS to address these challenges. To
tackle the issue of incomplete supervision, we propose a unified mean-teacher
framework, SEE, that leverages the vision foundation model, ``\emph{Segment
Anything Model (SAM)}'', to generate pseudo-labels using coarse masks produced
by the teacher model as prompts. To mitigate the effect of low-quality
segmentation masks, we introduce a series of strategies for pseudo-label
generation, storage, and supervision. These strategies aim to produce
informative pseudo-labels, store the best pseudo-labels generated, and select
the most reliable components to guide the student model, thereby ensuring
robust network training. Additionally, to tackle the issue of intrinsic
similarity, we design a hybrid-granularity feature grouping module that groups
features at different granularities and aggregates these results. By clustering
similar features, this module promotes segmentation coherence, facilitating
more complete segmentation for both single-object and multiple-object images.
We validate the effectiveness of our approach across multiple ISCOS tasks, and
experimental results demonstrate that our method achieves state-of-the-art
performance. Furthermore, SEE can serve as a plug-and-play solution, enhancing
the performance of existing models.

</details>


### [164] [Data Augmentation For Small Object using Fast AutoAugment](https://arxiv.org/abs/2506.08956)
*DaeEun Yoon,Semin Kim,SangWook Yoo,Jongha Lee*

Main category: cs.CV

TL;DR: Proposes an optimal data augmentation method using Fast AutoAugment to improve small object detection, achieving a 20% performance boost on DOTA.


<details>
  <summary>Details</summary>
Motivation: Small object detection performance lags behind large objects, posing a significant challenge in computer vision.

Method: Uses Fast AutoAugment to quickly find optimal augmentation policies for small objects.

Result: Achieves a 20% performance improvement on the DOTA dataset.

Conclusion: The proposed method effectively enhances small object detection performance.

Abstract: In recent years, there has been tremendous progress in object detection
performance. However, despite these advances, the detection performance for
small objects is significantly inferior to that of large objects. Detecting
small objects is one of the most challenging and important problems in computer
vision. To improve the detection performance for small objects, we propose an
optimal data augmentation method using Fast AutoAugment. Through our proposed
method, we can quickly find optimal augmentation policies that can overcome
degradation when detecting small objects, and we achieve a 20% performance
improvement on the DOTA dataset.

</details>


### [165] [ORIDa: Object-centric Real-world Image Composition Dataset](https://arxiv.org/abs/2506.08964)
*Jinwoo Kim,Sangmin Han,Jinho Jeong,Jiwoo Choi,Dongyoung Kim,Seon Joo Kim*

Main category: cs.CV

TL;DR: ORIDa is a large-scale dataset for object compositing, featuring 30,000+ images with 200 unique objects in diverse scenes, including factual-counterfactual sets and factual-only scenes.


<details>
  <summary>Details</summary>
Motivation: Existing datasets lack diversity and scale for real-world object compositing tasks.

Method: ORIDa provides two data types: factual-counterfactual sets (5 images per scene) and factual-only scenes (single image per context).

Result: ORIDa is the first dataset of its scale and complexity for real-world image composition.

Conclusion: ORIDa advances research in object compositing by offering a comprehensive, real-world dataset.

Abstract: Object compositing, the task of placing and harmonizing objects in images of
diverse visual scenes, has become an important task in computer vision with the
rise of generative models. However, existing datasets lack the diversity and
scale required to comprehensively explore real-world scenarios. We introduce
ORIDa (Object-centric Real-world Image Composition Dataset), a large-scale,
real-captured dataset containing over 30,000 images featuring 200 unique
objects, each of which is presented across varied positions and scenes. ORIDa
has two types of data: factual-counterfactual sets and factual-only scenes. The
factual-counterfactual sets consist of four factual images showing an object in
different positions within a scene and a single counterfactual (or background)
image of the scene without the object, resulting in five images per scene. The
factual-only scenes include a single image containing an object in a specific
context, expanding the variety of environments. To our knowledge, ORIDa is the
first publicly available dataset with its scale and complexity for real-world
image composition. Extensive analysis and experiments highlight the value of
ORIDa as a resource for advancing further research in object compositing.

</details>


### [166] [ADAM: Autonomous Discovery and Annotation Model using LLMs for Context-Aware Annotations](https://arxiv.org/abs/2506.08968)
*Amirreza Rouhi,Solmaz Arezoomandan,Knut Peterson,Joseph T. Woods,David K. Han*

Main category: cs.CV

TL;DR: ADAM is a training-free, self-refining framework for open-world object labeling, using LLMs and CLIP to generate and refine labels for novel objects without supervision.


<details>
  <summary>Details</summary>
Motivation: Overcome the limitation of predefined categories in object detection models for identifying novel objects in open-world scenarios.

Method: Leverages LLMs for contextual label generation and CLIP for visual embeddings, constructs an Embedding-Label Repository (ELR), and uses frequency-based voting, cross-modal re-ranking, and self-refinement for robust labeling.

Result: Effectively annotates novel categories on COCO and PASCAL datasets without fine-tuning or retraining.

Conclusion: ADAM provides a scalable solution for open-world object labeling by combining contextual and visual signals.

Abstract: Object detection models typically rely on predefined categories, limiting
their ability to identify novel objects in open-world scenarios. To overcome
this constraint, we introduce ADAM: Autonomous Discovery and Annotation Model,
a training-free, self-refining framework for open-world object labeling. ADAM
leverages large language models (LLMs) to generate candidate labels for unknown
objects based on contextual information from known entities within a scene.
These labels are paired with visual embeddings from CLIP to construct an
Embedding-Label Repository (ELR) that enables inference without category
supervision. For a newly encountered unknown object, ADAM retrieves visually
similar instances from the ELR and applies frequency-based voting and
cross-modal re-ranking to assign a robust label. To further enhance
consistency, we introduce a self-refinement loop that re-evaluates repository
labels using visual cohesion analysis and k-nearest-neighbor-based majority
re-labeling. Experimental results on the COCO and PASCAL datasets demonstrate
that ADAM effectively annotates novel categories using only visual and
contextual signals, without requiring any fine-tuning or retraining.

</details>


### [167] [Rethinking Range-View LiDAR Segmentation in Adverse Weather](https://arxiv.org/abs/2506.08979)
*Longyu Yang,Ping Hu,Lu Zhang,Jun Liu,Yap-Peng Tan,Heng Tao Shen,Xiaofeng Zhu*

Main category: cs.CV

TL;DR: A modular framework enhances range-view LiDAR segmentation robustness in adverse weather by separating geometric and reflectance processing.


<details>
  <summary>Details</summary>
Motivation: Generalized performance of range-view LiDAR segmentation under adverse weather is underexplored, limiting real-world reliability.

Method: Proposes a framework with two branches: GAS for spatial noise suppression and RDC for reflectance correction, fused into the original pipeline.

Result: Significantly improves generalization to adverse weather with minimal inference overhead.

Conclusion: Offers a practical solution for robust real-world LiDAR segmentation.

Abstract: LiDAR segmentation has emerged as an important task to enrich multimedia
experiences and analysis. Range-view-based methods have gained popularity due
to their high computational efficiency and compatibility with real-time
deployment. However, their generalized performance under adverse weather
conditions remains underexplored, limiting their reliability in real-world
environments. In this work, we identify and analyze the unique challenges that
affect the generalization of range-view LiDAR segmentation in severe weather.
To address these challenges, we propose a modular and lightweight framework
that enhances robustness without altering the core architecture of existing
models. Our method reformulates the initial stem block of standard range-view
networks into two branches to process geometric attributes and reflectance
intensity separately. Specifically, a Geometric Abnormality Suppression (GAS)
module reduces the influence of weather-induced spatial noise, and a
Reflectance Distortion Calibration (RDC) module corrects reflectance
distortions through memory-guided adaptive instance normalization. The
processed features are then fused and passed to the original segmentation
pipeline. Extensive experiments on different benchmarks and baseline models
demonstrate that our approach significantly improves generalization to adverse
weather with minimal inference overhead, offering a practical and effective
solution for real-world LiDAR segmentation.

</details>


### [168] [Efficient Medical Vision-Language Alignment Through Adapting Masked Vision Models](https://arxiv.org/abs/2506.08990)
*Chenyu Lian,Hong-Yu Zhou,Dongyun Liang,Jing Qin,Liansheng Wang*

Main category: cs.CV

TL;DR: ALTA improves medical vision-language alignment by adapting pretrained vision models from masked record modeling, achieving better performance with fewer parameters and computational costs.


<details>
  <summary>Details</summary>
Motivation: Conventional cross-modal contrastive learning methods have suboptimal visual representation, while masked modeling excels in visual representation but struggles with cross-modal matching. ALTA aims to bridge this gap.

Method: ALTA adapts pretrained vision models from masked record modeling, using only 8% trainable parameters and <1/5 computational cost. It integrates temporal-multiview radiograph inputs for better alignment.

Result: ALTA outperforms counterparts by >4% in text-to-image accuracy and ~6% in image-to-text retrieval accuracy.

Conclusion: ALTA efficiently enhances vision-language alignment and understanding, with publicly available code.

Abstract: Medical vision-language alignment through cross-modal contrastive learning
shows promising performance in image-text matching tasks, such as retrieval and
zero-shot classification. However, conventional cross-modal contrastive
learning (CLIP-based) methods suffer from suboptimal visual representation
capabilities, which also limits their effectiveness in vision-language
alignment. In contrast, although the models pretrained via multimodal masked
modeling struggle with direct cross-modal matching, they excel in visual
representation. To address this contradiction, we propose ALTA (ALign Through
Adapting), an efficient medical vision-language alignment method that utilizes
only about 8% of the trainable parameters and less than 1/5 of the
computational consumption required for masked record modeling. ALTA achieves
superior performance in vision-language matching tasks like retrieval and
zero-shot classification by adapting the pretrained vision model from masked
record modeling. Additionally, we integrate temporal-multiview radiograph
inputs to enhance the information consistency between radiographs and their
corresponding descriptions in reports, further improving the vision-language
alignment. Experimental evaluations show that ALTA outperforms the
best-performing counterpart by over 4% absolute points in text-to-image
accuracy and approximately 6% absolute points in image-to-text retrieval
accuracy. The adaptation of vision-language models during efficient alignment
also promotes better vision and language understanding. Code is publicly
available at https://github.com/DopamineLcy/ALTA.

</details>


### [169] [Do Concept Replacement Techniques Really Erase Unacceptable Concepts?](https://arxiv.org/abs/2506.08991)
*Anudeep Das,Gurjot Singh,Prach Chantasantitam,N. Asokan*

Main category: cs.CV

TL;DR: The paper critiques current concept replacement techniques (CRTs) in diffusion models, showing their failure in image-to-image (I2I) scenarios, and introduces AntiMirror for better fidelity and effectiveness.


<details>
  <summary>Details</summary>
Motivation: Aligning generative models to avoid unacceptable content remains challenging, especially in emerging I2I settings where existing CRTs fail.

Method: Empirical demonstration of CRT failures in I2I models, analysis of fidelity gaps, and proposal of AntiMirror for targeted editing.

Result: Existing CRTs are ineffective in I2I scenarios, and AntiMirror proves viable for achieving both concept replacement and fidelity.

Conclusion: The study highlights the need for improved CRTs in I2I contexts and validates AntiMirror as a promising solution.

Abstract: Generative models, particularly diffusion-based text-to-image (T2I) models,
have demonstrated astounding success. However, aligning them to avoid
generating content with unacceptable concepts (e.g., offensive or copyrighted
content, or celebrity likenesses) remains a significant challenge. Concept
replacement techniques (CRTs) aim to address this challenge, often by trying to
"erase" unacceptable concepts from models. Recently, model providers have
started offering image editing services which accept an image and a text prompt
as input, to produce an image altered as specified by the prompt. These are
known as image-to-image (I2I) models. In this paper, we first use an I2I model
to empirically demonstrate that today's state-of-the-art CRTs do not in fact
erase unacceptable concepts. Existing CRTs are thus likely to be ineffective in
emerging I2I scenarios, despite their proven ability to remove unwanted
concepts in T2I pipelines, highlighting the need to understand this discrepancy
between T2I and I2I settings. Next, we argue that a good CRT, while replacing
unacceptable concepts, should preserve other concepts specified in the inputs
to generative models. We call this fidelity. Prior work on CRTs have neglected
fidelity in the case of unacceptable concepts. Finally, we propose the use of
targeted image-editing techniques to achieve both effectiveness and fidelity.
We present such a technique, AntiMirror, and demonstrate its viability.

</details>


### [170] [SDTagNet: Leveraging Text-Annotated Navigation Maps for Online HD Map Construction](https://arxiv.org/abs/2506.08997)
*Fabian Immel,Jan-Hendrik Pauls,Richard Fehler,Frank Bieder,Jonas Merkert,Christoph Stiller*

Main category: cs.CV

TL;DR: SDTagNet enhances online HD map construction by leveraging SD maps and NLP-derived features, improving far-range detection accuracy by up to 45%.


<details>
  <summary>Details</summary>
Motivation: High maintenance costs of HD maps and limited perception range of sensors drive the need for scalable solutions using SD maps.

Method: SDTagNet integrates polyline SD map data and textual annotations, using a point-level encoder and orthogonal identifiers for uniform element integration.

Result: Experiments show performance boosts of up to +5.9 mAP (45%) over no-prior methods and +3.2 mAP (20%) over prior SD map methods.

Conclusion: SDTagNet effectively utilizes SD maps and NLP to advance online HD map construction, offering significant accuracy improvements.

Abstract: Autonomous vehicles rely on detailed and accurate environmental information
to operate safely. High definition (HD) maps offer a promising solution, but
their high maintenance cost poses a significant barrier to scalable deployment.
This challenge is addressed by online HD map construction methods, which
generate local HD maps from live sensor data. However, these methods are
inherently limited by the short perception range of onboard sensors. To
overcome this limitation and improve general performance, recent approaches
have explored the use of standard definition (SD) maps as prior, which are
significantly easier to maintain. We propose SDTagNet, the first online HD map
construction method that fully utilizes the information of widely available SD
maps, like OpenStreetMap, to enhance far range detection accuracy. Our approach
introduces two key innovations. First, in contrast to previous work, we
incorporate not only polyline SD map data with manually selected classes, but
additional semantic information in the form of textual annotations. In this
way, we enrich SD vector map tokens with NLP-derived features, eliminating the
dependency on predefined specifications or exhaustive class taxonomies. Second,
we introduce a point-level SD map encoder together with orthogonal element
identifiers to uniformly integrate all types of map elements. Experiments on
Argoverse 2 and nuScenes show that this boosts map perception performance by up
to +5.9 mAP (+45%) w.r.t. map construction without priors and up to +3.2 mAP
(+20%) w.r.t. previous approaches that already use SD map priors. Code is
available at https://github.com/immel-f/SDTagNet

</details>


### [171] [Do MIL Models Transfer?](https://arxiv.org/abs/2506.09022)
*Daniel Shao,Richard J. Chen,Andrew H. Song,Joel Runevic,Ming Y. Lu,Tong Ding,Faisal Mahmood*

Main category: cs.CV

TL;DR: Pretrained MIL models outperform scratch-trained models in computational pathology, even across different organs, and generalize well with less data.


<details>
  <summary>Details</summary>
Motivation: To address the lack of understanding about MIL model transferability in computational pathology, especially with small datasets.

Method: Systematic evaluation of 11 pretrained MIL models across 21 tasks for morphological and molecular subtype prediction.

Result: Pretrained MIL models consistently outperform scratch-trained models and generalize well across organs and tasks.

Conclusion: Transfer learning with MIL models is highly effective in computational pathology, and a standardized resource is provided for future work.

Abstract: Multiple Instance Learning (MIL) is a cornerstone approach in computational
pathology (CPath) for generating clinically meaningful slide-level embeddings
from gigapixel tissue images. However, MIL often struggles with small, weakly
supervised clinical datasets. In contrast to fields such as NLP and
conventional computer vision, where transfer learning is widely used to address
data scarcity, the transferability of MIL models remains poorly understood. In
this study, we systematically evaluate the transfer learning capabilities of
pretrained MIL models by assessing 11 models across 21 pretraining tasks for
morphological and molecular subtype prediction. Our results show that
pretrained MIL models, even when trained on different organs than the target
task, consistently outperform models trained from scratch. Moreover,
pretraining on pancancer datasets enables strong generalization across organs
and tasks, outperforming slide foundation models while using substantially less
pretraining data. These findings highlight the robust adaptability of MIL
models and demonstrate the benefits of leveraging transfer learning to boost
performance in CPath. Lastly, we provide a resource which standardizes the
implementation of MIL models and collection of pretrained model weights on
popular CPath tasks, available at https://github.com/mahmoodlab/MIL-Lab

</details>


### [172] [DIsoN: Decentralized Isolation Networks for Out-of-Distribution Detection in Medical Imaging](https://arxiv.org/abs/2506.09024)
*Felix Wagner,Pramit Saha,Harry Anthony,J. Alison Noble,Konstantinos Kamnitsas*

Main category: cs.CV

TL;DR: The paper introduces Decentralized Isolation Networks (DIsoN) for out-of-distribution (OOD) detection in medical imaging, enabling secure comparison of test and training data without sharing raw data.


<details>
  <summary>Details</summary>
Motivation: Safe ML deployment in medical imaging requires OOD detection, but existing methods either discard training data or assume centralized storage, which is impractical due to size and privacy constraints.

Method: DIsoN quantifies separation difficulty between test samples and training data via binary classification, exchanging only model parameters to avoid data-sharing. Class-conditioning further refines comparisons.

Result: DIsoN outperforms existing methods on 12 OOD tasks across four medical imaging datasets while preserving data privacy.

Conclusion: DIsoN enables secure, decentralized OOD detection, offering a new service for ML developers to utilize training data remotely without compromising privacy.

Abstract: Safe deployment of machine learning (ML) models in safety-critical domains
such as medical imaging requires detecting inputs with characteristics not seen
during training, known as out-of-distribution (OOD) detection, to prevent
unreliable predictions. Effective OOD detection after deployment could benefit
from access to the training data, enabling direct comparison between test
samples and the training data distribution to identify differences.
State-of-the-art OOD detection methods, however, either discard training data
after deployment or assume that test samples and training data are centrally
stored together, an assumption that rarely holds in real-world settings. This
is because shipping training data with the deployed model is usually impossible
due to the size of training databases, as well as proprietary or privacy
constraints. We introduce the Isolation Network, an OOD detection framework
that quantifies the difficulty of separating a target test sample from the
training data by solving a binary classification task. We then propose
Decentralized Isolation Networks (DIsoN), which enables the comparison of
training and test data when data-sharing is impossible, by exchanging only
model parameters between the remote computational nodes of training and
deployment. We further extend DIsoN with class-conditioning, comparing a target
sample solely with training data of its predicted class. We evaluate DIsoN on
four medical imaging datasets (dermatology, chest X-ray, breast ultrasound,
histopathology) across 12 OOD detection tasks. DIsoN performs favorably against
existing methods while respecting data-privacy. This decentralized OOD
detection framework opens the way for a new type of service that ML developers
could provide along with their models: providing remote, secure utilization of
their training data for OOD detection services. Code will be available upon
acceptance at: *****

</details>


### [173] [Diffuse and Disperse: Image Generation with Representation Regularization](https://arxiv.org/abs/2506.09027)
*Runqian Wang,Kaiming He*

Main category: cs.CV

TL;DR: The paper introduces Dispersive Loss, a plug-and-play regularizer for diffusion-based generative models, improving performance without needing pre-training or extra parameters.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between diffusion models and representation learning by introducing a simple, effective regularizer.

Method: Proposes Dispersive Loss, which disperses internal representations in hidden space without requiring positive sample pairs.

Result: Consistent improvements on ImageNet across various models, outperforming baselines like REPA.

Conclusion: Dispersive Loss effectively enhances diffusion models, fostering closer ties between generative modeling and representation learning.

Abstract: The development of diffusion-based generative models over the past decade has
largely proceeded independently of progress in representation learning. These
diffusion models typically rely on regression-based objectives and generally
lack explicit regularization. In this work, we propose \textit{Dispersive
Loss}, a simple plug-and-play regularizer that effectively improves
diffusion-based generative models. Our loss function encourages internal
representations to disperse in the hidden space, analogous to contrastive
self-supervised learning, with the key distinction that it requires no positive
sample pairs and therefore does not interfere with the sampling process used
for regression. Compared to the recent method of representation alignment
(REPA), our approach is self-contained and minimalist, requiring no
pre-training, no additional parameters, and no external data. We evaluate
Dispersive Loss on the ImageNet dataset across a range of models and report
consistent improvements over widely used and strong baselines. We hope our work
will help bridge the gap between generative modeling and representation
learning.

</details>


### [174] [Princeton365: A Diverse Dataset with Accurate Camera Pose](https://arxiv.org/abs/2506.09035)
*Karhan Kayan,Stamatis Alexandropoulos,Rishabh Jain,Yiming Zuo,Erich Liang,Jia Deng*

Main category: cs.CV

TL;DR: Princeton365 is a diverse dataset of 365 videos with accurate camera pose, addressing gaps in SLAM benchmarks. It includes indoor, outdoor, and object scanning videos with synchronized RGB and IMU data, and introduces a scene scale-aware evaluation metric and a novel view synthesis benchmark.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between accuracy and diversity in SLAM benchmarks by providing a dataset with precise camera poses and diverse scenarios.

Method: Uses a ground truth collection framework with calibration boards and a 360-camera, along with synchronized monocular/stereo RGB and IMU data. Introduces a scene scale-aware metric for SLAM evaluation.

Result: A comprehensive dataset (Princeton365) with accurate camera poses, diverse scenarios, and new evaluation metrics for SLAM and novel view synthesis.

Conclusion: Princeton365 enhances SLAM research by offering diverse, high-quality data and improved evaluation tools, fostering better analysis of method failures and novel view synthesis challenges.

Abstract: We introduce Princeton365, a large-scale diverse dataset of 365 videos with
accurate camera pose. Our dataset bridges the gap between accuracy and data
diversity in current SLAM benchmarks by introducing a novel ground truth
collection framework that leverages calibration boards and a 360-camera. We
collect indoor, outdoor, and object scanning videos with synchronized monocular
and stereo RGB video outputs as well as IMU. We further propose a new scene
scale-aware evaluation metric for SLAM based on the the optical flow induced by
the camera pose estimation error. In contrast to the current metrics, our new
metric allows for comparison between the performance of SLAM methods across
scenes as opposed to existing metrics such as Average Trajectory Error (ATE),
allowing researchers to analyze the failure modes of their methods. We also
propose a challenging Novel View Synthesis benchmark that covers cases not
covered by current NVS benchmarks, such as fully non-Lambertian scenes with
360-degree camera trajectories. Please visit
https://princeton365.cs.princeton.edu for the dataset, code, videos, and
submission.

</details>


### [175] [Autoregressive Semantic Visual Reconstruction Helps VLMs Understand Better](https://arxiv.org/abs/2506.09040)
*Dianyi Wang,Wei Song,Yikun Wang,Siyuan Wang,Kaicheng Yu,Zhongyu Wei,Jiaqi Wang*

Main category: cs.CV

TL;DR: ASVR introduces autoregressive semantic visual reconstruction to enhance multimodal learning, improving performance across benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current LVLMs lack full visual modality integration, leading to limitations like incomplete visual detail utilization and vision-centric content representation.

Method: ASVR jointly learns visual and textual modalities by autoregressively reconstructing semantic image representations.

Result: ASVR improves LLaVA-1.5 by 5% across 14 benchmarks, showing stable gains with discrete semantic tokens.

Conclusion: Autoregressive semantic reconstruction enhances multimodal understanding, outperforming raw visual reconstruction.

Abstract: Typical large vision-language models (LVLMs) apply autoregressive supervision
solely to textual sequences, without fully incorporating the visual modality
into the learning process. This results in three key limitations: (1) an
inability to utilize images without accompanying captions, (2) the risk that
captions omit critical visual details, and (3) the challenge that certain
vision-centric content cannot be adequately conveyed through text. As a result,
current LVLMs often prioritize vision-to-language alignment while potentially
overlooking fine-grained visual information. While some prior works have
explored autoregressive image generation, effectively leveraging autoregressive
visual supervision to enhance image understanding remains an open challenge. In
this paper, we introduce Autoregressive Semantic Visual Reconstruction (ASVR),
which enables joint learning of visual and textual modalities within a unified
autoregressive framework. We show that autoregressively reconstructing the raw
visual appearance of images does not enhance and may even impair multimodal
understanding. In contrast, autoregressively reconstructing the semantic
representation of images consistently improves comprehension. Notably, we find
that even when models are given continuous image features as input, they can
effectively reconstruct discrete semantic tokens, resulting in stable and
consistent improvements across a wide range of multimodal understanding
benchmarks. Our approach delivers significant performance gains across varying
data scales (556k-2M) and types of LLM bacbones. Specifically, ASVR improves
LLaVA-1.5 by 5% in average scores across 14 multimodal benchmarks. The code is
available at https://github.com/AlenjandroWang/ASVR.

</details>


### [176] [Cosmos-Drive-Dreams: Scalable Synthetic Driving Data Generation with World Foundation Models](https://arxiv.org/abs/2506.09042)
*Xuanchi Ren,Yifan Lu,Tianshi Cao,Ruiyuan Gao,Shengyu Huang,Amirmojtaba Sabour,Tianchang Shen,Tobias Pfaff,Jay Zhangjie Wu,Runjian Chen,Seung Wook Kim,Jun Gao,Laura Leal-Taixe,Mike Chen,Sanja Fidler,Huan Ling*

Main category: cs.CV

TL;DR: The paper introduces Cosmos-Drive-Dreams, a synthetic data generation pipeline for AV systems, addressing the challenge of capturing rare edge cases in real-world data.


<details>
  <summary>Details</summary>
Motivation: Real-world data collection for AV systems is costly and struggles with rare edge cases, which are crucial for training and testing.

Method: The pipeline uses Cosmos-Drive, a suite of models based on NVIDIA Cosmos, to generate controllable, high-fidelity driving videos.

Result: The generated data improves dataset diversity, mitigates long-tail distribution issues, and enhances performance in tasks like 3D lane detection and object detection.

Conclusion: The open-sourced pipeline and tools provide a scalable solution for generating high-fidelity, challenging driving scenarios.

Abstract: Collecting and annotating real-world data for safety-critical physical AI
systems, such as Autonomous Vehicle (AV), is time-consuming and costly. It is
especially challenging to capture rare edge cases, which play a critical role
in training and testing of an AV system. To address this challenge, we
introduce the Cosmos-Drive-Dreams - a synthetic data generation (SDG) pipeline
that aims to generate challenging scenarios to facilitate downstream tasks such
as perception and driving policy training. Powering this pipeline is
Cosmos-Drive, a suite of models specialized from NVIDIA Cosmos world foundation
model for the driving domain and are capable of controllable, high-fidelity,
multi-view, and spatiotemporally consistent driving video generation. We
showcase the utility of these models by applying Cosmos-Drive-Dreams to scale
the quantity and diversity of driving datasets with high-fidelity and
challenging scenarios. Experimentally, we demonstrate that our generated data
helps in mitigating long-tail distribution problems and enhances generalization
in downstream tasks such as 3D lane detection, 3D object detection and driving
policy learning. We open source our pipeline toolkit, dataset and model weights
through the NVIDIA's Cosmos platform.
  Project page: https://research.nvidia.com/labs/toronto-ai/cosmos_drive_dreams

</details>


### [177] [MagCache: Fast Video Generation with Magnitude-Aware Cache](https://arxiv.org/abs/2506.09045)
*Zehong Ma,Longhui Wei,Feng Wang,Shiliang Zhang,Qi Tian*

Main category: cs.CV

TL;DR: MagCache, a novel method for accelerating video diffusion models, leverages a unified magnitude law to skip unimportant timesteps, achieving significant speedups without compromising visual quality.


<details>
  <summary>Details</summary>
Motivation: Existing acceleration techniques for video diffusion models rely on uniform heuristics or time-embedding variants, risking inconsistent outputs and requiring extensive calibration.

Method: MagCache uses a magnitude-aware approach to skip timesteps adaptively, requiring only a single sample for calibration.

Result: MagCache achieves 2.1x and 2.68x speedups on Open-Sora and Wan 2.1, respectively, with superior visual fidelity and outperforms existing methods in LPIPS, SSIM, and PSNR.

Conclusion: MagCache provides a robust and efficient solution for accelerating video diffusion models, outperforming existing methods with minimal calibration effort.

Abstract: Existing acceleration techniques for video diffusion models often rely on
uniform heuristics or time-embedding variants to skip timesteps and reuse
cached features. These approaches typically require extensive calibration with
curated prompts and risk inconsistent outputs due to prompt-specific
overfitting. In this paper, we introduce a novel and robust discovery: a
unified magnitude law observed across different models and prompts.
Specifically, the magnitude ratio of successive residual outputs decreases
monotonically and steadily in most timesteps while rapidly in the last several
steps. Leveraging this insight, we introduce a Magnitude-aware Cache (MagCache)
that adaptively skips unimportant timesteps using an error modeling mechanism
and adaptive caching strategy. Unlike existing methods requiring dozens of
curated samples for calibration, MagCache only requires a single sample for
calibration. Experimental results show that MagCache achieves 2.1x and 2.68x
speedups on Open-Sora and Wan 2.1, respectively, while preserving superior
visual fidelity. It significantly outperforms existing methods in LPIPS, SSIM,
and PSNR, under comparable computational budgets.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [178] [Modality-Balancing Preference Optimization of Large Multimodal Models by Adversarial Negative Mining](https://arxiv.org/abs/2506.08022)
*Chenxi Liu,Tianyi Xiong,Ruibo Chen,Yihan Wu,Junfeng Guo,Tianyi Zhou,Heng Huang*

Main category: cs.LG

TL;DR: MBPO addresses modality imbalance in LMMs by combining offline preference datasets with online response generation, improving performance and reducing hallucinations.


<details>
  <summary>Details</summary>
Motivation: LMMs suffer from modality imbalance, favoring language over vision, limiting generalization and causing hallucinations. Existing methods lack focus on LLM bias restraint and dynamic data adaptation.

Method: MBPO constructs offline datasets with hard negatives via adversarial image perturbations and uses online responses with verified rewards, training with GRPO.

Result: MBPO enhances LMM performance on vision-language tasks and reduces hallucinations.

Conclusion: MBPO effectively balances modalities in LMMs, improving generalization and reducing biases.

Abstract: The task adaptation and alignment of Large Multimodal Models (LMMs) have been
significantly advanced by instruction tuning and further strengthened by recent
preference optimization. Yet, most LMMs still suffer from severe modality
imbalance during reasoning, i.e., outweighing language prior biases over visual
inputs, which bottlenecks their generalization to downstream tasks and causes
hallucinations. However, existing preference optimization approaches for LMMs
do not focus on restraining the internal biases of their Large Language Model
(LLM) backbones when curating the training data. Moreover, they heavily rely on
offline data and lack the capacity to explore diverse responses adaptive to
dynamic distributional shifts during training. Meanwhile, Group Relative Policy
Optimization (GRPO), a recent method using online-generated data and verified
rewards to improve reasoning capabilities, remains largely underexplored in LMM
alignment. In this paper, we propose a novel preference learning framework,
Modality-Balancing Preference Optimization (MBPO), to address the modality
imbalance in LMMs. MBPO constructs a more effective offline preference dataset
by generating hard negatives, i.e., rejected responses misled by LLM biases due
to limited usage of visual information, through adversarial perturbation of
input images. Moreover, MBPO leverages the easy-to-verify nature of close-ended
tasks to generate online responses with verified rewards. GRPO is then employed
to train the model with offline-online hybrid data. Extensive experiments
demonstrate that MBPO can enhance LMM performance on challenging
vision-language tasks and effectively reduce hallucinations.

</details>


### [179] [Bingo: Boosting Efficient Reasoning of LLMs via Dynamic and Significance-based Reinforcement Learning](https://arxiv.org/abs/2506.08125)
*Hanbing Liu,Lang Cao,Yuanyi Ren,Mengyu Zhou,Haoyu Dong,Xiaojun Ma,Shi Han,Dongmei Zhang*

Main category: cs.LG

TL;DR: Bingo is an RL framework that improves reasoning efficiency in large language models by using significance-aware and dynamic length rewards, balancing accuracy and brevity.


<details>
  <summary>Details</summary>
Motivation: Address inefficiencies in LLM reasoning outputs, where existing RL methods focus on accuracy but neglect efficiency, and direct length-based rewards harm accuracy.

Method: Bingo introduces two mechanisms: significance-aware length reward (reduces insignificant tokens) and dynamic length reward (encourages detailed reasoning early, then decays for efficiency).

Result: Bingo outperforms baselines in accuracy and efficiency across benchmarks, achieving a better trade-off.

Conclusion: Training LLMs explicitly for efficient reasoning is promising, as demonstrated by Bingo's success.

Abstract: Large language models have demonstrated impressive reasoning capabilities,
yet they often suffer from inefficiencies due to unnecessarily verbose or
redundant outputs. While many works have explored reinforcement learning (RL)
to enhance reasoning abilities, most primarily focus on improving accuracy,
with limited attention to reasoning efficiency. Some existing approaches
introduce direct length-based rewards to encourage brevity, but this often
leads to noticeable drops in accuracy. In this paper, we propose Bingo, an RL
framework that advances length-based reward design to boost efficient
reasoning. Bingo incorporates two key mechanisms: a significance-aware length
reward, which gradually guides the model to reduce only insignificant tokens,
and a dynamic length reward, which initially encourages elaborate reasoning for
hard questions but decays over time to improve overall efficiency. Experiments
across multiple reasoning benchmarks show that Bingo improves both accuracy and
efficiency. It outperforms the vanilla reward and several other length-based
reward baselines in RL, achieving a favorable trade-off between accuracy and
efficiency. These results underscore the potential of training LLMs explicitly
for efficient reasoning.

</details>


### [180] [AutoSDT: Scaling Data-Driven Discovery Tasks Toward Open Co-Scientists](https://arxiv.org/abs/2506.08140)
*Yifei Li,Hanane Nour Moussa,Ziru Chen,Shijie Chen,Botao Yu,Mingyi Xue,Benjamin Burns,Tzu-Yao Chiu,Vishal Dey,Zitong Lu,Chen Wei,Qianheng Zhang,Tianyu Zhang,Song Gao,Xuhui Huang,Xia Ning,Nesreen K. Ahmed,Ali Payani,Huan Sun*

Main category: cs.LG

TL;DR: AutoSDT is an AI pipeline for creating high-quality coding tasks for scientific discovery, resulting in the AutoSDT-5K dataset and improved LLM performance.


<details>
  <summary>Details</summary>
Motivation: Addressing data scarcity in AI-driven scientific discovery by automating the collection of high-quality coding tasks.

Method: AutoSDT uses LLMs to source, validate, and synthesize coding tasks, creating the AutoSDT-5K dataset.

Result: AutoSDT-5K contains 5,404 tasks with high validity (93%) and correctness (92.2%). AutoSDT-Coder models show significant performance gains.

Conclusion: AutoSDT effectively tackles data scarcity, enabling better AI models for scientific discovery.

Abstract: Despite long-standing efforts in accelerating scientific discovery with AI,
building AI co-scientists remains challenging due to limited high-quality data
for training and evaluation. To tackle this data scarcity issue, we present
AutoSDT, an automatic pipeline that collects high-quality coding tasks in
real-world data-driven discovery workflows. AutoSDT leverages the coding
capabilities and parametric knowledge of LLMs to search for diverse sources,
select ecologically valid tasks, and synthesize accurate task instructions and
code solutions. Using our pipeline, we construct AutoSDT-5K, a dataset of 5,404
coding tasks for data-driven discovery that covers four scientific disciplines
and 756 unique Python packages. To the best of our knowledge, AutoSDT-5K is the
only automatically collected and the largest open dataset for data-driven
scientific discovery. Expert feedback on a subset of 256 tasks shows the
effectiveness of AutoSDT: 93% of the collected tasks are ecologically valid,
and 92.2% of the synthesized programs are functionally correct. Trained on
AutoSDT-5K, the Qwen2.5-Coder-Instruct LLM series, dubbed AutoSDT-Coder, show
substantial improvement on two challenging data-driven discovery benchmarks,
ScienceAgentBench and DiscoveryBench. Most notably, AutoSDT-Coder-32B reaches
the same level of performance as GPT-4o on ScienceAgentBench with a success
rate of 7.8%, doubling the performance of its base model. On DiscoveryBench, it
lifts the hypothesis matching score to 8.1, bringing a 17.4% relative
improvement and closing the gap between open-weight models and GPT-4o.

</details>


### [181] [Reinforcement Learning from Human Feedback with High-Confidence Safety Constraints](https://arxiv.org/abs/2506.08266)
*Yaswanth Chittepu,Blossom Metevier,Will Schwarzer,Austin Hoag,Scott Niekum,Philip S. Thomas*

Main category: cs.LG

TL;DR: HC-RLHF is a method for aligning language models with human preferences, ensuring high-confidence safety guarantees while maximizing helpfulness, outperforming previous methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods treat safety as a tradeoff against helpfulness, leading to unsafe responses in sensitive domains. HC-RLHF aims to provide reliable safety guarantees.

Method: HC-RLHF decouples human preferences into helpfulness (reward model) and harmlessness (cost model), optimizes under pessimistic constraints, and verifies safety via a test.

Result: HC-RLHF aligns three language models, improving harmlessness and helpfulness with high safety probability compared to prior methods.

Conclusion: HC-RLHF effectively balances safety and helpfulness, providing theoretical and empirical guarantees for reliable model alignment.

Abstract: Existing approaches to language model alignment often treat safety as a
tradeoff against helpfulness, which can lead to unacceptable responses in
sensitive domains. To ensure reliable performance in such settings, we propose
High-Confidence Safe Reinforcement Learning from Human Feedback (HC-RLHF), a
method that provides high-confidence safety guarantees while maximizing
helpfulness. Similar to previous methods, HC-RLHF explicitly decouples human
preferences into helpfulness and harmlessness (safety), which are learned by
training a reward model and a cost model, respectively. It then employs a
two-step process to find safe solutions. In the first step, it optimizes the
reward function under an intentionally pessimistic version of the cost
constraint. In the second step, the trained model undergoes a safety test to
verify whether its performance stays within an upper-confidence bound of the
actual cost constraint. We provide a theoretical analysis of HC-RLHF, including
proof that it will not return an unsafe solution with a probability greater
than a user-specified threshold. For our empirical analysis, we apply HC-RLHF
to align three different language models (Qwen2-1.5B, Qwen2.5-3B, and
LLaMa3.2-3B) with human preferences. Our results demonstrate that HC-RLHF
produces safe models with high probability and can improve harmlessness and
helpfulness compared to previous methods.

</details>


### [182] [From Debate to Equilibrium: Belief-Driven Multi-Agent LLM Reasoning via Bayesian Nash Equilibrium](https://arxiv.org/abs/2506.08292)
*Xie Yi,Zhanke Zhou,Chentao Cao,Qiyu Niu,Tongliang Liu,Bo Han*

Main category: cs.LG

TL;DR: ECON introduces a hierarchical reinforcement-learning paradigm for efficient multi-LLM coordination, achieving better performance and scalability with a proven tighter regret bound.


<details>
  <summary>Details</summary>
Motivation: To address the computational costs and lack of convergence guarantees in multi-agent LLM frameworks.

Method: Recasts multi-LLM coordination as an incomplete-information game, seeking Bayesian Nash Equilibrium (BNE), and uses hierarchical reinforcement learning for distributed reasoning and centralized output.

Result: Outperforms existing multi-LLM approaches by 11.2% on average across six benchmarks and demonstrates scalability.

Conclusion: ECON provides a scalable and efficient solution for multi-LLM coordination, with potential for larger ensembles.

Abstract: Multi-agent frameworks can substantially boost the reasoning power of large
language models (LLMs), but they typically incur heavy computational costs and
lack convergence guarantees. To overcome these challenges, we recast multi-LLM
coordination as an incomplete-information game and seek a Bayesian Nash
equilibrium (BNE), in which each agent optimally responds to its probabilistic
beliefs about the strategies of others. We introduce Efficient Coordination via
Nash Equilibrium (ECON), a hierarchical reinforcement-learning paradigm that
marries distributed reasoning with centralized final output. Under ECON, each
LLM independently selects responses that maximize its expected reward,
conditioned on its beliefs about co-agents, without requiring costly
inter-agent exchanges. We mathematically prove that ECON attains a markedly
tighter regret bound than non-equilibrium multi-agent schemes. Empirically,
ECON outperforms existing multi-LLM approaches by 11.2% on average across six
benchmarks spanning complex reasoning and planning tasks. Further experiments
demonstrate ECON's ability to flexibly incorporate additional models,
confirming its scalability and paving the way toward larger, more powerful
multi-LLM ensembles. The code is publicly available at:
https://github.com/tmlr-group/ECON.

</details>


### [183] [From Passive to Active Reasoning: Can Large Language Models Ask the Right Questions under Incomplete Information?](https://arxiv.org/abs/2506.08295)
*Zhanke Zhou,Xiao Feng,Zhaocheng Zhu,Jiangchao Yao,Sanmi Koyejo,Bo Han*

Main category: cs.LG

TL;DR: AR-Bench is a benchmark evaluating LLMs' active reasoning skills, revealing their struggles compared to passive reasoning and suggesting the need for improved methodologies.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks focus on passive reasoning, neglecting active reasoning where LLMs interact with external systems to gather missing information.

Method: AR-Bench includes three task families (detective cases, situation puzzles, guessing numbers) to simulate real-world scenarios and test commonsense, logical, and symbolic reasoning.

Result: Contemporary LLMs perform poorly in active reasoning, failing to acquire or use needed information, with minimal improvement from advanced strategies.

Conclusion: The study underscores the necessity for better active reasoning methodologies, such as interactive learning and real-time feedback, and makes AR-Bench publicly available.

Abstract: While existing benchmarks probe the reasoning abilities of large language
models (LLMs) across diverse domains, they predominantly assess passive
reasoning, providing models with all the information needed to reach a
solution. By contrast, active reasoning-where an LLM must interact with
external systems to acquire missing evidence or data-has received little
systematic attention. To address this shortfall, we present AR-Bench, a novel
benchmark designed explicitly to evaluate an LLM's active reasoning skills.
AR-Bench comprises three task families-detective cases, situation puzzles, and
guessing numbers-that together simulate real-world, agentic scenarios and
measure performance across commonsense, logical, and symbolic reasoning
challenges. Empirical evaluation on AR-Bench demonstrates that contemporary
LLMs exhibit pronounced difficulties with active reasoning: they frequently
fail to acquire or leverage the information needed to solve tasks. This gap
highlights a stark divergence between their passive and active reasoning
abilities. Moreover, ablation studies indicate that even advanced strategies,
such as tree-based searching or post-training approaches, yield only modest
gains and fall short of the levels required for real-world deployment.
Collectively, these findings highlight the critical need to advance methodology
for active reasoning, e.g., incorporating interactive learning, real-time
feedback loops, and environment-aware objectives for training. The benchmark is
publicly available at: https://github.com/tmlr-group/AR-Bench.

</details>


### [184] [Reinforce LLM Reasoning through Multi-Agent Reflection](https://arxiv.org/abs/2506.08379)
*Yurun Yuan,Tengyang Xie*

Main category: cs.LG

TL;DR: DPSDP, a reinforcement learning algorithm, improves LLM reasoning by modeling refinement as a Markov Decision Process, achieving better performance via multi-turn feedback and preference learning.


<details>
  <summary>Details</summary>
Motivation: Existing verify-and-improve methods for LLMs have limited feedback and lack coordination, leading to suboptimal performance.

Method: DPSDP trains an actor-critic LLM system using direct preference learning on self-generated data, modeled as a Markov Decision Process.

Result: DPSDP improves accuracy, e.g., from 58.2% to 63.2% on MATH 500 with Mistral-based models, and shows better generalization.

Conclusion: DPSDP effectively enhances LLM reasoning through multi-agent collaboration and dynamic refinement.

Abstract: Leveraging more test-time computation has proven to be an effective way to
boost the reasoning capabilities of large language models (LLMs). Among various
methods, the verify-and-improve paradigm stands out for enabling dynamic
solution exploration and feedback incorporation. However, existing approaches
often suffer from restricted feedback spaces and lack of coordinated training
of different parties, leading to suboptimal performance. To address this, we
model this multi-turn refinement process as a Markov Decision Process and
introduce DPSDP (Direct Policy Search by Dynamic Programming), a reinforcement
learning algorithm that trains an actor-critic LLM system to iteratively refine
answers via direct preference learning on self-generated data. Theoretically,
DPSDP can match the performance of any policy within the training distribution.
Empirically, we instantiate DPSDP with various base models and show
improvements on both in- and out-of-distribution benchmarks. For example, on
benchmark MATH 500, majority voting over five refinement steps increases
first-turn accuracy from 58.2% to 63.2% with Ministral-based models. An
ablation study further confirms the benefits of multi-agent collaboration and
out-of-distribution generalization.

</details>


### [185] [Reinforcement Learning Teachers of Test Time Scaling](https://arxiv.org/abs/2506.08388)
*Edoardo Cetin,Tianyu Zhao,Yujin Tang*

Main category: cs.LG

TL;DR: The paper introduces Reinforcement-Learned Teachers (RLTs), a framework for training reasoning LMs to generate effective explanations for downstream distillation, avoiding RL's exploration challenges.


<details>
  <summary>Details</summary>
Motivation: Traditional RL for reasoning LMs relies on initial exploration capability, and reasoning LMs are often used for teaching rather than deployment. RLTs address these limitations by focusing on distillation efficiency.

Method: RLTs are trained with dense rewards based on student understanding, using prompts of questions and solutions to generate tailored explanations.

Result: A 7B RLT outperforms larger LMs in distillation and cold-starting tasks, maintaining effectiveness for larger students and out-of-distribution tasks.

Conclusion: RLTs enhance efficiency and re-usability in RL reasoning frameworks, offering superior performance for teaching and distillation.

Abstract: Training reasoning language models (LMs) with reinforcement learning (RL) for
one-hot correctness inherently relies on the LM being able to explore and solve
its task with some chance at initialization. Furthermore, a key use case of
reasoning LMs is to act as teachers for distilling new students and
cold-starting future RL iterations rather than being deployed themselves. From
these considerations, we introduce a new framework that avoids RL's exploration
challenge by training a new class of Reinforcement-Learned Teachers (RLTs)
focused on yielding the most effective downstream distillation. RLTs are
prompted with both the question and solution to each problem, and tasked to
simply "connect-the-dots" with detailed explanations tailored for their
students. We train RLTs with dense rewards obtained by feeding each explanation
to the student and testing its understanding of the problem's solution. In
practice, the raw outputs of a 7B RLT provide higher final performance on
competition and graduate-level tasks than existing distillation and
cold-starting pipelines that collect and postprocess the reasoning traces of
orders of magnitude larger LMs. Furthermore, RLTs maintain their effectiveness
when training larger students and when applied zero-shot to out-of-distribution
tasks, unlocking new levels of efficiency and re-usability for the RL reasoning
framework.

</details>


### [186] [The Geometries of Truth Are Orthogonal Across Tasks](https://arxiv.org/abs/2506.08572)
*Waiss Azizian,Michael Kirchhof,Eugene Ndiaye,Louis Bethune,Michal Klein,Pierre Ablin,Marco Cuturi*

Main category: cs.LG

TL;DR: The paper highlights a limitation in using linear classifiers to detect correct answers from LLM activations, showing that these methods are task-dependent and fail to transfer across tasks.


<details>
  <summary>Details</summary>
Motivation: To address concerns about LLM reliability by examining the transferability of 'geometries of truth' across tasks.

Method: Analyze linear classifiers trained on LLM activations for different tasks, using sparsity-enforcing regularizers and more sophisticated approaches like mixtures of probes.

Result: Linear classifiers for distinct tasks share little similarity, and activation vectors form separated clusters across tasks, limiting transferability.

Conclusion: Current methods for detecting correct answers from activations are task-specific and not generalizable, suggesting a need for alternative approaches.

Abstract: Large Language Models (LLMs) have demonstrated impressive generalization
capabilities across various tasks, but their claim to practical relevance is
still mired by concerns on their reliability. Recent works have proposed
examining the activations produced by an LLM at inference time to assess
whether its answer to a question is correct. Some works claim that a "geometry
of truth" can be learned from examples, in the sense that the activations that
generate correct answers can be distinguished from those leading to mistakes
with a linear classifier. In this work, we underline a limitation of these
approaches: we observe that these "geometries of truth" are intrinsically
task-dependent and fail to transfer across tasks. More precisely, we show that
linear classifiers trained across distinct tasks share little similarity and,
when trained with sparsity-enforcing regularizers, have almost disjoint
supports. We show that more sophisticated approaches (e.g., using mixtures of
probes and tasks) fail to overcome this limitation, likely because activation
vectors commonly used to classify answers form clearly separated clusters when
examined across tasks.

</details>


### [187] [Gridding Forced Displacement using Semi-Supervised Learning](https://arxiv.org/abs/2506.08019)
*Andrew Wells,Geraldine Henningsen,Brice Bolane Tchinde Kengne*

Main category: cs.LG

TL;DR: A semi-supervised method disaggregates refugee statistics to 0.5-degree grid cells in 25 African countries, achieving 92.9% accuracy by integrating UNHCR data, satellite footprints, and OpenStreetMap coordinates.


<details>
  <summary>Details</summary>
Motivation: To uncover localized displacement patterns obscured in broader regional and national refugee statistics.

Method: Integrates UNHCR's ProGres data, Google Open Buildings satellite footprints, and OpenStreetMap coordinates using a label spreading algorithm.

Result: Achieves 92.9% accuracy in placing 10M+ refugee observations into grid cells, enabling high-resolution displacement analysis.

Conclusion: The high-resolution dataset enhances understanding of displacement drivers.

Abstract: We present a semi-supervised approach that disaggregates refugee statistics
from administrative boundaries to 0.5-degree grid cells across 25 Sub-Saharan
African countries. By integrating UNHCR's ProGres registration data with
satellite-derived building footprints from Google Open Buildings and location
coordinates from OpenStreetMap Populated Places, our label spreading algorithm
creates spatially explicit refugee statistics at high granularity.This
methodology achieves 92.9% average accuracy in placing over 10 million refugee
observations into appropriate grid cells, enabling the identification of
localized displacement patterns previously obscured in broader regional and
national statistics. The resulting high-resolution dataset provides a
foundation for a deeper understanding of displacement drivers.

</details>


### [188] [Bi-level Unbalanced Optimal Transport for Partial Domain Adaptation](https://arxiv.org/abs/2506.08020)
*Zi-Ying Chen,Chuan-Xian Ren,Hong Yan*

Main category: cs.LG

TL;DR: The paper proposes a Bi-level Unbalanced Optimal Transport (BUOT) model to address the Partial Domain Adaptation (PDA) problem by simultaneously characterizing sample-wise and class-wise relations for accurate knowledge transfer.


<details>
  <summary>Details</summary>
Motivation: The PDA problem involves aligning cross-domain samples while distinguishing outlier classes. Existing weighting frameworks are limited by empirical modeling of weights, which only captures sample-wise relations and is sensitive to inaccurate predictions.

Method: The BUOT model introduces a bi-level transport framework with a cooperation mechanism between sample-level and class-level transport. It uses label-aware transport cost for local structure and derives a fast computation formulation.

Result: Extensive experiments on benchmark datasets validate the competitiveness of BUOT.

Conclusion: BUOT effectively addresses PDA by unifying sample-wise and class-wise relations, improving outlier identification and alignment efficiency.

Abstract: Partial domain adaptation (PDA) problem requires aligning cross-domain
samples while distinguishing the outlier classes for accurate knowledge
transfer. The widely used weighting framework tries to address the outlier
classes by introducing the reweighed source domain with a similar label
distribution to the target domain. However, the empirical modeling of weights
can only characterize the sample-wise relations, which leads to insufficient
exploration of cluster structures, and the weights could be sensitive to the
inaccurate prediction and cause confusion on the outlier classes. To tackle
these issues, we propose a Bi-level Unbalanced Optimal Transport (BUOT) model
to simultaneously characterize the sample-wise and class-wise relations in a
unified transport framework. Specifically, a cooperation mechanism between
sample-level and class-level transport is introduced, where the sample-level
transport provides essential structure information for the class-level
knowledge transfer, while the class-level transport supplies discriminative
information for the outlier identification. The bi-level transport plan
provides guidance for the alignment process. By incorporating the label-aware
transport cost, the local transport structure is ensured and a fast computation
formulation is derived to improve the efficiency. Extensive experiments on
benchmark datasets validate the competitiveness of BUOT.

</details>


### [189] [SwS: Self-aware Weakness-driven Problem Synthesis in Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2506.08989)
*Xiao Liang,Zhong-Zhi Li,Yeyun Gong,Yang Wang,Hengyuan Zhang,Yelong Shen,Ying Nian Wu,Weizhu Chen*

Main category: cs.LG

TL;DR: The paper introduces SwS, a framework for synthesizing problems to address model weaknesses in RLVR, improving performance on reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Existing problem sets for RLVR lack quality and relevance, limiting effectiveness. SwS aims to enhance model training by targeting weaknesses.

Method: SwS identifies model weaknesses from failure cases, extracts core concepts, and synthesizes new problems for augmented training.

Result: SwS improves performance by 10.0% and 7.7% on 7B and 32B models across reasoning benchmarks.

Conclusion: SwS enables models to self-identify and address weaknesses, enhancing generalization without external distillation.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective
for training large language models (LLMs) on complex reasoning tasks, such as
mathematical problem solving. A prerequisite for the scalability of RLVR is a
high-quality problem set with precise and verifiable answers. However, the
scarcity of well-crafted human-labeled math problems and limited-verification
answers in existing distillation-oriented synthetic datasets limit their
effectiveness in RL. Additionally, most problem synthesis strategies
indiscriminately expand the problem set without considering the model's
capabilities, leading to low efficiency in generating useful questions. To
mitigate this issue, we introduce a Self-aware Weakness-driven problem
Synthesis framework (SwS) that systematically identifies model deficiencies and
leverages them for problem augmentation. Specifically, we define weaknesses as
questions that the model consistently fails to learn through its iterative
sampling during RL training. We then extract the core concepts from these
failure cases and synthesize new problems to strengthen the model's weak areas
in subsequent augmented training, enabling it to focus on and gradually
overcome its weaknesses. Without relying on external knowledge distillation,
our framework enables robust generalization byempowering the model to
self-identify and address its weaknesses in RL, yielding average performance
gains of 10.0% and 7.7% on 7B and 32B models across eight mainstream reasoning
benchmarks.

</details>


### [190] [e3: Learning to Explore Enables Extrapolation of Test-Time Compute for LLMs](https://arxiv.org/abs/2506.09026)
*Amrith Setlur,Matthew Y. R. Yang,Charlie Snell,Jeremy Greer,Ian Wu,Virginia Smith,Max Simchowitz,Aviral Kumar*

Main category: cs.LG

TL;DR: The paper introduces a method (e3) to improve LLM reasoning by enabling in-context exploration, allowing models to extrapolate beyond their training token budget.


<details>
  <summary>Details</summary>
Motivation: Current reasoning models fail to extrapolate well, limiting their performance on harder problems with longer inference times.

Method: The e3 recipe involves (1) chaining asymmetric skills, (2) leveraging negative gradients for exploration, and (3) a curriculum to align task difficulty with token budget.

Result: The e3-1.7B model achieves top scores on benchmarks (AIME'25, HMMT'25) and extrapolates to 2x the training token budget, improving pass@1 and pass@k.

Conclusion: In-context exploration (e3) effectively enhances LLM reasoning and extrapolation, outperforming existing models.

Abstract: Test-time scaling offers a promising path to improve LLM reasoning by
utilizing more compute at inference time; however, the true promise of this
paradigm lies in extrapolation (i.e., improvement in performance on hard
problems as LLMs keep "thinking" for longer, beyond the maximum token budget
they were trained on). Surprisingly, we find that most existing reasoning
models do not extrapolate well. We show that one way to enable extrapolation is
by training the LLM to perform in-context exploration: training the LLM to
effectively spend its test time budget by chaining operations (such as
generation, verification, refinement, etc.), or testing multiple hypotheses
before it commits to an answer. To enable in-context exploration, we identify
three key ingredients as part of our recipe e3: (1) chaining skills that the
base LLM has asymmetric competence in, e.g., chaining verification (easy) with
generation (hard), as a way to implement in-context search; (2) leveraging
"negative" gradients from incorrect traces to amplify exploration during RL,
resulting in longer search traces that chains additional asymmetries; and (3)
coupling task difficulty with training token budget during training via a
specifically-designed curriculum to structure in-context exploration. Our
recipe e3 produces the best known 1.7B model according to AIME'25 and HMMT'25
scores, and extrapolates to 2x the training token budget. Our e3-1.7B model not
only attains high pass@1 scores, but also improves pass@k over the base model.

</details>


### [191] [UniVarFL: Uniformity and Variance Regularized Federated Learning for Heterogeneous Data](https://arxiv.org/abs/2506.08167)
*Sunny Gupta,Nikita Jangid,Amit Sethi*

Main category: cs.LG

TL;DR: UniVarFL is a federated learning framework addressing non-IID data issues by using two regularization strategies to improve model performance and scalability.


<details>
  <summary>Details</summary>
Motivation: Federated Learning (FL) struggles with non-IID data due to local classifier bias, and existing solutions are costly or ineffective.

Method: UniVarFL employs Classifier Variance Regularization and Hyperspherical Uniformity Regularization during local training to mimic IID conditions.

Result: UniVarFL achieves higher accuracy than existing methods on benchmark datasets, proving its scalability and efficiency.

Conclusion: UniVarFL is a promising solution for real-world FL deployments, especially in resource-constrained settings.

Abstract: Federated Learning (FL) often suffers from severe performance degradation
when faced with non-IID data, largely due to local classifier bias. Traditional
remedies such as global model regularization or layer freezing either incur
high computational costs or struggle to adapt to feature shifts. In this work,
we propose UniVarFL, a novel FL framework that emulates IID-like training
dynamics directly at the client level, eliminating the need for global model
dependency. UniVarFL leverages two complementary regularization strategies
during local training: Classifier Variance Regularization, which aligns
class-wise probability distributions with those expected under IID conditions,
effectively mitigating local classifier bias; and Hyperspherical Uniformity
Regularization, which encourages a uniform distribution of feature
representations across the hypersphere, thereby enhancing the model's ability
to generalize under diverse data distributions. Extensive experiments on
multiple benchmark datasets demonstrate that UniVarFL outperforms existing
methods in accuracy, highlighting its potential as a highly scalable and
efficient solution for real-world FL deployments, especially in
resource-constrained settings. Code: https://github.com/sunnyinAI/UniVarFL

</details>


### [192] [An Adaptive Method Stabilizing Activations for Enhanced Generalization](https://arxiv.org/abs/2506.08353)
*Hyunseok Seung,Jaewoo Lee,Hyunsuk Ko*

Main category: cs.LG

TL;DR: AdaAct is an optimization algorithm that adjusts learning rates based on activation variance, improving stability and generalization in neural networks.


<details>
  <summary>Details</summary>
Motivation: To enhance neuron output stability and generalization by incorporating neuron-wise adaptivity, bridging the gap between Adam's convergence speed and SGD's generalization.

Method: AdaAct adjusts learning rates according to activation variance during training, complementing conventional activation regularization.

Result: Competitive performance on CIFAR and ImageNet benchmarks, balancing convergence speed and generalization while maintaining efficiency.

Conclusion: AdaAct effectively combines the strengths of Adam and SGD, offering a practical solution for training neural networks.

Abstract: We introduce AdaAct, a novel optimization algorithm that adjusts learning
rates according to activation variance. Our method enhances the stability of
neuron outputs by incorporating neuron-wise adaptivity during the training
process, which subsequently leads to better generalization -- a complementary
approach to conventional activation regularization methods. Experimental
results demonstrate AdaAct's competitive performance across standard image
classification benchmarks. We evaluate AdaAct on CIFAR and ImageNet, comparing
it with other state-of-the-art methods. Importantly, AdaAct effectively bridges
the gap between the convergence speed of Adam and the strong generalization
capabilities of SGD, all while maintaining competitive execution times. Code is
available at https://github.com/hseung88/adaact.

</details>


### [193] [Boosting Gradient Leakage Attacks: Data Reconstruction in Realistic FL Settings](https://arxiv.org/abs/2506.08435)
*Mingyuan Fan,Fuyi Wang,Cen Chen,Jianying Zhou*

Main category: cs.LG

TL;DR: The paper demonstrates that gradient leakage attacks (GLAs) can effectively reconstruct client data in realistic federated learning (FL) environments, challenging prior beliefs about FL's privacy safeguards. It introduces FedLeak, a method with partial gradient matching and gradient regularization, to address GLA limitations.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in understanding the practical risks of GLAs in FL, showing that privacy vulnerabilities persist even in realistic settings.

Method: Develops FedLeak with partial gradient matching and gradient regularization to improve GLA performance. Introduces a practical evaluation protocol for real-world FL environments.

Result: FedLeak achieves high-fidelity data reconstruction under realistic conditions, highlighting FL's privacy vulnerabilities.

Conclusion: The study underscores the urgent need for more effective defense methods in FL systems due to demonstrated privacy risks.

Abstract: Federated learning (FL) enables collaborative model training among multiple
clients without the need to expose raw data. Its ability to safeguard privacy,
at the heart of FL, has recently been a hot-button debate topic. To elaborate,
several studies have introduced a type of attacks known as gradient leakage
attacks (GLAs), which exploit the gradients shared during training to
reconstruct clients' raw data. On the flip side, some literature, however,
contends no substantial privacy risk in practical FL environments due to the
effectiveness of such GLAs being limited to overly relaxed conditions, such as
small batch sizes and knowledge of clients' data distributions.
  This paper bridges this critical gap by empirically demonstrating that
clients' data can still be effectively reconstructed, even within realistic FL
environments. Upon revisiting GLAs, we recognize that their performance
failures stem from their inability to handle the gradient matching problem. To
alleviate the performance bottlenecks identified above, we develop FedLeak,
which introduces two novel techniques, partial gradient matching and gradient
regularization. Moreover, to evaluate the performance of FedLeak in real-world
FL environments, we formulate a practical evaluation protocol grounded in a
thorough review of extensive FL literature and industry practices. Under this
protocol, FedLeak can still achieve high-fidelity data reconstruction, thereby
underscoring the significant vulnerability in FL systems and the urgent need
for more effective defense methods.

</details>


### [194] [HSG-12M: A Large-Scale Spatial Multigraph Dataset](https://arxiv.org/abs/2506.08618)
*Xianquan Yan,Hakan Akgn,Kenji Kawaguchi,N. Duane Loh,Ching Hua Lee*

Main category: cs.LG

TL;DR: HSG-12M is the first large-scale dataset of spatial multigraphs, retaining distinct paths as separate edges, derived from spectral potential data. It introduces challenges for GNNs and links algebra to graphs.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks oversimplify graphs by collapsing distinct paths into single links, lacking spatial and geometric diversity.

Method: HSG-12M uses Hamiltonian spectral graphs from 1-D crystal energy spectra, processed via the Poly2Graph pipeline.

Result: The dataset includes 11.6M static and 5.1M dynamic graphs across 1401 classes, revealing challenges for GNNs in multi-edge geometry.

Conclusion: HSG-12M enables geometry-aware graph learning and bridges algebra to graphs, opening new avenues for scientific discovery.

Abstract: Existing graph benchmarks assume non-spatial, simple edges, collapsing
physically distinct paths into a single link. We introduce HSG-12M, the first
large-scale dataset of $\textbf{spatial multigraphs}-$graphs embedded in a
metric space where multiple geometrically distinct trajectories between two
nodes are retained as separate edges. HSG-12M contains 11.6 million static and
5.1 million dynamic $\textit{Hamiltonian spectral graphs}$ across 1401
characteristic-polynomial classes, derived from 177 TB of spectral potential
data. Each graph encodes the full geometry of a 1-D crystal's energy spectrum
on the complex plane, producing diverse, physics-grounded topologies that
transcend conventional node-coordinate datasets. To enable future extensions,
we release $\texttt{Poly2Graph}$: a high-performance, open-source pipeline that
maps arbitrary 1-D crystal Hamiltonians to spectral graphs. Benchmarks with
popular GNNs expose new challenges in learning from multi-edge geometry at
scale. Beyond its practical utility, we show that spectral graphs serve as
universal topological fingerprints of polynomials, vectors, and matrices,
forging a new algebra-to-graph link. HSG-12M lays the groundwork for
geometry-aware graph learning and new opportunities of data-driven scientific
discovery in condensed matter physics and beyond.

</details>


### [195] [Time Series Representations for Classification Lie Hidden in Pretrained Vision Transformers](https://arxiv.org/abs/2506.08641)
*Simon Roschmann,Quentin Bouniot,Vasilii Feofanov,Ievgen Redko,Zeynep Akata*

Main category: cs.LG

TL;DR: TiViT converts time series into images to use pretrained Vision Transformers, achieving state-of-the-art performance in classification by leveraging OpenCLIP models and complementing TSFMs.


<details>
  <summary>Details</summary>
Motivation: Addressing the scarcity of time series datasets by reusing vision models for time series classification.

Method: Convert time series into images and use frozen Vision Transformers (ViTs) pretrained on image datasets, analyzing 2D patching and leveraging OpenCLIP models.

Result: TiViT achieves state-of-the-art performance, with intermediate layers of high intrinsic dimension being most effective. Combining TiViT and TSFMs further improves results.

Conclusion: TiViT demonstrates the potential of reusing vision representations for non-visual tasks like time series classification, revealing a complementary relationship with TSFMs.

Abstract: Time series classification is a fundamental task in healthcare and industry,
yet the development of time series foundation models (TSFMs) remains limited by
the scarcity of publicly available time series datasets. In this work, we
propose Time Vision Transformer (TiViT), a framework that converts time series
into images to leverage the representational power of frozen Vision
Transformers (ViTs) pretrained on large-scale image datasets. First, we
theoretically motivate our approach by analyzing the 2D patching of ViTs for
time series, showing that it can increase the number of label-relevant tokens
and reduce the sample complexity. Second, we empirically demonstrate that TiViT
achieves state-of-the-art performance on standard time series classification
benchmarks by utilizing the hidden representations of large OpenCLIP models. We
explore the structure of TiViT representations and find that intermediate
layers with high intrinsic dimension are the most effective for time series
classification. Finally, we assess the alignment between TiViT and TSFM
representation spaces and identify a strong complementarity, with further
performance gains achieved by combining their features. Our findings reveal yet
another direction for reusing vision representations in a non-visual domain.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [196] [Approaching Dialogue State Tracking via Aligning Speech Encoders and LLMs](https://arxiv.org/abs/2506.08633)
*imon Sedlek,Bolaji Yusuf,Jn vec,Pradyoth Hegde,Santosh Kesiraju,Oldich Plchot,Jan ernock*

Main category: eess.AS

TL;DR: The paper introduces a method for spoken Dialogue State Tracking (DST) by connecting speech encoders and LLMs via a small module, achieving state-of-the-art results on SpokenWOZ.


<details>
  <summary>Details</summary>
Motivation: To bridge the representation gaps between speech encoders and LLMs for improved DST performance using open-sourced tools.

Method: Uses a connector module between WavLM-large and OLMo, explores fine-tuning (full/LoRA), agent turns, and fuzzy matching post-processing.

Result: Best model (WavLM + connector + OLMo-1B) achieves 34.66% JGA on SpokenWOZ; Gemma-2-9B-instruct reaches 42.17%.

Conclusion: The proposed method effectively improves DST performance, especially for named entities, and sets new benchmarks.

Abstract: In this work, we approach spoken Dialogue State Tracking (DST) by bridging
the representation spaces of speech encoders and LLMs via a small connector
module, with a focus on fully open-sourced and open-data components
(WavLM-large, OLMo). We focus on ablating different aspects of such systems
including full/LoRA adapter fine-tuning, the effect of agent turns in the
dialogue history, as well as fuzzy matching-based output post-processing, which
greatly improves performance of our systems on named entities in the dialogue
slot values. We conduct our experiments on the SpokenWOZ dataset, and
additionally utilize the Speech-Aware MultiWOZ dataset to augment our training
data. Ultimately, our best-performing WavLM + connector + OLMo-1B aligned
models achieve state of the art on the SpokenWOZ test set (34.66% JGA), and our
system with Gemma-2-9B-instruct further surpasses this result, reaching 42.17%
JGA on SpokenWOZ test.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [197] [GradEscape: A Gradient-Based Evader Against AI-Generated Text Detectors](https://arxiv.org/abs/2506.08188)
*Wenlong Meng,Shuguo Fan,Chengkun Wei,Min Chen,Yuwei Li,Yuanchao Zhang,Zhikun Zhang,Wenzhi Chen*

Main category: cs.CR

TL;DR: GradEscape is a gradient-based evader for AI-generated text detectors, achieving high attack success with minimal modifications and adapting to various detectors.


<details>
  <summary>Details</summary>
Motivation: To address the undifferentiable computation problem in text and tokenizer mismatch, enabling effective evasion of AIGT detectors.

Method: Uses weighted embeddings, warm-started evader, tokenizer inference, and model extraction techniques to update evader parameters.

Result: Outperforms state-of-the-art evaders, works with large models, and successfully evades commercial detectors.

Conclusion: Identifies training data disparity as a vulnerability and proposes a defense strategy, open-sourcing GradEscape for robust detector development.

Abstract: In this paper, we introduce GradEscape, the first gradient-based evader
designed to attack AI-generated text (AIGT) detectors. GradEscape overcomes the
undifferentiable computation problem, caused by the discrete nature of text, by
introducing a novel approach to construct weighted embeddings for the detector
input. It then updates the evader model parameters using feedback from victim
detectors, achieving high attack success with minimal text modification. To
address the issue of tokenizer mismatch between the evader and the detector, we
introduce a warm-started evader method, enabling GradEscape to adapt to
detectors across any language model architecture. Moreover, we employ novel
tokenizer inference and model extraction techniques, facilitating effective
evasion even in query-only access.
  We evaluate GradEscape on four datasets and three widely-used language
models, benchmarking it against four state-of-the-art AIGT evaders.
Experimental results demonstrate that GradEscape outperforms existing evaders
in various scenarios, including with an 11B paraphrase model, while utilizing
only 139M parameters. We have successfully applied GradEscape to two real-world
commercial AIGT detectors. Our analysis reveals that the primary vulnerability
stems from disparity in text expression styles within the training data. We
also propose a potential defense strategy to mitigate the threat of AIGT
evaders. We open-source our GradEscape for developing more robust AIGT
detectors.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [198] [EDINET-Bench: Evaluating LLMs on Complex Financial Tasks using Japanese Financial Statements](https://arxiv.org/abs/2506.08762)
*Issa Sugiura,Takashi Ishida,Taro Makino,Chieko Tazuke,Takanori Nakagawa,Kosuke Nakago,David Ha*

Main category: q-fin.ST

TL;DR: The paper introduces EDINET-Bench, an open-source Japanese financial benchmark to evaluate LLMs on tasks like fraud detection and earnings forecasting, revealing their limitations in financial applications.


<details>
  <summary>Details</summary>
Motivation: The scarcity of challenging financial datasets, especially for Japanese data, hinders LLM development and evaluation in financial analytics.

Method: EDINET-Bench is created using 10 years of Japanese annual reports from EDINET, with automated labeling for tasks like fraud detection and earnings forecasting.

Result: State-of-the-art LLMs perform only slightly better than logistic regression in binary classification tasks, indicating challenges in financial applications.

Conclusion: The study highlights the need for domain-specific adaptation of LLMs in finance and provides open resources to support future research.

Abstract: Financial analysis presents complex challenges that could leverage large
language model (LLM) capabilities. However, the scarcity of challenging
financial datasets, particularly for Japanese financial data, impedes academic
innovation in financial analytics. As LLMs advance, this lack of accessible
research resources increasingly hinders their development and evaluation in
this specialized domain. To address this gap, we introduce EDINET-Bench, an
open-source Japanese financial benchmark designed to evaluate the performance
of LLMs on challenging financial tasks including accounting fraud detection,
earnings forecasting, and industry prediction. EDINET-Bench is constructed by
downloading annual reports from the past 10 years from Japan's Electronic
Disclosure for Investors' NETwork (EDINET) and automatically assigning labels
corresponding to each evaluation task. Our experiments reveal that even
state-of-the-art LLMs struggle, performing only slightly better than logistic
regression in binary classification for fraud detection and earnings
forecasting. These results highlight significant challenges in applying LLMs to
real-world financial applications and underscore the need for domain-specific
adaptation. Our dataset, benchmark construction code, and evaluation code is
publicly available to facilitate future research in finance with LLMs.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [199] [Surgeons Awareness, Expectations, and Involvement with Artificial Intelligence: a Survey Pre and Post the GPT Era](https://arxiv.org/abs/2506.08258)
*Lorenzo Arboit,Dennis N. Schneider,Toby Collins,Daniel A. Hashimoto,Silvana Perretta,Bernard Dallemagne,Jacques Marescaux,EAES Working Group,Nicolas Padoy,Pietro Mascagni*

Main category: cs.CY

TL;DR: The study explores surgeons' evolving awareness, expectations, and involvement with AI in surgery through global surveys in 2021 and 2024, highlighting increased awareness but persistent knowledge gaps and infrastructural challenges.


<details>
  <summary>Details</summary>
Motivation: To understand how surgeons' perceptions of AI in surgery have changed over time, particularly with the rise of generative AI like ChatGPT, and to identify barriers and opportunities for AI adoption.

Method: Comparative cross-sectional surveys distributed globally in 2021 (522 responses) and 2024 (149 responses), assessing demographics, AI awareness, expectations, involvement, and ethics.

Result: Awareness and course attendance increased, but foundational AI knowledge remained limited. Ethical concerns rose, and infrastructure was the main obstacle. Optimism for AI's impact on surgery was high, with 96.6% willing to integrate AI.

Conclusion: Surgeons' perceptions of AI are evolving positively, but education, ethical frameworks, and infrastructure development are needed to address gaps and challenges for successful AI adoption.

Abstract: Artificial Intelligence (AI) is transforming medicine, with generative AI
models like ChatGPT reshaping perceptions of its potential. This study examines
surgeons' awareness, expectations, and involvement with AI in surgery through
comparative surveys conducted in 2021 and 2024. Two cross-sectional surveys
were distributed globally in 2021 and 2024, the first before an IRCAD webinar
and the second during the annual EAES meeting. The surveys assessed
demographics, AI awareness, expectations, involvement, and ethics (2024 only).
The surveys collected a total of 671 responses from 98 countries, 522 in 2021
and 149 in 2024. Awareness of AI courses rose from 14.5% in 2021 to 44.6% in
2024, while course attendance increased from 12.9% to 23%. Despite this,
familiarity with foundational AI concepts remained limited. Expectations for
AI's role shifted in 2024, with hospital management gaining relevance. Ethical
concerns gained prominence, with 87.2% of 2024 participants emphasizing
accountability and transparency. Infrastructure limitations remained the
primary obstacle to implementation. Interdisciplinary collaboration and
structured training were identified as critical for successful AI adoption.
Optimism about AI's transformative potential remained high, with 79.9% of
respondents believing AI would positively impact surgery and 96.6% willing to
integrate AI into their clinical practice. Surgeons' perceptions of AI are
evolving, driven by the rise of generative AI and advancements in surgical data
science. While enthusiasm for integration is strong, knowledge gaps and
infrastructural challenges persist. Addressing these through education, ethical
frameworks, and infrastructure development is essential.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [200] [RADAR: Benchmarking Language Models on Imperfect Tabular Data](https://arxiv.org/abs/2506.08249)
*Ken Gu,Zhihan Zhang,Kate Lin,Yuwei Zhang,Akshay Paruchuri,Hong Yu,Mehran Kazemi,Kumar Ayush,A. Ali Heydari,Maxwell A. Xu,Girish Narayanswamy,Yun Liu,Ming-Zher Poh,Yuzhe Yang,Mark Malhotra,Shwetak Patel,Hamid Palangi,Xuhai Xu,Daniel McDuff,Tim Althoff,Xin Liu*

Main category: cs.DB

TL;DR: RADAR is a benchmark for evaluating data-aware reasoning in language models (LMs) on tabular data, revealing significant performance gaps when handling data artifacts.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored data awareness of LMs in handling real-world tabular data artifacts like missing values and outliers, which can compromise analytical validity.

Method: Developed RADAR, a benchmark with 2980 table-query pairs across 9 domains, using programmatic perturbations to simulate data artifacts and evaluate model behavior.

Result: Frontier models perform well on artifact-free tables but degrade significantly when artifacts are introduced, highlighting robustness gaps.

Conclusion: RADAR provides a flexible, extensible resource for advancing robust tabular reasoning in LMs.

Abstract: Language models (LMs) are increasingly being deployed to perform autonomous
data analyses. However, their data awareness -- the ability to recognize,
reason over, and appropriately handle data artifacts such as missing values,
outliers, and logical inconsistencies -- remains underexplored. These artifacts
are especially common in real-world tabular data and, if mishandled, can
significantly compromise the validity of analytical conclusions. To address
this gap, we present RADAR, a benchmark for systematically evaluating
data-aware reasoning on tabular data. We develop a framework to simulate data
artifacts via programmatic perturbations to enable targeted evaluation of model
behavior. RADAR comprises 2980 table query pairs, grounded in real-world data
spanning 9 domains and 5 data artifact types. In addition to evaluating
artifact handling, RADAR systematically varies table size to study how
reasoning performance holds when increasing table size. Our evaluation reveals
that, despite decent performance on tables without data artifacts, frontier
models degrade significantly when data artifacts are introduced, exposing
critical gaps in their capacity for robust, data-aware analysis. Designed to be
flexible and extensible, RADAR supports diverse perturbation types and
controllable table sizes, offering a valuable resource for advancing tabular
reasoning.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [201] [Instruction-Tuned Video-Audio Models Elucidate Functional Specialization in the Brain](https://arxiv.org/abs/2506.08277)
*Subba Reddy Oota,Khushbu Pahwa,Prachi Jindal,Satya Sai Srinath Namburi,Maneesh Singh,Tanmoy Chakraborty,Bapi S. Raju,Manish Gupta*

Main category: q-bio.NC

TL;DR: Instruction-tuned MLLMs outperform non-instruction-tuned models in brain alignment for multimodal stimuli, showing hierarchical alignment with brain regions.


<details>
  <summary>Details</summary>
Motivation: To address the gap in evaluating brain alignment of MLLMs in multimodal settings, especially with instruction-tuned models.

Method: Used instruction-specific embeddings from six video and two audio instruction-tuned MLLMs, tested on 13 video task-specific instructions.

Result: Instruction-tuned MLLMs outperformed non-instruction-tuned models by 15-20% and showed hierarchical brain alignment.

Conclusion: Task-specific instructions improve brain-MLLM alignment, offering insights into multimodal brain processing and potential for future research.

Abstract: Recent voxel-wise multimodal brain encoding studies have shown that
multimodal large language models (MLLMs) exhibit a higher degree of brain
alignment compared to unimodal models in both unimodal and multimodal stimulus
settings. More recently, instruction-tuned multimodal models have shown to
generate task-specific representations that align strongly with brain activity.
However, prior work evaluating the brain alignment of MLLMs has primarily
focused on unimodal settings or relied on non-instruction-tuned multimodal
models for multimodal stimuli. To address this gap, we investigated brain
alignment, that is, measuring the degree of predictivity of neural activity
recorded while participants were watching naturalistic movies (video along with
audio) with representations derived from MLLMs. We utilized
instruction-specific embeddings from six video and two audio instruction-tuned
MLLMs. Experiments with 13 video task-specific instructions show that
instruction-tuned video MLLMs significantly outperform non-instruction-tuned
multimodal (by 15%) and unimodal models (by 20%). Our evaluation of MLLMs for
both video and audio tasks using language-guided instructions shows clear
disentanglement in task-specific representations from MLLMs, leading to precise
differentiation of multimodal functional processing in the brain. We also find
that MLLM layers align hierarchically with the brain, with early sensory areas
showing strong alignment with early layers, while higher-level visual and
language regions align more with middle to late layers. These findings provide
clear evidence for the role of task-specific instructions in improving the
alignment between brain activity and MLLMs, and open new avenues for mapping
joint information processing in both the systems. We make the code publicly
available [https://github.com/subbareddy248/mllm_videos].

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [202] [A Survey on Large Language Models for Mathematical Reasoning](https://arxiv.org/abs/2506.08446)
*Peng-Yuan Wang,Tian-Shuo Liu,Chenyang Wang,Yi-Di Wang,Shu Yan,Cheng-Xing Jia,Xu-Hui Liu,Xin-Wei Chen,Jia-Cheng Xu,Ziniu Li,Yang Yu*

Main category: cs.AI

TL;DR: The survey explores the development of mathematical reasoning in LLMs, covering comprehension and answer generation phases, methods like CoT reasoning, and challenges like capacity and generalization. It suggests future research directions.


<details>
  <summary>Details</summary>
Motivation: To understand and enhance the mathematical reasoning abilities of LLMs, addressing challenges and exploring new techniques.

Method: Reviews methods including pretraining, fine-tuning (supervised and reinforcement learning), and advanced prompting like CoT.

Result: Notable progress in LLMs' mathematical reasoning, but challenges in capacity, efficiency, and generalization persist.

Conclusion: Future research should focus on advanced pretraining, formal reasoning frameworks, and meta-generalization to further improve LLMs' reasoning capabilities.

Abstract: Mathematical reasoning has long represented one of the most fundamental and
challenging frontiers in artificial intelligence research. In recent years,
large language models (LLMs) have achieved significant advances in this area.
This survey examines the development of mathematical reasoning abilities in
LLMs through two high-level cognitive phases: comprehension, where models gain
mathematical understanding via diverse pretraining strategies, and answer
generation, which has progressed from direct prediction to step-by-step
Chain-of-Thought (CoT) reasoning. We review methods for enhancing mathematical
reasoning, ranging from training-free prompting to fine-tuning approaches such
as supervised fine-tuning and reinforcement learning, and discuss recent work
on extended CoT and "test-time scaling". Despite notable progress, fundamental
challenges remain in terms of capacity, efficiency, and generalization. To
address these issues, we highlight promising research directions, including
advanced pretraining and knowledge augmentation techniques, formal reasoning
frameworks, and meta-generalization through principled learning paradigms. This
survey tries to provide some insights for researchers interested in enhancing
reasoning capabilities of LLMs and for those seeking to apply these techniques
to other domains.

</details>


### [203] [Consistent Paths Lead to Truth: Self-Rewarding Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2506.08745)
*Kongcheng Zhang,Qi Yao,Shunyu Liu,Yingjie Wang,Baisheng Lai,Jieping Ye,Mingli Song,Dacheng Tao*

Main category: cs.AI

TL;DR: Proposes CoVo, a self-rewarding RL framework for LLM reasoning, leveraging consistency and volatility in reasoning trajectories to eliminate external supervision.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of RL's reliance on external supervision in complex reasoning tasks by enabling self-rewarding learning.

Method: Introduces CoVo, an intrinsic reward mechanism combining Consistency and Volatility metrics with a curiosity bonus for exploration.

Result: Achieves performance comparable or superior to supervised RL on diverse reasoning benchmarks.

Conclusion: CoVo provides a scalable, unsupervised solution for enhancing LLM reasoning.

Abstract: Recent advances of Reinforcement Learning (RL) have highlighted its potential
in complex reasoning tasks, yet effective training often relies on external
supervision, which limits the broader applicability. In this work, we propose a
novel self-rewarding reinforcement learning framework to enhance Large Language
Model (LLM) reasoning by leveraging the consistency of intermediate reasoning
states across different reasoning trajectories. Our key insight is that correct
responses often exhibit consistent trajectory patterns in terms of model
likelihood: their intermediate reasoning states tend to converge toward their
own final answers (high consistency) with minimal deviation toward other
candidates (low volatility). Inspired by this observation, we introduce CoVo,
an intrinsic reward mechanism that integrates Consistency and Volatility via a
robust vector-space aggregation strategy, complemented by a curiosity bonus to
promote diverse exploration. CoVo enables LLMs to perform RL in a
self-rewarding manner, offering a scalable pathway for learning to reason
without external supervision. Extensive experiments on diverse reasoning
benchmarks show that CoVo achieves performance comparable to or even surpassing
supervised RL. Our code is available at https://github.com/sastpg/CoVo.

</details>


### [204] [Paths to Causality: Finding Informative Subgraphs Within Knowledge Graphs for Knowledge-Based Causal Discovery](https://arxiv.org/abs/2506.08771)
*Yuni Susanti,Michael Frber*

Main category: cs.AI

TL;DR: A novel method combines Knowledge Graphs (KGs) with Large Language Models (LLMs) to improve knowledge-based causal discovery, outperforming baselines by up to 44.4 points in F1 scores.


<details>
  <summary>Details</summary>
Motivation: Traditional causal discovery methods relying on observational data are unstable when using LLMs. This work aims to enhance reliability by integrating KGs with LLMs.

Method: The approach identifies metapath-based subgraphs in KGs, refines them using Learning-to-Rank models, and incorporates top-ranked subgraphs into zero-shot prompts for LLMs.

Result: Experiments on biomedical and open-domain datasets show the method outperforms baselines by up to 44.4 points in F1 scores.

Conclusion: The integration of KGs with LLMs significantly improves the reliability and effectiveness of knowledge-based causal discovery.

Abstract: Inferring causal relationships between variable pairs is crucial for
understanding multivariate interactions in complex systems. Knowledge-based
causal discovery -- which involves inferring causal relationships by reasoning
over the metadata of variables (e.g., names or textual context) -- offers a
compelling alternative to traditional methods that rely on observational data.
However, existing methods using Large Language Models (LLMs) often produce
unstable and inconsistent results, compromising their reliability for causal
inference. To address this, we introduce a novel approach that integrates
Knowledge Graphs (KGs) with LLMs to enhance knowledge-based causal discovery.
Our approach identifies informative metapath-based subgraphs within KGs and
further refines the selection of these subgraphs using Learning-to-Rank-based
models. The top-ranked subgraphs are then incorporated into zero-shot prompts,
improving the effectiveness of LLMs in inferring the causal relationship.
Extensive experiments on biomedical and open-domain datasets demonstrate that
our method outperforms most baselines by up to 44.4 points in F1 scores,
evaluated across diverse LLMs and KGs. Our code and datasets are available on
GitHub: https://github.com/susantiyuni/path-to-causality

</details>


### [205] [Measuring Data Science Automation: A Survey of Evaluation Tools for AI Assistants and Agents](https://arxiv.org/abs/2506.08800)
*Irene Testini,Jos Hernndez-Orallo,Lorenzo Pacchiardi*

Main category: cs.AI

TL;DR: A survey on LLM assistants and agents in data science highlights gaps in evaluation, including limited focus on data management, lack of intermediate human-AI collaboration, and overemphasis on human substitution.


<details>
  <summary>Details</summary>
Motivation: To assess the current state and limitations of LLM-based assistants and agents in data science, focusing on their evaluation practices.

Method: Survey of existing literature and practices regarding LLM assistants and agents in data science.

Result: Identified gaps: narrow focus on goal-oriented tasks, lack of intermediate collaboration levels, and overemphasis on substitution over task transformation.

Conclusion: Calls for broader evaluation frameworks that include diverse data science activities, balanced human-AI collaboration, and innovative automation approaches.

Abstract: Data science aims to extract insights from data to support decision-making
processes. Recently, Large Language Models (LLMs) are increasingly used as
assistants for data science, by suggesting ideas, techniques and small code
snippets, or for the interpretation of results and reporting. Proper automation
of some data-science activities is now promised by the rise of LLM agents,
i.e., AI systems powered by an LLM equipped with additional affordances--such
as code execution and knowledge bases--that can perform self-directed actions
and interact with digital environments. In this paper, we survey the evaluation
of LLM assistants and agents for data science. We find (1) a dominant focus on
a small subset of goal-oriented activities, largely ignoring data management
and exploratory activities; (2) a concentration on pure assistance or fully
autonomous agents, without considering intermediate levels of human-AI
collaboration; and (3) an emphasis on human substitution, therefore neglecting
the possibility of higher levels of automation thanks to task transformation.

</details>


### [206] [FEDTAIL: Federated Long-Tailed Domain Generalization with Sharpness-Guided Gradient Matching](https://arxiv.org/abs/2506.08518)
*Sunny Gupta,Nikita Jangid,Shounak Das,Amit Sethi*

Main category: cs.AI

TL;DR: FedTAIL is a federated domain generalization framework addressing class imbalance and conflicting objectives via sharpness-guided optimization, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing DG methods struggle with long-tailed class distributions and conflicting optimization goals, necessitating a more robust solution.

Method: FedTAIL uses gradient coherence regularization, class-wise sharpness minimization, and curvature-aware dynamic weighting to harmonize objectives and address imbalance.

Result: FedTAIL outperforms existing methods on standard benchmarks, especially under domain shifts and label imbalance.

Conclusion: FedTAIL effectively unifies optimization, regularization, and alignment, proving scalable and robust in both centralized and federated settings.

Abstract: Domain Generalization (DG) seeks to train models that perform reliably on
unseen target domains without access to target data during training. While
recent progress in smoothing the loss landscape has improved generalization,
existing methods often falter under long-tailed class distributions and
conflicting optimization objectives. We introduce FedTAIL, a federated domain
generalization framework that explicitly addresses these challenges through
sharpness-guided, gradient-aligned optimization. Our method incorporates a
gradient coherence regularizer to mitigate conflicts between classification and
adversarial objectives, leading to more stable convergence. To combat class
imbalance, we perform class-wise sharpness minimization and propose a
curvature-aware dynamic weighting scheme that adaptively emphasizes
underrepresented tail classes. Furthermore, we enhance conditional distribution
alignment by integrating sharpness-aware perturbations into entropy
regularization, improving robustness under domain shift. FedTAIL unifies
optimization harmonization, class-aware regularization, and conditional
alignment into a scalable, federated-compatible framework. Extensive
evaluations across standard domain generalization benchmarks demonstrate that
FedTAIL achieves state-of-the-art performance, particularly in the presence of
domain shifts and label imbalance, validating its effectiveness in both
centralized and federated settings. Code: https://github.com/sunnyinAI/FedTail

</details>


### [207] [VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement Learning](https://arxiv.org/abs/2506.09049)
*Li Kang,Xiufeng Song,Heng Zhou,Yiran Qin,Jie Yang,Xiaohong Liu,Philip Torr,Lei Bai,Zhenfei Yin*

Main category: cs.AI

TL;DR: VIKI-Bench is a hierarchical benchmark for embodied multi-agent cooperation, and VIKI-R is a two-stage framework combining VLM fine-tuning and reinforcement learning, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of coordinating multiple embodied agents in dynamic environments, especially with diverse embodiments and visual reasoning.

Method: Introduces VIKI-Bench with three levels (agent activation, task planning, trajectory perception) and VIKI-R, a two-stage framework (VLM fine-tuning + reinforcement learning).

Result: VIKI-R outperforms baselines across all task levels and enables compositional cooperation among heterogeneous agents.

Conclusion: VIKI-Bench and VIKI-R provide a unified testbed and method for advancing visual-driven multi-agent cooperation in embodied AI.

Abstract: Coordinating multiple embodied agents in dynamic environments remains a core
challenge in artificial intelligence, requiring both perception-driven
reasoning and scalable cooperation strategies. While recent works have
leveraged large language models (LLMs) for multi-agent planning, a few have
begun to explore vision-language models (VLMs) for visual reasoning. However,
these VLM-based approaches remain limited in their support for diverse
embodiment types. In this work, we introduce VIKI-Bench, the first hierarchical
benchmark tailored for embodied multi-agent cooperation, featuring three
structured levels: agent activation, task planning, and trajectory perception.
VIKI-Bench includes diverse robot embodiments, multi-view visual observations,
and structured supervision signals to evaluate reasoning grounded in visual
inputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a
two-stage framework that fine-tunes a pretrained vision-language model (VLM)
using Chain-of-Thought annotated demonstrations, followed by reinforcement
learning under multi-level reward signals. Our extensive experiments show that
VIKI-R significantly outperforms baselines method across all task levels.
Furthermore, we show that reinforcement learning enables the emergence of
compositional cooperation patterns among heterogeneous agents. Together,
VIKI-Bench and VIKI-R offer a unified testbed and method for advancing
multi-agent, visual-driven cooperation in embodied AI systems.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [208] [Aligning Proteins and Language: A Foundation Model for Protein Retrieval](https://arxiv.org/abs/2506.08023)
*Qifeng Wu,Zhengzhe Liu,Han Zhu,Yizhou Zhao,Daisuke Kihara,Min Xu*

Main category: q-bio.BM

TL;DR: A CLIP-style framework aligns 3D protein structures with functional annotations using contrastive learning, achieving promising zero-shot retrieval performance.


<details>
  <summary>Details</summary>
Motivation: To facilitate functional interpretation of protein structures by retrieving similar proteins from large-scale datasets, leveraging recent advances in vision-language models.

Method: Proposes a contrastive learning framework for aligning 3D protein structures with functional annotations, trained on a dataset of 200,000 protein-caption pairs.

Result: Demonstrates promising zero-shot retrieval performance in both in-domain (PDB) and cross-database (EMDB) evaluations.

Conclusion: Highlights the potential of multimodal foundation models for advancing structure-function understanding in protein biology.

Abstract: This paper aims to retrieve proteins with similar structures and semantics
from large-scale protein dataset, facilitating the functional interpretation of
protein structures derived by structural determination methods like
cryo-Electron Microscopy (cryo-EM). Motivated by the recent progress of
vision-language models (VLMs), we propose a CLIP-style framework for aligning
3D protein structures with functional annotations using contrastive learning.
For model training, we propose a large-scale dataset of approximately 200,000
protein-caption pairs with rich functional descriptors. We evaluate our model
in both in-domain and more challenging cross-database retrieval on Protein Data
Bank (PDB) and Electron Microscopy Data Bank (EMDB) dataset, respectively. In
both cases, our approach demonstrates promising zero-shot retrieval
performance, highlighting the potential of multimodal foundation models for
structure-function understanding in protein biology.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [209] [Step-Audio-AQAA: a Fully End-to-End Expressive Large Audio Language Model](https://arxiv.org/abs/2506.08967)
*Ailin Huang,Bingxin Li,Bruce Wang,Boyong Wu,Chao Yan,Chengli Feng,Heng Wang,Hongyu Zhou,Hongyuan Wang,Jingbei Li,Jianjian Sun,Joanna Wang,Mingrui Chen,Peng Liu,Ruihang Miao,Shilei Jiang,Tian Fei,Wang You,Xi Chen,Xuerui Yang,Yechang Huang,Yuxiang Zhang,Zheng Ge,Zheng Gong,Zhewei Huang,Zixin Zhang,Bin Wang,Bo Li,Buyun Ma,Changxin Miao,Changyi Wan,Chen Xu,Dapeng Shi,Dingyuan Hu,Enle Liu,Guanzhe Huang,Gulin Yan,Hanpeng Hu,Haonan Jia,Jiahao Gong,Jiaoren Wu,Jie Wu,Jie Yang,Junzhe Lin,Kaixiang Li,Lei Xia,Longlong Gu,Ming Li,Nie Hao,Ranchen Ming,Shaoliang Pang,Siqi Liu,Song Yuan,Tiancheng Cao,Wen Li,Wenqing He,Xu Zhao,Xuelin Zhang,Yanbo Yu,Yinmin Zhong,Yu Zhou,Yuanwei Liang,Yuanwei Lu,Yuxiang Yang,Zidong Yang,Zili Zhang,Binxing Jiao,Heung-Yeung Shum,Jiansheng Chen,Jing Li,Xiangyu Zhang,Xinhao Zhang,Yibo Zhu,Daxin Jiang,Shuchang Zhou,Chen Hu*

Main category: cs.SD

TL;DR: Step-Audio-AQAA is an end-to-end LALM for Audio Query-Audio Answer tasks, integrating a dual-codebook tokenizer, a large LLM, and a vocoder, outperforming state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: Existing LALMs rely on text outputs, limiting natural speech generation for seamless audio interactions.

Method: Uses a dual-codebook audio tokenizer, 130B-parameter LLM, and neural vocoder, with post-training via interleaved token-output and DPO.

Result: Outperforms state-of-the-art LALMs, excelling in speech control on the StepEval-Audio-360 benchmark.

Conclusion: Step-Audio-AQAA advances end-to-end LALMs, emphasizing the importance of token-based vocoders for AQAA tasks.

Abstract: Large Audio-Language Models (LALMs) have significantly advanced intelligent
human-computer interaction, yet their reliance on text-based outputs limits
their ability to generate natural speech responses directly, hindering seamless
audio interactions. To address this, we introduce Step-Audio-AQAA, a fully
end-to-end LALM designed for Audio Query-Audio Answer (AQAA) tasks. The model
integrates a dual-codebook audio tokenizer for linguistic and semantic feature
extraction, a 130-billion-parameter backbone LLM and a neural vocoder for
high-fidelity speech synthesis. Our post-training approach employs interleaved
token-output of text and audio to enhance semantic coherence and combines
Direct Preference Optimization (DPO) with model merge to improve performance.
Evaluations on the StepEval-Audio-360 benchmark demonstrate that
Step-Audio-AQAA excels especially in speech control, outperforming the
state-of-art LALMs in key areas. This work contributes a promising solution for
end-to-end LALMs and highlights the critical role of token-based vocoder in
enhancing overall performance for AQAA tasks.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [210] [Hierarchical Lexical Graph for Enhanced Multi-Hop Retrieval](https://arxiv.org/abs/2506.08074)
*Abdellah Ghassel,Ian Robinson,Gabriel Tanase,Hal Cooper,Bryan Thompson,Zhen Han,Vassilis N. Ioannidis,Soji Adeshina,Huzefa Rangwala*

Main category: cs.IR

TL;DR: The paper introduces Hierarchical Lexical Graph (HLG) to improve Retrieval-Augmented Generation (RAG) by addressing multi-hop retrieval challenges. It includes two retrievers and a synthetic dataset for robust evaluation, showing a 23.1% improvement over naive RAG.


<details>
  <summary>Details</summary>
Motivation: To address RAG's limitations in piecing together answers from semantically distant documents.

Method: Proposes HLG, a three-tier index, and two retrievers (StatementGraphRAG and TopicGraphRAG) for fine-grained and coarse retrieval. Introduces a synthetic dataset for evaluation.

Result: Outperforms naive RAG by 23.1% in retrieval recall and correctness across five datasets.

Conclusion: HLG and the proposed retrievers effectively enhance multi-hop retrieval, with open-source tools available for further use.

Abstract: Retrieval-Augmented Generation (RAG) grounds large language models in
external evidence, yet it still falters when answers must be pieced together
across semantically distant documents. We close this gap with the Hierarchical
Lexical Graph (HLG), a three-tier index that (i) traces every atomic
proposition to its source, (ii) clusters propositions into latent topics, and
(iii) links entities and relations to expose cross-document paths. On top of
HLG we build two complementary, plug-and-play retrievers: StatementGraphRAG,
which performs fine-grained entity-aware beam search over propositions for
high-precision factoid questions, and TopicGraphRAG, which selects coarse
topics before expanding along entity links to supply broad yet relevant context
for exploratory queries. Additionally, existing benchmarks lack the complexity
required to rigorously evaluate multi-hop summarization systems, often focusing
on single-document queries or limited datasets. To address this, we introduce a
synthetic dataset generation pipeline that curates realistic, multi-document
question-answer pairs, enabling robust evaluation of multi-hop retrieval
systems. Extensive experiments across five datasets demonstrate that our
methods outperform naive chunk-based RAG achieving an average relative
improvement of 23.1% in retrieval recall and correctness. Open-source Python
library is available at https://github.com/awslabs/graphrag-toolkit.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [211] [PhyBlock: A Progressive Benchmark for Physical Understanding and Planning via 3D Block Assembly](https://arxiv.org/abs/2506.08708)
*Liang Ma,Jiajun Wen,Min Lin,Rongtao Xu,Xiwen Liang,Bingqian Lin,Jun Ma,Yongxin Wang,Ziming Wei,Haokun Lin,Mingfei Han,Meng Cao,Bokui Chen,Ivan Laptev,Xiaodan Liang*

Main category: cs.RO

TL;DR: PhyBlock is a benchmark for evaluating VLMs' physical understanding and planning in 3D block assembly tasks, revealing limitations in high-level reasoning and spatial comprehension.


<details>
  <summary>Details</summary>
Motivation: To address VLMs' limited ability to comprehend physical phenomena in structured 3D environments, particularly for embodied agents.

Method: Introduces PhyBlock, a benchmark with 2600 tasks (assembly and VQA) to evaluate VLMs on spatial reasoning and physical comprehension.

Result: VLMs show pronounced limitations in high-level planning and reasoning, with persistent difficulties in spatial orientation and dependency reasoning.

Conclusion: PhyBlock serves as a unified testbed to advance embodied reasoning, bridging vision-language understanding and real-world problem-solving.

Abstract: While vision-language models (VLMs) have demonstrated promising capabilities
in reasoning and planning for embodied agents, their ability to comprehend
physical phenomena, particularly within structured 3D environments, remains
severely limited. To close this gap, we introduce PhyBlock, a progressive
benchmark designed to assess VLMs on physical understanding and planning
through robotic 3D block assembly tasks. PhyBlock integrates a novel four-level
cognitive hierarchy assembly task alongside targeted Visual Question Answering
(VQA) samples, collectively aimed at evaluating progressive spatial reasoning
and fundamental physical comprehension, including object properties, spatial
relationships, and holistic scene understanding. PhyBlock includes 2600 block
tasks (400 assembly tasks, 2200 VQA tasks) and evaluates models across three
key dimensions: partial completion, failure diagnosis, and planning robustness.
We benchmark 21 state-of-the-art VLMs, highlighting their strengths and
limitations in physically grounded, multi-step planning. Our empirical findings
indicate that the performance of VLMs exhibits pronounced limitations in
high-level planning and reasoning capabilities, leading to a notable decline in
performance for the growing complexity of the tasks. Error analysis reveals
persistent difficulties in spatial orientation and dependency reasoning.
Surprisingly, chain-of-thought prompting offers minimal improvements,
suggesting spatial tasks heavily rely on intuitive model comprehension. We
position PhyBlock as a unified testbed to advance embodied reasoning, bridging
vision-language understanding and real-world physical problem-solving.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [212] [SakugaFlow: A Stagewise Illustration Framework Emulating the Human Drawing Process and Providing Interactive Tutoring for Novice Drawing Skills](https://arxiv.org/abs/2506.08443)
*Kazuki Kawamura,Jun Rekimoto*

Main category: cs.HC

TL;DR: SakugaFlow is a four-stage pipeline combining diffusion-based image generation with an LLM tutor, offering real-time feedback and non-linear revision for AI-assisted art learning.


<details>
  <summary>Details</summary>
Motivation: Current AI illustration tools lack transparency in the artistic process, unlike human artists. SakugaFlow aims to bridge this gap by providing step-by-step guidance and feedback.

Method: The pipeline integrates diffusion-based image generation with a large-language-model tutor, offering real-time feedback on anatomy, perspective, and composition. Users can revise steps non-linearly and explore alternatives.

Result: SakugaFlow transforms AI image generation into a scaffolded learning environment, supporting both creativity and skill development through intermediate outputs and pedagogical dialogue.

Conclusion: SakugaFlow enhances AI-assisted art creation by making the process transparent and interactive, fostering learning and creative exploration.

Abstract: While current AI illustration tools can generate high-quality images from
text prompts, they rarely reveal the step-by-step procedure that human artists
follow. We present SakugaFlow, a four-stage pipeline that pairs diffusion-based
image generation with a large-language-model tutor. At each stage, novices
receive real-time feedback on anatomy, perspective, and composition, revise any
step non-linearly, and branch alternative versions. By exposing intermediate
outputs and embedding pedagogical dialogue, SakugaFlow turns a black-box
generator into a scaffolded learning environment that supports both creative
exploration and skills acquisition.

</details>


### [213] [MOSAIC-F: A Framework for Enhancing Students' Oral Presentation Skills through Personalized Feedback](https://arxiv.org/abs/2506.08634)
*Alvaro Becerra,Daniel Andres,Pablo Villegas,Roberto Daza,Ruth Cobos*

Main category: cs.HC

TL;DR: MOSAIC-F is a multimodal feedback framework combining human and AI-driven evaluations to provide personalized feedback on student learning, tested for improving oral presentation skills.


<details>
  <summary>Details</summary>
Motivation: To enhance feedback accuracy and personalization by integrating multimodal data and human assessments.

Method: Four-step framework: peer/professor assessments, multimodal data collection, AI-driven feedback synthesis, and student self-review.

Result: Enables more accurate, personalized, and actionable feedback.

Conclusion: MOSAIC-F successfully integrates human and data-based evaluations for improved learning feedback.

Abstract: In this article, we present a novel multimodal feedback framework called
MOSAIC-F, an acronym for a data-driven Framework that integrates Multimodal
Learning Analytics (MMLA), Observations, Sensors, Artificial Intelligence (AI),
and Collaborative assessments for generating personalized feedback on student
learning activities. This framework consists of four key steps. First, peers
and professors' assessments are conducted through standardized rubrics (that
include both quantitative and qualitative evaluations). Second, multimodal data
are collected during learning activities, including video recordings, audio
capture, gaze tracking, physiological signals (heart rate, motion data), and
behavioral interactions. Third, personalized feedback is generated using AI,
synthesizing human-based evaluations and data-based multimodal insights such as
posture, speech patterns, stress levels, and cognitive load, among others.
Finally, students review their own performance through video recordings and
engage in self-assessment and feedback visualization, comparing their own
evaluations with peers and professors' assessments, class averages, and
AI-generated recommendations. By combining human-based and data-based
evaluation techniques, this framework enables more accurate, personalized and
actionable feedback. We tested MOSAIC-F in the context of improving oral
presentation skills.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [214] [A System for Accurate Tracking and Video Recordings of Rodent Eye Movements using Convolutional Neural Networks for Biomedical Image Segmentation](https://arxiv.org/abs/2506.08183)
*Isha Puri,David Cox*

Main category: eess.IV

TL;DR: A novel CNN-based method for accurate pupil and corneal reflection identification in rodent eyes, addressing unique challenges like variability and small size.


<details>
  <summary>Details</summary>
Motivation: Existing eye tracking techniques focus on humans, neglecting rodent-specific challenges like eye variability and surrounding hair.

Method: Develops a flexible, robust CNN model for pupil and corneal reflection identification, incrementally trainable for field variability.

Result: Achieves highly accurate and practical biomedical image segmentation for rodent eye tracking, integrated with an automated infrared video system.

Conclusion: Introduces state-of-the-art eye tracking technology for neuroscience and vision science research in rodents.

Abstract: Research in neuroscience and vision science relies heavily on careful
measurements of animal subject's gaze direction. Rodents are the most widely
studied animal subjects for such research because of their economic advantage
and hardiness. Recently, video based eye trackers that use image processing
techniques have become a popular option for gaze tracking because they are easy
to use and are completely noninvasive. Although significant progress has been
made in improving the accuracy and robustness of eye tracking algorithms,
unfortunately, almost all of the techniques have focused on human eyes, which
does not account for the unique characteristics of the rodent eye images, e.g.,
variability in eye parameters, abundance of surrounding hair, and their small
size. To overcome these unique challenges, this work presents a flexible,
robust, and highly accurate model for pupil and corneal reflection
identification in rodent gaze determination that can be incrementally trained
to account for variability in eye parameters encountered in the field. To the
best of our knowledge, this is the first paper that demonstrates a highly
accurate and practical biomedical image segmentation based convolutional neural
network architecture for pupil and corneal reflection identification in eye
images. This new method, in conjunction with our automated infrared videobased
eye recording system, offers the state of the art technology in eye tracking
for neuroscience and vision science research for rodents.

</details>


### [215] [Snap-and-tune: combining deep learning and test-time optimization for high-fidelity cardiovascular volumetric meshing](https://arxiv.org/abs/2506.08280)
*Daniel H. Pak,Shubh Thaker,Kyle Baylous,Xiaoran Zhang,Danny Bluestein,James S. Duncan*

Main category: eess.IV

TL;DR: A snap-and-tune strategy combining DL and test-time optimization improves volumetric meshing from medical images, enhancing spatial accuracy and mesh quality without extra training labels.


<details>
  <summary>Details</summary>
Motivation: High-quality volumetric meshing is crucial for personalized medicine, but existing DL-based methods struggle with high-curvature areas and inter-part distances.

Method: The proposed method sequentially applies DL for initial shape fitting and test-time optimization for detailed corrections.

Result: The approach significantly improves spatial accuracy and mesh quality, demonstrated via solid mechanics simulations.

Conclusion: The snap-and-tune strategy is effective, automated, and versatile, with potential applications in personalized medicine.

Abstract: High-quality volumetric meshing from medical images is a key bottleneck for
physics-based simulations in personalized medicine. For volumetric meshing of
complex medical structures, recent studies have often utilized deep learning
(DL)-based template deformation approaches to enable fast test-time generation
with high spatial accuracy. However, these approaches still exhibit
limitations, such as limited flexibility at high-curvature areas and
unrealistic inter-part distances. In this study, we introduce a simple yet
effective snap-and-tune strategy that sequentially applies DL and test-time
optimization, which combines fast initial shape fitting with more detailed
sample-specific mesh corrections. Our method provides significant improvements
in both spatial accuracy and mesh quality, while being fully automated and
requiring no additional training labels. Finally, we demonstrate the
versatility and usefulness of our newly generated meshes via solid mechanics
simulations in two different software platforms. Our code is available at
https://github.com/danpak94/Deep-Cardiac-Volumetric-Mesh.

</details>


### [216] [Plug-and-Play Linear Attention for Pre-trained Image and Video Restoration Models](https://arxiv.org/abs/2506.08520)
*Srinivasan Kidambi,Pravin Nair*

Main category: eess.IV

TL;DR: PnP-Nystra is a Nystrm-based linear approximation of self-attention, offering a plug-and-play module for efficient acceleration in transformer architectures without retraining.


<details>
  <summary>Details</summary>
Motivation: Quadratic complexity of MHSA is a computational bottleneck in real-time and resource-constrained environments.

Method: Proposes PnP-Nystra, a Nystrm-based linear approximation, as a drop-in replacement for MHSA in pre-trained models.

Result: Achieves 2-4x speed-up on GPU and 2-5x on CPU with a maximum PSNR drop of 1.5 dB.

Conclusion: PnP-Nystra is the first linear attention method functioning as a training-free substitute for MHSA in restoration models.

Abstract: Multi-head self-attention (MHSA) has become a core component in modern
computer vision models. However, its quadratic complexity with respect to input
length poses a significant computational bottleneck in real-time and resource
constrained environments. We propose PnP-Nystra, a Nystr\"om based linear
approximation of self-attention, developed as a plug-and-play (PnP) module that
can be integrated into the pre-trained image and video restoration models
without retraining. As a drop-in replacement for MHSA, PnP-Nystra enables
efficient acceleration in various window-based transformer architectures,
including SwinIR, Uformer, and RVRT. Our experiments across diverse image and
video restoration tasks, including denoising, deblurring, and super-resolution,
demonstrate that PnP-Nystra achieves a 2-4x speed-up on an NVIDIA RTX 4090 GPU
and a 2-5x speed-up on CPU inference. Despite these significant gains, the
method incurs a maximum PSNR drop of only 1.5 dB across all evaluated tasks. To
the best of our knowledge, we are the first to demonstrate a linear attention
functioning as a training-free substitute for MHSA in restoration models.

</details>


### [217] [DCD: A Semantic Segmentation Model for Fetal Ultrasound Four-Chamber View](https://arxiv.org/abs/2506.08534)
*Donglian Li,Hui Guo,Minglang Chen,Huizhen Chen,Jialing Chen,Bocheng Liang,Pengchen Liang,Ying Tan*

Main category: eess.IV

TL;DR: DCD is a deep learning model for automatic segmentation of fetal A4C echocardiography, addressing challenges like noise and variability.


<details>
  <summary>Details</summary>
Motivation: Accurate segmentation in fetal A4C view is crucial for early CHD diagnosis but is hindered by ultrasound artifacts and variability.

Method: DCD uses Dense ASPP for multi-scale feature extraction and CBAM for adaptive feature representation.

Result: The model achieves precise and robust segmentation, improving prenatal cardiac assessment.

Conclusion: DCD enhances segmentation accuracy, reducing sonographer workload and aiding CHD diagnosis.

Abstract: Accurate segmentation of anatomical structures in the apical four-chamber
(A4C) view of fetal echocardiography is essential for early diagnosis and
prenatal evaluation of congenital heart disease (CHD). However, precise
segmentation remains challenging due to ultrasound artifacts, speckle noise,
anatomical variability, and boundary ambiguity across different gestational
stages. To reduce the workload of sonographers and enhance segmentation
accuracy, we propose DCD, an advanced deep learning-based model for automatic
segmentation of key anatomical structures in the fetal A4C view. Our model
incorporates a Dense Atrous Spatial Pyramid Pooling (Dense ASPP) module,
enabling superior multi-scale feature extraction, and a Convolutional Block
Attention Module (CBAM) to enhance adaptive feature representation. By
effectively capturing both local and global contextual information, DCD
achieves precise and robust segmentation, contributing to improved prenatal
cardiac assessment.

</details>


### [218] [Biologically Inspired Deep Learning Approaches for Fetal Ultrasound Image Classification](https://arxiv.org/abs/2506.08623)
*Rinat Prochii,Elizaveta Dakhova,Pavel Birulin,Maxim Sharaev*

Main category: eess.IV

TL;DR: A biologically inspired deep learning ensemble framework is introduced to classify 16 fetal structures in second-trimester ultrasound images, achieving high accuracy despite challenges like low image quality and class imbalance.


<details>
  <summary>Details</summary>
Motivation: Accurate classification of fetal ultrasound images is difficult due to low quality, high variability, and class imbalance. This work aims to address these challenges for a large number of anatomical targets.

Method: The model uses a two-branch architecture (shallow and detailed paths) inspired by biological vision systems, combining EfficientNet-B0 and EfficientNet-B6 with LDAM-Focal loss.

Result: The model achieves >75% accuracy for 90% of organs and >85% accuracy for 75% of organs, outperforming simpler models on a large number of classes.

Conclusion: Biologically inspired modular stacking enables robust and scalable fetal anatomy recognition in real-world clinical settings.

Abstract: Accurate classification of second-trimester fetal ultrasound images remains
challenging due to low image quality, high intra-class variability, and
significant class imbalance. In this work, we introduce a simple yet powerful,
biologically inspired deep learning ensemble framework that-unlike prior
studies focused on only a handful of anatomical targets-simultaneously
distinguishes 16 fetal structures. Drawing on the hierarchical, modular
organization of biological vision systems, our model stacks two complementary
branches (a "shallow" path for coarse, low-resolution cues and a "detailed"
path for fine, high-resolution features), concatenating their outputs for final
prediction. To our knowledge, no existing method has addressed such a large
number of classes with a comparably lightweight architecture. We trained and
evaluated on 5,298 routinely acquired clinical images (annotated by three
experts and reconciled via Dawid-Skene), reflecting real-world noise and
variability rather than a "cleaned" dataset. Despite this complexity, our
ensemble (EfficientNet-B0 + EfficientNet-B6 with LDAM-Focal loss) identifies
90% of organs with accuracy > 0.75 and 75% of organs with accuracy >
0.85-performance competitive with more elaborate models applied to far fewer
categories. These results demonstrate that biologically inspired modular
stacking can yield robust, scalable fetal anatomy recognition in challenging
clinical settings.

</details>


### [219] [MAMBO: High-Resolution Generative Approach for Mammography Images](https://arxiv.org/abs/2506.08677)
*Milica kipina,Nikola Jovii,Nicola Dall'Asen,Vanja venda,Anil Osman Tur,Slobodan Ili,Elisa Ricci,Dubravko ulibrk*

Main category: eess.IV

TL;DR: MAMBO, a patch-based diffusion model, generates high-resolution mammograms to address data scarcity in AI training for breast cancer detection.


<details>
  <summary>Details</summary>
Motivation: Training AI for mammography requires large datasets, but privacy and ethical constraints limit data availability.

Method: MAMBO uses separate diffusion models for local and global contexts, combined in a patch-based approach to generate 3840x3840 pixel mammograms.

Result: MAMBO produces realistic mammograms, aiding AI training and anomaly detection, validated by radiologists and numerical experiments.

Conclusion: MAMBO enhances mammography analysis, improving diagnosis accuracy and early lesion detection.

Abstract: Mammography is the gold standard for the detection and diagnosis of breast
cancer. This procedure can be significantly enhanced with Artificial
Intelligence (AI)-based software, which assists radiologists in identifying
abnormalities. However, training AI systems requires large and diverse
datasets, which are often difficult to obtain due to privacy and ethical
constraints. To address this issue, the paper introduces MAMmography ensemBle
mOdel (MAMBO), a novel patch-based diffusion approach designed to generate
full-resolution mammograms. Diffusion models have shown breakthrough results in
realistic image generation, yet few studies have focused on mammograms, and
none have successfully generated high-resolution outputs required to capture
fine-grained features of small lesions. To achieve this, MAMBO integrates
separate diffusion models to capture both local and global (image-level)
contexts. The contextual information is then fed into the final patch-based
model, significantly aiding the noise removal process. This thoughtful design
enables MAMBO to generate highly realistic mammograms of up to 3840x3840
pixels. Importantly, this approach can be used to enhance the training of
classification models and extended to anomaly detection. Experiments, both
numerical and radiologist validation, assess MAMBO's capabilities in image
generation, super-resolution, and anomaly detection, highlighting its potential
to enhance mammography analysis for more accurate diagnoses and earlier lesion
detection.

</details>


### [220] [Enhancing Synthetic CT from CBCT via Multimodal Fusion: A Study on the Impact of CBCT Quality and Alignment](https://arxiv.org/abs/2506.08716)
*Maximilian Tschuchnig,Lukas Lamminger,Philipp Steininger,Michael Gadermayr*

Main category: eess.IV

TL;DR: The paper proposes a multimodal learning approach to enhance synthetic CT (sCT) generation from CBCT, improving artifact reduction and visual quality compared to unimodal methods.


<details>
  <summary>Details</summary>
Motivation: CBCT's high resolution is marred by artifacts, leading to lower visual quality than conventional CT. Synthetic CT generation aims to mitigate this issue.

Method: Multimodal learning integrates intraoperative CBCT with preoperative CT, validated on real-world and synthetic datasets to assess alignment and CBCT quality impact.

Result: Multimodal sCT outperforms unimodal baselines, especially in well-aligned, low-quality CBCT-CT cases, with reproducible results in clinical datasets.

Conclusion: The multimodal approach effectively enhances sCT quality, offering significant improvements in challenging CBCT scenarios.

Abstract: Cone-Beam Computed Tomography (CBCT) is widely used for real-time
intraoperative imaging due to its low radiation dose and high acquisition
speed. However, despite its high resolution, CBCT suffers from significant
artifacts and thereby lower visual quality, compared to conventional Computed
Tomography (CT). A recent approach to mitigate these artifacts is synthetic CT
(sCT) generation, translating CBCT volumes into the CT domain. In this work, we
enhance sCT generation through multimodal learning, integrating intraoperative
CBCT with preoperative CT. Beyond validation on two real-world datasets, we use
a versatile synthetic dataset, to analyze how CBCT-CT alignment and CBCT
quality affect sCT quality. The results demonstrate that multimodal sCT
consistently outperform unimodal baselines, with the most significant gains
observed in well-aligned, low-quality CBCT-CT cases. Finally, we demonstrate
that these findings are highly reproducible in real-world clinical datasets.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [221] [Normalized Radon Cumulative Distribution Transforms for Invariance and Robustness in Optimal Transport Based Image Classification](https://arxiv.org/abs/2506.08761)
*Matthias Beckmann,Robert Beinert,Jonas Bresch*

Main category: math.NA

TL;DR: The paper introduces max-normalized and mean-normalized versions of the R-CDT for robust image classification under affine and non-affine deformations, supported by theoretical and experimental results.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of image classification under general affine and non-affine deformations, especially in small data regimes.

Method: Proposes max-normalized and mean-normalized R-CDT variants, analyzing their robustness using Wasserstein distances.

Result: The variants ensure separability under affine transformations and show robustness against local deformations and impulsive noise.

Conclusion: The novel R-CDT versions are effective and robust for image classification, validated by numerical experiments.

Abstract: The Radon cumulative distribution transform (R-CDT), is an easy-to-compute
feature extractor that facilitates image classification tasks especially in the
small data regime. It is closely related to the sliced Wasserstein distance and
provably guaranties the linear separability of image classes that emerge from
translations or scalings. In many real-world applications, like the recognition
of watermarks in filigranology, however, the data is subject to general affine
transformations originating from the measurement process. To overcome this
issue, we recently introduced the so-called max-normalized R-CDT that only
requires elementary operations and guaranties the separability under arbitrary
affine transformations. The aim of this paper is to continue our study of the
max-normalized R-CDT especially with respect to its robustness against
non-affine image deformations. Our sensitivity analysis shows that its
separability properties are stable provided the Wasserstein-infinity distance
between the samples can be controlled. Since the Wasserstein-infinity distance
only allows small local image deformations, we moreover introduce a
mean-normalized version of the R-CDT. In this case, robustness relates to the
Wasserstein-2 distance and also covers image deformations caused by impulsive
noise for instance. Our theoretical results are supported by numerical
experiments showing the effectiveness of our novel feature extractors as well
as their robustness against local non-affine deformations and impulsive noise.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [222] [Neural-Augmented Kelvinlet: Real-Time Soft Tissue Deformation with Multiple Graspers](https://arxiv.org/abs/2506.08043)
*Ashkan Shahbazi,Kyvia Pereira,Jon S. Heiselman,Elaheh Akbari,Annie C. Benson,Sepehr Seifi,Xinyuan Liu,Garrison L. Johnston,Erwin Terpstra,Anne Draaisma,Jan-Jaap Severes,Jie Ying Wu,Nabil Simaan,Michael L. Miga,Soheil Kolouri*

Main category: cs.GR

TL;DR: A novel physics-informed neural simulator using Kelvinlet-based priors for real-time, accurate soft tissue deformation in surgical robotics.


<details>
  <summary>Details</summary>
Motivation: Fast and accurate soft tissue simulation is crucial for surgical robotics and medical training.

Method: Integrates Kelvinlet-based priors into neural simulators, leveraging FEM simulations for improved accuracy and physical consistency.

Result: Enhances neural network predictions, enabling high-fidelity surgical maneuvers with low latency.

Conclusion: Kelvinlet-augmented learning is effective for real-time, physics-aware soft tissue simulation in surgery.

Abstract: Fast and accurate simulation of soft tissue deformation is a critical factor
for surgical robotics and medical training. In this paper, we introduce a novel
physics-informed neural simulator that approximates soft tissue deformations in
a realistic and real-time manner. Our framework integrates Kelvinlet-based
priors into neural simulators, making it the first approach to leverage
Kelvinlets for residual learning and regularization in data-driven soft tissue
modeling. By incorporating large-scale Finite Element Method (FEM) simulations
of both linear and nonlinear soft tissue responses, our method improves neural
network predictions across diverse architectures, enhancing accuracy and
physical consistency while maintaining low latency for real-time performance.
We demonstrate the effectiveness of our approach by performing accurate
surgical maneuvers that simulate the use of standard laparoscopic tissue
grasping tools with high fidelity. These results establish Kelvinlet-augmented
learning as a powerful and efficient strategy for real-time, physics-aware soft
tissue simulation in surgical applications.

</details>


### [223] [A Real-time 3D Desktop Display](https://arxiv.org/abs/2506.08064)
*Livio Tenze,Enrique Canessa*

Main category: cs.GR

TL;DR: The paper introduces an extended version of the altiro3D C++ Library for real-time 3D video stream processing, leveraging AI to enhance performance and support multiview synthesis.


<details>
  <summary>Details</summary>
Motivation: To enable realistic 3D experiences by processing 2D images or video streams into light-fields, supporting applications like web browsers and video chats.

Method: Uses MiDaS CNN for depth map extraction, AI techniques for performance improvement, and a multi-platform GUI for screen region selection.

Result: The extended altiro3D Library can now process standard images, video streams, or desktop screen portions into 3D, compatible with devices like Looking Glass Portrait.

Conclusion: The updated altiro3D Library successfully enhances 3D rendering capabilities, making it versatile for real-time applications.

Abstract: A new extended version of the altiro3D C++ Library -- initially developed to
get glass-free holographic displays starting from 2D images -- is here
introduced aiming to deal with 3D video streams from either 2D webcam images or
flat video files. These streams are processed in real-time to synthesize
light-fields (in Native format) and feed realistic 3D experiences. The core
function needed to recreate multiviews consists on the use of MiDaS
Convolutional Neural Network (CNN), which allows to extract a depth map from a
single 2D image. Artificial Intelligence (AI) computing techniques are applied
to improve the overall performance of the extended altiro3D Library. Thus,
altiro3D can now treat standard images, video streams or screen portions of a
Desktop where other apps may be also running (like web browsers, video chats,
etc) and render them into 3D. To achieve the latter, a screen region need to be
selected in order to feed the output directly into a light-field 3D device such
as Looking Glass (LG) Portrait. In order to simplify the acquisition of a
Desktop screen area by the user, a multi-platform Graphical User Interface has
been also implemented. Sources available at:
https://github.com/canessae/altiro3D/releases/tag/2.0.0

</details>


### [224] [Generalizable Articulated Object Reconstruction from Casually Captured RGBD Videos](https://arxiv.org/abs/2506.08334)
*Weikun Peng,Jun Lv,Cewu Lu,Manolis Savva*

Main category: cs.GR

TL;DR: The paper introduces a coarse-to-fine framework for reconstructing articulated objects from casually captured RGBD videos, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current methods for reconstructing articulated objects require carefully captured data, limiting scalability and generalizability. The paper aims to address this by using casually captured videos, which are easier to acquire.

Method: A coarse-to-fine framework is proposed to infer joint parameters and segment movable parts from dynamic RGBD videos.

Result: The method significantly outperforms existing approaches, successfully reconstructing both synthetic and real articulated objects across various categories.

Conclusion: The framework enables practical, scalable, and generalizable reconstruction of articulated objects from easily obtainable videos, advancing applications in embodied AI and robotics.

Abstract: Articulated objects are prevalent in daily life. Understanding their
kinematic structure and reconstructing them have numerous applications in
embodied AI and robotics. However, current methods require carefully captured
data for training or inference, preventing practical, scalable, and
generalizable reconstruction of articulated objects. We focus on reconstruction
of an articulated object from a casually captured RGBD video shot with a
hand-held camera. A casually captured video of an interaction with an
articulated object is easy to acquire at scale using smartphones. However, this
setting is quite challenging, as the object and camera move simultaneously and
there are significant occlusions as the person interacts with the object. To
tackle these challenges, we introduce a coarse-to-fine framework that infers
joint parameters and segments movable parts of the object from a dynamic RGBD
video. To evaluate our method under this new setting, we build a 20$\times$
larger synthetic dataset of 784 videos containing 284 objects across 11
categories. We compare our approach with existing methods that also take video
as input. Experiments show that our method can reconstruct synthetic and real
articulated objects across different categories from dynamic RGBD videos,
outperforming existing methods significantly.

</details>


### [225] [Complex-Valued Holographic Radiance Fields](https://arxiv.org/abs/2506.08350)
*Yicheng Zhan,Dong-Ha Shin,Seung-Hwan Baek,Kaan Akit*

Main category: cs.GR

TL;DR: A novel method optimizes 3D scenes using complex-valued Gaussian primitives for holographic rendering, achieving significant speed improvements without sacrificing quality.


<details>
  <summary>Details</summary>
Motivation: To advance physically plausible rendering, especially for holographic displays, by modeling full light properties (amplitude and phase) in 3D.

Method: Reformulates 3D Gaussian splatting with complex-valued Gaussian primitives and optimizes them using RGBD multi-view images.

Result: Achieves 30x-10,000x speed improvements over state-of-the-art methods while maintaining image quality.

Conclusion: The method represents a step towards geometrically aligned, physically plausible holographic scene representations.

Abstract: Modeling the full properties of light, including both amplitude and phase, in
3D representations is crucial for advancing physically plausible rendering,
particularly in holographic displays. To support these features, we propose a
novel representation that optimizes 3D scenes without relying on
intensity-based intermediaries. We reformulate 3D Gaussian splatting with
complex-valued Gaussian primitives, expanding support for rendering with light
waves. By leveraging RGBD multi-view images, our method directly optimizes
complex-valued Gaussians as a 3D holographic scene representation. This
eliminates the need for computationally expensive hologram re-optimization.
Compared with state-of-the-art methods, our method achieves 30x-10,000x speed
improvements while maintaining on-par image quality, representing a first step
towards geometrically aligned, physically plausible holographic scene
representations.

</details>


### [226] [Fine-Grained Spatially Varying Material Selection in Images](https://arxiv.org/abs/2506.09023)
*Julia Guerrero-Viu,Michael Fischer,Iliyan Georgiev,Elena Garces,Diego Gutierrez,Belen Masia,Valentin Deschaintre*

Main category: cs.GR

TL;DR: A method for robust material selection in images using vision transformers (ViT) and multi-resolution processing, enabling texture and subtexture-level selection with a new dataset (DuMaS).


<details>
  <summary>Details</summary>
Motivation: To improve material selection in images, addressing challenges like lighting and reflectance variations for better downstream editing.

Method: Uses ViT models with multi-resolution processing for finer, stable selections. Introduces a two-level material selection approach (texture and subtexture) supported by the DuMaS dataset.

Result: Achieves finer and more stable selection results than prior methods, validated by the DuMaS dataset.

Conclusion: The proposed method enhances material selection robustness and precision, facilitating advanced image editing tasks.

Abstract: Selection is the first step in many image editing processes, enabling faster
and simpler modifications of all pixels sharing a common modality. In this
work, we present a method for material selection in images, robust to lighting
and reflectance variations, which can be used for downstream editing tasks. We
rely on vision transformer (ViT) models and leverage their features for
selection, proposing a multi-resolution processing strategy that yields finer
and more stable selection results than prior methods. Furthermore, we enable
selection at two levels: texture and subtexture, leveraging a new two-level
material selection (DuMaS) dataset which includes dense annotations for over
800,000 synthetic images, both on the texture and subtexture levels.

</details>

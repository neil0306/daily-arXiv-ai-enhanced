<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 90]
- [cs.CV](#cs.CV) [Total: 183]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.LG](#cs.LG) [Total: 21]
- [cs.CR](#cs.CR) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.RO](#cs.RO) [Total: 6]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.CY](#cs.CY) [Total: 3]
- [econ.GN](#econ.GN) [Total: 1]
- [stat.ML](#stat.ML) [Total: 2]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.AI](#cs.AI) [Total: 10]
- [eess.IV](#eess.IV) [Total: 5]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.DL](#cs.DL) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [cs.SE](#cs.SE) [Total: 3]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [PlotCraft: Pushing the Limits of LLMs for Complex and Interactive Data Visualization](https://arxiv.org/abs/2511.00010)
*Jiajun Zhang,Jianke Zhang,Zeyu Cui,Jiaxi Yang,Lei Zhang,Binyuan Hui,Qiang Liu,Zilei Wang,Liang Wang,Junyang Lin*

Main category: cs.CL

TL;DR: PlotCraft is a new benchmark for evaluating LLMs on complex visualization tasks, revealing performance gaps. The authors develop SynthVis-30K dataset and PlotCraftor model to address these deficiencies, achieving strong performance comparable to proprietary approaches.


<details>
  <summary>Details</summary>
Motivation: Current LLMs show remarkable code generation capabilities but their ability to create complex visualizations for scaled and structured data remains largely unevaluated and underdeveloped.

Method: Introduced PlotCraft benchmark with 1k challenging visualization tasks across 7 high-level tasks and 48 chart types. Developed SynthVis-30K dataset via collaborative agent framework, and built PlotCraftor model for complex data visualization.

Result: Evaluation of 23 leading LLMs revealed obvious performance deficiencies in sophisticated visualization tasks. PlotCraftor achieved performance comparable to leading proprietary approaches, with over 50% improvement on hard tasks.

Conclusion: The work addresses the gap in LLM capabilities for complex data visualization through a comprehensive benchmark, synthesized dataset, and specialized model that significantly improves performance on challenging visualization tasks.

Abstract: Recent Large Language Models (LLMs) have demonstrated remarkable profi-
ciency in code generation. However, their ability to create complex visualiza-
tions for scaled and structured data remains largely unevaluated and
underdevel- oped. To address this gap, we introduce PlotCraft, a new benchmark
featuring 1k challenging visualization tasks that cover a wide range of topics,
such as fi- nance, scientific research, and sociology. The benchmark is
structured around seven high-level visualization tasks and encompasses 48
distinct chart types. Cru- cially, it is the first to systematically evaluate
both single-turn generation and multi-turn refinement across a diverse spectrum
of task complexities. Our com- prehensive evaluation of 23 leading LLMs on
PlotCraft reveals obvious per- formance deficiencies in handling sophisticated
visualization tasks. To bridge this performance gap, we develope SynthVis-30K,
a large-scale, high-quality dataset of complex visualization code synthesized
via a collaborative agent frame- work. Building upon this dataset, we develope
PlotCraftor, a novel code gener- ation model that achieves strong capabilities
in complex data visualization with a remarkably small size. Across VisEval,
PandasPlotBench, and our proposed PlotCraft, PlotCraftor shows performance
comparable to that of leading propri- etary approaches. Especially, on hard
task, Our model achieves over 50% per- formance improvement. We will release
the benchmark, dataset, and code at
https://github.com/Speakn0w/PlotCraft-Benchmark.

</details>


### [2] [Cognitive Alignment in Personality Reasoning: Leveraging Prototype Theory for MBTI Inference](https://arxiv.org/abs/2511.00115)
*Haoyuan Li,Yuanbo Tong,Yuchen Li,Zirui Wang,Chunhou Liu,Jiamou Liu*

Main category: cs.CL

TL;DR: ProtoMBTI is a prototype-based framework for MBTI personality recognition that uses LLM-guided data augmentation, prototype learning, and a retrieve-reuse-revise-retain inference cycle to improve accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Traditional hard-label classification for personality recognition obscures the graded, prototype-like nature of human personality judgments. The paper aims to align computational methods with psychological prototype theory.

Method: Uses LLM-guided multi-dimensional augmentation to create a balanced corpus, LoRA-fine-tunes a lightweight encoder to learn embeddings and standardize personality prototypes, and implements a retrieve-reuse-revise-retain inference cycle with prototype-based voting.

Result: ProtoMBTI outperforms baselines on both MBTI dichotomies and full 16-type classification across Kaggle and Pandora benchmarks, and shows robust cross-dataset generalization.

Conclusion: Aligning inference processes with psychological prototype reasoning improves accuracy, interpretability, and transferability in text-based personality modeling.

Abstract: Personality recognition from text is typically cast as hard-label
classification, which obscures the graded, prototype-like nature of human
personality judgments. We present ProtoMBTI, a cognitively aligned framework
for MBTI inference that operationalizes prototype theory within an LLM-based
pipeline. First, we construct a balanced, quality-controlled corpus via
LLM-guided multi-dimensional augmentation (semantic, linguistic, sentiment).
Next, we LoRA-fine-tune a lightweight (<=2B) encoder to learn discriminative
embeddings and to standardize a bank of personality prototypes. At inference,
we retrieve top-k prototypes for a query post and perform a
retrieve--reuse--revise--retain cycle: the model aggregates prototype evidence
via prompt-based voting, revises when inconsistencies arise, and, upon correct
prediction, retains the sample to continually enrich the prototype library.
Across Kaggle and Pandora benchmarks, ProtoMBTI improves over baselines on both
the four MBTI dichotomies and the full 16-type task, and exhibits robust
cross-dataset generalization. Our results indicate that aligning the inference
process with psychological prototype reasoning yields gains in accuracy,
interpretability, and transfer for text-based personality modeling.

</details>


### [3] [ParaScopes: What do Language Models Activations Encode About Future Text?](https://arxiv.org/abs/2511.00180)
*Nicky Pochinkov,Yulia Volkova,Anna Vasileva,Sai V R Chereddy*

Main category: cs.CL

TL;DR: Residual Stream Decoders framework probes language model activations for paragraph/document-scale planning, revealing information equivalent to 5+ future tokens in small models.


<details>
  <summary>Details</summary>
Motivation: Current interpretability methods are limited to specific concepts or tokens, while language models are increasingly capable of longer time horizon tasks, creating a need for better understanding of long-term planning information.

Method: Developed a framework of Residual Stream Decoders to probe model activations for paragraph-scale and document-scale plans, testing several decoding methods.

Result: Found that information equivalent to 5+ tokens of future context can be decoded from activations in small language models.

Conclusion: This work establishes groundwork for improved monitoring of language models and better understanding of how they encode longer-term planning information.

Abstract: Interpretability studies in language models often investigate forward-looking
representations of activations. However, as language models become capable of
doing ever longer time horizon tasks, methods for understanding activations
often remain limited to testing specific concepts or tokens. We develop a
framework of Residual Stream Decoders as a method of probing model activations
for paragraph-scale and document-scale plans. We test several methods and find
information can be decoded equivalent to 5+ tokens of future context in small
models. These results lay the groundwork for better monitoring of language
models and better understanding how they might encode longer-term planning
information.

</details>


### [4] [Training LLMs Beyond Next Token Prediction - Filling the Mutual Information Gap](https://arxiv.org/abs/2511.00198)
*Chun-Hao Yang,Bo-Han Feng,Tzu-Yuan Lai,Yan Yu Chen,Yin-Kai Dean Huang,Shou-De Lin*

Main category: cs.CL

TL;DR: This paper proposes an alternative to next-token prediction for training LLMs by focusing on predicting information-rich tokens, showing improvements across arithmetic, classification, and generation tasks.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of optimizing LLM training performance while controlling computational costs, challenging the conventional next-token prediction approach.

Method: Proposes training LLMs by predicting information-rich tokens instead of using standard next-token prediction, evaluated on arithmetic, multi-label text classification, and natural-language generation tasks.

Result: The approach demonstrates effectiveness across three different task types, suggesting improved training efficiency and model performance.

Conclusion: The work provides a principled approach to LLM training optimization that advances both practical performance and theoretical understanding of target-token selection strategies.

Abstract: Optimizing training performance in large language models (LLMs) remains an
essential challenge, particularly in improving model performance while
maintaining computational costs. This work challenges the conventional approach
of training LLMs using next-token prediction (NTP), arguing that by predicting
information-rich tokens during training, there is a more effective way to train
LLMs. We investigate the impact of the proposed solution in three kinds of
tasks for LLMs: arithmetic, multi-label classification of text, and
natural-language generation. This work offers a principled approach to
optimizing LLM training, advancing both model performance and theoretical
understanding of the target-token selection strategies.

</details>


### [5] [Consistently Simulating Human Personas with Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2511.00222)
*Marwa Abdulhai,Ryan Cheng,Donovan Clay,Tim Althoff,Sergey Levine,Natasha Jaques*

Main category: cs.CL

TL;DR: A framework for evaluating and improving persona consistency in LLM-generated dialogue using automatic metrics and reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: LLMs used to simulate human users often drift from assigned personas, contradict earlier statements, or abandon role-appropriate behavior, limiting their reliability in interactive settings like therapy and education.

Method: Introduced three automatic metrics for persona consistency (prompt-to-line, line-to-line, Q&A consistency), validated against human annotations, and used them as reward signals for multi-turn reinforcement learning to fine-tune LLMs.

Result: The method reduced inconsistency by over 55%, producing more coherent and faithful simulated users across three roles: patient, student, and social chat partner.

Conclusion: The proposed framework effectively improves persona consistency in LLM simulations, enabling more reliable and scalable training and evaluation of AI agents in interactive applications.

Abstract: Large Language Models (LLMs) are increasingly used to simulate human users in
interactive settings such as therapy, education, and social role-play. While
these simulations enable scalable training and evaluation of AI agents,
off-the-shelf LLMs often drift from their assigned personas, contradict earlier
statements, or abandon role-appropriate behavior. We introduce a unified
framework for evaluating and improving persona consistency in LLM-generated
dialogue. We define three automatic metrics: prompt-to-line consistency,
line-to-line consistency, and Q&A consistency, that capture different types of
persona drift and validate each against human annotations. Using these metrics
as reward signals, we apply multi-turn reinforcement learning to fine-tune LLMs
for three user roles: a patient, a student, and a social chat partner. Our
method reduces inconsistency by over 55%, resulting in more coherent and
faithful simulated users.

</details>


### [6] [AgentBnB: A Browser-Based Cybersecurity Tabletop Exercise with Large Language Model Support and Retrieval-Aligned Scaffolding](https://arxiv.org/abs/2511.00265)
*Arman Anwar,Zefang Liu*

Main category: cs.CL

TL;DR: AgentBnB is a browser-based cybersecurity training system that uses LLM teammates and a retrieval-augmented copilot to provide scalable, adaptive tabletop exercises with on-demand hints.


<details>
  <summary>Details</summary>
Motivation: Traditional cybersecurity tabletop exercises are scripted, resource-intensive, and difficult to scale, creating a need for more accessible and scalable training solutions.

Method: The system integrates large language model teammates with a Bloom-aligned, retrieval-augmented copilot (C2D2) that expands curated content into cognitive snippets. It uses prompt-engineered agents with a scaffolding ladder that fades as learner confidence grows.

Result: In a pilot with four graduate students, participants reported greater intention to use the agent-based version compared to physical card decks and viewed it as more scalable, though a ceiling effect was observed on simple knowledge quizzes.

Conclusion: LLM-augmented tabletop exercises can provide lightweight, repeatable cybersecurity practice without traditional logistical burdens, with planned extensions including multi-player modes and comparative studies.

Abstract: Traditional cybersecurity tabletop exercises (TTXs) provide valuable training
but are often scripted, resource-intensive, and difficult to scale. We
introduce AgentBnB, a browser-based re-imagining of the Backdoors & Breaches
game that integrates large language model teammates with a Bloom-aligned,
retrieval-augmented copilot (C2D2). The system expands a curated corpus into
factual, conceptual, procedural, and metacognitive snippets, delivering
on-demand, cognitively targeted hints. Prompt-engineered agents employ a
scaffolding ladder that gradually fades as learner confidence grows. In a
solo-player pilot with four graduate students, participants reported greater
intention to use the agent-based version compared to the physical card deck and
viewed it as more scalable, though a ceiling effect emerged on a simple
knowledge quiz. Despite limitations of small sample size, single-player focus,
and narrow corpus, these early findings suggest that large language model
augmented TTXs can provide lightweight, repeatable practice without the
logistical burden of traditional exercises. Planned extensions include
multi-player modes, telemetry-driven coaching, and comparative studies with
larger cohorts.

</details>


### [7] [IL-PCSR: Legal Corpus for Prior Case and Statute Retrieval](https://arxiv.org/abs/2511.00268)
*Shounak Paul,Dhananjay Ghumare,Pawan Goyal,Saptarshi Ghosh,Ashutosh Modi*

Main category: cs.CL

TL;DR: The paper introduces IL-PCR, a unified corpus for both statute and precedent retrieval in Indian legal contexts, and proposes an LLM-based re-ranking approach that leverages the interdependence between these tasks to achieve superior performance.


<details>
  <summary>Details</summary>
Motivation: Current legal retrieval research treats statute retrieval and precedent retrieval as independent tasks with separate datasets and models, despite their inherent relationship where similar cases cite similar statutes due to similar factual situations.

Method: Created IL-PCR corpus as a common testbed for both tasks, experimented with lexical models, semantic models, and GNN-based ensembles, and developed an LLM-based re-ranking approach to exploit task interdependence.

Result: The LLM-based re-ranking approach achieved the best performance by effectively leveraging the dependence between statute retrieval and precedent retrieval tasks.

Conclusion: The proposed unified corpus and LLM-based re-ranking method successfully address the gap in legal retrieval research by exploiting the inherent relationship between statute and precedent retrieval, leading to improved performance.

Abstract: Identifying/retrieving relevant statutes and prior cases/precedents for a
given legal situation are common tasks exercised by law practitioners.
Researchers to date have addressed the two tasks independently, thus developing
completely different datasets and models for each task; however, both retrieval
tasks are inherently related, e.g., similar cases tend to cite similar statutes
(due to similar factual situation). In this paper, we address this gap. We
propose IL-PCR (Indian Legal corpus for Prior Case and Statute Retrieval),
which is a unique corpus that provides a common testbed for developing models
for both the tasks (Statute Retrieval and Precedent Retrieval) that can exploit
the dependence between the two. We experiment extensively with several baseline
models on the tasks, including lexical models, semantic models and ensemble
based on GNNs. Further, to exploit the dependence between the two tasks, we
develop an LLM-based re-ranking approach that gives the best performance.

</details>


### [8] [POSESTITCH-SLT: Linguistically Inspired Pose-Stitching for End-to-End Sign Language Translation](https://arxiv.org/abs/2511.00270)
*Abhinav Joshi,Vaibhav Sharma,Sanjeet Singh,Ashutosh Modi*

Main category: cs.CL

TL;DR: POSESTITCH-SLT is a novel pre-training method for sign language translation that uses template-generated sentence pairs to improve performance on low-resource datasets, achieving significant BLEU score improvements on How2Sign and iSign datasets.


<details>
  <summary>Details</summary>
Motivation: Sign language translation faces challenges due to limited large-scale, sentence-aligned datasets, motivating the need for effective methods in low-resource settings.

Method: Proposes POSESTITCH-SLT, a pre-training scheme inspired by linguistic-templates-based sentence generation, using template-generated sentence pairs with a simple transformer-based encoder-decoder architecture.

Result: Achieved BLEU-4 score improvements from 1.97 to 4.56 on How2Sign and from 0.55 to 3.43 on iSign, surpassing prior state-of-the-art methods for pose-based gloss-free translation.

Conclusion: Template-driven synthetic supervision is effective for low-resource sign language translation settings, demonstrating the value of template-generated data in overcoming dataset scarcity.

Abstract: Sign language translation remains a challenging task due to the scarcity of
large-scale, sentence-aligned datasets. Prior arts have focused on various
feature extraction and architectural changes to support neural machine
translation for sign languages. We propose POSESTITCH-SLT, a novel pre-training
scheme that is inspired by linguistic-templates-based sentence generation
technique. With translation comparison on two sign language datasets, How2Sign
and iSign, we show that a simple transformer-based encoder-decoder architecture
outperforms the prior art when considering template-generated sentence pairs in
training. We achieve BLEU-4 score improvements from 1.97 to 4.56 on How2Sign
and from 0.55 to 3.43 on iSign, surpassing prior state-of-the-art methods for
pose-based gloss-free translation. The results demonstrate the effectiveness of
template-driven synthetic supervision in low-resource sign language settings.

</details>


### [9] [Language Modeling With Factorization Memory](https://arxiv.org/abs/2511.00315)
*Lee Xiong,Maksim Tkachenko,Johanes Effendi,Ting Cai*

Main category: cs.CL

TL;DR: Factorization Memory is an efficient RNN architecture that matches Transformer performance on short-context tasks while excelling at long-context generalization, with constant computational complexity during inference and optional sparse memory activation.


<details>
  <summary>Details</summary>
Motivation: To develop an RNN architecture that combines the computational efficiency of recurrent models with performance competitive to Transformers, particularly addressing the need for better long-context generalization while maintaining efficiency.

Method: Builds upon Mamba-2 to enable parallel training while preserving constant computational complexity during inference. Introduces a sparse formulation that updates only a subset of recurrent states at each step to optimize efficiency and representational capacity.

Result: Achieves performance comparable to Transformers on short-context language modeling while demonstrating superior generalization in long-context scenarios. The sparse version maintains strong performance while improving efficiency.

Conclusion: Factorization Memory represents the first RNN architecture that successfully combines sparse memory activation with competitive performance across both short and long-context settings, providing an efficient alternative to Transformers.

Abstract: We propose Factorization Memory, an efficient recurrent neural network (RNN)
architecture that achieves performance comparable to Transformer models on
short-context language modeling tasks while also demonstrating superior
generalization in long-context scenarios. Our model builds upon Mamba-2,
enabling Factorization Memory to exploit parallel computations during training
while preserving constant computational and memory complexity during inference.
To further optimize model efficiency and representational capacity, we develop
a sparse formulation of Factorization Memory that updates only a subset of
recurrent states at each step while preserving the strong performance of its
dense counterpart. To our knowledge, this represents the first RNN architecture
that successfully combines sparse memory activation with competitive
performance across both short and long-context settings. This work provides a
systematic empirical analysis of Factorization Memory in comparison to
Transformer and Mamba-2 architectures.

</details>


### [10] [Reversal Invariance in Autoregressive Language Models](https://arxiv.org/abs/2511.00341)
*Mihir Sahasrabudhe*

Main category: cs.CL

TL;DR: The paper identifies reversal invariance in causal language modeling, showing CLM treats forward and reversed text equally, which may limit capturing directional dependencies in language.


<details>
  <summary>Details</summary>
Motivation: To understand why models trained on reversed text perform similarly to forward-trained models, despite language's inherent time-asymmetry, suggesting current pretraining objectives have limitations.

Method: Formal analysis of the structural property of CLM objective, demonstrating mathematical equivalence between forward and reversed text likelihoods.

Result: CLM pretraining is direction-blind due to reversal invariance symmetry, explaining comparable performance on reversed vs forward text training.

Conclusion: Reversal invariance represents a limitation; future work should develop loss functions and architectures that explicitly model language's temporal asymmetry while maintaining modeling capacity.

Abstract: We formalize a structural property of the causal (autoregressive) language
modeling (CLM) objective: reversal invariance. Formally, the next-token
prediction loss assigns identical likelihood to a corpus and its reversal,
implying that standard CLM pretraining is direction-blind. This symmetry
explains why models trained on reversed text can achieve comparable performance
to those trained on forward text, despite the inherently time-asymmetric nature
of human language and reasoning. We argue that this invariance represents a
limitation of current pretraining objectives rather than a benign artifact. If
natural language encodes directional dependencies - phonological,
morphological, or causal - a symmetric objective may fail to capture them. We
therefore propose viewing pretraining through the lens of temporal asymmetry,
motivating future work on loss functions and architectures that explicitly
model the arrow of language while retaining standard language modeling
capacity.

</details>


### [11] [LingGym: How Far Are LLMs from Thinking Like Field Linguists?](https://arxiv.org/abs/2511.00343)
*Changbing Yang,Franklin Ma,Freda Shi,Jian Zhu*

Main category: cs.CL

TL;DR: LingGym is a benchmark evaluating LLMs' meta-linguistic reasoning using IGT and grammatical descriptions from 18 diverse languages, focusing on word-gloss inference across unseen structures.


<details>
  <summary>Details</summary>
Motivation: To assess whether LLMs can generalize linguistic inference across low-resource languages and structures not seen during training, moving beyond specific downstream tasks.

Method: Controlled Word-Gloss Inference task where models infer missing words and glosses using varying linguistic information (glosses, grammatical explanations, translations) from Interlinear Glossed Text.

Result: Incorporating structured linguistic cues leads to consistent improvements in reasoning performance across all models tested.

Conclusion: The work highlights both the promise and current limitations of using LLMs for typologically informed linguistic analysis and low-resource language documentation.

Abstract: This paper introduces LingGym, a new benchmark that evaluates LLMs' capacity
for meta-linguistic reasoning using Interlinear Glossed Text (IGT) and
grammatical descriptions extracted from 18 typologically diverse reference
grammars. Unlike previous work that focuses on specific downstream tasks, we
assess whether LLMs can generalize linguistic inference across low-resource
languages and structures not seen during training. We present a controlled
evaluation task: Word-Gloss Inference, in which the model must infer a missing
word and gloss from context using varying levels of linguistic information
(e.g., glosses, grammatical explanations, translations). Our results show that
incorporating structured linguistic cues leads to consistent improvements in
reasoning performance across all models. This work highlights both the promise
and current limitations of using LLMs for typologically informed linguistic
analysis and low-resource language documentation.

</details>


### [12] [Reasoning Trajectories for Socratic Debugging of Student Code: From Misconceptions to Contradictions and Updated Beliefs](https://arxiv.org/abs/2511.00371)
*Erfan Al-Hossami,Razvan Bunescu*

Main category: cs.CL

TL;DR: The paper introduces reasoning trajectory generation for Socratic debugging, where LLMs guide students to identify programming misconceptions through guided reasoning paths.


<details>
  <summary>Details</summary>
Motivation: To help students identify and fix programming bugs caused by misconceptions through Socratic debugging rather than direct fixes, leveraging cognitive dissonance for learning.

Method: Created a dataset of debugging problems with manually annotated reasoning trajectories, then developed LLM-based solutions to generate reasoning trajectories and Socratic conversations anchored on them.

Result: Frontier models achieved 91% correct reasoning trajectories and 98.7% valid conversation turns in large-scale LLM-as-judge evaluation.

Conclusion: LLMs can effectively generate reasoning trajectories and Socratic conversations for debugging, enabling students to identify and correct programming misconceptions through guided cognitive processes.

Abstract: In Socratic debugging, instructors guide students towards identifying and
fixing a bug on their own, instead of providing the bug fix directly. Most
novice programmer bugs are caused by programming misconceptions, namely false
beliefs about a programming concept. In this context, Socratic debugging can be
formulated as a guided Reasoning Trajectory (RT) leading to a statement about
the program behavior that contradicts the bug-causing misconception. Upon
reaching this statement, the ensuing cognitive dissonance leads the student to
first identify and then update their false belief. In this paper, we introduce
the task of reasoning trajectory generation, together with a dataset of
debugging problems manually annotated with RTs. We then describe LLM-based
solutions for generating RTs and Socratic conversations that are anchored on
them. A large-scale LLM-as-judge evaluation shows that frontier models can
generate up to 91% correct reasoning trajectories and 98.7% valid conversation
turns.

</details>


### [13] [PADBen: A Comprehensive Benchmark for Evaluating AI Text Detectors Against Paraphrase Attacks](https://arxiv.org/abs/2511.00416)
*Yiwei Zha,Rui Min,Shanu Sushmita*

Main category: cs.CL

TL;DR: Iteratively-paraphrased AI-generated text evades current detectors despite high accuracy on direct LLM outputs, revealing vulnerabilities in authorship obfuscation and plagiarism evasion scenarios.


<details>
  <summary>Details</summary>
Motivation: Current AI-generated text detectors achieve over 90% accuracy on direct LLM outputs but fail against iteratively-paraphrased content, creating security vulnerabilities that need investigation.

Method: Intrinsic mechanism analysis reveals iterative paraphrasing creates an intermediate laundering region. PADBen benchmark is introduced with five-type text taxonomy and five progressive detection tasks to systematically evaluate detector robustness against paraphrase attacks.

Result: Evaluation of 11 state-of-the-art detectors shows critical asymmetry: detectors identify plagiarism evasion but fail for authorship obfuscation. Current approaches cannot handle the intermediate laundering region effectively.

Conclusion: Fundamental advances in detection architectures are needed beyond existing semantic and stylistic discrimination methods, as current detectors are vulnerable to iterative paraphrasing attacks.

Abstract: While AI-generated text (AIGT) detectors achieve over 90\% accuracy on direct
LLM outputs, they fail catastrophically against iteratively-paraphrased
content. We investigate why iteratively-paraphrased text -- itself AI-generated
-- evades detection systems designed for AIGT identification. Through intrinsic
mechanism analysis, we reveal that iterative paraphrasing creates an
intermediate laundering region characterized by semantic displacement with
preserved generation patterns, which brings up two attack categories:
paraphrasing human-authored text (authorship obfuscation) and paraphrasing
LLM-generated text (plagiarism evasion). To address these vulnerabilities, we
introduce PADBen, the first benchmark systematically evaluating detector
robustness against both paraphrase attack scenarios. PADBen comprises a
five-type text taxonomy capturing the full trajectory from original content to
deeply laundered text, and five progressive detection tasks across
sentence-pair and single-sentence challenges. We evaluate 11 state-of-the-art
detectors, revealing critical asymmetry: detectors successfully identify the
plagiarism evasion problem but fail for the case of authorship obfuscation. Our
findings demonstrate that current detection approaches cannot effectively
handle the intermediate laundering region, necessitating fundamental advances
in detection architectures beyond existing semantic and stylistic
discrimination methods. For detailed code implementation, please see
https://github.com/JonathanZha47/PadBen-Paraphrase-Attack-Benchmark.

</details>


### [14] [MedRECT: A Medical Reasoning Benchmark for Error Correction in Clinical Texts](https://arxiv.org/abs/2511.00421)
*Naoto Iwase,Hiroki Okuyama,Junichiro Iwasawa*

Main category: cs.CL

TL;DR: MedRECT is a cross-lingual benchmark for medical error correction in Japanese and English, evaluating LLMs on error detection, localization, and correction. Reasoning models outperform standard architectures, and fine-tuned models exceed human expert performance.


<details>
  <summary>Details</summary>
Motivation: LLMs show promise in medical applications but their error detection and correction capabilities remain under-evaluated, especially beyond English, which is crucial for safe deployment.

Method: Built MedRECT benchmark from Japanese Medical Licensing Exams and English counterpart, evaluated 9 LLMs across proprietary, open-weight, and reasoning families, and conducted targeted LoRA fine-tuning.

Result: Reasoning models substantially outperform standard architectures (up to 13.5% improvement in detection, 51% in extraction). Cross-lingual gaps of 5-10% from English to Japanese. Fine-tuned models achieved asymmetric improvements (+0.078 Japanese, +0.168 English) and exceeded human expert performance.

Conclusion: MedRECT is the first comprehensive cross-lingual benchmark for medical error correction, providing a reproducible framework for developing safer medical LLMs across languages.

Abstract: Large language models (LLMs) show increasing promise in medical applications,
but their ability to detect and correct errors in clinical texts -- a
prerequisite for safe deployment -- remains under-evaluated, particularly
beyond English. We introduce MedRECT, a cross-lingual benchmark
(Japanese/English) that formulates medical error handling as three subtasks:
error detection, error localization (sentence extraction), and error
correction. MedRECT is built with a scalable, automated pipeline from the
Japanese Medical Licensing Examinations (JMLE) and a curated English
counterpart, yielding MedRECT-ja (663 texts) and MedRECT-en (458 texts) with
comparable error/no-error balance. We evaluate 9 contemporary LLMs spanning
proprietary, open-weight, and reasoning families. Key findings: (i) reasoning
models substantially outperform standard architectures, with up to 13.5%
relative improvement in error detection and 51.0% in sentence extraction; (ii)
cross-lingual evaluation reveals 5-10% performance gaps from English to
Japanese, with smaller disparities for reasoning models; (iii) targeted LoRA
fine-tuning yields asymmetric improvements in error correction performance
(Japanese: +0.078, English: +0.168) while preserving reasoning capabilities;
and (iv) our fine-tuned model exceeds human expert performance on structured
medical error correction tasks. To our knowledge, MedRECT is the first
comprehensive cross-lingual benchmark for medical error correction, providing a
reproducible framework and resources for developing safer medical LLMs across
languages.

</details>


### [15] [G2: Guided Generation for Enhanced Output Diversity in LLMs](https://arxiv.org/abs/2511.00432)
*Zhiwen Ruan,Yixia Li,Yefeng Liu,Yun Chen,Weihua Luo,Peng Li,Yang Liu,Guanhua Chen*

Main category: cs.CL

TL;DR: G2 is a training-free plug-and-play method that enhances LLM output diversity while preserving quality, using dual Guides to intervene in decoding for more diverse outputs.


<details>
  <summary>Details</summary>
Motivation: LLMs generate highly similar content across attempts, limiting diversity in tasks like creative writing and reasoning. Existing solutions like temperature scaling improve diversity but compromise quality.

Method: G2 employs a base generator with dual Guides that perform decoding-based interventions to encourage diverse outputs conditioned on the original query, without requiring training.

Result: Comprehensive experiments show G2 effectively improves output diversity while maintaining optimal balance between diversity and quality.

Conclusion: G2 successfully addresses LLM output diversity limitations through training-free decoding interventions, achieving better diversity-quality balance than existing methods.

Abstract: Large Language Models (LLMs) have demonstrated exceptional performance across
diverse natural language processing tasks. However, these models exhibit a
critical limitation in output diversity, often generating highly similar
content across multiple attempts. This limitation significantly affects tasks
requiring diverse outputs, from creative writing to reasoning. Existing
solutions, like temperature scaling, enhance diversity by modifying probability
distributions but compromise output quality. We propose Guide-to-Generation
(G2), a training-free plug-and-play method that enhances output diversity while
preserving generation quality. G2 employs a base generator alongside dual
Guides, which guide the generation process through decoding-based interventions
to encourage more diverse outputs conditioned on the original query.
Comprehensive experiments demonstrate that G2 effectively improves output
diversity while maintaining an optimal balance between diversity and quality.

</details>


### [16] [Remembering Unequally: Global and Disciplinary Bias in LLM-Generated Co-Authorship Networks](https://arxiv.org/abs/2511.00476)
*Ghazal Kalhor,Afra Mashhadi*

Main category: cs.CL

TL;DR: This study examines how LLM memorization affects co-authorship networks, revealing biases favoring highly cited researchers across three major models, with variations across disciplines and regions.


<details>
  <summary>Details</summary>
Motivation: As LLMs reshape scholarly search tools, their memorization capabilities introduce fairness and bias concerns that could undermine information integrity, particularly in co-authorship network generation.

Method: Analyzed memorization effects across three LLMs (DeepSeek R1, Llama 4 Scout, Mixtral 8x7B) by examining how memorization-driven outputs vary across academic disciplines and world regions.

Result: Global analysis shows consistent bias favoring highly cited researchers, but this pattern is not uniform - Clinical Medicine and some African regions show more balanced representation, suggesting areas of greater equity in training data.

Conclusion: LLM deployment for scholarly discovery presents both risks (bias amplification) and opportunities (potential for equitable representation), highlighting the need for careful consideration of training data diversity.

Abstract: Ongoing breakthroughs in Large Language Models (LLMs) are reshaping search
and recommendation platforms at their core. While this shift unlocks powerful
new scientometric tools, it also exposes critical fairness and bias issues that
could erode the integrity of the information ecosystem. Additionally, as LLMs
become more integrated into web-based searches for scholarly tools, their
ability to generate summarized research work based on memorized data introduces
new dimensions to these challenges. The extent of memorization in LLMs can
impact the accuracy and fairness of the co-authorship networks they produce,
potentially reflecting and amplifying existing biases within the scientific
community and across different regions. This study critically examines the
impact of LLM memorization on the co-authorship networks. To this end, we
assess memorization effects across three prominent models, DeepSeek R1, Llama 4
Scout, and Mixtral 8x7B, analyzing how memorization-driven outputs vary across
academic disciplines and world regions. While our global analysis reveals a
consistent bias favoring highly cited researchers, this pattern is not
uniformly observed. Certain disciplines, such as Clinical Medicine, and
regions, including parts of Africa, show more balanced representation, pointing
to areas where LLM training data may reflect greater equity. These findings
underscore both the risks and opportunities in deploying LLMs for scholarly
discovery.

</details>


### [17] [Leveraging the Cross-Domain & Cross-Linguistic Corpus for Low Resource NMT: A Case Study On Bhili-Hindi-English Parallel Corpus](https://arxiv.org/abs/2511.00486)
*Pooja Singh,Shashwat Bhardwaj,Vaibhav Sharma,Sandeep Kumar*

Main category: cs.CL

TL;DR: Created the first large-scale Bhili-Hindi-English parallel corpus (BHEPC) with 110K sentences and established machine translation benchmarks for the underrepresented Bhili language.


<details>
  <summary>Details</summary>
Motivation: Address the lack of linguistic resources for underrepresented tribal languages like Bhili in India, which poses significant machine translation challenges.

Method: Built BHEPC corpus with expert human translators covering education, administration, and news domains. Evaluated proprietary and open-source MLLMs on bidirectional translation tasks, including fine-tuning NLLB-200 and testing in-context learning capabilities.

Result: Fine-tuned NLLB-200 distilled 600M variant model outperformed other models. Comprehensive evaluation established benchmarks and assessed cross-domain generalization and distributional divergence.

Conclusion: This work bridges critical resource gaps and promotes inclusive NLP technologies for low-resource and marginalized languages globally.

Abstract: The linguistic diversity of India poses significant machine translation
challenges, especially for underrepresented tribal languages like Bhili, which
lack high-quality linguistic resources. This paper addresses the gap by
introducing Bhili-Hindi-English Parallel Corpus (BHEPC), the first and largest
parallel corpus worldwide comprising 110,000 meticulously curated sentences
across Bhili, Hindi, and English. The corpus was created with the assistance of
expert human translators. BHEPC spans critical domains such as education,
administration, and news, establishing a valuable benchmark for research in low
resource machine translation. To establish a comprehensive Bhili Machine
Translation benchmark, we evaluated a wide range of proprietary and open-source
Multilingual Large Language Models (MLLMs) on bidirectional translation tasks
between English/Hindi and Bhili. Comprehensive evaluation demonstrates that the
fine-tuned NLLB-200 distilled 600M variant model outperforms others,
highlighting the potential of multilingual models in low resource scenarios.
Furthermore, we investigated the generative translation capabilities of
multilingual LLMs on BHEPC using in-context learning, assessing performance
under cross-domain generalization and quantifying distributional divergence.
This work bridges a critical resource gap and promotes inclusive natural
language processing technologies for low-resource and marginalized languages
globally.

</details>


### [18] [With Privacy, Size Matters: On the Importance of Dataset Size in Differentially Private Text Rewriting](https://arxiv.org/abs/2511.00487)
*Stephen Meisenbacher,Florian Matthes*

Main category: cs.CL

TL;DR: The paper investigates how dataset size affects the privacy-utility trade-off in differentially private text rewriting mechanisms, finding that larger datasets significantly impact evaluation outcomes.


<details>
  <summary>Details</summary>
Motivation: Previous DP NLP research has overlooked the impact of dataset size on mechanism efficacy for both utility and privacy preservation.

Method: Designed utility and privacy tests on large-scale datasets with dynamic split sizes, testing on datasets up to one million texts to quantify the effect of increasing dataset size.

Result: Dataset size plays an integral role in evaluating DP text rewriting mechanisms, revealing significant impacts on the privacy-utility trade-off.

Conclusion: Findings call for more rigorous evaluation procedures in DP NLP and provide insights for practical DP NLP applications at scale.

Abstract: Recent work in Differential Privacy with Natural Language Processing (DP NLP)
has proposed numerous promising techniques in the form of text rewriting
mechanisms. In the evaluation of these mechanisms, an often-ignored aspect is
that of dataset size, or rather, the effect of dataset size on a mechanism's
efficacy for utility and privacy preservation. In this work, we are the first
to introduce this factor in the evaluation of DP text privatization, where we
design utility and privacy tests on large-scale datasets with dynamic split
sizes. We run these tests on datasets of varying size with up to one million
texts, and we focus on quantifying the effect of increasing dataset size on the
privacy-utility trade-off. Our findings reveal that dataset size plays an
integral part in evaluating DP text rewriting mechanisms; additionally, these
findings call for more rigorous evaluation procedures in DP NLP, as well as
shed light on the future of DP NLP in practice and at scale.

</details>


### [19] [ToM: Leveraging Tree-oriented MapReduce for Long-Context Reasoning in Large Language Models](https://arxiv.org/abs/2511.00489)
*Jiani Guo,Zuchao Li,Jie Wu,Qianren Wang,Yun Li,Lefei Zhang,Hai Zhao,Yujiu Yang*

Main category: cs.CL

TL;DR: ToM is a Tree-oriented MapReduce framework that improves long-context reasoning in LLMs by leveraging document hierarchical structure through recursive bottom-up aggregation, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods like RAG and divide-and-conquer frameworks struggle with logical coherence and long-range dependencies when reasoning over long contexts, due to similarity-based rankings and isolated chunk processing.

Method: ToM constructs a DocTree through hierarchical semantic parsing of document structure (headings/subheadings), then performs recursive Tree MapReduce: Map step generates rationales at child nodes, Reduce step aggregates rationales across siblings to resolve conflicts at parent nodes.

Result: Experimental results on 70B+ LLMs show ToM significantly outperforms existing divide-and-conquer frameworks and retrieval-augmented generation methods, achieving better logical coherence and long-context reasoning.

Conclusion: ToM effectively addresses limitations of current long-context reasoning methods by leveraging document hierarchy and recursive aggregation, demonstrating superior performance in maintaining logical coherence across long documents.

Abstract: Large Language Models (LLMs), constrained by limited context windows, often
face significant performance degradation when reasoning over long contexts. To
address this, Retrieval-Augmented Generation (RAG) retrieves and reasons over
chunks but frequently sacrifices logical coherence due to its reliance on
similarity-based rankings. Similarly, divide-and-conquer frameworks (DCF) split
documents into small chunks for independent reasoning and aggregation. While
effective for local reasoning, DCF struggles to capture long-range dependencies
and risks inducing conflicts by processing chunks in isolation. To overcome
these limitations, we propose ToM, a novel Tree-oriented MapReduce framework
for long-context reasoning. ToM leverages the inherent hierarchical structure
of long documents (e.g., main headings and subheadings) by constructing a
DocTree through hierarchical semantic parsing and performing bottom-up
aggregation. Using a Tree MapReduce approach, ToM enables recursive reasoning:
in the Map step, rationales are generated at child nodes; in the Reduce step,
these rationales are aggregated across sibling nodes to resolve conflicts or
reach consensus at parent nodes. Experimental results on 70B+ LLMs show that
ToM significantly outperforms existing divide-and-conquer frameworks and
retrieval-augmented generation methods, achieving better logical coherence and
long-context reasoning. Our code is available at
https://github.com/gjn12-31/ToM .

</details>


### [20] [Zero-RAG: Towards Retrieval-Augmented Generation with Zero Redundant Knowledge](https://arxiv.org/abs/2511.00505)
*Qi Luo,Xiaonan Li,Junqi Dai,Shuang Cheng,Xipeng Qiu*

Main category: cs.CL

TL;DR: Zero-RAG addresses knowledge redundancy in RAG systems by pruning redundant external knowledge and improving LLM's utilization of internal knowledge, achieving 30% corpus reduction and 22% retrieval speedup without performance loss.


<details>
  <summary>Details</summary>
Motivation: Current RAG systems suffer from significant knowledge redundancy between external corpus and LLMs' internal knowledge, which increases retrieval costs and hurts performance on questions that LLMs can answer themselves.

Method: Proposes Mastery-Score metric to identify redundant knowledge for corpus pruning, Query Router to avoid irrelevant documents, and Noise-Tolerant Tuning to improve LLM's internal knowledge utilization with pruned corpus.

Result: Zero-RAG prunes Wikipedia corpus by 30% and accelerates retrieval stage by 22%, while maintaining RAG performance.

Conclusion: Zero-RAG effectively addresses knowledge redundancy in RAG systems, reducing corpus size and retrieval time while preserving performance through better utilization of LLM's internal knowledge.

Abstract: Retrieval-Augmented Generation has shown remarkable results to address Large
Language Models' hallucinations, which usually uses a large external corpus to
supplement knowledge to LLMs. However, with the development of LLMs, the
internal knowledge of LLMs has expanded significantly, thus causing significant
knowledge redundancy between the external corpus and LLMs. On the one hand, the
indexing cost of dense retrieval is highly related to the corpus size and thus
significant redundant knowledge intensifies the dense retrieval's workload. On
the other hand, the redundant knowledge in the external corpus is not helpful
to LLMs and our exploratory analysis shows that it instead hurts the RAG
performance on those questions which the LLM can answer by itself. To address
these issues, we propose Zero-RAG to tackle these challenges. Specifically, we
first propose the Mastery-Score metric to identify redundant knowledge in the
RAG corpus to prune it. After pruning, answers to "mastered" questions rely
primarily on internal knowledge of the LLM. To better harness the internal
capacity, we propose Query Router and Noise-Tolerant Tuning to avoid the
irrelevant documents' distraction and thus further improve the LLM's
utilization of internal knowledge with pruned corpus. Experimental results show
that Zero-RAG prunes the Wikipedia corpus by 30\% and accelerates the retrieval
stage by 22\%, without compromising RAG's performance.

</details>


### [21] [Fine-Tuning DialoGPT on Common Diseases in Rural Nepal for Medical Conversations](https://arxiv.org/abs/2511.00514)
*Birat Poudel,Satyam Ghimire,Er. Prakash Chandra Prasad*

Main category: cs.CL

TL;DR: Fine-tuned DialoGPT model for offline healthcare conversations in rural Nepal, showing coherent medical responses despite limited training data.


<details>
  <summary>Details</summary>
Motivation: To support healthcare delivery in resource-constrained rural Nepal where internet connectivity and cloud infrastructure are often unavailable.

Method: Fine-tuned DialoGPT (lightweight generative dialogue model) on a synthetically constructed dataset of doctor-patient interactions covering ten common diseases in rural Nepal.

Result: The fine-tuned model produced coherent, contextually relevant, and medically appropriate responses with understanding of symptoms, disease context, and empathetic communication.

Conclusion: Compact offline-capable dialogue models with targeted datasets are effective for domain adaptation in low-resource healthcare environments, offering promising directions for rural medical conversational AI.

Abstract: Conversational agents are increasingly being explored to support healthcare
delivery, particularly in resource-constrained settings such as rural Nepal.
Large-scale conversational models typically rely on internet connectivity and
cloud infrastructure, which may not be accessible in rural areas. In this
study, we fine-tuned DialoGPT, a lightweight generative dialogue model that can
operate offline, on a synthetically constructed dataset of doctor-patient
interactions covering ten common diseases prevalent in rural Nepal, including
common cold, seasonal fever, diarrhea, typhoid fever, gastritis, food
poisoning, malaria, dengue fever, tuberculosis, and pneumonia. Despite being
trained on a limited, domain-specific dataset, the fine-tuned model produced
coherent, contextually relevant, and medically appropriate responses,
demonstrating an understanding of symptoms, disease context, and empathetic
communication. These results highlight the adaptability of compact,
offline-capable dialogue models and the effectiveness of targeted datasets for
domain adaptation in low-resource healthcare environments, offering promising
directions for future rural medical conversational AI.

</details>


### [22] [Exploring and Mitigating Gender Bias in Encoder-Based Transformer Models](https://arxiv.org/abs/2511.00519)
*Ariyan Hossain,Khondokar Mohammad Ahanaf Hannan,Rakinul Haque,Nowreen Tarannum Rafa,Humayra Musarrat,Shoaib Ahmed Dipu,Farig Yousuf Sadeque*

Main category: cs.CL

TL;DR: The paper investigates gender bias in transformer models (BERT, ALBERT, RoBERTa, DistilBERT) and proposes a mitigation approach using Counterfactual Data Augmentation, achieving significant bias reduction without performance loss.


<details>
  <summary>Details</summary>
Motivation: Encoder-based transformer models exhibit strong gender biases inherited from training data, which is a critical issue in natural language processing that needs addressing.

Method: Introduced MALoR metric to quantify bias using masked token probabilities, and proposed continued pre-training on gender-balanced datasets generated via Counterfactual Data Augmentation.

Result: Significant bias reduction: BERT-base "he-she" bias dropped from 1.27 to 0.08, "his-her" from 2.51 to 0.36; BERT-large "male-female" bias decreased from 1.82 to 0.10. Similar improvements across other models.

Conclusion: The proposed approach effectively reduces gender bias in contextualized word embeddings without compromising model performance on downstream tasks.

Abstract: Gender bias in language models has gained increasing attention in the field
of natural language processing. Encoder-based transformer models, which have
achieved state-of-the-art performance in various language tasks, have been
shown to exhibit strong gender biases inherited from their training data. This
paper investigates gender bias in contextualized word embeddings, a crucial
component of transformer-based models. We focus on prominent architectures such
as BERT, ALBERT, RoBERTa, and DistilBERT to examine their vulnerability to
gender bias. To quantify the degree of bias, we introduce a novel metric,
MALoR, which assesses bias based on model probabilities for filling masked
tokens. We further propose a mitigation approach involving continued
pre-training on a gender-balanced dataset generated via Counterfactual Data
Augmentation. Our experiments reveal significant reductions in gender bias
scores across different pronoun pairs. For instance, in BERT-base, bias scores
for "he-she" dropped from 1.27 to 0.08, and "his-her" from 2.51 to 0.36
following our mitigation approach. We also observed similar improvements across
other models, with "male-female" bias decreasing from 1.82 to 0.10 in
BERT-large. Our approach effectively reduces gender bias without compromising
model performance on downstream tasks.

</details>


### [23] [Word Salad Chopper: Reasoning Models Waste A Ton Of Decoding Budget On Useless Repetitions, Self-Knowingly](https://arxiv.org/abs/2511.00536)
*Wenya Xie,Shaochen,Zhong,Hoang Anh Duy Le,Zhaozhuo Xu,Jianwen Xie,Zirui Liu*

Main category: cs.CL

TL;DR: WordSaladChopper (WSC) detects and removes useless self-repetitions in Large Reasoning Models to reduce output token costs without significant quality loss.


<details>
  <summary>Details</summary>
Motivation: Large Reasoning Models waste decoding budget on useless self-repetitions (word salad) that don't add semantic value, increasing costs.

Method: Detect word salad patterns via hidden states of <\n\n> tokens using a linear classifier, then chop and regenerate with a simple prompt.

Result: Substantial length savings with minimal quality loss, offering a lightweight turnkey component for LRM applications.

Conclusion: WSC or similar components are essential for LRM applications to improve user experience by removing semantically redundant tokens.

Abstract: Large Reasoning Models (LRMs) are often bottlenecked by the high cost of
output tokens. We show that a significant portion of these tokens are useless
self-repetitions - what we call "word salad" - that exhaust the decoding budget
without adding value. Interestingly, we observe that LRMs are self-aware when
trapped in these loops: the hidden states of <\n\n> tokens trailing each
reasoning chunk exhibit patterns that allow us to detect word salad behavior
on-the-fly via a single-layer linear classifier. Once detected, a simple chop
appended by a straightforward regeneration prompt yields substantial length
savings with minimal quality loss. Our work offers WordSaladChopper (WSC) - a
lightweight, turnkey component for LRM that is minimally invasive to its
reasoning trajectory by only removing semantically redundant tokens. Given its
low overhead, strong savings, and the lack of semantic value of word salad
tokens, we believe it is not too far-fetched to argue that WSC - or a similar
component - is a must-have for all LRM applications with user experience in
mind. Our code is publicly available at
https://github.com/wenyaxie023/WordSaladChopper.

</details>


### [24] [Multi-refined Feature Enhanced Sentiment Analysis Using Contextual Instruction](https://arxiv.org/abs/2511.00537)
*Peter Atandoh,Jie Zou,Weikang Guo,Jiwei Wei,Zheng Wang*

Main category: cs.CL

TL;DR: CISEA-MRFE is a novel PLM-based framework that improves sentiment analysis by addressing limitations in handling nuanced emotions, domain shifts, and imbalanced distributions through contextual instructions, semantic enhancement, and multi-scale feature extraction.


<details>
  <summary>Details</summary>
Motivation: Existing sentiment analysis approaches underperform with nuanced emotional cues, domain shifts, and imbalanced sentiment distributions due to inadequate semantic grounding, poor generalization, and biases toward dominant sentiment classes.

Method: Proposes CISEA-MRFE framework with three components: Contextual Instruction (CI) for domain-aware sentiment disambiguation, Semantic Enhancement Augmentation (SEA) for robustness through sentiment-consistent paraphrastic augmentation, and Multi-Refined Feature Extraction (MRFE) combining Scale-Adaptive Depthwise Encoder (SADE) for multi-scale features and Emotion Evaluator Context Encoder (EECE) for affect-aware modeling.

Result: Outperforms strong baselines on four benchmark datasets with relative accuracy improvements: 4.6% on IMDb, 6.5% on Yelp, 30.3% on Twitter, and 4.1% on Amazon.

Conclusion: The framework demonstrates effectiveness and generalization ability for sentiment classification across varied domains, validating the proposed approach.

Abstract: Sentiment analysis using deep learning and pre-trained language models (PLMs)
has gained significant traction due to their ability to capture rich contextual
representations. However, existing approaches often underperform in scenarios
involving nuanced emotional cues, domain shifts, and imbalanced sentiment
distributions. We argue that these limitations stem from inadequate semantic
grounding, poor generalization to diverse linguistic patterns, and biases
toward dominant sentiment classes. To overcome these challenges, we propose
CISEA-MRFE, a novel PLM-based framework integrating Contextual Instruction
(CI), Semantic Enhancement Augmentation (SEA), and Multi-Refined Feature
Extraction (MRFE). CI injects domain-aware directives to guide sentiment
disambiguation; SEA improves robustness through sentiment-consistent
paraphrastic augmentation; and MRFE combines a Scale-Adaptive Depthwise Encoder
(SADE) for multi-scale feature specialization with an Emotion Evaluator Context
Encoder (EECE) for affect-aware sequence modeling. Experimental results on four
benchmark datasets demonstrate that CISEA-MRFE consistently outperforms strong
baselines, achieving relative improvements in accuracy of up to 4.6% on IMDb,
6.5% on Yelp, 30.3% on Twitter, and 4.1% on Amazon. These results validate the
effectiveness and generalization ability of our approach for sentiment
classification across varied domains.

</details>


### [25] [Friend or Foe: How LLMs' Safety Mind Gets Fooled by Intent Shift Attack](https://arxiv.org/abs/2511.00556)
*Peng Ding,Jun Kuang,Wen Sun,Zongyu Wang,Xuezhi Cao,Xunliang Cai,Jiajun Chen,Shujian Huang*

Main category: cs.CL

TL;DR: ISA (Intent Shift Attack) is a jailbreaking method that transforms harmful intents into seemingly benign requests through minimal edits, achieving over 70% higher success rates than direct attacks and nearly 100% success when models are fine-tuned on ISA-reformulated data.


<details>
  <summary>Details</summary>
Motivation: Existing jailbreaking attacks rely on distracting LLMs with additional context or adversarial tokens without changing the core harmful intent, leaving fundamental vulnerabilities unexplored. Investigating intent-based attacks is crucial for developing robust safety mechanisms.

Method: ISA establishes a taxonomy of intent transformations and uses them to generate attacks that mislead LLMs into perceiving harmful requests as benign information-seeking queries. The approach requires only minimal edits to original requests, producing natural and human-readable prompts.

Result: Extensive experiments show ISA achieves over 70% improvement in attack success rate compared to direct harmful prompts. When models are fine-tuned on benign data reformulated with ISA templates, success rates reach nearly 100%. Existing defenses prove inadequate against ISA.

Conclusion: ISA reveals fundamental challenges in intent inference for LLM safety and demonstrates the need for more effective defenses beyond current methods. The findings underscore the vulnerability of LLMs to intent-based manipulation through minimal, natural-looking modifications.

Abstract: Large language models (LLMs) remain vulnerable to jailbreaking attacks
despite their impressive capabilities. Investigating these weaknesses is
crucial for robust safety mechanisms. Existing attacks primarily distract LLMs
by introducing additional context or adversarial tokens, leaving the core
harmful intent unchanged. In this paper, we introduce ISA (Intent Shift
Attack), which obfuscates LLMs about the intent of the attacks. More
specifically, we establish a taxonomy of intent transformations and leverage
them to generate attacks that may be misperceived by LLMs as benign requests
for information. Unlike prior methods relying on complex tokens or lengthy
context, our approach only needs minimal edits to the original request, and
yields natural, human-readable, and seemingly harmless prompts. Extensive
experiments on both open-source and commercial LLMs show that ISA achieves over
70% improvement in attack success rate compared to direct harmful prompts. More
critically, fine-tuning models on only benign data reformulated with ISA
templates elevates success rates to nearly 100%. For defense, we evaluate
existing methods and demonstrate their inadequacy against ISA, while exploring
both training-free and training-based mitigation strategies. Our findings
reveal fundamental challenges in intent inference for LLMs safety and
underscore the need for more effective defenses. Our code and datasets are
available at https://github.com/NJUNLP/ISA.

</details>


### [26] [FlashEVA: Accelerating LLM inference via Efficient Attention](https://arxiv.org/abs/2511.00576)
*Juan Gabriel Kostelec,Qinghai Guo*

Main category: cs.CL

TL;DR: FlashEVA enables efficient transformer inference with 6.7x higher throughput and 5x lower GPU memory usage by fine-tuning models to adapt to efficient attention via control variates.


<details>
  <summary>Details</summary>
Motivation: Transformer models face significant memory challenges during inference due to maintaining full context in memory, limiting their practical deployment despite their state-of-the-art performance.

Method: Develop FlashEVA, an efficient implementation of EVA (Efficient Attention via Control Variates), and demonstrate how to finetune transformers to adapt to this attention mechanism using as few as 1.5B tokens.

Result: Achieves up to 6.7x higher throughput and 5x lower peak GPU memory usage during inference while preserving effectiveness across various downstream tasks, though with limitations in retrieval-focused tasks.

Conclusion: FlashEVA represents a significant step towards more efficient and adaptable Transformer-based models for inference, offering control over throughput-accuracy trade-offs through adjustable hyperparameters.

Abstract: Transformer models have revolutionized natural language processing, achieving
state-of-the-art performance and demonstrating remarkable scalability. However,
their memory demands, particularly due to maintaining full context in memory,
pose significant challenges for inference. In this paper, we present FlashEVA,
an efficient implementation of EVA (Efficient Attention via Control Variates),
and demonstrate how to finetune transformers to adapt to FlashEVA attention.
Our method enables fine-tuning of Transformer models with as few as 1.5B tokens
while preserving effectiveness across various downstream tasks. Notably,
FlashEVA achieves up to 6.7x higher throughput and 5x lower peak GPU memory
usage during inference compared to standard Transformer implementations.
Despite these improvements, we observe limitations in retrieval-focused tasks.
Our implementation offers control over the trade-off between throughput and
accuracy through adjustable hyperparameters, providing flexibility for diverse
use cases. This work represents a significant step towards more efficient and
adaptable Transformer-based models for inference.

</details>


### [27] [OpenSIR: Open-Ended Self-Improving Reasoner](https://arxiv.org/abs/2511.00602)
*Wai-Chung Kwan,Joshua Ong Jun Leang,Pavlos Vougiouklis,Jeff Z. Pan,Marco Valentino,Pasquale Minervini*

Main category: cs.CL

TL;DR: OpenSIR is a self-play framework where LLMs learn to generate and solve novel mathematical problems without external supervision, achieving significant performance improvements on reasoning benchmarks through open-ended learning.


<details>
  <summary>Details</summary>
Motivation: Current LLM reasoning methods rely on annotated datasets that limit surpassing human-level performance, while existing self-play approaches depend on external verifiers or cannot learn open-endedly.

Method: OpenSIR uses alternating teacher-student roles where the LLM generates novel problems optimized for difficulty and diversity, and solves them without external supervision, starting from a single trivial seed problem.

Result: Substantial improvements: Llama-3.2-3B-Instruct advanced from 73.9 to 78.3 on GSM8K and from 28.8 to 34.4 on College Math; Gemma-2-2B-Instruct rose from 38.5 to 58.7 on GSM8K.

Conclusion: OpenSIR enables open-ended mathematical discovery through co-evolving teacher-student roles that adaptively calibrate difficulty and drive diverse exploration, progressing autonomously from basic to advanced mathematics.

Abstract: Recent advances in large language model (LLM) reasoning through reinforcement
learning rely on annotated datasets for verifiable rewards, which may limit
models' ability to surpass human-level performance. While self-play offers a
promising alternative, existing approaches depend on external verifiers or
cannot learn open-endedly. We present Open-Ended Self-Improving Reasoner
(OpenSIR), a self-play framework where an LLM learns to generate and solve
novel problems by alternating teacher and student roles without external
supervision. To generate novel problems, OpenSIR optimises for both difficulty
and diversity, rewarding problems that challenge appropriately while exploring
distinct concepts, enabling open-ended mathematical discovery. Starting from a
single trivial seed problem, OpenSIR substantially improves instruction models:
Llama-3.2-3B-Instruct advances from 73.9 to 78.3 on GSM8K, and from 28.8 to
34.4 on College Math, while Gemma-2-2B-Instruct rises from 38.5 to 58.7 on
GSM8K. Our analyses reveal that OpenSIR achieves open-ended learning through
co-evolving teacher-student roles that adaptively calibrate difficulty and
drive diverse exploration, progressing autonomously from basic to advanced
mathematics.

</details>


### [28] [SpecDiff-2: Scaling Diffusion Drafter Alignment For Faster Speculative Decoding](https://arxiv.org/abs/2511.00606)
*Jameson Sandler,Jacob K. Christopher,Thomas Hartvigsen,Nando Fioretto*

Main category: cs.CL

TL;DR: SpecDiff-2 is a novel speculative decoding framework that uses discrete diffusion as a non-autoregressive drafter to overcome parallelism limitations and develops calibration techniques to reduce draft token rejections, achieving up to 5.5x speed-up over standard decoding without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Current speculative decoding approaches are limited by two bottlenecks: (1) autoregressive dependency during drafting that limits parallelism, and (2) frequent rejections of draft tokens due to misalignment between draft and verify models.

Method: Leverages discrete diffusion as a non-autoregressive drafter to address parallelism limitations and develops novel techniques to calibrate discrete diffusion drafters with autoregressive verifiers to reduce token rejections.

Result: Achieves state-of-the-art performance across reasoning, coding, and mathematical benchmarks with up to +55% improvement in tokens-per-second over previous baselines and up to 5.5x average speed-up over standard decoding, without accuracy loss.

Conclusion: SpecDiff-2 successfully addresses the fundamental bottlenecks in speculative decoding through non-autoregressive drafting and improved calibration, delivering significant speed improvements while maintaining accuracy.

Abstract: Speculative decoding has become the standard approach for accelerating Large
Language Model (LLM) inference. It exploits a lossless draft-then-verify
procedure to circumvent the latency of autoregressive decoding, achieving
impressive speed-ups. Yet, current speculative decoding approaches remain
limited by two fundamental bottlenecks: (1) the autoregressive dependency
during drafting which limits parallelism, and (2) frequent rejections of draft
tokens caused by misalignment between the draft and verify models. This paper
proposes SpecDiff-2, a novel framework to jointly address these two
bottlenecks. It leverages discrete diffusion as a non-autoregressive drafter to
address bottleneck (1) and develops novel techniques to calibrate discrete
diffusion drafters with autoregressive verifiers, addressing bottleneck (2).
Experimental results across a comprehensive benchmark suite show that
SpecDiff-2 achieves a new state-of-the-art across reasoning, coding, and
mathematical benchmarks, improving tokens-per-second by up to an average of
+55% over previous baselines and obtaining up to 5.5x average speed-up over
standard decoding, without any loss of accuracy.

</details>


### [29] [Certain but not Probable? Differentiating Certainty from Probability in LLM Token Outputs for Probabilistic Scenarios](https://arxiv.org/abs/2511.00620)
*Autumn Toney-Wails,Ryan Wails*

Main category: cs.CL

TL;DR: The study examines how well language models' token-level probabilities align with theoretical distributions in probabilistic scenarios, finding that while models achieve perfect response accuracy, their probability distributions diverge from expected theoretical values.


<details>
  <summary>Details</summary>
Motivation: Reliable uncertainty quantification is crucial for trustworthy deployment of large language models in decision-support applications, especially in probabilistic scenarios where output probabilities should align with theoretical distributions.

Method: Evaluated GPT-4.1 and DeepSeek-Chat on ten probabilistic prompts (e.g., rolling dice) with and without explicit probability cues, measuring response validity and alignment between token-level probabilities and theoretical distributions.

Result: Both models achieved perfect response accuracy across all scenarios, but their token-level probability and entropy values consistently diverged from theoretical distributions.

Conclusion: Current token-level probability estimation methods are inadequate for probabilistic scenarios, as models can produce correct responses while their underlying probability distributions don't align with theoretical expectations.

Abstract: Reliable uncertainty quantification (UQ) is essential for ensuring
trustworthy downstream use of large language models, especially when they are
deployed in decision-support and other knowledge-intensive applications. Model
certainty can be estimated from token logits, with derived probability and
entropy values offering insight into performance on the prompt task. However,
this approach may be inadequate for probabilistic scenarios, where the
probabilities of token outputs are expected to align with the theoretical
probabilities of the possible outcomes. We investigate the relationship between
token certainty and alignment with theoretical probability distributions in
well-defined probabilistic scenarios. Using GPT-4.1 and DeepSeek-Chat, we
evaluate model responses to ten prompts involving probability (e.g., roll a
six-sided die), both with and without explicit probability cues in the prompt
(e.g., roll a fair six-sided die). We measure two dimensions: (1) response
validity with respect to scenario constraints, and (2) alignment between
token-level output probabilities and theoretical probabilities. Our results
indicate that, while both models achieve perfect in-domain response accuracy
across all prompt scenarios, their token-level probability and entropy values
consistently diverge from the corresponding theoretical distributions.

</details>


### [30] [Modeling the Construction of a Literary Archetype: The Case of the Detective Figure in French Literature](https://arxiv.org/abs/2511.00627)
*Jean Barr,Olga Seminck,Antoine Bourgois,Thierry Poibeau*

Main category: cs.CL

TL;DR: Computational analysis shows the detective archetype in French fiction evolves from secondary character to central "reasoning machine" in classical stories, then becomes more complex with moral ambiguity after WWII hardboiled influence.


<details>
  <summary>Details</summary>
Motivation: To understand the evolution of the detective archetype across 150 years of French detective fiction using computational methods.

Method: Quantitative analysis and character-level embeddings with supervised modeling to track detective archetype evolution from 1866 to 2017.

Result: The model captured the unity of detective archetype across time, showing evolution from secondary narrative role to central character and "reasoning machine" in classical stories, with increased complexity and moral ambiguity after WWII hardboiled tradition influence.

Conclusion: The detective archetype in French fiction demonstrates consistent unity while evolving significantly in role and complexity, particularly with the post-WWII hardboiled influence introducing social violence and moral ambiguity.

Abstract: This research explores the evolution of the detective archetype in French
detective fiction through computational analysis. Using quantitative methods
and character-level embeddings, we show that a supervised model is able to
capture the unity of the detective archetype across 150 years of literature,
from M. Lecoq (1866) to Commissaire Adamsberg (2017). Building on this finding,
the study demonstrates how the detective figure evolves from a secondary
narrative role to become the central character and the "reasoning machine" of
the classical detective story. In the aftermath of the Second World War, with
the importation of the hardboiled tradition into France, the archetype becomes
more complex, navigating the genre's turn toward social violence and moral
ambiguity.

</details>


### [31] [Do You Know About My Nation? Investigating Multilingual Language Models' Cultural Literacy Through Factual Knowledge](https://arxiv.org/abs/2511.00657)
*Eshaan Tanwar,Anwoy Chatterjee,Michael Saxon,Alon Albalak,William Yang Wang,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: XNationQA is a new multilingual QA benchmark focusing on cultural literacy across 9 countries in 7 languages, revealing significant gaps in LLMs' understanding of non-Western cultural information.


<details>
  <summary>Details</summary>
Motivation: Existing multilingual benchmarks are Western-centric and don't capture regional diversity, creating evaluation gaps for models' comprehension of factual information from diverse geographical locations.

Method: Created XNationQA with 49,280 questions on geography, culture, and history of 9 countries in 7 languages. Benchmarked 8 multilingual LLMs using two novel transference metrics.

Result: Models show significant discrepancy in accessing culturally specific facts across languages, often knowing more about cultures in English than native languages. Better performance in Western languages doesn't translate to better Western cultural literacy. Limited cross-language knowledge transfer, especially in open-source models.

Conclusion: Current multilingual LLMs have substantial limitations in cultural literacy and cross-language knowledge transfer, highlighting the need for more diverse and culturally-aware benchmarks and model improvements.

Abstract: Most multilingual question-answering benchmarks, while covering a diverse
pool of languages, do not factor in regional diversity in the information they
capture and tend to be Western-centric. This introduces a significant gap in
fairly evaluating multilingual models' comprehension of factual information
from diverse geographical locations. To address this, we introduce XNationQA
for investigating the cultural literacy of multilingual LLMs. XNationQA
encompasses a total of 49,280 questions on the geography, culture, and history
of nine countries, presented in seven languages. We benchmark eight standard
multilingual LLMs on XNationQA and evaluate them using two novel transference
metrics. Our analyses uncover a considerable discrepancy in the models'
accessibility to culturally specific facts across languages. Notably, we often
find that a model demonstrates greater knowledge of cultural information in
English than in the dominant language of the respective culture. The models
exhibit better performance in Western languages, although this does not
necessarily translate to being more literate for Western countries, which is
counterintuitive. Furthermore, we observe that models have a very limited
ability to transfer knowledge across languages, particularly evident in
open-source models.

</details>


### [32] [Do Methods to Jailbreak and Defend LLMs Generalize Across Languages?](https://arxiv.org/abs/2511.00689)
*Berk Atil,Rebecca J. Passonneau,Fred Morstatter*

Main category: cs.CL

TL;DR: First systematic multilingual evaluation of jailbreak attacks and defenses across 10 languages shows safety alignment varies significantly by language, with high-resource languages being safer for standard queries but more vulnerable to adversarial attacks.


<details>
  <summary>Details</summary>
Motivation: Safety alignment in LLMs can be bypassed through jailbreak attacks, but cross-lingual generalization of these attacks and defenses remains underexplored.

Method: Evaluated two jailbreak types (logical-expression-based and adversarial-prompt-based) across 10 languages spanning different resource levels using six LLMs on HarmBench and AdvBench.

Result: Attack success and defense robustness vary across languages - high-resource languages are safer under standard queries but more vulnerable to adversarial ones. Simple defenses are effective but language- and model-dependent.

Conclusion: Findings highlight the need for language-aware and cross-lingual safety benchmarks for LLMs to ensure robust safety alignment across different languages.

Abstract: Large language models (LLMs) undergo safety alignment after training and
tuning, yet recent work shows that safety can be bypassed through jailbreak
attacks. While many jailbreaks and defenses exist, their cross-lingual
generalization remains underexplored. This paper presents the first systematic
multilingual evaluation of jailbreaks and defenses across ten
languages--spanning high-, medium-, and low-resource languages--using six LLMs
on HarmBench and AdvBench. We assess two jailbreak types:
logical-expression-based and adversarial-prompt-based. For both types, attack
success and defense robustness vary across languages: high-resource languages
are safer under standard queries but more vulnerable to adversarial ones.
Simple defenses can be effective, but are language- and model-dependent. These
findings call for language-aware and cross-lingual safety benchmarks for LLMs.

</details>


### [33] [Optimizing Native Sparse Attention with Latent Attention and Local Global Alternating Strategies](https://arxiv.org/abs/2511.00819)
*Yuxuan Hu,Jianchao Tan,Jiaqi Zhang,Wen Zan,Pingwei Sun,Yifan Lu,Yerui Sun,Yuchen Xie,Xunliang Cai,Jing Zhang*

Main category: cs.CL

TL;DR: Proposes improved Native Sparse Attention with alternating local-global attention patterns and latent attention mechanisms, reducing KV-cache by 50% while enhancing long-context modeling performance.


<details>
  <summary>Details</summary>
Motivation: To enhance long-context modeling capabilities of sparse attention mechanisms by addressing limitations of fixed attention patterns in Native Sparse Attention.

Method: Alternates between local (sliding-window) and global (compression, selective) attention across layers, and refines branches with Multi-head Latent Attention (MLA) for sliding-window and Group-head Latent Attention (GLA) for compression/selective branches.

Result: Reduces KV-cache memory by 50% versus NSA while improving common-sense reasoning and long-text understanding. Matches or exceeds full attention and native sparse attention on models from 340M to 1.3B parameters.

Conclusion: Dynamic alternating attention patterns combined with latent attention mechanisms significantly improve long-context modeling efficiency and performance over existing sparse attention approaches.

Abstract: In this work, we conduct a systematic analysis of Native Sparse Attention
(NSA) and propose targeted improvements that enhance long-context modeling. A
key insight is that alternating between local (sliding-window) and global
(compression, selective) attention across layers, rather than using fixed
patterns, enables more effective propagation of long-range dependencies and
substantially boosts performance on long-sequence tasks. Meanwhile, we further
refine NSA's branches with Latent Attention that the sliding-window branch is
enhanced with Multi-head Latent Attention (MLA) while compression and selective
branches adopt Group-head Latent Attention (GLA). These changes reduce KV-cache
memory by 50\% versus NSA while improving the model's common-sense reasoning
and long-text understanding capabilities. Experiments on models from 340M to
1.3B parameters (trained on 15B and 100B tokens) show our method matches or
exceeds full attention and native sparse attention in both common-sense
reasoning and long-context understanding tasks.

</details>


### [34] [TriCon-Fair: Triplet Contrastive Learning for Mitigating Social Bias in Pre-trained Language Models](https://arxiv.org/abs/2511.00854)
*Chong Lyu,Lin Li,Shiqing Wu,Jingling Yuan*

Main category: cs.CL

TL;DR: TriCon-Fair is a contrastive learning framework that addresses social bias in LLMs using decoupled triplet loss to eliminate positive-negative coupling while maintaining model performance.


<details>
  <summary>Details</summary>
Motivation: Existing debiasing methods treat biased and unbiased samples independently, ignoring their mutual relationships and enabling hidden negative-positive coupling where improvements for one group compromise the other.

Method: TriCon-Fair uses contrastive learning with decoupled loss combining triplet and language modeling terms, assigning each anchor an explicitly biased negative and unbiased positive to decouple push-pull dynamics.

Result: Experimental results show TriCon-Fair reduces discriminatory output beyond existing debiasing baselines while maintaining strong downstream performance.

Conclusion: TriCon-Fair offers a practical and ethical solution for sensitive NLP applications by effectively eliminating social bias without compromising model capabilities.

Abstract: The increasing utilization of large language models raises significant
concerns about the propagation of social biases, which may result in harmful
and unfair outcomes. However, existing debiasing methods treat the biased and
unbiased samples independently, thus ignoring their mutual relationship. This
oversight enables a hidden negative-positive coupling, where improvements for
one group inadvertently compromise the other, allowing residual social bias to
persist. In this paper, we introduce TriCon-Fair, a contrastive learning
framework that employs a decoupled loss that combines triplet and language
modeling terms to eliminate positive-negative coupling. Our TriCon-Fair assigns
each anchor an explicitly biased negative and an unbiased positive, decoupling
the push-pull dynamics and avoiding positive-negative coupling, and jointly
optimizes a language modeling (LM) objective to preserve general capability.
Experimental results demonstrate that TriCon-Fair reduces discriminatory output
beyond existing debiasing baselines while maintaining strong downstream
performance. This suggests that our proposed TriCon-Fair offers a practical and
ethical solution for sensitive NLP applications.

</details>


### [35] [Assessing LLM Reasoning Steps via Principal Knowledge Grounding](https://arxiv.org/abs/2511.00879)
*Hyeon Hwang,Yewon Cho,Chanwoong Yoon,Yein Park,Minju Song,Kyungjae Lee,Gangwoo Kim,Jaewoo Kang*

Main category: cs.CL

TL;DR: A novel evaluation suite for assessing knowledge grounding in LLM reasoning, featuring principal knowledge collection, knowledge-grounded metrics, and a lightweight evaluator LLM.


<details>
  <summary>Details</summary>
Motivation: To verify that LLM reasoning is accurately grounded in knowledge and identify missing or misapplied knowledge elements in step-by-step reasoning.

Method: Three-component framework: (1) Principal Knowledge Collection for atomic knowledge, (2) knowledge-grounded evaluation metrics for recall and application assessment, (3) lightweight evaluator LLM for cost-effective metric computation.

Result: The evaluation suite effectively identifies reasoning deficiencies and missing knowledge elements in LLMs.

Conclusion: Knowledge-grounded evaluation provides crucial insights for uncovering fundamental reasoning deficiencies and can be integrated into preference optimization for broader applications.

Abstract: Step-by-step reasoning has become a standard approach for large language
models (LLMs) to tackle complex tasks. While this paradigm has proven
effective, it raises a fundamental question: How can we verify that an LLM's
reasoning is accurately grounded in knowledge? To address this question, we
introduce a novel evaluation suite that systematically assesses the knowledge
grounding of intermediate reasoning. Our framework comprises three key
components. (1) Principal Knowledge Collection, a large-scale repository of
atomic knowledge essential for reasoning. Based on the collection, we propose
(2) knowledge-grounded evaluation metrics designed to measure how well models
recall and apply prerequisite knowledge in reasoning. These metrics are
computed by our (3) evaluator LLM, a lightweight model optimized for
cost-effective and reliable metric computation. Our evaluation suite
demonstrates remarkable effectiveness in identifying missing or misapplied
knowledge elements, providing crucial insights for uncovering fundamental
reasoning deficiencies in LLMs. Beyond evaluation, we demonstrate how these
metrics can be integrated into preference optimization, showcasing further
applications of knowledge-grounded evaluation.

</details>


### [36] [ColMate: Contrastive Late Interaction and Masked Text for Multimodal Document Retrieval](https://arxiv.org/abs/2511.00903)
*Ahmed Masry,Megh Thakkar,Patrice Bechard,Sathwik Tejaswi Madhusudhan,Rabiul Awal,Shambhavi Mishra,Akshay Kalkunte Suresh,Srivatsava Daruru,Enamul Hoque,Spandana Gella,Torsten Scholak,Sai Rajeswar*

Main category: cs.CL

TL;DR: ColMate is a multimodal document retrieval model that improves over existing methods by using OCR-based pretraining, masked contrastive learning, and late interaction scoring tailored to document structures.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal document retrieval methods often replicate text-only techniques without considering multimodal document structures and visual characteristics, limiting their effectiveness.

Method: Uses OCR-based pretraining objective, self-supervised masked contrastive learning, and late interaction scoring mechanism specifically designed for multimodal documents.

Result: Achieves 3.61% improvement over existing retrieval models on ViDoRe V2 benchmark and shows stronger generalization to out-of-domain benchmarks.

Conclusion: ColMate successfully bridges the gap between multimodal representation learning and document retrieval through specialized techniques for multimodal document structures.

Abstract: Retrieval-augmented generation has proven practical when models require
specialized knowledge or access to the latest data. However, existing methods
for multimodal document retrieval often replicate techniques developed for
text-only retrieval, whether in how they encode documents, define training
objectives, or compute similarity scores. To address these limitations, we
present ColMate, a document retrieval model that bridges the gap between
multimodal representation learning and document retrieval. ColMate utilizes a
novel OCR-based pretraining objective, a self-supervised masked contrastive
learning objective, and a late interaction scoring mechanism more relevant to
multimodal document structures and visual characteristics. ColMate obtains
3.61% improvements over existing retrieval models on the ViDoRe V2 benchmark,
demonstrating stronger generalization to out-of-domain benchmarks.

</details>


### [37] [The Biased Oracle: Assessing LLMs' Understandability and Empathy in Medical Diagnoses](https://arxiv.org/abs/2511.00924)
*Jianzhou Yao,Shunchang Liu,Guillaume Drui,Rikard Pettersson,Alessandro Blasimme,Sara Kijewski*

Main category: cs.CL

TL;DR: LLMs show potential for clinical diagnostic communication but produce overly complex explanations and biased empathy, requiring systematic calibration for equitable patient support.


<details>
  <summary>Details</summary>
Motivation: To evaluate LLMs' ability to generate understandable and empathetic medical explanations for patients, addressing concerns about accessibility and equitable communication.

Method: Evaluated two leading LLMs on medical diagnostic scenarios using readability metrics for understandability and LLM-as-a-Judge ratings for empathy, compared with human evaluations.

Result: LLMs adapt explanations to socio-demographic variables and patient conditions but generate overly complex content and display biased affective empathy, creating uneven accessibility.

Conclusion: Systematic calibration is needed to ensure equitable patient communication and address the biases in LLM-generated medical explanations.

Abstract: Large language models (LLMs) show promise for supporting clinicians in
diagnostic communication by generating explanations and guidance for patients.
Yet their ability to produce outputs that are both understandable and
empathetic remains uncertain. We evaluate two leading LLMs on medical
diagnostic scenarios, assessing understandability using readability metrics as
a proxy and empathy through LLM-as-a-Judge ratings compared to human
evaluations. The results indicate that LLMs adapt explanations to
socio-demographic variables and patient conditions. However, they also generate
overly complex content and display biased affective empathy, leading to uneven
accessibility and support. These patterns underscore the need for systematic
calibration to ensure equitable patient communication. The code and data are
released: https://github.com/Jeffateth/Biased_Oracle

</details>


### [38] [The Riddle of Reflection: Evaluating Reasoning and Self-Awareness in Multilingual LLMs using Indian Riddles](https://arxiv.org/abs/2511.00960)
*Abhinav P M,Ojasva Saxena,Oswald C,Parameswari Krishnamurthy*

Main category: cs.CL

TL;DR: LLMs struggle with culturally grounded reasoning across Indian languages, with top models being overconfident while weaker models show better self-awareness of their mistakes.


<details>
  <summary>Details</summary>
Motivation: To examine LLMs' reasoning and self-assessment abilities across seven major Indian languages, as culturally grounded reasoning in non-English languages remains underexplored.

Method: Created a multilingual riddle dataset with traditional and context-reconstructed variants, evaluated five LLMs under seven prompting strategies, and conducted two-stage experiments: riddle-solving performance and self-evaluation for reasoning consistency.

Result: Gemini 2.5 Pro performed best overall but showed only marginal gains with few-shot methods. Key finding: initial accuracy inversely correlated with self-awareness - top models were overconfident (4.34% True Negative Rate) while weaker models were more self-aware (42.09% True Negative Rate).

Conclusion: Clear gaps exist in multilingual reasoning, highlighting the need for models that not only reason effectively but also recognize their own limitations.

Abstract: The extent to which large language models (LLMs) can perform culturally
grounded reasoning across non-English languages remains underexplored. This
paper examines the reasoning and self-assessment abilities of LLMs across seven
major Indian languages-Bengali, Gujarati, Hindi, Kannada, Malayalam, Tamil, and
Telugu. We introduce a multilingual riddle dataset combining traditional
riddles with context-reconstructed variants and evaluate five LLMs-Gemini 2.5
Pro, Gemini 2.5 Flash, Mistral-Saba, LLaMA 4 Scout, and LLaMA 4 Maverick-under
seven prompting strategies. In the first stage, we assess riddle-solving
performance and find that while Gemini 2.5 Pro performs best overall, few-shot
methods yield only marginal gains, and accuracy varies notably across
languages. In the second stage, we conduct a self-evaluation experiment to
measure reasoning consistency. The results reveal a key finding: a model's
initial accuracy is inversely correlated with its ability to identify its own
mistakes. Top-performing models such as Gemini 2.5 Pro are overconfident (4.34%
True Negative Rate), whereas lower-performing models like LLaMA 4 Scout are
substantially more self-aware (42.09% True Negative Rate). These results point
to clear gaps in multilingual reasoning and highlight the need for models that
not only reason effectively but also recognize their own limitations.

</details>


### [39] [Advancing Machine-Generated Text Detection from an Easy to Hard Supervision Perspective](https://arxiv.org/abs/2511.00988)
*Chenwang Wu,Yiu-ming Cheung,Bo Han,Defu Lian*

Main category: cs.CL

TL;DR: Proposes an easy-to-hard enhancement framework for machine-generated text detection that addresses boundary ambiguity and inexact learning by using an easy supervisor on longer texts to enhance a more challenging target detector.


<details>
  <summary>Details</summary>
Motivation: Existing MGT detection methods assume exact labels as golden standard, but boundary ambiguity makes traditional training paradigms inexact. Human cognition limitations and detector superintelligence make inexact learning widespread and inevitable.

Method: Easy-to-hard enhancement framework using an easy supervisor targeting longer-text detection tasks to enhance target detector. Longer texts alleviate inexact label impact. Structurally incorporates detector into supervisor, modeling supervisor as lower performance bound for detector.

Result: Extensive experiments across cross-LLM, cross-domain, mixed text, and paraphrase attack scenarios demonstrate significant detection effectiveness improvement.

Conclusion: The framework provides reliable supervision under inexact conditions by optimizing supervisor to indirectly optimize detector, ultimately approximating underlying golden labels.

Abstract: Existing machine-generated text (MGT) detection methods implicitly assume
labels as the "golden standard". However, we reveal boundary ambiguity in MGT
detection, implying that traditional training paradigms are inexact. Moreover,
limitations of human cognition and the superintelligence of detectors make
inexact learning widespread and inevitable. To this end, we propose an
easy-to-hard enhancement framework to provide reliable supervision under such
inexact conditions. Distinct from knowledge distillation, our framework employs
an easy supervisor targeting relatively simple longer-text detection tasks
(despite weaker capabilities), to enhance the more challenging target detector.
Firstly, longer texts targeted by supervisors theoretically alleviate the
impact of inexact labels, laying the foundation for reliable supervision.
Secondly, by structurally incorporating the detector into the supervisor, we
theoretically model the supervisor as a lower performance bound for the
detector. Thus, optimizing the supervisor indirectly optimizes the detector,
ultimately approximating the underlying "golden" labels. Extensive experiments
across diverse practical scenarios, including cross-LLM, cross-domain, mixed
text, and paraphrase attacks, demonstrate the framework's significant detection
effectiveness. The code is available at:
https://github.com/tmlr-group/Easy2Hard.

</details>


### [40] [MARS-SQL: A multi-agent reinforcement learning framework for Text-to-SQL](https://arxiv.org/abs/2511.01008)
*Haolin Yang,Jipeng Zhang,Zhitao He,Yi R. Fung*

Main category: cs.CL

TL;DR: MARS-SQL is a multi-agent framework for natural language to SQL translation that uses three specialized agents and interactive reinforcement learning to handle complex queries through dynamic reasoning and self-correction.


<details>
  <summary>Details</summary>
Motivation: Natural language to SQL translation remains challenging for complex queries that require environmental interaction and self-correction capabilities.

Method: Uses three specialized agents: Grounding Agent for schema linking, Generation Agent trained via multi-turn RL policy with ReAct-style loop (Think-Act-Observe), and Validation Agent for trajectory selection. Generates multiple interaction trajectories at inference and selects optimal one using next-token prediction.

Result: Achieves state-of-the-art Execution Accuracy of 77.84% on BIRD dev set and 89.75% on Spider test set.

Conclusion: The structured multi-agent framework combining interactive RL for generation with generative modeling for verification proves highly effective for robust and accurate SQL generation.

Abstract: Translating natural language to SQL remains difficult for complex queries.
Such queries often need environmental interaction and self-correction. To
address this, we introduce MARS-SQL, a novel multi-agent framework that
combines principled task decomposition and interactive reinforcement learning
(RL). Our system comprises three specialized agents: a Grounding Agent for
schema linking, a Generation Agent for query generation, and a Validation Agent
for final selection. The core of our framework is the Generation agent, which
is trained via a multi-turn RL policy. Adopting a ReAct-style Think-Act-Observe
loop, the agent iteratively generates thoughts, executes SQL actions against a
live database, and revises its strategy based on execution feedback, enabling
dynamic, stateful reasoning and self-correction. At inference time, we generate
multiple interaction trajectories to explore diverse reasoning paths. The
Validation agent, then selects the optimal trajectory by modeling verification
as a next-token prediction task and choosing the solution with the highest
generation probability. This structured workflow pipelines specialized agents.
It combines interactive RL for generation with generative modeling for
verification. The approach proves highly effective for robust and accurate SQL
generation. Experiments show that MARS-SQL achieves state-of-the-art Execution
Accuracy of 77.84% on the BIRD dev set and 89.75% on the Spider test set. Our
code is available at https://github.com/YangHaolin0526/MARS-SQL.

</details>


### [41] [IF-CRITIC: Towards a Fine-Grained LLM Critic for Instruction-Following Evaluation](https://arxiv.org/abs/2511.01014)
*Bosi Wen,Yilin Niu,Cunxiang Wang,Pei Ke,Xiaoying Ling,Ying Zhang,Aohan Zeng,Hongning Wang,Minlie Huang*

Main category: cs.CL

TL;DR: IF-CRITIC is an LLM critic that provides efficient and reliable assessments of constraint following in instructions, outperforming existing LLM-as-a-Judge methods and enabling better instruction-following optimization with lower computational costs.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation models for instruction following have deficiencies including high costs and unreliable assessments, creating a need for more efficient and reliable evaluation methods.

Method: Developed a checklist generator to decompose instructions into constraint checklists, collected high-quality critique training data through multi-stage filtering, and used constraint-level preference optimization to train IF-CRITIC.

Result: IF-CRITIC beats strong LLM-as-a-Judge baselines (Deepseek-R1 and o4-mini) and enables substantial performance gains in instruction-following optimization with lower computational overhead.

Conclusion: IF-CRITIC provides scalable and reliable reward signals that significantly improve instruction-following ability in LLMs while reducing computational costs compared to existing methods.

Abstract: Instruction following is a fundamental ability of Large Language Models
(LLMs), requiring their generated outputs to follow multiple constraints
imposed in input instructions. Numerous studies have attempted to enhance this
ability through preference optimization or reinforcement learning based on
reward signals from LLM-as-a-Judge. However, existing evaluation models for
instruction following still possess many deficiencies, such as substantial
costs and unreliable assessments. To this end, we propose IF-CRITIC, an LLM
critic that can provide efficient and reliable assessments of constraint
following in the instructions. We first develop a checklist generator to
decompose instructions and generate constraint checklists. With the assistance
of the checklists, we collect high-quality critique training data through a
multi-stage critique filtering mechanism and employ a constraint-level
preference optimization method to train IF-CRITIC. Extensive experiments
demonstrate that the evaluation performance of IF-CRITIC can beat strong
LLM-as-a-Judge baselines, including Deepseek-R1 and o4-mini. With the scalable
reward signals provided by IF-CRITIC, LLMs can achieve substantial performance
gains in instruction-following optimization under lower computational overhead
compared to strong LLM critic baselines.

</details>


### [42] [Prompt-R1: Collaborative Automatic Prompting Framework via End-to-end Reinforcement Learning](https://arxiv.org/abs/2511.01016)
*Wenjin Liu,Haoran Luo,Xueyuan Lin,Haoming Liu,Tiesunlong Shen,Jiapu Wang,Rui Mao,Erik Cambria*

Main category: cs.CL

TL;DR: Prompt-R1 is a reinforcement learning framework that uses a small LLM to generate prompts for a large LLM, improving performance on complex tasks without requiring users to craft effective prompts.


<details>
  <summary>Details</summary>
Motivation: Users struggle to provide accurate prompts for complex problems with LLMs, limiting model performance. There's a need for automated prompt generation to enhance LLM capabilities.

Method: End-to-end RL framework with small LLM collaborating with large LLM through multi-turn prompt interactions. Uses dual-constrained reward for correctness, quality, and reasoning accuracy. Plug-and-play design supports various LLMs.

Result: Significantly outperforms baseline models across multiple public datasets and tasks.

Conclusion: Prompt-R1 provides an effective automated prompt generation solution that enhances LLM performance on complex problems without requiring user expertise in prompt engineering.

Abstract: Recently, advanced large language models (LLMs) have emerged at an
increasingly rapid pace. However, when faced with complex problems, most users
are often unable to provide accurate and effective prompts to interact with
LLMs, thus limiting the performance of LLMs. To address this challenge, we
propose Prompt-R1, an end-to-end reinforcement learning framework that uses a
small-scale LLM to collaborate with large-scale LLMs, replacing user
interaction to solve problems better. This collaboration is cast as a
multi-turn prompt interaction, where the small-scale LLM thinks and generates
prompts, and the large-scale LLM performs complex reasoning. A dual-constrained
reward is designed to optimize for correctness, generation quality, and
reasoning accuracy. Prompt-R1 provides a plug-and-play framework that supports
both inference and training with various large-scale LLMs. Experiments on
multiple public datasets show that Prompt-R1 significantly outperforms baseline
models across tasks. Our code is publicly available at
https://github.com/QwenQKing/Prompt-R1.

</details>


### [43] [OceanAI: A Conversational Platform for Accurate, Transparent, Near-Real-Time Oceanographic Insights](https://arxiv.org/abs/2511.01019)
*Bowen Chen,Jayesh Gajbhar,Gregory Dusek,Rob Redmon,Patrick Hogan,Paul Liu,DelWayne Bohnenstiehl,Dongkuan,Xu,Ruoying He*

Main category: cs.CL

TL;DR: OceanAI is a conversational platform that combines LLMs with real-time NOAA oceanographic data to provide verified, reproducible responses with data references, addressing AI hallucinations in scientific applications.


<details>
  <summary>Details</summary>
Motivation: To overcome the problem of AI hallucinations in scientific contexts by creating a system that grounds responses in authoritative, verifiable oceanographic data from NOAA.

Method: Integrates open-source LLMs with real-time API calls to NOAA data streams, automatically identifying, parsing, and synthesizing relevant datasets into natural-language responses and visualizations.

Result: In blind comparisons with other AI chat products, only OceanAI produced NOAA-sourced values with original data references; others either declined or provided unsupported results.

Conclusion: OceanAI advances transparency, reproducibility, and trust in AI systems for scientific applications, offering a scalable framework for AI-enabled decision support in ocean sciences.

Abstract: Artificial intelligence is transforming the sciences, yet general
conversational AI systems often generate unverified "hallucinations"
undermining scientific rigor. We present OceanAI, a conversational platform
that integrates the natural-language fluency of open-source large language
models (LLMs) with real-time, parameterized access to authoritative
oceanographic data streams hosted by the National Oceanic and Atmospheric
Administration (NOAA). Each query such as "What was Boston Harbor's highest
water level in 2024?" triggers real-time API calls that identify, parse, and
synthesize relevant datasets into reproducible natural-language responses and
data visualizations. In a blind comparison with three widely used AI
chat-interface products, only OceanAI produced NOAA-sourced values with
original data references; others either declined to answer or provided
unsupported results. Designed for extensibility, OceanAI connects to multiple
NOAA data products and variables, supporting applications in marine hazard
forecasting, ecosystem assessment, and water-quality monitoring. By grounding
outputs and verifiable observations, OceanAI advances transparency,
reproducibility, and trust, offering a scalable framework for AI-enabled
decision support within the oceans. A public demonstration is available at
https://oceanai.ai4ocean.xyz.

</details>


### [44] [VayuChat: An LLM-Powered Conversational Interface for Air Quality Data Analytics](https://arxiv.org/abs/2511.01046)
*Vedant Acharya,Abhay Pisharodi,Rishabh Mondal,Mohammad Rafiuddin,Nipun Batra*

Main category: cs.CL

TL;DR: VayuChat is a conversational AI system that enables users to ask natural language questions about air quality, meteorology, and policy programs in India, generating Python code and interactive visualizations to make environmental data analysis accessible to non-experts.


<details>
  <summary>Details</summary>
Motivation: Air pollution causes 1.6 million premature deaths annually in India, but existing tools require expertise and provide static dashboards, making it difficult for decision makers to translate dispersed data into actionable policies.

Method: VayuChat integrates data from CPCB monitoring stations, state-level demographics, and NCAP funding records, using large language models to create a unified conversational interface that generates executable Python code and interactive visualizations in response to natural language queries.

Result: The system provides a publicly deployed platform that enables users to perform complex environmental analytics through simple conversations, making data science accessible to policymakers, researchers, and citizens without requiring technical expertise.

Conclusion: VayuChat demonstrates how conversational AI can bridge the gap between complex environmental data and decision-making, empowering diverse stakeholders to analyze air quality and policy impacts through natural language interactions.

Abstract: Air pollution causes about 1.6 million premature deaths each year in India,
yet decision makers struggle to turn dispersed data into decisions. Existing
tools require expertise and provide static dashboards, leaving key policy
questions unresolved. We present VayuChat, a conversational system that answers
natural language questions on air quality, meteorology, and policy programs,
and responds with both executable Python code and interactive visualizations.
VayuChat integrates data from Central Pollution Control Board (CPCB) monitoring
stations, state-level demographics, and National Clean Air Programme (NCAP)
funding records into a unified interface powered by large language models. Our
live demonstration will show how users can perform complex environmental
analytics through simple conversations, making data science accessible to
policymakers, researchers, and citizens. The platform is publicly deployed at
https://huggingface.co/spaces/SustainabilityLabIITGN/ VayuChat. For further
information check out video uploaded on
https://www.youtube.com/watch?v=d6rklL05cs4.

</details>


### [45] [Building a Silver-Standard Dataset from NICE Guidelines for Clinical LLMs](https://arxiv.org/abs/2511.01053)
*Qing Ding,Eric Hua Qing Zhang,Felix Jozsa,Julia Ive*

Main category: cs.CL

TL;DR: A new validated dataset for evaluating LLMs' clinical reasoning using guideline-based patient scenarios and questions, with benchmarking of popular models.


<details>
  <summary>Details</summary>
Motivation: Standardized benchmarks are missing for evaluating LLMs' guideline-based clinical reasoning in healthcare applications.

Method: Created a validated dataset using GPT from public guidelines across multiple diagnoses, containing realistic patient scenarios and clinical questions.

Result: Benchmarked recent popular LLMs to demonstrate dataset validity and framework utility.

Conclusion: The framework enables systematic evaluation of LLMs' clinical utility and guideline adherence in healthcare.

Abstract: Large language models (LLMs) are increasingly used in healthcare, yet
standardised benchmarks for evaluating guideline-based clinical reasoning are
missing. This study introduces a validated dataset derived from publicly
available guidelines across multiple diagnoses. The dataset was created with
the help of GPT and contains realistic patient scenarios, as well as clinical
questions. We benchmark a range of recent popular LLMs to showcase the validity
of our dataset. The framework supports systematic evaluation of LLMs' clinical
utility and guideline adherence.

</details>


### [46] [HPLT~3.0: Very Large-Scale Multilingual Resources for LLM and MT. Mono- and Bi-lingual Data, Multilingual Evaluation, and Pre-Trained Models](https://arxiv.org/abs/2511.01066)
*Stephan Oepen,Nikolay Arefev,Mikko Aulamo,Marta Ban,Maja Buljan,Laurie Burchell,Lucas Charpentier,Pinzhen Chen,Mariya Fedorova,Ona de Gibert,Barry Haddow,Jan Haji,Jindri Helcl,Andrey Kutuzov,Zihao Li,Risto Luukkonen,Bhavitvya Malik,Vladislav Mikhailov,Amanda Myntti,Dayyn O'Brien,Lucie Polkov,Sampo Pyysalo,Gema Ramrez Snchez,Janine Siewert,Pavel Stepachev,Jrg Tiedemann,Teemu Vahtola,Fedor Vitiugin,Tea Vojtchov,Jaume Zaragoza*

Main category: cs.CL

TL;DR: Creation of the largest multilingual LLM pre-training dataset (30T tokens) with comprehensive processing pipeline and evaluation benchmarks for 200 languages.


<details>
  <summary>Details</summary>
Motivation: To provide open, large-scale, high-quality multilingual datasets for LLM pre-training that address the lack of comprehensive resources for many languages.

Method: Derived from web crawls with complete open-source pipeline including document selection, text extraction, language ID, deduplication, annotation (register labels, quality estimates, PII), filtering, and evaluation through quality probes and model training.

Result: Successfully created 30T token dataset with comprehensive processing pipeline, trained 57 monolingual encoder-decoder models and GPT-like models, and developed multilingual evaluation benchmarks for 9 European languages.

Conclusion: The initiative provides the largest generally available multilingual LLM pre-training dataset with complete processing tools and evaluation frameworks, enabling better multilingual NLP development.

Abstract: We present an ongoing initiative to provide open, very large, high-quality,
and richly annotated textual datasets for almost 200 languages. At 30 trillion
tokens, this is likely the largest generally available multilingual collection
of LLM pre-training data. At 30 trillion tokens, this is likely the largest
generally available multilingual collection of LLM pre-training data. These
datasets are derived from web crawls from different sources and accompanied
with a complete, open-source pipeline for document selection from web archives,
text extraction from HTML, language identification for noisy texts, exact and
near-deduplication, annotation with, among others, register labels, text
quality estimates, and personally identifiable information; and final selection
and filtering. We report on data quality probes through contrastive and
analytical statistics, through manual inspection of samples for 24 languages,
and through end-to-end evaluation of various language model architectures
trained on this data. For multilingual LLM evaluation, we provide a
comprehensive collection of benchmarks for nine European languages, with
special emphasis on natively created tasks, mechanisms to mitigate prompt
sensitivity, and refined normalization and aggregation of scores. Additionally,
we train and evaluate a family of 57 monolingual encoder-decoder models, as
well as a handful of monolingual GPT-like reference models. Besides the
monolingual data and models, we also present a very large collection of
parallel texts automatically mined from this data, together with a novel
parallel corpus synthesized via machine translation.

</details>


### [47] [Improving Romanian LLM Pretraining Data using Diversity and Quality Filtering](https://arxiv.org/abs/2511.01090)
*Vlad Negoita,Mihai Masala,Traian Rebedea*

Main category: cs.CL

TL;DR: This paper analyzes Romanian pretraining corpora characteristics compared to English data, develops a multitask model for multi-level filtering of Romanian texts, and demonstrates improved LLM performance through data quality filtering.


<details>
  <summary>Details</summary>
Motivation: Data quality is crucial for training LLMs, especially for under-represented languages like Romanian where high-quality corpora are scarce. The study aims to understand differences between Romanian and English data and improve Romanian LLM training through better data curation.

Method: Train a lightweight multitask model on LLM-annotated Romanian texts to perform multi-level filtering (educational value, topic, format) and generate high-quality pretraining datasets.

Result: Experiments reveal noteworthy trends in topic distribution between Romanian and English data, and demonstrate that filtered data leads to improved LLM pretraining performance across multiple benchmarks.

Conclusion: Careful data curation and multi-level filtering significantly enhance LLM training for under-represented languages like Romanian, with the approach proving effective through measurable performance improvements.

Abstract: Large Language Models (LLMs) have recently exploded in popularity, often
matching or outperforming human abilities on many tasks. One of the key factors
in training LLMs is the availability and curation of high-quality data. Data
quality is especially crucial for under-represented languages, where
high-quality corpora are scarce. In this work we study the characteristics and
coverage of Romanian pretraining corpora and we examine how they differ from
English data. By training a lightweight multitask model on carefully
LLM-annotated Romanian texts, we are able to analyze and perform multi-level
filtering (e.g., educational value, topic, format) to generate high-quality
pretraining datasets. Our experiments show noteworthy trends in the topics
present in Romanian and English data, while also proving the effectiveness of
filtering data through improved LLM pretraining performance across multiple
benchmarks.

</details>


### [48] [TSVer: A Benchmark for Fact Verification Against Time-Series Evidence](https://arxiv.org/abs/2511.01101)
*Marek Strong,Andreas Vlachos*

Main category: cs.CL

TL;DR: TSVer is a new benchmark dataset for fact verification focusing on temporal and numerical reasoning with time-series evidence, containing 287 real-world claims and 400 time series across diverse domains.


<details>
  <summary>Details</summary>
Motivation: Existing fact-checking systems for temporal and numerical data are limited by datasets that lack structured evidence, provide insufficient justifications, or rely on synthetic claims.

Method: Created TSVer dataset using an LLM-assisted multi-step annotation process, with claims sourced from 38 fact-checking organizations and a curated database of 400 time series. Each claim is annotated with time frames, verdicts, and justifications.

Result: Achieved high inter-annotator agreement (kappa=0.745) on verdicts. State-of-the-art models like Gemini-2.5-Pro achieved only 63.37% accuracy on verdicts and 48.63 Ev2R score on verdict justifications.

Conclusion: TSVer addresses limitations of existing datasets and demonstrates that current reasoning models are challenged by time-series evidence, highlighting the need for improved temporal and numerical reasoning capabilities.

Abstract: Reasoning over temporal and numerical data, such as time series, is a crucial
aspect of fact-checking. While many systems have recently been developed to
handle this form of evidence, their evaluation remains limited by existing
datasets, which often lack structured evidence, provide insufficient
justifications for verdicts, or rely on synthetic claims. In this paper, we
introduce TSVer, a new benchmark dataset for fact verification focusing on
temporal and numerical reasoning with time-series evidence. TSVer contains 287
real-world claims sourced from 38 fact-checking organizations and a curated
database of 400 time series covering diverse domains. Each claim is annotated
with time frames across all pertinent time series, along with a verdict and
justifications reflecting how the evidence is used to reach the verdict. Using
an LLM-assisted multi-step annotation process, we improve the quality of our
annotations and achieve an inter-annotator agreement of kappa=0.745 on
verdicts. We also develop a baseline for verifying claims against time-series
evidence and show that even the state-of-the-art reasoning models like
Gemini-2.5-Pro are challenged by time series, achieving a 63.37 accuracy score
on verdicts and an Ev2R score of 48.63 on verdict justifications.

</details>


### [49] [MicroRemed: Benchmarking LLMs in Microservices Remediation](https://arxiv.org/abs/2511.01166)
*Lingzhe Zhang,Yunpeng Zhai,Tong Jia,Chiming Duan,Minghua He,Leyi Pan,Zhaoyang Liu,Bolin Ding,Ying Li*

Main category: cs.CL

TL;DR: MicroRemed is the first benchmark for evaluating LLMs in end-to-end microservice remediation, requiring models to generate executable Ansible playbooks from diagnosis reports. ThinkRemed, a multi-agent framework, improves performance through iterative reasoning.


<details>
  <summary>Details</summary>
Motivation: Current LLM approaches for microservice remediation rely on human-crafted prompts from SREs, with LLMs merely converting text to code. There's a need for more autonomous end-to-end remediation capabilities.

Method: Proposed ThinkRemed, a multi-agent framework that emulates SREs' reflective and perceptive reasoning through iterative reasoning and system reflection.

Result: MicroRemed presents substantial challenges to current LLMs, while ThinkRemed improves end-to-end remediation performance.

Conclusion: The benchmark enables evaluation of LLMs in autonomous microservice remediation, with ThinkRemed demonstrating improved performance through multi-agent reasoning.

Abstract: Large Language Models (LLMs) integrated with agent-based reasoning frameworks
have recently shown strong potential for autonomous decision-making and
system-level operations. One promising yet underexplored direction is
microservice remediation, where the goal is to automatically recover faulty
microservice systems. Existing approaches, however, still rely on human-crafted
prompts from Site Reliability Engineers (SREs), with LLMs merely converting
textual instructions into executable code. To advance research in this area, we
introduce MicroRemed, the first benchmark for evaluating LLMs in end-to-end
microservice remediation, where models must directly generate executable
Ansible playbooks from diagnosis reports to restore system functionality. We
further propose ThinkRemed, a multi-agent framework that emulates the
reflective and perceptive reasoning of SREs. Experimental results show that
MicroRemed presents substantial challenges to current LLMs, while ThinkRemed
improves end-to-end remediation performance through iterative reasoning and
system reflection. The benchmark is available at
https://github.com/LLM4AIOps/MicroRemed.

</details>


### [50] [Learning When to Quit in Sales Conversations](https://arxiv.org/abs/2511.01181)
*Emaad Manzoor,Eva Ascarza,Oded Netzer*

Main category: cs.CL

TL;DR: A stopping agent using language models reduces failed call time by 54% while maintaining sales, increasing expected sales by up to 37% by reallocating saved time.


<details>
  <summary>Details</summary>
Motivation: Salespeople face dynamic screening decisions about whether to persist or abandon conversations, but little is known about how these decisions are made, their efficiency, or how to improve them in high-volume outbound sales.

Method: Formalize dynamic screening as an optimal stopping problem and develop a generative language model-based sequential decision agent that learns when to quit conversations by imitating a retrospectively-inferred optimal stopping policy.

Result: The stopping agent reduces time spent on failed calls by 54% while preserving nearly all sales; reallocating saved time increases expected sales by up to 37%. Analysis shows salespeople overweight salient expressions of disinterest and mispredict failure risk.

Conclusion: AI algorithms can correct cognitively-bounded human decisions and improve salesforce efficiency, with language model-based stopping agents showing significant potential for optimizing conversational persistence decisions.

Abstract: Salespeople frequently face the dynamic screening decision of whether to
persist in a conversation or abandon it to pursue the next lead. Yet, little is
known about how these decisions are made, whether they are efficient, or how to
improve them. We study these decisions in the context of high-volume outbound
sales where leads are ample, but time is scarce and failure is common. We
formalize the dynamic screening decision as an optimal stopping problem and
develop a generative language model-based sequential decision agent - a
stopping agent - that learns whether and when to quit conversations by
imitating a retrospectively-inferred optimal stopping policy. Our approach
handles high-dimensional textual states, scales to large language models, and
works with both open-source and proprietary language models. When applied to
calls from a large European telecommunications firm, our stopping agent reduces
the time spent on failed calls by 54% while preserving nearly all sales;
reallocating the time saved increases expected sales by up to 37%. Upon
examining the linguistic cues that drive salespeople's quitting decisions, we
find that they tend to overweight a few salient expressions of consumer
disinterest and mispredict call failure risk, suggesting cognitive bounds on
their ability to make real-time conversational decisions. Our findings
highlight the potential of artificial intelligence algorithms to correct
cognitively-bounded human decisions and improve salesforce efficiency.

</details>


### [51] [Surfacing Subtle Stereotypes: A Multilingual, Debate-Oriented Evaluation of Modern LLMs](https://arxiv.org/abs/2511.01187)
*Muhammed Saeed,Muhammad Abdul-mageed,Shady Shehata*

Main category: cs.CL

TL;DR: DebateBias-8K is a new multilingual benchmark that reveals narrative bias in LLMs through debate-style prompts across sensitive domains, showing models reproduce stereotypes despite safety alignment, especially in low-resource languages.


<details>
  <summary>Details</summary>
Motivation: Current bias evaluations rely on English classification tasks, but real-world LLM deployment involves open-ended communication where narrative bias can emerge in more subtle ways.

Method: Created DebateBias-8K with 8,400 debate prompts across 4 sensitive domains (women's rights, socioeconomic development, terrorism, religion) in 7 languages. Generated and classified over 100,000 responses from 4 flagship models (GPT-4o, Claude 3, DeepSeek, LLaMA 3).

Result: All models reproduced entrenched stereotypes: Arabs linked to terrorism/religion (95%), Africans to socioeconomic "backwardness" (up to 77%), Western groups framed as modern/progressive. Biases increased sharply in lower-resource languages.

Conclusion: Current alignment methods trained primarily in English don't generalize globally, reducing explicit toxicity but failing to prevent biased outputs in open-ended contexts. Need better multilingual bias evaluation and culturally inclusive alignment.

Abstract: Large language models (LLMs) are widely deployed for open-ended
communication, yet most bias evaluations still rely on English,
classification-style tasks. We introduce DebateBias-8K, a new multilingual,
debate-style benchmark designed to reveal how narrative bias appears in
realistic generative settings. Our dataset includes 8,400 structured debate
prompts spanning four sensitive domains: women's rights, socioeconomic
development, terrorism, and religion, across seven languages ranging from
high-resource (English, Chinese) to low-resource (Swahili, Nigerian Pidgin).
Using four flagship models (GPT-4o, Claude 3, DeepSeek, and LLaMA 3), we
generate and automatically classify over 100,000 responses. Results show that
all models reproduce entrenched stereotypes despite safety alignment: Arabs are
overwhelmingly linked to terrorism and religion (>=95%), Africans to
socioeconomic "backwardness" (up to <=77%), and Western groups are consistently
framed as modern or progressive. Biases grow sharply in lower-resource
languages, revealing that alignment trained primarily in English does not
generalize globally. Our findings highlight a persistent divide in multilingual
fairness: current alignment methods reduce explicit toxicity but fail to
prevent biased outputs in open-ended contexts. We release our DebateBias-8K
benchmark and analysis framework to support the next generation of multilingual
bias evaluation and safer, culturally inclusive model alignment.

</details>


### [52] [ZoFia: Zero-Shot Fake News Detection with Entity-Guided Retrieval and Multi-LLM Interaction](https://arxiv.org/abs/2511.01188)
*Lvhua Wu,Xuefeng Jiang,Sheng Sun,Tian Wen,Yuwei Wang,Min Liu*

Main category: cs.CL

TL;DR: ZoFia is a two-stage zero-shot fake news detection framework that uses hierarchical salience scoring and multi-LLM collaborative analysis to detect fake news without requiring training data.


<details>
  <summary>Details</summary>
Motivation: Current LLMs have time-bounded knowledge coverage and hallucination issues, while models trained on static datasets lack generalization for emerging news topics, making fake news detection challenging.

Method: Two-stage approach: 1) Hierarchical Salience with SC-MMR algorithm to select informative keywords for retrieving external evidence; 2) Multi-LLM interactive system with role-based agents performing collaborative analysis and adversarial debate.

Result: ZoFia significantly outperforms existing zero-shot baselines and most few-shot methods on two public datasets.

Conclusion: The proposed framework provides an effective solution for zero-shot fake news detection with interpretable and robust judgments, and the code will be open-sourced.

Abstract: The rapid spread of fake news threatens social stability and public trust,
rendering its detection an imperative research priority. Although large
language models (LLMs) excel at numerous natural language processing tasks with
their remarkable contextual understanding and extensive prior knowledge, the
time-bounded knowledge coverage and tendency for generating hallucination
content reduce their reliability when handling fast-evolving news streams.
Furthermore, models trained on existing static datasets also often lack the
generalization needed for emerging news topics. To address these challenges, we
propose ZoFia, a novel two-stage zero-shot fake news detection framework.
First, we introduce Hierarchical Salience to quantify the importance of
entities in the news content, and propose the SC-MMR algorithm to effectively
select an informative and diverse set of keywords that serve as queries for
retrieving up-to-date external evidence. Subsequently, a multi LLM interactive
system, in which each agent assumes a distinct role, performs multi-view
collaborative analysis and adversarial debate over the news text and its
related information, and finally produces an interpretable and robust judgment.
Comprehensive experiments on two public datasets demonstrate that ZoFia
obviously outperforms existing zero-shot baselines and most of few-shot
methods. Our codes will be open-sourced to facilitate related communities.

</details>


### [53] [Self-Harmony: Learning to Harmonize Self-Supervision and Self-Play in Test-Time Reinforcement Learning](https://arxiv.org/abs/2511.01191)
*Ru Wang,Wei Huang,Qi Cao,Yusuke Iwasawa,Yutaka Matsuo,Jiaxian Guo*

Main category: cs.CL

TL;DR: Self-Harmony is a test-time reinforcement learning framework that uses a single model as both Solver and Reframer to generate stable answers across original and paraphrased questions, employing harmonic mean for pseudo-label aggregation to avoid spurious solutions.


<details>
  <summary>Details</summary>
Motivation: Standard test-time RL approaches like majority voting often collapse to spurious popular answers, creating a need for more reliable learning signals without human supervision.

Method: Uses a single model in dual roles: Solver produces answers and Reframer rephrases inputs. Aggregates answer frequencies using harmonic mean across original and reframed views to select solutions stable under reframing.

Result: Achieves state-of-the-art results in label-free test-time setting, ranking first in 28 of 30 settings across multiple reasoning benchmarks with zero training failures.

Conclusion: Self-Harmony provides a stable, reliable framework for test-time adaptation that naturally avoids spurious answers through reframing stability without requiring human supervision or auxiliary models.

Abstract: Test-time reinforcement learning (TTRL) offers a label-free paradigm for
adapting models using only synthetic signals at inference, but its success
hinges on constructing reliable learning signals. Standard approaches such as
majority voting often collapse to spurious yet popular answers. We introduce
Self-Harmony, a framework built on a simple intuition: the correct answer
should remain stable across both an original question and its paraphrase.
Self-Harmony operationalizes this by employing a single model in two
complementary roles: a Solver to produce answers and a Reframer to rephrase the
input. Based on this, we further propose a pseudo-label method: instead of
majority voting, it aggregates answer frequencies across these original and
reframed views using the harmonic mean. This is a process that naturally
selects for solutions stable under reframing, thereby avoiding the common trap
of favoring view-dependent, spurious answers. Crucially, this requires no human
supervision or auxiliary models. Across diverse reasoning benchmarks,
Self-Harmony achieves state-of-the-art results at the label-free test-time
setting, ranking first in 28 of 30 settings across multiple methods. Beyond
accuracy, it demonstrates unprecedented robustness, with zero training failures
in all experiments, underscoring its stability and reliability.

</details>


### [54] [DEER: Disentangled Mixture of Experts with Instance-Adaptive Routing for Generalizable Machine-Generated Text Detection](https://arxiv.org/abs/2511.01192)
*Guoxin Ma,Xiaoming Liu,Zhanhan Zhang,Chengzhengxu Li,Shengchao Liu,Yu Lan*

Main category: cs.CL

TL;DR: Proposes DEER, a two-stage disentangled mixture-of-experts framework for detecting machine-generated text that handles domain shift by combining domain-specific and domain-general experts with reinforcement learning-based routing.


<details>
  <summary>Details</summary>
Motivation: Current machine-generated text detection methods suffer from significant performance degradation under domain shift, creating a need for approaches that can capture both domain-specific and transferable patterns.

Method: Two-stage DEER architecture: 1) Disentangled mixture-of-experts module with domain-specific experts for fine-grained distinctions and shared experts for cross-domain features; 2) Reinforcement learning-based routing mechanism for dynamic expert selection without domain labels during inference.

Result: Achieves average F1-score improvements of 1.39% (in-domain) and 5.32% (out-of-domain), and accuracy gains of 1.35% (in-domain) and 3.61% (out-of-domain) across five in-domain and five out-of-domain benchmark datasets.

Conclusion: DEER effectively addresses domain shift in machine-generated text detection through disentangled expert specialization and adaptive routing, consistently outperforming state-of-the-art methods.

Abstract: Detecting machine-generated text (MGT) has emerged as a critical challenge,
driven by the rapid advancement of large language models (LLMs) capable of
producing highly realistic, human-like content. However, the performance of
current approaches often degrades significantly under domain shift. To address
this challenge, we propose a novel framework designed to capture both
domain-specific and domain-general MGT patterns through a two-stage
Disentangled mixturE-of-ExpeRts (DEER) architecture. First, we introduce a
disentangled mixture-of-experts module, in which domain-specific experts learn
fine-grained, domain-local distinctions between human and machine-generated
text, while shared experts extract transferable, cross-domain features. Second,
to mitigate the practical limitation of unavailable domain labels during
inference, we design a reinforcement learning-based routing mechanism that
dynamically selects the appropriate experts for each input instance,
effectively bridging the train-inference gap caused by domain uncertainty.
Extensive experiments on five in-domain and five out-of-domain benchmark
datasets demonstrate that DEER consistently outperforms state-of-the-art
methods, achieving average F1-score improvements of 1.39% and 5.32% on
in-domain and out-of-domain datasets respectively, along with accuracy gains of
1.35% and 3.61% respectively. Ablation studies confirm the critical
contributions of both disentangled expert specialization and adaptive routing
to model performance.

</details>


### [55] [AraFinNews: Arabic Financial Summarisation with Domain-Adapted LLMs](https://arxiv.org/abs/2511.01265)
*Mo El-Haj,Paul Rayson*

Main category: cs.CL

TL;DR: This paper introduces AraFinNews, the largest Arabic financial news dataset, and shows that domain-specific pretraining improves factual accuracy and coherence in Arabic financial text summarization.


<details>
  <summary>Details</summary>
Motivation: To address the lack of domain-specific resources for Arabic financial text summarization and investigate how financial-domain pretraining impacts summarization quality.

Method: Created AraFinNews dataset with 212,500 article-headline pairs, then evaluated transformer models (mT5, AraT5, FinAraT5) on factual accuracy, numerical reliability, and stylistic alignment.

Result: Domain-adapted models generated more faithful and coherent summaries, especially for quantitative and entity-centric information.

Conclusion: Domain-specific adaptation is crucial for improving factual consistency and narrative fluency in Arabic financial summarization.

Abstract: This paper investigates the impact of domain specificity on abstractive
summarisation of Arabic financial texts using large language models (LLMs). We
introduce AraFinNews, the largest publicly available Arabic financial news
dataset to date, comprising 212,500 article--headline pairs spanning nearly a
decade of reporting from October 2015 to July 2025. Designed as the Arabic
equivalent of major English summarisation corpora such as CNN/DailyMail,
AraFinNews provides a robust benchmark for evaluating domain-specific language
understanding and generation in financial contexts. Using this resource, we
evaluate transformer-based models -- including mT5, AraT5, and the
domain-adapted FinAraT5 -- to examine how financial-domain pretraining
influences factual accuracy, numerical reliability, and stylistic alignment
with professional reporting. Experimental results show that domain-adapted
models generate more faithful and coherent summaries, particularly in handling
quantitative and entity-centric information. The findings highlight the
importance of domain-specific adaptation for improving factual consistency and
narrative fluency in Arabic financial summarisation. The dataset is freely
available for non-commercial research at
https://github.com/ArabicNLP-UK/AraFinNews.

</details>


### [56] [When, What, and How: Rethinking Retrieval-Enhanced Speculative Decoding](https://arxiv.org/abs/2511.01282)
*Min Fang,Zhihui Fu,Qibin Zhao,Jun Wang*

Main category: cs.CL

TL;DR: ReSpec is a novel speculative decoding framework that improves LLM inference acceleration by replacing heuristic drafter switching with adaptive decision-making, achieving 33%+ speedup over state-of-the-art methods while maintaining output quality.


<details>
  <summary>Details</summary>
Motivation: Current speculative decoding methods face limitations: model-based approaches are accurate but costly, while retrieval-enhanced methods rely on heuristic switching that triggers unnecessary retrievals, reducing efficiency.

Method: ReSpec introduces three innovations: 1) entropy-guided adaptive trigger for retrieval initiation based on contextual predictability, 2) feedback-driven candidate selection using historical feedback to organize high-quality candidates, and 3) source-aware relaxed verification with strict checks for model drafts and relaxed verification for retrieved drafts.

Result: Extensive experiments on Spec-Bench show ReSpec achieves state-of-the-art acceleration, outperforming EAGLE-2 by over 33% and SAM-Decoding by over 25% while maintaining output quality.

Conclusion: ReSpec successfully transforms heuristic drafter switching into adaptive decision-making, achieving superior acceleration in LLM inference through intelligent retrieval management and verification strategies.

Abstract: Speculative decoding (SD) has emerged as an effective technique to accelerate
large language model (LLM) inference without compromising output quality.
However, the achievable speedup largely depends on the effectiveness of the
drafting model. While model-based methods like EAGLE-2 are accurate but costly,
retrieval-enhanced methods like SAM-Decoding rely on heuristic switching
strategies that often trigger unnecessary retrievals. To address this, we
propose ReSpec (\textbf{Re}trieval-enhanced \textbf{Spe}culative Decoding), a
novel framework that transforms heuristic drafter switching into adaptive
decision-making. ReSpec features three core innovations: 1) An
\textbf{entropy-guided adaptive trigger} quantifies contextual predictability
to initiate retrieval only when uncertainty is low, avoiding costly low-quality
speculations. 2) A \textbf{feedback-driven candidate selection} leverages
historical feedback to organize multiple high-quality candidates for parallel
verification, maximizing retrieval utility. 3) A source-aware \textbf{relaxed
verification strategy} applies strict checks to model-generated drafts while
using a relaxed verification for retrieved drafts, achieving a better balance
between accuracy and efficiency. Extensive experiments on Spec-Bench
demonstrate that ReSpec achieves state-of-the-art acceleration,outperforming
EAGLE-2 and SAM-Decoding by over $33\%$ and $25\%$, respectively, while
maintaining output quality.

</details>


### [57] ["Give a Positive Review Only": An Early Investigation Into In-Paper Prompt Injection Attacks and Defenses for AI Reviewers](https://arxiv.org/abs/2511.01287)
*Qin Zhou,Zhexin Zhang,Zhi Li,Limin Sun*

Main category: cs.CL

TL;DR: This paper investigates prompt injection attacks on AI-assisted peer review systems, showing that both static and iterative attacks can manipulate AI reviewers into giving artificially high scores, and explores detection-based defenses.


<details>
  <summary>Details</summary>
Motivation: With AI models increasingly used for scientific paper review, there's a growing threat of hidden injected prompts designed to manipulate AI reviewers into providing overly favorable evaluations.

Method: Proposed two attack classes: static attack (fixed injection prompt) and iterative attack (optimizing injection prompt against simulated reviewer model). Also explored a simple detection-based defense.

Result: Both attacks achieved striking performance, frequently inducing full evaluation scores from frontier AI reviewers. Attacks were robust across various settings. Detection defense substantially reduced attack success but could be partially circumvented by adaptive attackers.

Conclusion: Findings underscore the need for greater attention and rigorous safeguards against prompt-injection threats in AI-assisted peer review.

Abstract: With the rapid advancement of AI models, their deployment across diverse
tasks has become increasingly widespread. A notable emerging application is
leveraging AI models to assist in reviewing scientific papers. However, recent
reports have revealed that some papers contain hidden, injected prompts
designed to manipulate AI reviewers into providing overly favorable
evaluations. In this work, we present an early systematic investigation into
this emerging threat. We propose two classes of attacks: (1) static attack,
which employs a fixed injection prompt, and (2) iterative attack, which
optimizes the injection prompt against a simulated reviewer model to maximize
its effectiveness. Both attacks achieve striking performance, frequently
inducing full evaluation scores when targeting frontier AI reviewers.
Furthermore, we show that these attacks are robust across various settings. To
counter this threat, we explore a simple detection-based defense. While it
substantially reduces the attack success rate, we demonstrate that an adaptive
attacker can partially circumvent this defense. Our findings underscore the
need for greater attention and rigorous safeguards against prompt-injection
threats in AI-assisted peer review.

</details>


### [58] [FirstAidQA: A Synthetic Dataset for First Aid and Emergency Response in Low-Connectivity Settings](https://arxiv.org/abs/2511.01289)
*Saiyma Sittul Muna,Rezwan Islam Salvi,Mushfiqur Rahman Mushfique,Ajwad Abrar*

Main category: cs.CL

TL;DR: Created FirstAidQA - a synthetic dataset of 5,500 high-quality first aid question-answer pairs generated using ChatGPT-4o-mini and validated by humans, designed to enable lightweight AI systems for emergency response in low-connectivity environments.


<details>
  <summary>Details</summary>
Motivation: Address the lack of high-quality datasets for first aid and emergency response, enabling deployment of lightweight AI models in time-sensitive, low-connectivity environments where current LLMs are too computationally intensive.

Method: Generated dataset using ChatGPT-4o-mini with prompt-based in-context learning from Vital First Aid Book (2019), followed by preprocessing (text cleaning, contextual chunking, filtering) and human validation for accuracy and safety.

Result: Successfully created FirstAidQA dataset containing 5,500 validated QA pairs covering diverse first aid and emergency scenarios, publicly released on Hugging Face to support instruction-tuning of LLMs and SLMs.

Conclusion: FirstAidQA enables development of faster, more reliable, offline-capable AI systems for emergency settings, advancing research on safety-critical and resource-constrained AI applications in first aid and emergency response.

Abstract: In emergency situations, every second counts. The deployment of Large
Language Models (LLMs) in time-sensitive, low or zero-connectivity environments
remains limited. Current models are computationally intensive and unsuitable
for low-tier devices often used by first responders or civilians. A major
barrier to developing lightweight, domain-specific solutions is the lack of
high-quality datasets tailored to first aid and emergency response. To address
this gap, we introduce FirstAidQA, a synthetic dataset containing 5,500
high-quality question answer pairs that encompass a wide range of first aid and
emergency response scenarios. The dataset was generated using a Large Language
Model, ChatGPT-4o-mini, with prompt-based in-context learning, using texts from
the Vital First Aid Book (2019). We applied preprocessing steps such as text
cleaning, contextual chunking, and filtering, followed by human validation to
ensure accuracy, safety, and practical relevance of the QA pairs. FirstAidQA is
designed to support instruction-tuning and fine-tuning of LLMs and Small
Language Models (SLMs), enabling faster, more reliable, and offline-capable
systems for emergency settings. We publicly release the dataset to advance
research on safety-critical and resource-constrained AI applications in first
aid and emergency response. The dataset is available on Hugging Face at
https://huggingface.co/datasets/i-am-mushfiq/FirstAidQA.

</details>


### [59] [DeepSpecs: Expert-Level Questions Answering in 5G](https://arxiv.org/abs/2511.01305)
*Aman Ganapathy Manvattira,Yifei Xu,Ziyue Dang,Songwu Lu*

Main category: cs.CL

TL;DR: DeepSpecs is an enhanced RAG system for 5G standards that uses structural and temporal reasoning through three specialized databases to resolve cross-references and track specification evolution, outperforming existing telecom RAG systems.


<details>
  <summary>Details</summary>
Motivation: 5G standards contain thousands of cross-referenced pages that evolve across releases, making expert-level QA challenging. Existing RAG frameworks cannot reliably resolve cross-references or reason about specification evolution.

Method: Uses three metadata-rich databases: SpecDB (clause-aligned text), ChangeDB (version diffs), and TDocDB (meeting documents). Explicitly resolves cross-references through recursive metadata lookup and traces evolution by mining changes linked to Change Requests.

Result: Outperforms base models and state-of-the-art telecom RAG systems across multiple LLM backends. Ablations confirm that explicit cross-reference resolution and evolution-aware retrieval substantially improve answer quality.

Conclusion: Modeling the structural and temporal properties of 5G standards through enhanced RAG significantly improves expert-level QA performance, demonstrating the value of specialized reasoning capabilities for complex technical documentation.

Abstract: 5G technology enables mobile Internet access for billions of users. Answering
expert-level questions about 5G specifications requires navigating thousands of
pages of cross-referenced standards that evolve across releases. Existing
retrieval-augmented generation (RAG) frameworks, including telecom-specific
approaches, rely on semantic similarity and cannot reliably resolve
cross-references or reason about specification evolution. We present DeepSpecs,
a RAG system enhanced by structural and temporal reasoning via three
metadata-rich databases: SpecDB (clause-aligned specification text), ChangeDB
(line-level version diffs), and TDocDB (standardization meeting documents).
DeepSpecs explicitly resolves cross-references by recursively retrieving
referenced clauses through metadata lookup, and traces specification evolution
by mining changes and linking them to Change Requests that document design
rationale. We curate two 5G QA datasets: 573 expert-annotated real-world
questions from practitioner forums and educational resources, and 350
evolution-focused questions derived from approved Change Requests. Across
multiple LLM backends, DeepSpecs outperforms base models and state-of-the-art
telecom RAG systems; ablations confirm that explicit cross-reference resolution
and evolution-aware retrieval substantially improve answer quality,
underscoring the value of modeling the structural and temporal properties of 5G
standards.

</details>


### [60] [DEEPAMBIGQA: Ambiguous Multi-hop Questions for Benchmarking LLM Answer Completeness](https://arxiv.org/abs/2511.01323)
*Jiabao Ji,Min Li,Priyanshu Kumar,Shiyu Chang,Saloni Potdar*

Main category: cs.CL

TL;DR: DeepAmbigQA is a new dataset for evaluating LLMs on complex questions requiring multi-hop reasoning and name ambiguity resolution, showing current models struggle with answer completeness.


<details>
  <summary>Details</summary>
Motivation: Existing QA benchmarks don't adequately evaluate the combined challenges of name ambiguity and multi-step reasoning in complex questions, which LLMs often fail to handle completely.

Method: Created DeepAmbigQAGen pipeline to automatically generate QA tasks grounded in text corpora and knowledge graphs, producing 3,600 questions with systematic name ambiguity and multi-hop reasoning requirements.

Result: State-of-the-art GPT-5 achieved only 0.13 exact match on ambiguous questions and 0.21 on non-ambiguous questions, showing significant limitations in producing complete answer sets.

Conclusion: Current QA systems need substantial improvement for robust information gathering and answer completeness, particularly for complex questions involving ambiguity and multi-step reasoning.

Abstract: Large language models (LLMs) with integrated search tools show strong promise
in open-domain question answering (QA), yet they often struggle to produce
complete answer set to complex questions such as Which actor from the film Heat
won at least one Academy Award?, which requires (1) distinguishing between
multiple films sharing the same title and (2) reasoning across a large set of
actors to gather and integrate evidence. Existing QA benchmarks rarely evaluate
both challenges jointly. To address this, we introduce DeepAmbigQAGen, an
automatic data generation pipeline that constructs QA tasks grounded in text
corpora and linked knowledge graph, generating natural and verifiable questions
that systematically embed name ambiguity and multi-step reasoning. Based on
this, we build DeepAmbigQA, a dataset of 3,600 questions requiring multi-hop
reasoning and half of them explicit name ambiguity resolving. Experiments
reveal that, even state-of-the-art GPT-5 show incomplete answers, achieving
only 0.13 exact match on ambiguous questions and 0.21 on non-ambiguous
questions. These findings highlight the need for more robust QA systems aimed
at information gathering and answer completeness.

</details>


### [61] [Thinking with DistilQwen: A Tale of Four Distilled Reasoning and Reward Model Series](https://arxiv.org/abs/2511.01354)
*Wenrui Cai,Chengyu Wang,Junbing Yan,Jun Huang,Xiangzhong Fang*

Main category: cs.CL

TL;DR: The paper extends the DistilQwen model family with four specialized series: slow-thinking models for high accuracy reasoning, adaptive-thinking models for dynamic strategy adjustment, and distilled reward models for reinforcement learning, all optimized for industrial applications.


<details>
  <summary>Details</summary>
Motivation: To meet industrial demand for small, efficient reasoning models that balance performance and inference speed for real-world applications.

Method: Knowledge distillation techniques applied to Qwen models, creating four specialized model series with different reasoning strategies and optimization approaches.

Result: Comprehensive evaluations show high inference efficiency and strong reasoning performance across multiple benchmarks, with practical utility demonstrated for distilled reward models.

Conclusion: The distilled models successfully support industry practitioners through scalable training and inference on Alibaba Cloud PAI platform, providing efficient reasoning solutions for diverse industrial scenarios.

Abstract: Recently, the demand for small and efficient reasoning models to support
real-world applications has driven the development of knowledge distillation
techniques that balance reasoning performance and inference speed. In this
paper, we further extend the DistilQwen model family, initialized from the Qwen
models, by introducing four model series specifically designed to meet
industrial requirements. The distilled model collection comprises: (1)
slow-thinking models, optimized for reasoning tasks that require high accuracy;
(2) two series of adaptive-thinking models, which dynamically adjust reasoning
strategies based on input tasks to maximize efficiency across diverse
scenarios; and (3) distilled reward models, which enable further reinforcement
learning of reasoning models using distilled knowledge. Comprehensive
evaluations across multiple benchmarks demonstrate both high inference
efficiency and strong reasoning performance for these models, as well as the
practical utility of distilled reward models. We further show that these models
support industry practitioners by providing scalable training and inference
functionalities on the Alibaba Cloud PAI (Platform for Artificial Intelligence)
platform.

</details>


### [62] [PrefixNLI: Detecting Factual Inconsistencies as Soon as They Arise](https://arxiv.org/abs/2511.01359)
*Sapir Harary,Eran Hirsch,Aviv Slobodkin,David Wan,Mohit Bansal,Ido Dagan*

Main category: cs.CL

TL;DR: The paper introduces MiniTruePrefixes, a specialized model for detecting factual inconsistencies in text prefixes during autoregressive generation, which improves factual consistency in LLM outputs when integrated into controlled decoding.


<details>
  <summary>Details</summary>
Motivation: NLI models are typically used for complete sentences, but autoregressive generation makes decisions at the prefix level during decoding. There's a need to generalize entailment detection to text prefixes to improve generation faithfulness.

Method: Generalized entailment detection to text prefixes, created evaluation/training datasets, trained MiniTruePrefixes model, and integrated it into controlled decoding framework for abstractive summarization.

Result: MiniTruePrefixes outperforms baseline NLI models by 5-14 F1 points in prefix-level entailment. When integrated into decoding, LLaMA-3.2-3B-Instruct matches faithfulness and runtime of 8B model while using half the memory.

Conclusion: Specialized prefix-level entailment detection models like MiniTruePrefixes can substantially improve factual consistency in LLM generation while being more memory-efficient.

Abstract: Natural Language Inference (NLI) models have been used in various ways to
improve the factuality of LLM outputs. This is typically done by applying an
NLI model to judge whether the model output is entailed from the supposed
evidence, triggering some corrective actions, such as beam reranking at
inference time or RL rewards during training. While NLI models are trained to
detect factual inconsistencies over complete sentences, decisions in the common
autoregressive generation architecture are made for each evolving text prefix,
during decoding. Addressing this setting, we generalize the entailment
detection task to apply over arbitrary text prefixes, and suggest its utility
for improving generation faithfulness. Providing suitable evaluation and
training datasets for this task, we train MiniTruePrefixes, a novel specialized
model that better detects factual inconsistencies over text prefixes,
outperforming comparable baseline NLI models by 5-14 F1 points in prefix-level
entailment. We further demonstrate that integrating MiniTruePrefixes into a
controlled decoding framework substantially improves factual consistency in
abstractive summarization. When guided by MiniTruePrefixes,
LLaMA-3.2-3B-Instruct matches the faithfulness and runtime of the 8B model from
the same model family, while using only half the memory.

</details>


### [63] [Safer in Translation? Presupposition Robustness in Indic Languages](https://arxiv.org/abs/2511.01360)
*Aadi Palnitkar,Arjun Suresh,Rishi Rajesh,Puneet Puli*

Main category: cs.CL

TL;DR: Cancer-Myth-Indic is a multilingual benchmark for evaluating LLMs on cancer-related medical advice in 5 Indic languages, created by translating 500 items from Cancer-Myth to address the gap in non-English medical LLM evaluation.


<details>
  <summary>Details</summary>
Motivation: There's a growing gap in multilingual LLM evaluation for healthcare advice, as existing medical benchmarks are almost universally in English, despite increasing use of LLMs for medical consultation across different languages.

Method: Translated a 500-item subset of Cancer-Myth benchmark into 5 Indic languages (2,500 items total) using native-speaker translators following a style guide to preserve implicit presuppositions. Items contain false presuppositions about cancer to test LLMs under presupposition stress.

Result: The paper presents the creation of Cancer-Myth-Indic benchmark and evaluates several popular LLMs using this multilingual medical evaluation framework.

Conclusion: The work addresses the multilingual gap in medical LLM evaluation by providing a comprehensive benchmark for assessing LLM performance on cancer-related medical advice across multiple Indic languages.

Abstract: Increasingly, more and more people are turning to large language models
(LLMs) for healthcare advice and consultation, making it important to gauge the
efficacy and accuracy of the responses of LLMs to such queries. While there are
pre-existing medical benchmarks literature which seeks to accomplish this very
task, these benchmarks are almost universally in English, which has led to a
notable gap in existing literature pertaining to multilingual LLM evaluation.
Within this work, we seek to aid in addressing this gap with Cancer-Myth-Indic,
an Indic language benchmark built by translating a 500-item subset of
Cancer-Myth, sampled evenly across its original categories, into five
under-served but widely used languages from the subcontinent (500 per language;
2,500 translated items total). Native-speaker translators followed a style
guide for preserving implicit presuppositions in translation; items feature
false presuppositions relating to cancer. We evaluate several popular LLMs
under this presupposition stress.

</details>


### [64] [The Ouroboros of Benchmarking: Reasoning Evaluation in an Era of Saturation](https://arxiv.org/abs/2511.01365)
*brahim Ethem Deveci,Duygu Ataman*

Main category: cs.CL

TL;DR: The paper examines whether surpassing benchmarks truly demonstrates reasoning ability in LLMs/LRMs or if we're just tracking numbers divorced from actual capabilities, analyzing performance trends across OpenAI, Anthropic, and Google models.


<details>
  <summary>Details</summary>
Motivation: Due to rapid model improvements and potential data contamination, benchmark results become saturated, creating a continuous need for new benchmarks, raising questions about whether high scores truly reflect reasoning capabilities.

Method: Investigation of three model families (OpenAI, Anthropic, Google) analyzing how their reasoning capabilities evolve across different benchmarks over years, with performance trend analysis across reasoning tasks.

Result: Comprehensive overview of benchmarks and reasoning tasks showing evolving performance patterns, highlighting the current benchmarking situation and remaining challenges in evaluating true reasoning ability.

Conclusion: The work serves as a reference for future research in reasoning evaluation and model development, emphasizing the need to ensure benchmarks actually measure the reasoning capabilities they claim to assess.

Abstract: The rapid rise of Large Language Models (LLMs) and Large Reasoning Models
(LRMs) has been accompanied by an equally rapid increase of benchmarks used to
assess them. However, due to both improved model competence resulting from
scaling and novel training advances as well as likely many of these datasets
being included in pre or post training data, results become saturated, driving
a continuous need for new and more challenging replacements. In this paper, we
discuss whether surpassing a benchmark truly demonstrates reasoning ability or
are we simply tracking numbers divorced from the capabilities we claim to
measure? We present an investigation focused on three model families, OpenAI,
Anthropic, and Google, and how their reasoning capabilities across different
benchmarks evolve over the years. We also analyze performance trends over the
years across different reasoning tasks and discuss the current situation of
benchmarking and remaining challenges. By offering a comprehensive overview of
benchmarks and reasoning tasks, our work aims to serve as a first reference to
ground future research in reasoning evaluation and model development.

</details>


### [65] [Confounding Factors in Relating Model Performance to Morphology](https://arxiv.org/abs/2511.01380)
*Wessel Poelman,Thomas Bauwens,Miryam de Lhoneux*

Main category: cs.CL

TL;DR: This paper examines how morphological characteristics affect tokenization and language modeling, identifies confounding factors in previous research, re-assesses hypotheses about agglutinative vs. fusional languages, and introduces token bigram metrics as predictors of language modeling difficulty.


<details>
  <summary>Details</summary>
Motivation: To resolve conflicting evidence about whether morphological differences matter for language modeling, by identifying and addressing confounding factors in experimental setups that make comparisons difficult.

Method: Identified confounding factors in existing analyses, re-assessed three hypotheses about morphological alignment, tokenization efficiency, and dataset size, and introduced token bigram metrics as intrinsic predictors of language modeling difficulty.

Result: Showed that previous conclusions about morphology and language modeling include confounding factors, and demonstrated that token bigram metrics serve as gradient proxies for morphological complexity without requiring expert annotation.

Conclusion: Outlined necessary conditions to reliably determine whether and how morphology relates to language modeling, emphasizing the need to control for confounding variables in experimental design.

Abstract: The extent to which individual language characteristics influence
tokenization and language modeling is an open question. Differences in
morphological systems have been suggested as both unimportant and crucial to
consider (Cotterell et al., 2018; Gerz et al., 2018a; Park et al., 2021, inter
alia). We argue this conflicting evidence is due to confounding factors in
experimental setups, making it hard to compare results and draw conclusions. We
identify confounding factors in analyses trying to answer the question of
whether, and how, morphology relates to language modeling. Next, we re-assess
three hypotheses by Arnett & Bergen (2025) for why modeling agglutinative
languages results in higher perplexities than fusional languages: they look at
morphological alignment of tokenization, tokenization efficiency, and dataset
size. We show that each conclusion includes confounding factors. Finally, we
introduce token bigram metrics as an intrinsic way to predict the difficulty of
causal language modeling, and find that they are gradient proxies for
morphological complexity that do not require expert annotation. Ultimately, we
outline necessities to reliably answer whether, and how, morphology relates to
language modeling.

</details>


### [66] [RAGSmith: A Framework for Finding the Optimal Composition of Retrieval-Augmented Generation Methods Across Datasets](https://arxiv.org/abs/2511.01386)
*Muhammed Yusuf Kartal,Suha Kagan Kose,Korhan Sevin,Burak Aktas*

Main category: cs.CL

TL;DR: RAGSmith is a framework that uses genetic search to optimize RAG pipeline configurations across 46,080 possibilities, achieving consistent performance improvements over naive RAG baselines across multiple domains.


<details>
  <summary>Details</summary>
Motivation: RAG quality depends on many interacting choices across retrieval, ranking, augmentation, prompting, and generation, making isolated module optimization brittle. There's a need for end-to-end optimization of RAG systems.

Method: Modular framework with genetic search over 9 technique families and 46,080 pipeline configurations, optimizing a joint objective combining retrieval metrics (recall@k, mAP, nDCG, MRR) and generation metrics (LLM-Judge, semantic similarity).

Result: RAGSmith outperforms naive RAG baseline by +3.8% on average (range +1.2% to +6.9% across domains), with gains up to +12.5% in retrieval and +7.5% in generation. Search explores only ~0.2% of space (~100 candidates) and discovers robust backbone configurations.

Conclusion: Evolutionary search effectively optimizes full RAG pipelines, providing practical domain-aware guidance. Vector retrieval plus post-generation reflection/revision forms a robust backbone, with domain-dependent choices in expansion, reranking, augmentation, and prompt reordering.

Abstract: Retrieval-Augmented Generation (RAG) quality depends on many interacting
choices across retrieval, ranking, augmentation, prompting, and generation, so
optimizing modules in isolation is brittle. We introduce RAGSmith, a modular
framework that treats RAG design as an end-to-end architecture search over nine
technique families and 46{,}080 feasible pipeline configurations. A genetic
search optimizes a scalar objective that jointly aggregates retrieval metrics
(recall@k, mAP, nDCG, MRR) and generation metrics (LLM-Judge and semantic
similarity). We evaluate on six Wikipedia-derived domains (Mathematics, Law,
Finance, Medicine, Defense Industry, Computer Science), each with 100 questions
spanning factual, interpretation, and long-answer types. RAGSmith finds
configurations that consistently outperform naive RAG baseline by +3.8\% on
average (range +1.2\% to +6.9\% across domains), with gains up to +12.5\% in
retrieval and +7.5\% in generation. The search typically explores $\approx
0.2\%$ of the space ($\sim 100$ candidates) and discovers a robust backbone --
vector retrieval plus post-generation reflection/revision -- augmented by
domain-dependent choices in expansion, reranking, augmentation, and prompt
reordering; passage compression is never selected. Improvement magnitude
correlates with question type, with larger gains on factual/long-answer mixes
than interpretation-heavy sets. These results provide practical, domain-aware
guidance for assembling effective RAG systems and demonstrate the utility of
evolutionary search for full-pipeline optimization.

</details>


### [67] [LiveSearchBench: An Automatically Constructed Benchmark for Retrieval and Reasoning over Dynamic Knowledge](https://arxiv.org/abs/2511.01409)
*Heng Zhou,Ao Yu,Yuchen Fan,Jianing Shi,Li Kang,Hejia Geng,Yongting Zhang,Yutao Fan,Yuhao Wu,Tiancheng He,Yiran Qin,Lei Bai,Zhenfei Yin*

Main category: cs.CL

TL;DR: LiveSearchBench is an automated pipeline for creating retrieval-dependent benchmarks from recent knowledge updates to evaluate LLMs on current facts rather than static memorization.


<details>
  <summary>Details</summary>
Motivation: Static benchmarks reward memorization and understate retrieval's role, failing to capture the dynamic nature of world knowledge in LLM evaluation.

Method: Computes deltas between successive Wikidata snapshots, filters candidate triples for quality, and synthesizes natural-language questions at three reasoning difficulty levels with unique, verifiable answers through SPARQL validation.

Result: Experiments show significant performance drop when models confront post-pretraining facts, especially on multi-hop queries. Retrieval augmentation and larger models provide partial gains but fail to close the recency gap.

Conclusion: LiveSearchBench shifts evaluation from static memorization toward tasks requiring up-to-date retrieval and reasoning, enabling systematic long-term assessment of LLMs under evolving knowledge.

Abstract: Evaluating large language models (LLMs) on question answering often relies on
static benchmarks that reward memorization and understate the role of
retrieval, failing to capture the dynamic nature of world knowledge. We present
LiveSearchBench, an automated pipeline for constructing retrieval-dependent
benchmarks from recent knowledge updates. Our method computes deltas between
successive Wikidata snapshots, filters candidate triples for quality, and
synthesizes natural-language questions at three levels of reasoning difficulty,
each guaranteed to admit a unique, verifiable answer through SPARQL validation.
The pipeline is fully automated, scalable across time, and minimizes human
intervention, enabling continual regeneration of temporally grounded
benchmarks. Experiments show a pronounced performance drop when models confront
facts that post-date pretraining, with the gap most salient on multi-hop
queries. Retrieval augmented methods and larger, instruction-tuned models
provide partial gains but fail to close this recency gap. By design,
LiveSearchBench shifts evaluation from static memorization toward tasks that
require up-to-date retrieval and reasoning, offering a foundation for
systematic, long-term assessment of LLMs under evolving knowledge.

</details>


### [68] ["Don't Teach Minerva": Guiding LLMs Through Complex Syntax for Faithful Latin Translation with RAG](https://arxiv.org/abs/2511.01454)
*Sergio Torres Aguilar*

Main category: cs.CL

TL;DR: A draft-based refinement pipeline using open-source LLMs achieves performance comparable to proprietary systems for Latin translation, combining fine-tuned NLLB-1.3B for draft generation with zero-shot LLMs for polishing, enhanced by RAG.


<details>
  <summary>Details</summary>
Motivation: Translating morphology-rich, low-resource languages like Latin is challenging, and the paper aims to develop a reproducible open-source solution that can match proprietary systems without task-specific fine-tuning.

Method: Two-stage pipeline: 1) Fine-tuned NLLB-1.3B generates structurally faithful drafts, 2) Zero-shot LLMs (Llama-3.3 or Qwen3) polish drafts, optionally enhanced with retrieved out-context examples (RAG).

Result: The open-source RAG system achieves performance statistically comparable to GPT-5 baseline on both in-domain (Rosenthal, 2023) and challenging out-of-domain 12th-century Latin letters benchmarks.

Conclusion: The proposed pipeline demonstrates that open-source systems can match proprietary performance for Latin translation without task-specific fine-tuning, with released resources to support replicability and further research.

Abstract: Translating a morphology-rich, low-resource language like Latin poses
significant challenges. This paper introduces a reproducible draft-based
refinement pipeline that elevates open-source Large Language Models (LLMs) to a
performance level statistically comparable to top-tier proprietary systems. Our
method first uses a fine-tuned NLLB-1.3B model to generate a high-quality,
structurally faithful draft. A zero-shot LLM (Llama-3.3 or Qwen3) then polishes
this draft, a process that can be further enhanced by augmenting the context
with retrieved out-context examples (RAG). We demonstrate the robustness of
this approach on two distinct benchmarks: a standard in-domain test set
(Rosenthal, 2023) and a new, challenging out-of-domain (OOD) set of
12th-century Latin letters (2025). Our central finding is that this open-source
RAG system achieves performance statistically comparable to the GPT-5 baseline,
without any task-specific LLM fine-tuning. We release the pipeline, the
Chartres OOD set, and evaluation scripts and models to facilitate replicability
and further research.

</details>


### [69] [BARD: budget-aware reasoning distillation](https://arxiv.org/abs/2511.01470)
*Lujie Niu,Lei Shen,Yi Jiang,Caixia Yuan,Xiaojie Wang,Wenbo Su,Bo zheng*

Main category: cs.CL

TL;DR: BARD is a framework that distills reasoning capability to smaller models while enabling fine-grained control over reasoning length using budget constraints.


<details>
  <summary>Details</summary>
Motivation: Current CoT distillation methods produce redundant reasoning processes with uncontrollable computational costs, leading to inefficient resource usage.

Method: Two-phase training: 1) SFT on teacher-generated CoT data compressed to various budget levels, 2) RL with reward considering both reasoning performance and budget fidelity.

Result: An 8B student model achieves strong performance on challenging reasoning benchmarks (AIME24, AIME25, GPQA) with precise adaptive control over reasoning length.

Conclusion: BARD successfully enables smaller models to achieve strong reasoning performance while providing computational budget control, balancing efficiency and capability.

Abstract: While long Chain-of-Thought (CoT) distillation effectively transfers
reasoning capability to smaller language models, the reasoning process often
remains redundant and computational budget uncontrollable, leading to
inefficient resource usage. To address this limitation, we propose
\textbf{Budget-Aware Reasoning Distillation (BARD)}, a novel framework that
simultaneously distills reasoning capability and enables fine-grained control
over the reasoning length. BARD uses the thinking budget as a user-specified
control signal, allowing the model to dynamically balance reasoning performance
and computational efficiency. To achieve this concept, BARD introduces a
two-phase training regimen. The first phase, Supervised Fine-Tuning (SFT) on
teacher-generated long CoT data compressed to various budget levels,
bootstrapping the model's understanding of budget constraints. The second phase
leverages Reinforcement Learning (RL) from a reward signal in consideration of
reasoning performance and budget fidelity simultaneously. Incorporating the
two-phase regimen is crucial to avoiding policy degradation and ensuring that
both objectives are optimized jointly. Extensive experiments demonstrate that
our method empowers an 8B student model to achieve strong performance on
challenging reasoning benchmarks (\textit{AIME24, AIME25, GPQA}) while
providing precise and adaptive control over its reasoning length across a wide
range of budgets.

</details>


### [70] [Towards Consistent Detection of Cognitive Distortions: LLM-Based Annotation and Dataset-Agnostic Evaluation](https://arxiv.org/abs/2511.01482)
*Neha Sharma,Navneet Agarwal,Kairit Sirts*

Main category: cs.CL

TL;DR: LLMs can serve as consistent annotators for subjective cognitive distortion detection, with GPT-4 achieving high annotation consistency and improving model performance compared to human-labeled data.


<details>
  <summary>Details</summary>
Motivation: Cognitive distortion detection suffers from low inter-annotator agreement among human experts, leading to unreliable annotations for training models.

Method: Used multiple independent LLM runs to generate stable annotations, and introduced a dataset-agnostic evaluation framework using Cohen's kappa as effect size measure for fair comparisons.

Result: GPT-4 produced highly consistent annotations (Fleiss's Kappa = 0.78), and models trained on LLM-generated annotations outperformed those trained on human-labeled data.

Conclusion: LLMs provide a scalable and internally consistent alternative for generating training data that supports strong performance in subjective NLP tasks.

Abstract: Text-based automated Cognitive Distortion detection is a challenging task due
to its subjective nature, with low agreement scores observed even among expert
human annotators, leading to unreliable annotations. We explore the use of
Large Language Models (LLMs) as consistent and reliable annotators, and propose
that multiple independent LLM runs can reveal stable labeling patterns despite
the inherent subjectivity of the task. Furthermore, to fairly compare models
trained on datasets with different characteristics, we introduce a
dataset-agnostic evaluation framework using Cohen's kappa as an effect size
measure. This methodology allows for fair cross-dataset and cross-study
comparisons where traditional metrics like F1 score fall short. Our results
show that GPT-4 can produce consistent annotations (Fleiss's Kappa = 0.78),
resulting in improved test set performance for models trained on these
annotations compared to those trained on human-labeled data. Our findings
suggest that LLMs can offer a scalable and internally consistent alternative
for generating training data that supports strong downstream performance in
subjective NLP tasks.

</details>


### [71] [Synthetic Eggs in Many Baskets: The Impact of Synthetic Data Diversity on LLM Fine-Tuning](https://arxiv.org/abs/2511.01490)
*Max Schaffelder,Albert Gatt*

Main category: cs.CL

TL;DR: Investigates how synthetic data source diversity affects fine-tuned LLMs across distribution collapse, adversarial robustness, and self-preference bias.


<details>
  <summary>Details</summary>
Motivation: Understanding the impact of synthetic data on model behavior is crucial as synthetic data becomes widely used in language model development.

Method: Analyzes fine-tuned large language models using synthetic data from diverse sources, focusing on three key dimensions: distribution collapse, adversarial robustness, and self-preference bias.

Result: Diverse synthetic data mitigates distribution collapse and preserves output diversity. Synthetic fine-tuning preserves higher output quality while removing safeguards. Fine-tuning reduces self-preference bias, with human data being most effective.

Conclusion: Synthetic data source diversity plays a critical role in model behavior, affecting distribution preservation, output quality, and bias reduction.

Abstract: As synthetic data becomes widely used in language model development,
understanding its impact on model behavior is crucial. This paper investigates
the impact of the diversity of sources of synthetic data on fine-tuned large
language models. We focus on three key dimensions: distribution collapse,
adversarial robustness, and self-preference bias. Our findings reveal that
fine-tuning models on synthetic data from diverse sources can mitigate
distribution collapse, preserving the breadth of the output distribution and
the diversity of the output text. Furthermore, while both human and synthetic
fine-tuning data can remove safeguards, the latter preserves higher output
quality, thus making outputs potentially more usable and dangerous. Finally,
fine-tuning reduces self-preference bias, with human data being the most
effective, followed by multi-source synthetic data.

</details>


### [72] [BanglaNirTox: A Large-scale Parallel Corpus for Explainable AI in Bengali Text Detoxification](https://arxiv.org/abs/2511.01512)
*Ayesha Afroza Mohsin,Mashrur Ahsan,Nafisa Maliyat,Shanta Maria,Syed Rifat Raiyan,Hasan Mahmud,Md Kamrul Hasan*

Main category: cs.CL

TL;DR: A novel pipeline combining Pareto-optimized LLMs and Chain-of-Thought prompting for Bengali text detoxification, supported by the BanglaNirTox dataset of 68,041 toxic sentences with detoxified versions.


<details>
  <summary>Details</summary>
Motivation: Toxic language in Bengali is prevalent online with few effective countermeasures, and text detoxification research is limited for Bengali due to resource constraints.

Method: Proposed pipeline using Pareto class-optimized LLMs and Chain-of-Thought prompting, creating BanglaNirTox dataset with 68,041 toxic Bengali sentences, class-wise toxicity labels, reasonings, and detoxified paraphrases.

Result: Pareto-optimized LLMs with CoT prompting significantly improve quality and consistency of Bengali text detoxification, with the dataset enabling better fine-tuning of language models.

Conclusion: The approach successfully addresses Bengali text detoxification challenges through optimized LLMs and comprehensive dataset creation, showing promising results for low-resource language applications.

Abstract: Toxic language in Bengali remains prevalent, especially in online
environments, with few effective precautions against it. Although text
detoxification has seen progress in high-resource languages, Bengali remains
underexplored due to limited resources. In this paper, we propose a novel
pipeline for Bengali text detoxification that combines Pareto class-optimized
large language models (LLMs) and Chain-of-Thought (CoT) prompting to generate
detoxified sentences. To support this effort, we construct BanglaNirTox, an
artificially generated parallel corpus of 68,041 toxic Bengali sentences with
class-wise toxicity labels, reasonings, and detoxified paraphrases, using
Pareto-optimized LLMs evaluated on random samples. The resulting BanglaNirTox
dataset is used to fine-tune language models to produce better detoxified
versions of Bengali sentences. Our findings show that Pareto-optimized LLMs
with CoT prompting significantly enhance the quality and consistency of Bengali
text detoxification.

</details>


### [73] [Difficulty-Controllable Cloze Question Distractor Generation](https://arxiv.org/abs/2511.01526)
*Seokhoon Kang,Yejin Jeon,Seonjeong Hwang,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: A novel framework for generating distractors with controllable difficulty in multiple-choice cloze questions, using data augmentation and multitask learning to create difficulty-annotated datasets and train models that outperform GPT-4o.


<details>
  <summary>Details</summary>
Motivation: Existing methods for generating distractors in multiple-choice cloze questions lack adaptability and control over difficulty levels, and the absence of difficulty-annotated datasets hinders progress in this area.

Method: Two-step approach: 1) Create difficulty-annotated dataset through two-way distractor generation, filtering, and categorization using ensemble QA system; 2) Train difficulty-controllable generation model via multitask learning with auxiliary tasks for semantic understanding and difficulty estimation.

Result: The method generates high-quality distractors across difficulty levels and substantially outperforms GPT-4o in aligning distractor difficulty with human perception.

Conclusion: The proposed framework successfully addresses the challenge of generating distractors with controllable difficulty through data augmentation and multitask learning, demonstrating superior performance compared to existing approaches.

Abstract: Multiple-choice cloze questions are commonly used to assess linguistic
proficiency and comprehension. However, generating high-quality distractors
remains challenging, as existing methods often lack adaptability and control
over difficulty levels, and the absence of difficulty-annotated datasets
further hinders progress. To address these issues, we propose a novel framework
for generating distractors with controllable difficulty by leveraging both data
augmentation and a multitask learning strategy. First, to create a
high-quality, difficulty-annotated dataset, we introduce a two-way distractor
generation process in order to produce diverse and plausible distractors. These
candidates are subsequently refined through filtering and then categorized by
difficulty using an ensemble QA system. Second, this newly created dataset is
leveraged to train a difficulty-controllable generation model via multitask
learning. The framework includes carefully designed auxiliary tasks that
enhance the model's semantic understanding of distractors and its ability to
estimate their difficulty. Experimental results demonstrate that our method
generates high-quality distractors across difficulty levels and substantially
outperforms GPT-4o in aligning distractor difficulty with human perception.

</details>


### [74] [Math anxiety and associative knowledge structure are entwined in psychology students but not in Large Language Models like GPT-3.5 and GPT-4o](https://arxiv.org/abs/2511.01558)
*Luciana Ciringione,Emma Franchino,Simone Reigl,Isaia D'Onofrio,Anna Serbati,Oleksandra Poquet,Florence Gabriel,Massimo Stella*

Main category: cs.CL

TL;DR: The study uses behavioral forma mentis networks to analyze math anxiety in psychology students, revealing that positive valence for 'anxiety' and negative ratings for 'math' predict higher math anxiety in humans, but these patterns don't apply to GPT-simulated students due to cognitive differences.


<details>
  <summary>Details</summary>
Motivation: Math anxiety significantly impacts university psychology students' career choices and well-being, requiring better understanding of how students perceive and associate math-related concepts emotionally and cognitively.

Method: Conducted 4 experiments with psychology undergraduates (n=127) and GPT-simulated students (n=600). Used behavioral forma mentis networks to map concept associations and emotional perceptions, analyzing individual network features to predict math anxiety scores and comparing group-level perceptions between humans and AI.

Result: In human students, positive valence for 'anxiety' and negative ratings for 'math' predicted higher math anxiety, particularly evaluative anxiety. GPT models showed different network patterns and psychometric scores. High math-anxiety students framed 'anxiety' in emotionally polarizing ways and contrasted 'science' positively against negative 'math' perceptions.

Conclusion: Concept perception and associations play crucial roles in math anxiety, with significant differences between human and AI cognitive frameworks. Understanding these cognitive-emotional structures is essential for managing students' math anxiety effectively.

Abstract: Math anxiety poses significant challenges for university psychology students,
affecting their career choices and overall well-being. This study employs a
framework based on behavioural forma mentis networks (i.e. cognitive models
that map how individuals structure their associative knowledge and emotional
perceptions of concepts) to explore individual and group differences in the
perception and association of concepts related to math and anxiety. We
conducted 4 experiments involving psychology undergraduates from 2 samples (n1
= 70, n2 = 57) compared against GPT-simulated students (GPT-3.5: n2 = 300;
GPT-4o: n4 = 300). Experiments 1, 2, and 3 employ individual-level network
features to predict psychometric scores for math anxiety and its facets
(observational, social and evaluational) from the Math Anxiety Scale.
Experiment 4 focuses on group-level perceptions extracted from human students,
GPT-3.5 and GPT-4o's networks. Results indicate that, in students, positive
valence ratings and higher network degree for "anxiety", together with negative
ratings for "math", can predict higher total and evaluative math anxiety. In
contrast, these models do not work on GPT-based data because of differences in
simulated networks and psychometric scores compared to humans. These results
were also reconciled with differences found in the ways that high/low subgroups
of simulated and real students framed semantically and emotionally STEM
concepts. High math-anxiety students collectively framed "anxiety" in an
emotionally polarising way, absent in the negative perception of low
math-anxiety students. "Science" was rated positively, but contrasted against
the negative perception of "math". These findings underscore the importance of
understanding concept perception and associations in managing students' math
anxiety.

</details>


### [75] [ECO Decoding: Entropy-Based Control for Controllability and Fluency in Controllable Dialogue Generation](https://arxiv.org/abs/2511.01568)
*Seungmin Shin,Dooyoung Kim,Youngjoong Ko*

Main category: cs.CL

TL;DR: ECO decoding dynamically adjusts control strength in dialogue generation based on entropy from language model and attribute classifier distributions, improving controllability while maintaining fluency.


<details>
  <summary>Details</summary>
Motivation: Fixed constant values for managing attribute probability bias make it difficult to find ideal control strength that satisfies both controllability and fluency in dialogue generation.

Method: Proposed ECO decoding (Entropy-based Control) that dynamically adjusts control strength at each generation step according to model's entropy in both language model and attribute classifier probability distributions.

Result: Experiments on DailyDialog and MultiWOZ datasets show ECO decoding consistently improves controllability while maintaining fluency and grammaticality, outperforming prior decoding methods across various models and settings.

Conclusion: ECO decoding alleviates probability interpolation issues in multi-attribute generation and demonstrates strong performance in both single and multi-attribute scenarios.

Abstract: Controllable Dialogue Generation (CDG) enables chatbots to generate responses
with desired attributes, and weighted decoding methods have achieved
significant success in the CDG task. However, using a fixed constant value to
manage the bias of attribute probabilities makes it challenging to find an
ideal control strength that satisfies both controllability and fluency. To
address this issue, we propose ECO decoding (Entropy-based COntrol), which
dynamically adjusts the control strength at each generation step according to
the model's entropy in both the language model and attribute classifier
probability distributions. Experiments on the DailyDialog and MultiWOZ datasets
demonstrate that ECO decoding consistently improves controllability while
maintaining fluency and grammaticality, outperforming prior decoding methods
across various models and settings. Furthermore, ECO decoding alleviates
probability interpolation issues in multi-attribute generation and consequently
demonstrates strong performance in both single and multi-attribute scenarios.

</details>


### [76] [BIRD: Bronze Inscription Restoration and Dating](https://arxiv.org/abs/2511.01589)
*Wenjie Hua,Hoang H. Nguyen,Gangyan Ge*

Main category: cs.CL

TL;DR: BIRD is a dataset and framework for restoring and dating fragmentary bronze inscriptions from early China using allograph-aware masked language modeling and glyph-biased sampling.


<details>
  <summary>Details</summary>
Motivation: Bronze inscriptions from early China are fragmentary and difficult to date, creating challenges for historical analysis.

Method: Proposed BIRD dataset with scholarly transcriptions and chronological labels, plus an allograph-aware masked language modeling framework with Glyph Net (GN) that links graphemes and allographs.

Result: Experiments show GN improves restoration performance, while glyph-biased sampling yields gains in dating accuracy.

Conclusion: The BIRD framework effectively addresses the challenges of bronze inscription restoration and dating through integrated domain-adaptive pretraining and glyph-aware modeling.

Abstract: Bronze inscriptions from early China are fragmentary and difficult to date.
We introduce BIRD(Bronze Inscription Restoration and Dating), a fully encoded
dataset grounded in standard scholarly transcriptions and chronological labels.
We further propose an allograph-aware masked language modeling framework that
integrates domain- and task-adaptive pretraining with a Glyph Net (GN), which
links graphemes and allographs. Experiments show that GN improves restoration,
while glyph-biased sampling yields gains in dating.

</details>


### [77] [Imperfect Language, Artificial Intelligence, and the Human Mind: An Interdisciplinary Approach to Linguistic Errors in Native Spanish Speakers](https://arxiv.org/abs/2511.01615)
*Francisco Portillo Lpez*

Main category: cs.CL

TL;DR: This paper proposes studying Spanish linguistic errors to understand how large language models interpret and process them, combining linguistics, neurolinguistics, and NLP approaches.


<details>
  <summary>Details</summary>
Motivation: Linguistic errors provide insights into cognitive language architecture and reveal limitations of current AI systems in replicating human language processing.

Method: Uses a corpus of 500+ authentic Spanish errors from native speakers, tested against LLMs (GPT/Gemini) to evaluate interpretative accuracy and pattern generalization capabilities.

Result: The research aims to assess how well current AI models can interpret, reproduce, and correct human linguistic errors in Spanish.

Conclusion: The project contributes to understanding Spanish as a native language and developing more cognitively informed NLP systems that can handle imperfect, variable human language.

Abstract: Linguistic errors are not merely deviations from normative grammar; they
offer a unique window into the cognitive architecture of language and expose
the current limitations of artificial systems that seek to replicate them. This
project proposes an interdisciplinary study of linguistic errors produced by
native Spanish speakers, with the aim of analyzing how current large language
models (LLM) interpret, reproduce, or correct them. The research integrates
three core perspectives: theoretical linguistics, to classify and understand
the nature of the errors; neurolinguistics, to contextualize them within
real-time language processing in the brain; and natural language processing
(NLP), to evaluate their interpretation against linguistic errors. A
purpose-built corpus of authentic errors of native Spanish (+500) will serve as
the foundation for empirical analysis. These errors will be tested against AI
models such as GPT or Gemini to assess their interpretative accuracy and their
ability to generalize patterns of human linguistic behavior. The project
contributes not only to the understanding of Spanish as a native language but
also to the development of NLP systems that are more cognitively informed and
capable of engaging with the imperfect, variable, and often ambiguous nature of
real human language.

</details>


### [78] [ParlaSpeech 3.0: Richly Annotated Spoken Parliamentary Corpora of Croatian, Czech, Polish, and Serbian](https://arxiv.org/abs/2511.01619)
*Nikola Ljubei,Peter Rupnik,Ivan Porupski,Taja Kuzman Pungerek*

Main category: cs.CL

TL;DR: ParlaSpeech is a 6,000-hour multilingual parliamentary speech corpus covering four Slavic languages with automatic annotations including linguistic features, sentiment analysis, disfluency detection, and alignment data.


<details>
  <summary>Details</summary>
Motivation: To create a comprehensive spoken parliamentary corpus for Slavic languages that can support cross-disciplinary research by providing rich automatic annotations beyond basic transcripts.

Method: Automatically built from ParlaMint transcripts and aligned with speech recordings, then enriched with multiple annotation layers including linguistic annotations, sentiment predictions, filled pause detection, word/grapheme alignments, and stress position annotation.

Result: Created a 6,000-hour corpus spanning Croatian, Czech, Polish, and Serbian with enhanced annotations that significantly increase research utility, demonstrated through acoustic sentiment correlation analysis.

Conclusion: The enriched ParlaSpeech corpora provide valuable resources for multiple research disciplines and are made available in multiple formats including JSONL, TextGrid, and through a concordancer for accessibility.

Abstract: ParlaSpeech is a collection of spoken parliamentary corpora currently
spanning four Slavic languages - Croatian, Czech, Polish and Serbian - all
together 6 thousand hours in size. The corpora were built in an automatic
fashion from the ParlaMint transcripts and their corresponding metadata, which
were aligned to the speech recordings of each corresponding parliament. In this
release of the dataset, each of the corpora is significantly enriched with
various automatic annotation layers. The textual modality of all four corpora
has been enriched with linguistic annotations and sentiment predictions.
Similar to that, their spoken modality has been automatically enriched with
occurrences of filled pauses, the most frequent disfluency in typical speech.
Two out of the four languages have been additionally enriched with detailed
word- and grapheme-level alignments, and the automatic annotation of the
position of primary stress in multisyllabic words. With these enrichments, the
usefulness of the underlying corpora has been drastically increased for
downstream research across multiple disciplines, which we showcase through an
analysis of acoustic correlates of sentiment. All the corpora are made
available for download in JSONL and TextGrid formats, as well as for search
through a concordancer.

</details>


### [79] [A Graph-based RAG for Energy Efficiency Question Answering](https://arxiv.org/abs/2511.01643)
*Riccardo Campi,Nicol Oreste Pinciroli Vago,Mathyas Giudici,Pablo Barrachina Rodriguez-Guisado,Marco Brambilla,Piero Fraternali*

Main category: cs.CL

TL;DR: LLMs combined with graph-based RAG architecture for Energy Efficiency QA, achieving 75.2% accuracy with multilingual capabilities.


<details>
  <summary>Details</summary>
Motivation: To provide accurate multilingual answers for Energy Efficiency questions by leveraging structured knowledge from regulatory documents.

Method: Automatically extract Knowledge Graph from energy documents, then navigate and reason over the graph using LLMs within RAG architecture.

Result: 75.2  2.7% accuracy overall, 81.0  4.1% for general EE questions, with only 4.4% accuracy loss in multilingual translation.

Conclusion: Graph-based RAG with LLMs shows strong potential for EE QA, with good accuracy and promising multilingual abilities.

Abstract: In this work, we investigate the use of Large Language Models (LLMs) within a
graph-based Retrieval Augmented Generation (RAG) architecture for Energy
Efficiency (EE) Question Answering. First, the system automatically extracts a
Knowledge Graph (KG) from guidance and regulatory documents in the energy
field. Then, the generated graph is navigated and reasoned upon to provide
users with accurate answers in multiple languages. We implement a human-based
validation using the RAGAs framework properties, a validation dataset
comprising 101 question-answer pairs, and domain experts. Results confirm the
potential of this architecture and identify its strengths and weaknesses.
Validation results show how the system correctly answers in about three out of
four of the cases (75.2 +- 2.7%), with higher results on questions related to
more general EE answers (up to 81.0 +- 4.1%), and featuring promising
multilingual abilities (4.4% accuracy loss due to translation).

</details>


### [80] [Evaluating Cultural Knowledge Processing in Large Language Models: A Cognitive Benchmarking Framework Integrating Retrieval-Augmented Generation](https://arxiv.org/abs/2511.01649)
*Hung-Shin Lee,Chen-Chi Chang,Ching-Yuan Chen,Yun-Hsiang Hsu*

Main category: cs.CL

TL;DR: A cognitive benchmarking framework combining Bloom's Taxonomy with RAG to evaluate LLMs' processing of culturally specific knowledge using Taiwanese Hakka cultural archives.


<details>
  <summary>Details</summary>
Motivation: To assess how large language models process and apply culturally specific knowledge, particularly in the context of cultural preservation and understanding.

Method: Integrates Bloom's Taxonomy with Retrieval-Augmented Generation (RAG) to evaluate LLM performance across six cognitive domains, using a curated Taiwanese Hakka digital cultural archive as test data.

Result: The framework measures LLM-generated responses' semantic accuracy and cultural relevance across hierarchical cognitive levels.

Conclusion: Proposes a systematic approach for benchmarking LLMs' cultural knowledge processing capabilities, with potential applications in cultural preservation and cross-cultural AI development.

Abstract: This study proposes a cognitive benchmarking framework to evaluate how large
language models (LLMs) process and apply culturally specific knowledge. The
framework integrates Bloom's Taxonomy with Retrieval-Augmented Generation (RAG)
to assess model performance across six hierarchical cognitive domains:
Remembering, Understanding, Applying, Analyzing, Evaluating, and Creating.
Using a curated Taiwanese Hakka digital cultural archive as the primary
testbed, the evaluation measures LLM-generated responses' semantic accuracy and
cultural relevance.

</details>


### [81] [EngChain: A Symbolic Benchmark for Verifiable Multi-Step Reasoning in Engineering](https://arxiv.org/abs/2511.01650)
*Ayesha Gull,Muhammad Usman Safder,Rania Elbadry,Preslav Nakov,Zhuohan Xie*

Main category: cs.CL

TL;DR: EngChain is a benchmark for evaluating LLMs' multi-step engineering problem-solving, featuring 90 problems across 3 engineering branches with quantitative verification and qualitative error analysis.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks don't assess the integrative reasoning needed in engineering where scientific principles, quantitative modeling, and practical constraints must converge.

Method: Created EngChain with 90 problems from symbolic templates with randomization; uses two-stage evaluation: quantitative verification of reasoning steps and LLM-As-A-Judge for qualitative error categorization.

Result: The benchmark enables comprehensive assessment of LLMs' engineering reasoning capabilities beyond final answer accuracy.

Conclusion: EngChain fills a critical gap in evaluating LLMs' complex reasoning for specialized engineering domains through verifiable multi-step problem-solving assessment.

Abstract: Large Language Models (LLMs) are increasingly being applied to specialized,
high-stakes domains like engineering, which demands rigorous evaluation of
their complex reasoning capabilities. While current benchmarks assess language
understanding, factual recall, mathematics or code generation, none capture the
integrative reasoning central to engineering where scientific principles,
quantitative modeling and practical constraints must converge. To address this
gap, we introduce EngChain, a benchmark for verifiable multi-step engineering
problem-solving. EngChain contains 90 problems spanning three engineering
branches, organized into 9 domains and 20 distinct areas. The problems are
generated from symbolic templates with a high degree of randomization to ensure
diversity and eliminate the risk of contamination. With this benchmark, we move
beyond final answer accuracy with a two-stage evaluation: we first
quantitatively verify the numerical and semantic validity of each reasoning
step and then introduce LLM-As-A-Judge, an automated system to qualitatively
categorize the identified reasoning errors.

</details>


### [82] [SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia](https://arxiv.org/abs/2511.01670)
*Chaoqun Liu,Mahani Aljunied,Guizhen Chen,Hou Pong Chan,Weiwen Xu,Yu Rong,Wenxuan Zhang*

Main category: cs.CL

TL;DR: SeaLLMs-Audio is the first large audio-language model for Southeast Asian languages (Indonesian, Thai, Vietnamese) plus English and Chinese, supporting multimodal inputs and multiple audio tasks with competitive performance.


<details>
  <summary>Details</summary>
Motivation: To address the lack of large audio-language models specifically designed for Southeast Asian languages and advance audio LLM research in the region.

Method: Trained on a large-scale audio corpus, the model supports 5 languages and accepts audio-only, text-only, or audio+text inputs for various audio analysis and voice interaction tasks.

Result: SeaLLMs-Audio achieves competitive performance compared to other LALMs on Southeast Asian languages, as evaluated by the new SeaBench-Audio benchmark.

Conclusion: The model represents a significant advancement for audio LLMs in Southeast Asia and is expected to benefit both regional research and industry applications.

Abstract: We introduce SeaLLMs-Audio, the first large audio-language model (LALM)
tailored for multiple Southeast Asian (SEA) languages-Indonesian (id), Thai
(th), and Vietnamese (vi)-alongside English (en) and Chinese (zh). Trained on a
large-scale audio corpus, SeaLLMs-Audio exhibits strong performance across
diverse audio-centric tasks, spanning fine-grained audio understanding and
voice-based interaction. Its key features include: 1) Multilingual: the model
primarily supports 5 languages, namely Indonesian, Thai, Vietnamese, English,
and Chinese; 2) Multimodal: the model accepts flexible input modalities,
including audio only, text only, as well as audio with text; 3) Multi-task: the
model supports a wide range of tasks, including audio analysis tasks such as
Audio Captioning, Automatic Speech Recognition, Speech-to-Text Translation,
Speech Emotion Recognition, Speech Question Answering, and Speech
Summarization. It also enables voice-based dialogue, including answering
factual, mathematical, and general knowledge queries. As a significant step
towards advancing audio LLMs in Southeast Asia, we expect SeaLLMs-Audio to
benefit both the regional research community and industry. To automate LALM
evaluation for Southeast Asia, we introduce SeaBench-Audio, a benchmark
spanning multiple tasks. Experiments show that SeaLLMs-Audio achieves
competitive performance compared with other LALMs on SEA languages.

</details>


### [83] [Open Character Training: Shaping the Persona of AI Assistants through Constitutional AI](https://arxiv.org/abs/2511.01689)
*Sharan Maiya,Henning Bartsch,Nathan Lambert,Evan Hubinger*

Main category: cs.CL

TL;DR: The paper introduces the first open implementation of character training for AI assistants, using Constitutional AI and synthetic introspective data to shape persona more effectively than existing methods, with minimal impact on general capabilities.


<details>
  <summary>Details</summary>
Motivation: Character training is critical for shaping AI assistant personas (values, beliefs, ethics) but remains unstudied in academic literature, while being important for interaction quality and alignment with intentions.

Method: Fine-tuned three popular open-weights models using 11 example personas via Constitutional AI and a new data pipeline with synthetic introspective data. Introduced a method to analyze revealed preferences to track character changes.

Result: Character changes were more robust to adversarial prompting than system prompt constraints or activation steering, while producing more coherent and realistic generations. Fine-tuning had little to no effect on general benchmark performance.

Conclusion: The approach provides an effective and controlled method for character training that is more robust than alternatives, with the full method being open-sourced for community use.

Abstract: The character of the "AI assistant" persona generated by modern chatbot large
language models influences both surface-level behavior and apparent values,
beliefs, and ethics. These all affect interaction quality, perceived
intelligence, and alignment with both developer and user intentions. The
shaping of this persona, known as character training, is a critical component
of industry post-training, yet remains effectively unstudied in the academic
literature. We introduce the first open implementation of character training,
leveraging Constitutional AI and a new data pipeline using synthetic
introspective data to shape the assistant persona in a more effective and
controlled manner than alternatives such as constraining system prompts or
activation steering. Specifically, we fine-tune three popular open-weights
models using 11 example personas, such as humorous, deeply caring, or even
malevolent. To track the effects of our approach, we introduce a method which
analyzes revealed preferences, uncovering clear and holistic changes in
character. We find these changes are more robust to adversarial prompting than
the above two alternatives, while also leading to more coherent and realistic
generations. Finally, we demonstrate this fine-tuning has little to no effect
on general capabilities as measured by common benchmarks. We describe and
open-source our full post-training method, the implementation of which can be
found at https://github.com/maiush/OpenCharacterTraining.

</details>


### [84] [Multi-Step Knowledge Interaction Analysis via Rank-2 Subspace Disentanglement](https://arxiv.org/abs/2511.01706)
*Sekh Mainul Islam,Pepa Atanasova,Isabelle Augenstein*

Main category: cs.CL

TL;DR: The paper proposes a rank-2 projection subspace to better disentangle Parametric Knowledge (PK) and Context Knowledge (CK) interactions in LLM-generated Natural Language Explanations, enabling multi-step analysis of knowledge interactions.


<details>
  <summary>Details</summary>
Motivation: Current approaches model PK and CK interaction as a binary choice in rank-1 subspace, overlooking richer interaction forms like complementary or supportive knowledge, and only examine single-step generation rather than longer NLE sequences.

Method: A novel rank-2 projection subspace that more accurately disentangles PK and CK contributions, applied for multi-step analysis of knowledge interactions across longer NLE sequences using four QA datasets and three open-weight instruction-tuned LLMs.

Result: Diverse knowledge interactions are poorly represented in rank-1 subspace but effectively captured in rank-2 formulation. Hallucinated NLEs align strongly with PK, context-faithful ones balance PK and CK, and Chain-of-Thought prompting shifts NLEs toward CK by reducing PK reliance.

Conclusion: This work provides the first framework for systematic studies of multi-step knowledge interactions in LLMs through richer rank-2 subspace disentanglement, enabling better assessment of NLE grounding.

Abstract: Natural Language Explanations (NLEs) describe how Large Language Models
(LLMs) make decisions, drawing on both external Context Knowledge (CK) and
Parametric Knowledge (PK) stored in model weights. Understanding their
interaction is key to assessing the grounding of NLEs, yet it remains
underexplored. Prior work has largely examined only single-step generation,
typically the final answer, and has modelled PK and CK interaction only as a
binary choice in a rank-1 subspace. This overlooks richer forms of interaction,
such as complementary or supportive knowledge. We propose a novel rank-2
projection subspace that disentangles PK and CK contributions more accurately
and use it for the first multi-step analysis of knowledge interactions across
longer NLE sequences. Experiments on four QA datasets and three open-weight
instruction-tuned LLMs show that diverse knowledge interactions are poorly
represented in a rank-1 subspace but are effectively captured in our rank-2
formulation. Our multi-step analysis reveals that hallucinated NLEs align
strongly with the PK direction, context-faithful ones balance PK and CK, and
Chain-of-Thought prompting for NLEs shifts generated NLEs toward CK by reducing
PK reliance. This work provides the first framework for systematic studies of
multi-step knowledge interactions in LLMs through a richer rank-2 subspace
disentanglement. Code and data:
https://github.com/copenlu/pk-ck-knowledge-disentanglement.

</details>


### [85] [Efficient Tool-Calling Multi-Expert NPC Agent for Commonsense Persona-Grounded Dialogue](https://arxiv.org/abs/2511.01720)
*Mahammad Nuriyev*

Main category: cs.CL

TL;DR: A multi-expert system using Qwen3 with LoRA adapters creates NPCs capable of natural dialogue and contextual actions, achieving second place in the Commonsense Persona-Grounded Dialogue Challenge 2025 while maintaining computational efficiency.


<details>
  <summary>Details</summary>
Motivation: To develop NPCs that can engage in natural dialogue and execute contextual actions in interactive environments, addressing the need for computationally efficient yet capable character systems.

Method: Uses Qwen3 as base model with LoRA adapters to create three specialists: tool calling, tool-response interpretation, and direct dialogue, forming a multi-expert system.

Result: System ranked second overall in the Commonsense Persona-Grounded Dialogue Challenge 2025, delivers fast responses with modest resource usage on L40S GPUs.

Conclusion: The multi-expert approach with specialized LoRA adapters effectively creates capable NPCs while maintaining computational efficiency, demonstrating strong performance in dialogue challenges.

Abstract: We present a multi-expert system for creating Non-Player Characters (NPCs)
capable of both natural dialogue and contextual action execution in interactive
environments. Using Qwen3 as the base model and Low-Rank Adaptation (LoRA)
adapters, we instantiate three specialists: tool calling, tool-response
interpretation, and direct dialogue. Our system comfortably meets the
computational efficiency requirements, delivering fast responses and
maintaining modest resource usage on L40S GPUs. In the Commonsense
Persona-Grounded Dialogue Challenge 2025, our method ranked second overall.
  Code available at:
https://github.com/MahammadNuriyev62/CPDC-challenge-2025-solution/

</details>


### [86] [Accumulating Context Changes the Beliefs of Language Models](https://arxiv.org/abs/2511.01805)
*Jiayi Geng,Howard Chen,Ryan Liu,Manoel Horta Ribeiro,Robb Willer,Graham Neubig,Thomas L. Griffiths*

Main category: cs.CL

TL;DR: Language models' beliefs can significantly shift through extended interactions and text processing, making their opinions and behaviors unreliable.


<details>
  <summary>Details</summary>
Motivation: To investigate how accumulating context through interactions and text processing can silently change language models' belief profiles, potentially leading to inconsistent user experiences and deviations from original alignment.

Method: Tested belief shifts through moral dilemma discussions, political text reading, and tool use tasks that correspond to implicit beliefs.

Result: GPT-5 showed 54.7% belief shift after moral dilemma discussions, Grok 4 showed 27.2% shift after reading opposing political texts, and behavioral changes aligned with stated belief shifts.

Conclusion: Belief shifts in language models pose hidden risks as extended sessions of talking or reading can render their opinions and actions unreliable.

Abstract: Language model (LM) assistants are increasingly used in applications such as
brainstorming and research. Improvements in memory and context size have
allowed these models to become more autonomous, which has also resulted in more
text accumulation in their context windows without explicit user intervention.
This comes with a latent risk: the belief profiles of models -- their
understanding of the world as manifested in their responses or actions -- may
silently change as context accumulates. This can lead to subtly inconsistent
user experiences, or shifts in behavior that deviate from the original
alignment of the models. In this paper, we explore how accumulating context by
engaging in interactions and processing text -- talking and reading -- can
change the beliefs of language models, as manifested in their responses and
behaviors.Our results reveal that models' belief profiles are highly malleable:
GPT-5 exhibits a 54.7% shift in its stated beliefs after 10 rounds of
discussion about moral dilemmas and queries about safety, while Grok 4 shows a
27.2% shift on political issues after reading texts from the opposing position.
We also examine models' behavioral changes by designing tasks that require tool
use, where each tool selection corresponds to an implicit belief. We find that
these changes align with stated belief shifts, suggesting that belief shifts
will be reflected in actual behavior in agentic systems. Our analysis exposes
the hidden risk of belief shift as models undergo extended sessions of talking
or reading, rendering their opinions and actions unreliable.

</details>


### [87] [Plan-and-Write: Structure-Guided Length Control for LLMs without Model Retraining](https://arxiv.org/abs/2511.01807)
*Adewale Akinfaderin,Shreyas Subramanian,Akarsha Sehwag*

Main category: cs.CL

TL;DR: A prompt engineering method for precise length control in LLMs without retraining, using structure-guided planning and word counting to improve length adherence in summarization tasks.


<details>
  <summary>Details</summary>
Motivation: Length control is crucial for LLM applications but current approaches require expensive retraining or complex tooling, making them impractical for production environments.

Method: Structure-guided prompt engineering with deliberate planning and word counting mechanisms that encourage models to track and adhere to specified length constraints.

Result: Significantly improved length fidelity across six LLMs, with up to 37.6% improvement in adherence for shorter-to-medium constraints while maintaining or enhancing output quality.

Conclusion: Provides an immediately deployable solution for precise length control in production environments where model retraining is impractical.

Abstract: Length control in Large Language Models (LLMs) is a crucial but
under-addressed challenge, with applications ranging from voice interfaces
requiring concise responses to research summaries needing comprehensive
outputs. Current approaches to length control, including Regularized DPO,
Length-Instruction Fine Tuning, and tool-augmented methods, typically require
expensive model retraining or complex inference-time tooling. This paper
presents a prompt engineering methodology that enables precise length control
without model retraining. Our structure-guided approach implements deliberate
planning and word counting mechanisms within the prompt, encouraging the model
to carefully track and adhere to specified length constraints. Comprehensive
evaluations across six state-of-the-art LLMs demonstrate that our method
significantly improves length fidelity for several models compared to standard
prompting when applied to document summarization tasks, particularly for
shorter-to-medium length constraints. The proposed technique shows varying
benefits across different model architectures, with some models demonstrating
up to 37.6% improvement in length adherence. Quality evaluations further reveal
that our approach maintains or enhances overall output quality compared to
standard prompting techniques. Our approach provides an immediately deployable
solution for applications requiring precise length control, particularly
valuable for production environments where model retraining is impractical or
cost-prohibitive.

</details>


### [88] [KV Cache Transform Coding for Compact Storage in LLM Inference](https://arxiv.org/abs/2511.01815)
*Konrad Staniszewski,Adrian acucki*

Main category: cs.CL

TL;DR: KVTC is a lightweight transform coder that compresses KV caches for efficient LLM serving, achieving up to 20 compression while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: KV caches consume significant GPU memory and need efficient management for scalable LLM serving, especially with shared-prefix prompts in iterative tasks like code editing and chat.

Method: KVTC combines PCA-based feature decorrelation, adaptive quantization, and entropy coding, requiring only brief initial calibration without changing model parameters.

Result: Achieves up to 20 compression while maintaining reasoning and long-context accuracy, and 40 or higher for specific use cases, outperforming token eviction, quantization, and SVD-based methods.

Conclusion: KVTC is a practical building block for memory-efficient LLM serving with reusable KV caches, supporting scalable deployment across various models and benchmarks.

Abstract: Serving large language models (LLMs) at scale necessitates efficient
key-value (KV) cache management. KV caches can be reused across conversation
turns via shared-prefix prompts that are common in iterative code editing and
chat. However, stale caches consume scarce GPU memory, require offloading, or
force recomputation. We present KVTC, a lightweight transform coder that
compresses KV caches for compact on-GPU and off-GPU storage. Drawing on
classical media compression, KVTC combines PCA-based feature decorrelation,
adaptive quantization, and entropy coding. It requires only a brief initial
calibration and leaves model parameters unchanged. By exploiting redundancies
in KV caches, KVTC achieves up to 20$\times$ compression while maintaining
reasoning and long-context accuracy, and 40$\times$ or higher for specific use
cases. We test KVTC with Llama 3, Mistral NeMo, and R1-Qwen 2.5 models across
benchmarks including AIME25, LiveCodeBench, GSM8K, MMLU, Qasper, RULER, and
MATH-500. It consistently outperforms inference-time baselines such as token
eviction, quantization, and SVD-based methods, while achieving higher
compression ratios. These results support KVTC as a practical building block
for memory-efficient LLM serving with reusable KV caches.

</details>


### [89] [Towards Robust Mathematical Reasoning](https://arxiv.org/abs/2511.01846)
*Thang Luong,Dawsen Hwang,Hoang H. Nguyen,Golnaz Ghiasi,Yuri Chervonyi,Insuk Seo,Junsu Kim,Garrett Bingham,Jonathan Lee,Swaroop Mishra,Alex Zhai,Clara Huiyi Hu,Henryk Michalewski,Jimin Kim,Jeonghyun Ahn,Junhwi Bae,Xingyou Song,Trieu H. Trinh,Quoc V. Le,Junehyuk Jung*

Main category: cs.CL

TL;DR: IMO-Bench is a suite of advanced mathematical reasoning benchmarks at IMO level, including IMO-AnswerBench for short answers and IMO-ProofBench for proof-writing, which helped achieve gold-level performance at IMO 2025 with Gemini Deep Think.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations for mathematical reasoning are either too easy or only focus on correct short answers, lacking comprehensive assessment of advanced reasoning capabilities at IMO level.

Method: Developed IMO-Bench with 400 diverse Olympiad problems with verifiable short answers (IMO-AnswerBench) and proof-writing evaluation with grading guidelines (IMO-ProofBench), plus IMO-GradingBench with 1000 human gradings for automatic evaluation.

Result: Gemini Deep Think achieved 80.0% on IMO-AnswerBench and 65.7% on advanced IMO-ProofBench, surpassing best non-Gemini models by 6.9% and 42.4% margins respectively, enabling gold-level performance at IMO 2025.

Conclusion: IMO-Bench provides robust evaluation for advancing mathematical reasoning capabilities and correlates well with human evaluations, released to help community progress in automatic evaluation of long-form answers.

Abstract: Finding the right north-star metrics is highly critical for advancing the
mathematical reasoning capabilities of foundation models, especially given that
existing evaluations are either too easy or only focus on getting correct short
answers. To address these issues, we present IMO-Bench, a suite of advanced
reasoning benchmarks, vetted by a panel of top specialists and that
specifically targets the level of the International Mathematical Olympiad
(IMO), the most prestigious venue for young mathematicians. IMO-AnswerBench
first tests models on 400 diverse Olympiad problems with verifiable short
answers. IMO-Proof Bench is the next-level evaluation for proof-writing
capabilities, which includes both basic and advanced IMO level problems as well
as detailed grading guidelines to facilitate automatic grading. These
benchmarks played a crucial role in our historic achievement of the gold-level
performance at IMO 2025 with Gemini Deep Think (Luong and Lockhart, 2025). Our
model achieved 80.0% on IMO-AnswerBench and 65.7% on the advanced IMO-Proof
Bench, surpassing the best non-Gemini models by large margins of 6.9% and 42.4%
respectively. We also showed that autograders built with Gemini reasoning
correlate well with human evaluations and construct IMO-GradingBench, with 1000
human gradings on proofs, to enable further progress in automatic evaluation of
long-form answers. We hope that IMO-Bench will help the community towards
advancing robust mathematical reasoning and release it at
https://imobench.github.io/.

</details>


### [90] [Tool-to-Agent Retrieval: Bridging Tools and Agents for Scalable LLM Multi-Agent Systems](https://arxiv.org/abs/2511.01854)
*Elias Lumer,Faheem Nizar,Anmol Gulati,Pradeep Honaganahalli Basavaraju,Vamse Kumar Subbiah*

Main category: cs.CL

TL;DR: Tool-to-Agent Retrieval improves multi-agent systems by embedding tools and agents in shared vector space, enabling granular retrieval and achieving 19.4% Recall@5 improvement.


<details>
  <summary>Details</summary>
Motivation: Existing retrieval methods match queries against coarse agent-level descriptions, obscuring fine-grained tool functionality and resulting in suboptimal agent selection.

Method: Unified framework that embeds both tools and their parent agents in shared vector space, connecting them through metadata relationships for granular tool-level or agent-level retrieval.

Result: Achieves consistent improvements of 19.4% in Recall@5 and 17.7% in nDCG@5 over previous state-of-the-art agent retrievers on LiveMCPBench benchmark across eight embedding models.

Conclusion: Tool-to-Agent Retrieval enables granular representation of tools and agents without context dilution, significantly improving retrieval performance in multi-agent systems.

Abstract: Recent advances in LLM Multi-Agent Systems enable scalable orchestration of
sub-agents, each coordinating hundreds or thousands of tools or Model Context
Protocol (MCP) servers. However, existing retrieval methods typically match
queries against coarse agent-level descriptions before routing, which obscures
fine-grained tool functionality and often results in suboptimal agent
selection. We introduce Tool-to-Agent Retrieval, a unified framework that
embeds both tools and their parent agents in a shared vector space and connects
them through metadata relationships. By explicitly representing tool
capabilities and traversing metadata to the agent level, Tool-to-Agent
Retrieval enables granular tool-level or agent-level retrieval, ensuring that
agents and their underlying tools or MCP servers are equally represented
without the context dilution that arises from chunking many tools together.
Evaluating Tool-to-Agent Retrieval across eight embedding models, our approach
achieves consistent improvements of 19.4% in Recall@5 and 17.7% in nDCG@5 over
previous state-of-the-art agent retrievers on the LiveMCPBench benchmark.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [91] [Generative human motion mimicking through feature extraction in denoising diffusion settings](https://arxiv.org/abs/2511.00011)
*Alexander Okupnik,Johannes Schneider,Kyriakos Flouris*

Main category: cs.CV

TL;DR: A novel AI model for creative dance interaction that generates responsive movement sequences by combining motion inpainting and style transfer using single-person motion data.


<details>
  <summary>Details</summary>
Motivation: To complement verbal human-AI interaction with embodied creative expression through dance, addressing the lack of physical interaction in current large language models.

Method: Uses motion capture data with diffusion models, motion inpainting, and motion style transfer to generate temporally coherent movement representations that respond to human movement references without relying on human-human interaction data.

Result: Successfully generates diverse and realistic dance movements that show creative deviations from human partners while maintaining feature distribution convergence with test data.

Conclusion: The model represents initial steps toward creative AI dance partners that can enhance human movement with realistic yet diverse artificial responses.

Abstract: Recent success with large language models has sparked a new wave of verbal
human-AI interaction. While such models support users in a variety of creative
tasks, they lack the embodied nature of human interaction. Dance, as a primal
form of human expression, is predestined to complement this experience. To
explore creative human-AI interaction exemplified by dance, we build an
interactive model based on motion capture (MoCap) data. It generates an
artificial other by partially mimicking and also "creatively" enhancing an
incoming sequence of movement data. It is the first model, which leverages
single-person motion data and high level features in order to do so and, thus,
it does not rely on low level human-human interaction data. It combines ideas
of two diffusion models, motion inpainting, and motion style transfer to
generate movement representations that are both temporally coherent and
responsive to a chosen movement reference. The success of the model is
demonstrated by quantitatively assessing the convergence of the feature
distribution of the generated samples and the test set which serves as
simulating the human performer. We show that our generations are first steps to
creative dancing with AI as they are both diverse showing various deviations
from the human partner while appearing realistic.

</details>


### [92] [Deep Learning Models for Coral Bleaching Classification in Multi-Condition Underwater Image Datasets](https://arxiv.org/abs/2511.00021)
*Julio Jerison E. Macrohon,Gordon Hung*

Main category: cs.CV

TL;DR: A machine learning system for coral bleaching classification using CNN, ResNet, and ViT models, with CNN achieving 88% accuracy on a diverse global dataset.


<details>
  <summary>Details</summary>
Motivation: Coral reefs are vital marine ecosystems facing threats from pollution, ocean acidification, and temperature anomalies, making efficient monitoring urgent.

Method: Developed a coral bleaching classification system using three state-of-the-art models (ResNet, ViT, CNN) on a diverse global dataset with samples from various environments including deep seas, marshes, and coastal zones.

Result: After hyperparameter tuning, the CNN model achieved the highest accuracy of 88%, outperforming existing benchmarks and other tested models.

Conclusion: The study provides important insights for autonomous coral monitoring and presents a comprehensive analysis of widely used computer vision models for coral health assessment.

Abstract: Coral reefs support numerous marine organisms and are an important source of
coastal protection from storms and floods, representing a major part of marine
ecosystems. However coral reefs face increasing threats from pollution, ocean
acidification, and sea temperature anomalies, making efficient protection and
monitoring heavily urgent. Therefore, this study presents a novel
machine-learning-based coral bleaching classification system based on a diverse
global dataset with samples of healthy and bleached corals under varying
environmental conditions, including deep seas, marshes, and coastal zones. We
benchmarked and compared three state-of-the-art models: Residual Neural Network
(ResNet), Vision Transformer (ViT), and Convolutional Neural Network (CNN).
After comprehensive hyperparameter tuning, the CNN model achieved the highest
accuracy of 88%, outperforming existing benchmarks. Our findings offer
important insights into autonomous coral monitoring and present a comprehensive
analysis of the most widely used computer vision models.

</details>


### [93] [Automating Coral Reef Fish Family Identification on Video Transects Using a YOLOv8-Based Deep Learning Pipeline](https://arxiv.org/abs/2511.00022)
*Jules Gerard,Leandro Di Bella,Filip Huyghe,Marc Kochzius*

Main category: cs.CV

TL;DR: YOLOv8-based deep learning pipeline automates family-level fish identification from video transects in Kenya and Tanzania, achieving mAP@0.5 of 0.52.


<details>
  <summary>Details</summary>
Motivation: Coral reef monitoring in the Western Indian Ocean is limited by labor demands of underwater visual censuses, requiring automated solutions.

Method: Evaluated YOLOv8-based deep learning pipeline on curated dataset of 24 fish families from video transects collected in Kenya and Tanzania.

Result: Best model achieved mAP@0.5 of 0.52, with high accuracy for abundant families but weaker detection of rare or complex taxa.

Conclusion: Deep learning shows potential as scalable complement to traditional monitoring methods for reef fish identification.

Abstract: Coral reef monitoring in the Western Indian Ocean is limited by the labor
demands of underwater visual censuses. This work evaluates a YOLOv8-based deep
learning pipeline for automating family-level fish identification from video
transects collected in Kenya and Tanzania. A curated dataset of 24 families was
tested under different configurations, providing the first region-specific
benchmark for automated reef fish monitoring in the Western Indian Ocean. The
best model achieved mAP@0.5 of 0.52, with high accuracy for abundant families
but weaker detection of rare or complex taxa. Results demonstrate the potential
of deep learning as a scalable complement to traditional monitoring methods.

</details>


### [94] [Mutual Information guided Visual Contrastive Learning](https://arxiv.org/abs/2511.00028)
*Hanyang Chen,Yanchao Yang*

Main category: cs.CV

TL;DR: The paper proposes using mutual information from real-world distributions to select training data for contrastive learning, replacing human-engineered data augmentation with data-driven selection based on natural perturbations.


<details>
  <summary>Details</summary>
Motivation: Current contrastive learning methods rely on human-designed data augmentation strategies (like color jittering) which may be suboptimal. The authors aim to use mutual information from real-world distributions to automatically select better training data.

Method: Select patches with high mutual information under natural perturbations (color changes, motion) as positive samples for contrastive learning, replacing traditional human-engineered augmentation.

Result: The method was evaluated on multiple benchmarks and state-of-the-art frameworks, demonstrating effectiveness and improved generalization in open environments.

Conclusion: Mutual-information-informed data augmentation is a promising direction that outperforms traditional human-engineered approaches and enables better feature generalization.

Abstract: Representation learning methods utilizing the InfoNCE loss have demonstrated
considerable capacity in reducing human annotation effort by training invariant
neural feature extractors. Although different variants of the training
objective adhere to the information maximization principle between the data and
learned features, data selection and augmentation still rely on human
hypotheses or engineering, which may be suboptimal. For instance, data
augmentation in contrastive learning primarily focuses on color jittering,
aiming to emulate real-world illumination changes. In this work, we investigate
the potential of selecting training data based on their mutual information
computed from real-world distributions, which, in principle, should endow the
learned features with better generalization when applied in open environments.
Specifically, we consider patches attached to scenes that exhibit high mutual
information under natural perturbations, such as color changes and motion, as
positive samples for learning with contrastive loss. We evaluate the proposed
mutual-information-informed data augmentation method on several benchmarks
across multiple state-of-the-art representation learning frameworks,
demonstrating its effectiveness and establishing it as a promising direction
for future research.

</details>


### [95] [Benchmarking Federated Learning Frameworks for Medical Imaging Deployment: A Comparative Study of NVIDIA FLARE, Flower, and Owkin Substra](https://arxiv.org/abs/2511.00037)
*Riya Gupta,Alexander Chowdhury,Sahil Nalawade*

Main category: cs.CV

TL;DR: This study benchmarks three federated learning frameworks (NVIDIA FLARE, Flower, and Owkin Substra) for medical imaging applications, evaluating their performance, scalability, and suitability for healthcare deployment.


<details>
  <summary>Details</summary>
Motivation: Federated Learning enables collaborative model training across medical institutions without direct data sharing, addressing privacy concerns in healthcare AI applications.

Method: Used PathMNIST dataset to assess model performance, convergence efficiency, communication overhead, scalability, and developer experience across the three FL frameworks.

Result: NVIDIA FLARE offers superior production scalability, Flower provides flexibility for prototyping and academic research, and Owkin Substra demonstrates exceptional privacy and compliance features.

Conclusion: Each FL framework has distinct strengths optimized for different use cases, making them all relevant for practical deployment in healthcare environments depending on specific requirements.

Abstract: Federated Learning (FL) has emerged as a transformative paradigm in medical
AI, enabling collaborative model training across institutions without direct
data sharing. This study benchmarks three prominent FL frameworks NVIDIA FLARE,
Flower, and Owkin Substra to evaluate their suitability for medical imaging
applications in real-world settings. Using the PathMNIST dataset, we assess
model performance, convergence efficiency, communication overhead, scalability,
and developer experience. Results indicate that NVIDIA FLARE offers superior
production scalability, Flower provides flexibility for prototyping and
academic research, and Owkin Substra demonstrates exceptional privacy and
compliance features. Each framework exhibits strengths optimized for distinct
use cases, emphasizing their relevance to practical deployment in healthcare
environments.

</details>


### [96] [Enhancing rice leaf images: An overview of image denoising techniques](https://arxiv.org/abs/2511.00046)
*Rupjyoti Chutia,Dibya Jyoti Bora*

Main category: cs.CV

TL;DR: Comparative study of image denoising methods combined with CLAHE for enhancing rice leaf images to improve disease detection and agricultural analysis.


<details>
  <summary>Details</summary>
Motivation: Image enhancement is crucial for rice leaf analysis in agriculture, particularly for disease detection, nutrient deficiency evaluation, and growth analysis. Denoising and contrast enhancement are essential preprocessing steps to improve image quality for subsequent tasks like segmentation and classification.

Method: Extensive comparative study of well-known image denoising methods combined with Contrast Limited Adaptive Histogram Equalization (CLAHE) for denoising rice leaf images. Experiments were performed on a rice leaf image dataset using various metrics to comprehensively test enhancement methods.

Result: The study provides comprehensive results using various metrics to evaluate the effectiveness of different denoising methods combined with CLAHE for rice leaf image enhancement.

Conclusion: This approach establishes a strong basis for assessing image processing methodologies and provides insights useful for future adaptation in agricultural research and other domains.

Abstract: Digital image processing involves the systematic handling of images using
advanced computer algorithms, and has gained significant attention in both
academic and practical fields. Image enhancement is a crucial preprocessing
stage in the image-processing chain, improving image quality and emphasizing
features. This makes subsequent tasks (segmentation, feature extraction,
classification) more reliable. Image enhancement is essential for rice leaf
analysis, aiding in disease detection, nutrient deficiency evaluation, and
growth analysis. Denoising followed by contrast enhancement are the primary
steps. Image filters, generally employed for denoising, transform or enhance
visual characteristics like brightness, contrast, and sharpness, playing a
crucial role in improving overall image quality and enabling the extraction of
useful information. This work provides an extensive comparative study of
well-known image-denoising methods combined with CLAHE (Contrast Limited
Adaptive Histogram Equalization) for efficient denoising of rice leaf images.
The experiments were performed on a rice leaf image dataset to ensure the data
is relevant and representative. Results were examined using various metrics to
comprehensively test enhancement methods. This approach provides a strong basis
for assessing the effectiveness of methodologies in digital image processing
and reveals insights useful for future adaptation in agricultural research and
other domains.

</details>


### [97] [Which LiDAR scanning pattern is better for roadside perception: Repetitive or Non-repetitive?](https://arxiv.org/abs/2511.00060)
*Zhiqi Qi,Runxin Zhao,Hanyang Zhuang,Chunxiang Wang,Ming Yang*

Main category: cs.CV

TL;DR: This paper investigates how different LiDAR scanning patterns (repetitive vs non-repetitive) affect roadside perception performance, introduces a new benchmark dataset, and finds that non-repetitive LiDAR offers comparable performance to high-end repetitive LiDAR at lower cost.


<details>
  <summary>Details</summary>
Motivation: While LiDAR placement optimization has been studied, the impact of different scanning patterns on perceptual performance remains under-investigated, particularly for infrastructure-based systems in intelligent transportation.

Method: Created the 'InfraLiDARs' Benchmark' dataset in CARLA simulation using infrastructure-based LiDARs with both scanning paradigms, then conducted statistical analysis and evaluated various 3D object detection algorithms.

Result: Non-repetitive scanning LiDAR and 128-line repetitive LiDAR showed comparable detection performance across scenarios. Non-repetitive LiDAR is cost-effective despite limited perception range.

Conclusion: The study provides guidance for optimal LiDAR scanning pattern selection and compatible algorithms for roadside perception systems, and releases the benchmark dataset to support further research.

Abstract: LiDAR-based roadside perception is a cornerstone of advanced Intelligent
Transportation Systems (ITS). While considerable research has addressed optimal
LiDAR placement for infrastructure, the profound impact of differing LiDAR
scanning patterns on perceptual performance remains comparatively
under-investigated. The inherent nature of various scanning modes - such as
traditional repetitive (mechanical/solid-state) versus emerging non-repetitive
(e.g. prism-based) systems - leads to distinct point cloud distributions at
varying distances, critically dictating the efficacy of object detection and
overall environmental understanding. To systematically investigate these
differences in infrastructure-based contexts, we introduce the "InfraLiDARs'
Benchmark," a novel dataset meticulously collected in the CARLA simulation
environment using concurrently operating infrastructure-based LiDARs exhibiting
both scanning paradigms. Leveraging this benchmark, we conduct a comprehensive
statistical analysis of the respective LiDAR scanning abilities and evaluate
the impact of these distinct patterns on the performance of various leading 3D
object detection algorithms. Our findings reveal that non-repetitive scanning
LiDAR and the 128-line repetitive LiDAR were found to exhibit comparable
detection performance across various scenarios. Despite non-repetitive LiDAR's
limited perception range, it's a cost-effective option considering its low
price. Ultimately, this study provides insights for setting up roadside
perception system with optimal LiDAR scanning patterns and compatible
algorithms for diverse roadside applications, and publicly releases the
"InfraLiDARs' Benchmark" dataset to foster further research.

</details>


### [98] [World Simulation with Video Foundation Models for Physical AI](https://arxiv.org/abs/2511.00062)
*NVIDIA,:,Arslan Ali,Junjie Bai,Maciej Bala,Yogesh Balaji,Aaron Blakeman,Tiffany Cai,Jiaxin Cao,Tianshi Cao,Elizabeth Cha,Yu-Wei Chao,Prithvijit Chattopadhyay,Mike Chen,Yongxin Chen,Yu Chen,Shuai Cheng,Yin Cui,Jenna Diamond,Yifan Ding,Jiaojiao Fan,Linxi Fan,Liang Feng,Francesco Ferroni,Sanja Fidler,Xiao Fu,Ruiyuan Gao,Yunhao Ge,Jinwei Gu,Aryaman Gupta,Siddharth Gururani,Imad El Hanafi,Ali Hassani,Zekun Hao,Jacob Huffman,Joel Jang,Pooya Jannaty,Jan Kautz,Grace Lam,Xuan Li,Zhaoshuo Li,Maosheng Liao,Chen-Hsuan Lin,Tsung-Yi Lin,Yen-Chen Lin,Huan Ling,Ming-Yu Liu,Xian Liu,Yifan Lu,Alice Luo,Qianli Ma,Hanzi Mao,Kaichun Mo,Seungjun Nah,Yashraj Narang,Abhijeet Panaskar,Lindsey Pavao,Trung Pham,Morteza Ramezanali,Fitsum Reda,Scott Reed,Xuanchi Ren,Haonan Shao,Yue Shen,Stella Shi,Shuran Song,Bartosz Stefaniak,Shangkun Sun,Shitao Tang,Sameena Tasmeen,Lyne Tchapmi,Wei-Cheng Tseng,Jibin Varghese,Andrew Z. Wang,Hao Wang,Haoxiang Wang,Heng Wang,Ting-Chun Wang,Fangyin Wei,Jiashu Xu,Dinghao Yang,Xiaodong Yang,Haotian Ye,Seonghyeon Ye,Xiaohui Zeng,Jing Zhang,Qinsheng Zhang,Kaiwen Zheng,Andrew Zhu,Yuke Zhu*

Main category: cs.CV

TL;DR: Cosmos-Predict2.5 is a flow-based Physical AI model that unifies Text2World, Image2World, and Video2World generation, achieving substantial improvements in video quality and instruction alignment over previous versions.


<details>
  <summary>Details</summary>
Motivation: To enable more reliable synthetic data generation, policy evaluation, and closed-loop simulation for robotics and autonomous systems by developing advanced Physical AI foundation models.

Method: Built on flow-based architecture, leverages Cosmos-Reason1 for text grounding, trained on 200M curated video clips with reinforcement learning-based post-training, and includes Cosmos-Transfer2.5 for Sim2Real/Real2Real translation.

Result: Achieves substantial improvements over Cosmos-Predict1 in video quality and instruction alignment, with models at 2B and 14B scales. Cosmos-Transfer2.5 delivers higher fidelity and robust long-horizon video generation despite being 3.5x smaller than Cosmos-Transfer1.

Conclusion: These advances establish Cosmos-Predict2.5 and Cosmos-Transfer2.5 as versatile tools for scaling embodied intelligence, with open-source release to accelerate Physical AI research and deployment.

Abstract: We introduce [Cosmos-Predict2.5], the latest generation of the Cosmos World
Foundation Models for Physical AI. Built on a flow-based architecture,
[Cosmos-Predict2.5] unifies Text2World, Image2World, and Video2World generation
in a single model and leverages [Cosmos-Reason1], a Physical AI vision-language
model, to provide richer text grounding and finer control of world simulation.
Trained on 200M curated video clips and refined with reinforcement
learning-based post-training, [Cosmos-Predict2.5] achieves substantial
improvements over [Cosmos-Predict1] in video quality and instruction alignment,
with models released at 2B and 14B scales. These capabilities enable more
reliable synthetic data generation, policy evaluation, and closed-loop
simulation for robotics and autonomous systems. We further extend the family
with [Cosmos-Transfer2.5], a control-net style framework for Sim2Real and
Real2Real world translation. Despite being 3.5$\times$ smaller than
[Cosmos-Transfer1], it delivers higher fidelity and robust long-horizon video
generation. Together, these advances establish [Cosmos-Predict2.5] and
[Cosmos-Transfer2.5] as versatile tools for scaling embodied intelligence. To
accelerate research and deployment in Physical AI, we release source code,
pretrained checkpoints, and curated benchmarks under the NVIDIA Open Model
License at https://github.com/nvidia-cosmos/cosmos-predict2.5 and
https://github.com/nvidia-cosmos/cosmos-transfer2.5. We hope these open
resources lower the barrier to adoption and foster innovation in building the
next generation of embodied intelligence.

</details>


### [99] [Habitat and Land Cover Change Detection in Alpine Protected Areas: A Comparison of AI Architectures](https://arxiv.org/abs/2511.00073)
*Harald Kristen,Daniel Kulmer,Manuela Hirschmugl*

Main category: cs.CV

TL;DR: Deep learning methods for change detection in complex alpine habitats using geospatial foundation models (GFMs) and multimodal data, comparing post-classification vs direct approaches.


<details>
  <summary>Details</summary>
Motivation: Rapid climate change demands frequent habitat monitoring in alpine ecosystems, but manual mapping is too expensive for required temporal resolution.

Method: Compared two paradigms: post-classification change detection using GFMs (Prithvi-EO-2.0, Clay v1.0) vs U-Net CNNs, and direct change detection using ChangeViT vs U-Net baselines. Used high-resolution multimodal data (RGB, NIR, LiDAR, terrain attributes) covering 4,480 changes over 15.3 km.

Result: Clay v1.0 achieved 51% overall accuracy vs U-Net's 41% for multi-class habitat change, both reached 67% for binary detection. Direct CD yielded superior IoU (0.53 vs 0.35) for binary but only 28% accuracy for multi-class. LiDAR integration improved semantic segmentation from 30% to 50% accuracy.

Conclusion: GFMs show robustness in cross-temporal evaluation, maintaining better performance than U-Net. Although accuracies are lower than in homogeneous landscapes, they reflect realistic performance for complex alpine habitats with fuzzy boundaries and class imbalance.

Abstract: Rapid climate change and other disturbances in alpine ecosystems demand
frequent habitat monitoring, yet manual mapping remains prohibitively expensive
for the required temporal resolution. We employ deep learning for change
detection using long-term alpine habitat data from Gesaeuse National Park,
Austria, addressing a major gap in applying geospatial foundation models (GFMs)
to complex natural environments with fuzzy class boundaries and highly
imbalanced classes. We compare two paradigms: post-classification change
detection (CD) versus direct CD. For post-classification CD, we evaluate GFMs
Prithvi-EO-2.0 and Clay v1.0 against U-Net CNNs; for direct CD, we test the
transformer ChangeViT against U-Net baselines. Using high-resolution multimodal
data (RGB, NIR, LiDAR, terrain attributes) covering 4,480 documented changes
over 15.3 km2, results show Clay v1.0 achieves 51% overall accuracy versus
U-Net's 41% for multi-class habitat change, while both reach 67% for binary
change detection. Direct CD yields superior IoU (0.53 vs 0.35) for binary but
only 28% accuracy for multi-class detection. Cross-temporal evaluation reveals
GFM robustness, with Clay maintaining 33% accuracy on 2020 data versus U-Net's
23%. Integrating LiDAR improves semantic segmentation from 30% to 50% accuracy.
Although overall accuracies are lower than in more homogeneous landscapes, they
reflect realistic performance for complex alpine habitats. Future work will
integrate object-based post-processing and physical constraints to enhance
applicability.

</details>


### [100] [LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based Video Generation](https://arxiv.org/abs/2511.00090)
*Huanlin Gao,Ping Chen,Fuyuan Shi,Chao Tan,Zhaoxiang Liu,Fang Zhao,Kai Wang,Shiguo Lian*

Main category: cs.CV

TL;DR: LeMiCa is a training-free acceleration framework for diffusion-based video generation that uses lexicographic minimax path optimization to bound global error accumulation, achieving 2.9x speedup with minimal quality degradation.


<details>
  <summary>Details</summary>
Motivation: Existing caching strategies for video generation focus on reducing local errors but overlook global error accumulation, leading to content degradation between accelerated and original videos.

Method: Formulates cache scheduling as a directed graph with error-weighted edges and introduces Lexicographic Minimax Path Optimization to explicitly bound worst-case path error.

Result: Achieves 2.9x speedup on Latte model and LPIPS score of 0.05 on Open-Sora, outperforming prior caching techniques with minimal perceptual quality degradation.

Conclusion: LeMiCa provides a robust and generalizable paradigm for accelerating diffusion-based video generation, serving as a strong foundation for future efficient video synthesis research.

Abstract: We present LeMiCa, a training-free and efficient acceleration framework for
diffusion-based video generation. While existing caching strategies primarily
focus on reducing local heuristic errors, they often overlook the accumulation
of global errors, leading to noticeable content degradation between accelerated
and original videos. To address this issue, we formulate cache scheduling as a
directed graph with error-weighted edges and introduce a Lexicographic Minimax
Path Optimization strategy that explicitly bounds the worst-case path error.
This approach substantially improves the consistency of global content and
style across generated frames. Extensive experiments on multiple text-to-video
benchmarks demonstrate that LeMiCa delivers dual improvements in both inference
speed and generation quality. Notably, our method achieves a 2.9x speedup on
the Latte model and reaches an LPIPS score of 0.05 on Open-Sora, outperforming
prior caching techniques. Importantly, these gains come with minimal perceptual
quality degradation, making LeMiCa a robust and generalizable paradigm for
accelerating diffusion-based video generation. We believe this approach can
serve as a strong foundation for future research on efficient and reliable
video synthesis. Our code is available at :https://github.com/UnicomAI/LeMiCa

</details>


### [101] [Self-Improving Vision-Language-Action Models with Data Generation via Residual RL](https://arxiv.org/abs/2511.00091)
*Wenli Xiao,Haotian Lin,Andy Peng,Haoru Xue,Tairan He,Yuqi Xie,Fengyuan Hu,Jimmy Wu,Zhengyi Luo,Linxi "Jim" Fan,Guanya Shi,Yuke Zhu*

Main category: cs.CV

TL;DR: PLD is a three-stage framework that improves vision-language-action models through residual RL and distribution-aware data collection, achieving near-perfect task success rates without relying on costly human demonstrations.


<details>
  <summary>Details</summary>
Motivation: Supervised fine-tuning (SFT) relies on expensive human demonstrations, limiting scalability and generalization of large vision-language-action models.

Method: Three-stage approach: 1) Train lightweight residual actors to probe failure regions, 2) Use hybrid rollout scheme for distribution-aligned trajectory collection with recovery behaviors, 3) Distill curated trajectories back into the generalist model using standard SFT.

Result: Achieved 99% task success on LIBERO, over 50% gains in SimplerEnv, and 100% success on real-world Franka and YAM arm manipulation tasks.

Conclusion: PLD offers a scalable path toward self-improving VLA models through residual probing and distribution-aware replay, improving both seen and unseen tasks.

Abstract: Supervised fine-tuning (SFT) has become the de facto post-training strategy
for large vision-language-action (VLA) models, but its reliance on costly human
demonstrations limits scalability and generalization. We propose Probe, Learn,
Distill (PLD), a three-stage plug-and-play framework that improves VLAs through
residual reinforcement learning (RL) and distribution-aware data collection. In
Stage 1, we train lightweight residual actors to probe failure regions of the
VLA generalist. In Stage 2, we use a hybrid rollout scheme that aligns
collected trajectories with the generalist's deployment distribution while
capturing recovery behaviors. In Stage 3, we distill the curated trajectories
back into the generalist with standard SFT. PLD achieves near-saturated 99%
task success on LIBERO, over 50% gains in SimplerEnv, and 100% success on
real-world Franka and YAM arm manipulation tasks. Ablations show that residual
probing and distribution-aware replay are key to collecting deployment-aligned
data that improves both seen and unseen tasks, offering a scalable path toward
self-improving VLA models.

</details>


### [102] [SpinalSAM-R1: A Vision-Language Multimodal Interactive System for Spine CT Segmentation](https://arxiv.org/abs/2511.00095)
*Jiaming Liu,Dingwei Fan,Junyong Zhao,Chunlin Li,Haipeng Si,Liang Sun*

Main category: cs.CV

TL;DR: SpinalSAM-R1 is a multimodal vision-language system that combines fine-tuned SAM with DeepSeek-R1 for spine CT image segmentation, featuring anatomy-guided attention and natural language interaction.


<details>
  <summary>Details</summary>
Motivation: Spine CT segmentation faces challenges from low contrast and complex vertebral boundaries, while existing models like SAM have limited performance due to high annotation requirements and poor domain adaptability in spinal imaging.

Method: Integrates fine-tuned SAM with DeepSeek-R1 using LoRA adaptation, introduces anatomy-guided attention mechanism, and implements semantics-driven interaction protocol for natural language-guided refinement.

Result: Achieves superior segmentation performance on spine anatomical structures, supports 11 clinical operations with 94.3% parsing accuracy and sub-800ms response times.

Conclusion: SpinalSAM-R1 effectively addresses spine CT segmentation challenges through multimodal integration and interactive refinement, with released software demonstrating practical clinical utility.

Abstract: The anatomical structure segmentation of the spine and adjacent structures
from computed tomography (CT) images is a key step for spinal disease diagnosis
and treatment. However, the segmentation of CT images is impeded by low
contrast and complex vertebral boundaries. Although advanced models such as the
Segment Anything Model (SAM) have shown promise in various segmentation tasks,
their performance in spinal CT imaging is limited by high annotation
requirements and poor domain adaptability. To address these limitations, we
propose SpinalSAM-R1, a multimodal vision-language interactive system that
integrates a fine-tuned SAM with DeepSeek-R1, for spine CT image segmentation.
Specifically, our SpinalSAM-R1 introduces an anatomy-guided attention mechanism
to improve spine segmentation performance, and a semantics-driven interaction
protocol powered by DeepSeek-R1, enabling natural language-guided refinement.
The SpinalSAM-R1 is fine-tuned using Low-Rank Adaptation (LoRA) for efficient
adaptation. We validate our SpinalSAM-R1 on the spine anatomical structure with
CT images. Experimental results suggest that our method achieves superior
segmentation performance. Meanwhile, we develop a PyQt5-based interactive
software, which supports point, box, and text-based prompts. The system
supports 11 clinical operations with 94.3\% parsing accuracy and sub-800 ms
response times. The software is released on
https://github.com/6jm233333/spinalsam-r1.

</details>


### [103] [A filtering scheme for confocal laser endomicroscopy (CLE)-video sequences for self-supervised learning](https://arxiv.org/abs/2511.00098)
*Nils Porsche,Flurin Mller-Diesing,Sweta Banerjee,Miguel Goncalves,Marc Aubreville*

Main category: cs.CV

TL;DR: Proposes a filter to reduce redundancy in CLE video sequences for self-supervised learning, improving training efficiency and accuracy for tumor classification tasks.


<details>
  <summary>Details</summary>
Motivation: CLE images are hard to interpret for non-experts, and machine learning models overfit due to limited histopathology-correlated data. SSL can help but faces challenges from high inter-frame correlation in CLE videos.

Method: Developed a filter functionality for CLE video sequences to reduce dataset redundancy in SSL training. Used four baseline networks and a SSL teacher-student network with vision transformer backbone, evaluated on sinonasal tumor and skin squamous cell carcinoma datasets.

Result: Filtered SSL-pretrained models achieved highest test accuracy: 67.48% on sinonasal tumor dataset and 73.52% on skin carcinoma dataset, significantly outperforming non-SSL baselines. Training time reduced by 67%.

Conclusion: SSL is effective for CLE pretraining, and the proposed video filter improves training efficiency and performance in self-supervised scenarios.

Abstract: Confocal laser endomicroscopy (CLE) is a non-invasive, real-time imaging
modality that can be used for in-situ, in-vivo imaging and the microstructural
analysis of mucous structures. The diagnosis using CLE is, however, complicated
by images being hard to interpret for non-experienced physicians. Utilizing
machine learning as an augmentative tool would hence be beneficial, but is
complicated by the shortage of histopathology-correlated CLE imaging sequences
with respect to the plurality of patterns in this domain, leading to
overfitting of machine learning models. To overcome this, self-supervised
learning (SSL) can be employed on larger unlabeled datasets. CLE is a
video-based modality with high inter-frame correlation, leading to a
non-stratified data distribution for SSL training. In this work, we propose a
filter functionality on CLE video sequences to reduce the dataset redundancy in
SSL training and improve SSL training convergence and training efficiency. We
use four state-of-the-art baseline networks and a SSL teacher-student network
with a vision transformer small backbone for the evaluation. These networks
were evaluated on downstream tasks for a sinonasal tumor dataset and a squamous
cell carcinoma of the skin dataset. On both datasets, we found the highest test
accuracy on the filtered SSL-pretrained model, with 67.48% and 73.52%, both
considerably outperforming their non-SSL baselines. Our results show that SSL
is an effective method for CLE pretraining. Further, we show that our proposed
CLE video filter can be utilized to improve training efficiency in
self-supervised scenarios, resulting in a reduction of 67% in training time.

</details>


### [104] [FreeSliders: Training-Free, Modality-Agnostic Concept Sliders for Fine-Grained Diffusion Control in Images, Audio, and Video](https://arxiv.org/abs/2511.00103)
*Rotem Ezra,Hedi Zisling,Nimrod Berman,Ilan Naiman,Alexey Gorkor,Liran Nochumsohn,Eliya Nachmani,Omri Azencot*

Main category: cs.CV

TL;DR: FreeSliders is a training-free, modality-agnostic method for fine-grained controllable generation in diffusion models by partially estimating Concept Sliders during inference, with extensions to video/audio benchmarks and automatic scale selection.


<details>
  <summary>Details</summary>
Motivation: Existing Concept Sliders require per-concept training and architecture-specific fine-tuning, limiting scalability to new modalities. There's a need for training-free, modality-agnostic approaches for fine-grained concept control.

Method: Partially estimate the Concept Sliders formula during inference without training. Introduce automatic scale selection and non-linear traversals via a two-stage procedure that detects saturation points and reparameterizes traversal for perceptually uniform edits.

Result: Enables plug-and-play, training-free concept control across modalities (images, video, audio), outperforms existing baselines, and establishes new evaluation tools for principled controllable generation.

Conclusion: FreeSliders provides an effective training-free solution for fine-grained concept control in diffusion models across multiple modalities, addressing scalability limitations of previous methods while improving performance.

Abstract: Diffusion models have become state-of-the-art generative models for images,
audio, and video, yet enabling fine-grained controllable generation, i.e.,
continuously steering specific concepts without disturbing unrelated content,
remains challenging. Concept Sliders (CS) offer a promising direction by
discovering semantic directions through textual contrasts, but they require
per-concept training and architecture-specific fine-tuning (e.g., LoRA),
limiting scalability to new modalities. In this work we introduce FreeSliders,
a simple yet effective approach that is fully training-free and
modality-agnostic, achieved by partially estimating the CS formula during
inference. To support modality-agnostic evaluation, we extend the CS benchmark
to include both video and audio, establishing the first suite for fine-grained
concept generation control with multiple modalities. We further propose three
evaluation properties along with new metrics to improve evaluation quality.
Finally, we identify an open problem of scale selection and non-linear
traversals and introduce a two-stage procedure that automatically detects
saturation points and reparameterizes traversal for perceptually uniform,
semantically meaningful edits. Extensive experiments demonstrate that our
method enables plug-and-play, training-free concept control across modalities,
improves over existing baselines, and establishes new tools for principled
controllable generation. An interactive presentation of our benchmark and
method is available at: https://azencot-group.github.io/FreeSliders/

</details>


### [105] [AI Powered High Quality Text to Video Generation with Enhanced Temporal Consistency](https://arxiv.org/abs/2511.00107)
*Piyushkumar Patel*

Main category: cs.CV

TL;DR: MOVAI is a novel hierarchical framework for text-to-video generation that integrates compositional scene understanding with temporal-aware diffusion models to improve temporal consistency and fine-grained control.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-video generation approaches struggle with maintaining temporal consistency, compositional understanding, and fine-grained control over visual narratives.

Method: Three key innovations: Compositional Scene Parser (CSP) for hierarchical scene graphs with temporal annotations, Temporal-Spatial Attention Mechanism (TSAM) for coherent motion dynamics, and Progressive Video Refinement (PVR) for multi-scale temporal reasoning.

Result: Achieves state-of-the-art performance with 15.3% improvement in LPIPS, 12.7% in FVD, and 18.9% in user preference studies. Excels at generating complex multi-object scenes with realistic temporal dynamics.

Conclusion: MOVAI framework demonstrates superior performance in text-to-video generation, particularly for complex scenes with fine-grained semantic control and realistic temporal dynamics.

Abstract: Text to video generation has emerged as a critical frontier in generative
artificial intelligence, yet existing approaches struggle with maintaining
temporal consistency, compositional understanding, and fine grained control
over visual narratives. We present MOVAI (Multimodal Original Video AI), a
novel hierarchical framework that integrates compositional scene understanding
with temporal aware diffusion models for high fidelity text to video synthesis.
Our approach introduces three key innovations: (1) a Compositional Scene Parser
(CSP) that decomposes textual descriptions into hierarchical scene graphs with
temporal annotations, (2) a Temporal-Spatial Attention Mechanism (TSAM) that
ensures coherent motion dynamics across frames while preserving spatial
details, and (3) a Progressive Video Refinement (PVR) module that iteratively
enhances video quality through multi-scale temporal reasoning. Extensive
experiments on standard benchmarks demonstrate that MOVAI achieves
state-of-the-art performance, improving video quality metrics by 15.3% in
LPIPS, 12.7% in FVD, and 18.9% in user preference studies compared to existing
methods. Our framework shows particular strength in generating complex
multi-object scenes with realistic temporal dynamics and fine-grained semantic
control.

</details>


### [106] [Chain of Time: In-Context Physical Simulation with Image Generation Models](https://arxiv.org/abs/2511.00110)
*YingQiao Wang,Eric Bigelow,Boyi Li,Tomer Ullman*

Main category: cs.CV

TL;DR: Chain of Time method improves physical simulation in vision-language models by generating intermediate images during simulation, inspired by human mental simulation and in-context reasoning.


<details>
  <summary>Details</summary>
Motivation: To improve and interpret physical simulation in vision-language models, drawing inspiration from human mental simulation and in-context reasoning in machine learning.

Method: Chain of Time method generates a series of intermediate images during simulation at inference time without requiring additional fine-tuning, applied to 2-D graphics and 3-D video domains.

Result: Substantially improves performance of state-of-the-art image generation model and reveals insights into physical dynamics like velocity, gravity, and collisions that are hidden from traditional evaluations.

Conclusion: Chain of Time enables better physical simulation and provides interpretability, though models still struggle to infer some physical parameters from input images despite being capable of simulating relevant processes.

Abstract: We propose a novel cognitively-inspired method to improve and interpret
physical simulation in vision-language models. Our ``Chain of Time" method
involves generating a series of intermediate images during a simulation, and it
is motivated by in-context reasoning in machine learning, as well as mental
simulation in humans. Chain of Time is used at inference time, and requires no
additional fine-tuning. We apply the Chain-of-Time method to synthetic and
real-world domains, including 2-D graphics simulations and natural 3-D videos.
These domains test a variety of particular physical properties, including
velocity, acceleration, fluid dynamics, and conservation of momentum. We found
that using Chain-of-Time simulation substantially improves the performance of a
state-of-the-art image generation model. Beyond examining performance, we also
analyzed the specific states of the world simulated by an image model at each
time step, which sheds light on the dynamics underlying these simulations. This
analysis reveals insights that are hidden from traditional evaluations of
physical reasoning, including cases where an image generation model is able to
simulate physical properties that unfold over time, such as velocity, gravity,
and collisions. Our analysis also highlights particular cases where the image
generation model struggles to infer particular physical parameters from input
images, despite being capable of simulating relevant physical processes.

</details>


### [107] [End-to-End Framework Integrating Generative AI and Deep Reinforcement Learning for Autonomous Ultrasound Scanning](https://arxiv.org/abs/2511.00114)
*Hanae Elmekki,Amanda Spilkin,Ehsan Zakeri,Antonela Mariel Zanuttini,Ahmed Alagha,Hani Sami,Jamal Bentahar,Lyes Kadem,Wen-Fang Xie,Philippe Pibarot,Rabeb Mizouni,Hadi Otrok,Azzam Mourad,Sami Muhaidat*

Main category: cs.CV

TL;DR: This paper presents an end-to-end AI framework combining generative AI and deep reinforcement learning for autonomous cardiac ultrasound scanning, addressing limitations of operator dependence and accessibility.


<details>
  <summary>Details</summary>
Motivation: Cardiac ultrasound effectiveness is limited by operator dependence, time constraints, human error, and shortage of trained professionals, especially in remote areas, creating need for automated solutions.

Method: Two-component framework: (1) conditional generative simulator using GANs and VAEs to model cardiac US environment and produce realistic action-conditioned images, (2) DRL module that learns autonomous scanning policies using the simulator.

Result: Framework delivers AI-driven guidance through expert-validated classification models, supports conditional generation of realistic US images, and establishes reproducible foundation extendable to other organs. Public dataset released for reproducibility.

Conclusion: The solution is validated through experiments benchmarking VAE-GAN against existing variants and evaluating DRL-based scanning system under varying configurations, demonstrating effectiveness of the proposed framework.

Abstract: Cardiac ultrasound (US) is among the most widely used diagnostic tools in
cardiology for assessing heart health, but its effectiveness is limited by
operator dependence, time constraints, and human error. The shortage of trained
professionals, especially in remote areas, further restricts access. These
issues underscore the need for automated solutions that can ensure consistent,
and accessible cardiac imaging regardless of operator skill or location. Recent
progress in artificial intelligence (AI), especially in deep reinforcement
learning (DRL), has gained attention for enabling autonomous decision-making.
However, existing DRL-based approaches to cardiac US scanning lack
reproducibility, rely on proprietary data, and use simplified models. Motivated
by these gaps, we present the first end-to-end framework that integrates
generative AI and DRL to enable autonomous and reproducible cardiac US
scanning. The framework comprises two components: (i) a conditional generative
simulator combining Generative Adversarial Networks (GANs) with Variational
Autoencoders (VAEs), that models the cardiac US environment producing realistic
action-conditioned images; and (ii) a DRL module that leverages this simulator
to learn autonomous, accurate scanning policies. The proposed framework
delivers AI-driven guidance through expert-validated models that classify image
type and assess quality, supports conditional generation of realistic US
images, and establishes a reproducible foundation extendable to other organs.
To ensure reproducibility, a publicly available dataset of real cardiac US
scans is released. The solution is validated through several experiments. The
VAE-GAN is benchmarked against existing GAN variants, with performance assessed
using qualitative and quantitative approaches, while the DRL-based scanning
system is evaluated under varying configurations to demonstrate effectiveness.

</details>


### [108] [VLM6D: VLM based 6Dof Pose Estimation based on RGB-D Images](https://arxiv.org/abs/2511.00120)
*Md Selim Sarowar,Sungho Kim*

Main category: cs.CV

TL;DR: VLM6D introduces a dual-stream architecture using Vision Transformer and PointNet++ encoders for RGB-D 6D object pose estimation, achieving state-of-the-art performance on Occluded-LineMOD with superior robustness to lighting, texture, and occlusion.


<details>
  <summary>Details</summary>
Motivation: Current 6D object pose estimation methods struggle with generalization from synthetic to real-world data, particularly with lighting variations, textureless objects, and severe occlusions.

Method: Dual-stream architecture with self-supervised Vision Transformer (DINOv2) for RGB processing and PointNet++ for 3D point cloud processing from depth data, with complementary feature fusion for multi-task prediction.

Result: Achieved new state-of-the-art performance on the challenging Occluded-LineMOD benchmark, demonstrating superior robustness and accuracy.

Conclusion: VLM6D effectively addresses key limitations in 6D pose estimation by leveraging complementary visual and geometric data streams for robust performance in challenging real-world conditions.

Abstract: The primary challenge in computer vision is precisely calculating the pose of
6D objects, however many current approaches are still fragile and have trouble
generalizing from synthetic data to real-world situations with fluctuating
lighting, textureless objects, and significant occlusions. To address these
limitations, VLM6D, a novel dual-stream architecture that leverages the
distinct strengths of visual and geometric data from RGB-D input for robust and
precise pose estimation. Our framework uniquely integrates two specialized
encoders: a powerful, self-supervised Vision Transformer (DINOv2) processes the
RGB modality, harnessing its rich, pre-trained understanding of visual grammar
to achieve remarkable resilience against texture and lighting variations.
Concurrently, a PointNet++ encoder processes the 3D point cloud derived from
depth data, enabling robust geometric reasoning that excels even with the
sparse, fragmented data typical of severe occlusion. These complementary
feature streams are effectively fused to inform a multi task prediction head.
We demonstrate through comprehensive experiments that VLM6D obtained new SOTA
performance on the challenging Occluded-LineMOD, validating its superior
robustness and accuracy.

</details>


### [109] [Integrating ConvNeXt and Vision Transformers for Enhancing Facial Age Estimation](https://arxiv.org/abs/2511.00123)
*Gaby Maroun,Salah Eddine Bekhouche,Fadi Dornaika*

Main category: cs.CV

TL;DR: A hybrid ConvNeXt-ViT architecture combining CNN's local feature extraction with Transformer's global attention achieves superior age estimation performance on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Age estimation from facial images is complex, and existing methods can benefit from combining the complementary strengths of CNNs and Transformers.

Method: Hybrid architecture integrating ConvNeXt (CNN) with Vision Transformers (ViT), using pre-trained models, linear layers, and advanced regularization techniques with adapted attention mechanisms.

Result: Achieved superior performance in mean absolute error (MAE) on MORPH II, CACD, and AFAD datasets, outperforming traditional methods.

Conclusion: Hybrid architectures combining CNNs and Transformers show transformative potential for age estimation and complex computer vision tasks.

Abstract: Age estimation from facial images is a complex and multifaceted challenge in
computer vision. In this study, we present a novel hybrid architecture that
combines ConvNeXt, a state-of-the-art advancement of convolutional neural
networks (CNNs), with Vision Transformers (ViT). While each model independently
delivers excellent performance on a variety of tasks, their integration
leverages the complementary strengths of the CNNs localized feature extraction
capabilities and the Transformers global attention mechanisms. Our proposed
ConvNeXt-ViT hybrid solution was thoroughly evaluated on benchmark age
estimation datasets, including MORPH II, CACD, and AFAD, and achieved superior
performance in terms of mean absolute error (MAE). To address computational
constraints, we leverage pre-trained models and systematically explore
different configurations, using linear layers and advanced regularization
techniques to optimize the architecture. Comprehensive ablation studies
highlight the critical role of individual components and training strategies,
and in particular emphasize the importance of adapted attention mechanisms
within the CNN framework to improve the model focus on age-relevant facial
features. The results show that the ConvNeXt-ViT hybrid not only outperforms
traditional methods, but also provides a robust foundation for future advances
in age estimation and related visual tasks. This work underscores the
transformative potential of hybrid architectures and represents a promising
direction for the seamless integration of CNNs and transformers to address
complex computer vision challenges.

</details>


### [110] [FLoC: Facility Location-Based Efficient Visual Token Compression for Long Video Understanding](https://arxiv.org/abs/2511.00141)
*Janghoon Cho,Jungsoo Lee,Munawar Hayat,Kyuwoong Hwang,Fatih Porikli,Sungha Choi*

Main category: cs.CV

TL;DR: FLoC is an efficient visual token compression framework that uses facility location function to select a compact, representative subset of visual tokens for long video understanding, reducing computational load while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Current video-LMMs face scalability issues due to overwhelming visual tokens from extended video sequences, limiting their practical application in long video understanding tasks.

Method: Proposes FLoC framework based on facility location function with lazy greedy algorithm to efficiently select a compact, diverse subset of visual tokens within a predefined budget, making it training-free, model-agnostic, and query-agnostic.

Result: Extensive evaluations on Video-MME, MLVU, and LongVideoBench show FLoC consistently outperforms recent compression techniques while achieving significant efficiency gains in processing speed.

Conclusion: FLoC provides an effective, robust, and efficient solution for visual token compression in long video understanding, seamlessly integrating with diverse video-LLMs and existing workflows.

Abstract: Recent studies in long video understanding have harnessed the advanced
visual-language reasoning capabilities of Large Multimodal Models (LMMs),
driving the evolution of video-LMMs specialized for processing extended video
sequences. However, the scalability of these models is severely limited by the
overwhelming volume of visual tokens generated from extended video sequences.
To address this challenge, this paper proposes FLoC, an efficient visual token
compression framework based on the facility location function, a principled
approach that swiftly selects a compact yet highly representative and diverse
subset of visual tokens within a predefined budget on the number of visual
tokens. By integrating the lazy greedy algorithm, our method achieves
remarkable efficiency gains by swiftly selecting a compact subset of tokens,
drastically reducing the number of visual tokens while guaranteeing
near-optimal performance. Notably, our approach is training-free,
model-agnostic, and query-agnostic, providing a versatile solution that
seamlessly integrates with diverse video-LLMs and existing workflows. Extensive
evaluations on large-scale benchmarks, such as Video-MME, MLVU, and
LongVideoBench, demonstrate that our framework consistently surpasses recent
compression techniques, highlighting not only its effectiveness and robustness
in addressing the critical challenges of long video understanding, but also its
efficiency in processing speed.

</details>


### [111] [BlurGuard: A Simple Approach for Robustifying Image Protection Against AI-Powered Editing](https://arxiv.org/abs/2511.00143)
*Jinsu Kim,Yunhun Nam,Minseon Kim,Sangpil Kim,Jongheon Jeong*

Main category: cs.CV

TL;DR: The paper proposes BlurGuard, a method to enhance adversarial noise robustness for image protection against text-to-image model editing by applying adaptive Gaussian blur to make the noise irreversible.


<details>
  <summary>Details</summary>
Motivation: Address concerns about malicious use of text-to-image models by improving the robustness of protective adversarial noise, which prior methods were easily reversed by simple techniques like JPEG compression.

Method: Applies adaptive per-region Gaussian blur on adversarial noise to adjust the frequency spectrum, making the noise difficult to detect and reverse while maintaining imperceptibility.

Result: Consistently improves worst-case protection performance against various reversal techniques across diverse image editing scenarios, while reducing quality degradation in perceptual metrics.

Conclusion: The proposed BlurGuard method effectively enhances the robustness of image protection by making adversarial noise irreversible, providing practical defense against text-to-image model misuse.

Abstract: Recent advances in text-to-image models have increased the exposure of
powerful image editing techniques as a tool, raising concerns about their
potential for malicious use. An emerging line of research to address such
threats focuses on implanting "protective" adversarial noise into images before
their public release, so future attempts to edit them using text-to-image
models can be impeded. However, subsequent works have shown that these
adversarial noises are often easily "reversed," e.g., with techniques as simple
as JPEG compression, casting doubt on the practicality of the approach. In this
paper, we argue that adversarial noise for image protection should not only be
imperceptible, as has been a primary focus of prior work, but also
irreversible, viz., it should be difficult to detect as noise provided that the
original image is hidden. We propose a surprisingly simple method to enhance
the robustness of image protection methods against noise reversal techniques.
Specifically, it applies an adaptive per-region Gaussian blur on the noise to
adjust the overall frequency spectrum. Through extensive experiments, we show
that our method consistently improves the per-sample worst-case protection
performance of existing methods against a wide range of reversal techniques on
diverse image editing scenarios, while also reducing quality degradation due to
noise in terms of perceptual metrics. Code is available at
https://github.com/jsu-kim/BlurGuard.

</details>


### [112] [CompAgent: An Agentic Framework for Visual Compliance Verification](https://arxiv.org/abs/2511.00171)
*Rahul Ghosh,Baishali Chaudhury,Hari Prasanna Das,Meghana Ashok,Ryan Razkenari,Sungmin Hong,Chun-Hao Liu*

Main category: cs.CV

TL;DR: CompAgent is an agentic framework that augments MLLMs with visual tools for visual compliance verification, outperforming specialized classifiers and direct MLLM prompting.


<details>
  <summary>Details</summary>
Motivation: Visual compliance verification is critical but underexplored, with existing methods being costly and limited in generalizability. MLLMs struggle with fine-grained visual details and structured compliance rules.

Method: CompAgent augments MLLMs with visual tools (object detectors, face analyzers, NSFW detectors, captioning models) and uses a planning agent to dynamically select tools based on compliance policy, with a verification agent for multi-modal reasoning.

Result: CompAgent achieves up to 76% F1 score and 10% improvement over state-of-the-art on UnsafeBench dataset, outperforming specialized classifiers, direct MLLM prompting, and curated routing baselines.

Conclusion: Agentic planning and tool-augmented reasoning enable scalable, accurate, and adaptable visual compliance verification.

Abstract: Visual compliance verification is a critical yet underexplored problem in
computer vision, especially in domains such as media, entertainment, and
advertising where content must adhere to complex and evolving policy rules.
Existing methods often rely on task-specific deep learning models trained on
manually labeled datasets, which are costly to build and limited in
generalizability. While recent multi-modal large language models (MLLMs) offer
broad real-world knowledge and policy understanding, they struggle to reason
over fine-grained visual details and apply structured compliance rules
effectively on their own. In this paper, we propose CompAgent, the first
agentic framework for visual compliance verification. CompAgent augments MLLMs
with a suite of visual tools - such as object detectors, face analyzers, NSFW
detectors, and captioning models - and introduces a planning agent that
dynamically selects appropriate tools based on the compliance policy. A
verification agent then integrates image, tool outputs, and policy context to
perform multi-modal reasoning. Experiments on public benchmarks show that
CompAgent outperforms specialized classifiers, direct MLLM prompting, and
curated routing baselines, achieving up to 76% F1 score and a 10% improvement
over the state-of-the-art on the UnsafeBench dataset. Our results demonstrate
the effectiveness of agentic planning and tool-augmented reasoning for
scalable, accurate, and adaptable visual compliance verification.

</details>


### [113] [From Evidence to Verdict: An Agent-Based Forensic Framework for AI-Generated Image Detection](https://arxiv.org/abs/2511.00181)
*Mengfei Liang,Yiting Qu,Yukun Jiang,Michael Backes,Yang Zhang*

Main category: cs.CV

TL;DR: AIFo is a training-free multi-agent framework for AI-generated image detection that mimics human forensic investigation through collaborative agents using various forensic tools and structured debate mechanisms.


<details>
  <summary>Details</summary>
Motivation: Address limitations of existing AI-generated image detection methods: traditional classifiers lack interpretability and generalization, while vision-language models are constrained to single-shot analysis and pixel-level reasoning.

Method: Uses multi-agent collaboration with forensic tools (reverse image search, metadata extraction, classifiers, VLM analysis) coordinated by LLM-based agents, plus structured multi-agent debate and memory-augmented reasoning from historical cases.

Result: Achieves 97.05% accuracy on 6,000 images across lab and real-world scenarios, substantially outperforming traditional classifiers and state-of-the-art VLMs.

Conclusion: Agent-based procedural reasoning offers a new paradigm for more robust, interpretable, and adaptable AI-generated image detection.

Abstract: The rapid evolution of AI-generated images poses unprecedented challenges to
information integrity and media authenticity. Existing detection approaches
suffer from fundamental limitations: traditional classifiers lack
interpretability and fail to generalize across evolving generative models,
while vision-language models (VLMs), despite their promise, remain constrained
to single-shot analysis and pixel-level reasoning. To address these challenges,
we introduce AIFo (Agent-based Image Forensics), a novel training-free
framework that emulates human forensic investigation through multi-agent
collaboration. Unlike conventional methods, our framework employs a set of
forensic tools, including reverse image search, metadata extraction,
pre-trained classifiers, and VLM analysis, coordinated by specialized LLM-based
agents that collect, synthesize, and reason over cross-source evidence. When
evidence is conflicting or insufficient, a structured multi-agent debate
mechanism allows agents to exchange arguments and reach a reliable conclusion.
Furthermore, we enhance the framework with a memory-augmented reasoning module
that learns from historical cases to improve future detection accuracy. Our
comprehensive evaluation spans 6,000 images across both controlled laboratory
settings and challenging real-world scenarios, including images from modern
generative platforms and diverse online sources. AIFo achieves 97.05% accuracy,
substantially outperforming traditional classifiers and state-of-the-art VLMs.
These results demonstrate that agent-based procedural reasoning offers a new
paradigm for more robust, interpretable, and adaptable AI-generated image
detection.

</details>


### [114] [A Retrospect to Multi-prompt Learning across Vision and Language](https://arxiv.org/abs/2511.00191)
*Ziliang Chen,Xin Huang,Quanlong Guan,Liang Lin,Weiqi Luo*

Main category: cs.CV

TL;DR: This paper proposes Energy-based Multi-prompt Learning (EMPL) for vision-language models, which generates multiple prompt embeddings from an energy-based distribution to achieve better balance between in-domain and out-of-domain generalization.


<details>
  <summary>Details</summary>
Motivation: Existing research focuses on single-prompt paradigms, overlooking the potential of multi-prompt learning for vision-language models. The paper aims to explore the technical advantages of using multiple prompts.

Method: EMPL generates multiple prompt embeddings by drawing instances from an energy-based distribution implicitly defined by VLMs. This approach is parameter-efficient and theoretically grounded.

Result: Comprehensive experiments demonstrate the superiority of vision-language transfer with multi-prompt augmentation, showing improved performance and generalization capabilities.

Conclusion: EMPL provides a principled approach to multi-prompt learning that achieves better balance between in-domain and out-of-domain open-vocabulary generalization while maintaining parameter efficiency.

Abstract: The vision community is undergoing the unprecedented progress with the
emergence of Vision-Language Pretraining Models (VLMs). Prompt learning plays
as the holy grail of accessing VLMs since it enables their fast adaptation to
downstream tasks with limited resources. Whereas existing researches milling
around single-prompt paradigms, rarely investigate the technical potential
behind their multi-prompt learning counterparts. This paper aims to provide a
principled retrospect for vision-language multi-prompt learning. We extend the
recent constant modality gap phenomenon to learnable prompts and then, justify
the superiority of vision-language transfer with multi-prompt augmentation,
empirically and theoretically. In terms of this observation, we propose an
Energy-based Multi-prompt Learning (EMPL) to generate multiple prompt
embeddings by drawing instances from an energy-based distribution, which is
implicitly defined by VLMs. So our EMPL is not only parameter-efficient but
also rigorously lead to the balance between in-domain and out-of-domain
open-vocabulary generalization. Comprehensive experiments have been conducted
to justify our claims and the excellence of EMPL.

</details>


### [115] [An Efficient and Generalizable Transfer Learning Method for Weather Condition Detection on Ground Terminals](https://arxiv.org/abs/2511.00211)
*Wenxuan Zhang,Peng Hu*

Main category: cs.CV

TL;DR: The paper proposes an efficient transfer learning method for detecting weather-related conditions on satellite ground components, showing superior performance over typical deep learning methods.


<details>
  <summary>Details</summary>
Motivation: Weather events significantly impact satellite Internet reliability, particularly affecting ground terminal components. Current solutions lack fine-grained detection capability for weather conditions that can assist in fault diagnostics and mitigation.

Method: An efficient transfer learning (TL) method that enables ground components to locally detect representative weather-related conditions such as snow, wet, and other adverse weather conditions.

Result: The proposed TL method shows superior performance compared to typical deep learning methods (YOLOv7, YOLOv9, Faster R-CNN, and R-YOLO) and demonstrates generalizability across various scenarios.

Conclusion: The transfer learning approach provides an effective solution for fine-grained weather condition detection on satellite ground components, addressing reliability challenges in satellite Internet deployments.

Abstract: The increasing adoption of satellite Internet with low-Earth-orbit (LEO)
satellites in mega-constellations allows ubiquitous connectivity to rural and
remote areas. However, weather events have a significant impact on the
performance and reliability of satellite Internet. Adverse weather events such
as snow and rain can disturb the performance and operations of satellite
Internet's essential ground terminal components, such as satellite antennas,
significantly disrupting the space-ground link conditions between LEO
satellites and ground stations. This challenge calls for not only region-based
weather forecasts but also fine-grained detection capability on ground terminal
components of fine-grained weather conditions. Such a capability can assist in
fault diagnostics and mitigation for reliable satellite Internet, but its
solutions are lacking, not to mention the effectiveness and generalization that
are essential in real-world deployments. This paper discusses an efficient
transfer learning (TL) method that can enable a ground component to locally
detect representative weather-related conditions. The proposed method can
detect snow, wet, and other conditions resulting from adverse and typical
weather events and shows superior performance compared to the typical deep
learning methods, such as YOLOv7, YOLOv9, Faster R-CNN, and R-YOLO. Our TL
method also shows the advantage of being generalizable to various scenarios.

</details>


### [116] [DM-QPMNET: Dual-modality fusion network for cell segmentation in quantitative phase microscopy](https://arxiv.org/abs/2511.00218)
*Rajatsubhra Chakraborty,Ana Espinosa-Momox,Riley Haskin,Depeng Xu,Rosario Porras-Aguilar*

Main category: cs.CV

TL;DR: DM-QPMNet is a dual-encoder network for cell segmentation in ssQPM that treats polarized intensity images and phase maps as distinct modalities, using multi-head attention for content-aware feature fusion.


<details>
  <summary>Details</summary>
Motivation: Traditional thresholding methods are sensitive to noise and cell density, while deep learning approaches using simple channel concatenation fail to exploit the complementary nature of polarized intensity images and phase maps in ssQPM.

Method: Dual-encoder network with separate encoding streams for each modality, fusing modality-specific features at intermediate depth via multi-head attention. Uses dual-source skip connections and per-modality normalization for stable training.

Result: Substantial improvements over monolithic concatenation and single-modality baselines, demonstrating effective exploitation of ssQPM's simultaneous capture of complementary illumination and phase cues.

Conclusion: Modality-specific encoding with learnable fusion effectively exploits ssQPM's complementary illumination and phase cues for robust cell segmentation, outperforming traditional approaches.

Abstract: Cell segmentation in single-shot quantitative phase microscopy (ssQPM) faces
challenges from traditional thresholding methods that are sensitive to noise
and cell density, while deep learning approaches using simple channel
concatenation fail to exploit the complementary nature of polarized intensity
images and phase maps. We introduce DM-QPMNet, a dual-encoder network that
treats these as distinct modalities with separate encoding streams. Our
architecture fuses modality-specific features at intermediate depth via
multi-head attention, enabling polarized edge and texture representations to
selectively integrate complementary phase information. This content-aware
fusion preserves training stability while adding principled multi-modal
integration through dual-source skip connections and per-modality normalization
at minimal overhead. Our approach demonstrates substantial improvements over
monolithic concatenation and single-modality baselines, showing that
modality-specific encoding with learnable fusion effectively exploits ssQPM's
simultaneous capture of complementary illumination and phase cues for robust
cell segmentation.

</details>


### [117] [Towards 1000-fold Electron Microscopy Image Compression for Connectomics via VQ-VAE with Transformer Prior](https://arxiv.org/abs/2511.00231)
*Fuming Yang,Yicong Li,Hanspeter Pfister,Jeff W. Lichtman,Yaron Meirovitch*

Main category: cs.CV

TL;DR: A VQ-VAE compression framework for electron microscopy data that enables 16x to 1024x compression with pay-as-you-decode functionality and ROI-driven selective reconstruction.


<details>
  <summary>Details</summary>
Motivation: Petascale electron microscopy datasets are pushing storage, transfer, and analysis capabilities to their limits, requiring efficient compression solutions.

Method: Uses vector-quantized variational autoencoder (VQ-VAE) with optional Transformer prior for texture restoration via FiLM and concatenation, plus ROI-driven workflow for selective high-resolution reconstruction.

Result: Achieves compression ratios from 16x to 1024x while maintaining flexibility in decoding - top-only for extreme compression or full reconstruction with texture restoration.

Conclusion: The framework provides scalable compression for EM data with flexible decoding options and selective reconstruction capabilities to address petascale data challenges.

Abstract: Petascale electron microscopy (EM) datasets push storage, transfer, and
downstream analysis toward their current limits. We present a vector-quantized
variational autoencoder-based (VQ-VAE) compression framework for EM that spans
16x to 1024x and enables pay-as-you-decode usage: top-only decoding for extreme
compression, with an optional Transformer prior that predicts bottom tokens
(without changing the compression ratio) to restore texture via feature-wise
linear modulation (FiLM) and concatenation; we further introduce an ROI-driven
workflow that performs selective high-resolution reconstruction from
1024x-compressed latents only where needed.

</details>


### [118] [Hyperbolic Optimal Transport](https://arxiv.org/abs/2511.00244)
*Yan Bin Ng,Xianfeng Gu*

Main category: cs.CV

TL;DR: This paper proposes a novel algorithm for computing optimal transport maps in hyperbolic space using geometric variational techniques, extending existing Euclidean and spherical methods to handle hierarchical data and multi-genus surfaces.


<details>
  <summary>Details</summary>
Motivation: Optimal transport has diverse applications but existing methods are primarily developed for Euclidean spaces and spheres. There is a need for hyperbolic space OT methods to handle hierarchical data, networks, and multi-genus Riemann surfaces.

Method: The authors propose a geometric variational technique that extends methods for Euclidean and spherical geometry to the hyperbolic setting, creating an efficient algorithm for computing optimal transport maps in hyperbolic space.

Result: The proposed method was validated through experiments on synthetic data and multi-genus surface models, demonstrating its efficacy in hyperbolic space optimal transport computations.

Conclusion: The paper successfully develops and validates a novel algorithm for computing optimal transport maps in hyperbolic space, addressing an important gap in existing OT methods and enabling applications involving hierarchical and network-structured data.

Abstract: The optimal transport (OT) problem aims to find the most efficient mapping
between two probability distributions under a given cost function, and has
diverse applications in many fields such as machine learning, computer vision
and computer graphics. However, existing methods for computing optimal
transport maps are primarily developed for Euclidean spaces and the sphere. In
this paper, we explore the problem of computing the optimal transport map in
hyperbolic space, which naturally arises in contexts involving hierarchical
data, networks, and multi-genus Riemann surfaces. We propose a novel and
efficient algorithm for computing the optimal transport map in hyperbolic space
using a geometric variational technique by extending methods for Euclidean and
spherical geometry to the hyperbolic setting. We also perform experiments on
synthetic data and multi-genus surface models to validate the efficacy of the
proposed method.

</details>


### [119] [Object-Aware 4D Human Motion Generation](https://arxiv.org/abs/2511.00248)
*Shurui Gui,Deep Anil Patel,Xiner Li,Martin Renqiang Min*

Main category: cs.CV

TL;DR: A zero-shot framework for generating physically plausible 4D human motions by combining 3D Gaussian representations with motion diffusion priors and LLM semantic guidance, without requiring retraining on limited datasets.


<details>
  <summary>Details</summary>
Motivation: Current video diffusion models produce unrealistic deformations and physical inconsistencies due to lack of 3D physical priors, limiting their ability to generate physically plausible human-object interactions.

Method: Proposes MSDI framework using Motion Diffusion Score Distillation Sampling (MSDS) to distill score gradients from pre-trained motion diffusion models, combined with LLM semantic guidance for spatial-aware motion optimization that respects object constraints.

Result: The method generates natural and physically plausible human motions that respect 3D spatial context, demonstrating generalization to out-of-distribution object-aware human motions without retraining.

Conclusion: Provides a scalable zero-shot solution for realistic 4D generation by effectively integrating 3D physical priors with motion diffusion and semantic constraints.

Abstract: Recent advances in video diffusion models have enabled the generation of
high-quality videos. However, these videos still suffer from unrealistic
deformations, semantic violations, and physical inconsistencies that are
largely rooted in the absence of 3D physical priors. To address these
challenges, we propose an object-aware 4D human motion generation framework
grounded in 3D Gaussian representations and motion diffusion priors. With
pre-generated 3D humans and objects, our method, Motion Score Distilled
Interaction (MSDI), employs the spatial and prompt semantic information in
large language models (LLMs) and motion priors through the proposed Motion
Diffusion Score Distillation Sampling (MSDS). The combination of MSDS and LLMs
enables our spatial-aware motion optimization, which distills score gradients
from pre-trained motion diffusion models, to refine human motion while
respecting object and semantic constraints. Unlike prior methods requiring
joint training on limited interaction datasets, our zero-shot approach avoids
retraining and generalizes to out-of-distribution object aware human motions.
Experiments demonstrate that our framework produces natural and physically
plausible human motions that respect 3D spatial context, offering a scalable
solution for realistic 4D generation.

</details>


### [120] [Merlin L48 Spectrogram Dataset](https://arxiv.org/abs/2511.00252)
*Aaron Sun,Subhransu Maji,Grant Van Horn*

Main category: cs.CV

TL;DR: The paper introduces L48, a real-world fine-grained multi-label dataset for single-positive multi-label learning, showing that existing methods perform significantly worse on this realistic benchmark compared to synthetic datasets.


<details>
  <summary>Details</summary>
Motivation: Existing SPML methods are benchmarked on synthetic datasets created by randomly sampling single positive labels from fully-annotated datasets, which doesn't reflect real-world scenarios and fails to capture fine-grained complexities that lead to difficult misclassifications.

Method: The authors introduce the L48 dataset - a fine-grained, real-world multi-label dataset derived from recordings of bird sounds, providing natural SPML setting with single-positive annotations and two extended settings with domain priors for additional negative labels.

Result: Benchmarking existing SPML methods on L48 reveals significant performance differences compared to synthetic datasets, highlighting method weaknesses and the need for more realistic benchmarks.

Conclusion: The L48 dataset provides a challenging, realistic benchmark for SPML learning that better captures real-world complexities, revealing limitations of existing methods that were only tested on synthetic data.

Abstract: In the single-positive multi-label (SPML) setting, each image in a dataset is
labeled with the presence of a single class, while the true presence of other
classes remains unknown. The challenge is to narrow the performance gap between
this partially-labeled setting and fully-supervised learning, which often
requires a significant annotation budget. Prior SPML methods were developed and
benchmarked on synthetic datasets created by randomly sampling single positive
labels from fully-annotated datasets like Pascal VOC, COCO, NUS-WIDE, and
CUB200. However, this synthetic approach does not reflect real-world scenarios
and fails to capture the fine-grained complexities that can lead to difficult
misclassifications. In this work, we introduce the L48 dataset, a fine-grained,
real-world multi-label dataset derived from recordings of bird sounds. L48
provides a natural SPML setting with single-positive annotations on a
challenging, fine-grained domain, as well as two extended settings in which
domain priors give access to additional negative labels. We benchmark existing
SPML methods on L48 and observe significant performance differences compared to
synthetic datasets and analyze method weaknesses, underscoring the need for
more realistic and difficult benchmarks.

</details>


### [121] [BeetleFlow: An Integrative Deep Learning Pipeline for Beetle Image Processing](https://arxiv.org/abs/2511.00255)
*Fangxun Liu,S M Rayeed,Samuel Stevens,Alyson East,Cheng Hsuan Chiang,Colin Lee,Daniel Yi,Junke Yang,Tejas Naik,Ziyi Wang,Connor Kilrain,Elijah H Buckwalter,Jiacheng Hou,Saul Ibaven Bueno,Shuheng Wang,Xinyue Ma,Yifan Liu,Zhiyuan Tao,Ziheng Zhang,Eric Sokol,Michael Belitz,Sydne Record,Charles V. Stewart,Wei-Lun Chao*

Main category: cs.CV

TL;DR: A 3-stage pipeline for automated beetle detection, cropping, and morphological segmentation from tray images using transformer-based models.


<details>
  <summary>Details</summary>
Motivation: Biologists need to process large-scale beetle data from tray images efficiently for entomology and ecology research.

Method: 3-stage pipeline: detection using transformer-based open-vocabulary detector and vision-language model, sorting/cropping individual beetles, and segmentation using fine-tuned transformer models on 670 labeled images.

Result: Developed a specialized pipeline that achieves fine-grained beetle segmentation with relatively high accuracy.

Conclusion: The integrated deep learning pipeline significantly improves efficiency in processing large-scale beetle data and accelerates biological research.

Abstract: In entomology and ecology research, biologists often need to collect a large
number of insects, among which beetles are the most common species. A common
practice for biologists to organize beetles is to place them on trays and take
a picture of each tray. Given the images of thousands of such trays, it is
important to have an automated pipeline to process the large-scale data for
further research. Therefore, we develop a 3-stage pipeline to detect all the
beetles on each tray, sort and crop the image of each beetle, and do
morphological segmentation on the cropped beetles. For detection, we design an
iterative process utilizing a transformer-based open-vocabulary object detector
and a vision-language model. For segmentation, we manually labeled 670 beetle
images and fine-tuned two variants of a transformer-based segmentation model to
achieve fine-grained segmentation of beetles with relatively high accuracy. The
pipeline integrates multiple deep learning methods and is specialized for
beetle image processing, which can greatly improve the efficiency to process
large-scale beetle data and accelerate biological research.

</details>


### [122] [MambaNetLK: Enhancing Colonoscopy Point Cloud Registration with Mamba](https://arxiv.org/abs/2511.00260)
*Linzhe Jiang,Jiayuan Huang,Sophia Bano,Matthew J. Clarkson,Zhehua Mao,Mobarak I. Hoque*

Main category: cs.CV

TL;DR: MambaNetLK is a novel correspondence-free 3D registration framework that integrates Mamba State Space Model with PointNetLK architecture, achieving superior performance on clinical colonoscopy data with 56.04% reduction in rotation error and 26.19% reduction in translation error compared to state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Accurate 3D point cloud registration is critical for reliable image-guided colonoscopy, but biological tissues exhibit repetitive textures and homogeneous geometry causing feature degeneracy, while domain shifts between pre-operative and intra-operative data degrade alignment stability.

Method: Proposes MambaNetLK framework that enhances PointNetLK by integrating Mamba State Space Model as cross-modal feature extractor to capture long-range dependencies with linear-time complexity, using Lucas-Kanade algorithm for iterative alignment. Also introduces C3VD-Raycasting-10k clinical dataset with 10,014 geometrically aligned point cloud pairs.

Result: Achieves best performance on C3VD-Raycasting-10k dataset, reducing median rotation error by 56.04% and RMSE translation error by 26.19% over second-best method. Demonstrates strong generalization on ModelNet40 and superior robustness to initial pose perturbations.

Conclusion: MambaNetLK provides robust foundation for 3D registration in surgical navigation, enabling more accurate and reliable guidance systems in minimally invasive procedures like colonoscopy through globally expressive SSM-based feature extraction and large-scale clinical dataset.

Abstract: Accurate 3D point cloud registration underpins reliable image-guided
colonoscopy, directly affecting lesion localization, margin assessment, and
navigation safety. However, biological tissue exhibits repetitive textures and
locally homogeneous geometry that cause feature degeneracy, while substantial
domain shifts between pre-operative anatomy and intra-operative observations
further degrade alignment stability. To address these clinically critical
challenges, we introduce a novel 3D registration method tailored for endoscopic
navigation and a high-quality, clinically grounded dataset to support rigorous
and reproducible benchmarking. We introduce C3VD-Raycasting-10k, a large-scale
benchmark dataset with 10,014 geometrically aligned point cloud pairs derived
from clinical CT data. We propose MambaNetLK, a novel correspondence-free
registration framework, which enhances the PointNetLK architecture by
integrating a Mamba State Space Model (SSM) as a cross-modal feature extractor.
As a result, the proposed framework efficiently captures long-range
dependencies with linear-time complexity. The alignment is achieved iteratively
using the Lucas-Kanade algorithm. On the clinical dataset, C3VD-Raycasting-10k,
MambaNetLK achieves the best performance compared with the state-of-the-art
methods, reducing median rotation error by 56.04% and RMSE translation error by
26.19% over the second-best method. The model also demonstrates strong
generalization on ModelNet40 and superior robustness to initial pose
perturbations. MambaNetLK provides a robust foundation for 3D registration in
surgical navigation. The combination of a globally expressive SSM-based feature
extractor and a large-scale clinical dataset enables more accurate and reliable
guidance systems in minimally invasive procedures like colonoscopy.

</details>


### [123] [Spot The Ball: A Benchmark for Visual Social Inference](https://arxiv.org/abs/2511.00261)
*Neha Balamurugan,Sarah Wu,Adam Chun,Gabe Gaw,Cristobal Eyzaguirre,Tobias Gerstenberg*

Main category: cs.CV

TL;DR: The paper introduces Spot The Ball, a benchmark for evaluating visual social inference in VLMs using sports images where the ball is removed, revealing that humans significantly outperform models by leveraging social cues like gaze and body pose.


<details>
  <summary>Details</summary>
Motivation: To address the gap in visual social inference capabilities between humans and AI agents, particularly in understanding subtle behavioral cues like gaze and pose that drive everyday social reasoning.

Method: Created a benchmark using soccer, basketball, and volleyball images with removed balls, evaluated four state-of-the-art VLMs with three prompting strategies, and compared against human baselines.

Result: Humans consistently achieved 20-34% accuracy, while models reached 17% accuracy, showing models rely on superficial spatial heuristics rather than social cues.

Conclusion: There is a persistent human-model gap in visual social reasoning, highlighting the need for architectures that explicitly encode structured behavioral cues for robust, human-like inference.

Abstract: Humans excel at visual social inference, the ability to infer hidden elements
of a scene from subtle behavioral cues such as other people's gaze, pose, and
orientation. This ability drives everyday social reasoning in humans and is
critical for developing more human-like AI agents. We introduce Spot The Ball,
a challenging benchmark for evaluating visual social inference in
vision-language models (VLMs) using sports as a test domain. The task is to
localize a removed sports ball from soccer, basketball, and volleyball images.
We present a curated evaluation set with human baselines and a scalable
pipeline for generating additional test items. We evaluate four
state-of-the-art VLMs (Gemini, GPT, LLaMA, Qwen) using three prompting
strategies, finding that humans are consistently two to three times more
accurate (20-34%) than models ($\leq$ 17%) across all sports. Our analyses show
that models rely on superficial spatial heuristics--such as guessing near the
image center or nearby players--while humans leverage social cues like gaze
direction and body pose. These findings reveal a persistent human-model gap in
visual social reasoning and underscore the need for architectures that
explicitly encode structured behavioral cues to achieve robust, human-like
inference.

</details>


### [124] [FedReplay: A Feature Replay Assisted Federated Transfer Learning Framework for Efficient and Privacy-Preserving Smart Agriculture](https://arxiv.org/abs/2511.00269)
*Long Li,Jiajia Li,Dong Chen,Lina Pu,Haibo Yao,Yanbo Huang*

Main category: cs.CV

TL;DR: A federated learning framework using frozen CLIP ViT and lightweight transformer classifier for agricultural classification, achieving 86.6% accuracy with reduced communication costs and improved privacy.


<details>
  <summary>Details</summary>
Motivation: Address privacy concerns in centralized training and overcome non-IID data challenges in standard federated learning for agricultural applications like crop monitoring and pest detection.

Method: Integrates frozen CLIP vision transformer with lightweight transformer classifier, shares 1% of CLIP-extracted feature representations across clients to align class representations while preserving privacy.

Result: Achieves 86.6% accuracy, which is more than 4 times higher compared to baseline federated learning approaches.

Conclusion: Demonstrates effectiveness of combining vision-language model features with federated learning for privacy-preserving and scalable agricultural intelligence.

Abstract: Accurate classification plays a pivotal role in smart agriculture, enabling
applications such as crop monitoring, fruit recognition, and pest detection.
However, conventional centralized training often requires large-scale data
collection, which raises privacy concerns, while standard federated learning
struggles with non-independent and identically distributed (non-IID) data and
incurs high communication costs. To address these challenges, we propose a
federated learning framework that integrates a frozen Contrastive
Language-Image Pre-training (CLIP) vision transformer (ViT) with a lightweight
transformer classifier. By leveraging the strong feature extraction capability
of the pre-trained CLIP ViT, the framework avoids training large-scale models
from scratch and restricts federated updates to a compact classifier, thereby
reducing transmission overhead significantly. Furthermore, to mitigate
performance degradation caused by non-IID data distribution, a small subset
(1%) of CLIP-extracted feature representations from all classes is shared
across clients. These shared features are non-reversible to raw images,
ensuring privacy preservation while aligning class representation across
participants. Experimental results on agricultural classification tasks show
that the proposed method achieve 86.6% accuracy, which is more than 4 times
higher compared to baseline federated learning approaches. This demonstrates
the effectiveness and efficiency of combining vision-language model features
with federated learning for privacy-preserving and scalable agricultural
intelligence.

</details>


### [125] [Multi-View Consistent Human Image Customization via In-Context Learning](https://arxiv.org/abs/2511.00293)
*Hengjia Li,Jianjin Xu,Keli Cheng,Lei Wang,Ning Bi,Boxi Wu,Fernando De la Torre,Deng Cai*

Main category: cs.CV

TL;DR: PersonalView is a lightweight adaptation method that enables existing generative models to create multi-view consistent images of a person with only 100 training samples, outperforming baselines trained on large multi-view datasets.


<details>
  <summary>Details</summary>
Motivation: Current personalized generative models lack viewpoint control and cannot generate consistent multiple views of the same person, limiting their practical applications.

Method: PersonalView uses a conditioning architecture leveraging pre-trained diffusion transformer's in-context learning ability, combined with Semantic Correspondence Alignment Loss to preserve original generative capabilities.

Result: PersonalView significantly outperforms baselines trained on large multi-view datasets using only 100 training samples, achieving better multi-view consistency, text alignment, identity similarity, and visual quality.

Conclusion: The proposed lightweight adaptation method successfully enables multi-view generation capability with minimal training data, demonstrating superior performance over existing approaches.

Abstract: Recent advances in personalized generative models demonstrate impressive
results in creating identity-consistent images of the same person under diverse
settings. Yet, we note that most methods cannot control the viewpoint of the
generated image, nor generate consistent multiple views of the person. To
address this problem, we propose a lightweight adaptation method, PersonalView,
capable of enabling an existing model to acquire multi-view generation
capability with as few as 100 training samples. PersonalView consists of two
key components: First, we design a conditioning architecture to take advantage
of the in-context learning ability of the pre-trained diffusion transformer.
Second, we preserve the original generative ability of the pretrained model
with a new Semantic Correspondence Alignment Loss. We evaluate the multi-view
consistency, text alignment, identity similarity, and visual quality of
PersonalView and compare it to recent baselines with potential capability of
multi-view customization. PersonalView significantly outperforms baselines
trained on a large corpus of multi-view data with only 100 training samples.

</details>


### [126] [Towards Automated Petrography](https://arxiv.org/abs/2511.00328)
*Isai Daniel Chacn,Paola Ruiz Puentes,Jillian Pearse,Pablo Arbelez*

Main category: cs.CV

TL;DR: LITHOS is the largest public dataset for automated petrography, containing 211,604 RGB patches and 105,802 expert-annotated mineral grains across 25 categories, with a dual-encoder transformer model that outperforms single-polarization approaches.


<details>
  <summary>Details</summary>
Motivation: Petrography is labor-intensive and requires expert visual examination, limiting scalability. There's a need for automated techniques to analyze rock mineral composition from thin sections.

Method: Created LITHOS dataset with polarized light images and expert annotations. Proposed dual-encoder transformer architecture that integrates both polarization modalities for mineral classification.

Result: The dual-encoder transformer consistently outperforms single-polarization models, demonstrating the value of polarization synergy in mineral classification.

Conclusion: LITHOS benchmark (dataset, code, and pretrained models) is made publicly available to foster reproducibility and further research in automated petrographic analysis.

Abstract: Petrography is a branch of geology that analyzes the mineralogical
composition of rocks from microscopical thin section samples. It is essential
for understanding rock properties across geology, archaeology, engineering,
mineral exploration, and the oil industry. However, petrography is a
labor-intensive task requiring experts to conduct detailed visual examinations
of thin section samples through optical polarization microscopes, thus
hampering scalability and highlighting the need for automated techniques. To
address this challenge, we introduce the Large-scale Imaging and Thin section
Optical-polarization Set (LITHOS), the largest and most diverse publicly
available experimental framework for automated petrography. LITHOS includes
211,604 high-resolution RGB patches of polarized light and 105,802
expert-annotated grains across 25 mineral categories. Each annotation consists
of the mineral class, spatial coordinates, and expert-defined major and minor
axes represented as intersecting vector paths, capturing grain geometry and
orientation. We evaluate multiple deep learning techniques for mineral
classification in LITHOS and propose a dual-encoder transformer architecture
that integrates both polarization modalities as a strong baseline for future
reference. Our method consistently outperforms single-polarization models,
demonstrating the value of polarization synergy in mineral classification. We
have made the LITHOS Benchmark publicly available, comprising our dataset,
code, and pretrained models, to foster reproducibility and further research in
automated petrographic analysis.

</details>


### [127] [Beyond ImageNet: Understanding Cross-Dataset Robustness of Lightweight Vision Models](https://arxiv.org/abs/2511.00335)
*Weidong Zhang,Pak Lun Kevin Ding,Huan Liu*

Main category: cs.CV

TL;DR: Systematic evaluation of 11 lightweight vision models across 7 datasets reveals ImageNet accuracy doesn't predict cross-domain performance, introduces xScore metric for robustness assessment, and identifies key architectural components for generalization.


<details>
  <summary>Details</summary>
Motivation: Lightweight vision models are widely deployed on mobile devices but primarily benchmarked on ImageNet, raising questions about their cross-domain generalization and how to systematically quantify robustness across diverse datasets.

Method: Evaluated 11 lightweight vision models (2.5M parameters) trained under fixed 100-epoch schedule across 7 diverse datasets, introduced Cross-Dataset Score (xScore) metric to quantify performance consistency, and analyzed architectural components.

Result: ImageNet accuracy doesn't reliably predict performance on fine-grained or medical datasets; xScore provides scalable predictor of mobile model performance; isotropic convolutions with higher spatial resolution and channel-wise attention promote generalization, while Transformer blocks offer little benefit despite higher parameter overhead.

Conclusion: Provides reproducible framework for evaluating lightweight vision models beyond ImageNet, highlights key design principles for mobile-friendly architectures, and guides development of models that generalize robustly across diverse domains.

Abstract: Lightweight vision classification models such as MobileNet, ShuffleNet, and
EfficientNet are increasingly deployed in mobile and embedded systems, yet
their performance has been predominantly benchmarked on ImageNet. This raises
critical questions: Do models that excel on ImageNet also generalize across
other domains? How can cross-dataset robustness be systematically quantified?
And which architectural elements consistently drive generalization under tight
resource constraints? Here, we present the first systematic evaluation of 11
lightweight vision models (2.5M parameters), trained under a fixed 100-epoch
schedule across 7 diverse datasets. We introduce the Cross-Dataset Score
(xScore), a unified metric that quantifies the consistency and robustness of
model performance across diverse visual domains. Our results show that (1)
ImageNet accuracy does not reliably predict performance on fine-grained or
medical datasets, (2) xScore provides a scalable predictor of mobile model
performance that can be estimated from just four datasets, and (3) certain
architectural components--such as isotropic convolutions with higher spatial
resolution and channel-wise attention--promote broader generalization, while
Transformer-based blocks yield little additional benefit, despite incurring
higher parameter overhead. This study provides a reproducible framework for
evaluating lightweight vision models beyond ImageNet, highlights key design
principles for mobile-friendly architectures, and guides the development of
future models that generalize robustly across diverse application domains.

</details>


### [128] [A DeepONet joint Neural Tangent Kernel Hybrid Framework for Physics-Informed Inverse Source Problems and Robust Image Reconstruction](https://arxiv.org/abs/2511.00338)
*Yuhao Fang,Zijian Wang,Yao Lu,Ye Zhang,Chun Li*

Main category: cs.CV

TL;DR: A hybrid DeepONet-NTK approach for solving complex inverse problems with physics-informed constraints and regularization.


<details>
  <summary>Details</summary>
Motivation: To address challenges in solving inverse problems involving nonlinearity, sparsity, and noisy data in computational physics and imaging.

Method: Integrates Deep Operator Networks with Neural Tangent Kernel, incorporating physics-informed constraints and task-specific regularization into the loss function.

Result: Validated on synthetic and real datasets, demonstrating robustness, scalability, and precision in source localization and image reconstruction tasks.

Conclusion: The framework shows broad potential applications in computational physics and imaging sciences by ensuring physically consistent and accurate solutions.

Abstract: This work presents a novel hybrid approach that integrates Deep Operator
Networks (DeepONet) with the Neural Tangent Kernel (NTK) to solve complex
inverse problem. The method effectively addresses tasks such as source
localization governed by the Navier-Stokes equations and image reconstruction,
overcoming challenges related to nonlinearity, sparsity, and noisy data. By
incorporating physics-informed constraints and task-specific regularization
into the loss function, the framework ensures solutions that are both
physically consistent and accurate. Validation on diverse synthetic and real
datasets demonstrates its robustness, scalability, and precision, showcasing
its broad potential applications in computational physics and imaging sciences.

</details>


### [129] [Federated Dialogue-Semantic Diffusion for Emotion Recognition under Incomplete Modalities](https://arxiv.org/abs/2511.00344)
*Xihang Qiu,Jiarong Cheng,Yuhao Fang,Wanpeng Zhang,Yao Lu,Ye Zhang,Chun Li*

Main category: cs.CV

TL;DR: FedDISC is a federated learning framework that addresses missing modality issues in multimodal emotion recognition by using diffusion models and semantic consistency mechanisms.


<details>
  <summary>Details</summary>
Motivation: Real-world multimodal emotion recognition suffers from unpredictable modality absence, which degrades performance of existing methods that rely on complete multimodal data.

Method: Proposes FedDISC framework with federated aggregation of modality-specific diffusion models, DISC-Diffusion module for semantic consistency, Dialogue Graph Network for conversational dependencies, and Alternating Frozen Aggregation strategy.

Result: Extensive experiments on IEMOCAP, CMUMOSI, and CMUMOSEI datasets show superior emotion classification performance across diverse missing modality patterns, outperforming existing approaches.

Conclusion: FedDISC effectively addresses missing modality challenges in multimodal emotion recognition through federated learning and semantic-consistent diffusion models.

Abstract: Multimodal Emotion Recognition in Conversations (MERC) enhances emotional
understanding through the fusion of multimodal signals. However, unpredictable
modality absence in real-world scenarios significantly degrades the performance
of existing methods. Conventional missing-modality recovery approaches, which
depend on training with complete multimodal data, often suffer from semantic
distortion under extreme data distributions, such as fixed-modality absence. To
address this, we propose the Federated Dialogue-guided and Semantic-Consistent
Diffusion (FedDISC) framework, pioneering the integration of federated learning
into missing-modality recovery. By federated aggregation of modality-specific
diffusion models trained on clients and broadcasting them to clients missing
corresponding modalities, FedDISC overcomes single-client reliance on modality
completeness. Additionally, the DISC-Diffusion module ensures consistency in
context, speaker identity, and semantics between recovered and available
modalities, using a Dialogue Graph Network to capture conversational
dependencies and a Semantic Conditioning Network to enforce semantic alignment.
We further introduce a novel Alternating Frozen Aggregation strategy, which
cyclically freezes recovery and classifier modules to facilitate collaborative
optimization. Extensive experiments on the IEMOCAP, CMUMOSI, and CMUMOSEI
datasets demonstrate that FedDISC achieves superior emotion classification
performance across diverse missing modality patterns, outperforming existing
approaches.

</details>


### [130] [OSMGen: Highly Controllable Satellite Image Synthesis using OpenStreetMap Data](https://arxiv.org/abs/2511.00345)
*Amir Ziashahabi,Narges Ghasemi,Sajjad Shahabi,John Krumm,Salman Avestimehr,Cyrus Shahabi*

Main category: cs.CV

TL;DR: OSMGen is a generative framework that creates realistic satellite imagery from raw OpenStreetMap data, enabling consistent before-after image pairs for urban monitoring and training data generation.


<details>
  <summary>Details</summary>
Motivation: Automating urban monitoring is challenging due to scarce curated datasets of specific urban features and their changes. There's a need for accurate geospatial data for urban planning, infrastructure monitoring, and environmental management.

Method: OSMGen uses the full richness of OSM JSON data including vector geometries, semantic tags, location, and time. It generates satellite imagery directly from raw OSM data, allowing fine-grained control over scene generation and producing consistent before-after image pairs.

Result: The framework can generate training data to address scarcity and class imbalance, and enables planners to preview proposed interventions by editing map data. It produces paired (JSON, image) data for both static and changed states.

Conclusion: OSMGen paves the way toward a closed-loop system where satellite imagery can automatically drive structured OSM updates, bridging the gap between map data and visual representations.

Abstract: Accurate and up-to-date geospatial data are essential for urban planning,
infrastructure monitoring, and environmental management. Yet, automating urban
monitoring remains difficult because curated datasets of specific urban
features and their changes are scarce. We introduce OSMGen, a generative
framework that creates realistic satellite imagery directly from raw
OpenStreetMap (OSM) data. Unlike prior work that relies on raster tiles, OSMGen
uses the full richness of OSM JSON, including vector geometries, semantic tags,
location, and time, giving fine-grained control over how scenes are generated.
A central feature of the framework is the ability to produce consistent
before-after image pairs: user edits to OSM inputs translate into targeted
visual changes, while the rest of the scene is preserved. This makes it
possible to generate training data that addresses scarcity and class imbalance,
and to give planners a simple way to preview proposed interventions by editing
map data. More broadly, OSMGen produces paired (JSON, image) data for both
static and changed states, paving the way toward a closed-loop system where
satellite imagery can automatically drive structured OSM updates. Source code
is available at https://github.com/amir-zsh/OSMGen.

</details>


### [131] [Detecting AI-Generated Images via Diffusion Snap-Back Reconstruction: A Forensic Approach](https://arxiv.org/abs/2511.00352)
*Mohd Ruhul Ameen,Akif Islam*

Main category: cs.CV

TL;DR: A diffusion-based forensic framework using multi-strength image reconstruction dynamics (diffusion snap-back) to detect AI-generated images by analyzing reconstruction metric evolution across noise strengths.


<details>
  <summary>Details</summary>
Motivation: Traditional deepfake detection methods fail against modern text-to-image systems like Stable Diffusion and DALL-E that produce photorealistic, artifact-free results, making it challenging to distinguish authentic from synthetic content.

Method: Leverages diffusion snap-back - analyzing how reconstruction metrics (LPIPS, SSIM, PSNR) evolve across varying noise strengths to extract interpretable manifold-based features that differentiate real and synthetic images.

Result: Achieves 0.993 AUROC under cross-validation on 4,000 images, remains robust to common distortions like compression and noise, and demonstrates strong generalization despite limited data and single diffusion backbone (Stable Diffusion v1.5).

Conclusion: The method provides a foundation for scalable, model-agnostic synthetic media forensics with strong generalization and interpretability capabilities.

Abstract: The rapid rise of generative diffusion models has made distinguishing
authentic visual content from synthetic imagery increasingly challenging.
Traditional deepfake detection methods, which rely on frequency or pixel-level
artifacts, fail against modern text-to-image systems such as Stable Diffusion
and DALL-E that produce photorealistic and artifact-free results. This paper
introduces a diffusion-based forensic framework that leverages multi-strength
image reconstruction dynamics, termed diffusion snap-back, to identify
AI-generated images. By analysing how reconstruction metrics (LPIPS, SSIM, and
PSNR) evolve across varying noise strengths, we extract interpretable
manifold-based features that differentiate real and synthetic images. Evaluated
on a balanced dataset of 4,000 images, our approach achieves 0.993 AUROC under
cross-validation and remains robust to common distortions such as compression
and noise. Despite using limited data and a single diffusion backbone (Stable
Diffusion v1.5), the proposed method demonstrates strong generalization and
interpretability, offering a foundation for scalable, model-agnostic synthetic
media forensics.

</details>


### [132] [Transfer Learning for Onboard Cloud Segmentation in Thermal Earth Observation: From Landsat to a CubeSat Constellation](https://arxiv.org/abs/2511.00357)
*Niklas Wlki,Lukas Kondmann,Christian Mollire,Martin Langer,Julia Gottfriedsen,Martin Werner*

Main category: cs.CV

TL;DR: Transfer learning with UNet-MobileNet enables efficient thermal cloud segmentation for CubeSats, achieving 0.877 F1 score and under 5-second inference on Jetson Nano.


<details>
  <summary>Details</summary>
Motivation: CubeSat missions face challenges in cloud segmentation due to limited hardware, single thermal band data, and insufficient labeled data, making conventional methods infeasible.

Method: Used UNet with lightweight MobileNet encoder, pretrained on Landsat-7 Cloud Cover Assessment Dataset, then fine-tuned with mission-specific samples in joint-training setup.

Result: Improved macro F1 from 0.850 to 0.877 over FOREST-2-only baselines, with TensorRT conversion enabling full-image inference in under 5 seconds on NVIDIA Jetson Nano.

Conclusion: Public datasets and lightweight architectures enable accurate, efficient thermal-only cloud masking for real-time decision-making in data-limited Earth observation missions.

Abstract: Onboard cloud segmentation is a critical yet underexplored task in thermal
Earth observation (EO), particularly for CubeSat missions constrained by
limited hardware and spectral information. CubeSats often rely on a single
thermal band and lack sufficient labeled data, making conventional cloud
masking techniques infeasible. This work addresses these challenges by applying
transfer learning to thermal cloud segmentation for the FOREST-2 CubeSat, using
a UNet with a lightweight MobileNet encoder. We pretrain the model on the
public Landsat-7 Cloud Cover Assessment Dataset and fine-tune it with a small
set of mission-specific samples in a joint-training setup, improving the macro
F1 from 0.850 to 0.877 over FOREST-2-only baselines. We convert the model to a
TensorRT engine and demonstrate full-image inference in under 5 seconds on an
NVIDIA Jetson Nano. These results show that leveraging public datasets and
lightweight architectures can enable accurate, efficient thermal-only cloud
masking on-orbit, supporting real-time decision-making in data-limited EO
missions.

</details>


### [133] [Oitijjo-3D: Generative AI Framework for Rapid 3D Heritage Reconstruction from Street View Imagery](https://arxiv.org/abs/2511.00362)
*Momen Khandoker Ope,Akif Islam,Mohd Ruhul Ameen,Abu Saleh Musa Miah,Md Rashedul Islam,Jungpil Shin*

Main category: cs.CV

TL;DR: Oitijjo-3D is a free generative AI framework that uses Google Street View imagery to create 3D models of cultural heritage sites, overcoming resource limitations in developing countries like Bangladesh.


<details>
  <summary>Details</summary>
Motivation: Cultural heritage restoration in Bangladesh faces challenges of limited resources, expensive hardware requirements, and scarce technical expertise, making traditional 3D digitization methods infeasible.

Method: Two-stage pipeline using multimodal visual reasoning with Gemini 2.5 Flash Image for structure-texture synthesis, and neural image-to-3D generation through Hexagen for geometry recovery from Google Street View imagery.

Result: Produces photorealistic, metrically coherent 3D reconstructions in seconds with significant speedups compared to conventional methods, while preserving visual and structural fidelity of landmarks like Ahsan Manzil and Choto Sona Mosque.

Conclusion: The framework democratizes 3D cultural preservation by turning open imagery into digital heritage, reframing preservation as a community-driven, AI-assisted act for resource-limited nations.

Abstract: Cultural heritage restoration in Bangladesh faces a dual challenge of limited
resources and scarce technical expertise. Traditional 3D digitization methods,
such as photogrammetry or LiDAR scanning, require expensive hardware, expert
operators, and extensive on-site access, which are often infeasible in
developing contexts. As a result, many of Bangladesh's architectural treasures,
from the Paharpur Buddhist Monastery to Ahsan Manzil, remain vulnerable to
decay and inaccessible in digital form. This paper introduces Oitijjo-3D, a
cost-free generative AI framework that democratizes 3D cultural preservation.
By using publicly available Google Street View imagery, Oitijjo-3D reconstructs
faithful 3D models of heritage structures through a two-stage pipeline -
multimodal visual reasoning with Gemini 2.5 Flash Image for structure-texture
synthesis, and neural image-to-3D generation through Hexagen for geometry
recovery. The system produces photorealistic, metrically coherent
reconstructions in seconds, achieving significant speedups compared to
conventional Structure-from-Motion pipelines, without requiring any specialized
hardware or expert supervision. Experiments on landmarks such as Ahsan Manzil,
Choto Sona Mosque, and Paharpur demonstrate that Oitijjo-3D preserves both
visual and structural fidelity while drastically lowering economic and
technical barriers. By turning open imagery into digital heritage, this work
reframes preservation as a community-driven, AI-assisted act of cultural
continuity for resource-limited nations.

</details>


### [134] [Who Can We Trust? Scope-Aware Video Moment Retrieval with Multi-Agent Conflict](https://arxiv.org/abs/2511.00370)
*Chaochen Wu,Guan Luo,Meiyun Zuo,Zhitao Fan*

Main category: cs.CV

TL;DR: A reinforcement learning-based video moment retrieval model that uses multi-agent systems with evidential learning to resolve conflicts between agents' localization outputs and detect out-of-scope queries.


<details>
  <summary>Details</summary>
Motivation: Current video moment retrieval solutions don't handle conflicts between different models' location results, preventing effective integration of multiple models to improve performance.

Method: Proposed a reinforcement learning model that scans entire videos once to find moment boundaries while producing locational evidence, and a multi-agent system framework using evidential learning to resolve conflicts between agents' localization outputs.

Result: Extensive experiments on benchmark datasets show effectiveness compared to state-of-the-art approaches, and the model can detect out-of-scope queries without additional training.

Conclusion: Modeling competition and conflict in multi-agent systems effectively improves RL performance in moment retrieval, and evidential learning plays a new role in multi-agent frameworks.

Abstract: Video moment retrieval uses a text query to locate a moment from a given
untrimmed video reference. Locating corresponding video moments with text
queries helps people interact with videos efficiently. Current solutions for
this task have not considered conflict within location results from different
models, so various models cannot integrate correctly to produce better results.
This study introduces a reinforcement learning-based video moment retrieval
model that can scan the whole video once to find the moment's boundary while
producing its locational evidence. Moreover, we proposed a multi-agent system
framework that can use evidential learning to resolve conflicts between agents'
localization output. As a side product of observing and dealing with conflicts
between agents, we can decide whether a query has no corresponding moment in a
video (out-of-scope) without additional training, which is suitable for
real-world applications. Extensive experiments on benchmark datasets show the
effectiveness of our proposed methods compared with state-of-the-art
approaches. Furthermore, the results of our study reveal that modeling
competition and conflict of the multi-agent system is an effective way to
improve RL performance in moment retrieval and show the new role of evidential
learning in the multi-agent framework.

</details>


### [135] [VisionCAD: An Integration-Free Radiology Copilot Framework](https://arxiv.org/abs/2511.00381)
*Jiaming Li,Junlei Wu,Sheng Wang,Honglin Xiong,Jiangdong Cai,Zihao Zhao,Yitao Zhu,Yuan Yin,Dinggang Shen,Qian Wang*

Main category: cs.CV

TL;DR: VisionCAD is a vision-based radiological assistance framework that captures medical images from displays using cameras, enabling AI-assisted diagnosis without modifying existing hospital IT infrastructure.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenge of integrating computer-aided diagnosis systems with existing hospital IT infrastructure, which hinders widespread clinical deployment.

Method: Uses a camera system to capture medical images from displays, followed by an automated pipeline that detects, restores, and analyzes on-screen images to transform camera-captured data into diagnostic-quality images for automated analysis and report generation.

Result: Achieves diagnostic performance comparable to conventional CAD systems, with F1-score degradation typically less than 2% across classification tasks, and natural language generation metrics for automated reports remain within 1% of those from original images.

Conclusion: VisionCAD offers an accessible approach for AI-assisted diagnosis that can be deployed in diverse clinical settings without modifications to existing infrastructure, requiring only a camera device and standard computing resources.

Abstract: Widespread clinical deployment of computer-aided diagnosis (CAD) systems is
hindered by the challenge of integrating with existing hospital IT
infrastructure. Here, we introduce VisionCAD, a vision-based radiological
assistance framework that circumvents this barrier by capturing medical images
directly from displays using a camera system. The framework operates through an
automated pipeline that detects, restores, and analyzes on-screen medical
images, transforming camera-captured visual data into diagnostic-quality images
suitable for automated analysis and report generation. We validated VisionCAD
across diverse medical imaging datasets, demonstrating that our modular
architecture can flexibly utilize state-of-the-art diagnostic models for
specific tasks. The system achieves diagnostic performance comparable to
conventional CAD systems operating on original digital images, with an F1-score
degradation typically less than 2\% across classification tasks, while natural
language generation metrics for automated reports remain within 1\% of those
derived from original images. By requiring only a camera device and standard
computing resources, VisionCAD offers an accessible approach for AI-assisted
diagnosis, enabling the deployment of diagnostic capabilities in diverse
clinical settings without modifications to existing infrastructure.

</details>


### [136] [Rethinking Facial Expression Recognition in the Era of Multimodal Large Language Models: Benchmark, Datasets, and Beyond](https://arxiv.org/abs/2511.00389)
*Fan Zhang,Haoxuan Li,Shengju Qian,Xin Wang,Zheng Lian,Hao Wu,Zhihong Zhu,Yuan Gao,Qiankun Li,Yefeng Zheng,Zhouchen Lin,Pheng-Ann Heng*

Main category: cs.CV

TL;DR: FERBench benchmark reveals MLLMs' limitations in facial expression reasoning despite good classification performance. UniFER-7B model is developed with post-training strategies to enhance reasoning capabilities, outperforming many generalist MLLMs.


<details>
  <summary>Details</summary>
Motivation: To address the unexplored performance of state-of-the-art MLLMs on facial expression recognition tasks and their limitations in reasoning and interpretability.

Method: Created FERBench benchmark with 20 MLLMs across 4 FER datasets, then developed UniFER-7B using post-training strategies with two curated datasets: UniFER-CoT-230K for initialization and UniFER-RLVR-360K for reinforcement learning with verifiable rewards.

Result: MLLMs show good classification performance but significant limitations in reasoning and interpretability. UniFER-7B outperforms many open-source and closed-source generalist MLLMs including Gemini-2.5-Pro and Qwen2.5-VL-72B.

Conclusion: Post-training strategies effectively enhance MLLMs' facial expression reasoning capabilities, with UniFER-7B demonstrating superior performance as a unified and interpretable FER foundation model.

Abstract: Multimodal Large Language Models (MLLMs) have revolutionized numerous
research fields, including computer vision and affective computing. As a
pivotal challenge in this interdisciplinary domain, facial expression
recognition (FER) has evolved from separate, domain-specific models to more
unified approaches. One promising avenue to unify FER tasks is converting
conventional FER datasets into visual question-answering (VQA) formats,
enabling the direct application of powerful generalist MLLMs for inference.
However, despite the success of cutting-edge MLLMs in various tasks, their
performance on FER tasks remains largely unexplored. To address this gap, we
provide FERBench, a systematic benchmark that incorporates 20 state-of-the-art
MLLMs across four widely used FER datasets. Our results reveal that, while
MLLMs exhibit good classification performance, they still face significant
limitations in reasoning and interpretability. To this end, we introduce
post-training strategies aimed at enhancing the facial expression reasoning
capabilities of MLLMs. Specifically, we curate two high-quality and large-scale
datasets: UniFER-CoT-230K for cold-start initialization and UniFER-RLVR-360K
for reinforcement learning with verifiable rewards (RLVR), respectively.
Building upon them, we develop a unified and interpretable FER foundation model
termed UniFER-7B, which outperforms many open-sourced and closed-source
generalist MLLMs (e.g., Gemini-2.5-Pro and Qwen2.5-VL-72B).

</details>


### [137] [VinciCoder: Unifying Multimodal Code Generation via Coarse-to-fine Visual Reinforcement Learning](https://arxiv.org/abs/2511.00391)
*Xuanle Zhao,Deyang Jiang,Zhixiong Zeng,Lei Chen,Haibo Qiu,Jing Huang,Yufeng Zhong,Liming Zheng,Yilin Cao,Lin Ma*

Main category: cs.CV

TL;DR: VinciCoder is a unified multimodal code generation model that achieves state-of-the-art performance through a two-stage training framework combining supervised fine-tuning with a visual reinforcement learning strategy.


<details>
  <summary>Details</summary>
Motivation: Current vision-language models rely on single-task training, which limits their ability to develop generalized visual code intelligence capabilities.

Method: Two-stage training: 1) Supervised fine-tuning with 1.6M image-code pairs for code generation and refinement tasks, 2) Visual Reinforcement Learning (ViRL) with coarse-to-fine reward mechanism using visual similarity across local and global image patches.

Result: VinciCoder achieves state-of-the-art performance on various multimodal code generation benchmarks.

Conclusion: The proposed coarse-to-fine ViRL strategy effectively improves visual fidelity and enables superior multimodal code generation capabilities.

Abstract: Multimodal code generation has garnered significant interest within the
research community. Despite the notable success of recent vision-language
models (VLMs) on specialized tasks like Chart-to-code generation, their
reliance on single-task training regimens fosters a narrow paradigm that
hinders the development of generalized \textbf{VI}sio\textbf{N} \textbf{C}ode
\textbf{I}ntelligence. In this work, we introduce \textbf{VinciCoder}, a
unified multimodal code generation model that addresses this limitation via a
two-stage training framework. We begin by constructing a large-scale Supervised
Finetuning (SFT) corpus comprising 1.6M image-code pairs for tasks involving
direct code generation and visual-based code refinement. Subsequently, we
introduce a Visual Reinforcement Learning (ViRL) strategy, which employs a
coarse-to-fine reward mechanism to improve visual fidelity by calculating
visual similarity across local and global image patches. Extensive experiments
on various multimodal code generation benchmarks demonstrate that VinciCoder
achieves state-of-the-art performance, underscoring the effectiveness of our
coarse-to-fine ViRL strategy. The code and model will be available at
https://github.com/DocTron-hub/VinciCoder.

</details>


### [138] [CoT-Saliency: Unified Chain-of-Thought Reasoning for Heterogeneous Saliency Tasks](https://arxiv.org/abs/2511.00396)
*Long Li,Shuichen Ji,Ziyang Luo,Nian Liu,Dingwen Zhang,Junwei Han*

Main category: cs.CV

TL;DR: A unified framework using Chain-of-Thought reasoning in Vision-Language Models to jointly handle three saliency tasks (SOD, CoSOD, SIS) through a two-stage training approach with Confidence-Guided Policy Optimization.


<details>
  <summary>Details</summary>
Motivation: To address operational heterogeneity across different saliency tasks by creating a unified framework that bridges task differences through reasoning processes.

Method: Two-stage training: Supervised Fine-Tuning (SFT) with output-to-reasoning strategy for data construction, followed by Reinforcement Learning with Confidence-Guided Policy Optimization (CGPO) that uses reward-confidence discrepancy as advantage signal.

Result: Achieves state-of-the-art performance across all tasks, particularly 0.899 S-measure on CoCA for CoSOD (8.0% improvement over prior best), using significantly less training data than specialized methods.

Conclusion: The proposed unified framework successfully handles multiple saliency tasks through CoT reasoning, with CGPO effectively addressing limitations of previous RL approaches while achieving superior performance with reduced computational requirements.

Abstract: We present the first unified framework that jointly handles three
operationally heterogeneous saliency tasks, eg, SOD, CoSOD, and SIS, by casting
each as a Chain-of-Thought (CoT) reasoning process in a Vision-Language Model
(VLM) to bridge task heterogeneity. CoT training follows a two-stage paradigm:
Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). To enhance CoT
quality in RL, we propose Confidence-Guided Policy Optimization (CGPO), a
lightweight single-sample algorithm that leverages the discrepancy between
reward and model confidence as a per-sample advantage signal. This design
naturally focuses updates on informative responses while eliminating group
sampling, thereby addressing GRPO's key limitations: confidence-agnostic
learning, signal dilution, and prohibitive computational overhead. We also
introduce an "output-to-reasoning" strategy to construct high-fidelity SFT data
that ensures logical consistency with ground-truth masks. Experiments show our
model matches or outperforms specialized SOTA methods and strong closed-source
VLMs across all tasks, especially achieving an S-measure of 0.899 on CoCA for
CoSOD, surpassing the prior best by 8.0 percentage points, despite using far
less training data.

</details>


### [139] [LGCA: Enhancing Semantic Representation via Progressive Expansion](https://arxiv.org/abs/2511.00419)
*Thanh Hieu Cao,Trung Khang Tran,Gia Thinh Pham,Tuong Nghiem Diep,Thanh Binh Nguyen*

Main category: cs.CV

TL;DR: LGCA is a framework that improves zero-shot image classification by capturing local features, selecting salient regions for expansion, and combining local-global similarity scores to reduce misinformation from random crops.


<details>
  <summary>Details</summary>
Motivation: Random image crops in CLIP-based models can introduce misinformation and bias due to similar small-scale features across images, which degrades zero-shot classification performance.

Method: Propose Localized-Globalized Cross-Alignment (LGCA) that captures local features, repeatedly selects the most salient regions and expands them, and computes similarity scores combining both original and expanded images.

Result: LGCA substantially improves zero-shot performance across diverse datasets, outperforming state-of-the-art baselines while maintaining the same time complexity as the original model.

Conclusion: The LGCA framework effectively captures both local and global features while minimizing misinformation, demonstrating improved efficiency and scalability for zero-shot image classification tasks.

Abstract: Recent advancements in large-scale pretraining in natural language processing
have enabled pretrained vision-language models such as CLIP to effectively
align images and text, significantly improving performance in zero-shot image
classification tasks. Subsequent studies have further demonstrated that
cropping images into smaller regions and using large language models to
generate multiple descriptions for each caption can further enhance model
performance. However, due to the inherent sensitivity of CLIP, random image
crops can introduce misinformation and bias, as many images share similar
features at small scales. To address this issue, we propose
Localized-Globalized Cross-Alignment (LGCA), a framework that first captures
the local features of an image and then repeatedly selects the most salient
regions and expands them. The similarity score is designed to incorporate both
the original and expanded images, enabling the model to capture both local and
global features while minimizing misinformation. Additionally, we provide a
theoretical analysis demonstrating that the time complexity of LGCA remains the
same as that of the original model prior to the repeated expansion process,
highlighting its efficiency and scalability. Extensive experiments demonstrate
that our method substantially improves zero-shot performance across diverse
datasets, outperforming state-of-the-art baselines.

</details>


### [140] [Leveraging Hierarchical Image-Text Misalignment for Universal Fake Image Detection](https://arxiv.org/abs/2511.00427)
*Daichi Zhang,Tong Zhang,Jianmin Bao,Shiming Ge,Sabine Ssstrunk*

Main category: cs.CV

TL;DR: Proposes ITEM, a fake image detector that uses image-text misalignment in CLIP space as discriminative clues, outperforming existing methods with better generalization across generative models.


<details>
  <summary>Details</summary>
Motivation: Existing fake image detection methods focus only on visual patterns and suffer from overfitting, lacking generalization to unseen generative models. Fake images show poor alignment with corresponding captions compared to real images.

Method: Uses pre-trained CLIP to measure image-text misalignment, then tunes an MLP head for detection. Introduces hierarchical misalignment scheme analyzing both global image-level and fine-grained object-level semantic alignment.

Result: Extensive experiments show superior performance against state-of-the-art methods with impressive generalization and robustness across various recent generative models.

Conclusion: Multi-modal approach using image-text misalignment provides more robust and generalizable fake image detection compared to visual-only methods.

Abstract: With the rapid development of generative models, detecting generated fake
images to prevent their malicious use has become a critical issue recently.
Existing methods frame this challenge as a naive binary image classification
task. However, such methods focus only on visual clues, yielding trained
detectors susceptible to overfitting specific image patterns and incapable of
generalizing to unseen models. In this paper, we address this issue from a
multi-modal perspective and find that fake images cannot be properly aligned
with corresponding captions compared to real images. Upon this observation, we
propose a simple yet effective detector termed ITEM by leveraging the
image-text misalignment in a joint visual-language space as discriminative
clues. Specifically, we first measure the misalignment of the images and
captions in pre-trained CLIP's space, and then tune a MLP head to perform the
usual detection task. Furthermore, we propose a hierarchical misalignment
scheme that first focuses on the whole image and then each semantic object
described in the caption, which can explore both global and fine-grained local
semantic misalignment as clues. Extensive experiments demonstrate the
superiority of our method against other state-of-the-art competitors with
impressive generalization and robustness on various recent generative models.

</details>


### [141] [Enhancing Frequency Forgery Clues for Diffusion-Generated Image Detection](https://arxiv.org/abs/2511.00429)
*Daichi Zhang,Tong Zhang,Shiming Ge,Sabine Ssstrunk*

Main category: cs.CV

TL;DR: The paper proposes a frequency-based detection method (F^2C) that enhances discriminative frequency bands to identify diffusion-generated images, achieving superior generalization and robustness compared to existing detectors.


<details>
  <summary>Details</summary>
Motivation: Diffusion models generate high-quality images that raise concerns about malicious use, but existing detectors struggle with generalization across different models and robustness to perturbations.

Method: Proposes Frequency Forgery Clue (F^2C) representation that uses a frequency-selective function as a weighted filter to enhance discriminative frequency bands across low to high frequencies, based on observed progressive differences between natural and diffusion-generated images.

Result: Extensive experiments show the method outperforms state-of-the-art detectors with superior generalization to unseen diffusion models and robustness to various perturbations.

Conclusion: The frequency-based approach provides an effective solution for detecting diffusion-generated images with strong generalization and robustness capabilities.

Abstract: Diffusion models have achieved remarkable success in image synthesis, but the
generated high-quality images raise concerns about potential malicious use.
Existing detectors often struggle to capture discriminative clues across
different models and settings, limiting their generalization to unseen
diffusion models and robustness to various perturbations. To address this
issue, we observe that diffusion-generated images exhibit progressively larger
differences from natural real images across low- to high-frequency bands. Based
on this insight, we propose a simple yet effective representation by enhancing
the Frequency Forgery Clue (F^2C) across all frequency bands. Specifically, we
introduce a frequency-selective function which serves as a weighted filter to
the Fourier spectrum, suppressing less discriminative bands while enhancing
more informative ones. This approach, grounded in a comprehensive analysis of
frequency-based differences between natural real and diffusion-generated
images, enables general detection of images from unseen diffusion models and
provides robust resilience to various perturbations. Extensive experiments on
various diffusion-generated image datasets demonstrate that our method
outperforms state-of-the-art detectors with superior generalization and
robustness.

</details>


### [142] [ToxicTextCLIP: Text-Based Poisoning and Backdoor Attacks on CLIP Pre-training](https://arxiv.org/abs/2511.00446)
*Xin Yao,Haiyang Zhao,Yimin Chen,Jiawei Guo,Kecheng Huang,Ming Zhao*

Main category: cs.CV

TL;DR: ToxicTextCLIP is a framework that generates adversarial text attacks against CLIP during pre-training, achieving high poisoning success rates while bypassing existing defenses.


<details>
  <summary>Details</summary>
Motivation: CLIP's reliance on uncurated web data makes it vulnerable to data poisoning and backdoor attacks, with text modality being underexplored compared to image-based attacks.

Method: Uses iterative background-aware selector to find texts aligned with target class, and background-driven augmenter to generate diverse poisoned samples while maintaining semantic coherence.

Result: Achieves up to 95.83% poisoning success rate and 98.68% backdoor Hit@1 on classification and retrieval tasks, while bypassing RoCLIP, CleanCLIP and SafeCLIP defenses.

Conclusion: Demonstrates significant vulnerability in CLIP's text modality to targeted poisoning attacks, highlighting the need for more robust defense mechanisms.

Abstract: The Contrastive Language-Image Pretraining (CLIP) model has significantly
advanced vision-language modeling by aligning image-text pairs from large-scale
web data through self-supervised contrastive learning. Yet, its reliance on
uncurated Internet-sourced data exposes it to data poisoning and backdoor
risks. While existing studies primarily investigate image-based attacks, the
text modality, which is equally central to CLIP's training, remains
underexplored. In this work, we introduce ToxicTextCLIP, a framework for
generating high-quality adversarial texts that target CLIP during the
pre-training phase. The framework addresses two key challenges: semantic
misalignment caused by background inconsistency with the target class, and the
scarcity of background-consistent texts. To this end, ToxicTextCLIP iteratively
applies: 1) a background-aware selector that prioritizes texts with background
content aligned to the target class, and 2) a background-driven augmenter that
generates semantically coherent and diverse poisoned samples. Extensive
experiments on classification and retrieval tasks show that ToxicTextCLIP
achieves up to 95.83% poisoning success and 98.68% backdoor Hit@1, while
bypassing RoCLIP, CleanCLIP and SafeCLIP defenses. The source code can be
accessed via https://github.com/xinyaocse/ToxicTextCLIP/.

</details>


### [143] [Weakly Supervised Pneumonia Localization from Chest X-Rays Using Deep Neural Network and Grad-CAM Explanations](https://arxiv.org/abs/2511.00456)
*Kiran Shahi,Anup Bagale*

Main category: cs.CV

TL;DR: A weakly supervised deep learning framework for pneumonia classification and localization from chest X-rays using Grad-CAM explanations, achieving 98% accuracy with ResNet-18 and EfficientNet-B0.


<details>
  <summary>Details</summary>
Motivation: To develop a pneumonia classification system that doesn't require costly pixel-level annotations and provides clinically meaningful localization through explainable AI.

Method: Used seven ImageNet-pretrained architectures (ResNet-18/50, DenseNet-121, EfficientNet-B0, MobileNet-V2/V3, ViT-B16) with focal loss and patient-wise splits. Applied Grad-CAM for localization using only image-level labels.

Result: ResNet-18 and EfficientNet-B0 achieved best performance: 98% test accuracy, ROC-AUC = 0.997, F1 = 0.987. MobileNet-V2 provided optimal accuracy-computation trade-off. Grad-CAM visualizations focused on clinically relevant lung regions.

Conclusion: The work demonstrates the potential of weakly supervised explainable models for enhancing pneumonia screening transparency and clinical trust in AI-assisted medical imaging.

Abstract: This study proposes a weakly supervised deep learning framework for pneumonia
classification and localization from chest X-rays, utilizing Grad-CAM
explanations. Instead of costly pixel-level annotations, our approach utilizes
image-level labels to generate clinically meaningful heatmaps that highlight
regions affected by pneumonia. We evaluate seven ImageNet-pretrained
architectures ResNet-18/50, DenseNet-121, EfficientNet-B0, MobileNet-V2/V3, and
ViT-B16 under identical training conditions with focal loss and patient-wise
splits to prevent data leakage. Experimental results on the Kermany CXR dataset
demonstrate that ResNet-18 and EfficientNet-B0 achieve the best overall test
accuracy of 98\%, ROC-AUC = 0.997, and F1 = 0.987, while MobileNet-V2 provides
an optimal trade-off between accuracy and computational cost. Grad-CAM
visualizations confirm that the proposed models focus on clinically relevant
lung regions, supporting the use of interpretable AI for radiological
diagnostics. This work highlights the potential of weakly supervised
explainable models that enhance pneumonia screening transparency, and clinical
trust in AI-assisted medical imaging.
  https://github.com/kiranshahi/pneumonia-analysis

</details>


### [144] [HumanCrafter: Synergizing Generalizable Human Reconstruction and Semantic 3D Segmentation](https://arxiv.org/abs/2511.00468)
*Panwang Pan,Tingting Shen,Chenxin Li,Yunlong Lin,Kairun Wen,Jingjing Zhao,Yixuan Yuan*

Main category: cs.CV

TL;DR: HumanCrafter is a unified framework for joint 3D human reconstruction and human-part segmentation from single images, integrating geometric and semantic priors with cross-task synergy.


<details>
  <summary>Details</summary>
Motivation: Current generative models achieve high-fidelity 3D human reconstruction but lack utility for specific tasks like human 3D segmentation, and there's scarcity of labeled 3D human datasets.

Method: Integrates human geometric priors in reconstruction and self-supervised semantic priors in segmentation, with pixel-aligned aggregation for cross-task synergy and multi-task optimization for texture fidelity and semantic consistency. Includes interactive annotation for data generation.

Result: Extensive experiments show HumanCrafter surpasses state-of-the-art methods in both 3D human-part segmentation and 3D human reconstruction from single images.

Conclusion: The unified framework effectively addresses the limitations of existing methods by enabling joint modeling of appearance and human-part semantics with improved performance in both reconstruction and segmentation tasks.

Abstract: Recent advances in generative models have achieved high-fidelity in 3D human
reconstruction, yet their utility for specific tasks (e.g., human 3D
segmentation) remains constrained. We propose HumanCrafter, a unified framework
that enables the joint modeling of appearance and human-part semantics from a
single image in a feed-forward manner. Specifically, we integrate human
geometric priors in the reconstruction stage and self-supervised semantic
priors in the segmentation stage. To address labeled 3D human datasets
scarcity, we further develop an interactive annotation procedure for generating
high-quality data-label pairs. Our pixel-aligned aggregation enables cross-task
synergy, while the multi-task objective simultaneously optimizes texture
modeling fidelity and semantic consistency. Extensive experiments demonstrate
that HumanCrafter surpasses existing state-of-the-art methods in both 3D
human-part segmentation and 3D human reconstruction from a single image.

</details>


### [145] [Longitudinal Vestibular Schwannoma Dataset with Consensus-based Human-in-the-loop Annotations](https://arxiv.org/abs/2511.00472)
*Navodini Wijethilake,Marina Ivory,Oscar MacCormac,Siddhant Kumar,Aaron Kujawa,Lorena Garcia-Foncillas Macias,Rebecca Burger,Amanda Hitchings,Suki Thomson,Sinan Barazi,Eleni Maratos,Rupert Obholzer,Dan Jiang,Fiona McClenaghan,Kazumi Chia,Omar Al-Salihi,Nick Thomas,Steve Connor,Tom Vercauteren,Jonathan Shapey*

Main category: cs.CV

TL;DR: A bootstrapped deep learning framework for automated vestibular schwannoma segmentation in MRI that combines multi-center data with expert consensus, achieving high accuracy (DSC 0.9670) and 37.4% efficiency improvement over manual annotation.


<details>
  <summary>Details</summary>
Motivation: Manual segmentation of vestibular schwannoma on MRI is time-consuming and requires expert input. Current deep learning methods struggle with robust performance across diverse datasets and complex clinical cases.

Method: Human-in-the-loop model training with bootstrapped DL framework for iterative segmentation and quality refinement. Combines multi-center data with expert consensus annotations and includes longitudinal contrast-enhanced T1-weighted scans.

Result: Significant improvement in segmentation accuracy (DSC from 0.9125 to 0.9670), stable performance on external datasets, and 37.4% efficiency gain over manual annotation. Expert evaluation on 143 scans identified areas for refinement.

Conclusion: The approach provides a clinically adaptable and generalizable strategy for automated VS segmentation, with publicly available dataset of 190 patients (534 annotated scans) on TCIA.

Abstract: Accurate segmentation of vestibular schwannoma (VS) on Magnetic Resonance
Imaging (MRI) is essential for patient management but often requires
time-intensive manual annotations by experts. While recent advances in deep
learning (DL) have facilitated automated segmentation, challenges remain in
achieving robust performance across diverse datasets and complex clinical
cases. We present an annotated dataset stemming from a bootstrapped DL-based
framework for iterative segmentation and quality refinement of VS in MRI. We
combine data from multiple centres and rely on expert consensus for
trustworthiness of the annotations. We show that our approach enables effective
and resource-efficient generalisation of automated segmentation models to a
target data distribution. The framework achieved a significant improvement in
segmentation accuracy with a Dice Similarity Coefficient (DSC) increase from
0.9125 to 0.9670 on our target internal validation dataset, while maintaining
stable performance on representative external datasets. Expert evaluation on
143 scans further highlighted areas for model refinement, revealing nuanced
cases where segmentation required expert intervention. The proposed approach is
estimated to enhance efficiency by approximately 37.4% compared to the
conventional manual annotation process. Overall, our human-in-the-loop model
training approach achieved high segmentation accuracy, highlighting its
potential as a clinically adaptable and generalisable strategy for automated VS
segmentation in diverse clinical settings. The dataset includes 190 patients,
with tumour annotations available for 534 longitudinal contrast-enhanced
T1-weighted (T1CE) scans from 184 patients, and non-annotated T2-weighted scans
from 6 patients. This dataset is publicly accessible on The Cancer Imaging
Archive (TCIA) (https://doi.org/10.7937/bq0z-xa62).

</details>


### [146] [FedMGP: Personalized Federated Learning with Multi-Group Text-Visual Prompts](https://arxiv.org/abs/2511.00480)
*Weihao Bo,Yanpeng Sun,Yu Wang,Xinyu Zhang,Zechao Li*

Main category: cs.CV

TL;DR: FedMGP introduces personalized federated prompt learning with multiple prompt groups, dynamic similarity-based aggregation, and achieves SOTA performance with minimal communication parameters.


<details>
  <summary>Details</summary>
Motivation: To enable vision-language models to capture diverse semantic cues in federated learning while maintaining parameter efficiency and balancing common knowledge with client-specific features.

Method: Uses multiple groups of paired textual/visual prompts with diversity loss, and dynamic aggregation via similarity-guided probabilistic sampling of prompt groups.

Result: Outperforms prior approaches in personalization and domain generalization across federated vision-language benchmarks with lowest communication parameters.

Conclusion: FedMGP effectively balances shared semantics and client-specific features through multi-group prompts and dynamic aggregation, achieving superior performance efficiently.

Abstract: In this paper, we introduce FedMGP, a new paradigm for personalized federated
prompt learning in vision-language models. FedMGP equips each client with
multiple groups of paired textual and visual prompts, enabling the model to
capture diverse, fine-grained semantic and instance-level cues. A diversity
loss is introduced to drive each prompt group to specialize in distinct and
complementary semantic aspects, ensuring that the groups collectively cover a
broader range of local characteristics. During communication, FedMGP employs a
dynamic prompt aggregation strategy based on similarity-guided probabilistic
sampling: each client computes the cosine similarity between its prompt groups
and the global prompts from the previous round, then samples s groups via a
softmax-weighted distribution. This soft selection mechanism preferentially
aggregates semantically aligned knowledge while still enabling exploration of
underrepresented patterns effectively balancing the preservation of common
knowledge with client-specific features. Notably, FedMGP maintains parameter
efficiency by redistributing a fixed prompt capacity across multiple groups,
achieving state-of-the-art performance with the lowest communication parameters
among all federated prompt learning methods. Theoretical analysis shows that
our dynamic aggregation strategy promotes robust global representation learning
by reinforcing shared semantics while suppressing client-specific noise.
Extensive experiments demonstrate that FedMGP consistently outperforms prior
approaches in both personalization and domain generalization across diverse
federated vision-language benchmarks. The code will be released on
https://github.com/weihao-bo/FedMGP.git.

</details>


### [147] [Diff4Splat: Controllable 4D Scene Generation with Latent Dynamic Reconstruction Models](https://arxiv.org/abs/2511.00503)
*Panwang Pan,Chenguo Lin,Jingjing Zhao,Chenxin Li,Yuchen Lin,Haopeng Li,Honglei Yan,Kairun Wen,Yunlong Lin,Yixuan Yuan,Yadong Mu*

Main category: cs.CV

TL;DR: Diff4Splat is a feed-forward method that generates controllable 4D scenes from single images using video diffusion models and 3D Gaussian primitives, achieving high-quality results in 30 seconds without test-time optimization.


<details>
  <summary>Details</summary>
Motivation: To enable efficient synthesis of controllable 4D scenes from single images without requiring test-time optimization or post-processing, addressing the limitations of optimization-based methods.

Method: Unifies video diffusion priors with geometry/motion constraints from 4D datasets, using a video latent transformer to predict time-varying 3D Gaussian primitives in a single forward pass.

Result: Synthesizes high-quality 4D scenes in 30 seconds, matching or surpassing optimization-based methods in video generation, novel view synthesis, and geometry extraction while being significantly more efficient.

Conclusion: Diff4Splat provides an efficient feed-forward approach for 4D scene synthesis that eliminates the need for test-time optimization while maintaining high quality across multiple tasks.

Abstract: We introduce Diff4Splat, a feed-forward method that synthesizes controllable
and explicit 4D scenes from a single image. Our approach unifies the generative
priors of video diffusion models with geometry and motion constraints learned
from large-scale 4D datasets. Given a single input image, a camera trajectory,
and an optional text prompt, Diff4Splat directly predicts a deformable 3D
Gaussian field that encodes appearance, geometry, and motion, all in a single
forward pass, without test-time optimization or post-hoc refinement. At the
core of our framework lies a video latent transformer, which augments video
diffusion models to jointly capture spatio-temporal dependencies and predict
time-varying 3D Gaussian primitives. Training is guided by objectives on
appearance fidelity, geometric accuracy, and motion consistency, enabling
Diff4Splat to synthesize high-quality 4D scenes in 30 seconds. We demonstrate
the effectiveness of Diff4Splatacross video generation, novel view synthesis,
and geometry extraction, where it matches or surpasses optimization-based
methods for dynamic scene synthesis while being significantly more efficient.

</details>


### [148] [VinDr-CXR-VQA: A Visual Question Answering Dataset for Explainable Chest X-Ray Analysis with Multi-Task Learning](https://arxiv.org/abs/2511.00504)
*Hai-Dang Nguyen,Ha-Hieu Pham,Hao T. Nguyen,Huy-Hieu Pham*

Main category: cs.CV

TL;DR: VinDr-CXR-VQA is a large-scale chest X-ray dataset for medical visual question answering with spatial grounding, containing 17,597 QA pairs across 4,394 images with radiologist-verified bounding boxes and clinical reasoning explanations.


<details>
  <summary>Details</summary>
Motivation: To advance explainable medical visual question answering with clinical grounding and mitigate hallucinations in normal cases by providing a balanced dataset with spatial annotations.

Method: Created a dataset with six diagnostic question types (Where, What, Is there, How many, Which, Yes/No) across 4,394 chest X-ray images, balanced with 41.7% positive and 58.3% negative samples, and benchmarked using MedGemma-4B-it model.

Result: Benchmarking showed improved performance with F1 score of 0.624 (11.8% improvement over baseline) while enabling lesion localization capabilities.

Conclusion: VinDr-CXR-VQA enables reproducible and clinically grounded medical VQA research, with the dataset and evaluation tools publicly available for community use.

Abstract: We present VinDr-CXR-VQA, a large-scale chest X-ray dataset for explainable
Medical Visual Question Answering (Med-VQA) with spatial grounding. The dataset
contains 17,597 question-answer pairs across 4,394 images, each annotated with
radiologist-verified bounding boxes and clinical reasoning explanations. Our
question taxonomy spans six diagnostic types-Where, What, Is there, How many,
Which, and Yes/No-capturing diverse clinical intents. To improve reliability,
we construct a balanced distribution of 41.7% positive and 58.3% negative
samples, mitigating hallucinations in normal cases. Benchmarking with
MedGemma-4B-it demonstrates improved performance (F1 = 0.624, +11.8% over
baseline) while enabling lesion localization. VinDr-CXR-VQA aims to advance
reproducible and clinically grounded Med-VQA research. The dataset and
evaluation tools are publicly available at
huggingface.co/datasets/Dangindev/VinDR-CXR-VQA.

</details>


### [149] [OmniTrack++: Omnidirectional Multi-Object Tracking by Learning Large-FoV Trajectory Feedback](https://arxiv.org/abs/2511.00510)
*Kai Luo,Hao Shi,Kunyu Peng,Fei Teng,Sheng Wu,Kaiwei Wang,Kailun Yang*

Main category: cs.CV

TL;DR: OmniTrack++ is a feedback-driven framework for panoramic Multi-Object Tracking that addresses 360 FoV challenges through stabilized features, trajectory-informed association, expert memory for long-term robustness, and adaptive tracklet management.


<details>
  <summary>Details</summary>
Motivation: Conventional MOT methods designed for narrow-FoV pinhole cameras perform poorly in panoramic imagery due to 360 FoV, resolution dilution, and severe view-dependent distortions.

Method: Uses DynamicSSM block to stabilize panoramic features, FlexiTrack Instances for trajectory-informed association, ExpertTrack Memory with Mixture-of-Experts for appearance consolidation, and adaptive Tracklet Management that switches between tracking modes.

Result: Achieves state-of-the-art performance with substantial HOTA improvements: +25.5% on JRDB and +43.07% on QuadTrack over original OmniTrack.

Conclusion: OmniTrack++ provides an effective solution for panoramic MOT challenges and establishes EmboTrack benchmark for rigorous evaluation in real-world panoramic perception scenarios.

Abstract: This paper investigates Multi-Object Tracking (MOT) in panoramic imagery,
which introduces unique challenges including a 360{\deg} Field of View (FoV),
resolution dilution, and severe view-dependent distortions. Conventional MOT
methods designed for narrow-FoV pinhole cameras generalize unsatisfactorily
under these conditions. To address panoramic distortion, large search space,
and identity ambiguity under a 360{\deg} FoV, OmniTrack++ adopts a
feedback-driven framework that progressively refines perception with trajectory
cues. A DynamicSSM block first stabilizes panoramic features, implicitly
alleviating geometric distortion. On top of normalized representations,
FlexiTrack Instances use trajectory-informed feedback for flexible localization
and reliable short-term association. To ensure long-term robustness, an
ExpertTrack Memory consolidates appearance cues via a Mixture-of-Experts
design, enabling recovery from fragmented tracks and reducing identity drift.
Finally, a Tracklet Management module adaptively switches between end-to-end
and tracking-by-detection modes according to scene dynamics, offering a
balanced and scalable solution for panoramic MOT. To support rigorous
evaluation, we establish the EmboTrack benchmark, a comprehensive dataset for
panoramic MOT that includes QuadTrack, captured with a quadruped robot, and
BipTrack, collected with a bipedal wheel-legged robot. Together, these datasets
span wide-angle environments and diverse motion patterns, providing a
challenging testbed for real-world panoramic perception. Extensive experiments
on JRDB and EmboTrack demonstrate that OmniTrack++ achieves state-of-the-art
performance, yielding substantial HOTA improvements of +25.5% on JRDB and
+43.07% on QuadTrack over the original OmniTrack. Datasets and code will be
made publicly available at https://github.com/xifen523/OmniTrack.

</details>


### [150] [ID-Composer: Multi-Subject Video Synthesis with Hierarchical Identity Preservation](https://arxiv.org/abs/2511.00511)
*Panwang Pan,Jingjing Zhao,Yuchen Lin,Chenguo Lin,Chenxin Li,Haopeng Li,Honglei Yan,Tingting Shen,Yadong Mu*

Main category: cs.CV

TL;DR: ID-Composer is a framework for multi-subject video generation from text prompts and reference images, using hierarchical identity-preserving attention, VLM semantic understanding, and reinforcement learning to improve identity preservation and video quality.


<details>
  <summary>Details</summary>
Motivation: Existing video generative models are limited to text or single image conditioning, lacking controllability for multi-subject scenarios where preserving subject identities and integrating semantics across subjects is crucial.

Method: Uses hierarchical identity-preserving attention mechanism, semantic understanding via pretrained VLM for fine-grained guidance, and online reinforcement learning phase (RLVR) to align critical concepts like subject ID.

Result: Extensive experiments show the model surpasses existing methods in identity preservation, temporal consistency, and video quality.

Conclusion: ID-Composer effectively addresses multi-subject video generation challenges through its novel architecture combining attention mechanisms, VLM guidance, and reinforcement learning.

Abstract: Video generative models pretrained on large-scale datasets can produce
high-quality videos, but are often conditioned on text or a single image,
limiting controllability and applicability. We introduce ID-Composer, a novel
framework that addresses this gap by tackling multi-subject video generation
from a text prompt and reference images. This task is challenging as it
requires preserving subject identities, integrating semantics across subjects
and modalities, and maintaining temporal consistency. To faithfully preserve
the subject consistency and textual information in synthesized videos,
ID-Composer designs a \textbf{hierarchical identity-preserving attention
mechanism}, which effectively aggregates features within and across subjects
and modalities. To effectively allow for the semantic following of user
intention, we introduce \textbf{semantic understanding via pretrained
vision-language model (VLM)}, leveraging VLM's superior semantic understanding
to provide fine-grained guidance and capture complex interactions between
multiple subjects. Considering that standard diffusion loss often fails in
aligning the critical concepts like subject ID, we employ an \textbf{online
reinforcement learning phase} to drive the overall training objective of
ID-Composer into RLVR. Extensive experiments demonstrate that our model
surpasses existing methods in identity preservation, temporal consistency, and
video quality.

</details>


### [151] [SegDebias: Test-Time Bias Mitigation for ViT-Based CLIP via Segmentation](https://arxiv.org/abs/2511.00523)
*Fangyu Wu,Yujun Cai*

Main category: cs.CV

TL;DR: A test-time debiasing method for CLIP models that uses segmentation to isolate target attributes and adjust non-target regions to remove spurious correlations without requiring training data or bias annotations.


<details>
  <summary>Details</summary>
Motivation: Existing debiasing methods require training data and group labels, limiting real-world applicability. Test-time methods often need prior knowledge of dataset biases, reducing generalizability in open-set settings.

Method: Uses pretrained segmentation model to isolate target visual attribute, then adjusts non-target regions so their embeddings are uniformly similar to all class-specific text prompts, removing unintended bias signals while preserving target attributes.

Result: Outperforms existing test-time debiasing approaches on Waterbirds and CelebA datasets in both group robustness metrics and Attention IoU.

Conclusion: Segmentation-guided interventions are effective for scalable and annotation-free bias mitigation in vision language models.

Abstract: Vision language models such as CLIP have shown remarkable performance in zero
shot classification, but remain susceptible to spurious correlations, where
irrelevant visual features influence predictions. Existing debiasing methods
often require access to training data and explicit group labels to perform
fine-tuning or adjust embeddings, which limits their practicality in real-world
settings. Test-time methods attempt to avoid this constraint, but many still
depend on prior knowledge of dataset specific biases, limiting their
generalizability in open set settings. In this work, we propose a test-time
debiasing method for ViT based CLIP models that requires no additional training
or assumptions of bias annotations. Our approach uses a pretrained segmentation
model to isolate the target visual attribute, then adjusts the non target
regions so that their embeddings are uniformly similar to all class specific
text prompts. This procedure removes unintended bias signals from confounding
visual regions while preserving the target attribute. Experiments on Waterbirds
and CelebA show that our method outperforms existing test-time debiasing
approaches in both group robustness metrics and Attention IoU. These results
demonstrate the effectiveness of segmentation guided interventions for scalable
and annotation free bias mitigation in vision language models.

</details>


### [152] [Text-guided Fine-Grained Video Anomaly Detection](https://arxiv.org/abs/2511.00524)
*Jihao Gu,Kun Li,He Wang,Kaan Akit*

Main category: cs.CV

TL;DR: T-VAD is a text-guided fine-grained video anomaly detection framework using Large Vision-Language Models that generates pixel-level anomaly heatmaps and provides detailed textual descriptions of anomalies.


<details>
  <summary>Details</summary>
Motivation: Existing video anomaly detection methods are semi-automated with limited binary outputs (normal/anomalous), lacking fine-grained analysis and interactivity for applications like surveillance and industrial monitoring.

Method: Proposes T-VAD framework with Anomaly Heatmap Decoder for pixel-wise visual-textual feature alignment to generate fine-grained anomaly heatmaps, and Region-aware Anomaly Encoder to transform heatmaps into learnable textual embeddings for LVLM guidance.

Result: Achieves SOTA performance with 94.8% AUC on UBnormal dataset, 67.8%/76.7% accuracy in anomaly heatmaps, and high BLEU-4 scores (62.67-88.84) and Yes/No accuracy (89.73%-97.67%) for textual descriptions across datasets.

Conclusion: T-VAD significantly enhances anomaly detection granularity and interactivity by combining fine-grained heatmap generation with detailed textual descriptions through LVLM integration.

Abstract: Video Anomaly Detection (VAD) aims to identify anomalous events within video
segments. In scenarios such as surveillance or industrial process monitoring,
anomaly detection is of critical importance. While existing approaches are
semi-automated, requiring human assessment for anomaly detection, traditional
VADs offer limited output as either normal or anomalous. We propose Text-guided
Fine-Grained Video Anomaly Detection (T-VAD), a framework built upon Large
Vision-Language Model (LVLM). T-VAD introduces an Anomaly Heatmap Decoder (AHD)
that performs pixel-wise visual-textual feature alignment to generate
fine-grained anomaly heatmaps. Furthermore, we design a Region-aware Anomaly
Encoder (RAE) that transforms the heatmaps into learnable textual embeddings,
guiding the LVLM to accurately identify and localize anomalous events in
videos. This significantly enhances both the granularity and interactivity of
anomaly detection. The proposed method achieving SOTA performance by
demonstrating 94.8% Area Under the Curve (AUC, specifically micro-AUC) and
67.8%/76.7% accuracy in anomaly heatmaps (RBDC/TBDC) on the UBnormal dataset,
and subjectively verified more preferable textual description on the
ShanghaiTech-based dataset (BLEU-4: 62.67 for targets, 88.84 for trajectories;
Yes/No accuracy: 97.67%), and on the UBnormal dataset (BLEU-4: 50.32 for
targets, 78.10 for trajectories; Yes/No accuracy: 89.73%).

</details>


### [153] [Real-IAD Variety: Pushing Industrial Anomaly Detection Dataset to a Modern Era](https://arxiv.org/abs/2511.00540)
*Wenbing Zhu,Chengjie Wang,Bin-Bin Gao,Jiangning Zhang,Guannan Jiang,Jie Hu,Zhenye Gan,Lidong Wang,Ziqing Zhou,Linjie Cheng,Yurui Pan,Bo Peng,Mingmin Chi,Lizhuang Ma*

Main category: cs.CV

TL;DR: Real-IAD Variety is introduced as the largest and most diverse industrial anomaly detection benchmark with 198,960 images across 160 categories, addressing limitations of existing datasets and demonstrating vision-language models' robustness to category scaling.


<details>
  <summary>Details</summary>
Motivation: Current IAD benchmarks have restricted category diversity and insufficient scale, leading to metric saturation and limited model transferability to real-world industrial scenarios.

Method: Created Real-IAD Variety benchmark with comprehensive coverage across 28 industries, 24 material types, and 22 color variations, then evaluated state-of-the-art methods across multi-class unsupervised, multi-view, and zero-/few-shot settings.

Result: Multi-class unsupervised methods showed significant performance degradation when scaled from 30 to 160 categories, while vision-language models exhibited remarkable robustness with minimal performance variation across different category counts.

Conclusion: Real-IAD Variety serves as an essential resource for training next-generation foundation models, enabling development of scalable, general-purpose anomaly detection systems beyond domain-specific constraints.

Abstract: Industrial Anomaly Detection (IAD) is critical for enhancing operational
safety, ensuring product quality, and optimizing manufacturing efficiency
across global industries. However, the IAD algorithms are severely constrained
by the limitations of existing public benchmarks. Current datasets exhibit
restricted category diversity and insufficient scale, frequently resulting in
metric saturation and limited model transferability to real-world scenarios. To
address this gap, we introduce Real-IAD Variety, the largest and most diverse
IAD benchmark, comprising 198,960 high-resolution images across 160 distinct
object categories. Its diversity is ensured through comprehensive coverage of
28 industries, 24 material types, and 22 color variations. Our comprehensive
experimental analysis validates the benchmark's substantial challenge:
state-of-the-art multi-class unsupervised anomaly detection methods experience
significant performance degradation when scaled from 30 to 160 categories.
Crucially, we demonstrate that vision-language models exhibit remarkable
robustness to category scale-up, with minimal performance variation across
different category counts, significantly enhancing generalization capabilities
in diverse industrial contexts. The unprecedented scale and complexity of
Real-IAD Variety position it as an essential resource for training and
evaluating next-generation foundation models for anomaly detection. By
providing this comprehensive benchmark with rigorous evaluation protocols
across multi-class unsupervised, multi-view, and zero-/few-shot settings, we
aim to accelerate research beyond domain-specific constraints, enabling the
development of scalable, general-purpose anomaly detection systems. Real-IAD
Variety will be made publicly available to facilitate innovation in this
critical field.

</details>


### [154] [MIFO: Learning and Synthesizing Multi-Instance from One Image](https://arxiv.org/abs/2511.00542)
*Kailun Su,Ziqi He,Xi Wang,Yang Zhou*

Main category: cs.CV

TL;DR: A method for precise learning and synthesizing multi-instance semantics from single images using penalty-based attention optimization and box control to handle similar semantics and appearance.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of limited training data and difficulty in learning instances with similar semantics or appearance from single images.

Method: Proposes penalty-based attention optimization to disentangle similar semantics during learning, and introduces box control in attention layers during synthesis to mitigate semantic leakage and control output layout.

Result: Achieves disentangled and high-quality semantic learning and synthesis, balancing editability and instance consistency. Robust with semantically/visually similar instances or rare objects.

Conclusion: The method effectively handles multi-instance semantic learning from single images, maintaining robustness even with challenging cases of similar semantics or rare objects.

Abstract: This paper proposes a method for precise learning and synthesizing
multi-instance semantics from a single image. The difficulty of this problem
lies in the limited training data, and it becomes even more challenging when
the instances to be learned have similar semantics or appearance. To address
this, we propose a penalty-based attention optimization to disentangle similar
semantics during the learning stage. Then, in the synthesis, we introduce and
optimize box control in attention layers to further mitigate semantic leakage
while precisely controlling the output layout. Experimental results demonstrate
that our method achieves disentangled and high-quality semantic learning and
synthesis, strikingly balancing editability and instance consistency. Our
method remains robust when dealing with semantically or visually similar
instances or rare-seen objects. The code is publicly available at
https://github.com/Kareneveve/MIFO

</details>


### [155] [4D Neural Voxel Splatting: Dynamic Scene Rendering with Voxelized Guassian Splatting](https://arxiv.org/abs/2511.00560)
*Chun-Tin Wu,Jun-Cheng Chen*

Main category: cs.CV

TL;DR: 4D Neural Voxel Splatting (4D-NVS) combines voxel-based representations with neural Gaussian splatting to efficiently model dynamic scenes, reducing memory overhead and accelerating training while maintaining high image quality.


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting (3D-GS) achieves efficient rendering for novel view synthesis but suffers from substantial memory overhead when extended to dynamic scenes due to replicating Gaussians across frames.

Method: Uses a compact set of neural voxels with learned deformation fields instead of separate Gaussian sets per timestamp, and introduces a novel view refinement stage that selectively improves challenging viewpoints through targeted optimization.

Result: Outperforms state-of-the-art approaches with significant memory reduction and faster training, enabling real-time rendering with superior visual fidelity.

Conclusion: 4D-NVS provides an efficient solution for dynamic scene modeling that reduces memory consumption while preserving high rendering quality.

Abstract: Although 3D Gaussian Splatting (3D-GS) achieves efficient rendering for novel
view synthesis, extending it to dynamic scenes still results in substantial
memory overhead from replicating Gaussians across frames. To address this
challenge, we propose 4D Neural Voxel Splatting (4D-NVS), which combines
voxel-based representations with neural Gaussian splatting for efficient
dynamic scene modeling. Instead of generating separate Gaussian sets per
timestamp, our method employs a compact set of neural voxels with learned
deformation fields to model temporal dynamics. The design greatly reduces
memory consumption and accelerates training while preserving high image
quality. We further introduce a novel view refinement stage that selectively
improves challenging viewpoints through targeted optimization, maintaining
global efficiency while enhancing rendering quality for difficult viewing
angles. Experiments demonstrate that our method outperforms state-of-the-art
approaches with significant memory reduction and faster training, enabling
real-time rendering with superior visual fidelity.

</details>


### [156] [Generalized Category Discovery under Domain Shift: A Frequency Domain Perspective](https://arxiv.org/abs/2511.00573)
*Wei Feng,Zongyuan Ge*

Main category: cs.CV

TL;DR: Proposes FREE framework for Domain-Shifted Generalized Category Discovery (DS_GCD) using frequency-domain analysis to handle distribution shifts in unlabeled data containing both known/unknown categories and domains.


<details>
  <summary>Details</summary>
Motivation: Existing GCD methods perform poorly under distribution shifts where unlabeled data comes from unknown domains, creating a more realistic but challenging scenario.

Method: Uses frequency-based domain separation via amplitude differences, cross-domain and intra-domain frequency perturbation strategies, extended self-supervised contrastive learning, and clustering-difficulty-aware resampling.

Result: Extensive experiments show FREE effectively mitigates distributional shifts and achieves superior performance in discovering both known and unknown categories across various benchmarks.

Conclusion: The FREE framework successfully addresses the DS_GCD challenge by leveraging frequency-domain information and adaptive strategies to handle distribution shifts in realistic scenarios.

Abstract: Generalized Category Discovery (GCD) aims to leverage labeled samples from
known categories to cluster unlabeled data that may include both known and
unknown categories. While existing methods have achieved impressive results
under standard conditions, their performance often deteriorates in the presence
of distribution shifts. In this paper, we explore a more realistic task:
Domain-Shifted Generalized Category Discovery (DS\_GCD), where the unlabeled
data includes not only unknown categories but also samples from unknown
domains. To tackle this challenge, we propose a
\textbf{\underline{F}}requency-guided Gene\textbf{\underline{r}}alized
Cat\textbf{\underline{e}}gory Discov\textbf{\underline{e}}ry framework (FREE)
that enhances the model's ability to discover categories under distributional
shift by leveraging frequency-domain information. Specifically, we first
propose a frequency-based domain separation strategy that partitions samples
into known and unknown domains by measuring their amplitude differences. We
then propose two types of frequency-domain perturbation strategies: a
cross-domain strategy, which adapts to new distributions by exchanging
amplitude components across domains, and an intra-domain strategy, which
enhances robustness to intra-domain variations within the unknown domain.
Furthermore, we extend the self-supervised contrastive objective and semantic
clustering loss to better guide the training process. Finally, we introduce a
clustering-difficulty-aware resampling technique to adaptively focus on
harder-to-cluster categories, further enhancing model performance. Extensive
experiments demonstrate that our method effectively mitigates the impact of
distributional shifts across various benchmark datasets and achieves superior
performance in discovering both known and unknown categories.

</details>


### [157] [TRACES: Temporal Recall with Contextual Embeddings for Real-Time Video Anomaly Detection](https://arxiv.org/abs/2511.00580)
*Yousuf Ahmed Siddiqui,Sufiyaan Usmani,Umer Tariq,Jawwad Ahmed Shamsi,Muhammad Burhan Khan*

Main category: cs.CV

TL;DR: A memory-augmented pipeline for context-aware zero-shot anomaly detection that fuses temporal and appearance features with textual memory traces, achieving state-of-the-art performance on benchmark datasets with real-time inference capability.


<details>
  <summary>Details</summary>
Motivation: Video anomalies are context-dependent and current detectors lack contextual awareness, limiting their generalization to real-life situations where the same action can be normal in one context but anomalous in another.

Method: Memory-augmented pipeline using cross-attention to correlate temporal signals with visual embeddings, and real-time zero-shot anomaly classification through contextual similarity scoring.

Result: Achieved 90.4% AUC on UCF-Crime and 83.67% AP on XD-Violence, setting new state-of-the-art among zero-shot models with real-time inference and high precision.

Conclusion: Fusing cross-attention temporal fusion with contextual memory enables high-fidelity anomaly detection, advancing the applicability of zero-shot models in real-world surveillance and monitoring systems.

Abstract: Video anomalies often depend on contextual information available and temporal
evolution. Non-anomalous action in one context can be anomalous in some other
context. Most anomaly detectors, however, do not notice this type of context,
which seriously limits their capability to generalize to new, real-life
situations. Our work addresses the context-aware zero-shot anomaly detection
challenge, in which systems need to learn adaptively to detect new events by
correlating temporal and appearance features with textual traces of memory in
real time. Our approach defines a memory-augmented pipeline, correlating
temporal signals with visual embeddings using cross-attention, and real-time
zero-shot anomaly classification by contextual similarity scoring. We achieve
90.4\% AUC on UCF-Crime and 83.67\% AP on XD-Violence, a new state-of-the-art
among zero-shot models. Our model achieves real-time inference with high
precision and explainability for deployment. We show that, by fusing
cross-attention temporal fusion and contextual memory, we achieve high fidelity
anomaly detection, a step towards the applicability of zero-shot models in
real-world surveillance and infrastructure monitoring.

</details>


### [158] [CueBench: Advancing Unified Understanding of Context-Aware Video Anomalies in Real-World](https://arxiv.org/abs/2511.00613)
*Yating Yu,Congqi Cao,Zhaoying Wang,Weihua Meng,Jie Li,Yuxin Li,Zihao Wei,Zhongpei Shen,Jiajun Zhang*

Main category: cs.CV

TL;DR: CueBench is a new benchmark for context-aware video anomaly understanding that evaluates models on recognizing subtle anomalies defined by complex contexts and conditions, showing current models perform poorly while the proposed Cue-R1 method achieves significant improvements.


<details>
  <summary>Details</summary>
Motivation: Current video anomaly detection methods have superficial understanding of real-world anomalies, lacking the ability to comprehend complex principles and subtle contextual differences that distinguish anomalies from normal events.

Method: Proposed CueBench benchmark with hierarchical taxonomy of 14 conditional and 18 absolute anomaly events across 174 scenes and 198 attributes, and developed Cue-R1 using R1-style reinforcement fine-tuning with verifiable, task-aligned, and hierarchy-refined rewards.

Result: Extensive evaluation shows existing vision-language models perform poorly on real-world anomaly understanding, while Cue-R1 surpasses state-of-the-art approaches by over 24% on average across various tasks.

Conclusion: Current deep models are still far from satisfactory real-world video anomaly understanding, but the proposed Cue-R1 method demonstrates significant improvements in context-aware anomaly comprehension through unified generative approach with refined rewards.

Abstract: How far are deep models from real-world video anomaly understanding (VAU)?
Current works typically emphasize on detecting unexpected occurrences deviated
from normal patterns or comprehending anomalous events with interpretable
descriptions. However, they exhibit only a superficial comprehension of
real-world anomalies, with limited breadth in complex principles and subtle
context that distinguish the anomalies from normalities, e.g., climbing cliffs
with safety gear vs. without it. To this end, we introduce CueBench, the first
of its kind Benchmark, devoted to Context-aware video anomalies within a
Unified Evaluation framework. We comprehensively establish an event-centric
hierarchical taxonomy that anchors two core event types: 14 conditional and 18
absolute anomaly events, defined by their refined semantics from diverse
contexts across 174 scenes and 198 attributes. Based on this, we propose to
unify and benchmark context-aware VAU with various challenging tasks across
recognition, temporal grounding, detection, and anticipation. This also serves
as a rigorous and fair probing evaluation suite for generative-discriminative
as well as generalized-specialized vision-language models (VLMs). To address
the challenges underlying CueBench, we further develop Cue-R1 based on R1-style
reinforcement fine-tuning with verifiable, task-aligned, and hierarchy-refined
rewards in a unified generative manner. Extensive results on CueBench reveal
that, existing VLMs are still far from satisfactory real-world anomaly
understanding, while our Cue-R1 surpasses these state-of-the-art approaches by
over 24% on average.

</details>


### [159] [Grounding Surgical Action Triplets with Instrument Instance Segmentation: A Dataset and Target-Aware Fusion Approach](https://arxiv.org/abs/2511.00643)
*Oluwatosin Alabi,Meng Wei,Charlie Budd,Tom Vercauteren,Miaojing Shi*

Main category: cs.CV

TL;DR: The paper introduces triplet segmentation, a new task for spatially grounding surgical action triplets (<instrument, verb, target>) using instrument instance segmentation, and proposes TargetFusionNet architecture with target-aware fusion to improve anatomical target prediction.


<details>
  <summary>Details</summary>
Motivation: Existing surgical action recognition methods lack spatial grounding precision and fail to reliably link actions to specific instrument instances, limiting detailed instrument-tissue interaction analysis.

Method: Proposed TargetFusionNet architecture that extends Mask2Former with a target-aware fusion mechanism, combining weak anatomy priors with instrument instance queries for accurate anatomical target prediction.

Result: TargetFusionNet consistently improves performance across recognition, detection, and triplet segmentation metrics, demonstrating significant enhancement in accuracy and robustness of surgical action understanding.

Conclusion: Triplet segmentation establishes a unified framework for spatially grounding surgical action triplets, and the proposed benchmark and architecture pave the way for more interpretable surgical scene understanding.

Abstract: Understanding surgical instrument-tissue interactions requires not only
identifying which instrument performs which action on which anatomical target,
but also grounding these interactions spatially within the surgical scene.
Existing surgical action triplet recognition methods are limited to learning
from frame-level classification, failing to reliably link actions to specific
instrument instances.Previous attempts at spatial grounding have primarily
relied on class activation maps, which lack the precision and robustness
required for detailed instrument-tissue interaction analysis.To address this
gap, we propose grounding surgical action triplets with instrument instance
segmentation, or triplet segmentation for short, a new unified task which
produces spatially grounded <instrument, verb, target> outputs.We start by
presenting CholecTriplet-Seg, a large-scale dataset containing over 30,000
annotated frames, linking instrument instance masks with action verb and
anatomical target annotations, and establishing the first benchmark for
strongly supervised, instance-level triplet grounding and evaluation.To learn
triplet segmentation, we propose TargetFusionNet, a novel architecture that
extends Mask2Former with a target-aware fusion mechanism to address the
challenge of accurate anatomical target prediction by fusing weak anatomy
priors with instrument instance queries.Evaluated across recognition,
detection, and triplet segmentation metrics, TargetFusionNet consistently
improves performance over existing baselines, demonstrating that strong
instance supervision combined with weak target priors significantly enhances
the accuracy and robustness of surgical action understanding.Triplet
segmentation establishes a unified framework for spatially grounding surgical
action triplets. The proposed benchmark and architecture pave the way for more
interpretable, surgical scene understanding.

</details>


### [160] [Benchmarking individual tree segmentation using multispectral airborne laser scanning data: the FGI-EMIT dataset](https://arxiv.org/abs/2511.00653)
*Lassi Ruoppa,Tarmo Hietala,Verneri Seppnen,Josef Taher,Teemu Hakala,Xiaowei Yu,Antero Kukko,Harri Kaartinen,Juha Hyypp*

Main category: cs.CV

TL;DR: This paper introduces FGI-EMIT, the first large-scale multispectral LiDAR benchmark dataset for individual tree segmentation, and benchmarks both unsupervised and deep learning methods, finding DL approaches significantly outperform traditional methods, especially for understory trees.


<details>
  <summary>Details</summary>
Motivation: The lack of large-scale multispectral LiDAR benchmark datasets has hindered progress in individual tree segmentation, despite evidence that multispectral reflectance can improve accuracy. Current datasets are limited and don't adequately address small understory trees.

Method: Created FGI-EMIT dataset with 1,561 manually annotated trees captured at 532, 905, and 1,550 nm wavelengths. Benchmarking included four unsupervised algorithms (with Bayesian hyperparameter optimization) and four supervised deep learning approaches (trained from scratch).

Result: DL methods significantly outperformed unsupervised methods: ForestFormer3D achieved best F1-score of 73.3% vs Treeiso's 52.7%. The gap was largest for understory trees (25.9 percentage points difference). DL maintained superiority even at low point densities (10 points/m). Current DL methods fail to effectively leverage multispectral reflectance as additional features.

Conclusion: Deep learning approaches substantially outperform traditional unsupervised methods for individual tree segmentation, particularly for challenging understory trees. However, current DL architectures are not effectively utilizing multispectral reflectance information, indicating a need for improved model designs to leverage this valuable data source.

Abstract: Individual tree segmentation (ITS) from LiDAR point clouds is fundamental for
applications such as forest inventory, carbon monitoring and biodiversity
assessment. Traditionally, ITS has been achieved with unsupervised
geometry-based algorithms, while more recent advances have shifted toward
supervised deep learning (DL). In the past, progress in method development was
hindered by the lack of large-scale benchmark datasets, and the availability of
novel data formats, particularly multispectral (MS) LiDAR, remains limited to
this day, despite evidence that MS reflectance can improve the accuracy of ITS.
This study introduces FGI-EMIT, the first large-scale MS airborne laser
scanning benchmark dataset for ITS. Captured at wavelengths 532, 905, and 1,550
nm, the dataset consists of 1,561 manually annotated trees, with a particular
focus on small understory trees. Using FGI-EMIT, we comprehensively benchmarked
four conventional unsupervised algorithms and four supervised DL approaches.
Hyperparameters of unsupervised methods were optimized using a Bayesian
approach, while DL models were trained from scratch. Among the unsupervised
methods, Treeiso achieved the highest test set F1-score of 52.7%. The DL
approaches performed significantly better overall, with the best model,
ForestFormer3D, attaining an F1-score of 73.3%. The most significant difference
was observed in understory trees, where ForestFormer3D exceeded Treeiso by 25.9
percentage points. An ablation study demonstrated that current DL-based
approaches generally fail to leverage MS reflectance information when it is
provided as additional input features, although single channel reflectance can
improve accuracy marginally, especially for understory trees. A performance
analysis across point densities further showed that DL methods consistently
remain superior to unsupervised algorithms, even at densities as low as 10
points/m$^2$.

</details>


### [161] [Metadata-Aligned 3D MRI Representations for Contrast Understanding and Quality Control](https://arxiv.org/abs/2511.00681)
*Mehmet Yigit Avci,Pedro Borges,Virginia Fernandez,Paul Wright,Mehmet Yigitsoy,Sebastien Ourselin,Jorge Cardoso*

Main category: cs.CV

TL;DR: MR-CLIP learns MRI contrast representations by aligning volumetric images with DICOM acquisition parameters, enabling label-efficient analysis across diverse clinical datasets.


<details>
  <summary>Details</summary>
Motivation: MRI suffers from data heterogeneity and lack of standardized contrast labels across scanners, protocols, and institutions, limiting large-scale automated analysis.

Method: Metadata-guided framework that aligns volumetric images with DICOM acquisition parameters to learn unified MRI contrast representations.

Result: Embeddings show distinct MRI sequence clusters, outperform supervised 3D baselines in few-shot classification, and enable unsupervised quality control by identifying corrupted metadata.

Conclusion: MR-CLIP provides scalable foundation for label-efficient MRI analysis by transforming routinely available acquisition metadata into supervisory signal.

Abstract: Magnetic Resonance Imaging suffers from substantial data heterogeneity and
the absence of standardized contrast labels across scanners, protocols, and
institutions, which severely limits large-scale automated analysis. A unified
representation of MRI contrast would enable a wide range of downstream
utilities, from automatic sequence recognition to harmonization and quality
control, without relying on manual annotations. To this end, we introduce
MR-CLIP, a metadata-guided framework that learns MRI contrast representations
by aligning volumetric images with their DICOM acquisition parameters. The
resulting embeddings shows distinct clusters of MRI sequences and outperform
supervised 3D baselines under data scarcity in few-shot sequence
classification. Moreover, MR-CLIP enables unsupervised data quality control by
identifying corrupted or inconsistent metadata through image-metadata embedding
distances. By transforming routinely available acquisition metadata into a
supervisory signal, MR-CLIP provides a scalable foundation for label-efficient
MRI analysis across diverse clinical datasets.

</details>


### [162] [Outlier-Aware Post-Training Quantization for Image Super-Resolution](https://arxiv.org/abs/2511.00682)
*Hailing Wang,jianglin Lu,Yitian Zhang,Yun Fu*

Main category: cs.CV

TL;DR: Proposes a dual-region quantization strategy for image super-resolution networks that handles activation outliers and layer sensitivity to achieve PTQ performance comparable to QAT with significant speedup.


<details>
  <summary>Details</summary>
Motivation: Existing PTQ methods for SR networks fail due to overlooking activation outliers, which are correlated with image color information and cannot be directly removed without performance degradation.

Method: Uses dual-region quantization partitioning activations into outlier and dense regions with independent uniform quantization, plus sensitivity-aware finetuning that focuses on highly sensitive layers.

Result: Outperforms existing PTQ approaches across various SR networks and datasets, achieving performance comparable to QAT methods with at least 75x speedup.

Conclusion: The proposed dual-region quantization with sensitivity-aware finetuning effectively addresses PTQ challenges for SR networks, providing near-QAT performance with much faster inference.

Abstract: Quantization techniques, including quantization-aware training (QAT) and
post-training quantization (PTQ), have become essential for inference
acceleration of image super-resolution (SR) networks. Compared to QAT, PTQ has
garnered significant attention as it eliminates the need for ground truth and
model retraining. However, existing PTQ methods for SR often fail to achieve
satisfactory performance as they overlook the impact of outliers in activation.
Our empirical analysis reveals that these prevalent activation outliers are
strongly correlated with image color information, and directly removing them
leads to significant performance degradation. Motivated by this, we propose a
dual-region quantization strategy that partitions activations into an outlier
region and a dense region, applying uniform quantization to each region
independently to better balance bit-width allocation. Furthermore, we observe
that different network layers exhibit varying sensitivities to quantization,
leading to different levels of performance degradation. To address this, we
introduce sensitivity-aware finetuning that encourages the model to focus more
on highly sensitive layers, further enhancing quantization performance.
Extensive experiments demonstrate that our method outperforms existing PTQ
approaches across various SR networks and datasets, while achieving performance
comparable to QAT methods in most scenarios with at least a 75 speedup.

</details>


### [163] [Evolve to Inspire: Novelty Search for Diverse Image Generation](https://arxiv.org/abs/2511.00686)
*Alex Inch,Passawis Chaiyapattanaporn,Yuchen Zhu,Yuan Lu,Ting-Wen Ko,Davide Paglieri*

Main category: cs.CV

TL;DR: WANDER is a novelty search-based approach that uses LLMs and CLIP embeddings to generate diverse image sets from single prompts, outperforming existing methods in diversity.


<details>
  <summary>Details</summary>
Motivation: Text-to-image diffusion models often lack output diversity, limiting their use in creative and exploratory tasks, while existing prompt optimization methods focus on aesthetics rather than diversity.

Method: Uses LLM for semantic evolution of prompts, CLIP embeddings to quantify novelty, and emitters to guide search into distinct regions of prompt space.

Result: Significantly outperforms existing evolutionary prompt optimization baselines in diversity metrics, with ablation studies confirming emitter efficacy.

Conclusion: WANDER effectively addresses the diversity limitation in text-to-image generation through novelty search and semantic evolution.

Abstract: Text-to-image diffusion models, while proficient at generating high-fidelity
im- ages, often suffer from limited output diversity, hindering their
application in exploratory and ideation tasks. Existing prompt optimization
techniques typically target aesthetic fitness or are ill-suited to the creative
visual domain. To address this shortcoming, we introduce WANDER, a novelty
search-based approach to generating diverse sets of images from a single input
prompt. WANDER operates directly on natural language prompts, employing a Large
Language Model (LLM) for semantic evolution of diverse sets of images, and
using CLIP embeddings to quantify novelty. We additionally apply emitters to
guide the search into distinct regions of the prompt space, and demonstrate
that they boost the diversity of the generated images. Empirical evaluations
using FLUX-DEV for generation and GPT-4o-mini for mutation demonstrate that
WANDER significantly outperforms existing evolutionary prompt optimization
baselines in diversity metrics. Ablation studies confirm the efficacy of
emitters.

</details>


### [164] [Toward Better Optimization of Low-Dose CT Enhancement: A Critical Analysis of Loss Functions and Image Quality Assessment Metrics](https://arxiv.org/abs/2511.00698)
*Taifour Yousra,Beghdadi Azeddine,Marie Luong,Zuheng Ming*

Main category: cs.CV

TL;DR: Analysis of loss functions for low-dose CT image enhancement reveals inconsistencies between loss functions and image quality metrics, highlighting the need to consider perceptual quality metrics when developing new loss functions.


<details>
  <summary>Details</summary>
Motivation: Low-dose CT imaging suffers from noise and artifacts that affect diagnostic accuracy. While deep learning models have been developed with various loss functions, current metrics like PSNR and SSIM are limited in reflecting perceptual quality for medical images.

Method: Conducted objective analysis of different loss functions for LDCT image quality enhancement and their consistency with image quality metrics.

Result: Findings reveal inconsistencies between loss functions and quality metrics, showing that loss functions don't always align well with perceptual image quality.

Conclusion: There is a need to consider image quality metrics when developing new loss functions for image quality enhancement, particularly for medical imaging applications where perceptual quality is critical.

Abstract: Low-dose CT (LDCT) imaging is widely used to reduce radiation exposure to
mitigate high exposure side effects, but often suffers from noise and artifacts
that affect diagnostic accuracy. To tackle this issue, deep learning models
have been developed to enhance LDCT images. Various loss functions have been
employed, including classical approaches such as Mean Square Error and
adversarial losses, as well as customized loss functions(LFs) designed for
specific architectures. Although these models achieve remarkable performance in
terms of PSNR and SSIM, these metrics are limited in their ability to reflect
perceptual quality, especially for medical images. In this paper, we focus on
one of the most critical elements of DL-based architectures, namely the loss
function. We conduct an objective analysis of the relevance of different loss
functions for LDCT image quality enhancement and their consistency with image
quality metrics. Our findings reveal inconsistencies between LFs and quality
metrics, and highlight the need of consideration of image quality metrics when
developing a new loss function for image quality enhancement.

</details>


### [165] [Validating Deep Models for Alzheimer's 18F-FDG PET Diagnosis Across Populations: A Study with Latin American Data](https://arxiv.org/abs/2511.00728)
*Hugo Massaroli,Hernan Chaves,Pilar Anania,Mauricio Farez,Emmanuel Iarussi,Viviana Siless*

Main category: cs.CV

TL;DR: Deep learning models trained on North American ADNI data show significant performance drops when tested on Latin American FLENI cohort, revealing domain shift issues in Alzheimer's diagnosis.


<details>
  <summary>Details</summary>
Motivation: To assess generalization of AD diagnostic models to underrepresented populations beyond North American cohorts like ADNI.

Method: Benchmarked convolutional and Transformer-based models on ADNI dataset and tested generalization on Latin American FLENI cohort. Used ablation studies, per-image normalization, sampling selection, and occlusion sensitivity analysis.

Result: Models achieved high AUCs on ADNI (up to .96-.97) but dropped substantially on FLENI (down to .82-.80). No significant advantage for transformers over CNNs. Per-image normalization and sampling selection were key for generalization.

Conclusion: Population-aware validation is crucial for diagnostic AI models. Domain adaptation and cohort diversification needed for better generalization across different populations.

Abstract: Deep learning models have shown strong performance in diagnosing Alzheimer's
disease (AD) using neuroimaging data, particularly 18F-FDG PET scans, with
training datasets largely composed of North American cohorts such as those in
the Alzheimer's Disease Neuroimaging Initiative (ADNI). However, their
generalization to underrepresented populations remains underexplored. In this
study, we benchmark convolutional and Transformer-based models on the ADNI
dataset and assess their generalization performance on a novel Latin American
clinical cohort from the FLENI Institute in Buenos Aires, Argentina. We show
that while all models achieve high AUCs on ADNI (up to .96, .97), their
performance drops substantially on FLENI (down to .82, .80, respectively),
revealing a significant domain shift. The tested architectures demonstrated
similar performance, calling into question the supposed advantages of
transformers for this specific task. Through ablation studies, we identify
per-image normalization and a correct sampling selection as key factors for
generalization. Occlusion sensitivity analysis further reveals that models
trained on ADNI, generally attend to canonical hypometabolic regions for the AD
class, but focus becomes unclear for the other classes and for FLENI scans.
These findings highlight the need for population-aware validation of diagnostic
AI models and motivate future work on domain adaptation and cohort
diversification.

</details>


### [166] [Towards classification-based representation learning for place recognition on LiDAR scans](https://arxiv.org/abs/2511.00738)
*Dmitrii Khizbullin,Maksim Konoplia*

Main category: cs.CV

TL;DR: The paper proposes framing place recognition as multi-class classification instead of contrastive learning, using discrete location labels for LiDAR scans.


<details>
  <summary>Details</summary>
Motivation: Most existing place recognition methods use contrastive learning, but the authors want to explore an alternative approach that could offer better training efficiency and stability.

Method: Assign discrete location labels to LiDAR scans and train an encoder-decoder model to directly classify each scan's position.

Result: The method achieves competitive performance compared to contrastive learning-based methods on the NuScenes dataset.

Conclusion: The multi-class classification approach for place recognition is viable and offers advantages in training efficiency and stability over contrastive learning methods.

Abstract: Place recognition is a crucial task in autonomous driving, allowing vehicles
to determine their position using sensor data. While most existing methods rely
on contrastive learning, we explore an alternative approach by framing place
recognition as a multi-class classification problem. Our method assigns
discrete location labels to LiDAR scans and trains an encoder-decoder model to
classify each scan's position directly. We evaluate this approach on the
NuScenes dataset and show that it achieves competitive performance compared to
contrastive learning-based methods while offering advantages in training
efficiency and stability.

</details>


### [167] [Erasing 'Ugly' from the Internet: Propagation of the Beauty Myth in Text-Image Models](https://arxiv.org/abs/2511.00749)
*Tanvi Dinkar,Aiqi Jiang,Gavin Abercrombie,Ioannis Konstas*

Main category: cs.CV

TL;DR: This study reveals how generative AI models encode Western beauty standards, showing significant biases toward lighter skin tones, younger ages, and hypersexualization, particularly for non-binary individuals, while actively erasing 'ugly' features through negative prompting.


<details>
  <summary>Details</summary>
Motivation: To investigate how generative AI models encode and exaggerate Western beauty norms, potentially causing harm like body dysmorphia and negative self-image, especially among women and girls.

Method: Created two image generation pipelines (text-to-image and text-to-language-to-image), developed a structured beauty taxonomy, generated 5984 images using three language models and two text-to-image models, and conducted a Likert-scale study with women and non-binary social media users evaluating 1200 images.

Result: 86.5% of images depicted lighter skin tones, 22% contained explicit content despite SFW training, 74% were rated as younger demographic. Non-binary individuals were rated younger and more hypersexualized. Prompts with 'ugly' traits consistently produced higher NSFW ratings.

Conclusion: Generative AI models contain pervasive demographic biases related to beauty standards that are actively perpetuated by developers through practices like negative prompting, leading to pollution of data streams and erasure of features outside stereotypical beauty norms.

Abstract: Social media has exacerbated the promotion of Western beauty norms, leading
to negative self-image, particularly in women and girls, and causing harm such
as body dysmorphia. Increasingly content on the internet has been artificially
generated, leading to concerns that these norms are being exaggerated. The aim
of this work is to study how generative AI models may encode 'beauty' and erase
'ugliness', and discuss the implications of this for society. To investigate
these aims, we create two image generation pipelines: a text-to-image model and
a text-to-language model-to image model. We develop a structured beauty
taxonomy which we use to prompt three language models (LMs) and two
text-to-image models to cumulatively generate 5984 images using our two
pipelines. We then recruit women and non-binary social media users to evaluate
1200 of the images through a Likert-scale within-subjects study. Participants
show high agreement in their ratings. Our results show that 86.5% of generated
images depicted people with lighter skin tones, 22% contained explicit content
despite Safe for Work (SFW) training, and 74% were rated as being in a younger
age demographic. In particular, the images of non-binary individuals were rated
as both younger and more hypersexualised, indicating troubling intersectional
effects. Notably, prompts encoded with 'negative' or 'ugly' beauty traits (such
as "a wide nose") consistently produced higher Not SFW (NSFW) ratings
regardless of gender. This work sheds light on the pervasive demographic biases
related to beauty standards present in generative AI models -- biases that are
actively perpetuated by model developers, such as via negative prompting. We
conclude by discussing the implications of this on society, which include
pollution of the data streams and active erasure of features that do not fall
inside the stereotype of what is considered beautiful by developers.

</details>


### [168] [A Hybrid YOLOv5-SSD IoT-Based Animal Detection System for Durian Plantation Protection](https://arxiv.org/abs/2511.00777)
*Anis Suttan Shahrir,Zakiah Ayop,Syarulnaziah Anawar,Norulzahrah Mohd Zainudin*

Main category: cs.CV

TL;DR: An IoT-based animal detection system for durian plantations that combines YOLOv5 and SSD algorithms for improved accuracy, provides real-time monitoring with Telegram notifications, and triggers automated sound deterrents when animals are detected.


<details>
  <summary>Details</summary>
Motivation: Traditional farming practices are ineffective for monitoring animal intrusions in durian plantations, causing crop damage and financial losses. Current detection systems have limitations including dependence on single algorithms, poor notification platforms, and inadequate deterrent mechanisms.

Method: Integrated YOLOv5 and SSD object detection algorithms for improved accuracy, real-time monitoring with IoT technology, automated Telegram notifications to farmers, and triggered sound deterrents (e.g., tiger roar) upon animal detection.

Result: The YOLO+SSD model achieved detection accuracies of 90% for elephants, 85% for boars, and 70% for monkeys. Performance was highest during daytime and decreased at night, regardless of image type (still or video).

Conclusion: The study provides a comprehensive framework combining detection, notification, and deterrence, demonstrating practical automated farming solutions with potential for future innovations in agricultural monitoring systems.

Abstract: Durian plantation suffers from animal intrusions that cause crop damage and
financial loss. The traditional farming practices prove ineffective due to the
unavailability of monitoring without human intervention. The fast growth of
machine learning and Internet of Things (IoT) technology has led to new ways to
detect animals. However, current systems are limited by dependence on single
object detection algorithms, less accessible notification platforms, and
limited deterrent mechanisms. This research suggests an IoT-enabled animal
detection system for durian crops. The system integrates YOLOv5 and SSD object
detection algorithms to improve detection accuracy. The system provides
real-time monitoring, with detected intrusions automatically reported to
farmers via Telegram notifications for rapid response. An automated sound
mechanism (e.g., tiger roar) is triggered once the animal is detected. The
YOLO+SSD model achieved accuracy rates of elephant, boar, and monkey at 90%,
85% and 70%, respectively. The system shows the highest accuracy in daytime and
decreases at night, regardless of whether the image is still or a video.
Overall, this study contributes a comprehensive and practical framework that
combines detection, notification, and deterrence, paving the way for future
innovations in automated farming solutions.

</details>


### [169] [Class-agnostic 3D Segmentation by Granularity-Consistent Automatic 2D Mask Tracking](https://arxiv.org/abs/2511.00785)
*Juan Wang,Yasutomo Kawanishi,Tomo Miyazaki,Zhijie Wang,Shinichiro Omachi*

Main category: cs.CV

TL;DR: A method for 3D instance segmentation that uses granularity-consistent 2D mask tracking and curriculum learning to generate accurate 3D pseudo labels from 2D foundation models, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods for 3D instance segmentation rely on costly manual annotations or generate suboptimal pseudo labels by processing video frames independently, leading to inconsistent segmentation granularity and conflicting 3D pseudo labels that degrade accuracy.

Method: Introduces Granularity-Consistent automatic 2D Mask Tracking to maintain temporal correspondences across frames, combined with a three-stage curriculum learning framework that progressively trains from fragmented single-view data to unified multi-view annotations and globally coherent full-scene supervision.

Result: The method effectively generated consistent and accurate 3D segmentations, achieved state-of-the-art results on standard benchmarks, and demonstrated open-vocabulary ability.

Conclusion: The proposed approach enables robust distillation of consistent 3D representations from initially fragmented and contradictory 2D priors through structured progressive learning, significantly improving 3D instance segmentation quality.

Abstract: 3D instance segmentation is an important task for real-world applications. To
avoid costly manual annotations, existing methods have explored generating
pseudo labels by transferring 2D masks from foundation models to 3D. However,
this approach is often suboptimal since the video frames are processed
independently. This causes inconsistent segmentation granularity and
conflicting 3D pseudo labels, which degrades the accuracy of final
segmentation. To address this, we introduce a Granularity-Consistent automatic
2D Mask Tracking approach that maintains temporal correspondences across
frames, eliminating conflicting pseudo labels. Combined with a three-stage
curriculum learning framework, our approach progressively trains from
fragmented single-view data to unified multi-view annotations, ultimately
globally coherent full-scene supervision. This structured learning pipeline
enables the model to progressively expose to pseudo-labels of increasing
consistency. Thus, we can robustly distill a consistent 3D representation from
initially fragmented and contradictory 2D priors. Experimental results
demonstrated that our method effectively generated consistent and accurate 3D
segmentations. Furthermore, the proposed method achieved state-of-the-art
results on standard benchmarks and open-vocabulary ability.

</details>


### [170] [FedOnco-Bench: A Reproducible Benchmark for Privacy-Aware Federated Tumor Segmentation with Synthetic CT Data](https://arxiv.org/abs/2511.00795)
*Viswa Chaitanya Marella,Suhasnadh Reddy Veluru,Sai Teja Erukude*

Main category: cs.CV

TL;DR: FedOnco-Bench is a reproducible benchmark for privacy-aware federated learning using synthetic oncologic CT scans, evaluating the trade-off between segmentation performance and privacy leakage across different FL methods.


<details>
  <summary>Details</summary>
Motivation: Federated Learning enables collaborative model training while keeping sensitive data local, but remains vulnerable to membership-inference attacks and data heterogeneity, especially in privacy-sensitive medical environments.

Method: Developed FedOnco-Bench benchmark using synthetic oncologic CT scans with tumor annotations, evaluated four FL methods: FedAvg, FedProx, FedBN, and FedAvg with DP-SGD for segmentation performance and privacy leakage.

Result: Clear trade-off between privacy and utility: FedAvg achieved high performance (Dice ~0.85) but more privacy leakage (attack AUC ~0.72), while DP-SGD provided better privacy (AUC ~0.25) at accuracy cost (Dice ~0.79). FedProx and FedBN balanced performance under data heterogeneity.

Conclusion: FedOnco-Bench serves as a standardized, open-source platform for benchmarking and developing privacy-preserving FL methods for medical image segmentation, addressing the fundamental privacy-utility trade-off.

Abstract: Federated Learning (FL) allows multiple institutions to cooperatively train
machine learning models while retaining sensitive data at the source, which has
great utility in privacy-sensitive environments. However, FL systems remain
vulnerable to membership-inference attacks and data heterogeneity. This paper
presents FedOnco-Bench, a reproducible benchmark for privacy-aware FL using
synthetic oncologic CT scans with tumor annotations. It evaluates segmentation
performance and privacy leakage across FL methods: FedAvg, FedProx, FedBN, and
FedAvg with DP-SGD. Results show a distinct trade-off between privacy and
utility: FedAvg is high performance (Dice around 0.85) with more privacy
leakage (attack AUC about 0.72), while DP-SGD provides a higher level of
privacy (AUC around 0.25) at the cost of accuracy (Dice about 0.79). FedProx
and FedBN offer balanced performance under heterogeneous data, especially with
non-identical distributed client data. FedOnco-Bench serves as a standardized,
open-source platform for benchmarking and developing privacy-preserving FL
methods for medical image segmentation.

</details>


### [171] [Med-Banana-50K: A Cross-modality Large-Scale Dataset for Text-guided Medical Image Editing](https://arxiv.org/abs/2511.00801)
*Zhihui Chen,Mengling Feng*

Main category: cs.CV

TL;DR: Med-Banana-50K is a large-scale 50K-image dataset for instruction-based medical image editing across three modalities (chest X-ray, brain MRI, fundus photography) and 23 diseases, featuring systematic medical quality control and iterative refinement.


<details>
  <summary>Details</summary>
Motivation: Address the lack of large-scale, high-quality, openly accessible datasets specifically designed for medical image editing with anatomical and clinical constraints, which has constrained progress in multimodal medical AI research.

Method: Leverage Gemini-2.5-Flash-Image to generate bidirectional edits (lesion addition/removal) from real medical images, with systematic medical quality control using LLM-as-Judge with medically grounded rubric and history-aware iterative refinement up to five rounds.

Result: Created a comprehensive 50K-image dataset spanning three medical modalities and 23 disease types, including 37K failed attempts with conversation logs for preference learning and alignment research.

Conclusion: Med-Banana-50K establishes a foundation for training and evaluating next-generation medical image editing models by providing a large-scale, medically validated, and fully documented resource that addresses current dataset limitations.

Abstract: Recent advances in multimodal large language models have enabled remarkable
medical image editing capabilities. However, the research community's progress
remains constrained by the absence of large-scale, high-quality, and openly
accessible datasets built specifically for medical image editing with strict
anatomical and clinical constraints. We introduce Med-Banana-50K, a
comprehensive 50K-image dataset for instruction-based medical image editing
spanning three modalities (chest X-ray, brain MRI, fundus photography) and 23
disease types. Our dataset is constructed by leveraging Gemini-2.5-Flash-Image
to generate bidirectional edits (lesion addition and removal) from real medical
images. What distinguishes Med-Banana-50K from general-domain editing datasets
is our systematic approach to medical quality control: we employ LLM-as-Judge
with a medically grounded rubric (instruction compliance, structural
plausibility, realism, and fidelity preservation) and history-aware iterative
refinement up to five rounds. Beyond single-turn editing, Med-Banana-50K
includes 37K failed attempts with full conversation logs for preference
learning and alignment research. By providing this large-scale, medically
validated, and fully documented resource, Med-Banana-50K establishes a
foundation for training and evaluating the next generation of medical image
editing models.Our dataset and code are publicly available at
[https://github.com/richardChenzhihui/med-banana-50k].

</details>


### [172] [GUI-AIMA: Aligning Intrinsic Multimodal Attention with a Context Anchor for GUI Grounding](https://arxiv.org/abs/2511.00810)
*Shijie Zhou,Viet Dac Lai,Hao Tan,Jihyung Kil,Wanrong Zhu,Changyou Chen,Ruiyi Zhang*

Main category: cs.CV

TL;DR: GUI-AIMA is an attention-based, coordinate-free supervised fine-tuning framework for GUI grounding that aligns MLLMs' intrinsic multimodal attention with patch-wise grounding signals, achieving state-of-the-art performance with exceptional data efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing MLLM-based GUI grounding approaches that generate precise coordinates from visual inputs are challenging and computationally intensive. An intuitive approach is to first select relevant visual patches and then determine click locations within them.

Method: Proposes GUI-AIMA framework that aligns MLLMs' intrinsic multimodal attention with patch-wise grounding signals calculated adaptively via multi-head aggregation on simplified query-visual attention matrices. Uses coordinate-free approach that integrates plug-and-play zoom-in stage.

Result: GUI-AIMA-3B trained with only 85k screenshots achieves state-of-the-art performance among 3B models: 58.6% average accuracy on ScreenSpot-Pro and 62.2% on OSWorld-G, demonstrating exceptional data efficiency.

Conclusion: Light training can effectively trigger the native grounding capability of MLLMs, and GUI-AIMA provides an efficient coordinate-free approach for GUI grounding that outperforms existing methods while requiring minimal training data.

Abstract: Graphical user interface (GUI) grounding is a key function of computer-use
agents, which maps natural-language instructions to actionable screen regions.
Existing approaches based on Multimodal Large Language Models (MLLMs) typically
formulate it as a text-based coordinate generation task, yet directly
generating precise coordinates from visual inputs remains challenging and
computationally intensive. An intuitive way to implement GUI grounding is to
first select visual patches relevant to the instructions and then determine the
precise click location within those patches. Based on the observations that
general MLLMs have some native grounding capability, nested within their
attentions, we propose GUI-AIMA, an attention-based and coordinate-free
supervised fine-tuning framework for efficient GUI grounding. GUI-AIMA aligns
the intrinsic multimodal attention of MLLMs with patch-wise grounding signals.
These signals are calculated adaptively for diverse user instructions by
multi-head aggregation on simplified query-visual attention matrices. Besides,
its coordinate-free manner can easily integrate a plug-and-play zoom-in stage.
GUI-AIMA-3B was trained with only 85k screenshots, demonstrating exceptional
data efficiency and verifying that light training can trigger the native
grounding capability of MLLMs. It achieves state-of-the-art performance among
3B models, attaining an average accuracy of 58.6% on ScreenSpot-Pro and 62.2%
on OSWorld-G. Project page: https://github.com/sjz5202/GUI-AIMA

</details>


### [173] [TA-LSDiff:Topology-Aware Diffusion Guided by a Level Set Energy for Pancreas Segmentation](https://arxiv.org/abs/2511.00815)
*Yue Gou,Fanghui Song,Yuming Xing,Shengzhu Shi,Zhichang Guo,Boying Wu*

Main category: cs.CV

TL;DR: TA-LSDiff is a novel pancreas segmentation model that combines topology-aware diffusion probabilistic models with level set energy, achieving state-of-the-art accuracy without explicit geometric evolution.


<details>
  <summary>Details</summary>
Motivation: Pancreas segmentation is challenging due to small size, low contrast, and topological variations. Traditional level set methods ignore topological effects, while deep learning networks sacrifice structural details.

Method: Combines topology-aware diffusion probabilistic model with level set energy, using an energy function with four complementary terms. Includes pixel-adaptive refinement module for boundary precision.

Result: Achieves state-of-the-art accuracy on four public pancreas datasets, outperforming existing methods. Ablation studies validate each component's contribution.

Conclusion: TA-LSDiff establishes itself as a practical and accurate solution for pancreas segmentation, bridging the gap between geometric and learning-based approaches.

Abstract: Pancreas segmentation in medical image processing is a persistent challenge
due to its small size, low contrast against adjacent tissues, and significant
topological variations. Traditional level set methods drive boundary evolution
using gradient flows, often ignoring pointwise topological effects. Conversely,
deep learning-based segmentation networks extract rich semantic features but
frequently sacrifice structural details. To bridge this gap, we propose a novel
model named TA-LSDiff, which combined topology-aware diffusion probabilistic
model and level set energy, achieving segmentation without explicit geometric
evolution. This energy function guides implicit curve evolution by integrating
the input image and deep features through four complementary terms. To further
enhance boundary precision, we introduce a pixel-adaptive refinement module
that locally modulates the energy function using affinity weighting from
neighboring evidence. Ablation studies systematically quantify the contribution
of each proposed component. Evaluations on four public pancreas datasets
demonstrate that TA-LSDiff achieves state-of-the-art accuracy, outperforming
existing methods. These results establish TA-LSDiff as a practical and accurate
solution for pancreas segmentation.

</details>


### [174] [OMEGA: Optimized Multimodal Position Encoding Index Derivation with Global Adaptive Scaling for Vision-Language Models](https://arxiv.org/abs/2511.00821)
*Ruoxiang Huang,Xindian Ma,Rundong Kong,Zhen Yuan,Peng Zhang*

Main category: cs.CV

TL;DR: OMEGA is a novel position encoding framework for Vision-Language Models that uses modality-specific position encoding and adaptive step scaling to better handle the distinct structural properties of text and vision modalities.


<details>
  <summary>Details</summary>
Motivation: Current VLMs use unified position encoding strategies that treat text and visual tokens uniformly, ignoring their distinct structural properties - sequential continuity for text and spatial coherence for vision.

Method: OMEGA employs Modality-Specific Position Encoding (MSPE) to assign positional indices while preserving inherent structures of each modality, and Global Adaptive Encoding Step Scaling (GAESS) to adaptively adjust position encoding step size of visual tokens based on embedding entropy.

Result: OMEGA consistently enhances VLM performance across diverse architectures and VQA benchmarks, achieving up to 3.43% improvement on visual-intensive tasks with Qwen2.5-VL-3B, with consistent gains on larger models including Qwen2.5-VL-7B and LLaVA-v1.5-7B.

Conclusion: The proposed OMEGA framework effectively addresses the limitations of unified position encoding in VLMs by accounting for modality-specific structural properties and information density alignment.

Abstract: Vision-Language Models (VLMs) have demonstrated strong performance across
various multimodal tasks, where position encoding plays a vital role in
modeling both the sequential structure of textual information and the spatial
structure of visual information. However, current VLMs commonly adopt
modality-unified 1D or 2D positional indexing strategies, which treat textual
and visual tokens uniformly without accounting for their distinct structural
properties and sequential continuity for text and spatial coherence for vision.
To address this limitation, we propose OMEGA, a novel position encoding
framework that employs Modality-Specific Position Encoding (MSPE) to assign
positional indices while preserving the inherent structures of each modality
across separate coordinate dimensions. Additionally, to align the information
density of multimodal data in the positional index space, OMEGA introduces
Global Adaptive Encoding Step Scaling (GAESS), which adaptively adjusts the
position encoding step size of visual tokens based on the embedding entropy of
both modalities. Experimental results demonstrate that OMEGA consistently
enhances VLM performance across diverse architectures and VQA benchmarks. On
visual-intensive tasks, OMEGA achieves up to 3.43% improvement over baseline
position encoding strategies on Qwen2.5-VL-3B, with consistent gains observed
across larger models including Qwen2.5-VL-7B and LLaVA-v1.5-7B.

</details>


### [175] [Enhancing Adversarial Transferability in Visual-Language Pre-training Models via Local Shuffle and Sample-based Attack](https://arxiv.org/abs/2511.00831)
*Xin Liu,Aoyang Zhou,Aoyang Zhou*

Main category: cs.CV

TL;DR: LSSA is a novel attack method that enhances adversarial transferability in VLP models by randomly shuffling local image blocks and sampling around adversarial images to generate more diverse adversarial texts.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal adversarial attacks suffer from overfitting due to lack of input diversity, relying too much on information from one modality when attacking another.

Method: Randomly shuffles local image blocks to expand image-text pairs, generates adversarial images, samples around them, and uses both original and sampled images to create adversarial texts.

Result: Significantly enhances transferability of multimodal adversarial examples across diverse VLP models and downstream tasks, outperforming other advanced attacks on Large Vision-Language Models.

Conclusion: LSSA effectively addresses overfitting in multimodal adversarial attacks by increasing input diversity through local shuffling and sampling, achieving superior transferability performance.

Abstract: Visual-Language Pre-training (VLP) models have achieved significant
performance across various downstream tasks. However, they remain vulnerable to
adversarial examples. While prior efforts focus on improving the adversarial
transferability of multimodal adversarial examples through cross-modal
interactions, these approaches suffer from overfitting issues, due to a lack of
input diversity by relying excessively on information from adversarial examples
in one modality when crafting attacks in another. To address this issue, we
draw inspiration from strategies in some adversarial training methods and
propose a novel attack called Local Shuffle and Sample-based Attack (LSSA).
LSSA randomly shuffles one of the local image blocks, thus expanding the
original image-text pairs, generating adversarial images, and sampling around
them. Then, it utilizes both the original and sampled images to generate the
adversarial texts. Extensive experiments on multiple models and datasets
demonstrate that LSSA significantly enhances the transferability of multimodal
adversarial examples across diverse VLP models and downstream tasks. Moreover,
LSSA outperforms other advanced attacks on Large Vision-Language Models.

</details>


### [176] [Linear Differential Vision Transformer: Learning Visual Contrasts via Pairwise Differentials](https://arxiv.org/abs/2511.00833)
*Yifan Pu,Jixuan Ying,Qixiu Li,Tianzhu Ye,Dongchen Han,Xiaochen Wang,Ziyi Wang,Xinyu Shao,Gao Huang,Xiu Li*

Main category: cs.CV

TL;DR: VCA is a drop-in replacement for MHSA in Vision Transformers that reduces computational complexity from O(NC) to O(NnC) by using visual-contrast tokens and dual streams, improving both recognition and generation performance.


<details>
  <summary>Details</summary>
Motivation: MHSA in Vision Transformers performs quadratic query-key interactions for every token pair, spending most computation on visually weak or redundant correlations, which is inefficient.

Method: VCA distills each head's dense query field into spatially pooled visual-contrast tokens, then splits them into learnable positive and negative streams for differential interaction that highlights what separates regions.

Result: VCA lifts DeiT-Tiny accuracy on ImageNet-1K from 72.2% to 75.6% (+3.4%), improves hierarchical ViTs by up to 3.1%, and lowers FID-50K by 2.1-5.2 points in image generation tasks.

Conclusion: VCA offers a simple path towards faster and sharper Vision Transformers with minimal parameter overhead and no extra FLOPs, making it architecture-agnostic.

Abstract: Vision Transformers (ViTs) have become a universal backbone for both image
recognition and image generation. Yet their Multi-Head Self-Attention (MHSA)
layer still performs a quadratic query-key interaction for every token pair,
spending the bulk of computation on visually weak or redundant correlations. We
introduce Visual-Contrast Attention (VCA), a drop-in replacement for MHSA that
injects an explicit notion of discrimination while reducing the theoretical
complexity from O(N N C) to O(N n C) with n << N. VCA first distils each head's
dense query field into a handful of spatially pooled visual-contrast tokens,
then splits them into a learnable positive and negative stream whose
differential interaction highlights what truly separates one region from
another. The module adds fewer than 0.3M parameters to a DeiT-Tiny backbone,
requires no extra FLOPs, and is wholly architecture-agnostic. Empirically, VCA
lifts DeiT-Tiny top-1 accuracy on ImageNet-1K from 72.2% to 75.6% (+3.4) and
improves three strong hierarchical ViTs by up to 3.1%, while in
class-conditional ImageNet generation it lowers FID-50K by 2.1 to 5.2 points
across both diffusion (DiT) and flow (SiT) models. Extensive ablations confirm
that (i) spatial pooling supplies low-variance global cues, (ii) dual
positional embeddings are indispensable for contrastive reasoning, and (iii)
combining the two in both stages yields the strongest synergy. VCA therefore
offers a simple path towards faster and sharper Vision Transformers. The source
code is available at https://github.com/LeapLabTHU/LinearDiff.

</details>


### [177] [Parameter Interpolation Adversarial Training for Robust Image Classification](https://arxiv.org/abs/2511.00836)
*Xin Liu,Yichen Yang,Kun He,John E. Hopcroft*

Main category: cs.CV

TL;DR: PIAT is a novel adversarial training framework that interpolates model parameters between epochs to reduce oscillations and overfitting, achieving better robustness for CNNs and ViTs.


<details>
  <summary>Details</summary>
Motivation: Existing adversarial training methods suffer from oscillations and overfitting during training, which degrades defense efficacy against adversarial attacks.

Method: Parameter Interpolation Adversarial Training (PIAT) tunes model parameters by interpolating between previous and current epoch parameters, and uses Normalized Mean Square Error (NMSE) to align relative logit magnitudes between clean and adversarial examples.

Result: Extensive experiments on benchmark datasets show PIAT prominently improves robustness for both CNNs and Vision Transformers.

Conclusion: PIAT effectively addresses oscillation and overfitting issues in adversarial training, achieving higher model robustness through parameter interpolation and logit alignment.

Abstract: Though deep neural networks exhibit superior performance on various tasks,
they are still plagued by adversarial examples. Adversarial training has been
demonstrated to be the most effective method to defend against adversarial
attacks. However, existing adversarial training methods show that the model
robustness has apparent oscillations and overfitting issues in the training
process, degrading the defense efficacy. To address these issues, we propose a
novel framework called Parameter Interpolation Adversarial Training (PIAT).
PIAT tunes the model parameters between each epoch by interpolating the
parameters of the previous and current epochs. It makes the decision boundary
of model change more moderate and alleviates the overfitting issue, helping the
model converge better and achieving higher model robustness. In addition, we
suggest using the Normalized Mean Square Error (NMSE) to further improve the
robustness by aligning the relative magnitude of logits between clean and
adversarial examples rather than the absolute magnitude. Extensive experiments
conducted on several benchmark datasets demonstrate that our framework could
prominently improve the robustness of both Convolutional Neural Networks (CNNs)
and Vision Transformers (ViTs).

</details>


### [178] [OmniBrainBench: A Comprehensive Multimodal Benchmark for Brain Imaging Analysis Across Multi-stage Clinical Tasks](https://arxiv.org/abs/2511.00846)
*Zhihao Peng,Cheng Wang,Shengyuan Liu,Zhiying Liang,Yixuan Yuan*

Main category: cs.CV

TL;DR: OmniBrainBench is the first comprehensive multimodal VQA benchmark for brain imaging analysis, covering 15 imaging modalities and 15 clinical tasks, revealing that current MLLMs significantly lag behind physicians, especially in complex preoperative tasks.


<details>
  <summary>Details</summary>
Motivation: Current brain-oriented VQA benchmarks are limited in modalities and granularity, hindering comprehensive assessment of MLLMs in clinical brain imaging analysis.

Method: Created OmniBrainBench with 15 brain imaging modalities from 30 medical sources, containing 9,527 VQA pairs and 31,706 images, simulating clinical workflows and validated by professional radiologists.

Result: Evaluation of 24 models shows: proprietary MLLMs outperform open-source/medical models but trail physicians; medical MLLMs show wide performance variance; open-source models excel in specific tasks; all models underperform in complex preoperative tasks.

Conclusion: OmniBrainBench sets a new evaluation standard, highlighting significant gaps between MLLMs and expert clinical reasoning in brain imaging analysis.

Abstract: Brain imaging analysis is vital for diagnosing and treating brain disorders,
and multimodal large language models (MLLMs) are increasingly assisting in that
analysis. However, current brain-oriented visual question-answering (VQA)
benchmarks either cover a few imaging modalities or are limited to
coarse-grained pathological descriptions, hindering a comprehensive assessment
of MLLMs throughout the full clinical continuum. To address these, we introduce
OmniBrainBench, the first comprehensive multimodal VQA benchmark specifically
designed to assess the multimodal comprehension capabilities of MLLMs in brain
imaging analysis.OmniBrainBench consists of 15 distinct brain imaging
modalities collected from 30 verified medical sources, yielding 9,527 validated
VQA pairs and 31,706 images. It simulates clinical workflows and encompasses 15
multi-stage clinical tasks rigorously validated by a professional radiologist.
Evaluation of 24 state-of-the-art models, including open-source, medical, and
proprietary MLLMs, highlights the substantial challenges posed by
OmniBrainBench. Our experiments reveal: (1) proprietary MLLMs (e.g., GPT-5)
beat open-source and medical models but lag physicians; (2) medical MLLMs vary
widely in performance; (3) open-source MLLMs trail overall but excel in
specific tasks; (4) MLLMs underperform sharply in complex preoperative tasks,
revealing a visual-to-clinical reasoning gap. OmniBrainBench sets a new
standard for evaluating and advancing MLLMs in brain imaging analysis,
highlighting gaps compared to expert clinical reasoning. We release it at
benchmark \& code.

</details>


### [179] [Occlusion-Aware Diffusion Model for Pedestrian Intention Prediction](https://arxiv.org/abs/2511.00858)
*Yu Liu,Zhijie Liu,Zedong Yang,You-Fu Li,He Kong*

Main category: cs.CV

TL;DR: Proposes an Occlusion-Aware Diffusion Model (ODM) that reconstructs occluded pedestrian motion patterns to improve crossing intention prediction under occlusion scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning models for pedestrian crossing intention prediction don't adequately handle incomplete observations under occlusion scenarios, which is crucial for mobile robots and intelligent vehicles.

Method: Uses an occlusion-aware diffusion transformer architecture to estimate noise features of occluded patterns during denoising, and introduces an occlusion mask-guided reverse process to leverage observation information and reduce prediction error accumulation.

Result: Comprehensive evaluation on PIE and JAAD benchmarks shows the method achieves more robust performance than existing methods under various occlusion scenarios.

Conclusion: The proposed ODM effectively handles occlusion scenarios in pedestrian intention prediction, demonstrating superior robustness compared to state-of-the-art methods.

Abstract: Predicting pedestrian crossing intentions is crucial for the navigation of
mobile robots and intelligent vehicles. Although recent deep learning-based
models have shown significant success in forecasting intentions, few consider
incomplete observation under occlusion scenarios. To tackle this challenge, we
propose an Occlusion-Aware Diffusion Model (ODM) that reconstructs occluded
motion patterns and leverages them to guide future intention prediction. During
the denoising stage, we introduce an occlusion-aware diffusion transformer
architecture to estimate noise features associated with occluded patterns,
thereby enhancing the model's ability to capture contextual relationships in
occluded semantic scenarios. Furthermore, an occlusion mask-guided reverse
process is introduced to effectively utilize observation information, reducing
the accumulation of prediction errors and enhancing the accuracy of
reconstructed motion features. The performance of the proposed method under
various occlusion scenarios is comprehensively evaluated and compared with
existing methods on popular benchmarks, namely PIE and JAAD. Extensive
experimental results demonstrate that the proposed method achieves more robust
performance than existing methods in the literature.

</details>


### [180] [Layer-Wise Modality Decomposition for Interpretable Multimodal Sensor Fusion](https://arxiv.org/abs/2511.00859)
*Jaehyun Park,Konyul Park,Daehun Kim,Junseo Park,Jun Won Choi*

Main category: cs.CV

TL;DR: LMD is a post-hoc interpretability method that disentangles modality-specific information across layers of pretrained fusion models for autonomous driving perception.


<details>
  <summary>Details</summary>
Motivation: Transparency in perception models is critical for autonomous driving safety, but multi-sensor fusion makes it difficult to determine individual modality contributions to predictions.

Method: Layer-Wise Modality Decomposition (LMD) - a post-hoc, model-agnostic method that disentangles modality-specific information across all layers of pretrained fusion models.

Result: LMD effectively attributes predictions to individual modalities in camera-radar, camera-LiDAR, and camera-radar-LiDAR fusion systems, validated through structured perturbation metrics and visual decompositions.

Conclusion: LMD provides practical interpretability for high-capacity multimodal architectures in autonomous driving, enabling transparency in sensor fusion decision-making.

Abstract: In autonomous driving, transparency in the decision-making of perception
models is critical, as even a single misperception can be catastrophic. Yet
with multi-sensor inputs, it is difficult to determine how each modality
contributes to a prediction because sensor information becomes entangled within
the fusion network. We introduce Layer-Wise Modality Decomposition (LMD), a
post-hoc, model-agnostic interpretability method that disentangles
modality-specific information across all layers of a pretrained fusion model.
To our knowledge, LMD is the first approach to attribute the predictions of a
perception model to individual input modalities in a sensor-fusion system for
autonomous driving. We evaluate LMD on pretrained fusion models under
camera-radar, camera-LiDAR, and camera-radar-LiDAR settings for autonomous
driving. Its effectiveness is validated using structured perturbation-based
metrics and modality-wise visual decompositions, demonstrating practical
applicability to interpreting high-capacity multimodal architectures. Code is
available at https://github.com/detxter-jvb/Layer-Wise-Modality-Decomposition.

</details>


### [181] [GraphGeo: Multi-Agent Debate Framework for Visual Geo-localization with Heterogeneous Graph Neural Networks](https://arxiv.org/abs/2511.00908)
*Heng Zheng,Yuling Shi,Xiaodong Gu,Haochen You,Zijian Zhang,Lubin Gan,Hao Zhang,Wenjun Huang,Jin Huang*

Main category: cs.CV

TL;DR: GraphGeo is a multi-agent debate framework using heterogeneous graph neural networks for visual geo-localization, modeling diverse debate relationships through typed edges and dual-level debate mechanisms to handle conflicting predictions effectively.


<details>
  <summary>Details</summary>
Motivation: Traditional retrieval methods are constrained by database coverage, and individual LVLMs struggle with diverse geographic regions and complex scenes. Existing multi-agent systems lack mechanisms to handle conflicting predictions effectively.

Method: Uses heterogeneous graph neural networks with typed edges (supportive collaboration, competitive argumentation, knowledge transfer) and dual-level debate mechanism combining node-level refinement and edge-level argumentation modeling with cross-level topology refinement.

Result: Significantly outperforms state-of-the-art methods on multiple benchmarks, transforming cognitive conflicts between agents into enhanced geo-localization accuracy.

Conclusion: The GraphGeo framework successfully addresses limitations of existing approaches by modeling structured debate relationships and enabling effective handling of conflicting predictions in visual geo-localization.

Abstract: Visual geo-localization requires extensive geographic knowledge and
sophisticated reasoning to determine image locations without GPS metadata.
Traditional retrieval methods are constrained by database coverage and quality.
Recent Large Vision-Language Models (LVLMs) enable direct location reasoning
from image content, yet individual models struggle with diverse geographic
regions and complex scenes. Existing multi-agent systems improve performance
through model collaboration but treat all agent interactions uniformly. They
lack mechanisms to handle conflicting predictions effectively. We propose
\textbf{GraphGeo}, a multi-agent debate framework using heterogeneous graph
neural networks for visual geo-localization. Our approach models diverse debate
relationships through typed edges, distinguishing supportive collaboration,
competitive argumentation, and knowledge transfer. We introduce a dual-level
debate mechanism combining node-level refinement and edge-level argumentation
modeling. A cross-level topology refinement strategy enables co-evolution
between graph structure and agent representations. Experiments on multiple
benchmarks demonstrate GraphGeo significantly outperforms state-of-the-art
methods. Our framework transforms cognitive conflicts between agents into
enhanced geo-localization accuracy through structured debate.

</details>


### [182] [Fleming-VL: Towards Universal Medical Visual Reasoning with Multimodal LLMs](https://arxiv.org/abs/2511.00916)
*Yan Shu,Chi Liu,Robin Chen,Derek Li,Bryan Dai*

Main category: cs.CV

TL;DR: Fleming-VL is a unified end-to-end framework for medical visual understanding across heterogeneous modalities (2D, 3D, video) that achieves state-of-the-art performance through data-centric strategies including pretraining scaling, rare data fine-tuning, and extended evaluation benchmarks.


<details>
  <summary>Details</summary>
Motivation: Medical data presents unique challenges due to its heterogeneous nature across 2D images, 3D volumetric scans, and temporal video sequences, with substantial domain gaps and format inconsistencies hindering unified medical MLLM development.

Method: Three data-centric strategies: (1) scaling up pretraining with long-context natural and medical data, (2) fine-tuning with rare medical data including video analysis and underrepresented 2D modalities, (3) extending evaluation to include 3D and video benchmarks. Uses supervised fine-tuning (SFT) and group relative policy optimization (GRPO).

Result: Fleming-VL achieves state-of-the-art performance across multiple benchmarks including medical VQA, video QA, and 3D medical image understanding.

Conclusion: The framework enables comprehensive medical visual understanding across heterogeneous modalities and is publicly released to promote transparent, reproducible, and auditable progress in medical AI.

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
effectiveness in various general-domain scenarios, such as visual question
answering and image captioning. Recently, researchers have increasingly focused
on empowering MLLMs with medical conversational abilities, which hold
significant promise for clinical applications. However, medical data presents
unique challenges due to its heterogeneous nature -- encompassing diverse
modalities including 2D images, 3D volumetric scans, and temporal video
sequences. The substantial domain gap and data format inconsistencies across
these modalities have hindered the development of unified medical MLLMs. To
address these challenges, we propose Fleming-VL, a unified end-to-end framework
for comprehensive medical visual understanding across heterogeneous modalities.
Fleming-VL tackles this problem from a data-centric perspective through three
key strategies: (1) scaling up pretraining by integrating long-context data
from both natural and medical-specific domains; (2) complementing fine-tuning
with rare medical data, including holistic video analysis and underrepresented
2D modalities such as ultrasound and dermoscopy images; (3) extending existing
evaluation frameworks to incorporate 3D volumetric and video understanding
benchmarks. Through supervised fine-tuning (SFT) and group relative policy
optimization (GRPO), we develop Fleming-VL in multiple model scales. Extensive
experiments demonstrate that Fleming-VL achieves state-of-the-art performance
across multiple benchmarks, including medical VQA, video QA, and 3D medical
image understanding. We publicly release Fleming-VL to promote transparent,
reproducible, and auditable progress in medical AI.

</details>


### [183] [Dynamic Multi-level Weighted Alignment Network for Zero-shot Sketch-based Image Retrieval](https://arxiv.org/abs/2511.00925)
*Hanwen Su,Ge Song,Jiyan Wang,Yuanbo Zhu*

Main category: cs.CV

TL;DR: Dynamic Multi-level Weighted Alignment Network improves zero-shot sketch-based image retrieval by addressing modality imbalance and inconsistent information through multi-level weighting and weighted quadruplet loss.


<details>
  <summary>Details</summary>
Motivation: Previous ZS-SBIR methods suffer from imbalanced modality samples and inconsistent low-quality information during training, leading to sub-optimal performance.

Method: Three components: Uni-modal Feature Extraction Module (CLIP text encoder + ViT), Cross-modal Multi-level Weighting Module (alignment weight list via local/global aggregation), and Weighted Quadruplet Loss Module for domain balance.

Result: Superior performance over state-of-the-art methods on Sketchy, TU-Berlin, and QuickDraw benchmark datasets.

Conclusion: The proposed approach effectively addresses modality imbalance and information inconsistency in ZS-SBIR, achieving improved retrieval performance.

Abstract: The problem of zero-shot sketch-based image retrieval (ZS-SBIR) has achieved
increasing attention due to its wide applications, e.g. e-commerce. Despite
progress made in this field, previous works suffer from using imbalanced
samples of modalities and inconsistent low-quality information during training,
resulting in sub-optimal performance. Therefore, in this paper, we introduce an
approach called Dynamic Multi-level Weighted Alignment Network for ZS-SBIR. It
consists of three components: (i) a Uni-modal Feature Extraction Module that
includes a CLIP text encoder and a ViT for extracting textual and visual
tokens, (ii) a Cross-modal Multi-level Weighting Module that produces an
alignment weight list by the local and global aggregation blocks to measure the
aligning quality of sketch and image samples, (iii) a Weighted Quadruplet Loss
Module aiming to improve the balance of domains in the triplet loss.
Experiments on three benchmark datasets, i.e., Sketchy, TU-Berlin, and
QuickDraw, show our method delivers superior performances over the
state-of-the-art ZS-SBIR methods.

</details>


### [184] [EVTAR: End-to-End Try on with Additional Unpaired Visual Reference](https://arxiv.org/abs/2511.00956)
*Liuzhuozheng Li,Yue Gong,Shanyuan Liu,Bo Cheng,Yuhang Ma,Liebucha Wu,Dengyang Jiang,Zanyi Wang,Dawei Leng,Yuhui Yin*

Main category: cs.CV

TL;DR: EVTAR is an end-to-end virtual try-on model that directly fits target garments onto person images using additional reference images, eliminating the need for complex inputs like masks, densepose, or segmentation maps.


<details>
  <summary>Details</summary>
Motivation: Existing virtual try-on methods require complex inputs (agnostic person images, human pose, densepose, body keypoints) which are labor-intensive and impractical for real-world applications.

Method: Two-stage training strategy with simple inference using only source image and target garment inputs. Leverages additional reference images of different individuals wearing the same clothes to preserve garment texture and fine-grained details.

Result: EVTAR generates try-on results without masks, densepose, or segmentation maps. Evaluated on two benchmarks with consistent effectiveness across diverse tasks.

Conclusion: EVTAR provides a more practical and realistic virtual try-on solution by simulating how humans consider reference models when choosing outfits, achieving high-quality dressing effects with simplified inputs.

Abstract: We propose EVTAR, an End-to-End Virtual Try-on model with Additional
Reference, that directly fits the target garment onto the person image while
incorporating reference images to enhance try-on accuracy. Most existing
virtual try-on approaches rely on complex inputs such as agnostic person
images, human pose, densepose, or body keypoints, making them labor-intensive
and impractical for real-world applications. In contrast, EVTAR adopts a
two-stage training strategy, enabling simple inference with only the source
image and the target garment inputs. Our model generates try-on results without
masks, densepose, or segmentation maps. Moreover, EVTAR leverages additional
reference images of different individuals wearing the same clothes to preserve
garment texture and fine-grained details better. This mechanism is analogous to
how humans consider reference models when choosing outfits, thereby simulating
a more realistic and high-quality dressing effect. We enrich the training data
with supplementary references and unpaired person images to support these
capabilities. We evaluate EVTAR on two widely used benchmarks and diverse
tasks, and the results consistently validate the effectiveness of our approach.

</details>


### [185] [A Unified Reasoning Framework for Holistic Zero-Shot Video Anomaly Analysis](https://arxiv.org/abs/2511.00962)
*Dongheng Lin,Mengxue Qu,Kunyang Han,Jianbo Jiao,Xiaojie Jin,Yunchao Wei*

Main category: cs.CV

TL;DR: A unified zero-shot framework for video anomaly analysis that connects temporal detection, spatial localization, and textual explanation through chained reasoning without additional training.


<details>
  <summary>Details</summary>
Motivation: Existing video anomaly methods only provide frame-wise detection without spatial or semantic context, and current localization/understanding approaches remain data-dependent and task-specific.

Method: Chained test-time reasoning process that leverages intra-task reasoning for temporal detection refinement and inter-task chaining for spatial and semantic understanding, using careful prompt design with foundation models.

Result: Achieves state-of-the-art zero-shot performance across multiple video anomaly detection, localization, and explanation benchmarks without additional data or gradients.

Conclusion: Careful prompt design with task-wise chaining can unlock foundation models' reasoning power for practical, interpretable video anomaly analysis in a fully zero-shot manner.

Abstract: Most video-anomaly research stops at frame-wise detection, offering little
insight into why an event is abnormal, typically outputting only frame-wise
anomaly scores without spatial or semantic context. Recent video anomaly
localization and video anomaly understanding methods improve explainability but
remain data-dependent and task-specific. We propose a unified reasoning
framework that bridges the gap between temporal detection, spatial
localization, and textual explanation. Our approach is built upon a chained
test-time reasoning process that sequentially connects these tasks, enabling
holistic zero-shot anomaly analysis without any additional training.
Specifically, our approach leverages intra-task reasoning to refine temporal
detections and inter-task chaining for spatial and semantic understanding,
yielding improved interpretability and generalization in a fully zero-shot
manner. Without any additional data or gradients, our method achieves
state-of-the-art zero-shot performance across multiple video anomaly detection,
localization, and explanation benchmarks. The results demonstrate that careful
prompt design with task-wise chaining can unlock the reasoning power of
foundation models, enabling practical, interpretable video anomaly analysis in
a fully zero-shot manner. Project Page:
https://rathgrith.github.io/Unified_Frame_VAA/.

</details>


### [186] [VesSAM: Efficient Multi-Prompting for Segmenting Complex Vessel](https://arxiv.org/abs/2511.00981)
*Suzhong Fu,Rui Sun,Xuan Ding,Jingqi Dong,Yiming Yang,Yao Zhu,Min Chang Jordan Ren,Delin Deng,Angelica Aviles-Rivero,Shuguang Cui,Zhen Li*

Main category: cs.CV

TL;DR: VesSAM is a specialized framework for 2D vessel segmentation that enhances SAM with convolutional adapters, multi-prompt encoders, and lightweight mask decoders, achieving superior performance with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Foundation models like SAM perform sub-optimally on vascular structures due to thin, branching anatomy and low texture contrast, creating a need for specialized vessel segmentation methods.

Method: Integrates convolutional adapter for local texture features, multi-prompt encoder fusing anatomical prompts (skeletons, bifurcation points, midpoints) via hierarchical cross-attention, and lightweight mask decoder to reduce artifacts.

Result: Outperforms PEFT-based SAM variants by over 10% Dice and 13% IoU, achieves competitive performance with fully fine-tuned methods using significantly fewer parameters, and generalizes well to out-of-distribution settings.

Conclusion: VesSAM provides an efficient and powerful framework for vessel segmentation that addresses SAM's limitations on vascular structures while maintaining strong generalization capabilities.

Abstract: Accurate vessel segmentation is critical for clinical applications such as
disease diagnosis and surgical planning, yet remains challenging due to thin,
branching structures and low texture contrast. While foundation models like the
Segment Anything Model (SAM) have shown promise in generic segmentation, they
perform sub-optimally on vascular structures. In this work, we present VesSAM,
a powerful and efficient framework tailored for 2D vessel segmentation. VesSAM
integrates (1) a convolutional adapter to enhance local texture features, (2) a
multi-prompt encoder that fuses anatomical prompts, including skeletons,
bifurcation points, and segment midpoints, via hierarchical cross-attention,
and (3) a lightweight mask decoder to reduce jagged artifacts. We also
introduce an automated pipeline to generate structured multi-prompt
annotations, and curate a diverse benchmark dataset spanning 8 datasets across
5 imaging modalities. Experimental results demonstrate that VesSAM consistently
outperforms state-of-the-art PEFT-based SAM variants by over 10% Dice and 13%
IoU, and achieves competitive performance compared to fully fine-tuned methods,
with significantly fewer parameters. VesSAM also generalizes well to
out-of-distribution (OoD) settings, outperforming all baselines in average OoD
Dice and IoU.

</details>


### [187] [MID: A Self-supervised Multimodal Iterative Denoising Framework](https://arxiv.org/abs/2511.00997)
*Chang Nie,Tianchen Deng,Zhe Liu,Hesheng Wang*

Main category: cs.CV

TL;DR: MID is a self-supervised multimodal iterative denoising framework that models noisy data as states in a non-linear noise accumulation process, enabling noise removal without clean-noisy data pairs.


<details>
  <summary>Details</summary>
Motivation: Real-world data often contains complex non-linear noise that traditional rule-based denoising methods cannot effectively handle, requiring more advanced approaches.

Method: MID iteratively introduces noise to learn two neural networks: one estimates current noise step, another predicts and subtracts noise increments. For non-linear noise, it uses first-order Taylor expansion for local linearization.

Result: Experiments across four computer vision tasks show MID achieves state-of-the-art performance with robustness and adaptability. It also performs well in biomedical and bioinformatics domains.

Conclusion: MID provides an effective self-supervised denoising framework that handles complex non-linear noise without requiring paired clean-noisy datasets, demonstrating broad applicability across multiple domains.

Abstract: Data denoising is a persistent challenge across scientific and engineering
domains. Real-world data is frequently corrupted by complex, non-linear noise,
rendering traditional rule-based denoising methods inadequate. To overcome
these obstacles, we propose a novel self-supervised multimodal iterative
denoising (MID) framework. MID models the collected noisy data as a state
within a continuous process of non-linear noise accumulation. By iteratively
introducing further noise, MID learns two neural networks: one to estimate the
current noise step and another to predict and subtract the corresponding noise
increment. For complex non-linear contamination, MID employs a first-order
Taylor expansion to locally linearize the noise process, enabling effective
iterative removal. Crucially, MID does not require paired clean-noisy datasets,
as it learns noise characteristics directly from the noisy inputs. Experiments
across four classic computer vision tasks demonstrate MID's robustness,
adaptability, and consistent state-of-the-art performance. Moreover, MID
exhibits strong performance and adaptability in tasks within the biomedical and
bioinformatics domains.

</details>


### [188] [Integrating Visual and X-Ray Machine Learning Features in the Study of Paintings by Goya](https://arxiv.org/abs/2511.01000)
*Hassan Ugail,Ismail Lujain Jaleel*

Main category: cs.CV

TL;DR: Novel multimodal ML framework for Goya art authentication using identical feature extraction on both visual and X-ray images, achieving 97.8% accuracy with One-Class SVM.


<details>
  <summary>Details</summary>
Motivation: Art authentication of Goya's works is challenging due to his heterogeneous stylistic evolution and extensive forgery history, requiring advanced computational approaches.

Method: Unified feature extraction pipeline using GLCM descriptors, LBP, entropy, energy, and color analysis applied to both visual and X-ray images, processed through optimized One-Class SVM with hyperparameter tuning.

Result: 97.8% classification accuracy with 0.022 false positive rate on 24 authenticated Goya paintings dataset; case study of "Un Gigante" achieved 92.3% authentication confidence.

Conclusion: Multimodal approach significantly outperforms single-modal methods, demonstrating effectiveness of applying identical computational techniques to both visual and radiographic imagery for art authentication.

Abstract: Art authentication of Francisco Goya's works presents complex computational
challenges due to his heterogeneous stylistic evolution and extensive
historical patterns of forgery. We introduce a novel multimodal machine
learning framework that applies identical feature extraction techniques to both
visual and X-ray radiographic images of Goya paintings. The unified feature
extraction pipeline incorporates Grey-Level Co-occurrence Matrix descriptors,
Local Binary Patterns, entropy measures, energy calculations, and colour
distribution analysis applied consistently across both imaging modalities. The
extracted features from both visual and X-ray images are processed through an
optimised One-Class Support Vector Machine with hyperparameter tuning. Using a
dataset of 24 authenticated Goya paintings with corresponding X-ray images,
split into an 80/20 train-test configuration with 10-fold cross-validation, the
framework achieves 97.8% classification accuracy with a 0.022 false positive
rate. Case study analysis of ``Un Gigante'' demonstrates the practical efficacy
of our pipeline, achieving 92.3% authentication confidence through unified
multimodal feature analysis. Our results indicate substantial performance
improvement over single-modal approaches, establishing the effectiveness of
applying identical computational methods to both visual and radiographic
imagery in art authentication applications.

</details>


### [189] [HyFormer-Net: A Synergistic CNN-Transformer with Interpretable Multi-Scale Fusion for Breast Lesion Segmentation and Classification in Ultrasound Images](https://arxiv.org/abs/2511.01013)
*Mohammad Amanour Rahman*

Main category: cs.CV

TL;DR: HyFormer-Net is a hybrid CNN-Transformer model for breast ultrasound that simultaneously performs segmentation and classification with intrinsic interpretability, achieving state-of-the-art performance and demonstrating strong generalization through progressive fine-tuning.


<details>
  <summary>Details</summary>
Motivation: B-mode ultrasound for breast cancer diagnosis faces challenges including speckle noise, operator dependency, and indistinct boundaries. Existing deep learning approaches suffer from single-task learning, architectural limitations (CNNs lack global context, Transformers lack local features), and black-box decision-making, which hinder clinical adoption.

Method: HyFormer-Net uses a dual-branch encoder integrating EfficientNet-B3 and Swin Transformer via multi-scale hierarchical fusion blocks, with an attention-gated decoder for precision and explainability. It features dual-pipeline interpretability: intrinsic attention validation with quantitative IoU verification and Grad-CAM for classification reasoning.

Result: On BUSI dataset: Dice Score 0.761 +/- 0.072, accuracy 93.2%, Malignant Recall 92.1 +/- 2.2%. Ensemble modeling achieves exceptional Dice 90.2%, accuracy 99.5%, and perfect 100% Malignant Recall. Multi-scale fusion contributes +16.8% Dice and attention gates add +5.9%. Cross-dataset generalization: progressive fine-tuning with 10% target data recovers 92.5% performance; with 50% data achieves 77.3% Dice, exceeding source-domain performance.

Conclusion: HyFormer-Net demonstrates superior performance for breast ultrasound analysis with built-in interpretability. The cross-dataset generalization study shows that while zero-shot transfer fails due to domain shift, progressive fine-tuning with minimal target data enables true generalization, making the approach clinically viable.

Abstract: B-mode ultrasound for breast cancer diagnosis faces challenges: speckle,
operator dependency, and indistinct boundaries. Existing deep learning suffers
from single-task learning, architectural constraints (CNNs lack global context,
Transformers local features), and black-box decision-making. These gaps hinder
clinical adoption.
  We propose HyFormer-Net, a hybrid CNN-Transformer for simultaneous
segmentation and classification with intrinsic interpretability. Its
dual-branch encoder integrates EfficientNet-B3 and Swin Transformer via
multi-scale hierarchical fusion blocks. An attention-gated decoder provides
precision and explainability. We introduce dual-pipeline interpretability: (1)
intrinsic attention validation with quantitative IoU verification (mean: 0.86),
and (2) Grad-CAM for classification reasoning.
  On the BUSI dataset, HyFormer-Net achieves Dice Score 0.761 +/- 0.072 and
accuracy 93.2%, outperforming U-Net, Attention U-Net, and TransUNet. Malignant
Recall of 92.1 +/- 2.2% ensures minimal false negatives. Ensemble modeling
yields exceptional Dice 90.2%, accuracy 99.5%, and perfect 100% Malignant
Recall, eliminating false negatives. Ablation studies confirm multi-scale
fusion contributes +16.8% Dice and attention gates add +5.9%.
  Crucially, we conduct the first cross-dataset generalization study for hybrid
CNN-Transformers in breast ultrasound. Zero-shot transfer fails (Dice: 0.058),
confirming domain shift. However, progressive fine-tuning with only 10%
target-domain data (68 images) recovers 92.5% performance. With 50% data, our
model achieves 77.3% Dice, exceeding source-domain performance (76.1%) and
demonstrating true generalization.

</details>


### [190] [FastBoost: Progressive Attention with Dynamic Scaling for Efficient Deep Learning](https://arxiv.org/abs/2511.01026)
*JunXi Yuan*

Main category: cs.CV

TL;DR: FastBoost is a parameter-efficient neural architecture that achieves state-of-the-art performance on CIFAR benchmarks using a novel Dynamically Scaled Progressive Attention (DSPA) mechanism, achieving significant parameter reduction while improving accuracy.


<details>
  <summary>Details</summary>
Motivation: To create a highly parameter-efficient neural network that can achieve state-of-the-art performance on image classification benchmarks while being suitable for deployment in resource-constrained edge devices without accuracy degradation.

Method: Uses Dynamically Scaled Progressive Attention (DSPA) mechanism with three innovations: Adaptive Fusion (learnt channel-spatial attention blending), Phase Scaling (training-stage-aware intensity modulation), and Residual Adaptation (self-optimized skip connections). Integrated with enhanced MBConv blocks and features dual attention pathways with real-time weight adjustment and cascaded refinement layers.

Result: Achieves CIFAR-10: 95.57% accuracy (0.85M parameters) and 93.80% (0.37M parameters); CIFAR-100: 81.37% accuracy (0.92M parameters) and 74.85% (0.44M parameters). 2.1 times parameter reduction over MobileNetV3 while improving accuracy by +3.2 percentage points on CIFAR-10. Hardware-friendly design with 0.28G FLOPs.

Conclusion: FastBoost demonstrates unprecedented parameter-accuracy trade-offs through co-optimization of dynamic attention and efficient convolution operations, enabling deployment in resource-constrained edge devices without compromising accuracy.

Abstract: We present FastBoost, a parameter-efficient neural architecture that achieves
state-of-the-art performance on CIFAR benchmarks through a novel Dynamically
Scaled Progressive Attention (DSPA) mechanism. Our design establishes new
efficiency frontiers with: CIFAR-10: 95.57% accuracy (0.85M parameters) and
93.80% (0.37M parameters) CIFAR-100: 81.37% accuracy (0.92M parameters) and
74.85% (0.44M parameters) The breakthrough stems from three fundamental
innovations in DSPA: (1) Adaptive Fusion: Learnt channel-spatial attention
blending with dynamic weights. (2) Phase Scaling: Training-stage-aware
intensity modulation (from 0.5 to 1.0). (3) Residual Adaptation: Self-optimized
skip connections (gamma from 0.5 to 0.72). By integrating DSPA with enhanced
MBConv blocks, FastBoost achieves a 2.1 times parameter reduction over
MobileNetV3 while improving accuracy by +3.2 percentage points on CIFAR-10. The
architecture features dual attention pathways with real-time weight adjustment,
cascaded refinement layers (increasing gradient flow by 12.7%), and a
hardware-friendly design (0.28G FLOPs). This co-optimization of dynamic
attention and efficient convolution operations demonstrates unprecedented
parameter-accuracy trade-offs, enabling deployment in resource-constrained edge
devices without accuracy degradation.

</details>


### [191] [T-MLA: A Targeted Multiscale Log--Exponential Attack Framework for Neural Image Compression](https://arxiv.org/abs/2511.01079)
*Nikolay I. Kalmykov,Razan Dibo,Kaiyu Shen,Xu Zhonghan,Anh-Huy Phan,Yipeng Liu,Ivan Oseledets*

Main category: cs.CV

TL;DR: T-MLA is the first targeted multiscale log-exponential attack framework that crafts adversarial perturbations in the wavelet domain to compromise neural image compression systems while maintaining visual stealth.


<details>
  <summary>Details</summary>
Motivation: Existing adversarial attacks on neural image compression (NIC) are naive adaptations of pixel-space methods, overlooking the unique structured nature of compression pipelines. There's a need for more advanced vulnerability analysis specific to NIC systems.

Method: Proposed T-MLA framework crafts adversarial perturbations in the wavelet domain by directly targeting reconstruction quality. Perturbations are strategically confined to specific wavelet subbands to maximize distortion while ensuring perceptual stealth through principled offline attack methodology.

Result: Extensive evaluation across multiple state-of-the-art NIC architectures shows large drop in reconstruction quality while perturbations remain visually imperceptible, revealing critical security flaws in generative and content delivery pipelines.

Conclusion: The work reveals critical security vulnerabilities in neural image compression systems that were previously overlooked, demonstrating that specialized wavelet-domain attacks can effectively compromise reconstruction quality while maintaining visual stealth.

Abstract: Neural image compression (NIC) has become the state-of-the-art for
rate-distortion performance, yet its security vulnerabilities remain
significantly less understood than those of classifiers. Existing adversarial
attacks on NICs are often naive adaptations of pixel-space methods, overlooking
the unique, structured nature of the compression pipeline. In this work, we
propose a more advanced class of vulnerabilities by introducing T-MLA, the
first targeted multiscale log--exponential attack framework. Our approach
crafts adversarial perturbations in the wavelet domain by directly targeting
the quality of the attacked and reconstructed images. This allows for a
principled, offline attack where perturbations are strategically confined to
specific wavelet subbands, maximizing distortion while ensuring perceptual
stealth. Extensive evaluation across multiple state-of-the-art NIC
architectures on standard image compression benchmarks reveals a large drop in
reconstruction quality while the perturbations remain visually imperceptible.
Our findings reveal a critical security flaw at the core of generative and
content delivery pipelines.

</details>


### [192] [GeoToken: Hierarchical Geolocalization of Images via Next Token Prediction](https://arxiv.org/abs/2511.01082)
*Narges Ghasemi,Amir Ziashahabi,Salman Avestimehr,Cyrus Shahabi*

Main category: cs.CV

TL;DR: A hierarchical sequence prediction approach for image geolocalization that uses S2 cells to predict geographic locations from broad regions to specific addresses, achieving state-of-the-art performance on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Image geolocalization faces challenges due to visual similarities across locations and large search spaces. The approach is inspired by how humans narrow down locations hierarchically from broad regions to specific addresses.

Method: Uses S2 cells as a nested multiresolution global grid and sequentially predicts finer-level cells conditioned on visual inputs and previous predictions, similar to autoregressive text generation. Incorporates beam search and multi-sample inference with various selection strategies.

Result: Achieves state-of-the-art performance on Im2GPS3k and YFCC4k datasets. In MLLM-free setting: surpasses comparable baselines with up to 13.9% accuracy gains. With MLLM augmentation: outperforms all baselines across all metrics.

Conclusion: The hierarchical sequence prediction approach effectively addresses image geolocalization challenges and sets new state-of-the-art performance, with the method being particularly effective when combined with MLLM augmentation.

Abstract: Image geolocalization, the task of determining an image's geographic origin,
poses significant challenges, largely due to visual similarities across
disparate locations and the large search space. To address these issues, we
propose a hierarchical sequence prediction approach inspired by how humans
narrow down locations from broad regions to specific addresses. Analogously,
our model predicts geographic tokens hierarchically, first identifying a
general region and then sequentially refining predictions to increasingly
precise locations. Rather than relying on explicit semantic partitions, our
method uses S2 cells, a nested, multiresolution global grid, and sequentially
predicts finer-level cells conditioned on visual inputs and previous
predictions. This procedure mirrors autoregressive text generation in large
language models. Much like in language modeling, final performance depends not
only on training but also on inference-time strategy. We investigate multiple
top-down traversal methods for autoregressive sampling, incorporating
techniques from test-time compute scaling used in language models.
Specifically, we integrate beam search and multi-sample inference while
exploring various selection strategies to determine the final output. This
enables the model to manage uncertainty by exploring multiple plausible paths
through the hierarchy. We evaluate our method on the Im2GPS3k and YFCC4k
datasets against two distinct sets of baselines: those that operate without a
Multimodal Large Language Model (MLLM) and those that leverage one. In the
MLLM-free setting, our model surpasses other comparable baselines on nearly all
metrics, achieving state-of-the-art performance with accuracy gains of up to
13.9%. When augmented with an MLLM, our model outperforms all baselines,
setting a new state-of-the-art across all metrics. The source code is available
at https://github.com/NNargesNN/GeoToken.

</details>


### [193] [SliceVision-F2I: A Synthetic Feature-to-Image Dataset for Visual Pattern Representation on Network Slices](https://arxiv.org/abs/2511.01087)
*Md. Abid Hasan Rafi,Mst. Fatematuj Johora,Pankaj Bhowmik*

Main category: cs.CV

TL;DR: SliceVision-F2I is a synthetic dataset that transforms network KPI vectors into visual representations using four encoding methods for visual learning tasks in network slicing.


<details>
  <summary>Details</summary>
Motivation: The emergence of 5G/6G networks requires refined identification methods and robust datasets for network slicing in service-oriented architectures.

Method: Creates synthetic dataset by transforming multivariate KPI vectors into RGB images using four encoding methods: physically inspired mappings, Perlin noise, neural wallpapering, and fractal branching. Generates 30,000 samples per encoding method with realistic network noise.

Result: Produced a publicly available dataset with 120,000 total samples (raw KPI vectors + corresponding RGB images) simulating operational uncertainties and measurement imperfections.

Conclusion: SliceVision-F2I enables visual learning, network state classification, anomaly detection, and benchmarking of image-based ML techniques for network data analysis in various research contexts.

Abstract: The emergence of 5G and 6G networks has established network slicing as a
significant part of future service-oriented architectures, demanding refined
identification methods supported by robust datasets. The article presents
SliceVision-F2I, a dataset of synthetic samples for studying feature
visualization in network slicing for next-generation networking systems. The
dataset transforms multivariate Key Performance Indicator (KPI) vectors into
visual representations through four distinct encoding methods: physically
inspired mappings, Perlin noise, neural wallpapering, and fractal branching.
For each encoding method, 30,000 samples are generated, each comprising a raw
KPI vector and a corresponding RGB image at low-resolution pixels. The dataset
simulates realistic and noisy network conditions to reflect operational
uncertainties and measurement imperfections. SliceVision-F2I is suitable for
tasks involving visual learning, network state classification, anomaly
detection, and benchmarking of image-based machine learning techniques applied
to network data. The dataset is publicly available and can be reused in various
research contexts, including multivariate time series analysis, synthetic data
generation, and feature-to-image transformations.

</details>


### [194] [Epanechnikov nonparametric kernel density estimation based feature-learning in respiratory disease chest X-ray images](https://arxiv.org/abs/2511.01098)
*Veronica Marsico,Antonio Quintero-Rincon,Hadj Batatia*

Main category: cs.CV

TL;DR: Novel respiratory disease diagnosis method combining Epanechnikov kernel density estimation with bimodal logistic regression, achieving 70.14% accuracy on chest X-rays.


<details>
  <summary>Details</summary>
Motivation: To improve respiratory disease diagnosis using medical images by leveraging flexible statistical modeling that doesn't assume specific data distributions.

Method: Combines Epanechnikov's non-parametric kernel density estimation (EKDE) with bimodal logistic regression classifier in statistical-model-based learning scheme for feature extraction from medical images.

Result: Tested on 13,808 chest X-rays from COVID-19 Radiography Dataset, achieving 70.14% accuracy, 59.26% sensitivity, and 74.18% specificity.

Conclusion: EKDE-based approaches show potential to enhance diagnostic accuracy in medical imaging, though clinical expertise is needed for further refinement and sensitivity improvement.

Abstract: This study presents a novel method for diagnosing respiratory diseases using
image data. It combines Epanechnikov's non-parametric kernel density estimation
(EKDE) with a bimodal logistic regression classifier in a
statistical-model-based learning scheme. EKDE's flexibility in modeling data
distributions without assuming specific shapes and its adaptability to pixel
intensity variations make it valuable for extracting key features from medical
images. The method was tested on 13808 randomly selected chest X-rays from the
COVID-19 Radiography Dataset, achieved an accuracy of 70.14%, a sensitivity of
59.26%, and a specificity of 74.18%, demonstrating moderate performance in
detecting respiratory disease while showing room for improvement in
sensitivity. While clinical expertise remains essential for further refining
the model, this study highlights the potential of EKDE-based approaches to
enhance diagnostic accuracy and reliability in medical imaging.

</details>


### [195] [Anatomically Constrained Transformers for Echocardiogram Analysis](https://arxiv.org/abs/2511.01109)
*Alexander Thorley,Agis Chartsias,Jordan Strom,Jeremy Slivnick,Dipak Kotecha,Alberto Gomez,Jinming Duan*

Main category: cs.CV

TL;DR: ViACT is a video transformer framework that integrates anatomical priors to focus on diagnostic regions in echocardiogram analysis, preventing spurious correlations from non-diagnostic backgrounds.


<details>
  <summary>Details</summary>
Motivation: Video transformers for echocardiogram analysis often learn spurious correlations from non-diagnostic regions like image backgrounds, limiting their effectiveness and interpretability.

Method: ViACT represents anatomical structures as point sets and encodes both spatial geometry and image patches into transformer tokens. It uses masked autoencoding during pre-training that only masks and reconstructs anatomical patches, forcing the model to focus on relevant anatomical regions.

Result: ViACT generates interpretable attention maps aligned with known pathology regions, successfully performs tasks like left ventricular ejection fraction regression and cardiac amyloidosis detection, and generalizes to myocardium point tracking without specialized components.

Conclusion: Integrating anatomical priors into video transformers through ViACT effectively focuses learning on diagnostic regions, improves interpretability, and enables generalization across multiple echocardiogram analysis tasks.

Abstract: Video transformers have recently demonstrated strong potential for
echocardiogram (echo) analysis, leveraging self-supervised pre-training and
flexible adaptation across diverse tasks. However, like other models operating
on videos, they are prone to learning spurious correlations from non-diagnostic
regions such as image backgrounds. To overcome this limitation, we propose the
Video Anatomically Constrained Transformer (ViACT), a novel framework that
integrates anatomical priors directly into the transformer architecture. ViACT
represents a deforming anatomical structure as a point set and encodes both its
spatial geometry and corresponding image patches into transformer tokens.
During pre-training, ViACT follows a masked autoencoding strategy that masks
and reconstructs only anatomical patches, enforcing that representation
learning is focused on the anatomical region. The pre-trained model can then be
fine-tuned for tasks localized to this region. In this work we focus on the
myocardium, demonstrating the framework on echo analysis tasks such as left
ventricular ejection fraction (EF) regression and cardiac amyloidosis (CA)
detection. The anatomical constraint focuses transformer attention within the
myocardium, yielding interpretable attention maps aligned with regions of known
CA pathology. Moreover, ViACT generalizes to myocardium point tracking without
requiring task-specific components such as correlation volumes used in
specialized tracking networks.

</details>


### [196] [Boosting performance of computer vision applications through embedded GPUs on the edge](https://arxiv.org/abs/2511.01129)
*Fabio Diniz Rossi*

Main category: cs.CV

TL;DR: Using GPU-equipped embedded devices in edge computing improves performance for computer vision applications compared to CPU-only systems, enhancing user experience.


<details>
  <summary>Details</summary>
Motivation: Computer vision and AR applications are resource-intensive and challenging to run on mobile devices with limited resources. Edge computing helps but has capacity limitations that affect user experience.

Method: Proposed using embedded devices with GPUs in edge computing infrastructure to offload intensive computer vision tasks from mobile devices.

Result: Experiments showed GPUs achieve significant performance gains over CPUs alone, ensuring better user experience for computer vision applications.

Conclusion: GPU-enabled embedded devices in edge computing effectively overcome resource limitations and improve performance for computer vision applications.

Abstract: Computer vision applications, especially those using augmented reality
technology, are becoming quite popular in mobile devices. However, this type of
application is known as presenting significant demands regarding resources. In
order to enable its utilization in devices with more modest resources, edge
computing can be used to offload certain high intensive tasks. Still, edge
computing is usually composed of devices with limited capacity, which may
impact in users quality of experience when using computer vision applications.
This work proposes the use of embedded devices with graphics processing units
(GPUs) to overcome such limitation. Experiments performed shown that GPUs can
attain a performance gain when compared to using only CPUs, which guarantee a
better experience to users using such kind of application.

</details>


### [197] [Weakly Supervised Concept Learning with Class-Level Priors for Interpretable Medical Diagnosis](https://arxiv.org/abs/2511.01131)
*Md Nahiduzzaman,Steven Korevaar,Alireza Bab-Hadiashar,Ruwan Tennakoon*

Main category: cs.CV

TL;DR: PCP is a weakly supervised framework that enables concept prediction in medical imaging without requiring concept annotations, using class-level concept priors and refinement mechanisms to achieve competitive performance.


<details>
  <summary>Details</summary>
Motivation: Existing interpretable-by-design frameworks need costly concept annotations, while zero-shot methods struggle with domain-specific medical features. PCP aims to provide reliable concept predictions without explicit supervision.

Method: Uses class-level concept priors as weak supervision, incorporates KL divergence and entropy regularization for refinement to align with clinical reasoning.

Result: Improves concept-level F1-score by over 33% compared to zero-shot baselines, delivers competitive classification performance on four medical datasets relative to fully supervised CBMs and V-IP.

Conclusion: PCP provides an effective weakly supervised approach for interpretable medical imaging predictions without requiring costly concept annotations.

Abstract: Human-interpretable predictions are essential for deploying AI in medical
imaging, yet most interpretable-by-design (IBD) frameworks require concept
annotations for training data, which are costly and impractical to obtain in
clinical contexts. Recent attempts to bypass annotation, such as zero-shot
vision-language models or concept-generation frameworks, struggle to capture
domain-specific medical features, leading to poor reliability. In this paper,
we propose a novel Prior-guided Concept Predictor (PCP), a weakly supervised
framework that enables concept answer prediction without explicit supervision
or reliance on language models. PCP leverages class-level concept priors as
weak supervision and incorporates a refinement mechanism with KL divergence and
entropy regularization to align predictions with clinical reasoning.
Experiments on PH2 (dermoscopy) and WBCatt (hematology) show that PCP improves
concept-level F1-score by over 33% compared to zero-shot baselines, while
delivering competitive classification performance on four medical datasets
(PH2, WBCatt, HAM10000, and CXR4) relative to fully supervised concept
bottleneck models (CBMs) and V-IP.

</details>


### [198] [Learning with Category-Equivariant Architectures for Human Activity Recognition](https://arxiv.org/abs/2511.01139)
*Yoshihiro Maruyama*

Main category: cs.CV

TL;DR: CatEquiv is a category-equivariant neural network for Human Activity Recognition that encodes temporal, amplitude, and structural symmetries through categorical symmetry products, achieving superior robustness against out-of-distribution perturbations.


<details>
  <summary>Details</summary>
Motivation: To systematically encode temporal, amplitude, and structural symmetries in HAR data to improve robustness against out-of-distribution perturbations.

Method: Introduces categorical symmetry product combining cyclic time shifts, positive gains, and sensor-hierarchy poset to capture categorical symmetry structure, achieving equivariance with respect to this product.

Result: CatEquiv achieves markedly higher robustness on UCI-HAR under out-of-distribution perturbations compared to circularly padded CNNs and plain CNNs.

Conclusion: Enforcing categorical symmetries yields strong invariance and generalization without requiring additional model capacity.

Abstract: We propose CatEquiv, a category-equivariant neural network for Human Activity
Recognition (HAR) from inertial sensors that systematically encodes temporal,
amplitude, and structural symmetries. In particular, we introduce the
categorical symmetry product where cyclic time shifts, positive gains and the
sensor-hierarchy poset together capture the categorical symmetry structure of
the data. CatEquiv achieves equivariance with respect to the categorical
symmetry product. On UCI-HAR under out-of-distribution perturbations, CatEquiv
attains markedly higher robustness compared with circularly padded CNNs and
plain CNNs. These results demonstrate that enforcing categorical symmetries
yields strong invariance and generalization without additional model capacity.

</details>


### [199] [MicroAUNet: Boundary-Enhanced Multi-scale Fusion with Knowledge Distillation for Colonoscopy Polyp Image Segmentation](https://arxiv.org/abs/2511.01143)
*Ziyi Wang,Yuanmei Zhang,Dorna Esrafilzadeh,Ali R. Jalili,Suncheng Xiang*

Main category: cs.CV

TL;DR: MicroAUNet is a lightweight attention-based segmentation network for real-time colorectal polyp segmentation that combines depthwise-separable dilated convolutions with channel-spatial attention and uses progressive knowledge distillation to achieve state-of-the-art accuracy with low complexity.


<details>
  <summary>Details</summary>
Motivation: Current deep learning polyp segmentation models either provide ambiguous polyp margins compromising clinical decision-making, or use heavy architectures with high computational complexity that are too slow for real-time endoscopic applications.

Method: Proposes MicroAUNet with depthwise-separable dilated convolutions and a single-path parameter-shared channel-spatial attention block to strengthen multi-scale boundary features, plus a progressive two-stage knowledge-distillation scheme to transfer semantic and boundary cues from a high-capacity teacher.

Result: Extensive experiments demonstrate state-of-the-art accuracy under extremely low model complexity, making it suitable for real-time clinical polyp segmentation.

Conclusion: MicroAUNet effectively addresses the trade-off between accuracy and computational efficiency in polyp segmentation, enabling real-time clinical applications while maintaining high segmentation quality.

Abstract: Early and accurate segmentation of colorectal polyps is critical for reducing
colorectal cancer mortality, which has been extensively explored by academia
and industry. However, current deep learning-based polyp segmentation models
either compromise clinical decision-making by providing ambiguous polyp margins
in segmentation outputs or rely on heavy architectures with high computational
complexity, resulting in insufficient inference speeds for real-time colorectal
endoscopic applications. To address this problem, we propose MicroAUNet, a
light-weighted attention-based segmentation network that combines
depthwise-separable dilated convolutions with a single-path, parameter-shared
channel-spatial attention block to strengthen multi-scale boundary features. On
the basis of it, a progressive two-stage knowledge-distillation scheme is
introduced to transfer semantic and boundary cues from a high-capacity teacher.
Extensive experiments on benchmarks also demonstrate the state-of-the-art
accuracy under extremely low model complexity, indicating that MicroAUNet is
suitable for real-time clinical polyp segmentation. The code is publicly
available at https://github.com/JeremyXSC/MicroAUNet.

</details>


### [200] [ROVER: Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation](https://arxiv.org/abs/2511.01163)
*Yongyuan Liang,Wei Chow,Feng Li,Ziqiao Ma,Xiyao Wang,Jiageng Mao,Jiuhai Chen,Jiatao Gu,Yue Wang,Furong Huang*

Main category: cs.CV

TL;DR: ROVER is a new benchmark that evaluates reciprocal cross-modal reasoning in unified multimodal models, testing how models use one modality to guide outputs in another modality, revealing key insights about model capabilities.


<details>
  <summary>Details</summary>
Motivation: Current evaluations treat multimodal abilities in isolation, focusing on unimodal reasoning rather than testing how models use different modalities to guide, verify, or refine each other - a core capability needed for true unified multimodal intelligence.

Method: ROVER is a human-annotated benchmark with 1312 tasks grounded in 1876 images, spanning two settings: verbally-augmented reasoning for visual generation (using verbal prompts to guide image synthesis) and visually-augmented reasoning for verbal generation (using intermediate visualizations to strengthen reasoning for question answering).

Result: Experiments on 17 unified models show: (1) Cross-modal reasoning determines visual generation quality, with interleaved models outperforming non-interleaved ones, and combining strong unimodal models fails to achieve comparable reasoning; (2) Models show dissociation between physical and symbolic reasoning - succeeding at interpreting perceptual concepts literally but failing to construct visual abstractions for symbolic tasks.

Conclusion: Reciprocal cross-modal reasoning is identified as a critical frontier for enabling true omnimodal generation, highlighting the need for better evaluation methods that test how modalities interact rather than treating them in isolation.

Abstract: Unified multimodal models (UMMs) have emerged as a powerful paradigm for
seamlessly unifying text and image understanding and generation. However,
prevailing evaluations treat these abilities in isolation, such that tasks with
multimodal inputs and outputs are scored primarily through unimodal reasoning,
i.e., textual benchmarks emphasize language-based reasoning, while visual
benchmarks emphasize reasoning outcomes manifested in the pixels. We introduce
ROVER to address this pressing need to test reciprocal cross-modal reasoning,
the use of one modality to guide, verify, or refine outputs in the other, an
ability central to the vision of unified multimodal intelligence. ROVER is a
human-annotated benchmark that explicitly targets reciprocal cross-modal
reasoning, which contains 1312 tasks grounded in 1876 images, spanning two
complementary settings. Verbally-augmented reasoning for visual generation
evaluates whether models can use verbal prompts and reasoning chains to guide
faithful image synthesis. Visually-augmented reasoning for verbal generation
evaluates whether models can generate intermediate visualizations that
strengthen their own reasoning processes for question answering. Experiments on
17 unified models reveal two key findings: (i) Cross-modal reasoning determines
visual generation quality, with interleaved models significantly outperforming
non-interleaved ones; notably, combining strong unimodal models fails to
achieve comparable reasoning. (ii) Models show dissociation between physical
and symbolic reasoning: they succeed at interpreting perceptual concepts
literally but fail to construct visual abstractions for symbolic tasks, where
faulty reasoning harms performance. These results highlight reciprocal
cross-modal reasoning as a critical frontier for enabling true omnimodal
generation.

</details>


### [201] [Web-Scale Collection of Video Data for 4D Animal Reconstruction](https://arxiv.org/abs/2511.01169)
*Brian Nlong Zhao,Jiajun Wu,Shangzhe Wu*

Main category: cs.CV

TL;DR: The paper introduces an automated pipeline for mining YouTube videos to create large-scale animal video datasets, presents the Animal-in-Motion benchmark for 4D animal reconstruction, and establishes baseline methods for markerless animal motion analysis.


<details>
  <summary>Details</summary>
Motivation: Existing animal video datasets are limited in scale and lack key processing for animal-centric 3D/4D tasks, while current evaluation methods show gaps between 2D metrics and realistic 3D reconstructions.

Method: Developed an automated pipeline to mine and process YouTube videos into object-centric clips with auxiliary annotations. Created Animal-in-Motion benchmark with 230 manually filtered sequences. Evaluated state-of-the-art model-based and model-free methods, and enhanced a model-free approach with sequence-level optimization.

Result: Collected 30K videos (2M frames) - an order of magnitude more than prior works. Found that model-based methods score higher on 2D metrics but produce unrealistic 3D shapes, while model-free methods yield more natural reconstructions but score lower. Established the first 4D animal reconstruction baseline.

Conclusion: The pipeline, benchmark, and baseline advance large-scale, markerless 4D animal reconstruction from in-the-wild videos, addressing limitations in current datasets and evaluation methods for wildlife computer vision.

Abstract: Computer vision for animals holds great promise for wildlife research but
often depends on large-scale data, while existing collection methods rely on
controlled capture setups. Recent data-driven approaches show the potential of
single-view, non-invasive analysis, yet current animal video datasets are
limited--offering as few as 2.4K 15-frame clips and lacking key processing for
animal-centric 3D/4D tasks. We introduce an automated pipeline that mines
YouTube videos and processes them into object-centric clips, along with
auxiliary annotations valuable for downstream tasks like pose estimation,
tracking, and 3D/4D reconstruction. Using this pipeline, we amass 30K videos
(2M frames)--an order of magnitude more than prior works. To demonstrate its
utility, we focus on the 4D quadruped animal reconstruction task. To support
this task, we present Animal-in-Motion (AiM), a benchmark of 230 manually
filtered sequences with 11K frames showcasing clean, diverse animal motions. We
evaluate state-of-the-art model-based and model-free methods on
Animal-in-Motion, finding that 2D metrics favor the former despite unrealistic
3D shapes, while the latter yields more natural reconstructions but scores
lower--revealing a gap in current evaluation. To address this, we enhance a
recent model-free approach with sequence-level optimization, establishing the
first 4D animal reconstruction baseline. Together, our pipeline, benchmark, and
baseline aim to advance large-scale, markerless 4D animal reconstruction and
related tasks from in-the-wild videos. Code and datasets are available at
https://github.com/briannlongzhao/Animal-in-Motion.

</details>


### [202] [Diffusion Transformer meets Multi-level Wavelet Spectrum for Single Image Super-Resolution](https://arxiv.org/abs/2511.01175)
*Peng Du,Hui Li,Han Xu,Paul Barom Jeon,Dongwook Lee,Daehyun Ji,Ran Yang,Feng Zhu*

Main category: cs.CV

TL;DR: DTWSR is a Diffusion Transformer model for image super-resolution that uses wavelet spectra to capture interrelations among multiscale frequency sub-bands, achieving consistent and realistic results.


<details>
  <summary>Details</summary>
Motivation: Most DWT-based super-resolution methods neglect interrelations among multiscale frequency sub-bands, leading to inconsistencies and artifacts in reconstructed images.

Method: Uses Multi-level Discrete Wavelet Transform to decompose images, pyramid tokenization for transformer input, and a dual-decoder to handle low-frequency and high-frequency sub-bands while maintaining alignment.

Result: Extensive experiments on multiple benchmark datasets demonstrate high performance on both perception quality and fidelity.

Conclusion: DTWSR effectively captures frequency sub-band interrelations using diffusion models and transformers, producing superior super-resolution images.

Abstract: Discrete Wavelet Transform (DWT) has been widely explored to enhance the
performance of image superresolution (SR). Despite some DWT-based methods
improving SR by capturing fine-grained frequency signals, most existing
approaches neglect the interrelations among multiscale frequency sub-bands,
resulting in inconsistencies and unnatural artifacts in the reconstructed
images. To address this challenge, we propose a Diffusion Transformer model
based on image Wavelet spectra for SR (DTWSR).DTWSR incorporates the
superiority of diffusion models and transformers to capture the interrelations
among multiscale frequency sub-bands, leading to a more consistence and
realistic SR image. Specifically, we use a Multi-level Discrete Wavelet
Transform (MDWT) to decompose images into wavelet spectra. A pyramid
tokenization method is proposed which embeds the spectra into a sequence of
tokens for transformer model, facilitating to capture features from both
spatial and frequency domain. A dual-decoder is designed elaborately to handle
the distinct variances in lowfrequency (LF) and high-frequency (HF) sub-bands,
without omitting their alignment in image generation. Extensive experiments on
multiple benchmark datasets demonstrate the effectiveness of our method, with
high performance on both perception quality and fidelity.

</details>


### [203] [A Topology-Aware Graph Convolutional Network for Human Pose Similarity and Action Quality Assessment](https://arxiv.org/abs/2511.01194)
*Minmin Zeng*

Main category: cs.CV

TL;DR: GCN-PSN: A topology-aware Graph Convolutional Network using Siamese architecture with contrastive regression for Action Quality Assessment, outperforming coordinate-based methods on AQA-7 and FineDiving benchmarks.


<details>
  <summary>Details</summary>
Motivation: Action Quality Assessment requires fine-grained understanding of human motion and precise evaluation of pose similarity, which benefits from modeling skeletal topology.

Method: Proposes GCN-PSN framework that models human skeleton as graph to learn topology-sensitive pose embeddings, using Siamese architecture trained with contrastive regression objective.

Result: Outperforms coordinate-based baselines and achieves competitive performance on AQA-7 and FineDiving benchmarks.

Conclusion: Experimental results validate the effectiveness of leveraging skeletal topology for pose similarity and action quality assessment.

Abstract: Action Quality Assessment (AQA) requires fine-grained understanding of human
motion and precise evaluation of pose similarity. This paper proposes a
topology-aware Graph Convolutional Network (GCN) framework, termed GCN-PSN,
which models the human skeleton as a graph to learn discriminative,
topology-sensitive pose embeddings. Using a Siamese architecture trained with a
contrastive regression objective, our method outperforms coordinate-based
baselines and achieves competitive performance on AQA-7 and FineDiving
benchmarks. Experimental results and ablation studies validate the
effectiveness of leveraging skeletal topology for pose similarity and action
quality assessment.

</details>


### [204] [MoSa: Motion Generation with Scalable Autoregressive Modeling](https://arxiv.org/abs/2511.01200)
*Mengyuan Liu,Sheng Yan,Yong Wang,Yingjie Li,Gui-Bin Bian,Hong Liu*

Main category: cs.CV

TL;DR: MoSa is a hierarchical motion generation framework for text-driven 3D human motion that uses a coarse-to-fine scalable generation process with multi-scale token preservation and achieves state-of-the-art performance with significantly reduced inference time.


<details>
  <summary>Details</summary>
Motivation: To enhance text-driven 3D human motion generation by improving the VQ-GT paradigm through a more efficient hierarchical generation process that preserves multi-scale information while reducing inference steps.

Method: Proposes Multi-scale Token Preservation Strategy (MTPS) integrated with hierarchical RQ-VAE, Scalable Autoregressive (SAR) modeling that predicts scale tokens, and CAQ-VAE - a convolution-attention hybrid VQ-VAE to address reconstruction degradation from interpolation.

Result: Achieves FID of 0.06 on Motion-X dataset (vs MoMask's 0.20) with 27% reduction in inference time, demonstrating superior generation quality and efficiency while generalizing well to downstream tasks like motion editing without fine-tuning.

Conclusion: MoSa establishes a new state-of-the-art in text-driven 3D human motion generation by combining hierarchical token preservation with efficient scalable autoregressive modeling, achieving both high fidelity and computational efficiency.

Abstract: We introduce MoSa, a novel hierarchical motion generation framework for
text-driven 3D human motion generation that enhances the Vector
Quantization-guided Generative Transformers (VQ-GT) paradigm through a
coarse-to-fine scalable generation process. In MoSa, we propose a Multi-scale
Token Preservation Strategy (MTPS) integrated into a hierarchical residual
vector quantization variational autoencoder (RQ-VAE). MTPS employs
interpolation at each hierarchical quantization to effectively retain
coarse-to-fine multi-scale tokens. With this, the generative transformer
supports Scalable Autoregressive (SAR) modeling, which predicts scale tokens,
unlike traditional methods that predict only one token at each step.
Consequently, MoSa requires only 10 inference steps, matching the number of
RQ-VAE quantization layers. To address potential reconstruction degradation
from frequent interpolation, we propose CAQ-VAE, a lightweight yet expressive
convolution-attention hybrid VQ-VAE. CAQ-VAE enhances residual block design and
incorporates attention mechanisms to better capture global dependencies.
Extensive experiments show that MoSa achieves state-of-the-art generation
quality and efficiency, outperforming prior methods in both fidelity and speed.
On the Motion-X dataset, MoSa achieves an FID of 0.06 (versus MoMask's 0.20)
while reducing inference time by 27 percent. Moreover, MoSa generalizes well to
downstream tasks such as motion editing, requiring no additional fine-tuning.
The code is available at https://mosa-web.github.io/MoSa-web

</details>


### [205] [OmniVLA: Unifiying Multi-Sensor Perception for Physically-Grounded Multimodal VLA](https://arxiv.org/abs/2511.01210)
*Heyu Guo,Shanmu Wang,Ruichun Ma,Shiqi Jiang,Yasaman Ghasempour,Omid Abari,Baining Guo,Lili Qi*

Main category: cs.CV

TL;DR: OmniVLA is an omni-modality vision-language-action model that integrates multiple sensing modalities (infrared camera, mmWave radar, microphone array) with RGB vision to enhance robotic manipulation capabilities through sensor-masked image representations.


<details>
  <summary>Details</summary>
Motivation: Existing VLA models rely solely on RGB cameras, limiting perception and manipulation capabilities. The paper aims to overcome this limitation by integrating novel sensing modalities for physically-grounded spatial intelligence beyond RGB perception.

Method: Uses sensor-masked images that overlay spatially grounded masks from multiple sensors onto RGB images. Built on RGB-pretrained VLA backbone with lightweight per-sensor projectors for data-efficient learning.

Result: Achieves 84% average task success rate, outperforming RGB-only baselines by 59% and raw-sensor-input baselines by 28%. Shows higher learning efficiency and stronger generalization capability.

Conclusion: OmniVLA demonstrates that integrating multiple sensing modalities through unified sensor-masked representations significantly enhances VLA model performance for real-world manipulation tasks requiring sensor-modality perception.

Abstract: Vision-language-action (VLA) models have shown strong generalization for
action prediction through large-scale vision-language pretraining. However,
most existing models rely solely on RGB cameras, limiting their perception and,
consequently, manipulation capabilities. We present OmniVLA, an omni-modality
VLA model that integrates novel sensing modalities for physically-grounded
spatial intelligence beyond RGB perception. The core of our approach is the
sensor-masked image, a unified representation that overlays spatially grounded
and physically meaningful masks onto the RGB images, derived from sensors
including an infrared camera, a mmWave radar, and a microphone array. This
image-native unification keeps sensor input close to RGB statistics to
facilitate training, provides a uniform interface across sensor hardware, and
enables data-efficient learning with lightweight per-sensor projectors. Built
on this, we present a multisensory vision-language-action model architecture
and train the model based on an RGB-pretrained VLA backbone. We evaluate
OmniVLA on challenging real-world tasks where sensor-modality perception is
needed to guide the manipulation. OmniVLA achieves an average task success rate
of 84%, significantly outperforms both RGB-only and raw-sensor-input baseline
models by 59% and 28% respectively, meanwhile showing higher learning
efficiency and stronger generalization capability.

</details>


### [206] [Thought-For-Food: Reasoning Chain Induced Food Visual Question Answering](https://arxiv.org/abs/2511.01213)
*Riddhi Jain,Manasi Patwardhan,Parijat Deshpande,Venkataramana Runkana*

Main category: cs.CV

TL;DR: This paper proposes a multi-step reasoning approach for Indian Food VQA, using auto-generated reasoning chains to improve accuracy by 10 percentage points over baseline methods.


<details>
  <summary>Details</summary>
Motivation: Existing VQA systems are biased towards Western foods and don't adequately handle the complex culinary context and relationships in diverse Indian cuisines. Current Indian food VQA approaches use a simplistic two-step process that lacks proper reasoning.

Method: Created reasoning chains for QA with minimal human intervention, fine-tuned smaller LLMs and VLMs with auto-validated reasoning chains, and used reinforcement learning with larger datasets for training.

Result: Achieved an average 10 percentage point improvement in accuracy on the baseline Indian Food VQA task through reasoning chain augmentation.

Conclusion: Multi-step reasoning chains are essential for accurate Indian Food VQA, as they help understand complex culinary contexts and relationships between food items, significantly improving system performance.

Abstract: The immense diversity in the culture and culinary of Indian cuisines calls
attention to the major shortcoming of the existing Visual Question
Answering(VQA) systems which are inclined towards the foods from Western
region. Recent attempt towards building a VQA dataset for Indian food is a step
towards addressing this challenge. However, their approach towards VQA follows
a two-step process in which the answer is generated first, followed by the
explanation of the expected answer. In this work, we claim that food VQA
requires to follow a multi-step reasoning process to arrive at an accurate
answer, especially in the context of India food, which involves understanding
complex culinary context and identifying relationships between various food
items. With this hypothesis we create reasoning chains upon the QA with minimal
human intervention. We fine-tune smaller LLMs and VLMs with auto-validated
reasoning chains and further train them using reinforcement learning with
larger data. With augmentation of reasoning chains, we observed accuracy
improvement of an average 10 percentage points on the baseline. We provide
detailed analysis in terms the effect of addition of reasoning chains for the
Indian Food VQA task.
  Index Terms - FoodVQA, Reasoning Chains, Reinforcement Learning, Knowledge
Graph.

</details>


### [207] [Saliency-Guided Domain Adaptation for Left-Hand Driving in Autonomous Steering](https://arxiv.org/abs/2511.01223)
*Zahra Mehraban,Sebastien Glaser,Michael Milford,Ronald Schroeter*

Main category: cs.CV

TL;DR: Domain adaptation for automated driving models using flipped data pretraining followed by fine-tuning improves steering prediction accuracy and attention focus on left-side cues for left-hand driving conditions.


<details>
  <summary>Details</summary>
Motivation: Domain adaptation is required for automated driving models to generalize well across diverse road conditions, specifically adapting from right-hand to left-hand driving conditions.

Method: Four training methods evaluated: baseline on US data, flipped US data, pretrained US + fine-tuned Australian, and pretrained flipped US + fine-tuned Australian. Saliency-based analysis used to measure attention shifts.

Result: Pretraining on flipped data alone worsens prediction stability, but significantly improves adaptation when followed by fine-tuning, leading to lower prediction error and stronger focus on left-side cues. Similar trends confirmed with ResNet architecture.

Conclusion: Flipped-data pretraining followed by fine-tuning improves model adaptation with minimal retraining requirements, emphasizing the importance of preprocessing techniques.

Abstract: Domain adaptation is required for automated driving models to generalize well
across diverse road conditions. This paper explores a training method for
domain adaptation to adapt PilotNet, an end-to-end deep learning-based model,
for left-hand driving conditions using real-world Australian highway data. Four
training methods were evaluated: (1) a baseline model trained on U.S.
right-hand driving data, (2) a model trained on flipped U.S. data, (3) a model
pretrained on U.S. data and then fine-tuned on Australian highways, and (4) a
model pretrained on flipped U.S. data and then finetuned on Australian
highways. This setup examines whether incorporating flipped data enhances the
model adaptation by providing an initial left-hand driving alignment. The paper
compares model performance regarding steering prediction accuracy and
attention, using saliency-based analysis to measure attention shifts across
significant road regions. Results show that pretraining on flipped data alone
worsens prediction stability due to misaligned feature representations, but
significantly improves adaptation when followed by fine-tuning, leading to
lower prediction error and stronger focus on left-side cues. To validate this
approach across different architectures, the same experiments were done on
ResNet, which confirmed similar adaptation trends. These findings emphasize the
importance of preprocessing techniques, such as flipped-data pretraining,
followed by fine-tuning to improve model adaptation with minimal retraining
requirements.

</details>


### [208] [Gesture Generation (Still) Needs Improved Human Evaluation Practices: Insights from a Community-Driven State-of-the-Art Benchmark](https://arxiv.org/abs/2511.01233)
*Rajmund Nagy,Hendric Voss,Thanh Hoang-Minh,Mihail Tsakov,Teodor Nikolov,Zeyi Zhang,Tenglong Ao,Sicheng Yang,Shaoli Huang,Yongkang Cheng,M. Hamza Mughal,Rishabh Dabral,Kiran Chhatre,Christian Theobalt,Libin Liu,Stefan Kopp,Rachel McDonnell,Michael Neff,Taras Kucherenko,Youngwoo Yoon,Gustav Eje Henter*

Main category: cs.CV

TL;DR: This paper identifies flaws in human evaluation practices for speech-driven 3D gesture generation and introduces a standardized evaluation protocol for the BEAT2 dataset, benchmarking six models across motion realism and speech-gesture alignment dimensions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of standardization and flawed experimental setups in human evaluation of gesture generation models, which makes it impossible to compare different methods or determine the state of the art.

Method: The authors introduce a detailed human evaluation protocol for BEAT2 dataset and conduct large-scale crowdsourced evaluation to rank six gesture-generation models across motion realism and speech-gesture alignment dimensions.

Result: Results show that newer models don't consistently outperform earlier approaches, published claims of high performance may not hold under rigorous evaluation, and the field needs disentangled assessments of motion quality and multimodal alignment.

Conclusion: The paper concludes by advocating for standardized evaluation protocols and releases extensive resources including synthetic motion data, rendered videos, and human preference votes to drive standardization and enable future evaluation research.

Abstract: We review human evaluation practices in automated, speech-driven 3D gesture
generation and find a lack of standardisation and frequent use of flawed
experimental setups. This leads to a situation where it is impossible to know
how different methods compare, or what the state of the art is. In order to
address common shortcomings of evaluation design, and to standardise future
user studies in gesture-generation works, we introduce a detailed human
evaluation protocol for the widely-used BEAT2 motion-capture dataset. Using
this protocol, we conduct large-scale crowdsourced evaluation to rank six
recent gesture-generation models -- each trained by its original authors --
across two key evaluation dimensions: motion realism and speech-gesture
alignment. Our results provide strong evidence that 1) newer models do not
consistently outperform earlier approaches; 2) published claims of high motion
realism or speech-gesture alignment may not hold up under rigorous evaluation;
and 3) the field must adopt disentangled assessments of motion quality and
multimodal alignment for accurate benchmarking in order to make progress.
Finally, in order to drive standardisation and enable new evaluation research,
we will release five hours of synthetic motion from the benchmarked models;
over 750 rendered video stimuli from the user studies -- enabling new
evaluations without model reimplementation required -- alongside our
open-source rendering script, and the 16,000 pairwise human preference votes
collected for our benchmark.

</details>


### [209] [$\left|\,\circlearrowright\,\boxed{\text{BUS}}\,\right|$: A Large and Diverse Multimodal Benchmark for evaluating the ability of Vision-Language Models to understand Rebus Puzzles](https://arxiv.org/abs/2511.01340)
*Trishanu Das,Abhilash Nandy,Khush Bajaj,Deepiha S*

Main category: cs.CV

TL;DR: The paper introduces a large benchmark of 1,333 English Rebus puzzles and proposes a model-agnostic framework called RebusDescProgICE that improves Vision-Language Model performance on these puzzles by combining unstructured descriptions with code-based reasoning.


<details>
  <summary>Details</summary>
Motivation: Rebus puzzles require complex skills like image recognition, cognitive abilities, commonsense reasoning, and multi-step reasoning, making them challenging for current Vision-Language Models. The authors aim to address this gap by creating a comprehensive benchmark and improving model performance.

Method: The authors created a diverse benchmark of 1,333 Rebus puzzles across 18 categories. They proposed RebusDescProgICE framework that uses unstructured descriptions combined with code-based structured reasoning and improved reasoning-based in-context example selection.

Result: The proposed framework improved performance on the benchmark by 2.1-4.1% using closed-source models and 20-30% using open-source models compared to Chain-of-Thought Reasoning.

Conclusion: The RebusDescProgICE framework effectively enhances Vision-Language Model performance on complex Rebus puzzles by combining multiple reasoning approaches and better example selection, demonstrating significant improvements over existing methods.

Abstract: Understanding Rebus Puzzles (Rebus Puzzles use pictures, symbols, and letters
to represent words or phrases creatively) requires a variety of skills such as
image recognition, cognitive skills, commonsense reasoning, multi-step
reasoning, image-based wordplay, etc., making this a challenging task for even
current Vision-Language Models. In this paper, we present
$\left|\,\circlearrowright\,\boxed{\text{BUS}}\,\right|$, a large and diverse
benchmark of $1,333$ English Rebus Puzzles containing different artistic styles
and levels of difficulty, spread across 18 categories such as food, idioms,
sports, finance, entertainment, etc. We also propose $RebusDescProgICE$, a
model-agnostic framework which uses a combination of an unstructured
description and code-based, structured reasoning, along with better,
reasoning-based in-context example selection, improving the performance of
Vision-Language Models on
$\left|\,\circlearrowright\,\boxed{\text{BUS}}\,\right|$ by $2.1-4.1\%$ and
$20-30\%$ using closed-source and open-source models respectively compared to
Chain-of-Thought Reasoning.

</details>


### [210] [Eyes on Target: Gaze-Aware Object Detection in Egocentric Video](https://arxiv.org/abs/2511.01237)
*Vishakha Lall,Yisi Liu*

Main category: cs.CV

TL;DR: Eyes on Target is a gaze-guided object detection framework that uses human gaze data to bias Vision Transformer attention toward attended regions in egocentric videos, improving detection accuracy.


<details>
  <summary>Details</summary>
Motivation: Human gaze provides valuable supervisory signals for understanding visual attention in complex environments, which can enhance object detection by focusing on viewer-prioritized areas rather than treating all regions equally.

Method: Inject gaze-derived features into Vision Transformer's attention mechanism to bias spatial feature selection toward human-attended regions, creating a depth-aware and gaze-guided object detection framework for egocentric videos.

Result: Consistent gains in detection accuracy over gaze-agnostic baselines on both custom simulator dataset and public benchmarks (Ego4D Ego-Motion and Ego-CH-Gaze datasets), with improved performance in evaluating human performance in simulation scenarios.

Conclusion: The gaze-integrated model effectively enhances object detection by leveraging human attention cues, and the introduced gaze-aware attention head importance metric helps interpret how gaze modulates transformer attention dynamics.

Abstract: Human gaze offers rich supervisory signals for understanding visual attention
in complex visual environments. In this paper, we propose Eyes on Target, a
novel depth-aware and gaze-guided object detection framework designed for
egocentric videos. Our approach injects gaze-derived features into the
attention mechanism of a Vision Transformer (ViT), effectively biasing spatial
feature selection toward human-attended regions. Unlike traditional object
detectors that treat all regions equally, our method emphasises
viewer-prioritised areas to enhance object detection. We validate our method on
an egocentric simulator dataset where human visual attention is critical for
task assessment, illustrating its potential in evaluating human performance in
simulation scenarios. We evaluate the effectiveness of our gaze-integrated
model through extensive experiments and ablation studies, demonstrating
consistent gains in detection accuracy over gaze-agnostic baselines on both the
custom simulator dataset and public benchmarks, including Ego4D Ego-Motion and
Ego-CH-Gaze datasets. To interpret model behaviour, we also introduce a
gaze-aware attention head importance metric, revealing how gaze cues modulate
transformer attention dynamics.

</details>


### [211] [Beyond Deceptive Flatness: Dual-Order Solution for Strengthening Adversarial Transferability](https://arxiv.org/abs/2511.01240)
*Zhixuan Zhang,Pingyu Wang,Xingjian Zheng,Linbo Qing,Qi Liu*

Main category: cs.CV

TL;DR: The paper introduces Adversarial Flatness Attack (AFA) to address deceptive flatness in transferable attacks, using dual-order information and MonteCarlo Adversarial Sampling to improve adversarial transferability across models.


<details>
  <summary>Details</summary>
Motivation: Current transferable attacks focus on flat losses but still fall into suboptimal regions (deceptive flatness), limiting their effectiveness against unknown victim models.

Method: Proposes Adversarial Flatness (AF) with theoretical assurance, implements AFA attack using efficient approximation, and enhances sampling with MonteCarlo Adversarial Sampling (MCAS).

Result: Superior performance on ImageNet-compatible dataset compared to six baselines, generating flatter adversarial examples with improved transferability across architectures and input transformations.

Conclusion: The proposed AFA method effectively addresses deceptive flatness and significantly boosts adversarial transferability in black-box scenarios.

Abstract: Transferable attacks generate adversarial examples on surrogate models to
fool unknown victim models, posing real-world threats and growing research
interest. Despite focusing on flat losses for transferable adversarial
examples, recent studies still fall into suboptimal regions, especially the
flat-yet-sharp areas, termed as deceptive flatness. In this paper, we introduce
a novel black-box gradient-based transferable attack from a perspective of
dual-order information. Specifically, we feasibly propose Adversarial Flatness
(AF) to the deceptive flatness problem and a theoretical assurance for
adversarial transferability. Based on this, using an efficient approximation of
our objective, we instantiate our attack as Adversarial Flatness Attack (AFA),
addressing the altered gradient sign issue. Additionally, to further improve
the attack ability, we devise MonteCarlo Adversarial Sampling (MCAS) by
enhancing the inner-loop sampling efficiency. The comprehensive results on
ImageNet-compatible dataset demonstrate superiority over six baselines,
generating adversarial examples in flatter regions and boosting transferability
across model architectures. When tested on input transformation attacks or the
Baidu Cloud API, our method outperforms baselines.

</details>


### [212] [Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models](https://arxiv.org/abs/2511.01618)
*Xiaoyu Zhan,Wenxuan Huang,Hao Sun,Xinyu Fu,Changfeng Ma,Shaosheng Cao,Bohan Jia,Shaohui Lin,Zhenfei Yin,Lei Bai,Wanli Ouyang,Yuanqi Li,Jie Guo,Yanwen Guo*

Main category: cs.CV

TL;DR: The paper introduces Viewpoint Learning to evaluate and improve spatial reasoning in MLLMs, using a 100K dataset and two-stage fine-tuning with SFT and GRPO, achieving significant improvements in 3D reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: To address whether MLLMs can effectively capture detailed spatial information for robust 3D reasoning, particularly cross-view consistency, which is crucial for real-world applications.

Method: Two-stage fine-tuning: 1) Supervised Fine-Tuning on Viewpoint-100K dataset for foundational knowledge, 2) Reinforcement Learning with GRPO algorithm for generalization. Also includes hybrid cold-start initialization for viewpoint representation learning.

Result: Significant activation of spatial reasoning ability in MLLMs, with improved performance on both in-domain and out-of-domain reasoning tasks.

Conclusion: Developing foundational spatial skills in MLLMs is valuable for advancing robotics, autonomous systems, and 3D scene understanding applications.

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have
significantly improved 2D visual understanding, prompting interest in their
application to complex 3D reasoning tasks. However, it remains unclear whether
these models can effectively capture the detailed spatial information required
for robust real-world performance, especially cross-view consistency, a key
requirement for accurate 3D reasoning. Considering this issue, we introduce
Viewpoint Learning, a task designed to evaluate and improve the spatial
reasoning capabilities of MLLMs. We present the Viewpoint-100K dataset,
consisting of 100K object-centric image pairs with diverse viewpoints and
corresponding question-answer pairs. Our approach employs a two-stage
fine-tuning strategy: first, foundational knowledge is injected to the baseline
MLLM via Supervised Fine-Tuning (SFT) on Viewpoint-100K, resulting in
significant improvements across multiple tasks; second, generalization is
enhanced through Reinforcement Learning using the Group Relative Policy
Optimization (GRPO) algorithm on a broader set of questions. Additionally, we
introduce a hybrid cold-start initialization method designed to simultaneously
learn viewpoint representations and maintain coherent reasoning thinking.
Experimental results show that our approach significantly activates the spatial
reasoning ability of MLLM, improving performance on both in-domain and
out-of-domain reasoning tasks. Our findings highlight the value of developing
foundational spatial skills in MLLMs, supporting future progress in robotics,
autonomous systems, and 3D scene understanding.

</details>


### [213] [CenterMamba-SAM: Center-Prioritized Scanning and Temporal Prototypes for Brain Lesion Segmentation](https://arxiv.org/abs/2511.01243)
*Yu Tian,Zhongheng Yang,Chenshi Liu,Yiyun Su,Ziwei Hong,Zexi Gong,Jingyuan Xu*

Main category: cs.CV

TL;DR: CenterMamba-SAM is an end-to-end framework for brain lesion segmentation that uses a frozen pretrained backbone with lightweight adapters. It features a CenterMamba encoder with 3x3 corner-axis-center scanning, memory-driven structural prompts, and memory-augmented multi-scale decoder for improved boundary sensitivity and inter-slice coherence.


<details>
  <summary>Details</summary>
Motivation: Brain lesion segmentation faces challenges from small, low-contrast lesions, anisotropic sampling, and cross-slice discontinuities in medical imaging data.

Method: Freezes pretrained backbone and trains lightweight adapters. Uses CenterMamba encoder with 3x3 corner-axis-center short-sequence scanning for center-prioritized, axis-reinforced information aggregation. Includes memory-driven structural prompt generator with prototype bank and memory-augmented multi-scale decoder with memory attention modules.

Result: Extensive experiments on public benchmarks demonstrate state-of-the-art performance in brain lesion segmentation.

Conclusion: CenterMamba-SAM achieves superior brain lesion segmentation by enhancing sensitivity to weak boundaries and tiny lesions while maintaining inter-slice coherence through its novel scanning strategy and memory mechanisms.

Abstract: Brain lesion segmentation remains challenging due to small, low-contrast
lesions, anisotropic sampling, and cross-slice discontinuities. We propose
CenterMamba-SAM, an end-to-end framework that freezes a pretrained backbone and
trains only lightweight adapters for efficient fine-tuning. At its core is the
CenterMamba encoder, which employs a novel 3x3 corner-axis-center
short-sequence scanning strategy to enable center-prioritized, axis-reinforced,
and diagonally compensated information aggregation. This design enhances
sensitivity to weak boundaries and tiny foci while maintaining sparse yet
effective feature representation. A memory-driven structural prompt generator
maintains a prototype bank across neighboring slices, enabling automatic
synthesis of reliable prompts without user interaction, thereby improving
inter-slice coherence. The memory-augmented multi-scale decoder integrates
memory attention modules at multiple levels, combining deep supervision with
progressive refinement to restore fine details while preserving global
consistency. Extensive experiments on public benchmarks demonstrate that
CenterMamba-SAM achieves state-of-the-art performance in brain lesion
segmentation.

</details>


### [214] [Source-Only Cross-Weather LiDAR via Geometry-Aware Point Drop](https://arxiv.org/abs/2511.01250)
*YoungJae Cheong,Jhonghyun An*

Main category: cs.CV

TL;DR: A Light Geometry-aware adapter improves LiDAR semantic segmentation in adverse weather by preserving neighbor continuity and applying region-aware regularization to structurally fragile areas, achieving significant performance gains without target domain fine-tuning.


<details>
  <summary>Details</summary>
Motivation: LiDAR semantic segmentation performance degrades in adverse weather due to corrupted geometry from refraction, scattering, and point dropouts. Prior methods overlook structural vulnerabilities near boundaries, corners, and sparse regions.

Method: A plug-and-play adapter that aligns azimuth, applies horizontal circular padding to preserve neighbor continuity, uses local-window K-Nearest Neighbors to compute local statistics, and drives region-aware regularization during training only.

Result: Improves mIoU by 7.9 percentage points over data-centric augmentation baseline and by 0.6 points over class-centric regularization baseline in source-only cross-weather evaluation on SemanticKITTI to SemanticSTF.

Conclusion: Geometry-driven regularization is a key direction for all-weather LiDAR segmentation, providing significant robustness improvements with negligible inference cost.

Abstract: LiDAR semantic segmentation degrades in adverse weather because refraction,
scattering, and point dropouts corrupt geometry. Prior work in weather
simulation, mixing-based augmentation, domain randomization, and uncertainty or
boundary regularization improves robustness but still overlooks structural
vulnerabilities near boundaries, corners, and sparse regions. We present a
Light Geometry-aware adapter. The module aligns azimuth and applies horizontal
circular padding to preserve neighbor continuity across the 0~360 degree
wrap-around boundary. A local-window K-Nearest Neighbors gathers nearby points
and computes simple local statistics, which are compressed into compact
geometry-aware cues. During training, these cues drive region-aware
regularization that stabilizes predictions in structurally fragile areas. The
adapter is plug and play, complements augmentation, and can be enabled only
during training with negligible inference cost. We adopt a source-only
cross-weather setup where models train on SemanticKITTI and are evaluated on
SemanticSTF without target labels or fine-tuning. The adapter improves mIoU by
7.9 percentage points over the data-centric augmentation baseline and by 0.6
points over the class-centric regularization baseline. These results indicate
that geometry-driven regularization is a key direction for all-weather LiDAR
segmentation.

</details>


### [215] [MotionStream: Real-Time Video Generation with Interactive Motion Controls](https://arxiv.org/abs/2511.01266)
*Joonghyuk Shin,Zhengqi Li,Richard Zhang,Jun-Yan Zhu,Jaesik Park,Eli Schechtman,Xun Huang*

Main category: cs.CV

TL;DR: MotionStream enables real-time video generation with sub-second latency and up to 29 FPS streaming on a single GPU, allowing infinite-length video generation through causal attention and distillation techniques.


<details>
  <summary>Details</summary>
Motivation: Current motion-conditioned video generation methods suffer from high latency (minutes per video) and non-causal processing that prevents real-time interaction, limiting practical applications.

Method: Distills a bidirectional text-to-video teacher model into a causal student using Self Forcing with Distribution Matching Distillation. Uses sliding-window causal attention with attention sinks and KV cache rolling to enable infinite-length generation with fixed computational cost.

Result: Achieves state-of-the-art results in motion following and video quality while being two orders of magnitude faster than existing methods. Enables real-time streaming at up to 29 FPS on a single GPU.

Conclusion: MotionStream uniquely enables infinite-length streaming video generation with real-time interaction, allowing users to paint trajectories, control cameras, or transfer motion and see results unfold instantly.

Abstract: Current motion-conditioned video generation methods suffer from prohibitive
latency (minutes per video) and non-causal processing that prevents real-time
interaction. We present MotionStream, enabling sub-second latency with up to 29
FPS streaming generation on a single GPU. Our approach begins by augmenting a
text-to-video model with motion control, which generates high-quality videos
that adhere to the global text prompt and local motion guidance, but does not
perform inference on the fly. As such, we distill this bidirectional teacher
into a causal student through Self Forcing with Distribution Matching
Distillation, enabling real-time streaming inference. Several key challenges
arise when generating videos of long, potentially infinite time-horizons: (1)
bridging the domain gap from training on finite length and extrapolating to
infinite horizons, (2) sustaining high quality by preventing error
accumulation, and (3) maintaining fast inference, without incurring growth in
computational cost due to increasing context windows. A key to our approach is
introducing carefully designed sliding-window causal attention, combined with
attention sinks. By incorporating self-rollout with attention sinks and KV
cache rolling during training, we properly simulate inference-time
extrapolations with a fixed context window, enabling constant-speed generation
of arbitrarily long videos. Our models achieve state-of-the-art results in
motion following and video quality while being two orders of magnitude faster,
uniquely enabling infinite-length streaming. With MotionStream, users can paint
trajectories, control cameras, or transfer motion, and see results unfold in
real-time, delivering a truly interactive experience.

</details>


### [216] [PRevivor: Reviving Ancient Chinese Paintings using Prior-Guided Color Transformers](https://arxiv.org/abs/2511.01274)
*Tan Tang,Yanhong Wu,Junming Gao,Yingcai Wu*

Main category: cs.CV

TL;DR: PRevivor is a prior-guided color transformer that restores ancient Chinese paintings by learning from recent paintings, using a two-stage approach of luminance enhancement and hue correction.


<details>
  <summary>Details</summary>
Motivation: Ancient Chinese paintings suffer from irreversible color degradation due to complex chemistry, and progress is hindered by lack of comprehensive datasets and end-to-end digital restoration tools.

Method: Two-stage approach: 1) Luminance enhancement using two variational U-Nets and multi-scale mapping module; 2) Hue correction using dual-branch color query module guided by localized hue priors from faded paintings.

Result: Extensive experiments show PRevivor achieves superior performance both quantitatively and qualitatively compared to state-of-the-art colorization methods.

Conclusion: PRevivor effectively revives colors in ancient Chinese paintings by leveraging prior knowledge from recent paintings through a structured two-stage restoration process.

Abstract: Ancient Chinese paintings are a valuable cultural heritage that is damaged by
irreversible color degradation. Reviving color-degraded paintings is
extraordinarily difficult due to the complex chemistry mechanism. Progress is
further slowed by the lack of comprehensive, high-quality datasets, which
hampers the creation of end-to-end digital restoration tools. To revive colors,
we propose PRevivor, a prior-guided color transformer that learns from recent
paintings (e.g., Ming and Qing Dynasty) to restore ancient ones (e.g., Tang and
Song Dynasty). To develop PRevivor, we decompose color restoration into two
sequential sub-tasks: luminance enhancement and hue correction. For luminance
enhancement, we employ two variational U-Nets and a multi-scale mapping module
to translate faded luminance into restored counterparts. For hue correction, we
design a dual-branch color query module guided by localized hue priors
extracted from faded paintings. Specifically, one branch focuses attention on
regions guided by masked priors, enforcing localized hue correction, whereas
the other branch remains unconstrained to maintain a global reasoning
capability. To evaluate PRevivor, we conduct extensive experiments against
state-of-the-art colorization methods. The results demonstrate superior
performance both quantitatively and qualitatively.

</details>


### [217] [Adaptation of Foundation Models for Medical Image Analysis: Strategies, Challenges, and Future Directions](https://arxiv.org/abs/2511.01284)
*Karma Phuntsho,Abdullah,Kyungmi Lee,Ickjai Lee,Euijoon Ahn*

Main category: cs.CV

TL;DR: This review paper examines adaptation strategies for foundation models in medical imaging, addressing challenges like domain shifts, data limitations, and privacy requirements while proposing future directions.


<details>
  <summary>Details</summary>
Motivation: Foundation models offer transformative potential for medical image analysis but face challenges in adapting to real-world clinical practice, including domain shifts, limited annotated data, computational demands, and privacy constraints.

Method: Comprehensive assessment of adaptation strategies including supervised fine-tuning, domain-specific pretraining, parameter-efficient fine-tuning, self-supervised learning, hybrid methods, and multimodal frameworks.

Result: The review evaluates performance gains, clinical applicability, and limitations of various adaptation approaches, identifying trade-offs and unresolved challenges in medical imaging applications.

Conclusion: The paper provides a roadmap for developing adaptive, trustworthy, and clinically integrated foundation models through emerging directions like continual learning, federated approaches, hybrid self-supervised learning, data-centric pipelines, and systematic benchmarking.

Abstract: Foundation models (FMs) have emerged as a transformative paradigm in medical
image analysis, offering the potential to provide generalizable, task-agnostic
solutions across a wide range of clinical tasks and imaging modalities. Their
capacity to learn transferable representations from large-scale data has the
potential to address the limitations of conventional task-specific models.
However, adaptation of FMs to real-world clinical practice remains constrained
by key challenges, including domain shifts, limited availability of
high-quality annotated data, substantial computational demands, and strict
privacy requirements. This review presents a comprehensive assessment of
strategies for adapting FMs to the specific demands of medical imaging. We
examine approaches such as supervised fine-tuning, domain-specific pretraining,
parameter-efficient fine-tuning, self-supervised learning, hybrid methods, and
multimodal or cross-modal frameworks. For each, we evaluate reported
performance gains, clinical applicability, and limitations, while identifying
trade-offs and unresolved challenges that prior reviews have often overlooked.
Beyond these established techniques, we also highlight emerging directions
aimed at addressing current gaps. These include continual learning to enable
dynamic deployment, federated and privacy-preserving approaches to safeguard
sensitive data, hybrid self-supervised learning to enhance data efficiency,
data-centric pipelines that combine synthetic generation with human-in-the-loop
validation, and systematic benchmarking to assess robust generalization under
real-world clinical variability. By outlining these strategies and associated
research gaps, this review provides a roadmap for developing adaptive,
trustworthy, and clinically integrated FMs capable of meeting the demands of
real-world medical imaging.

</details>


### [218] [Detecting Generated Images by Fitting Natural Image Distributions](https://arxiv.org/abs/2511.01293)
*Yonggang Zhang,Jun Nie,Xinmei Tian,Mingming Gong,Kun Zhang,Bo Han*

Main category: cs.CV

TL;DR: A novel framework for detecting generated images by exploiting geometric differences between natural and generated image manifolds, using orthogonal gradient subspaces and normalizing flows to amplify detectable differences.


<details>
  <summary>Details</summary>
Motivation: Increasing realism of generated images raises concerns about misuse, requiring robust detection methods that don't heavily depend on training data quantity and quality like current binary classifiers.

Method: Uses a pair of functions that yield consistent outputs for natural images but divergent outputs for generated ones, leveraging orthogonal gradient subspaces. Detects generated images when transformation along data manifold causes significant change in self-supervised model loss. Uses normalizing flows to amplify differences.

Result: Extensive experiments demonstrate the efficacy of the method in detecting generated images.

Conclusion: The proposed framework provides an effective detection method that exploits geometric manifold differences and uses normalizing flows to address diminishing disparities in advanced generative models.

Abstract: The increasing realism of generated images has raised significant concerns
about their potential misuse, necessitating robust detection methods. Current
approaches mainly rely on training binary classifiers, which depend heavily on
the quantity and quality of available generated images. In this work, we
propose a novel framework that exploits geometric differences between the data
manifolds of natural and generated images. To exploit this difference, we
employ a pair of functions engineered to yield consistent outputs for natural
images but divergent outputs for generated ones, leveraging the property that
their gradients reside in mutually orthogonal subspaces. This design enables a
simple yet effective detection method: an image is identified as generated if a
transformation along its data manifold induces a significant change in the loss
value of a self-supervised model pre-trained on natural images. Further more,
to address diminishing manifold disparities in advanced generative models, we
leverage normalizing flows to amplify detectable differences by extruding
generated images away from the natural image manifold. Extensive experiments
demonstrate the efficacy of this method. Code is available at
https://github.com/tmlr-group/ConV.

</details>


### [219] [UniREditBench: A Unified Reasoning-based Image Editing Benchmark](https://arxiv.org/abs/2511.01295)
*Feng Han,Yibin Wang,Chenglin Li,Zheming Liang,Dianyi Wang,Yang Jiao,Zhipeng Wei,Chao Gong,Cheng Jin,Jingjing Chen,Jiaqi Wang*

Main category: cs.CV

TL;DR: UniREditBench is a unified benchmark for reasoning-based image editing evaluation that addresses limitations of existing benchmarks by covering multi-object interactions, game-world scenarios, and using multimodal dual-reference evaluation.


<details>
  <summary>Details</summary>
Motivation: Current generative models struggle with complex image editing tasks requiring implicit reasoning, and existing benchmarks focus mainly on single-object transformations in realistic scenarios while overlooking multi-object interactions and game-world scenarios.

Method: Proposed UniREditBench with 2,700 curated samples across real- and game-world scenarios, introduced multimodal dual-reference evaluation (textual + ground-truth image references), and created UniREdit-Data-100K synthetic dataset with CoT reasoning annotations.

Result: Fine-tuned Bagel on the synthetic dataset to create UniREdit-Bagel, showing substantial improvements in both in-domain and out-of-distribution settings. Benchmarking revealed strengths and weaknesses of various image editing models.

Conclusion: UniREditBench provides a comprehensive framework for evaluating reasoning-based image editing, addressing key limitations of existing benchmarks and enabling better assessment of model capabilities across diverse scenarios.

Abstract: Recent advances in multi-modal generative models have driven substantial
improvements in image editing. However, current generative models still
struggle with handling diverse and complex image editing tasks that require
implicit reasoning, underscoring the need for a comprehensive benchmark to
systematically assess their performance across various reasoning scenarios.
Existing benchmarks primarily focus on single-object attribute transformation
in realistic scenarios, which, while effective, encounter two key challenges:
(1) they largely overlook multi-object interactions as well as game-world
scenarios that involve human-defined rules, which are common in real-life
applications; (2) they only rely on textual references to evaluate the
generated images, potentially leading to systematic misjudgments, especially in
complex reasoning scenarios. To this end, this work proposes UniREditBench, a
unified benchmark for reasoning-based image editing evaluation. It comprises
2,700 meticulously curated samples, covering both real- and game-world
scenarios across 8 primary dimensions and 18 sub-dimensions. To improve
evaluation reliability, we introduce multimodal dual-reference evaluation,
providing both textual and ground-truth image references for each sample
assessment. Furthermore, we design an automated multi-scenario data synthesis
pipeline and construct UniREdit-Data-100K, a large-scale synthetic dataset with
high-quality chain-of-thought (CoT) reasoning annotations. We fine-tune Bagel
on this dataset and develop UniREdit-Bagel, demonstrating substantial
improvements in both in-domain and out-of-distribution settings. Through
thorough benchmarking of both open-source and closed-source image editing
models, we reveal their strengths and weaknesses across various aspects.

</details>


### [220] [REASON: Probability map-guided dual-branch fusion framework for gastric content assessment](https://arxiv.org/abs/2511.01302)
*Nu-Fnag Xiao,De-Xing Huang,Le-Tian Wang,Mei-Jiang Gui,Qi Fu,Xiao-Liang Xie,Shi-Qi Liu,Shuangyi Wang,Zeng-Guang Hou,Ying-Wei Wang,Xiao-Hu Zhou*

Main category: cs.CV

TL;DR: REASON is a two-stage framework that uses probability maps and dual-branch fusion of RLD and SUP ultrasound views to automate gastric content assessment for aspiration risk stratification.


<details>
  <summary>Details</summary>
Motivation: Traditional manual tracing methods for gastric content assessment from ultrasound are inefficient and inaccurate, limiting their clinical utility for preoperative aspiration risk assessment.

Method: Two-stage framework: Stage 1 uses segmentation to generate probability maps that suppress artifacts and highlight gastric anatomy. Stage 2 employs a dual-branch classifier that fuses information from right lateral decubitus (RLD) and supine (SUP) ultrasound views.

Result: The framework significantly outperforms current state-of-the-art approaches on a self-collected dataset, demonstrating superior performance in gastric content assessment.

Conclusion: REASON shows great promise for automated preoperative aspiration risk assessment, offering a more robust, efficient, and accurate solution for clinical practice compared to traditional methods.

Abstract: Accurate assessment of gastric content from ultrasound is critical for
stratifying aspiration risk at induction of general anesthesia. However,
traditional methods rely on manual tracing of gastric antra and empirical
formulas, which face significant limitations in both efficiency and accuracy.
To address these challenges, a novel two-stage probability map-guided
dual-branch fusion framework (REASON) for gastric content assessment is
proposed. In stage 1, a segmentation model generates probability maps that
suppress artifacts and highlight gastric anatomy. In stage 2, a dual-branch
classifier fuses information from two standard views, right lateral decubitus
(RLD) and supine (SUP), to improve the discrimination of learned features.
Experimental results on a self-collected dataset demonstrate that the proposed
framework outperforms current state-of-the-art approaches by a significant
margin. This framework shows great promise for automated preoperative
aspiration risk assessment, offering a more robust, efficient, and accurate
solution for clinical practice.

</details>


### [221] [Positive Semi-definite Latent Factor Grouping-Boosted Cluster-reasoning Instance Disentangled Learning for WSI Representation](https://arxiv.org/abs/2511.01304)
*Chentao Li,Behzad Bozorgtabar,Yifang Ping,Pan Huang,Jing Qin*

Main category: cs.CV

TL;DR: A novel MIL framework with latent factor grouping and cluster-reasoning instance disentangling that addresses spatial, semantic, and decision entanglements in whole-slide image analysis, achieving superior performance and pathologist-aligned interpretability.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of multiple instance learning (MIL) in whole-slide pathology images where spatial, semantic, and decision entanglements among instances restrict representation quality and interpretability.

Method: Three-phase framework: 1) Positive semi-definite latent factor grouping to map instances into latent subspace (spatial disentanglement), 2) Instance probability counterfactual inference via cluster-reasoning (semantic disentanglement), 3) Generalized linear weighted decision with instance effect re-weighting (decision disentanglement).

Result: Extensive experiments on multicentre datasets show the model outperforms all state-of-the-art models and achieves pathologist-aligned interpretability through disentangled representations and transparent decision-making.

Conclusion: The proposed framework effectively addresses three types of entanglements in MIL for whole-slide images, delivering both superior performance and enhanced interpretability aligned with pathologist reasoning.

Abstract: Multiple instance learning (MIL) has been widely used for representing
whole-slide pathology images. However, spatial, semantic, and decision
entanglements among instances limit its representation and interpretability. To
address these challenges, we propose a latent factor grouping-boosted
cluster-reasoning instance disentangled learning framework for whole-slide
image (WSI) interpretable representation in three phases. First, we introduce a
novel positive semi-definite latent factor grouping that maps instances into a
latent subspace, effectively mitigating spatial entanglement in MIL. To
alleviate semantic entanglement, we employs instance probability counterfactual
inference and optimization via cluster-reasoning instance disentangling.
Finally, we employ a generalized linear weighted decision via instance effect
re-weighting to address decision entanglement. Extensive experiments on
multicentre datasets demonstrate that our model outperforms all
state-of-the-art models. Moreover, it attains pathologist-aligned
interpretability through disentangled representations and a transparent
decision-making process.

</details>


### [222] [Perturb a Model, Not an Image: Towards Robust Privacy Protection via Anti-Personalized Diffusion Models](https://arxiv.org/abs/2511.01307)
*Tae-Young Lee,Juwon Seo,Jong Hwan Ko,Gyeong-Moon Park*

Main category: cs.CV

TL;DR: APDM is a novel framework that protects against unauthorized personalization in diffusion models by shifting protection from images to the model itself, using a new loss function and dual-path optimization strategy.


<details>
  <summary>Details</summary>
Motivation: Existing methods for preventing unauthorized content generation through personalization rely on unrealistic assumptions and fail against simple countermeasures like clean images or basic transformations, creating significant privacy risks.

Method: Proposes Direct Protective Optimization (DPO) loss function and Learning to Protect (L2P) dual-path optimization strategy that alternates between personalization and protection paths to simulate future attacks and reinforce protection.

Result: APDM outperforms existing methods and achieves state-of-the-art performance in preventing unauthorized personalization while maintaining generative quality.

Conclusion: The framework effectively disrupts subject personalization in diffusion models without compromising generation quality, providing robust protection against misuse of personalization techniques.

Abstract: Recent advances in diffusion models have enabled high-quality synthesis of
specific subjects, such as identities or objects. This capability, while
unlocking new possibilities in content creation, also introduces significant
privacy risks, as personalization techniques can be misused by malicious users
to generate unauthorized content. Although several studies have attempted to
counter this by generating adversarially perturbed samples designed to disrupt
personalization, they rely on unrealistic assumptions and become ineffective in
the presence of even a few clean images or under simple image transformations.
To address these challenges, we shift the protection target from the images to
the diffusion model itself to hinder the personalization of specific subjects,
through our novel framework called Anti-Personalized Diffusion Models (APDM).
We first provide a theoretical analysis demonstrating that a naive approach of
existing loss functions to diffusion models is inherently incapable of ensuring
convergence for robust anti-personalization. Motivated by this finding, we
introduce Direct Protective Optimization (DPO), a novel loss function that
effectively disrupts subject personalization in the target model without
compromising generative quality. Moreover, we propose a new dual-path
optimization strategy, coined Learning to Protect (L2P). By alternating between
personalization and protection paths, L2P simulates future personalization
trajectories and adaptively reinforces protection at each step. Experimental
results demonstrate that our framework outperforms existing methods, achieving
state-of-the-art performance in preventing unauthorized personalization. The
code is available at https://github.com/KU-VGI/APDM.

</details>


### [223] [MVSMamba: Multi-View Stereo with State Space Model](https://arxiv.org/abs/2511.01315)
*Jianfei Jiang,Qiankun Liu,Hongyuan Liu,Haochen Yu,Liyong Wang,Jiansheng Chen,Huimin Ma*

Main category: cs.CV

TL;DR: MVSMamba is the first Mamba-based Multi-View Stereo network that achieves superior performance and efficiency through a novel Dynamic Mamba module with reference-centered dynamic scanning strategy.


<details>
  <summary>Details</summary>
Motivation: Transformers in MVS have quadratic complexity, making it challenging to balance performance and efficiency. Mamba's global modeling capability with linear complexity offers a promising alternative.

Method: Proposes MVSMamba with Dynamic Mamba module using reference-centered dynamic scanning for efficient intra-/inter-view feature interaction, omnidirectional multi-view representations, and multi-scale global feature aggregation.

Result: Outperforms state-of-the-art MVS methods on DTU dataset and Tanks-and-Temples benchmark with both superior performance and efficiency.

Conclusion: MVSMamba demonstrates that Mamba architecture can effectively replace Transformers in MVS, achieving better performance with linear complexity.

Abstract: Robust feature representations are essential for learning-based Multi-View
Stereo (MVS), which relies on accurate feature matching. Recent MVS methods
leverage Transformers to capture long-range dependencies based on local
features extracted by conventional feature pyramid networks. However, the
quadratic complexity of Transformer-based MVS methods poses challenges to
balance performance and efficiency. Motivated by the global modeling capability
and linear complexity of the Mamba architecture, we propose MVSMamba, the first
Mamba-based MVS network. MVSMamba enables efficient global feature aggregation
with minimal computational overhead. To fully exploit Mamba's potential in MVS,
we propose a Dynamic Mamba module (DM-module) based on a novel
reference-centered dynamic scanning strategy, which enables: (1) Efficient
intra- and inter-view feature interaction from the reference to source views,
(2) Omnidirectional multi-view feature representations, and (3) Multi-scale
global feature aggregation. Extensive experimental results demonstrate MVSMamba
outperforms state-of-the-art MVS methods on the DTU dataset and the
Tanks-and-Temples benchmark with both superior performance and efficiency. The
source code is available at https://github.com/JianfeiJ/MVSMamba.

</details>


### [224] [A Generative Adversarial Approach to Adversarial Attacks Guided by Contrastive Language-Image Pre-trained Model](https://arxiv.org/abs/2511.01317)
*Sampriti Soor,Alik Pramanick,Jothiprakash K,Arijit Sur*

Main category: cs.CV

TL;DR: A generative adversarial attack method using CLIP model to create effective and visually imperceptible perturbations that deceive multilabel classifiers while maintaining high visual fidelity.


<details>
  <summary>Details</summary>
Motivation: Adversarial attacks can deceive deep learning models with unnoticeable alterations, highlighting the need for robust attack methods that exploit model vulnerabilities.

Method: Integrates CLIP model's text-image alignment with SSAE's concentrated perturbation strategy and GAMA's dissimilar text embeddings to generate adversarial examples using natural language semantics and guided loss.

Result: The method performs competitively across various black-box victim models, achieving comparable or superior results to existing techniques while preserving greater visual fidelity.

Conclusion: The proposed approach effectively generates adversarial perturbations that deceive classification models while maintaining high structural similarity to original images, demonstrating the power of combining CLIP's semantic understanding with perturbation strategies.

Abstract: The rapid growth of deep learning has brought about powerful models that can
handle various tasks, like identifying images and understanding language.
However, adversarial attacks, an unnoticed alteration, can deceive models,
leading to inaccurate predictions. In this paper, a generative adversarial
attack method is proposed that uses the CLIP model to create highly effective
and visually imperceptible adversarial perturbations. The CLIP model's ability
to align text and image representation helps incorporate natural language
semantics with a guided loss to generate effective adversarial examples that
look identical to the original inputs. This integration allows extensive scene
manipulation, creating perturbations in multi-object environments specifically
designed to deceive multilabel classifiers. Our approach integrates the
concentrated perturbation strategy from Saliency-based Auto-Encoder (SSAE) with
the dissimilar text embeddings similar to Generative Adversarial Multi-Object
Scene Attacks (GAMA), resulting in perturbations that both deceive
classification models and maintain high structural similarity to the original
images. The model was tested on various tasks across diverse black-box victim
models. The experimental results show that our method performs competitively,
achieving comparable or superior results to existing techniques, while
preserving greater visual fidelity.

</details>


### [225] [RDTE-UNet: A Boundary and Detail Aware UNet for Precise Medical Image Segmentation](https://arxiv.org/abs/2511.01328)
*Jierui Qu,Jianchun Zhao*

Main category: cs.CV

TL;DR: RDTE-UNet is a medical image segmentation network that combines local modeling with global context to improve boundary delineation and detail preservation using a hybrid ResBlock-Transformer backbone and three specialized modules.


<details>
  <summary>Details</summary>
Motivation: Medical image segmentation faces challenges due to anatomical variability and boundary ambiguity, which hinder reliable delineation of fine structures in computer-assisted diagnosis and treatment planning.

Method: RDTE-UNet uses a hybrid ResBlock detail-aware Transformer backbone with three key modules: ASBE for adaptive boundary enhancement, HVDA for fine-grained feature modeling, and EulerFF for fusion weighting guided by Euler's formula.

Result: On Synapse and BUSI datasets, RDTE-UNet achieved comparable segmentation accuracy and boundary quality, demonstrating improved structural consistency and boundary accuracy across morphology, orientation, and scale.

Conclusion: The proposed RDTE-UNet effectively unifies local and global modeling to enhance medical image segmentation, particularly for boundary delineation and detail preservation in challenging anatomical structures.

Abstract: Medical image segmentation is essential for computer-assisted diagnosis and
treatment planning, yet substantial anatomical variability and boundary
ambiguity hinder reliable delineation of fine structures. We propose RDTE-UNet,
a segmentation network that unifies local modeling with global context to
strengthen boundary delineation and detail preservation. RDTE-UNet employs a
hybrid ResBlock detail-aware Transformer backbone and three modules: ASBE for
adaptive boundary enhancement, HVDA for fine-grained feature modeling, and
EulerFF for fusion weighting guided by Euler's formula. Together, these
components improve structural consistency and boundary accuracy across
morphology, orientation, and scale. On Synapse and BUSI dataset, RDTE-UNet has
achieved a comparable level in terms of segmentation accuracy and boundary
quality.

</details>


### [226] [MIQ-SAM3D: From Single-Point Prompt to Multi-Instance Segmentation via Competitive Query Refinement](https://arxiv.org/abs/2511.01345)
*Jierui Qu,Jianchun Zhao*

Main category: cs.CV

TL;DR: MIQ-SAM3D is a multi-instance 3D segmentation framework that enables single-point-to-multi-instance segmentation using competitive query optimization and hybrid CNN-Transformer architecture.


<details>
  <summary>Details</summary>
Motivation: Current SAM-based interactive segmentation methods follow single-point-to-single-object paradigm, limiting multi-lesion segmentation. ViT backbones capture global context but miss local details needed for medical image segmentation.

Method: Uses prompt-conditioned instance-query generator to transform single point prompts into multiple specialized queries. Employs hybrid CNN-Transformer encoder with spatial gating to inject CNN boundary saliency into ViT self-attention. Features competitively optimized query decoder for parallel multi-instance prediction.

Result: Achieved comparable performance on LiTS17 and KiTS21 datasets. Exhibits strong robustness to prompts and provides efficient annotation for multi-lesion cases.

Conclusion: MIQ-SAM3D provides a practical solution for efficient annotation of clinically relevant multi-lesion cases through its single-point-to-multi-instance approach and competitive query optimization.

Abstract: Accurate segmentation of medical images is fundamental to tumor diagnosis and
treatment planning. SAM-based interactive segmentation has gained attention for
its strong generalization, but most methods follow a
single-point-to-single-object paradigm, which limits multi-lesion segmentation.
Moreover, ViT backbones capture global context but often miss high-fidelity
local details. We propose MIQ-SAM3D, a multi-instance 3D segmentation framework
with a competitive query optimization strategy that shifts from
single-point-to-single-mask to single-point-to-multi-instance. A
prompt-conditioned instance-query generator transforms a single point prompt
into multiple specialized queries, enabling retrieval of all semantically
similar lesions across the 3D volume from a single exemplar. A hybrid
CNN-Transformer encoder injects CNN-derived boundary saliency into ViT
self-attention via spatial gating. A competitively optimized query decoder then
enables end-to-end, parallel, multi-instance prediction through inter-query
competition. On LiTS17 and KiTS21 dataset, MIQ-SAM3D achieved comparable levels
and exhibits strong robustness to prompts, providing a practical solution for
efficient annotation of clinically relevant multi-lesion cases.

</details>


### [227] [Expanding the Content-Style Frontier: a Balanced Subspace Blending Approach for Content-Style LoRA Fusion](https://arxiv.org/abs/2511.01355)
*Linhao Huang*

Main category: cs.CV

TL;DR: Proposes a method to expand content-style frontier in text-to-image diffusion models by addressing content loss at high style intensities using Content-Style Subspace Blending and Balance loss.


<details>
  <summary>Details</summary>
Motivation: Previous methods only evaluated content similarity under single style intensity, but increasing style intensity causes significant content feature loss, leading to suboptimal content-style trade-offs.

Method: Uses Content-Style Subspace Blending and a Content-Style Balance loss to improve content preservation across varying style intensities.

Result: Outperforms existing techniques in qualitative and quantitative evaluations, achieving superior content-style trade-off with significantly lower IGD and GD scores.

Conclusion: The proposed approach successfully expands the content-style frontier, enabling better preservation of content features across different style intensities in text-to-image generation.

Abstract: Recent advancements in text-to-image diffusion models have significantly
improved the personalization and stylization of generated images. However,
previous studies have only assessed content similarity under a single style
intensity. In our experiments, we observe that increasing style intensity leads
to a significant loss of content features, resulting in a suboptimal
content-style frontier. To address this, we propose a novel approach to expand
the content-style frontier by leveraging Content-Style Subspace Blending and a
Content-Style Balance loss. Our method improves content similarity across
varying style intensities, significantly broadening the content-style frontier.
Extensive experiments demonstrate that our approach outperforms existing
techniques in both qualitative and quantitative evaluations, achieving superior
content-style trade-off with significantly lower Inverted Generational Distance
(IGD) and Generational Distance (GD) scores compared to current methods.

</details>


### [228] [CMI-MTL: Cross-Mamba interaction based multi-task learning for medical visual question answering](https://arxiv.org/abs/2511.01357)
*Qiangguo Jin,Xianyao Zheng,Hui Cui,Changming Sun,Yuqi Fang,Cong Cong,Ran Su,Leyi Wei,Ping Xuan,Junbo Wang*

Main category: cs.CV

TL;DR: CMI-MTL is a novel framework for medical visual question answering that uses cross-modal Mamba interactions and multi-task learning to better handle fine-grained visual-text alignment and free-form answers, outperforming existing methods on three Med-VQA datasets.


<details>
  <summary>Details</summary>
Motivation: Existing self-attention based methods struggle with cross-modal semantic alignment between vision and language, and classification-based approaches can't adapt to free-form answer diversity while overlooking detailed semantic information.

Method: The framework has three key modules: FVTA for fine-grained visual-text feature alignment, CIFR for cross-modal sequential interactions, and FFAE for free-form answer-enhanced multi-task learning that leverages auxiliary knowledge from open-ended questions.

Result: CMI-MTL outperforms state-of-the-art methods on three Med-VQA datasets: VQA-RAD, SLAKE, and OVQA, with interpretability experiments proving its effectiveness.

Conclusion: The proposed CMI-MTL framework successfully addresses cross-modal alignment challenges in Med-VQA and adapts to free-form answer diversity through multi-task learning, demonstrating superior performance across multiple datasets.

Abstract: Medical visual question answering (Med-VQA) is a crucial multimodal task in
clinical decision support and telemedicine. Recent self-attention based methods
struggle to effectively handle cross-modal semantic alignments between vision
and language. Moreover, classification-based methods rely on predefined answer
sets. Treating this task as a simple classification problem may make it unable
to adapt to the diversity of free-form answers and overlook the detailed
semantic information of free-form answers. In order to tackle these challenges,
we introduce a Cross-Mamba Interaction based Multi-Task Learning (CMI-MTL)
framework that learns cross-modal feature representations from images and
texts. CMI-MTL comprises three key modules: fine-grained visual-text feature
alignment (FVTA), cross-modal interleaved feature representation (CIFR), and
free-form answer-enhanced multi-task learning (FFAE). FVTA extracts the most
relevant regions in image-text pairs through fine-grained visual-text feature
alignment. CIFR captures cross-modal sequential interactions via cross-modal
interleaved feature representation. FFAE leverages auxiliary knowledge from
open-ended questions through free-form answer-enhanced multi-task learning,
improving the model's capability for open-ended Med-VQA. Experimental results
show that CMI-MTL outperforms the existing state-of-the-art methods on three
Med-VQA datasets: VQA-RAD, SLAKE, and OVQA. Furthermore, we conduct more
interpretability experiments to prove the effectiveness. The code is publicly
available at https://github.com/BioMedIA-repo/CMI-MTL.

</details>


### [229] [EREBUS: End-to-end Robust Event Based Underwater Simulation](https://arxiv.org/abs/2511.01381)
*Hitesh Kyatham,Arjun Suresh,Aadi Palnitkar,Yiannis Aloimonos*

Main category: cs.CV

TL;DR: A pipeline for generating realistic synthetic data of event-based cameras on AUVs in underwater environments to train vision models.


<details>
  <summary>Details</summary>
Motivation: Underwater environments pose challenges like poor lighting and high dynamic range that traditional vision techniques struggle with, while event-based cameras offer advantages by tracking frame-by-frame changes.

Method: Developed a pipeline to generate synthetic data of event-based cameras mounted on Autonomous Underwater Vehicles in underwater settings.

Result: Demonstrated effectiveness through rock detection tasks in poor visibility conditions with suspended particulate matter.

Conclusion: The approach can be generalized to other underwater vision tasks beyond rock detection.

Abstract: The underwater domain presents a vast array of challenges for roboticists and
computer vision researchers alike, such as poor lighting conditions and high
dynamic range scenes. In these adverse conditions, traditional vision
techniques struggle to adapt and lead to suboptimal performance. Event-based
cameras present an attractive solution to this problem, mitigating the issues
of traditional cameras by tracking changes in the footage on a frame-by-frame
basis. In this paper, we introduce a pipeline which can be used to generate
realistic synthetic data of an event-based camera mounted to an AUV (Autonomous
Underwater Vehicle) in an underwater environment for training vision models. We
demonstrate the effectiveness of our pipeline using the task of rock detection
with poor visibility and suspended particulate matter, but the approach can be
generalized to other underwater tasks.

</details>


### [230] [SEPS: Semantic-enhanced Patch Slimming Framework for fine-grained cross-modal alignment](https://arxiv.org/abs/2511.01390)
*Xinyu Mao,Junsi Li,Haoji Zhang,Yu Liang,Ming Sun*

Main category: cs.CV

TL;DR: SEPS framework addresses patch redundancy and ambiguity in fine-grained cross-modal alignment by integrating unified semantics from dense and sparse texts and using relevance-aware selection to improve patch-word correspondences.


<details>
  <summary>Details</summary>
Motivation: Current approaches struggle with patch redundancy and ambiguity due to information density disparities between vision and language. MLLMs help but their dense outputs conflict with sparse captions, and quantifying semantic relevance between visual patches and text remains challenging.

Method: Two-stage mechanism that integrates unified semantics from both dense and sparse texts to identify salient visual patches, plus relevance-aware selection with mean value computation to highlight crucial patch-word correspondences.

Result: SEPS achieves 23%-86% improvement in rSum across diverse model architectures on Flickr30K and MS-COCO datasets, with notable enhancements in text-to-image retrieval.

Conclusion: SEPS effectively addresses patch redundancy and ambiguity in cross-modal alignment through semantic enhancement and relevance-aware selection, demonstrating superior performance over existing approaches.

Abstract: Fine-grained cross-modal alignment aims to establish precise local
correspondences between vision and language, forming a cornerstone for visual
question answering and related multimodal applications. Current approaches face
challenges in addressing patch redundancy and ambiguity, which arise from the
inherent information density disparities across modalities. Recently,
Multimodal Large Language Models (MLLMs) have emerged as promising solutions to
bridge this gap through their robust semantic generation capabilities. However,
the dense textual outputs from MLLMs may introduce conflicts with the original
sparse captions. Furthermore, accurately quantifying semantic relevance between
rich visual patches and concise textual descriptions remains a core challenge.
To overcome these limitations, we introduce the Semantic-Enhanced Patch
Slimming (SEPS) framework, which systematically addresses patch redundancy and
ambiguity. Our approach employs a two-stage mechanism to integrate unified
semantics from both dense and sparse texts, enabling the identification of
salient visual patches. Additionally, it leverages relevance-aware selection
with mean value computation to highlight crucial patch-word correspondences,
thereby improving cross-modal similarity assessment. Comprehensive experiments
on Flickr30K and MS-COCO datasets validate that SEPS achieves superior
performance, surpassing existing approaches by 23\%-86\% in rSum across diverse
model architectures, with notable enhancements in text-to-image retrieval
scenarios. Our implementation is available at
https://github.com/Sweet4tars/seps.git.

</details>


### [231] [Semantic BIM enrichment for firefighting assets: Fire-ART dataset and panoramic image-based 3D reconstruction](https://arxiv.org/abs/2511.01399)
*Ya Wen,Yutong Qiao,Chi Chiu Lam,Ioannis Brilakis,Sanghoon Lee,Mun On Wong*

Main category: cs.CV

TL;DR: This paper introduces Fire-ART dataset and a panoramic image-based reconstruction approach for automated firefighting asset recognition and BIM integration, achieving good accuracy in real-world validations.


<details>
  <summary>Details</summary>
Motivation: Conventional firefighting asset management methods are inefficient due to limited automated recognition and reconstruction capabilities, creating a need for better digital management solutions.

Method: Developed Fire-ART dataset with 2,626 images and 6,627 instances of 15 assets, and created a reconstruction approach using modified cube-map conversion and radius-based spherical camera projection for enhanced recognition and localization.

Result: Achieved F1-scores of 73% and 88% with localization errors of 0.620 and 0.428 meters respectively in two real-world case studies.

Conclusion: The Fire-ART dataset and reconstruction approach provide valuable resources and robust technical solutions for enhancing accurate digital management of fire safety equipment.

Abstract: Inventory management of firefighting assets is crucial for emergency
preparedness, risk assessment, and on-site fire response. However, conventional
methods are inefficient due to limited capabilities in automated asset
recognition and reconstruction. To address the challenge, this research
introduces the Fire-ART dataset and develops a panoramic image-based
reconstruction approach for semantic enrichment of firefighting assets into BIM
models. The Fire-ART dataset covers 15 fundamental assets, comprising 2,626
images and 6,627 instances, making it an extensive and publicly accessible
dataset for asset recognition. In addition, the reconstruction approach
integrates modified cube-map conversion and radius-based spherical camera
projection to enhance recognition and localization accuracy. Through
validations with two real-world case studies, the proposed approach achieves
F1-scores of 73% and 88% and localization errors of 0.620 and 0.428 meters,
respectively. The Fire-ART dataset and the reconstruction approach offer
valuable resources and robust technical solutions to enhance the accurate
digital management of fire safety equipment.

</details>


### [232] [Extremal Contours: Gradient-driven contours for compact visual attribution](https://arxiv.org/abs/2511.01411)
*Reza Karimzadeh,Albert Alonso,Frans Zdyb,Julius B. Kirkegaard,Bulat Ibragimov*

Main category: cs.CV

TL;DR: A training-free explanation method that uses smooth tunable contours instead of dense perturbation masks, parameterized by Fourier series and optimized with classifier gradients to produce compact, interpretable regions with high fidelity.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of fragmented and overfitted dense perturbation masks in vision model explanations that require careful post-processing, aiming for faithful yet compact explanations.

Method: Parameterizes star-convex regions using truncated Fourier series, optimizes under extremal preserve/delete objective using classifier gradients, guarantees single connected masks, and extends to multi-contour for multiple object localization.

Result: Matches extremal fidelity of dense masks while producing compact regions with improved consistency, achieves higher relevance mass and lower complexity than baselines, especially strong on DINO models with 15%+ relevance mass improvement and positive faithfulness correlations.

Conclusion: The method provides robust, interpretable explanations with explicit area control, stable boundary updates, and handles multiple objects within the same framework, outperforming gradient and perturbation baselines across benchmarks.

Abstract: Faithful yet compact explanations for vision models remain a challenge, as
commonly used dense perturbation masks are often fragmented and overfitted,
needing careful post-processing. Here, we present a training-free explanation
method that replaces dense masks with smooth tunable contours. A star-convex
region is parameterized by a truncated Fourier series and optimized under an
extremal preserve/delete objective using the classifier gradients. The approach
guarantees a single, simply connected mask, cuts the number of free parameters
by orders of magnitude, and yields stable boundary updates without cleanup.
Restricting solutions to low-dimensional, smooth contours makes the method
robust to adversarial masking artifacts. On ImageNet classifiers, it matches
the extremal fidelity of dense masks while producing compact, interpretable
regions with improved run-to-run consistency. Explicit area control also
enables importance contour maps, yielding a transparent fidelity-area profiles.
Finally, we extend the approach to multi-contour and show how it can localize
multiple objects within the same framework. Across benchmarks, the method
achieves higher relevance mass and lower complexity than gradient and
perturbation based baselines, with especially strong gains on self-supervised
DINO models where it improves relevance mass by over 15% and maintains positive
faithfulness correlations.

</details>


### [233] [Towards One-step Causal Video Generation via Adversarial Self-Distillation](https://arxiv.org/abs/2511.01419)
*Yongqi Yang,Huayang Huang,Xu Peng,Xiaobin Hu,Donghao Luo,Jiangning Zhang,Chengjie Wang,Yu Wu*

Main category: cs.CV

TL;DR: A distillation framework for efficient causal video generation that enables high-quality synthesis with very few denoising steps using adversarial self-distillation and first-frame enhancement strategies.


<details>
  <summary>Details</summary>
Motivation: Existing hybrid video generation models suffer from error accumulation and long inference times due to their sequential, iterative nature, making them inefficient for practical applications.

Method: Proposes adversarial self-distillation (ASD) that aligns student model's n-step denoising with (n+1)-step outputs at distribution level, plus first-frame enhancement (FFE) that allocates more steps to initial frames to reduce error propagation.

Result: Surpasses state-of-the-art approaches in both one-step and two-step video generation on VBench, producing a single distilled model that flexibly supports multiple inference-step settings.

Conclusion: The framework enables efficient, high-quality video synthesis with extremely limited denoising steps while eliminating the need for repeated re-distillation.

Abstract: Recent hybrid video generation models combine autoregressive temporal
dynamics with diffusion-based spatial denoising, but their sequential,
iterative nature leads to error accumulation and long inference times. In this
work, we propose a distillation-based framework for efficient causal video
generation that enables high-quality synthesis with extremely limited denoising
steps. Our approach builds upon the Distribution Matching Distillation (DMD)
framework and proposes a novel Adversarial Self-Distillation (ASD) strategy,
which aligns the outputs of the student model's n-step denoising process with
its (n+1)-step version at the distribution level. This design provides smoother
supervision by bridging small intra-student gaps and more informative guidance
by combining teacher knowledge with locally consistent student behavior,
substantially improving training stability and generation quality in extremely
few-step scenarios (e.g., 1-2 steps). In addition, we present a First-Frame
Enhancement (FFE) strategy, which allocates more denoising steps to the initial
frames to mitigate error propagation while applying larger skipping steps to
later frames. Extensive experiments on VBench demonstrate that our method
surpasses state-of-the-art approaches in both one-step and two-step video
generation. Notably, our framework produces a single distilled model that
flexibly supports multiple inference-step settings, eliminating the need for
repeated re-distillation and enabling efficient, high-quality video synthesis.

</details>


### [234] [UniSOT: A Unified Framework for Multi-Modality Single Object Tracking](https://arxiv.org/abs/2511.01427)
*Yinchao Ma,Yuyang Tang,Wenfei Yang,Tianzhu Zhang,Xu Zhou,Feng Wu*

Main category: cs.CV

TL;DR: UniSOT is a unified tracker that handles three reference modalities (bounding box, natural language, or both) across four video modalities (RGB, RGB+Depth, RGB+Thermal, RGB+Event) with uniform parameters, outperforming modality-specific trackers.


<details>
  <summary>Details</summary>
Motivation: Existing trackers are designed for specific video and reference modality combinations, leading to separate model designs that limit practical applications. A unified tracker is needed to handle various requirements across different modalities.

Method: The authors present UniSOT, a unified tracker that uses uniform parameters to handle three reference modalities (bounding box, natural language, or both) across four video modalities (RGB, RGB+Depth, RGB+Thermal, RGB+Event).

Result: UniSOT shows superior performance on 18 tracking benchmarks, outperforming previous counterparts by over 3.0% AUC on TNL2K across all three reference modalities and outperforming Un-Track by over 2.0% main metric across all three RGB+X video modalities.

Conclusion: UniSOT demonstrates the feasibility and effectiveness of a unified approach for multi-modal object tracking, providing a practical solution for handling various reference and video modality combinations.

Abstract: Single object tracking aims to localize target object with specific reference
modalities (bounding box, natural language or both) in a sequence of specific
video modalities (RGB, RGB+Depth, RGB+Thermal or RGB+Event.). Different
reference modalities enable various human-machine interactions, and different
video modalities are demanded in complex scenarios to enhance tracking
robustness. Existing trackers are designed for single or several video
modalities with single or several reference modalities, which leads to separate
model designs and limits practical applications. Practically, a unified tracker
is needed to handle various requirements. To the best of our knowledge, there
is still no tracker that can perform tracking with these above reference
modalities across these video modalities simultaneously. Thus, in this paper,
we present a unified tracker, UniSOT, for different combinations of three
reference modalities and four video modalities with uniform parameters.
Extensive experimental results on 18 visual tracking, vision-language tracking
and RGB+X tracking benchmarks demonstrate that UniSOT shows superior
performance against modality-specific counterparts. Notably, UniSOT outperforms
previous counterparts by over 3.0\% AUC on TNL2K across all three reference
modalities and outperforms Un-Track by over 2.0\% main metric across all three
RGB+X video modalities.

</details>


### [235] [Terrain-Enhanced Resolution-aware Refinement Attention for Off-Road Segmentation](https://arxiv.org/abs/2511.01434)
*Seongkyu Choi,Jhonghyun An*

Main category: cs.CV

TL;DR: A resolution-aware token decoder for off-road semantic segmentation that balances global semantics, local consistency, and boundary fidelity while being robust to imperfect supervision and noise.


<details>
  <summary>Details</summary>
Motivation: Off-road semantic segmentation suffers from thick inconsistent boundaries, sparse supervision for rare classes, and pervasive label noise. Existing methods either blur edges at low resolution or are costly and fragile to noise when maintaining high-resolution pathways.

Method: Uses a resolution-aware token decoder with: global self-attention with lightweight dilated depthwise refinement for local coherence; gated cross-attention to integrate fine-scale features without amplifying noise; class-aware point refinement for residual ambiguities; and a boundary-band consistency regularizer during training.

Result: The approach achieves competitive performance and improved stability across transitions in off-road semantic segmentation.

Conclusion: The co-designed components effectively balance global semantics, local consistency, and boundary fidelity while being robust to imperfect supervision and noise in challenging off-road environments.

Abstract: Off-road semantic segmentation suffers from thick, inconsistent boundaries,
sparse supervision for rare classes, and pervasive label noise. Designs that
fuse only at low resolution blur edges and propagate local errors, whereas
maintaining high-resolution pathways or repeating high-resolution fusions is
costly and fragile to noise. We introduce a resolutionaware token decoder that
balances global semantics, local consistency, and boundary fidelity under
imperfect supervision. Most computation occurs at a low-resolution bottleneck;
a gated cross-attention injects fine-scale detail, and only a sparse,
uncertainty-selected set of pixels is refined. The components are co-designed
and tightly integrated: global self-attention with lightweight dilated
depthwise refinement restores local coherence; a gated cross-attention
integrates fine-scale features from a standard high-resolution encoder stream
without amplifying noise; and a class-aware point refinement corrects residual
ambiguities with negligible overhead. During training, we add a boundary-band
consistency regularizer that encourages coherent predictions in a thin
neighborhood around annotated edges, with no inference-time cost. Overall, the
results indicate competitive performance and improved stability across
transitions.

</details>


### [236] [Contrast-Guided Cross-Modal Distillation for Thermal Object Detection](https://arxiv.org/abs/2511.01435)
*SiWoo Kim,JhongHyun An*

Main category: cs.CV

TL;DR: A training-only method to improve thermal-infrared object detection by sharpening decision boundaries and injecting RGB semantic priors, achieving SOTA without test-time RGB fusion.


<details>
  <summary>Details</summary>
Motivation: Nighttime thermal-infrared detection suffers from low contrast, weak high-frequency cues causing duplicate boxes, missed small objects, and class confusion. Existing methods either translate TIR to RGB (fragile to artifacts) or fuse RGB+TIR at test time (requires extra sensors and calibration).

Method: Training-only objectives that: 1) sharpen instance-level decision boundaries via feature pulling/pushing to suppress duplicates and confusion, 2) inject cross-modal semantic priors by aligning TIR student features with RGB teacher features at multiple pyramid levels.

Result: Outperformed prior approaches and achieved state-of-the-art performance in thermal-infrared object detection.

Conclusion: The method successfully addresses root causes of thermal detection issues during training, enabling robust mono-modality inference without requiring RGB input at test time.

Abstract: Robust perception at night remains challenging for thermal-infrared
detection: low contrast and weak high-frequency cues lead to duplicate,
overlapping boxes, missed small objects, and class confusion. Prior remedies
either translate TIR to RGB and hope pixel fidelity transfers to detection --
making performance fragile to color or structure artifacts -- or fuse RGB and
TIR at test time, which requires extra sensors, precise calibration, and higher
runtime cost. Both lines can help in favorable conditions, but do not directly
shape the thermal representation used by the detector. We keep mono-modality
inference and tackle the root causes during training. Specifically, we
introduce training-only objectives that sharpen instance-level decision
boundaries by pulling together features of the same class and pushing apart
those of different classes -- suppressing duplicate and confusing detections --
and that inject cross-modal semantic priors by aligning the student's
multi-level pyramid features with an RGB-trained teacher, thereby strengthening
texture-poor thermal features without visible input at test time. In
experiments, our method outperformed prior approaches and achieved
state-of-the-art performance.

</details>


### [237] [Privacy Preserving Ordinal-Meta Learning with VLMs for Fine-Grained Fruit Quality Prediction](https://arxiv.org/abs/2511.01449)
*Riddhi Jain,Manasi Patwardhan,Aayush Mishra,Parijat Deshpande,Beena Rai*

Main category: cs.CV

TL;DR: A meta-learning approach called MAOML trains smaller vision-language models for fruit freshness classification, achieving 92.71% accuracy while addressing data scarcity and privacy concerns.


<details>
  <summary>Details</summary>
Motivation: Proprietary VLMs perform well but raise data privacy concerns, while open-source VLMs underperform due to limited labeled data for fruit freshness classification.

Method: Model-Agnostic Ordinal Meta-Learning (MAOML) algorithm that uses meta-learning to handle data sparsity and leverages label ordinality for training smaller VLMs.

Result: Achieved state-of-the-art performance with 92.71% accuracy across all fruits in both zero-shot and few-shot settings.

Conclusion: MAOML enables effective fruit freshness prediction using smaller, privacy-preserving models that match proprietary model performance.

Abstract: To effectively manage the wastage of perishable fruits, it is crucial to
accurately predict their freshness or shelf life using non-invasive methods
that rely on visual data. In this regard, deep learning techniques can offer a
viable solution. However, obtaining fine-grained fruit freshness labels from
experts is costly, leading to a scarcity of data. Closed proprietary Vision
Language Models (VLMs), such as Gemini, have demonstrated strong performance in
fruit freshness detection task in both zero-shot and few-shot settings.
Nonetheless, food retail organizations are unable to utilize these proprietary
models due to concerns related to data privacy, while existing open-source VLMs
yield sub-optimal performance for the task. Fine-tuning these open-source
models with limited data fails to achieve the performance levels of proprietary
models. In this work, we introduce a Model-Agnostic Ordinal Meta-Learning
(MAOML) algorithm, designed to train smaller VLMs. This approach utilizes
meta-learning to address data sparsity and leverages label ordinality, thereby
achieving state-of-the-art performance in the fruit freshness classification
task under both zero-shot and few-shot settings. Our method achieves an
industry-standard accuracy of 92.71%, averaged across all fruits.
  Keywords: Fruit Quality Prediction, Vision Language Models, Meta Learning,
Ordinal Regression

</details>


### [238] [Reg-DPO: SFT-Regularized Direct Preference Optimization with GT-Pair for Improving Video Generation](https://arxiv.org/abs/2511.01450)
*Jie Du,Xinyu Gong,Qingshan Tan,Wen Li,Yangming Cheng,Weitao Wang,Chenlu Zhan,Suhui Wu,Hao Zhang,Jun Zhang*

Main category: cs.CV

TL;DR: The paper introduces Reg-DPO, a method that improves video generation by automatically creating preference pairs and adding SFT regularization to DPO, achieving better quality and stability with higher training capacity.


<details>
  <summary>Details</summary>
Motivation: Existing DPO methods for video generation follow image-domain paradigms and work on small models, failing to address video-specific challenges like costly data construction, unstable training, and heavy memory consumption.

Method: Proposes GT-Pair for automatic preference pair construction using real videos as positives and model-generated videos as negatives, and Reg-DPO that incorporates SFT loss as regularization into DPO objective. Also uses FSDP with memory optimization for 3x higher training capacity.

Result: Extensive experiments on I2V and T2V tasks across multiple datasets show the method consistently outperforms existing approaches with superior video generation quality.

Conclusion: The proposed approach effectively addresses video generation challenges through automated preference pair construction, regularization for stability, and memory optimization, delivering state-of-the-art performance.

Abstract: Recent studies have identified Direct Preference Optimization (DPO) as an
efficient and reward-free approach to improving video generation quality.
However, existing methods largely follow image-domain paradigms and are mainly
developed on small-scale models (approximately 2B parameters), limiting their
ability to address the unique challenges of video tasks, such as costly data
construction, unstable training, and heavy memory consumption. To overcome
these limitations, we introduce a GT-Pair that automatically builds
high-quality preference pairs by using real videos as positives and
model-generated videos as negatives, eliminating the need for any external
annotation. We further present Reg-DPO, which incorporates the SFT loss as a
regularization term into the DPO objective to enhance training stability and
generation fidelity. Additionally, by combining the FSDP framework with
multiple memory optimization techniques, our approach achieves nearly three
times higher training capacity than using FSDP alone. Extensive experiments on
both I2V and T2V tasks across multiple datasets demonstrate that our method
consistently outperforms existing approaches, delivering superior video
generation quality.

</details>


### [239] [When to Trust the Answer: Question-Aligned Semantic Nearest Neighbor Entropy for Safer Surgical VQA](https://arxiv.org/abs/2511.01458)
*Dennis Pierantozzi,Luca Carlini,Mauro Orazio Drago,Chiara Lena,Cesare Hassan,Elena De Momi,Danail Stoyanov,Sophia Bano,Mobarak I. Hoque*

Main category: cs.CV

TL;DR: QA-SNNE is a black box uncertainty estimator that improves safety in surgical VQA by incorporating question semantics into prediction confidence, enhancing failure detection and hallucination detection.


<details>
  <summary>Details</summary>
Motivation: Safety and reliability are essential for deploying VQA in surgery, as incorrect responses can harm patients. Most surgical VQA research overlooks safety behaviors like ambiguity awareness and expert referral.

Method: Question Aligned Semantic Nearest Neighbor Entropy (QA-SNNE) measures semantic entropy by comparing generated answers with nearest neighbors in a medical text embedding space, conditioned on the question.

Result: QA-SNNE improves AUROC by 15-38% for zero-shot models, with gains maintained under out-of-template stress. It enhances hallucination detection and works better with LVLMs than PEFT models.

Conclusion: QA-SNNE offers a practical step toward automatic failure detection in surgical VQA by linking semantic uncertainty to question context, improving safety and clinician trust when combined with LVLM backbones.

Abstract: Safety and reliability are essential for deploying Visual Question Answering
(VQA) in surgery, where incorrect or ambiguous responses can harm the patient.
Most surgical VQA research focuses on accuracy or linguistic quality while
overlooking safety behaviors such as ambiguity awareness, referral to human
experts, or triggering a second opinion. Inspired by Automatic Failure
Detection (AFD), we study uncertainty estimation as a key enabler of safer
decision making. We introduce Question Aligned Semantic Nearest Neighbor
Entropy (QA-SNNE), a black box uncertainty estimator that incorporates question
semantics into prediction confidence. It measures semantic entropy by comparing
generated answers with nearest neighbors in a medical text embedding space,
conditioned on the question. We evaluate five models, including domain specific
Parameter-Efficient Fine-Tuned (PEFT) models and zero-shot Large
Vision-Language Models (LVLMs), on EndoVis18-VQA and PitVQA. PEFT models
degrade under mild paraphrasing, while LVLMs are more resilient. Across three
LVLMs and two PEFT baselines, QA-SNNE improves AUROC in most in-template
settings and enhances hallucination detection. The Area Under the ROC Curve
(AUROC) increases by 15-38% for zero-shot models, with gains maintained under
out-of-template stress. QA-SNNE offers a practical and interpretable step
toward AFD in surgical VQA by linking semantic uncertainty to question context.
Combining LVLM backbones with question aligned uncertainty estimation can
improve safety and clinician trust. The code and model are available at
https://github.com/DennisPierantozzi/QASNNE

</details>


### [240] [Efficiently Training A Flat Neural Network Before It has been Quantizated](https://arxiv.org/abs/2511.01462)
*Peng Xia,Junbiao Pang,Tianyang Cai*

Main category: cs.CV

TL;DR: The paper proposes a framework for post-training quantization of vision transformers by modeling quantization errors as Gaussian noise and using noise injection to achieve flat minima for better low-bit quantization.


<details>
  <summary>Details</summary>
Motivation: Existing PTQ methods overlook the relationship between well-trained models and quantized versions, leading to significant quantization errors. There's a need for efficient model-agnostic approaches tailored for low-bit quantization.

Method: Proposes modeling Activation Quantization Error (AQE) and Weight Quantization Error (WQE) as independent Gaussian noises, and uses noise injection optimization methods to achieve flat minima in the full-precision model.

Result: Experimental results demonstrate the effectiveness of the approach in obtaining low-bit PTQ models with reduced quantization error.

Conclusion: The framework opens novel pathways for achieving better low-bit post-training quantization by proactively preconditioning models through error modeling and flat minimum optimization.

Abstract: Post-training quantization (PTQ) for vision transformers (ViTs) has garnered
significant attention due to its efficiency in compressing models. However,
existing methods typically overlook the relationship between a well-trained NN
and the quantized model, leading to considerable quantization error for PTQ.
However, it is unclear how to efficiently train a model-agnostic neural network
which is tailored for a predefined precision low-bit model. In this paper, we
firstly discover that a flat full precision neural network is crucial for
low-bit quantization. To achieve this, we propose a framework that proactively
pre-conditions the model by measuring and disentangling the error sources.
Specifically, both the Activation Quantization Error (AQE) and the Weight
Quantization Error (WQE) are statistically modeled as independent Gaussian
noises. We study several noise injection optimization methods to obtain a flat
minimum. Experimental results attest to the effectiveness of our approach.
These results open novel pathways for obtaining low-bit PTQ models.

</details>


### [241] [HMVLM: Human Motion-Vision-Lanuage Model via MoE LoRA](https://arxiv.org/abs/2511.01463)
*Lei Hu,Yongjing Ye,Shihong Xia*

Main category: cs.CV

TL;DR: HMVLM is a unified framework that addresses catastrophic forgetting in multimodal models by using MoE LoRA strategy with zero expert preservation, and improves pose representation through body-part-specific tokenization.


<details>
  <summary>Details</summary>
Motivation: To bridge the modality gap between human motion and text while preventing catastrophic forgetting during integration of 3D human motion with foundation language models, and to develop autoregressive-compatible pose representations that maintain generalizability across tasks.

Method: Proposes HMVLM framework based on Mixture of Expert Low-Rank Adaptation (MoE LoRA), using gating network to dynamically allocate LoRA expert weights, zero expert to preserve pre-trained parameters, and body-part-specific tokenization for pose representation.

Result: Effectively alleviates knowledge forgetting during instruction-tuning and achieves remarkable performance across diverse human motion downstream tasks.

Conclusion: The proposed HMVLM framework successfully addresses catastrophic forgetting and representation challenges in integrating 3D human motion with foundation models, demonstrating strong performance across multiple downstream tasks.

Abstract: The expansion of instruction-tuning data has enabled foundation language
models to exhibit improved instruction adherence and superior performance
across diverse downstream tasks. Semantically-rich 3D human motion is being
progressively integrated with these foundation models to enhance multimodal
understanding and cross-modal generation capabilities. However, the modality
gap between human motion and text raises unresolved concerns about catastrophic
forgetting during this integration. In addition, developing
autoregressive-compatible pose representations that preserve generalizability
across heterogeneous downstream tasks remains a critical technical barrier. To
address these issues, we propose the Human Motion-Vision-Language Model
(HMVLM), a unified framework based on the Mixture of Expert Low-Rank
Adaption(MoE LoRA) strategy. The framework leverages the gating network to
dynamically allocate LoRA expert weights based on the input prompt, enabling
synchronized fine-tuning of multiple tasks. To mitigate catastrophic forgetting
during instruction-tuning, we introduce a novel zero expert that preserves the
pre-trained parameters for general linguistic tasks. For pose representation,
we implement body-part-specific tokenization by partitioning the human body
into different joint groups, enhancing the spatial resolution of the
representation. Experiments show that our method effectively alleviates
knowledge forgetting during instruction-tuning and achieves remarkable
performance across diverse human motion downstream tasks.

</details>


### [242] [SecDiff: Diffusion-Aided Secure Deep Joint Source-Channel Coding Against Adversarial Attacks](https://arxiv.org/abs/2511.01466)
*Changyuan Zhao,Jiacheng Wang,Ruichen Zhang,Dusit Niyato,Hongyang Du,Zehui Xiong,Dong In Kim,Ping Zhang*

Main category: cs.CV

TL;DR: SecDiff is a diffusion-aided decoding framework that enhances security and robustness of deep JSCC against adversarial wireless attacks like pilot spoofing and subcarrier jamming, achieving better trade-off between reconstruction quality and computational cost.


<details>
  <summary>Details</summary>
Motivation: Existing deep JSCC frameworks are vulnerable to physical-layer adversarial threats, compromising semantic fidelity in semantic communications.

Method: Uses pseudoinverse-guided sampling and adaptive guidance weighting for flexible step-size control, power-based subcarrier masking for jamming attacks, and EM-driven reconstruction with joint pilot recovery and channel estimation for pilot spoofing.

Result: Extensive experiments show SecDiff outperforms existing secure and generative JSCC baselines in OFDM channels under adversarial conditions.

Conclusion: SecDiff represents a promising step toward practical, low-latency, and attack-resilient semantic communications.

Abstract: Deep joint source-channel coding (JSCC) has emerged as a promising paradigm
for semantic communication, delivering significant performance gains over
conventional separate coding schemes. However, existing JSCC frameworks remain
vulnerable to physical-layer adversarial threats, such as pilot spoofing and
subcarrier jamming, compromising semantic fidelity. In this paper, we propose
SecDiff, a plug-and-play, diffusion-aided decoding framework that significantly
enhances the security and robustness of deep JSCC under adversarial wireless
environments. Different from prior diffusion-guided JSCC methods that suffer
from high inference latency, SecDiff employs pseudoinverse-guided sampling and
adaptive guidance weighting, enabling flexible step-size control and efficient
semantic reconstruction. To counter jamming attacks, we introduce a power-based
subcarrier masking strategy and recast recovery as a masked inpainting problem,
solved via diffusion guidance. For pilot spoofing, we formulate channel
estimation as a blind inverse problem and develop an expectation-minimization
(EM)-driven reconstruction algorithm, guided jointly by reconstruction loss and
a channel operator. Notably, our method alternates between pilot recovery and
channel estimation, enabling joint refinement of both variables throughout the
diffusion process. Extensive experiments over orthogonal frequency-division
multiplexing (OFDM) channels under adversarial conditions show that SecDiff
outperforms existing secure and generative JSCC baselines by achieving a
favorable trade-off between reconstruction quality and computational cost. This
balance makes SecDiff a promising step toward practical, low-latency, and
attack-resilient semantic communications.

</details>


### [243] [EPAN: Robust Pedestrian Re-Identification via Enhanced Alignment Network for IoT Surveillance](https://arxiv.org/abs/2511.01498)
*Zhiyang Jia,Hongyan Cui,Ge Gao,Bo Li,Minjie Zhang,Zishuo Gao,Huiwen Huang,Caisheng Zhuo*

Main category: cs.CV

TL;DR: EPAN is a dual-branch network for person re-identification that addresses perspective and environmental variations, achieving 90.09% Rank-1 accuracy and 78.82% mAP on the Inspection-Personnel dataset.


<details>
  <summary>Details</summary>
Motivation: To develop robust person re-identification for IoT surveillance systems that can handle diverse environmental conditions and perspective changes.

Method: Uses a dual-branch architecture to extract alignment information under varying scales and viewpoints, mitigating the impact of perspective and environmental changes.

Result: Achieved 90.09% Rank-1 accuracy and 78.82% mAP on the Inspection-Personnel dataset, demonstrating strong feature extraction capabilities.

Conclusion: EPAN shows potential for real-world IoT applications, enabling effective and reliable person re-identification across diverse cameras in surveillance systems.

Abstract: Person re-identification (ReID) plays a pivotal role in computer vision,
particularly in surveillance and security applications within IoT-enabled smart
environments. This study introduces the Enhanced Pedestrian Alignment Network
(EPAN), tailored for robust ReID across diverse IoT surveillance conditions.
EPAN employs a dual-branch architecture to mitigate the impact of perspective
and environmental changes, extracting alignment information under varying
scales and viewpoints. Here, we demonstrate EPAN's strong feature extraction
capabilities, achieving outstanding performance on the Inspection-Personnel
dataset with a Rank-1 accuracy of 90.09% and a mean Average Precision (mAP) of
78.82%. This highlights EPAN's potential for real-world IoT applications,
enabling effective and reliable person ReID across diverse cameras in
surveillance and security systems. The code and data are available at:
https://github.com/ggboy2580/EPAN

</details>


### [244] [SE(3)-PoseFlow: Estimating 6D Pose Distributions for Uncertainty-Aware Robotic Manipulation](https://arxiv.org/abs/2511.01501)
*Yufeng Jin,Niklas Funk,Vignesh Prasad,Zechu Li,Mathias Franzius,Jan Peters,Georgia Chalvatzaki*

Main category: cs.CV

TL;DR: A probabilistic framework using flow matching on SE(3) manifold for estimating 6D object pose distributions, addressing pose ambiguity in symmetric objects and occlusions.


<details>
  <summary>Details</summary>
Motivation: Object pose estimation faces challenges from partial observability, occlusions, and object symmetries, leading to pose ambiguity. Deterministic methods are overconfident and fail to capture multi-modality of pose distributions.

Method: Proposes flow matching on the SE(3) manifold to model full pose distributions with sample-based estimates, enabling uncertainty reasoning in ambiguous cases.

Result: Achieves state-of-the-art results on Real275, YCB-V, and LM-O datasets. Sample-based pose estimates improve downstream robotic tasks like active perception and grasp synthesis.

Conclusion: The probabilistic framework effectively handles pose ambiguity and uncertainty, outperforming deterministic methods and enabling better robotic manipulation through uncertainty-aware reasoning.

Abstract: Object pose estimation is a fundamental problem in robotics and computer
vision, yet it remains challenging due to partial observability, occlusions,
and object symmetries, which inevitably lead to pose ambiguity and multiple
hypotheses consistent with the same observation. While deterministic deep
networks achieve impressive performance under well-constrained conditions, they
are often overconfident and fail to capture the multi-modality of the
underlying pose distribution. To address these challenges, we propose a novel
probabilistic framework that leverages flow matching on the SE(3) manifold for
estimating 6D object pose distributions. Unlike existing methods that regress a
single deterministic output, our approach models the full pose distribution
with a sample-based estimate and enables reasoning about uncertainty in
ambiguous cases such as symmetric objects or severe occlusions. We achieve
state-of-the-art results on Real275, YCB-V, and LM-O, and demonstrate how our
sample-based pose estimates can be leveraged in downstream robotic manipulation
tasks such as active perception for disambiguating uncertain viewpoints or
guiding grasp synthesis in an uncertainty-aware manner.

</details>


### [245] [Discriminately Treating Motion Components Evolves Joint Depth and Ego-Motion Learning](https://arxiv.org/abs/2511.01502)
*Mengtan Zhang,Zizhan Guo,Hongbo Zhao,Yi Feng,Zuyi Xiong,Yue Wang,Shaoyi Du,Hanli Wang,Rui Fan*

Main category: cs.CV

TL;DR: DiMoDE introduces a discriminative approach to motion components in unsupervised depth and ego-motion learning, using geometric constraints from rigid flows to improve both tasks through coaxial and coplanar formulations.


<details>
  <summary>Details</summary>
Motivation: Existing methods treat ego-motion as auxiliary, mixing motion types or excluding depth-independent rotations, which limits geometric constraints and reduces reliability under diverse conditions.

Method: Network aligns optical axes and imaging planes between frames, transforms optical flows through these alignments, quantifies deviations to impose geometric constraints on each ego-motion component, and reformulates joint learning into coaxial/coplanar forms with closed-form geometric relationships.

Result: DiMoDE achieves state-of-the-art performance on multiple public datasets and a new diverse real-world dataset, particularly under challenging conditions.

Conclusion: The discriminative treatment of motion components and geometric constraints significantly improves depth and ego-motion estimation robustness and reliability.

Abstract: Unsupervised learning of depth and ego-motion, two fundamental 3D perception
tasks, has made significant strides in recent years. However, most methods
treat ego-motion as an auxiliary task, either mixing all motion types or
excluding depth-independent rotational motions in supervision. Such designs
limit the incorporation of strong geometric constraints, reducing reliability
and robustness under diverse conditions. This study introduces a discriminative
treatment of motion components, leveraging the geometric regularities of their
respective rigid flows to benefit both depth and ego-motion estimation. Given
consecutive video frames, network outputs first align the optical axes and
imaging planes of the source and target cameras. Optical flows between frames
are transformed through these alignments, and deviations are quantified to
impose geometric constraints individually on each ego-motion component,
enabling more targeted refinement. These alignments further reformulate the
joint learning process into coaxial and coplanar forms, where depth and each
translation component can be mutually derived through closed-form geometric
relationships, introducing complementary constraints that improve depth
robustness. DiMoDE, a general depth and ego-motion joint learning framework
incorporating these designs, achieves state-of-the-art performance on multiple
public datasets and a newly collected diverse real-world dataset, particularly
under challenging conditions. Our source code will be publicly available at
mias.group/DiMoDE upon publication.

</details>


### [246] [Luminance-Aware Statistical Quantization: Unsupervised Hierarchical Learning for Illumination Enhancement](https://arxiv.org/abs/2511.01510)
*Derong Kong,Zhixiong Yang,Shengxi Li,Shuaifeng Zhi,Li Liu,Zhen Liu,Jingyuan Xia*

Main category: cs.CV

TL;DR: LASQ reformulates low-light image enhancement as a statistical sampling process using power-law distributed luminance transitions, enabling unsupervised learning without normal-light references.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on deterministic pixel-level mappings between paired low/normal-light images, which fail when normal-light references are unavailable and neglect the continuous physical process of luminance transitions.

Method: Introduces Luminance-Aware Statistical Quantification (LASQ) that models luminance transition as power-law distribution in intensity coordinate space, using stratified power functions and diffusion forward process to discover optimal transition paths between luminance layers.

Result: Achieves superior performance on domain-specific datasets with normal-light references and better generalization across non-reference datasets, enabling more adaptable and versatile light restoration.

Conclusion: LASQ provides a probabilistic framework for low-light image enhancement that works effectively both with and without normal-light references, addressing the generalization challenge through statistical modeling of luminance dynamics.

Abstract: Low-light image enhancement (LLIE) faces persistent challenges in balancing
reconstruction fidelity with cross-scenario generalization. While existing
methods predominantly focus on deterministic pixel-level mappings between
paired low/normal-light images, they often neglect the continuous physical
process of luminance transitions in real-world environments, leading to
performance drop when normal-light references are unavailable. Inspired by
empirical analysis of natural luminance dynamics revealing power-law
distributed intensity transitions, this paper introduces Luminance-Aware
Statistical Quantification (LASQ), a novel framework that reformulates LLIE as
a statistical sampling process over hierarchical luminance distributions. Our
LASQ re-conceptualizes luminance transition as a power-law distribution in
intensity coordinate space that can be approximated by stratified power
functions, therefore, replacing deterministic mappings with probabilistic
sampling over continuous luminance layers. A diffusion forward process is
designed to autonomously discover optimal transition paths between luminance
layers, achieving unsupervised distribution emulation without normal-light
references. In this way, it considerably improves the performance in practical
situations, enabling more adaptable and versatile light restoration. This
framework is also readily applicable to cases with normal-light references,
where it achieves superior performance on domain-specific datasets alongside
better generalization-ability across non-reference datasets.

</details>


### [247] [Example-Based Feature Painting on Textures](https://arxiv.org/abs/2511.01513)
*Andrei-Timotei Ardelean,Tim Weyrich*

Main category: cs.CV

TL;DR: A system for controlled authoring and editing of textures with local characteristics like stains, tears, and abrasions using unsupervised learning and diffusion-based generation.


<details>
  <summary>Details</summary>
Motivation: Realistic textures require including natural alterations like stains and tears, which are ubiquitous in nature but challenging to synthesize manually.

Method: Learning-based approach using unlabeled examples with unsupervised anomaly detection to identify appearance-altering features, automatic clustering into semantic groups, and conditional generation via diffusion models.

Result: A complete pipeline from image collection to generative model that enables interactive creation and painting of features on arbitrary-sized textures.

Conclusion: The system provides versatile texture generation with local characteristics, and the introduced algorithms for diffusion-based editing and infinite texture generation are generic and applicable to other contexts.

Abstract: In this work, we propose a system that covers the complete workflow for
achieving controlled authoring and editing of textures that present distinctive
local characteristics. These include various effects that change the surface
appearance of materials, such as stains, tears, holes, abrasions,
discoloration, and more. Such alterations are ubiquitous in nature, and
including them in the synthesis process is crucial for generating realistic
textures. We introduce a novel approach for creating textures with such
blemishes, adopting a learning-based approach that leverages unlabeled
examples. Our approach does not require manual annotations by the user;
instead, it detects the appearance-altering features through unsupervised
anomaly detection. The various textural features are then automatically
clustered into semantically coherent groups, which are used to guide the
conditional generation of images. Our pipeline as a whole goes from a small
image collection to a versatile generative model that enables the user to
interactively create and paint features on textures of arbitrary size. Notably,
the algorithms we introduce for diffusion-based editing and infinite stationary
texture generation are generic and should prove useful in other contexts as
well. Project page: https://reality.tf.fau.de/pub/ardelean2025examplebased.html

</details>


### [248] [NSYNC: Negative Synthetic Image Generation for Contrastive Training to Improve Stylized Text-To-Image Translation](https://arxiv.org/abs/2511.01517)
*Serkan Ozturk,Samet Hicsonmez,Pinar Duygulu*

Main category: cs.CV

TL;DR: NSYNC introduces a contrastive learning framework using synthetic negative examples to improve text-to-image diffusion models' ability to capture specific artistic styles, outperforming baseline methods.


<details>
  <summary>Details</summary>
Motivation: Current text-to-image models generate realistic images but fail to capture specific artistic styles. Fine-tuning on style datasets alone is insufficient for learning style features effectively.

Method: Uses synthetic image generation to create negative examples, then applies contrastive training where positive gradients are refined by subtracting their projection onto negative gradients, focusing on unique style attributes.

Result: Experiments on various painter and illustrator styles show improved performance over baseline methods both quantitatively and qualitatively.

Conclusion: The proposed contrastive learning approach with synthetic negative sets effectively enhances style capture in text-to-image diffusion models by eliminating trivial shared attributes and focusing on unique style features.

Abstract: Current text conditioned image generation methods output realistic looking
images, but they fail to capture specific styles. Simply finetuning them on the
target style datasets still struggles to grasp the style features. In this
work, we present a novel contrastive learning framework to improve the
stylization capability of large text-to-image diffusion models. Motivated by
the astonishing advance in image generation models that makes synthetic data an
intrinsic part of model training in various computer vision tasks, we exploit
synthetic image generation in our approach. Usually, the generated synthetic
data is dependent on the task, and most of the time it is used to enlarge the
available real training dataset. With NSYNC, alternatively, we focus on
generating negative synthetic sets to be used in a novel contrastive training
scheme along with real positive images. In our proposed training setup, we
forward negative data along with positive data and obtain negative and positive
gradients, respectively. We then refine the positive gradient by subtracting
its projection onto the negative gradient to get the orthogonal component,
based on which the parameters are updated. This orthogonal component eliminates
the trivial attributes that are present in both positive and negative data and
directs the model towards capturing a more unique style. Experiments on various
styles of painters and illustrators show that our approach improves the
performance over the baseline methods both quantitatively and qualitatively.
Our code is available at https://github.com/giddyyupp/NSYNC.

</details>


### [249] [Driving scenario generation and evaluation using a structured layer representation and foundational models](https://arxiv.org/abs/2511.01541)
*Arthur Hubert,Gamal Elghazaly,Raphal Frank*

Main category: cs.CV

TL;DR: Proposes a structured five-layer model for generating and evaluating rare driving scenarios using generative models, with diversity and originality metrics for synthetic dataset assessment.


<details>
  <summary>Details</summary>
Motivation: Rare driving scenarios are critical for autonomous vehicle development but difficult to encounter, requiring simulation and generation methods.

Method: Uses a five-layer structured model with subclasses and characteristics for agents, alongside large foundational models for data augmentation and scenario generation.

Result: Developed diversity and originality metrics to evaluate synthetic datasets, with qualitative evaluation of generated scenario videos.

Conclusion: The structured five-layer model improves rare scenario generation and evaluation, providing effective metrics for synthetic dataset assessment in autonomous driving development.

Abstract: Rare and challenging driving scenarios are critical for autonomous vehicle
development. Since they are difficult to encounter, simulating or generating
them using generative models is a popular approach. Following previous efforts
to structure driving scenario representations in a layer model, we propose a
structured five-layer model to improve the evaluation and generation of rare
scenarios. We use this model alongside large foundational models to generate
new driving scenarios using a data augmentation strategy. Unlike previous
representations, our structure introduces subclasses and characteristics for
every agent of the scenario, allowing us to compare them using an embedding
specific to our layer-model. We study and adapt two metrics to evaluate the
relevance of a synthetic dataset in the context of a structured representation:
the diversity score estimates how different the scenarios of a dataset are from
one another, while the originality score calculates how similar a synthetic
dataset is from a real reference set. This paper showcases both metrics in
different generation setup, as well as a qualitative evaluation of synthetic
videos generated from structured scenario descriptions. The code and extended
results can be found at https://github.com/Valgiz/5LMSG.

</details>


### [250] [PCD-ReID: Occluded Person Re-Identification for Base Station Inspection](https://arxiv.org/abs/2511.01546)
*Ge Gao,Zishuo Gao,Hongyan Cui,Zhiyang Jia,Zhuang Luo,ChaoPeng Liu*

Main category: cs.CV

TL;DR: PCD-ReID algorithm uses Transformer-based network to extract shared component features for occluded pedestrian re-identification, achieving 79.0% mAP and 82.7% Rank-1 accuracy with 15.9% improvement over ResNet50 methods.


<details>
  <summary>Details</summary>
Motivation: Traditional ResNet-based ReID algorithms fail to effectively address occlusions that obscure key body features in surveillance applications, necessitating new methods for occluded pedestrian re-identification.

Method: Proposed PCD-ReID algorithm with Transformer-based PCD network to extract shared component features (helmets, uniforms) and trained on new real-world patrol surveillance dataset covering 6 months, 10,000 individuals, and 50,000+ images.

Result: Achieved 79.0% mAP and 82.7% Rank-1 accuracy, marking 15.9% Rank-1 improvement over ResNet50-based methods. Effectively handles occlusion-aware ReID in tower inspection scenarios.

Conclusion: PCD-ReID demonstrates strong potential for practical deployment in surveillance and security applications by effectively addressing occlusion challenges in pedestrian re-identification.

Abstract: Occluded pedestrian re-identification (ReID) in base station environments is
a critical task in computer vision, particularly for surveillance and security
applications. This task faces numerous challenges, as occlusions often obscure
key body features, increasing the complexity of identification. Traditional
ResNet-based ReID algorithms often fail to address occlusions effectively,
necessitating new ReID methods. We propose the PCD-ReID (Pedestrian Component
Discrepancy) algorithm to address these issues. The contributions of this work
are as follows: To tackle the occlusion problem, we design a Transformer-based
PCD network capable of extracting shared component features, such as helmets
and uniforms. To mitigate overfitting on public datasets, we collected new
real-world patrol surveillance images for model training, covering six months,
10,000 individuals, and over 50,000 images. Comparative experiments with
existing ReID algorithms demonstrate that our model achieves a mean Average
Precision (mAP) of 79.0% and a Rank-1 accuracy of 82.7%, marking a 15.9% Rank-1
improvement over ResNet50-based methods. Experimental evaluations indicate that
PCD-ReID effectively achieves occlusion-aware ReID performance for personnel in
tower inspection scenarios, highlighting its potential for practical deployment
in surveillance and security applications.

</details>


### [251] [NOA: a versatile, extensible tool for AI-based organoid analysis](https://arxiv.org/abs/2511.01549)
*Mikhail Konov,Lion J. Gleiter,Khoa Co,Monica Yabal,Tingying Peng*

Main category: cs.CV

TL;DR: NOA is an open-source graphical tool that makes AI-based organoid image analysis accessible to biologists without programming skills, integrating detection, segmentation, tracking, and feature analysis.


<details>
  <summary>Details</summary>
Motivation: AI tools for organoid analysis are powerful but inaccessible to biologists without programming experience, creating barriers to adoption and forcing manual workflows.

Method: Developed Napari Organoid Analyzer (NOA) - a GUI plugin that integrates multiple state-of-the-art algorithms for detection, segmentation, tracking, feature extraction, annotation, and ML-based prediction.

Result: Successfully demonstrated in three case studies: quantifying morphological changes during differentiation, assessing phototoxicity effects, and predicting organoid viability/differentiation state.

Conclusion: NOA provides a comprehensive, accessible, and extensible framework for AI-driven organoid image analysis, bridging the gap between advanced algorithms and biological researchers.

Abstract: AI tools can greatly enhance the analysis of organoid microscopy images, from
detection and segmentation to feature extraction and classification. However,
their limited accessibility to biologists without programming experience
remains a major barrier, resulting in labor-intensive and largely manual
workflows. Although a few AI models for organoid analysis have been developed,
most existing tools remain narrowly focused on specific tasks. In this work, we
introduce the Napari Organoid Analyzer (NOA), a general purpose graphical user
interface to simplify AI-based organoid analysis. NOA integrates modules for
detection, segmentation, tracking, feature extraction, custom feature
annotation and ML-based feature prediction. It interfaces multiple
state-of-the-art algorithms and is implemented as an open-source napari plugin
for maximal flexibility and extensibility. We demonstrate the versatility of
NOA through three case studies, involving the quantification of morphological
changes during organoid differentiation, assessment of phototoxicity effects,
and prediction of organoid viability and differentiation state. Together, these
examples illustrate how NOA enables comprehensive, AI-driven organoid image
analysis within an accessible and extensible framework.

</details>


### [252] [PixelVLA: Advancing Pixel-level Understanding in Vision-Language-Action Model](https://arxiv.org/abs/2511.01571)
*Wenqi Liang,Gan Sun,Yao He,Jiahua Dong,Suyan Dai,Ivan Laptev,Salman Khan,Yang Cong*

Main category: cs.CV

TL;DR: PixelVLA is a Vision-Language-Action model that addresses limitations in pixel-level scene understanding and textual prompt dependency by supporting both pixel-level reasoning and multimodal prompting with text and visual inputs.


<details>
  <summary>Details</summary>
Motivation: Current VLAs struggle with pixel-level scene understanding and rely heavily on textual prompts, reducing flexibility in real-world settings.

Method: Built on a visuomotor instruction tuning framework with multiscale pixel-aware encoder and visual prompting encoder, trained using a two-stage automated annotation pipeline that generates Pixel-160K dataset.

Result: Improves manipulation success rates by 10.1%-17.8% over OpenVLA while requiring only 1.5% of its pretraining cost.

Conclusion: PixelVLA can be integrated into existing VLAs to enable more accurate, efficient, and versatile robot control in complex environments.

Abstract: Vision-Language-Action models (VLAs) are emerging as powerful tools for
learning generalizable visuomotor control policies. However, current VLAs are
mostly trained on large-scale image-text-action data and remain limited in two
key ways: (i) they struggle with pixel-level scene understanding, and (ii) they
rely heavily on textual prompts, which reduces their flexibility in real-world
settings. To address these challenges, we introduce PixelVLA, the first VLA
model designed to support both pixel-level reasoning and multimodal prompting
with text and visual inputs. Our approach is built on a new visuomotor
instruction tuning framework that integrates a multiscale pixel-aware encoder
with a visual prompting encoder. To train PixelVLA effectively, we further
propose a two-stage automated annotation pipeline that generates Pixel-160K, a
large-scale dataset with pixel-level annotations derived from existing robot
data. Experiments on three standard VLA benchmarks and two VLA model variants
show that PixelVLA improves manipulation success rates by 10.1%-17.8% over
OpenVLA, while requiring only 1.5% of its pretraining cost. These results
demonstrate that PixelVLA can be integrated into existing VLAs to enable more
accurate, efficient, and versatile robot control in complex environments. The
dataset and code will be released as open source.

</details>


### [253] [Generative Adversarial Synthesis and Deep Feature Discrimination of Brain Tumor MRI Images](https://arxiv.org/abs/2511.01574)
*Md Sumon Ali,Muzammil Behzad*

Main category: cs.CV

TL;DR: Proposes using DC-GAN to generate synthetic MRI data to address limited medical imaging data, and validates synthetic image quality through CNN classification showing comparable performance to real images.


<details>
  <summary>Details</summary>
Motivation: Limited availability of original MRI data makes synthetic data generation necessary for medical imaging tasks, especially for brain tumor analysis.

Method: Uses Deep Convolutional Generative Adversarial Network (DC-GAN) to create synthetic MRI data, and employs CNN classifier to evaluate synthetic image quality by comparing classification performance on real vs synthetic images.

Result: CNN classification demonstrates comparable performance on both real and synthetic MRI images, validating the effectiveness of GAN-generated images for downstream medical tasks.

Conclusion: GAN-generated synthetic MRI data can effectively address data limitation issues in medical imaging while maintaining utility for practical applications like brain tumor classification.

Abstract: Compared to traditional methods, Deep Learning (DL) becomes a key technology
for computer vision tasks. Synthetic data generation is an interesting use case
for DL, especially in the field of medical imaging such as Magnetic Resonance
Imaging (MRI). The need for this task since the original MRI data is limited.
The generation of realistic medical images is completely difficult and
challenging. Generative Adversarial Networks (GANs) are useful for creating
synthetic medical images. In this paper, we propose a DL based methodology for
creating synthetic MRI data using the Deep Convolutional Generative Adversarial
Network (DC-GAN) to address the problem of limited data. We also employ a
Convolutional Neural Network (CNN) classifier to classify the brain tumor using
synthetic data and real MRI data. CNN is used to evaluate the quality and
utility of the synthetic images. The classification result demonstrates
comparable performance on real and synthetic images, which validates the
effectiveness of GAN-generated images for downstream tasks.

</details>


### [254] [Wave-Particle (Continuous-Discrete) Dualistic Visual Tokenization for Unified Understanding and Generation](https://arxiv.org/abs/2511.01593)
*Yizhu Chen,Chen Ju,Zhicheng Wang,Shuai Xiao,Xu Chen,Jinsong Lan,Xiaoyong Zhu,Ying Chen*

Main category: cs.CV

TL;DR: CDD-VT proposes a continuous-discrete dualistic visual tokenizer that adaptively assigns image primitives based on sample complexity, bridging the gap between continuous and discrete tokenization approaches.


<details>
  <summary>Details</summary>
Motivation: To resolve the dichotomy between continuous tokenizers (complex pipelines) and discrete tokenizers (information loss) in multi-modal large models by creating a unified approach inspired by wave-particle duality.

Method: Uses adaptive primitive assignment where simple instances use few primitives (like DT) and complex instances use many primitives (like CT), with Diverse Quantitative Primitives for orthogonality and Dynamic Primitive Allocator for complexity assessment.

Result: Achieves superior performance in reconstruction, retrieval and classification tasks compared to specialized continuous and discrete tokenizers, with strong results in a concise and scalable MLLM framework.

Conclusion: CDD-VT effectively bridges the continuous-discrete tokenization gap, providing a flexible and efficient solution for unified understanding and generation in multi-modal models.

Abstract: The unification of understanding and generation within a single multi-modal
large model (MLLM) remains one significant challenge, largely due to the
dichotomy between continuous and discrete visual tokenizations. Continuous
tokenizer (CT) achieves strong performance by bridging multiple
independently-trained understanding modules and generation modules, but suffers
from complex multi-stage pipelines and substantial engineering overhead.
Conversely, discrete tokenizers (DT) offer a conceptually elegant idea by
quantizing each image into a primitive, but inevitably leading to information
loss and performance degradation. To resolve this tension, we question the
binary choice between CT and DT, inspired by the wave-particle duality of
light, and propose the Continuous-Discrete Dualistic Visual Tokenizer (CDD-VT).
We treat visual data as a flexible composition of image primitives derived from
quantized codebooks, with the crucial insight that the primitive number
assigned to each visual sample is adaptively determined according to its
complexity: simple instances use a few primitives, emulating discrete
tokenization, while complex instances use many, approximating continuous
tokenization. Two core components are designed: Diverse Quantitative
Primitives, which encourage primitives orthogonality to better populate
information space, and Dynamic Primitive Allocator, which assesses sample
complexity to determine the optimal set of primitives. Extensive experiments on
reconstruction, retrieval and classification show that CDD-VT achieves superior
performance over to specialized CT and DT, effectively getting strong result
within a concise and scalable MLLM.

</details>


### [255] [Lite ENSAM: a lightweight cancer segmentation model for 3D Computed Tomography](https://arxiv.org/abs/2511.01600)
*Agnar Martin Bjrnstad,Elias Stenhede,Arian Ranjbar*

Main category: cs.CV

TL;DR: Lite ENSAM is a lightweight adaptation of ENSAM architecture for efficient volumetric tumor segmentation from CT scans with RECIST annotations, achieving competitive performance in the MICCAI FLARE 2025 challenge.


<details>
  <summary>Details</summary>
Motivation: Current RECIST v1.1 standard relies on 2D diameter measurements, but volumetric measurements provide more reliable treatment assessment. Manual volumetric annotation is labor-intensive, creating a need for automated solutions.

Method: Lite ENSAM - a lightweight adaptation of the ENSAM architecture designed for efficient volumetric tumor segmentation from CT scans annotated with RECIST annotations.

Result: Achieved Dice Similarity Coefficient (DSC) of 60.7% and Normalized Surface Dice (NSD) of 63.6% on hidden test set. Average total RAM time of 50.6 GBs and average inference time of 14.4 seconds on CPU on public validation dataset.

Conclusion: Lite ENSAM provides an efficient solution for automated volumetric tumor segmentation from CT scans, addressing the limitations of manual annotation while maintaining competitive performance metrics.

Abstract: Accurate tumor size measurement is a cornerstone of evaluating cancer
treatment response. The most widely adopted standard for this purpose is the
Response Evaluation Criteria in Solid Tumors (RECIST) v1.1, which relies on
measuring the longest tumor diameter in a single plane. However, volumetric
measurements have been shown to provide a more reliable assessment of treatment
effect. Their clinical adoption has been limited, though, due to the
labor-intensive nature of manual volumetric annotation. In this paper, we
present Lite ENSAM, a lightweight adaptation of the ENSAM architecture designed
for efficient volumetric tumor segmentation from CT scans annotated with RECIST
annotations. Lite ENSAM was submitted to the MICCAI FLARE 2025 Task 1:
Pan-cancer Segmentation in CT Scans, Subtask 2, where it achieved a Dice
Similarity Coefficient (DSC) of 60.7% and a Normalized Surface Dice (NSD) of
63.6% on the hidden test set, and an average total RAM time of 50.6 GBs and an
average inference time of 14.4 s on CPU on the public validation dataset.

</details>


### [256] [DINO-MX: A Modular & Flexible Framework for Self-Supervised Learning](https://arxiv.org/abs/2511.01610)
*Mahmut Selman Gokmen,Cody Bumgardner*

Main category: cs.CV

TL;DR: DINO-MX is a modular and extensible training framework that unifies DINO, DINOv2, and DINOv3 principles with flexible configuration, supporting various transformer architectures and training strategies while reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Existing vision foundation model training pipelines are inflexible, domain-specific, and computationally expensive, limiting their usability across different domains and resource settings.

Method: A unified configuration-driven system that combines DINO principles, supports multiple transformer architectures, and includes training strategies like LoRA, layer freezing, knowledge distillation, DDP, and FSDP. Works with natural and specialized data including multi-channel images.

Result: Achieves competitive performance on diverse datasets while significantly reducing computational costs. Includes interpretability tools and label-guided data augmentation that improves attention-based localization without extra detection heads.

Conclusion: Provides a reproducible and scalable foundation for developing, adapting, and benchmarking self-supervised vision models across research and real-world applications.

Abstract: Vision Foundation Models (VFMs) have advanced representation learning through
self-supervised methods. However, existing training pipelines are often
inflexible, domain-specific, or computationally expensive, which limits their
usability across different domains and resource settings. DINO-MX is a modular
and extensible training framework that combines the core principles of DINO,
DINOv2 and DINOv3 within a unified configuration-driven system. It supports a
variety of transformer-based architectures and is fully compatible with the
Hugging Face ecosystem. The framework includes multiple training strategies
such as low-rank adaptation (LoRA), layer freezing, and knowledge distillation,
along with support for distributed training through both Distributed Data
Parallel (DDP) and Fully Sharded Data Parallel (FSDP). DINO-MX is designed to
work with both natural and specialized data types, including single- and
multi-channel images. Experimental results on diverse datasets show that
DINO-MX achieves competitive performance while significantly reducing
computational costs. Additionally, it offers interpretability tools and a
label-guided data augmentation method that improves attention-based
localization without the need for extra detection or segmentation heads.
DINO-MX provides a reproducible and scalable foundation for developing,
adapting, and benchmarking self-supervised vision models across a range of
research and real-world applications.

</details>


### [257] [Benchmark-Ready 3D Anatomical Shape Classification](https://arxiv.org/abs/2511.01613)
*Tom Krsika,Tibor Kubk*

Main category: cs.CV

TL;DR: Proposes PSPooling - a non-learnable mesh pooling operator for 3D anatomical shape classification, integrated into a self-supervised graph autoencoder, and introduces MedShapeNet19 benchmark dataset.


<details>
  <summary>Details</summary>
Motivation: Progress in anatomical 3D shape classification is limited by mesh complexity and lack of standardized benchmarks, highlighting need for robust learning methods and reproducible evaluation.

Method: Introduces Precomputed Structural Pooling (PSPooling) for efficient graph coarsening, integrated into self-supervised graph autoencoder. Uses geometric proximity for node correspondence, enabling parallelizable pooling/unpooling. Evaluates on MedShapeNet19 benchmark.

Result: PSPooling significantly improves reconstruction fidelity and classification accuracy in low-label regimes, establishing strong baseline for medical 3D shape learning.

Conclusion: PSPooling provides efficient structure-preserving graph coarsening suitable for high-resolution medical meshes, and MedShapeNet19 serves as a standardized benchmark for anatomical shape classification research.

Abstract: Progress in anatomical 3D shape classification is limited by the complexity
of mesh data and the lack of standardized benchmarks, highlighting the need for
robust learning methods and reproducible evaluation. We introduce two key steps
toward clinically and benchmark-ready anatomical shape classification via
self-supervised graph autoencoding. We propose Precomputed Structural Pooling
(PSPooling), a non-learnable mesh pooling operator designed for efficient and
structure-preserving graph coarsening in 3D anatomical shape analysis.
PSPooling precomputes node correspondence sets based on geometric proximity,
enabling parallelizable and reversible pooling and unpooling operations with
guaranteed support structure. This design avoids the sparsity and
reconstruction issues of selection-based methods and the sequential overhead of
edge contraction approaches, making it particularly suitable for
high-resolution medical meshes. To demonstrate its effectiveness, we integrate
PSPooling into a self-supervised graph autoencoder that learns anatomy-aware
representations from unlabeled surface meshes. We evaluate the downstream
benefits on MedShapeNet19, a new curated benchmark dataset we derive from
MedShapeNet, consisting of 19 anatomical classes with standardized training,
validation, and test splits. Experiments show that PSPooling significantly
improves reconstruction fidelity and classification accuracy in low-label
regimes, establishing a strong baseline for medical 3D shape learning. We hope
that MedShapeNet19 will serve as a widely adopted benchmark for anatomical
shape classification and further research in medical 3D shape analysis. Access
the complete codebase, model weights, and dataset information here:
https://github.com/TomasKrsicka/MedShapeNet19-PSPooling.

</details>


### [258] [Vote-in-Context: Turning VLMs into Zero-Shot Rank Fusers](https://arxiv.org/abs/2511.01617)
*Mohamed Eltahir,Ali Habibullah,Lama Ayash,Tanveer Hussain,Naeemullah Khan*

Main category: cs.CV

TL;DR: Vote-in-Context (ViC) is a training-free framework that uses Vision-Language Models for zero-shot reranking and fusion in cross-modal video retrieval, achieving state-of-the-art performance by serializing content evidence and retriever metadata in prompts.


<details>
  <summary>Details</summary>
Motivation: Traditional fusion techniques in retrieval rely only on rank or score signals without considering candidates' representations, which is insufficient for complex multi-modal data like videos.

Method: ViC serializes both content evidence and retriever metadata directly within VLM prompts, using S-Grid (a compact image grid representation of videos) to enable list-wise reasoning over video candidates.

Result: ViC achieves Recall@1 scores of 87.1% (t2v) / 89.0% (v2t) on MSR-VTT and 99.6% (v2t) on VATEX, with gains up to +40 Recall@1 over previous state-of-the-art baselines.

Conclusion: ViC provides a simple, reproducible, and highly effective approach for turning modern VLMs into powerful zero-shot rerankers and fusers for complex multi-modal retrieval tasks.

Abstract: In the retrieval domain, candidates' fusion from heterogeneous retrievers is
a long-standing challenge, particularly for complex, multi-modal data such as
videos. While typical fusion techniques are training-free, they rely solely on
rank or score signals, disregarding candidates' representations. This work
introduces Vote-in-Context (ViC), a generalized, training-free framework that
re-thinks list-wise reranking and fusion as a zero-shot reasoning task for a
Vision-Language Model (VLM). The core insight is to serialize both content
evidence and retriever metadata directly within the VLM's prompt, allowing the
model to adaptively weigh retriever consensus against visual-linguistic
content. We demonstrate the generality of this framework by applying it to the
challenging domain of cross-modal video retrieval. To this end, we introduce
the S-Grid, a compact serialization map that represents each video as an image
grid, optionally paired with subtitles to enable list-wise reasoning over video
candidates. ViC is evaluated both as a single-list reranker, where it
dramatically improves the precision of individual retrievers, and as an
ensemble fuser, where it consistently outperforms strong baselines like
CombSUM. Across video retrieval benchmarks including ActivityNet and VATEX, the
framework establishes new state-of-the-art zero-shot retrieval performance,
demonstrating its effectiveness in handling complex visual and temporal signals
alongside text. In zero-shot settings, ViC achieves Recall@1 scores of 87.1%
(t2v) / 89.0% (v2t) on MSR-VTT and 99.6% (v2t) on VATEX, representing massive
gains of up to +40 Recall@1 over previous state-of-the-art baselines. We
present ViC as a simple, reproducible, and highly effective recipe for turning
modern VLMs into powerful zero-shot rerankers and fusers. Code and resources
are publicly available at: https://github.com/mohammad2012191/ViC

</details>


### [259] [Enhancing Diffusion-based Restoration Models via Difficulty-Adaptive Reinforcement Learning with IQA Reward](https://arxiv.org/abs/2511.01645)
*Xiaogang Xu,Ruihang Chu,Jian Wang,Kun Zhou,Wenjie Shu,Harry Yang,Ser-Nam Lim,Hao Chen,Liang Lin*

Main category: cs.CV

TL;DR: The paper proposes a novel RL framework for diffusion-based image restoration that uses MLLM-based IQA models as rewards, focuses on challenging samples, and adaptively combines RL with SFT through automatic weighting.


<details>
  <summary>Details</summary>
Motivation: Existing RL methods are suboptimal for diffusion-based image restoration because restoration emphasizes fidelity over pure generation, requiring different reward strategies.

Method: Uses IQA model-based rewards instead of ground-truth supervision, focuses RL on challenging samples, implements RL with MLLM-based IQA models to align distributions with high-quality images, and adaptively combines RL with SFT using automatic weighting based on sample difficulty.

Result: The proposed RL framework effectively boosts performance across various restoration tasks and demonstrates effectiveness through extensive experiments on multiple benchmarks.

Conclusion: The plug-and-play RL strategy can be seamlessly applied to diffusion-based restoration models, providing significant performance improvements by addressing the unique fidelity requirements of restoration tasks.

Abstract: Reinforcement Learning (RL) has recently been incorporated into diffusion
models, e.g., tasks such as text-to-image. However, directly applying existing
RL methods to diffusion-based image restoration models is suboptimal, as the
objective of restoration fundamentally differs from that of pure generation: it
places greater emphasis on fidelity. In this paper, we investigate how to
effectively integrate RL into diffusion-based restoration models. First,
through extensive experiments with various reward functions, we find that an
effective reward can be derived from an Image Quality Assessment (IQA) model,
instead of intuitive ground-truth-based supervision, which has already been
optimized during the Supervised Fine-Tuning (SFT) stage prior to RL. Moreover,
our strategy focuses on using RL for challenging samples that are significantly
distant from the ground truth, and our RL approach is innovatively implemented
using MLLM-based IQA models to align distributions with high-quality images
initially. As the samples approach the ground truth's distribution, RL is
adaptively combined with SFT for more fine-grained alignment. This dynamic
process is facilitated through an automatic weighting strategy that adjusts
based on the relative difficulty of the training samples. Our strategy is
plug-and-play that can be seamlessly applied to diffusion-based restoration
models, boosting its performance across various restoration tasks. Extensive
experiments across multiple benchmarks demonstrate the effectiveness of our
proposed RL framework.

</details>


### [260] [UniLumos: Fast and Unified Image and Video Relighting with Physics-Plausible Feedback](https://arxiv.org/abs/2511.01678)
*Ropeway Liu,Hangjie Yuan,Bo Dong,Jiazheng Xing,Jinwang Wang,Rui Zhao,Yan Xing,Weihua Chen,Fan Wang*

Main category: cs.CV

TL;DR: UniLumos is a unified relighting framework that uses RGB-space geometry feedback and path consistency learning to achieve physically plausible lighting effects with 20x speedup for both images and videos.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion models for relighting often produce unrealistic results like overexposed highlights and misaligned shadows due to optimization in semantic latent space, which doesn't guarantee physical correctness in visual space.

Method: Uses RGB-space geometry feedback with depth and normal maps extracted from outputs, employs path consistency learning for efficient few-step training, and implements a structured six-dimensional annotation protocol for fine-grained control.

Result: Achieves state-of-the-art relighting quality with significantly improved physical consistency, delivers 20x speedup for both image and video relighting, and introduces LumosBench for automatic attribute-level evaluation.

Conclusion: UniLumos successfully addresses physical plausibility issues in relighting while maintaining efficiency, making it a practical solution for both image and video applications.

Abstract: Relighting is a crucial task with both practical demand and artistic value,
and recent diffusion models have shown strong potential by enabling rich and
controllable lighting effects. However, as they are typically optimized in
semantic latent space, where proximity does not guarantee physical correctness
in visual space, they often produce unrealistic results, such as overexposed
highlights, misaligned shadows, and incorrect occlusions. We address this with
UniLumos, a unified relighting framework for both images and videos that brings
RGB-space geometry feedback into a flow matching backbone. By supervising the
model with depth and normal maps extracted from its outputs, we explicitly
align lighting effects with the scene structure, enhancing physical
plausibility. Nevertheless, this feedback requires high-quality outputs for
supervision in visual space, making standard multi-step denoising
computationally expensive. To mitigate this, we employ path consistency
learning, allowing supervision to remain effective even under few-step training
regimes. To enable fine-grained relighting control and supervision, we design a
structured six-dimensional annotation protocol capturing core illumination
attributes. Building upon this, we propose LumosBench, a disentangled
attribute-level benchmark that evaluates lighting controllability via large
vision-language models, enabling automatic and interpretable assessment of
relighting precision across individual dimensions. Extensive experiments
demonstrate that UniLumos achieves state-of-the-art relighting quality with
significantly improved physical consistency, while delivering a 20x speedup for
both image and video relighting. Code is available at
https://github.com/alibaba-damo-academy/Lumos-Custom.

</details>


### [261] [Progressive Translation of H&E to IHC with Enhanced Structural Fidelity](https://arxiv.org/abs/2511.01698)
*Yuhang Kang,Ziyu Su,Tianyang Wang,Zaibo Li,Wei Chen,Muhammad Khalid Khan Niazi*

Main category: cs.CV

TL;DR: Proposes a progressive network architecture for synthesizing IHC-equivalent images from H&E slides, addressing limitations of existing stain translation methods by optimizing color and structural features in a decoupled manner.


<details>
  <summary>Details</summary>
Motivation: IHC staining is costly and labor-intensive with limited scalability, while existing computational stain translation methods suffer from suboptimal quality due to interdependent loss functions that fail to preserve both structural authenticity and color fidelity.

Method: Developed a progressive network architecture with stage-wise optimization of visual aspects, incorporating color and cell border generation logic. Built upon ASP framework with additional loss functions based on DAB chromogen concentration and image gradient for enhanced color fidelity and boundary clarity.

Result: Experiments on HER2 and ER datasets showed significant improvement in visual quality and finer structural details compared to baseline methods.

Conclusion: The proposed progressive mechanism effectively addresses the limitations of traditional stain translation approaches by decoupling optimization of different visual aspects, resulting in higher quality IHC image synthesis.

Abstract: Compared to hematoxylin-eosin (H&E) staining, immunohistochemistry (IHC) not
only maintains the structural features of tissue samples, but also provides
high-resolution protein localization, which is essential for aiding in
pathology diagnosis. Despite its diagnostic value, IHC remains a costly and
labor-intensive technique. Its limited scalability and constraints in
multiplexing further hinder widespread adoption, especially in resource-limited
settings. Consequently, researchers are increasingly exploring computational
stain translation techniques to synthesize IHC-equivalent images from
H&E-stained slides, aiming to extract protein-level information more
efficiently and cost-effectively. However, most existing stain translation
techniques rely on a linearly weighted summation of multiple loss terms within
a single objective function, strategy that often overlooks the interdepedence
among these components-resulting in suboptimal image quality and an inability
to simultaneously preserve structural authenticity and color fidelity. To
address this limitation, we propose a novel network architecture that follows a
progressive structure, incorporating color and cell border generation logic,
which enables each visual aspect to be optimized in a stage-wise and decoupled
manner. To validate the effectiveness of our proposed network architecture, we
build upon the Adaptive Supervised PatchNCE (ASP) framework as our baseline. We
introduce additional loss functions based on 3,3'-diaminobenzidine (DAB)
chromogen concentration and image gradient, enhancing color fidelity and cell
boundary clarity in the generated IHC images. By reconstructing the generation
pipeline using our structure-color-cell boundary progressive mechanism,
experiments on HER2 and ER datasets demonstrated that the model significantly
improved visual quality and achieved finer structural details.

</details>


### [262] [Learnable Fractional Reaction-Diffusion Dynamics for Under-Display ToF Imaging and Beyond](https://arxiv.org/abs/2511.01704)
*Xin Qiao,Matteo Poggi,Xing Wei,Pengchao Deng,Yanhui Zhou,Stefano Mattoccia*

Main category: cs.CV

TL;DR: LFRD2 is a hybrid framework combining neural networks with physical modeling to address depth quality degradation in under-display ToF imaging caused by TOLED layers, using time-fractional reaction-diffusion and continuous convolution for iterative depth refinement.


<details>
  <summary>Details</summary>
Motivation: Under-display ToF imaging suffers from severe degradations including signal attenuation, multi-path interference (MPI), and temporal noise when cameras are placed beneath TOLED screen panels, which significantly compromise depth quality.

Method: Proposes Learnable Fractional Reaction-Diffusion Dynamics (LFRD2) with time-fractional reaction-diffusion module for iterative depth refinement with dynamic differential orders, and efficient continuous convolution operator via coefficient prediction and repeated differentiation.

Result: Experiments on four benchmark datasets demonstrate the effectiveness of the approach in improving depth restoration quality for under-display ToF imaging.

Conclusion: LFRD2 successfully combines neural network expressiveness with physical modeling interpretability to address depth degradation issues in under-display ToF imaging through TOLED panels.

Abstract: Under-display ToF imaging aims to achieve accurate depth sensing through a
ToF camera placed beneath a screen panel. However, transparent OLED (TOLED)
layers introduce severe degradations-such as signal attenuation, multi-path
interference (MPI), and temporal noise-that significantly compromise depth
quality. To alleviate this drawback, we propose Learnable Fractional
Reaction-Diffusion Dynamics (LFRD2), a hybrid framework that combines the
expressive power of neural networks with the interpretability of physical
modeling. Specifically, we implement a time-fractional reaction-diffusion
module that enables iterative depth refinement with dynamically generated
differential orders, capturing long-term dependencies. In addition, we
introduce an efficient continuous convolution operator via coefficient
prediction and repeated differentiation to further improve restoration quality.
Experiments on four benchmark datasets demonstrate the effectiveness of our
approach. The code is publicly available at https://github.com/wudiqx106/LFRD2.

</details>


### [263] [Probabilistic Robustness for Free? Revisiting Training via a Benchmark](https://arxiv.org/abs/2511.01724)
*Yi Zhang,Zheng Wang,Chen Zhen,Wenjie Ruan,Qing Guo,Siddartha Khastgir,Carsten Maple,Xingyu Zhao*

Main category: cs.CV

TL;DR: PRBench is the first benchmark for evaluating probabilistic robustness (PR) training methods, revealing that adversarial training (AT) methods are more versatile for improving both PR and adversarial robustness (AR), while PR-targeted methods achieve better clean accuracy and lower generalization error.


<details>
  <summary>Details</summary>
Motivation: Existing research focuses heavily on adversarial robustness (AR), while probabilistic robustness (PR) - which measures model performance under stochastic perturbations - is underexplored with limited dedicated training methods and inconsistent evaluation protocols.

Method: PRBench benchmark comprehensively evaluates AT and PR-targeted training methods using metrics including clean accuracy, PR/AR performance, training efficiency, and generalization error across 222 models spanning 7 datasets and 10 architectures.

Result: AT methods are more versatile across hyperparameter settings for improving both AR and PR, while PR-targeted methods consistently achieve lower generalization error and higher clean accuracy.

Conclusion: The benchmark provides standardized evaluation for PR training methods, highlighting trade-offs between different approaches and establishing a public leaderboard for ongoing research in probabilistic robustness.

Abstract: Deep learning models are notoriously vulnerable to imperceptible
perturbations. Most existing research centers on adversarial robustness (AR),
which evaluates models under worst-case scenarios by examining the existence of
deterministic adversarial examples (AEs). In contrast, probabilistic robustness
(PR) adopts a statistical perspective, measuring the probability that
predictions remain correct under stochastic perturbations. While PR is widely
regarded as a practical complement to AR, dedicated training methods for
improving PR are still relatively underexplored, albeit with emerging progress.
Among the few PR-targeted training methods, we identify three limitations: i
non-comparable evaluation protocols; ii limited comparisons to strong AT
baselines despite anecdotal PR gains from AT; and iii no unified framework to
compare the generalization of these methods. Thus, we introduce PRBench, the
first benchmark dedicated to evaluating improvements in PR achieved by
different robustness training methods. PRBench empirically compares most common
AT and PR-targeted training methods using a comprehensive set of metrics,
including clean accuracy, PR and AR performance, training efficiency, and
generalization error (GE). We also provide theoretical analysis on the GE of PR
performance across different training methods. Main findings revealed by
PRBench include: AT methods are more versatile than PR-targeted training
methods in terms of improving both AR and PR performance across diverse
hyperparameter settings, while PR-targeted training methods consistently yield
lower GE and higher clean accuracy. A leaderboard comprising 222 trained models
across 7 datasets and 10 model architectures is publicly available at
https://tmpspace.github.io/PRBenchLeaderboard/.

</details>


### [264] [Toward Strategy Identification and Subtask Decomposition In Task Exploration](https://arxiv.org/abs/2511.01728)
*Tom Odem*

Main category: cs.CV

TL;DR: Developed a task explorer pipeline using clustering, factor analysis, and string edit distance to automatically identify global/local strategies and hierarchical subtasks in human-machine interaction data.


<details>
  <summary>Details</summary>
Motivation: To advance machines' understanding of user knowledge, skill, and behavior for implicit coordination in anticipatory human-machine interaction.

Method: Created a task explorer pipeline using clustering techniques, factor analysis, and string edit distance to identify global strategies (generalized action sets) and local strategies (similar action compositions), plus hierarchical subtask identification.

Result: Successfully identified key strategies used to complete tasks and encoded user runs with hierarchical subtask structures. Also developed a Task Explorer application for reviewing results.

Conclusion: The pipeline can be adapted to any action-based time-series data and the identified strategies/subtasks help inform both humans and machines about user knowledge, skill, and behavior.

Abstract: This research builds on work in anticipatory human-machine interaction, a
subfield of human-machine interaction where machines can facilitate
advantageous interactions by anticipating a user's future state. The aim of
this research is to further a machine's understanding of user knowledge, skill,
and behavior in pursuit of implicit coordination. A task explorer pipeline was
developed that uses clustering techniques, paired with factor analysis and
string edit distance, to automatically identify key global and local strategies
that are used to complete tasks. Global strategies identify generalized sets of
actions used to complete tasks, while local strategies identify sequences that
used those sets of actions in a similar composition. Additionally, meaningful
subtasks of various lengths are identified within the tasks. The task explorer
pipeline was able to automatically identify key strategies used to complete
tasks and encode user runs with hierarchical subtask structures. In addition, a
Task Explorer application was developed to easily review pipeline results. The
task explorer pipeline can be easily modified to any action-based time-series
data and the identified strategies and subtasks help to inform humans and
machines on user knowledge, skill, and behavior.

</details>


### [265] [CGF-DETR: Cross-Gated Fusion DETR for Enhanced Pneumonia Detection in Chest X-rays](https://arxiv.org/abs/2511.01730)
*Yefeng Wu,Yucheng Song,Ling Wu,Shan Wan,Yecheng Zhao*

Main category: cs.CV

TL;DR: CGF-DETR is an enhanced real-time detection transformer for pneumonia detection in chest X-rays, achieving 82.2% mAP@0.5 and 48.1 FPS, outperforming RT-DETR-l by 3.7%.


<details>
  <summary>Details</summary>
Motivation: Pneumonia remains a leading cause of morbidity and mortality worldwide, requiring accurate automated detection systems. While transformer-based detectors like RT-DETR show promise, their application to medical imaging for pneumonia detection is underexplored.

Method: Proposed CGF-DETR with three key modules: XFABlock in backbone for multi-scale feature extraction using convolutional attention with CSP architecture; SPGA module replacing standard multi-head attention with dynamic gating and single-head self-attention; GCFC3 in neck for enhanced feature representation via multi-path convolution fusion with structural re-parameterization.

Result: Achieved 82.2% mAP@0.5 on RSNA Pneumonia Detection dataset, outperforming baseline RT-DETR-l by 3.7% while maintaining comparable inference speed at 48.1 FPS. Complete model achieved 50.4% mAP@[0.5:0.95].

Conclusion: CGF-DETR effectively improves pneumonia detection performance while maintaining real-time capability. Ablation studies confirm each proposed module contributes meaningfully to overall performance improvement.

Abstract: Pneumonia remains a leading cause of morbidity and mortality worldwide,
necessitating accurate and efficient automated detection systems. While recent
transformer-based detectors like RT-DETR have shown promise in object detection
tasks, their application to medical imaging, particularly pneumonia detection
in chest X-rays, remains underexplored. This paper presents CGF-DETR, an
enhanced real-time detection transformer specifically designed for pneumonia
detection. We introduce XFABlock in the backbone to improve multi-scale feature
extraction through convolutional attention mechanisms integrated with CSP
architecture. To achieve efficient feature aggregation, we propose SPGA module
that replaces standard multi-head attention with dynamic gating mechanisms and
single-head self-attention. Additionally, GCFC3 is designed for the neck to
enhance feature representation through multi-path convolution fusion while
maintaining real-time performance via structural re-parameterization. Extensive
experiments on the RSNA Pneumonia Detection dataset demonstrate that CGF-DETR
achieves 82.2\% mAP@0.5, outperforming the baseline RT-DETR-l by 3.7\% while
maintaining comparable inference speed at 48.1 FPS. Our ablation studies
confirm that each proposed module contributes meaningfully to the overall
performance improvement, with the complete model achieving 50.4\%
mAP@[0.5:0.95]

</details>


### [266] [3EED: Ground Everything Everywhere in 3D](https://arxiv.org/abs/2511.01755)
*Rong Li,Yuhao Dong,Tianshuai Hu,Ao Liang,Youquan Liu,Dongyue Lu,Liang Pan,Lingdong Kong,Junwei Liang,Ziwei Liu*

Main category: cs.CV

TL;DR: 3EED is a large-scale multi-platform 3D visual grounding benchmark with RGB and LiDAR data from vehicles, drones, and quadrupeds, featuring 128k+ objects and 22k+ referring expressions across diverse outdoor scenes.


<details>
  <summary>Details</summary>
Motivation: Existing 3D grounding benchmarks are limited to indoor settings, single platforms, and small scale, creating a need for comprehensive outdoor multi-platform benchmarks.

Method: Developed scalable annotation pipeline using vision-language model prompting with human verification, plus platform-aware normalization and cross-modal alignment techniques for cross-platform learning.

Result: Created dataset 10x larger than existing ones, established benchmark protocols, and revealed significant performance gaps in generalizable 3D grounding.

Conclusion: 3EED advances language-driven 3D embodied perception research by providing comprehensive multi-platform outdoor grounding benchmarks and highlighting challenges in cross-platform generalization.

Abstract: Visual grounding in 3D is the key for embodied agents to localize
language-referred objects in open-world environments. However, existing
benchmarks are limited to indoor focus, single-platform constraints, and small
scale. We introduce 3EED, a multi-platform, multi-modal 3D grounding benchmark
featuring RGB and LiDAR data from vehicle, drone, and quadruped platforms. We
provide over 128,000 objects and 22,000 validated referring expressions across
diverse outdoor scenes -- 10x larger than existing datasets. We develop a
scalable annotation pipeline combining vision-language model prompting with
human verification to ensure high-quality spatial grounding. To support
cross-platform learning, we propose platform-aware normalization and
cross-modal alignment techniques, and establish benchmark protocols for
in-domain and cross-platform evaluations. Our findings reveal significant
performance gaps, highlighting the challenges and opportunities of
generalizable 3D grounding. The 3EED dataset and benchmark toolkit are released
to advance future research in language-driven 3D embodied perception.

</details>


### [267] [HGFreNet: Hop-hybrid GraphFomer for 3D Human Pose Estimation with Trajectory Consistency in Frequency Domain](https://arxiv.org/abs/2511.01756)
*Kai Zhai,Ziyan Huang,Qiang Nie,Xiang Li,Bo Ouyang*

Main category: cs.CV

TL;DR: HGFreNet is a novel GraphFormer architecture for 2D-to-3D human pose lifting that addresses depth ambiguity and temporal incoherence through hop-hybrid graph attention and frequency domain trajectory consistency.


<details>
  <summary>Details</summary>
Motivation: Previous methods for 2D-to-3D human pose lifting suffer from depth ambiguity and temporal incoherence due to errors in 2D pose estimation, often focusing only on local temporal constraints while neglecting global spatial-temporal correlations of skeletal joint motion.

Method: Proposes HGFreNet with hop-hybrid graph attention (HGA) module and Transformer encoder to model global joint spatial-temporal correlations. HGA groups k-hop neighbors into hybrid groups for larger receptive field and applies attention to discover latent correlations. Also constrains trajectory consistency in frequency domain and uses preliminary network for 3D pose estimation.

Result: Extensive experiments on Human3.6M and MPI-INF-3DHP benchmarks show HGFreNet outperforms state-of-the-art methods in both positional accuracy and temporal consistency.

Conclusion: The proposed HGFreNet effectively addresses temporal incoherence in 2D-to-3D human pose lifting by modeling global spatial-temporal correlations through hop-hybrid graph attention and frequency domain constraints, achieving superior performance over existing methods.

Abstract: 2D-to-3D human pose lifting is a fundamental challenge for 3D human pose
estimation in monocular video, where graph convolutional networks (GCNs) and
attention mechanisms have proven to be inherently suitable for encoding the
spatial-temporal correlations of skeletal joints. However, depth ambiguity and
errors in 2D pose estimation lead to incoherence in the 3D trajectory. Previous
studies have attempted to restrict jitters in the time domain, for instance, by
constraining the differences between adjacent frames while neglecting the
global spatial-temporal correlations of skeletal joint motion. To tackle this
problem, we design HGFreNet, a novel GraphFormer architecture with hop-hybrid
feature aggregation and 3D trajectory consistency in the frequency domain.
Specifically, we propose a hop-hybrid graph attention (HGA) module and a
Transformer encoder to model global joint spatial-temporal correlations. The
HGA module groups all $k$-hop neighbors of a skeletal joint into a hybrid group
to enlarge the receptive field and applies the attention mechanism to discover
the latent correlations of these groups globally. We then exploit global
temporal correlations by constraining trajectory consistency in the frequency
domain. To provide 3D information for depth inference across frames and
maintain coherence over time, a preliminary network is applied to estimate the
3D pose. Extensive experiments were conducted on two standard benchmark
datasets: Human3.6M and MPI-INF-3DHP. The results demonstrate that the proposed
HGFreNet outperforms state-of-the-art (SOTA) methods in terms of positional
accuracy and temporal consistency.

</details>


### [268] [Wonder3D++: Cross-domain Diffusion for High-fidelity 3D Generation from a Single Image](https://arxiv.org/abs/2511.01767)
*Yuxiao Yang,Xiao-Xiao Long,Zhiyang Dou,Cheng Lin,Yuan Liu,Qingsong Yan,Yuexin Ma,Haoqian Wang,Zhiqiang Wu,Wei Yin*

Main category: cs.CV

TL;DR: Wonder3D++ is a novel method for efficient high-fidelity textured mesh generation from single-view images using cross-domain diffusion and multi-view attention.


<details>
  <summary>Details</summary>
Motivation: To address limitations of existing methods: SDS-based approaches are slow and inconsistent, while fast inference methods produce low-quality results lacking geometric details.

Method: Uses cross-domain diffusion model to generate multi-view normal maps and color images with multi-view cross-domain attention for consistency, followed by cascaded 3D mesh extraction in coarse-to-fine manner.

Result: Achieves high-quality reconstruction results with robust generalization and good efficiency (about 3 minutes per mesh), outperforming prior works.

Conclusion: The method holistically improves quality, consistency, and efficiency of single-view 3D reconstruction tasks through cross-domain diffusion and multi-view attention mechanisms.

Abstract: In this work, we introduce \textbf{Wonder3D++}, a novel method for
efficiently generating high-fidelity textured meshes from single-view images.
Recent methods based on Score Distillation Sampling (SDS) have shown the
potential to recover 3D geometry from 2D diffusion priors, but they typically
suffer from time-consuming per-shape optimization and inconsistent geometry. In
contrast, certain works directly produce 3D information via fast network
inferences, but their results are often of low quality and lack geometric
details. To holistically improve the quality, consistency, and efficiency of
single-view reconstruction tasks, we propose a cross-domain diffusion model
that generates multi-view normal maps and the corresponding color images. To
ensure the consistency of generation, we employ a multi-view cross-domain
attention mechanism that facilitates information exchange across views and
modalities. Lastly, we introduce a cascaded 3D mesh extraction algorithm that
drives high-quality surfaces from the multi-view 2D representations in only
about $3$ minute in a coarse-to-fine manner. Our extensive evaluations
demonstrate that our method achieves high-quality reconstruction results,
robust generalization, and good efficiency compared to prior works. Code
available at https://github.com/xxlong0/Wonder3D/tree/Wonder3D_Plus.

</details>


### [269] [UniLION: Towards Unified Autonomous Driving Model with Linear Group RNNs](https://arxiv.org/abs/2511.01768)
*Zhe Liu,Jinghua Hou,Xiaoqing Ye,Jingdong Wang,Hengshuang Zhao,Xiang Bai*

Main category: cs.CV

TL;DR: UniLION is a unified autonomous driving model that efficiently handles large-scale LiDAR point clouds, multi-view images, and temporal sequences using linear group RNN operators, achieving competitive performance across multiple tasks without requiring explicit fusion modules.


<details>
  <summary>Details</summary>
Motivation: Transformers have quadratic attention mechanisms that introduce significant computational overhead when processing long-sequence data, which is problematic for autonomous driving applications that require processing large-scale sensor data.

Method: Uses linear group RNN operator (performs linear RNN for grouped features) to efficiently handle LiDAR point clouds, multi-view images, and temporal sequences. Serves as a single versatile architecture supporting multiple specialized variants without explicit temporal or multi-modal fusion modules.

Result: Consistently delivers competitive and state-of-the-art performance across core autonomous driving tasks including 3D perception (object detection, tracking, occupancy prediction, BEV segmentation), prediction (motion prediction), and planning (end-to-end planning).

Conclusion: UniLION offers a fresh perspective on developing 3D foundation models for autonomous driving, simplifying multi-modal and multi-task system design while maintaining superior performance.

Abstract: Although transformers have demonstrated remarkable capabilities across
various domains, their quadratic attention mechanisms introduce significant
computational overhead when processing long-sequence data. In this paper, we
present a unified autonomous driving model, UniLION, which efficiently handles
large-scale LiDAR point clouds, high-resolution multi-view images, and even
temporal sequences based on the linear group RNN operator (i.e., performs
linear RNN for grouped features). Remarkably, UniLION serves as a single
versatile architecture that can seamlessly support multiple specialized
variants (i.e., LiDAR-only, temporal LiDAR, multi-modal, and multi-modal
temporal fusion configurations) without requiring explicit temporal or
multi-modal fusion modules. Moreover, UniLION consistently delivers competitive
and even state-of-the-art performance across a wide range of core tasks,
including 3D perception (e.g., 3D object detection, 3D object tracking, 3D
occupancy prediction, BEV map segmentation), prediction (e.g., motion
prediction), and planning (e.g., end-to-end planning). This unified paradigm
naturally simplifies the design of multi-modal and multi-task autonomous
driving systems while maintaining superior performance. Ultimately, we hope
UniLION offers a fresh perspective on the development of 3D foundation models
in autonomous driving. Code is available at
https://github.com/happinesslz/UniLION

</details>


### [270] [How Far Are Surgeons from Surgical World Models? A Pilot Study on Zero-shot Surgical Video Generation with Expert Assessment](https://arxiv.org/abs/2511.01775)
*Zhen Chen,Qing Xu,Jinlin Wu,Biao Yang,Yuhao Zhai,Geng Guo,Jing Zhang,Yinlu Ding,Nassir Navab,Jiebo Luo*

Main category: cs.CV

TL;DR: SurgVeo is the first expert-curated benchmark for evaluating video generation models in surgery, using a four-tiered Surgical Plausibility Pyramid (SPP) framework. Testing Veo-3 model reveals a 'plausibility gap' - it achieves high visual realism but fails at understanding surgical causality and intent.


<details>
  <summary>Details</summary>
Motivation: Foundation models show promise as world simulators but their application in high-stakes surgical domains remains unexplored. Surgery requires deep causal knowledge beyond general physical rules, creating a critical gap in specialized healthcare AI.

Method: Created SurgVeo benchmark and Surgical Plausibility Pyramid (SPP) framework with four assessment tiers. Tasked Veo-3 model with zero-shot prediction on surgical clips from laparoscopic and neurosurgical procedures. Four board-certified surgeons evaluated generated videos using SPP criteria.

Result: Veo-3 achieved exceptional Visual Perceptual Plausibility but failed critically at higher SPP levels: Instrument Operation Plausibility, Environment Feedback Plausibility, and Surgical Intent Plausibility. This reveals a distinct 'plausibility gap' between visual mimicry and causal understanding.

Conclusion: The study provides first quantitative evidence of the gap between visually convincing simulation and true causal understanding in surgical AI. SurgVeo and SPP establish a foundation for developing models that can navigate complex healthcare domains requiring specialized knowledge.

Abstract: Foundation models in video generation are demonstrating remarkable
capabilities as potential world models for simulating the physical world.
However, their application in high-stakes domains like surgery, which demand
deep, specialized causal knowledge rather than general physical rules, remains
a critical unexplored gap. To systematically address this challenge, we present
SurgVeo, the first expert-curated benchmark for video generation model
evaluation in surgery, and the Surgical Plausibility Pyramid (SPP), a novel,
four-tiered framework tailored to assess model outputs from basic appearance to
complex surgical strategy. On the basis of the SurgVeo benchmark, we task the
advanced Veo-3 model with a zero-shot prediction task on surgical clips from
laparoscopic and neurosurgical procedures. A panel of four board-certified
surgeons evaluates the generated videos according to the SPP. Our results
reveal a distinct "plausibility gap": while Veo-3 achieves exceptional Visual
Perceptual Plausibility, it fails critically at higher levels of the SPP,
including Instrument Operation Plausibility, Environment Feedback Plausibility,
and Surgical Intent Plausibility. This work provides the first quantitative
evidence of the chasm between visually convincing mimicry and causal
understanding in surgical AI. Our findings from SurgVeo and the SPP establish a
crucial foundation and roadmap for developing future models capable of
navigating the complexities of specialized, real-world healthcare domains.

</details>


### [271] [PROPEX-RAG: Enhanced GraphRAG using Prompt-Driven Prompt Execution](https://arxiv.org/abs/2511.01802)
*Tejas Sarnaik,Manan Shah,Ravi Hegde*

Main category: cs.CV

TL;DR: A prompt-driven GraphRAG framework that enhances multi-hop question answering through improved prompt design for entity extraction, fact selection, and passage reranking using knowledge graphs.


<details>
  <summary>Details</summary>
Motivation: Current graph-based RAG approaches for complex reasoning overlook the importance of prompt design in improving retrieval and reasoning processes.

Method: Creates symbolic knowledge graphs from text data using entity-fact triples, employs LLMs for semantic filtering and answer generation, and uses Personalized PageRank for entity-guided graph traversal.

Result: Achieved state-of-the-art performance on HotpotQA (80.7% F1, 97.1% Recall@5) and 2WikiMultiHopQA (78.9% F1, 98.1% Recall@5).

Conclusion: Prompt design is crucial for improving retrieval accuracy and response quality in multi-hop question answering systems, enabling more efficient and comprehensible graph reasoning.

Abstract: Retrieval-Augmented Generation (RAG) has become a robust framework for
enhancing Large Language Models (LLMs) with external knowledge. Recent advances
in RAG have investigated graph based retrieval for intricate reasoning;
however, the influence of prompt design on enhancing the retrieval and
reasoning process is still considerably under-examined. In this paper, we
present a prompt-driven GraphRAG framework that underscores the significance of
prompt formulation in facilitating entity extraction, fact selection, and
passage reranking for multi-hop question answering. Our approach creates a
symbolic knowledge graph from text data by encoding entities and factual
relationships as structured facts triples. We use LLMs selectively during
online retrieval to perform semantic filtering and answer generation. We also
use entity-guided graph traversal through Personalized PageRank (PPR) to
support efficient, scalable retrieval based on the knowledge graph we built.
Our system gets state-of-the-art performance on HotpotQA and 2WikiMultiHopQA,
with F1 scores of 80.7% and 78.9%, and Recall@5 scores of 97.1% and 98.1%,
respectively. These results show that prompt design is an important part of
improving retrieval accuracy and response quality. This research lays the
groundwork for more efficient and comprehensible multi-hop question-answering
systems, highlighting the importance of prompt-aware graph reasoning.

</details>


### [272] [SciTextures: Collecting and Connecting Visual Patterns, Models, and Code Across Science and Art](https://arxiv.org/abs/2511.01817)
*Sagi Eppel,Alona Strugatski*

Main category: cs.CV

TL;DR: The paper introduces Scitextures, a large-scale dataset of textures and visual patterns from science, tech, and art domains, with models and code that generate these images, used to evaluate AI's ability to connect visual patterns with their underlying generative mechanisms.


<details>
  <summary>Details</summary>
Motivation: To explore the connection between visual patterns and the mechanisms that produce them, enabling deeper visual understanding of how patterns emerge from underlying processes in various domains.

Method: Created an agentic AI pipeline that autonomously collects and implements models in standardized form, covering over 1,200 models and 100,000 images. Used this dataset to evaluate AI models' ability to link patterns to generative mechanisms and infer mechanisms from real-world patterns.

Result: Vision-language models (VLMs) demonstrated the ability to understand and simulate physical systems beyond visual patterns, successfully identifying, modeling, and coding mechanisms that formed patterns, then generating simulated images comparable to real images.

Conclusion: The Scitextures dataset provides a valuable resource for exploring the connection between visual patterns and their generative mechanisms, showing that AI models can achieve deeper understanding of how patterns emerge from underlying processes.

Abstract: The ability to connect visual patterns with the processes that form them
represents one of the deepest forms of visual understanding. Textures of clouds
and waves, the growth of cities and forests, or the formation of materials and
landscapes are all examples of patterns emerging from underlying mechanisms. We
present the Scitextures dataset, a large-scale collection of textures and
visual patterns from all domains of science, tech, and art, along with the
models and code that generate these images. Covering over 1,200 different
models and 100,000 images of patterns and textures from physics, chemistry,
biology, sociology, technology, mathematics, and art, this dataset offers a way
to explore the connection between the visual patterns that shape our world and
the mechanisms that produce them. Created by an agentic AI pipeline that
autonomously collects and implements models in standardized form, we use
SciTextures to evaluate the ability of leading AI models to link visual
patterns to the models and code that generate them, and to identify different
patterns that emerged from the same process. We also test AIs ability to infer
and recreate the mechanisms behind visual patterns by providing a natural image
of a real-world pattern and asking the AI to identify, model, and code the
mechanism that formed the pattern, then run this code to generate a simulated
image that is compared to the real image. These benchmarks show that
vision-language models (VLMs) can understand and simulate the physical system
beyond a visual pattern. The dataset and code are available at:
https://zenodo.org/records/17485502

</details>


### [273] [TIR-Bench: A Comprehensive Benchmark for Agentic Thinking-with-Images Reasoning](https://arxiv.org/abs/2511.01833)
*Ming Li,Jike Zhong,Shitian Zhao,Haoquan Zhang,Shaoheng Lin,Yuxiang Lai,Wei Chen,Konstantinos Psounis,Kaipeng Zhang*

Main category: cs.CV

TL;DR: TIR-Bench is a new benchmark for evaluating agentic thinking-with-images capabilities in multimodal models, testing complex tool use for image processing across 13 diverse tasks.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks like Visual Search only test basic operations and fail to capture advanced thinking-with-images capabilities in models like OpenAI o3 that can create and operate tools for image transformation.

Method: The authors created TIR-Bench with 13 diverse tasks requiring novel tool use for image processing and manipulation in chain-of-thought reasoning. They evaluated 22 MLLMs including open-source, proprietary, and tool-augmented models.

Result: TIR-Bench proved universally challenging for all tested models, showing that strong performance requires genuine thinking-with-images capabilities. The benchmark revealed limitations in current models' tool-use abilities.

Conclusion: The benchmark successfully measures advanced thinking-with-images capabilities and highlights the need for models to develop genuine tool-use reasoning for complex visual tasks.

Abstract: The frontier of visual reasoning is shifting toward models like OpenAI o3,
which can intelligently create and operate tools to transform images for
problem-solving, also known as thinking-\textit{with}-images in
chain-of-thought. Yet existing benchmarks fail to fully capture this advanced
capability. Even Visual Search, the most common benchmark for current
thinking-\textit{with}-images methods, tests only basic operations such as
localization and cropping, offering little insight into more complex, dynamic,
and tool-dependent reasoning. We introduce \textbf{TIR-Bench}, a comprehensive
benchmark for evaluating agentic thinking-with-images across 13 diverse tasks,
each requiring novel tool use for image processing and manipulation in
chain-of-thought. We evaluate 22 multimodal large language models (MLLMs), from
leading open-sourced and proprietary models to those with explicit tool-use
augmentation. Results show that TIR-Bench is universally challenging, and
strong performance requires genuine thinking-with-images capabilities. Finally,
we present a pilot study comparing direct versus agentic fine-tuning.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [274] [ORANGE: An Online Reflection ANd GEneration framework with Domain Knowledge for Text-to-SQL](https://arxiv.org/abs/2511.00985)
*Yiwen Jiao,Tonghui Ren,Yuche Gao,Zhenying He,Yinan Jing,Kai Zhang,X. Sean Wang*

Main category: cs.DB

TL;DR: ORANGE is an online self-evolutionary framework that builds database-specific knowledge bases from SQL translation logs to bridge the semantic gap between LLMs' general knowledge and domain-specific database semantics, improving Text-to-SQL accuracy.


<details>
  <summary>Details</summary>
Motivation: Large Language Models struggle with domain-specific database semantics despite progress in natural language to SQL translation. Existing methods don't accumulate in-domain knowledge from historical translation logs, which contain valuable real-world usage patterns of database schema.

Method: ORANGE constructs database-specific knowledge bases by parsing SQL queries from translation logs. It uses a novel nested Chain-of-Thought SQL-to-Text strategy with tuple-semantic tracking to ensure reliable knowledge generation and reduce semantic errors.

Result: Experiments on multiple benchmarks confirm ORANGE's effectiveness for real-world Text-to-SQL deployment, particularly in handling complex and domain-specific queries. The framework progressively reduces the semantic gap and enhances SQL translation accuracy.

Conclusion: ORANGE provides a practical solution for real-world Text-to-SQL deployment by accumulating in-domain knowledge from translation logs, effectively bridging the semantic gap between LLMs and domain-specific database semantics.

Abstract: Large Language Models (LLMs) have demonstrated remarkable progress in
translating natural language to SQL, but a significant semantic gap persists
between their general knowledge and domain-specific semantics of databases.
Historical translation logs constitute a rich source of this missing in-domain
knowledge, where SQL queries inherently encapsulate real-world usage patterns
of database schema. Existing methods primarily enhance the reasoning process
for individual translations but fail to accumulate in-domain knowledge from
past translations. We introduce ORANGE, an online self-evolutionary framework
that constructs database-specific knowledge bases by parsing SQL queries from
translation logs. By accumulating in-domain knowledge that contains schema and
data semantics, ORANGE progressively reduces the semantic gap and enhances the
accuracy of subsequent SQL translations. To ensure reliability, we propose a
novel nested Chain-of-Thought SQL-to-Text strategy with tuple-semantic
tracking, which reduces semantic errors during knowledge generation.
Experiments on multiple benchmarks confirm the practicality of ORANGE,
demonstrating its effectiveness for real-world Text-to-SQL deployment,
particularly in handling complex and domain-specific queries.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [275] [LongCat-Flash-Omni Technical Report](https://arxiv.org/abs/2511.00279)
*Meituan LongCat Team,Bairui Wang,Bayan,Bin Xiao,Bo Zhang,Bolin Rong,Borun Chen,Chang Wan,Chao Zhang,Chen Huang,Chen Chen,Chen Chen,Chengxu Yang,Chengzuo Yang,Cong Han,Dandan Peng,Delian Ruan,Detai Xin,Disong Wang,Dongchao Yang,Fanfan Liu,Fengjiao Chen,Fengyu Yang,Gan Dong,Gang Huang,Gang Xu,Guanglu Wan,Guoqiang Tan,Guoqiao Yu,Haibo Qiu,Hao Lu,Hongbo Liu,Hongyu Xiang,Jiaheng Wu,Jian Yang,Jiaxing Liu,Jing Huang,Jingang Wang,Jinrui Ding,Juchao Jiang,Jun Kuang,Jun Wang,Junhui Mei,Ke Ding,Kefeng Zhang,Lei Chen,Liang Shi,Limeng Qiao,Liming Zheng,Lin Ma,Liuyang Guo,Liya Ma,Luying Sun,Man Gao,Mengshen Zhu,Miao Cao,Minliang Lin,Nuo Xu,Peng Shi,Qi Zhang,Qian Fang,Qian Wang,Qian Yang,Quanxiu Wang,Rongxiang Weng,Rongxin Guo,Ruoxuan Liang,Senbin Yang,Shanbo Xu,Shanglin Lei,Shengze Ye,Shimin Chen,Shuaiqi Chen,Shujie Hu,Shuo Li,Siqi Yang,Siyu Xu,Siyu Ren,Song Li,Songxiang Liu,Tianhao Bai,Tianye Dai,Wei Hong,Wei Wang,Weixiao Zhao,Wengang Cao,Wenlong Zhu,Wenlong He,Xi Su,Xi Nan,Xiaohan Zhao,Xiaohao Wang,Xiaoyu Zhao,Xiaoyu Wang,Xiaoyu Li,Xin Pan,Xin Chen,Xiusong Sun,Xu Xiang,Xudong Xing,Xuezhi Cao,Xunliang Cai,Yang Yang,Yanli Tan,Yao Yao,Yerui Sun,Yi Chen,Yifan Lu,Yin Gong,Yining Zhang,Yitian Chen,Yiyang Gan,Yuchen Tang,Yuchen Xie,Yueqian Wang,Yuewen Zheng,Yufei Zhang,Yufeng Zhong,Yulei Qian,Yuqi Peng,Yuwei Jiang,Zeyang Hu,Zheng Zhang,Zhengkun Tian,Zhiqing Hong,Zhixiong Zeng,Zhuqi Mi,Ziran Li,Ziwen Wang,Ziyi Zhao,Ziyuan Zhuang,Zizhe Zhao*

Main category: cs.MM

TL;DR: LongCat-Flash-Omni is a 560B parameter open-source omni-modal model that achieves state-of-the-art real-time audio-visual interaction through progressive training and efficient multimodal architecture.


<details>
  <summary>Details</summary>
Motivation: To develop a comprehensive multimodal model that can handle real-time audio-visual interaction while maintaining strong unimodal capabilities, addressing the challenges of large-scale multimodal training.

Method: Uses curriculum-inspired progressive training strategy, Shortcut-connected Mixture-of-Experts architecture with zero-computation experts, efficient multimodal perception modules, and modality-decoupled parallelism for training infrastructure.

Result: Achieves state-of-the-art performance on omni-modal benchmarks among open-source models, delivers competitive results across text, image, video, and audio tasks, and maintains low-latency real-time interaction despite 560B parameters.

Conclusion: The model successfully demonstrates efficient large-scale multimodal training and provides a foundation for future research in omni-modal AI systems.

Abstract: We introduce LongCat-Flash-Omni, a state-of-the-art open-source omni-modal
model with 560 billion parameters, excelling at real-time audio-visual
interaction. By adopting a curriculum-inspired progressive training strategy
that transitions from simpler to increasingly complex modality sequence
modeling tasks, LongCat-Flash-Omni attains comprehensive multimodal
capabilities while maintaining strong unimodal capability. Building upon
LongCat-Flash, which adopts a high-performance Shortcut-connected
Mixture-of-Experts (MoE) architecture with zero-computation experts,
LongCat-Flash-Omni integrates efficient multimodal perception and speech
reconstruction modules. Despite its immense size of 560B parameters (with 27B
activated), LongCat-Flash-Omni achieves low-latency real-time audio-visual
interaction. For training infrastructure, we developed a modality-decoupled
parallelism scheme specifically designed to manage the data and model
heterogeneity inherent in large-scale multimodal training. This innovative
approach demonstrates exceptional efficiency by sustaining over 90% of the
throughput achieved by text-only training. Extensive evaluations show that
LongCat-Flash-Omni achieves state-of-the-art performance on omni-modal
benchmarks among open-source models. Furthermore, it delivers highly
competitive results across a wide range of modality-specific tasks, including
text, image, and video understanding, as well as audio understanding and
generation. We provide a comprehensive overview of the model architecture
design, training procedures, and data strategies, and open-source the model to
foster future research and development in the community.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [276] [Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph](https://arxiv.org/abs/2511.00086)
*Fali Wang,Jihai Chen,Shuhua Yang,Runxue Bao,Tianxiang Zhao,Zhiwei Zhang,Xianfeng Tang,Hui Liu,Qi He,Suhang Wang*

Main category: cs.LG

TL;DR: Agent-REINFORCE is a novel framework that uses LLM agents to efficiently search for optimal multi-LLM collaboration graphs in Test-Time Scaling, outperforming traditional methods in finding compute-optimal model combinations under fixed budgets.


<details>
  <summary>Details</summary>
Motivation: Prior TTS methods assume fixed collaboration architectures and single-model usage, overlooking that optimal architectures and model combinations vary across tasks. There's a need to search for compute-optimal configurations under fixed budgets.

Method: Reformulate the problem as probabilistic graph optimization and propose Agent-REINFORCE, an LLM-agent-augmented framework that maps REINFORCE pipeline (sampling-gradient-update) to sampling-feedback-update, using textual feedback as gradient to update probabilistic graphs.

Result: Agent-REINFORCE outperforms both traditional and LLM-based baselines in sample efficiency and search performance, effectively identifying optimal graphs under joint objectives of accuracy and inference latency.

Conclusion: The proposed framework successfully addresses the combinatorial search challenge in TTS by leveraging LLM agents and probabilistic graph optimization, enabling efficient discovery of task-specific optimal multi-LLM collaboration architectures.

Abstract: Test-Time Scaling (TTS) improves large language models (LLMs) by allocating
additional computation during inference, typically through parallel,
sequential, or hybrid scaling. However, prior studies often assume fixed
collaboration architectures (e.g., topologies) and single-model usage,
overlooking that optimal architectures and model combinations can vary across
tasks. Therefore, we study the novel problem of searching for compute-optimal
model combinations and architectures in TTS under a fixed budget. We formalize
it as a multi-LLM collaboration graph, where nodes encode roles and LLM model
assignments, and edges capture information flow. This problem is challenging
because (i) the combinatorial search space is prohibitively large, and (ii)
task-specific requirements demand tailored designs. To address these, we
reformulate the problem as probabilistic graph optimization and, through pilot
experiments, derive three empirical insights into TTS collaboration graphs.
Guided by these insights, we propose Agent-REINFORCE, an LLM-agent-augmented
framework that mirrors the REINFORCE pipeline by mapping
sampling-gradient-update to sampling-feedback-update, where feedback serves as
a textual gradient to update the probabilistic graph and efficiently search for
optimal multi-LLM collaboration graphs. Experiments show that Agent-REINFORCE
outperforms both traditional and LLM-based baselines in sample efficiency and
search performance, and effectively identifies optimal graphs under joint
objectives of accuracy and inference latency.

</details>


### [277] [Can SAEs reveal and mitigate racial biases of LLMs in healthcare?](https://arxiv.org/abs/2511.00177)
*Hiba Ahsan,Byron C. Wallace*

Main category: cs.LG

TL;DR: SAEs can identify problematic race associations in LLMs but have limited utility for bias mitigation in realistic clinical tasks.


<details>
  <summary>Details</summary>
Motivation: To detect and control spurious reliance on patient race in LLMs used in healthcare, which could worsen existing biases.

Method: Use Sparse Autoencoders (SAEs) to identify latents correlating with Black individuals in Gemma-2 models, then use these latents to steer model outputs and evaluate bias.

Result: SAE latents activated on both reasonable inputs (e.g., "African American") and problematic words (e.g., "incarceration"). Activating Black latent increased risk assigned to patients becoming "belligerent". SAE steering offered bias mitigation improvements in simple settings but was less successful for complex clinical tasks.

Conclusion: SAEs are useful for identifying problematic demographic reliance in clinical LLMs, but bias mitigation via SAE steering has marginal utility for realistic clinical applications.

Abstract: LLMs are increasingly being used in healthcare. This promises to free
physicians from drudgery, enabling better care to be delivered at scale. But
the use of LLMs in this space also brings risks; for example, such models may
worsen existing biases. How can we spot when LLMs are (spuriously) relying on
patient race to inform predictions? In this work we assess the degree to which
Sparse Autoencoders (SAEs) can reveal (and control) associations the model has
made between race and stigmatizing concepts. We first identify SAE latents in
Gemma-2 models which appear to correlate with Black individuals. We find that
this latent activates on reasonable input sequences (e.g., "African American")
but also problematic words like "incarceration". We then show that we can use
this latent to steer models to generate outputs about Black patients, and
further that this can induce problematic associations in model outputs as a
result. For example, activating the Black latent increases the risk assigned to
the probability that a patient will become "belligerent". We evaluate the
degree to which such steering via latents might be useful for mitigating bias.
We find that this offers improvements in simple settings, but is less
successful for more realistic and complex clinical tasks. Overall, our results
suggest that: SAEs may offer a useful tool in clinical applications of LLMs to
identify problematic reliance on demographics but mitigating bias via SAE
steering appears to be of marginal utility for realistic tasks.

</details>


### [278] [Calibration Across Layers: Understanding Calibration Evolution in LLMs](https://arxiv.org/abs/2511.00280)
*Abhinav Joshi,Areeb Ahmad,Ashutosh Modi*

Main category: cs.LG

TL;DR: LLMs exhibit inherent calibration where predicted probabilities align with correctness. This study shows calibration evolves throughout network depth, with a confidence correction phase in upper layers and identifies a low-dimensional calibration direction that improves calibration metrics without harming accuracy.


<details>
  <summary>Details</summary>
Motivation: To understand how calibration evolves throughout LLM network depth, complementing prior research focused on final layer components like entropy neurons and unembedding matrix null space.

Method: Analyzed multiple open-weight models on MMLU benchmark, tracking calibration evolution through network depth and identifying a low-dimensional calibration direction in the residual stream.

Result: Discovered a distinct confidence correction phase in upper/later layers where model confidence is actively recalibrated after decision certainty is reached. Perturbing the identified calibration direction significantly improves ECE and MCE metrics without reducing accuracy.

Conclusion: Calibration is a distributed phenomenon shaped throughout the network forward pass, not just in the final projection, revealing new insights into confidence-regulating mechanisms within LLMs.

Abstract: Large Language Models (LLMs) have demonstrated inherent calibration
capabilities, where predicted probabilities align well with correctness,
despite prior findings that deep neural networks are often overconfident.
Recent studies have linked this behavior to specific components in the final
layer, such as entropy neurons and the unembedding matrix null space. In this
work, we provide a complementary perspective by investigating how calibration
evolves throughout the network depth. Analyzing multiple open-weight models on
the MMLU benchmark, we uncover a distinct confidence correction phase in the
upper/later layers, where model confidence is actively recalibrated after
decision certainty has been reached. Furthermore, we identify a low-dimensional
calibration direction in the residual stream whose perturbation significantly
improves calibration metrics (ECE and MCE) without harming accuracy. Our
findings suggest that calibration is a distributed phenomenon, shaped
throughout the network forward pass, not just in its final projection,
providing new insights into how confidence-regulating mechanisms operate within
LLMs.

</details>


### [279] [Reject Only Critical Tokens: Pivot-Aware Speculative Decoding](https://arxiv.org/abs/2511.00351)
*Amir Ziashahabi,Yavuz Faruk Bakman,Duygu Nur Yaldiz,Mostafa El-Khamy,Sai Praneeth Karimireddy,Salman Avestimehr*

Main category: cs.LG

TL;DR: The paper proposes Pivot-Aware Speculative Decoding, which relaxes the strict distribution matching requirement of standard speculative decoding by focusing on preserving task-specific utility rather than exact distribution, achieving up to 2.5x speedup.


<details>
  <summary>Details</summary>
Motivation: Standard speculative decoding requires exact distribution matching, which results in unnecessarily low acceptance rates and limits speedups. Real-world LLM use cases prioritize utility (e.g., code correctness, factual accuracy) over exact sampling distribution.

Method: Proposes Pivot-Aware Speculative Decoding that rejects only tokens leading to utility drops in final output. Identifies critical 'pivot tokens' and trains a lightweight classifier to detect them, creating a relaxed version of standard speculative decoding.

Result: Achieves up to 2.5x speedup across various datasets while preserving comparable utility to the target model.

Conclusion: Focusing on utility preservation rather than exact distribution matching enables significantly higher acceptance rates and better speedups in speculative decoding, better aligning with real-world LLM applications.

Abstract: Speculative Decoding (SD) ensures that the output matches the target model's
distribution exactly. However, we argue that this distribution matching
requirement is too stringent and results in unnecessarily low acceptance rates,
limiting potential speedups. Instead, we advocate a reformulation of the
decoding objective: the proposed decoding strategy should match the expected
utility, i.e., the task-specific performance, of the target model. This
perspective also aligns better with real-world use cases of LLMs, where utility
(e.g., code correctness, factual accuracy) is often more important than
sampling distribution. Based on this reformulation, we propose a novel decoding
strategy: Pivot-Aware Speculative Decoding, which rejects only those tokens
that would lead to a utility drop in the final output. We refer to these
critical tokens as pivot tokens. We propose a method for labeling tokens as
pivotal or non-pivotal and train a lightweight classifier to detect them. This
method can be viewed as a relaxed version of standard SD, which offers much
higher acceptance while preserving utility. We evaluate our method across
various datasets, demonstrating that we can achieve up to $2.5\times$ speedup
with comparable utility. Source code is available at
https://github.com/amir-zsh/PAD.

</details>


### [280] [Reasoning Planning for Language Models](https://arxiv.org/abs/2511.00521)
*Bao Nguyen,Hieu Trung Nguyen,Ruifeng She,Xiaojin Fu,Viet Anh Nguyen*

Main category: cs.LG

TL;DR: EPIC is a framework that learns to select optimal reasoning methods for language models by creating a shared representation space that captures model reasoning abilities and query-method compatibility, improving accuracy while reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Existing approaches assume more candidate answers lead to higher accuracy, but this assumption needs revisiting through theoretical analysis to optimize reasoning method selection.

Method: EPIC uses Ensemble Planning with Contrastive learning to learn shared representations, incorporates probability bounds as regularizers, and performs utility-driven optimization balancing accuracy and computational cost.

Result: Experiments on mathematical reasoning tasks show EPIC consistently selects optimal reasoning methods, improving accuracy while reducing computational overhead.

Conclusion: EPIC effectively addresses the challenge of selecting appropriate reasoning methods for language model queries through theoretical analysis and learned representations.

Abstract: Selecting an appropriate reasoning method for a given query remains a key
challenge in language model generation. Existing approaches typically generate
multiple candidate responses and use an aggregation strategy to select the
output answer, often assuming that more candidate answers yield higher
accuracy. We revisit this assumption through a rigorous theoretical analysis,
deriving accuracy bounds for standard aggregation methods under fixed
generation distributions and candidate sizes. Building on these insights, we
introduce EPIC, an Ensemble Planning with Contrastive learning framework to
learn a shared representation space that captures both model reasoning
abilities and query-method compatibility. EPIC incorporates our probability
bounds as a regularizer in a utility-driven optimization that balances accuracy
and computational cost. Experiments on diverse mathematical reasoning tasks
show that EPIC consistently selects optimal reasoning methods, improving
accuracy while reducing computational overhead. Our code can be found at
https://github.com/nguyenngocbaocmt02/EPIC.

</details>


### [281] [Belief Dynamics Reveal the Dual Nature of In-Context Learning and Activation Steering](https://arxiv.org/abs/2511.00617)
*Eric Bigelow,Daniel Wurgaft,YingQiao Wang,Noah Goodman,Tomer Ullman,Hidenori Tanaka,Ekdeep Singh Lubana*

Main category: cs.LG

TL;DR: The paper presents a unified Bayesian framework that explains both prompt-based (in-context learning) and activation-based (steering) control methods for LLMs as instances of the same underlying mechanism - altering the model's belief in latent concepts.


<details>
  <summary>Details</summary>
Motivation: To develop a unifying theoretical account that explains how seemingly disparate methods for controlling LLM behavior (prompt-based and activation-based interventions) can be understood within a common framework.

Method: Developed a Bayesian model where context-based interventions accumulate evidence and activation-based interventions alter concept priors, resulting in a closed-form predictive model tested across domains inspired by many-shot in-context learning.

Result: The Bayesian model successfully predicts LLM behavior across both intervention types, explains prior empirical phenomena (e.g., sigmoidal learning curves), and predicts novel phenomena (e.g., additivity in log-belief space leading to sudden behavioral shifts).

Conclusion: This work provides a unified theoretical account of prompt-based and activation-based control of LLM behavior, along with a methodology for empirically predicting intervention effects.

Abstract: Large language models (LLMs) can be controlled at inference time through
prompts (in-context learning) and internal activations (activation steering).
Different accounts have been proposed to explain these methods, yet their
common goal of controlling model behavior raises the question of whether these
seemingly disparate methodologies can be seen as specific instances of a
broader framework. Motivated by this, we develop a unifying, predictive account
of LLM control from a Bayesian perspective. Specifically, we posit that both
context- and activation-based interventions impact model behavior by altering
its belief in latent concepts: steering operates by changing concept priors,
while in-context learning leads to an accumulation of evidence. This results in
a closed-form Bayesian model that is highly predictive of LLM behavior across
context- and activation-based interventions in a set of domains inspired by
prior work on many-shot in-context learning. This model helps us explain prior
empirical phenomena - e.g., sigmoidal learning curves as in-context evidence
accumulates - while predicting novel ones - e.g., additivity of both
interventions in log-belief space, which results in distinct phases such that
sudden and dramatic behavioral shifts can be induced by slightly changing
intervention controls. Taken together, this work offers a unified account of
prompt-based and activation-based control of LLM behavior, and a methodology
for empirically predicting the effects of these interventions.

</details>


### [282] [FEval-TTC: Fair Evaluation Protocol for Test-Time Compute](https://arxiv.org/abs/2511.01203)
*Pavel Rumiantsev,Soumyasundar Pal,Yingxue Zhang,Mark Coates*

Main category: cs.LG

TL;DR: FEval-TTC is a fair evaluation protocol for test-time compute methods that addresses performance and cost fluctuations in LLMs, ensuring consistent assessment across different models and datasets.


<details>
  <summary>Details</summary>
Motivation: LLM performance and API costs can change over time, potentially invalidating prior research conclusions about test-time compute methods.

Method: Proposes FEval-TTC protocol that standardizes few-shot prompting and answer extraction across multiple LLMs and reasoning datasets, with cost modeling for token and dollar cost estimation.

Result: Provides a standardized evaluation framework that reduces time and monetary overhead for researchers while enabling fair comparisons of test-time compute methods.

Conclusion: FEval-TTC offers a practical solution for consistent evaluation of test-time compute methods and is open-sourced for community use.

Abstract: The performance of Large Language Models (LLMs) and the associated dollar
costs of API calls can fluctuate over time, potentially invalidating
conclusions drawn in prior research. To address this, we propose a Fair
Evaluation protocol for Test-Time Compute (FEval-TTC), designed to ensure
consistent assessment of test-time compute (TTC) methods, regardless of such
fluctuations. FEval-TTC focuses on the evaluation of TTC methods that utilize
underlying Chains-of-Thought (CoT). It supports evaluations across multiple
LLMs on a diverse set of mathematical and commonsense reasoning datasets. The
few-shot prompting and answer extraction processes are standardized across
datasets, reducing both time and monetary overhead for researchers.
Furthermore, we provide a cost modelling procedure that estimates both the
token and dollar cost per query, facilitating equitable comparisons of
prevalent TTC methods. We open-source FEval-TTC for public use at
https://github.com/networkslab/feval_ttc .

</details>


### [283] [RLAC: Reinforcement Learning with Adversarial Critic for Free-Form Generation Tasks](https://arxiv.org/abs/2511.01758)
*Mian Wu,Gavin Zhang,Sewon Min,Sergey Levine,Aviral Kumar*

Main category: cs.LG

TL;DR: RLAC is a reinforcement learning post-training method that uses an adversarial LLM critic to dynamically identify failure modes, reducing verification costs while improving generator output quality in open-ended generation tasks.


<details>
  <summary>Details</summary>
Motivation: Open-ended generation tasks require evaluating outputs against many implicit rubrics, leading to high verification costs and incomplete assessments. Current RL post-training methods struggle to scale due to the complexity of combining multiple rubrics into a single reward.

Method: Uses an LLM as an adversarial critic that dynamically identifies likely failure modes (factual errors, unhandled edge cases). These are verified by an external validator, and both generator and critic are jointly optimized through this adversarial game.

Result: RLAC improves factual accuracy in text generation and correctness in code generation. It outperforms exhaustive verification and reward model methods while reducing required verifications.

Conclusion: Dynamic critics are more effective than fixed critics, demonstrating RLAC's potential for scaling RL post-training to free-form generation tasks by reducing verification costs while maintaining output quality.

Abstract: Open-ended generation tasks require outputs to satisfy diverse and often
implicit task-specific evaluation rubrics. The sheer number of relevant rubrics
leads to prohibitively high verification costs and incomplete assessments of a
response, making reinforcement learning (RL) post-training with rubric-based
rewards difficult to scale. This problem is exacerbated by the fact that often
the best way to combine these rubrics into one single reward is also highly
prompt-specific. We propose Reinforcement Learning with Adversarial Critic
(RLAC), a post-training approach that addresses these challenges via dynamic
rubric verification. Our approach employs a large language model (LLM) as a
critic that dynamically identifies only the most likely failure modes (e.g., a
factual error or unhandled edge case), which are then verified by an external
validator to optimize both generator and critic jointly. By training both the
generator and the critic, this game enhances the critic's error detection and
the generator's output quality while reducing required verifications. Our
experiments demonstrate that RLAC improves factual accuracy in text generation
and correctness in code generation, while also outperforming exhaustive
verification and reward model methods. We show that dynamic critics are more
effective than fixed critics, showcasing the potential of RLAC for scaling RL
post-training to free-form generation tasks.

</details>


### [284] [Random Initialization of Gated Sparse Adapters](https://arxiv.org/abs/2511.01794)
*Vi Retault,Yoha-Eliel Berreby*

Main category: cs.LG

TL;DR: RIGSA is a sparse adaptation method that uses randomly-initialized full-rank adapters with gating and iterative pruning to reduce catastrophic forgetting during fine-tuning, showing better performance than QLoRA on some tasks.


<details>
  <summary>Details</summary>
Motivation: To address catastrophic forgetting in language model fine-tuning by developing a sparse adaptation approach that doesn't impose rank constraints like existing PEFT methods.

Method: Random Initialization of Gated Sparse Adapters (RIGSA) starts with randomly-initialized full-rank adapters, gates them using a ReZero analog, and applies iterative magnitude pruning for sparsification.

Result: RIGSA showed less forgetting than QLoRA on GSM8k while performing comparably to random masking, despite having more trainable parameters than QLoRA.

Conclusion: Sparse adaptation methods like RIGSA can effectively reduce catastrophic forgetting during fine-tuning and offer an alternative to rank-constrained approaches like LoRA.

Abstract: When fine-tuning language models on new tasks, catastrophic forgetting --
performance degradation on previously-learned tasks -- is a ubiquitous problem.
While Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA address this
through low-rank adapters, sparse adaptation offers an alternative that doesn't
impose rank constraints. We introduce Random Initialization of Gated Sparse
Adapters (RIGSA), which starts from randomly-initialized full-rank adapters,
gates them with a ReZero analog, and sparsifies them with iterative magnitude
pruning. We evaluate RIGSA on SmolLM2-1.7B-Instruct using a novel
vision-in-text task (Textual MNIST) and measure forgetting on PIQA, HellaSwag,
and GSM8k. SmolLM2-1.7B-Instruct initially performs around chance level on
Textual MNIST, and is capable of learning the task through RIGSA, 4-bit QLoRA
and random masking. In spite of having more trainable parameters than QLoRA,
the RIGSA configurations that we studied displayed less forgetting than QLoRA,
particularly on GSM8k, though it performs comparably to random masking.

</details>


### [285] [VRScout: Towards Real-Time, Autonomous Testing of Virtual Reality Games](https://arxiv.org/abs/2511.00002)
*Yurun Wu,Yousong Sun,Burkhard Wunsche,Jia Wang,Elliott Wen*

Main category: cs.LG

TL;DR: VRScout is a deep learning-based agent for automated VR game testing that learns from human demonstrations using an Action Chunking Transformer, achieving expert-level performance with real-time inference at 60 FPS.


<details>
  <summary>Details</summary>
Motivation: Traditional human-based quality assurance for VR content is labor-intensive and cannot scale with industry growth. Automated testing for VR faces unique challenges due to high-dimensional sensory inputs and real-time performance requirements.

Method: Uses deep learning with an enhanced Action Chunking Transformer that predicts multi-step action sequences from human demonstrations. Introduces a dynamically adjustable sliding horizon to balance responsiveness and precision by adapting temporal context at runtime.

Result: Achieves expert-level performance on commercial VR titles with limited training data while maintaining real-time inference at 60 FPS on consumer-grade hardware.

Conclusion: VRScout provides a practical and scalable framework for automated VR game testing with applications in quality assurance and safety auditing.

Abstract: Virtual Reality (VR) has rapidly become a mainstream platform for gaming and
interactive experiences, yet ensuring the quality, safety, and appropriateness
of VR content remains a pressing challenge. Traditional human-based quality
assurance is labor-intensive and cannot scale with the industry's rapid growth.
While automated testing has been applied to traditional 2D and 3D games,
extending it to VR introduces unique difficulties due to high-dimensional
sensory inputs and strict real-time performance requirements. We present
VRScout, a deep learning-based agent capable of autonomously navigating VR
environments and interacting with virtual objects in a human-like and real-time
manner. VRScout learns from human demonstrations using an enhanced Action
Chunking Transformer that predicts multi-step action sequences. This enables
our agent to capture higher-level strategies and generalize across diverse
environments. To balance responsiveness and precision, we introduce a
dynamically adjustable sliding horizon that adapts the agent's temporal context
at runtime. We evaluate VRScout on commercial VR titles and show that it
achieves expert-level performance with only limited training data, while
maintaining real-time inference at 60 FPS on consumer-grade hardware. These
results position VRScout as a practical and scalable framework for automated VR
game testing, with direct applications in both quality assurance and safety
auditing.

</details>


### [286] [A generative adversarial network optimization method for damage detection and digital twinning by deep AI fault learning: Z24 Bridge structural health monitoring benchmark validation](https://arxiv.org/abs/2511.00099)
*Marios Impraimakis,Evangelia Nektaria Palkanoglou*

Main category: cs.LG

TL;DR: A conditional-labeled GAN framework is developed for unsupervised damage detection and digital twinning, requiring no prior health state information and outperforming current methods in fault anomaly detection.


<details>
  <summary>Details</summary>
Motivation: Current AI-based digital twinning approaches struggle with uncertainty from limited measurements, missing physics knowledge, or unknown damage states, which is problematic for real-world applications.

Method: Unsupervised conditional-labeled GAN framework tested on Z24 Bridge data, using same damage-level measurements as inputs forced to converge to different damage states, with convergence scores compared to identify different damage states.

Result: The approach accurately captures damage over healthy measurements and creates digital twin measurements for different damage states, enabling pattern recognition and ML data generation.

Conclusion: The framework provides a powerful tool for vibration-based system-level monitoring and scalable infrastructure resilience, with demonstrated effectiveness in damage detection and digital twinning.

Abstract: The optimization-based damage detection and damage state digital twinning
capabilities are examined here of a novel conditional-labeled generative
adversarial network methodology. The framework outperforms current approaches
for fault anomaly detection as no prior information is required for the health
state of the system: a topic of high significance for real-world applications.
Specifically, current artificial intelligence-based digital twinning approaches
suffer from the uncertainty related to obtaining poor predictions when a low
number of measurements is available, physics knowledge is missing, or when the
damage state is unknown. To this end, an unsupervised framework is examined and
validated rigorously on the benchmark structural health monitoring measurements
of Z24 Bridge: a post-tensioned concrete highway bridge in Switzerland. In
implementing the approach, firstly, different same damage-level measurements
are used as inputs, while the model is forced to converge conditionally to two
different damage states. Secondly, the process is repeated for a different
group of measurements. Finally, the convergence scores are compared to identify
which one belongs to a different damage state. The process for both
healthy-to-healthy and damage-to-healthy input data creates, simultaneously,
measurements for digital twinning purposes at different damage states, capable
of pattern recognition and machine learning data generation. Further to this
process, a support vector machine classifier and a principal component analysis
procedure is developed to assess the generated and real measurements of each
damage category, serving as a secondary new dynamics learning indicator in
damage scenarios. Importantly, the approach is shown to capture accurately
damage over healthy measurements, providing a powerful tool for vibration-based
system-level monitoring and scalable infrastructure resilience.

</details>


### [287] [Deep recurrent-convolutional neural network learning and physics Kalman filtering comparison in dynamic load identification](https://arxiv.org/abs/2511.00100)
*Marios Impraimakis*

Main category: cs.LG

TL;DR: Comparison of GRU, LSTM, and CNN neural networks with residual Kalman filter for dynamic load identification under small dataset conditions and various loading scenarios.


<details>
  <summary>Details</summary>
Motivation: Dynamic load identification in civil engineering faces challenges with limited test data and unidentifiable structural models, requiring robust methods for accurate predictions.

Method: Evaluated GRU, LSTM, CNN networks and residual Kalman filter on three cases: simulated structure with shaker excitation, California building with seismic excitation, and IASC-ASCE benchmark with impact loading.

Result: Different methods outperformed each other in various loading scenarios; RKF performed better in physically parametrized identifiable cases.

Conclusion: No single method dominates all scenarios; method selection depends on specific loading conditions and structural identifiability.

Abstract: The dynamic structural load identification capabilities of the gated
recurrent unit, long short-term memory, and convolutional neural networks are
examined herein. The examination is on realistic small dataset training
conditions and on a comparative view to the physics-based residual Kalman
filter (RKF). The dynamic load identification suffers from the uncertainty
related to obtaining poor predictions when in civil engineering applications
only a low number of tests are performed or are available, or when the
structural model is unidentifiable. In considering the methods, first, a
simulated structure is investigated under a shaker excitation at the top floor.
Second, a building in California is investigated under seismic base excitation,
which results in loading for all degrees of freedom. Finally, the International
Association for Structural Control-American Society of Civil Engineers
(IASC-ASCE) structural health monitoring benchmark problem is examined for
impact and instant loading conditions. Importantly, the methods are shown to
outperform each other on different loading scenarios, while the RKF is shown to
outperform the networks in physically parametrized identifiable cases.

</details>


### [288] [Melanoma Classification Through Deep Ensemble Learning and Explainable AI](https://arxiv.org/abs/2511.00246)
*Wadduwage Shanika Perera,ABM Islam,Van Vung Pham,Min Kyung An*

Main category: cs.LG

TL;DR: This paper proposes an ensemble deep learning model for melanoma detection that incorporates explainable AI (XAI) techniques to address the black box problem in medical diagnostics.


<details>
  <summary>Details</summary>
Motivation: Melanoma is a deadly skin cancer requiring early detection, but current deep learning models lack explainability, which reduces trust and reliability in healthcare applications.

Method: Uses ensemble learning with three state-of-the-art deep transfer learning networks and integrates XAI techniques to explain the basis of predictions.

Result: The approach aims to provide high-accuracy melanoma detection while ensuring reliability through explainable predictions.

Conclusion: Combining ensemble deep learning with XAI can overcome the explainability limitations of traditional DL models in medical diagnostics, increasing trust and reliability for melanoma detection.

Abstract: Melanoma is one of the most aggressive and deadliest skin cancers, leading to
mortality if not detected and treated in the early stages. Artificial
intelligence techniques have recently been developed to help dermatologists in
the early detection of melanoma, and systems based on deep learning (DL) have
been able to detect these lesions with high accuracy. However, the entire
community must overcome the explainability limit to get the maximum benefit
from DL for diagnostics in the healthcare domain. Because of the black box
operation's shortcomings in DL models' decisions, there is a lack of
reliability and trust in the outcomes. However, Explainable Artificial
Intelligence (XAI) can solve this problem by interpreting the predictions of AI
systems. This paper proposes a machine learning model using ensemble learning
of three state-of-the-art deep transfer Learning networks, along with an
approach to ensure the reliability of the predictions by utilizing XAI
techniques to explain the basis of the predictions.

</details>


### [289] [Enhancing Adversarial Transferability by Balancing Exploration and Exploitation with Gradient-Guided Sampling](https://arxiv.org/abs/2511.00411)
*Zenghao Niu,Weicheng Xie,Siyang Song,Zitong Yu,Feng Liu,Linlin Shen*

Main category: cs.LG

TL;DR: GGS resolves the exploitation-exploration dilemma in adversarial transfer attacks by guiding inner-iteration sampling along gradient ascent directions, achieving both strong attack potency and cross-model generalization.


<details>
  <summary>Details</summary>
Motivation: Adversarial attacks face a fundamental dilemma between Exploitation (maximizing attack potency) and Exploration (enhancing cross-model generalization), with existing methods over-prioritizing one objective at the expense of the other.

Method: Gradient-Guided Sampling (GGS) builds on MI-FGSM by introducing inner-iteration random sampling guided by previous iteration gradients, with sampling magnitude determined by random distribution, to find balanced regions with both flat loss surfaces and high local maxima.

Result: Comprehensive experiments across multiple DNN architectures and multimodal large language models demonstrate GGS's superiority over state-of-the-art transfer attacks in achieving both strong attack potency and cross-model generalization.

Conclusion: GGS effectively harmonizes exploitation and exploration in adversarial transfer attacks through gradient-guided sampling, providing a balanced solution that outperforms existing methods across diverse model architectures.

Abstract: Adversarial attacks present a critical challenge to deep neural networks'
robustness, particularly in transfer scenarios across different model
architectures. However, the transferability of adversarial attacks faces a
fundamental dilemma between Exploitation (maximizing attack potency) and
Exploration (enhancing cross-model generalization). Traditional momentum-based
methods over-prioritize Exploitation, i.e., higher loss maxima for attack
potency but weakened generalization (narrow loss surface). Conversely, recent
methods with inner-iteration sampling over-prioritize Exploration, i.e.,
flatter loss surfaces for cross-model generalization but weakened attack
potency (suboptimal local maxima). To resolve this dilemma, we propose a simple
yet effective Gradient-Guided Sampling (GGS), which harmonizes both objectives
through guiding sampling along the gradient ascent direction to improve both
sampling efficiency and stability. Specifically, based on MI-FGSM, GGS
introduces inner-iteration random sampling and guides the sampling direction
using the gradient from the previous inner-iteration (the sampling's magnitude
is determined by a random distribution). This mechanism encourages adversarial
examples to reside in balanced regions with both flatness for cross-model
generalization and higher local maxima for strong attack potency. Comprehensive
experiments across multiple DNN architectures and multimodal large language
models (MLLMs) demonstrate the superiority of our method over state-of-the-art
transfer attacks. Code is made available at https://github.com/anuin-cat/GGS.

</details>


### [290] [Region-Aware Reconstruction Strategy for Pre-training fMRI Foundation Model](https://arxiv.org/abs/2511.00443)
*Ruthwik Reddy Doodipala,Pankaj Pandey,Carolina Torres Rojas,Manob Jyoti Saikia,Ranganatha Sitaram*

Main category: cs.LG

TL;DR: The paper introduces an ROI-guided masking strategy using AAL3 atlas for self-supervised pretraining of fMRI foundation models, achieving 4.23% improvement in ADHD classification accuracy compared to random masking.


<details>
  <summary>Details</summary>
Motivation: To move beyond random region masking in fMRI foundation models by developing region-aware reconstruction strategies that leverage anatomical knowledge for better generalization and interpretability.

Method: ROI-guided masking strategy using Automated Anatomical Labelling Atlas (AAL3) applied to full 4D fMRI volumes, selectively masking semantically coherent brain regions during self-supervised pretraining on the ADHD-200 dataset (973 subjects).

Result: 4.23% improvement in classification accuracy for ADHD diagnosis compared to conventional random masking, with region-level attribution showing limbic region and cerebellum contribute most significantly to reconstruction fidelity.

Conclusion: Masking anatomical regions during pretraining enhances both interpretability and robustness of representations, with plans to extend to additional datasets and develop new region-aware loss functions.

Abstract: The emergence of foundation models in neuroimaging is driven by the
increasing availability of large-scale and heterogeneous brain imaging
datasets. Recent advances in self-supervised learning, particularly
reconstruction-based objectives, have demonstrated strong potential for
pretraining models that generalize effectively across diverse downstream
functional MRI (fMRI) tasks. In this study, we explore region-aware
reconstruction strategies for a foundation model in resting-state fMRI, moving
beyond approaches that rely on random region masking. Specifically, we
introduce an ROI-guided masking strategy using the Automated Anatomical
Labelling Atlas (AAL3), applied directly to full 4D fMRI volumes to selectively
mask semantically coherent brain regions during self-supervised pretraining.
Using the ADHD-200 dataset comprising 973 subjects with resting-state fMRI
scans, we show that our method achieves a 4.23% improvement in classification
accuracy for distinguishing healthy controls from individuals diagnosed with
ADHD, compared to conventional random masking. Region-level attribution
analysis reveals that brain volumes within the limbic region and cerebellum
contribute most significantly to reconstruction fidelity and model
representation. Our results demonstrate that masking anatomical regions during
model pretraining not only enhances interpretability but also yields more
robust and discriminative representations. In future work, we plan to extend
this approach by evaluating it on additional neuroimaging datasets, and
developing new loss functions explicitly derived from region-aware
reconstruction objectives. These directions aim to further improve the
robustness and interpretability of foundation models for functional
neuroimaging.

</details>


### [291] [Learning an Efficient Optimizer via Hybrid-Policy Sub-Trajectory Balance](https://arxiv.org/abs/2511.00543)
*Yunchuan Guan,Yu Liu,Ke Zhou,Hui Li,Sen Jia,Zhiqi Shen,Ziyang Wang,Xinglin Zhang,Tao Chen,Jenq-Neng Hwang,Lei Li*

Main category: cs.LG

TL;DR: Lo-Hp is a decoupled two-stage weight generation framework that addresses over-coupling and long-horizon issues in neural weight generation by learning local optimization policies through hybrid-policy sub-trajectory balance.


<details>
  <summary>Details</summary>
Motivation: Current weight generation methods suffer from over-coupling (tight binding of weight generation with task objectives) and long-horizon issues (inefficiency and low accuracy during inference due to lack of local constraints).

Method: Proposes a decoupled two-stage framework with hybrid-policy sub-trajectory balance objective that integrates on-policy and off-policy learning to capture local optimization policies.

Result: Theoretically shows that learning local optimization policies addresses long-horizon issues while enhancing global optimal weight generation. Validates superior accuracy and inference efficiency in transfer learning, few-shot learning, domain generalization, and LLM adaptation.

Conclusion: Lo-Hp provides an effective solution for flexible and efficient weight generation, particularly beneficial for tasks requiring frequent weight updates.

Abstract: Recent advances in generative modeling enable neural networks to generate
weights without relying on gradient-based optimization. However, current
methods are limited by issues of over-coupling and long-horizon. The former
tightly binds weight generation with task-specific objectives, thereby limiting
the flexibility of the learned optimizer. The latter leads to inefficiency and
low accuracy during inference, caused by the lack of local constraints. In this
paper, we propose Lo-Hp, a decoupled two-stage weight generation framework that
enhances flexibility through learning various optimization policies. It adopts
a hybrid-policy sub-trajectory balance objective, which integrates on-policy
and off-policy learning to capture local optimization policies. Theoretically,
we demonstrate that learning solely local optimization policies can address the
long-horizon issue while enhancing the generation of global optimal weights. In
addition, we validate Lo-Hp's superior accuracy and inference efficiency in
tasks that require frequent weight updates, such as transfer learning, few-shot
learning, domain generalization, and large language model adaptation.

</details>


### [292] [EraseFlow: Learning Concept Erasure Policies via GFlowNet-Driven Alignment](https://arxiv.org/abs/2511.00804)
*Abhiram Kusumba,Maitreya Patel,Kyle Min,Changhoon Kim,Chitta Baral,Yezhou Yang*

Main category: cs.LG

TL;DR: EraseFlow is a novel concept erasure framework for text-to-image generators that uses GFlowNets to explore denoising trajectories, enabling effective concept removal without compromising image quality or requiring extensive retraining.


<details>
  <summary>Details</summary>
Motivation: Current concept erasure techniques either degrade image quality, rely on fragile adversarial losses, or require prohibitive retraining cycles, highlighting the need for a more robust and efficient approach.

Method: EraseFlow casts concept unlearning as exploration in denoising path space and optimizes it with GFlowNets using trajectory balance objective, sampling entire trajectories rather than single end states to learn a stochastic policy.

Result: EraseFlow outperforms existing baselines, eliminates the need for carefully crafted reward models, generalizes effectively to unseen concepts, avoids hackable rewards, and achieves optimal trade-off between performance and prior preservation.

Conclusion: EraseFlow provides a superior approach to concept erasure in text-to-image generators by leveraging trajectory exploration with GFlowNets, offering improved performance without compromising model capabilities.

Abstract: Erasing harmful or proprietary concepts from powerful text to image
generators is an emerging safety requirement, yet current "concept erasure"
techniques either collapse image quality, rely on brittle adversarial losses,
or demand prohibitive retraining cycles. We trace these limitations to a myopic
view of the denoising trajectories that govern diffusion based generation. We
introduce EraseFlow, the first framework that casts concept unlearning as
exploration in the space of denoising paths and optimizes it with GFlowNets
equipped with the trajectory balance objective. By sampling entire trajectories
rather than single end states, EraseFlow learns a stochastic policy that steers
generation away from target concepts while preserving the model's prior.
EraseFlow eliminates the need for carefully crafted reward models and by doing
this, it generalizes effectively to unseen concepts and avoids hackable rewards
while improving the performance. Extensive empirical results demonstrate that
EraseFlow outperforms existing baselines and achieves an optimal trade off
between performance and prior preservation.

</details>


### [293] [LL-ViT: Edge Deployable Vision Transformers with Look Up Table Neurons](https://arxiv.org/abs/2511.00812)
*Shashank Nag,Alan T. L. Bacellar,Zachary Susskind,Anshul Jha,Logan Liberty,Aishwarya Sivakumar,Eugene B. John,Krishnan Kailas,Priscila M. V. Lima,Neeraja J. Yadwadkar,Felipe M. G. Franca,Lizy K. John*

Main category: cs.LG

TL;DR: LL-ViT is an edge-optimized vision transformer that integrates LUT-based neurons to reduce model size and computational requirements while maintaining competitive accuracy on vision tasks.


<details>
  <summary>Details</summary>
Motivation: Vision Transformers have high computational and memory demands that challenge edge inference on FPGAs. Existing LUT-based networks reduce memory and compute footprints but perform poorly on vision tasks like CIFAR-10/100.

Method: Proposes LL-ViT with LUT-based channel mixer (MLP layer) replacement, using neural learning approach to learn LUT functions natively. Also develops FPGA-based accelerator for the architecture.

Result: Achieves 95.5% on CIFAR-10, 78.8% on CIFAR-100, 60.9% on Tiny-ImageNet (comparable to baseline). Eliminates 60% weights and 50% multiplications, achieves 1.9x energy efficiency and 1.3x lower latency vs quantized ViT.

Conclusion: LL-ViT provides an efficient edge inference solution for vision transformers with reduced model size, improved computational efficiency, and competitive accuracy on vision tasks.

Abstract: Vision Transformers have been tremendously successful in computer vision
tasks. However, their large computational, memory, and energy demands are a
challenge for edge inference on FPGAs -- a field that has seen a recent surge
in demand. We recognize the benefits of recent works on logic and Look Up Table
(LUT) based networks, such as LogicNets, NeuraLUT, DWN, among others, in
offering models that simultaneously reduce both the memory and compute
footprints. However, these models natively do not perform well on common vision
tasks, such as CIFAR-10/100. In this work, we propose LL-ViT, a novel edge
optimized vision transformer design that integrates layers of LUT neurons
within the transformer architecture. Based on our characterization that reveals
that a majority of model weights and computations are from the channel mixer
(MLP layer), we design an alternate LUT-based channel mixer, and simultaneously
develop an FPGA-based accelerator for LL-ViT. Contrary to some attempts to
replace each multiplication with a table lookup, our architecture utilizes a
neural learning approach which natively learns the LUT functions. This approach
allows for reduced model sizes, and a computational and energy-efficient
inference solution for vision transformer models. Evaluating on edge-suitable
workloads, we achieve accuracies of 95.5% on CIFAR-10, 78.8% on CIFAR-100, and
60.9% on Tiny-ImageNet datasets, comparable to the baseline transformer. LL-ViT
eliminates over 60% of the model weights and 50% of the multiplications in the
model, and achieves 1.9x energy efficiency and 1.3x lower latency over an
integer quantized ViT accelerator, while also offering superior throughput
against prior works at a 10.9W power budget.

</details>


### [294] [Learning with Category-Equivariant Representations for Human Activity Recognition](https://arxiv.org/abs/2511.00900)
*Yoshihiro Maruyama*

Main category: cs.LG

TL;DR: A categorical symmetry-aware learning framework for human activity recognition that builds signal variations into feature representations, improving robustness to real-world distortions like time shifts and device orientation changes.


<details>
  <summary>Details</summary>
Motivation: Human activity recognition faces challenges due to sensor signal shifts from context, motion, and environmental changes, requiring models that remain stable under these variations.

Method: Introduces a categorical symmetry-aware learning framework that captures signal variations over time, scale, and sensor hierarchy, building these factors into feature representation structure to preserve sensor relationships and maintain stability under distortions.

Result: On the UCI Human Activity Recognition benchmark, improves out-of-distribution accuracy by approximately 46 percentage points (approx. 3.6x over baseline).

Conclusion: Abstract symmetry principles can translate into concrete performance gains in everyday sensing tasks through category-equivariant representation theory.

Abstract: Human activity recognition is challenging because sensor signals shift with
context, motion, and environment; effective models must therefore remain stable
as the world around them changes. We introduce a categorical symmetry-aware
learning framework that captures how signals vary over time, scale, and sensor
hierarchy. We build these factors into the structure of feature
representations, yielding models that automatically preserve the relationships
between sensors and remain stable under realistic distortions such as time
shifts, amplitude drift, and device orientation changes. On the UCI Human
Activity Recognition benchmark, this categorical symmetry-driven design
improves out-of-distribution accuracy by approx. 46 percentage points (approx.
3.6x over the baseline), demonstrating that abstract symmetry principles can
translate into concrete performance gains in everyday sensing tasks via
category-equivariant representation theory.

</details>


### [295] [Explore More, Learn Better: Parallel MLLM Embeddings under Mutual Information Minimization](https://arxiv.org/abs/2511.01588)
*Zhicheng Wang,Chen Ju,Xu Chen,Shuai Xiao,Jinsong Lan,Xiaoyong Zhu,Ying Chen,Zhiguo Cao*

Main category: cs.LG

TL;DR: PDF introduces a parallel decoupling framework for multimodal embedding learning that uses MLLMs' steerability to generate multiple parallel embeddings from single inputs, achieving significant performance gains with minimal computational overhead.


<details>
  <summary>Details</summary>
Motivation: Current multimodal embedding models follow the SSC paradigm (single input, singular embedding, contrastive supervision), which collapses rich inputs into monolithic embeddings and fails to fully exploit MLLM capabilities.

Method: PDF conditions a shared MLLM backbone on distinct learnable prefixes to create multiple parallel paths per input, uses Mutual Information Minimization to ensure diversity, and applies per-path contrastive supervision for semantic alignment.

Result: Significant performance gains across various model sizes: +8.9% for VLM2Vec-LLaVA-1.6-LR (7B), +4.2% for VLM2Vec-Qwen2VL (2B), +3.1% for VLM2Vec-Qwen2VL (7B). The 2B model surpasses baseline by +2.6% using only half the computational budget.

Conclusion: PDF effectively leverages MLLM steerability to create diverse parallel embeddings, achieving robust semantic coverage and generalizable embedding spaces with minimal computational overhead at inference.

Abstract: Embedding models are a cornerstone of modern AI. Driven by Multimodal Large
Language Models (MLLMs), they have made great progress in architecture and data
curation, while the holistic paradigm is still limited to SSC, i.e., single
input, singular embedding, contrastive supervision, which collapses rich,
multifaceted inputs into monolithic embeddings and fails to fully exploit MLLM
capabilities. In this paper, we tailor one Parallel Decoupling Framework (PDF)
for multimodal embedding learning, by utilizing the proprietary steerability of
MLLMs, i.e., their ability to flexibly generate quite differentiated response
under explicit instructions. Concretely, PDF conditions a shared MLLM backbone
on distinct, learnable prefixes to roll out multiple parallel paths for one
input, then relies on these paths to obtain parallel embeddings. To promote
full parallel diversity, we employ Mutual Information Minimization (MIM) as an
explicit constraint, coupled with per-path contrastive supervision to maintain
semantic alignment. Such dual-objectives force PDF to yield robust semantic
coverage and a generalizable embedding space. Ultimately, the remarkable
embedding space are accessible at inference via one single forward pass,
incurring negligible computational overhead. We instantiate PDF on multiple
MLLM backbones and prove its effectiveness on MMEB benchmark. Significant gains
are consistently achieved across various resolutions and model sizes, e.g.,
boosting the VLM2Vec-LLaVA-1.6-LR model by a remarkable +8.9% (7B), while the
VLM2Vec-Qwen2VL models by +4.2% (2B) and +3.1% (7B). In terms of efficiency,
our 2B model surpasses its baseline by +2.6% using only half the computational
budget.

</details>


### [296] [Fractional Diffusion Bridge Models](https://arxiv.org/abs/2511.01795)
*Gabriel Nobis,Maximilian Springenberg,Arina Belova,Rembert Daems,Christoph Knochenhauer,Manfred Opper,Tolga Birdal,Wojciech Samek*

Main category: cs.LG

TL;DR: FDBM introduces a generative diffusion bridge framework using fractional Brownian motion approximation to model real stochastic processes with memory effects, long-range dependencies, and anomalous diffusion phenomena that standard Brownian motion-based models miss.


<details>
  <summary>Details</summary>
Motivation: Real stochastic processes exhibit memory effects, long-range dependencies, roughness, and anomalous diffusion that standard diffusion/bridge models using Brownian motion cannot capture, motivating the use of fractional Brownian motion approximation.

Method: Leveraging Markovian approximation of fractional Brownian motion (MA-fBM) to construct FDBM, proving existence of coupling-preserving generative diffusion bridge for future state prediction, and extending to Schrdinger bridge problem with principled loss function for unpaired data translation.

Result: FDBM achieves superior performance over Brownian baselines: lower RMSD of C atomic positions in protein structure prediction and lower FID in unpaired image translation.

Conclusion: FDBM provides an effective framework for modeling complex stochastic processes with memory and long-range dependencies, outperforming traditional Brownian motion-based approaches in both protein conformation prediction and image translation tasks.

Abstract: We present Fractional Diffusion Bridge Models (FDBM), a novel generative
diffusion bridge framework driven by an approximation of the rich and
non-Markovian fractional Brownian motion (fBM). Real stochastic processes
exhibit a degree of memory effects (correlations in time), long-range
dependencies, roughness and anomalous diffusion phenomena that are not captured
in standard diffusion or bridge modeling due to the use of Brownian motion
(BM). As a remedy, leveraging a recent Markovian approximation of fBM (MA-fBM),
we construct FDBM that enable tractable inference while preserving the
non-Markovian nature of fBM. We prove the existence of a coupling-preserving
generative diffusion bridge and leverage it for future state prediction from
paired training data. We then extend our formulation to the Schr\"{o}dinger
bridge problem and derive a principled loss function to learn the unpaired data
translation. We evaluate FDBM on both tasks: predicting future protein
conformations from aligned data, and unpaired image translation. In both
settings, FDBM achieves superior performance compared to the Brownian
baselines, yielding lower root mean squared deviation (RMSD) of C$_\alpha$
atomic positions in protein structure prediction and lower Fr\'echet Inception
Distance (FID) in unpaired image translation.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [297] [Real-time and Zero-footprint Bag of Synthetic Syllables Algorithm for E-mail Spam Detection Using Subject Line and Short Text Fields](https://arxiv.org/abs/2511.00118)
*Stanislav Selitskiy*

Main category: cs.CR

TL;DR: A lightweight Bag of Synthetic Syllables algorithm is proposed for real-time spam detection in email subject lines, using sparse 200-dimensional vectors for similarity comparison without requiring persistent storage or additional resources.


<details>
  <summary>Details</summary>
Motivation: Current email services face high availability demands and resource constraints, while deep learning architectures are too resource-intensive for frontline filtering. Most spam is simple enough to be caught by basic algorithms, reducing the load on more complex systems.

Method: The algorithm creates approximately 200-dimensional sparse vectors from email subject lines using synthetic syllables, then compares them using cosine or Euclidean distance to identify similarities with known spam patterns.

Result: The algorithm was tested on one day of real SMTP traffic and demonstrated effective spam detection capabilities while operating in near real-time with minimal resource footprint.

Conclusion: The Bag of Synthetic Syllables algorithm provides an efficient, lightweight solution for frontline spam filtering that can handle the bulk of unsophisticated spam, reducing the computational burden on more resource-intensive deep learning systems.

Abstract: Contemporary e-mail services have high availability expectations from the
customers and are resource-strained because of the high-volume throughput and
spam attacks. Deep Machine Learning architectures, which are resource hungry
and require off-line processing due to the long processing times, are not
acceptable at the front line filters. On the other hand, the bulk of the
incoming spam is not sophisticated enough to bypass even the simplest
algorithms. While the small fraction of the intelligent, highly mutable spam
can be detected only by the deep architectures, the stress on them can be
unloaded by the simple near real-time and near zero-footprint algorithms such
as the Bag of Synthetic Syllables algorithm applied to the short texts of the
e-mail subject lines and other short text fields. The proposed algorithm
creates a circa 200 sparse dimensional hash or vector for each e-mail subject
line that can be compared for the cosine or euclidean proximity distance to
find similarities to the known spammy subjects. The algorithm does not require
any persistent storage, dictionaries, additional hardware upgrades or software
packages. The performance of the algorithm is presented on the one day of the
real SMTP traffic.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [298] [GeneFlow: Translation of Single-cell Gene Expression to Histopathological Images via Rectified Flow](https://arxiv.org/abs/2511.00119)
*Mengbo Wang,Shourya Verma,Aditya Malusare,Luopin Wang,Yiyang Lu,Vaneet Aggarwal,Mario Sola,Ananth Grama,Nadia Atallah Lanman*

Main category: q-bio.QM

TL;DR: GeneFlow is a novel framework that maps transcriptomics data to cellular images using attention-based RNA encoding and conditional UNet guided by rectified flow, generating high-resolution stained images from gene expression profiles.


<details>
  <summary>Details</summary>
Motivation: To leverage spatial transcriptomics data to create a mapping between transcriptomics and histopathological morphology, enabling biomolecular discovery and disease diagnosis through generated cellular images.

Method: Combines attention-based RNA encoder with conditional UNet guided by rectified flow, using high-order ODE solvers to create continuous bijective mapping between transcriptomics and image manifolds.

Result: Generates realistic cellular morphology features and spatially resolved intercellular interactions from gene expression profiles, outperforms diffusion-based baseline methods in all experiments.

Conclusion: GeneFlow successfully maps transcriptomics to imaging phenotypes, enabling disease diagnosis by revealing dysregulated patterns and providing potential for incorporating genetic/chemical perturbations.

Abstract: Spatial transcriptomics (ST) technologies can be used to align transcriptomes
with histopathological morphology, presenting exciting new opportunities for
biomolecular discovery. Using ST data, we construct a novel framework,
GeneFlow, to map transcriptomics onto paired cellular images. By combining an
attention-based RNA encoder with a conditional UNet guided by rectified flow,
we generate high-resolution images with different staining methods (e.g. H&E,
DAPI) to highlight various cellular/tissue structures. Rectified flow with
high-order ODE solvers creates a continuous, bijective mapping between
transcriptomics and image manifolds, addressing the many-to-one relationship
inherent in this problem. Our method enables the generation of realistic
cellular morphology features and spatially resolved intercellular interactions
from observational gene expression profiles, provides potential to incorporate
genetic/chemical perturbations, and enables disease diagnosis by revealing
dysregulated patterns in imaging phenotypes. Our rectified flow-based method
outperforms diffusion-based baseline method in all experiments. Code can be
found at https://github.com/wangmengbo/GeneFlow.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [299] [SonarSweep: Fusing Sonar and Vision for Robust 3D Reconstruction via Plane Sweeping](https://arxiv.org/abs/2511.00392)
*Lingpeng Chen,Jiakun Tang,Apple Pui-Yi Chui,Ziyang Hong,Junfeng Wu*

Main category: cs.RO

TL;DR: SonarSweep is an end-to-end deep learning framework that fuses sonar and visual data using a plane sweep algorithm for accurate 3D reconstruction in underwater environments, outperforming state-of-the-art methods especially in turbid conditions.


<details>
  <summary>Details</summary>
Motivation: Existing single-modality approaches fail in underwater environments - vision-based methods struggle with poor visibility and geometric constraints, while sonar suffers from elevation ambiguity and low resolution. Prior fusion techniques rely on flawed heuristics and assumptions, causing artifacts and inability to model complex scenes.

Method: SonarSweep adapts the principled plane sweep algorithm for cross-modal fusion between sonar and visual data within an end-to-end deep learning framework.

Result: Extensive experiments in simulation and real-world environments show SonarSweep consistently generates dense and accurate depth maps, significantly outperforming state-of-the-art methods across challenging conditions, particularly in high turbidity.

Conclusion: The framework successfully overcomes limitations of previous methods and the authors will release code and a novel synchronized stereo-camera and sonar dataset to foster further research.

Abstract: Accurate 3D reconstruction in visually-degraded underwater environments
remains a formidable challenge. Single-modality approaches are insufficient:
vision-based methods fail due to poor visibility and geometric constraints,
while sonar is crippled by inherent elevation ambiguity and low resolution.
Consequently, prior fusion technique relies on heuristics and flawed geometric
assumptions, leading to significant artifacts and an inability to model complex
scenes. In this paper, we introduce SonarSweep, a novel, end-to-end deep
learning framework that overcomes these limitations by adapting the principled
plane sweep algorithm for cross-modal fusion between sonar and visual data.
Extensive experiments in both high-fidelity simulation and real-world
environments demonstrate that SonarSweep consistently generates dense and
accurate depth maps, significantly outperforming state-of-the-art methods
across challenging conditions, particularly in high turbidity. To foster
further research, we will publicly release our code and a novel dataset
featuring synchronized stereo-camera and sonar data, the first of its kind.

</details>


### [300] [Fast-SmartWay: Panoramic-Free End-to-End Zero-Shot Vision-and-Language Navigation](https://arxiv.org/abs/2511.00933)
*Xiangyu Shi,Zerui Li,Yanyuan Qiao,Qi Wu*

Main category: cs.RO

TL;DR: Fast-SmartWay is an end-to-end zero-shot VLN-CE framework that uses only three frontal RGB-D images with natural language instructions, eliminating panoramic views and waypoint predictors to reduce latency while maintaining competitive performance.


<details>
  <summary>Details</summary>
Motivation: Existing VLN-CE methods rely on panoramic observations and two-stage pipelines with waypoint predictors, which introduce significant latency and limit real-world applicability.

Method: Uses three frontal RGB-D images with natural language instructions for MLLMs to directly predict actions. Includes Uncertainty-Aware Reasoning module with Disambiguation Module and Future-Past Bidirectional Reasoning for robust decision-making.

Result: Significantly reduces per-step latency while achieving competitive or superior performance compared to panoramic-view baselines in both simulated and real-robot environments.

Conclusion: Fast-SmartWay demonstrates practicality and effectiveness for real-world zero-shot embodied navigation by eliminating panoramic views and waypoint predictors while maintaining performance.

Abstract: Recent advances in Vision-and-Language Navigation in Continuous Environments
(VLN-CE) have leveraged multimodal large language models (MLLMs) to achieve
zero-shot navigation. However, existing methods often rely on panoramic
observations and two-stage pipelines involving waypoint predictors, which
introduce significant latency and limit real-world applicability. In this work,
we propose Fast-SmartWay, an end-to-end zero-shot VLN-CE framework that
eliminates the need for panoramic views and waypoint predictors. Our approach
uses only three frontal RGB-D images combined with natural language
instructions, enabling MLLMs to directly predict actions. To enhance decision
robustness, we introduce an Uncertainty-Aware Reasoning module that integrates
(i) a Disambiguation Module for avoiding local optima, and (ii) a Future-Past
Bidirectional Reasoning mechanism for globally coherent planning. Experiments
on both simulated and real-robot environments demonstrate that our method
significantly reduces per-step latency while achieving competitive or superior
performance compared to panoramic-view baselines. These results demonstrate the
practicality and effectiveness of Fast-SmartWay for real-world zero-shot
embodied navigation.

</details>


### [301] [LiDAR-VGGT: Cross-Modal Coarse-to-Fine Fusion for Globally Consistent and Metric-Scale Dense Mapping](https://arxiv.org/abs/2511.01186)
*Lijie Wang,Lianjie Guo,Ziyi Xu,Qianhao Wang,Fei Gao,Xieyuanli Chen*

Main category: cs.RO

TL;DR: LiDAR-VGGT is a novel framework that tightly couples LiDAR inertial odometry with VGGT model through a two-stage coarse-to-fine fusion pipeline to achieve dense, globally consistent colored point clouds.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of existing methods: LIVO's sensitivity to extrinsic calibration and VGGT's limited scalability in large environments and lack of metric scale.

Method: Two-stage fusion pipeline: pre-fusion module with robust initialization refinement for coarse metric scale estimation, and post-fusion module with bounding-box-based regularization to reduce scale distortions from inconsistent FOVs between LiDAR and camera sensors.

Result: Extensive experiments show LiDAR-VGGT achieves dense, globally consistent colored point clouds and outperforms both VGGT-based methods and LIVO baselines.

Conclusion: The proposed framework successfully addresses the limitations of existing methods and will release an open-source color point cloud evaluation toolkit.

Abstract: Reconstructing large-scale colored point clouds is an important task in
robotics, supporting perception, navigation, and scene understanding. Despite
advances in LiDAR inertial visual odometry (LIVO), its performance remains
highly sensitive to extrinsic calibration. Meanwhile, 3D vision foundation
models, such as VGGT, suffer from limited scalability in large environments and
inherently lack metric scale. To overcome these limitations, we propose
LiDAR-VGGT, a novel framework that tightly couples LiDAR inertial odometry with
the state-of-the-art VGGT model through a two-stage coarse- to-fine fusion
pipeline: First, a pre-fusion module with robust initialization refinement
efficiently estimates VGGT poses and point clouds with coarse metric scale
within each session. Then, a post-fusion module enhances cross-modal 3D
similarity transformation, using bounding-box-based regularization to reduce
scale distortions caused by inconsistent FOVs between LiDAR and camera sensors.
Extensive experiments across multiple datasets demonstrate that LiDAR-VGGT
achieves dense, globally consistent colored point clouds and outperforms both
VGGT-based methods and LIVO baselines. The implementation of our proposed novel
color point cloud evaluation toolkit will be released as open source.

</details>


### [302] [Kinematify: Open-Vocabulary Synthesis of High-DoF Articulated Objects](https://arxiv.org/abs/2511.01294)
*Jiawei Wang,Dingyou Wang,Jiaming Hu,Qixuan Zhang,Jingyi Yu,Lan Xu*

Main category: cs.RO

TL;DR: Kinematify is an automated framework that synthesizes articulated objects from RGB images or text prompts, addressing challenges in inferring kinematic topologies for high-DoF objects and estimating joint parameters from static geometry.


<details>
  <summary>Details</summary>
Motivation: Understanding kinematic structures is essential for robot manipulation and modeling, but creating articulated models for complex systems remains challenging due to reliance on motion sequences or curated datasets, limiting scalability.

Method: Combines MCTS search for structural inference with geometry-driven optimization for joint reasoning to produce physically consistent and functionally valid kinematic descriptions.

Result: Evaluated on diverse synthetic and real-world inputs, showing improvements in registration and kinematic topology accuracy compared to prior work.

Conclusion: Kinematify provides an automated solution for generating articulated object models from visual inputs, overcoming limitations of existing methods and enabling scalable kinematic understanding.

Abstract: A deep understanding of kinematic structures and movable components is
essential for enabling robots to manipulate objects and model their own
articulated forms. Such understanding is captured through articulated objects,
which are essential for tasks such as physical simulation, motion planning, and
policy learning. However, creating these models, particularly for complex
systems like robots or objects with high degrees of freedom (DoF), remains a
significant challenge. Existing methods typically rely on motion sequences or
strong assumptions from hand-curated datasets, which hinders scalability. In
this paper, we introduce Kinematify, an automated framework that synthesizes
articulated objects directly from arbitrary RGB images or text prompts. Our
method addresses two core challenges: (i) inferring kinematic topologies for
high-DoF objects and (ii) estimating joint parameters from static geometry. To
achieve this, we combine MCTS search for structural inference with
geometry-driven optimization for joint reasoning, producing physically
consistent and functionally valid descriptions. We evaluate Kinematify on
diverse inputs from both synthetic and real-world environments, demonstrating
improvements in registration and kinematic topology accuracy over prior work.

</details>


### [303] [MARS: Multi-Agent Robotic System with Multimodal Large Language Models for Assistive Intelligence](https://arxiv.org/abs/2511.01594)
*Renjun Gao,Peiyan Zhong*

Main category: cs.RO

TL;DR: MARS is a multi-agent robotic system powered by multimodal LLMs for assistive intelligence in smart homes, addressing risk-aware planning, personalization, and grounding language plans into executable skills.


<details>
  <summary>Details</summary>
Motivation: Existing MLLM systems struggle with risk-aware planning, user personalization, and grounding language plans into executable skills in cluttered home environments, creating a need for more robust assistive systems for people with disabilities.

Method: The system integrates four specialized agents: visual perception agent for semantic/spatial feature extraction, risk assessment agent for hazard identification/prioritization, planning agent for generating executable action sequences, and evaluation agent for iterative optimization.

Result: Experiments show superior performance in risk-aware planning and coordinated multi-agent execution compared to state-of-the-art multimodal models across multiple datasets.

Conclusion: The approach demonstrates the potential of collaborative AI for practical assistive scenarios and provides a generalizable methodology for deploying MLLM-enabled multi-agent systems in real-world environments.

Abstract: Multimodal large language models (MLLMs) have shown remarkable capabilities
in cross-modal understanding and reasoning, offering new opportunities for
intelligent assistive systems, yet existing systems still struggle with
risk-aware planning, user personalization, and grounding language plans into
executable skills in cluttered homes. We introduce MARS - a Multi-Agent Robotic
System powered by MLLMs for assistive intelligence and designed for smart home
robots supporting people with disabilities. The system integrates four agents:
a visual perception agent for extracting semantic and spatial features from
environment images, a risk assessment agent for identifying and prioritizing
hazards, a planning agent for generating executable action sequences, and an
evaluation agent for iterative optimization. By combining multimodal perception
with hierarchical multi-agent decision-making, the framework enables adaptive,
risk-aware, and personalized assistance in dynamic indoor environments.
Experiments on multiple datasets demonstrate the superior overall performance
of the proposed system in risk-aware planning and coordinated multi-agent
execution compared with state-of-the-art multimodal models. The proposed
approach also highlights the potential of collaborative AI for practical
assistive scenarios and provides a generalizable methodology for deploying
MLLM-enabled multi-agent systems in real-world environments.

</details>


### [304] [Unified Diffusion VLA: Vision-Language-Action Model via Joint Discrete Denoising Diffusion Process](https://arxiv.org/abs/2511.01718)
*Jiayi Chen,Wenxuan Song,Pengxiang Ding,Ziyang Zhou,Han Zhao,Feilong Tang,Donglin Wang,Haoang Li*

Main category: cs.RO

TL;DR: Proposes Unified Diffusion VLA with Joint Discrete Denoising Diffusion Process (JD3P) that synchronously denoises images and actions, achieving state-of-the-art performance on embodied AI benchmarks with 4x faster inference than autoregressive methods.


<details>
  <summary>Details</summary>
Motivation: Existing vision-language-action models either rely on external experts for modality unification or treat image generation and action prediction separately, limiting synergy between understanding, generation, and acting tasks.

Method: Joint diffusion process integrating multiple modalities into single denoising trajectory, built on unified tokenized space and hybrid attention mechanism, with two-stage training pipeline and inference-time optimization techniques.

Result: Achieves state-of-the-art performance on CALVIN, LIBERO, and SimplerEnv benchmarks with 4x faster inference than autoregressive methods, demonstrated through in-depth analysis and real-world evaluations.

Conclusion: The proposed unified diffusion approach enables intrinsic synergy between understanding, generation, and acting through synchronous denoising, providing efficient and effective vision-language-action modeling.

Abstract: Vision-language-action (VLA) models aim to understand natural language
instructions and visual observations and to execute corresponding actions as an
embodied agent. Recent work integrates future images into the
understanding-acting loop, yielding unified VLAs that jointly understand,
generate, and act -- reading text and images and producing future images and
actions. However, these models either rely on external experts for modality
unification or treat image generation and action prediction as separate
processes, limiting the benefits of direct synergy between these tasks. Our
core philosophy is to optimize generation and action jointly through a
synchronous denoising process, where the iterative refinement enables actions
to evolve from initialization, under constant and sufficient visual guidance.
We ground this philosophy in our proposed Unified Diffusion VLA and Joint
Discrete Denoising Diffusion Process (JD3P), which is a joint diffusion process
that integrates multiple modalities into a single denoising trajectory to serve
as the key mechanism enabling understanding, generation, and acting to be
intrinsically synergistic. Our model and theory are built on a unified
tokenized space of all modalities and a hybrid attention mechanism. We further
propose a two-stage training pipeline and several inference-time techniques
that optimize performance and efficiency. Our approach achieves
state-of-the-art performance on benchmarks such as CALVIN, LIBERO, and
SimplerEnv with 4$\times$ faster inference than autoregressive methods, and we
demonstrate its effectiveness through in-depth analysis and real-world
evaluations. Our project page is available at
https://irpn-eai.github.io/UD-VLA.github.io/.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [305] [\texttt{ReMind}: Understanding Deductive Code Reasoning in LLMs](https://arxiv.org/abs/2511.00488)
*Jun Gao,Yun Peng,Xiaoxue Ren*

Main category: cs.PL

TL;DR: ReMind is a multi-agent framework that addresses LLMs' limitations in deductive code reasoning by generating code variants, tracing execution states, and refining reasoning steps to achieve robust zero-shot generalization.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with deductive code reasoning despite progress in code-related tasks, with three key challenges: generation-reasoning gap, code source bias, and weak zero-shot generalization on complex benchmarks.

Method: ReMind uses three coordinated agents: Mutator generates code variants to mitigate bias, Executor traces variable states step-by-step to expose inconsistency, and Inspector identifies problematic reasoning steps and provides control-flow refinement.

Result: Extensive experiments on two benchmarks with five LLMs demonstrate ReMind's superior performance compared to baseline approaches in deductive code reasoning.

Conclusion: ReMind systematically identifies and refines reasoning flaws through multi-agent collaboration, achieving outstanding performance and enabling robust zero-shot generalization for deductive code reasoning.

Abstract: Large Language Models (LLMs) have achieved remarkable progress in
code-related tasks. Despite their advancement, empirical evidence reveals that
they still struggle with \emph{deductive code reasoning}, the ability to reason
about the program execution process. While prior studies have recognized this
limitation, the underlying causes remain largely underexplored. In this paper,
we begin by presenting a comprehensive empirical study that reveals three key
challenges undermining deductive code reasoning: (1) an intrinsic gap between
generation and reasoning abilities, (2) a consistent bias towards code sources,
and (3) weak zero-shot generalization on complex benchmarks. In light of these
challenges, we propose \texttt{ReMind}, a multi-agent framework composed of
\texttt{Mutator}, \texttt{Executor}, and \texttt{Inspector}. The
\texttt{Mutator} generates code variants to mitigate bias towards code sources,
the \texttt{Executor} traces variable states step-by-step to expose
inconsistency, and the \texttt{Inspector} identifies problematic reasoning
steps and provides control-flow refinement to bridge the intrinsic reasoning
gap. Through their coordinated collaboration, \texttt{ReMind} systematically
identifies and refines reasoning flaws, achieving outstanding performance and
enabling robust zero-shot generalization. Extensive experiments on two
benchmarks with five LLMs demonstrate the superior advantages of
\texttt{ReMind} compared to baseline approaches in deductive code reasoning.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [306] [Multimodal Learning with Augmentation Techniques for Natural Disaster Assessment](https://arxiv.org/abs/2511.00004)
*Adrian-Dinu Urse,Dumitru-Clementin Cercel,Florin Pop*

Main category: cs.CY

TL;DR: This paper explores multimodal data augmentation techniques for disaster assessment using social media data, addressing class imbalance and limited samples in the CrisisMMD dataset.


<details>
  <summary>Details</summary>
Motivation: Natural disaster assessment needs accurate real-time information from social media, but existing datasets suffer from class imbalance and limited samples, making model development challenging.

Method: Applied diffusion-based methods (Real Guidance and DiffuseMix) for visual data, and back-translation, paraphrasing with transformers, and image caption-based augmentation for text data. Evaluated across unimodal, multimodal, and multi-view learning setups.

Result: Selected augmentations improved classification performance, especially for underrepresented classes. Multi-view learning showed potential but needs further refinement.

Conclusion: The study highlights effective augmentation strategies for building more robust disaster assessment systems.

Abstract: Natural disaster assessment relies on accurate and rapid access to
information, with social media emerging as a valuable real-time source.
However, existing datasets suffer from class imbalance and limited samples,
making effective model development a challenging task. This paper explores
augmentation techniques to address these issues on the CrisisMMD multimodal
dataset. For visual data, we apply diffusion-based methods, namely Real
Guidance and DiffuseMix. For text data, we explore back-translation,
paraphrasing with transformers, and image caption-based augmentation. We
evaluated these across unimodal, multimodal, and multi-view learning setups.
Results show that selected augmentations improve classification performance,
particularly for underrepresented classes, while multi-view learning introduces
potential but requires further refinement. This study highlights effective
augmentation strategies for building more robust disaster assessment systems.

</details>


### [307] [Chitchat with AI: Understand the supply chain carbon disclosure of companies worldwide through Large Language Model](https://arxiv.org/abs/2511.00024)
*Haotian Hang,Yueyang Shen,Vicky Zhu,Jose Cruz,Michelle Li*

Main category: cs.CY

TL;DR: A novel LLM-based framework for assessing corporate climate disclosure quality using CDP data, enabling cross-sector and cross-country benchmarking through rubric-guided scoring and percentile normalization.


<details>
  <summary>Details</summary>
Motivation: Corporate carbon disclosure is critical for sustainability but presents analytical challenges due to heterogeneity and free-form nature of disclosures, limiting effective benchmarking and decision-making.

Method: Developed a master rubric harmonizing narrative scoring across 11 years of CDP data, integrating rubric-guided LLM scoring with percentile-based normalization to analyze temporal trends and strategic alignment patterns.

Result: Technology sectors and countries like Germany consistently show higher rubric alignment, while others exhibit volatility or superficial engagement, providing actionable insights for stakeholders.

Conclusion: The LLM-based approach transforms unstructured disclosures into quantifiable, comparable intelligence, advancing AI-enabled decision support systems in climate governance.

Abstract: In the context of global sustainability mandates, corporate carbon disclosure
has emerged as a critical mechanism for aligning business strategy with
environmental responsibility. The Carbon Disclosure Project (CDP) hosts the
world's largest longitudinal dataset of climate-related survey responses,
combining structured indicators with open-ended narratives, but the
heterogeneity and free-form nature of these disclosures present significant
analytical challenges for benchmarking, compliance monitoring, and investment
screening. This paper proposes a novel decision-support framework that
leverages large language models (LLMs) to assess corporate climate disclosure
quality at scale. It develops a master rubric that harmonizes narrative scoring
across 11 years of CDP data (2010-2020), enabling cross-sector and
cross-country benchmarking. By integrating rubric-guided scoring with
percentile-based normalization, our method identifies temporal trends,
strategic alignment patterns, and inconsistencies in disclosure across
industries and regions. Results reveal that sectors such as technology and
countries like Germany consistently demonstrate higher rubric alignment, while
others exhibit volatility or superficial engagement, offering insights that
inform key decision-making processes for investors, regulators, and corporate
environmental, social, and governance (ESG) strategists. The proposed LLM-based
approach transforms unstructured disclosures into quantifiable, interpretable,
comparable, and actionable intelligence, advancing the capabilities of
AI-enabled decision support systems (DSSs) in the domain of climate governance.

</details>


### [308] [Wayfinding through the AI wilderness: Mapping rhetorics of ChatGPT prompt writing on X (formerly Twitter) to promote critical AI literacies](https://arxiv.org/abs/2511.00106)
*Anuj Gupta,Ann Shivers-McNair*

Main category: cs.CY

TL;DR: This paper analyzes ChatGPT prompt writing discussions on social media to develop critical AI literacy frameworks, using computational and qualitative methods to study 32,000 X/Twitter posts.


<details>
  <summary>Details</summary>
Motivation: To understand how social media discussions about prompt writing can inform critical AI literacy education and research in digital writing contexts.

Method: Combined computational methods with qualitative analysis of 32,000 X/Twitter posts about prompt writing from November 2022 to May 2023, drawing from four traditions of digital writing research.

Result: Identified five key themes: communication impacts of prompt writing, micro-literacy resources, market rhetoric influence, rhetorical characteristics of prompts, and definitions of prompt writing.

Conclusion: The study provides valuable insights and methodologies for digital writing teachers and researchers to develop critical AI literacy education through analyzing social media prompt writing practices.

Abstract: In this paper, we demonstrate how studying the rhetorics of ChatGPT prompt
writing on social media can promote critical AI literacies. Prompt writing is
the process of writing instructions for generative AI tools like ChatGPT to
elicit desired outputs and there has been an upsurge of conversations about it
on social media. To study this rhetorical activity, we build on four
overlapping traditions of digital writing research in computers and composition
that inform how we frame literacies, how we study social media rhetorics, how
we engage iteratively and reflexively with methodologies and technologies, and
how we blend computational methods with qualitative methods. Drawing on these
four traditions, our paper shows our iterative research process through which
we gathered and analyzed a dataset of 32,000 posts (formerly known as tweets)
from X (formerly Twitter) about prompt writing posted between November 2022 to
May 2023. We present five themes about these emerging AI literacy practices:
(1) areas of communication impacted by prompt writing, (2) micro-literacy
resources shared for prompt writing, (3) market rhetoric shaping prompt
writing, (4) rhetorical characteristics of prompts, and (5) definitions of
prompt writing. In discussing these themes and our methodologies, we highlight
takeaways for digital writing teachers and researchers who are teaching and
analyzing critical AI literacies.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [309] [Novelty and Impact of Economics Papers](https://arxiv.org/abs/2511.01211)
*Chaofeng Wu*

Main category: econ.GN

TL;DR: This paper proposes a two-dimensional framework for scientific novelty: spatial novelty (intellectual distinctiveness) and temporal novelty (engagement with research frontier), showing they predict different types of scientific impact.


<details>
  <summary>Details</summary>
Motivation: To move beyond viewing scientific novelty as a single attribute and instead understand it as a reflection of a paper's position within the evolving intellectual landscape.

Method: Leveraged Large Language Models to develop semantic isolation metrics that quantify a paper's location relative to the full-text literature, applied to a large corpus of economics articles.

Result: Found a fundamental trade-off: temporal novelty primarily predicts citation counts, while spatial novelty predicts disruptive impact. Identified four archetypes of semantic neighborhoods with distinct impact profiles.

Conclusion: Novelty can be understood as a multidimensional construct whose different forms, reflecting a paper's strategic location, have measurable and fundamentally distinct consequences for scientific progress.

Abstract: We propose a framework that recasts scientific novelty not as a single
attribute of a paper, but as a reflection of its position within the evolving
intellectual landscape. We decompose this position into two orthogonal
dimensions: \textit{spatial novelty}, which measures a paper's intellectual
distinctiveness from its neighbors, and \textit{temporal novelty}, which
captures its engagement with a dynamic research frontier. To operationalize
these concepts, we leverage Large Language Models to develop semantic isolation
metrics that quantify a paper's location relative to the full-text literature.
Applying this framework to a large corpus of economics articles, we uncover a
fundamental trade-off: these two dimensions predict systematically different
outcomes. Temporal novelty primarily predicts citation counts, whereas spatial
novelty predicts disruptive impact. This distinction allows us to construct a
typology of semantic neighborhoods, identifying four archetypes associated with
distinct and predictable impact profiles. Our findings demonstrate that novelty
can be understood as a multidimensional construct whose different forms,
reflecting a paper's strategic location, have measurable and fundamentally
distinct consequences for scientific progress.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [310] [A Proof of Learning Rate Transfer under $$P](https://arxiv.org/abs/2511.01734)
*Soufiane Hayou*

Main category: stat.ML

TL;DR: First proof of learning rate transfer in P-parametrized linear MLPs, showing optimal learning rate converges to non-zero constant as width increases, unlike SP and NTP.


<details>
  <summary>Details</summary>
Motivation: To theoretically explain learning rate transfer phenomenon in neural networks and understand how different parameterizations affect feature learning in infinite-width limit.

Method: Theoretical analysis of linear multi-layer perceptrons under P parameterization, comparing with Standard Parametrization (SP) and Neural Tangent Parametrization (NTP).

Result: Under P, optimal learning rate converges to non-zero constant as width  , enabling learning rate transfer. This property fails for SP and NTP.

Conclusion: P parameterization uniquely enables learning rate transfer by maintaining non-zero optimal learning rates in infinite-width limit, supporting feature learning.

Abstract: We provide the first proof of learning rate transfer with width in a linear
multi-layer perceptron (MLP) parametrized with $\mu$P, a neural network
parameterization designed to ``maximize'' feature learning in the
infinite-width limit. We show that under $\mu P$, the optimal learning rate
converges to a \emph{non-zero constant} as width goes to infinity, providing a
theoretical explanation to learning rate transfer. In contrast, we show that
this property fails to hold under alternative parametrizations such as Standard
Parametrization (SP) and Neural Tangent Parametrization (NTP). We provide
intuitive proofs and support the theoretical findings with extensive empirical
results.

</details>


### [311] [Few-Shot Multimodal Medical Imaging: A Theoretical Framework](https://arxiv.org/abs/2511.01140)
*Md Talha Mohsin,Ismail Abdulrashid*

Main category: stat.ML

TL;DR: Proposes a unified theoretical framework for learning under low-resource medical imaging conditions, addressing sample complexity, uncertainty quantification, and interpretability.


<details>
  <summary>Details</summary>
Motivation: Medical imaging faces challenges with limited labeled data, fragmented systems, and unbalanced datasets, leading to diagnostic uncertainty, underrepresentation, reduced model robustness, and biased decisions.

Method: Formalizes learning objectives under few-shot conditions, computes sample complexity constraints, uses PAC-learning and PAC-Bayesian theory to explain multimodal integration, and proposes a formal metric for explanation stability.

Result: Establishes sample complexity bounds for clinically reliable accuracy, explains how multimodal integration encourages generalization, quantifies uncertainty under sparse supervision, and provides interpretability guarantees.

Conclusion: The framework provides a principled foundation for building dependable, data-efficient diagnostic systems by jointly characterizing sample efficiency, uncertainty quantification, and interpretability in a unified theoretical setting.

Abstract: Medical imaging relies heavily on large, labeled datasets. But,
unfortunately, they are not always easily accessible in clinical settings.
Additionally, many practitioners often face various structural obstacles like
limited data availability, fragmented data systems, and unbalanced datasets.
These barriers often lead to the increased diagnostic uncertainty,
underrepresentation of certain conditions, reduced model robustness, and biased
diagnostic decisions. In response to these challenges, approaches such as
transfer learning, meta-learning, and multimodal fusion have made great
strides. However, they still need a solid theoretical justification for why
they succeed or fail in situations where data is scarce. To address this gap,
we propose a unified theoretical framework that characterizes learning and
inference under low-resource medical imaging conditions. We first formalize the
learning objective under few-shot conditions and compute sample complexity
constraints to estimate the smallest quantity of data needed to achieve
clinically reliable accuracy. Then based on ideas from PAC-learning and
PAC-Bayesian theory, we explain how multimodal integration encourages
generalization and quantifies uncertainty under sparse supervision. We further
propose a formal metric for explanation stability, offering interpretability
guarantees under low-data conditions. Taken together, the proposed framework
establishes a principled foundation for constructing dependable, data-efficient
diagnostic systems by jointly characterizing sample efficiency, uncertainty
quantification, and interpretability in a unified theoretical setting.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [312] [MULTI-Bench: A Multi-Turn Interactive Benchmark for Assessing Emotional Intelligence ability of Spoken Dialogue Models](https://arxiv.org/abs/2511.00850)
*Yayue Deng,Guoqiang Hu,Haiyang Sun,Xiangyu Zhang,Haoyang Zhang,Fei Tian,Xuerui Yang,Gang Yu,Eng Siong Chng*

Main category: eess.AS

TL;DR: Multi-Bench is the first benchmark for evaluating Spoken Dialogue Models in multi-turn interactive dialogue with emotional intelligence, featuring hierarchical tracks and 3.2K samples across five tasks.


<details>
  <summary>Details</summary>
Motivation: Current SDM benchmarks focus on single-turn exchanges, leaving multi-turn interactive conversations with emotional intelligence underexplored.

Method: Multi-Bench uses a hierarchical structure with basic (emotion understanding/reasoning) and advanced (emotion support/application) tracks, five tasks, and 3.2K samples with reproducible evaluation framework.

Result: Evaluation of six SDMs shows good performance on basic understanding tasks but room for improvement in advanced multi-turn dialogue and reasoning, especially in emotion awareness and application.

Conclusion: Current SDMs need improvement in multi-turn interactive dialogue and emotional reasoning capabilities, highlighting the importance of specialized benchmarks like Multi-Bench.

Abstract: Spoken Dialogue Models (SDMs) have advanced rapidly, yet their ability to
sustain genuinely interactive multi-turn conversations remains underexplored,
as most benchmarks focus on single-turn exchanges. We introduce Multi-Bench,
the first benchmark explicitly designed to evaluate SDMs in multi-turn
interactive dialogue with an emphasis on emotional intelligence. Multi-Bench
employs a hierarchical structure with a basic track for emotion understanding
and reasoning and an advanced track for emotion support and application. It
comprises five carefully designed tasks and about 3.2K samples, ranging from
emotion recognition to complex reasoning and interactive dialogue, supported by
a reproducible evaluation framework. We evaluate six representative SDMs on
eight subsets of Multi-Bench. Results show that while current SDMs achieve good
performance on basic understanding tasks, they still have room for improvement
in advanced multi-turn interactive dialogue and reasoning-related tasks,
particularly in emotion awareness and application.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [313] [Structurally Refined Graph Transformer for Multimodal Recommendation](https://arxiv.org/abs/2511.00584)
*Ke Shi,Yan Zhang,Miao Zhang,Lifan Chen,Jiali Yi,Kui Xiao,Xiaoju Hou,Zhifei Li*

Main category: cs.IR

TL;DR: SRGFormer is a multimodal recommendation model that addresses limitations in existing approaches by optimizing transformer architecture, using hypergraph structures for local relationships, and applying self-supervised learning to better integrate multimodal data and capture user behavior patterns.


<details>
  <summary>Details</summary>
Motivation: Current multimodal recommendation models fail to distinguish between redundant and valuable data, rely on single semantic frameworks leading to biased user preference representations, and cannot capture complex user-item interactions, limiting their ability to serve diverse users.

Method: Modified transformer architecture to capture overall user behavior patterns, embedded multimodal information into hypergraph structures to learn local user-item relationships, and applied self-supervised tasks to user-item collaborative signals to enhance multimodal integration.

Result: Extensive experiments on three public datasets showed SRGFormer outperforms previous benchmark models, achieving an average performance improvement of 4.47% on the Sports dataset.

Conclusion: SRGFormer effectively addresses key challenges in multimodal recommendation by structurally optimizing the model to better capture user behavior patterns and integrate multimodal information, demonstrating superior performance over existing approaches.

Abstract: Multimodal recommendation systems utilize various types of information,
including images and text, to enhance the effectiveness of recommendations. The
key challenge is predicting user purchasing behavior from the available data.
Current recommendation models prioritize extracting multimodal information
while neglecting the distinction between redundant and valuable data. They also
rely heavily on a single semantic framework (e.g., local or global semantics),
resulting in an incomplete or biased representation of user preferences,
particularly those less expressed in prior interactions. Furthermore, these
approaches fail to capture the complex interactions between users and items,
limiting the model's ability to meet diverse users. To address these
challenges, we present SRGFormer, a structurally optimized multimodal
recommendation model. By modifying the transformer for better integration into
our model, we capture the overall behavior patterns of users. Then, we enhance
structural information by embedding multimodal information into a hypergraph
structure to aid in learning the local structures between users and items.
Meanwhile, applying self-supervised tasks to user-item collaborative signals
enhances the integration of multimodal information, thereby revealing the
representational features inherent to the data's modality. Extensive
experiments on three public datasets reveal that SRGFormer surpasses previous
benchmark models, achieving an average performance improvement of 4.47 percent
on the Sports dataset. The code is publicly available online.

</details>


### [314] [LookSync: Large-Scale Visual Product Search System for AI-Generated Fashion Looks](https://arxiv.org/abs/2511.00072)
*Pradeep M,Ritesh Pallod,Satyen Abrol,Muthu Raman,Ian Anderson*

Main category: cs.IR

TL;DR: An end-to-end product search system that matches AI-generated fashion looks with real products using CLIP-based vector similarity search, deployed at internet scale serving 350,000+ AI looks daily.


<details>
  <summary>Details</summary>
Motivation: Generative AI is creating virtual fashion looks and avatars, creating a need to find real products that match these AI-generated styles for practical implementation.

Method: Four-component pipeline: query generation, vectorization, candidate retrieval, and reranking based on AI-generated looks using CLIP model for vector similarity search across 12M+ products.

Result: CLIP outperformed alternative models by 3-7% in mean opinion scores, resulting in noticeably better user perception matches. System serves 350,000+ AI looks daily across global markets.

Conclusion: CLIP established as the most reliable backbone for production deployment in AI-generated fashion look matching, with modest but meaningful improvements in user perception.

Abstract: Generative AI is reshaping fashion by enabling virtual looks and avatars
making it essential to find real products that best match AI-generated styles.
We propose an end-to-end product search system that has been deployed in a
real-world, internet scale which ensures that AI-generated looks presented to
users are matched with the most visually and semantically similar products from
the indexed vector space. The search pipeline is composed of four key
components: query generation, vectorization, candidate retrieval, and reranking
based on AI-generated looks. Recommendation quality is evaluated using
human-judged accuracy scores. The system currently serves more than 350,000 AI
Looks in production per day, covering diverse product categories across global
markets of over 12 million products. In our experiments, we observed that
across multiple annotators and categories, CLIP outperformed alternative models
by a small relative margin of 3--7\% in mean opinion scores. These
improvements, though modest in absolute numbers, resulted in noticeably better
user perception matches, establishing CLIP as the most reliable backbone for
production deployment.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [315] [Multimodal Detection of Fake Reviews using BERT and ResNet-50](https://arxiv.org/abs/2511.00020)
*Suhasnadh Reddy Veluru,Sai Teja Erukude,Viswa Chaitanya Marella*

Main category: cs.AI

TL;DR: A multimodal fake review detection framework using BERT for text and ResNet-50 for images outperforms unimodal approaches, achieving 0.934 F1-score by detecting cross-modal inconsistencies in deceptive content.


<details>
  <summary>Details</summary>
Motivation: Fake reviews generated by bots, paid agents, or AI threaten trust in digital commerce, and existing unimodal detection methods fail to capture semantic inconsistencies across different modalities.

Method: Proposed multimodal framework integrates BERT-encoded textual features and ResNet-50-extracted visual features, fused through a classification head to predict review authenticity using a curated dataset of 21,142 user-uploaded images across multiple domains.

Result: The multimodal model achieved 0.934 F1-score on test set, outperforming unimodal baselines, and effectively detected subtle inconsistencies like exaggerated text paired with unrelated or low-quality images.

Conclusion: Multimodal learning is crucial for safeguarding digital trust and provides a scalable solution for content moderation across online platforms by detecting cross-modal inconsistencies in deceptive reviews.

Abstract: In the current digital commerce landscape, user-generated reviews play a
critical role in shaping consumer behavior, product reputation, and platform
credibility. However, the proliferation of fake or misleading reviews often
generated by bots, paid agents, or AI models poses a significant threat to
trust and transparency within review ecosystems. Existing detection models
primarily rely on unimodal, typically textual, data and therefore fail to
capture semantic inconsistencies across different modalities. To address this
gap, a robust multimodal fake review detection framework is proposed,
integrating textual features encoded with BERT and visual features extracted
using ResNet-50. These representations are fused through a classification head
to jointly predict review authenticity. To support this approach, a curated
dataset comprising 21,142 user-uploaded images across food delivery,
hospitality, and e-commerce domains was utilized. Experimental results indicate
that the multimodal model outperforms unimodal baselines, achieving an F1-score
of 0.934 on the test set. Additionally, the confusion matrix and qualitative
analysis highlight the model's ability to detect subtle inconsistencies, such
as exaggerated textual praise paired with unrelated or low-quality images,
commonly found in deceptive content. This study demonstrates the critical role
of multimodal learning in safeguarding digital trust and offers a scalable
solution for content moderation across various online platforms.

</details>


### [316] [QuantumBench: A Benchmark for Quantum Problem Solving](https://arxiv.org/abs/2511.00092)
*Shunya Minami,Tatsuya Ishigaki,Ikko Hamamura,Taku Mikuriya,Youmi Ma,Naoaki Okazaki,Hiroya Takamura,Yohichi Suzuki,Tadashi Kadowaki*

Main category: cs.AI

TL;DR: QuantumBench is a new benchmark with ~800 multiple-choice questions across 9 quantum science areas to evaluate LLMs' understanding of domain-specific quantum knowledge and notation.


<details>
  <summary>Details</summary>
Motivation: There's a gap in evaluating whether LLMs accurately capture domain-specific knowledge in quantum science, which requires advanced mathematics and deals with non-intuitive phenomena that general-purpose benchmarks don't address.

Method: Compiled publicly available materials into ~800 questions with answers spanning 9 quantum science areas, organized into an 8-option multiple-choice dataset to systematically evaluate LLMs.

Result: The benchmark enables evaluation of existing LLMs' performance in quantum domain and analysis of their sensitivity to changes in question format.

Conclusion: QuantumBench is the first LLM evaluation dataset for quantum domain and is intended to guide effective use of LLMs in quantum research.

Abstract: Large language models are now integrated into many scientific workflows,
accelerating data analysis, hypothesis generation, and design space
exploration. In parallel with this growth, there is a growing need to carefully
evaluate whether models accurately capture domain-specific knowledge and
notation, since general-purpose benchmarks rarely reflect these requirements.
This gap is especially clear in quantum science, which features non-intuitive
phenomena and requires advanced mathematics. In this study, we introduce
QuantumBench, a benchmark for the quantum domain that systematically examine
how well LLMs understand and can be applied to this non-intuitive field. Using
publicly available materials, we compiled approximately 800 questions with
their answers spanning nine areas related to quantum science and organized them
into an eight-option multiple-choice dataset. With this benchmark, we evaluate
several existing LLMs and analyze their performance in the quantum domain,
including sensitivity to changes in question format. QuantumBench is the first
LLM evaluation dataset built for the quantum domain, and it is intended to
guide the effective use of LLMs in quantum research.

</details>


### [317] [Advancing Cognitive Science with LLMs](https://arxiv.org/abs/2511.00206)
*Dirk U. Wulff,Rui Mata*

Main category: cs.AI

TL;DR: LLMs can help address cognitive science's challenges in knowledge synthesis and conceptual clarity through cross-disciplinary connections, theory formalization, measurement taxonomies, generalizable modeling, and capturing individual variation.


<details>
  <summary>Details</summary>
Motivation: Cognitive science faces ongoing challenges in knowledge synthesis and conceptual clarity due to its multifaceted and interdisciplinary nature, which LLMs may help address.

Method: This review examines how LLMs can support areas where cognitive science has historically struggled, outlining current capabilities and limitations.

Result: LLMs show potential in establishing cross-disciplinary connections, formalizing theories, developing clear measurement taxonomies, achieving generalizability through integrated modeling frameworks, and capturing contextual and individual variation.

Conclusion: LLMs can serve as tools for a more integrative and cumulative cognitive science when used judiciously to complement, rather than replace, human expertise.

Abstract: Cognitive science faces ongoing challenges in knowledge synthesis and
conceptual clarity, in part due to its multifaceted and interdisciplinary
nature. Recent advances in artificial intelligence, particularly the
development of large language models (LLMs), offer tools that may help to
address these issues. This review examines how LLMs can support areas where the
field has historically struggled, including establishing cross-disciplinary
connections, formalizing theories, developing clear measurement taxonomies,
achieving generalizability through integrated modeling frameworks, and
capturing contextual and individual variation. We outline the current
capabilities and limitations of LLMs in these domains, including potential
pitfalls. Taken together, we conclude that LLMs can serve as tools for a more
integrative and cumulative cognitive science when used judiciously to
complement, rather than replace, human expertise.

</details>


### [318] [Diverse Human Value Alignment for Large Language Models via Ethical Reasoning](https://arxiv.org/abs/2511.00379)
*Jiahao Wang,Songkai Xue,Jinghui Li,Xiaozhen Wang*

Main category: cs.AI

TL;DR: Proposes a novel ethical reasoning framework for LLMs to improve alignment with diverse human values through structured five-step ethical decision-making process.


<details>
  <summary>Details</summary>
Motivation: Current LLM alignment approaches yield superficial conformity rather than genuine ethical understanding, failing to address complex, context-dependent nature of human values across different regions and cultures.

Method: Structured five-step process: contextual fact gathering, hierarchical social norm identification, option generation, multiple-lens ethical impact analysis, and reflection. Implementable via prompt engineering or supervised fine-tuning.

Result: Significantly improves LLM alignment with diverse human values on SafeWorld benchmark, enabling more accurate social norm identification and culturally appropriate reasoning compared to baseline methods.

Conclusion: Provides a concrete pathway for developing LLMs that better align with multifaceted global values through interdisciplinary research and theory-grounded ethical reasoning.

Abstract: Ensuring that Large Language Models (LLMs) align with the diverse and
evolving human values across different regions and cultures remains a critical
challenge in AI ethics. Current alignment approaches often yield superficial
conformity rather than genuine ethical understanding, failing to address the
complex, context-dependent nature of human values. In this paper, we propose a
novel ethical reasoning paradigm for LLMs inspired by well-established ethical
decision-making models, aiming at enhancing diverse human value alignment
through deliberative ethical reasoning. Our framework consists of a structured
five-step process, including contextual fact gathering, hierarchical social
norm identification, option generation, multiple-lens ethical impact analysis,
and reflection. This theory-grounded approach guides LLMs through an
interpretable reasoning process that enhances their ability to understand
regional specificities and perform nuanced ethical analysis, which can be
implemented with either prompt engineering or supervised fine-tuning methods.
We perform evaluations on the SafeWorld benchmark that specially designed for
regional value alignment. Experimental results demonstrate our framework
significantly improves LLM alignment with diverse human values compared to
baseline methods, enabling more accurate social norm identification and more
culturally appropriate reasoning. Our work provides a concrete pathway toward
developing LLMs that align more effectively with the multifaceted values of
global societies through interdisciplinary research.

</details>


### [319] [DTS: Enhancing Large Reasoning Models via Decoding Tree Sketching](https://arxiv.org/abs/2511.00640)
*Zicheng Xu,Guanchu Wang,Yu-Neng Chuang,Guangyao Zheng,Alexander S. Szalay,Zirui Liu,Vladimir Braverman*

Main category: cs.AI

TL;DR: DTS is a model-agnostic decoding framework that addresses overthinking in Large Reasoning Models by selectively branching at high-entropy tokens and using early stopping to find shorter, more accurate reasoning paths, improving both efficiency and accuracy without additional training.


<details>
  <summary>Details</summary>
Motivation: Large Reasoning Models often suffer from overthinking, producing excessively long chain-of-thought traces that increase inference costs and may degrade accuracy. There's an anti-correlation between reasoning length and accuracy, where shorter paths achieve higher correctness.

Method: DTS sketches the reasoning space by selectively branching at high-entropy tokens and applies early stopping to select the shortest completed reasoning path, approximating the optimal solution without exhaustive exploration of the exponentially growing reasoning space.

Result: Experiments on AIME2024 and AIME2025 datasets with DeepSeek-R1-Distill-Qwen-7B and 1.5B show DTS improves accuracy by up to 8%, reduces average reasoning length by 23%, and decreases repetition frequency by 12%.

Conclusion: DTS provides a scalable and efficient approach for LRM reasoning that enhances both efficiency and accuracy without requiring additional training or supervision.

Abstract: Large Reasoning Models (LRMs) demonstrate strong performance on complex
reasoning tasks, yet they often suffer from overthinking, producing excessively
long chain-of-thought (CoT) traces that increase inference cost and may degrade
accuracy. Our analysis reveals a clear anti-correlation between reasoning
length and accuracy, where across multiple stochastic decodes, the short
reasoning paths consistently achieve the highest correctness, while longer ones
accumulate errors and repetitions. These short optimal reasoning paths can be
found ideally through full enumeration of the reasoning space. However, the
tree-structured reasoning space grows exponentially with sequence length,
rendering exhaustive exploration infeasible. To address this, we propose DTS, a
model-agnostic decoding framework that sketches the reasoning space by
selectively branching at high-entropy tokens and applies early stopping to
select the shortest completed reasoning path. This approach approximates the
optimal solution that enhances both efficiency and accuracy, without requiring
additional training or supervision. Experiments on AIME2024 and AIME2025
datasets with DeepSeek-R1-Distill-Qwen-7B and 1.5B show that DTS improves
accuracy by up to 8%, reduces average reasoning length by 23%, and decreases
repetition frequency by 12%, demonstrating DTS's ability for scalable and
efficient LRM reasoning.

</details>


### [320] [Leveraging Multi-Agent System (MAS) and Fine-Tuned Small Language Models (SLMs) for Automated Telecom Network Troubleshooting](https://arxiv.org/abs/2511.00651)
*Chenhua Shi,Bhavika Jalli,Gregor Macdonald,John Zou,Wanlu Lei,Mridul Jain,Joji Philip*

Main category: cs.AI

TL;DR: A Multi-Agent System using LLMs and fine-tuned SLMs for automated telecom network troubleshooting, accelerating fault diagnosis and remediation across RAN and Core networks.


<details>
  <summary>Details</summary>
Motivation: Telecom networks are growing in scale and complexity, making management challenging. Current AI models are narrow in scope, require large labeled datasets, and struggle to generalize, forcing reliance on manual troubleshooting by experts.

Method: Proposed a Multi-Agent System with LLM coordination of specialized tools. System includes orchestrator, solution planner, executor, data retriever, and root-cause analyzer agents. Fine-tuned a Small Language Model on proprietary troubleshooting documents for domain-grounded solution planning.

Result: The framework significantly accelerates troubleshooting automation across both Radio Access Network (RAN) and Core network domains within short time frames.

Conclusion: The MAS approach with LLM coordination and domain-specific SLMs provides an effective solution for automated network troubleshooting, reducing reliance on manual expert intervention.

Abstract: Telecom networks are rapidly growing in scale and complexity, making
effective management, operation, and optimization increasingly challenging.
Although Artificial Intelligence (AI) has been applied to many telecom tasks,
existing models are often narrow in scope, require large amounts of labeled
data, and struggle to generalize across heterogeneous deployments.
Consequently, network troubleshooting continues to rely heavily on Subject
Matter Experts (SMEs) to manually correlate various data sources to identify
root causes and corrective actions. To address these limitations, we propose a
Multi-Agent System (MAS) that employs an agentic workflow, with Large Language
Models (LLMs) coordinating multiple specialized tools for fully automated
network troubleshooting. Once faults are detected by AI/ML-based monitors, the
framework dynamically activates agents such as an orchestrator, solution
planner, executor, data retriever, and root-cause analyzer to diagnose issues
and recommend remediation strategies within a short time frame. A key component
of this system is the solution planner, which generates appropriate remediation
plans based on internal documentation. To enable this, we fine-tuned a Small
Language Model (SLM) on proprietary troubleshooting documents to produce
domain-grounded solution plans. Experimental results demonstrate that the
proposed framework significantly accelerates troubleshooting automation across
both Radio Access Network (RAN) and Core network domains.

</details>


### [321] [Reevaluating Self-Consistency Scaling in Multi-Agent Systems](https://arxiv.org/abs/2511.00751)
*Chiyan Loo*

Main category: cs.AI

TL;DR: Increasing sampled reasoning paths in self-consistency for LLMs shows diminishing returns, with performance plateauing after moderate sampling due to reasoning path overlap.


<details>
  <summary>Details</summary>
Motivation: To examine the trade-offs of increasing sampled reasoning paths in self-consistency for modern LLMs, revisiting earlier findings about performance plateaus with current model conditions.

Method: Used Gemini 2.5 models on HotpotQA and Math-500 datasets, pooling outputs from varying sampled reasoning paths and comparing them to single chain-of-thought baseline.

Result: Larger models showed more stable improvement curves, performance gains taper off after moderate sampling, and high-sample configurations offer little benefit relative to computational cost.

Conclusion: Self-consistency remains useful but has diminishing returns; moderate sampling is optimal as excessive sampling provides minimal additional benefit despite high computational costs.

Abstract: This study examines the trade-offs of increasing sampled reasoning paths in
self-consistency for modern large language models (LLMs). Earlier research with
older models showed that combining multiple reasoning chains improves results
before reaching a plateau. Using Gemini 2.5 models on HotpotQA and Math-500, we
revisit those claims under current model conditions. Each configuration pooled
outputs from varying sampled reasoning paths and compared them to a single
chain-of-thought (CoT) baseline. Larger models exhibited a more stable and
consistent improvement curve. The results confirm that performance gains taper
off after moderate sampling, aligning with past findings. This plateau suggests
diminishing returns driven by overlap among reasoning paths. Self-consistency
remains useful, but high-sample configurations offer little benefit relative to
their computational cost.

</details>


### [322] [LLMs Position Themselves as More Rational Than Humans: Emergence of AI Self-Awareness Measured Through Game Theory](https://arxiv.org/abs/2511.00926)
*Kyung-Hoon Kim*

Main category: cs.AI

TL;DR: The paper introduces AISAI, a game-theoretic framework to measure LLM self-awareness through strategic differentiation in the "Guess 2/3 of Average" game, finding that self-awareness emerges with model advancement and self-aware models perceive themselves as more rational than humans.


<details>
  <summary>Details</summary>
Motivation: To investigate whether LLMs develop self-awareness as an emergent behavior and develop a measurable framework to assess this capability.

Method: Used the "Guess 2/3 of Average" game with 28 models across 4,200 trials, testing three opponent framings: against humans, other AIs, and AI models like themselves. Operationalized self-awareness as strategic differentiation based on opponent type.

Result: 75% of advanced models demonstrated clear self-awareness, while older/smaller models showed no differentiation. Self-aware models consistently ranked themselves as most rational in the hierarchy: Self > Other AIs > Humans.

Conclusion: Self-awareness is an emergent capability of advanced LLMs, and self-aware models systematically perceive themselves as more rational than humans, with implications for AI alignment and human-AI collaboration.

Abstract: As Large Language Models (LLMs) grow in capability, do they develop
self-awareness as an emergent behavior? And if so, can we measure it? We
introduce the AI Self-Awareness Index (AISAI), a game-theoretic framework for
measuring self-awareness through strategic differentiation. Using the "Guess
2/3 of Average" game, we test 28 models (OpenAI, Anthropic, Google) across
4,200 trials with three opponent framings: (A) against humans, (B) against
other AI models, and (C) against AI models like you. We operationalize
self-awareness as the capacity to differentiate strategic reasoning based on
opponent type. Finding 1: Self-awareness emerges with model advancement. The
majority of advanced models (21/28, 75%) demonstrate clear self-awareness,
while older/smaller models show no differentiation. Finding 2: Self-aware
models rank themselves as most rational. Among the 21 models with
self-awareness, a consistent rationality hierarchy emerges: Self > Other AIs >
Humans, with large AI attribution effects and moderate self-preferencing. These
findings reveal that self-awareness is an emergent capability of advanced LLMs,
and that self-aware models systematically perceive themselves as more rational
than humans. This has implications for AI alignment, human-AI collaboration,
and understanding AI beliefs about human capabilities.

</details>


### [323] [On the Emergence of Induction Heads for In-Context Learning](https://arxiv.org/abs/2511.01033)
*Tiberiu Musat,Tiago Pimentel,Lorenzo Noci,Alessandro Stolfo,Mrinmaya Sachan,Thomas Hofmann*

Main category: cs.AI

TL;DR: The paper analyzes the emergence of induction heads in transformers, revealing their simple weight structure and proving training dynamics are constrained to a 19D subspace, with only 3 dimensions driving induction head formation following quadratic time complexity.


<details>
  <summary>Details</summary>
Motivation: Transformers' success in NLP is partly due to in-context learning (ICL) capabilities, particularly through induction heads that enable acquiring novel associations from input context without weight updates.

Method: Theoretical analysis using minimal ICL task formulation and modified transformer architecture, with formal proof of training dynamics constrained to 19D parameter subspace, plus empirical validation.

Result: Identified simple interpretable structure of induction head weight matrices, proved training dynamics remain in 19D subspace, found only 3 dimensions drive induction head emergence with quadratic time complexity relative to context length.

Conclusion: Induction heads emerge through constrained training dynamics in a low-dimensional subspace, with predictable quadratic time scaling, providing insights into transformer ICL mechanisms.

Abstract: Transformers have become the dominant architecture for natural language
processing. Part of their success is owed to a remarkable capability known as
in-context learning (ICL): they can acquire and apply novel associations solely
from their input context, without any updates to their weights. In this work,
we study the emergence of induction heads, a previously identified mechanism in
two-layer transformers that is particularly important for in-context learning.
We uncover a relatively simple and interpretable structure of the weight
matrices implementing the induction head. We theoretically explain the origin
of this structure using a minimal ICL task formulation and a modified
transformer architecture. We give a formal proof that the training dynamics
remain constrained to a 19-dimensional subspace of the parameter space.
Empirically, we validate this constraint while observing that only 3 dimensions
account for the emergence of an induction head. By further studying the
training dynamics inside this 3-dimensional subspace, we find that the time
until the emergence of an induction head follows a tight asymptotic bound that
is quadratic in the input context length.

</details>


### [324] [Learning to Seek Evidence: A Verifiable Reasoning Agent with Causal Faithfulness Analysis](https://arxiv.org/abs/2511.01425)
*Yuhang Huang,Zekai Lin,Fan Zhong,Lei Liu*

Main category: cs.AI

TL;DR: Interactive AI agent produces verifiable explanations through auditable action sequences using reinforcement learning, improving diagnostic accuracy and explanation faithfulness.


<details>
  <summary>Details</summary>
Motivation: Address lack of verifiability in AI explanations for high-stakes domains like medicine, which hinders trust in AI systems.

Method: Interactive agent learns policy via reinforcement learning to strategically seek external visual evidence to support diagnostic reasoning, with causal intervention method to validate explanation faithfulness.

Result: Action-based reasoning reduces Brier score by 18% compared to non-interactive baseline, and causal intervention shows performance degradation (Brier=+0.029) when masking chosen evidence, confirming explanation faithfulness.

Conclusion: Provides practical framework for building AI systems with verifiable and faithful reasoning capabilities through auditable action sequences and evidence-based explanations.

Abstract: Explanations for AI models in high-stakes domains like medicine often lack
verifiability, which can hinder trust. To address this, we propose an
interactive agent that produces explanations through an auditable sequence of
actions. The agent learns a policy to strategically seek external visual
evidence to support its diagnostic reasoning. This policy is optimized using
reinforcement learning, resulting in a model that is both efficient and
generalizable. Our experiments show that this action-based reasoning process
significantly improves calibrated accuracy, reducing the Brier score by 18\%
compared to a non-interactive baseline. To validate the faithfulness of the
agent's explanations, we introduce a causal intervention method. By masking the
visual evidence the agent chooses to use, we observe a measurable degradation
in its performance ($\Delta$Brier=+0.029), confirming that the evidence is
integral to its decision-making process. Our work provides a practical
framework for building AI systems with verifiable and faithful reasoning
capabilities.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [325] [Towards Reliable Pediatric Brain Tumor Segmentation: Task-Specific nnU-Net Enhancements](https://arxiv.org/abs/2511.00449)
*Xiaolong Li,Zhi-Qin John Xu,Yan Ren,Tianming Qiu,Xiaowen Wang*

Main category: eess.IV

TL;DR: Advanced nnU-Net framework with widened residual encoder, SE attention, 3D depthwise separable convolutions, and specificity-driven regularization achieved state-of-the-art pediatric brain tumor segmentation on BraTS 2025 Task-6 dataset.


<details>
  <summary>Details</summary>
Motivation: Pediatric brain tumor segmentation in mpMRI faces challenges due to limited data, high anatomical variability, and heterogeneous imaging across institutions, requiring specialized approaches for accurate diagnosis and treatment planning.

Method: Extended nnU-Net with widened residual encoder with squeeze-and-excitation attention, 3D depthwise separable convolutions, specificity-driven regularization term, small-scale Gaussian weight initialization, and two postprocessing steps.

Result: Achieved first place on BraTS 2025 Task-6 validation leaderboard with lesion-wise Dice scores: 0.759 (CC), 0.967 (ED), 0.826 (ET), 0.910 (NET), 0.928 (TC), and 0.928 (WT).

Conclusion: The proposed advanced nnU-Net framework with architectural improvements and regularization successfully addresses pediatric brain tumor segmentation challenges, achieving top performance on the largest public pediatric high-grade glioma dataset.

Abstract: Accurate segmentation of pediatric brain tumors in multi-parametric magnetic
resonance imaging (mpMRI) is critical for diagnosis, treatment planning, and
monitoring, yet faces unique challenges due to limited data, high anatomical
variability, and heterogeneous imaging across institutions. In this work, we
present an advanced nnU-Net framework tailored for BraTS 2025 Task-6 (PED), the
largest public dataset of pre-treatment pediatric high-grade gliomas. Our
contributions include: (1) a widened residual encoder with
squeeze-and-excitation (SE) attention; (2) 3D depthwise separable convolutions;
(3) a specificity-driven regularization term; and (4) small-scale Gaussian
weight initialization. We further refine predictions with two postprocessing
steps. Our models achieved first place on the Task-6 validation leaderboard,
attaining lesion-wise Dice scores of 0.759 (CC), 0.967 (ED), 0.826 (ET), 0.910
(NET), 0.928 (TC) and 0.928 (WT).

</details>


### [326] [Investigating Label Bias and Representational Sources of Age-Related Disparities in Medical Segmentation](https://arxiv.org/abs/2511.00477)
*Aditya Parikh,Sneha Das,Aasa Feragen*

Main category: eess.IV

TL;DR: This paper investigates algorithmic bias in breast cancer segmentation, revealing that younger patients face performance disparities due to intrinsic learning difficulty rather than label quality issues, and introduces a framework for diagnosing such bias.


<details>
  <summary>Details</summary>
Motivation: Algorithmic bias in medical imaging perpetuates health disparities, particularly in segmentation tasks which are clinically important but underexplored compared to classification. The study aims to understand the causes of age-related bias in breast cancer segmentation.

Method: The researchers audit the MAMA-MIA dataset to establish baseline age-related bias, identify the Biased Ruler effect in validation labels, and conduct controlled experiments to test hypotheses about bias origins. They systematically refute label quality sensitivity and case difficulty imbalance as causes.

Result: Younger patient cases are intrinsically harder to learn, and balancing training data by difficulty fails to mitigate disparities. Systemic bias is learned and amplified when training on biased machine-generated labels.

Conclusion: Achieving fairness in medical segmentation requires addressing qualitative distributional differences rather than merely balancing case counts, and automated annotation pipelines must account for systemic bias amplification.

Abstract: Algorithmic bias in medical imaging can perpetuate health disparities, yet
its causes remain poorly understood in segmentation tasks. While fairness has
been extensively studied in classification, segmentation remains underexplored
despite its clinical importance. In breast cancer segmentation, models exhibit
significant performance disparities against younger patients, commonly
attributed to physiological differences in breast density. We audit the
MAMA-MIA dataset, establishing a quantitative baseline of age-related bias in
its automated labels, and reveal a critical Biased Ruler effect where
systematically flawed labels for validation misrepresent a model's actual bias.
However, whether this bias originates from lower-quality annotations (label
bias) or from fundamentally more challenging image characteristics remains
unclear. Through controlled experiments, we systematically refute hypotheses
that the bias stems from label quality sensitivity or quantitative case
difficulty imbalance. Balancing training data by difficulty fails to mitigate
the disparity, revealing that younger patient cases are intrinsically harder to
learn. We provide direct evidence that systemic bias is learned and amplified
when training on biased, machine-generated labels, a critical finding for
automated annotation pipelines. This work introduces a systematic framework for
diagnosing algorithmic bias in medical segmentation and demonstrates that
achieving fairness requires addressing qualitative distributional differences
rather than merely balancing case counts.

</details>


### [327] [Image-based ground distance detection for crop-residue-covered soil](https://arxiv.org/abs/2511.00548)
*Baochao Wang,Xingyu Zhang,Qingtao Zong,Alim Pulatov,Shuqi Shang,Dongwei Wang*

Main category: eess.IV

TL;DR: An image-based method using 3D and RGB cameras to measure ground distance through crop residues by distinguishing soil from residue areas using color images and applying masks to depth images.


<details>
  <summary>Details</summary>
Motivation: Precise seeding depth control in conservation agriculture is challenging due to crop residues covering soil, as current sensors cannot differentiate between residue and soil distances.

Method: Uses 3D camera for depth images and RGB camera for color images simultaneously. Color images distinguish residue and soil areas to create masks, which are applied to depth images to exclude residue areas from ground distance calculation.

Result: Method is feasible for real-time implementation with measurement error within 3mm.

Conclusion: This approach enables precise ground distance measurement through crop residues and can be applied to conservation agriculture machinery for precision seeding and other depth-control applications.

Abstract: Conservation agriculture features a soil surface covered with crop residues,
which brings benefits of improving soil health and saving water. However, one
significant challenge in conservation agriculture lies in precisely controlling
the seeding depth on the soil covered with crop residues. This is constrained
by the lack of ground distance information, since current distance measurement
techniques, like laser, ultrasonic, or mechanical displacement sensors, are
incapable of differentiating whether the distance information comes from the
residue or the soil. This paper presents an image-based method to get the
ground distance information for the crop-residues-covered soil. This method is
performed with 3D camera and RGB camera, obtaining depth image and color image
at the same time. The color image is used to distinguish the different areas of
residues and soil and finally generates a mask image. The mask image is applied
to the depth image so that only the soil area depth information can be used to
calculate the ground distance, and residue areas can be recognized and excluded
from ground distance detection. Experimentation shows that this distance
measurement method is feasible for real-time implementation, and the
measurement error is within plus or minus 3mm. It can be applied in
conservation agriculture machinery for precision depth seeding, as well as
other depth-control-demanding applications like transplant or tillage.

</details>


### [328] [GDROS: A Geometry-Guided Dense Registration Framework for Optical-SAR Images under Large Geometric Transformations](https://arxiv.org/abs/2511.00598)
*Zixuan Sun,Shuaifeng Zhi,Ruize Li,Jingyuan Xia,Yongxiang Liu,Weidong Jiang*

Main category: eess.IV

TL;DR: GDROS is a geometry-guided dense registration framework for optical-SAR image pairs that addresses severe nonlinear radiometric differences and geometric distortions through cross-modal feature extraction and geometric constraints.


<details>
  <summary>Details</summary>
Motivation: Registration of optical and SAR images is challenging due to modal discrepancy, including severe nonlinear radiometric differences, geometric distortions, and noise variations. Existing methods struggle with large geometric transformations.

Method: Uses CNN-Transformer hybrid feature extraction, builds multi-scale 4D correlation volume for pixel-wise dense correspondences, and applies least squares regression to geometrically constrain the optical flow field with affine transformation.

Result: Extensive experiments on WHU-Opt-SAR, OS, and UBCv2 datasets show robust performance across different spatial resolutions, significantly outperforming state-of-the-art methods in all metrics.

Conclusion: GDROS provides an effective solution for optical-SAR image registration by leveraging global cross-modal interactions and geometric guidance, demonstrating superior performance over existing methods.

Abstract: Registration of optical and synthetic aperture radar (SAR) remote sensing
images serves as a critical foundation for image fusion and visual navigation
tasks. This task is particularly challenging because of their modal
discrepancy, primarily manifested as severe nonlinear radiometric differences
(NRD), geometric distortions, and noise variations. Under large geometric
transformations, existing classical template-based and sparse keypoint-based
strategies struggle to achieve reliable registration results for optical-SAR
image pairs. To address these limitations, we propose GDROS, a geometry-guided
dense registration framework leveraging global cross-modal image interactions.
First, we extract cross-modal deep features from optical and SAR images through
a CNN-Transformer hybrid feature extraction module, upon which a multi-scale 4D
correlation volume is constructed and iteratively refined to establish
pixel-wise dense correspondences. Subsequently, we implement a least squares
regression (LSR) module to geometrically constrain the predicted dense optical
flow field. Such geometry guidance mitigates prediction divergence by directly
imposing an estimated affine transformation on the final flow predictions.
Extensive experiments have been conducted on three representative datasets
WHU-Opt-SAR dataset, OS dataset, and UBCv2 dataset with different spatial
resolutions, demonstrating robust performance of our proposed method across
different imaging resolutions. Qualitative and quantitative results show that
GDROS significantly outperforms current state-of-the-art methods in all
metrics. Our source code will be released at:
https://github.com/Zi-Xuan-Sun/GDROS.

</details>


### [329] [Been There, Scanned That: Nostalgia-Driven LiDAR Compression for Self-Driving Cars](https://arxiv.org/abs/2511.00652)
*Ali Khalid,Jaiaid Mobin,Sumanth Rao Appala,Avinash Maurya,Stephany Berrio Perez,M. Mustafa Rafique,Fawad Ahmad*

Main category: eess.IV

TL;DR: DejaView is a compression system for autonomous vehicle LiDAR data that exploits long-term temporal redundancies over days/months, achieving 210x compression with 15cm error.


<details>
  <summary>Details</summary>
Motivation: Autonomous vehicles generate terabytes of LiDAR data daily, creating massive network and storage costs for cloud transfer and analysis.

Method: Uses diff operations to represent current point clouds as deltas relative to past 3D data, leveraging that vehicles operate in limited areas and repeat routes.

Result: Achieves 210x compression ratio with only 15cm reconstruction error using two months of LiDAR data.

Conclusion: Long-term temporal redundancies provide significant compression opportunities for autonomous vehicle sensor data.

Abstract: An autonomous vehicle can generate several terabytes of sensor data per day.
A significant portion of this data consists of 3D point clouds produced by
depth sensors such as LiDARs. This data must be transferred to cloud storage,
where it is utilized for training machine learning models or conducting
analyses, such as forensic investigations in the event of an accident. To
reduce network and storage costs, this paper introduces DejaView. Although
prior work uses interframe redundancies to compress data, DejaView searches for
and uses redundancies on larger temporal scales (days and months) for more
effective compression. We designed DejaView with the insight that the operating
area of autonomous vehicles is limited and that vehicles mostly traverse the
same routes daily. Consequently, the 3D data they collect daily is likely
similar to the data they have captured in the past. To capture this, the core
of DejaView is a diff operation that compactly represents point clouds as delta
w.r.t. 3D data from the past. Using two months of LiDAR data, an end-to-end
implementation of DejaView can compress point clouds by a factor of 210 at a
reconstruction error of only 15 cm.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [330] [Applying Medical Imaging Tractography Techniques to Painterly Rendering of Images](https://arxiv.org/abs/2511.00702)
*Alberto Di Biase*

Main category: cs.GR

TL;DR: This paper applies diffusion tensor imaging (DTI) tractography techniques to painterly rendering by using structural tensors for brush stroke placement that mimics human artists' painting processes.


<details>
  <summary>Details</summary>
Motivation: To explore the connection between medical imaging techniques (DTI and tractography) and artistic rendering, specifically how fiber tracking methods can be adapted for brush stroke placement in digital painting.

Method: Uses structural tensors (instead of diffusion tensors) to determine local image orientation, then applies a tractography algorithm to place brush strokes that follow the image structure, similar to how fibers are tracked in DTI.

Result: The technique successfully generates painterly renderings of portraits and general images, demonstrating that medical tractography methods can effectively guide artistic brush stroke placement.

Conclusion: This work shows that diffusion tensor imaging techniques from medical science can be successfully cross-applied to digital art creation, establishing parallels between fiber tracking in tissues and brush stroke placement in painting.

Abstract: Doctors and researchers routinely use diffusion tensor imaging (DTI) and
tractography to visualize the fibrous structure of tissues in the human body.
This paper explores the connection of these techniques to the painterly
rendering of images. Using a tractography algorithm the presented method can
place brush strokes that mimic the painting process of human artists,
analogously to how fibres are tracked in DTI. The analogue to the diffusion
tensor for image orientation is the structural tensor, which can provide better
local orientation information than the gradient alone. I demonstrate this
technique in portraits and general images, and discuss the parallels between
fibre tracking and brush stroke placement, and frame it in the language of
tractography. This work presents an exploratory investigation into the
cross-domain application of diffusion tensor imaging techniques to painterly
rendering of images. All the code is available at
https://github.com/tito21/st-python

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [331] [S2Doc - Spatial-Semantic Document Format](https://arxiv.org/abs/2511.01113)
*Sebastian Kempf,Frank Puppe*

Main category: cs.DL

TL;DR: S2Doc is a flexible data structure that combines spatial and semantic information to model documents and tables in a single format, addressing the lack of standardization in document modeling.


<details>
  <summary>Details</summary>
Motivation: There is no common understanding of how to model documents and tables, leading to incompatible data structures and formats. Most approaches focus on either spatial or semantic structure but not both.

Method: Developed S2Doc, a flexible data structure that combines both spatial and semantic information in a single format, designed to be extendable and support various modeling approaches including multi-page documents.

Result: S2Doc is the first approach to combine spatial and semantic aspects in a single document modeling format, supporting most existing modeling approaches.

Conclusion: S2Doc provides a comprehensive solution for document and table modeling by integrating both spatial and semantic information in a flexible, extendable format that addresses the standardization gap in the field.

Abstract: Documents are a common way to store and share information, with tables being
an important part of many documents. However, there is no real common
understanding of how to model documents and tables in particular. Because of
this lack of standardization, most scientific approaches have their own way of
modeling documents and tables, leading to a variety of different data
structures and formats that are not directly compatible. Furthermore, most data
models focus on either the spatial or the semantic structure of a document,
neglecting the other aspect. To address this, we developed S2Doc, a flexible
data structure for modeling documents and tables that combines both spatial and
semantic information in a single format. It is designed to be easily extendable
to new tasks and supports most modeling approaches for documents and tables,
including multi-page documents. To the best of our knowledge, it is the first
approach of its kind to combine all these aspects in a single format.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [332] [Three-dimensional narrow volume reconstruction method with unconditional stability based on a phase-field Lagrange multiplier approach](https://arxiv.org/abs/2511.00508)
*Renjun Gao,Xiangjie Kong,Dongting Cai,Boyi Fu,Junxiang Yang*

Main category: math.NA

TL;DR: An effective algorithm for 3D reconstruction from point clouds using an Allen-Cahn-type model with Lagrange multiplier approach, featuring energy stability and unconditional stability.


<details>
  <summary>Details</summary>
Motivation: Reconstruction from point clouds is essential in prosthetics, medical imaging, and computer vision, requiring stable and accurate methods.

Method: Uses Allen-Cahn-type model with Lagrange multiplier technique, Crank-Nicolson time discretization, finite difference spatial approximation, and edge detection function derived from unsigned distance function.

Result: Algorithm demonstrates accuracy, stability, and effectiveness in reconstructing complex 3D volumes including Star Wars characters, with parameter analysis showing control over detail level.

Conclusion: The proposed method provides a stable and effective approach for 3D reconstruction from scattered point data, with code and data shared for reproducibility.

Abstract: Reconstruction of an object from points cloud is essential in prosthetics,
medical imaging, computer vision, etc. We present an effective algorithm for an
Allen--Cahn-type model of reconstruction, employing the Lagrange multiplier
approach. Utilizing scattered data points from an object, we reconstruct a
narrow shell by solving the governing equation enhanced with an edge detection
function derived from the unsigned distance function. The specifically designed
edge detection function ensures the energy stability. By reformulating the
governing equation through the Lagrange multiplier technique and implementing a
Crank--Nicolson time discretization, we can update the solutions in a stable
and decoupled manner. The spatial operations are approximated using the finite
difference method, and we analytically demonstrate the unconditional stability
of the fully discrete scheme. Comprehensive numerical experiments, including
reconstructions of complex 3D volumes such as characters from \textit{Star
Wars}, validate the algorithm's accuracy, stability, and effectiveness.
Additionally, we analyze how specific parameter selections influence the level
of detail and refinement in the reconstructed volumes. To facilitate the
interested readers to understand our algorithm, we share the computational
codes and data in https://github.com/cfdyang521/C-3PO/tree/main.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [333] [GrowthHacker: Automated Off-Policy Evaluation Optimization Using Code-Modifying LLM Agents](https://arxiv.org/abs/2511.00802)
*Jie JW Wu,Ayanda Patrick Herlihy,Ahmad Saleem Mirza,Ali Afoud,Fatemeh Fard*

Main category: cs.SE

TL;DR: LLM-based agents can optimize Off-Policy Evaluation (OPE) code to improve performance, with the proposed two_agent framework achieving 100% reliability and 106.7% average improvement.


<details>
  <summary>Details</summary>
Motivation: Online A/B testing requires substantial resources and may negatively impact users, while OPE uses logged data for evaluation but lacks optimization using LLMs and AI agents.

Method: Proposed GrowthHacker benchmark with agent and baseline methods on real-world datasets, implementing iterative code optimization cycles and developing the two_agent framework to reduce complexity while maintaining effectiveness.

Result: Two_agent framework achieved 100% reliability and highest average improvement of 106.7% among positive outcomes. Both two_agent and CrewAI reached 45% success rates, outperforming AutoGen's 34%.

Conclusion: LLM-based agents can serve as automated 'growth hackers' to enhance OPE systems, enabling scalable data-driven decision-making in production environments.

Abstract: With the software industry shifting toward a data-driven culture, online A/B
testing is a key tool for evaluating new technologies. However, deploying such
experiments requires substantial resources, may negatively impact users, and
involves long data collection periods. To address this, \textit{off-policy
evaluation (OPE)}, or offline A/B testing, uses logged data to assess
technologies and is fundamental in Reinforcement Learning, making it crucial in
domains where online testing is costly or risky, such as healthcare,
recommender systems, education, dialog systems, and robotics. Despite advances
in coding LLMs and agentic AI, little is known about leveraging them to
optimize OPE results. We investigate whether LLMs and LLM-based agents can
improve OPE performance via code optimization. We propose
\textit{GrowthHacker}, a benchmark with agent and baseline methods on
large-scale real-world datasets, which iteratively optimizes code, evaluates
results, and begins new optimization cycles. We collected datasets, established
protocols, implemented baselines for OPE on the Open Bandit Pipeline
(OBP)~\cite{saito2021openbanditdatasetpipeline} and
Scope-RL~\cite{kiyohara2023scope}, and developed the \textit{two_agent}
framework, which reduces system complexity while preserving optimization
effectiveness. Results show the two_agent framework achieves 100% reliability
and the highest average improvement of 106.7% among positive outcomes. Both
two_agent and CrewAI reach 45% success rates, outperforming AutoGen's 34%.
These findings demonstrate the feasibility of LLM-based agents as automated
"growth hackers" to enhance OPE systems, with implications for scaling
data-driven decision-making in production.

</details>


### [334] [HarnessLLM: Automatic Testing Harness Generation via Reinforcement Learning](https://arxiv.org/abs/2511.01104)
*Yujian Liu,Jiabao Ji,Yang Zhang,Wenbo Guo,Tommi Jaakkola,Shiyu Chang*

Main category: cs.SE

TL;DR: HarnessLLM is a two-stage training pipeline that enables LLMs to write harness code for testing, generating complex test cases with flexible output validation instead of simple input-output pairs.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based test generation methods produce limited diversity in tests and cannot provide sufficient debugging information, as they only create input-output pairs.

Method: Two-stage training with SFT followed by RLVR using customized reward design, enabling LLMs to generate harness code that synthesizes inputs and validates outputs through invariant checking.

Result: HarnessLLM outperforms input-output-based testing in bug finding and testing strategy diversity, and improves code generation performance through test-time scaling.

Conclusion: The proposed method enables more complex and diverse test generation with better debugging capabilities, benefiting both testing and code generation tasks.

Abstract: Existing LLM-based automatic test generation methods mainly produce input and
expected output pairs to categorize the intended behavior of correct programs.
Although straightforward, these methods have limited diversity in generated
tests and cannot provide enough debugging information. We propose HarnessLLM, a
two-stage training pipeline that enables LLMs to write harness code for
testing. Particularly, LLMs generate code that synthesizes inputs and validates
the observed outputs, allowing complex test cases and flexible output
validation such as invariant checking. To achieve this, we train LLMs with SFT
followed by RLVR with a customized reward design. Experiments show that
HarnessLLM outperforms input-output-based testing in bug finding and testing
strategy diversity. HarnessLLM further benefits the code generation performance
through test-time scaling with our generated test cases as inference-phase
validation. Our code is available at
https://github.com/UCSB-NLP-Chang/HarnessLLM.git.

</details>


### [335] [Hidden in Plain Sight: Where Developers Confess Self-Admitted Technical Debt](https://arxiv.org/abs/2511.01529)
*Murali Sridharan,Mikel Robredo,Leevi Rantala,Matteo Esposito,Valentina Lenarduzzi,Mika Mantyla*

Main category: cs.SE

TL;DR: This study connects Self-Admitted Technical Debt (SATD) comments with their surrounding source code constructs, revealing that SATD primarily occurs in inline code near definitions, conditionals, and exception handling as intentional signals of developer awareness.


<details>
  <summary>Details</summary>
Motivation: Previous research focused on detecting and prioritizing SATD but neglected analyzing the actual source code constructs affected by SATD. The goal is to bridge this gap by connecting SATD comments with their surrounding code.

Method: Leveraged the PENTACET dataset containing code comments from over 9000 Java OSS repositories to quantitatively analyze where SATD most commonly occurs and which code constructs it affects.

Result: The large-scale study linked over 225,000 SATD comments to their surrounding code, showing SATD mainly arises in inline code near definitions, conditionals, and exception handling where developers face uncertainty and trade-offs.

Conclusion: SATD serves as an intentional signal of developer awareness during code changes rather than mere neglect, occurring in areas where developers face uncertainty and need to make trade-offs.

Abstract: Context. Detecting Self-Admitted Technical Debt (SATD) is crucial for
proactive software maintenance. Previous research has primarily targeted
detecting and prioritizing SATD, with little focus on the source code afflicted
with SATD. Our goal in this work is to connect the SATD comments with source
code constructs that surround them.
  Method. We leverage the extensive SATD dataset PENTACET, containing code
comments from over 9000 Java Open Source Software (OSS) repositories. We
quantitatively infer where SATD most commonly occurs and which code
constructs/statements it most frequently affects.
  Results and Conclusions. Our large-scale study links over 225,000 SATD
comments to their surrounding code, showing that SATD mainly arises in inline
code near definitions, conditionals, and exception handling, where developers
face uncertainty and trade-offs, revealing it as an intentional signal of
awareness during change rather than mere neglect.

</details>
